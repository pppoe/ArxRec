<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250929.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth\n  Estimation and Bidirectional Warping", "author": "Yu Ma and Guoliang Wei and Yue Cheng", "abstract": "  Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D\nreconstruction, typically suffering from overfitting, geometric distortion, and\nincomplete scene recovery due to limited multi-view constraints. Although 3D\nGaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it\nsuffers from floating artifacts and structural inconsistencies under\nsparse-input settings. To address these issues, we propose DWGS, a novel\nunified framework that enhances 3DGS for sparse-view synthesis by integrating\nrobust structural cues, virtual view constraints, and occluded region\ncompletion. Our approach introduces three principal contributions: a\nHybrid-Loss Depth Estimation module that leverages dense matching priors with\nreprojection, point propagation, and smoothness constraints to enforce\nmulti-view consistency; a Bidirectional Warping Virtual View Synthesis method\ngenerates virtual training views to impose stronger geometric and photometric\nconstraints; and an Occlusion-Aware Reconstruction component that utilizes\ndepth-difference mask and a learning-based inpainting model to recover obscured\nregions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU)\nshow that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR\nand 0.189 LPIPS, while retaining real-time inference capabilities.\n", "link": "http://arxiv.org/abs/2509.24893v1", "date": "2025-09-29", "relevancy": 3.3815, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6977}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6756}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DWGS%3A%20Enhancing%20Sparse-View%20Gaussian%20Splatting%20with%20Hybrid-Loss%20Depth%0A%20%20Estimation%20and%20Bidirectional%20Warping&body=Title%3A%20DWGS%3A%20Enhancing%20Sparse-View%20Gaussian%20Splatting%20with%20Hybrid-Loss%20Depth%0A%20%20Estimation%20and%20Bidirectional%20Warping%0AAuthor%3A%20Yu%20Ma%20and%20Guoliang%20Wei%20and%20Yue%20Cheng%0AAbstract%3A%20%20%20Novel%20View%20Synthesis%20%28NVS%29%20from%20sparse%20views%20remains%20a%20core%20challenge%20in%203D%0Areconstruction%2C%20typically%20suffering%20from%20overfitting%2C%20geometric%20distortion%2C%20and%0Aincomplete%20scene%20recovery%20due%20to%20limited%20multi-view%20constraints.%20Although%203D%0AGaussian%20Splatting%20%283DGS%29%20enables%20real-time%2C%20high-fidelity%20rendering%2C%20it%0Asuffers%20from%20floating%20artifacts%20and%20structural%20inconsistencies%20under%0Asparse-input%20settings.%20To%20address%20these%20issues%2C%20we%20propose%20DWGS%2C%20a%20novel%0Aunified%20framework%20that%20enhances%203DGS%20for%20sparse-view%20synthesis%20by%20integrating%0Arobust%20structural%20cues%2C%20virtual%20view%20constraints%2C%20and%20occluded%20region%0Acompletion.%20Our%20approach%20introduces%20three%20principal%20contributions%3A%20a%0AHybrid-Loss%20Depth%20Estimation%20module%20that%20leverages%20dense%20matching%20priors%20with%0Areprojection%2C%20point%20propagation%2C%20and%20smoothness%20constraints%20to%20enforce%0Amulti-view%20consistency%3B%20a%20Bidirectional%20Warping%20Virtual%20View%20Synthesis%20method%0Agenerates%20virtual%20training%20views%20to%20impose%20stronger%20geometric%20and%20photometric%0Aconstraints%3B%20and%20an%20Occlusion-Aware%20Reconstruction%20component%20that%20utilizes%0Adepth-difference%20mask%20and%20a%20learning-based%20inpainting%20model%20to%20recover%20obscured%0Aregions.%20Extensive%20experiments%20on%20standard%20benchmarks%20%28LLFF%2C%20Blender%2C%20and%20DTU%29%0Ashow%20that%20DWGS%20achieves%20a%20new%20state-of-the-art%2C%20achieving%20up%20to%2021.13%20dB%20PSNR%0Aand%200.189%20LPIPS%2C%20while%20retaining%20real-time%20inference%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDWGS%253A%2520Enhancing%2520Sparse-View%2520Gaussian%2520Splatting%2520with%2520Hybrid-Loss%2520Depth%250A%2520%2520Estimation%2520and%2520Bidirectional%2520Warping%26entry.906535625%3DYu%2520Ma%2520and%2520Guoliang%2520Wei%2520and%2520Yue%2520Cheng%26entry.1292438233%3D%2520%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%2520from%2520sparse%2520views%2520remains%2520a%2520core%2520challenge%2520in%25203D%250Areconstruction%252C%2520typically%2520suffering%2520from%2520overfitting%252C%2520geometric%2520distortion%252C%2520and%250Aincomplete%2520scene%2520recovery%2520due%2520to%2520limited%2520multi-view%2520constraints.%2520Although%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520real-time%252C%2520high-fidelity%2520rendering%252C%2520it%250Asuffers%2520from%2520floating%2520artifacts%2520and%2520structural%2520inconsistencies%2520under%250Asparse-input%2520settings.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520DWGS%252C%2520a%2520novel%250Aunified%2520framework%2520that%2520enhances%25203DGS%2520for%2520sparse-view%2520synthesis%2520by%2520integrating%250Arobust%2520structural%2520cues%252C%2520virtual%2520view%2520constraints%252C%2520and%2520occluded%2520region%250Acompletion.%2520Our%2520approach%2520introduces%2520three%2520principal%2520contributions%253A%2520a%250AHybrid-Loss%2520Depth%2520Estimation%2520module%2520that%2520leverages%2520dense%2520matching%2520priors%2520with%250Areprojection%252C%2520point%2520propagation%252C%2520and%2520smoothness%2520constraints%2520to%2520enforce%250Amulti-view%2520consistency%253B%2520a%2520Bidirectional%2520Warping%2520Virtual%2520View%2520Synthesis%2520method%250Agenerates%2520virtual%2520training%2520views%2520to%2520impose%2520stronger%2520geometric%2520and%2520photometric%250Aconstraints%253B%2520and%2520an%2520Occlusion-Aware%2520Reconstruction%2520component%2520that%2520utilizes%250Adepth-difference%2520mask%2520and%2520a%2520learning-based%2520inpainting%2520model%2520to%2520recover%2520obscured%250Aregions.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520%2528LLFF%252C%2520Blender%252C%2520and%2520DTU%2529%250Ashow%2520that%2520DWGS%2520achieves%2520a%2520new%2520state-of-the-art%252C%2520achieving%2520up%2520to%252021.13%2520dB%2520PSNR%250Aand%25200.189%2520LPIPS%252C%2520while%2520retaining%2520real-time%2520inference%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DWGS%3A%20Enhancing%20Sparse-View%20Gaussian%20Splatting%20with%20Hybrid-Loss%20Depth%0A%20%20Estimation%20and%20Bidirectional%20Warping&entry.906535625=Yu%20Ma%20and%20Guoliang%20Wei%20and%20Yue%20Cheng&entry.1292438233=%20%20Novel%20View%20Synthesis%20%28NVS%29%20from%20sparse%20views%20remains%20a%20core%20challenge%20in%203D%0Areconstruction%2C%20typically%20suffering%20from%20overfitting%2C%20geometric%20distortion%2C%20and%0Aincomplete%20scene%20recovery%20due%20to%20limited%20multi-view%20constraints.%20Although%203D%0AGaussian%20Splatting%20%283DGS%29%20enables%20real-time%2C%20high-fidelity%20rendering%2C%20it%0Asuffers%20from%20floating%20artifacts%20and%20structural%20inconsistencies%20under%0Asparse-input%20settings.%20To%20address%20these%20issues%2C%20we%20propose%20DWGS%2C%20a%20novel%0Aunified%20framework%20that%20enhances%203DGS%20for%20sparse-view%20synthesis%20by%20integrating%0Arobust%20structural%20cues%2C%20virtual%20view%20constraints%2C%20and%20occluded%20region%0Acompletion.%20Our%20approach%20introduces%20three%20principal%20contributions%3A%20a%0AHybrid-Loss%20Depth%20Estimation%20module%20that%20leverages%20dense%20matching%20priors%20with%0Areprojection%2C%20point%20propagation%2C%20and%20smoothness%20constraints%20to%20enforce%0Amulti-view%20consistency%3B%20a%20Bidirectional%20Warping%20Virtual%20View%20Synthesis%20method%0Agenerates%20virtual%20training%20views%20to%20impose%20stronger%20geometric%20and%20photometric%0Aconstraints%3B%20and%20an%20Occlusion-Aware%20Reconstruction%20component%20that%20utilizes%0Adepth-difference%20mask%20and%20a%20learning-based%20inpainting%20model%20to%20recover%20obscured%0Aregions.%20Extensive%20experiments%20on%20standard%20benchmarks%20%28LLFF%2C%20Blender%2C%20and%20DTU%29%0Ashow%20that%20DWGS%20achieves%20a%20new%20state-of-the-art%2C%20achieving%20up%20to%2021.13%20dB%20PSNR%0Aand%200.189%20LPIPS%2C%20while%20retaining%20real-time%20inference%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24893v1&entry.124074799=Read"},
{"title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles", "author": "Jan Held and Renaud Vandeghen and Sanghyun Son and Daniel Rebain and Matheus Gadelha and Yi Zhou and Ming C. Lin and Marc Van Droogenbroeck and Andrea Tagliasacchi", "abstract": "  Reconstructing 3D scenes and synthesizing novel views has seen rapid progress\nin recent years. Neural Radiance Fields demonstrated that continuous volumetric\nradiance fields can achieve high-quality image synthesis, but their long\ntraining and rendering times limit practicality. 3D Gaussian Splatting (3DGS)\naddressed these issues by representing scenes with millions of Gaussians,\nenabling real-time rendering and fast optimization. However, Gaussian\nprimitives are not natively compatible with the mesh-based pipelines used in VR\nheadsets, and real-time graphics applications. Existing solutions attempt to\nconvert Gaussians into meshes through post-processing or two-stage pipelines,\nwhich increases complexity and degrades visual quality. In this work, we\nintroduce Triangle Splatting+, which directly optimizes triangles, the\nfundamental primitive of computer graphics, within a differentiable splatting\nframework. We formulate triangle parametrization to enable connectivity through\nshared vertices, and we design a training strategy that enforces opaque\ntriangles. The final output is immediately usable in standard graphics engines\nwithout post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples\ndatasets show that Triangle Splatting+achieves state-of-the-art performance in\nmesh-based novel view synthesis. Our method surpasses prior splatting\napproaches in visual fidelity while remaining efficient and fast to training.\nMoreover, the resulting semi-connected meshes support downstream applications\nsuch as physics-based simulation or interactive walkthroughs. The project page\nis https://trianglesplatting2.github.io/trianglesplatting2/.\n", "link": "http://arxiv.org/abs/2509.25122v1", "date": "2025-09-29", "relevancy": 3.2881, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6952}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6401}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triangle%20Splatting%2B%3A%20Differentiable%20Rendering%20with%20Opaque%20Triangles&body=Title%3A%20Triangle%20Splatting%2B%3A%20Differentiable%20Rendering%20with%20Opaque%20Triangles%0AAuthor%3A%20Jan%20Held%20and%20Renaud%20Vandeghen%20and%20Sanghyun%20Son%20and%20Daniel%20Rebain%20and%20Matheus%20Gadelha%20and%20Yi%20Zhou%20and%20Ming%20C.%20Lin%20and%20Marc%20Van%20Droogenbroeck%20and%20Andrea%20Tagliasacchi%0AAbstract%3A%20%20%20Reconstructing%203D%20scenes%20and%20synthesizing%20novel%20views%20has%20seen%20rapid%20progress%0Ain%20recent%20years.%20Neural%20Radiance%20Fields%20demonstrated%20that%20continuous%20volumetric%0Aradiance%20fields%20can%20achieve%20high-quality%20image%20synthesis%2C%20but%20their%20long%0Atraining%20and%20rendering%20times%20limit%20practicality.%203D%20Gaussian%20Splatting%20%283DGS%29%0Aaddressed%20these%20issues%20by%20representing%20scenes%20with%20millions%20of%20Gaussians%2C%0Aenabling%20real-time%20rendering%20and%20fast%20optimization.%20However%2C%20Gaussian%0Aprimitives%20are%20not%20natively%20compatible%20with%20the%20mesh-based%20pipelines%20used%20in%20VR%0Aheadsets%2C%20and%20real-time%20graphics%20applications.%20Existing%20solutions%20attempt%20to%0Aconvert%20Gaussians%20into%20meshes%20through%20post-processing%20or%20two-stage%20pipelines%2C%0Awhich%20increases%20complexity%20and%20degrades%20visual%20quality.%20In%20this%20work%2C%20we%0Aintroduce%20Triangle%20Splatting%2B%2C%20which%20directly%20optimizes%20triangles%2C%20the%0Afundamental%20primitive%20of%20computer%20graphics%2C%20within%20a%20differentiable%20splatting%0Aframework.%20We%20formulate%20triangle%20parametrization%20to%20enable%20connectivity%20through%0Ashared%20vertices%2C%20and%20we%20design%20a%20training%20strategy%20that%20enforces%20opaque%0Atriangles.%20The%20final%20output%20is%20immediately%20usable%20in%20standard%20graphics%20engines%0Awithout%20post-processing.%20Experiments%20on%20the%20Mip-NeRF360%20and%20Tanks%20%26%20Temples%0Adatasets%20show%20that%20Triangle%20Splatting%2Bachieves%20state-of-the-art%20performance%20in%0Amesh-based%20novel%20view%20synthesis.%20Our%20method%20surpasses%20prior%20splatting%0Aapproaches%20in%20visual%20fidelity%20while%20remaining%20efficient%20and%20fast%20to%20training.%0AMoreover%2C%20the%20resulting%20semi-connected%20meshes%20support%20downstream%20applications%0Asuch%20as%20physics-based%20simulation%20or%20interactive%20walkthroughs.%20The%20project%20page%0Ais%20https%3A//trianglesplatting2.github.io/trianglesplatting2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriangle%2520Splatting%252B%253A%2520Differentiable%2520Rendering%2520with%2520Opaque%2520Triangles%26entry.906535625%3DJan%2520Held%2520and%2520Renaud%2520Vandeghen%2520and%2520Sanghyun%2520Son%2520and%2520Daniel%2520Rebain%2520and%2520Matheus%2520Gadelha%2520and%2520Yi%2520Zhou%2520and%2520Ming%2520C.%2520Lin%2520and%2520Marc%2520Van%2520Droogenbroeck%2520and%2520Andrea%2520Tagliasacchi%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520scenes%2520and%2520synthesizing%2520novel%2520views%2520has%2520seen%2520rapid%2520progress%250Ain%2520recent%2520years.%2520Neural%2520Radiance%2520Fields%2520demonstrated%2520that%2520continuous%2520volumetric%250Aradiance%2520fields%2520can%2520achieve%2520high-quality%2520image%2520synthesis%252C%2520but%2520their%2520long%250Atraining%2520and%2520rendering%2520times%2520limit%2520practicality.%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Aaddressed%2520these%2520issues%2520by%2520representing%2520scenes%2520with%2520millions%2520of%2520Gaussians%252C%250Aenabling%2520real-time%2520rendering%2520and%2520fast%2520optimization.%2520However%252C%2520Gaussian%250Aprimitives%2520are%2520not%2520natively%2520compatible%2520with%2520the%2520mesh-based%2520pipelines%2520used%2520in%2520VR%250Aheadsets%252C%2520and%2520real-time%2520graphics%2520applications.%2520Existing%2520solutions%2520attempt%2520to%250Aconvert%2520Gaussians%2520into%2520meshes%2520through%2520post-processing%2520or%2520two-stage%2520pipelines%252C%250Awhich%2520increases%2520complexity%2520and%2520degrades%2520visual%2520quality.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520Triangle%2520Splatting%252B%252C%2520which%2520directly%2520optimizes%2520triangles%252C%2520the%250Afundamental%2520primitive%2520of%2520computer%2520graphics%252C%2520within%2520a%2520differentiable%2520splatting%250Aframework.%2520We%2520formulate%2520triangle%2520parametrization%2520to%2520enable%2520connectivity%2520through%250Ashared%2520vertices%252C%2520and%2520we%2520design%2520a%2520training%2520strategy%2520that%2520enforces%2520opaque%250Atriangles.%2520The%2520final%2520output%2520is%2520immediately%2520usable%2520in%2520standard%2520graphics%2520engines%250Awithout%2520post-processing.%2520Experiments%2520on%2520the%2520Mip-NeRF360%2520and%2520Tanks%2520%2526%2520Temples%250Adatasets%2520show%2520that%2520Triangle%2520Splatting%252Bachieves%2520state-of-the-art%2520performance%2520in%250Amesh-based%2520novel%2520view%2520synthesis.%2520Our%2520method%2520surpasses%2520prior%2520splatting%250Aapproaches%2520in%2520visual%2520fidelity%2520while%2520remaining%2520efficient%2520and%2520fast%2520to%2520training.%250AMoreover%252C%2520the%2520resulting%2520semi-connected%2520meshes%2520support%2520downstream%2520applications%250Asuch%2520as%2520physics-based%2520simulation%2520or%2520interactive%2520walkthroughs.%2520The%2520project%2520page%250Ais%2520https%253A//trianglesplatting2.github.io/trianglesplatting2/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triangle%20Splatting%2B%3A%20Differentiable%20Rendering%20with%20Opaque%20Triangles&entry.906535625=Jan%20Held%20and%20Renaud%20Vandeghen%20and%20Sanghyun%20Son%20and%20Daniel%20Rebain%20and%20Matheus%20Gadelha%20and%20Yi%20Zhou%20and%20Ming%20C.%20Lin%20and%20Marc%20Van%20Droogenbroeck%20and%20Andrea%20Tagliasacchi&entry.1292438233=%20%20Reconstructing%203D%20scenes%20and%20synthesizing%20novel%20views%20has%20seen%20rapid%20progress%0Ain%20recent%20years.%20Neural%20Radiance%20Fields%20demonstrated%20that%20continuous%20volumetric%0Aradiance%20fields%20can%20achieve%20high-quality%20image%20synthesis%2C%20but%20their%20long%0Atraining%20and%20rendering%20times%20limit%20practicality.%203D%20Gaussian%20Splatting%20%283DGS%29%0Aaddressed%20these%20issues%20by%20representing%20scenes%20with%20millions%20of%20Gaussians%2C%0Aenabling%20real-time%20rendering%20and%20fast%20optimization.%20However%2C%20Gaussian%0Aprimitives%20are%20not%20natively%20compatible%20with%20the%20mesh-based%20pipelines%20used%20in%20VR%0Aheadsets%2C%20and%20real-time%20graphics%20applications.%20Existing%20solutions%20attempt%20to%0Aconvert%20Gaussians%20into%20meshes%20through%20post-processing%20or%20two-stage%20pipelines%2C%0Awhich%20increases%20complexity%20and%20degrades%20visual%20quality.%20In%20this%20work%2C%20we%0Aintroduce%20Triangle%20Splatting%2B%2C%20which%20directly%20optimizes%20triangles%2C%20the%0Afundamental%20primitive%20of%20computer%20graphics%2C%20within%20a%20differentiable%20splatting%0Aframework.%20We%20formulate%20triangle%20parametrization%20to%20enable%20connectivity%20through%0Ashared%20vertices%2C%20and%20we%20design%20a%20training%20strategy%20that%20enforces%20opaque%0Atriangles.%20The%20final%20output%20is%20immediately%20usable%20in%20standard%20graphics%20engines%0Awithout%20post-processing.%20Experiments%20on%20the%20Mip-NeRF360%20and%20Tanks%20%26%20Temples%0Adatasets%20show%20that%20Triangle%20Splatting%2Bachieves%20state-of-the-art%20performance%20in%0Amesh-based%20novel%20view%20synthesis.%20Our%20method%20surpasses%20prior%20splatting%0Aapproaches%20in%20visual%20fidelity%20while%20remaining%20efficient%20and%20fast%20to%20training.%0AMoreover%2C%20the%20resulting%20semi-connected%20meshes%20support%20downstream%20applications%0Asuch%20as%20physics-based%20simulation%20or%20interactive%20walkthroughs.%20The%20project%20page%0Ais%20https%3A//trianglesplatting2.github.io/trianglesplatting2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25122v1&entry.124074799=Read"},
{"title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers", "author": "Tooba Imtiaz and Lucy Chai and Kathryn Heal and Xuan Luo and Jungyeon Park and Jennifer Dy and John Flynn", "abstract": "  Large transformer models are proving to be a powerful tool for 3D vision and\nnovel view synthesis. However, the standard Transformer's well-known quadratic\ncomplexity makes it difficult to scale these methods to large scenes. To\naddress this challenge, we propose the Local View Transformer (LVT), a\nlarge-scale scene reconstruction and novel view synthesis architecture that\ncircumvents the need for the quadratic attention operation. Motivated by the\ninsight that spatially nearby views provide more useful signal about the local\nscene composition than distant views, our model processes all information in a\nlocal neighborhood around each view. To attend to tokens in nearby views, we\nleverage a novel positional encoding that conditions on the relative geometric\ntransformation between the query and nearby views. We decode the output of our\nmodel into a 3D Gaussian Splat scene representation that includes both color\nand opacity view-dependence. Taken together, the Local View Transformer enables\nreconstruction of arbitrarily large, high-resolution scenes in a single forward\npass. See our project page for results and interactive demos\nhttps://toobaimt.github.io/lvt/.\n", "link": "http://arxiv.org/abs/2509.25001v1", "date": "2025-09-29", "relevancy": 3.1051, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6356}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.619}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVT%3A%20Large-Scale%20Scene%20Reconstruction%20via%20Local%20View%20Transformers&body=Title%3A%20LVT%3A%20Large-Scale%20Scene%20Reconstruction%20via%20Local%20View%20Transformers%0AAuthor%3A%20Tooba%20Imtiaz%20and%20Lucy%20Chai%20and%20Kathryn%20Heal%20and%20Xuan%20Luo%20and%20Jungyeon%20Park%20and%20Jennifer%20Dy%20and%20John%20Flynn%0AAbstract%3A%20%20%20Large%20transformer%20models%20are%20proving%20to%20be%20a%20powerful%20tool%20for%203D%20vision%20and%0Anovel%20view%20synthesis.%20However%2C%20the%20standard%20Transformer%27s%20well-known%20quadratic%0Acomplexity%20makes%20it%20difficult%20to%20scale%20these%20methods%20to%20large%20scenes.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20Local%20View%20Transformer%20%28LVT%29%2C%20a%0Alarge-scale%20scene%20reconstruction%20and%20novel%20view%20synthesis%20architecture%20that%0Acircumvents%20the%20need%20for%20the%20quadratic%20attention%20operation.%20Motivated%20by%20the%0Ainsight%20that%20spatially%20nearby%20views%20provide%20more%20useful%20signal%20about%20the%20local%0Ascene%20composition%20than%20distant%20views%2C%20our%20model%20processes%20all%20information%20in%20a%0Alocal%20neighborhood%20around%20each%20view.%20To%20attend%20to%20tokens%20in%20nearby%20views%2C%20we%0Aleverage%20a%20novel%20positional%20encoding%20that%20conditions%20on%20the%20relative%20geometric%0Atransformation%20between%20the%20query%20and%20nearby%20views.%20We%20decode%20the%20output%20of%20our%0Amodel%20into%20a%203D%20Gaussian%20Splat%20scene%20representation%20that%20includes%20both%20color%0Aand%20opacity%20view-dependence.%20Taken%20together%2C%20the%20Local%20View%20Transformer%20enables%0Areconstruction%20of%20arbitrarily%20large%2C%20high-resolution%20scenes%20in%20a%20single%20forward%0Apass.%20See%20our%20project%20page%20for%20results%20and%20interactive%20demos%0Ahttps%3A//toobaimt.github.io/lvt/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVT%253A%2520Large-Scale%2520Scene%2520Reconstruction%2520via%2520Local%2520View%2520Transformers%26entry.906535625%3DTooba%2520Imtiaz%2520and%2520Lucy%2520Chai%2520and%2520Kathryn%2520Heal%2520and%2520Xuan%2520Luo%2520and%2520Jungyeon%2520Park%2520and%2520Jennifer%2520Dy%2520and%2520John%2520Flynn%26entry.1292438233%3D%2520%2520Large%2520transformer%2520models%2520are%2520proving%2520to%2520be%2520a%2520powerful%2520tool%2520for%25203D%2520vision%2520and%250Anovel%2520view%2520synthesis.%2520However%252C%2520the%2520standard%2520Transformer%2527s%2520well-known%2520quadratic%250Acomplexity%2520makes%2520it%2520difficult%2520to%2520scale%2520these%2520methods%2520to%2520large%2520scenes.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Local%2520View%2520Transformer%2520%2528LVT%2529%252C%2520a%250Alarge-scale%2520scene%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520architecture%2520that%250Acircumvents%2520the%2520need%2520for%2520the%2520quadratic%2520attention%2520operation.%2520Motivated%2520by%2520the%250Ainsight%2520that%2520spatially%2520nearby%2520views%2520provide%2520more%2520useful%2520signal%2520about%2520the%2520local%250Ascene%2520composition%2520than%2520distant%2520views%252C%2520our%2520model%2520processes%2520all%2520information%2520in%2520a%250Alocal%2520neighborhood%2520around%2520each%2520view.%2520To%2520attend%2520to%2520tokens%2520in%2520nearby%2520views%252C%2520we%250Aleverage%2520a%2520novel%2520positional%2520encoding%2520that%2520conditions%2520on%2520the%2520relative%2520geometric%250Atransformation%2520between%2520the%2520query%2520and%2520nearby%2520views.%2520We%2520decode%2520the%2520output%2520of%2520our%250Amodel%2520into%2520a%25203D%2520Gaussian%2520Splat%2520scene%2520representation%2520that%2520includes%2520both%2520color%250Aand%2520opacity%2520view-dependence.%2520Taken%2520together%252C%2520the%2520Local%2520View%2520Transformer%2520enables%250Areconstruction%2520of%2520arbitrarily%2520large%252C%2520high-resolution%2520scenes%2520in%2520a%2520single%2520forward%250Apass.%2520See%2520our%2520project%2520page%2520for%2520results%2520and%2520interactive%2520demos%250Ahttps%253A//toobaimt.github.io/lvt/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVT%3A%20Large-Scale%20Scene%20Reconstruction%20via%20Local%20View%20Transformers&entry.906535625=Tooba%20Imtiaz%20and%20Lucy%20Chai%20and%20Kathryn%20Heal%20and%20Xuan%20Luo%20and%20Jungyeon%20Park%20and%20Jennifer%20Dy%20and%20John%20Flynn&entry.1292438233=%20%20Large%20transformer%20models%20are%20proving%20to%20be%20a%20powerful%20tool%20for%203D%20vision%20and%0Anovel%20view%20synthesis.%20However%2C%20the%20standard%20Transformer%27s%20well-known%20quadratic%0Acomplexity%20makes%20it%20difficult%20to%20scale%20these%20methods%20to%20large%20scenes.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20Local%20View%20Transformer%20%28LVT%29%2C%20a%0Alarge-scale%20scene%20reconstruction%20and%20novel%20view%20synthesis%20architecture%20that%0Acircumvents%20the%20need%20for%20the%20quadratic%20attention%20operation.%20Motivated%20by%20the%0Ainsight%20that%20spatially%20nearby%20views%20provide%20more%20useful%20signal%20about%20the%20local%0Ascene%20composition%20than%20distant%20views%2C%20our%20model%20processes%20all%20information%20in%20a%0Alocal%20neighborhood%20around%20each%20view.%20To%20attend%20to%20tokens%20in%20nearby%20views%2C%20we%0Aleverage%20a%20novel%20positional%20encoding%20that%20conditions%20on%20the%20relative%20geometric%0Atransformation%20between%20the%20query%20and%20nearby%20views.%20We%20decode%20the%20output%20of%20our%0Amodel%20into%20a%203D%20Gaussian%20Splat%20scene%20representation%20that%20includes%20both%20color%0Aand%20opacity%20view-dependence.%20Taken%20together%2C%20the%20Local%20View%20Transformer%20enables%0Areconstruction%20of%20arbitrarily%20large%2C%20high-resolution%20scenes%20in%20a%20single%20forward%0Apass.%20See%20our%20project%20page%20for%20results%20and%20interactive%20demos%0Ahttps%3A//toobaimt.github.io/lvt/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25001v1&entry.124074799=Read"},
{"title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures", "author": "Marco Bronzini and Carlo Nicolini and Bruno Lepri and Jacopo Staiano and Andrea Passerini", "abstract": "  Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.\n", "link": "http://arxiv.org/abs/2509.25045v1", "date": "2025-09-29", "relevancy": 3.0593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperdimensional%20Probe%3A%20Decoding%20LLM%20Representations%20via%20Vector%20Symbolic%0A%20%20Architectures&body=Title%3A%20Hyperdimensional%20Probe%3A%20Decoding%20LLM%20Representations%20via%20Vector%20Symbolic%0A%20%20Architectures%0AAuthor%3A%20Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini%0AAbstract%3A%20%20%20Despite%20their%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20remain%20opaque%20with%0Alimited%20understanding%20of%20their%20internal%20representations.%20Current%0Ainterpretability%20methods%2C%20such%20as%20direct%20logit%20attribution%20%28DLA%29%20and%20sparse%0Aautoencoders%20%28SAEs%29%2C%20provide%20restricted%20insight%20due%20to%20limitations%20such%20as%20the%0Amodel%27s%20output%20vocabulary%20or%20unclear%20feature%20names.%20This%20work%20introduces%0AHyperdimensional%20Probe%2C%20a%20novel%20paradigm%20for%20decoding%20information%20from%20the%20LLM%0Avector%20space.%20It%20combines%20ideas%20from%20symbolic%20representations%20and%20neural%0Aprobing%20to%20project%20the%20model%27s%20residual%20stream%20into%20interpretable%20concepts%20via%0AVector%20Symbolic%20Architectures%20%28VSAs%29.%20This%20probe%20combines%20the%20strengths%20of%20SAEs%0Aand%20conventional%20probes%20while%20overcoming%20their%20key%20limitations.%20We%20validate%20our%0Adecoding%20paradigm%20with%20controlled%20input-completion%20tasks%2C%20probing%20the%20model%27s%0Afinal%20state%20before%20next-token%20prediction%20on%20inputs%20spanning%20syntactic%20pattern%0Arecognition%2C%20key-value%20associations%2C%20and%20abstract%20inference.%20We%20further%20assess%0Ait%20in%20a%20question-answering%20setting%2C%20examining%20the%20state%20of%20the%20model%20both%0Abefore%20and%20after%20text%20generation.%20Our%20experiments%20show%20that%20our%20probe%20reliably%0Aextracts%20meaningful%20concepts%20across%20varied%20LLMs%2C%20embedding%20sizes%2C%20and%20input%0Adomains%2C%20also%20helping%20identify%20LLM%20failures.%20Our%20work%20advances%20information%0Adecoding%20in%20LLM%20vector%20space%2C%20enabling%20extracting%20more%20informative%2C%0Ainterpretable%2C%20and%20structured%20features%20from%20neural%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperdimensional%2520Probe%253A%2520Decoding%2520LLM%2520Representations%2520via%2520Vector%2520Symbolic%250A%2520%2520Architectures%26entry.906535625%3DMarco%2520Bronzini%2520and%2520Carlo%2520Nicolini%2520and%2520Bruno%2520Lepri%2520and%2520Jacopo%2520Staiano%2520and%2520Andrea%2520Passerini%26entry.1292438233%3D%2520%2520Despite%2520their%2520capabilities%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520remain%2520opaque%2520with%250Alimited%2520understanding%2520of%2520their%2520internal%2520representations.%2520Current%250Ainterpretability%2520methods%252C%2520such%2520as%2520direct%2520logit%2520attribution%2520%2528DLA%2529%2520and%2520sparse%250Aautoencoders%2520%2528SAEs%2529%252C%2520provide%2520restricted%2520insight%2520due%2520to%2520limitations%2520such%2520as%2520the%250Amodel%2527s%2520output%2520vocabulary%2520or%2520unclear%2520feature%2520names.%2520This%2520work%2520introduces%250AHyperdimensional%2520Probe%252C%2520a%2520novel%2520paradigm%2520for%2520decoding%2520information%2520from%2520the%2520LLM%250Avector%2520space.%2520It%2520combines%2520ideas%2520from%2520symbolic%2520representations%2520and%2520neural%250Aprobing%2520to%2520project%2520the%2520model%2527s%2520residual%2520stream%2520into%2520interpretable%2520concepts%2520via%250AVector%2520Symbolic%2520Architectures%2520%2528VSAs%2529.%2520This%2520probe%2520combines%2520the%2520strengths%2520of%2520SAEs%250Aand%2520conventional%2520probes%2520while%2520overcoming%2520their%2520key%2520limitations.%2520We%2520validate%2520our%250Adecoding%2520paradigm%2520with%2520controlled%2520input-completion%2520tasks%252C%2520probing%2520the%2520model%2527s%250Afinal%2520state%2520before%2520next-token%2520prediction%2520on%2520inputs%2520spanning%2520syntactic%2520pattern%250Arecognition%252C%2520key-value%2520associations%252C%2520and%2520abstract%2520inference.%2520We%2520further%2520assess%250Ait%2520in%2520a%2520question-answering%2520setting%252C%2520examining%2520the%2520state%2520of%2520the%2520model%2520both%250Abefore%2520and%2520after%2520text%2520generation.%2520Our%2520experiments%2520show%2520that%2520our%2520probe%2520reliably%250Aextracts%2520meaningful%2520concepts%2520across%2520varied%2520LLMs%252C%2520embedding%2520sizes%252C%2520and%2520input%250Adomains%252C%2520also%2520helping%2520identify%2520LLM%2520failures.%2520Our%2520work%2520advances%2520information%250Adecoding%2520in%2520LLM%2520vector%2520space%252C%2520enabling%2520extracting%2520more%2520informative%252C%250Ainterpretable%252C%2520and%2520structured%2520features%2520from%2520neural%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperdimensional%20Probe%3A%20Decoding%20LLM%20Representations%20via%20Vector%20Symbolic%0A%20%20Architectures&entry.906535625=Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini&entry.1292438233=%20%20Despite%20their%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20remain%20opaque%20with%0Alimited%20understanding%20of%20their%20internal%20representations.%20Current%0Ainterpretability%20methods%2C%20such%20as%20direct%20logit%20attribution%20%28DLA%29%20and%20sparse%0Aautoencoders%20%28SAEs%29%2C%20provide%20restricted%20insight%20due%20to%20limitations%20such%20as%20the%0Amodel%27s%20output%20vocabulary%20or%20unclear%20feature%20names.%20This%20work%20introduces%0AHyperdimensional%20Probe%2C%20a%20novel%20paradigm%20for%20decoding%20information%20from%20the%20LLM%0Avector%20space.%20It%20combines%20ideas%20from%20symbolic%20representations%20and%20neural%0Aprobing%20to%20project%20the%20model%27s%20residual%20stream%20into%20interpretable%20concepts%20via%0AVector%20Symbolic%20Architectures%20%28VSAs%29.%20This%20probe%20combines%20the%20strengths%20of%20SAEs%0Aand%20conventional%20probes%20while%20overcoming%20their%20key%20limitations.%20We%20validate%20our%0Adecoding%20paradigm%20with%20controlled%20input-completion%20tasks%2C%20probing%20the%20model%27s%0Afinal%20state%20before%20next-token%20prediction%20on%20inputs%20spanning%20syntactic%20pattern%0Arecognition%2C%20key-value%20associations%2C%20and%20abstract%20inference.%20We%20further%20assess%0Ait%20in%20a%20question-answering%20setting%2C%20examining%20the%20state%20of%20the%20model%20both%0Abefore%20and%20after%20text%20generation.%20Our%20experiments%20show%20that%20our%20probe%20reliably%0Aextracts%20meaningful%20concepts%20across%20varied%20LLMs%2C%20embedding%20sizes%2C%20and%20input%0Adomains%2C%20also%20helping%20identify%20LLM%20failures.%20Our%20work%20advances%20information%0Adecoding%20in%20LLM%20vector%20space%2C%20enabling%20extracting%20more%20informative%2C%0Ainterpretable%2C%20and%20structured%20features%20from%20neural%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25045v1&entry.124074799=Read"},
{"title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D\n  Generation", "author": "Guanjun Wu and Jiemin Fang and Chen Yang and Sikuang Li and Taoran Yi and Jia Lu and Zanwei Zhou and Jiazhong Cen and Lingxi Xie and Xiaopeng Zhang and Wei Wei and Wenyu Liu and Xinggang Wang and Qi Tian", "abstract": "  High-fidelity 3D asset generation is crucial for various industries. While\nrecent 3D pretrained models show strong capability in producing realistic\ncontent, most are built upon diffusion models and follow a two-stage pipeline\nthat first generates geometry and then synthesizes appearance. Such a decoupled\ndesign tends to produce geometry-texture misalignment and non-negligible cost.\nIn this paper, we propose UniLat3D, a unified framework that encodes geometry\nand appearance in a single latent space, enabling direct single-stage\ngeneration. Our key contribution is a geometry-appearance Unified VAE, which\ncompresses high-resolution sparse features into a compact latent representation\n-- UniLat. UniLat integrates structural and visual information into a dense\nlow-resolution latent, which can be efficiently decoded into diverse 3D\nformats, e.g., 3D Gaussians and meshes. Based on this unified representation,\nwe train a single flow-matching model to map Gaussian noise directly into\nUniLat, eliminating redundant stages. Trained solely on public datasets,\nUniLat3D produces high-quality 3D assets in seconds from a single image,\nachieving superior appearance fidelity and geometric quality. More demos \\&\ncode are available at https://unilat3d.github.io/\n", "link": "http://arxiv.org/abs/2509.25079v1", "date": "2025-09-29", "relevancy": 3.0322, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6045}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLat3D%3A%20Geometry-Appearance%20Unified%20Latents%20for%20Single-Stage%203D%0A%20%20Generation&body=Title%3A%20UniLat3D%3A%20Geometry-Appearance%20Unified%20Latents%20for%20Single-Stage%203D%0A%20%20Generation%0AAuthor%3A%20Guanjun%20Wu%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Sikuang%20Li%20and%20Taoran%20Yi%20and%20Jia%20Lu%20and%20Zanwei%20Zhou%20and%20Jiazhong%20Cen%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Wei%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Qi%20Tian%0AAbstract%3A%20%20%20High-fidelity%203D%20asset%20generation%20is%20crucial%20for%20various%20industries.%20While%0Arecent%203D%20pretrained%20models%20show%20strong%20capability%20in%20producing%20realistic%0Acontent%2C%20most%20are%20built%20upon%20diffusion%20models%20and%20follow%20a%20two-stage%20pipeline%0Athat%20first%20generates%20geometry%20and%20then%20synthesizes%20appearance.%20Such%20a%20decoupled%0Adesign%20tends%20to%20produce%20geometry-texture%20misalignment%20and%20non-negligible%20cost.%0AIn%20this%20paper%2C%20we%20propose%20UniLat3D%2C%20a%20unified%20framework%20that%20encodes%20geometry%0Aand%20appearance%20in%20a%20single%20latent%20space%2C%20enabling%20direct%20single-stage%0Ageneration.%20Our%20key%20contribution%20is%20a%20geometry-appearance%20Unified%20VAE%2C%20which%0Acompresses%20high-resolution%20sparse%20features%20into%20a%20compact%20latent%20representation%0A--%20UniLat.%20UniLat%20integrates%20structural%20and%20visual%20information%20into%20a%20dense%0Alow-resolution%20latent%2C%20which%20can%20be%20efficiently%20decoded%20into%20diverse%203D%0Aformats%2C%20e.g.%2C%203D%20Gaussians%20and%20meshes.%20Based%20on%20this%20unified%20representation%2C%0Awe%20train%20a%20single%20flow-matching%20model%20to%20map%20Gaussian%20noise%20directly%20into%0AUniLat%2C%20eliminating%20redundant%20stages.%20Trained%20solely%20on%20public%20datasets%2C%0AUniLat3D%20produces%20high-quality%203D%20assets%20in%20seconds%20from%20a%20single%20image%2C%0Aachieving%20superior%20appearance%20fidelity%20and%20geometric%20quality.%20More%20demos%20%5C%26%0Acode%20are%20available%20at%20https%3A//unilat3d.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLat3D%253A%2520Geometry-Appearance%2520Unified%2520Latents%2520for%2520Single-Stage%25203D%250A%2520%2520Generation%26entry.906535625%3DGuanjun%2520Wu%2520and%2520Jiemin%2520Fang%2520and%2520Chen%2520Yang%2520and%2520Sikuang%2520Li%2520and%2520Taoran%2520Yi%2520and%2520Jia%2520Lu%2520and%2520Zanwei%2520Zhou%2520and%2520Jiazhong%2520Cen%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wei%2520Wei%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520asset%2520generation%2520is%2520crucial%2520for%2520various%2520industries.%2520While%250Arecent%25203D%2520pretrained%2520models%2520show%2520strong%2520capability%2520in%2520producing%2520realistic%250Acontent%252C%2520most%2520are%2520built%2520upon%2520diffusion%2520models%2520and%2520follow%2520a%2520two-stage%2520pipeline%250Athat%2520first%2520generates%2520geometry%2520and%2520then%2520synthesizes%2520appearance.%2520Such%2520a%2520decoupled%250Adesign%2520tends%2520to%2520produce%2520geometry-texture%2520misalignment%2520and%2520non-negligible%2520cost.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520UniLat3D%252C%2520a%2520unified%2520framework%2520that%2520encodes%2520geometry%250Aand%2520appearance%2520in%2520a%2520single%2520latent%2520space%252C%2520enabling%2520direct%2520single-stage%250Ageneration.%2520Our%2520key%2520contribution%2520is%2520a%2520geometry-appearance%2520Unified%2520VAE%252C%2520which%250Acompresses%2520high-resolution%2520sparse%2520features%2520into%2520a%2520compact%2520latent%2520representation%250A--%2520UniLat.%2520UniLat%2520integrates%2520structural%2520and%2520visual%2520information%2520into%2520a%2520dense%250Alow-resolution%2520latent%252C%2520which%2520can%2520be%2520efficiently%2520decoded%2520into%2520diverse%25203D%250Aformats%252C%2520e.g.%252C%25203D%2520Gaussians%2520and%2520meshes.%2520Based%2520on%2520this%2520unified%2520representation%252C%250Awe%2520train%2520a%2520single%2520flow-matching%2520model%2520to%2520map%2520Gaussian%2520noise%2520directly%2520into%250AUniLat%252C%2520eliminating%2520redundant%2520stages.%2520Trained%2520solely%2520on%2520public%2520datasets%252C%250AUniLat3D%2520produces%2520high-quality%25203D%2520assets%2520in%2520seconds%2520from%2520a%2520single%2520image%252C%250Aachieving%2520superior%2520appearance%2520fidelity%2520and%2520geometric%2520quality.%2520More%2520demos%2520%255C%2526%250Acode%2520are%2520available%2520at%2520https%253A//unilat3d.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLat3D%3A%20Geometry-Appearance%20Unified%20Latents%20for%20Single-Stage%203D%0A%20%20Generation&entry.906535625=Guanjun%20Wu%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Sikuang%20Li%20and%20Taoran%20Yi%20and%20Jia%20Lu%20and%20Zanwei%20Zhou%20and%20Jiazhong%20Cen%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Wei%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Qi%20Tian&entry.1292438233=%20%20High-fidelity%203D%20asset%20generation%20is%20crucial%20for%20various%20industries.%20While%0Arecent%203D%20pretrained%20models%20show%20strong%20capability%20in%20producing%20realistic%0Acontent%2C%20most%20are%20built%20upon%20diffusion%20models%20and%20follow%20a%20two-stage%20pipeline%0Athat%20first%20generates%20geometry%20and%20then%20synthesizes%20appearance.%20Such%20a%20decoupled%0Adesign%20tends%20to%20produce%20geometry-texture%20misalignment%20and%20non-negligible%20cost.%0AIn%20this%20paper%2C%20we%20propose%20UniLat3D%2C%20a%20unified%20framework%20that%20encodes%20geometry%0Aand%20appearance%20in%20a%20single%20latent%20space%2C%20enabling%20direct%20single-stage%0Ageneration.%20Our%20key%20contribution%20is%20a%20geometry-appearance%20Unified%20VAE%2C%20which%0Acompresses%20high-resolution%20sparse%20features%20into%20a%20compact%20latent%20representation%0A--%20UniLat.%20UniLat%20integrates%20structural%20and%20visual%20information%20into%20a%20dense%0Alow-resolution%20latent%2C%20which%20can%20be%20efficiently%20decoded%20into%20diverse%203D%0Aformats%2C%20e.g.%2C%203D%20Gaussians%20and%20meshes.%20Based%20on%20this%20unified%20representation%2C%0Awe%20train%20a%20single%20flow-matching%20model%20to%20map%20Gaussian%20noise%20directly%20into%0AUniLat%2C%20eliminating%20redundant%20stages.%20Trained%20solely%20on%20public%20datasets%2C%0AUniLat3D%20produces%20high-quality%203D%20assets%20in%20seconds%20from%20a%20single%20image%2C%0Aachieving%20superior%20appearance%20fidelity%20and%20geometric%20quality.%20More%20demos%20%5C%26%0Acode%20are%20available%20at%20https%3A//unilat3d.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25079v1&entry.124074799=Read"},
{"title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular\n  Clue Exploration with Interactive Agents", "author": "Jiahua Li and Kun Wei and Zhe Xu and Zibo Su and Xu Yang and Cheng Deng", "abstract": "  Long videos, characterized by temporal complexity and sparse task-relevant\ninformation, pose significant reasoning challenges for AI systems. Although\nvarious Large Language Model (LLM)-based approaches have advanced long video\nunderstanding, they still struggle to achieve both completeness and efficiency\nin capturing task-critical information. Inspired by human progressive visual\ncognition, we propose CogniGPT, a framework that leverages an interactive loop\nbetween Multi-Granular Perception Agent (MGPA) and Verification-Enhanced\nReflection Agent (VERA) for efficient and reliable long video understanding.\nSpecifically, MGPA mimics human visual divergent and focused attention to\ncapture task-related information, while VERA verifies perceived key clues to\nmitigate hallucination and optimize subsequent perception strategies. Through\nthis interactive process, CogniGPT explores a minimal set of informative and\nreliable task-related clues. Extensive experiments on EgoSchema, Video-MME,\nNExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both\naccuracy and efficiency. Notably, on EgoSchema, it surpasses existing\ntraining-free methods using only 11.2 frames and achieves performance\ncomparable to Gemini 1.5-Pro.\n", "link": "http://arxiv.org/abs/2509.24943v1", "date": "2025-09-29", "relevancy": 3.0237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6114}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceive%2C%20Reflect%20and%20Understand%20Long%20Video%3A%20Progressive%20Multi-Granular%0A%20%20Clue%20Exploration%20with%20Interactive%20Agents&body=Title%3A%20Perceive%2C%20Reflect%20and%20Understand%20Long%20Video%3A%20Progressive%20Multi-Granular%0A%20%20Clue%20Exploration%20with%20Interactive%20Agents%0AAuthor%3A%20Jiahua%20Li%20and%20Kun%20Wei%20and%20Zhe%20Xu%20and%20Zibo%20Su%20and%20Xu%20Yang%20and%20Cheng%20Deng%0AAbstract%3A%20%20%20Long%20videos%2C%20characterized%20by%20temporal%20complexity%20and%20sparse%20task-relevant%0Ainformation%2C%20pose%20significant%20reasoning%20challenges%20for%20AI%20systems.%20Although%0Avarious%20Large%20Language%20Model%20%28LLM%29-based%20approaches%20have%20advanced%20long%20video%0Aunderstanding%2C%20they%20still%20struggle%20to%20achieve%20both%20completeness%20and%20efficiency%0Ain%20capturing%20task-critical%20information.%20Inspired%20by%20human%20progressive%20visual%0Acognition%2C%20we%20propose%20CogniGPT%2C%20a%20framework%20that%20leverages%20an%20interactive%20loop%0Abetween%20Multi-Granular%20Perception%20Agent%20%28MGPA%29%20and%20Verification-Enhanced%0AReflection%20Agent%20%28VERA%29%20for%20efficient%20and%20reliable%20long%20video%20understanding.%0ASpecifically%2C%20MGPA%20mimics%20human%20visual%20divergent%20and%20focused%20attention%20to%0Acapture%20task-related%20information%2C%20while%20VERA%20verifies%20perceived%20key%20clues%20to%0Amitigate%20hallucination%20and%20optimize%20subsequent%20perception%20strategies.%20Through%0Athis%20interactive%20process%2C%20CogniGPT%20explores%20a%20minimal%20set%20of%20informative%20and%0Areliable%20task-related%20clues.%20Extensive%20experiments%20on%20EgoSchema%2C%20Video-MME%2C%0ANExT-QA%2C%20and%20MovieChat%20datasets%20demonstrate%20CogniGPT%27s%20superiority%20in%20both%0Aaccuracy%20and%20efficiency.%20Notably%2C%20on%20EgoSchema%2C%20it%20surpasses%20existing%0Atraining-free%20methods%20using%20only%2011.2%20frames%20and%20achieves%20performance%0Acomparable%20to%20Gemini%201.5-Pro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceive%252C%2520Reflect%2520and%2520Understand%2520Long%2520Video%253A%2520Progressive%2520Multi-Granular%250A%2520%2520Clue%2520Exploration%2520with%2520Interactive%2520Agents%26entry.906535625%3DJiahua%2520Li%2520and%2520Kun%2520Wei%2520and%2520Zhe%2520Xu%2520and%2520Zibo%2520Su%2520and%2520Xu%2520Yang%2520and%2520Cheng%2520Deng%26entry.1292438233%3D%2520%2520Long%2520videos%252C%2520characterized%2520by%2520temporal%2520complexity%2520and%2520sparse%2520task-relevant%250Ainformation%252C%2520pose%2520significant%2520reasoning%2520challenges%2520for%2520AI%2520systems.%2520Although%250Avarious%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520approaches%2520have%2520advanced%2520long%2520video%250Aunderstanding%252C%2520they%2520still%2520struggle%2520to%2520achieve%2520both%2520completeness%2520and%2520efficiency%250Ain%2520capturing%2520task-critical%2520information.%2520Inspired%2520by%2520human%2520progressive%2520visual%250Acognition%252C%2520we%2520propose%2520CogniGPT%252C%2520a%2520framework%2520that%2520leverages%2520an%2520interactive%2520loop%250Abetween%2520Multi-Granular%2520Perception%2520Agent%2520%2528MGPA%2529%2520and%2520Verification-Enhanced%250AReflection%2520Agent%2520%2528VERA%2529%2520for%2520efficient%2520and%2520reliable%2520long%2520video%2520understanding.%250ASpecifically%252C%2520MGPA%2520mimics%2520human%2520visual%2520divergent%2520and%2520focused%2520attention%2520to%250Acapture%2520task-related%2520information%252C%2520while%2520VERA%2520verifies%2520perceived%2520key%2520clues%2520to%250Amitigate%2520hallucination%2520and%2520optimize%2520subsequent%2520perception%2520strategies.%2520Through%250Athis%2520interactive%2520process%252C%2520CogniGPT%2520explores%2520a%2520minimal%2520set%2520of%2520informative%2520and%250Areliable%2520task-related%2520clues.%2520Extensive%2520experiments%2520on%2520EgoSchema%252C%2520Video-MME%252C%250ANExT-QA%252C%2520and%2520MovieChat%2520datasets%2520demonstrate%2520CogniGPT%2527s%2520superiority%2520in%2520both%250Aaccuracy%2520and%2520efficiency.%2520Notably%252C%2520on%2520EgoSchema%252C%2520it%2520surpasses%2520existing%250Atraining-free%2520methods%2520using%2520only%252011.2%2520frames%2520and%2520achieves%2520performance%250Acomparable%2520to%2520Gemini%25201.5-Pro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceive%2C%20Reflect%20and%20Understand%20Long%20Video%3A%20Progressive%20Multi-Granular%0A%20%20Clue%20Exploration%20with%20Interactive%20Agents&entry.906535625=Jiahua%20Li%20and%20Kun%20Wei%20and%20Zhe%20Xu%20and%20Zibo%20Su%20and%20Xu%20Yang%20and%20Cheng%20Deng&entry.1292438233=%20%20Long%20videos%2C%20characterized%20by%20temporal%20complexity%20and%20sparse%20task-relevant%0Ainformation%2C%20pose%20significant%20reasoning%20challenges%20for%20AI%20systems.%20Although%0Avarious%20Large%20Language%20Model%20%28LLM%29-based%20approaches%20have%20advanced%20long%20video%0Aunderstanding%2C%20they%20still%20struggle%20to%20achieve%20both%20completeness%20and%20efficiency%0Ain%20capturing%20task-critical%20information.%20Inspired%20by%20human%20progressive%20visual%0Acognition%2C%20we%20propose%20CogniGPT%2C%20a%20framework%20that%20leverages%20an%20interactive%20loop%0Abetween%20Multi-Granular%20Perception%20Agent%20%28MGPA%29%20and%20Verification-Enhanced%0AReflection%20Agent%20%28VERA%29%20for%20efficient%20and%20reliable%20long%20video%20understanding.%0ASpecifically%2C%20MGPA%20mimics%20human%20visual%20divergent%20and%20focused%20attention%20to%0Acapture%20task-related%20information%2C%20while%20VERA%20verifies%20perceived%20key%20clues%20to%0Amitigate%20hallucination%20and%20optimize%20subsequent%20perception%20strategies.%20Through%0Athis%20interactive%20process%2C%20CogniGPT%20explores%20a%20minimal%20set%20of%20informative%20and%0Areliable%20task-related%20clues.%20Extensive%20experiments%20on%20EgoSchema%2C%20Video-MME%2C%0ANExT-QA%2C%20and%20MovieChat%20datasets%20demonstrate%20CogniGPT%27s%20superiority%20in%20both%0Aaccuracy%20and%20efficiency.%20Notably%2C%20on%20EgoSchema%2C%20it%20surpasses%20existing%0Atraining-free%20methods%20using%20only%2011.2%20frames%20and%20achieves%20performance%0Acomparable%20to%20Gemini%201.5-Pro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24943v1&entry.124074799=Read"},
{"title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM\n  Reconstruction", "author": "Huaizhi Qu and Xiao Wang and Gengwei Zhang and Jie Peng and Tianlong Chen", "abstract": "  Cryo-electron microscopy (cryo-EM) has become a central tool for\nhigh-resolution structural biology, yet the massive scale of datasets (often\nexceeding 100k particle images) renders 3D reconstruction both computationally\nexpensive and memory intensive. Traditional Fourier-space methods are efficient\nbut lose fidelity due to repeated transforms, while recent real-space\napproaches based on neural radiance fields (NeRFs) improve accuracy but incur\ncubic memory and computation overhead. Therefore, we introduce GEM, a novel\ncryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that\noperates directly in real-space while maintaining high efficiency. Instead of\nmodeling the entire density volume, GEM represents proteins with compact 3D\nGaussians, each parameterized by only 11 values. To further improve the\ntraining efficiency, we designed a novel gradient computation to 3D Gaussians\nthat contribute to each voxel. This design substantially reduced both memory\nfootprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to\n48% faster training and 12% lower memory usage compared to state-of-the-art\nmethods, while improving local resolution by as much as 38.8%. These results\nestablish GEM as a practical and scalable paradigm for cryo-EM reconstruction,\nunifying speed, efficiency, and high-resolution accuracy. Our code is available\nat https://github.com/UNITES-Lab/GEM.\n", "link": "http://arxiv.org/abs/2509.25075v1", "date": "2025-09-29", "relevancy": 2.9959, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6311}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5843}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEM%3A%203D%20Gaussian%20Splatting%20for%20Efficient%20and%20Accurate%20Cryo-EM%0A%20%20Reconstruction&body=Title%3A%20GEM%3A%203D%20Gaussian%20Splatting%20for%20Efficient%20and%20Accurate%20Cryo-EM%0A%20%20Reconstruction%0AAuthor%3A%20Huaizhi%20Qu%20and%20Xiao%20Wang%20and%20Gengwei%20Zhang%20and%20Jie%20Peng%20and%20Tianlong%20Chen%0AAbstract%3A%20%20%20Cryo-electron%20microscopy%20%28cryo-EM%29%20has%20become%20a%20central%20tool%20for%0Ahigh-resolution%20structural%20biology%2C%20yet%20the%20massive%20scale%20of%20datasets%20%28often%0Aexceeding%20100k%20particle%20images%29%20renders%203D%20reconstruction%20both%20computationally%0Aexpensive%20and%20memory%20intensive.%20Traditional%20Fourier-space%20methods%20are%20efficient%0Abut%20lose%20fidelity%20due%20to%20repeated%20transforms%2C%20while%20recent%20real-space%0Aapproaches%20based%20on%20neural%20radiance%20fields%20%28NeRFs%29%20improve%20accuracy%20but%20incur%0Acubic%20memory%20and%20computation%20overhead.%20Therefore%2C%20we%20introduce%20GEM%2C%20a%20novel%0Acryo-EM%20reconstruction%20framework%20built%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20that%0Aoperates%20directly%20in%20real-space%20while%20maintaining%20high%20efficiency.%20Instead%20of%0Amodeling%20the%20entire%20density%20volume%2C%20GEM%20represents%20proteins%20with%20compact%203D%0AGaussians%2C%20each%20parameterized%20by%20only%2011%20values.%20To%20further%20improve%20the%0Atraining%20efficiency%2C%20we%20designed%20a%20novel%20gradient%20computation%20to%203D%20Gaussians%0Athat%20contribute%20to%20each%20voxel.%20This%20design%20substantially%20reduced%20both%20memory%0Afootprint%20and%20training%20cost.%20On%20standard%20cryo-EM%20benchmarks%2C%20GEM%20achieves%20up%20to%0A48%25%20faster%20training%20and%2012%25%20lower%20memory%20usage%20compared%20to%20state-of-the-art%0Amethods%2C%20while%20improving%20local%20resolution%20by%20as%20much%20as%2038.8%25.%20These%20results%0Aestablish%20GEM%20as%20a%20practical%20and%20scalable%20paradigm%20for%20cryo-EM%20reconstruction%2C%0Aunifying%20speed%2C%20efficiency%2C%20and%20high-resolution%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/UNITES-Lab/GEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEM%253A%25203D%2520Gaussian%2520Splatting%2520for%2520Efficient%2520and%2520Accurate%2520Cryo-EM%250A%2520%2520Reconstruction%26entry.906535625%3DHuaizhi%2520Qu%2520and%2520Xiao%2520Wang%2520and%2520Gengwei%2520Zhang%2520and%2520Jie%2520Peng%2520and%2520Tianlong%2520Chen%26entry.1292438233%3D%2520%2520Cryo-electron%2520microscopy%2520%2528cryo-EM%2529%2520has%2520become%2520a%2520central%2520tool%2520for%250Ahigh-resolution%2520structural%2520biology%252C%2520yet%2520the%2520massive%2520scale%2520of%2520datasets%2520%2528often%250Aexceeding%2520100k%2520particle%2520images%2529%2520renders%25203D%2520reconstruction%2520both%2520computationally%250Aexpensive%2520and%2520memory%2520intensive.%2520Traditional%2520Fourier-space%2520methods%2520are%2520efficient%250Abut%2520lose%2520fidelity%2520due%2520to%2520repeated%2520transforms%252C%2520while%2520recent%2520real-space%250Aapproaches%2520based%2520on%2520neural%2520radiance%2520fields%2520%2528NeRFs%2529%2520improve%2520accuracy%2520but%2520incur%250Acubic%2520memory%2520and%2520computation%2520overhead.%2520Therefore%252C%2520we%2520introduce%2520GEM%252C%2520a%2520novel%250Acryo-EM%2520reconstruction%2520framework%2520built%2520on%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520that%250Aoperates%2520directly%2520in%2520real-space%2520while%2520maintaining%2520high%2520efficiency.%2520Instead%2520of%250Amodeling%2520the%2520entire%2520density%2520volume%252C%2520GEM%2520represents%2520proteins%2520with%2520compact%25203D%250AGaussians%252C%2520each%2520parameterized%2520by%2520only%252011%2520values.%2520To%2520further%2520improve%2520the%250Atraining%2520efficiency%252C%2520we%2520designed%2520a%2520novel%2520gradient%2520computation%2520to%25203D%2520Gaussians%250Athat%2520contribute%2520to%2520each%2520voxel.%2520This%2520design%2520substantially%2520reduced%2520both%2520memory%250Afootprint%2520and%2520training%2520cost.%2520On%2520standard%2520cryo-EM%2520benchmarks%252C%2520GEM%2520achieves%2520up%2520to%250A48%2525%2520faster%2520training%2520and%252012%2525%2520lower%2520memory%2520usage%2520compared%2520to%2520state-of-the-art%250Amethods%252C%2520while%2520improving%2520local%2520resolution%2520by%2520as%2520much%2520as%252038.8%2525.%2520These%2520results%250Aestablish%2520GEM%2520as%2520a%2520practical%2520and%2520scalable%2520paradigm%2520for%2520cryo-EM%2520reconstruction%252C%250Aunifying%2520speed%252C%2520efficiency%252C%2520and%2520high-resolution%2520accuracy.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/UNITES-Lab/GEM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEM%3A%203D%20Gaussian%20Splatting%20for%20Efficient%20and%20Accurate%20Cryo-EM%0A%20%20Reconstruction&entry.906535625=Huaizhi%20Qu%20and%20Xiao%20Wang%20and%20Gengwei%20Zhang%20and%20Jie%20Peng%20and%20Tianlong%20Chen&entry.1292438233=%20%20Cryo-electron%20microscopy%20%28cryo-EM%29%20has%20become%20a%20central%20tool%20for%0Ahigh-resolution%20structural%20biology%2C%20yet%20the%20massive%20scale%20of%20datasets%20%28often%0Aexceeding%20100k%20particle%20images%29%20renders%203D%20reconstruction%20both%20computationally%0Aexpensive%20and%20memory%20intensive.%20Traditional%20Fourier-space%20methods%20are%20efficient%0Abut%20lose%20fidelity%20due%20to%20repeated%20transforms%2C%20while%20recent%20real-space%0Aapproaches%20based%20on%20neural%20radiance%20fields%20%28NeRFs%29%20improve%20accuracy%20but%20incur%0Acubic%20memory%20and%20computation%20overhead.%20Therefore%2C%20we%20introduce%20GEM%2C%20a%20novel%0Acryo-EM%20reconstruction%20framework%20built%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20that%0Aoperates%20directly%20in%20real-space%20while%20maintaining%20high%20efficiency.%20Instead%20of%0Amodeling%20the%20entire%20density%20volume%2C%20GEM%20represents%20proteins%20with%20compact%203D%0AGaussians%2C%20each%20parameterized%20by%20only%2011%20values.%20To%20further%20improve%20the%0Atraining%20efficiency%2C%20we%20designed%20a%20novel%20gradient%20computation%20to%203D%20Gaussians%0Athat%20contribute%20to%20each%20voxel.%20This%20design%20substantially%20reduced%20both%20memory%0Afootprint%20and%20training%20cost.%20On%20standard%20cryo-EM%20benchmarks%2C%20GEM%20achieves%20up%20to%0A48%25%20faster%20training%20and%2012%25%20lower%20memory%20usage%20compared%20to%20state-of-the-art%0Amethods%2C%20while%20improving%20local%20resolution%20by%20as%20much%20as%2038.8%25.%20These%20results%0Aestablish%20GEM%20as%20a%20practical%20and%20scalable%20paradigm%20for%20cryo-EM%20reconstruction%2C%0Aunifying%20speed%2C%20efficiency%2C%20and%20high-resolution%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/UNITES-Lab/GEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25075v1&entry.124074799=Read"},
{"title": "Unsupervised Representation Learning for 3D Mesh Parameterization with\n  Semantic and Visibility Objectives", "author": "AmirHossein Zamani and Bruno Roy and Arianna Rampini", "abstract": "  Recent 3D generative models produce high-quality textures for 3D mesh\nobjects. However, they commonly rely on the heavy assumption that input 3D\nmeshes are accompanied by manual mesh parameterization (UV mapping), a manual\ntask that requires both technical precision and artistic judgment. Industry\nsurveys show that this process often accounts for a significant share of asset\ncreation, creating a major bottleneck for 3D content creators. Moreover,\nexisting automatic methods often ignore two perceptually important criteria:\n(1) semantic awareness (UV charts should align semantically similar 3D parts\nacross shapes) and (2) visibility awareness (cutting seams should lie in\nregions unlikely to be seen). To overcome these shortcomings and to automate\nthe mesh parameterization process, we present an unsupervised differentiable\nframework that augments standard geometry-preserving UV learning with semantic-\nand visibility-aware objectives. For semantic-awareness, our pipeline (i)\nsegments the mesh into semantic 3D parts, (ii) applies an unsupervised learned\nper-part UV-parameterization backbone, and (iii) aggregates per-part charts\ninto a unified UV atlas. For visibility-awareness, we use ambient occlusion\n(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted\nseam objective to steer cutting seams toward occluded regions. By conducting\nqualitative and quantitative evaluations against state-of-the-art methods, we\nshow that the proposed method produces UV atlases that better support texture\ngeneration and reduce perceptible seam artifacts compared to recent baselines.\nOur implementation code is publicly available at:\nhttps://github.com/AHHHZ975/Semantic-Visibility-UV-Param.\n", "link": "http://arxiv.org/abs/2509.25094v1", "date": "2025-09-29", "relevancy": 2.9829, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6045}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Representation%20Learning%20for%203D%20Mesh%20Parameterization%20with%0A%20%20Semantic%20and%20Visibility%20Objectives&body=Title%3A%20Unsupervised%20Representation%20Learning%20for%203D%20Mesh%20Parameterization%20with%0A%20%20Semantic%20and%20Visibility%20Objectives%0AAuthor%3A%20AmirHossein%20Zamani%20and%20Bruno%20Roy%20and%20Arianna%20Rampini%0AAbstract%3A%20%20%20Recent%203D%20generative%20models%20produce%20high-quality%20textures%20for%203D%20mesh%0Aobjects.%20However%2C%20they%20commonly%20rely%20on%20the%20heavy%20assumption%20that%20input%203D%0Ameshes%20are%20accompanied%20by%20manual%20mesh%20parameterization%20%28UV%20mapping%29%2C%20a%20manual%0Atask%20that%20requires%20both%20technical%20precision%20and%20artistic%20judgment.%20Industry%0Asurveys%20show%20that%20this%20process%20often%20accounts%20for%20a%20significant%20share%20of%20asset%0Acreation%2C%20creating%20a%20major%20bottleneck%20for%203D%20content%20creators.%20Moreover%2C%0Aexisting%20automatic%20methods%20often%20ignore%20two%20perceptually%20important%20criteria%3A%0A%281%29%20semantic%20awareness%20%28UV%20charts%20should%20align%20semantically%20similar%203D%20parts%0Aacross%20shapes%29%20and%20%282%29%20visibility%20awareness%20%28cutting%20seams%20should%20lie%20in%0Aregions%20unlikely%20to%20be%20seen%29.%20To%20overcome%20these%20shortcomings%20and%20to%20automate%0Athe%20mesh%20parameterization%20process%2C%20we%20present%20an%20unsupervised%20differentiable%0Aframework%20that%20augments%20standard%20geometry-preserving%20UV%20learning%20with%20semantic-%0Aand%20visibility-aware%20objectives.%20For%20semantic-awareness%2C%20our%20pipeline%20%28i%29%0Asegments%20the%20mesh%20into%20semantic%203D%20parts%2C%20%28ii%29%20applies%20an%20unsupervised%20learned%0Aper-part%20UV-parameterization%20backbone%2C%20and%20%28iii%29%20aggregates%20per-part%20charts%0Ainto%20a%20unified%20UV%20atlas.%20For%20visibility-awareness%2C%20we%20use%20ambient%20occlusion%0A%28AO%29%20as%20an%20exposure%20proxy%20and%20back-propagate%20a%20soft%20differentiable%20AO-weighted%0Aseam%20objective%20to%20steer%20cutting%20seams%20toward%20occluded%20regions.%20By%20conducting%0Aqualitative%20and%20quantitative%20evaluations%20against%20state-of-the-art%20methods%2C%20we%0Ashow%20that%20the%20proposed%20method%20produces%20UV%20atlases%20that%20better%20support%20texture%0Ageneration%20and%20reduce%20perceptible%20seam%20artifacts%20compared%20to%20recent%20baselines.%0AOur%20implementation%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/AHHHZ975/Semantic-Visibility-UV-Param.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Representation%2520Learning%2520for%25203D%2520Mesh%2520Parameterization%2520with%250A%2520%2520Semantic%2520and%2520Visibility%2520Objectives%26entry.906535625%3DAmirHossein%2520Zamani%2520and%2520Bruno%2520Roy%2520and%2520Arianna%2520Rampini%26entry.1292438233%3D%2520%2520Recent%25203D%2520generative%2520models%2520produce%2520high-quality%2520textures%2520for%25203D%2520mesh%250Aobjects.%2520However%252C%2520they%2520commonly%2520rely%2520on%2520the%2520heavy%2520assumption%2520that%2520input%25203D%250Ameshes%2520are%2520accompanied%2520by%2520manual%2520mesh%2520parameterization%2520%2528UV%2520mapping%2529%252C%2520a%2520manual%250Atask%2520that%2520requires%2520both%2520technical%2520precision%2520and%2520artistic%2520judgment.%2520Industry%250Asurveys%2520show%2520that%2520this%2520process%2520often%2520accounts%2520for%2520a%2520significant%2520share%2520of%2520asset%250Acreation%252C%2520creating%2520a%2520major%2520bottleneck%2520for%25203D%2520content%2520creators.%2520Moreover%252C%250Aexisting%2520automatic%2520methods%2520often%2520ignore%2520two%2520perceptually%2520important%2520criteria%253A%250A%25281%2529%2520semantic%2520awareness%2520%2528UV%2520charts%2520should%2520align%2520semantically%2520similar%25203D%2520parts%250Aacross%2520shapes%2529%2520and%2520%25282%2529%2520visibility%2520awareness%2520%2528cutting%2520seams%2520should%2520lie%2520in%250Aregions%2520unlikely%2520to%2520be%2520seen%2529.%2520To%2520overcome%2520these%2520shortcomings%2520and%2520to%2520automate%250Athe%2520mesh%2520parameterization%2520process%252C%2520we%2520present%2520an%2520unsupervised%2520differentiable%250Aframework%2520that%2520augments%2520standard%2520geometry-preserving%2520UV%2520learning%2520with%2520semantic-%250Aand%2520visibility-aware%2520objectives.%2520For%2520semantic-awareness%252C%2520our%2520pipeline%2520%2528i%2529%250Asegments%2520the%2520mesh%2520into%2520semantic%25203D%2520parts%252C%2520%2528ii%2529%2520applies%2520an%2520unsupervised%2520learned%250Aper-part%2520UV-parameterization%2520backbone%252C%2520and%2520%2528iii%2529%2520aggregates%2520per-part%2520charts%250Ainto%2520a%2520unified%2520UV%2520atlas.%2520For%2520visibility-awareness%252C%2520we%2520use%2520ambient%2520occlusion%250A%2528AO%2529%2520as%2520an%2520exposure%2520proxy%2520and%2520back-propagate%2520a%2520soft%2520differentiable%2520AO-weighted%250Aseam%2520objective%2520to%2520steer%2520cutting%2520seams%2520toward%2520occluded%2520regions.%2520By%2520conducting%250Aqualitative%2520and%2520quantitative%2520evaluations%2520against%2520state-of-the-art%2520methods%252C%2520we%250Ashow%2520that%2520the%2520proposed%2520method%2520produces%2520UV%2520atlases%2520that%2520better%2520support%2520texture%250Ageneration%2520and%2520reduce%2520perceptible%2520seam%2520artifacts%2520compared%2520to%2520recent%2520baselines.%250AOur%2520implementation%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/AHHHZ975/Semantic-Visibility-UV-Param.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Representation%20Learning%20for%203D%20Mesh%20Parameterization%20with%0A%20%20Semantic%20and%20Visibility%20Objectives&entry.906535625=AmirHossein%20Zamani%20and%20Bruno%20Roy%20and%20Arianna%20Rampini&entry.1292438233=%20%20Recent%203D%20generative%20models%20produce%20high-quality%20textures%20for%203D%20mesh%0Aobjects.%20However%2C%20they%20commonly%20rely%20on%20the%20heavy%20assumption%20that%20input%203D%0Ameshes%20are%20accompanied%20by%20manual%20mesh%20parameterization%20%28UV%20mapping%29%2C%20a%20manual%0Atask%20that%20requires%20both%20technical%20precision%20and%20artistic%20judgment.%20Industry%0Asurveys%20show%20that%20this%20process%20often%20accounts%20for%20a%20significant%20share%20of%20asset%0Acreation%2C%20creating%20a%20major%20bottleneck%20for%203D%20content%20creators.%20Moreover%2C%0Aexisting%20automatic%20methods%20often%20ignore%20two%20perceptually%20important%20criteria%3A%0A%281%29%20semantic%20awareness%20%28UV%20charts%20should%20align%20semantically%20similar%203D%20parts%0Aacross%20shapes%29%20and%20%282%29%20visibility%20awareness%20%28cutting%20seams%20should%20lie%20in%0Aregions%20unlikely%20to%20be%20seen%29.%20To%20overcome%20these%20shortcomings%20and%20to%20automate%0Athe%20mesh%20parameterization%20process%2C%20we%20present%20an%20unsupervised%20differentiable%0Aframework%20that%20augments%20standard%20geometry-preserving%20UV%20learning%20with%20semantic-%0Aand%20visibility-aware%20objectives.%20For%20semantic-awareness%2C%20our%20pipeline%20%28i%29%0Asegments%20the%20mesh%20into%20semantic%203D%20parts%2C%20%28ii%29%20applies%20an%20unsupervised%20learned%0Aper-part%20UV-parameterization%20backbone%2C%20and%20%28iii%29%20aggregates%20per-part%20charts%0Ainto%20a%20unified%20UV%20atlas.%20For%20visibility-awareness%2C%20we%20use%20ambient%20occlusion%0A%28AO%29%20as%20an%20exposure%20proxy%20and%20back-propagate%20a%20soft%20differentiable%20AO-weighted%0Aseam%20objective%20to%20steer%20cutting%20seams%20toward%20occluded%20regions.%20By%20conducting%0Aqualitative%20and%20quantitative%20evaluations%20against%20state-of-the-art%20methods%2C%20we%0Ashow%20that%20the%20proposed%20method%20produces%20UV%20atlases%20that%20better%20support%20texture%0Ageneration%20and%20reduce%20perceptible%20seam%20artifacts%20compared%20to%20recent%20baselines.%0AOur%20implementation%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/AHHHZ975/Semantic-Visibility-UV-Param.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25094v1&entry.124074799=Read"},
{"title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning", "author": "Wenhao Li and Qiangchang Wang and Xianjing Meng and Zhibin Wu and Yilong Yin", "abstract": "  Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.\n", "link": "http://arxiv.org/abs/2509.25033v1", "date": "2025-09-29", "relevancy": 2.9707, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VT-FSL%3A%20Bridging%20Vision%20and%20Text%20with%20LLMs%20for%20Few-Shot%20Learning&body=Title%3A%20VT-FSL%3A%20Bridging%20Vision%20and%20Text%20with%20LLMs%20for%20Few-Shot%20Learning%0AAuthor%3A%20Wenhao%20Li%20and%20Qiangchang%20Wang%20and%20Xianjing%20Meng%20and%20Zhibin%20Wu%20and%20Yilong%20Yin%0AAbstract%3A%20%20%20Few-shot%20learning%20%28FSL%29%20aims%20to%20recognize%20novel%20concepts%20from%20only%20a%20few%0Alabeled%20support%20samples.%20Recent%20studies%20enhance%20support%20features%20by%0Aincorporating%20additional%20semantic%20information%20or%20designing%20complex%20semantic%0Afusion%20modules.%20However%2C%20they%20still%20suffer%20from%20hallucinating%20semantics%20that%0Acontradict%20the%20visual%20evidence%20due%20to%20the%20lack%20of%20grounding%20in%20actual%0Ainstances%2C%20resulting%20in%20noisy%20guidance%20and%20costly%20corrections.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20novel%20framework%2C%20bridging%20Vision%20and%20Text%20with%20LLMs%20for%0AFew-Shot%20Learning%20%28VT-FSL%29%2C%20which%20constructs%20precise%20cross-modal%20prompts%0Aconditioned%20on%20Large%20Language%20Models%20%28LLMs%29%20and%20support%20images%2C%20seamlessly%0Aintegrating%20them%20through%20a%20geometry-aware%20alignment.%20It%20mainly%20consists%20of%0ACross-modal%20Iterative%20Prompting%20%28CIP%29%20and%20Cross-modal%20Geometric%20Alignment%0A%28CGA%29.%20Specifically%2C%20the%20CIP%20conditions%20an%20LLM%20on%20both%20class%20names%20and%20support%0Aimages%20to%20generate%20precise%20class%20descriptions%20iteratively%20in%20a%20single%0Astructured%20reasoning%20pass.%20These%20descriptions%20not%20only%20enrich%20the%20semantic%0Aunderstanding%20of%20novel%20classes%20but%20also%20enable%20the%20zero-shot%20synthesis%20of%0Asemantically%20consistent%20images.%20The%20descriptions%20and%20synthetic%20images%20act%0Arespectively%20as%20complementary%20textual%20and%20visual%20prompts%2C%20providing%20high-level%0Aclass%20semantics%20and%20low-level%20intra-class%20diversity%20to%20compensate%20for%20limited%0Asupport%20data.%20Furthermore%2C%20the%20CGA%20jointly%20aligns%20the%20fused%20textual%2C%20support%2C%0Aand%20synthetic%20visual%20representations%20by%20minimizing%20the%20kernelized%20volume%20of%20the%0A3-dimensional%20parallelotope%20they%20span.%20It%20captures%20global%20and%20nonlinear%0Arelationships%20among%20all%20representations%2C%20enabling%20structured%20and%20consistent%0Amultimodal%20integration.%20The%20proposed%20VT-FSL%20method%20establishes%20new%0Astate-of-the-art%20performance%20across%20ten%20diverse%20benchmarks%2C%20including%20standard%2C%0Across-domain%2C%20and%20fine-grained%20few-shot%20learning%20scenarios.%20Code%20is%20available%0Aat%20https%3A//github.com/peacelwh/VT-FSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVT-FSL%253A%2520Bridging%2520Vision%2520and%2520Text%2520with%2520LLMs%2520for%2520Few-Shot%2520Learning%26entry.906535625%3DWenhao%2520Li%2520and%2520Qiangchang%2520Wang%2520and%2520Xianjing%2520Meng%2520and%2520Zhibin%2520Wu%2520and%2520Yilong%2520Yin%26entry.1292438233%3D%2520%2520Few-shot%2520learning%2520%2528FSL%2529%2520aims%2520to%2520recognize%2520novel%2520concepts%2520from%2520only%2520a%2520few%250Alabeled%2520support%2520samples.%2520Recent%2520studies%2520enhance%2520support%2520features%2520by%250Aincorporating%2520additional%2520semantic%2520information%2520or%2520designing%2520complex%2520semantic%250Afusion%2520modules.%2520However%252C%2520they%2520still%2520suffer%2520from%2520hallucinating%2520semantics%2520that%250Acontradict%2520the%2520visual%2520evidence%2520due%2520to%2520the%2520lack%2520of%2520grounding%2520in%2520actual%250Ainstances%252C%2520resulting%2520in%2520noisy%2520guidance%2520and%2520costly%2520corrections.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520bridging%2520Vision%2520and%2520Text%2520with%2520LLMs%2520for%250AFew-Shot%2520Learning%2520%2528VT-FSL%2529%252C%2520which%2520constructs%2520precise%2520cross-modal%2520prompts%250Aconditioned%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520support%2520images%252C%2520seamlessly%250Aintegrating%2520them%2520through%2520a%2520geometry-aware%2520alignment.%2520It%2520mainly%2520consists%2520of%250ACross-modal%2520Iterative%2520Prompting%2520%2528CIP%2529%2520and%2520Cross-modal%2520Geometric%2520Alignment%250A%2528CGA%2529.%2520Specifically%252C%2520the%2520CIP%2520conditions%2520an%2520LLM%2520on%2520both%2520class%2520names%2520and%2520support%250Aimages%2520to%2520generate%2520precise%2520class%2520descriptions%2520iteratively%2520in%2520a%2520single%250Astructured%2520reasoning%2520pass.%2520These%2520descriptions%2520not%2520only%2520enrich%2520the%2520semantic%250Aunderstanding%2520of%2520novel%2520classes%2520but%2520also%2520enable%2520the%2520zero-shot%2520synthesis%2520of%250Asemantically%2520consistent%2520images.%2520The%2520descriptions%2520and%2520synthetic%2520images%2520act%250Arespectively%2520as%2520complementary%2520textual%2520and%2520visual%2520prompts%252C%2520providing%2520high-level%250Aclass%2520semantics%2520and%2520low-level%2520intra-class%2520diversity%2520to%2520compensate%2520for%2520limited%250Asupport%2520data.%2520Furthermore%252C%2520the%2520CGA%2520jointly%2520aligns%2520the%2520fused%2520textual%252C%2520support%252C%250Aand%2520synthetic%2520visual%2520representations%2520by%2520minimizing%2520the%2520kernelized%2520volume%2520of%2520the%250A3-dimensional%2520parallelotope%2520they%2520span.%2520It%2520captures%2520global%2520and%2520nonlinear%250Arelationships%2520among%2520all%2520representations%252C%2520enabling%2520structured%2520and%2520consistent%250Amultimodal%2520integration.%2520The%2520proposed%2520VT-FSL%2520method%2520establishes%2520new%250Astate-of-the-art%2520performance%2520across%2520ten%2520diverse%2520benchmarks%252C%2520including%2520standard%252C%250Across-domain%252C%2520and%2520fine-grained%2520few-shot%2520learning%2520scenarios.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/peacelwh/VT-FSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VT-FSL%3A%20Bridging%20Vision%20and%20Text%20with%20LLMs%20for%20Few-Shot%20Learning&entry.906535625=Wenhao%20Li%20and%20Qiangchang%20Wang%20and%20Xianjing%20Meng%20and%20Zhibin%20Wu%20and%20Yilong%20Yin&entry.1292438233=%20%20Few-shot%20learning%20%28FSL%29%20aims%20to%20recognize%20novel%20concepts%20from%20only%20a%20few%0Alabeled%20support%20samples.%20Recent%20studies%20enhance%20support%20features%20by%0Aincorporating%20additional%20semantic%20information%20or%20designing%20complex%20semantic%0Afusion%20modules.%20However%2C%20they%20still%20suffer%20from%20hallucinating%20semantics%20that%0Acontradict%20the%20visual%20evidence%20due%20to%20the%20lack%20of%20grounding%20in%20actual%0Ainstances%2C%20resulting%20in%20noisy%20guidance%20and%20costly%20corrections.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20novel%20framework%2C%20bridging%20Vision%20and%20Text%20with%20LLMs%20for%0AFew-Shot%20Learning%20%28VT-FSL%29%2C%20which%20constructs%20precise%20cross-modal%20prompts%0Aconditioned%20on%20Large%20Language%20Models%20%28LLMs%29%20and%20support%20images%2C%20seamlessly%0Aintegrating%20them%20through%20a%20geometry-aware%20alignment.%20It%20mainly%20consists%20of%0ACross-modal%20Iterative%20Prompting%20%28CIP%29%20and%20Cross-modal%20Geometric%20Alignment%0A%28CGA%29.%20Specifically%2C%20the%20CIP%20conditions%20an%20LLM%20on%20both%20class%20names%20and%20support%0Aimages%20to%20generate%20precise%20class%20descriptions%20iteratively%20in%20a%20single%0Astructured%20reasoning%20pass.%20These%20descriptions%20not%20only%20enrich%20the%20semantic%0Aunderstanding%20of%20novel%20classes%20but%20also%20enable%20the%20zero-shot%20synthesis%20of%0Asemantically%20consistent%20images.%20The%20descriptions%20and%20synthetic%20images%20act%0Arespectively%20as%20complementary%20textual%20and%20visual%20prompts%2C%20providing%20high-level%0Aclass%20semantics%20and%20low-level%20intra-class%20diversity%20to%20compensate%20for%20limited%0Asupport%20data.%20Furthermore%2C%20the%20CGA%20jointly%20aligns%20the%20fused%20textual%2C%20support%2C%0Aand%20synthetic%20visual%20representations%20by%20minimizing%20the%20kernelized%20volume%20of%20the%0A3-dimensional%20parallelotope%20they%20span.%20It%20captures%20global%20and%20nonlinear%0Arelationships%20among%20all%20representations%2C%20enabling%20structured%20and%20consistent%0Amultimodal%20integration.%20The%20proposed%20VT-FSL%20method%20establishes%20new%0Astate-of-the-art%20performance%20across%20ten%20diverse%20benchmarks%2C%20including%20standard%2C%0Across-domain%2C%20and%20fine-grained%20few-shot%20learning%20scenarios.%20Code%20is%20available%0Aat%20https%3A//github.com/peacelwh/VT-FSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25033v1&entry.124074799=Read"},
{"title": "Training-Free Token Pruning via Zeroth-Order Gradient Estimation in\n  Vision-Language Models", "author": "Youngeun Kim and Youjia Zhang and Huiling Liu and Aecheon Jung and Sunwoo Lee and Sungeun Hong", "abstract": "  Large Vision-Language Models (VLMs) enable strong multimodal reasoning but\nincur heavy inference costs from redundant visual tokens. Token pruning\nalleviates this issue, yet existing approaches face limitations.\nAttention-based methods rely on raw attention scores, which are often unstable\nacross layers and heads and can lead to redundant selections. Diversity-based\nmethods improve robustness by selecting tokens far apart in feature space but\nrisk dropping regions needed for accurate prediction. We propose \\ours, a\ntraining-free framework built on a simple intuition: tokens with higher\nsensitivity are more likely to influence the model's output, and they should\nalso capture complementary visual cues rather than overlapping information. To\nachieve this, we estimate token sensitivity using zeroth-order perturbations at\nthe projection layer, a shallow and computationally light component of the\nmodel. This approach measures how small random perturbations affect the\nprojection outputs, allowing us to approximate each token's influence through\nlightweight forward passes without backpropagation. Extensive experiments\nacross multiple VLMs and benchmarks show that \\ours consistently outperforms\nprior methods, pruning up to 94.4\\% of tokens while maintaining accuracy and\nsignificantly improving efficiency, achieving up to 2.30x faster end-to-end\ninference over the baseline.\n", "link": "http://arxiv.org/abs/2509.24837v1", "date": "2025-09-29", "relevancy": 2.8627, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Token%20Pruning%20via%20Zeroth-Order%20Gradient%20Estimation%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Training-Free%20Token%20Pruning%20via%20Zeroth-Order%20Gradient%20Estimation%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Youngeun%20Kim%20and%20Youjia%20Zhang%20and%20Huiling%20Liu%20and%20Aecheon%20Jung%20and%20Sunwoo%20Lee%20and%20Sungeun%20Hong%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20enable%20strong%20multimodal%20reasoning%20but%0Aincur%20heavy%20inference%20costs%20from%20redundant%20visual%20tokens.%20Token%20pruning%0Aalleviates%20this%20issue%2C%20yet%20existing%20approaches%20face%20limitations.%0AAttention-based%20methods%20rely%20on%20raw%20attention%20scores%2C%20which%20are%20often%20unstable%0Aacross%20layers%20and%20heads%20and%20can%20lead%20to%20redundant%20selections.%20Diversity-based%0Amethods%20improve%20robustness%20by%20selecting%20tokens%20far%20apart%20in%20feature%20space%20but%0Arisk%20dropping%20regions%20needed%20for%20accurate%20prediction.%20We%20propose%20%5Cours%2C%20a%0Atraining-free%20framework%20built%20on%20a%20simple%20intuition%3A%20tokens%20with%20higher%0Asensitivity%20are%20more%20likely%20to%20influence%20the%20model%27s%20output%2C%20and%20they%20should%0Aalso%20capture%20complementary%20visual%20cues%20rather%20than%20overlapping%20information.%20To%0Aachieve%20this%2C%20we%20estimate%20token%20sensitivity%20using%20zeroth-order%20perturbations%20at%0Athe%20projection%20layer%2C%20a%20shallow%20and%20computationally%20light%20component%20of%20the%0Amodel.%20This%20approach%20measures%20how%20small%20random%20perturbations%20affect%20the%0Aprojection%20outputs%2C%20allowing%20us%20to%20approximate%20each%20token%27s%20influence%20through%0Alightweight%20forward%20passes%20without%20backpropagation.%20Extensive%20experiments%0Aacross%20multiple%20VLMs%20and%20benchmarks%20show%20that%20%5Cours%20consistently%20outperforms%0Aprior%20methods%2C%20pruning%20up%20to%2094.4%5C%25%20of%20tokens%20while%20maintaining%20accuracy%20and%0Asignificantly%20improving%20efficiency%2C%20achieving%20up%20to%202.30x%20faster%20end-to-end%0Ainference%20over%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Token%2520Pruning%2520via%2520Zeroth-Order%2520Gradient%2520Estimation%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYoungeun%2520Kim%2520and%2520Youjia%2520Zhang%2520and%2520Huiling%2520Liu%2520and%2520Aecheon%2520Jung%2520and%2520Sunwoo%2520Lee%2520and%2520Sungeun%2520Hong%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520enable%2520strong%2520multimodal%2520reasoning%2520but%250Aincur%2520heavy%2520inference%2520costs%2520from%2520redundant%2520visual%2520tokens.%2520Token%2520pruning%250Aalleviates%2520this%2520issue%252C%2520yet%2520existing%2520approaches%2520face%2520limitations.%250AAttention-based%2520methods%2520rely%2520on%2520raw%2520attention%2520scores%252C%2520which%2520are%2520often%2520unstable%250Aacross%2520layers%2520and%2520heads%2520and%2520can%2520lead%2520to%2520redundant%2520selections.%2520Diversity-based%250Amethods%2520improve%2520robustness%2520by%2520selecting%2520tokens%2520far%2520apart%2520in%2520feature%2520space%2520but%250Arisk%2520dropping%2520regions%2520needed%2520for%2520accurate%2520prediction.%2520We%2520propose%2520%255Cours%252C%2520a%250Atraining-free%2520framework%2520built%2520on%2520a%2520simple%2520intuition%253A%2520tokens%2520with%2520higher%250Asensitivity%2520are%2520more%2520likely%2520to%2520influence%2520the%2520model%2527s%2520output%252C%2520and%2520they%2520should%250Aalso%2520capture%2520complementary%2520visual%2520cues%2520rather%2520than%2520overlapping%2520information.%2520To%250Aachieve%2520this%252C%2520we%2520estimate%2520token%2520sensitivity%2520using%2520zeroth-order%2520perturbations%2520at%250Athe%2520projection%2520layer%252C%2520a%2520shallow%2520and%2520computationally%2520light%2520component%2520of%2520the%250Amodel.%2520This%2520approach%2520measures%2520how%2520small%2520random%2520perturbations%2520affect%2520the%250Aprojection%2520outputs%252C%2520allowing%2520us%2520to%2520approximate%2520each%2520token%2527s%2520influence%2520through%250Alightweight%2520forward%2520passes%2520without%2520backpropagation.%2520Extensive%2520experiments%250Aacross%2520multiple%2520VLMs%2520and%2520benchmarks%2520show%2520that%2520%255Cours%2520consistently%2520outperforms%250Aprior%2520methods%252C%2520pruning%2520up%2520to%252094.4%255C%2525%2520of%2520tokens%2520while%2520maintaining%2520accuracy%2520and%250Asignificantly%2520improving%2520efficiency%252C%2520achieving%2520up%2520to%25202.30x%2520faster%2520end-to-end%250Ainference%2520over%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Token%20Pruning%20via%20Zeroth-Order%20Gradient%20Estimation%20in%0A%20%20Vision-Language%20Models&entry.906535625=Youngeun%20Kim%20and%20Youjia%20Zhang%20and%20Huiling%20Liu%20and%20Aecheon%20Jung%20and%20Sunwoo%20Lee%20and%20Sungeun%20Hong&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20enable%20strong%20multimodal%20reasoning%20but%0Aincur%20heavy%20inference%20costs%20from%20redundant%20visual%20tokens.%20Token%20pruning%0Aalleviates%20this%20issue%2C%20yet%20existing%20approaches%20face%20limitations.%0AAttention-based%20methods%20rely%20on%20raw%20attention%20scores%2C%20which%20are%20often%20unstable%0Aacross%20layers%20and%20heads%20and%20can%20lead%20to%20redundant%20selections.%20Diversity-based%0Amethods%20improve%20robustness%20by%20selecting%20tokens%20far%20apart%20in%20feature%20space%20but%0Arisk%20dropping%20regions%20needed%20for%20accurate%20prediction.%20We%20propose%20%5Cours%2C%20a%0Atraining-free%20framework%20built%20on%20a%20simple%20intuition%3A%20tokens%20with%20higher%0Asensitivity%20are%20more%20likely%20to%20influence%20the%20model%27s%20output%2C%20and%20they%20should%0Aalso%20capture%20complementary%20visual%20cues%20rather%20than%20overlapping%20information.%20To%0Aachieve%20this%2C%20we%20estimate%20token%20sensitivity%20using%20zeroth-order%20perturbations%20at%0Athe%20projection%20layer%2C%20a%20shallow%20and%20computationally%20light%20component%20of%20the%0Amodel.%20This%20approach%20measures%20how%20small%20random%20perturbations%20affect%20the%0Aprojection%20outputs%2C%20allowing%20us%20to%20approximate%20each%20token%27s%20influence%20through%0Alightweight%20forward%20passes%20without%20backpropagation.%20Extensive%20experiments%0Aacross%20multiple%20VLMs%20and%20benchmarks%20show%20that%20%5Cours%20consistently%20outperforms%0Aprior%20methods%2C%20pruning%20up%20to%2094.4%5C%25%20of%20tokens%20while%20maintaining%20accuracy%20and%0Asignificantly%20improving%20efficiency%2C%20achieving%20up%20to%202.30x%20faster%20end-to-end%0Ainference%20over%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24837v1&entry.124074799=Read"},
{"title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning", "author": "Shenghao Fu and Qize Yang and Yuan-Ming Li and Xihan Wei and Xiaohua Xie and Wei-Shi Zheng", "abstract": "  Long video understanding is still challenging for recent Large Video-Language\nModels (LVLMs) due to the conflict between long-form temporal understanding and\ndetailed spatial perception. LVLMs with a uniform frame sampling mechanism,\nwhich samples frames with an equal frame size and fixed sampling rate,\ninevitably sacrifice either temporal clues or spatial details, resulting in\nsuboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model\nthat can adaptively zoom in on a video clip. The model is first provided with\ndensely sampled frames but in a small resolution. If some spatial details are\nneeded, the model can zoom in on a clip of interest with a large frame\nresolution based on its reasoning until key visual information is obtained. The\nwhole process is implemented as a multi-step reasoning process. To train the\nreasoning ability, we first finetune the model on our collected 38k\nhigh-quality CoT data and enhance it with decoupled reinforcement finetuning.\nAs outcome rewards can not provide fine-grained process supervision, we\ndecouple multi-step reasoning into multiple single-step reasoning and optimize\nthe internal zoom-in ability explicitly. Experiments on long video\nunderstanding benchmarks show that our model with the slow-fast adaptive frame\nsampling mechanism achieves a great trade-off between sampling density and\nframe resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an\naverage of 3.1% points across 4 common long video understanding benchmarks.\n", "link": "http://arxiv.org/abs/2509.24786v1", "date": "2025-09-29", "relevancy": 2.8519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOVE-R1%3A%20Advancing%20Long%20Video%20Understanding%20with%20an%20Adaptive%20Zoom-in%0A%20%20Mechanism%20via%20Multi-Step%20Reasoning&body=Title%3A%20LOVE-R1%3A%20Advancing%20Long%20Video%20Understanding%20with%20an%20Adaptive%20Zoom-in%0A%20%20Mechanism%20via%20Multi-Step%20Reasoning%0AAuthor%3A%20Shenghao%20Fu%20and%20Qize%20Yang%20and%20Yuan-Ming%20Li%20and%20Xihan%20Wei%20and%20Xiaohua%20Xie%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20%20%20Long%20video%20understanding%20is%20still%20challenging%20for%20recent%20Large%20Video-Language%0AModels%20%28LVLMs%29%20due%20to%20the%20conflict%20between%20long-form%20temporal%20understanding%20and%0Adetailed%20spatial%20perception.%20LVLMs%20with%20a%20uniform%20frame%20sampling%20mechanism%2C%0Awhich%20samples%20frames%20with%20an%20equal%20frame%20size%20and%20fixed%20sampling%20rate%2C%0Ainevitably%20sacrifice%20either%20temporal%20clues%20or%20spatial%20details%2C%20resulting%20in%0Asuboptimal%20solutions.%20To%20mitigate%20this%20dilemma%2C%20we%20propose%20LOVE-R1%2C%20a%20model%0Athat%20can%20adaptively%20zoom%20in%20on%20a%20video%20clip.%20The%20model%20is%20first%20provided%20with%0Adensely%20sampled%20frames%20but%20in%20a%20small%20resolution.%20If%20some%20spatial%20details%20are%0Aneeded%2C%20the%20model%20can%20zoom%20in%20on%20a%20clip%20of%20interest%20with%20a%20large%20frame%0Aresolution%20based%20on%20its%20reasoning%20until%20key%20visual%20information%20is%20obtained.%20The%0Awhole%20process%20is%20implemented%20as%20a%20multi-step%20reasoning%20process.%20To%20train%20the%0Areasoning%20ability%2C%20we%20first%20finetune%20the%20model%20on%20our%20collected%2038k%0Ahigh-quality%20CoT%20data%20and%20enhance%20it%20with%20decoupled%20reinforcement%20finetuning.%0AAs%20outcome%20rewards%20can%20not%20provide%20fine-grained%20process%20supervision%2C%20we%0Adecouple%20multi-step%20reasoning%20into%20multiple%20single-step%20reasoning%20and%20optimize%0Athe%20internal%20zoom-in%20ability%20explicitly.%20Experiments%20on%20long%20video%0Aunderstanding%20benchmarks%20show%20that%20our%20model%20with%20the%20slow-fast%20adaptive%20frame%0Asampling%20mechanism%20achieves%20a%20great%20trade-off%20between%20sampling%20density%20and%0Aframe%20resolutions%2C%20and%20LOVE-R1%20outperforms%20our%20baseline%20Qwen2.5-VL%20by%20an%0Aaverage%20of%203.1%25%20points%20across%204%20common%20long%20video%20understanding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOVE-R1%253A%2520Advancing%2520Long%2520Video%2520Understanding%2520with%2520an%2520Adaptive%2520Zoom-in%250A%2520%2520Mechanism%2520via%2520Multi-Step%2520Reasoning%26entry.906535625%3DShenghao%2520Fu%2520and%2520Qize%2520Yang%2520and%2520Yuan-Ming%2520Li%2520and%2520Xihan%2520Wei%2520and%2520Xiaohua%2520Xie%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3D%2520%2520Long%2520video%2520understanding%2520is%2520still%2520challenging%2520for%2520recent%2520Large%2520Video-Language%250AModels%2520%2528LVLMs%2529%2520due%2520to%2520the%2520conflict%2520between%2520long-form%2520temporal%2520understanding%2520and%250Adetailed%2520spatial%2520perception.%2520LVLMs%2520with%2520a%2520uniform%2520frame%2520sampling%2520mechanism%252C%250Awhich%2520samples%2520frames%2520with%2520an%2520equal%2520frame%2520size%2520and%2520fixed%2520sampling%2520rate%252C%250Ainevitably%2520sacrifice%2520either%2520temporal%2520clues%2520or%2520spatial%2520details%252C%2520resulting%2520in%250Asuboptimal%2520solutions.%2520To%2520mitigate%2520this%2520dilemma%252C%2520we%2520propose%2520LOVE-R1%252C%2520a%2520model%250Athat%2520can%2520adaptively%2520zoom%2520in%2520on%2520a%2520video%2520clip.%2520The%2520model%2520is%2520first%2520provided%2520with%250Adensely%2520sampled%2520frames%2520but%2520in%2520a%2520small%2520resolution.%2520If%2520some%2520spatial%2520details%2520are%250Aneeded%252C%2520the%2520model%2520can%2520zoom%2520in%2520on%2520a%2520clip%2520of%2520interest%2520with%2520a%2520large%2520frame%250Aresolution%2520based%2520on%2520its%2520reasoning%2520until%2520key%2520visual%2520information%2520is%2520obtained.%2520The%250Awhole%2520process%2520is%2520implemented%2520as%2520a%2520multi-step%2520reasoning%2520process.%2520To%2520train%2520the%250Areasoning%2520ability%252C%2520we%2520first%2520finetune%2520the%2520model%2520on%2520our%2520collected%252038k%250Ahigh-quality%2520CoT%2520data%2520and%2520enhance%2520it%2520with%2520decoupled%2520reinforcement%2520finetuning.%250AAs%2520outcome%2520rewards%2520can%2520not%2520provide%2520fine-grained%2520process%2520supervision%252C%2520we%250Adecouple%2520multi-step%2520reasoning%2520into%2520multiple%2520single-step%2520reasoning%2520and%2520optimize%250Athe%2520internal%2520zoom-in%2520ability%2520explicitly.%2520Experiments%2520on%2520long%2520video%250Aunderstanding%2520benchmarks%2520show%2520that%2520our%2520model%2520with%2520the%2520slow-fast%2520adaptive%2520frame%250Asampling%2520mechanism%2520achieves%2520a%2520great%2520trade-off%2520between%2520sampling%2520density%2520and%250Aframe%2520resolutions%252C%2520and%2520LOVE-R1%2520outperforms%2520our%2520baseline%2520Qwen2.5-VL%2520by%2520an%250Aaverage%2520of%25203.1%2525%2520points%2520across%25204%2520common%2520long%2520video%2520understanding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOVE-R1%3A%20Advancing%20Long%20Video%20Understanding%20with%20an%20Adaptive%20Zoom-in%0A%20%20Mechanism%20via%20Multi-Step%20Reasoning&entry.906535625=Shenghao%20Fu%20and%20Qize%20Yang%20and%20Yuan-Ming%20Li%20and%20Xihan%20Wei%20and%20Xiaohua%20Xie%20and%20Wei-Shi%20Zheng&entry.1292438233=%20%20Long%20video%20understanding%20is%20still%20challenging%20for%20recent%20Large%20Video-Language%0AModels%20%28LVLMs%29%20due%20to%20the%20conflict%20between%20long-form%20temporal%20understanding%20and%0Adetailed%20spatial%20perception.%20LVLMs%20with%20a%20uniform%20frame%20sampling%20mechanism%2C%0Awhich%20samples%20frames%20with%20an%20equal%20frame%20size%20and%20fixed%20sampling%20rate%2C%0Ainevitably%20sacrifice%20either%20temporal%20clues%20or%20spatial%20details%2C%20resulting%20in%0Asuboptimal%20solutions.%20To%20mitigate%20this%20dilemma%2C%20we%20propose%20LOVE-R1%2C%20a%20model%0Athat%20can%20adaptively%20zoom%20in%20on%20a%20video%20clip.%20The%20model%20is%20first%20provided%20with%0Adensely%20sampled%20frames%20but%20in%20a%20small%20resolution.%20If%20some%20spatial%20details%20are%0Aneeded%2C%20the%20model%20can%20zoom%20in%20on%20a%20clip%20of%20interest%20with%20a%20large%20frame%0Aresolution%20based%20on%20its%20reasoning%20until%20key%20visual%20information%20is%20obtained.%20The%0Awhole%20process%20is%20implemented%20as%20a%20multi-step%20reasoning%20process.%20To%20train%20the%0Areasoning%20ability%2C%20we%20first%20finetune%20the%20model%20on%20our%20collected%2038k%0Ahigh-quality%20CoT%20data%20and%20enhance%20it%20with%20decoupled%20reinforcement%20finetuning.%0AAs%20outcome%20rewards%20can%20not%20provide%20fine-grained%20process%20supervision%2C%20we%0Adecouple%20multi-step%20reasoning%20into%20multiple%20single-step%20reasoning%20and%20optimize%0Athe%20internal%20zoom-in%20ability%20explicitly.%20Experiments%20on%20long%20video%0Aunderstanding%20benchmarks%20show%20that%20our%20model%20with%20the%20slow-fast%20adaptive%20frame%0Asampling%20mechanism%20achieves%20a%20great%20trade-off%20between%20sampling%20density%20and%0Aframe%20resolutions%2C%20and%20LOVE-R1%20outperforms%20our%20baseline%20Qwen2.5-VL%20by%20an%0Aaverage%20of%203.1%25%20points%20across%204%20common%20long%20video%20understanding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24786v1&entry.124074799=Read"},
{"title": "BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation", "author": "Dingning Liu and Haoyu Guo and Jingyi Zhou and Tong He", "abstract": "  Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.\n", "link": "http://arxiv.org/abs/2509.25077v1", "date": "2025-09-29", "relevancy": 2.8436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRIDGE%20--%20Building%20Reinforcement-Learning%20Depth-to-Image%20Data%20Generation%0A%20%20Engine%20for%20Monocular%20Depth%20Estimation&body=Title%3A%20BRIDGE%20--%20Building%20Reinforcement-Learning%20Depth-to-Image%20Data%20Generation%0A%20%20Engine%20for%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Dingning%20Liu%20and%20Haoyu%20Guo%20and%20Jingyi%20Zhou%20and%20Tong%20He%0AAbstract%3A%20%20%20Monocular%20Depth%20Estimation%20%28MDE%29%20is%20a%20foundational%20task%20for%20computer%20vision.%0ATraditional%20methods%20are%20limited%20by%20data%20scarcity%20and%20quality%2C%20hindering%20their%0Arobustness.%20To%20overcome%20this%2C%20we%20propose%20BRIDGE%2C%20an%20RL-optimized%20depth-to-image%0A%28D2I%29%20generation%20framework%20that%20synthesizes%20over%2020M%20realistic%20and%0Ageometrically%20accurate%20RGB%20images%2C%20each%20intrinsically%20paired%20with%20its%20ground%0Atruth%20depth%2C%20from%20diverse%20source%20depth%20maps.%20Then%20we%20train%20our%20depth%20estimation%0Amodel%20on%20this%20dataset%2C%20employing%20a%20hybrid%20supervision%20strategy%20that%20integrates%0Ateacher%20pseudo-labels%20with%20ground%20truth%20depth%20for%20comprehensive%20and%20robust%0Atraining.%20This%20innovative%20data%20generation%20and%20training%20paradigm%20enables%20BRIDGE%0Ato%20achieve%20breakthroughs%20in%20scale%20and%20domain%20diversity%2C%20consistently%0Aoutperforming%20existing%20state-of-the-art%20approaches%20quantitatively%20and%20in%0Acomplex%20scene%20detail%20capture%2C%20thereby%20fostering%20general%20and%20robust%20depth%0Afeatures.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//dingning-liu.github.io/bridge.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRIDGE%2520--%2520Building%2520Reinforcement-Learning%2520Depth-to-Image%2520Data%2520Generation%250A%2520%2520Engine%2520for%2520Monocular%2520Depth%2520Estimation%26entry.906535625%3DDingning%2520Liu%2520and%2520Haoyu%2520Guo%2520and%2520Jingyi%2520Zhou%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520Monocular%2520Depth%2520Estimation%2520%2528MDE%2529%2520is%2520a%2520foundational%2520task%2520for%2520computer%2520vision.%250ATraditional%2520methods%2520are%2520limited%2520by%2520data%2520scarcity%2520and%2520quality%252C%2520hindering%2520their%250Arobustness.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520BRIDGE%252C%2520an%2520RL-optimized%2520depth-to-image%250A%2528D2I%2529%2520generation%2520framework%2520that%2520synthesizes%2520over%252020M%2520realistic%2520and%250Ageometrically%2520accurate%2520RGB%2520images%252C%2520each%2520intrinsically%2520paired%2520with%2520its%2520ground%250Atruth%2520depth%252C%2520from%2520diverse%2520source%2520depth%2520maps.%2520Then%2520we%2520train%2520our%2520depth%2520estimation%250Amodel%2520on%2520this%2520dataset%252C%2520employing%2520a%2520hybrid%2520supervision%2520strategy%2520that%2520integrates%250Ateacher%2520pseudo-labels%2520with%2520ground%2520truth%2520depth%2520for%2520comprehensive%2520and%2520robust%250Atraining.%2520This%2520innovative%2520data%2520generation%2520and%2520training%2520paradigm%2520enables%2520BRIDGE%250Ato%2520achieve%2520breakthroughs%2520in%2520scale%2520and%2520domain%2520diversity%252C%2520consistently%250Aoutperforming%2520existing%2520state-of-the-art%2520approaches%2520quantitatively%2520and%2520in%250Acomplex%2520scene%2520detail%2520capture%252C%2520thereby%2520fostering%2520general%2520and%2520robust%2520depth%250Afeatures.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//dingning-liu.github.io/bridge.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRIDGE%20--%20Building%20Reinforcement-Learning%20Depth-to-Image%20Data%20Generation%0A%20%20Engine%20for%20Monocular%20Depth%20Estimation&entry.906535625=Dingning%20Liu%20and%20Haoyu%20Guo%20and%20Jingyi%20Zhou%20and%20Tong%20He&entry.1292438233=%20%20Monocular%20Depth%20Estimation%20%28MDE%29%20is%20a%20foundational%20task%20for%20computer%20vision.%0ATraditional%20methods%20are%20limited%20by%20data%20scarcity%20and%20quality%2C%20hindering%20their%0Arobustness.%20To%20overcome%20this%2C%20we%20propose%20BRIDGE%2C%20an%20RL-optimized%20depth-to-image%0A%28D2I%29%20generation%20framework%20that%20synthesizes%20over%2020M%20realistic%20and%0Ageometrically%20accurate%20RGB%20images%2C%20each%20intrinsically%20paired%20with%20its%20ground%0Atruth%20depth%2C%20from%20diverse%20source%20depth%20maps.%20Then%20we%20train%20our%20depth%20estimation%0Amodel%20on%20this%20dataset%2C%20employing%20a%20hybrid%20supervision%20strategy%20that%20integrates%0Ateacher%20pseudo-labels%20with%20ground%20truth%20depth%20for%20comprehensive%20and%20robust%0Atraining.%20This%20innovative%20data%20generation%20and%20training%20paradigm%20enables%20BRIDGE%0Ato%20achieve%20breakthroughs%20in%20scale%20and%20domain%20diversity%2C%20consistently%0Aoutperforming%20existing%20state-of-the-art%20approaches%20quantitatively%20and%20in%0Acomplex%20scene%20detail%20capture%2C%20thereby%20fostering%20general%20and%20robust%20depth%0Afeatures.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//dingning-liu.github.io/bridge.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25077v1&entry.124074799=Read"},
{"title": "Vision Function Layer in Multimodal LLMs", "author": "Cheng Shi and Yizhou Yu and Sibei Yang", "abstract": "  This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.\n", "link": "http://arxiv.org/abs/2509.24791v1", "date": "2025-09-29", "relevancy": 2.8239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Function%20Layer%20in%20Multimodal%20LLMs&body=Title%3A%20Vision%20Function%20Layer%20in%20Multimodal%20LLMs%0AAuthor%3A%20Cheng%20Shi%20and%20Yizhou%20Yu%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20This%20study%20identifies%20that%20visual-related%20functional%20decoding%20is%20distributed%0Aacross%20different%20decoder%20layers%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%0ATypically%2C%20each%20function%2C%20such%20as%20counting%2C%20grounding%2C%20or%20OCR%20recognition%2C%0Anarrows%20down%20to%20two%20or%20three%20layers%2C%20which%20we%20define%20as%20Vision%20Function%20Layers%0A%28VFL%29.%20Additionally%2C%20the%20depth%20and%20its%20order%20of%20different%20VFLs%20exhibits%20a%0Aconsistent%20pattern%20across%20different%20MLLMs%2C%20which%20is%20well-aligned%20with%20human%0Abehaviors%20%28e.g.%2C%20recognition%20occurs%20first%2C%20followed%20by%20counting%2C%20and%20then%0Agrounding%29.%20These%20findings%20are%20derived%20from%20Visual%20Token%20Swapping%2C%20our%20novel%0Aanalytical%20framework%20that%20modifies%20targeted%20KV%20cache%20entries%20to%20precisely%0Aelucidate%20layer-specific%20functions%20during%20decoding.%20Furthermore%2C%20these%20insights%0Aoffer%20substantial%20utility%20in%20tailoring%20MLLMs%20for%20real-world%20downstream%0Aapplications.%20For%20instance%2C%20when%20LoRA%20training%20is%20selectively%20applied%20to%20VFLs%0Awhose%20functions%20align%20with%20the%20training%20data%2C%20VFL-LoRA%20not%20only%20outperform%0Afull-LoRA%20but%20also%20prevent%20out-of-domain%20function%20forgetting.%20Moreover%2C%20by%0Aanalyzing%20the%20performance%20differential%20on%20training%20data%20when%20particular%20VFLs%0Aare%20ablated%2C%20VFL-select%20automatically%20classifies%20data%20by%20function%2C%20enabling%0Ahighly%20efficient%20data%20selection%20to%20directly%20bolster%20corresponding%20capabilities.%0AConsequently%2C%20VFL-select%20surpasses%20human%20experts%20in%20data%20selection%2C%20and%0Aachieves%2098%25%20of%20full-data%20performance%20with%20only%2020%25%20of%20the%20original%20dataset.%0AThis%20study%20delivers%20deeper%20comprehension%20of%20MLLM%20visual%20processing%2C%20fostering%0Athe%20creation%20of%20more%20efficient%2C%20interpretable%2C%20and%20robust%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Function%2520Layer%2520in%2520Multimodal%2520LLMs%26entry.906535625%3DCheng%2520Shi%2520and%2520Yizhou%2520Yu%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520This%2520study%2520identifies%2520that%2520visual-related%2520functional%2520decoding%2520is%2520distributed%250Aacross%2520different%2520decoder%2520layers%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%250ATypically%252C%2520each%2520function%252C%2520such%2520as%2520counting%252C%2520grounding%252C%2520or%2520OCR%2520recognition%252C%250Anarrows%2520down%2520to%2520two%2520or%2520three%2520layers%252C%2520which%2520we%2520define%2520as%2520Vision%2520Function%2520Layers%250A%2528VFL%2529.%2520Additionally%252C%2520the%2520depth%2520and%2520its%2520order%2520of%2520different%2520VFLs%2520exhibits%2520a%250Aconsistent%2520pattern%2520across%2520different%2520MLLMs%252C%2520which%2520is%2520well-aligned%2520with%2520human%250Abehaviors%2520%2528e.g.%252C%2520recognition%2520occurs%2520first%252C%2520followed%2520by%2520counting%252C%2520and%2520then%250Agrounding%2529.%2520These%2520findings%2520are%2520derived%2520from%2520Visual%2520Token%2520Swapping%252C%2520our%2520novel%250Aanalytical%2520framework%2520that%2520modifies%2520targeted%2520KV%2520cache%2520entries%2520to%2520precisely%250Aelucidate%2520layer-specific%2520functions%2520during%2520decoding.%2520Furthermore%252C%2520these%2520insights%250Aoffer%2520substantial%2520utility%2520in%2520tailoring%2520MLLMs%2520for%2520real-world%2520downstream%250Aapplications.%2520For%2520instance%252C%2520when%2520LoRA%2520training%2520is%2520selectively%2520applied%2520to%2520VFLs%250Awhose%2520functions%2520align%2520with%2520the%2520training%2520data%252C%2520VFL-LoRA%2520not%2520only%2520outperform%250Afull-LoRA%2520but%2520also%2520prevent%2520out-of-domain%2520function%2520forgetting.%2520Moreover%252C%2520by%250Aanalyzing%2520the%2520performance%2520differential%2520on%2520training%2520data%2520when%2520particular%2520VFLs%250Aare%2520ablated%252C%2520VFL-select%2520automatically%2520classifies%2520data%2520by%2520function%252C%2520enabling%250Ahighly%2520efficient%2520data%2520selection%2520to%2520directly%2520bolster%2520corresponding%2520capabilities.%250AConsequently%252C%2520VFL-select%2520surpasses%2520human%2520experts%2520in%2520data%2520selection%252C%2520and%250Aachieves%252098%2525%2520of%2520full-data%2520performance%2520with%2520only%252020%2525%2520of%2520the%2520original%2520dataset.%250AThis%2520study%2520delivers%2520deeper%2520comprehension%2520of%2520MLLM%2520visual%2520processing%252C%2520fostering%250Athe%2520creation%2520of%2520more%2520efficient%252C%2520interpretable%252C%2520and%2520robust%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Function%20Layer%20in%20Multimodal%20LLMs&entry.906535625=Cheng%20Shi%20and%20Yizhou%20Yu%20and%20Sibei%20Yang&entry.1292438233=%20%20This%20study%20identifies%20that%20visual-related%20functional%20decoding%20is%20distributed%0Aacross%20different%20decoder%20layers%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%0ATypically%2C%20each%20function%2C%20such%20as%20counting%2C%20grounding%2C%20or%20OCR%20recognition%2C%0Anarrows%20down%20to%20two%20or%20three%20layers%2C%20which%20we%20define%20as%20Vision%20Function%20Layers%0A%28VFL%29.%20Additionally%2C%20the%20depth%20and%20its%20order%20of%20different%20VFLs%20exhibits%20a%0Aconsistent%20pattern%20across%20different%20MLLMs%2C%20which%20is%20well-aligned%20with%20human%0Abehaviors%20%28e.g.%2C%20recognition%20occurs%20first%2C%20followed%20by%20counting%2C%20and%20then%0Agrounding%29.%20These%20findings%20are%20derived%20from%20Visual%20Token%20Swapping%2C%20our%20novel%0Aanalytical%20framework%20that%20modifies%20targeted%20KV%20cache%20entries%20to%20precisely%0Aelucidate%20layer-specific%20functions%20during%20decoding.%20Furthermore%2C%20these%20insights%0Aoffer%20substantial%20utility%20in%20tailoring%20MLLMs%20for%20real-world%20downstream%0Aapplications.%20For%20instance%2C%20when%20LoRA%20training%20is%20selectively%20applied%20to%20VFLs%0Awhose%20functions%20align%20with%20the%20training%20data%2C%20VFL-LoRA%20not%20only%20outperform%0Afull-LoRA%20but%20also%20prevent%20out-of-domain%20function%20forgetting.%20Moreover%2C%20by%0Aanalyzing%20the%20performance%20differential%20on%20training%20data%20when%20particular%20VFLs%0Aare%20ablated%2C%20VFL-select%20automatically%20classifies%20data%20by%20function%2C%20enabling%0Ahighly%20efficient%20data%20selection%20to%20directly%20bolster%20corresponding%20capabilities.%0AConsequently%2C%20VFL-select%20surpasses%20human%20experts%20in%20data%20selection%2C%20and%0Aachieves%2098%25%20of%20full-data%20performance%20with%20only%2020%25%20of%20the%20original%20dataset.%0AThis%20study%20delivers%20deeper%20comprehension%20of%20MLLM%20visual%20processing%2C%20fostering%0Athe%20creation%20of%20more%20efficient%2C%20interpretable%2C%20and%20robust%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24791v1&entry.124074799=Read"},
{"title": "TACO-Net: Topological Signatures Triumph in 3D Object Classification", "author": "Anirban Ghosh and Ayan Dutta", "abstract": "  3D object classification is a crucial problem due to its significant\npractical relevance in many fields, including computer vision, robotics, and\nautonomous driving. Although deep learning methods applied to point clouds\nsampled on CAD models of the objects and/or captured by LiDAR or RGBD cameras\nhave achieved remarkable success in recent years, achieving high classification\naccuracy remains a challenging problem due to the unordered point clouds and\ntheir irregularity and noise. To this end, we propose a novel state-of-the-art\n(SOTA) 3D object classification technique that combines topological data\nanalysis with various image filtration techniques to classify objects when they\nare represented using point clouds. We transform every point cloud into a\nvoxelized binary 3D image to extract distinguishing topological features. Next,\nwe train a lightweight one-dimensional Convolutional Neural Network (1D CNN)\nusing the extracted feature set from the training dataset. Our framework,\nTACO-Net, sets a new state-of-the-art by achieving $99.05\\%$ and $99.52\\%$\naccuracy on the widely used synthetic benchmarks ModelNet40 and ModelNet10, and\nfurther demonstrates its robustness on the large-scale real-world OmniObject3D\ndataset. When tested with ten different kinds of corrupted ModelNet40 inputs,\nthe proposed TACO-Net demonstrates strong resiliency overall.\n", "link": "http://arxiv.org/abs/2509.24802v1", "date": "2025-09-29", "relevancy": 2.8151, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.592}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5508}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACO-Net%3A%20Topological%20Signatures%20Triumph%20in%203D%20Object%20Classification&body=Title%3A%20TACO-Net%3A%20Topological%20Signatures%20Triumph%20in%203D%20Object%20Classification%0AAuthor%3A%20Anirban%20Ghosh%20and%20Ayan%20Dutta%0AAbstract%3A%20%20%203D%20object%20classification%20is%20a%20crucial%20problem%20due%20to%20its%20significant%0Apractical%20relevance%20in%20many%20fields%2C%20including%20computer%20vision%2C%20robotics%2C%20and%0Aautonomous%20driving.%20Although%20deep%20learning%20methods%20applied%20to%20point%20clouds%0Asampled%20on%20CAD%20models%20of%20the%20objects%20and/or%20captured%20by%20LiDAR%20or%20RGBD%20cameras%0Ahave%20achieved%20remarkable%20success%20in%20recent%20years%2C%20achieving%20high%20classification%0Aaccuracy%20remains%20a%20challenging%20problem%20due%20to%20the%20unordered%20point%20clouds%20and%0Atheir%20irregularity%20and%20noise.%20To%20this%20end%2C%20we%20propose%20a%20novel%20state-of-the-art%0A%28SOTA%29%203D%20object%20classification%20technique%20that%20combines%20topological%20data%0Aanalysis%20with%20various%20image%20filtration%20techniques%20to%20classify%20objects%20when%20they%0Aare%20represented%20using%20point%20clouds.%20We%20transform%20every%20point%20cloud%20into%20a%0Avoxelized%20binary%203D%20image%20to%20extract%20distinguishing%20topological%20features.%20Next%2C%0Awe%20train%20a%20lightweight%20one-dimensional%20Convolutional%20Neural%20Network%20%281D%20CNN%29%0Ausing%20the%20extracted%20feature%20set%20from%20the%20training%20dataset.%20Our%20framework%2C%0ATACO-Net%2C%20sets%20a%20new%20state-of-the-art%20by%20achieving%20%2499.05%5C%25%24%20and%20%2499.52%5C%25%24%0Aaccuracy%20on%20the%20widely%20used%20synthetic%20benchmarks%20ModelNet40%20and%20ModelNet10%2C%20and%0Afurther%20demonstrates%20its%20robustness%20on%20the%20large-scale%20real-world%20OmniObject3D%0Adataset.%20When%20tested%20with%20ten%20different%20kinds%20of%20corrupted%20ModelNet40%20inputs%2C%0Athe%20proposed%20TACO-Net%20demonstrates%20strong%20resiliency%20overall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACO-Net%253A%2520Topological%2520Signatures%2520Triumph%2520in%25203D%2520Object%2520Classification%26entry.906535625%3DAnirban%2520Ghosh%2520and%2520Ayan%2520Dutta%26entry.1292438233%3D%2520%25203D%2520object%2520classification%2520is%2520a%2520crucial%2520problem%2520due%2520to%2520its%2520significant%250Apractical%2520relevance%2520in%2520many%2520fields%252C%2520including%2520computer%2520vision%252C%2520robotics%252C%2520and%250Aautonomous%2520driving.%2520Although%2520deep%2520learning%2520methods%2520applied%2520to%2520point%2520clouds%250Asampled%2520on%2520CAD%2520models%2520of%2520the%2520objects%2520and/or%2520captured%2520by%2520LiDAR%2520or%2520RGBD%2520cameras%250Ahave%2520achieved%2520remarkable%2520success%2520in%2520recent%2520years%252C%2520achieving%2520high%2520classification%250Aaccuracy%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520unordered%2520point%2520clouds%2520and%250Atheir%2520irregularity%2520and%2520noise.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520state-of-the-art%250A%2528SOTA%2529%25203D%2520object%2520classification%2520technique%2520that%2520combines%2520topological%2520data%250Aanalysis%2520with%2520various%2520image%2520filtration%2520techniques%2520to%2520classify%2520objects%2520when%2520they%250Aare%2520represented%2520using%2520point%2520clouds.%2520We%2520transform%2520every%2520point%2520cloud%2520into%2520a%250Avoxelized%2520binary%25203D%2520image%2520to%2520extract%2520distinguishing%2520topological%2520features.%2520Next%252C%250Awe%2520train%2520a%2520lightweight%2520one-dimensional%2520Convolutional%2520Neural%2520Network%2520%25281D%2520CNN%2529%250Ausing%2520the%2520extracted%2520feature%2520set%2520from%2520the%2520training%2520dataset.%2520Our%2520framework%252C%250ATACO-Net%252C%2520sets%2520a%2520new%2520state-of-the-art%2520by%2520achieving%2520%252499.05%255C%2525%2524%2520and%2520%252499.52%255C%2525%2524%250Aaccuracy%2520on%2520the%2520widely%2520used%2520synthetic%2520benchmarks%2520ModelNet40%2520and%2520ModelNet10%252C%2520and%250Afurther%2520demonstrates%2520its%2520robustness%2520on%2520the%2520large-scale%2520real-world%2520OmniObject3D%250Adataset.%2520When%2520tested%2520with%2520ten%2520different%2520kinds%2520of%2520corrupted%2520ModelNet40%2520inputs%252C%250Athe%2520proposed%2520TACO-Net%2520demonstrates%2520strong%2520resiliency%2520overall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACO-Net%3A%20Topological%20Signatures%20Triumph%20in%203D%20Object%20Classification&entry.906535625=Anirban%20Ghosh%20and%20Ayan%20Dutta&entry.1292438233=%20%203D%20object%20classification%20is%20a%20crucial%20problem%20due%20to%20its%20significant%0Apractical%20relevance%20in%20many%20fields%2C%20including%20computer%20vision%2C%20robotics%2C%20and%0Aautonomous%20driving.%20Although%20deep%20learning%20methods%20applied%20to%20point%20clouds%0Asampled%20on%20CAD%20models%20of%20the%20objects%20and/or%20captured%20by%20LiDAR%20or%20RGBD%20cameras%0Ahave%20achieved%20remarkable%20success%20in%20recent%20years%2C%20achieving%20high%20classification%0Aaccuracy%20remains%20a%20challenging%20problem%20due%20to%20the%20unordered%20point%20clouds%20and%0Atheir%20irregularity%20and%20noise.%20To%20this%20end%2C%20we%20propose%20a%20novel%20state-of-the-art%0A%28SOTA%29%203D%20object%20classification%20technique%20that%20combines%20topological%20data%0Aanalysis%20with%20various%20image%20filtration%20techniques%20to%20classify%20objects%20when%20they%0Aare%20represented%20using%20point%20clouds.%20We%20transform%20every%20point%20cloud%20into%20a%0Avoxelized%20binary%203D%20image%20to%20extract%20distinguishing%20topological%20features.%20Next%2C%0Awe%20train%20a%20lightweight%20one-dimensional%20Convolutional%20Neural%20Network%20%281D%20CNN%29%0Ausing%20the%20extracted%20feature%20set%20from%20the%20training%20dataset.%20Our%20framework%2C%0ATACO-Net%2C%20sets%20a%20new%20state-of-the-art%20by%20achieving%20%2499.05%5C%25%24%20and%20%2499.52%5C%25%24%0Aaccuracy%20on%20the%20widely%20used%20synthetic%20benchmarks%20ModelNet40%20and%20ModelNet10%2C%20and%0Afurther%20demonstrates%20its%20robustness%20on%20the%20large-scale%20real-world%20OmniObject3D%0Adataset.%20When%20tested%20with%20ten%20different%20kinds%20of%20corrupted%20ModelNet40%20inputs%2C%0Athe%20proposed%20TACO-Net%20demonstrates%20strong%20resiliency%20overall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24802v1&entry.124074799=Read"},
{"title": "DAM: Dual Active Learning with Multimodal Foundation Model for\n  Source-Free Domain Adaptation", "author": "Xi Chen and Hongxun Yao and Zhaopan Xu and Kui Jiang", "abstract": "  Source-free active domain adaptation (SFADA) enhances knowledge transfer from\na source model to an unlabeled target domain using limited manual labels\nselected via active learning. While recent domain adaptation studies have\nintroduced Vision-and-Language (ViL) models to improve pseudo-label quality or\nfeature alignment, they often treat ViL-based and data supervision as separate\nsources, lacking effective fusion. To overcome this limitation, we propose Dual\nActive learning with Multimodal (DAM) foundation model, a novel framework that\nintegrates multimodal supervision from a ViL model to complement sparse human\nannotations, thereby forming a dual supervisory signal. DAM initializes stable\nViL-guided targets and employs a bidirectional distillation mechanism to foster\nmutual knowledge exchange between the target model and the dual supervisions\nduring iterative adaptation. Extensive experiments demonstrate that DAM\nconsistently outperforms existing methods and sets a new state-of-the-art\nacross multiple SFADA benchmarks and active learning strategies.\n", "link": "http://arxiv.org/abs/2509.24896v1", "date": "2025-09-29", "relevancy": 2.7925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAM%3A%20Dual%20Active%20Learning%20with%20Multimodal%20Foundation%20Model%20for%0A%20%20Source-Free%20Domain%20Adaptation&body=Title%3A%20DAM%3A%20Dual%20Active%20Learning%20with%20Multimodal%20Foundation%20Model%20for%0A%20%20Source-Free%20Domain%20Adaptation%0AAuthor%3A%20Xi%20Chen%20and%20Hongxun%20Yao%20and%20Zhaopan%20Xu%20and%20Kui%20Jiang%0AAbstract%3A%20%20%20Source-free%20active%20domain%20adaptation%20%28SFADA%29%20enhances%20knowledge%20transfer%20from%0Aa%20source%20model%20to%20an%20unlabeled%20target%20domain%20using%20limited%20manual%20labels%0Aselected%20via%20active%20learning.%20While%20recent%20domain%20adaptation%20studies%20have%0Aintroduced%20Vision-and-Language%20%28ViL%29%20models%20to%20improve%20pseudo-label%20quality%20or%0Afeature%20alignment%2C%20they%20often%20treat%20ViL-based%20and%20data%20supervision%20as%20separate%0Asources%2C%20lacking%20effective%20fusion.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Dual%0AActive%20learning%20with%20Multimodal%20%28DAM%29%20foundation%20model%2C%20a%20novel%20framework%20that%0Aintegrates%20multimodal%20supervision%20from%20a%20ViL%20model%20to%20complement%20sparse%20human%0Aannotations%2C%20thereby%20forming%20a%20dual%20supervisory%20signal.%20DAM%20initializes%20stable%0AViL-guided%20targets%20and%20employs%20a%20bidirectional%20distillation%20mechanism%20to%20foster%0Amutual%20knowledge%20exchange%20between%20the%20target%20model%20and%20the%20dual%20supervisions%0Aduring%20iterative%20adaptation.%20Extensive%20experiments%20demonstrate%20that%20DAM%0Aconsistently%20outperforms%20existing%20methods%20and%20sets%20a%20new%20state-of-the-art%0Aacross%20multiple%20SFADA%20benchmarks%20and%20active%20learning%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAM%253A%2520Dual%2520Active%2520Learning%2520with%2520Multimodal%2520Foundation%2520Model%2520for%250A%2520%2520Source-Free%2520Domain%2520Adaptation%26entry.906535625%3DXi%2520Chen%2520and%2520Hongxun%2520Yao%2520and%2520Zhaopan%2520Xu%2520and%2520Kui%2520Jiang%26entry.1292438233%3D%2520%2520Source-free%2520active%2520domain%2520adaptation%2520%2528SFADA%2529%2520enhances%2520knowledge%2520transfer%2520from%250Aa%2520source%2520model%2520to%2520an%2520unlabeled%2520target%2520domain%2520using%2520limited%2520manual%2520labels%250Aselected%2520via%2520active%2520learning.%2520While%2520recent%2520domain%2520adaptation%2520studies%2520have%250Aintroduced%2520Vision-and-Language%2520%2528ViL%2529%2520models%2520to%2520improve%2520pseudo-label%2520quality%2520or%250Afeature%2520alignment%252C%2520they%2520often%2520treat%2520ViL-based%2520and%2520data%2520supervision%2520as%2520separate%250Asources%252C%2520lacking%2520effective%2520fusion.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Dual%250AActive%2520learning%2520with%2520Multimodal%2520%2528DAM%2529%2520foundation%2520model%252C%2520a%2520novel%2520framework%2520that%250Aintegrates%2520multimodal%2520supervision%2520from%2520a%2520ViL%2520model%2520to%2520complement%2520sparse%2520human%250Aannotations%252C%2520thereby%2520forming%2520a%2520dual%2520supervisory%2520signal.%2520DAM%2520initializes%2520stable%250AViL-guided%2520targets%2520and%2520employs%2520a%2520bidirectional%2520distillation%2520mechanism%2520to%2520foster%250Amutual%2520knowledge%2520exchange%2520between%2520the%2520target%2520model%2520and%2520the%2520dual%2520supervisions%250Aduring%2520iterative%2520adaptation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DAM%250Aconsistently%2520outperforms%2520existing%2520methods%2520and%2520sets%2520a%2520new%2520state-of-the-art%250Aacross%2520multiple%2520SFADA%2520benchmarks%2520and%2520active%2520learning%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAM%3A%20Dual%20Active%20Learning%20with%20Multimodal%20Foundation%20Model%20for%0A%20%20Source-Free%20Domain%20Adaptation&entry.906535625=Xi%20Chen%20and%20Hongxun%20Yao%20and%20Zhaopan%20Xu%20and%20Kui%20Jiang&entry.1292438233=%20%20Source-free%20active%20domain%20adaptation%20%28SFADA%29%20enhances%20knowledge%20transfer%20from%0Aa%20source%20model%20to%20an%20unlabeled%20target%20domain%20using%20limited%20manual%20labels%0Aselected%20via%20active%20learning.%20While%20recent%20domain%20adaptation%20studies%20have%0Aintroduced%20Vision-and-Language%20%28ViL%29%20models%20to%20improve%20pseudo-label%20quality%20or%0Afeature%20alignment%2C%20they%20often%20treat%20ViL-based%20and%20data%20supervision%20as%20separate%0Asources%2C%20lacking%20effective%20fusion.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Dual%0AActive%20learning%20with%20Multimodal%20%28DAM%29%20foundation%20model%2C%20a%20novel%20framework%20that%0Aintegrates%20multimodal%20supervision%20from%20a%20ViL%20model%20to%20complement%20sparse%20human%0Aannotations%2C%20thereby%20forming%20a%20dual%20supervisory%20signal.%20DAM%20initializes%20stable%0AViL-guided%20targets%20and%20employs%20a%20bidirectional%20distillation%20mechanism%20to%20foster%0Amutual%20knowledge%20exchange%20between%20the%20target%20model%20and%20the%20dual%20supervisions%0Aduring%20iterative%20adaptation.%20Extensive%20experiments%20demonstrate%20that%20DAM%0Aconsistently%20outperforms%20existing%20methods%20and%20sets%20a%20new%20state-of-the-art%0Aacross%20multiple%20SFADA%20benchmarks%20and%20active%20learning%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24896v1&entry.124074799=Read"},
{"title": "TemMed-Bench: Evaluating Temporal Medical Image Reasoning in\n  Vision-Language Models", "author": "Junyi Zhang and Jia-Chen Gu and Wenbo Hu and Yu Zhou and Robinson Piramuthu and Nanyun Peng", "abstract": "  Existing medical reasoning benchmarks for vision-language models primarily\nfocus on analyzing a patient's condition based on an image from a single visit.\nHowever, this setting deviates significantly from real-world clinical practice,\nwhere doctors typically refer to a patient's historical conditions to provide a\ncomprehensive assessment by tracking their changes over time. In this paper, we\nintroduce TemMed-Bench, the first benchmark designed for analyzing changes in\npatients' conditions between different clinical visits, which challenges large\nvision-language models (LVLMs) to reason over temporal medical images.\nTemMed-Bench consists of a test set comprising three tasks - visual\nquestion-answering (VQA), report generation, and image-pair selection - and a\nsupplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we\nconduct an evaluation of six proprietary and six open-source LVLMs. Our results\nshow that most LVLMs lack the ability to analyze patients' condition changes\nover temporal medical images, and a large proportion perform only at a\nrandom-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini\nand Claude 3.5 Sonnet demonstrate comparatively decent performance, though they\nhave yet to reach the desired level. Furthermore, we explore augmenting the\ninput with both retrieved visual and textual modalities in the medical domain.\nWe also show that multi-modal retrieval augmentation yields notably higher\nperformance gains than no retrieval and textual retrieval alone across most\nmodels on our benchmark, with the VQA task showing an average improvement of\n2.59%. Overall, we compose a benchmark grounded on real-world clinical\npractice, and it reveals LVLMs' limitations in temporal medical image\nreasoning, as well as highlighting the use of multi-modal retrieval\naugmentation as a potentially promising direction worth exploring to address\nthis challenge.\n", "link": "http://arxiv.org/abs/2509.25143v1", "date": "2025-09-29", "relevancy": 2.7854, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TemMed-Bench%3A%20Evaluating%20Temporal%20Medical%20Image%20Reasoning%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20TemMed-Bench%3A%20Evaluating%20Temporal%20Medical%20Image%20Reasoning%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Junyi%20Zhang%20and%20Jia-Chen%20Gu%20and%20Wenbo%20Hu%20and%20Yu%20Zhou%20and%20Robinson%20Piramuthu%20and%20Nanyun%20Peng%0AAbstract%3A%20%20%20Existing%20medical%20reasoning%20benchmarks%20for%20vision-language%20models%20primarily%0Afocus%20on%20analyzing%20a%20patient%27s%20condition%20based%20on%20an%20image%20from%20a%20single%20visit.%0AHowever%2C%20this%20setting%20deviates%20significantly%20from%20real-world%20clinical%20practice%2C%0Awhere%20doctors%20typically%20refer%20to%20a%20patient%27s%20historical%20conditions%20to%20provide%20a%0Acomprehensive%20assessment%20by%20tracking%20their%20changes%20over%20time.%20In%20this%20paper%2C%20we%0Aintroduce%20TemMed-Bench%2C%20the%20first%20benchmark%20designed%20for%20analyzing%20changes%20in%0Apatients%27%20conditions%20between%20different%20clinical%20visits%2C%20which%20challenges%20large%0Avision-language%20models%20%28LVLMs%29%20to%20reason%20over%20temporal%20medical%20images.%0ATemMed-Bench%20consists%20of%20a%20test%20set%20comprising%20three%20tasks%20-%20visual%0Aquestion-answering%20%28VQA%29%2C%20report%20generation%2C%20and%20image-pair%20selection%20-%20and%20a%0Asupplementary%20knowledge%20corpus%20of%20over%2017%2C000%20instances.%20With%20TemMed-Bench%2C%20we%0Aconduct%20an%20evaluation%20of%20six%20proprietary%20and%20six%20open-source%20LVLMs.%20Our%20results%0Ashow%20that%20most%20LVLMs%20lack%20the%20ability%20to%20analyze%20patients%27%20condition%20changes%0Aover%20temporal%20medical%20images%2C%20and%20a%20large%20proportion%20perform%20only%20at%20a%0Arandom-guessing%20level%20in%20the%20closed-book%20setting.%20In%20contrast%2C%20GPT%20o3%2C%20o4-mini%0Aand%20Claude%203.5%20Sonnet%20demonstrate%20comparatively%20decent%20performance%2C%20though%20they%0Ahave%20yet%20to%20reach%20the%20desired%20level.%20Furthermore%2C%20we%20explore%20augmenting%20the%0Ainput%20with%20both%20retrieved%20visual%20and%20textual%20modalities%20in%20the%20medical%20domain.%0AWe%20also%20show%20that%20multi-modal%20retrieval%20augmentation%20yields%20notably%20higher%0Aperformance%20gains%20than%20no%20retrieval%20and%20textual%20retrieval%20alone%20across%20most%0Amodels%20on%20our%20benchmark%2C%20with%20the%20VQA%20task%20showing%20an%20average%20improvement%20of%0A2.59%25.%20Overall%2C%20we%20compose%20a%20benchmark%20grounded%20on%20real-world%20clinical%0Apractice%2C%20and%20it%20reveals%20LVLMs%27%20limitations%20in%20temporal%20medical%20image%0Areasoning%2C%20as%20well%20as%20highlighting%20the%20use%20of%20multi-modal%20retrieval%0Aaugmentation%20as%20a%20potentially%20promising%20direction%20worth%20exploring%20to%20address%0Athis%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemMed-Bench%253A%2520Evaluating%2520Temporal%2520Medical%2520Image%2520Reasoning%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DJunyi%2520Zhang%2520and%2520Jia-Chen%2520Gu%2520and%2520Wenbo%2520Hu%2520and%2520Yu%2520Zhou%2520and%2520Robinson%2520Piramuthu%2520and%2520Nanyun%2520Peng%26entry.1292438233%3D%2520%2520Existing%2520medical%2520reasoning%2520benchmarks%2520for%2520vision-language%2520models%2520primarily%250Afocus%2520on%2520analyzing%2520a%2520patient%2527s%2520condition%2520based%2520on%2520an%2520image%2520from%2520a%2520single%2520visit.%250AHowever%252C%2520this%2520setting%2520deviates%2520significantly%2520from%2520real-world%2520clinical%2520practice%252C%250Awhere%2520doctors%2520typically%2520refer%2520to%2520a%2520patient%2527s%2520historical%2520conditions%2520to%2520provide%2520a%250Acomprehensive%2520assessment%2520by%2520tracking%2520their%2520changes%2520over%2520time.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520TemMed-Bench%252C%2520the%2520first%2520benchmark%2520designed%2520for%2520analyzing%2520changes%2520in%250Apatients%2527%2520conditions%2520between%2520different%2520clinical%2520visits%252C%2520which%2520challenges%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%2520to%2520reason%2520over%2520temporal%2520medical%2520images.%250ATemMed-Bench%2520consists%2520of%2520a%2520test%2520set%2520comprising%2520three%2520tasks%2520-%2520visual%250Aquestion-answering%2520%2528VQA%2529%252C%2520report%2520generation%252C%2520and%2520image-pair%2520selection%2520-%2520and%2520a%250Asupplementary%2520knowledge%2520corpus%2520of%2520over%252017%252C000%2520instances.%2520With%2520TemMed-Bench%252C%2520we%250Aconduct%2520an%2520evaluation%2520of%2520six%2520proprietary%2520and%2520six%2520open-source%2520LVLMs.%2520Our%2520results%250Ashow%2520that%2520most%2520LVLMs%2520lack%2520the%2520ability%2520to%2520analyze%2520patients%2527%2520condition%2520changes%250Aover%2520temporal%2520medical%2520images%252C%2520and%2520a%2520large%2520proportion%2520perform%2520only%2520at%2520a%250Arandom-guessing%2520level%2520in%2520the%2520closed-book%2520setting.%2520In%2520contrast%252C%2520GPT%2520o3%252C%2520o4-mini%250Aand%2520Claude%25203.5%2520Sonnet%2520demonstrate%2520comparatively%2520decent%2520performance%252C%2520though%2520they%250Ahave%2520yet%2520to%2520reach%2520the%2520desired%2520level.%2520Furthermore%252C%2520we%2520explore%2520augmenting%2520the%250Ainput%2520with%2520both%2520retrieved%2520visual%2520and%2520textual%2520modalities%2520in%2520the%2520medical%2520domain.%250AWe%2520also%2520show%2520that%2520multi-modal%2520retrieval%2520augmentation%2520yields%2520notably%2520higher%250Aperformance%2520gains%2520than%2520no%2520retrieval%2520and%2520textual%2520retrieval%2520alone%2520across%2520most%250Amodels%2520on%2520our%2520benchmark%252C%2520with%2520the%2520VQA%2520task%2520showing%2520an%2520average%2520improvement%2520of%250A2.59%2525.%2520Overall%252C%2520we%2520compose%2520a%2520benchmark%2520grounded%2520on%2520real-world%2520clinical%250Apractice%252C%2520and%2520it%2520reveals%2520LVLMs%2527%2520limitations%2520in%2520temporal%2520medical%2520image%250Areasoning%252C%2520as%2520well%2520as%2520highlighting%2520the%2520use%2520of%2520multi-modal%2520retrieval%250Aaugmentation%2520as%2520a%2520potentially%2520promising%2520direction%2520worth%2520exploring%2520to%2520address%250Athis%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TemMed-Bench%3A%20Evaluating%20Temporal%20Medical%20Image%20Reasoning%20in%0A%20%20Vision-Language%20Models&entry.906535625=Junyi%20Zhang%20and%20Jia-Chen%20Gu%20and%20Wenbo%20Hu%20and%20Yu%20Zhou%20and%20Robinson%20Piramuthu%20and%20Nanyun%20Peng&entry.1292438233=%20%20Existing%20medical%20reasoning%20benchmarks%20for%20vision-language%20models%20primarily%0Afocus%20on%20analyzing%20a%20patient%27s%20condition%20based%20on%20an%20image%20from%20a%20single%20visit.%0AHowever%2C%20this%20setting%20deviates%20significantly%20from%20real-world%20clinical%20practice%2C%0Awhere%20doctors%20typically%20refer%20to%20a%20patient%27s%20historical%20conditions%20to%20provide%20a%0Acomprehensive%20assessment%20by%20tracking%20their%20changes%20over%20time.%20In%20this%20paper%2C%20we%0Aintroduce%20TemMed-Bench%2C%20the%20first%20benchmark%20designed%20for%20analyzing%20changes%20in%0Apatients%27%20conditions%20between%20different%20clinical%20visits%2C%20which%20challenges%20large%0Avision-language%20models%20%28LVLMs%29%20to%20reason%20over%20temporal%20medical%20images.%0ATemMed-Bench%20consists%20of%20a%20test%20set%20comprising%20three%20tasks%20-%20visual%0Aquestion-answering%20%28VQA%29%2C%20report%20generation%2C%20and%20image-pair%20selection%20-%20and%20a%0Asupplementary%20knowledge%20corpus%20of%20over%2017%2C000%20instances.%20With%20TemMed-Bench%2C%20we%0Aconduct%20an%20evaluation%20of%20six%20proprietary%20and%20six%20open-source%20LVLMs.%20Our%20results%0Ashow%20that%20most%20LVLMs%20lack%20the%20ability%20to%20analyze%20patients%27%20condition%20changes%0Aover%20temporal%20medical%20images%2C%20and%20a%20large%20proportion%20perform%20only%20at%20a%0Arandom-guessing%20level%20in%20the%20closed-book%20setting.%20In%20contrast%2C%20GPT%20o3%2C%20o4-mini%0Aand%20Claude%203.5%20Sonnet%20demonstrate%20comparatively%20decent%20performance%2C%20though%20they%0Ahave%20yet%20to%20reach%20the%20desired%20level.%20Furthermore%2C%20we%20explore%20augmenting%20the%0Ainput%20with%20both%20retrieved%20visual%20and%20textual%20modalities%20in%20the%20medical%20domain.%0AWe%20also%20show%20that%20multi-modal%20retrieval%20augmentation%20yields%20notably%20higher%0Aperformance%20gains%20than%20no%20retrieval%20and%20textual%20retrieval%20alone%20across%20most%0Amodels%20on%20our%20benchmark%2C%20with%20the%20VQA%20task%20showing%20an%20average%20improvement%20of%0A2.59%25.%20Overall%2C%20we%20compose%20a%20benchmark%20grounded%20on%20real-world%20clinical%0Apractice%2C%20and%20it%20reveals%20LVLMs%27%20limitations%20in%20temporal%20medical%20image%0Areasoning%2C%20as%20well%20as%20highlighting%20the%20use%20of%20multi-modal%20retrieval%0Aaugmentation%20as%20a%20potentially%20promising%20direction%20worth%20exploring%20to%20address%0Athis%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25143v1&entry.124074799=Read"},
{"title": "NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision", "author": "Tengkai Wang and Weihao Li and Ruikai Cui and Shi Qiu and Nick Barnes", "abstract": "  Reconstructing accurate implicit surface representations from point clouds\nremains a challenging task, particularly when data is captured using\nlow-quality scanning devices. These point clouds often contain substantial\nnoise, leading to inaccurate surface reconstructions. Inspired by the\nNoise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel\nmethod designed to extend this concept to 3D neural fields. Our approach\nenables learning clean neural SDFs directly from noisy point clouds through\nnoisy supervision by minimizing the MSE loss between noisy SDF representations,\nallowing the network to implicitly denoise and refine surface estimations. We\nevaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the\nShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that\nour framework significantly improves surface reconstruction quality from noisy\ninputs.\n", "link": "http://arxiv.org/abs/2507.13595v2", "date": "2025-09-29", "relevancy": 2.7824, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5781}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5608}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoiseSDF2NoiseSDF%3A%20Learning%20Clean%20Neural%20Fields%20from%20Noisy%20Supervision&body=Title%3A%20NoiseSDF2NoiseSDF%3A%20Learning%20Clean%20Neural%20Fields%20from%20Noisy%20Supervision%0AAuthor%3A%20Tengkai%20Wang%20and%20Weihao%20Li%20and%20Ruikai%20Cui%20and%20Shi%20Qiu%20and%20Nick%20Barnes%0AAbstract%3A%20%20%20Reconstructing%20accurate%20implicit%20surface%20representations%20from%20point%20clouds%0Aremains%20a%20challenging%20task%2C%20particularly%20when%20data%20is%20captured%20using%0Alow-quality%20scanning%20devices.%20These%20point%20clouds%20often%20contain%20substantial%0Anoise%2C%20leading%20to%20inaccurate%20surface%20reconstructions.%20Inspired%20by%20the%0ANoise2Noise%20paradigm%20for%202D%20images%2C%20we%20introduce%20NoiseSDF2NoiseSDF%2C%20a%20novel%0Amethod%20designed%20to%20extend%20this%20concept%20to%203D%20neural%20fields.%20Our%20approach%0Aenables%20learning%20clean%20neural%20SDFs%20directly%20from%20noisy%20point%20clouds%20through%0Anoisy%20supervision%20by%20minimizing%20the%20MSE%20loss%20between%20noisy%20SDF%20representations%2C%0Aallowing%20the%20network%20to%20implicitly%20denoise%20and%20refine%20surface%20estimations.%20We%0Aevaluate%20the%20effectiveness%20of%20NoiseSDF2NoiseSDF%20on%20benchmarks%2C%20including%20the%0AShapeNet%2C%20ABC%2C%20Famous%2C%20and%20Real%20datasets.%20Experimental%20results%20demonstrate%20that%0Aour%20framework%20significantly%20improves%20surface%20reconstruction%20quality%20from%20noisy%0Ainputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoiseSDF2NoiseSDF%253A%2520Learning%2520Clean%2520Neural%2520Fields%2520from%2520Noisy%2520Supervision%26entry.906535625%3DTengkai%2520Wang%2520and%2520Weihao%2520Li%2520and%2520Ruikai%2520Cui%2520and%2520Shi%2520Qiu%2520and%2520Nick%2520Barnes%26entry.1292438233%3D%2520%2520Reconstructing%2520accurate%2520implicit%2520surface%2520representations%2520from%2520point%2520clouds%250Aremains%2520a%2520challenging%2520task%252C%2520particularly%2520when%2520data%2520is%2520captured%2520using%250Alow-quality%2520scanning%2520devices.%2520These%2520point%2520clouds%2520often%2520contain%2520substantial%250Anoise%252C%2520leading%2520to%2520inaccurate%2520surface%2520reconstructions.%2520Inspired%2520by%2520the%250ANoise2Noise%2520paradigm%2520for%25202D%2520images%252C%2520we%2520introduce%2520NoiseSDF2NoiseSDF%252C%2520a%2520novel%250Amethod%2520designed%2520to%2520extend%2520this%2520concept%2520to%25203D%2520neural%2520fields.%2520Our%2520approach%250Aenables%2520learning%2520clean%2520neural%2520SDFs%2520directly%2520from%2520noisy%2520point%2520clouds%2520through%250Anoisy%2520supervision%2520by%2520minimizing%2520the%2520MSE%2520loss%2520between%2520noisy%2520SDF%2520representations%252C%250Aallowing%2520the%2520network%2520to%2520implicitly%2520denoise%2520and%2520refine%2520surface%2520estimations.%2520We%250Aevaluate%2520the%2520effectiveness%2520of%2520NoiseSDF2NoiseSDF%2520on%2520benchmarks%252C%2520including%2520the%250AShapeNet%252C%2520ABC%252C%2520Famous%252C%2520and%2520Real%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520framework%2520significantly%2520improves%2520surface%2520reconstruction%2520quality%2520from%2520noisy%250Ainputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoiseSDF2NoiseSDF%3A%20Learning%20Clean%20Neural%20Fields%20from%20Noisy%20Supervision&entry.906535625=Tengkai%20Wang%20and%20Weihao%20Li%20and%20Ruikai%20Cui%20and%20Shi%20Qiu%20and%20Nick%20Barnes&entry.1292438233=%20%20Reconstructing%20accurate%20implicit%20surface%20representations%20from%20point%20clouds%0Aremains%20a%20challenging%20task%2C%20particularly%20when%20data%20is%20captured%20using%0Alow-quality%20scanning%20devices.%20These%20point%20clouds%20often%20contain%20substantial%0Anoise%2C%20leading%20to%20inaccurate%20surface%20reconstructions.%20Inspired%20by%20the%0ANoise2Noise%20paradigm%20for%202D%20images%2C%20we%20introduce%20NoiseSDF2NoiseSDF%2C%20a%20novel%0Amethod%20designed%20to%20extend%20this%20concept%20to%203D%20neural%20fields.%20Our%20approach%0Aenables%20learning%20clean%20neural%20SDFs%20directly%20from%20noisy%20point%20clouds%20through%0Anoisy%20supervision%20by%20minimizing%20the%20MSE%20loss%20between%20noisy%20SDF%20representations%2C%0Aallowing%20the%20network%20to%20implicitly%20denoise%20and%20refine%20surface%20estimations.%20We%0Aevaluate%20the%20effectiveness%20of%20NoiseSDF2NoiseSDF%20on%20benchmarks%2C%20including%20the%0AShapeNet%2C%20ABC%2C%20Famous%2C%20and%20Real%20datasets.%20Experimental%20results%20demonstrate%20that%0Aour%20framework%20significantly%20improves%20surface%20reconstruction%20quality%20from%20noisy%0Ainputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13595v2&entry.124074799=Read"},
{"title": "Light-SQ: Structure-aware Shape Abstraction with Superquadrics for\n  Generated Meshes", "author": "Yuhan Wang and Weikai Chen and Zeyu Hu and Runze Zhang and Yingda Yin and Ruoyu Wu and Keyang Luo and Shengju Qian and Yiyan Ma and Hongyi Li and Yuan Gao and Yuhuan Zhou and Hao Luo and Wan Wang and Xiaobin Shen and Zhaowei Li and Kuixin Zhu and Chuanlang Hong and Yueyue Wang and Lijie Feng and Xin Wang and Chen Change Loy", "abstract": "  In user-generated-content (UGC) applications, non-expert users often rely on\nimage-to-3D generative models to create 3D assets. In this context,\nprimitive-based shape abstraction offers a promising solution for UGC scenarios\nby compressing high-resolution meshes into compact, editable representations.\nTowards this end, effective shape abstraction must therefore be\nstructure-aware, characterized by low overlap between primitives, part-aware\nalignment, and primitive compactness. We present Light-SQ, a novel\nsuperquadric-based optimization framework that explicitly emphasizes\nstructure-awareness from three aspects. (a) We introduce SDF carving to\niteratively udpate the target signed distance field, discouraging overlap\nbetween primitives. (b) We propose a block-regrow-fill strategy guided by\nstructure-aware volumetric decomposition, enabling structural partitioning to\ndrive primitive placement. (c) We implement adaptive residual pruning based on\nSDF update history to surpress over-segmentation and ensure compact results. In\naddition, Light-SQ supports multiscale fitting, enabling localized refinement\nto preserve fine geometric details. To evaluate our method, we introduce\n3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both\nreconstruction quality and primitive-level editability. Extensive experiments\ndemonstrate that Light-SQ enables efficient, high-fidelity, and editable shape\nabstraction with superquadrics for complex generated geometry, advancing the\nfeasibility of 3D UGC creation.\n", "link": "http://arxiv.org/abs/2509.24986v1", "date": "2025-09-29", "relevancy": 2.7724, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.559}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5522}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light-SQ%3A%20Structure-aware%20Shape%20Abstraction%20with%20Superquadrics%20for%0A%20%20Generated%20Meshes&body=Title%3A%20Light-SQ%3A%20Structure-aware%20Shape%20Abstraction%20with%20Superquadrics%20for%0A%20%20Generated%20Meshes%0AAuthor%3A%20Yuhan%20Wang%20and%20Weikai%20Chen%20and%20Zeyu%20Hu%20and%20Runze%20Zhang%20and%20Yingda%20Yin%20and%20Ruoyu%20Wu%20and%20Keyang%20Luo%20and%20Shengju%20Qian%20and%20Yiyan%20Ma%20and%20Hongyi%20Li%20and%20Yuan%20Gao%20and%20Yuhuan%20Zhou%20and%20Hao%20Luo%20and%20Wan%20Wang%20and%20Xiaobin%20Shen%20and%20Zhaowei%20Li%20and%20Kuixin%20Zhu%20and%20Chuanlang%20Hong%20and%20Yueyue%20Wang%20and%20Lijie%20Feng%20and%20Xin%20Wang%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20In%20user-generated-content%20%28UGC%29%20applications%2C%20non-expert%20users%20often%20rely%20on%0Aimage-to-3D%20generative%20models%20to%20create%203D%20assets.%20In%20this%20context%2C%0Aprimitive-based%20shape%20abstraction%20offers%20a%20promising%20solution%20for%20UGC%20scenarios%0Aby%20compressing%20high-resolution%20meshes%20into%20compact%2C%20editable%20representations.%0ATowards%20this%20end%2C%20effective%20shape%20abstraction%20must%20therefore%20be%0Astructure-aware%2C%20characterized%20by%20low%20overlap%20between%20primitives%2C%20part-aware%0Aalignment%2C%20and%20primitive%20compactness.%20We%20present%20Light-SQ%2C%20a%20novel%0Asuperquadric-based%20optimization%20framework%20that%20explicitly%20emphasizes%0Astructure-awareness%20from%20three%20aspects.%20%28a%29%20We%20introduce%20SDF%20carving%20to%0Aiteratively%20udpate%20the%20target%20signed%20distance%20field%2C%20discouraging%20overlap%0Abetween%20primitives.%20%28b%29%20We%20propose%20a%20block-regrow-fill%20strategy%20guided%20by%0Astructure-aware%20volumetric%20decomposition%2C%20enabling%20structural%20partitioning%20to%0Adrive%20primitive%20placement.%20%28c%29%20We%20implement%20adaptive%20residual%20pruning%20based%20on%0ASDF%20update%20history%20to%20surpress%20over-segmentation%20and%20ensure%20compact%20results.%20In%0Aaddition%2C%20Light-SQ%20supports%20multiscale%20fitting%2C%20enabling%20localized%20refinement%0Ato%20preserve%20fine%20geometric%20details.%20To%20evaluate%20our%20method%2C%20we%20introduce%0A3DGen-Prim%2C%20a%20benchmark%20extending%203DGen-Bench%20with%20new%20metrics%20for%20both%0Areconstruction%20quality%20and%20primitive-level%20editability.%20Extensive%20experiments%0Ademonstrate%20that%20Light-SQ%20enables%20efficient%2C%20high-fidelity%2C%20and%20editable%20shape%0Aabstraction%20with%20superquadrics%20for%20complex%20generated%20geometry%2C%20advancing%20the%0Afeasibility%20of%203D%20UGC%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight-SQ%253A%2520Structure-aware%2520Shape%2520Abstraction%2520with%2520Superquadrics%2520for%250A%2520%2520Generated%2520Meshes%26entry.906535625%3DYuhan%2520Wang%2520and%2520Weikai%2520Chen%2520and%2520Zeyu%2520Hu%2520and%2520Runze%2520Zhang%2520and%2520Yingda%2520Yin%2520and%2520Ruoyu%2520Wu%2520and%2520Keyang%2520Luo%2520and%2520Shengju%2520Qian%2520and%2520Yiyan%2520Ma%2520and%2520Hongyi%2520Li%2520and%2520Yuan%2520Gao%2520and%2520Yuhuan%2520Zhou%2520and%2520Hao%2520Luo%2520and%2520Wan%2520Wang%2520and%2520Xiaobin%2520Shen%2520and%2520Zhaowei%2520Li%2520and%2520Kuixin%2520Zhu%2520and%2520Chuanlang%2520Hong%2520and%2520Yueyue%2520Wang%2520and%2520Lijie%2520Feng%2520and%2520Xin%2520Wang%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520In%2520user-generated-content%2520%2528UGC%2529%2520applications%252C%2520non-expert%2520users%2520often%2520rely%2520on%250Aimage-to-3D%2520generative%2520models%2520to%2520create%25203D%2520assets.%2520In%2520this%2520context%252C%250Aprimitive-based%2520shape%2520abstraction%2520offers%2520a%2520promising%2520solution%2520for%2520UGC%2520scenarios%250Aby%2520compressing%2520high-resolution%2520meshes%2520into%2520compact%252C%2520editable%2520representations.%250ATowards%2520this%2520end%252C%2520effective%2520shape%2520abstraction%2520must%2520therefore%2520be%250Astructure-aware%252C%2520characterized%2520by%2520low%2520overlap%2520between%2520primitives%252C%2520part-aware%250Aalignment%252C%2520and%2520primitive%2520compactness.%2520We%2520present%2520Light-SQ%252C%2520a%2520novel%250Asuperquadric-based%2520optimization%2520framework%2520that%2520explicitly%2520emphasizes%250Astructure-awareness%2520from%2520three%2520aspects.%2520%2528a%2529%2520We%2520introduce%2520SDF%2520carving%2520to%250Aiteratively%2520udpate%2520the%2520target%2520signed%2520distance%2520field%252C%2520discouraging%2520overlap%250Abetween%2520primitives.%2520%2528b%2529%2520We%2520propose%2520a%2520block-regrow-fill%2520strategy%2520guided%2520by%250Astructure-aware%2520volumetric%2520decomposition%252C%2520enabling%2520structural%2520partitioning%2520to%250Adrive%2520primitive%2520placement.%2520%2528c%2529%2520We%2520implement%2520adaptive%2520residual%2520pruning%2520based%2520on%250ASDF%2520update%2520history%2520to%2520surpress%2520over-segmentation%2520and%2520ensure%2520compact%2520results.%2520In%250Aaddition%252C%2520Light-SQ%2520supports%2520multiscale%2520fitting%252C%2520enabling%2520localized%2520refinement%250Ato%2520preserve%2520fine%2520geometric%2520details.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520introduce%250A3DGen-Prim%252C%2520a%2520benchmark%2520extending%25203DGen-Bench%2520with%2520new%2520metrics%2520for%2520both%250Areconstruction%2520quality%2520and%2520primitive-level%2520editability.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Light-SQ%2520enables%2520efficient%252C%2520high-fidelity%252C%2520and%2520editable%2520shape%250Aabstraction%2520with%2520superquadrics%2520for%2520complex%2520generated%2520geometry%252C%2520advancing%2520the%250Afeasibility%2520of%25203D%2520UGC%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light-SQ%3A%20Structure-aware%20Shape%20Abstraction%20with%20Superquadrics%20for%0A%20%20Generated%20Meshes&entry.906535625=Yuhan%20Wang%20and%20Weikai%20Chen%20and%20Zeyu%20Hu%20and%20Runze%20Zhang%20and%20Yingda%20Yin%20and%20Ruoyu%20Wu%20and%20Keyang%20Luo%20and%20Shengju%20Qian%20and%20Yiyan%20Ma%20and%20Hongyi%20Li%20and%20Yuan%20Gao%20and%20Yuhuan%20Zhou%20and%20Hao%20Luo%20and%20Wan%20Wang%20and%20Xiaobin%20Shen%20and%20Zhaowei%20Li%20and%20Kuixin%20Zhu%20and%20Chuanlang%20Hong%20and%20Yueyue%20Wang%20and%20Lijie%20Feng%20and%20Xin%20Wang%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20In%20user-generated-content%20%28UGC%29%20applications%2C%20non-expert%20users%20often%20rely%20on%0Aimage-to-3D%20generative%20models%20to%20create%203D%20assets.%20In%20this%20context%2C%0Aprimitive-based%20shape%20abstraction%20offers%20a%20promising%20solution%20for%20UGC%20scenarios%0Aby%20compressing%20high-resolution%20meshes%20into%20compact%2C%20editable%20representations.%0ATowards%20this%20end%2C%20effective%20shape%20abstraction%20must%20therefore%20be%0Astructure-aware%2C%20characterized%20by%20low%20overlap%20between%20primitives%2C%20part-aware%0Aalignment%2C%20and%20primitive%20compactness.%20We%20present%20Light-SQ%2C%20a%20novel%0Asuperquadric-based%20optimization%20framework%20that%20explicitly%20emphasizes%0Astructure-awareness%20from%20three%20aspects.%20%28a%29%20We%20introduce%20SDF%20carving%20to%0Aiteratively%20udpate%20the%20target%20signed%20distance%20field%2C%20discouraging%20overlap%0Abetween%20primitives.%20%28b%29%20We%20propose%20a%20block-regrow-fill%20strategy%20guided%20by%0Astructure-aware%20volumetric%20decomposition%2C%20enabling%20structural%20partitioning%20to%0Adrive%20primitive%20placement.%20%28c%29%20We%20implement%20adaptive%20residual%20pruning%20based%20on%0ASDF%20update%20history%20to%20surpress%20over-segmentation%20and%20ensure%20compact%20results.%20In%0Aaddition%2C%20Light-SQ%20supports%20multiscale%20fitting%2C%20enabling%20localized%20refinement%0Ato%20preserve%20fine%20geometric%20details.%20To%20evaluate%20our%20method%2C%20we%20introduce%0A3DGen-Prim%2C%20a%20benchmark%20extending%203DGen-Bench%20with%20new%20metrics%20for%20both%0Areconstruction%20quality%20and%20primitive-level%20editability.%20Extensive%20experiments%0Ademonstrate%20that%20Light-SQ%20enables%20efficient%2C%20high-fidelity%2C%20and%20editable%20shape%0Aabstraction%20with%20superquadrics%20for%20complex%20generated%20geometry%2C%20advancing%20the%0Afeasibility%20of%203D%20UGC%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24986v1&entry.124074799=Read"},
{"title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding", "author": "Mengyue Wang and Shuo Chen and Kristian Kersting and Volker Tresp and Yunpu Ma", "abstract": "  Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.\n", "link": "http://arxiv.org/abs/2506.02850v2", "date": "2025-09-29", "relevancy": 2.761, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20METok%3A%20Multi-Stage%20Event-based%20Token%20Compression%20for%20Efficient%20Long%0A%20%20Video%20Understanding&body=Title%3A%20METok%3A%20Multi-Stage%20Event-based%20Token%20Compression%20for%20Efficient%20Long%0A%20%20Video%20Understanding%0AAuthor%3A%20Mengyue%20Wang%20and%20Shuo%20Chen%20and%20Kristian%20Kersting%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Recent%20advances%20in%20Video%20Large%20Language%20Models%20%28VLLMs%29%20have%20significantly%0Aenhanced%20their%20ability%20to%20understand%20video%20content.%20Nonetheless%2C%20processing%0Along%20videos%20remains%20challenging%20due%20to%20high%20computational%20demands%20and%20the%0Aredundancy%20present%20in%20the%20visual%20data.%20In%20this%20work%2C%20we%20propose%20METok%2C%20a%0Atraining-free%2C%20Multi-stage%20Event-based%20Token%20compression%20framework%20designed%20to%0Aaccelerate%20VLLMs%27%20inference%20while%20preserving%20accuracy.%20METok%20progressively%0Aeliminates%20redundant%20visual%20tokens%20across%20three%20critical%20stages%3A%20%281%29%0Aevent-aware%20compression%20during%20vision%20encoding%2C%20%282%29%20hierarchical%20token%20pruning%0Ain%20the%20prefilling%20stage%20based%20on%20semantic%20alignment%20and%20event%20importance%2C%20and%0A%283%29%20a%20decoding-stage%20KV%20Cache%20optimization%20that%20further%20reduces%20memory%0Aconsumption.%20Our%20experiments%20on%20diverse%20video%20benchmarks%20demonstrate%20that%20METok%0Aachieves%20an%20optimal%20trade-off%20between%20efficiency%20and%20accuracy%20by%20dynamically%0Aselecting%20informative%20visual%20tokens.%20For%20instance%2C%20equipping%20LongVA-7B%20with%0AMETok%20realizes%20an%2080.6%25%20FLOPs%20reduction%20and%2093.5%25%20KV%20Cache%20memory%20savings%2C%20all%0Awhile%20maintaining%20comparable%20or%20even%20superior%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMETok%253A%2520Multi-Stage%2520Event-based%2520Token%2520Compression%2520for%2520Efficient%2520Long%250A%2520%2520Video%2520Understanding%26entry.906535625%3DMengyue%2520Wang%2520and%2520Shuo%2520Chen%2520and%2520Kristian%2520Kersting%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Video%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520have%2520significantly%250Aenhanced%2520their%2520ability%2520to%2520understand%2520video%2520content.%2520Nonetheless%252C%2520processing%250Along%2520videos%2520remains%2520challenging%2520due%2520to%2520high%2520computational%2520demands%2520and%2520the%250Aredundancy%2520present%2520in%2520the%2520visual%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520METok%252C%2520a%250Atraining-free%252C%2520Multi-stage%2520Event-based%2520Token%2520compression%2520framework%2520designed%2520to%250Aaccelerate%2520VLLMs%2527%2520inference%2520while%2520preserving%2520accuracy.%2520METok%2520progressively%250Aeliminates%2520redundant%2520visual%2520tokens%2520across%2520three%2520critical%2520stages%253A%2520%25281%2529%250Aevent-aware%2520compression%2520during%2520vision%2520encoding%252C%2520%25282%2529%2520hierarchical%2520token%2520pruning%250Ain%2520the%2520prefilling%2520stage%2520based%2520on%2520semantic%2520alignment%2520and%2520event%2520importance%252C%2520and%250A%25283%2529%2520a%2520decoding-stage%2520KV%2520Cache%2520optimization%2520that%2520further%2520reduces%2520memory%250Aconsumption.%2520Our%2520experiments%2520on%2520diverse%2520video%2520benchmarks%2520demonstrate%2520that%2520METok%250Aachieves%2520an%2520optimal%2520trade-off%2520between%2520efficiency%2520and%2520accuracy%2520by%2520dynamically%250Aselecting%2520informative%2520visual%2520tokens.%2520For%2520instance%252C%2520equipping%2520LongVA-7B%2520with%250AMETok%2520realizes%2520an%252080.6%2525%2520FLOPs%2520reduction%2520and%252093.5%2525%2520KV%2520Cache%2520memory%2520savings%252C%2520all%250Awhile%2520maintaining%2520comparable%2520or%2520even%2520superior%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=METok%3A%20Multi-Stage%20Event-based%20Token%20Compression%20for%20Efficient%20Long%0A%20%20Video%20Understanding&entry.906535625=Mengyue%20Wang%20and%20Shuo%20Chen%20and%20Kristian%20Kersting%20and%20Volker%20Tresp%20and%20Yunpu%20Ma&entry.1292438233=%20%20Recent%20advances%20in%20Video%20Large%20Language%20Models%20%28VLLMs%29%20have%20significantly%0Aenhanced%20their%20ability%20to%20understand%20video%20content.%20Nonetheless%2C%20processing%0Along%20videos%20remains%20challenging%20due%20to%20high%20computational%20demands%20and%20the%0Aredundancy%20present%20in%20the%20visual%20data.%20In%20this%20work%2C%20we%20propose%20METok%2C%20a%0Atraining-free%2C%20Multi-stage%20Event-based%20Token%20compression%20framework%20designed%20to%0Aaccelerate%20VLLMs%27%20inference%20while%20preserving%20accuracy.%20METok%20progressively%0Aeliminates%20redundant%20visual%20tokens%20across%20three%20critical%20stages%3A%20%281%29%0Aevent-aware%20compression%20during%20vision%20encoding%2C%20%282%29%20hierarchical%20token%20pruning%0Ain%20the%20prefilling%20stage%20based%20on%20semantic%20alignment%20and%20event%20importance%2C%20and%0A%283%29%20a%20decoding-stage%20KV%20Cache%20optimization%20that%20further%20reduces%20memory%0Aconsumption.%20Our%20experiments%20on%20diverse%20video%20benchmarks%20demonstrate%20that%20METok%0Aachieves%20an%20optimal%20trade-off%20between%20efficiency%20and%20accuracy%20by%20dynamically%0Aselecting%20informative%20visual%20tokens.%20For%20instance%2C%20equipping%20LongVA-7B%20with%0AMETok%20realizes%20an%2080.6%25%20FLOPs%20reduction%20and%2093.5%25%20KV%20Cache%20memory%20savings%2C%20all%0Awhile%20maintaining%20comparable%20or%20even%20superior%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02850v2&entry.124074799=Read"},
{"title": "Towards Understanding the Shape of Representations in Protein Language\n  Models", "author": "Kosio Beshkov and Anders Malthe-S\u00f8renssen", "abstract": "  While protein language models (PLMs) are one of the most promising avenues of\nresearch for future de novo protein design, the way in which they transform\nsequences to hidden representations, as well as the information encoded in such\nrepresentations is yet to be fully understood. Several works have attempted to\npropose interpretability tools for PLMs, but they have focused on understanding\nhow individual sequences are transformed by such models. Therefore, the way in\nwhich PLMs transform the whole space of sequences along with their relations is\nstill unknown. In this work we attempt to understand this transformed space of\nsequences by identifying protein structure and representation with square-root\nvelocity (SRV) representations and graph filtrations. Both approaches naturally\nlead to a metric space in which pairs of proteins or protein representations\ncan be compared with each other.\n  We analyze different types of proteins from the SCOP dataset and show that\nthe Karcher mean and effective dimension of the SRV shape space follow a\nnon-linear pattern as a function of the layers in ESM2 models of different\nsizes. Furthermore, we use graph filtrations as a tool to study the context\nlengths at which models encode the structural features of proteins. We find\nthat PLMs preferentially encode immediate as well as local relations between\nresidues, but start to degrade for larger context lengths. The most\nstructurally faithful encoding tends to occur close to, but before the last\nlayer of the models, indicating that training a folding model ontop of these\nlayers might lead to improved folding performance.\n", "link": "http://arxiv.org/abs/2509.24895v1", "date": "2025-09-29", "relevancy": 2.7551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20the%20Shape%20of%20Representations%20in%20Protein%20Language%0A%20%20Models&body=Title%3A%20Towards%20Understanding%20the%20Shape%20of%20Representations%20in%20Protein%20Language%0A%20%20Models%0AAuthor%3A%20Kosio%20Beshkov%20and%20Anders%20Malthe-S%C3%B8renssen%0AAbstract%3A%20%20%20While%20protein%20language%20models%20%28PLMs%29%20are%20one%20of%20the%20most%20promising%20avenues%20of%0Aresearch%20for%20future%20de%20novo%20protein%20design%2C%20the%20way%20in%20which%20they%20transform%0Asequences%20to%20hidden%20representations%2C%20as%20well%20as%20the%20information%20encoded%20in%20such%0Arepresentations%20is%20yet%20to%20be%20fully%20understood.%20Several%20works%20have%20attempted%20to%0Apropose%20interpretability%20tools%20for%20PLMs%2C%20but%20they%20have%20focused%20on%20understanding%0Ahow%20individual%20sequences%20are%20transformed%20by%20such%20models.%20Therefore%2C%20the%20way%20in%0Awhich%20PLMs%20transform%20the%20whole%20space%20of%20sequences%20along%20with%20their%20relations%20is%0Astill%20unknown.%20In%20this%20work%20we%20attempt%20to%20understand%20this%20transformed%20space%20of%0Asequences%20by%20identifying%20protein%20structure%20and%20representation%20with%20square-root%0Avelocity%20%28SRV%29%20representations%20and%20graph%20filtrations.%20Both%20approaches%20naturally%0Alead%20to%20a%20metric%20space%20in%20which%20pairs%20of%20proteins%20or%20protein%20representations%0Acan%20be%20compared%20with%20each%20other.%0A%20%20We%20analyze%20different%20types%20of%20proteins%20from%20the%20SCOP%20dataset%20and%20show%20that%0Athe%20Karcher%20mean%20and%20effective%20dimension%20of%20the%20SRV%20shape%20space%20follow%20a%0Anon-linear%20pattern%20as%20a%20function%20of%20the%20layers%20in%20ESM2%20models%20of%20different%0Asizes.%20Furthermore%2C%20we%20use%20graph%20filtrations%20as%20a%20tool%20to%20study%20the%20context%0Alengths%20at%20which%20models%20encode%20the%20structural%20features%20of%20proteins.%20We%20find%0Athat%20PLMs%20preferentially%20encode%20immediate%20as%20well%20as%20local%20relations%20between%0Aresidues%2C%20but%20start%20to%20degrade%20for%20larger%20context%20lengths.%20The%20most%0Astructurally%20faithful%20encoding%20tends%20to%20occur%20close%20to%2C%20but%20before%20the%20last%0Alayer%20of%20the%20models%2C%20indicating%20that%20training%20a%20folding%20model%20ontop%20of%20these%0Alayers%20might%20lead%20to%20improved%20folding%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520the%2520Shape%2520of%2520Representations%2520in%2520Protein%2520Language%250A%2520%2520Models%26entry.906535625%3DKosio%2520Beshkov%2520and%2520Anders%2520Malthe-S%25C3%25B8renssen%26entry.1292438233%3D%2520%2520While%2520protein%2520language%2520models%2520%2528PLMs%2529%2520are%2520one%2520of%2520the%2520most%2520promising%2520avenues%2520of%250Aresearch%2520for%2520future%2520de%2520novo%2520protein%2520design%252C%2520the%2520way%2520in%2520which%2520they%2520transform%250Asequences%2520to%2520hidden%2520representations%252C%2520as%2520well%2520as%2520the%2520information%2520encoded%2520in%2520such%250Arepresentations%2520is%2520yet%2520to%2520be%2520fully%2520understood.%2520Several%2520works%2520have%2520attempted%2520to%250Apropose%2520interpretability%2520tools%2520for%2520PLMs%252C%2520but%2520they%2520have%2520focused%2520on%2520understanding%250Ahow%2520individual%2520sequences%2520are%2520transformed%2520by%2520such%2520models.%2520Therefore%252C%2520the%2520way%2520in%250Awhich%2520PLMs%2520transform%2520the%2520whole%2520space%2520of%2520sequences%2520along%2520with%2520their%2520relations%2520is%250Astill%2520unknown.%2520In%2520this%2520work%2520we%2520attempt%2520to%2520understand%2520this%2520transformed%2520space%2520of%250Asequences%2520by%2520identifying%2520protein%2520structure%2520and%2520representation%2520with%2520square-root%250Avelocity%2520%2528SRV%2529%2520representations%2520and%2520graph%2520filtrations.%2520Both%2520approaches%2520naturally%250Alead%2520to%2520a%2520metric%2520space%2520in%2520which%2520pairs%2520of%2520proteins%2520or%2520protein%2520representations%250Acan%2520be%2520compared%2520with%2520each%2520other.%250A%2520%2520We%2520analyze%2520different%2520types%2520of%2520proteins%2520from%2520the%2520SCOP%2520dataset%2520and%2520show%2520that%250Athe%2520Karcher%2520mean%2520and%2520effective%2520dimension%2520of%2520the%2520SRV%2520shape%2520space%2520follow%2520a%250Anon-linear%2520pattern%2520as%2520a%2520function%2520of%2520the%2520layers%2520in%2520ESM2%2520models%2520of%2520different%250Asizes.%2520Furthermore%252C%2520we%2520use%2520graph%2520filtrations%2520as%2520a%2520tool%2520to%2520study%2520the%2520context%250Alengths%2520at%2520which%2520models%2520encode%2520the%2520structural%2520features%2520of%2520proteins.%2520We%2520find%250Athat%2520PLMs%2520preferentially%2520encode%2520immediate%2520as%2520well%2520as%2520local%2520relations%2520between%250Aresidues%252C%2520but%2520start%2520to%2520degrade%2520for%2520larger%2520context%2520lengths.%2520The%2520most%250Astructurally%2520faithful%2520encoding%2520tends%2520to%2520occur%2520close%2520to%252C%2520but%2520before%2520the%2520last%250Alayer%2520of%2520the%2520models%252C%2520indicating%2520that%2520training%2520a%2520folding%2520model%2520ontop%2520of%2520these%250Alayers%2520might%2520lead%2520to%2520improved%2520folding%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20the%20Shape%20of%20Representations%20in%20Protein%20Language%0A%20%20Models&entry.906535625=Kosio%20Beshkov%20and%20Anders%20Malthe-S%C3%B8renssen&entry.1292438233=%20%20While%20protein%20language%20models%20%28PLMs%29%20are%20one%20of%20the%20most%20promising%20avenues%20of%0Aresearch%20for%20future%20de%20novo%20protein%20design%2C%20the%20way%20in%20which%20they%20transform%0Asequences%20to%20hidden%20representations%2C%20as%20well%20as%20the%20information%20encoded%20in%20such%0Arepresentations%20is%20yet%20to%20be%20fully%20understood.%20Several%20works%20have%20attempted%20to%0Apropose%20interpretability%20tools%20for%20PLMs%2C%20but%20they%20have%20focused%20on%20understanding%0Ahow%20individual%20sequences%20are%20transformed%20by%20such%20models.%20Therefore%2C%20the%20way%20in%0Awhich%20PLMs%20transform%20the%20whole%20space%20of%20sequences%20along%20with%20their%20relations%20is%0Astill%20unknown.%20In%20this%20work%20we%20attempt%20to%20understand%20this%20transformed%20space%20of%0Asequences%20by%20identifying%20protein%20structure%20and%20representation%20with%20square-root%0Avelocity%20%28SRV%29%20representations%20and%20graph%20filtrations.%20Both%20approaches%20naturally%0Alead%20to%20a%20metric%20space%20in%20which%20pairs%20of%20proteins%20or%20protein%20representations%0Acan%20be%20compared%20with%20each%20other.%0A%20%20We%20analyze%20different%20types%20of%20proteins%20from%20the%20SCOP%20dataset%20and%20show%20that%0Athe%20Karcher%20mean%20and%20effective%20dimension%20of%20the%20SRV%20shape%20space%20follow%20a%0Anon-linear%20pattern%20as%20a%20function%20of%20the%20layers%20in%20ESM2%20models%20of%20different%0Asizes.%20Furthermore%2C%20we%20use%20graph%20filtrations%20as%20a%20tool%20to%20study%20the%20context%0Alengths%20at%20which%20models%20encode%20the%20structural%20features%20of%20proteins.%20We%20find%0Athat%20PLMs%20preferentially%20encode%20immediate%20as%20well%20as%20local%20relations%20between%0Aresidues%2C%20but%20start%20to%20degrade%20for%20larger%20context%20lengths.%20The%20most%0Astructurally%20faithful%20encoding%20tends%20to%20occur%20close%20to%2C%20but%20before%20the%20last%0Alayer%20of%20the%20models%2C%20indicating%20that%20training%20a%20folding%20model%20ontop%20of%20these%0Alayers%20might%20lead%20to%20improved%20folding%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24895v1&entry.124074799=Read"},
{"title": "Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio\n  Classification", "author": "Lukas Rauch and Ren\u00e9 Heinrich and Houtan Ghaffari and Lukas Miklautz and Ilyass Moummad and Bernhard Sick and Christoph Scholz", "abstract": "  Although probing frozen models has become a standard evaluation paradigm,\nself-supervised learning in audio defaults to fine-tuning. A key reason is that\nglobal pooling creates an information bottleneck causing linear probes to\nmisrepresent the embedding quality: The $\\texttt{cls}$-token discards crucial\ntoken information about dispersed, localized events in multi-label audio. This\nweakness is rooted in the mismatch between the pretraining objective (operating\nglobally) and the downstream task (localized events). Across a comprehensive\nbenchmark of 13 datasets and 6 spectrogram-based encoders, we first investigate\nthe global pooling bottleneck. We then introduce binarized prototypical probes:\na lightweight and simple pooling method that learns prototypes to perform\nclass-wise information aggregation. Despite its simplicity, our method notably\noutperforms linear and attentive probing. Our work establishes probing as a\ncompetitive and efficient paradigm for evaluating audio SSL models, challenging\nthe reliance on costly fine-tuning.\n", "link": "http://arxiv.org/abs/2509.24901v1", "date": "2025-09-29", "relevancy": 2.7241, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unmute%20the%20Patch%20Tokens%3A%20Rethinking%20Probing%20in%20Multi-Label%20Audio%0A%20%20Classification&body=Title%3A%20Unmute%20the%20Patch%20Tokens%3A%20Rethinking%20Probing%20in%20Multi-Label%20Audio%0A%20%20Classification%0AAuthor%3A%20Lukas%20Rauch%20and%20Ren%C3%A9%20Heinrich%20and%20Houtan%20Ghaffari%20and%20Lukas%20Miklautz%20and%20Ilyass%20Moummad%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Although%20probing%20frozen%20models%20has%20become%20a%20standard%20evaluation%20paradigm%2C%0Aself-supervised%20learning%20in%20audio%20defaults%20to%20fine-tuning.%20A%20key%20reason%20is%20that%0Aglobal%20pooling%20creates%20an%20information%20bottleneck%20causing%20linear%20probes%20to%0Amisrepresent%20the%20embedding%20quality%3A%20The%20%24%5Ctexttt%7Bcls%7D%24-token%20discards%20crucial%0Atoken%20information%20about%20dispersed%2C%20localized%20events%20in%20multi-label%20audio.%20This%0Aweakness%20is%20rooted%20in%20the%20mismatch%20between%20the%20pretraining%20objective%20%28operating%0Aglobally%29%20and%20the%20downstream%20task%20%28localized%20events%29.%20Across%20a%20comprehensive%0Abenchmark%20of%2013%20datasets%20and%206%20spectrogram-based%20encoders%2C%20we%20first%20investigate%0Athe%20global%20pooling%20bottleneck.%20We%20then%20introduce%20binarized%20prototypical%20probes%3A%0Aa%20lightweight%20and%20simple%20pooling%20method%20that%20learns%20prototypes%20to%20perform%0Aclass-wise%20information%20aggregation.%20Despite%20its%20simplicity%2C%20our%20method%20notably%0Aoutperforms%20linear%20and%20attentive%20probing.%20Our%20work%20establishes%20probing%20as%20a%0Acompetitive%20and%20efficient%20paradigm%20for%20evaluating%20audio%20SSL%20models%2C%20challenging%0Athe%20reliance%20on%20costly%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnmute%2520the%2520Patch%2520Tokens%253A%2520Rethinking%2520Probing%2520in%2520Multi-Label%2520Audio%250A%2520%2520Classification%26entry.906535625%3DLukas%2520Rauch%2520and%2520Ren%25C3%25A9%2520Heinrich%2520and%2520Houtan%2520Ghaffari%2520and%2520Lukas%2520Miklautz%2520and%2520Ilyass%2520Moummad%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Although%2520probing%2520frozen%2520models%2520has%2520become%2520a%2520standard%2520evaluation%2520paradigm%252C%250Aself-supervised%2520learning%2520in%2520audio%2520defaults%2520to%2520fine-tuning.%2520A%2520key%2520reason%2520is%2520that%250Aglobal%2520pooling%2520creates%2520an%2520information%2520bottleneck%2520causing%2520linear%2520probes%2520to%250Amisrepresent%2520the%2520embedding%2520quality%253A%2520The%2520%2524%255Ctexttt%257Bcls%257D%2524-token%2520discards%2520crucial%250Atoken%2520information%2520about%2520dispersed%252C%2520localized%2520events%2520in%2520multi-label%2520audio.%2520This%250Aweakness%2520is%2520rooted%2520in%2520the%2520mismatch%2520between%2520the%2520pretraining%2520objective%2520%2528operating%250Aglobally%2529%2520and%2520the%2520downstream%2520task%2520%2528localized%2520events%2529.%2520Across%2520a%2520comprehensive%250Abenchmark%2520of%252013%2520datasets%2520and%25206%2520spectrogram-based%2520encoders%252C%2520we%2520first%2520investigate%250Athe%2520global%2520pooling%2520bottleneck.%2520We%2520then%2520introduce%2520binarized%2520prototypical%2520probes%253A%250Aa%2520lightweight%2520and%2520simple%2520pooling%2520method%2520that%2520learns%2520prototypes%2520to%2520perform%250Aclass-wise%2520information%2520aggregation.%2520Despite%2520its%2520simplicity%252C%2520our%2520method%2520notably%250Aoutperforms%2520linear%2520and%2520attentive%2520probing.%2520Our%2520work%2520establishes%2520probing%2520as%2520a%250Acompetitive%2520and%2520efficient%2520paradigm%2520for%2520evaluating%2520audio%2520SSL%2520models%252C%2520challenging%250Athe%2520reliance%2520on%2520costly%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unmute%20the%20Patch%20Tokens%3A%20Rethinking%20Probing%20in%20Multi-Label%20Audio%0A%20%20Classification&entry.906535625=Lukas%20Rauch%20and%20Ren%C3%A9%20Heinrich%20and%20Houtan%20Ghaffari%20and%20Lukas%20Miklautz%20and%20Ilyass%20Moummad%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20Although%20probing%20frozen%20models%20has%20become%20a%20standard%20evaluation%20paradigm%2C%0Aself-supervised%20learning%20in%20audio%20defaults%20to%20fine-tuning.%20A%20key%20reason%20is%20that%0Aglobal%20pooling%20creates%20an%20information%20bottleneck%20causing%20linear%20probes%20to%0Amisrepresent%20the%20embedding%20quality%3A%20The%20%24%5Ctexttt%7Bcls%7D%24-token%20discards%20crucial%0Atoken%20information%20about%20dispersed%2C%20localized%20events%20in%20multi-label%20audio.%20This%0Aweakness%20is%20rooted%20in%20the%20mismatch%20between%20the%20pretraining%20objective%20%28operating%0Aglobally%29%20and%20the%20downstream%20task%20%28localized%20events%29.%20Across%20a%20comprehensive%0Abenchmark%20of%2013%20datasets%20and%206%20spectrogram-based%20encoders%2C%20we%20first%20investigate%0Athe%20global%20pooling%20bottleneck.%20We%20then%20introduce%20binarized%20prototypical%20probes%3A%0Aa%20lightweight%20and%20simple%20pooling%20method%20that%20learns%20prototypes%20to%20perform%0Aclass-wise%20information%20aggregation.%20Despite%20its%20simplicity%2C%20our%20method%20notably%0Aoutperforms%20linear%20and%20attentive%20probing.%20Our%20work%20establishes%20probing%20as%20a%0Acompetitive%20and%20efficient%20paradigm%20for%20evaluating%20audio%20SSL%20models%2C%20challenging%0Athe%20reliance%20on%20costly%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24901v1&entry.124074799=Read"},
{"title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing\n  Reasoning", "author": "Mustansar Fiaz and Hiyam Debary and Paolo Fraccaro and Danda Paudel and Luc Van Gool and Fahad Khan and Salman Khan", "abstract": "  Recent advances in reinforcement learning (RL) have delivered strong\nreasoning capabilities in natural image domains, yet their potential for Earth\nObservation (EO) remains largely unexplored. EO tasks introduce unique\nchallenges, spanning referred object detection, image or region captioning,\nchange detection, grounding, and temporal analysis, that demand task aware\nreasoning. We propose a novel post training framework that incorporates task\naware rewards to enable effective adaptation of reasoning based RL models to\ndiverse EO tasks. This training strategy enhances reasoning capabilities for\nremote sensing images, stabilizes optimization, and improves robustness.\nExtensive experiments across multiple EO benchmarks show consistent performance\ngains over state of the art generic and specialized vision language models.\nCode and models will be released publicly at\nhttps://mustansarfiaz.github.io/GeoVLM-R1/ .\n", "link": "http://arxiv.org/abs/2509.25026v1", "date": "2025-09-29", "relevancy": 2.7083, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoVLM-R1%3A%20Reinforcement%20Fine-Tuning%20for%20Improved%20Remote%20Sensing%0A%20%20Reasoning&body=Title%3A%20GeoVLM-R1%3A%20Reinforcement%20Fine-Tuning%20for%20Improved%20Remote%20Sensing%0A%20%20Reasoning%0AAuthor%3A%20Mustansar%20Fiaz%20and%20Hiyam%20Debary%20and%20Paolo%20Fraccaro%20and%20Danda%20Paudel%20and%20Luc%20Van%20Gool%20and%20Fahad%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20delivered%20strong%0Areasoning%20capabilities%20in%20natural%20image%20domains%2C%20yet%20their%20potential%20for%20Earth%0AObservation%20%28EO%29%20remains%20largely%20unexplored.%20EO%20tasks%20introduce%20unique%0Achallenges%2C%20spanning%20referred%20object%20detection%2C%20image%20or%20region%20captioning%2C%0Achange%20detection%2C%20grounding%2C%20and%20temporal%20analysis%2C%20that%20demand%20task%20aware%0Areasoning.%20We%20propose%20a%20novel%20post%20training%20framework%20that%20incorporates%20task%0Aaware%20rewards%20to%20enable%20effective%20adaptation%20of%20reasoning%20based%20RL%20models%20to%0Adiverse%20EO%20tasks.%20This%20training%20strategy%20enhances%20reasoning%20capabilities%20for%0Aremote%20sensing%20images%2C%20stabilizes%20optimization%2C%20and%20improves%20robustness.%0AExtensive%20experiments%20across%20multiple%20EO%20benchmarks%20show%20consistent%20performance%0Agains%20over%20state%20of%20the%20art%20generic%20and%20specialized%20vision%20language%20models.%0ACode%20and%20models%20will%20be%20released%20publicly%20at%0Ahttps%3A//mustansarfiaz.github.io/GeoVLM-R1/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoVLM-R1%253A%2520Reinforcement%2520Fine-Tuning%2520for%2520Improved%2520Remote%2520Sensing%250A%2520%2520Reasoning%26entry.906535625%3DMustansar%2520Fiaz%2520and%2520Hiyam%2520Debary%2520and%2520Paolo%2520Fraccaro%2520and%2520Danda%2520Paudel%2520and%2520Luc%2520Van%2520Gool%2520and%2520Fahad%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520delivered%2520strong%250Areasoning%2520capabilities%2520in%2520natural%2520image%2520domains%252C%2520yet%2520their%2520potential%2520for%2520Earth%250AObservation%2520%2528EO%2529%2520remains%2520largely%2520unexplored.%2520EO%2520tasks%2520introduce%2520unique%250Achallenges%252C%2520spanning%2520referred%2520object%2520detection%252C%2520image%2520or%2520region%2520captioning%252C%250Achange%2520detection%252C%2520grounding%252C%2520and%2520temporal%2520analysis%252C%2520that%2520demand%2520task%2520aware%250Areasoning.%2520We%2520propose%2520a%2520novel%2520post%2520training%2520framework%2520that%2520incorporates%2520task%250Aaware%2520rewards%2520to%2520enable%2520effective%2520adaptation%2520of%2520reasoning%2520based%2520RL%2520models%2520to%250Adiverse%2520EO%2520tasks.%2520This%2520training%2520strategy%2520enhances%2520reasoning%2520capabilities%2520for%250Aremote%2520sensing%2520images%252C%2520stabilizes%2520optimization%252C%2520and%2520improves%2520robustness.%250AExtensive%2520experiments%2520across%2520multiple%2520EO%2520benchmarks%2520show%2520consistent%2520performance%250Agains%2520over%2520state%2520of%2520the%2520art%2520generic%2520and%2520specialized%2520vision%2520language%2520models.%250ACode%2520and%2520models%2520will%2520be%2520released%2520publicly%2520at%250Ahttps%253A//mustansarfiaz.github.io/GeoVLM-R1/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoVLM-R1%3A%20Reinforcement%20Fine-Tuning%20for%20Improved%20Remote%20Sensing%0A%20%20Reasoning&entry.906535625=Mustansar%20Fiaz%20and%20Hiyam%20Debary%20and%20Paolo%20Fraccaro%20and%20Danda%20Paudel%20and%20Luc%20Van%20Gool%20and%20Fahad%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20delivered%20strong%0Areasoning%20capabilities%20in%20natural%20image%20domains%2C%20yet%20their%20potential%20for%20Earth%0AObservation%20%28EO%29%20remains%20largely%20unexplored.%20EO%20tasks%20introduce%20unique%0Achallenges%2C%20spanning%20referred%20object%20detection%2C%20image%20or%20region%20captioning%2C%0Achange%20detection%2C%20grounding%2C%20and%20temporal%20analysis%2C%20that%20demand%20task%20aware%0Areasoning.%20We%20propose%20a%20novel%20post%20training%20framework%20that%20incorporates%20task%0Aaware%20rewards%20to%20enable%20effective%20adaptation%20of%20reasoning%20based%20RL%20models%20to%0Adiverse%20EO%20tasks.%20This%20training%20strategy%20enhances%20reasoning%20capabilities%20for%0Aremote%20sensing%20images%2C%20stabilizes%20optimization%2C%20and%20improves%20robustness.%0AExtensive%20experiments%20across%20multiple%20EO%20benchmarks%20show%20consistent%20performance%0Agains%20over%20state%20of%20the%20art%20generic%20and%20specialized%20vision%20language%20models.%0ACode%20and%20models%20will%20be%20released%20publicly%20at%0Ahttps%3A//mustansarfiaz.github.io/GeoVLM-R1/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25026v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders Make Audio Foundation Models more Explainable", "author": "Th\u00e9o Mariotte and Martin Lebourdais and Antonio Almud\u00e9var and Marie Tahon and Alfonso Ortega and Nicolas Dugu\u00e9", "abstract": "  Audio pretrained models are widely employed to solve various tasks in speech\nprocessing, sound event detection, or music information retrieval. However, the\nrepresentations learned by these models are unclear, and their analysis mainly\nrestricts to linear probing of the hidden representations. In this work, we\nexplore the use of Sparse Autoencoders (SAEs) to analyze the hidden\nrepresentations of pretrained models, focusing on a case study in singing\ntechnique classification. We first demonstrate that SAEs retain both\ninformation about the original representations and class labels, enabling their\ninternal structure to provide insights into self-supervised learning systems.\nFurthermore, we show that SAEs enhance the disentanglement of vocal attributes,\nestablishing them as an effective tool for identifying the underlying factors\nencoded in the representations.\n", "link": "http://arxiv.org/abs/2509.24793v1", "date": "2025-09-29", "relevancy": 2.6627, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20Make%20Audio%20Foundation%20Models%20more%20Explainable&body=Title%3A%20Sparse%20Autoencoders%20Make%20Audio%20Foundation%20Models%20more%20Explainable%0AAuthor%3A%20Th%C3%A9o%20Mariotte%20and%20Martin%20Lebourdais%20and%20Antonio%20Almud%C3%A9var%20and%20Marie%20Tahon%20and%20Alfonso%20Ortega%20and%20Nicolas%20Dugu%C3%A9%0AAbstract%3A%20%20%20Audio%20pretrained%20models%20are%20widely%20employed%20to%20solve%20various%20tasks%20in%20speech%0Aprocessing%2C%20sound%20event%20detection%2C%20or%20music%20information%20retrieval.%20However%2C%20the%0Arepresentations%20learned%20by%20these%20models%20are%20unclear%2C%20and%20their%20analysis%20mainly%0Arestricts%20to%20linear%20probing%20of%20the%20hidden%20representations.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20Sparse%20Autoencoders%20%28SAEs%29%20to%20analyze%20the%20hidden%0Arepresentations%20of%20pretrained%20models%2C%20focusing%20on%20a%20case%20study%20in%20singing%0Atechnique%20classification.%20We%20first%20demonstrate%20that%20SAEs%20retain%20both%0Ainformation%20about%20the%20original%20representations%20and%20class%20labels%2C%20enabling%20their%0Ainternal%20structure%20to%20provide%20insights%20into%20self-supervised%20learning%20systems.%0AFurthermore%2C%20we%20show%20that%20SAEs%20enhance%20the%20disentanglement%20of%20vocal%20attributes%2C%0Aestablishing%20them%20as%20an%20effective%20tool%20for%20identifying%20the%20underlying%20factors%0Aencoded%20in%20the%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520Make%2520Audio%2520Foundation%2520Models%2520more%2520Explainable%26entry.906535625%3DTh%25C3%25A9o%2520Mariotte%2520and%2520Martin%2520Lebourdais%2520and%2520Antonio%2520Almud%25C3%25A9var%2520and%2520Marie%2520Tahon%2520and%2520Alfonso%2520Ortega%2520and%2520Nicolas%2520Dugu%25C3%25A9%26entry.1292438233%3D%2520%2520Audio%2520pretrained%2520models%2520are%2520widely%2520employed%2520to%2520solve%2520various%2520tasks%2520in%2520speech%250Aprocessing%252C%2520sound%2520event%2520detection%252C%2520or%2520music%2520information%2520retrieval.%2520However%252C%2520the%250Arepresentations%2520learned%2520by%2520these%2520models%2520are%2520unclear%252C%2520and%2520their%2520analysis%2520mainly%250Arestricts%2520to%2520linear%2520probing%2520of%2520the%2520hidden%2520representations.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520the%2520use%2520of%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520to%2520analyze%2520the%2520hidden%250Arepresentations%2520of%2520pretrained%2520models%252C%2520focusing%2520on%2520a%2520case%2520study%2520in%2520singing%250Atechnique%2520classification.%2520We%2520first%2520demonstrate%2520that%2520SAEs%2520retain%2520both%250Ainformation%2520about%2520the%2520original%2520representations%2520and%2520class%2520labels%252C%2520enabling%2520their%250Ainternal%2520structure%2520to%2520provide%2520insights%2520into%2520self-supervised%2520learning%2520systems.%250AFurthermore%252C%2520we%2520show%2520that%2520SAEs%2520enhance%2520the%2520disentanglement%2520of%2520vocal%2520attributes%252C%250Aestablishing%2520them%2520as%2520an%2520effective%2520tool%2520for%2520identifying%2520the%2520underlying%2520factors%250Aencoded%2520in%2520the%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20Make%20Audio%20Foundation%20Models%20more%20Explainable&entry.906535625=Th%C3%A9o%20Mariotte%20and%20Martin%20Lebourdais%20and%20Antonio%20Almud%C3%A9var%20and%20Marie%20Tahon%20and%20Alfonso%20Ortega%20and%20Nicolas%20Dugu%C3%A9&entry.1292438233=%20%20Audio%20pretrained%20models%20are%20widely%20employed%20to%20solve%20various%20tasks%20in%20speech%0Aprocessing%2C%20sound%20event%20detection%2C%20or%20music%20information%20retrieval.%20However%2C%20the%0Arepresentations%20learned%20by%20these%20models%20are%20unclear%2C%20and%20their%20analysis%20mainly%0Arestricts%20to%20linear%20probing%20of%20the%20hidden%20representations.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20Sparse%20Autoencoders%20%28SAEs%29%20to%20analyze%20the%20hidden%0Arepresentations%20of%20pretrained%20models%2C%20focusing%20on%20a%20case%20study%20in%20singing%0Atechnique%20classification.%20We%20first%20demonstrate%20that%20SAEs%20retain%20both%0Ainformation%20about%20the%20original%20representations%20and%20class%20labels%2C%20enabling%20their%0Ainternal%20structure%20to%20provide%20insights%20into%20self-supervised%20learning%20systems.%0AFurthermore%2C%20we%20show%20that%20SAEs%20enhance%20the%20disentanglement%20of%20vocal%20attributes%2C%0Aestablishing%20them%20as%20an%20effective%20tool%20for%20identifying%20the%20underlying%20factors%0Aencoded%20in%20the%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24793v1&entry.124074799=Read"},
{"title": "Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy", "author": "Aymen Bouguerra and Daniel Montoya and Alexandra Gomez-Villa and Fabio Arnez and Chokri Mraidha", "abstract": "  The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.\n", "link": "http://arxiv.org/abs/2509.21173v2", "date": "2025-09-29", "relevancy": 2.6521, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Less%20Precise%20Be%20More%20Reliable%3F%20A%20Systematic%20Evaluation%20of%0A%20%20Quantization%27s%20Impact%20on%20CLIP%20Beyond%20Accuracy&body=Title%3A%20Can%20Less%20Precise%20Be%20More%20Reliable%3F%20A%20Systematic%20Evaluation%20of%0A%20%20Quantization%27s%20Impact%20on%20CLIP%20Beyond%20Accuracy%0AAuthor%3A%20Aymen%20Bouguerra%20and%20Daniel%20Montoya%20and%20Alexandra%20Gomez-Villa%20and%20Fabio%20Arnez%20and%20Chokri%20Mraidha%0AAbstract%3A%20%20%20The%20powerful%20zero-shot%20generalization%20capabilities%20of%20vision-language%20models%0A%28VLMs%29%20like%20CLIP%20have%20enabled%20new%20paradigms%20for%20safety-related%20tasks%20such%20as%0Aout-of-distribution%20%28OOD%29%20detection.%20However%2C%20additional%20aspects%20crucial%20for%0Athe%20computationally%20efficient%20and%20reliable%20deployment%20of%20CLIP%20are%20still%0Aoverlooked.%20In%20particular%2C%20the%20impact%20of%20quantization%20on%20CLIP%27s%20performance%0Abeyond%20accuracy%20remains%20underexplored.%20This%20work%20presents%20a%20large-scale%0Aevaluation%20of%20quantization%20on%20CLIP%20models%2C%20assessing%20not%20only%20in-distribution%0Aaccuracy%20but%20a%20comprehensive%20suite%20of%20reliability%20metrics%20and%20revealing%0Acounterintuitive%20results%20driven%20by%20pre-training%20source.%20We%20demonstrate%20that%0Aquantization%20consistently%20improves%20calibration%20for%20typically%20underconfident%0Apre-trained%20models%2C%20while%20often%20degrading%20it%20for%20overconfident%20variants.%0AIntriguingly%2C%20this%20degradation%20in%20calibration%20does%20not%20preclude%20gains%20in%20other%0Areliability%20metrics%3B%20we%20find%20that%20OOD%20detection%20can%20still%20improve%20for%20these%0Asame%20poorly%20calibrated%20models.%20Furthermore%2C%20we%20identify%20specific%0Aquantization-aware%20training%20%28QAT%29%20methods%20that%20yield%20simultaneous%20gains%20in%0Azero-shot%20accuracy%2C%20calibration%2C%20and%20OOD%20robustness%2C%20challenging%20the%20view%20of%20a%0Astrict%20efficiency-performance%20trade-off.%20These%20findings%20offer%20critical%20insights%0Afor%20navigating%20the%20multi-objective%20problem%20of%20deploying%20efficient%2C%20reliable%2C%0Aand%20robust%20VLMs%20by%20utilizing%20quantization%20beyond%20its%20conventional%20role.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Less%2520Precise%2520Be%2520More%2520Reliable%253F%2520A%2520Systematic%2520Evaluation%2520of%250A%2520%2520Quantization%2527s%2520Impact%2520on%2520CLIP%2520Beyond%2520Accuracy%26entry.906535625%3DAymen%2520Bouguerra%2520and%2520Daniel%2520Montoya%2520and%2520Alexandra%2520Gomez-Villa%2520and%2520Fabio%2520Arnez%2520and%2520Chokri%2520Mraidha%26entry.1292438233%3D%2520%2520The%2520powerful%2520zero-shot%2520generalization%2520capabilities%2520of%2520vision-language%2520models%250A%2528VLMs%2529%2520like%2520CLIP%2520have%2520enabled%2520new%2520paradigms%2520for%2520safety-related%2520tasks%2520such%2520as%250Aout-of-distribution%2520%2528OOD%2529%2520detection.%2520However%252C%2520additional%2520aspects%2520crucial%2520for%250Athe%2520computationally%2520efficient%2520and%2520reliable%2520deployment%2520of%2520CLIP%2520are%2520still%250Aoverlooked.%2520In%2520particular%252C%2520the%2520impact%2520of%2520quantization%2520on%2520CLIP%2527s%2520performance%250Abeyond%2520accuracy%2520remains%2520underexplored.%2520This%2520work%2520presents%2520a%2520large-scale%250Aevaluation%2520of%2520quantization%2520on%2520CLIP%2520models%252C%2520assessing%2520not%2520only%2520in-distribution%250Aaccuracy%2520but%2520a%2520comprehensive%2520suite%2520of%2520reliability%2520metrics%2520and%2520revealing%250Acounterintuitive%2520results%2520driven%2520by%2520pre-training%2520source.%2520We%2520demonstrate%2520that%250Aquantization%2520consistently%2520improves%2520calibration%2520for%2520typically%2520underconfident%250Apre-trained%2520models%252C%2520while%2520often%2520degrading%2520it%2520for%2520overconfident%2520variants.%250AIntriguingly%252C%2520this%2520degradation%2520in%2520calibration%2520does%2520not%2520preclude%2520gains%2520in%2520other%250Areliability%2520metrics%253B%2520we%2520find%2520that%2520OOD%2520detection%2520can%2520still%2520improve%2520for%2520these%250Asame%2520poorly%2520calibrated%2520models.%2520Furthermore%252C%2520we%2520identify%2520specific%250Aquantization-aware%2520training%2520%2528QAT%2529%2520methods%2520that%2520yield%2520simultaneous%2520gains%2520in%250Azero-shot%2520accuracy%252C%2520calibration%252C%2520and%2520OOD%2520robustness%252C%2520challenging%2520the%2520view%2520of%2520a%250Astrict%2520efficiency-performance%2520trade-off.%2520These%2520findings%2520offer%2520critical%2520insights%250Afor%2520navigating%2520the%2520multi-objective%2520problem%2520of%2520deploying%2520efficient%252C%2520reliable%252C%250Aand%2520robust%2520VLMs%2520by%2520utilizing%2520quantization%2520beyond%2520its%2520conventional%2520role.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Less%20Precise%20Be%20More%20Reliable%3F%20A%20Systematic%20Evaluation%20of%0A%20%20Quantization%27s%20Impact%20on%20CLIP%20Beyond%20Accuracy&entry.906535625=Aymen%20Bouguerra%20and%20Daniel%20Montoya%20and%20Alexandra%20Gomez-Villa%20and%20Fabio%20Arnez%20and%20Chokri%20Mraidha&entry.1292438233=%20%20The%20powerful%20zero-shot%20generalization%20capabilities%20of%20vision-language%20models%0A%28VLMs%29%20like%20CLIP%20have%20enabled%20new%20paradigms%20for%20safety-related%20tasks%20such%20as%0Aout-of-distribution%20%28OOD%29%20detection.%20However%2C%20additional%20aspects%20crucial%20for%0Athe%20computationally%20efficient%20and%20reliable%20deployment%20of%20CLIP%20are%20still%0Aoverlooked.%20In%20particular%2C%20the%20impact%20of%20quantization%20on%20CLIP%27s%20performance%0Abeyond%20accuracy%20remains%20underexplored.%20This%20work%20presents%20a%20large-scale%0Aevaluation%20of%20quantization%20on%20CLIP%20models%2C%20assessing%20not%20only%20in-distribution%0Aaccuracy%20but%20a%20comprehensive%20suite%20of%20reliability%20metrics%20and%20revealing%0Acounterintuitive%20results%20driven%20by%20pre-training%20source.%20We%20demonstrate%20that%0Aquantization%20consistently%20improves%20calibration%20for%20typically%20underconfident%0Apre-trained%20models%2C%20while%20often%20degrading%20it%20for%20overconfident%20variants.%0AIntriguingly%2C%20this%20degradation%20in%20calibration%20does%20not%20preclude%20gains%20in%20other%0Areliability%20metrics%3B%20we%20find%20that%20OOD%20detection%20can%20still%20improve%20for%20these%0Asame%20poorly%20calibrated%20models.%20Furthermore%2C%20we%20identify%20specific%0Aquantization-aware%20training%20%28QAT%29%20methods%20that%20yield%20simultaneous%20gains%20in%0Azero-shot%20accuracy%2C%20calibration%2C%20and%20OOD%20robustness%2C%20challenging%20the%20view%20of%20a%0Astrict%20efficiency-performance%20trade-off.%20These%20findings%20offer%20critical%20insights%0Afor%20navigating%20the%20multi-objective%20problem%20of%20deploying%20efficient%2C%20reliable%2C%0Aand%20robust%20VLMs%20by%20utilizing%20quantization%20beyond%20its%20conventional%20role.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21173v2&entry.124074799=Read"},
{"title": "$\\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and\n  Delayed Generalization", "author": "Yuandong Tian", "abstract": "  While the phenomenon of grokking, i.e., delayed generalization, has been\nstudied extensively, it remains an open problem whether there is a mathematical\nframework to characterize what kind of features will emerge, how and in which\nconditions it happens from training, for complex structured inputs. We propose\na novel framework, named $\\mathbf{Li_2}$, that captures three key stages for\nthe grokking behavior of 2-layer nonlinear networks: (I)\n\\underline{\\textbf{L}}azy learning, (II) \\underline{\\textbf{i}}ndependent\nfeature learning and (III) \\underline{\\textbf{i}}nteractive feature learning.\nAt the lazy learning stage, top layer overfits to random hidden representation\nand the model appears to memorize. Thanks to lazy learning and weight decay,\nthe \\emph{backpropagated gradient} $G_F$ from the top layer now carries\ninformation about the target label, with a specific structure that enables each\nhidden node to learn their representation \\emph{independently}. Interestingly,\nthe independent dynamics follows exactly the \\emph{gradient ascent} of an\nenergy function $E$, and its local maxima are precisely the emerging features.\nWe study whether these local-optima induced features are generalizable, their\nrepresentation power, and how they change on sample size, in group arithmetic\ntasks. When hidden nodes start to interact in the later stage of learning, we\nprovably show how $G_F$ changes to focus on missing features that need to be\nlearned. Our study sheds lights on roles played by key hyperparameters such as\nweight decay, learning rate and sample sizes in grokking, leads to provable\nscaling laws of memorization and generalization, and reveals the underlying\ncause why recent optimizers such as Muon can be effective, from the first\nprinciples of gradient dynamics. Our analysis can be extended to multi-layer\narchitectures.\n", "link": "http://arxiv.org/abs/2509.21519v2", "date": "2025-09-29", "relevancy": 2.6511, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5364}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5343}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cmathbf%7BLi_2%7D%24%3A%20A%20Framework%20on%20Dynamics%20of%20Feature%20Emergence%20and%0A%20%20Delayed%20Generalization&body=Title%3A%20%24%5Cmathbf%7BLi_2%7D%24%3A%20A%20Framework%20on%20Dynamics%20of%20Feature%20Emergence%20and%0A%20%20Delayed%20Generalization%0AAuthor%3A%20Yuandong%20Tian%0AAbstract%3A%20%20%20While%20the%20phenomenon%20of%20grokking%2C%20i.e.%2C%20delayed%20generalization%2C%20has%20been%0Astudied%20extensively%2C%20it%20remains%20an%20open%20problem%20whether%20there%20is%20a%20mathematical%0Aframework%20to%20characterize%20what%20kind%20of%20features%20will%20emerge%2C%20how%20and%20in%20which%0Aconditions%20it%20happens%20from%20training%2C%20for%20complex%20structured%20inputs.%20We%20propose%0Aa%20novel%20framework%2C%20named%20%24%5Cmathbf%7BLi_2%7D%24%2C%20that%20captures%20three%20key%20stages%20for%0Athe%20grokking%20behavior%20of%202-layer%20nonlinear%20networks%3A%20%28I%29%0A%5Cunderline%7B%5Ctextbf%7BL%7D%7Dazy%20learning%2C%20%28II%29%20%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dndependent%0Afeature%20learning%20and%20%28III%29%20%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dnteractive%20feature%20learning.%0AAt%20the%20lazy%20learning%20stage%2C%20top%20layer%20overfits%20to%20random%20hidden%20representation%0Aand%20the%20model%20appears%20to%20memorize.%20Thanks%20to%20lazy%20learning%20and%20weight%20decay%2C%0Athe%20%5Cemph%7Bbackpropagated%20gradient%7D%20%24G_F%24%20from%20the%20top%20layer%20now%20carries%0Ainformation%20about%20the%20target%20label%2C%20with%20a%20specific%20structure%20that%20enables%20each%0Ahidden%20node%20to%20learn%20their%20representation%20%5Cemph%7Bindependently%7D.%20Interestingly%2C%0Athe%20independent%20dynamics%20follows%20exactly%20the%20%5Cemph%7Bgradient%20ascent%7D%20of%20an%0Aenergy%20function%20%24E%24%2C%20and%20its%20local%20maxima%20are%20precisely%20the%20emerging%20features.%0AWe%20study%20whether%20these%20local-optima%20induced%20features%20are%20generalizable%2C%20their%0Arepresentation%20power%2C%20and%20how%20they%20change%20on%20sample%20size%2C%20in%20group%20arithmetic%0Atasks.%20When%20hidden%20nodes%20start%20to%20interact%20in%20the%20later%20stage%20of%20learning%2C%20we%0Aprovably%20show%20how%20%24G_F%24%20changes%20to%20focus%20on%20missing%20features%20that%20need%20to%20be%0Alearned.%20Our%20study%20sheds%20lights%20on%20roles%20played%20by%20key%20hyperparameters%20such%20as%0Aweight%20decay%2C%20learning%20rate%20and%20sample%20sizes%20in%20grokking%2C%20leads%20to%20provable%0Ascaling%20laws%20of%20memorization%20and%20generalization%2C%20and%20reveals%20the%20underlying%0Acause%20why%20recent%20optimizers%20such%20as%20Muon%20can%20be%20effective%2C%20from%20the%20first%0Aprinciples%20of%20gradient%20dynamics.%20Our%20analysis%20can%20be%20extended%20to%20multi-layer%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cmathbf%257BLi_2%257D%2524%253A%2520A%2520Framework%2520on%2520Dynamics%2520of%2520Feature%2520Emergence%2520and%250A%2520%2520Delayed%2520Generalization%26entry.906535625%3DYuandong%2520Tian%26entry.1292438233%3D%2520%2520While%2520the%2520phenomenon%2520of%2520grokking%252C%2520i.e.%252C%2520delayed%2520generalization%252C%2520has%2520been%250Astudied%2520extensively%252C%2520it%2520remains%2520an%2520open%2520problem%2520whether%2520there%2520is%2520a%2520mathematical%250Aframework%2520to%2520characterize%2520what%2520kind%2520of%2520features%2520will%2520emerge%252C%2520how%2520and%2520in%2520which%250Aconditions%2520it%2520happens%2520from%2520training%252C%2520for%2520complex%2520structured%2520inputs.%2520We%2520propose%250Aa%2520novel%2520framework%252C%2520named%2520%2524%255Cmathbf%257BLi_2%257D%2524%252C%2520that%2520captures%2520three%2520key%2520stages%2520for%250Athe%2520grokking%2520behavior%2520of%25202-layer%2520nonlinear%2520networks%253A%2520%2528I%2529%250A%255Cunderline%257B%255Ctextbf%257BL%257D%257Dazy%2520learning%252C%2520%2528II%2529%2520%255Cunderline%257B%255Ctextbf%257Bi%257D%257Dndependent%250Afeature%2520learning%2520and%2520%2528III%2529%2520%255Cunderline%257B%255Ctextbf%257Bi%257D%257Dnteractive%2520feature%2520learning.%250AAt%2520the%2520lazy%2520learning%2520stage%252C%2520top%2520layer%2520overfits%2520to%2520random%2520hidden%2520representation%250Aand%2520the%2520model%2520appears%2520to%2520memorize.%2520Thanks%2520to%2520lazy%2520learning%2520and%2520weight%2520decay%252C%250Athe%2520%255Cemph%257Bbackpropagated%2520gradient%257D%2520%2524G_F%2524%2520from%2520the%2520top%2520layer%2520now%2520carries%250Ainformation%2520about%2520the%2520target%2520label%252C%2520with%2520a%2520specific%2520structure%2520that%2520enables%2520each%250Ahidden%2520node%2520to%2520learn%2520their%2520representation%2520%255Cemph%257Bindependently%257D.%2520Interestingly%252C%250Athe%2520independent%2520dynamics%2520follows%2520exactly%2520the%2520%255Cemph%257Bgradient%2520ascent%257D%2520of%2520an%250Aenergy%2520function%2520%2524E%2524%252C%2520and%2520its%2520local%2520maxima%2520are%2520precisely%2520the%2520emerging%2520features.%250AWe%2520study%2520whether%2520these%2520local-optima%2520induced%2520features%2520are%2520generalizable%252C%2520their%250Arepresentation%2520power%252C%2520and%2520how%2520they%2520change%2520on%2520sample%2520size%252C%2520in%2520group%2520arithmetic%250Atasks.%2520When%2520hidden%2520nodes%2520start%2520to%2520interact%2520in%2520the%2520later%2520stage%2520of%2520learning%252C%2520we%250Aprovably%2520show%2520how%2520%2524G_F%2524%2520changes%2520to%2520focus%2520on%2520missing%2520features%2520that%2520need%2520to%2520be%250Alearned.%2520Our%2520study%2520sheds%2520lights%2520on%2520roles%2520played%2520by%2520key%2520hyperparameters%2520such%2520as%250Aweight%2520decay%252C%2520learning%2520rate%2520and%2520sample%2520sizes%2520in%2520grokking%252C%2520leads%2520to%2520provable%250Ascaling%2520laws%2520of%2520memorization%2520and%2520generalization%252C%2520and%2520reveals%2520the%2520underlying%250Acause%2520why%2520recent%2520optimizers%2520such%2520as%2520Muon%2520can%2520be%2520effective%252C%2520from%2520the%2520first%250Aprinciples%2520of%2520gradient%2520dynamics.%2520Our%2520analysis%2520can%2520be%2520extended%2520to%2520multi-layer%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cmathbf%7BLi_2%7D%24%3A%20A%20Framework%20on%20Dynamics%20of%20Feature%20Emergence%20and%0A%20%20Delayed%20Generalization&entry.906535625=Yuandong%20Tian&entry.1292438233=%20%20While%20the%20phenomenon%20of%20grokking%2C%20i.e.%2C%20delayed%20generalization%2C%20has%20been%0Astudied%20extensively%2C%20it%20remains%20an%20open%20problem%20whether%20there%20is%20a%20mathematical%0Aframework%20to%20characterize%20what%20kind%20of%20features%20will%20emerge%2C%20how%20and%20in%20which%0Aconditions%20it%20happens%20from%20training%2C%20for%20complex%20structured%20inputs.%20We%20propose%0Aa%20novel%20framework%2C%20named%20%24%5Cmathbf%7BLi_2%7D%24%2C%20that%20captures%20three%20key%20stages%20for%0Athe%20grokking%20behavior%20of%202-layer%20nonlinear%20networks%3A%20%28I%29%0A%5Cunderline%7B%5Ctextbf%7BL%7D%7Dazy%20learning%2C%20%28II%29%20%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dndependent%0Afeature%20learning%20and%20%28III%29%20%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dnteractive%20feature%20learning.%0AAt%20the%20lazy%20learning%20stage%2C%20top%20layer%20overfits%20to%20random%20hidden%20representation%0Aand%20the%20model%20appears%20to%20memorize.%20Thanks%20to%20lazy%20learning%20and%20weight%20decay%2C%0Athe%20%5Cemph%7Bbackpropagated%20gradient%7D%20%24G_F%24%20from%20the%20top%20layer%20now%20carries%0Ainformation%20about%20the%20target%20label%2C%20with%20a%20specific%20structure%20that%20enables%20each%0Ahidden%20node%20to%20learn%20their%20representation%20%5Cemph%7Bindependently%7D.%20Interestingly%2C%0Athe%20independent%20dynamics%20follows%20exactly%20the%20%5Cemph%7Bgradient%20ascent%7D%20of%20an%0Aenergy%20function%20%24E%24%2C%20and%20its%20local%20maxima%20are%20precisely%20the%20emerging%20features.%0AWe%20study%20whether%20these%20local-optima%20induced%20features%20are%20generalizable%2C%20their%0Arepresentation%20power%2C%20and%20how%20they%20change%20on%20sample%20size%2C%20in%20group%20arithmetic%0Atasks.%20When%20hidden%20nodes%20start%20to%20interact%20in%20the%20later%20stage%20of%20learning%2C%20we%0Aprovably%20show%20how%20%24G_F%24%20changes%20to%20focus%20on%20missing%20features%20that%20need%20to%20be%0Alearned.%20Our%20study%20sheds%20lights%20on%20roles%20played%20by%20key%20hyperparameters%20such%20as%0Aweight%20decay%2C%20learning%20rate%20and%20sample%20sizes%20in%20grokking%2C%20leads%20to%20provable%0Ascaling%20laws%20of%20memorization%20and%20generalization%2C%20and%20reveals%20the%20underlying%0Acause%20why%20recent%20optimizers%20such%20as%20Muon%20can%20be%20effective%2C%20from%20the%20first%0Aprinciples%20of%20gradient%20dynamics.%20Our%20analysis%20can%20be%20extended%20to%20multi-layer%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21519v2&entry.124074799=Read"},
{"title": "Vehicle Classification under Extreme Imbalance: A Comparative Study of\n  Ensemble Learning and CNNs", "author": "Abu Hanif Muhammad Syarubany", "abstract": "  Accurate vehicle type recognition underpins intelligent transportation and\nlogistics, but severe class imbalance in public datasets suppresses performance\non rare categories. We curate a 16-class corpus (~47k images) by merging\nKaggle, ImageNet, and web-crawled data, and create six balanced variants via\nSMOTE oversampling and targeted undersampling. Lightweight ensembles, such as\nRandom Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2\nfeatures are benchmarked against a configurable ResNet-style CNN trained with\nstrong augmentation and label smoothing. The best ensemble (SMOTE-combined)\nattains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set\nand 81.25% on an unseen inference batch, confirming the advantage of deep\nmodels. Nonetheless, the most under-represented class (Barge) remains a failure\nmode, highlighting the limits of rebalancing alone. Results suggest\nprioritizing additional minority-class collection and cost-sensitive objectives\n(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine\ninterpretability with representational power.\n", "link": "http://arxiv.org/abs/2509.24880v1", "date": "2025-09-29", "relevancy": 2.6465, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5601}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vehicle%20Classification%20under%20Extreme%20Imbalance%3A%20A%20Comparative%20Study%20of%0A%20%20Ensemble%20Learning%20and%20CNNs&body=Title%3A%20Vehicle%20Classification%20under%20Extreme%20Imbalance%3A%20A%20Comparative%20Study%20of%0A%20%20Ensemble%20Learning%20and%20CNNs%0AAuthor%3A%20Abu%20Hanif%20Muhammad%20Syarubany%0AAbstract%3A%20%20%20Accurate%20vehicle%20type%20recognition%20underpins%20intelligent%20transportation%20and%0Alogistics%2C%20but%20severe%20class%20imbalance%20in%20public%20datasets%20suppresses%20performance%0Aon%20rare%20categories.%20We%20curate%20a%2016-class%20corpus%20%28~47k%20images%29%20by%20merging%0AKaggle%2C%20ImageNet%2C%20and%20web-crawled%20data%2C%20and%20create%20six%20balanced%20variants%20via%0ASMOTE%20oversampling%20and%20targeted%20undersampling.%20Lightweight%20ensembles%2C%20such%20as%0ARandom%20Forest%2C%20AdaBoost%2C%20and%20a%20soft-voting%20combiner%20built%20on%20MobileNet-V2%0Afeatures%20are%20benchmarked%20against%20a%20configurable%20ResNet-style%20CNN%20trained%20with%0Astrong%20augmentation%20and%20label%20smoothing.%20The%20best%20ensemble%20%28SMOTE-combined%29%0Aattains%2074.8%25%20test%20accuracy%2C%20while%20the%20CNN%20achieves%2079.19%25%20on%20the%20full%20test%20set%0Aand%2081.25%25%20on%20an%20unseen%20inference%20batch%2C%20confirming%20the%20advantage%20of%20deep%0Amodels.%20Nonetheless%2C%20the%20most%20under-represented%20class%20%28Barge%29%20remains%20a%20failure%0Amode%2C%20highlighting%20the%20limits%20of%20rebalancing%20alone.%20Results%20suggest%0Aprioritizing%20additional%20minority-class%20collection%20and%20cost-sensitive%20objectives%0A%28e.g.%2C%20focal%20loss%29%20and%20exploring%20hybrid%20ensemble%20or%20CNN%20pipelines%20to%20combine%0Ainterpretability%20with%20representational%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVehicle%2520Classification%2520under%2520Extreme%2520Imbalance%253A%2520A%2520Comparative%2520Study%2520of%250A%2520%2520Ensemble%2520Learning%2520and%2520CNNs%26entry.906535625%3DAbu%2520Hanif%2520Muhammad%2520Syarubany%26entry.1292438233%3D%2520%2520Accurate%2520vehicle%2520type%2520recognition%2520underpins%2520intelligent%2520transportation%2520and%250Alogistics%252C%2520but%2520severe%2520class%2520imbalance%2520in%2520public%2520datasets%2520suppresses%2520performance%250Aon%2520rare%2520categories.%2520We%2520curate%2520a%252016-class%2520corpus%2520%2528~47k%2520images%2529%2520by%2520merging%250AKaggle%252C%2520ImageNet%252C%2520and%2520web-crawled%2520data%252C%2520and%2520create%2520six%2520balanced%2520variants%2520via%250ASMOTE%2520oversampling%2520and%2520targeted%2520undersampling.%2520Lightweight%2520ensembles%252C%2520such%2520as%250ARandom%2520Forest%252C%2520AdaBoost%252C%2520and%2520a%2520soft-voting%2520combiner%2520built%2520on%2520MobileNet-V2%250Afeatures%2520are%2520benchmarked%2520against%2520a%2520configurable%2520ResNet-style%2520CNN%2520trained%2520with%250Astrong%2520augmentation%2520and%2520label%2520smoothing.%2520The%2520best%2520ensemble%2520%2528SMOTE-combined%2529%250Aattains%252074.8%2525%2520test%2520accuracy%252C%2520while%2520the%2520CNN%2520achieves%252079.19%2525%2520on%2520the%2520full%2520test%2520set%250Aand%252081.25%2525%2520on%2520an%2520unseen%2520inference%2520batch%252C%2520confirming%2520the%2520advantage%2520of%2520deep%250Amodels.%2520Nonetheless%252C%2520the%2520most%2520under-represented%2520class%2520%2528Barge%2529%2520remains%2520a%2520failure%250Amode%252C%2520highlighting%2520the%2520limits%2520of%2520rebalancing%2520alone.%2520Results%2520suggest%250Aprioritizing%2520additional%2520minority-class%2520collection%2520and%2520cost-sensitive%2520objectives%250A%2528e.g.%252C%2520focal%2520loss%2529%2520and%2520exploring%2520hybrid%2520ensemble%2520or%2520CNN%2520pipelines%2520to%2520combine%250Ainterpretability%2520with%2520representational%2520power.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vehicle%20Classification%20under%20Extreme%20Imbalance%3A%20A%20Comparative%20Study%20of%0A%20%20Ensemble%20Learning%20and%20CNNs&entry.906535625=Abu%20Hanif%20Muhammad%20Syarubany&entry.1292438233=%20%20Accurate%20vehicle%20type%20recognition%20underpins%20intelligent%20transportation%20and%0Alogistics%2C%20but%20severe%20class%20imbalance%20in%20public%20datasets%20suppresses%20performance%0Aon%20rare%20categories.%20We%20curate%20a%2016-class%20corpus%20%28~47k%20images%29%20by%20merging%0AKaggle%2C%20ImageNet%2C%20and%20web-crawled%20data%2C%20and%20create%20six%20balanced%20variants%20via%0ASMOTE%20oversampling%20and%20targeted%20undersampling.%20Lightweight%20ensembles%2C%20such%20as%0ARandom%20Forest%2C%20AdaBoost%2C%20and%20a%20soft-voting%20combiner%20built%20on%20MobileNet-V2%0Afeatures%20are%20benchmarked%20against%20a%20configurable%20ResNet-style%20CNN%20trained%20with%0Astrong%20augmentation%20and%20label%20smoothing.%20The%20best%20ensemble%20%28SMOTE-combined%29%0Aattains%2074.8%25%20test%20accuracy%2C%20while%20the%20CNN%20achieves%2079.19%25%20on%20the%20full%20test%20set%0Aand%2081.25%25%20on%20an%20unseen%20inference%20batch%2C%20confirming%20the%20advantage%20of%20deep%0Amodels.%20Nonetheless%2C%20the%20most%20under-represented%20class%20%28Barge%29%20remains%20a%20failure%0Amode%2C%20highlighting%20the%20limits%20of%20rebalancing%20alone.%20Results%20suggest%0Aprioritizing%20additional%20minority-class%20collection%20and%20cost-sensitive%20objectives%0A%28e.g.%2C%20focal%20loss%29%20and%20exploring%20hybrid%20ensemble%20or%20CNN%20pipelines%20to%20combine%0Ainterpretability%20with%20representational%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24880v1&entry.124074799=Read"},
{"title": "Multimodal Iterative RAG for Knowledge-Intensive Visual Question\n  Answering", "author": "Changin Choi and Wonseok Lee and Jungmin Ko and Wonjong Rhee", "abstract": "  Recent advances in Multimodal Large Language Models~(MLLMs) have\nsignificantly enhanced the ability of these models in multimodal understanding\nand reasoning. However, the performance of MLLMs for knowledge-intensive visual\nquestions, which require external knowledge beyond the visual content of an\nimage, still remains limited. While Retrieval-Augmented Generation (RAG) has\nbecome a promising solution to provide models with external knowledge, its\nconventional single-pass framework often fails to gather sufficient knowledge.\nTo overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG\nframework that leverages reasoning to enhance retrieval and incorporates\nknowledge synthesis to refine its understanding. At each iteration, the model\nformulates a reasoning-guided multi-query to explore multiple facets of\nknowledge. Subsequently, these queries drive a joint search across\nheterogeneous knowledge bases, retrieving diverse knowledge. This retrieved\nknowledge is then synthesized to enrich the reasoning record, progressively\ndeepening the model's understanding. Experiments on challenging benchmarks,\nincluding Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG\nsignificantly improves both retrieval recall and answer accuracy, establishing\na scalable approach for compositional reasoning in knowledge-intensive VQA.\n", "link": "http://arxiv.org/abs/2509.00798v4", "date": "2025-09-29", "relevancy": 2.6335, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Iterative%20RAG%20for%20Knowledge-Intensive%20Visual%20Question%0A%20%20Answering&body=Title%3A%20Multimodal%20Iterative%20RAG%20for%20Knowledge-Intensive%20Visual%20Question%0A%20%20Answering%0AAuthor%3A%20Changin%20Choi%20and%20Wonseok%20Lee%20and%20Jungmin%20Ko%20and%20Wonjong%20Rhee%0AAbstract%3A%20%20%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models~%28MLLMs%29%20have%0Asignificantly%20enhanced%20the%20ability%20of%20these%20models%20in%20multimodal%20understanding%0Aand%20reasoning.%20However%2C%20the%20performance%20of%20MLLMs%20for%20knowledge-intensive%20visual%0Aquestions%2C%20which%20require%20external%20knowledge%20beyond%20the%20visual%20content%20of%20an%0Aimage%2C%20still%20remains%20limited.%20While%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%0Abecome%20a%20promising%20solution%20to%20provide%20models%20with%20external%20knowledge%2C%20its%0Aconventional%20single-pass%20framework%20often%20fails%20to%20gather%20sufficient%20knowledge.%0ATo%20overcome%20this%20limitation%2C%20we%20propose%20MI-RAG%2C%20a%20Multimodal%20Iterative%20RAG%0Aframework%20that%20leverages%20reasoning%20to%20enhance%20retrieval%20and%20incorporates%0Aknowledge%20synthesis%20to%20refine%20its%20understanding.%20At%20each%20iteration%2C%20the%20model%0Aformulates%20a%20reasoning-guided%20multi-query%20to%20explore%20multiple%20facets%20of%0Aknowledge.%20Subsequently%2C%20these%20queries%20drive%20a%20joint%20search%20across%0Aheterogeneous%20knowledge%20bases%2C%20retrieving%20diverse%20knowledge.%20This%20retrieved%0Aknowledge%20is%20then%20synthesized%20to%20enrich%20the%20reasoning%20record%2C%20progressively%0Adeepening%20the%20model%27s%20understanding.%20Experiments%20on%20challenging%20benchmarks%2C%0Aincluding%20Encyclopedic%20VQA%2C%20InfoSeek%2C%20and%20OK-VQA%2C%20show%20that%20MI-RAG%0Asignificantly%20improves%20both%20retrieval%20recall%20and%20answer%20accuracy%2C%20establishing%0Aa%20scalable%20approach%20for%20compositional%20reasoning%20in%20knowledge-intensive%20VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00798v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Iterative%2520RAG%2520for%2520Knowledge-Intensive%2520Visual%2520Question%250A%2520%2520Answering%26entry.906535625%3DChangin%2520Choi%2520and%2520Wonseok%2520Lee%2520and%2520Jungmin%2520Ko%2520and%2520Wonjong%2520Rhee%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models~%2528MLLMs%2529%2520have%250Asignificantly%2520enhanced%2520the%2520ability%2520of%2520these%2520models%2520in%2520multimodal%2520understanding%250Aand%2520reasoning.%2520However%252C%2520the%2520performance%2520of%2520MLLMs%2520for%2520knowledge-intensive%2520visual%250Aquestions%252C%2520which%2520require%2520external%2520knowledge%2520beyond%2520the%2520visual%2520content%2520of%2520an%250Aimage%252C%2520still%2520remains%2520limited.%2520While%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%250Abecome%2520a%2520promising%2520solution%2520to%2520provide%2520models%2520with%2520external%2520knowledge%252C%2520its%250Aconventional%2520single-pass%2520framework%2520often%2520fails%2520to%2520gather%2520sufficient%2520knowledge.%250ATo%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520MI-RAG%252C%2520a%2520Multimodal%2520Iterative%2520RAG%250Aframework%2520that%2520leverages%2520reasoning%2520to%2520enhance%2520retrieval%2520and%2520incorporates%250Aknowledge%2520synthesis%2520to%2520refine%2520its%2520understanding.%2520At%2520each%2520iteration%252C%2520the%2520model%250Aformulates%2520a%2520reasoning-guided%2520multi-query%2520to%2520explore%2520multiple%2520facets%2520of%250Aknowledge.%2520Subsequently%252C%2520these%2520queries%2520drive%2520a%2520joint%2520search%2520across%250Aheterogeneous%2520knowledge%2520bases%252C%2520retrieving%2520diverse%2520knowledge.%2520This%2520retrieved%250Aknowledge%2520is%2520then%2520synthesized%2520to%2520enrich%2520the%2520reasoning%2520record%252C%2520progressively%250Adeepening%2520the%2520model%2527s%2520understanding.%2520Experiments%2520on%2520challenging%2520benchmarks%252C%250Aincluding%2520Encyclopedic%2520VQA%252C%2520InfoSeek%252C%2520and%2520OK-VQA%252C%2520show%2520that%2520MI-RAG%250Asignificantly%2520improves%2520both%2520retrieval%2520recall%2520and%2520answer%2520accuracy%252C%2520establishing%250Aa%2520scalable%2520approach%2520for%2520compositional%2520reasoning%2520in%2520knowledge-intensive%2520VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00798v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Iterative%20RAG%20for%20Knowledge-Intensive%20Visual%20Question%0A%20%20Answering&entry.906535625=Changin%20Choi%20and%20Wonseok%20Lee%20and%20Jungmin%20Ko%20and%20Wonjong%20Rhee&entry.1292438233=%20%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models~%28MLLMs%29%20have%0Asignificantly%20enhanced%20the%20ability%20of%20these%20models%20in%20multimodal%20understanding%0Aand%20reasoning.%20However%2C%20the%20performance%20of%20MLLMs%20for%20knowledge-intensive%20visual%0Aquestions%2C%20which%20require%20external%20knowledge%20beyond%20the%20visual%20content%20of%20an%0Aimage%2C%20still%20remains%20limited.%20While%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%0Abecome%20a%20promising%20solution%20to%20provide%20models%20with%20external%20knowledge%2C%20its%0Aconventional%20single-pass%20framework%20often%20fails%20to%20gather%20sufficient%20knowledge.%0ATo%20overcome%20this%20limitation%2C%20we%20propose%20MI-RAG%2C%20a%20Multimodal%20Iterative%20RAG%0Aframework%20that%20leverages%20reasoning%20to%20enhance%20retrieval%20and%20incorporates%0Aknowledge%20synthesis%20to%20refine%20its%20understanding.%20At%20each%20iteration%2C%20the%20model%0Aformulates%20a%20reasoning-guided%20multi-query%20to%20explore%20multiple%20facets%20of%0Aknowledge.%20Subsequently%2C%20these%20queries%20drive%20a%20joint%20search%20across%0Aheterogeneous%20knowledge%20bases%2C%20retrieving%20diverse%20knowledge.%20This%20retrieved%0Aknowledge%20is%20then%20synthesized%20to%20enrich%20the%20reasoning%20record%2C%20progressively%0Adeepening%20the%20model%27s%20understanding.%20Experiments%20on%20challenging%20benchmarks%2C%0Aincluding%20Encyclopedic%20VQA%2C%20InfoSeek%2C%20and%20OK-VQA%2C%20show%20that%20MI-RAG%0Asignificantly%20improves%20both%20retrieval%20recall%20and%20answer%20accuracy%2C%20establishing%0Aa%20scalable%20approach%20for%20compositional%20reasoning%20in%20knowledge-intensive%20VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00798v4&entry.124074799=Read"},
{"title": "On-the-Fly Data Augmentation for Brain Tumor Segmentation", "author": "Ishika Jain and Siri Willems and Steven Latre and Tom De Schepper", "abstract": "  Robust segmentation across both pre-treatment and post-treatment glioma scans\ncan be helpful for consistent tumor monitoring and treatment planning. BraTS\n2025 Task 1 addresses this by challenging models to generalize across varying\ntumor appearances throughout the treatment timeline. However, training such\ngeneralized models requires access to diverse, high-quality annotated data,\nwhich is often limited. While data augmentation can alleviate this, storing\nlarge volumes of augmented 3D data is computationally expensive. To address\nthese challenges, we propose an on-the-fly augmentation strategy that\ndynamically inserts synthetic tumors using pretrained generative adversarial\nnetworks (GliGANs) during training. We evaluate three nnU-Net-based models and\ntheir ensembles: (1) a baseline without external augmentation, (2) a regular\non-the-fly augmented model, and (3) a model with customized on-the-fly\naugmentation. Built upon the nnU-Net framework, our pipeline leverages\npretrained GliGAN weights and tumor insertion methods from prior\nchallenge-winning solutions. An ensemble of the three models achieves\nlesion-wise Dice scores of 0.79 (ET), 0.749 (NETC), 0.872 (RC), 0.825 (SNFH),\n0.79 (TC), and 0.88 (WT) on the online BraTS 2025 validation platform. This\nwork ranked first in the BraTS Lighthouse Challenge 2025 Task 1- Adult Glioma\nSegmentation.\n", "link": "http://arxiv.org/abs/2509.24973v1", "date": "2025-09-29", "relevancy": 2.6248, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5355}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5221}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-the-Fly%20Data%20Augmentation%20for%20Brain%20Tumor%20Segmentation&body=Title%3A%20On-the-Fly%20Data%20Augmentation%20for%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Ishika%20Jain%20and%20Siri%20Willems%20and%20Steven%20Latre%20and%20Tom%20De%20Schepper%0AAbstract%3A%20%20%20Robust%20segmentation%20across%20both%20pre-treatment%20and%20post-treatment%20glioma%20scans%0Acan%20be%20helpful%20for%20consistent%20tumor%20monitoring%20and%20treatment%20planning.%20BraTS%0A2025%20Task%201%20addresses%20this%20by%20challenging%20models%20to%20generalize%20across%20varying%0Atumor%20appearances%20throughout%20the%20treatment%20timeline.%20However%2C%20training%20such%0Ageneralized%20models%20requires%20access%20to%20diverse%2C%20high-quality%20annotated%20data%2C%0Awhich%20is%20often%20limited.%20While%20data%20augmentation%20can%20alleviate%20this%2C%20storing%0Alarge%20volumes%20of%20augmented%203D%20data%20is%20computationally%20expensive.%20To%20address%0Athese%20challenges%2C%20we%20propose%20an%20on-the-fly%20augmentation%20strategy%20that%0Adynamically%20inserts%20synthetic%20tumors%20using%20pretrained%20generative%20adversarial%0Anetworks%20%28GliGANs%29%20during%20training.%20We%20evaluate%20three%20nnU-Net-based%20models%20and%0Atheir%20ensembles%3A%20%281%29%20a%20baseline%20without%20external%20augmentation%2C%20%282%29%20a%20regular%0Aon-the-fly%20augmented%20model%2C%20and%20%283%29%20a%20model%20with%20customized%20on-the-fly%0Aaugmentation.%20Built%20upon%20the%20nnU-Net%20framework%2C%20our%20pipeline%20leverages%0Apretrained%20GliGAN%20weights%20and%20tumor%20insertion%20methods%20from%20prior%0Achallenge-winning%20solutions.%20An%20ensemble%20of%20the%20three%20models%20achieves%0Alesion-wise%20Dice%20scores%20of%200.79%20%28ET%29%2C%200.749%20%28NETC%29%2C%200.872%20%28RC%29%2C%200.825%20%28SNFH%29%2C%0A0.79%20%28TC%29%2C%20and%200.88%20%28WT%29%20on%20the%20online%20BraTS%202025%20validation%20platform.%20This%0Awork%20ranked%20first%20in%20the%20BraTS%20Lighthouse%20Challenge%202025%20Task%201-%20Adult%20Glioma%0ASegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-the-Fly%2520Data%2520Augmentation%2520for%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DIshika%2520Jain%2520and%2520Siri%2520Willems%2520and%2520Steven%2520Latre%2520and%2520Tom%2520De%2520Schepper%26entry.1292438233%3D%2520%2520Robust%2520segmentation%2520across%2520both%2520pre-treatment%2520and%2520post-treatment%2520glioma%2520scans%250Acan%2520be%2520helpful%2520for%2520consistent%2520tumor%2520monitoring%2520and%2520treatment%2520planning.%2520BraTS%250A2025%2520Task%25201%2520addresses%2520this%2520by%2520challenging%2520models%2520to%2520generalize%2520across%2520varying%250Atumor%2520appearances%2520throughout%2520the%2520treatment%2520timeline.%2520However%252C%2520training%2520such%250Ageneralized%2520models%2520requires%2520access%2520to%2520diverse%252C%2520high-quality%2520annotated%2520data%252C%250Awhich%2520is%2520often%2520limited.%2520While%2520data%2520augmentation%2520can%2520alleviate%2520this%252C%2520storing%250Alarge%2520volumes%2520of%2520augmented%25203D%2520data%2520is%2520computationally%2520expensive.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520an%2520on-the-fly%2520augmentation%2520strategy%2520that%250Adynamically%2520inserts%2520synthetic%2520tumors%2520using%2520pretrained%2520generative%2520adversarial%250Anetworks%2520%2528GliGANs%2529%2520during%2520training.%2520We%2520evaluate%2520three%2520nnU-Net-based%2520models%2520and%250Atheir%2520ensembles%253A%2520%25281%2529%2520a%2520baseline%2520without%2520external%2520augmentation%252C%2520%25282%2529%2520a%2520regular%250Aon-the-fly%2520augmented%2520model%252C%2520and%2520%25283%2529%2520a%2520model%2520with%2520customized%2520on-the-fly%250Aaugmentation.%2520Built%2520upon%2520the%2520nnU-Net%2520framework%252C%2520our%2520pipeline%2520leverages%250Apretrained%2520GliGAN%2520weights%2520and%2520tumor%2520insertion%2520methods%2520from%2520prior%250Achallenge-winning%2520solutions.%2520An%2520ensemble%2520of%2520the%2520three%2520models%2520achieves%250Alesion-wise%2520Dice%2520scores%2520of%25200.79%2520%2528ET%2529%252C%25200.749%2520%2528NETC%2529%252C%25200.872%2520%2528RC%2529%252C%25200.825%2520%2528SNFH%2529%252C%250A0.79%2520%2528TC%2529%252C%2520and%25200.88%2520%2528WT%2529%2520on%2520the%2520online%2520BraTS%25202025%2520validation%2520platform.%2520This%250Awork%2520ranked%2520first%2520in%2520the%2520BraTS%2520Lighthouse%2520Challenge%25202025%2520Task%25201-%2520Adult%2520Glioma%250ASegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-the-Fly%20Data%20Augmentation%20for%20Brain%20Tumor%20Segmentation&entry.906535625=Ishika%20Jain%20and%20Siri%20Willems%20and%20Steven%20Latre%20and%20Tom%20De%20Schepper&entry.1292438233=%20%20Robust%20segmentation%20across%20both%20pre-treatment%20and%20post-treatment%20glioma%20scans%0Acan%20be%20helpful%20for%20consistent%20tumor%20monitoring%20and%20treatment%20planning.%20BraTS%0A2025%20Task%201%20addresses%20this%20by%20challenging%20models%20to%20generalize%20across%20varying%0Atumor%20appearances%20throughout%20the%20treatment%20timeline.%20However%2C%20training%20such%0Ageneralized%20models%20requires%20access%20to%20diverse%2C%20high-quality%20annotated%20data%2C%0Awhich%20is%20often%20limited.%20While%20data%20augmentation%20can%20alleviate%20this%2C%20storing%0Alarge%20volumes%20of%20augmented%203D%20data%20is%20computationally%20expensive.%20To%20address%0Athese%20challenges%2C%20we%20propose%20an%20on-the-fly%20augmentation%20strategy%20that%0Adynamically%20inserts%20synthetic%20tumors%20using%20pretrained%20generative%20adversarial%0Anetworks%20%28GliGANs%29%20during%20training.%20We%20evaluate%20three%20nnU-Net-based%20models%20and%0Atheir%20ensembles%3A%20%281%29%20a%20baseline%20without%20external%20augmentation%2C%20%282%29%20a%20regular%0Aon-the-fly%20augmented%20model%2C%20and%20%283%29%20a%20model%20with%20customized%20on-the-fly%0Aaugmentation.%20Built%20upon%20the%20nnU-Net%20framework%2C%20our%20pipeline%20leverages%0Apretrained%20GliGAN%20weights%20and%20tumor%20insertion%20methods%20from%20prior%0Achallenge-winning%20solutions.%20An%20ensemble%20of%20the%20three%20models%20achieves%0Alesion-wise%20Dice%20scores%20of%200.79%20%28ET%29%2C%200.749%20%28NETC%29%2C%200.872%20%28RC%29%2C%200.825%20%28SNFH%29%2C%0A0.79%20%28TC%29%2C%20and%200.88%20%28WT%29%20on%20the%20online%20BraTS%202025%20validation%20platform.%20This%0Awork%20ranked%20first%20in%20the%20BraTS%20Lighthouse%20Challenge%202025%20Task%201-%20Adult%20Glioma%0ASegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24973v1&entry.124074799=Read"},
{"title": "CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image\n  Segmentation", "author": "Max Curie and Paulo da Costa", "abstract": "  We introduce CLASP (Clustering via Adaptive Spectral Processing), a\nlightweight framework for unsupervised image segmentation that operates without\nany labeled data or finetuning. CLASP first extracts per patch features using a\nself supervised ViT encoder (DINO); then, it builds an affinity matrix and\napplies spectral clustering. To avoid manual tuning, we select the segment\ncount automatically with a eigengap silhouette search, and we sharpen the\nboundaries with a fully connected DenseCRF. Despite its simplicity and training\nfree nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff\nand ADE20K, matching recent unsupervised baselines. The zero training design\nmakes CLASP a strong, easily reproducible baseline for large unannotated\ncorpora especially common in digital advertising and marketing workflows such\nas brand safety screening, creative asset curation, and social media content\nmoderation\n", "link": "http://arxiv.org/abs/2509.25016v1", "date": "2025-09-29", "relevancy": 2.6121, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5103}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLASP%3A%20Adaptive%20Spectral%20Clustering%20for%20Unsupervised%20Per-Image%0A%20%20Segmentation&body=Title%3A%20CLASP%3A%20Adaptive%20Spectral%20Clustering%20for%20Unsupervised%20Per-Image%0A%20%20Segmentation%0AAuthor%3A%20Max%20Curie%20and%20Paulo%20da%20Costa%0AAbstract%3A%20%20%20We%20introduce%20CLASP%20%28Clustering%20via%20Adaptive%20Spectral%20Processing%29%2C%20a%0Alightweight%20framework%20for%20unsupervised%20image%20segmentation%20that%20operates%20without%0Aany%20labeled%20data%20or%20finetuning.%20CLASP%20first%20extracts%20per%20patch%20features%20using%20a%0Aself%20supervised%20ViT%20encoder%20%28DINO%29%3B%20then%2C%20it%20builds%20an%20affinity%20matrix%20and%0Aapplies%20spectral%20clustering.%20To%20avoid%20manual%20tuning%2C%20we%20select%20the%20segment%0Acount%20automatically%20with%20a%20eigengap%20silhouette%20search%2C%20and%20we%20sharpen%20the%0Aboundaries%20with%20a%20fully%20connected%20DenseCRF.%20Despite%20its%20simplicity%20and%20training%0Afree%20nature%2C%20CLASP%20attains%20competitive%20mIoU%20and%20pixel%20accuracy%20on%20COCO%20Stuff%0Aand%20ADE20K%2C%20matching%20recent%20unsupervised%20baselines.%20The%20zero%20training%20design%0Amakes%20CLASP%20a%20strong%2C%20easily%20reproducible%20baseline%20for%20large%20unannotated%0Acorpora%20especially%20common%20in%20digital%20advertising%20and%20marketing%20workflows%20such%0Aas%20brand%20safety%20screening%2C%20creative%20asset%20curation%2C%20and%20social%20media%20content%0Amoderation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLASP%253A%2520Adaptive%2520Spectral%2520Clustering%2520for%2520Unsupervised%2520Per-Image%250A%2520%2520Segmentation%26entry.906535625%3DMax%2520Curie%2520and%2520Paulo%2520da%2520Costa%26entry.1292438233%3D%2520%2520We%2520introduce%2520CLASP%2520%2528Clustering%2520via%2520Adaptive%2520Spectral%2520Processing%2529%252C%2520a%250Alightweight%2520framework%2520for%2520unsupervised%2520image%2520segmentation%2520that%2520operates%2520without%250Aany%2520labeled%2520data%2520or%2520finetuning.%2520CLASP%2520first%2520extracts%2520per%2520patch%2520features%2520using%2520a%250Aself%2520supervised%2520ViT%2520encoder%2520%2528DINO%2529%253B%2520then%252C%2520it%2520builds%2520an%2520affinity%2520matrix%2520and%250Aapplies%2520spectral%2520clustering.%2520To%2520avoid%2520manual%2520tuning%252C%2520we%2520select%2520the%2520segment%250Acount%2520automatically%2520with%2520a%2520eigengap%2520silhouette%2520search%252C%2520and%2520we%2520sharpen%2520the%250Aboundaries%2520with%2520a%2520fully%2520connected%2520DenseCRF.%2520Despite%2520its%2520simplicity%2520and%2520training%250Afree%2520nature%252C%2520CLASP%2520attains%2520competitive%2520mIoU%2520and%2520pixel%2520accuracy%2520on%2520COCO%2520Stuff%250Aand%2520ADE20K%252C%2520matching%2520recent%2520unsupervised%2520baselines.%2520The%2520zero%2520training%2520design%250Amakes%2520CLASP%2520a%2520strong%252C%2520easily%2520reproducible%2520baseline%2520for%2520large%2520unannotated%250Acorpora%2520especially%2520common%2520in%2520digital%2520advertising%2520and%2520marketing%2520workflows%2520such%250Aas%2520brand%2520safety%2520screening%252C%2520creative%2520asset%2520curation%252C%2520and%2520social%2520media%2520content%250Amoderation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLASP%3A%20Adaptive%20Spectral%20Clustering%20for%20Unsupervised%20Per-Image%0A%20%20Segmentation&entry.906535625=Max%20Curie%20and%20Paulo%20da%20Costa&entry.1292438233=%20%20We%20introduce%20CLASP%20%28Clustering%20via%20Adaptive%20Spectral%20Processing%29%2C%20a%0Alightweight%20framework%20for%20unsupervised%20image%20segmentation%20that%20operates%20without%0Aany%20labeled%20data%20or%20finetuning.%20CLASP%20first%20extracts%20per%20patch%20features%20using%20a%0Aself%20supervised%20ViT%20encoder%20%28DINO%29%3B%20then%2C%20it%20builds%20an%20affinity%20matrix%20and%0Aapplies%20spectral%20clustering.%20To%20avoid%20manual%20tuning%2C%20we%20select%20the%20segment%0Acount%20automatically%20with%20a%20eigengap%20silhouette%20search%2C%20and%20we%20sharpen%20the%0Aboundaries%20with%20a%20fully%20connected%20DenseCRF.%20Despite%20its%20simplicity%20and%20training%0Afree%20nature%2C%20CLASP%20attains%20competitive%20mIoU%20and%20pixel%20accuracy%20on%20COCO%20Stuff%0Aand%20ADE20K%2C%20matching%20recent%20unsupervised%20baselines.%20The%20zero%20training%20design%0Amakes%20CLASP%20a%20strong%2C%20easily%20reproducible%20baseline%20for%20large%20unannotated%0Acorpora%20especially%20common%20in%20digital%20advertising%20and%20marketing%20workflows%20such%0Aas%20brand%20safety%20screening%2C%20creative%20asset%20curation%2C%20and%20social%20media%20content%0Amoderation%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25016v1&entry.124074799=Read"},
{"title": "BLADE: Block-Sparse Attention Meets Step Distillation for Efficient\n  Video Generation", "author": "Youping Gu and Xiaolong Li and Yuhao Hu and Minqi Chen and Bohan Zhuang", "abstract": "  Diffusion Transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm, built upon Trajectory Distribution Matching (TDM),\ndirectly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step and features fast convergence. We\nvalidate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B, and\nour framework demonstrates remarkable efficiency gains across different scales.\nOn Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over\na 50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Project is available at\nhttp://ziplab.co/BLADE-Homepage/.\n", "link": "http://arxiv.org/abs/2508.10774v2", "date": "2025-09-29", "relevancy": 2.5528, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6439}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6343}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLADE%3A%20Block-Sparse%20Attention%20Meets%20Step%20Distillation%20for%20Efficient%0A%20%20Video%20Generation&body=Title%3A%20BLADE%3A%20Block-Sparse%20Attention%20Meets%20Step%20Distillation%20for%20Efficient%0A%20%20Video%20Generation%0AAuthor%3A%20Youping%20Gu%20and%20Xiaolong%20Li%20and%20Yuhao%20Hu%20and%20Minqi%20Chen%20and%20Bohan%20Zhuang%0AAbstract%3A%20%20%20Diffusion%20Transformers%20currently%20lead%20the%20field%20in%20high-quality%20video%0Ageneration%2C%20but%20their%20slow%20iterative%20denoising%20process%20and%20prohibitive%0Aquadratic%20attention%20costs%20for%20long%20sequences%20create%20significant%20inference%0Abottlenecks.%20While%20both%20step%20distillation%20and%20sparse%20attention%20mechanisms%20have%0Ashown%20promise%20as%20independent%20acceleration%20strategies%2C%20effectively%20combining%0Athese%20approaches%20presents%20critical%20challenges%20--%20training-free%20integration%0Ayields%20suboptimal%20results%2C%20while%20separately%20training%20sparse%20attention%20after%0Astep%20distillation%20requires%20prohibitively%20expensive%20high-quality%20video%20data.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20BLADE%2C%20an%20innovative%20data-free%20joint%0Atraining%20framework%20that%20introduces%3A%20%281%29%20an%20Adaptive%20Block-Sparse%20Attention%0A%28ASA%29%20mechanism%20for%20dynamically%20generating%20content-aware%20sparsity%20masks%20to%0Afocus%20computation%20on%20salient%20spatiotemporal%20features%2C%20and%20%282%29%20a%20sparsity-aware%0Astep%20distillation%20paradigm%2C%20built%20upon%20Trajectory%20Distribution%20Matching%20%28TDM%29%2C%0Adirectly%20incorporates%20sparsity%20into%20the%20distillation%20process%20rather%20than%0Atreating%20it%20as%20a%20separate%20compression%20step%20and%20features%20fast%20convergence.%20We%0Avalidate%20BLADE%20on%20text-to-video%20models%20like%20CogVideoX-5B%20and%20Wan2.1-1.3B%2C%20and%0Aour%20framework%20demonstrates%20remarkable%20efficiency%20gains%20across%20different%20scales.%0AOn%20Wan2.1-1.3B%2C%20BLADE%20achieves%20a%2014.10x%20end-to-end%20inference%20acceleration%20over%0Aa%2050-step%20baseline.%20Moreover%2C%20on%20models%20such%20as%20CogVideoX-5B%20with%20short%20video%0Asequence%20lengths%2C%20our%20framework%20delivers%20a%20robust%208.89x%20speedup.%20Crucially%2C%20the%0Aacceleration%20is%20accompanied%20by%20a%20consistent%20quality%20improvement.%20On%20the%0AVBench-2.0%20benchmark%2C%20BLADE%20boosts%20the%20score%20of%20CogVideoX-5B%20to%200.569%20%28from%0A0.534%29%20and%20Wan2.1-1.3B%20to%200.570%20%28from%200.563%29%2C%20results%20that%20are%20further%0Acorroborated%20by%20superior%20ratings%20in%20human%20evaluations.%20Project%20is%20available%20at%0Ahttp%3A//ziplab.co/BLADE-Homepage/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLADE%253A%2520Block-Sparse%2520Attention%2520Meets%2520Step%2520Distillation%2520for%2520Efficient%250A%2520%2520Video%2520Generation%26entry.906535625%3DYouping%2520Gu%2520and%2520Xiaolong%2520Li%2520and%2520Yuhao%2520Hu%2520and%2520Minqi%2520Chen%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520currently%2520lead%2520the%2520field%2520in%2520high-quality%2520video%250Ageneration%252C%2520but%2520their%2520slow%2520iterative%2520denoising%2520process%2520and%2520prohibitive%250Aquadratic%2520attention%2520costs%2520for%2520long%2520sequences%2520create%2520significant%2520inference%250Abottlenecks.%2520While%2520both%2520step%2520distillation%2520and%2520sparse%2520attention%2520mechanisms%2520have%250Ashown%2520promise%2520as%2520independent%2520acceleration%2520strategies%252C%2520effectively%2520combining%250Athese%2520approaches%2520presents%2520critical%2520challenges%2520--%2520training-free%2520integration%250Ayields%2520suboptimal%2520results%252C%2520while%2520separately%2520training%2520sparse%2520attention%2520after%250Astep%2520distillation%2520requires%2520prohibitively%2520expensive%2520high-quality%2520video%2520data.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520BLADE%252C%2520an%2520innovative%2520data-free%2520joint%250Atraining%2520framework%2520that%2520introduces%253A%2520%25281%2529%2520an%2520Adaptive%2520Block-Sparse%2520Attention%250A%2528ASA%2529%2520mechanism%2520for%2520dynamically%2520generating%2520content-aware%2520sparsity%2520masks%2520to%250Afocus%2520computation%2520on%2520salient%2520spatiotemporal%2520features%252C%2520and%2520%25282%2529%2520a%2520sparsity-aware%250Astep%2520distillation%2520paradigm%252C%2520built%2520upon%2520Trajectory%2520Distribution%2520Matching%2520%2528TDM%2529%252C%250Adirectly%2520incorporates%2520sparsity%2520into%2520the%2520distillation%2520process%2520rather%2520than%250Atreating%2520it%2520as%2520a%2520separate%2520compression%2520step%2520and%2520features%2520fast%2520convergence.%2520We%250Avalidate%2520BLADE%2520on%2520text-to-video%2520models%2520like%2520CogVideoX-5B%2520and%2520Wan2.1-1.3B%252C%2520and%250Aour%2520framework%2520demonstrates%2520remarkable%2520efficiency%2520gains%2520across%2520different%2520scales.%250AOn%2520Wan2.1-1.3B%252C%2520BLADE%2520achieves%2520a%252014.10x%2520end-to-end%2520inference%2520acceleration%2520over%250Aa%252050-step%2520baseline.%2520Moreover%252C%2520on%2520models%2520such%2520as%2520CogVideoX-5B%2520with%2520short%2520video%250Asequence%2520lengths%252C%2520our%2520framework%2520delivers%2520a%2520robust%25208.89x%2520speedup.%2520Crucially%252C%2520the%250Aacceleration%2520is%2520accompanied%2520by%2520a%2520consistent%2520quality%2520improvement.%2520On%2520the%250AVBench-2.0%2520benchmark%252C%2520BLADE%2520boosts%2520the%2520score%2520of%2520CogVideoX-5B%2520to%25200.569%2520%2528from%250A0.534%2529%2520and%2520Wan2.1-1.3B%2520to%25200.570%2520%2528from%25200.563%2529%252C%2520results%2520that%2520are%2520further%250Acorroborated%2520by%2520superior%2520ratings%2520in%2520human%2520evaluations.%2520Project%2520is%2520available%2520at%250Ahttp%253A//ziplab.co/BLADE-Homepage/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLADE%3A%20Block-Sparse%20Attention%20Meets%20Step%20Distillation%20for%20Efficient%0A%20%20Video%20Generation&entry.906535625=Youping%20Gu%20and%20Xiaolong%20Li%20and%20Yuhao%20Hu%20and%20Minqi%20Chen%20and%20Bohan%20Zhuang&entry.1292438233=%20%20Diffusion%20Transformers%20currently%20lead%20the%20field%20in%20high-quality%20video%0Ageneration%2C%20but%20their%20slow%20iterative%20denoising%20process%20and%20prohibitive%0Aquadratic%20attention%20costs%20for%20long%20sequences%20create%20significant%20inference%0Abottlenecks.%20While%20both%20step%20distillation%20and%20sparse%20attention%20mechanisms%20have%0Ashown%20promise%20as%20independent%20acceleration%20strategies%2C%20effectively%20combining%0Athese%20approaches%20presents%20critical%20challenges%20--%20training-free%20integration%0Ayields%20suboptimal%20results%2C%20while%20separately%20training%20sparse%20attention%20after%0Astep%20distillation%20requires%20prohibitively%20expensive%20high-quality%20video%20data.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20BLADE%2C%20an%20innovative%20data-free%20joint%0Atraining%20framework%20that%20introduces%3A%20%281%29%20an%20Adaptive%20Block-Sparse%20Attention%0A%28ASA%29%20mechanism%20for%20dynamically%20generating%20content-aware%20sparsity%20masks%20to%0Afocus%20computation%20on%20salient%20spatiotemporal%20features%2C%20and%20%282%29%20a%20sparsity-aware%0Astep%20distillation%20paradigm%2C%20built%20upon%20Trajectory%20Distribution%20Matching%20%28TDM%29%2C%0Adirectly%20incorporates%20sparsity%20into%20the%20distillation%20process%20rather%20than%0Atreating%20it%20as%20a%20separate%20compression%20step%20and%20features%20fast%20convergence.%20We%0Avalidate%20BLADE%20on%20text-to-video%20models%20like%20CogVideoX-5B%20and%20Wan2.1-1.3B%2C%20and%0Aour%20framework%20demonstrates%20remarkable%20efficiency%20gains%20across%20different%20scales.%0AOn%20Wan2.1-1.3B%2C%20BLADE%20achieves%20a%2014.10x%20end-to-end%20inference%20acceleration%20over%0Aa%2050-step%20baseline.%20Moreover%2C%20on%20models%20such%20as%20CogVideoX-5B%20with%20short%20video%0Asequence%20lengths%2C%20our%20framework%20delivers%20a%20robust%208.89x%20speedup.%20Crucially%2C%20the%0Aacceleration%20is%20accompanied%20by%20a%20consistent%20quality%20improvement.%20On%20the%0AVBench-2.0%20benchmark%2C%20BLADE%20boosts%20the%20score%20of%20CogVideoX-5B%20to%200.569%20%28from%0A0.534%29%20and%20Wan2.1-1.3B%20to%200.570%20%28from%200.563%29%2C%20results%20that%20are%20further%0Acorroborated%20by%20superior%20ratings%20in%20human%20evaluations.%20Project%20is%20available%20at%0Ahttp%3A//ziplab.co/BLADE-Homepage/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10774v2&entry.124074799=Read"},
{"title": "Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local\n  Feature Matching", "author": "Zimin Xia and Chenghao Xu and Alexandre Alahi", "abstract": "  We propose an accurate and interpretable fine-grained cross-view localization\nmethod that estimates the 3 Degrees of Freedom (DoF) pose of a ground-level\nimage by matching its local features with a reference aerial image. Unlike\nprior approaches that rely on global descriptors or bird's-eye-view (BEV)\ntransformations, our method directly learns ground-aerial image-plane\ncorrespondences using weak supervision from camera poses. The matched ground\npoints are lifted into BEV space with monocular depth predictions, and\nscale-aware Procrustes alignment is then applied to estimate camera rotation,\ntranslation, and optionally the scale between relative depth and the aerial\nmetric space. This formulation is lightweight, end-to-end trainable, and\nrequires no pixel-level annotations. Experiments show state-of-the-art accuracy\nin challenging scenarios such as cross-area testing and unknown orientation.\nFurthermore, our method offers strong interpretability: correspondence quality\ndirectly reflects localization accuracy and enables outlier rejection via\nRANSAC, while overlaying the re-scaled ground layout on the aerial image\nprovides an intuitive visual cue of localization accuracy.\n", "link": "http://arxiv.org/abs/2509.09792v2", "date": "2025-09-29", "relevancy": 2.5497, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6775}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6185}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loc%24%5E2%24%3A%20Interpretable%20Cross-View%20Localization%20via%20Depth-Lifted%20Local%0A%20%20Feature%20Matching&body=Title%3A%20Loc%24%5E2%24%3A%20Interpretable%20Cross-View%20Localization%20via%20Depth-Lifted%20Local%0A%20%20Feature%20Matching%0AAuthor%3A%20Zimin%20Xia%20and%20Chenghao%20Xu%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20We%20propose%20an%20accurate%20and%20interpretable%20fine-grained%20cross-view%20localization%0Amethod%20that%20estimates%20the%203%20Degrees%20of%20Freedom%20%28DoF%29%20pose%20of%20a%20ground-level%0Aimage%20by%20matching%20its%20local%20features%20with%20a%20reference%20aerial%20image.%20Unlike%0Aprior%20approaches%20that%20rely%20on%20global%20descriptors%20or%20bird%27s-eye-view%20%28BEV%29%0Atransformations%2C%20our%20method%20directly%20learns%20ground-aerial%20image-plane%0Acorrespondences%20using%20weak%20supervision%20from%20camera%20poses.%20The%20matched%20ground%0Apoints%20are%20lifted%20into%20BEV%20space%20with%20monocular%20depth%20predictions%2C%20and%0Ascale-aware%20Procrustes%20alignment%20is%20then%20applied%20to%20estimate%20camera%20rotation%2C%0Atranslation%2C%20and%20optionally%20the%20scale%20between%20relative%20depth%20and%20the%20aerial%0Ametric%20space.%20This%20formulation%20is%20lightweight%2C%20end-to-end%20trainable%2C%20and%0Arequires%20no%20pixel-level%20annotations.%20Experiments%20show%20state-of-the-art%20accuracy%0Ain%20challenging%20scenarios%20such%20as%20cross-area%20testing%20and%20unknown%20orientation.%0AFurthermore%2C%20our%20method%20offers%20strong%20interpretability%3A%20correspondence%20quality%0Adirectly%20reflects%20localization%20accuracy%20and%20enables%20outlier%20rejection%20via%0ARANSAC%2C%20while%20overlaying%20the%20re-scaled%20ground%20layout%20on%20the%20aerial%20image%0Aprovides%20an%20intuitive%20visual%20cue%20of%20localization%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoc%2524%255E2%2524%253A%2520Interpretable%2520Cross-View%2520Localization%2520via%2520Depth-Lifted%2520Local%250A%2520%2520Feature%2520Matching%26entry.906535625%3DZimin%2520Xia%2520and%2520Chenghao%2520Xu%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520accurate%2520and%2520interpretable%2520fine-grained%2520cross-view%2520localization%250Amethod%2520that%2520estimates%2520the%25203%2520Degrees%2520of%2520Freedom%2520%2528DoF%2529%2520pose%2520of%2520a%2520ground-level%250Aimage%2520by%2520matching%2520its%2520local%2520features%2520with%2520a%2520reference%2520aerial%2520image.%2520Unlike%250Aprior%2520approaches%2520that%2520rely%2520on%2520global%2520descriptors%2520or%2520bird%2527s-eye-view%2520%2528BEV%2529%250Atransformations%252C%2520our%2520method%2520directly%2520learns%2520ground-aerial%2520image-plane%250Acorrespondences%2520using%2520weak%2520supervision%2520from%2520camera%2520poses.%2520The%2520matched%2520ground%250Apoints%2520are%2520lifted%2520into%2520BEV%2520space%2520with%2520monocular%2520depth%2520predictions%252C%2520and%250Ascale-aware%2520Procrustes%2520alignment%2520is%2520then%2520applied%2520to%2520estimate%2520camera%2520rotation%252C%250Atranslation%252C%2520and%2520optionally%2520the%2520scale%2520between%2520relative%2520depth%2520and%2520the%2520aerial%250Ametric%2520space.%2520This%2520formulation%2520is%2520lightweight%252C%2520end-to-end%2520trainable%252C%2520and%250Arequires%2520no%2520pixel-level%2520annotations.%2520Experiments%2520show%2520state-of-the-art%2520accuracy%250Ain%2520challenging%2520scenarios%2520such%2520as%2520cross-area%2520testing%2520and%2520unknown%2520orientation.%250AFurthermore%252C%2520our%2520method%2520offers%2520strong%2520interpretability%253A%2520correspondence%2520quality%250Adirectly%2520reflects%2520localization%2520accuracy%2520and%2520enables%2520outlier%2520rejection%2520via%250ARANSAC%252C%2520while%2520overlaying%2520the%2520re-scaled%2520ground%2520layout%2520on%2520the%2520aerial%2520image%250Aprovides%2520an%2520intuitive%2520visual%2520cue%2520of%2520localization%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loc%24%5E2%24%3A%20Interpretable%20Cross-View%20Localization%20via%20Depth-Lifted%20Local%0A%20%20Feature%20Matching&entry.906535625=Zimin%20Xia%20and%20Chenghao%20Xu%20and%20Alexandre%20Alahi&entry.1292438233=%20%20We%20propose%20an%20accurate%20and%20interpretable%20fine-grained%20cross-view%20localization%0Amethod%20that%20estimates%20the%203%20Degrees%20of%20Freedom%20%28DoF%29%20pose%20of%20a%20ground-level%0Aimage%20by%20matching%20its%20local%20features%20with%20a%20reference%20aerial%20image.%20Unlike%0Aprior%20approaches%20that%20rely%20on%20global%20descriptors%20or%20bird%27s-eye-view%20%28BEV%29%0Atransformations%2C%20our%20method%20directly%20learns%20ground-aerial%20image-plane%0Acorrespondences%20using%20weak%20supervision%20from%20camera%20poses.%20The%20matched%20ground%0Apoints%20are%20lifted%20into%20BEV%20space%20with%20monocular%20depth%20predictions%2C%20and%0Ascale-aware%20Procrustes%20alignment%20is%20then%20applied%20to%20estimate%20camera%20rotation%2C%0Atranslation%2C%20and%20optionally%20the%20scale%20between%20relative%20depth%20and%20the%20aerial%0Ametric%20space.%20This%20formulation%20is%20lightweight%2C%20end-to-end%20trainable%2C%20and%0Arequires%20no%20pixel-level%20annotations.%20Experiments%20show%20state-of-the-art%20accuracy%0Ain%20challenging%20scenarios%20such%20as%20cross-area%20testing%20and%20unknown%20orientation.%0AFurthermore%2C%20our%20method%20offers%20strong%20interpretability%3A%20correspondence%20quality%0Adirectly%20reflects%20localization%20accuracy%20and%20enables%20outlier%20rejection%20via%0ARANSAC%2C%20while%20overlaying%20the%20re-scaled%20ground%20layout%20on%20the%20aerial%20image%0Aprovides%20an%20intuitive%20visual%20cue%20of%20localization%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09792v2&entry.124074799=Read"},
{"title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from\n  RNA-Seq Data", "author": "Oussama Kharouiche and Aris Markogiannakis and Xiao Fei and Michail Chatzianastasis and Michalis Vazirgiannis", "abstract": "  Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells.\n", "link": "http://arxiv.org/abs/2509.24840v1", "date": "2025-09-29", "relevancy": 2.547, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cell2Text%3A%20Multimodal%20LLM%20for%20Generating%20Single-Cell%20Descriptions%20from%0A%20%20RNA-Seq%20Data&body=Title%3A%20Cell2Text%3A%20Multimodal%20LLM%20for%20Generating%20Single-Cell%20Descriptions%20from%0A%20%20RNA-Seq%20Data%0AAuthor%3A%20Oussama%20Kharouiche%20and%20Aris%20Markogiannakis%20and%20Xiao%20Fei%20and%20Michail%20Chatzianastasis%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Single-cell%20RNA%20sequencing%20has%20transformed%20biology%20by%20enabling%20the%0Ameasurement%20of%20gene%20expression%20at%20cellular%20resolution%2C%20providing%20information%0Afor%20cell%20types%2C%20states%2C%20and%20disease%20contexts.%20Recently%2C%20single-cell%20foundation%0Amodels%20have%20emerged%20as%20powerful%20tools%20for%20learning%20transferable%20representations%0Adirectly%20from%20expression%20profiles%2C%20improving%20performance%20on%20classification%20and%0Aclustering%20tasks.%20However%2C%20these%20models%20are%20limited%20to%20discrete%20prediction%0Aheads%2C%20which%20collapse%20cellular%20complexity%20into%20predefined%20labels%20that%20fail%20to%0Acapture%20the%20richer%2C%20contextual%20explanations%20biologists%20need.%20We%20introduce%0ACell2Text%2C%20a%20multimodal%20generative%20framework%20that%20translates%20scRNA-seq%20profiles%0Ainto%20structured%20natural%20language%20descriptions.%20By%20integrating%20gene-level%0Aembeddings%20from%20single-cell%20foundation%20models%20with%20pretrained%20large%20language%0Amodels%2C%20Cell2Text%20generates%20coherent%20summaries%20that%20capture%20cellular%20identity%2C%0Atissue%20origin%2C%20disease%20associations%2C%20and%20pathway%20activity%2C%20generalizing%20to%0Aunseen%20cells.%20Empirically%2C%20Cell2Text%20outperforms%20baselines%20on%20classification%0Aaccuracy%2C%20demonstrates%20strong%20ontological%20consistency%20using%20PageRank-based%0Asimilarity%20metrics%2C%20and%20achieves%20high%20semantic%20fidelity%20in%20text%20generation.%0AThese%20results%20demonstrate%20that%20coupling%20expression%20data%20with%20natural%20language%0Aoffers%20both%20stronger%20predictive%20performance%20and%20inherently%20interpretable%0Aoutputs%2C%20pointing%20to%20a%20scalable%20path%20for%20label-efficient%20characterization%20of%0Aunseen%20cells.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCell2Text%253A%2520Multimodal%2520LLM%2520for%2520Generating%2520Single-Cell%2520Descriptions%2520from%250A%2520%2520RNA-Seq%2520Data%26entry.906535625%3DOussama%2520Kharouiche%2520and%2520Aris%2520Markogiannakis%2520and%2520Xiao%2520Fei%2520and%2520Michail%2520Chatzianastasis%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Single-cell%2520RNA%2520sequencing%2520has%2520transformed%2520biology%2520by%2520enabling%2520the%250Ameasurement%2520of%2520gene%2520expression%2520at%2520cellular%2520resolution%252C%2520providing%2520information%250Afor%2520cell%2520types%252C%2520states%252C%2520and%2520disease%2520contexts.%2520Recently%252C%2520single-cell%2520foundation%250Amodels%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520learning%2520transferable%2520representations%250Adirectly%2520from%2520expression%2520profiles%252C%2520improving%2520performance%2520on%2520classification%2520and%250Aclustering%2520tasks.%2520However%252C%2520these%2520models%2520are%2520limited%2520to%2520discrete%2520prediction%250Aheads%252C%2520which%2520collapse%2520cellular%2520complexity%2520into%2520predefined%2520labels%2520that%2520fail%2520to%250Acapture%2520the%2520richer%252C%2520contextual%2520explanations%2520biologists%2520need.%2520We%2520introduce%250ACell2Text%252C%2520a%2520multimodal%2520generative%2520framework%2520that%2520translates%2520scRNA-seq%2520profiles%250Ainto%2520structured%2520natural%2520language%2520descriptions.%2520By%2520integrating%2520gene-level%250Aembeddings%2520from%2520single-cell%2520foundation%2520models%2520with%2520pretrained%2520large%2520language%250Amodels%252C%2520Cell2Text%2520generates%2520coherent%2520summaries%2520that%2520capture%2520cellular%2520identity%252C%250Atissue%2520origin%252C%2520disease%2520associations%252C%2520and%2520pathway%2520activity%252C%2520generalizing%2520to%250Aunseen%2520cells.%2520Empirically%252C%2520Cell2Text%2520outperforms%2520baselines%2520on%2520classification%250Aaccuracy%252C%2520demonstrates%2520strong%2520ontological%2520consistency%2520using%2520PageRank-based%250Asimilarity%2520metrics%252C%2520and%2520achieves%2520high%2520semantic%2520fidelity%2520in%2520text%2520generation.%250AThese%2520results%2520demonstrate%2520that%2520coupling%2520expression%2520data%2520with%2520natural%2520language%250Aoffers%2520both%2520stronger%2520predictive%2520performance%2520and%2520inherently%2520interpretable%250Aoutputs%252C%2520pointing%2520to%2520a%2520scalable%2520path%2520for%2520label-efficient%2520characterization%2520of%250Aunseen%2520cells.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cell2Text%3A%20Multimodal%20LLM%20for%20Generating%20Single-Cell%20Descriptions%20from%0A%20%20RNA-Seq%20Data&entry.906535625=Oussama%20Kharouiche%20and%20Aris%20Markogiannakis%20and%20Xiao%20Fei%20and%20Michail%20Chatzianastasis%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Single-cell%20RNA%20sequencing%20has%20transformed%20biology%20by%20enabling%20the%0Ameasurement%20of%20gene%20expression%20at%20cellular%20resolution%2C%20providing%20information%0Afor%20cell%20types%2C%20states%2C%20and%20disease%20contexts.%20Recently%2C%20single-cell%20foundation%0Amodels%20have%20emerged%20as%20powerful%20tools%20for%20learning%20transferable%20representations%0Adirectly%20from%20expression%20profiles%2C%20improving%20performance%20on%20classification%20and%0Aclustering%20tasks.%20However%2C%20these%20models%20are%20limited%20to%20discrete%20prediction%0Aheads%2C%20which%20collapse%20cellular%20complexity%20into%20predefined%20labels%20that%20fail%20to%0Acapture%20the%20richer%2C%20contextual%20explanations%20biologists%20need.%20We%20introduce%0ACell2Text%2C%20a%20multimodal%20generative%20framework%20that%20translates%20scRNA-seq%20profiles%0Ainto%20structured%20natural%20language%20descriptions.%20By%20integrating%20gene-level%0Aembeddings%20from%20single-cell%20foundation%20models%20with%20pretrained%20large%20language%0Amodels%2C%20Cell2Text%20generates%20coherent%20summaries%20that%20capture%20cellular%20identity%2C%0Atissue%20origin%2C%20disease%20associations%2C%20and%20pathway%20activity%2C%20generalizing%20to%0Aunseen%20cells.%20Empirically%2C%20Cell2Text%20outperforms%20baselines%20on%20classification%0Aaccuracy%2C%20demonstrates%20strong%20ontological%20consistency%20using%20PageRank-based%0Asimilarity%20metrics%2C%20and%20achieves%20high%20semantic%20fidelity%20in%20text%20generation.%0AThese%20results%20demonstrate%20that%20coupling%20expression%20data%20with%20natural%20language%0Aoffers%20both%20stronger%20predictive%20performance%20and%20inherently%20interpretable%0Aoutputs%2C%20pointing%20to%20a%20scalable%20path%20for%20label-efficient%20characterization%20of%0Aunseen%20cells.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24840v1&entry.124074799=Read"},
{"title": "Adaptive Canonicalization with Application to Invariant Anisotropic\n  Geometric Networks", "author": "Ya-Wei Eileen Lin and Ron Levie", "abstract": "  Canonicalization is a widely used strategy in equivariant machine learning,\nenforcing symmetry in neural networks by mapping each input to a standard form.\nYet, it often introduces discontinuities that can affect stability during\ntraining, limit generalization, and complicate universal approximation\ntheorems. In this paper, we address this by introducing \\emph{adaptive\ncanonicalization}, a general framework in which the canonicalization depends\nboth on the input and the network. Specifically, we present the adaptive\ncanonicalization based on prior maximization, where the standard form of the\ninput is chosen to maximize the predictive confidence of the network. We prove\nthat this construction yields continuous and symmetry-respecting models that\nadmit universal approximation properties.\n  We propose two applications of our setting: (i) resolving eigenbasis\nambiguities in spectral graph neural networks, and (ii) handling rotational\nsymmetries in point clouds. We empirically validate our methods on molecular\nand protein classification, as well as point cloud classification tasks. Our\nadaptive canonicalization outperforms the three other common solutions to\nequivariant machine learning: data augmentation, standard canonicalization, and\nequivariant architectures.\n", "link": "http://arxiv.org/abs/2509.24886v1", "date": "2025-09-29", "relevancy": 2.5337, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5089}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5057}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Canonicalization%20with%20Application%20to%20Invariant%20Anisotropic%0A%20%20Geometric%20Networks&body=Title%3A%20Adaptive%20Canonicalization%20with%20Application%20to%20Invariant%20Anisotropic%0A%20%20Geometric%20Networks%0AAuthor%3A%20Ya-Wei%20Eileen%20Lin%20and%20Ron%20Levie%0AAbstract%3A%20%20%20Canonicalization%20is%20a%20widely%20used%20strategy%20in%20equivariant%20machine%20learning%2C%0Aenforcing%20symmetry%20in%20neural%20networks%20by%20mapping%20each%20input%20to%20a%20standard%20form.%0AYet%2C%20it%20often%20introduces%20discontinuities%20that%20can%20affect%20stability%20during%0Atraining%2C%20limit%20generalization%2C%20and%20complicate%20universal%20approximation%0Atheorems.%20In%20this%20paper%2C%20we%20address%20this%20by%20introducing%20%5Cemph%7Badaptive%0Acanonicalization%7D%2C%20a%20general%20framework%20in%20which%20the%20canonicalization%20depends%0Aboth%20on%20the%20input%20and%20the%20network.%20Specifically%2C%20we%20present%20the%20adaptive%0Acanonicalization%20based%20on%20prior%20maximization%2C%20where%20the%20standard%20form%20of%20the%0Ainput%20is%20chosen%20to%20maximize%20the%20predictive%20confidence%20of%20the%20network.%20We%20prove%0Athat%20this%20construction%20yields%20continuous%20and%20symmetry-respecting%20models%20that%0Aadmit%20universal%20approximation%20properties.%0A%20%20We%20propose%20two%20applications%20of%20our%20setting%3A%20%28i%29%20resolving%20eigenbasis%0Aambiguities%20in%20spectral%20graph%20neural%20networks%2C%20and%20%28ii%29%20handling%20rotational%0Asymmetries%20in%20point%20clouds.%20We%20empirically%20validate%20our%20methods%20on%20molecular%0Aand%20protein%20classification%2C%20as%20well%20as%20point%20cloud%20classification%20tasks.%20Our%0Aadaptive%20canonicalization%20outperforms%20the%20three%20other%20common%20solutions%20to%0Aequivariant%20machine%20learning%3A%20data%20augmentation%2C%20standard%20canonicalization%2C%20and%0Aequivariant%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Canonicalization%2520with%2520Application%2520to%2520Invariant%2520Anisotropic%250A%2520%2520Geometric%2520Networks%26entry.906535625%3DYa-Wei%2520Eileen%2520Lin%2520and%2520Ron%2520Levie%26entry.1292438233%3D%2520%2520Canonicalization%2520is%2520a%2520widely%2520used%2520strategy%2520in%2520equivariant%2520machine%2520learning%252C%250Aenforcing%2520symmetry%2520in%2520neural%2520networks%2520by%2520mapping%2520each%2520input%2520to%2520a%2520standard%2520form.%250AYet%252C%2520it%2520often%2520introduces%2520discontinuities%2520that%2520can%2520affect%2520stability%2520during%250Atraining%252C%2520limit%2520generalization%252C%2520and%2520complicate%2520universal%2520approximation%250Atheorems.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520by%2520introducing%2520%255Cemph%257Badaptive%250Acanonicalization%257D%252C%2520a%2520general%2520framework%2520in%2520which%2520the%2520canonicalization%2520depends%250Aboth%2520on%2520the%2520input%2520and%2520the%2520network.%2520Specifically%252C%2520we%2520present%2520the%2520adaptive%250Acanonicalization%2520based%2520on%2520prior%2520maximization%252C%2520where%2520the%2520standard%2520form%2520of%2520the%250Ainput%2520is%2520chosen%2520to%2520maximize%2520the%2520predictive%2520confidence%2520of%2520the%2520network.%2520We%2520prove%250Athat%2520this%2520construction%2520yields%2520continuous%2520and%2520symmetry-respecting%2520models%2520that%250Aadmit%2520universal%2520approximation%2520properties.%250A%2520%2520We%2520propose%2520two%2520applications%2520of%2520our%2520setting%253A%2520%2528i%2529%2520resolving%2520eigenbasis%250Aambiguities%2520in%2520spectral%2520graph%2520neural%2520networks%252C%2520and%2520%2528ii%2529%2520handling%2520rotational%250Asymmetries%2520in%2520point%2520clouds.%2520We%2520empirically%2520validate%2520our%2520methods%2520on%2520molecular%250Aand%2520protein%2520classification%252C%2520as%2520well%2520as%2520point%2520cloud%2520classification%2520tasks.%2520Our%250Aadaptive%2520canonicalization%2520outperforms%2520the%2520three%2520other%2520common%2520solutions%2520to%250Aequivariant%2520machine%2520learning%253A%2520data%2520augmentation%252C%2520standard%2520canonicalization%252C%2520and%250Aequivariant%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Canonicalization%20with%20Application%20to%20Invariant%20Anisotropic%0A%20%20Geometric%20Networks&entry.906535625=Ya-Wei%20Eileen%20Lin%20and%20Ron%20Levie&entry.1292438233=%20%20Canonicalization%20is%20a%20widely%20used%20strategy%20in%20equivariant%20machine%20learning%2C%0Aenforcing%20symmetry%20in%20neural%20networks%20by%20mapping%20each%20input%20to%20a%20standard%20form.%0AYet%2C%20it%20often%20introduces%20discontinuities%20that%20can%20affect%20stability%20during%0Atraining%2C%20limit%20generalization%2C%20and%20complicate%20universal%20approximation%0Atheorems.%20In%20this%20paper%2C%20we%20address%20this%20by%20introducing%20%5Cemph%7Badaptive%0Acanonicalization%7D%2C%20a%20general%20framework%20in%20which%20the%20canonicalization%20depends%0Aboth%20on%20the%20input%20and%20the%20network.%20Specifically%2C%20we%20present%20the%20adaptive%0Acanonicalization%20based%20on%20prior%20maximization%2C%20where%20the%20standard%20form%20of%20the%0Ainput%20is%20chosen%20to%20maximize%20the%20predictive%20confidence%20of%20the%20network.%20We%20prove%0Athat%20this%20construction%20yields%20continuous%20and%20symmetry-respecting%20models%20that%0Aadmit%20universal%20approximation%20properties.%0A%20%20We%20propose%20two%20applications%20of%20our%20setting%3A%20%28i%29%20resolving%20eigenbasis%0Aambiguities%20in%20spectral%20graph%20neural%20networks%2C%20and%20%28ii%29%20handling%20rotational%0Asymmetries%20in%20point%20clouds.%20We%20empirically%20validate%20our%20methods%20on%20molecular%0Aand%20protein%20classification%2C%20as%20well%20as%20point%20cloud%20classification%20tasks.%20Our%0Aadaptive%20canonicalization%20outperforms%20the%20three%20other%20common%20solutions%20to%0Aequivariant%20machine%20learning%3A%20data%20augmentation%2C%20standard%20canonicalization%2C%20and%0Aequivariant%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24886v1&entry.124074799=Read"},
{"title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo\n  Collections", "author": "Zeyu Cai and Ziyang Li and Xiaoben Li and Boqian Li and Zeyu Wang and Zhenyu Zhang and Yuliang Xiu", "abstract": "  We present UP2You, the first tuning-free solution for reconstructing\nhigh-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D\nphotos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body\nimages with minimal occlusions, or well-calibrated cross-view captures), UP2You\ndirectly processes raw, unstructured photographs, which may vary significantly\nin pose, viewpoint, cropping, and occlusion. Instead of compressing data into\ntokens for slow online text-to-3D optimization, we introduce a data rectifier\nparadigm that efficiently converts unconstrained inputs into clean, orthogonal\nmulti-view images in a single forward pass within seconds, simplifying the 3D\nreconstruction. Central to UP2You is a pose-correlated feature aggregation\nmodule (PCFA), that selectively fuses information from multiple reference\nimages w.r.t. target poses, enabling better identity preservation and nearly\nconstant memory footprint, with more observations. We also introduce a\nperceiver-based multi-reference shape predictor, removing the need for\npre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and\nin-the-wild captures demonstrate that UP2You consistently surpasses previous\nmethods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and\ntexture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5\nminutes per person), and versatile (supports arbitrary pose control, and\ntraining-free multi-garment 3D virtual try-on), making it practical for\nreal-world scenarios where humans are casually captured. Both models and code\nwill be released to facilitate future research on this underexplored task.\nProject Page: https://zcai0612.github.io/UP2You\n", "link": "http://arxiv.org/abs/2509.24817v1", "date": "2025-09-29", "relevancy": 2.516, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6341}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6276}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UP2You%3A%20Fast%20Reconstruction%20of%20Yourself%20from%20Unconstrained%20Photo%0A%20%20Collections&body=Title%3A%20UP2You%3A%20Fast%20Reconstruction%20of%20Yourself%20from%20Unconstrained%20Photo%0A%20%20Collections%0AAuthor%3A%20Zeyu%20Cai%20and%20Ziyang%20Li%20and%20Xiaoben%20Li%20and%20Boqian%20Li%20and%20Zeyu%20Wang%20and%20Zhenyu%20Zhang%20and%20Yuliang%20Xiu%0AAbstract%3A%20%20%20We%20present%20UP2You%2C%20the%20first%20tuning-free%20solution%20for%20reconstructing%0Ahigh-fidelity%203D%20clothed%20portraits%20from%20extremely%20unconstrained%20in-the-wild%202D%0Aphotos.%20Unlike%20previous%20approaches%20that%20require%20%22clean%22%20inputs%20%28e.g.%2C%20full-body%0Aimages%20with%20minimal%20occlusions%2C%20or%20well-calibrated%20cross-view%20captures%29%2C%20UP2You%0Adirectly%20processes%20raw%2C%20unstructured%20photographs%2C%20which%20may%20vary%20significantly%0Ain%20pose%2C%20viewpoint%2C%20cropping%2C%20and%20occlusion.%20Instead%20of%20compressing%20data%20into%0Atokens%20for%20slow%20online%20text-to-3D%20optimization%2C%20we%20introduce%20a%20data%20rectifier%0Aparadigm%20that%20efficiently%20converts%20unconstrained%20inputs%20into%20clean%2C%20orthogonal%0Amulti-view%20images%20in%20a%20single%20forward%20pass%20within%20seconds%2C%20simplifying%20the%203D%0Areconstruction.%20Central%20to%20UP2You%20is%20a%20pose-correlated%20feature%20aggregation%0Amodule%20%28PCFA%29%2C%20that%20selectively%20fuses%20information%20from%20multiple%20reference%0Aimages%20w.r.t.%20target%20poses%2C%20enabling%20better%20identity%20preservation%20and%20nearly%0Aconstant%20memory%20footprint%2C%20with%20more%20observations.%20We%20also%20introduce%20a%0Aperceiver-based%20multi-reference%20shape%20predictor%2C%20removing%20the%20need%20for%0Apre-captured%20body%20templates.%20Extensive%20experiments%20on%204D-Dress%2C%20PuzzleIOI%2C%20and%0Ain-the-wild%20captures%20demonstrate%20that%20UP2You%20consistently%20surpasses%20previous%0Amethods%20in%20both%20geometric%20accuracy%20%28Chamfer-15%25%2C%20P2S-18%25%20on%20PuzzleIOI%29%20and%0Atexture%20fidelity%20%28PSNR-21%25%2C%20LPIPS-46%25%20on%204D-Dress%29.%20UP2You%20is%20efficient%20%281.5%0Aminutes%20per%20person%29%2C%20and%20versatile%20%28supports%20arbitrary%20pose%20control%2C%20and%0Atraining-free%20multi-garment%203D%20virtual%20try-on%29%2C%20making%20it%20practical%20for%0Areal-world%20scenarios%20where%20humans%20are%20casually%20captured.%20Both%20models%20and%20code%0Awill%20be%20released%20to%20facilitate%20future%20research%20on%20this%20underexplored%20task.%0AProject%20Page%3A%20https%3A//zcai0612.github.io/UP2You%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUP2You%253A%2520Fast%2520Reconstruction%2520of%2520Yourself%2520from%2520Unconstrained%2520Photo%250A%2520%2520Collections%26entry.906535625%3DZeyu%2520Cai%2520and%2520Ziyang%2520Li%2520and%2520Xiaoben%2520Li%2520and%2520Boqian%2520Li%2520and%2520Zeyu%2520Wang%2520and%2520Zhenyu%2520Zhang%2520and%2520Yuliang%2520Xiu%26entry.1292438233%3D%2520%2520We%2520present%2520UP2You%252C%2520the%2520first%2520tuning-free%2520solution%2520for%2520reconstructing%250Ahigh-fidelity%25203D%2520clothed%2520portraits%2520from%2520extremely%2520unconstrained%2520in-the-wild%25202D%250Aphotos.%2520Unlike%2520previous%2520approaches%2520that%2520require%2520%2522clean%2522%2520inputs%2520%2528e.g.%252C%2520full-body%250Aimages%2520with%2520minimal%2520occlusions%252C%2520or%2520well-calibrated%2520cross-view%2520captures%2529%252C%2520UP2You%250Adirectly%2520processes%2520raw%252C%2520unstructured%2520photographs%252C%2520which%2520may%2520vary%2520significantly%250Ain%2520pose%252C%2520viewpoint%252C%2520cropping%252C%2520and%2520occlusion.%2520Instead%2520of%2520compressing%2520data%2520into%250Atokens%2520for%2520slow%2520online%2520text-to-3D%2520optimization%252C%2520we%2520introduce%2520a%2520data%2520rectifier%250Aparadigm%2520that%2520efficiently%2520converts%2520unconstrained%2520inputs%2520into%2520clean%252C%2520orthogonal%250Amulti-view%2520images%2520in%2520a%2520single%2520forward%2520pass%2520within%2520seconds%252C%2520simplifying%2520the%25203D%250Areconstruction.%2520Central%2520to%2520UP2You%2520is%2520a%2520pose-correlated%2520feature%2520aggregation%250Amodule%2520%2528PCFA%2529%252C%2520that%2520selectively%2520fuses%2520information%2520from%2520multiple%2520reference%250Aimages%2520w.r.t.%2520target%2520poses%252C%2520enabling%2520better%2520identity%2520preservation%2520and%2520nearly%250Aconstant%2520memory%2520footprint%252C%2520with%2520more%2520observations.%2520We%2520also%2520introduce%2520a%250Aperceiver-based%2520multi-reference%2520shape%2520predictor%252C%2520removing%2520the%2520need%2520for%250Apre-captured%2520body%2520templates.%2520Extensive%2520experiments%2520on%25204D-Dress%252C%2520PuzzleIOI%252C%2520and%250Ain-the-wild%2520captures%2520demonstrate%2520that%2520UP2You%2520consistently%2520surpasses%2520previous%250Amethods%2520in%2520both%2520geometric%2520accuracy%2520%2528Chamfer-15%2525%252C%2520P2S-18%2525%2520on%2520PuzzleIOI%2529%2520and%250Atexture%2520fidelity%2520%2528PSNR-21%2525%252C%2520LPIPS-46%2525%2520on%25204D-Dress%2529.%2520UP2You%2520is%2520efficient%2520%25281.5%250Aminutes%2520per%2520person%2529%252C%2520and%2520versatile%2520%2528supports%2520arbitrary%2520pose%2520control%252C%2520and%250Atraining-free%2520multi-garment%25203D%2520virtual%2520try-on%2529%252C%2520making%2520it%2520practical%2520for%250Areal-world%2520scenarios%2520where%2520humans%2520are%2520casually%2520captured.%2520Both%2520models%2520and%2520code%250Awill%2520be%2520released%2520to%2520facilitate%2520future%2520research%2520on%2520this%2520underexplored%2520task.%250AProject%2520Page%253A%2520https%253A//zcai0612.github.io/UP2You%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UP2You%3A%20Fast%20Reconstruction%20of%20Yourself%20from%20Unconstrained%20Photo%0A%20%20Collections&entry.906535625=Zeyu%20Cai%20and%20Ziyang%20Li%20and%20Xiaoben%20Li%20and%20Boqian%20Li%20and%20Zeyu%20Wang%20and%20Zhenyu%20Zhang%20and%20Yuliang%20Xiu&entry.1292438233=%20%20We%20present%20UP2You%2C%20the%20first%20tuning-free%20solution%20for%20reconstructing%0Ahigh-fidelity%203D%20clothed%20portraits%20from%20extremely%20unconstrained%20in-the-wild%202D%0Aphotos.%20Unlike%20previous%20approaches%20that%20require%20%22clean%22%20inputs%20%28e.g.%2C%20full-body%0Aimages%20with%20minimal%20occlusions%2C%20or%20well-calibrated%20cross-view%20captures%29%2C%20UP2You%0Adirectly%20processes%20raw%2C%20unstructured%20photographs%2C%20which%20may%20vary%20significantly%0Ain%20pose%2C%20viewpoint%2C%20cropping%2C%20and%20occlusion.%20Instead%20of%20compressing%20data%20into%0Atokens%20for%20slow%20online%20text-to-3D%20optimization%2C%20we%20introduce%20a%20data%20rectifier%0Aparadigm%20that%20efficiently%20converts%20unconstrained%20inputs%20into%20clean%2C%20orthogonal%0Amulti-view%20images%20in%20a%20single%20forward%20pass%20within%20seconds%2C%20simplifying%20the%203D%0Areconstruction.%20Central%20to%20UP2You%20is%20a%20pose-correlated%20feature%20aggregation%0Amodule%20%28PCFA%29%2C%20that%20selectively%20fuses%20information%20from%20multiple%20reference%0Aimages%20w.r.t.%20target%20poses%2C%20enabling%20better%20identity%20preservation%20and%20nearly%0Aconstant%20memory%20footprint%2C%20with%20more%20observations.%20We%20also%20introduce%20a%0Aperceiver-based%20multi-reference%20shape%20predictor%2C%20removing%20the%20need%20for%0Apre-captured%20body%20templates.%20Extensive%20experiments%20on%204D-Dress%2C%20PuzzleIOI%2C%20and%0Ain-the-wild%20captures%20demonstrate%20that%20UP2You%20consistently%20surpasses%20previous%0Amethods%20in%20both%20geometric%20accuracy%20%28Chamfer-15%25%2C%20P2S-18%25%20on%20PuzzleIOI%29%20and%0Atexture%20fidelity%20%28PSNR-21%25%2C%20LPIPS-46%25%20on%204D-Dress%29.%20UP2You%20is%20efficient%20%281.5%0Aminutes%20per%20person%29%2C%20and%20versatile%20%28supports%20arbitrary%20pose%20control%2C%20and%0Atraining-free%20multi-garment%203D%20virtual%20try-on%29%2C%20making%20it%20practical%20for%0Areal-world%20scenarios%20where%20humans%20are%20casually%20captured.%20Both%20models%20and%20code%0Awill%20be%20released%20to%20facilitate%20future%20research%20on%20this%20underexplored%20task.%0AProject%20Page%3A%20https%3A//zcai0612.github.io/UP2You%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24817v1&entry.124074799=Read"},
{"title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion\n  Transformer", "author": "Mohsen Ghafoorian and Denis Korzhenkov and Amirhossein Habibian", "abstract": "  Transformer-based video diffusion models (VDMs) deliver state-of-the-art\nvideo generation quality but are constrained by the quadratic cost of\nself-attention, making long sequences and high resolutions computationally\nexpensive. While linear attention offers sub-quadratic complexity, prior\nattempts fail to match the expressiveness of softmax attention without costly\nretraining. We introduce \\textit{Attention Surgery}, an efficient framework for\n\\textit{linearizing} or \\textit{hybridizing} attention in pretrained VDMs\nwithout training from scratch. Inspired by recent advances in language models,\nour method combines a novel hybrid attention mechanism-mixing softmax and\nlinear tokens-with a lightweight distillation and fine-tuning pipeline\nrequiring only a few GPU-days. Additionally, we incorporate a cost-aware\nblock-rate strategy to balance expressiveness and efficiency across layers.\nApplied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery\nachieves the first competitive sub-quadratic attention video diffusion models,\nreducing attention cost by up to 40\\% in terms of FLOPs, while maintaining\ngeneration quality as measured on the standard VBench and VBench-2.0\nbenchmarks.\n", "link": "http://arxiv.org/abs/2509.24899v1", "date": "2025-09-29", "relevancy": 2.4884, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6333}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6296}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Surgery%3A%20An%20Efficient%20Recipe%20to%20Linearize%20Your%20Video%20Diffusion%0A%20%20Transformer&body=Title%3A%20Attention%20Surgery%3A%20An%20Efficient%20Recipe%20to%20Linearize%20Your%20Video%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Mohsen%20Ghafoorian%20and%20Denis%20Korzhenkov%20and%20Amirhossein%20Habibian%0AAbstract%3A%20%20%20Transformer-based%20video%20diffusion%20models%20%28VDMs%29%20deliver%20state-of-the-art%0Avideo%20generation%20quality%20but%20are%20constrained%20by%20the%20quadratic%20cost%20of%0Aself-attention%2C%20making%20long%20sequences%20and%20high%20resolutions%20computationally%0Aexpensive.%20While%20linear%20attention%20offers%20sub-quadratic%20complexity%2C%20prior%0Aattempts%20fail%20to%20match%20the%20expressiveness%20of%20softmax%20attention%20without%20costly%0Aretraining.%20We%20introduce%20%5Ctextit%7BAttention%20Surgery%7D%2C%20an%20efficient%20framework%20for%0A%5Ctextit%7Blinearizing%7D%20or%20%5Ctextit%7Bhybridizing%7D%20attention%20in%20pretrained%20VDMs%0Awithout%20training%20from%20scratch.%20Inspired%20by%20recent%20advances%20in%20language%20models%2C%0Aour%20method%20combines%20a%20novel%20hybrid%20attention%20mechanism-mixing%20softmax%20and%0Alinear%20tokens-with%20a%20lightweight%20distillation%20and%20fine-tuning%20pipeline%0Arequiring%20only%20a%20few%20GPU-days.%20Additionally%2C%20we%20incorporate%20a%20cost-aware%0Ablock-rate%20strategy%20to%20balance%20expressiveness%20and%20efficiency%20across%20layers.%0AApplied%20to%20Wan2.1%201.3B%2C%20a%20state-of-the-art%20DiT-based%20VDM%2C%20Attention%20Surgery%0Aachieves%20the%20first%20competitive%20sub-quadratic%20attention%20video%20diffusion%20models%2C%0Areducing%20attention%20cost%20by%20up%20to%2040%5C%25%20in%20terms%20of%20FLOPs%2C%20while%20maintaining%0Ageneration%20quality%20as%20measured%20on%20the%20standard%20VBench%20and%20VBench-2.0%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Surgery%253A%2520An%2520Efficient%2520Recipe%2520to%2520Linearize%2520Your%2520Video%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DMohsen%2520Ghafoorian%2520and%2520Denis%2520Korzhenkov%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3D%2520%2520Transformer-based%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520deliver%2520state-of-the-art%250Avideo%2520generation%2520quality%2520but%2520are%2520constrained%2520by%2520the%2520quadratic%2520cost%2520of%250Aself-attention%252C%2520making%2520long%2520sequences%2520and%2520high%2520resolutions%2520computationally%250Aexpensive.%2520While%2520linear%2520attention%2520offers%2520sub-quadratic%2520complexity%252C%2520prior%250Aattempts%2520fail%2520to%2520match%2520the%2520expressiveness%2520of%2520softmax%2520attention%2520without%2520costly%250Aretraining.%2520We%2520introduce%2520%255Ctextit%257BAttention%2520Surgery%257D%252C%2520an%2520efficient%2520framework%2520for%250A%255Ctextit%257Blinearizing%257D%2520or%2520%255Ctextit%257Bhybridizing%257D%2520attention%2520in%2520pretrained%2520VDMs%250Awithout%2520training%2520from%2520scratch.%2520Inspired%2520by%2520recent%2520advances%2520in%2520language%2520models%252C%250Aour%2520method%2520combines%2520a%2520novel%2520hybrid%2520attention%2520mechanism-mixing%2520softmax%2520and%250Alinear%2520tokens-with%2520a%2520lightweight%2520distillation%2520and%2520fine-tuning%2520pipeline%250Arequiring%2520only%2520a%2520few%2520GPU-days.%2520Additionally%252C%2520we%2520incorporate%2520a%2520cost-aware%250Ablock-rate%2520strategy%2520to%2520balance%2520expressiveness%2520and%2520efficiency%2520across%2520layers.%250AApplied%2520to%2520Wan2.1%25201.3B%252C%2520a%2520state-of-the-art%2520DiT-based%2520VDM%252C%2520Attention%2520Surgery%250Aachieves%2520the%2520first%2520competitive%2520sub-quadratic%2520attention%2520video%2520diffusion%2520models%252C%250Areducing%2520attention%2520cost%2520by%2520up%2520to%252040%255C%2525%2520in%2520terms%2520of%2520FLOPs%252C%2520while%2520maintaining%250Ageneration%2520quality%2520as%2520measured%2520on%2520the%2520standard%2520VBench%2520and%2520VBench-2.0%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Surgery%3A%20An%20Efficient%20Recipe%20to%20Linearize%20Your%20Video%20Diffusion%0A%20%20Transformer&entry.906535625=Mohsen%20Ghafoorian%20and%20Denis%20Korzhenkov%20and%20Amirhossein%20Habibian&entry.1292438233=%20%20Transformer-based%20video%20diffusion%20models%20%28VDMs%29%20deliver%20state-of-the-art%0Avideo%20generation%20quality%20but%20are%20constrained%20by%20the%20quadratic%20cost%20of%0Aself-attention%2C%20making%20long%20sequences%20and%20high%20resolutions%20computationally%0Aexpensive.%20While%20linear%20attention%20offers%20sub-quadratic%20complexity%2C%20prior%0Aattempts%20fail%20to%20match%20the%20expressiveness%20of%20softmax%20attention%20without%20costly%0Aretraining.%20We%20introduce%20%5Ctextit%7BAttention%20Surgery%7D%2C%20an%20efficient%20framework%20for%0A%5Ctextit%7Blinearizing%7D%20or%20%5Ctextit%7Bhybridizing%7D%20attention%20in%20pretrained%20VDMs%0Awithout%20training%20from%20scratch.%20Inspired%20by%20recent%20advances%20in%20language%20models%2C%0Aour%20method%20combines%20a%20novel%20hybrid%20attention%20mechanism-mixing%20softmax%20and%0Alinear%20tokens-with%20a%20lightweight%20distillation%20and%20fine-tuning%20pipeline%0Arequiring%20only%20a%20few%20GPU-days.%20Additionally%2C%20we%20incorporate%20a%20cost-aware%0Ablock-rate%20strategy%20to%20balance%20expressiveness%20and%20efficiency%20across%20layers.%0AApplied%20to%20Wan2.1%201.3B%2C%20a%20state-of-the-art%20DiT-based%20VDM%2C%20Attention%20Surgery%0Aachieves%20the%20first%20competitive%20sub-quadratic%20attention%20video%20diffusion%20models%2C%0Areducing%20attention%20cost%20by%20up%20to%2040%5C%25%20in%20terms%20of%20FLOPs%2C%20while%20maintaining%0Ageneration%20quality%20as%20measured%20on%20the%20standard%20VBench%20and%20VBench-2.0%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24899v1&entry.124074799=Read"},
{"title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech", "author": "Chengyao Wang and Zhisheng Zhong and Bohao Peng and Senqiao Yang and Yuqi Liu and Haokun Gui and Bin Xia and Jingyao Li and Bei Yu and Jiaya Jia", "abstract": "  We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.\n", "link": "http://arxiv.org/abs/2509.25131v1", "date": "2025-09-29", "relevancy": 2.4661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGM-Omni%3A%20Scaling%20Omni%20LLMs%20to%20Personalized%20Long-Horizon%20Speech&body=Title%3A%20MGM-Omni%3A%20Scaling%20Omni%20LLMs%20to%20Personalized%20Long-Horizon%20Speech%0AAuthor%3A%20Chengyao%20Wang%20and%20Zhisheng%20Zhong%20and%20Bohao%20Peng%20and%20Senqiao%20Yang%20and%20Yuqi%20Liu%20and%20Haokun%20Gui%20and%20Bin%20Xia%20and%20Jingyao%20Li%20and%20Bei%20Yu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20We%20present%20MGM-Omni%2C%20a%20unified%20Omni%20LLM%20for%20omni-modal%20understanding%20and%0Aexpressive%2C%20long-horizon%20speech%20generation.%20Unlike%20cascaded%20pipelines%20that%0Aisolate%20speech%20synthesis%2C%20MGM-Omni%20adopts%20a%20%22brain-mouth%22%20design%20with%20a%0Adual-track%2C%20token-based%20architecture%20that%20cleanly%20decouples%20multimodal%0Areasoning%20from%20real-time%20speech%20generation.%20This%20design%20enables%20efficient%0Across-modal%20interaction%20and%20low-latency%2C%20streaming%20speech%20generation.%20For%0Aunderstanding%2C%20a%20unified%20training%20strategy%20coupled%20with%20a%20dual%20audio%20encoder%0Adesign%20enables%20long-form%20audio%20perception%20across%20diverse%20acoustic%20conditions.%0AFor%20generation%2C%20a%20chunk-based%20parallel%20decoding%20scheme%20narrows%20the%20text%20speech%0Atoken-rate%20gap%2C%20accelerating%20inference%20and%20supporting%20streaming%20zero-shot%20voice%0Acloning%20with%20stable%20timbre%20over%20extended%20durations.%20Compared%20to%20concurrent%0Awork%2C%20MGM-Omni%20achieves%20these%20capabilities%20with%20markedly%20data-efficient%0Atraining.%20Extensive%20experiments%20demonstrate%20that%20MGM-Omni%20outperforms%20existing%0Aopen%20source%20models%20in%20preserving%20timbre%20identity%20across%20extended%20sequences%2C%0Aproducing%20natural%20and%20context-aware%20speech%2C%20and%20achieving%20superior%20long-form%0Aaudio%20and%20omnimodal%20understanding.%20MGM-Omni%20establishes%20an%20efficient%2C%0Aend-to-end%20paradigm%20for%20omnimodal%20understanding%20and%20controllable%2C%20personalised%0Along-horizon%20speech%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGM-Omni%253A%2520Scaling%2520Omni%2520LLMs%2520to%2520Personalized%2520Long-Horizon%2520Speech%26entry.906535625%3DChengyao%2520Wang%2520and%2520Zhisheng%2520Zhong%2520and%2520Bohao%2520Peng%2520and%2520Senqiao%2520Yang%2520and%2520Yuqi%2520Liu%2520and%2520Haokun%2520Gui%2520and%2520Bin%2520Xia%2520and%2520Jingyao%2520Li%2520and%2520Bei%2520Yu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520We%2520present%2520MGM-Omni%252C%2520a%2520unified%2520Omni%2520LLM%2520for%2520omni-modal%2520understanding%2520and%250Aexpressive%252C%2520long-horizon%2520speech%2520generation.%2520Unlike%2520cascaded%2520pipelines%2520that%250Aisolate%2520speech%2520synthesis%252C%2520MGM-Omni%2520adopts%2520a%2520%2522brain-mouth%2522%2520design%2520with%2520a%250Adual-track%252C%2520token-based%2520architecture%2520that%2520cleanly%2520decouples%2520multimodal%250Areasoning%2520from%2520real-time%2520speech%2520generation.%2520This%2520design%2520enables%2520efficient%250Across-modal%2520interaction%2520and%2520low-latency%252C%2520streaming%2520speech%2520generation.%2520For%250Aunderstanding%252C%2520a%2520unified%2520training%2520strategy%2520coupled%2520with%2520a%2520dual%2520audio%2520encoder%250Adesign%2520enables%2520long-form%2520audio%2520perception%2520across%2520diverse%2520acoustic%2520conditions.%250AFor%2520generation%252C%2520a%2520chunk-based%2520parallel%2520decoding%2520scheme%2520narrows%2520the%2520text%2520speech%250Atoken-rate%2520gap%252C%2520accelerating%2520inference%2520and%2520supporting%2520streaming%2520zero-shot%2520voice%250Acloning%2520with%2520stable%2520timbre%2520over%2520extended%2520durations.%2520Compared%2520to%2520concurrent%250Awork%252C%2520MGM-Omni%2520achieves%2520these%2520capabilities%2520with%2520markedly%2520data-efficient%250Atraining.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MGM-Omni%2520outperforms%2520existing%250Aopen%2520source%2520models%2520in%2520preserving%2520timbre%2520identity%2520across%2520extended%2520sequences%252C%250Aproducing%2520natural%2520and%2520context-aware%2520speech%252C%2520and%2520achieving%2520superior%2520long-form%250Aaudio%2520and%2520omnimodal%2520understanding.%2520MGM-Omni%2520establishes%2520an%2520efficient%252C%250Aend-to-end%2520paradigm%2520for%2520omnimodal%2520understanding%2520and%2520controllable%252C%2520personalised%250Along-horizon%2520speech%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGM-Omni%3A%20Scaling%20Omni%20LLMs%20to%20Personalized%20Long-Horizon%20Speech&entry.906535625=Chengyao%20Wang%20and%20Zhisheng%20Zhong%20and%20Bohao%20Peng%20and%20Senqiao%20Yang%20and%20Yuqi%20Liu%20and%20Haokun%20Gui%20and%20Bin%20Xia%20and%20Jingyao%20Li%20and%20Bei%20Yu%20and%20Jiaya%20Jia&entry.1292438233=%20%20We%20present%20MGM-Omni%2C%20a%20unified%20Omni%20LLM%20for%20omni-modal%20understanding%20and%0Aexpressive%2C%20long-horizon%20speech%20generation.%20Unlike%20cascaded%20pipelines%20that%0Aisolate%20speech%20synthesis%2C%20MGM-Omni%20adopts%20a%20%22brain-mouth%22%20design%20with%20a%0Adual-track%2C%20token-based%20architecture%20that%20cleanly%20decouples%20multimodal%0Areasoning%20from%20real-time%20speech%20generation.%20This%20design%20enables%20efficient%0Across-modal%20interaction%20and%20low-latency%2C%20streaming%20speech%20generation.%20For%0Aunderstanding%2C%20a%20unified%20training%20strategy%20coupled%20with%20a%20dual%20audio%20encoder%0Adesign%20enables%20long-form%20audio%20perception%20across%20diverse%20acoustic%20conditions.%0AFor%20generation%2C%20a%20chunk-based%20parallel%20decoding%20scheme%20narrows%20the%20text%20speech%0Atoken-rate%20gap%2C%20accelerating%20inference%20and%20supporting%20streaming%20zero-shot%20voice%0Acloning%20with%20stable%20timbre%20over%20extended%20durations.%20Compared%20to%20concurrent%0Awork%2C%20MGM-Omni%20achieves%20these%20capabilities%20with%20markedly%20data-efficient%0Atraining.%20Extensive%20experiments%20demonstrate%20that%20MGM-Omni%20outperforms%20existing%0Aopen%20source%20models%20in%20preserving%20timbre%20identity%20across%20extended%20sequences%2C%0Aproducing%20natural%20and%20context-aware%20speech%2C%20and%20achieving%20superior%20long-form%0Aaudio%20and%20omnimodal%20understanding.%20MGM-Omni%20establishes%20an%20efficient%2C%0Aend-to-end%20paradigm%20for%20omnimodal%20understanding%20and%20controllable%2C%20personalised%0Along-horizon%20speech%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25131v1&entry.124074799=Read"},
{"title": "Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel", "author": "Haotian Dong and Wenjing Wang and Chen Li and Di Lin", "abstract": "  RGBA video generation, which includes an alpha channel to represent\ntransparency, is gaining increasing attention across a wide range of\napplications. However, existing methods often neglect visual quality, limiting\ntheir practical usability. In this paper, we propose \\textit{Wan-Alpha}, a new\nframework that generates transparent videos by learning both RGB and alpha\nchannels jointly. We design an effective variational autoencoder (VAE) that\nencodes the alpha channel into the RGB latent space. Then, to support the\ntraining of our diffusion transformer, we construct a high-quality and diverse\nRGBA video dataset. Compared with state-of-the-art methods, our model\ndemonstrates superior performance in visual quality, motion realism, and\ntransparency rendering. Notably, our model can generate a wide variety of\nsemi-transparent objects, glowing effects, and fine-grained details such as\nhair strands. The released model is available on our website:\n\\href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.\n", "link": "http://arxiv.org/abs/2509.24979v1", "date": "2025-09-29", "relevancy": 2.4335, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6181}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6138}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wan-Alpha%3A%20High-Quality%20Text-to-Video%20Generation%20with%20Alpha%20Channel&body=Title%3A%20Wan-Alpha%3A%20High-Quality%20Text-to-Video%20Generation%20with%20Alpha%20Channel%0AAuthor%3A%20Haotian%20Dong%20and%20Wenjing%20Wang%20and%20Chen%20Li%20and%20Di%20Lin%0AAbstract%3A%20%20%20RGBA%20video%20generation%2C%20which%20includes%20an%20alpha%20channel%20to%20represent%0Atransparency%2C%20is%20gaining%20increasing%20attention%20across%20a%20wide%20range%20of%0Aapplications.%20However%2C%20existing%20methods%20often%20neglect%20visual%20quality%2C%20limiting%0Atheir%20practical%20usability.%20In%20this%20paper%2C%20we%20propose%20%5Ctextit%7BWan-Alpha%7D%2C%20a%20new%0Aframework%20that%20generates%20transparent%20videos%20by%20learning%20both%20RGB%20and%20alpha%0Achannels%20jointly.%20We%20design%20an%20effective%20variational%20autoencoder%20%28VAE%29%20that%0Aencodes%20the%20alpha%20channel%20into%20the%20RGB%20latent%20space.%20Then%2C%20to%20support%20the%0Atraining%20of%20our%20diffusion%20transformer%2C%20we%20construct%20a%20high-quality%20and%20diverse%0ARGBA%20video%20dataset.%20Compared%20with%20state-of-the-art%20methods%2C%20our%20model%0Ademonstrates%20superior%20performance%20in%20visual%20quality%2C%20motion%20realism%2C%20and%0Atransparency%20rendering.%20Notably%2C%20our%20model%20can%20generate%20a%20wide%20variety%20of%0Asemi-transparent%20objects%2C%20glowing%20effects%2C%20and%20fine-grained%20details%20such%20as%0Ahair%20strands.%20The%20released%20model%20is%20available%20on%20our%20website%3A%0A%5Chref%7Bhttps%3A//donghaotian123.github.io/Wan-Alpha/%7D%7Bhttps%3A//donghaotian123.github.io/Wan-Alpha/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWan-Alpha%253A%2520High-Quality%2520Text-to-Video%2520Generation%2520with%2520Alpha%2520Channel%26entry.906535625%3DHaotian%2520Dong%2520and%2520Wenjing%2520Wang%2520and%2520Chen%2520Li%2520and%2520Di%2520Lin%26entry.1292438233%3D%2520%2520RGBA%2520video%2520generation%252C%2520which%2520includes%2520an%2520alpha%2520channel%2520to%2520represent%250Atransparency%252C%2520is%2520gaining%2520increasing%2520attention%2520across%2520a%2520wide%2520range%2520of%250Aapplications.%2520However%252C%2520existing%2520methods%2520often%2520neglect%2520visual%2520quality%252C%2520limiting%250Atheir%2520practical%2520usability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextit%257BWan-Alpha%257D%252C%2520a%2520new%250Aframework%2520that%2520generates%2520transparent%2520videos%2520by%2520learning%2520both%2520RGB%2520and%2520alpha%250Achannels%2520jointly.%2520We%2520design%2520an%2520effective%2520variational%2520autoencoder%2520%2528VAE%2529%2520that%250Aencodes%2520the%2520alpha%2520channel%2520into%2520the%2520RGB%2520latent%2520space.%2520Then%252C%2520to%2520support%2520the%250Atraining%2520of%2520our%2520diffusion%2520transformer%252C%2520we%2520construct%2520a%2520high-quality%2520and%2520diverse%250ARGBA%2520video%2520dataset.%2520Compared%2520with%2520state-of-the-art%2520methods%252C%2520our%2520model%250Ademonstrates%2520superior%2520performance%2520in%2520visual%2520quality%252C%2520motion%2520realism%252C%2520and%250Atransparency%2520rendering.%2520Notably%252C%2520our%2520model%2520can%2520generate%2520a%2520wide%2520variety%2520of%250Asemi-transparent%2520objects%252C%2520glowing%2520effects%252C%2520and%2520fine-grained%2520details%2520such%2520as%250Ahair%2520strands.%2520The%2520released%2520model%2520is%2520available%2520on%2520our%2520website%253A%250A%255Chref%257Bhttps%253A//donghaotian123.github.io/Wan-Alpha/%257D%257Bhttps%253A//donghaotian123.github.io/Wan-Alpha/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wan-Alpha%3A%20High-Quality%20Text-to-Video%20Generation%20with%20Alpha%20Channel&entry.906535625=Haotian%20Dong%20and%20Wenjing%20Wang%20and%20Chen%20Li%20and%20Di%20Lin&entry.1292438233=%20%20RGBA%20video%20generation%2C%20which%20includes%20an%20alpha%20channel%20to%20represent%0Atransparency%2C%20is%20gaining%20increasing%20attention%20across%20a%20wide%20range%20of%0Aapplications.%20However%2C%20existing%20methods%20often%20neglect%20visual%20quality%2C%20limiting%0Atheir%20practical%20usability.%20In%20this%20paper%2C%20we%20propose%20%5Ctextit%7BWan-Alpha%7D%2C%20a%20new%0Aframework%20that%20generates%20transparent%20videos%20by%20learning%20both%20RGB%20and%20alpha%0Achannels%20jointly.%20We%20design%20an%20effective%20variational%20autoencoder%20%28VAE%29%20that%0Aencodes%20the%20alpha%20channel%20into%20the%20RGB%20latent%20space.%20Then%2C%20to%20support%20the%0Atraining%20of%20our%20diffusion%20transformer%2C%20we%20construct%20a%20high-quality%20and%20diverse%0ARGBA%20video%20dataset.%20Compared%20with%20state-of-the-art%20methods%2C%20our%20model%0Ademonstrates%20superior%20performance%20in%20visual%20quality%2C%20motion%20realism%2C%20and%0Atransparency%20rendering.%20Notably%2C%20our%20model%20can%20generate%20a%20wide%20variety%20of%0Asemi-transparent%20objects%2C%20glowing%20effects%2C%20and%20fine-grained%20details%20such%20as%0Ahair%20strands.%20The%20released%20model%20is%20available%20on%20our%20website%3A%0A%5Chref%7Bhttps%3A//donghaotian123.github.io/Wan-Alpha/%7D%7Bhttps%3A//donghaotian123.github.io/Wan-Alpha/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24979v1&entry.124074799=Read"},
{"title": "The Emergence of Social Science of Large Language Models", "author": "Xiao Jia and Zhanzhan Zhao", "abstract": "  The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence.\n", "link": "http://arxiv.org/abs/2509.24877v1", "date": "2025-09-29", "relevancy": 2.424, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Emergence%20of%20Social%20Science%20of%20Large%20Language%20Models&body=Title%3A%20The%20Emergence%20of%20Social%20Science%20of%20Large%20Language%20Models%0AAuthor%3A%20Xiao%20Jia%20and%20Zhanzhan%20Zhao%0AAbstract%3A%20%20%20The%20social%20science%20of%20large%20language%20models%20%28LLMs%29%20examines%20how%20these%20systems%0Aevoke%20mind%20attributions%2C%20interact%20with%20one%20another%2C%20and%20transform%20human%0Aactivity%20and%20institutions.%20We%20conducted%20a%20systematic%20review%20of%20270%20studies%2C%0Acombining%20text%20embeddings%2C%20unsupervised%20clustering%20and%20topic%20modeling%20to%20build%0Aa%20computational%20taxonomy.%20Three%20domains%20emerge%20organically%20across%20the%20reviewed%0Aliterature.%20LLM%20as%20Social%20Minds%20examines%20whether%20and%20when%20models%20display%0Abehaviors%20that%20elicit%20attributions%20of%20cognition%2C%20morality%20and%20bias%2C%20while%0Aaddressing%20challenges%20such%20as%20test%20leakage%20and%20surface%20cues.%20LLM%20Societies%0Aexamines%20multi-agent%20settings%20where%20interaction%20protocols%2C%20architectures%20and%0Amechanism%20design%20shape%20coordination%2C%20norms%2C%20institutions%20and%20collective%0Aepistemic%20processes.%20LLM-Human%20Interactions%20examines%20how%20LLMs%20reshape%20tasks%2C%0Alearning%2C%20trust%2C%20work%20and%20governance%2C%20and%20how%20risks%20arise%20at%20the%20human-AI%0Ainterface.%20This%20taxonomy%20provides%20a%20reproducible%20map%20of%20a%20fragmented%20field%2C%0Aclarifies%20evidentiary%20standards%20across%20levels%20of%20analysis%2C%20and%20highlights%0Aopportunities%20for%20cumulative%20progress%20in%20the%20social%20science%20of%20artificial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Emergence%2520of%2520Social%2520Science%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DXiao%2520Jia%2520and%2520Zhanzhan%2520Zhao%26entry.1292438233%3D%2520%2520The%2520social%2520science%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520examines%2520how%2520these%2520systems%250Aevoke%2520mind%2520attributions%252C%2520interact%2520with%2520one%2520another%252C%2520and%2520transform%2520human%250Aactivity%2520and%2520institutions.%2520We%2520conducted%2520a%2520systematic%2520review%2520of%2520270%2520studies%252C%250Acombining%2520text%2520embeddings%252C%2520unsupervised%2520clustering%2520and%2520topic%2520modeling%2520to%2520build%250Aa%2520computational%2520taxonomy.%2520Three%2520domains%2520emerge%2520organically%2520across%2520the%2520reviewed%250Aliterature.%2520LLM%2520as%2520Social%2520Minds%2520examines%2520whether%2520and%2520when%2520models%2520display%250Abehaviors%2520that%2520elicit%2520attributions%2520of%2520cognition%252C%2520morality%2520and%2520bias%252C%2520while%250Aaddressing%2520challenges%2520such%2520as%2520test%2520leakage%2520and%2520surface%2520cues.%2520LLM%2520Societies%250Aexamines%2520multi-agent%2520settings%2520where%2520interaction%2520protocols%252C%2520architectures%2520and%250Amechanism%2520design%2520shape%2520coordination%252C%2520norms%252C%2520institutions%2520and%2520collective%250Aepistemic%2520processes.%2520LLM-Human%2520Interactions%2520examines%2520how%2520LLMs%2520reshape%2520tasks%252C%250Alearning%252C%2520trust%252C%2520work%2520and%2520governance%252C%2520and%2520how%2520risks%2520arise%2520at%2520the%2520human-AI%250Ainterface.%2520This%2520taxonomy%2520provides%2520a%2520reproducible%2520map%2520of%2520a%2520fragmented%2520field%252C%250Aclarifies%2520evidentiary%2520standards%2520across%2520levels%2520of%2520analysis%252C%2520and%2520highlights%250Aopportunities%2520for%2520cumulative%2520progress%2520in%2520the%2520social%2520science%2520of%2520artificial%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Emergence%20of%20Social%20Science%20of%20Large%20Language%20Models&entry.906535625=Xiao%20Jia%20and%20Zhanzhan%20Zhao&entry.1292438233=%20%20The%20social%20science%20of%20large%20language%20models%20%28LLMs%29%20examines%20how%20these%20systems%0Aevoke%20mind%20attributions%2C%20interact%20with%20one%20another%2C%20and%20transform%20human%0Aactivity%20and%20institutions.%20We%20conducted%20a%20systematic%20review%20of%20270%20studies%2C%0Acombining%20text%20embeddings%2C%20unsupervised%20clustering%20and%20topic%20modeling%20to%20build%0Aa%20computational%20taxonomy.%20Three%20domains%20emerge%20organically%20across%20the%20reviewed%0Aliterature.%20LLM%20as%20Social%20Minds%20examines%20whether%20and%20when%20models%20display%0Abehaviors%20that%20elicit%20attributions%20of%20cognition%2C%20morality%20and%20bias%2C%20while%0Aaddressing%20challenges%20such%20as%20test%20leakage%20and%20surface%20cues.%20LLM%20Societies%0Aexamines%20multi-agent%20settings%20where%20interaction%20protocols%2C%20architectures%20and%0Amechanism%20design%20shape%20coordination%2C%20norms%2C%20institutions%20and%20collective%0Aepistemic%20processes.%20LLM-Human%20Interactions%20examines%20how%20LLMs%20reshape%20tasks%2C%0Alearning%2C%20trust%2C%20work%20and%20governance%2C%20and%20how%20risks%20arise%20at%20the%20human-AI%0Ainterface.%20This%20taxonomy%20provides%20a%20reproducible%20map%20of%20a%20fragmented%20field%2C%0Aclarifies%20evidentiary%20standards%20across%20levels%20of%20analysis%2C%20and%20highlights%0Aopportunities%20for%20cumulative%20progress%20in%20the%20social%20science%20of%20artificial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24877v1&entry.124074799=Read"},
{"title": "Vision-and-Language Navigation with Analogical Textual Descriptions in\n  LLMs", "author": "Yue Zhang and Tianyi Ma and Zun Wang and Yanyuan Qiao and Parisa Kordjamshidi", "abstract": "  Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.\n", "link": "http://arxiv.org/abs/2509.25139v1", "date": "2025-09-29", "relevancy": 2.4226, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.615}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-and-Language%20Navigation%20with%20Analogical%20Textual%20Descriptions%20in%0A%20%20LLMs&body=Title%3A%20Vision-and-Language%20Navigation%20with%20Analogical%20Textual%20Descriptions%20in%0A%20%20LLMs%0AAuthor%3A%20Yue%20Zhang%20and%20Tianyi%20Ma%20and%20Zun%20Wang%20and%20Yanyuan%20Qiao%20and%20Parisa%20Kordjamshidi%0AAbstract%3A%20%20%20Integrating%20large%20language%20models%20%28LLMs%29%20into%20embodied%20AI%20models%20is%20becoming%0Aincreasingly%20prevalent.%20However%2C%20existing%20zero-shot%20LLM-based%0AVision-and-Language%20Navigation%20%28VLN%29%20agents%20either%20encode%20images%20as%20textual%0Ascene%20descriptions%2C%20potentially%20oversimplifying%20visual%20details%2C%20or%20process%20raw%0Aimage%20inputs%2C%20which%20can%20fail%20to%20capture%20abstract%20semantics%20required%20for%0Ahigh-level%20reasoning.%20In%20this%20paper%2C%20we%20improve%20the%20navigation%20agent%27s%0Acontextual%20understanding%20by%20incorporating%20textual%20descriptions%20from%20multiple%0Aperspectives%20that%20facilitate%20analogical%20reasoning%20across%20images.%20By%20leveraging%0Atext-based%20analogical%20reasoning%2C%20the%20agent%20enhances%20its%20global%20scene%0Aunderstanding%20and%20spatial%20reasoning%2C%20leading%20to%20more%20accurate%20action%20decisions.%0AWe%20evaluate%20our%20approach%20on%20the%20R2R%20dataset%2C%20where%20our%20experiments%20demonstrate%0Asignificant%20improvements%20in%20navigation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-and-Language%2520Navigation%2520with%2520Analogical%2520Textual%2520Descriptions%2520in%250A%2520%2520LLMs%26entry.906535625%3DYue%2520Zhang%2520and%2520Tianyi%2520Ma%2520and%2520Zun%2520Wang%2520and%2520Yanyuan%2520Qiao%2520and%2520Parisa%2520Kordjamshidi%26entry.1292438233%3D%2520%2520Integrating%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520embodied%2520AI%2520models%2520is%2520becoming%250Aincreasingly%2520prevalent.%2520However%252C%2520existing%2520zero-shot%2520LLM-based%250AVision-and-Language%2520Navigation%2520%2528VLN%2529%2520agents%2520either%2520encode%2520images%2520as%2520textual%250Ascene%2520descriptions%252C%2520potentially%2520oversimplifying%2520visual%2520details%252C%2520or%2520process%2520raw%250Aimage%2520inputs%252C%2520which%2520can%2520fail%2520to%2520capture%2520abstract%2520semantics%2520required%2520for%250Ahigh-level%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520improve%2520the%2520navigation%2520agent%2527s%250Acontextual%2520understanding%2520by%2520incorporating%2520textual%2520descriptions%2520from%2520multiple%250Aperspectives%2520that%2520facilitate%2520analogical%2520reasoning%2520across%2520images.%2520By%2520leveraging%250Atext-based%2520analogical%2520reasoning%252C%2520the%2520agent%2520enhances%2520its%2520global%2520scene%250Aunderstanding%2520and%2520spatial%2520reasoning%252C%2520leading%2520to%2520more%2520accurate%2520action%2520decisions.%250AWe%2520evaluate%2520our%2520approach%2520on%2520the%2520R2R%2520dataset%252C%2520where%2520our%2520experiments%2520demonstrate%250Asignificant%2520improvements%2520in%2520navigation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-and-Language%20Navigation%20with%20Analogical%20Textual%20Descriptions%20in%0A%20%20LLMs&entry.906535625=Yue%20Zhang%20and%20Tianyi%20Ma%20and%20Zun%20Wang%20and%20Yanyuan%20Qiao%20and%20Parisa%20Kordjamshidi&entry.1292438233=%20%20Integrating%20large%20language%20models%20%28LLMs%29%20into%20embodied%20AI%20models%20is%20becoming%0Aincreasingly%20prevalent.%20However%2C%20existing%20zero-shot%20LLM-based%0AVision-and-Language%20Navigation%20%28VLN%29%20agents%20either%20encode%20images%20as%20textual%0Ascene%20descriptions%2C%20potentially%20oversimplifying%20visual%20details%2C%20or%20process%20raw%0Aimage%20inputs%2C%20which%20can%20fail%20to%20capture%20abstract%20semantics%20required%20for%0Ahigh-level%20reasoning.%20In%20this%20paper%2C%20we%20improve%20the%20navigation%20agent%27s%0Acontextual%20understanding%20by%20incorporating%20textual%20descriptions%20from%20multiple%0Aperspectives%20that%20facilitate%20analogical%20reasoning%20across%20images.%20By%20leveraging%0Atext-based%20analogical%20reasoning%2C%20the%20agent%20enhances%20its%20global%20scene%0Aunderstanding%20and%20spatial%20reasoning%2C%20leading%20to%20more%20accurate%20action%20decisions.%0AWe%20evaluate%20our%20approach%20on%20the%20R2R%20dataset%2C%20where%20our%20experiments%20demonstrate%0Asignificant%20improvements%20in%20navigation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25139v1&entry.124074799=Read"},
{"title": "Efficient Sketching and Nearest Neighbor Search Algorithms for Sparse\n  Vector Sets", "author": "Sebastian Bruch and Franco Maria Nardini and Cosimo Rulli and Rossano Venturini", "abstract": "  Sparse embeddings of data form an attractive class due to their inherent\ninterpretability: Every dimension is tied to a term in some vocabulary, making\nit easy to visually decipher the latent space. Sparsity, however, poses unique\nchallenges for Approximate Nearest Neighbor Search (ANNS) which finds, from a\ncollection of vectors, the k vectors closest to a query. To encourage research\non this underexplored topic, sparse ANNS featured prominently in a BigANN\nChallenge at NeurIPS 2023, where approximate algorithms were evaluated on large\nbenchmark datasets by throughput and accuracy. In this work, we introduce a set\nof novel data structures and algorithmic methods, a combination of which leads\nto an elegant, effective, and highly efficient solution to sparse ANNS. Our\ncontributions range from a theoretically-grounded sketching algorithm for\nsparse vectors to reduce their effective dimensionality while preserving inner\nproduct-induced ranks; a geometric organization of the inverted index; and the\nblending of local and global information to improve the efficiency and efficacy\nof ANNS. Empirically, our final algorithm, dubbed Seismic, reaches\nsub-millisecond per-query latency with high accuracy on a large-scale benchmark\ndataset using a single CPU.\n", "link": "http://arxiv.org/abs/2509.24815v1", "date": "2025-09-29", "relevancy": 2.4177, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.493}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Sketching%20and%20Nearest%20Neighbor%20Search%20Algorithms%20for%20Sparse%0A%20%20Vector%20Sets&body=Title%3A%20Efficient%20Sketching%20and%20Nearest%20Neighbor%20Search%20Algorithms%20for%20Sparse%0A%20%20Vector%20Sets%0AAuthor%3A%20Sebastian%20Bruch%20and%20Franco%20Maria%20Nardini%20and%20Cosimo%20Rulli%20and%20Rossano%20Venturini%0AAbstract%3A%20%20%20Sparse%20embeddings%20of%20data%20form%20an%20attractive%20class%20due%20to%20their%20inherent%0Ainterpretability%3A%20Every%20dimension%20is%20tied%20to%20a%20term%20in%20some%20vocabulary%2C%20making%0Ait%20easy%20to%20visually%20decipher%20the%20latent%20space.%20Sparsity%2C%20however%2C%20poses%20unique%0Achallenges%20for%20Approximate%20Nearest%20Neighbor%20Search%20%28ANNS%29%20which%20finds%2C%20from%20a%0Acollection%20of%20vectors%2C%20the%20k%20vectors%20closest%20to%20a%20query.%20To%20encourage%20research%0Aon%20this%20underexplored%20topic%2C%20sparse%20ANNS%20featured%20prominently%20in%20a%20BigANN%0AChallenge%20at%20NeurIPS%202023%2C%20where%20approximate%20algorithms%20were%20evaluated%20on%20large%0Abenchmark%20datasets%20by%20throughput%20and%20accuracy.%20In%20this%20work%2C%20we%20introduce%20a%20set%0Aof%20novel%20data%20structures%20and%20algorithmic%20methods%2C%20a%20combination%20of%20which%20leads%0Ato%20an%20elegant%2C%20effective%2C%20and%20highly%20efficient%20solution%20to%20sparse%20ANNS.%20Our%0Acontributions%20range%20from%20a%20theoretically-grounded%20sketching%20algorithm%20for%0Asparse%20vectors%20to%20reduce%20their%20effective%20dimensionality%20while%20preserving%20inner%0Aproduct-induced%20ranks%3B%20a%20geometric%20organization%20of%20the%20inverted%20index%3B%20and%20the%0Ablending%20of%20local%20and%20global%20information%20to%20improve%20the%20efficiency%20and%20efficacy%0Aof%20ANNS.%20Empirically%2C%20our%20final%20algorithm%2C%20dubbed%20Seismic%2C%20reaches%0Asub-millisecond%20per-query%20latency%20with%20high%20accuracy%20on%20a%20large-scale%20benchmark%0Adataset%20using%20a%20single%20CPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Sketching%2520and%2520Nearest%2520Neighbor%2520Search%2520Algorithms%2520for%2520Sparse%250A%2520%2520Vector%2520Sets%26entry.906535625%3DSebastian%2520Bruch%2520and%2520Franco%2520Maria%2520Nardini%2520and%2520Cosimo%2520Rulli%2520and%2520Rossano%2520Venturini%26entry.1292438233%3D%2520%2520Sparse%2520embeddings%2520of%2520data%2520form%2520an%2520attractive%2520class%2520due%2520to%2520their%2520inherent%250Ainterpretability%253A%2520Every%2520dimension%2520is%2520tied%2520to%2520a%2520term%2520in%2520some%2520vocabulary%252C%2520making%250Ait%2520easy%2520to%2520visually%2520decipher%2520the%2520latent%2520space.%2520Sparsity%252C%2520however%252C%2520poses%2520unique%250Achallenges%2520for%2520Approximate%2520Nearest%2520Neighbor%2520Search%2520%2528ANNS%2529%2520which%2520finds%252C%2520from%2520a%250Acollection%2520of%2520vectors%252C%2520the%2520k%2520vectors%2520closest%2520to%2520a%2520query.%2520To%2520encourage%2520research%250Aon%2520this%2520underexplored%2520topic%252C%2520sparse%2520ANNS%2520featured%2520prominently%2520in%2520a%2520BigANN%250AChallenge%2520at%2520NeurIPS%25202023%252C%2520where%2520approximate%2520algorithms%2520were%2520evaluated%2520on%2520large%250Abenchmark%2520datasets%2520by%2520throughput%2520and%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520set%250Aof%2520novel%2520data%2520structures%2520and%2520algorithmic%2520methods%252C%2520a%2520combination%2520of%2520which%2520leads%250Ato%2520an%2520elegant%252C%2520effective%252C%2520and%2520highly%2520efficient%2520solution%2520to%2520sparse%2520ANNS.%2520Our%250Acontributions%2520range%2520from%2520a%2520theoretically-grounded%2520sketching%2520algorithm%2520for%250Asparse%2520vectors%2520to%2520reduce%2520their%2520effective%2520dimensionality%2520while%2520preserving%2520inner%250Aproduct-induced%2520ranks%253B%2520a%2520geometric%2520organization%2520of%2520the%2520inverted%2520index%253B%2520and%2520the%250Ablending%2520of%2520local%2520and%2520global%2520information%2520to%2520improve%2520the%2520efficiency%2520and%2520efficacy%250Aof%2520ANNS.%2520Empirically%252C%2520our%2520final%2520algorithm%252C%2520dubbed%2520Seismic%252C%2520reaches%250Asub-millisecond%2520per-query%2520latency%2520with%2520high%2520accuracy%2520on%2520a%2520large-scale%2520benchmark%250Adataset%2520using%2520a%2520single%2520CPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Sketching%20and%20Nearest%20Neighbor%20Search%20Algorithms%20for%20Sparse%0A%20%20Vector%20Sets&entry.906535625=Sebastian%20Bruch%20and%20Franco%20Maria%20Nardini%20and%20Cosimo%20Rulli%20and%20Rossano%20Venturini&entry.1292438233=%20%20Sparse%20embeddings%20of%20data%20form%20an%20attractive%20class%20due%20to%20their%20inherent%0Ainterpretability%3A%20Every%20dimension%20is%20tied%20to%20a%20term%20in%20some%20vocabulary%2C%20making%0Ait%20easy%20to%20visually%20decipher%20the%20latent%20space.%20Sparsity%2C%20however%2C%20poses%20unique%0Achallenges%20for%20Approximate%20Nearest%20Neighbor%20Search%20%28ANNS%29%20which%20finds%2C%20from%20a%0Acollection%20of%20vectors%2C%20the%20k%20vectors%20closest%20to%20a%20query.%20To%20encourage%20research%0Aon%20this%20underexplored%20topic%2C%20sparse%20ANNS%20featured%20prominently%20in%20a%20BigANN%0AChallenge%20at%20NeurIPS%202023%2C%20where%20approximate%20algorithms%20were%20evaluated%20on%20large%0Abenchmark%20datasets%20by%20throughput%20and%20accuracy.%20In%20this%20work%2C%20we%20introduce%20a%20set%0Aof%20novel%20data%20structures%20and%20algorithmic%20methods%2C%20a%20combination%20of%20which%20leads%0Ato%20an%20elegant%2C%20effective%2C%20and%20highly%20efficient%20solution%20to%20sparse%20ANNS.%20Our%0Acontributions%20range%20from%20a%20theoretically-grounded%20sketching%20algorithm%20for%0Asparse%20vectors%20to%20reduce%20their%20effective%20dimensionality%20while%20preserving%20inner%0Aproduct-induced%20ranks%3B%20a%20geometric%20organization%20of%20the%20inverted%20index%3B%20and%20the%0Ablending%20of%20local%20and%20global%20information%20to%20improve%20the%20efficiency%20and%20efficacy%0Aof%20ANNS.%20Empirically%2C%20our%20final%20algorithm%2C%20dubbed%20Seismic%2C%20reaches%0Asub-millisecond%20per-query%20latency%20with%20high%20accuracy%20on%20a%20large-scale%20benchmark%0Adataset%20using%20a%20single%20CPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24815v1&entry.124074799=Read"},
{"title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction\n  Prediction", "author": "Yuehua Song and Yong Gao", "abstract": "  Accurately predicting drug-target interactions (DTIs) is pivotal for\nadvancing drug discovery and target validation techniques. While machine\nlearning approaches including those that are based on Graph Neural Networks\n(GNN) have achieved notable success in DTI prediction, many of them have\ndifficulties in effectively integrating the diverse features of drugs, targets\nand their interactions. To address this limitation, we introduce a novel\nframework to take advantage of the power of both transductive learning and\ninductive learning so that features at molecular level and drug-target\ninteraction network level can be exploited. Within this framework is a\nGNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and\ntarget molecular structures as meta-nodes in a drug-target interaction graph,\nenabling a detailed exploration of their intricate relationships. To evaluate\nthe proposed model, we have compiled a special benchmark comprising drug\nSMILES, protein sequences, and their interaction data, which is interesting in\nits own right. Our experimental results demonstrate that the GiG model\nsignificantly outperforms existing approaches across all evaluation metrics,\nhighlighting the benefits of integrating different learning paradigms and\ninteraction data.\n", "link": "http://arxiv.org/abs/2507.11757v2", "date": "2025-09-29", "relevancy": 2.4171, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4935}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Graph-in-Graph%20Learning%20Framework%20for%20Drug-Target%20Interaction%0A%20%20Prediction&body=Title%3A%20A%20Graph-in-Graph%20Learning%20Framework%20for%20Drug-Target%20Interaction%0A%20%20Prediction%0AAuthor%3A%20Yuehua%20Song%20and%20Yong%20Gao%0AAbstract%3A%20%20%20Accurately%20predicting%20drug-target%20interactions%20%28DTIs%29%20is%20pivotal%20for%0Aadvancing%20drug%20discovery%20and%20target%20validation%20techniques.%20While%20machine%0Alearning%20approaches%20including%20those%20that%20are%20based%20on%20Graph%20Neural%20Networks%0A%28GNN%29%20have%20achieved%20notable%20success%20in%20DTI%20prediction%2C%20many%20of%20them%20have%0Adifficulties%20in%20effectively%20integrating%20the%20diverse%20features%20of%20drugs%2C%20targets%0Aand%20their%20interactions.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20novel%0Aframework%20to%20take%20advantage%20of%20the%20power%20of%20both%20transductive%20learning%20and%0Ainductive%20learning%20so%20that%20features%20at%20molecular%20level%20and%20drug-target%0Ainteraction%20network%20level%20can%20be%20exploited.%20Within%20this%20framework%20is%20a%0AGNN-based%20model%20called%20Graph-in-Graph%20%28GiG%29%20that%20represents%20graphs%20of%20drug%20and%0Atarget%20molecular%20structures%20as%20meta-nodes%20in%20a%20drug-target%20interaction%20graph%2C%0Aenabling%20a%20detailed%20exploration%20of%20their%20intricate%20relationships.%20To%20evaluate%0Athe%20proposed%20model%2C%20we%20have%20compiled%20a%20special%20benchmark%20comprising%20drug%0ASMILES%2C%20protein%20sequences%2C%20and%20their%20interaction%20data%2C%20which%20is%20interesting%20in%0Aits%20own%20right.%20Our%20experimental%20results%20demonstrate%20that%20the%20GiG%20model%0Asignificantly%20outperforms%20existing%20approaches%20across%20all%20evaluation%20metrics%2C%0Ahighlighting%20the%20benefits%20of%20integrating%20different%20learning%20paradigms%20and%0Ainteraction%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11757v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Graph-in-Graph%2520Learning%2520Framework%2520for%2520Drug-Target%2520Interaction%250A%2520%2520Prediction%26entry.906535625%3DYuehua%2520Song%2520and%2520Yong%2520Gao%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520drug-target%2520interactions%2520%2528DTIs%2529%2520is%2520pivotal%2520for%250Aadvancing%2520drug%2520discovery%2520and%2520target%2520validation%2520techniques.%2520While%2520machine%250Alearning%2520approaches%2520including%2520those%2520that%2520are%2520based%2520on%2520Graph%2520Neural%2520Networks%250A%2528GNN%2529%2520have%2520achieved%2520notable%2520success%2520in%2520DTI%2520prediction%252C%2520many%2520of%2520them%2520have%250Adifficulties%2520in%2520effectively%2520integrating%2520the%2520diverse%2520features%2520of%2520drugs%252C%2520targets%250Aand%2520their%2520interactions.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520novel%250Aframework%2520to%2520take%2520advantage%2520of%2520the%2520power%2520of%2520both%2520transductive%2520learning%2520and%250Ainductive%2520learning%2520so%2520that%2520features%2520at%2520molecular%2520level%2520and%2520drug-target%250Ainteraction%2520network%2520level%2520can%2520be%2520exploited.%2520Within%2520this%2520framework%2520is%2520a%250AGNN-based%2520model%2520called%2520Graph-in-Graph%2520%2528GiG%2529%2520that%2520represents%2520graphs%2520of%2520drug%2520and%250Atarget%2520molecular%2520structures%2520as%2520meta-nodes%2520in%2520a%2520drug-target%2520interaction%2520graph%252C%250Aenabling%2520a%2520detailed%2520exploration%2520of%2520their%2520intricate%2520relationships.%2520To%2520evaluate%250Athe%2520proposed%2520model%252C%2520we%2520have%2520compiled%2520a%2520special%2520benchmark%2520comprising%2520drug%250ASMILES%252C%2520protein%2520sequences%252C%2520and%2520their%2520interaction%2520data%252C%2520which%2520is%2520interesting%2520in%250Aits%2520own%2520right.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520the%2520GiG%2520model%250Asignificantly%2520outperforms%2520existing%2520approaches%2520across%2520all%2520evaluation%2520metrics%252C%250Ahighlighting%2520the%2520benefits%2520of%2520integrating%2520different%2520learning%2520paradigms%2520and%250Ainteraction%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11757v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Graph-in-Graph%20Learning%20Framework%20for%20Drug-Target%20Interaction%0A%20%20Prediction&entry.906535625=Yuehua%20Song%20and%20Yong%20Gao&entry.1292438233=%20%20Accurately%20predicting%20drug-target%20interactions%20%28DTIs%29%20is%20pivotal%20for%0Aadvancing%20drug%20discovery%20and%20target%20validation%20techniques.%20While%20machine%0Alearning%20approaches%20including%20those%20that%20are%20based%20on%20Graph%20Neural%20Networks%0A%28GNN%29%20have%20achieved%20notable%20success%20in%20DTI%20prediction%2C%20many%20of%20them%20have%0Adifficulties%20in%20effectively%20integrating%20the%20diverse%20features%20of%20drugs%2C%20targets%0Aand%20their%20interactions.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20novel%0Aframework%20to%20take%20advantage%20of%20the%20power%20of%20both%20transductive%20learning%20and%0Ainductive%20learning%20so%20that%20features%20at%20molecular%20level%20and%20drug-target%0Ainteraction%20network%20level%20can%20be%20exploited.%20Within%20this%20framework%20is%20a%0AGNN-based%20model%20called%20Graph-in-Graph%20%28GiG%29%20that%20represents%20graphs%20of%20drug%20and%0Atarget%20molecular%20structures%20as%20meta-nodes%20in%20a%20drug-target%20interaction%20graph%2C%0Aenabling%20a%20detailed%20exploration%20of%20their%20intricate%20relationships.%20To%20evaluate%0Athe%20proposed%20model%2C%20we%20have%20compiled%20a%20special%20benchmark%20comprising%20drug%0ASMILES%2C%20protein%20sequences%2C%20and%20their%20interaction%20data%2C%20which%20is%20interesting%20in%0Aits%20own%20right.%20Our%20experimental%20results%20demonstrate%20that%20the%20GiG%20model%0Asignificantly%20outperforms%20existing%20approaches%20across%20all%20evaluation%20metrics%2C%0Ahighlighting%20the%20benefits%20of%20integrating%20different%20learning%20paradigms%20and%0Ainteraction%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11757v2&entry.124074799=Read"},
{"title": "Learning in an Echo Chamber: Online Learning with Replay Adversary", "author": "Daniil Dmitriev and Harald Eskelund Franck and Carolin Heinzler and Amartya Sanyal", "abstract": "  As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms.\n", "link": "http://arxiv.org/abs/2509.25135v1", "date": "2025-09-29", "relevancy": 2.3825, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.483}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4733}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20in%20an%20Echo%20Chamber%3A%20Online%20Learning%20with%20Replay%20Adversary&body=Title%3A%20Learning%20in%20an%20Echo%20Chamber%3A%20Online%20Learning%20with%20Replay%20Adversary%0AAuthor%3A%20Daniil%20Dmitriev%20and%20Harald%20Eskelund%20Franck%20and%20Carolin%20Heinzler%20and%20Amartya%20Sanyal%0AAbstract%3A%20%20%20As%20machine%20learning%20systems%20increasingly%20train%20on%20self-annotated%20data%2C%20they%0Arisk%20reinforcing%20errors%20and%20becoming%20echo%20chambers%20of%20their%20own%20beliefs.%20We%0Amodel%20this%20phenomenon%20by%20introducing%20a%20learning-theoretic%20framework%3A%20Online%0ALearning%20in%20the%20Replay%20Setting.%20In%20round%20%24t%24%2C%20the%20learner%20outputs%20a%20hypothesis%0A%24%5Chat%7Bh%7D_t%24%3B%20the%20adversary%20then%20reveals%20either%20the%20true%20label%20%24f%5E%5Cast%28x_t%29%24%20or%0Aa%20replayed%20label%20%24%5Chat%7Bh%7D_i%28x_t%29%24%20from%20an%20earlier%20round%20%24i%20%3C%20t%24.%20A%20mistake%20is%0Acounted%20only%20when%20the%20true%20label%20is%20shown%2C%20yet%20classical%20algorithms%20such%20as%20the%0ASOA%20or%20the%20halving%20algorithm%20are%20easily%20misled%20by%20the%20replayed%20errors.%0A%20%20We%20introduce%20the%20Extended%20Threshold%20dimension%2C%20%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%2C%0Aand%20prove%20matching%20upper%20and%20lower%20bounds%20that%20make%0A%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%20the%20exact%20measure%20of%20learnability%20in%20this%20model.%0AA%20closure-based%20learner%20makes%20at%20most%20%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%20mistakes%0Aagainst%20any%20adaptive%20adversary%2C%20and%20no%20algorithm%20can%20perform%20better.%20For%0Astochastic%20adversaries%2C%20we%20prove%20a%20similar%20bound%20for%20every%20intersection-closed%0Aclass.%20The%20replay%20setting%20is%20provably%20harder%20than%20the%20classical%20mistake%20bound%0Asetting%3A%20some%20classes%20have%20constant%20Littlestone%20dimension%20but%20arbitrarily%20large%0A%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24.%20Proper%20learning%20exhibits%20an%20even%20sharper%0Aseparation%3A%20a%20class%20is%20properly%20learnable%20under%20replay%20if%20and%20only%20if%20it%20is%0A%28almost%29%20intersection-closed.%20Otherwise%2C%20every%20proper%20learner%20suffers%0A%24%5COmega%28T%29%24%20errors%2C%20whereas%20our%20improper%20algorithm%20still%20achieves%20the%0A%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%20bound.%20These%20results%20give%20the%20first%20tight%0Aanalysis%20of%20learning%20against%20replay%20adversaries%2C%20based%20on%20new%20results%20for%0Aclosure-type%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520in%2520an%2520Echo%2520Chamber%253A%2520Online%2520Learning%2520with%2520Replay%2520Adversary%26entry.906535625%3DDaniil%2520Dmitriev%2520and%2520Harald%2520Eskelund%2520Franck%2520and%2520Carolin%2520Heinzler%2520and%2520Amartya%2520Sanyal%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520systems%2520increasingly%2520train%2520on%2520self-annotated%2520data%252C%2520they%250Arisk%2520reinforcing%2520errors%2520and%2520becoming%2520echo%2520chambers%2520of%2520their%2520own%2520beliefs.%2520We%250Amodel%2520this%2520phenomenon%2520by%2520introducing%2520a%2520learning-theoretic%2520framework%253A%2520Online%250ALearning%2520in%2520the%2520Replay%2520Setting.%2520In%2520round%2520%2524t%2524%252C%2520the%2520learner%2520outputs%2520a%2520hypothesis%250A%2524%255Chat%257Bh%257D_t%2524%253B%2520the%2520adversary%2520then%2520reveals%2520either%2520the%2520true%2520label%2520%2524f%255E%255Cast%2528x_t%2529%2524%2520or%250Aa%2520replayed%2520label%2520%2524%255Chat%257Bh%257D_i%2528x_t%2529%2524%2520from%2520an%2520earlier%2520round%2520%2524i%2520%253C%2520t%2524.%2520A%2520mistake%2520is%250Acounted%2520only%2520when%2520the%2520true%2520label%2520is%2520shown%252C%2520yet%2520classical%2520algorithms%2520such%2520as%2520the%250ASOA%2520or%2520the%2520halving%2520algorithm%2520are%2520easily%2520misled%2520by%2520the%2520replayed%2520errors.%250A%2520%2520We%2520introduce%2520the%2520Extended%2520Threshold%2520dimension%252C%2520%2524%255Cmathrm%257BExThD%257D%2528%255Cmathcal%257BH%257D%2529%2524%252C%250Aand%2520prove%2520matching%2520upper%2520and%2520lower%2520bounds%2520that%2520make%250A%2524%255Cmathrm%257BExThD%257D%2528%255Cmathcal%257BH%257D%2529%2524%2520the%2520exact%2520measure%2520of%2520learnability%2520in%2520this%2520model.%250AA%2520closure-based%2520learner%2520makes%2520at%2520most%2520%2524%255Cmathrm%257BExThD%257D%2528%255Cmathcal%257BH%257D%2529%2524%2520mistakes%250Aagainst%2520any%2520adaptive%2520adversary%252C%2520and%2520no%2520algorithm%2520can%2520perform%2520better.%2520For%250Astochastic%2520adversaries%252C%2520we%2520prove%2520a%2520similar%2520bound%2520for%2520every%2520intersection-closed%250Aclass.%2520The%2520replay%2520setting%2520is%2520provably%2520harder%2520than%2520the%2520classical%2520mistake%2520bound%250Asetting%253A%2520some%2520classes%2520have%2520constant%2520Littlestone%2520dimension%2520but%2520arbitrarily%2520large%250A%2524%255Cmathrm%257BExThD%257D%2528%255Cmathcal%257BH%257D%2529%2524.%2520Proper%2520learning%2520exhibits%2520an%2520even%2520sharper%250Aseparation%253A%2520a%2520class%2520is%2520properly%2520learnable%2520under%2520replay%2520if%2520and%2520only%2520if%2520it%2520is%250A%2528almost%2529%2520intersection-closed.%2520Otherwise%252C%2520every%2520proper%2520learner%2520suffers%250A%2524%255COmega%2528T%2529%2524%2520errors%252C%2520whereas%2520our%2520improper%2520algorithm%2520still%2520achieves%2520the%250A%2524%255Cmathrm%257BExThD%257D%2528%255Cmathcal%257BH%257D%2529%2524%2520bound.%2520These%2520results%2520give%2520the%2520first%2520tight%250Aanalysis%2520of%2520learning%2520against%2520replay%2520adversaries%252C%2520based%2520on%2520new%2520results%2520for%250Aclosure-type%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20in%20an%20Echo%20Chamber%3A%20Online%20Learning%20with%20Replay%20Adversary&entry.906535625=Daniil%20Dmitriev%20and%20Harald%20Eskelund%20Franck%20and%20Carolin%20Heinzler%20and%20Amartya%20Sanyal&entry.1292438233=%20%20As%20machine%20learning%20systems%20increasingly%20train%20on%20self-annotated%20data%2C%20they%0Arisk%20reinforcing%20errors%20and%20becoming%20echo%20chambers%20of%20their%20own%20beliefs.%20We%0Amodel%20this%20phenomenon%20by%20introducing%20a%20learning-theoretic%20framework%3A%20Online%0ALearning%20in%20the%20Replay%20Setting.%20In%20round%20%24t%24%2C%20the%20learner%20outputs%20a%20hypothesis%0A%24%5Chat%7Bh%7D_t%24%3B%20the%20adversary%20then%20reveals%20either%20the%20true%20label%20%24f%5E%5Cast%28x_t%29%24%20or%0Aa%20replayed%20label%20%24%5Chat%7Bh%7D_i%28x_t%29%24%20from%20an%20earlier%20round%20%24i%20%3C%20t%24.%20A%20mistake%20is%0Acounted%20only%20when%20the%20true%20label%20is%20shown%2C%20yet%20classical%20algorithms%20such%20as%20the%0ASOA%20or%20the%20halving%20algorithm%20are%20easily%20misled%20by%20the%20replayed%20errors.%0A%20%20We%20introduce%20the%20Extended%20Threshold%20dimension%2C%20%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%2C%0Aand%20prove%20matching%20upper%20and%20lower%20bounds%20that%20make%0A%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%20the%20exact%20measure%20of%20learnability%20in%20this%20model.%0AA%20closure-based%20learner%20makes%20at%20most%20%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%20mistakes%0Aagainst%20any%20adaptive%20adversary%2C%20and%20no%20algorithm%20can%20perform%20better.%20For%0Astochastic%20adversaries%2C%20we%20prove%20a%20similar%20bound%20for%20every%20intersection-closed%0Aclass.%20The%20replay%20setting%20is%20provably%20harder%20than%20the%20classical%20mistake%20bound%0Asetting%3A%20some%20classes%20have%20constant%20Littlestone%20dimension%20but%20arbitrarily%20large%0A%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24.%20Proper%20learning%20exhibits%20an%20even%20sharper%0Aseparation%3A%20a%20class%20is%20properly%20learnable%20under%20replay%20if%20and%20only%20if%20it%20is%0A%28almost%29%20intersection-closed.%20Otherwise%2C%20every%20proper%20learner%20suffers%0A%24%5COmega%28T%29%24%20errors%2C%20whereas%20our%20improper%20algorithm%20still%20achieves%20the%0A%24%5Cmathrm%7BExThD%7D%28%5Cmathcal%7BH%7D%29%24%20bound.%20These%20results%20give%20the%20first%20tight%0Aanalysis%20of%20learning%20against%20replay%20adversaries%2C%20based%20on%20new%20results%20for%0Aclosure-type%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25135v1&entry.124074799=Read"},
{"title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation", "author": "Chaoran Zhu and Hengyi Wang and Yik Lung Pang and Changjae Oh", "abstract": "  Visual-textual understanding is essential for language-guided robot\nmanipulation. Recent works leverage pre-trained vision-language models to\nmeasure the similarity between encoded visual observations and textual\ninstructions, and then train a model to map this similarity to robot actions.\nHowever, this two-step approach limits the model to capture the relationship\nbetween visual observations and textual instructions, leading to reduced\nprecision in manipulation tasks. We propose to learn visual-textual\nassociations through a self-supervised pretext task: reconstructing a masked\ngoal image conditioned on an input image and textual instructions. This\nformulation allows the model to learn visual-action representations without\nrobot action supervision. The learned representations can then be fine-tuned\nfor manipulation tasks with only a few demonstrations. We also introduce the\n\\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot\ntabletop manipulation episodes, including 180 object classes and 3,200\ninstances with corresponding textual instructions. This dataset enables the\nmodel to acquire diverse object priors and allows for a more comprehensive\nevaluation of its generalisation capability across object instances.\nExperimental results on the five benchmarks, including both simulated and\nreal-robot validations, demonstrate that our method outperforms prior art.\n", "link": "http://arxiv.org/abs/2508.19391v2", "date": "2025-09-29", "relevancy": 2.3816, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaVA-Man%3A%20Learning%20Visual%20Action%20Representations%20for%20Robot%20Manipulation&body=Title%3A%20LaVA-Man%3A%20Learning%20Visual%20Action%20Representations%20for%20Robot%20Manipulation%0AAuthor%3A%20Chaoran%20Zhu%20and%20Hengyi%20Wang%20and%20Yik%20Lung%20Pang%20and%20Changjae%20Oh%0AAbstract%3A%20%20%20Visual-textual%20understanding%20is%20essential%20for%20language-guided%20robot%0Amanipulation.%20Recent%20works%20leverage%20pre-trained%20vision-language%20models%20to%0Ameasure%20the%20similarity%20between%20encoded%20visual%20observations%20and%20textual%0Ainstructions%2C%20and%20then%20train%20a%20model%20to%20map%20this%20similarity%20to%20robot%20actions.%0AHowever%2C%20this%20two-step%20approach%20limits%20the%20model%20to%20capture%20the%20relationship%0Abetween%20visual%20observations%20and%20textual%20instructions%2C%20leading%20to%20reduced%0Aprecision%20in%20manipulation%20tasks.%20We%20propose%20to%20learn%20visual-textual%0Aassociations%20through%20a%20self-supervised%20pretext%20task%3A%20reconstructing%20a%20masked%0Agoal%20image%20conditioned%20on%20an%20input%20image%20and%20textual%20instructions.%20This%0Aformulation%20allows%20the%20model%20to%20learn%20visual-action%20representations%20without%0Arobot%20action%20supervision.%20The%20learned%20representations%20can%20then%20be%20fine-tuned%0Afor%20manipulation%20tasks%20with%20only%20a%20few%20demonstrations.%20We%20also%20introduce%20the%0A%5Ctextit%7BOmni-Object%20Pick-and-Place%7D%20dataset%2C%20which%20consists%20of%20annotated%20robot%0Atabletop%20manipulation%20episodes%2C%20including%20180%20object%20classes%20and%203%2C200%0Ainstances%20with%20corresponding%20textual%20instructions.%20This%20dataset%20enables%20the%0Amodel%20to%20acquire%20diverse%20object%20priors%20and%20allows%20for%20a%20more%20comprehensive%0Aevaluation%20of%20its%20generalisation%20capability%20across%20object%20instances.%0AExperimental%20results%20on%20the%20five%20benchmarks%2C%20including%20both%20simulated%20and%0Areal-robot%20validations%2C%20demonstrate%20that%20our%20method%20outperforms%20prior%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaVA-Man%253A%2520Learning%2520Visual%2520Action%2520Representations%2520for%2520Robot%2520Manipulation%26entry.906535625%3DChaoran%2520Zhu%2520and%2520Hengyi%2520Wang%2520and%2520Yik%2520Lung%2520Pang%2520and%2520Changjae%2520Oh%26entry.1292438233%3D%2520%2520Visual-textual%2520understanding%2520is%2520essential%2520for%2520language-guided%2520robot%250Amanipulation.%2520Recent%2520works%2520leverage%2520pre-trained%2520vision-language%2520models%2520to%250Ameasure%2520the%2520similarity%2520between%2520encoded%2520visual%2520observations%2520and%2520textual%250Ainstructions%252C%2520and%2520then%2520train%2520a%2520model%2520to%2520map%2520this%2520similarity%2520to%2520robot%2520actions.%250AHowever%252C%2520this%2520two-step%2520approach%2520limits%2520the%2520model%2520to%2520capture%2520the%2520relationship%250Abetween%2520visual%2520observations%2520and%2520textual%2520instructions%252C%2520leading%2520to%2520reduced%250Aprecision%2520in%2520manipulation%2520tasks.%2520We%2520propose%2520to%2520learn%2520visual-textual%250Aassociations%2520through%2520a%2520self-supervised%2520pretext%2520task%253A%2520reconstructing%2520a%2520masked%250Agoal%2520image%2520conditioned%2520on%2520an%2520input%2520image%2520and%2520textual%2520instructions.%2520This%250Aformulation%2520allows%2520the%2520model%2520to%2520learn%2520visual-action%2520representations%2520without%250Arobot%2520action%2520supervision.%2520The%2520learned%2520representations%2520can%2520then%2520be%2520fine-tuned%250Afor%2520manipulation%2520tasks%2520with%2520only%2520a%2520few%2520demonstrations.%2520We%2520also%2520introduce%2520the%250A%255Ctextit%257BOmni-Object%2520Pick-and-Place%257D%2520dataset%252C%2520which%2520consists%2520of%2520annotated%2520robot%250Atabletop%2520manipulation%2520episodes%252C%2520including%2520180%2520object%2520classes%2520and%25203%252C200%250Ainstances%2520with%2520corresponding%2520textual%2520instructions.%2520This%2520dataset%2520enables%2520the%250Amodel%2520to%2520acquire%2520diverse%2520object%2520priors%2520and%2520allows%2520for%2520a%2520more%2520comprehensive%250Aevaluation%2520of%2520its%2520generalisation%2520capability%2520across%2520object%2520instances.%250AExperimental%2520results%2520on%2520the%2520five%2520benchmarks%252C%2520including%2520both%2520simulated%2520and%250Areal-robot%2520validations%252C%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520prior%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaVA-Man%3A%20Learning%20Visual%20Action%20Representations%20for%20Robot%20Manipulation&entry.906535625=Chaoran%20Zhu%20and%20Hengyi%20Wang%20and%20Yik%20Lung%20Pang%20and%20Changjae%20Oh&entry.1292438233=%20%20Visual-textual%20understanding%20is%20essential%20for%20language-guided%20robot%0Amanipulation.%20Recent%20works%20leverage%20pre-trained%20vision-language%20models%20to%0Ameasure%20the%20similarity%20between%20encoded%20visual%20observations%20and%20textual%0Ainstructions%2C%20and%20then%20train%20a%20model%20to%20map%20this%20similarity%20to%20robot%20actions.%0AHowever%2C%20this%20two-step%20approach%20limits%20the%20model%20to%20capture%20the%20relationship%0Abetween%20visual%20observations%20and%20textual%20instructions%2C%20leading%20to%20reduced%0Aprecision%20in%20manipulation%20tasks.%20We%20propose%20to%20learn%20visual-textual%0Aassociations%20through%20a%20self-supervised%20pretext%20task%3A%20reconstructing%20a%20masked%0Agoal%20image%20conditioned%20on%20an%20input%20image%20and%20textual%20instructions.%20This%0Aformulation%20allows%20the%20model%20to%20learn%20visual-action%20representations%20without%0Arobot%20action%20supervision.%20The%20learned%20representations%20can%20then%20be%20fine-tuned%0Afor%20manipulation%20tasks%20with%20only%20a%20few%20demonstrations.%20We%20also%20introduce%20the%0A%5Ctextit%7BOmni-Object%20Pick-and-Place%7D%20dataset%2C%20which%20consists%20of%20annotated%20robot%0Atabletop%20manipulation%20episodes%2C%20including%20180%20object%20classes%20and%203%2C200%0Ainstances%20with%20corresponding%20textual%20instructions.%20This%20dataset%20enables%20the%0Amodel%20to%20acquire%20diverse%20object%20priors%20and%20allows%20for%20a%20more%20comprehensive%0Aevaluation%20of%20its%20generalisation%20capability%20across%20object%20instances.%0AExperimental%20results%20on%20the%20five%20benchmarks%2C%20including%20both%20simulated%20and%0Areal-robot%20validations%2C%20demonstrate%20that%20our%20method%20outperforms%20prior%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19391v2&entry.124074799=Read"},
{"title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation", "author": "Yen-Ju Lu and Thomas Thebaud and Laureano Moro-Velazquez and Najim Dehak and Jesus Villalba", "abstract": "  We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.\n", "link": "http://arxiv.org/abs/2509.25144v1", "date": "2025-09-29", "relevancy": 2.3754, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.486}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.47}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paired%20by%20the%20Teacher%3A%20Turning%20Unpaired%20Data%20into%20High-Fidelity%20Pairs%0A%20%20for%20Low-Resource%20Text%20Generation&body=Title%3A%20Paired%20by%20the%20Teacher%3A%20Turning%20Unpaired%20Data%20into%20High-Fidelity%20Pairs%0A%20%20for%20Low-Resource%20Text%20Generation%0AAuthor%3A%20Yen-Ju%20Lu%20and%20Thomas%20Thebaud%20and%20Laureano%20Moro-Velazquez%20and%20Najim%20Dehak%20and%20Jesus%20Villalba%0AAbstract%3A%20%20%20We%20present%20Paired%20by%20the%20Teacher%20%28PbT%29%2C%20a%20two-stage%20teacher-student%20pipeline%0Athat%20synthesizes%20accurate%20input-output%20pairs%20without%20human%20labels%20or%20parallel%0Adata.%20In%20many%20low-resource%20natural%20language%20generation%20%28NLG%29%20scenarios%2C%0Apractitioners%20may%20have%20only%20raw%20outputs%2C%20like%20highlights%2C%20recaps%2C%20or%20questions%2C%0Aor%20only%20raw%20inputs%2C%20such%20as%20articles%2C%20dialogues%2C%20or%20paragraphs%2C%20but%20seldom%0Aboth.%20This%20mismatch%20forces%20small%20models%20to%20learn%20from%20very%20few%20examples%20or%20rely%0Aon%20costly%2C%20broad-scope%20synthetic%20examples%20produced%20by%20large%20LLMs.%20PbT%20addresses%0Athis%20by%20asking%20a%20teacher%20LLM%20to%20compress%20each%20unpaired%20example%20into%20a%20concise%0Aintermediate%20representation%20%28IR%29%2C%20and%20training%20a%20student%20to%20reconstruct%20inputs%0Afrom%20IRs.%20This%20enables%20outputs%20to%20be%20paired%20with%20student-generated%20inputs%2C%0Ayielding%20high-quality%20synthetic%20data.%20We%20evaluate%20PbT%20on%20five%0Abenchmarks-document%20summarization%20%28XSum%2C%20CNNDM%29%2C%20dialogue%20summarization%0A%28SAMSum%2C%20DialogSum%29%2C%20and%20question%20generation%20%28SQuAD%29-as%20well%20as%20an%20unpaired%0Asetting%20on%20SwitchBoard%20%28paired%20with%20DialogSum%20summaries%29.%20An%208B%20student%20trained%0Aonly%20on%20PbT%20data%20outperforms%20models%20trained%20on%2070%20B%20teacher-generated%20corpora%0Aand%20other%20unsupervised%20baselines%2C%20coming%20within%201.2%20ROUGE-L%20of%20human-annotated%0Apairs%20and%20closing%2082%25%20of%20the%20oracle%20gap%20at%20one-third%20the%20annotation%20cost%20of%0Adirect%20synthesis.%20Human%20evaluation%20on%20SwitchBoard%20further%20confirms%20that%20only%0APbT%20produces%20concise%2C%20faithful%20summaries%20aligned%20with%20the%20target%20style%2C%0Ahighlighting%20its%20advantage%20of%20generating%20in-domain%20sources%20that%20avoid%20the%0Amismatch%2C%20limiting%20direct%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaired%2520by%2520the%2520Teacher%253A%2520Turning%2520Unpaired%2520Data%2520into%2520High-Fidelity%2520Pairs%250A%2520%2520for%2520Low-Resource%2520Text%2520Generation%26entry.906535625%3DYen-Ju%2520Lu%2520and%2520Thomas%2520Thebaud%2520and%2520Laureano%2520Moro-Velazquez%2520and%2520Najim%2520Dehak%2520and%2520Jesus%2520Villalba%26entry.1292438233%3D%2520%2520We%2520present%2520Paired%2520by%2520the%2520Teacher%2520%2528PbT%2529%252C%2520a%2520two-stage%2520teacher-student%2520pipeline%250Athat%2520synthesizes%2520accurate%2520input-output%2520pairs%2520without%2520human%2520labels%2520or%2520parallel%250Adata.%2520In%2520many%2520low-resource%2520natural%2520language%2520generation%2520%2528NLG%2529%2520scenarios%252C%250Apractitioners%2520may%2520have%2520only%2520raw%2520outputs%252C%2520like%2520highlights%252C%2520recaps%252C%2520or%2520questions%252C%250Aor%2520only%2520raw%2520inputs%252C%2520such%2520as%2520articles%252C%2520dialogues%252C%2520or%2520paragraphs%252C%2520but%2520seldom%250Aboth.%2520This%2520mismatch%2520forces%2520small%2520models%2520to%2520learn%2520from%2520very%2520few%2520examples%2520or%2520rely%250Aon%2520costly%252C%2520broad-scope%2520synthetic%2520examples%2520produced%2520by%2520large%2520LLMs.%2520PbT%2520addresses%250Athis%2520by%2520asking%2520a%2520teacher%2520LLM%2520to%2520compress%2520each%2520unpaired%2520example%2520into%2520a%2520concise%250Aintermediate%2520representation%2520%2528IR%2529%252C%2520and%2520training%2520a%2520student%2520to%2520reconstruct%2520inputs%250Afrom%2520IRs.%2520This%2520enables%2520outputs%2520to%2520be%2520paired%2520with%2520student-generated%2520inputs%252C%250Ayielding%2520high-quality%2520synthetic%2520data.%2520We%2520evaluate%2520PbT%2520on%2520five%250Abenchmarks-document%2520summarization%2520%2528XSum%252C%2520CNNDM%2529%252C%2520dialogue%2520summarization%250A%2528SAMSum%252C%2520DialogSum%2529%252C%2520and%2520question%2520generation%2520%2528SQuAD%2529-as%2520well%2520as%2520an%2520unpaired%250Asetting%2520on%2520SwitchBoard%2520%2528paired%2520with%2520DialogSum%2520summaries%2529.%2520An%25208B%2520student%2520trained%250Aonly%2520on%2520PbT%2520data%2520outperforms%2520models%2520trained%2520on%252070%2520B%2520teacher-generated%2520corpora%250Aand%2520other%2520unsupervised%2520baselines%252C%2520coming%2520within%25201.2%2520ROUGE-L%2520of%2520human-annotated%250Apairs%2520and%2520closing%252082%2525%2520of%2520the%2520oracle%2520gap%2520at%2520one-third%2520the%2520annotation%2520cost%2520of%250Adirect%2520synthesis.%2520Human%2520evaluation%2520on%2520SwitchBoard%2520further%2520confirms%2520that%2520only%250APbT%2520produces%2520concise%252C%2520faithful%2520summaries%2520aligned%2520with%2520the%2520target%2520style%252C%250Ahighlighting%2520its%2520advantage%2520of%2520generating%2520in-domain%2520sources%2520that%2520avoid%2520the%250Amismatch%252C%2520limiting%2520direct%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paired%20by%20the%20Teacher%3A%20Turning%20Unpaired%20Data%20into%20High-Fidelity%20Pairs%0A%20%20for%20Low-Resource%20Text%20Generation&entry.906535625=Yen-Ju%20Lu%20and%20Thomas%20Thebaud%20and%20Laureano%20Moro-Velazquez%20and%20Najim%20Dehak%20and%20Jesus%20Villalba&entry.1292438233=%20%20We%20present%20Paired%20by%20the%20Teacher%20%28PbT%29%2C%20a%20two-stage%20teacher-student%20pipeline%0Athat%20synthesizes%20accurate%20input-output%20pairs%20without%20human%20labels%20or%20parallel%0Adata.%20In%20many%20low-resource%20natural%20language%20generation%20%28NLG%29%20scenarios%2C%0Apractitioners%20may%20have%20only%20raw%20outputs%2C%20like%20highlights%2C%20recaps%2C%20or%20questions%2C%0Aor%20only%20raw%20inputs%2C%20such%20as%20articles%2C%20dialogues%2C%20or%20paragraphs%2C%20but%20seldom%0Aboth.%20This%20mismatch%20forces%20small%20models%20to%20learn%20from%20very%20few%20examples%20or%20rely%0Aon%20costly%2C%20broad-scope%20synthetic%20examples%20produced%20by%20large%20LLMs.%20PbT%20addresses%0Athis%20by%20asking%20a%20teacher%20LLM%20to%20compress%20each%20unpaired%20example%20into%20a%20concise%0Aintermediate%20representation%20%28IR%29%2C%20and%20training%20a%20student%20to%20reconstruct%20inputs%0Afrom%20IRs.%20This%20enables%20outputs%20to%20be%20paired%20with%20student-generated%20inputs%2C%0Ayielding%20high-quality%20synthetic%20data.%20We%20evaluate%20PbT%20on%20five%0Abenchmarks-document%20summarization%20%28XSum%2C%20CNNDM%29%2C%20dialogue%20summarization%0A%28SAMSum%2C%20DialogSum%29%2C%20and%20question%20generation%20%28SQuAD%29-as%20well%20as%20an%20unpaired%0Asetting%20on%20SwitchBoard%20%28paired%20with%20DialogSum%20summaries%29.%20An%208B%20student%20trained%0Aonly%20on%20PbT%20data%20outperforms%20models%20trained%20on%2070%20B%20teacher-generated%20corpora%0Aand%20other%20unsupervised%20baselines%2C%20coming%20within%201.2%20ROUGE-L%20of%20human-annotated%0Apairs%20and%20closing%2082%25%20of%20the%20oracle%20gap%20at%20one-third%20the%20annotation%20cost%20of%0Adirect%20synthesis.%20Human%20evaluation%20on%20SwitchBoard%20further%20confirms%20that%20only%0APbT%20produces%20concise%2C%20faithful%20summaries%20aligned%20with%20the%20target%20style%2C%0Ahighlighting%20its%20advantage%20of%20generating%20in-domain%20sources%20that%20avoid%20the%0Amismatch%2C%20limiting%20direct%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25144v1&entry.124074799=Read"},
{"title": "Real-time Recognition of Human Interactions from a Single RGB-D Camera\n  for Socially-Aware Robot Navigation", "author": "Thanh Long Nguyen and Duc Phu Nguyen and Thanh Thao Ton Nu and Quan Le and Thuan Hoang Tran and Manh Duong Phung", "abstract": "  {Recognizing human interactions is essential for social robots as it enables\nthem to navigate safely and naturally in shared environments. Conventional\nrobotic systems however often focus on obstacle avoidance, neglecting social\ncues necessary for seamless human-robot interaction. To address this gap, we\npropose a framework to recognize human group interactions for socially aware\nnavigation. Our method utilizes color and depth frames from a monocular RGB-D\ncamera to estimate 3D human keypoints and positions. Principal component\nanalysis (PCA) is then used to determine dominant interaction directions. The\nshoelace formula is finally applied to compute interest points and engagement\nareas. Extensive experiments have been conducted to evaluate the validity of\nthe proposed method. The results show that our method is capable of recognizing\ngroup interactions across different scenarios with varying numbers of\nindividuals. It also achieves high-speed performance, processing each frame in\napproximately 4 ms on a single-board computer used in robotic systems. The\nmethod is implemented as a ROS 2 package making it simple to integrate into\nexisting navigation systems. Source code is available at\nhttps://github.com/thanhlong103/social-interaction-detector\n", "link": "http://arxiv.org/abs/2509.24907v1", "date": "2025-09-29", "relevancy": 2.3617, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6027}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Recognition%20of%20Human%20Interactions%20from%20a%20Single%20RGB-D%20Camera%0A%20%20for%20Socially-Aware%20Robot%20Navigation&body=Title%3A%20Real-time%20Recognition%20of%20Human%20Interactions%20from%20a%20Single%20RGB-D%20Camera%0A%20%20for%20Socially-Aware%20Robot%20Navigation%0AAuthor%3A%20Thanh%20Long%20Nguyen%20and%20Duc%20Phu%20Nguyen%20and%20Thanh%20Thao%20Ton%20Nu%20and%20Quan%20Le%20and%20Thuan%20Hoang%20Tran%20and%20Manh%20Duong%20Phung%0AAbstract%3A%20%20%20%7BRecognizing%20human%20interactions%20is%20essential%20for%20social%20robots%20as%20it%20enables%0Athem%20to%20navigate%20safely%20and%20naturally%20in%20shared%20environments.%20Conventional%0Arobotic%20systems%20however%20often%20focus%20on%20obstacle%20avoidance%2C%20neglecting%20social%0Acues%20necessary%20for%20seamless%20human-robot%20interaction.%20To%20address%20this%20gap%2C%20we%0Apropose%20a%20framework%20to%20recognize%20human%20group%20interactions%20for%20socially%20aware%0Anavigation.%20Our%20method%20utilizes%20color%20and%20depth%20frames%20from%20a%20monocular%20RGB-D%0Acamera%20to%20estimate%203D%20human%20keypoints%20and%20positions.%20Principal%20component%0Aanalysis%20%28PCA%29%20is%20then%20used%20to%20determine%20dominant%20interaction%20directions.%20The%0Ashoelace%20formula%20is%20finally%20applied%20to%20compute%20interest%20points%20and%20engagement%0Aareas.%20Extensive%20experiments%20have%20been%20conducted%20to%20evaluate%20the%20validity%20of%0Athe%20proposed%20method.%20The%20results%20show%20that%20our%20method%20is%20capable%20of%20recognizing%0Agroup%20interactions%20across%20different%20scenarios%20with%20varying%20numbers%20of%0Aindividuals.%20It%20also%20achieves%20high-speed%20performance%2C%20processing%20each%20frame%20in%0Aapproximately%204%20ms%20on%20a%20single-board%20computer%20used%20in%20robotic%20systems.%20The%0Amethod%20is%20implemented%20as%20a%20ROS%202%20package%20making%20it%20simple%20to%20integrate%20into%0Aexisting%20navigation%20systems.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/thanhlong103/social-interaction-detector%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Recognition%2520of%2520Human%2520Interactions%2520from%2520a%2520Single%2520RGB-D%2520Camera%250A%2520%2520for%2520Socially-Aware%2520Robot%2520Navigation%26entry.906535625%3DThanh%2520Long%2520Nguyen%2520and%2520Duc%2520Phu%2520Nguyen%2520and%2520Thanh%2520Thao%2520Ton%2520Nu%2520and%2520Quan%2520Le%2520and%2520Thuan%2520Hoang%2520Tran%2520and%2520Manh%2520Duong%2520Phung%26entry.1292438233%3D%2520%2520%257BRecognizing%2520human%2520interactions%2520is%2520essential%2520for%2520social%2520robots%2520as%2520it%2520enables%250Athem%2520to%2520navigate%2520safely%2520and%2520naturally%2520in%2520shared%2520environments.%2520Conventional%250Arobotic%2520systems%2520however%2520often%2520focus%2520on%2520obstacle%2520avoidance%252C%2520neglecting%2520social%250Acues%2520necessary%2520for%2520seamless%2520human-robot%2520interaction.%2520To%2520address%2520this%2520gap%252C%2520we%250Apropose%2520a%2520framework%2520to%2520recognize%2520human%2520group%2520interactions%2520for%2520socially%2520aware%250Anavigation.%2520Our%2520method%2520utilizes%2520color%2520and%2520depth%2520frames%2520from%2520a%2520monocular%2520RGB-D%250Acamera%2520to%2520estimate%25203D%2520human%2520keypoints%2520and%2520positions.%2520Principal%2520component%250Aanalysis%2520%2528PCA%2529%2520is%2520then%2520used%2520to%2520determine%2520dominant%2520interaction%2520directions.%2520The%250Ashoelace%2520formula%2520is%2520finally%2520applied%2520to%2520compute%2520interest%2520points%2520and%2520engagement%250Aareas.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520to%2520evaluate%2520the%2520validity%2520of%250Athe%2520proposed%2520method.%2520The%2520results%2520show%2520that%2520our%2520method%2520is%2520capable%2520of%2520recognizing%250Agroup%2520interactions%2520across%2520different%2520scenarios%2520with%2520varying%2520numbers%2520of%250Aindividuals.%2520It%2520also%2520achieves%2520high-speed%2520performance%252C%2520processing%2520each%2520frame%2520in%250Aapproximately%25204%2520ms%2520on%2520a%2520single-board%2520computer%2520used%2520in%2520robotic%2520systems.%2520The%250Amethod%2520is%2520implemented%2520as%2520a%2520ROS%25202%2520package%2520making%2520it%2520simple%2520to%2520integrate%2520into%250Aexisting%2520navigation%2520systems.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/thanhlong103/social-interaction-detector%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Recognition%20of%20Human%20Interactions%20from%20a%20Single%20RGB-D%20Camera%0A%20%20for%20Socially-Aware%20Robot%20Navigation&entry.906535625=Thanh%20Long%20Nguyen%20and%20Duc%20Phu%20Nguyen%20and%20Thanh%20Thao%20Ton%20Nu%20and%20Quan%20Le%20and%20Thuan%20Hoang%20Tran%20and%20Manh%20Duong%20Phung&entry.1292438233=%20%20%7BRecognizing%20human%20interactions%20is%20essential%20for%20social%20robots%20as%20it%20enables%0Athem%20to%20navigate%20safely%20and%20naturally%20in%20shared%20environments.%20Conventional%0Arobotic%20systems%20however%20often%20focus%20on%20obstacle%20avoidance%2C%20neglecting%20social%0Acues%20necessary%20for%20seamless%20human-robot%20interaction.%20To%20address%20this%20gap%2C%20we%0Apropose%20a%20framework%20to%20recognize%20human%20group%20interactions%20for%20socially%20aware%0Anavigation.%20Our%20method%20utilizes%20color%20and%20depth%20frames%20from%20a%20monocular%20RGB-D%0Acamera%20to%20estimate%203D%20human%20keypoints%20and%20positions.%20Principal%20component%0Aanalysis%20%28PCA%29%20is%20then%20used%20to%20determine%20dominant%20interaction%20directions.%20The%0Ashoelace%20formula%20is%20finally%20applied%20to%20compute%20interest%20points%20and%20engagement%0Aareas.%20Extensive%20experiments%20have%20been%20conducted%20to%20evaluate%20the%20validity%20of%0Athe%20proposed%20method.%20The%20results%20show%20that%20our%20method%20is%20capable%20of%20recognizing%0Agroup%20interactions%20across%20different%20scenarios%20with%20varying%20numbers%20of%0Aindividuals.%20It%20also%20achieves%20high-speed%20performance%2C%20processing%20each%20frame%20in%0Aapproximately%204%20ms%20on%20a%20single-board%20computer%20used%20in%20robotic%20systems.%20The%0Amethod%20is%20implemented%20as%20a%20ROS%202%20package%20making%20it%20simple%20to%20integrate%20into%0Aexisting%20navigation%20systems.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/thanhlong103/social-interaction-detector%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24907v1&entry.124074799=Read"},
{"title": "Vision At Night: Exploring Biologically Inspired Preprocessing For\n  Improved Robustness Via Color And Contrast Transformations", "author": "Lorena Stracke and Lia Nimmermann and Shashank Agnihotri and Margret Keuper and Volker Blanz", "abstract": "  Inspired by the human visual system's mechanisms for contrast enhancement and\ncolor-opponency, we explore biologically motivated input preprocessing for\nrobust semantic segmentation. By applying Difference-of-Gaussians (DoG)\nfiltering to RGB, grayscale, and opponent-color channels, we enhance local\ncontrast without modifying model architecture or training. Evaluations on\nCityscapes, ACDC, and Dark Zurich show that such preprocessing maintains\nin-distribution performance while improving robustness to adverse conditions\nlike night, fog, and snow. As this processing is model-agnostic and\nlightweight, it holds potential for integration into imaging pipelines,\nenabling imaging systems to deliver task-ready, robust inputs for downstream\nvision models in safety-critical environments.\n", "link": "http://arxiv.org/abs/2509.24863v1", "date": "2025-09-29", "relevancy": 2.3574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20At%20Night%3A%20Exploring%20Biologically%20Inspired%20Preprocessing%20For%0A%20%20Improved%20Robustness%20Via%20Color%20And%20Contrast%20Transformations&body=Title%3A%20Vision%20At%20Night%3A%20Exploring%20Biologically%20Inspired%20Preprocessing%20For%0A%20%20Improved%20Robustness%20Via%20Color%20And%20Contrast%20Transformations%0AAuthor%3A%20Lorena%20Stracke%20and%20Lia%20Nimmermann%20and%20Shashank%20Agnihotri%20and%20Margret%20Keuper%20and%20Volker%20Blanz%0AAbstract%3A%20%20%20Inspired%20by%20the%20human%20visual%20system%27s%20mechanisms%20for%20contrast%20enhancement%20and%0Acolor-opponency%2C%20we%20explore%20biologically%20motivated%20input%20preprocessing%20for%0Arobust%20semantic%20segmentation.%20By%20applying%20Difference-of-Gaussians%20%28DoG%29%0Afiltering%20to%20RGB%2C%20grayscale%2C%20and%20opponent-color%20channels%2C%20we%20enhance%20local%0Acontrast%20without%20modifying%20model%20architecture%20or%20training.%20Evaluations%20on%0ACityscapes%2C%20ACDC%2C%20and%20Dark%20Zurich%20show%20that%20such%20preprocessing%20maintains%0Ain-distribution%20performance%20while%20improving%20robustness%20to%20adverse%20conditions%0Alike%20night%2C%20fog%2C%20and%20snow.%20As%20this%20processing%20is%20model-agnostic%20and%0Alightweight%2C%20it%20holds%20potential%20for%20integration%20into%20imaging%20pipelines%2C%0Aenabling%20imaging%20systems%20to%20deliver%20task-ready%2C%20robust%20inputs%20for%20downstream%0Avision%20models%20in%20safety-critical%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520At%2520Night%253A%2520Exploring%2520Biologically%2520Inspired%2520Preprocessing%2520For%250A%2520%2520Improved%2520Robustness%2520Via%2520Color%2520And%2520Contrast%2520Transformations%26entry.906535625%3DLorena%2520Stracke%2520and%2520Lia%2520Nimmermann%2520and%2520Shashank%2520Agnihotri%2520and%2520Margret%2520Keuper%2520and%2520Volker%2520Blanz%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520human%2520visual%2520system%2527s%2520mechanisms%2520for%2520contrast%2520enhancement%2520and%250Acolor-opponency%252C%2520we%2520explore%2520biologically%2520motivated%2520input%2520preprocessing%2520for%250Arobust%2520semantic%2520segmentation.%2520By%2520applying%2520Difference-of-Gaussians%2520%2528DoG%2529%250Afiltering%2520to%2520RGB%252C%2520grayscale%252C%2520and%2520opponent-color%2520channels%252C%2520we%2520enhance%2520local%250Acontrast%2520without%2520modifying%2520model%2520architecture%2520or%2520training.%2520Evaluations%2520on%250ACityscapes%252C%2520ACDC%252C%2520and%2520Dark%2520Zurich%2520show%2520that%2520such%2520preprocessing%2520maintains%250Ain-distribution%2520performance%2520while%2520improving%2520robustness%2520to%2520adverse%2520conditions%250Alike%2520night%252C%2520fog%252C%2520and%2520snow.%2520As%2520this%2520processing%2520is%2520model-agnostic%2520and%250Alightweight%252C%2520it%2520holds%2520potential%2520for%2520integration%2520into%2520imaging%2520pipelines%252C%250Aenabling%2520imaging%2520systems%2520to%2520deliver%2520task-ready%252C%2520robust%2520inputs%2520for%2520downstream%250Avision%2520models%2520in%2520safety-critical%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20At%20Night%3A%20Exploring%20Biologically%20Inspired%20Preprocessing%20For%0A%20%20Improved%20Robustness%20Via%20Color%20And%20Contrast%20Transformations&entry.906535625=Lorena%20Stracke%20and%20Lia%20Nimmermann%20and%20Shashank%20Agnihotri%20and%20Margret%20Keuper%20and%20Volker%20Blanz&entry.1292438233=%20%20Inspired%20by%20the%20human%20visual%20system%27s%20mechanisms%20for%20contrast%20enhancement%20and%0Acolor-opponency%2C%20we%20explore%20biologically%20motivated%20input%20preprocessing%20for%0Arobust%20semantic%20segmentation.%20By%20applying%20Difference-of-Gaussians%20%28DoG%29%0Afiltering%20to%20RGB%2C%20grayscale%2C%20and%20opponent-color%20channels%2C%20we%20enhance%20local%0Acontrast%20without%20modifying%20model%20architecture%20or%20training.%20Evaluations%20on%0ACityscapes%2C%20ACDC%2C%20and%20Dark%20Zurich%20show%20that%20such%20preprocessing%20maintains%0Ain-distribution%20performance%20while%20improving%20robustness%20to%20adverse%20conditions%0Alike%20night%2C%20fog%2C%20and%20snow.%20As%20this%20processing%20is%20model-agnostic%20and%0Alightweight%2C%20it%20holds%20potential%20for%20integration%20into%20imaging%20pipelines%2C%0Aenabling%20imaging%20systems%20to%20deliver%20task-ready%2C%20robust%20inputs%20for%20downstream%0Avision%20models%20in%20safety-critical%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24863v1&entry.124074799=Read"},
{"title": "RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark", "author": "Yang Shi and Yuhao Dong and Yue Ding and Yuran Wang and Xuanyu Zhu and Sheng Zhou and Wenting Liu and Haochen Tian and Rundong Wang and Huanqian Wang and Zuyan Liu and Bohan Zeng and Ruizhe Chen and Qixun Wang and Zhuoran Zhang and Xinlong Chen and Chengzhuo Tong and Bozhou Li and Chaoyou Fu and Qiang Liu and Haotian Wang and Wenjing Yang and Yuanxing Zhang and Pengfei Wan and Yi-Fan Zhang and Ziwei Liu", "abstract": "  The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.\n", "link": "http://arxiv.org/abs/2509.24897v1", "date": "2025-09-29", "relevancy": 2.3447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealUnify%3A%20Do%20Unified%20Models%20Truly%20Benefit%20from%20Unification%3F%20A%0A%20%20Comprehensive%20Benchmark&body=Title%3A%20RealUnify%3A%20Do%20Unified%20Models%20Truly%20Benefit%20from%20Unification%3F%20A%0A%20%20Comprehensive%20Benchmark%0AAuthor%3A%20Yang%20Shi%20and%20Yuhao%20Dong%20and%20Yue%20Ding%20and%20Yuran%20Wang%20and%20Xuanyu%20Zhu%20and%20Sheng%20Zhou%20and%20Wenting%20Liu%20and%20Haochen%20Tian%20and%20Rundong%20Wang%20and%20Huanqian%20Wang%20and%20Zuyan%20Liu%20and%20Bohan%20Zeng%20and%20Ruizhe%20Chen%20and%20Qixun%20Wang%20and%20Zhuoran%20Zhang%20and%20Xinlong%20Chen%20and%20Chengzhuo%20Tong%20and%20Bozhou%20Li%20and%20Chaoyou%20Fu%20and%20Qiang%20Liu%20and%20Haotian%20Wang%20and%20Wenjing%20Yang%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Yi-Fan%20Zhang%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20The%20integration%20of%20visual%20understanding%20and%20generation%20into%20unified%0Amultimodal%20models%20represents%20a%20significant%20stride%20toward%20general-purpose%20AI.%0AHowever%2C%20a%20fundamental%20question%20remains%20unanswered%20by%20existing%20benchmarks%3A%20does%0Athis%20architectural%20unification%20actually%20enable%20synergetic%20interaction%20between%0Athe%20constituent%20capabilities%3F%20Existing%20evaluation%20paradigms%2C%20which%20primarily%0Aassess%20understanding%20and%20generation%20in%20isolation%2C%20are%20insufficient%20for%0Adetermining%20whether%20a%20unified%20model%20can%20leverage%20its%20understanding%20to%20enhance%0Aits%20generation%2C%20or%20use%20generative%20simulation%20to%20facilitate%20deeper%0Acomprehension.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20RealUnify%2C%20a%0Abenchmark%20specifically%20designed%20to%20evaluate%20bidirectional%20capability%20synergy.%0ARealUnify%20comprises%201%2C000%20meticulously%20human-annotated%20instances%20spanning%2010%0Acategories%20and%2032%20subtasks.%20It%20is%20structured%20around%20two%20core%20axes%3A%201%29%0AUnderstanding%20Enhances%20Generation%2C%20which%20requires%20reasoning%20%28e.g.%2C%20commonsense%2C%0Alogic%29%20to%20guide%20image%20generation%2C%20and%202%29%20Generation%20Enhances%20Understanding%2C%0Awhich%20necessitates%20mental%20simulation%20or%20reconstruction%20%28e.g.%2C%20of%20transformed%20or%0Adisordered%20visual%20inputs%29%20to%20solve%20reasoning%20tasks.%20A%20key%20contribution%20is%20our%0Adual-evaluation%20protocol%2C%20which%20combines%20direct%20end-to-end%20assessment%20with%20a%0Adiagnostic%20stepwise%20evaluation%20that%20decomposes%20tasks%20into%20distinct%0Aunderstanding%20and%20generation%20phases.%20This%20protocol%20allows%20us%20to%20precisely%0Adiscern%20whether%20performance%20bottlenecks%20stem%20from%20deficiencies%20in%20core%0Aabilities%20or%20from%20a%20failure%20to%20integrate%20them.%20Through%20large-scale%20evaluations%0Aof%2012%20leading%20unified%20models%20and%206%20specialized%20baselines%2C%20we%20find%20that%20current%0Aunified%20models%20still%20struggle%20to%20achieve%20effective%20synergy%2C%20indicating%20that%0Aarchitectural%20unification%20alone%20is%20insufficient.%20These%20results%20highlight%20the%0Aneed%20for%20new%20training%20strategies%20and%20inductive%20biases%20to%20fully%20unlock%20the%0Apotential%20of%20unified%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealUnify%253A%2520Do%2520Unified%2520Models%2520Truly%2520Benefit%2520from%2520Unification%253F%2520A%250A%2520%2520Comprehensive%2520Benchmark%26entry.906535625%3DYang%2520Shi%2520and%2520Yuhao%2520Dong%2520and%2520Yue%2520Ding%2520and%2520Yuran%2520Wang%2520and%2520Xuanyu%2520Zhu%2520and%2520Sheng%2520Zhou%2520and%2520Wenting%2520Liu%2520and%2520Haochen%2520Tian%2520and%2520Rundong%2520Wang%2520and%2520Huanqian%2520Wang%2520and%2520Zuyan%2520Liu%2520and%2520Bohan%2520Zeng%2520and%2520Ruizhe%2520Chen%2520and%2520Qixun%2520Wang%2520and%2520Zhuoran%2520Zhang%2520and%2520Xinlong%2520Chen%2520and%2520Chengzhuo%2520Tong%2520and%2520Bozhou%2520Li%2520and%2520Chaoyou%2520Fu%2520and%2520Qiang%2520Liu%2520and%2520Haotian%2520Wang%2520and%2520Wenjing%2520Yang%2520and%2520Yuanxing%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Yi-Fan%2520Zhang%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520visual%2520understanding%2520and%2520generation%2520into%2520unified%250Amultimodal%2520models%2520represents%2520a%2520significant%2520stride%2520toward%2520general-purpose%2520AI.%250AHowever%252C%2520a%2520fundamental%2520question%2520remains%2520unanswered%2520by%2520existing%2520benchmarks%253A%2520does%250Athis%2520architectural%2520unification%2520actually%2520enable%2520synergetic%2520interaction%2520between%250Athe%2520constituent%2520capabilities%253F%2520Existing%2520evaluation%2520paradigms%252C%2520which%2520primarily%250Aassess%2520understanding%2520and%2520generation%2520in%2520isolation%252C%2520are%2520insufficient%2520for%250Adetermining%2520whether%2520a%2520unified%2520model%2520can%2520leverage%2520its%2520understanding%2520to%2520enhance%250Aits%2520generation%252C%2520or%2520use%2520generative%2520simulation%2520to%2520facilitate%2520deeper%250Acomprehension.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520RealUnify%252C%2520a%250Abenchmark%2520specifically%2520designed%2520to%2520evaluate%2520bidirectional%2520capability%2520synergy.%250ARealUnify%2520comprises%25201%252C000%2520meticulously%2520human-annotated%2520instances%2520spanning%252010%250Acategories%2520and%252032%2520subtasks.%2520It%2520is%2520structured%2520around%2520two%2520core%2520axes%253A%25201%2529%250AUnderstanding%2520Enhances%2520Generation%252C%2520which%2520requires%2520reasoning%2520%2528e.g.%252C%2520commonsense%252C%250Alogic%2529%2520to%2520guide%2520image%2520generation%252C%2520and%25202%2529%2520Generation%2520Enhances%2520Understanding%252C%250Awhich%2520necessitates%2520mental%2520simulation%2520or%2520reconstruction%2520%2528e.g.%252C%2520of%2520transformed%2520or%250Adisordered%2520visual%2520inputs%2529%2520to%2520solve%2520reasoning%2520tasks.%2520A%2520key%2520contribution%2520is%2520our%250Adual-evaluation%2520protocol%252C%2520which%2520combines%2520direct%2520end-to-end%2520assessment%2520with%2520a%250Adiagnostic%2520stepwise%2520evaluation%2520that%2520decomposes%2520tasks%2520into%2520distinct%250Aunderstanding%2520and%2520generation%2520phases.%2520This%2520protocol%2520allows%2520us%2520to%2520precisely%250Adiscern%2520whether%2520performance%2520bottlenecks%2520stem%2520from%2520deficiencies%2520in%2520core%250Aabilities%2520or%2520from%2520a%2520failure%2520to%2520integrate%2520them.%2520Through%2520large-scale%2520evaluations%250Aof%252012%2520leading%2520unified%2520models%2520and%25206%2520specialized%2520baselines%252C%2520we%2520find%2520that%2520current%250Aunified%2520models%2520still%2520struggle%2520to%2520achieve%2520effective%2520synergy%252C%2520indicating%2520that%250Aarchitectural%2520unification%2520alone%2520is%2520insufficient.%2520These%2520results%2520highlight%2520the%250Aneed%2520for%2520new%2520training%2520strategies%2520and%2520inductive%2520biases%2520to%2520fully%2520unlock%2520the%250Apotential%2520of%2520unified%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealUnify%3A%20Do%20Unified%20Models%20Truly%20Benefit%20from%20Unification%3F%20A%0A%20%20Comprehensive%20Benchmark&entry.906535625=Yang%20Shi%20and%20Yuhao%20Dong%20and%20Yue%20Ding%20and%20Yuran%20Wang%20and%20Xuanyu%20Zhu%20and%20Sheng%20Zhou%20and%20Wenting%20Liu%20and%20Haochen%20Tian%20and%20Rundong%20Wang%20and%20Huanqian%20Wang%20and%20Zuyan%20Liu%20and%20Bohan%20Zeng%20and%20Ruizhe%20Chen%20and%20Qixun%20Wang%20and%20Zhuoran%20Zhang%20and%20Xinlong%20Chen%20and%20Chengzhuo%20Tong%20and%20Bozhou%20Li%20and%20Chaoyou%20Fu%20and%20Qiang%20Liu%20and%20Haotian%20Wang%20and%20Wenjing%20Yang%20and%20Yuanxing%20Zhang%20and%20Pengfei%20Wan%20and%20Yi-Fan%20Zhang%20and%20Ziwei%20Liu&entry.1292438233=%20%20The%20integration%20of%20visual%20understanding%20and%20generation%20into%20unified%0Amultimodal%20models%20represents%20a%20significant%20stride%20toward%20general-purpose%20AI.%0AHowever%2C%20a%20fundamental%20question%20remains%20unanswered%20by%20existing%20benchmarks%3A%20does%0Athis%20architectural%20unification%20actually%20enable%20synergetic%20interaction%20between%0Athe%20constituent%20capabilities%3F%20Existing%20evaluation%20paradigms%2C%20which%20primarily%0Aassess%20understanding%20and%20generation%20in%20isolation%2C%20are%20insufficient%20for%0Adetermining%20whether%20a%20unified%20model%20can%20leverage%20its%20understanding%20to%20enhance%0Aits%20generation%2C%20or%20use%20generative%20simulation%20to%20facilitate%20deeper%0Acomprehension.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20RealUnify%2C%20a%0Abenchmark%20specifically%20designed%20to%20evaluate%20bidirectional%20capability%20synergy.%0ARealUnify%20comprises%201%2C000%20meticulously%20human-annotated%20instances%20spanning%2010%0Acategories%20and%2032%20subtasks.%20It%20is%20structured%20around%20two%20core%20axes%3A%201%29%0AUnderstanding%20Enhances%20Generation%2C%20which%20requires%20reasoning%20%28e.g.%2C%20commonsense%2C%0Alogic%29%20to%20guide%20image%20generation%2C%20and%202%29%20Generation%20Enhances%20Understanding%2C%0Awhich%20necessitates%20mental%20simulation%20or%20reconstruction%20%28e.g.%2C%20of%20transformed%20or%0Adisordered%20visual%20inputs%29%20to%20solve%20reasoning%20tasks.%20A%20key%20contribution%20is%20our%0Adual-evaluation%20protocol%2C%20which%20combines%20direct%20end-to-end%20assessment%20with%20a%0Adiagnostic%20stepwise%20evaluation%20that%20decomposes%20tasks%20into%20distinct%0Aunderstanding%20and%20generation%20phases.%20This%20protocol%20allows%20us%20to%20precisely%0Adiscern%20whether%20performance%20bottlenecks%20stem%20from%20deficiencies%20in%20core%0Aabilities%20or%20from%20a%20failure%20to%20integrate%20them.%20Through%20large-scale%20evaluations%0Aof%2012%20leading%20unified%20models%20and%206%20specialized%20baselines%2C%20we%20find%20that%20current%0Aunified%20models%20still%20struggle%20to%20achieve%20effective%20synergy%2C%20indicating%20that%0Aarchitectural%20unification%20alone%20is%20insufficient.%20These%20results%20highlight%20the%0Aneed%20for%20new%20training%20strategies%20and%20inductive%20biases%20to%20fully%20unlock%20the%0Apotential%20of%20unified%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24897v1&entry.124074799=Read"},
{"title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts", "author": "Michal Golovanevsky and William Rudman and Michael Lepori and Amir Bar and Ritambhara Singh and Carsten Eickhoff", "abstract": "  Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 99.3% of\ncolor and 80.8% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.\n", "link": "http://arxiv.org/abs/2505.17127v2", "date": "2025-09-29", "relevancy": 2.338, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixels%20Versus%20Priors%3A%20Controlling%20Knowledge%20Priors%20in%20Vision-Language%0A%20%20Models%20through%20Visual%20Counterfacts&body=Title%3A%20Pixels%20Versus%20Priors%3A%20Controlling%20Knowledge%20Priors%20in%20Vision-Language%0A%20%20Models%20through%20Visual%20Counterfacts%0AAuthor%3A%20Michal%20Golovanevsky%20and%20William%20Rudman%20and%20Michael%20Lepori%20and%20Amir%20Bar%20and%20Ritambhara%20Singh%20and%20Carsten%20Eickhoff%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20perform%20well%20on%20tasks%20such%20as%20visual%0Aquestion%20answering%2C%20but%20it%20remains%20unclear%20whether%20their%20reasoning%20relies%20more%0Aon%20memorized%20world%20knowledge%20or%20on%20the%20visual%20information%20present%20in%20the%20input%0Aimage.%20To%20investigate%20this%2C%20we%20introduce%20Visual%20CounterFact%2C%20a%20new%20dataset%20of%0Avisually-realistic%20counterfactuals%20that%20put%20world%20knowledge%20priors%20%28e.g%2C%20red%0Astrawberry%29%20into%20direct%20conflict%20with%20visual%20input%20%28e.g%2C%20blue%20strawberry%29.%0AUsing%20Visual%20CounterFact%2C%20we%20show%20that%20model%20predictions%20initially%20reflect%0Amemorized%20priors%2C%20but%20shift%20toward%20visual%20evidence%20in%20mid-to-late%20layers.%20This%0Adynamic%20reveals%20a%20competition%20between%20the%20two%20modalities%2C%20with%20visual%20input%0Aultimately%20overriding%20priors%20during%20evaluation.%20To%20control%20this%20behavior%2C%20we%0Apropose%20Pixels%20Versus%20Priors%20%28PvP%29%20steering%20vectors%2C%20a%20mechanism%20for%0Acontrolling%20model%20outputs%20toward%20either%20world%20knowledge%20or%20visual%20input%20through%0Aactivation-level%20interventions.%20On%20average%2C%20PvP%20successfully%20shifts%2099.3%25%20of%0Acolor%20and%2080.8%25%20of%20size%20predictions%20from%20priors%20to%20counterfactuals.%20Together%2C%0Athese%20findings%20offer%20new%20tools%20for%20interpreting%20and%20controlling%20factual%0Abehavior%20in%20multimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixels%2520Versus%2520Priors%253A%2520Controlling%2520Knowledge%2520Priors%2520in%2520Vision-Language%250A%2520%2520Models%2520through%2520Visual%2520Counterfacts%26entry.906535625%3DMichal%2520Golovanevsky%2520and%2520William%2520Rudman%2520and%2520Michael%2520Lepori%2520and%2520Amir%2520Bar%2520and%2520Ritambhara%2520Singh%2520and%2520Carsten%2520Eickhoff%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520perform%2520well%2520on%2520tasks%2520such%2520as%2520visual%250Aquestion%2520answering%252C%2520but%2520it%2520remains%2520unclear%2520whether%2520their%2520reasoning%2520relies%2520more%250Aon%2520memorized%2520world%2520knowledge%2520or%2520on%2520the%2520visual%2520information%2520present%2520in%2520the%2520input%250Aimage.%2520To%2520investigate%2520this%252C%2520we%2520introduce%2520Visual%2520CounterFact%252C%2520a%2520new%2520dataset%2520of%250Avisually-realistic%2520counterfactuals%2520that%2520put%2520world%2520knowledge%2520priors%2520%2528e.g%252C%2520red%250Astrawberry%2529%2520into%2520direct%2520conflict%2520with%2520visual%2520input%2520%2528e.g%252C%2520blue%2520strawberry%2529.%250AUsing%2520Visual%2520CounterFact%252C%2520we%2520show%2520that%2520model%2520predictions%2520initially%2520reflect%250Amemorized%2520priors%252C%2520but%2520shift%2520toward%2520visual%2520evidence%2520in%2520mid-to-late%2520layers.%2520This%250Adynamic%2520reveals%2520a%2520competition%2520between%2520the%2520two%2520modalities%252C%2520with%2520visual%2520input%250Aultimately%2520overriding%2520priors%2520during%2520evaluation.%2520To%2520control%2520this%2520behavior%252C%2520we%250Apropose%2520Pixels%2520Versus%2520Priors%2520%2528PvP%2529%2520steering%2520vectors%252C%2520a%2520mechanism%2520for%250Acontrolling%2520model%2520outputs%2520toward%2520either%2520world%2520knowledge%2520or%2520visual%2520input%2520through%250Aactivation-level%2520interventions.%2520On%2520average%252C%2520PvP%2520successfully%2520shifts%252099.3%2525%2520of%250Acolor%2520and%252080.8%2525%2520of%2520size%2520predictions%2520from%2520priors%2520to%2520counterfactuals.%2520Together%252C%250Athese%2520findings%2520offer%2520new%2520tools%2520for%2520interpreting%2520and%2520controlling%2520factual%250Abehavior%2520in%2520multimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixels%20Versus%20Priors%3A%20Controlling%20Knowledge%20Priors%20in%20Vision-Language%0A%20%20Models%20through%20Visual%20Counterfacts&entry.906535625=Michal%20Golovanevsky%20and%20William%20Rudman%20and%20Michael%20Lepori%20and%20Amir%20Bar%20and%20Ritambhara%20Singh%20and%20Carsten%20Eickhoff&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20perform%20well%20on%20tasks%20such%20as%20visual%0Aquestion%20answering%2C%20but%20it%20remains%20unclear%20whether%20their%20reasoning%20relies%20more%0Aon%20memorized%20world%20knowledge%20or%20on%20the%20visual%20information%20present%20in%20the%20input%0Aimage.%20To%20investigate%20this%2C%20we%20introduce%20Visual%20CounterFact%2C%20a%20new%20dataset%20of%0Avisually-realistic%20counterfactuals%20that%20put%20world%20knowledge%20priors%20%28e.g%2C%20red%0Astrawberry%29%20into%20direct%20conflict%20with%20visual%20input%20%28e.g%2C%20blue%20strawberry%29.%0AUsing%20Visual%20CounterFact%2C%20we%20show%20that%20model%20predictions%20initially%20reflect%0Amemorized%20priors%2C%20but%20shift%20toward%20visual%20evidence%20in%20mid-to-late%20layers.%20This%0Adynamic%20reveals%20a%20competition%20between%20the%20two%20modalities%2C%20with%20visual%20input%0Aultimately%20overriding%20priors%20during%20evaluation.%20To%20control%20this%20behavior%2C%20we%0Apropose%20Pixels%20Versus%20Priors%20%28PvP%29%20steering%20vectors%2C%20a%20mechanism%20for%0Acontrolling%20model%20outputs%20toward%20either%20world%20knowledge%20or%20visual%20input%20through%0Aactivation-level%20interventions.%20On%20average%2C%20PvP%20successfully%20shifts%2099.3%25%20of%0Acolor%20and%2080.8%25%20of%20size%20predictions%20from%20priors%20to%20counterfactuals.%20Together%2C%0Athese%20findings%20offer%20new%20tools%20for%20interpreting%20and%20controlling%20factual%0Abehavior%20in%20multimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17127v2&entry.124074799=Read"},
{"title": "Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of\n  Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence", "author": "Sanish Suwal and Dipkamal Bhusal and Michael Clifford and Nidhi Rastogi", "abstract": "  Prior works have shown that neural networks can be heavily pruned while\npreserving performance, but the impact of pruning on model interpretability\nremains unclear. In this work, we investigate how magnitude-based pruning\nfollowed by fine-tuning affects both low-level saliency maps and high-level\nconcept representations. Using a ResNet-18 trained on ImageNette, we compare\npost-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)\nacross pruning levels, evaluating sparsity and faithfulness. We further apply\nCRAFT-based concept extraction to track changes in semantic coherence of\nlearned concepts. Our results show that light-to-moderate pruning improves\nsaliency-map focus and faithfulness while retaining distinct, semantically\nmeaningful concepts. In contrast, aggressive pruning merges heterogeneous\nfeatures, reducing saliency map sparsity and concept coherence despite\nmaintaining accuracy. These findings suggest that while pruning can shape\ninternal representations toward more human-aligned attention patterns,\nexcessive pruning undermines interpretability.\n", "link": "http://arxiv.org/abs/2509.21387v2", "date": "2025-09-29", "relevancy": 2.3295, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Sparse%20Subnetworks%20Exhibit%20Cognitively%20Aligned%20Attention%3F%20Effects%20of%0A%20%20Pruning%20on%20Saliency%20Map%20Fidelity%2C%20Sparsity%2C%20and%20Concept%20Coherence&body=Title%3A%20Do%20Sparse%20Subnetworks%20Exhibit%20Cognitively%20Aligned%20Attention%3F%20Effects%20of%0A%20%20Pruning%20on%20Saliency%20Map%20Fidelity%2C%20Sparsity%2C%20and%20Concept%20Coherence%0AAuthor%3A%20Sanish%20Suwal%20and%20Dipkamal%20Bhusal%20and%20Michael%20Clifford%20and%20Nidhi%20Rastogi%0AAbstract%3A%20%20%20Prior%20works%20have%20shown%20that%20neural%20networks%20can%20be%20heavily%20pruned%20while%0Apreserving%20performance%2C%20but%20the%20impact%20of%20pruning%20on%20model%20interpretability%0Aremains%20unclear.%20In%20this%20work%2C%20we%20investigate%20how%20magnitude-based%20pruning%0Afollowed%20by%20fine-tuning%20affects%20both%20low-level%20saliency%20maps%20and%20high-level%0Aconcept%20representations.%20Using%20a%20ResNet-18%20trained%20on%20ImageNette%2C%20we%20compare%0Apost-hoc%20explanations%20from%20Vanilla%20Gradients%20%28VG%29%20and%20Integrated%20Gradients%20%28IG%29%0Aacross%20pruning%20levels%2C%20evaluating%20sparsity%20and%20faithfulness.%20We%20further%20apply%0ACRAFT-based%20concept%20extraction%20to%20track%20changes%20in%20semantic%20coherence%20of%0Alearned%20concepts.%20Our%20results%20show%20that%20light-to-moderate%20pruning%20improves%0Asaliency-map%20focus%20and%20faithfulness%20while%20retaining%20distinct%2C%20semantically%0Ameaningful%20concepts.%20In%20contrast%2C%20aggressive%20pruning%20merges%20heterogeneous%0Afeatures%2C%20reducing%20saliency%20map%20sparsity%20and%20concept%20coherence%20despite%0Amaintaining%20accuracy.%20These%20findings%20suggest%20that%20while%20pruning%20can%20shape%0Ainternal%20representations%20toward%20more%20human-aligned%20attention%20patterns%2C%0Aexcessive%20pruning%20undermines%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Sparse%2520Subnetworks%2520Exhibit%2520Cognitively%2520Aligned%2520Attention%253F%2520Effects%2520of%250A%2520%2520Pruning%2520on%2520Saliency%2520Map%2520Fidelity%252C%2520Sparsity%252C%2520and%2520Concept%2520Coherence%26entry.906535625%3DSanish%2520Suwal%2520and%2520Dipkamal%2520Bhusal%2520and%2520Michael%2520Clifford%2520and%2520Nidhi%2520Rastogi%26entry.1292438233%3D%2520%2520Prior%2520works%2520have%2520shown%2520that%2520neural%2520networks%2520can%2520be%2520heavily%2520pruned%2520while%250Apreserving%2520performance%252C%2520but%2520the%2520impact%2520of%2520pruning%2520on%2520model%2520interpretability%250Aremains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520magnitude-based%2520pruning%250Afollowed%2520by%2520fine-tuning%2520affects%2520both%2520low-level%2520saliency%2520maps%2520and%2520high-level%250Aconcept%2520representations.%2520Using%2520a%2520ResNet-18%2520trained%2520on%2520ImageNette%252C%2520we%2520compare%250Apost-hoc%2520explanations%2520from%2520Vanilla%2520Gradients%2520%2528VG%2529%2520and%2520Integrated%2520Gradients%2520%2528IG%2529%250Aacross%2520pruning%2520levels%252C%2520evaluating%2520sparsity%2520and%2520faithfulness.%2520We%2520further%2520apply%250ACRAFT-based%2520concept%2520extraction%2520to%2520track%2520changes%2520in%2520semantic%2520coherence%2520of%250Alearned%2520concepts.%2520Our%2520results%2520show%2520that%2520light-to-moderate%2520pruning%2520improves%250Asaliency-map%2520focus%2520and%2520faithfulness%2520while%2520retaining%2520distinct%252C%2520semantically%250Ameaningful%2520concepts.%2520In%2520contrast%252C%2520aggressive%2520pruning%2520merges%2520heterogeneous%250Afeatures%252C%2520reducing%2520saliency%2520map%2520sparsity%2520and%2520concept%2520coherence%2520despite%250Amaintaining%2520accuracy.%2520These%2520findings%2520suggest%2520that%2520while%2520pruning%2520can%2520shape%250Ainternal%2520representations%2520toward%2520more%2520human-aligned%2520attention%2520patterns%252C%250Aexcessive%2520pruning%2520undermines%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Sparse%20Subnetworks%20Exhibit%20Cognitively%20Aligned%20Attention%3F%20Effects%20of%0A%20%20Pruning%20on%20Saliency%20Map%20Fidelity%2C%20Sparsity%2C%20and%20Concept%20Coherence&entry.906535625=Sanish%20Suwal%20and%20Dipkamal%20Bhusal%20and%20Michael%20Clifford%20and%20Nidhi%20Rastogi&entry.1292438233=%20%20Prior%20works%20have%20shown%20that%20neural%20networks%20can%20be%20heavily%20pruned%20while%0Apreserving%20performance%2C%20but%20the%20impact%20of%20pruning%20on%20model%20interpretability%0Aremains%20unclear.%20In%20this%20work%2C%20we%20investigate%20how%20magnitude-based%20pruning%0Afollowed%20by%20fine-tuning%20affects%20both%20low-level%20saliency%20maps%20and%20high-level%0Aconcept%20representations.%20Using%20a%20ResNet-18%20trained%20on%20ImageNette%2C%20we%20compare%0Apost-hoc%20explanations%20from%20Vanilla%20Gradients%20%28VG%29%20and%20Integrated%20Gradients%20%28IG%29%0Aacross%20pruning%20levels%2C%20evaluating%20sparsity%20and%20faithfulness.%20We%20further%20apply%0ACRAFT-based%20concept%20extraction%20to%20track%20changes%20in%20semantic%20coherence%20of%0Alearned%20concepts.%20Our%20results%20show%20that%20light-to-moderate%20pruning%20improves%0Asaliency-map%20focus%20and%20faithfulness%20while%20retaining%20distinct%2C%20semantically%0Ameaningful%20concepts.%20In%20contrast%2C%20aggressive%20pruning%20merges%20heterogeneous%0Afeatures%2C%20reducing%20saliency%20map%20sparsity%20and%20concept%20coherence%20despite%0Amaintaining%20accuracy.%20These%20findings%20suggest%20that%20while%20pruning%20can%20shape%0Ainternal%20representations%20toward%20more%20human-aligned%20attention%20patterns%2C%0Aexcessive%20pruning%20undermines%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21387v2&entry.124074799=Read"},
{"title": "Visual serial processing deficits explain divergences in human and VLM\n  reasoning", "author": "Nicholas Budny and Kia Ghods and Declan Campbell and Raja Marjieh and Amogh Joshi and Sreejan Kumar and Jonathan D. Cohen and Taylor W. Webb and Thomas L. Griffiths", "abstract": "  Why do Vision Language Models (VLMs), despite success on standard benchmarks,\noften fail to match human performance on surprisingly simple visual reasoning\ntasks? While the underlying computational principles are still debated, we\nhypothesize that a crucial factor is a deficit in visually-grounded serial\nprocessing. To test this hypothesis, we compared human and VLM performance\nacross tasks designed to vary serial processing demands in three distinct\ndomains: geometric reasoning, perceptual enumeration, and mental rotation.\nTasks within each domain varied serial processing load by manipulating factors\nsuch as geometric concept complexity, perceptual individuation load, and\ntransformation difficulty. Across all domains, our results revealed a\nconsistent pattern: decreased VLM accuracy was strongly correlated with\nincreased human reaction time (used as a proxy for serial processing load). As\ntasks require more demanding serial processing -- whether composing concepts,\nenumerating items, or performing mental transformations -- the VLM-human\nperformance gap widens reliably. These findings support our hypothesis,\nindicating that limitations in serial, visually grounded reasoning represent a\nfundamental bottleneck that distinguishes current VLMs from humans.\n", "link": "http://arxiv.org/abs/2509.25142v1", "date": "2025-09-29", "relevancy": 2.2998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20serial%20processing%20deficits%20explain%20divergences%20in%20human%20and%20VLM%0A%20%20reasoning&body=Title%3A%20Visual%20serial%20processing%20deficits%20explain%20divergences%20in%20human%20and%20VLM%0A%20%20reasoning%0AAuthor%3A%20Nicholas%20Budny%20and%20Kia%20Ghods%20and%20Declan%20Campbell%20and%20Raja%20Marjieh%20and%20Amogh%20Joshi%20and%20Sreejan%20Kumar%20and%20Jonathan%20D.%20Cohen%20and%20Taylor%20W.%20Webb%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Why%20do%20Vision%20Language%20Models%20%28VLMs%29%2C%20despite%20success%20on%20standard%20benchmarks%2C%0Aoften%20fail%20to%20match%20human%20performance%20on%20surprisingly%20simple%20visual%20reasoning%0Atasks%3F%20While%20the%20underlying%20computational%20principles%20are%20still%20debated%2C%20we%0Ahypothesize%20that%20a%20crucial%20factor%20is%20a%20deficit%20in%20visually-grounded%20serial%0Aprocessing.%20To%20test%20this%20hypothesis%2C%20we%20compared%20human%20and%20VLM%20performance%0Aacross%20tasks%20designed%20to%20vary%20serial%20processing%20demands%20in%20three%20distinct%0Adomains%3A%20geometric%20reasoning%2C%20perceptual%20enumeration%2C%20and%20mental%20rotation.%0ATasks%20within%20each%20domain%20varied%20serial%20processing%20load%20by%20manipulating%20factors%0Asuch%20as%20geometric%20concept%20complexity%2C%20perceptual%20individuation%20load%2C%20and%0Atransformation%20difficulty.%20Across%20all%20domains%2C%20our%20results%20revealed%20a%0Aconsistent%20pattern%3A%20decreased%20VLM%20accuracy%20was%20strongly%20correlated%20with%0Aincreased%20human%20reaction%20time%20%28used%20as%20a%20proxy%20for%20serial%20processing%20load%29.%20As%0Atasks%20require%20more%20demanding%20serial%20processing%20--%20whether%20composing%20concepts%2C%0Aenumerating%20items%2C%20or%20performing%20mental%20transformations%20--%20the%20VLM-human%0Aperformance%20gap%20widens%20reliably.%20These%20findings%20support%20our%20hypothesis%2C%0Aindicating%20that%20limitations%20in%20serial%2C%20visually%20grounded%20reasoning%20represent%20a%0Afundamental%20bottleneck%20that%20distinguishes%20current%20VLMs%20from%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520serial%2520processing%2520deficits%2520explain%2520divergences%2520in%2520human%2520and%2520VLM%250A%2520%2520reasoning%26entry.906535625%3DNicholas%2520Budny%2520and%2520Kia%2520Ghods%2520and%2520Declan%2520Campbell%2520and%2520Raja%2520Marjieh%2520and%2520Amogh%2520Joshi%2520and%2520Sreejan%2520Kumar%2520and%2520Jonathan%2520D.%2520Cohen%2520and%2520Taylor%2520W.%2520Webb%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Why%2520do%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520despite%2520success%2520on%2520standard%2520benchmarks%252C%250Aoften%2520fail%2520to%2520match%2520human%2520performance%2520on%2520surprisingly%2520simple%2520visual%2520reasoning%250Atasks%253F%2520While%2520the%2520underlying%2520computational%2520principles%2520are%2520still%2520debated%252C%2520we%250Ahypothesize%2520that%2520a%2520crucial%2520factor%2520is%2520a%2520deficit%2520in%2520visually-grounded%2520serial%250Aprocessing.%2520To%2520test%2520this%2520hypothesis%252C%2520we%2520compared%2520human%2520and%2520VLM%2520performance%250Aacross%2520tasks%2520designed%2520to%2520vary%2520serial%2520processing%2520demands%2520in%2520three%2520distinct%250Adomains%253A%2520geometric%2520reasoning%252C%2520perceptual%2520enumeration%252C%2520and%2520mental%2520rotation.%250ATasks%2520within%2520each%2520domain%2520varied%2520serial%2520processing%2520load%2520by%2520manipulating%2520factors%250Asuch%2520as%2520geometric%2520concept%2520complexity%252C%2520perceptual%2520individuation%2520load%252C%2520and%250Atransformation%2520difficulty.%2520Across%2520all%2520domains%252C%2520our%2520results%2520revealed%2520a%250Aconsistent%2520pattern%253A%2520decreased%2520VLM%2520accuracy%2520was%2520strongly%2520correlated%2520with%250Aincreased%2520human%2520reaction%2520time%2520%2528used%2520as%2520a%2520proxy%2520for%2520serial%2520processing%2520load%2529.%2520As%250Atasks%2520require%2520more%2520demanding%2520serial%2520processing%2520--%2520whether%2520composing%2520concepts%252C%250Aenumerating%2520items%252C%2520or%2520performing%2520mental%2520transformations%2520--%2520the%2520VLM-human%250Aperformance%2520gap%2520widens%2520reliably.%2520These%2520findings%2520support%2520our%2520hypothesis%252C%250Aindicating%2520that%2520limitations%2520in%2520serial%252C%2520visually%2520grounded%2520reasoning%2520represent%2520a%250Afundamental%2520bottleneck%2520that%2520distinguishes%2520current%2520VLMs%2520from%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20serial%20processing%20deficits%20explain%20divergences%20in%20human%20and%20VLM%0A%20%20reasoning&entry.906535625=Nicholas%20Budny%20and%20Kia%20Ghods%20and%20Declan%20Campbell%20and%20Raja%20Marjieh%20and%20Amogh%20Joshi%20and%20Sreejan%20Kumar%20and%20Jonathan%20D.%20Cohen%20and%20Taylor%20W.%20Webb%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Why%20do%20Vision%20Language%20Models%20%28VLMs%29%2C%20despite%20success%20on%20standard%20benchmarks%2C%0Aoften%20fail%20to%20match%20human%20performance%20on%20surprisingly%20simple%20visual%20reasoning%0Atasks%3F%20While%20the%20underlying%20computational%20principles%20are%20still%20debated%2C%20we%0Ahypothesize%20that%20a%20crucial%20factor%20is%20a%20deficit%20in%20visually-grounded%20serial%0Aprocessing.%20To%20test%20this%20hypothesis%2C%20we%20compared%20human%20and%20VLM%20performance%0Aacross%20tasks%20designed%20to%20vary%20serial%20processing%20demands%20in%20three%20distinct%0Adomains%3A%20geometric%20reasoning%2C%20perceptual%20enumeration%2C%20and%20mental%20rotation.%0ATasks%20within%20each%20domain%20varied%20serial%20processing%20load%20by%20manipulating%20factors%0Asuch%20as%20geometric%20concept%20complexity%2C%20perceptual%20individuation%20load%2C%20and%0Atransformation%20difficulty.%20Across%20all%20domains%2C%20our%20results%20revealed%20a%0Aconsistent%20pattern%3A%20decreased%20VLM%20accuracy%20was%20strongly%20correlated%20with%0Aincreased%20human%20reaction%20time%20%28used%20as%20a%20proxy%20for%20serial%20processing%20load%29.%20As%0Atasks%20require%20more%20demanding%20serial%20processing%20--%20whether%20composing%20concepts%2C%0Aenumerating%20items%2C%20or%20performing%20mental%20transformations%20--%20the%20VLM-human%0Aperformance%20gap%20widens%20reliably.%20These%20findings%20support%20our%20hypothesis%2C%0Aindicating%20that%20limitations%20in%20serial%2C%20visually%20grounded%20reasoning%20represent%20a%0Afundamental%20bottleneck%20that%20distinguishes%20current%20VLMs%20from%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25142v1&entry.124074799=Read"},
{"title": "Scaling Synthetic Task Generation for Agents via Exploration", "author": "Ram Ramrakhya and Andrew Szot and Omar Attia and Yuhao Yang and Anh Nguyen and Bogdan Mazoure and Zhe Gan and Harsh Agrawal and Alexander Toshev", "abstract": "  Post-Training Multimodal Large Language Models (MLLMs) to build interactive\nagents holds promise across domains such as computer-use, web navigation, and\nrobotics. A key challenge in scaling such post-training is lack of high-quality\ndownstream agentic task datasets with tasks that are diverse, feasible, and\nverifiable. Existing approaches for task generation rely heavily on human\nannotation or prompting MLLM with limited downstream environment information,\nwhich is either costly or poorly scalable as it yield tasks with limited\ncoverage. To remedy this, we present AutoPlay, a scalable pipeline for task\ngeneration that explicitly explores interactive environments to discover\npossible interactions and current state information to synthesize\nenvironment-grounded tasks. AutoPlay operates in two stages: (i) an exploration\nphase, where an MLLM explorer agent systematically uncovers novel environment\nstates and functionalities, and (ii) a task generation phase, where a task\ngenerator leverages exploration trajectories and a set of task guideline\nprompts as context to synthesize diverse, executable, and verifiable tasks. We\nshow AutoPlay generates 20k tasks across 20 Android applications and 10k tasks\nacross 13 applications Ubuntu applications to train mobile-use and computer-use\nagents. AutoPlay generated tasks enable large-scale task demonstration\nsynthesis without human annotation by employing an MLLM task executor and\nverifier. This data enables training MLLM-based UI agents that improve success\nrates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In\naddition, AutoPlay generated tasks combined with MLLM verifier-based rewards\nenable scaling reinforcement learning training of UI agents, leading to an\nadditional $5.7\\%$ gain. coverage. These results establish AutoPlay as a\nscalable approach for post-training capable MLLM agents reducing reliance on\nhuman annotation.\n", "link": "http://arxiv.org/abs/2509.25047v1", "date": "2025-09-29", "relevancy": 2.2947, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6158}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5898}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Synthetic%20Task%20Generation%20for%20Agents%20via%20Exploration&body=Title%3A%20Scaling%20Synthetic%20Task%20Generation%20for%20Agents%20via%20Exploration%0AAuthor%3A%20Ram%20Ramrakhya%20and%20Andrew%20Szot%20and%20Omar%20Attia%20and%20Yuhao%20Yang%20and%20Anh%20Nguyen%20and%20Bogdan%20Mazoure%20and%20Zhe%20Gan%20and%20Harsh%20Agrawal%20and%20Alexander%20Toshev%0AAbstract%3A%20%20%20Post-Training%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20build%20interactive%0Aagents%20holds%20promise%20across%20domains%20such%20as%20computer-use%2C%20web%20navigation%2C%20and%0Arobotics.%20A%20key%20challenge%20in%20scaling%20such%20post-training%20is%20lack%20of%20high-quality%0Adownstream%20agentic%20task%20datasets%20with%20tasks%20that%20are%20diverse%2C%20feasible%2C%20and%0Averifiable.%20Existing%20approaches%20for%20task%20generation%20rely%20heavily%20on%20human%0Aannotation%20or%20prompting%20MLLM%20with%20limited%20downstream%20environment%20information%2C%0Awhich%20is%20either%20costly%20or%20poorly%20scalable%20as%20it%20yield%20tasks%20with%20limited%0Acoverage.%20To%20remedy%20this%2C%20we%20present%20AutoPlay%2C%20a%20scalable%20pipeline%20for%20task%0Ageneration%20that%20explicitly%20explores%20interactive%20environments%20to%20discover%0Apossible%20interactions%20and%20current%20state%20information%20to%20synthesize%0Aenvironment-grounded%20tasks.%20AutoPlay%20operates%20in%20two%20stages%3A%20%28i%29%20an%20exploration%0Aphase%2C%20where%20an%20MLLM%20explorer%20agent%20systematically%20uncovers%20novel%20environment%0Astates%20and%20functionalities%2C%20and%20%28ii%29%20a%20task%20generation%20phase%2C%20where%20a%20task%0Agenerator%20leverages%20exploration%20trajectories%20and%20a%20set%20of%20task%20guideline%0Aprompts%20as%20context%20to%20synthesize%20diverse%2C%20executable%2C%20and%20verifiable%20tasks.%20We%0Ashow%20AutoPlay%20generates%2020k%20tasks%20across%2020%20Android%20applications%20and%2010k%20tasks%0Aacross%2013%20applications%20Ubuntu%20applications%20to%20train%20mobile-use%20and%20computer-use%0Aagents.%20AutoPlay%20generated%20tasks%20enable%20large-scale%20task%20demonstration%0Asynthesis%20without%20human%20annotation%20by%20employing%20an%20MLLM%20task%20executor%20and%0Averifier.%20This%20data%20enables%20training%20MLLM-based%20UI%20agents%20that%20improve%20success%0Arates%20up%20to%20%2420.0%5C%25%24%20on%20mobile-use%20and%20%2410.9%5C%25%24%20on%20computer-use%20scenarios.%20In%0Aaddition%2C%20AutoPlay%20generated%20tasks%20combined%20with%20MLLM%20verifier-based%20rewards%0Aenable%20scaling%20reinforcement%20learning%20training%20of%20UI%20agents%2C%20leading%20to%20an%0Aadditional%20%245.7%5C%25%24%20gain.%20coverage.%20These%20results%20establish%20AutoPlay%20as%20a%0Ascalable%20approach%20for%20post-training%20capable%20MLLM%20agents%20reducing%20reliance%20on%0Ahuman%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Synthetic%2520Task%2520Generation%2520for%2520Agents%2520via%2520Exploration%26entry.906535625%3DRam%2520Ramrakhya%2520and%2520Andrew%2520Szot%2520and%2520Omar%2520Attia%2520and%2520Yuhao%2520Yang%2520and%2520Anh%2520Nguyen%2520and%2520Bogdan%2520Mazoure%2520and%2520Zhe%2520Gan%2520and%2520Harsh%2520Agrawal%2520and%2520Alexander%2520Toshev%26entry.1292438233%3D%2520%2520Post-Training%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520build%2520interactive%250Aagents%2520holds%2520promise%2520across%2520domains%2520such%2520as%2520computer-use%252C%2520web%2520navigation%252C%2520and%250Arobotics.%2520A%2520key%2520challenge%2520in%2520scaling%2520such%2520post-training%2520is%2520lack%2520of%2520high-quality%250Adownstream%2520agentic%2520task%2520datasets%2520with%2520tasks%2520that%2520are%2520diverse%252C%2520feasible%252C%2520and%250Averifiable.%2520Existing%2520approaches%2520for%2520task%2520generation%2520rely%2520heavily%2520on%2520human%250Aannotation%2520or%2520prompting%2520MLLM%2520with%2520limited%2520downstream%2520environment%2520information%252C%250Awhich%2520is%2520either%2520costly%2520or%2520poorly%2520scalable%2520as%2520it%2520yield%2520tasks%2520with%2520limited%250Acoverage.%2520To%2520remedy%2520this%252C%2520we%2520present%2520AutoPlay%252C%2520a%2520scalable%2520pipeline%2520for%2520task%250Ageneration%2520that%2520explicitly%2520explores%2520interactive%2520environments%2520to%2520discover%250Apossible%2520interactions%2520and%2520current%2520state%2520information%2520to%2520synthesize%250Aenvironment-grounded%2520tasks.%2520AutoPlay%2520operates%2520in%2520two%2520stages%253A%2520%2528i%2529%2520an%2520exploration%250Aphase%252C%2520where%2520an%2520MLLM%2520explorer%2520agent%2520systematically%2520uncovers%2520novel%2520environment%250Astates%2520and%2520functionalities%252C%2520and%2520%2528ii%2529%2520a%2520task%2520generation%2520phase%252C%2520where%2520a%2520task%250Agenerator%2520leverages%2520exploration%2520trajectories%2520and%2520a%2520set%2520of%2520task%2520guideline%250Aprompts%2520as%2520context%2520to%2520synthesize%2520diverse%252C%2520executable%252C%2520and%2520verifiable%2520tasks.%2520We%250Ashow%2520AutoPlay%2520generates%252020k%2520tasks%2520across%252020%2520Android%2520applications%2520and%252010k%2520tasks%250Aacross%252013%2520applications%2520Ubuntu%2520applications%2520to%2520train%2520mobile-use%2520and%2520computer-use%250Aagents.%2520AutoPlay%2520generated%2520tasks%2520enable%2520large-scale%2520task%2520demonstration%250Asynthesis%2520without%2520human%2520annotation%2520by%2520employing%2520an%2520MLLM%2520task%2520executor%2520and%250Averifier.%2520This%2520data%2520enables%2520training%2520MLLM-based%2520UI%2520agents%2520that%2520improve%2520success%250Arates%2520up%2520to%2520%252420.0%255C%2525%2524%2520on%2520mobile-use%2520and%2520%252410.9%255C%2525%2524%2520on%2520computer-use%2520scenarios.%2520In%250Aaddition%252C%2520AutoPlay%2520generated%2520tasks%2520combined%2520with%2520MLLM%2520verifier-based%2520rewards%250Aenable%2520scaling%2520reinforcement%2520learning%2520training%2520of%2520UI%2520agents%252C%2520leading%2520to%2520an%250Aadditional%2520%25245.7%255C%2525%2524%2520gain.%2520coverage.%2520These%2520results%2520establish%2520AutoPlay%2520as%2520a%250Ascalable%2520approach%2520for%2520post-training%2520capable%2520MLLM%2520agents%2520reducing%2520reliance%2520on%250Ahuman%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Synthetic%20Task%20Generation%20for%20Agents%20via%20Exploration&entry.906535625=Ram%20Ramrakhya%20and%20Andrew%20Szot%20and%20Omar%20Attia%20and%20Yuhao%20Yang%20and%20Anh%20Nguyen%20and%20Bogdan%20Mazoure%20and%20Zhe%20Gan%20and%20Harsh%20Agrawal%20and%20Alexander%20Toshev&entry.1292438233=%20%20Post-Training%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20build%20interactive%0Aagents%20holds%20promise%20across%20domains%20such%20as%20computer-use%2C%20web%20navigation%2C%20and%0Arobotics.%20A%20key%20challenge%20in%20scaling%20such%20post-training%20is%20lack%20of%20high-quality%0Adownstream%20agentic%20task%20datasets%20with%20tasks%20that%20are%20diverse%2C%20feasible%2C%20and%0Averifiable.%20Existing%20approaches%20for%20task%20generation%20rely%20heavily%20on%20human%0Aannotation%20or%20prompting%20MLLM%20with%20limited%20downstream%20environment%20information%2C%0Awhich%20is%20either%20costly%20or%20poorly%20scalable%20as%20it%20yield%20tasks%20with%20limited%0Acoverage.%20To%20remedy%20this%2C%20we%20present%20AutoPlay%2C%20a%20scalable%20pipeline%20for%20task%0Ageneration%20that%20explicitly%20explores%20interactive%20environments%20to%20discover%0Apossible%20interactions%20and%20current%20state%20information%20to%20synthesize%0Aenvironment-grounded%20tasks.%20AutoPlay%20operates%20in%20two%20stages%3A%20%28i%29%20an%20exploration%0Aphase%2C%20where%20an%20MLLM%20explorer%20agent%20systematically%20uncovers%20novel%20environment%0Astates%20and%20functionalities%2C%20and%20%28ii%29%20a%20task%20generation%20phase%2C%20where%20a%20task%0Agenerator%20leverages%20exploration%20trajectories%20and%20a%20set%20of%20task%20guideline%0Aprompts%20as%20context%20to%20synthesize%20diverse%2C%20executable%2C%20and%20verifiable%20tasks.%20We%0Ashow%20AutoPlay%20generates%2020k%20tasks%20across%2020%20Android%20applications%20and%2010k%20tasks%0Aacross%2013%20applications%20Ubuntu%20applications%20to%20train%20mobile-use%20and%20computer-use%0Aagents.%20AutoPlay%20generated%20tasks%20enable%20large-scale%20task%20demonstration%0Asynthesis%20without%20human%20annotation%20by%20employing%20an%20MLLM%20task%20executor%20and%0Averifier.%20This%20data%20enables%20training%20MLLM-based%20UI%20agents%20that%20improve%20success%0Arates%20up%20to%20%2420.0%5C%25%24%20on%20mobile-use%20and%20%2410.9%5C%25%24%20on%20computer-use%20scenarios.%20In%0Aaddition%2C%20AutoPlay%20generated%20tasks%20combined%20with%20MLLM%20verifier-based%20rewards%0Aenable%20scaling%20reinforcement%20learning%20training%20of%20UI%20agents%2C%20leading%20to%20an%0Aadditional%20%245.7%5C%25%24%20gain.%20coverage.%20These%20results%20establish%20AutoPlay%20as%20a%0Ascalable%20approach%20for%20post-training%20capable%20MLLM%20agents%20reducing%20reliance%20on%0Ahuman%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25047v1&entry.124074799=Read"},
{"title": "The Physical Basis of Prediction: World Model Formation in Neural\n  Organoids via an LLM-Generated Curriculum", "author": "Brennen Hill", "abstract": "  The capacity of an embodied agent to understand, predict, and interact with\nits environment is fundamentally contingent on an internal world model. This\npaper introduces a novel framework for investigating the formation and\nadaptation of such world models within a biological substrate: human neural\norganoids. We present a curriculum of three scalable, closed-loop virtual\nenvironments designed to train these biological agents and probe the underlying\nsynaptic mechanisms of learning, such as long-term potentiation (LTP) and\nlong-term depression (LTD). We detail the design of three distinct task\nenvironments that demand progressively more sophisticated world models for\nsuccessful decision-making: (1) a conditional avoidance task for learning\nstatic state-action contingencies, (2) a one-dimensional predator-prey scenario\nfor goal-directed interaction, and (3) a replication of the classic Pong game\nfor modeling dynamic, continuous-time systems. For each environment, we\nformalize the state and action spaces, the sensory encoding and motor decoding\nmechanisms, and the feedback protocols based on predictable (reward) and\nunpredictable (punishment) stimulation, which serve to drive model refinement.\nIn a significant methodological advance, we propose a meta-learning approach\nwhere a Large Language Model automates the generative design and optimization\nof experimental protocols, thereby scaling the process of environment and\ncurriculum design. Finally, we outline a multi-modal evaluation strategy that\nmoves beyond task performance to directly measure the physical correlates of\nthe learned world model by quantifying synaptic plasticity at\nelectrophysiological, cellular, and molecular levels. This work bridges the gap\nbetween model-based reinforcement learning and computational neuroscience,\noffering a unique platform for studying embodiment, decision-making, and the\nphysical basis of intelligence.\n", "link": "http://arxiv.org/abs/2509.04633v2", "date": "2025-09-29", "relevancy": 2.2856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6277}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Physical%20Basis%20of%20Prediction%3A%20World%20Model%20Formation%20in%20Neural%0A%20%20Organoids%20via%20an%20LLM-Generated%20Curriculum&body=Title%3A%20The%20Physical%20Basis%20of%20Prediction%3A%20World%20Model%20Formation%20in%20Neural%0A%20%20Organoids%20via%20an%20LLM-Generated%20Curriculum%0AAuthor%3A%20Brennen%20Hill%0AAbstract%3A%20%20%20The%20capacity%20of%20an%20embodied%20agent%20to%20understand%2C%20predict%2C%20and%20interact%20with%0Aits%20environment%20is%20fundamentally%20contingent%20on%20an%20internal%20world%20model.%20This%0Apaper%20introduces%20a%20novel%20framework%20for%20investigating%20the%20formation%20and%0Aadaptation%20of%20such%20world%20models%20within%20a%20biological%20substrate%3A%20human%20neural%0Aorganoids.%20We%20present%20a%20curriculum%20of%20three%20scalable%2C%20closed-loop%20virtual%0Aenvironments%20designed%20to%20train%20these%20biological%20agents%20and%20probe%20the%20underlying%0Asynaptic%20mechanisms%20of%20learning%2C%20such%20as%20long-term%20potentiation%20%28LTP%29%20and%0Along-term%20depression%20%28LTD%29.%20We%20detail%20the%20design%20of%20three%20distinct%20task%0Aenvironments%20that%20demand%20progressively%20more%20sophisticated%20world%20models%20for%0Asuccessful%20decision-making%3A%20%281%29%20a%20conditional%20avoidance%20task%20for%20learning%0Astatic%20state-action%20contingencies%2C%20%282%29%20a%20one-dimensional%20predator-prey%20scenario%0Afor%20goal-directed%20interaction%2C%20and%20%283%29%20a%20replication%20of%20the%20classic%20Pong%20game%0Afor%20modeling%20dynamic%2C%20continuous-time%20systems.%20For%20each%20environment%2C%20we%0Aformalize%20the%20state%20and%20action%20spaces%2C%20the%20sensory%20encoding%20and%20motor%20decoding%0Amechanisms%2C%20and%20the%20feedback%20protocols%20based%20on%20predictable%20%28reward%29%20and%0Aunpredictable%20%28punishment%29%20stimulation%2C%20which%20serve%20to%20drive%20model%20refinement.%0AIn%20a%20significant%20methodological%20advance%2C%20we%20propose%20a%20meta-learning%20approach%0Awhere%20a%20Large%20Language%20Model%20automates%20the%20generative%20design%20and%20optimization%0Aof%20experimental%20protocols%2C%20thereby%20scaling%20the%20process%20of%20environment%20and%0Acurriculum%20design.%20Finally%2C%20we%20outline%20a%20multi-modal%20evaluation%20strategy%20that%0Amoves%20beyond%20task%20performance%20to%20directly%20measure%20the%20physical%20correlates%20of%0Athe%20learned%20world%20model%20by%20quantifying%20synaptic%20plasticity%20at%0Aelectrophysiological%2C%20cellular%2C%20and%20molecular%20levels.%20This%20work%20bridges%20the%20gap%0Abetween%20model-based%20reinforcement%20learning%20and%20computational%20neuroscience%2C%0Aoffering%20a%20unique%20platform%20for%20studying%20embodiment%2C%20decision-making%2C%20and%20the%0Aphysical%20basis%20of%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04633v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Physical%2520Basis%2520of%2520Prediction%253A%2520World%2520Model%2520Formation%2520in%2520Neural%250A%2520%2520Organoids%2520via%2520an%2520LLM-Generated%2520Curriculum%26entry.906535625%3DBrennen%2520Hill%26entry.1292438233%3D%2520%2520The%2520capacity%2520of%2520an%2520embodied%2520agent%2520to%2520understand%252C%2520predict%252C%2520and%2520interact%2520with%250Aits%2520environment%2520is%2520fundamentally%2520contingent%2520on%2520an%2520internal%2520world%2520model.%2520This%250Apaper%2520introduces%2520a%2520novel%2520framework%2520for%2520investigating%2520the%2520formation%2520and%250Aadaptation%2520of%2520such%2520world%2520models%2520within%2520a%2520biological%2520substrate%253A%2520human%2520neural%250Aorganoids.%2520We%2520present%2520a%2520curriculum%2520of%2520three%2520scalable%252C%2520closed-loop%2520virtual%250Aenvironments%2520designed%2520to%2520train%2520these%2520biological%2520agents%2520and%2520probe%2520the%2520underlying%250Asynaptic%2520mechanisms%2520of%2520learning%252C%2520such%2520as%2520long-term%2520potentiation%2520%2528LTP%2529%2520and%250Along-term%2520depression%2520%2528LTD%2529.%2520We%2520detail%2520the%2520design%2520of%2520three%2520distinct%2520task%250Aenvironments%2520that%2520demand%2520progressively%2520more%2520sophisticated%2520world%2520models%2520for%250Asuccessful%2520decision-making%253A%2520%25281%2529%2520a%2520conditional%2520avoidance%2520task%2520for%2520learning%250Astatic%2520state-action%2520contingencies%252C%2520%25282%2529%2520a%2520one-dimensional%2520predator-prey%2520scenario%250Afor%2520goal-directed%2520interaction%252C%2520and%2520%25283%2529%2520a%2520replication%2520of%2520the%2520classic%2520Pong%2520game%250Afor%2520modeling%2520dynamic%252C%2520continuous-time%2520systems.%2520For%2520each%2520environment%252C%2520we%250Aformalize%2520the%2520state%2520and%2520action%2520spaces%252C%2520the%2520sensory%2520encoding%2520and%2520motor%2520decoding%250Amechanisms%252C%2520and%2520the%2520feedback%2520protocols%2520based%2520on%2520predictable%2520%2528reward%2529%2520and%250Aunpredictable%2520%2528punishment%2529%2520stimulation%252C%2520which%2520serve%2520to%2520drive%2520model%2520refinement.%250AIn%2520a%2520significant%2520methodological%2520advance%252C%2520we%2520propose%2520a%2520meta-learning%2520approach%250Awhere%2520a%2520Large%2520Language%2520Model%2520automates%2520the%2520generative%2520design%2520and%2520optimization%250Aof%2520experimental%2520protocols%252C%2520thereby%2520scaling%2520the%2520process%2520of%2520environment%2520and%250Acurriculum%2520design.%2520Finally%252C%2520we%2520outline%2520a%2520multi-modal%2520evaluation%2520strategy%2520that%250Amoves%2520beyond%2520task%2520performance%2520to%2520directly%2520measure%2520the%2520physical%2520correlates%2520of%250Athe%2520learned%2520world%2520model%2520by%2520quantifying%2520synaptic%2520plasticity%2520at%250Aelectrophysiological%252C%2520cellular%252C%2520and%2520molecular%2520levels.%2520This%2520work%2520bridges%2520the%2520gap%250Abetween%2520model-based%2520reinforcement%2520learning%2520and%2520computational%2520neuroscience%252C%250Aoffering%2520a%2520unique%2520platform%2520for%2520studying%2520embodiment%252C%2520decision-making%252C%2520and%2520the%250Aphysical%2520basis%2520of%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04633v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Physical%20Basis%20of%20Prediction%3A%20World%20Model%20Formation%20in%20Neural%0A%20%20Organoids%20via%20an%20LLM-Generated%20Curriculum&entry.906535625=Brennen%20Hill&entry.1292438233=%20%20The%20capacity%20of%20an%20embodied%20agent%20to%20understand%2C%20predict%2C%20and%20interact%20with%0Aits%20environment%20is%20fundamentally%20contingent%20on%20an%20internal%20world%20model.%20This%0Apaper%20introduces%20a%20novel%20framework%20for%20investigating%20the%20formation%20and%0Aadaptation%20of%20such%20world%20models%20within%20a%20biological%20substrate%3A%20human%20neural%0Aorganoids.%20We%20present%20a%20curriculum%20of%20three%20scalable%2C%20closed-loop%20virtual%0Aenvironments%20designed%20to%20train%20these%20biological%20agents%20and%20probe%20the%20underlying%0Asynaptic%20mechanisms%20of%20learning%2C%20such%20as%20long-term%20potentiation%20%28LTP%29%20and%0Along-term%20depression%20%28LTD%29.%20We%20detail%20the%20design%20of%20three%20distinct%20task%0Aenvironments%20that%20demand%20progressively%20more%20sophisticated%20world%20models%20for%0Asuccessful%20decision-making%3A%20%281%29%20a%20conditional%20avoidance%20task%20for%20learning%0Astatic%20state-action%20contingencies%2C%20%282%29%20a%20one-dimensional%20predator-prey%20scenario%0Afor%20goal-directed%20interaction%2C%20and%20%283%29%20a%20replication%20of%20the%20classic%20Pong%20game%0Afor%20modeling%20dynamic%2C%20continuous-time%20systems.%20For%20each%20environment%2C%20we%0Aformalize%20the%20state%20and%20action%20spaces%2C%20the%20sensory%20encoding%20and%20motor%20decoding%0Amechanisms%2C%20and%20the%20feedback%20protocols%20based%20on%20predictable%20%28reward%29%20and%0Aunpredictable%20%28punishment%29%20stimulation%2C%20which%20serve%20to%20drive%20model%20refinement.%0AIn%20a%20significant%20methodological%20advance%2C%20we%20propose%20a%20meta-learning%20approach%0Awhere%20a%20Large%20Language%20Model%20automates%20the%20generative%20design%20and%20optimization%0Aof%20experimental%20protocols%2C%20thereby%20scaling%20the%20process%20of%20environment%20and%0Acurriculum%20design.%20Finally%2C%20we%20outline%20a%20multi-modal%20evaluation%20strategy%20that%0Amoves%20beyond%20task%20performance%20to%20directly%20measure%20the%20physical%20correlates%20of%0Athe%20learned%20world%20model%20by%20quantifying%20synaptic%20plasticity%20at%0Aelectrophysiological%2C%20cellular%2C%20and%20molecular%20levels.%20This%20work%20bridges%20the%20gap%0Abetween%20model-based%20reinforcement%20learning%20and%20computational%20neuroscience%2C%0Aoffering%20a%20unique%20platform%20for%20studying%20embodiment%2C%20decision-making%2C%20and%20the%0Aphysical%20basis%20of%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04633v2&entry.124074799=Read"},
{"title": "Causal Attention with Lookahead Keys", "author": "Zhuoqing Song and Peng Sun and Huizhuo Yuan and Quanquan Gu", "abstract": "  In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.\n", "link": "http://arxiv.org/abs/2509.07301v2", "date": "2025-09-29", "relevancy": 2.2832, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Attention%20with%20Lookahead%20Keys&body=Title%3A%20Causal%20Attention%20with%20Lookahead%20Keys%0AAuthor%3A%20Zhuoqing%20Song%20and%20Peng%20Sun%20and%20Huizhuo%20Yuan%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20In%20standard%20causal%20attention%2C%20each%20token%27s%20query%2C%20key%2C%20and%20value%20%28QKV%29%20are%0Astatic%20and%20encode%20only%20preceding%20context.%20We%20introduce%20CAuSal%20aTtention%20with%0ALookahead%20kEys%20%28CASTLE%29%2C%20an%20attention%20mechanism%20that%20continually%20updates%20each%0Atoken%27s%20keys%20as%20the%20context%20unfolds.%20We%20term%20these%20updated%20keys%20lookahead%20keys%0Abecause%20they%20belong%20to%20earlier%20positions%20yet%20integrate%20information%20from%20tokens%0Athat%20appear%20later%20relative%20to%20those%20positions%2C%20while%20strictly%20preserving%20the%0Aautoregressive%20property.%20Although%20the%20mechanism%20appears%20sequential%2C%20we%20derive%20a%0Amathematical%20equivalence%20that%20avoids%20explicitly%20materializing%20lookahead%20keys%20at%0Aeach%20position%20and%20enables%20efficient%20parallel%20training.%20On%20language%20modeling%0Abenchmarks%2C%20CASTLE%20consistently%20outperforms%20standard%20causal%20attention%20across%0Amodel%20scales%2C%20reducing%20validation%20perplexity%20and%20improving%20performance%20on%20a%0Arange%20of%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Attention%2520with%2520Lookahead%2520Keys%26entry.906535625%3DZhuoqing%2520Song%2520and%2520Peng%2520Sun%2520and%2520Huizhuo%2520Yuan%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520In%2520standard%2520causal%2520attention%252C%2520each%2520token%2527s%2520query%252C%2520key%252C%2520and%2520value%2520%2528QKV%2529%2520are%250Astatic%2520and%2520encode%2520only%2520preceding%2520context.%2520We%2520introduce%2520CAuSal%2520aTtention%2520with%250ALookahead%2520kEys%2520%2528CASTLE%2529%252C%2520an%2520attention%2520mechanism%2520that%2520continually%2520updates%2520each%250Atoken%2527s%2520keys%2520as%2520the%2520context%2520unfolds.%2520We%2520term%2520these%2520updated%2520keys%2520lookahead%2520keys%250Abecause%2520they%2520belong%2520to%2520earlier%2520positions%2520yet%2520integrate%2520information%2520from%2520tokens%250Athat%2520appear%2520later%2520relative%2520to%2520those%2520positions%252C%2520while%2520strictly%2520preserving%2520the%250Aautoregressive%2520property.%2520Although%2520the%2520mechanism%2520appears%2520sequential%252C%2520we%2520derive%2520a%250Amathematical%2520equivalence%2520that%2520avoids%2520explicitly%2520materializing%2520lookahead%2520keys%2520at%250Aeach%2520position%2520and%2520enables%2520efficient%2520parallel%2520training.%2520On%2520language%2520modeling%250Abenchmarks%252C%2520CASTLE%2520consistently%2520outperforms%2520standard%2520causal%2520attention%2520across%250Amodel%2520scales%252C%2520reducing%2520validation%2520perplexity%2520and%2520improving%2520performance%2520on%2520a%250Arange%2520of%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Attention%20with%20Lookahead%20Keys&entry.906535625=Zhuoqing%20Song%20and%20Peng%20Sun%20and%20Huizhuo%20Yuan%20and%20Quanquan%20Gu&entry.1292438233=%20%20In%20standard%20causal%20attention%2C%20each%20token%27s%20query%2C%20key%2C%20and%20value%20%28QKV%29%20are%0Astatic%20and%20encode%20only%20preceding%20context.%20We%20introduce%20CAuSal%20aTtention%20with%0ALookahead%20kEys%20%28CASTLE%29%2C%20an%20attention%20mechanism%20that%20continually%20updates%20each%0Atoken%27s%20keys%20as%20the%20context%20unfolds.%20We%20term%20these%20updated%20keys%20lookahead%20keys%0Abecause%20they%20belong%20to%20earlier%20positions%20yet%20integrate%20information%20from%20tokens%0Athat%20appear%20later%20relative%20to%20those%20positions%2C%20while%20strictly%20preserving%20the%0Aautoregressive%20property.%20Although%20the%20mechanism%20appears%20sequential%2C%20we%20derive%20a%0Amathematical%20equivalence%20that%20avoids%20explicitly%20materializing%20lookahead%20keys%20at%0Aeach%20position%20and%20enables%20efficient%20parallel%20training.%20On%20language%20modeling%0Abenchmarks%2C%20CASTLE%20consistently%20outperforms%20standard%20causal%20attention%20across%0Amodel%20scales%2C%20reducing%20validation%20perplexity%20and%20improving%20performance%20on%20a%0Arange%20of%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07301v2&entry.124074799=Read"},
{"title": "Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention\n  and Self-Supervised Multi-Event Representation Learning", "author": "Donghwa Kang and Junho Kim and Dongwoo Kang", "abstract": "  Event cameras offer unique advantages for facial keypoint alignment under\nchallenging conditions, such as low light and rapid motion, due to their high\ntemporal resolution and robustness to varying illumination. However, existing\nRGB facial keypoint alignment methods do not perform well on event data, and\ntraining solely on event data often leads to suboptimal performance because of\nits limited spatial information. Moreover, the lack of comprehensive labeled\nevent datasets further hinders progress in this area. To address these issues,\nwe propose a novel framework based on cross-modal fusion attention (CMFA) and\nself-supervised multi-event representation learning (SSMER) for event-based\nfacial keypoint alignment. Our framework employs CMFA to integrate\ncorresponding RGB data, guiding the model to extract robust facial features\nfrom event input images. In parallel, SSMER enables effective feature learning\nfrom unlabeled event data, overcoming spatial limitations. Extensive\nexperiments on our real-event E-SIE dataset and a synthetic-event version of\nthe public WFLW-V benchmark show that our approach consistently surpasses\nstate-of-the-art methods across multiple evaluation metrics.\n", "link": "http://arxiv.org/abs/2509.24968v1", "date": "2025-09-29", "relevancy": 2.2787, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5515}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-based%20Facial%20Keypoint%20Alignment%20via%20Cross-Modal%20Fusion%20Attention%0A%20%20and%20Self-Supervised%20Multi-Event%20Representation%20Learning&body=Title%3A%20Event-based%20Facial%20Keypoint%20Alignment%20via%20Cross-Modal%20Fusion%20Attention%0A%20%20and%20Self-Supervised%20Multi-Event%20Representation%20Learning%0AAuthor%3A%20Donghwa%20Kang%20and%20Junho%20Kim%20and%20Dongwoo%20Kang%0AAbstract%3A%20%20%20Event%20cameras%20offer%20unique%20advantages%20for%20facial%20keypoint%20alignment%20under%0Achallenging%20conditions%2C%20such%20as%20low%20light%20and%20rapid%20motion%2C%20due%20to%20their%20high%0Atemporal%20resolution%20and%20robustness%20to%20varying%20illumination.%20However%2C%20existing%0ARGB%20facial%20keypoint%20alignment%20methods%20do%20not%20perform%20well%20on%20event%20data%2C%20and%0Atraining%20solely%20on%20event%20data%20often%20leads%20to%20suboptimal%20performance%20because%20of%0Aits%20limited%20spatial%20information.%20Moreover%2C%20the%20lack%20of%20comprehensive%20labeled%0Aevent%20datasets%20further%20hinders%20progress%20in%20this%20area.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20novel%20framework%20based%20on%20cross-modal%20fusion%20attention%20%28CMFA%29%20and%0Aself-supervised%20multi-event%20representation%20learning%20%28SSMER%29%20for%20event-based%0Afacial%20keypoint%20alignment.%20Our%20framework%20employs%20CMFA%20to%20integrate%0Acorresponding%20RGB%20data%2C%20guiding%20the%20model%20to%20extract%20robust%20facial%20features%0Afrom%20event%20input%20images.%20In%20parallel%2C%20SSMER%20enables%20effective%20feature%20learning%0Afrom%20unlabeled%20event%20data%2C%20overcoming%20spatial%20limitations.%20Extensive%0Aexperiments%20on%20our%20real-event%20E-SIE%20dataset%20and%20a%20synthetic-event%20version%20of%0Athe%20public%20WFLW-V%20benchmark%20show%20that%20our%20approach%20consistently%20surpasses%0Astate-of-the-art%20methods%20across%20multiple%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-based%2520Facial%2520Keypoint%2520Alignment%2520via%2520Cross-Modal%2520Fusion%2520Attention%250A%2520%2520and%2520Self-Supervised%2520Multi-Event%2520Representation%2520Learning%26entry.906535625%3DDonghwa%2520Kang%2520and%2520Junho%2520Kim%2520and%2520Dongwoo%2520Kang%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520unique%2520advantages%2520for%2520facial%2520keypoint%2520alignment%2520under%250Achallenging%2520conditions%252C%2520such%2520as%2520low%2520light%2520and%2520rapid%2520motion%252C%2520due%2520to%2520their%2520high%250Atemporal%2520resolution%2520and%2520robustness%2520to%2520varying%2520illumination.%2520However%252C%2520existing%250ARGB%2520facial%2520keypoint%2520alignment%2520methods%2520do%2520not%2520perform%2520well%2520on%2520event%2520data%252C%2520and%250Atraining%2520solely%2520on%2520event%2520data%2520often%2520leads%2520to%2520suboptimal%2520performance%2520because%2520of%250Aits%2520limited%2520spatial%2520information.%2520Moreover%252C%2520the%2520lack%2520of%2520comprehensive%2520labeled%250Aevent%2520datasets%2520further%2520hinders%2520progress%2520in%2520this%2520area.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520a%2520novel%2520framework%2520based%2520on%2520cross-modal%2520fusion%2520attention%2520%2528CMFA%2529%2520and%250Aself-supervised%2520multi-event%2520representation%2520learning%2520%2528SSMER%2529%2520for%2520event-based%250Afacial%2520keypoint%2520alignment.%2520Our%2520framework%2520employs%2520CMFA%2520to%2520integrate%250Acorresponding%2520RGB%2520data%252C%2520guiding%2520the%2520model%2520to%2520extract%2520robust%2520facial%2520features%250Afrom%2520event%2520input%2520images.%2520In%2520parallel%252C%2520SSMER%2520enables%2520effective%2520feature%2520learning%250Afrom%2520unlabeled%2520event%2520data%252C%2520overcoming%2520spatial%2520limitations.%2520Extensive%250Aexperiments%2520on%2520our%2520real-event%2520E-SIE%2520dataset%2520and%2520a%2520synthetic-event%2520version%2520of%250Athe%2520public%2520WFLW-V%2520benchmark%2520show%2520that%2520our%2520approach%2520consistently%2520surpasses%250Astate-of-the-art%2520methods%2520across%2520multiple%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-based%20Facial%20Keypoint%20Alignment%20via%20Cross-Modal%20Fusion%20Attention%0A%20%20and%20Self-Supervised%20Multi-Event%20Representation%20Learning&entry.906535625=Donghwa%20Kang%20and%20Junho%20Kim%20and%20Dongwoo%20Kang&entry.1292438233=%20%20Event%20cameras%20offer%20unique%20advantages%20for%20facial%20keypoint%20alignment%20under%0Achallenging%20conditions%2C%20such%20as%20low%20light%20and%20rapid%20motion%2C%20due%20to%20their%20high%0Atemporal%20resolution%20and%20robustness%20to%20varying%20illumination.%20However%2C%20existing%0ARGB%20facial%20keypoint%20alignment%20methods%20do%20not%20perform%20well%20on%20event%20data%2C%20and%0Atraining%20solely%20on%20event%20data%20often%20leads%20to%20suboptimal%20performance%20because%20of%0Aits%20limited%20spatial%20information.%20Moreover%2C%20the%20lack%20of%20comprehensive%20labeled%0Aevent%20datasets%20further%20hinders%20progress%20in%20this%20area.%20To%20address%20these%20issues%2C%0Awe%20propose%20a%20novel%20framework%20based%20on%20cross-modal%20fusion%20attention%20%28CMFA%29%20and%0Aself-supervised%20multi-event%20representation%20learning%20%28SSMER%29%20for%20event-based%0Afacial%20keypoint%20alignment.%20Our%20framework%20employs%20CMFA%20to%20integrate%0Acorresponding%20RGB%20data%2C%20guiding%20the%20model%20to%20extract%20robust%20facial%20features%0Afrom%20event%20input%20images.%20In%20parallel%2C%20SSMER%20enables%20effective%20feature%20learning%0Afrom%20unlabeled%20event%20data%2C%20overcoming%20spatial%20limitations.%20Extensive%0Aexperiments%20on%20our%20real-event%20E-SIE%20dataset%20and%20a%20synthetic-event%20version%20of%0Athe%20public%20WFLW-V%20benchmark%20show%20that%20our%20approach%20consistently%20surpasses%0Astate-of-the-art%20methods%20across%20multiple%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24968v1&entry.124074799=Read"},
{"title": "Signal in the Noise: Polysemantic Interference Transfers and Predicts\n  Cross-Model Influence", "author": "Bofan Gong and Shiyang Lai and James Evans and Dawn Song", "abstract": "  Polysemanticity is pervasive in language models and remains a major challenge\nfor interpretation and model behavioral control. Leveraging sparse autoencoders\n(SAEs), we map the polysemantic topology of two small models (Pythia-70M and\nGPT-2-Small) to identify SAE feature pairs that are semantically unrelated yet\nexhibit interference within models. We intervene at four loci (prompt, token,\nfeature, neuron) and measure induced shifts in the next-token prediction\ndistribution, uncovering polysemantic structures that expose a systematic\nvulnerability in these models. Critically, interventions distilled from\ncounterintuitive interference patterns shared by two small models transfer\nreliably to larger instruction-tuned models (Llama-3.1-8B/70B-Instruct and\nGemma-2-9B-Instruct), yielding predictable behavioral shifts without access to\nmodel internals. These findings challenge the view that polysemanticity is\npurely stochastic, demonstrating instead that interference structures\ngeneralize across scale and family. Such generalization suggests a convergent,\nhigher-order organization of internal representations, which is only weakly\naligned with intuition and structured by latent regularities, offering new\npossibilities for both black-box control and theoretical insight into human and\nartificial cognition.\n", "link": "http://arxiv.org/abs/2505.11611v2", "date": "2025-09-29", "relevancy": 2.2689, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signal%20in%20the%20Noise%3A%20Polysemantic%20Interference%20Transfers%20and%20Predicts%0A%20%20Cross-Model%20Influence&body=Title%3A%20Signal%20in%20the%20Noise%3A%20Polysemantic%20Interference%20Transfers%20and%20Predicts%0A%20%20Cross-Model%20Influence%0AAuthor%3A%20Bofan%20Gong%20and%20Shiyang%20Lai%20and%20James%20Evans%20and%20Dawn%20Song%0AAbstract%3A%20%20%20Polysemanticity%20is%20pervasive%20in%20language%20models%20and%20remains%20a%20major%20challenge%0Afor%20interpretation%20and%20model%20behavioral%20control.%20Leveraging%20sparse%20autoencoders%0A%28SAEs%29%2C%20we%20map%20the%20polysemantic%20topology%20of%20two%20small%20models%20%28Pythia-70M%20and%0AGPT-2-Small%29%20to%20identify%20SAE%20feature%20pairs%20that%20are%20semantically%20unrelated%20yet%0Aexhibit%20interference%20within%20models.%20We%20intervene%20at%20four%20loci%20%28prompt%2C%20token%2C%0Afeature%2C%20neuron%29%20and%20measure%20induced%20shifts%20in%20the%20next-token%20prediction%0Adistribution%2C%20uncovering%20polysemantic%20structures%20that%20expose%20a%20systematic%0Avulnerability%20in%20these%20models.%20Critically%2C%20interventions%20distilled%20from%0Acounterintuitive%20interference%20patterns%20shared%20by%20two%20small%20models%20transfer%0Areliably%20to%20larger%20instruction-tuned%20models%20%28Llama-3.1-8B/70B-Instruct%20and%0AGemma-2-9B-Instruct%29%2C%20yielding%20predictable%20behavioral%20shifts%20without%20access%20to%0Amodel%20internals.%20These%20findings%20challenge%20the%20view%20that%20polysemanticity%20is%0Apurely%20stochastic%2C%20demonstrating%20instead%20that%20interference%20structures%0Ageneralize%20across%20scale%20and%20family.%20Such%20generalization%20suggests%20a%20convergent%2C%0Ahigher-order%20organization%20of%20internal%20representations%2C%20which%20is%20only%20weakly%0Aaligned%20with%20intuition%20and%20structured%20by%20latent%20regularities%2C%20offering%20new%0Apossibilities%20for%20both%20black-box%20control%20and%20theoretical%20insight%20into%20human%20and%0Aartificial%20cognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignal%2520in%2520the%2520Noise%253A%2520Polysemantic%2520Interference%2520Transfers%2520and%2520Predicts%250A%2520%2520Cross-Model%2520Influence%26entry.906535625%3DBofan%2520Gong%2520and%2520Shiyang%2520Lai%2520and%2520James%2520Evans%2520and%2520Dawn%2520Song%26entry.1292438233%3D%2520%2520Polysemanticity%2520is%2520pervasive%2520in%2520language%2520models%2520and%2520remains%2520a%2520major%2520challenge%250Afor%2520interpretation%2520and%2520model%2520behavioral%2520control.%2520Leveraging%2520sparse%2520autoencoders%250A%2528SAEs%2529%252C%2520we%2520map%2520the%2520polysemantic%2520topology%2520of%2520two%2520small%2520models%2520%2528Pythia-70M%2520and%250AGPT-2-Small%2529%2520to%2520identify%2520SAE%2520feature%2520pairs%2520that%2520are%2520semantically%2520unrelated%2520yet%250Aexhibit%2520interference%2520within%2520models.%2520We%2520intervene%2520at%2520four%2520loci%2520%2528prompt%252C%2520token%252C%250Afeature%252C%2520neuron%2529%2520and%2520measure%2520induced%2520shifts%2520in%2520the%2520next-token%2520prediction%250Adistribution%252C%2520uncovering%2520polysemantic%2520structures%2520that%2520expose%2520a%2520systematic%250Avulnerability%2520in%2520these%2520models.%2520Critically%252C%2520interventions%2520distilled%2520from%250Acounterintuitive%2520interference%2520patterns%2520shared%2520by%2520two%2520small%2520models%2520transfer%250Areliably%2520to%2520larger%2520instruction-tuned%2520models%2520%2528Llama-3.1-8B/70B-Instruct%2520and%250AGemma-2-9B-Instruct%2529%252C%2520yielding%2520predictable%2520behavioral%2520shifts%2520without%2520access%2520to%250Amodel%2520internals.%2520These%2520findings%2520challenge%2520the%2520view%2520that%2520polysemanticity%2520is%250Apurely%2520stochastic%252C%2520demonstrating%2520instead%2520that%2520interference%2520structures%250Ageneralize%2520across%2520scale%2520and%2520family.%2520Such%2520generalization%2520suggests%2520a%2520convergent%252C%250Ahigher-order%2520organization%2520of%2520internal%2520representations%252C%2520which%2520is%2520only%2520weakly%250Aaligned%2520with%2520intuition%2520and%2520structured%2520by%2520latent%2520regularities%252C%2520offering%2520new%250Apossibilities%2520for%2520both%2520black-box%2520control%2520and%2520theoretical%2520insight%2520into%2520human%2520and%250Aartificial%2520cognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signal%20in%20the%20Noise%3A%20Polysemantic%20Interference%20Transfers%20and%20Predicts%0A%20%20Cross-Model%20Influence&entry.906535625=Bofan%20Gong%20and%20Shiyang%20Lai%20and%20James%20Evans%20and%20Dawn%20Song&entry.1292438233=%20%20Polysemanticity%20is%20pervasive%20in%20language%20models%20and%20remains%20a%20major%20challenge%0Afor%20interpretation%20and%20model%20behavioral%20control.%20Leveraging%20sparse%20autoencoders%0A%28SAEs%29%2C%20we%20map%20the%20polysemantic%20topology%20of%20two%20small%20models%20%28Pythia-70M%20and%0AGPT-2-Small%29%20to%20identify%20SAE%20feature%20pairs%20that%20are%20semantically%20unrelated%20yet%0Aexhibit%20interference%20within%20models.%20We%20intervene%20at%20four%20loci%20%28prompt%2C%20token%2C%0Afeature%2C%20neuron%29%20and%20measure%20induced%20shifts%20in%20the%20next-token%20prediction%0Adistribution%2C%20uncovering%20polysemantic%20structures%20that%20expose%20a%20systematic%0Avulnerability%20in%20these%20models.%20Critically%2C%20interventions%20distilled%20from%0Acounterintuitive%20interference%20patterns%20shared%20by%20two%20small%20models%20transfer%0Areliably%20to%20larger%20instruction-tuned%20models%20%28Llama-3.1-8B/70B-Instruct%20and%0AGemma-2-9B-Instruct%29%2C%20yielding%20predictable%20behavioral%20shifts%20without%20access%20to%0Amodel%20internals.%20These%20findings%20challenge%20the%20view%20that%20polysemanticity%20is%0Apurely%20stochastic%2C%20demonstrating%20instead%20that%20interference%20structures%0Ageneralize%20across%20scale%20and%20family.%20Such%20generalization%20suggests%20a%20convergent%2C%0Ahigher-order%20organization%20of%20internal%20representations%2C%20which%20is%20only%20weakly%0Aaligned%20with%20intuition%20and%20structured%20by%20latent%20regularities%2C%20offering%20new%0Apossibilities%20for%20both%20black-box%20control%20and%20theoretical%20insight%20into%20human%20and%0Aartificial%20cognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11611v2&entry.124074799=Read"},
{"title": "Trajectory Prediction via Bayesian Intention Inference under Unknown\n  Goals and Kinematics", "author": "Shunan Yin and Zehui Lu and Shaoshuai Mou", "abstract": "  This work introduces an adaptive Bayesian algorithm for real-time trajectory\nprediction via intention inference, where a target's intentions and motion\ncharacteristics are unknown and subject to change. The method concurrently\nestimates two critical variables: the target's current intention, modeled as a\nMarkovian latent state, and an intention parameter that describes the target's\nadherence to a shortest-path policy. By integrating this joint update\ntechnique, the algorithm maintains robustness against abrupt intention shifts\nand unknown motion dynamics. A sampling-based trajectory prediction mechanism\nthen exploits these adaptive estimates to generate probabilistic forecasts with\nquantified uncertainty. We validate the framework through numerical\nexperiments: Ablation studies of two cases, and a 500-trial Monte Carlo\nanalysis; Hardware demonstrations on quadrotor and quadrupedal platforms.\nExperimental results demonstrate that the proposed approach significantly\noutperforms non-adaptive and partially adaptive methods. The method operates in\nreal time around 270 Hz without requiring training or detailed prior knowledge\nof target behavior, showcasing its applicability in various robotic systems.\n", "link": "http://arxiv.org/abs/2509.24928v1", "date": "2025-09-29", "relevancy": 2.2624, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6928}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5644}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Prediction%20via%20Bayesian%20Intention%20Inference%20under%20Unknown%0A%20%20Goals%20and%20Kinematics&body=Title%3A%20Trajectory%20Prediction%20via%20Bayesian%20Intention%20Inference%20under%20Unknown%0A%20%20Goals%20and%20Kinematics%0AAuthor%3A%20Shunan%20Yin%20and%20Zehui%20Lu%20and%20Shaoshuai%20Mou%0AAbstract%3A%20%20%20This%20work%20introduces%20an%20adaptive%20Bayesian%20algorithm%20for%20real-time%20trajectory%0Aprediction%20via%20intention%20inference%2C%20where%20a%20target%27s%20intentions%20and%20motion%0Acharacteristics%20are%20unknown%20and%20subject%20to%20change.%20The%20method%20concurrently%0Aestimates%20two%20critical%20variables%3A%20the%20target%27s%20current%20intention%2C%20modeled%20as%20a%0AMarkovian%20latent%20state%2C%20and%20an%20intention%20parameter%20that%20describes%20the%20target%27s%0Aadherence%20to%20a%20shortest-path%20policy.%20By%20integrating%20this%20joint%20update%0Atechnique%2C%20the%20algorithm%20maintains%20robustness%20against%20abrupt%20intention%20shifts%0Aand%20unknown%20motion%20dynamics.%20A%20sampling-based%20trajectory%20prediction%20mechanism%0Athen%20exploits%20these%20adaptive%20estimates%20to%20generate%20probabilistic%20forecasts%20with%0Aquantified%20uncertainty.%20We%20validate%20the%20framework%20through%20numerical%0Aexperiments%3A%20Ablation%20studies%20of%20two%20cases%2C%20and%20a%20500-trial%20Monte%20Carlo%0Aanalysis%3B%20Hardware%20demonstrations%20on%20quadrotor%20and%20quadrupedal%20platforms.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20approach%20significantly%0Aoutperforms%20non-adaptive%20and%20partially%20adaptive%20methods.%20The%20method%20operates%20in%0Areal%20time%20around%20270%20Hz%20without%20requiring%20training%20or%20detailed%20prior%20knowledge%0Aof%20target%20behavior%2C%20showcasing%20its%20applicability%20in%20various%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Prediction%2520via%2520Bayesian%2520Intention%2520Inference%2520under%2520Unknown%250A%2520%2520Goals%2520and%2520Kinematics%26entry.906535625%3DShunan%2520Yin%2520and%2520Zehui%2520Lu%2520and%2520Shaoshuai%2520Mou%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520an%2520adaptive%2520Bayesian%2520algorithm%2520for%2520real-time%2520trajectory%250Aprediction%2520via%2520intention%2520inference%252C%2520where%2520a%2520target%2527s%2520intentions%2520and%2520motion%250Acharacteristics%2520are%2520unknown%2520and%2520subject%2520to%2520change.%2520The%2520method%2520concurrently%250Aestimates%2520two%2520critical%2520variables%253A%2520the%2520target%2527s%2520current%2520intention%252C%2520modeled%2520as%2520a%250AMarkovian%2520latent%2520state%252C%2520and%2520an%2520intention%2520parameter%2520that%2520describes%2520the%2520target%2527s%250Aadherence%2520to%2520a%2520shortest-path%2520policy.%2520By%2520integrating%2520this%2520joint%2520update%250Atechnique%252C%2520the%2520algorithm%2520maintains%2520robustness%2520against%2520abrupt%2520intention%2520shifts%250Aand%2520unknown%2520motion%2520dynamics.%2520A%2520sampling-based%2520trajectory%2520prediction%2520mechanism%250Athen%2520exploits%2520these%2520adaptive%2520estimates%2520to%2520generate%2520probabilistic%2520forecasts%2520with%250Aquantified%2520uncertainty.%2520We%2520validate%2520the%2520framework%2520through%2520numerical%250Aexperiments%253A%2520Ablation%2520studies%2520of%2520two%2520cases%252C%2520and%2520a%2520500-trial%2520Monte%2520Carlo%250Aanalysis%253B%2520Hardware%2520demonstrations%2520on%2520quadrotor%2520and%2520quadrupedal%2520platforms.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520significantly%250Aoutperforms%2520non-adaptive%2520and%2520partially%2520adaptive%2520methods.%2520The%2520method%2520operates%2520in%250Areal%2520time%2520around%2520270%2520Hz%2520without%2520requiring%2520training%2520or%2520detailed%2520prior%2520knowledge%250Aof%2520target%2520behavior%252C%2520showcasing%2520its%2520applicability%2520in%2520various%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Prediction%20via%20Bayesian%20Intention%20Inference%20under%20Unknown%0A%20%20Goals%20and%20Kinematics&entry.906535625=Shunan%20Yin%20and%20Zehui%20Lu%20and%20Shaoshuai%20Mou&entry.1292438233=%20%20This%20work%20introduces%20an%20adaptive%20Bayesian%20algorithm%20for%20real-time%20trajectory%0Aprediction%20via%20intention%20inference%2C%20where%20a%20target%27s%20intentions%20and%20motion%0Acharacteristics%20are%20unknown%20and%20subject%20to%20change.%20The%20method%20concurrently%0Aestimates%20two%20critical%20variables%3A%20the%20target%27s%20current%20intention%2C%20modeled%20as%20a%0AMarkovian%20latent%20state%2C%20and%20an%20intention%20parameter%20that%20describes%20the%20target%27s%0Aadherence%20to%20a%20shortest-path%20policy.%20By%20integrating%20this%20joint%20update%0Atechnique%2C%20the%20algorithm%20maintains%20robustness%20against%20abrupt%20intention%20shifts%0Aand%20unknown%20motion%20dynamics.%20A%20sampling-based%20trajectory%20prediction%20mechanism%0Athen%20exploits%20these%20adaptive%20estimates%20to%20generate%20probabilistic%20forecasts%20with%0Aquantified%20uncertainty.%20We%20validate%20the%20framework%20through%20numerical%0Aexperiments%3A%20Ablation%20studies%20of%20two%20cases%2C%20and%20a%20500-trial%20Monte%20Carlo%0Aanalysis%3B%20Hardware%20demonstrations%20on%20quadrotor%20and%20quadrupedal%20platforms.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20approach%20significantly%0Aoutperforms%20non-adaptive%20and%20partially%20adaptive%20methods.%20The%20method%20operates%20in%0Areal%20time%20around%20270%20Hz%20without%20requiring%20training%20or%20detailed%20prior%20knowledge%0Aof%20target%20behavior%2C%20showcasing%20its%20applicability%20in%20various%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24928v1&entry.124074799=Read"},
{"title": "Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range\n  Images", "author": "Ruixin Wu and Zihan Li and Jin Wang and Xiangyu Xu and Zhi Zheng and Kaixiang Huang and Guodong Lu", "abstract": "  Millimeter-wave (mmWave) radar has attracted significant attention in\nrobotics and autonomous driving. However, despite the perception stability in\nharsh environments, the point cloud generated by mmWave radar is relatively\nsparse while containing significant noise, which limits its further\ndevelopment. Traditional mmWave radar enhancement approaches often struggle to\nleverage the effectiveness of diffusion models in super-resolution, largely due\nto the unnatural range-azimuth heatmap (RAH) or bird's eye view (BEV)\nrepresentation. To overcome this limitation, we propose a novel method that\npioneers the application of fusing range images with image diffusion models,\nachieving accurate and dense mmWave radar point clouds that are similar to\nLiDAR. Benefitting from the projection that aligns with human observation, the\nrange image representation of mmWave radar is close to natural images, allowing\nthe knowledge from pre-trained image diffusion models to be effectively\ntransferred, significantly improving the overall performance. Extensive\nevaluations on both public datasets and self-constructed datasets demonstrate\nthat our approach provides substantial improvements, establishing a new\nstate-of-the-art performance in generating truly three-dimensional LiDAR-like\npoint clouds via mmWave radar. Code will be released after publication.\n", "link": "http://arxiv.org/abs/2503.02300v2", "date": "2025-09-29", "relevancy": 2.2558, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5708}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5626}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20mmWave%20Radar%20Point%20Cloud%20Enhancement%20Driven%20by%20Range%0A%20%20Images&body=Title%3A%20Diffusion-Based%20mmWave%20Radar%20Point%20Cloud%20Enhancement%20Driven%20by%20Range%0A%20%20Images%0AAuthor%3A%20Ruixin%20Wu%20and%20Zihan%20Li%20and%20Jin%20Wang%20and%20Xiangyu%20Xu%20and%20Zhi%20Zheng%20and%20Kaixiang%20Huang%20and%20Guodong%20Lu%0AAbstract%3A%20%20%20Millimeter-wave%20%28mmWave%29%20radar%20has%20attracted%20significant%20attention%20in%0Arobotics%20and%20autonomous%20driving.%20However%2C%20despite%20the%20perception%20stability%20in%0Aharsh%20environments%2C%20the%20point%20cloud%20generated%20by%20mmWave%20radar%20is%20relatively%0Asparse%20while%20containing%20significant%20noise%2C%20which%20limits%20its%20further%0Adevelopment.%20Traditional%20mmWave%20radar%20enhancement%20approaches%20often%20struggle%20to%0Aleverage%20the%20effectiveness%20of%20diffusion%20models%20in%20super-resolution%2C%20largely%20due%0Ato%20the%20unnatural%20range-azimuth%20heatmap%20%28RAH%29%20or%20bird%27s%20eye%20view%20%28BEV%29%0Arepresentation.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20method%20that%0Apioneers%20the%20application%20of%20fusing%20range%20images%20with%20image%20diffusion%20models%2C%0Aachieving%20accurate%20and%20dense%20mmWave%20radar%20point%20clouds%20that%20are%20similar%20to%0ALiDAR.%20Benefitting%20from%20the%20projection%20that%20aligns%20with%20human%20observation%2C%20the%0Arange%20image%20representation%20of%20mmWave%20radar%20is%20close%20to%20natural%20images%2C%20allowing%0Athe%20knowledge%20from%20pre-trained%20image%20diffusion%20models%20to%20be%20effectively%0Atransferred%2C%20significantly%20improving%20the%20overall%20performance.%20Extensive%0Aevaluations%20on%20both%20public%20datasets%20and%20self-constructed%20datasets%20demonstrate%0Athat%20our%20approach%20provides%20substantial%20improvements%2C%20establishing%20a%20new%0Astate-of-the-art%20performance%20in%20generating%20truly%20three-dimensional%20LiDAR-like%0Apoint%20clouds%20via%20mmWave%20radar.%20Code%20will%20be%20released%20after%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Based%2520mmWave%2520Radar%2520Point%2520Cloud%2520Enhancement%2520Driven%2520by%2520Range%250A%2520%2520Images%26entry.906535625%3DRuixin%2520Wu%2520and%2520Zihan%2520Li%2520and%2520Jin%2520Wang%2520and%2520Xiangyu%2520Xu%2520and%2520Zhi%2520Zheng%2520and%2520Kaixiang%2520Huang%2520and%2520Guodong%2520Lu%26entry.1292438233%3D%2520%2520Millimeter-wave%2520%2528mmWave%2529%2520radar%2520has%2520attracted%2520significant%2520attention%2520in%250Arobotics%2520and%2520autonomous%2520driving.%2520However%252C%2520despite%2520the%2520perception%2520stability%2520in%250Aharsh%2520environments%252C%2520the%2520point%2520cloud%2520generated%2520by%2520mmWave%2520radar%2520is%2520relatively%250Asparse%2520while%2520containing%2520significant%2520noise%252C%2520which%2520limits%2520its%2520further%250Adevelopment.%2520Traditional%2520mmWave%2520radar%2520enhancement%2520approaches%2520often%2520struggle%2520to%250Aleverage%2520the%2520effectiveness%2520of%2520diffusion%2520models%2520in%2520super-resolution%252C%2520largely%2520due%250Ato%2520the%2520unnatural%2520range-azimuth%2520heatmap%2520%2528RAH%2529%2520or%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529%250Arepresentation.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%250Apioneers%2520the%2520application%2520of%2520fusing%2520range%2520images%2520with%2520image%2520diffusion%2520models%252C%250Aachieving%2520accurate%2520and%2520dense%2520mmWave%2520radar%2520point%2520clouds%2520that%2520are%2520similar%2520to%250ALiDAR.%2520Benefitting%2520from%2520the%2520projection%2520that%2520aligns%2520with%2520human%2520observation%252C%2520the%250Arange%2520image%2520representation%2520of%2520mmWave%2520radar%2520is%2520close%2520to%2520natural%2520images%252C%2520allowing%250Athe%2520knowledge%2520from%2520pre-trained%2520image%2520diffusion%2520models%2520to%2520be%2520effectively%250Atransferred%252C%2520significantly%2520improving%2520the%2520overall%2520performance.%2520Extensive%250Aevaluations%2520on%2520both%2520public%2520datasets%2520and%2520self-constructed%2520datasets%2520demonstrate%250Athat%2520our%2520approach%2520provides%2520substantial%2520improvements%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520performance%2520in%2520generating%2520truly%2520three-dimensional%2520LiDAR-like%250Apoint%2520clouds%2520via%2520mmWave%2520radar.%2520Code%2520will%2520be%2520released%2520after%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20mmWave%20Radar%20Point%20Cloud%20Enhancement%20Driven%20by%20Range%0A%20%20Images&entry.906535625=Ruixin%20Wu%20and%20Zihan%20Li%20and%20Jin%20Wang%20and%20Xiangyu%20Xu%20and%20Zhi%20Zheng%20and%20Kaixiang%20Huang%20and%20Guodong%20Lu&entry.1292438233=%20%20Millimeter-wave%20%28mmWave%29%20radar%20has%20attracted%20significant%20attention%20in%0Arobotics%20and%20autonomous%20driving.%20However%2C%20despite%20the%20perception%20stability%20in%0Aharsh%20environments%2C%20the%20point%20cloud%20generated%20by%20mmWave%20radar%20is%20relatively%0Asparse%20while%20containing%20significant%20noise%2C%20which%20limits%20its%20further%0Adevelopment.%20Traditional%20mmWave%20radar%20enhancement%20approaches%20often%20struggle%20to%0Aleverage%20the%20effectiveness%20of%20diffusion%20models%20in%20super-resolution%2C%20largely%20due%0Ato%20the%20unnatural%20range-azimuth%20heatmap%20%28RAH%29%20or%20bird%27s%20eye%20view%20%28BEV%29%0Arepresentation.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20method%20that%0Apioneers%20the%20application%20of%20fusing%20range%20images%20with%20image%20diffusion%20models%2C%0Aachieving%20accurate%20and%20dense%20mmWave%20radar%20point%20clouds%20that%20are%20similar%20to%0ALiDAR.%20Benefitting%20from%20the%20projection%20that%20aligns%20with%20human%20observation%2C%20the%0Arange%20image%20representation%20of%20mmWave%20radar%20is%20close%20to%20natural%20images%2C%20allowing%0Athe%20knowledge%20from%20pre-trained%20image%20diffusion%20models%20to%20be%20effectively%0Atransferred%2C%20significantly%20improving%20the%20overall%20performance.%20Extensive%0Aevaluations%20on%20both%20public%20datasets%20and%20self-constructed%20datasets%20demonstrate%0Athat%20our%20approach%20provides%20substantial%20improvements%2C%20establishing%20a%20new%0Astate-of-the-art%20performance%20in%20generating%20truly%20three-dimensional%20LiDAR-like%0Apoint%20clouds%20via%20mmWave%20radar.%20Code%20will%20be%20released%20after%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02300v2&entry.124074799=Read"},
{"title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching", "author": "Xinye Zhao and Spyridon Mastorakis", "abstract": "  As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.\n", "link": "http://arxiv.org/abs/2509.24832v1", "date": "2025-09-29", "relevancy": 2.2538, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemShareKV%3A%20Efficient%20KVCache%20Sharing%20for%20Semantically%20Similar%20Prompts%0A%20%20via%20Token-Level%20LSH%20Matching&body=Title%3A%20SemShareKV%3A%20Efficient%20KVCache%20Sharing%20for%20Semantically%20Similar%20Prompts%0A%20%20via%20Token-Level%20LSH%20Matching%0AAuthor%3A%20Xinye%20Zhao%20and%20Spyridon%20Mastorakis%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20scale%2C%20the%20memory%20footprint%20of%0Akey-value%20%28KV%29%20caches%20during%20inference%20has%20become%20a%20significant%20bottleneck.%0AExisting%20approaches%20primarily%20focus%20on%20compressing%20KV%20caches%20within%20a%20single%0Aprompt%20or%20reusing%20shared%20prefixes%20or%20frequently%20ocurred%20text%20segments%20across%0Aprompts.%20However%2C%20such%20strategies%20are%20limited%20in%20scenarios%20where%20prompts%20are%0Asemantically%20similar%20but%20lexically%20different%2C%20which%20frequently%20occurs%20in%20tasks%0Asuch%20as%20multi-document%20summarization%20and%20conversational%20agents.%20We%20propose%0A%5Ctextit%7BSemShareKV%7D%2C%20a%20KV%20cache%20sharing%20and%20compression%20framework%20that%0Aaccelerates%20LLM%20inference%20by%20reusing%20KVCache%20in%20semantically%20similar%20prompts.%0AInstead%20of%20relying%20on%20exact%20token%20matches%2C%20SemShareKV%20applies%20fuzzy%20token%0Amatching%20using%20locality-sensitive%20hashing%20%28LSH%29%20on%20token%20embeddings%20and%0Aincorporates%20Rotary%20Position%20Embedding%20%28RoPE%29%20to%20better%20preserve%20positional%0Ainformation.%20By%20selectively%20reusing%20relevant%20key-value%20pairs%20from%20a%20reference%0Aprompt%27s%20cache%2C%20SemShareKV%20reduces%20redundant%20computation%20while%20maintaining%0Aoutput%20quality.%20Experiments%20on%20diverse%20summarization%20datasets%20show%20up%20to%0A6.25%24%5Ctimes%24%20speedup%20and%2042%5C%25%20lower%20GPU%20memory%20usage%20with%205k%20tokens%20input%2C%20with%0Anegligible%20quality%20degradation.%20These%20results%20highlight%20the%20potential%20of%0Asemantic-aware%20cache%20sharing%20for%20efficient%20LLM%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemShareKV%253A%2520Efficient%2520KVCache%2520Sharing%2520for%2520Semantically%2520Similar%2520Prompts%250A%2520%2520via%2520Token-Level%2520LSH%2520Matching%26entry.906535625%3DXinye%2520Zhao%2520and%2520Spyridon%2520Mastorakis%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520continue%2520to%2520scale%252C%2520the%2520memory%2520footprint%2520of%250Akey-value%2520%2528KV%2529%2520caches%2520during%2520inference%2520has%2520become%2520a%2520significant%2520bottleneck.%250AExisting%2520approaches%2520primarily%2520focus%2520on%2520compressing%2520KV%2520caches%2520within%2520a%2520single%250Aprompt%2520or%2520reusing%2520shared%2520prefixes%2520or%2520frequently%2520ocurred%2520text%2520segments%2520across%250Aprompts.%2520However%252C%2520such%2520strategies%2520are%2520limited%2520in%2520scenarios%2520where%2520prompts%2520are%250Asemantically%2520similar%2520but%2520lexically%2520different%252C%2520which%2520frequently%2520occurs%2520in%2520tasks%250Asuch%2520as%2520multi-document%2520summarization%2520and%2520conversational%2520agents.%2520We%2520propose%250A%255Ctextit%257BSemShareKV%257D%252C%2520a%2520KV%2520cache%2520sharing%2520and%2520compression%2520framework%2520that%250Aaccelerates%2520LLM%2520inference%2520by%2520reusing%2520KVCache%2520in%2520semantically%2520similar%2520prompts.%250AInstead%2520of%2520relying%2520on%2520exact%2520token%2520matches%252C%2520SemShareKV%2520applies%2520fuzzy%2520token%250Amatching%2520using%2520locality-sensitive%2520hashing%2520%2528LSH%2529%2520on%2520token%2520embeddings%2520and%250Aincorporates%2520Rotary%2520Position%2520Embedding%2520%2528RoPE%2529%2520to%2520better%2520preserve%2520positional%250Ainformation.%2520By%2520selectively%2520reusing%2520relevant%2520key-value%2520pairs%2520from%2520a%2520reference%250Aprompt%2527s%2520cache%252C%2520SemShareKV%2520reduces%2520redundant%2520computation%2520while%2520maintaining%250Aoutput%2520quality.%2520Experiments%2520on%2520diverse%2520summarization%2520datasets%2520show%2520up%2520to%250A6.25%2524%255Ctimes%2524%2520speedup%2520and%252042%255C%2525%2520lower%2520GPU%2520memory%2520usage%2520with%25205k%2520tokens%2520input%252C%2520with%250Anegligible%2520quality%2520degradation.%2520These%2520results%2520highlight%2520the%2520potential%2520of%250Asemantic-aware%2520cache%2520sharing%2520for%2520efficient%2520LLM%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemShareKV%3A%20Efficient%20KVCache%20Sharing%20for%20Semantically%20Similar%20Prompts%0A%20%20via%20Token-Level%20LSH%20Matching&entry.906535625=Xinye%20Zhao%20and%20Spyridon%20Mastorakis&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20scale%2C%20the%20memory%20footprint%20of%0Akey-value%20%28KV%29%20caches%20during%20inference%20has%20become%20a%20significant%20bottleneck.%0AExisting%20approaches%20primarily%20focus%20on%20compressing%20KV%20caches%20within%20a%20single%0Aprompt%20or%20reusing%20shared%20prefixes%20or%20frequently%20ocurred%20text%20segments%20across%0Aprompts.%20However%2C%20such%20strategies%20are%20limited%20in%20scenarios%20where%20prompts%20are%0Asemantically%20similar%20but%20lexically%20different%2C%20which%20frequently%20occurs%20in%20tasks%0Asuch%20as%20multi-document%20summarization%20and%20conversational%20agents.%20We%20propose%0A%5Ctextit%7BSemShareKV%7D%2C%20a%20KV%20cache%20sharing%20and%20compression%20framework%20that%0Aaccelerates%20LLM%20inference%20by%20reusing%20KVCache%20in%20semantically%20similar%20prompts.%0AInstead%20of%20relying%20on%20exact%20token%20matches%2C%20SemShareKV%20applies%20fuzzy%20token%0Amatching%20using%20locality-sensitive%20hashing%20%28LSH%29%20on%20token%20embeddings%20and%0Aincorporates%20Rotary%20Position%20Embedding%20%28RoPE%29%20to%20better%20preserve%20positional%0Ainformation.%20By%20selectively%20reusing%20relevant%20key-value%20pairs%20from%20a%20reference%0Aprompt%27s%20cache%2C%20SemShareKV%20reduces%20redundant%20computation%20while%20maintaining%0Aoutput%20quality.%20Experiments%20on%20diverse%20summarization%20datasets%20show%20up%20to%0A6.25%24%5Ctimes%24%20speedup%20and%2042%5C%25%20lower%20GPU%20memory%20usage%20with%205k%20tokens%20input%2C%20with%0Anegligible%20quality%20degradation.%20These%20results%20highlight%20the%20potential%20of%0Asemantic-aware%20cache%20sharing%20for%20efficient%20LLM%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24832v1&entry.124074799=Read"},
{"title": "CharGen: Fast and Fluent Portrait Modification", "author": "Jan-Niklas Dihlmann and Arnela Killguss and Hendrik P. A. Lensch", "abstract": "  Interactive editing of character images with diffusion models remains\nchallenging due to the inherent trade-off between fine-grained control,\ngeneration speed, and visual fidelity. We introduce CharGen, a\ncharacter-focused editor that combines attribute-specific Concept Sliders,\ntrained to isolate and manipulate attributes such as facial feature size,\nexpression, and decoration with the StreamDiffusion sampling pipeline for more\ninteractive performance. To counteract the loss of detail that often\naccompanies accelerated sampling, we propose a lightweight Repair Step that\nreinstates fine textures without compromising structural consistency.\nThroughout extensive ablation studies and in comparison to open-source\nInstructPix2Pix and closed-source Google Gemini, and a comprehensive user\nstudy, CharGen achieves two-to-four-fold faster edit turnaround with precise\nediting control and identity-consistent results. Project page:\nhttps://chargen.jdihlmann.com/\n", "link": "http://arxiv.org/abs/2509.25058v1", "date": "2025-09-29", "relevancy": 2.2519, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5857}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5525}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CharGen%3A%20Fast%20and%20Fluent%20Portrait%20Modification&body=Title%3A%20CharGen%3A%20Fast%20and%20Fluent%20Portrait%20Modification%0AAuthor%3A%20Jan-Niklas%20Dihlmann%20and%20Arnela%20Killguss%20and%20Hendrik%20P.%20A.%20Lensch%0AAbstract%3A%20%20%20Interactive%20editing%20of%20character%20images%20with%20diffusion%20models%20remains%0Achallenging%20due%20to%20the%20inherent%20trade-off%20between%20fine-grained%20control%2C%0Ageneration%20speed%2C%20and%20visual%20fidelity.%20We%20introduce%20CharGen%2C%20a%0Acharacter-focused%20editor%20that%20combines%20attribute-specific%20Concept%20Sliders%2C%0Atrained%20to%20isolate%20and%20manipulate%20attributes%20such%20as%20facial%20feature%20size%2C%0Aexpression%2C%20and%20decoration%20with%20the%20StreamDiffusion%20sampling%20pipeline%20for%20more%0Ainteractive%20performance.%20To%20counteract%20the%20loss%20of%20detail%20that%20often%0Aaccompanies%20accelerated%20sampling%2C%20we%20propose%20a%20lightweight%20Repair%20Step%20that%0Areinstates%20fine%20textures%20without%20compromising%20structural%20consistency.%0AThroughout%20extensive%20ablation%20studies%20and%20in%20comparison%20to%20open-source%0AInstructPix2Pix%20and%20closed-source%20Google%20Gemini%2C%20and%20a%20comprehensive%20user%0Astudy%2C%20CharGen%20achieves%20two-to-four-fold%20faster%20edit%20turnaround%20with%20precise%0Aediting%20control%20and%20identity-consistent%20results.%20Project%20page%3A%0Ahttps%3A//chargen.jdihlmann.com/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharGen%253A%2520Fast%2520and%2520Fluent%2520Portrait%2520Modification%26entry.906535625%3DJan-Niklas%2520Dihlmann%2520and%2520Arnela%2520Killguss%2520and%2520Hendrik%2520P.%2520A.%2520Lensch%26entry.1292438233%3D%2520%2520Interactive%2520editing%2520of%2520character%2520images%2520with%2520diffusion%2520models%2520remains%250Achallenging%2520due%2520to%2520the%2520inherent%2520trade-off%2520between%2520fine-grained%2520control%252C%250Ageneration%2520speed%252C%2520and%2520visual%2520fidelity.%2520We%2520introduce%2520CharGen%252C%2520a%250Acharacter-focused%2520editor%2520that%2520combines%2520attribute-specific%2520Concept%2520Sliders%252C%250Atrained%2520to%2520isolate%2520and%2520manipulate%2520attributes%2520such%2520as%2520facial%2520feature%2520size%252C%250Aexpression%252C%2520and%2520decoration%2520with%2520the%2520StreamDiffusion%2520sampling%2520pipeline%2520for%2520more%250Ainteractive%2520performance.%2520To%2520counteract%2520the%2520loss%2520of%2520detail%2520that%2520often%250Aaccompanies%2520accelerated%2520sampling%252C%2520we%2520propose%2520a%2520lightweight%2520Repair%2520Step%2520that%250Areinstates%2520fine%2520textures%2520without%2520compromising%2520structural%2520consistency.%250AThroughout%2520extensive%2520ablation%2520studies%2520and%2520in%2520comparison%2520to%2520open-source%250AInstructPix2Pix%2520and%2520closed-source%2520Google%2520Gemini%252C%2520and%2520a%2520comprehensive%2520user%250Astudy%252C%2520CharGen%2520achieves%2520two-to-four-fold%2520faster%2520edit%2520turnaround%2520with%2520precise%250Aediting%2520control%2520and%2520identity-consistent%2520results.%2520Project%2520page%253A%250Ahttps%253A//chargen.jdihlmann.com/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CharGen%3A%20Fast%20and%20Fluent%20Portrait%20Modification&entry.906535625=Jan-Niklas%20Dihlmann%20and%20Arnela%20Killguss%20and%20Hendrik%20P.%20A.%20Lensch&entry.1292438233=%20%20Interactive%20editing%20of%20character%20images%20with%20diffusion%20models%20remains%0Achallenging%20due%20to%20the%20inherent%20trade-off%20between%20fine-grained%20control%2C%0Ageneration%20speed%2C%20and%20visual%20fidelity.%20We%20introduce%20CharGen%2C%20a%0Acharacter-focused%20editor%20that%20combines%20attribute-specific%20Concept%20Sliders%2C%0Atrained%20to%20isolate%20and%20manipulate%20attributes%20such%20as%20facial%20feature%20size%2C%0Aexpression%2C%20and%20decoration%20with%20the%20StreamDiffusion%20sampling%20pipeline%20for%20more%0Ainteractive%20performance.%20To%20counteract%20the%20loss%20of%20detail%20that%20often%0Aaccompanies%20accelerated%20sampling%2C%20we%20propose%20a%20lightweight%20Repair%20Step%20that%0Areinstates%20fine%20textures%20without%20compromising%20structural%20consistency.%0AThroughout%20extensive%20ablation%20studies%20and%20in%20comparison%20to%20open-source%0AInstructPix2Pix%20and%20closed-source%20Google%20Gemini%2C%20and%20a%20comprehensive%20user%0Astudy%2C%20CharGen%20achieves%20two-to-four-fold%20faster%20edit%20turnaround%20with%20precise%0Aediting%20control%20and%20identity-consistent%20results.%20Project%20page%3A%0Ahttps%3A//chargen.jdihlmann.com/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25058v1&entry.124074799=Read"},
{"title": "Scalable GANs with Transformers", "author": "Sangeek Hyun and MinKyu Lee and Jae-Pil Heo", "abstract": "  Scalability has driven recent advances in generative modeling, yet its\nprinciples remain underexplored for adversarial learning. We investigate the\nscalability of Generative Adversarial Networks (GANs) through two design\nchoices that have proven to be effective in other types of generative models:\ntraining in a compact Variational Autoencoder latent space and adopting purely\ntransformer-based generators and discriminators. Training in latent space\nenables efficient computation while preserving perceptual fidelity, and this\nefficiency pairs naturally with plain transformers, whose performance scales\nwith computational budget. Building on these choices, we analyze failure modes\nthat emerge when naively scaling GANs. Specifically, we find issues as\nunderutilization of early layers in the generator and optimization instability\nas the network scales. Accordingly, we provide simple and scale-friendly\nsolutions as lightweight intermediate supervision and width-aware learning-rate\nadjustment. Our experiments show that GAT, a purely transformer-based and\nlatent-space GANs, can be easily trained reliably across a wide range of\ncapacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art\nsingle-step, class-conditional generation performance (FID of 2.96) on\nImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.\n", "link": "http://arxiv.org/abs/2509.24935v1", "date": "2025-09-29", "relevancy": 2.2304, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6273}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5491}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20GANs%20with%20Transformers&body=Title%3A%20Scalable%20GANs%20with%20Transformers%0AAuthor%3A%20Sangeek%20Hyun%20and%20MinKyu%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Scalability%20has%20driven%20recent%20advances%20in%20generative%20modeling%2C%20yet%20its%0Aprinciples%20remain%20underexplored%20for%20adversarial%20learning.%20We%20investigate%20the%0Ascalability%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20through%20two%20design%0Achoices%20that%20have%20proven%20to%20be%20effective%20in%20other%20types%20of%20generative%20models%3A%0Atraining%20in%20a%20compact%20Variational%20Autoencoder%20latent%20space%20and%20adopting%20purely%0Atransformer-based%20generators%20and%20discriminators.%20Training%20in%20latent%20space%0Aenables%20efficient%20computation%20while%20preserving%20perceptual%20fidelity%2C%20and%20this%0Aefficiency%20pairs%20naturally%20with%20plain%20transformers%2C%20whose%20performance%20scales%0Awith%20computational%20budget.%20Building%20on%20these%20choices%2C%20we%20analyze%20failure%20modes%0Athat%20emerge%20when%20naively%20scaling%20GANs.%20Specifically%2C%20we%20find%20issues%20as%0Aunderutilization%20of%20early%20layers%20in%20the%20generator%20and%20optimization%20instability%0Aas%20the%20network%20scales.%20Accordingly%2C%20we%20provide%20simple%20and%20scale-friendly%0Asolutions%20as%20lightweight%20intermediate%20supervision%20and%20width-aware%20learning-rate%0Aadjustment.%20Our%20experiments%20show%20that%20GAT%2C%20a%20purely%20transformer-based%20and%0Alatent-space%20GANs%2C%20can%20be%20easily%20trained%20reliably%20across%20a%20wide%20range%20of%0Acapacities%20%28S%20through%20XL%29.%20Moreover%2C%20GAT-XL/2%20achieves%20state-of-the-art%0Asingle-step%2C%20class-conditional%20generation%20performance%20%28FID%20of%202.96%29%20on%0AImageNet-256%20in%20just%2040%20epochs%2C%206x%20fewer%20epochs%20than%20strong%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520GANs%2520with%2520Transformers%26entry.906535625%3DSangeek%2520Hyun%2520and%2520MinKyu%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Scalability%2520has%2520driven%2520recent%2520advances%2520in%2520generative%2520modeling%252C%2520yet%2520its%250Aprinciples%2520remain%2520underexplored%2520for%2520adversarial%2520learning.%2520We%2520investigate%2520the%250Ascalability%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520through%2520two%2520design%250Achoices%2520that%2520have%2520proven%2520to%2520be%2520effective%2520in%2520other%2520types%2520of%2520generative%2520models%253A%250Atraining%2520in%2520a%2520compact%2520Variational%2520Autoencoder%2520latent%2520space%2520and%2520adopting%2520purely%250Atransformer-based%2520generators%2520and%2520discriminators.%2520Training%2520in%2520latent%2520space%250Aenables%2520efficient%2520computation%2520while%2520preserving%2520perceptual%2520fidelity%252C%2520and%2520this%250Aefficiency%2520pairs%2520naturally%2520with%2520plain%2520transformers%252C%2520whose%2520performance%2520scales%250Awith%2520computational%2520budget.%2520Building%2520on%2520these%2520choices%252C%2520we%2520analyze%2520failure%2520modes%250Athat%2520emerge%2520when%2520naively%2520scaling%2520GANs.%2520Specifically%252C%2520we%2520find%2520issues%2520as%250Aunderutilization%2520of%2520early%2520layers%2520in%2520the%2520generator%2520and%2520optimization%2520instability%250Aas%2520the%2520network%2520scales.%2520Accordingly%252C%2520we%2520provide%2520simple%2520and%2520scale-friendly%250Asolutions%2520as%2520lightweight%2520intermediate%2520supervision%2520and%2520width-aware%2520learning-rate%250Aadjustment.%2520Our%2520experiments%2520show%2520that%2520GAT%252C%2520a%2520purely%2520transformer-based%2520and%250Alatent-space%2520GANs%252C%2520can%2520be%2520easily%2520trained%2520reliably%2520across%2520a%2520wide%2520range%2520of%250Acapacities%2520%2528S%2520through%2520XL%2529.%2520Moreover%252C%2520GAT-XL/2%2520achieves%2520state-of-the-art%250Asingle-step%252C%2520class-conditional%2520generation%2520performance%2520%2528FID%2520of%25202.96%2529%2520on%250AImageNet-256%2520in%2520just%252040%2520epochs%252C%25206x%2520fewer%2520epochs%2520than%2520strong%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20GANs%20with%20Transformers&entry.906535625=Sangeek%20Hyun%20and%20MinKyu%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Scalability%20has%20driven%20recent%20advances%20in%20generative%20modeling%2C%20yet%20its%0Aprinciples%20remain%20underexplored%20for%20adversarial%20learning.%20We%20investigate%20the%0Ascalability%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20through%20two%20design%0Achoices%20that%20have%20proven%20to%20be%20effective%20in%20other%20types%20of%20generative%20models%3A%0Atraining%20in%20a%20compact%20Variational%20Autoencoder%20latent%20space%20and%20adopting%20purely%0Atransformer-based%20generators%20and%20discriminators.%20Training%20in%20latent%20space%0Aenables%20efficient%20computation%20while%20preserving%20perceptual%20fidelity%2C%20and%20this%0Aefficiency%20pairs%20naturally%20with%20plain%20transformers%2C%20whose%20performance%20scales%0Awith%20computational%20budget.%20Building%20on%20these%20choices%2C%20we%20analyze%20failure%20modes%0Athat%20emerge%20when%20naively%20scaling%20GANs.%20Specifically%2C%20we%20find%20issues%20as%0Aunderutilization%20of%20early%20layers%20in%20the%20generator%20and%20optimization%20instability%0Aas%20the%20network%20scales.%20Accordingly%2C%20we%20provide%20simple%20and%20scale-friendly%0Asolutions%20as%20lightweight%20intermediate%20supervision%20and%20width-aware%20learning-rate%0Aadjustment.%20Our%20experiments%20show%20that%20GAT%2C%20a%20purely%20transformer-based%20and%0Alatent-space%20GANs%2C%20can%20be%20easily%20trained%20reliably%20across%20a%20wide%20range%20of%0Acapacities%20%28S%20through%20XL%29.%20Moreover%2C%20GAT-XL/2%20achieves%20state-of-the-art%0Asingle-step%2C%20class-conditional%20generation%20performance%20%28FID%20of%202.96%29%20on%0AImageNet-256%20in%20just%2040%20epochs%2C%206x%20fewer%20epochs%20than%20strong%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24935v1&entry.124074799=Read"},
{"title": "StreamForest: Efficient Online Video Understanding with Persistent Event\n  Memory", "author": "Xiangyu Zeng and Kefan Qiu and Qingyu Zhang and Xinhao Li and Jing Wang and Jiaxin Li and Ziang Yan and Kun Tian and Meng Tian and Xinhai Zhao and Yi Wang and Limin Wang", "abstract": "  Multimodal Large Language Models (MLLMs) have recently achieved remarkable\nprogress in video understanding. However, their effectiveness in real-time\nstreaming scenarios remains limited due to storage constraints of historical\nvisual features and insufficient real-time spatiotemporal reasoning. To address\nthese challenges, we propose StreamForest, a novel architecture specifically\ndesigned for streaming video understanding. Central to StreamForest is the\nPersistent Event Memory Forest, a memory mechanism that adaptively organizes\nvideo frames into multiple event-level tree structures. This process is guided\nby penalty functions based on temporal distance, content similarity, and merge\nfrequency, enabling efficient long-term memory retention under limited\ncomputational resources. To enhance real-time perception, we introduce a\nFine-grained Spatiotemporal Window, which captures detailed short-term visual\ncues to improve current scene perception. Additionally, we present OnlineIT, an\ninstruction-tuning dataset tailored for streaming video tasks. OnlineIT\nsignificantly boosts MLLM performance in both real-time perception and future\nprediction. To evaluate generalization in practical applications, we introduce\nODV-Bench, a new benchmark focused on real-time streaming video understanding\nin autonomous driving scenarios. Experimental results demonstrate that\nStreamForest achieves the state-of-the-art performance, with accuracies of\n77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In\nparticular, even under extreme visual token compression (limited to 1024\ntokens), the model retains 96.8% of its average accuracy in eight benchmarks\nrelative to the default setting. These results underscore the robustness,\nefficiency, and generalizability of StreamForest for streaming video\nunderstanding.\n", "link": "http://arxiv.org/abs/2509.24871v1", "date": "2025-09-29", "relevancy": 2.2211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamForest%3A%20Efficient%20Online%20Video%20Understanding%20with%20Persistent%20Event%0A%20%20Memory&body=Title%3A%20StreamForest%3A%20Efficient%20Online%20Video%20Understanding%20with%20Persistent%20Event%0A%20%20Memory%0AAuthor%3A%20Xiangyu%20Zeng%20and%20Kefan%20Qiu%20and%20Qingyu%20Zhang%20and%20Xinhao%20Li%20and%20Jing%20Wang%20and%20Jiaxin%20Li%20and%20Ziang%20Yan%20and%20Kun%20Tian%20and%20Meng%20Tian%20and%20Xinhai%20Zhao%20and%20Yi%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20achieved%20remarkable%0Aprogress%20in%20video%20understanding.%20However%2C%20their%20effectiveness%20in%20real-time%0Astreaming%20scenarios%20remains%20limited%20due%20to%20storage%20constraints%20of%20historical%0Avisual%20features%20and%20insufficient%20real-time%20spatiotemporal%20reasoning.%20To%20address%0Athese%20challenges%2C%20we%20propose%20StreamForest%2C%20a%20novel%20architecture%20specifically%0Adesigned%20for%20streaming%20video%20understanding.%20Central%20to%20StreamForest%20is%20the%0APersistent%20Event%20Memory%20Forest%2C%20a%20memory%20mechanism%20that%20adaptively%20organizes%0Avideo%20frames%20into%20multiple%20event-level%20tree%20structures.%20This%20process%20is%20guided%0Aby%20penalty%20functions%20based%20on%20temporal%20distance%2C%20content%20similarity%2C%20and%20merge%0Afrequency%2C%20enabling%20efficient%20long-term%20memory%20retention%20under%20limited%0Acomputational%20resources.%20To%20enhance%20real-time%20perception%2C%20we%20introduce%20a%0AFine-grained%20Spatiotemporal%20Window%2C%20which%20captures%20detailed%20short-term%20visual%0Acues%20to%20improve%20current%20scene%20perception.%20Additionally%2C%20we%20present%20OnlineIT%2C%20an%0Ainstruction-tuning%20dataset%20tailored%20for%20streaming%20video%20tasks.%20OnlineIT%0Asignificantly%20boosts%20MLLM%20performance%20in%20both%20real-time%20perception%20and%20future%0Aprediction.%20To%20evaluate%20generalization%20in%20practical%20applications%2C%20we%20introduce%0AODV-Bench%2C%20a%20new%20benchmark%20focused%20on%20real-time%20streaming%20video%20understanding%0Ain%20autonomous%20driving%20scenarios.%20Experimental%20results%20demonstrate%20that%0AStreamForest%20achieves%20the%20state-of-the-art%20performance%2C%20with%20accuracies%20of%0A77.3%25%20on%20StreamingBench%2C%2060.5%25%20on%20OVBench%2C%20and%2055.6%25%20on%20OVO-Bench.%20In%0Aparticular%2C%20even%20under%20extreme%20visual%20token%20compression%20%28limited%20to%201024%0Atokens%29%2C%20the%20model%20retains%2096.8%25%20of%20its%20average%20accuracy%20in%20eight%20benchmarks%0Arelative%20to%20the%20default%20setting.%20These%20results%20underscore%20the%20robustness%2C%0Aefficiency%2C%20and%20generalizability%20of%20StreamForest%20for%20streaming%20video%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamForest%253A%2520Efficient%2520Online%2520Video%2520Understanding%2520with%2520Persistent%2520Event%250A%2520%2520Memory%26entry.906535625%3DXiangyu%2520Zeng%2520and%2520Kefan%2520Qiu%2520and%2520Qingyu%2520Zhang%2520and%2520Xinhao%2520Li%2520and%2520Jing%2520Wang%2520and%2520Jiaxin%2520Li%2520and%2520Ziang%2520Yan%2520and%2520Kun%2520Tian%2520and%2520Meng%2520Tian%2520and%2520Xinhai%2520Zhao%2520and%2520Yi%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520recently%2520achieved%2520remarkable%250Aprogress%2520in%2520video%2520understanding.%2520However%252C%2520their%2520effectiveness%2520in%2520real-time%250Astreaming%2520scenarios%2520remains%2520limited%2520due%2520to%2520storage%2520constraints%2520of%2520historical%250Avisual%2520features%2520and%2520insufficient%2520real-time%2520spatiotemporal%2520reasoning.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520StreamForest%252C%2520a%2520novel%2520architecture%2520specifically%250Adesigned%2520for%2520streaming%2520video%2520understanding.%2520Central%2520to%2520StreamForest%2520is%2520the%250APersistent%2520Event%2520Memory%2520Forest%252C%2520a%2520memory%2520mechanism%2520that%2520adaptively%2520organizes%250Avideo%2520frames%2520into%2520multiple%2520event-level%2520tree%2520structures.%2520This%2520process%2520is%2520guided%250Aby%2520penalty%2520functions%2520based%2520on%2520temporal%2520distance%252C%2520content%2520similarity%252C%2520and%2520merge%250Afrequency%252C%2520enabling%2520efficient%2520long-term%2520memory%2520retention%2520under%2520limited%250Acomputational%2520resources.%2520To%2520enhance%2520real-time%2520perception%252C%2520we%2520introduce%2520a%250AFine-grained%2520Spatiotemporal%2520Window%252C%2520which%2520captures%2520detailed%2520short-term%2520visual%250Acues%2520to%2520improve%2520current%2520scene%2520perception.%2520Additionally%252C%2520we%2520present%2520OnlineIT%252C%2520an%250Ainstruction-tuning%2520dataset%2520tailored%2520for%2520streaming%2520video%2520tasks.%2520OnlineIT%250Asignificantly%2520boosts%2520MLLM%2520performance%2520in%2520both%2520real-time%2520perception%2520and%2520future%250Aprediction.%2520To%2520evaluate%2520generalization%2520in%2520practical%2520applications%252C%2520we%2520introduce%250AODV-Bench%252C%2520a%2520new%2520benchmark%2520focused%2520on%2520real-time%2520streaming%2520video%2520understanding%250Ain%2520autonomous%2520driving%2520scenarios.%2520Experimental%2520results%2520demonstrate%2520that%250AStreamForest%2520achieves%2520the%2520state-of-the-art%2520performance%252C%2520with%2520accuracies%2520of%250A77.3%2525%2520on%2520StreamingBench%252C%252060.5%2525%2520on%2520OVBench%252C%2520and%252055.6%2525%2520on%2520OVO-Bench.%2520In%250Aparticular%252C%2520even%2520under%2520extreme%2520visual%2520token%2520compression%2520%2528limited%2520to%25201024%250Atokens%2529%252C%2520the%2520model%2520retains%252096.8%2525%2520of%2520its%2520average%2520accuracy%2520in%2520eight%2520benchmarks%250Arelative%2520to%2520the%2520default%2520setting.%2520These%2520results%2520underscore%2520the%2520robustness%252C%250Aefficiency%252C%2520and%2520generalizability%2520of%2520StreamForest%2520for%2520streaming%2520video%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamForest%3A%20Efficient%20Online%20Video%20Understanding%20with%20Persistent%20Event%0A%20%20Memory&entry.906535625=Xiangyu%20Zeng%20and%20Kefan%20Qiu%20and%20Qingyu%20Zhang%20and%20Xinhao%20Li%20and%20Jing%20Wang%20and%20Jiaxin%20Li%20and%20Ziang%20Yan%20and%20Kun%20Tian%20and%20Meng%20Tian%20and%20Xinhai%20Zhao%20and%20Yi%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20achieved%20remarkable%0Aprogress%20in%20video%20understanding.%20However%2C%20their%20effectiveness%20in%20real-time%0Astreaming%20scenarios%20remains%20limited%20due%20to%20storage%20constraints%20of%20historical%0Avisual%20features%20and%20insufficient%20real-time%20spatiotemporal%20reasoning.%20To%20address%0Athese%20challenges%2C%20we%20propose%20StreamForest%2C%20a%20novel%20architecture%20specifically%0Adesigned%20for%20streaming%20video%20understanding.%20Central%20to%20StreamForest%20is%20the%0APersistent%20Event%20Memory%20Forest%2C%20a%20memory%20mechanism%20that%20adaptively%20organizes%0Avideo%20frames%20into%20multiple%20event-level%20tree%20structures.%20This%20process%20is%20guided%0Aby%20penalty%20functions%20based%20on%20temporal%20distance%2C%20content%20similarity%2C%20and%20merge%0Afrequency%2C%20enabling%20efficient%20long-term%20memory%20retention%20under%20limited%0Acomputational%20resources.%20To%20enhance%20real-time%20perception%2C%20we%20introduce%20a%0AFine-grained%20Spatiotemporal%20Window%2C%20which%20captures%20detailed%20short-term%20visual%0Acues%20to%20improve%20current%20scene%20perception.%20Additionally%2C%20we%20present%20OnlineIT%2C%20an%0Ainstruction-tuning%20dataset%20tailored%20for%20streaming%20video%20tasks.%20OnlineIT%0Asignificantly%20boosts%20MLLM%20performance%20in%20both%20real-time%20perception%20and%20future%0Aprediction.%20To%20evaluate%20generalization%20in%20practical%20applications%2C%20we%20introduce%0AODV-Bench%2C%20a%20new%20benchmark%20focused%20on%20real-time%20streaming%20video%20understanding%0Ain%20autonomous%20driving%20scenarios.%20Experimental%20results%20demonstrate%20that%0AStreamForest%20achieves%20the%20state-of-the-art%20performance%2C%20with%20accuracies%20of%0A77.3%25%20on%20StreamingBench%2C%2060.5%25%20on%20OVBench%2C%20and%2055.6%25%20on%20OVO-Bench.%20In%0Aparticular%2C%20even%20under%20extreme%20visual%20token%20compression%20%28limited%20to%201024%0Atokens%29%2C%20the%20model%20retains%2096.8%25%20of%20its%20average%20accuracy%20in%20eight%20benchmarks%0Arelative%20to%20the%20default%20setting.%20These%20results%20underscore%20the%20robustness%2C%0Aefficiency%2C%20and%20generalizability%20of%20StreamForest%20for%20streaming%20video%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24871v1&entry.124074799=Read"},
{"title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning", "author": "Xinhao Li and Ziang Yan and Desen Meng and Lu Dong and Xiangyu Zeng and Yinan He and Yali Wang and Yu Qiao and Yi Wang and Limin Wang", "abstract": "  Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.\n", "link": "http://arxiv.org/abs/2504.06958v4", "date": "2025-09-29", "relevancy": 2.2156, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5788}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.549}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoChat-R1%3A%20Enhancing%20Spatio-Temporal%20Perception%20via%20Reinforcement%0A%20%20Fine-Tuning&body=Title%3A%20VideoChat-R1%3A%20Enhancing%20Spatio-Temporal%20Perception%20via%20Reinforcement%0A%20%20Fine-Tuning%0AAuthor%3A%20Xinhao%20Li%20and%20Ziang%20Yan%20and%20Desen%20Meng%20and%20Lu%20Dong%20and%20Xiangyu%20Zeng%20and%20Yinan%20He%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Yi%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reinforcement%20learning%20have%20significantly%20advanced%20the%0Areasoning%20capabilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%0Aapproaches%20such%20as%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20and%20rule-based%0Areward%20mechanisms%20demonstrate%20promise%20in%20text%20and%20image%20domains%2C%20their%0Aapplication%20to%20video%20understanding%20remains%20limited.%20This%20paper%20presents%20a%0Asystematic%20exploration%20of%20Reinforcement%20Fine-Tuning%20%28RFT%29%20with%20GRPO%20for%20video%0AMLLMs%2C%20aiming%20to%20enhance%20spatio-temporal%20perception%20while%20maintaining%20general%0Acapabilities.%20Our%20experiments%20reveal%20that%20RFT%20is%20highly%20data-efficient%20for%0Atask-specific%20improvements.%20Through%20multi-task%20RFT%20on%20spatio-temporal%0Aperception%20objectives%20with%20limited%20samples%2C%20we%20develop%20VideoChat-R1%2C%20a%20powerful%0Avideo%20MLLM%20that%20achieves%20state-of-the-art%20performance%20on%20spatio-temporal%0Aperception%20tasks%20without%20sacrificing%20chat%20ability%2C%20while%20exhibiting%20emerging%0Aspatio-temporal%20reasoning%20abilities.%20Compared%20to%20Qwen2.5-VL-7B%2C%20VideoChat-R1%0Aboosts%20performance%20several-fold%20in%20tasks%20like%20temporal%20grounding%20%28%2B31.8%29%20and%0Aobject%20tracking%20%28%2B31.2%29.%20Additionally%2C%20it%20significantly%20improves%20on%20general%20QA%0Abenchmarks%20such%20as%20VideoMME%20%28%2B0.9%29%2C%20MVBench%20%28%2B1.0%29%2C%20and%20Perception%20Test%20%28%2B0.9%29.%0AOur%20findings%20underscore%20the%20potential%20of%20RFT%20for%20specialized%20task%20enhancement%0Aof%20Video%20MLLMs.%20We%20hope%20our%20work%20offers%20valuable%20insights%20for%20future%20RL%0Aresearch%20in%20video%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06958v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoChat-R1%253A%2520Enhancing%2520Spatio-Temporal%2520Perception%2520via%2520Reinforcement%250A%2520%2520Fine-Tuning%26entry.906535625%3DXinhao%2520Li%2520and%2520Ziang%2520Yan%2520and%2520Desen%2520Meng%2520and%2520Lu%2520Dong%2520and%2520Xiangyu%2520Zeng%2520and%2520Yinan%2520He%2520and%2520Yali%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Yi%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reinforcement%2520learning%2520have%2520significantly%2520advanced%2520the%250Areasoning%2520capabilities%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520While%250Aapproaches%2520such%2520as%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520and%2520rule-based%250Areward%2520mechanisms%2520demonstrate%2520promise%2520in%2520text%2520and%2520image%2520domains%252C%2520their%250Aapplication%2520to%2520video%2520understanding%2520remains%2520limited.%2520This%2520paper%2520presents%2520a%250Asystematic%2520exploration%2520of%2520Reinforcement%2520Fine-Tuning%2520%2528RFT%2529%2520with%2520GRPO%2520for%2520video%250AMLLMs%252C%2520aiming%2520to%2520enhance%2520spatio-temporal%2520perception%2520while%2520maintaining%2520general%250Acapabilities.%2520Our%2520experiments%2520reveal%2520that%2520RFT%2520is%2520highly%2520data-efficient%2520for%250Atask-specific%2520improvements.%2520Through%2520multi-task%2520RFT%2520on%2520spatio-temporal%250Aperception%2520objectives%2520with%2520limited%2520samples%252C%2520we%2520develop%2520VideoChat-R1%252C%2520a%2520powerful%250Avideo%2520MLLM%2520that%2520achieves%2520state-of-the-art%2520performance%2520on%2520spatio-temporal%250Aperception%2520tasks%2520without%2520sacrificing%2520chat%2520ability%252C%2520while%2520exhibiting%2520emerging%250Aspatio-temporal%2520reasoning%2520abilities.%2520Compared%2520to%2520Qwen2.5-VL-7B%252C%2520VideoChat-R1%250Aboosts%2520performance%2520several-fold%2520in%2520tasks%2520like%2520temporal%2520grounding%2520%2528%252B31.8%2529%2520and%250Aobject%2520tracking%2520%2528%252B31.2%2529.%2520Additionally%252C%2520it%2520significantly%2520improves%2520on%2520general%2520QA%250Abenchmarks%2520such%2520as%2520VideoMME%2520%2528%252B0.9%2529%252C%2520MVBench%2520%2528%252B1.0%2529%252C%2520and%2520Perception%2520Test%2520%2528%252B0.9%2529.%250AOur%2520findings%2520underscore%2520the%2520potential%2520of%2520RFT%2520for%2520specialized%2520task%2520enhancement%250Aof%2520Video%2520MLLMs.%2520We%2520hope%2520our%2520work%2520offers%2520valuable%2520insights%2520for%2520future%2520RL%250Aresearch%2520in%2520video%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06958v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoChat-R1%3A%20Enhancing%20Spatio-Temporal%20Perception%20via%20Reinforcement%0A%20%20Fine-Tuning&entry.906535625=Xinhao%20Li%20and%20Ziang%20Yan%20and%20Desen%20Meng%20and%20Lu%20Dong%20and%20Xiangyu%20Zeng%20and%20Yinan%20He%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Yi%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20reinforcement%20learning%20have%20significantly%20advanced%20the%0Areasoning%20capabilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%0Aapproaches%20such%20as%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20and%20rule-based%0Areward%20mechanisms%20demonstrate%20promise%20in%20text%20and%20image%20domains%2C%20their%0Aapplication%20to%20video%20understanding%20remains%20limited.%20This%20paper%20presents%20a%0Asystematic%20exploration%20of%20Reinforcement%20Fine-Tuning%20%28RFT%29%20with%20GRPO%20for%20video%0AMLLMs%2C%20aiming%20to%20enhance%20spatio-temporal%20perception%20while%20maintaining%20general%0Acapabilities.%20Our%20experiments%20reveal%20that%20RFT%20is%20highly%20data-efficient%20for%0Atask-specific%20improvements.%20Through%20multi-task%20RFT%20on%20spatio-temporal%0Aperception%20objectives%20with%20limited%20samples%2C%20we%20develop%20VideoChat-R1%2C%20a%20powerful%0Avideo%20MLLM%20that%20achieves%20state-of-the-art%20performance%20on%20spatio-temporal%0Aperception%20tasks%20without%20sacrificing%20chat%20ability%2C%20while%20exhibiting%20emerging%0Aspatio-temporal%20reasoning%20abilities.%20Compared%20to%20Qwen2.5-VL-7B%2C%20VideoChat-R1%0Aboosts%20performance%20several-fold%20in%20tasks%20like%20temporal%20grounding%20%28%2B31.8%29%20and%0Aobject%20tracking%20%28%2B31.2%29.%20Additionally%2C%20it%20significantly%20improves%20on%20general%20QA%0Abenchmarks%20such%20as%20VideoMME%20%28%2B0.9%29%2C%20MVBench%20%28%2B1.0%29%2C%20and%20Perception%20Test%20%28%2B0.9%29.%0AOur%20findings%20underscore%20the%20potential%20of%20RFT%20for%20specialized%20task%20enhancement%0Aof%20Video%20MLLMs.%20We%20hope%20our%20work%20offers%20valuable%20insights%20for%20future%20RL%0Aresearch%20in%20video%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06958v4&entry.124074799=Read"},
{"title": "Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis", "author": "Tian Xia and Matthew Sinclair and Andreas Schuh and Fabio De Sousa Ribeiro and Raghav Mehta and Rajat Rasal and Esther Puyol-Ant\u00f3n and Samuel Gerber and Kersten Petersen and Michiel Schaap and Ben Glocker", "abstract": "  Counterfactual image generation is a powerful tool for augmenting training\ndata, de-biasing datasets, and modeling disease. Current approaches rely on\nexternal classifiers or regressors to increase the effectiveness of\nsubject-level interventions (e.g., changing the patient's age). For\nstructure-specific interventions (e.g., changing the area of the left lung in a\nchest radiograph), we show that this is insufficient, and can result in\nundesirable global effects across the image domain. Previous work used\npixel-level label maps as guidance, requiring a user to provide hypothetical\nsegmentations which are tedious and difficult to obtain. We propose\nSegmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the\nsimplicity of intervening on scalar-valued, structure-specific variables while\nproducing locally coherent and effective counterfactuals. We demonstrate the\ncapability of generating realistic chest radiographs, and we show promising\nresults for modeling coronary artery disease. Code:\nhttps://github.com/biomedia-mira/seg-cft.\n", "link": "http://arxiv.org/abs/2509.24913v1", "date": "2025-09-29", "relevancy": 2.203, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5618}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5529}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentor-Guided%20Counterfactual%20Fine-Tuning%20for%20Image%20Synthesis&body=Title%3A%20Segmentor-Guided%20Counterfactual%20Fine-Tuning%20for%20Image%20Synthesis%0AAuthor%3A%20Tian%20Xia%20and%20Matthew%20Sinclair%20and%20Andreas%20Schuh%20and%20Fabio%20De%20Sousa%20Ribeiro%20and%20Raghav%20Mehta%20and%20Rajat%20Rasal%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Samuel%20Gerber%20and%20Kersten%20Petersen%20and%20Michiel%20Schaap%20and%20Ben%20Glocker%0AAbstract%3A%20%20%20Counterfactual%20image%20generation%20is%20a%20powerful%20tool%20for%20augmenting%20training%0Adata%2C%20de-biasing%20datasets%2C%20and%20modeling%20disease.%20Current%20approaches%20rely%20on%0Aexternal%20classifiers%20or%20regressors%20to%20increase%20the%20effectiveness%20of%0Asubject-level%20interventions%20%28e.g.%2C%20changing%20the%20patient%27s%20age%29.%20For%0Astructure-specific%20interventions%20%28e.g.%2C%20changing%20the%20area%20of%20the%20left%20lung%20in%20a%0Achest%20radiograph%29%2C%20we%20show%20that%20this%20is%20insufficient%2C%20and%20can%20result%20in%0Aundesirable%20global%20effects%20across%20the%20image%20domain.%20Previous%20work%20used%0Apixel-level%20label%20maps%20as%20guidance%2C%20requiring%20a%20user%20to%20provide%20hypothetical%0Asegmentations%20which%20are%20tedious%20and%20difficult%20to%20obtain.%20We%20propose%0ASegmentor-guided%20Counterfactual%20Fine-Tuning%20%28Seg-CFT%29%2C%20which%20preserves%20the%0Asimplicity%20of%20intervening%20on%20scalar-valued%2C%20structure-specific%20variables%20while%0Aproducing%20locally%20coherent%20and%20effective%20counterfactuals.%20We%20demonstrate%20the%0Acapability%20of%20generating%20realistic%20chest%20radiographs%2C%20and%20we%20show%20promising%0Aresults%20for%20modeling%20coronary%20artery%20disease.%20Code%3A%0Ahttps%3A//github.com/biomedia-mira/seg-cft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentor-Guided%2520Counterfactual%2520Fine-Tuning%2520for%2520Image%2520Synthesis%26entry.906535625%3DTian%2520Xia%2520and%2520Matthew%2520Sinclair%2520and%2520Andreas%2520Schuh%2520and%2520Fabio%2520De%2520Sousa%2520Ribeiro%2520and%2520Raghav%2520Mehta%2520and%2520Rajat%2520Rasal%2520and%2520Esther%2520Puyol-Ant%25C3%25B3n%2520and%2520Samuel%2520Gerber%2520and%2520Kersten%2520Petersen%2520and%2520Michiel%2520Schaap%2520and%2520Ben%2520Glocker%26entry.1292438233%3D%2520%2520Counterfactual%2520image%2520generation%2520is%2520a%2520powerful%2520tool%2520for%2520augmenting%2520training%250Adata%252C%2520de-biasing%2520datasets%252C%2520and%2520modeling%2520disease.%2520Current%2520approaches%2520rely%2520on%250Aexternal%2520classifiers%2520or%2520regressors%2520to%2520increase%2520the%2520effectiveness%2520of%250Asubject-level%2520interventions%2520%2528e.g.%252C%2520changing%2520the%2520patient%2527s%2520age%2529.%2520For%250Astructure-specific%2520interventions%2520%2528e.g.%252C%2520changing%2520the%2520area%2520of%2520the%2520left%2520lung%2520in%2520a%250Achest%2520radiograph%2529%252C%2520we%2520show%2520that%2520this%2520is%2520insufficient%252C%2520and%2520can%2520result%2520in%250Aundesirable%2520global%2520effects%2520across%2520the%2520image%2520domain.%2520Previous%2520work%2520used%250Apixel-level%2520label%2520maps%2520as%2520guidance%252C%2520requiring%2520a%2520user%2520to%2520provide%2520hypothetical%250Asegmentations%2520which%2520are%2520tedious%2520and%2520difficult%2520to%2520obtain.%2520We%2520propose%250ASegmentor-guided%2520Counterfactual%2520Fine-Tuning%2520%2528Seg-CFT%2529%252C%2520which%2520preserves%2520the%250Asimplicity%2520of%2520intervening%2520on%2520scalar-valued%252C%2520structure-specific%2520variables%2520while%250Aproducing%2520locally%2520coherent%2520and%2520effective%2520counterfactuals.%2520We%2520demonstrate%2520the%250Acapability%2520of%2520generating%2520realistic%2520chest%2520radiographs%252C%2520and%2520we%2520show%2520promising%250Aresults%2520for%2520modeling%2520coronary%2520artery%2520disease.%2520Code%253A%250Ahttps%253A//github.com/biomedia-mira/seg-cft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentor-Guided%20Counterfactual%20Fine-Tuning%20for%20Image%20Synthesis&entry.906535625=Tian%20Xia%20and%20Matthew%20Sinclair%20and%20Andreas%20Schuh%20and%20Fabio%20De%20Sousa%20Ribeiro%20and%20Raghav%20Mehta%20and%20Rajat%20Rasal%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Samuel%20Gerber%20and%20Kersten%20Petersen%20and%20Michiel%20Schaap%20and%20Ben%20Glocker&entry.1292438233=%20%20Counterfactual%20image%20generation%20is%20a%20powerful%20tool%20for%20augmenting%20training%0Adata%2C%20de-biasing%20datasets%2C%20and%20modeling%20disease.%20Current%20approaches%20rely%20on%0Aexternal%20classifiers%20or%20regressors%20to%20increase%20the%20effectiveness%20of%0Asubject-level%20interventions%20%28e.g.%2C%20changing%20the%20patient%27s%20age%29.%20For%0Astructure-specific%20interventions%20%28e.g.%2C%20changing%20the%20area%20of%20the%20left%20lung%20in%20a%0Achest%20radiograph%29%2C%20we%20show%20that%20this%20is%20insufficient%2C%20and%20can%20result%20in%0Aundesirable%20global%20effects%20across%20the%20image%20domain.%20Previous%20work%20used%0Apixel-level%20label%20maps%20as%20guidance%2C%20requiring%20a%20user%20to%20provide%20hypothetical%0Asegmentations%20which%20are%20tedious%20and%20difficult%20to%20obtain.%20We%20propose%0ASegmentor-guided%20Counterfactual%20Fine-Tuning%20%28Seg-CFT%29%2C%20which%20preserves%20the%0Asimplicity%20of%20intervening%20on%20scalar-valued%2C%20structure-specific%20variables%20while%0Aproducing%20locally%20coherent%20and%20effective%20counterfactuals.%20We%20demonstrate%20the%0Acapability%20of%20generating%20realistic%20chest%20radiographs%2C%20and%20we%20show%20promising%0Aresults%20for%20modeling%20coronary%20artery%20disease.%20Code%3A%0Ahttps%3A//github.com/biomedia-mira/seg-cft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24913v1&entry.124074799=Read"},
{"title": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?", "author": "Millicent Li and Alberto Mario Ceballos Arroyo and Giordano Rogers and Naomi Saphra and Byron C. Wallace", "abstract": "  Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey can succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets may not be ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the knowledge of the target LLM whose activations are\ndecoded. Taken together, our results indicate a need for targeted benchmarks\nand experimental controls to rigorously assess whether verbalization methods\nprovide meaningful insights into the operations of LLMs.\n", "link": "http://arxiv.org/abs/2509.13316v2", "date": "2025-09-29", "relevancy": 2.1991, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%0A%20%20Information%3F&body=Title%3A%20Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%0A%20%20Information%3F%0AAuthor%3A%20Millicent%20Li%20and%20Alberto%20Mario%20Ceballos%20Arroyo%20and%20Giordano%20Rogers%20and%20Naomi%20Saphra%20and%20Byron%20C.%20Wallace%0AAbstract%3A%20%20%20Recent%20interpretability%20methods%20have%20proposed%20to%20translate%20LLM%20internal%0Arepresentations%20into%20natural%20language%20descriptions%20using%20a%20second%20verbalizer%0ALLM.%20This%20is%20intended%20to%20illuminate%20how%20the%20target%20model%20represents%20and%0Aoperates%20on%20inputs.%20But%20do%20such%20activation%20verbalization%20approaches%20actually%0Aprovide%20privileged%20knowledge%20about%20the%20internal%20workings%20of%20the%20target%20model%2C%0Aor%20do%20they%20merely%20convey%20information%20about%20its%20inputs%3F%20We%20critically%20evaluate%0Apopular%20verbalization%20methods%20across%20datasets%20used%20in%20prior%20work%20and%20find%20that%0Athey%20can%20succeed%20at%20benchmarks%20without%20any%20access%20to%20target%20model%20internals%2C%0Asuggesting%20that%20these%20datasets%20may%20not%20be%20ideal%20for%20evaluating%20verbalization%0Amethods.%20We%20then%20run%20controlled%20experiments%20which%20reveal%20that%20verbalizations%0Aoften%20reflect%20the%20parametric%20knowledge%20of%20the%20verbalizer%20LLM%20which%20generated%0Athem%2C%20rather%20than%20the%20knowledge%20of%20the%20target%20LLM%20whose%20activations%20are%0Adecoded.%20Taken%20together%2C%20our%20results%20indicate%20a%20need%20for%20targeted%20benchmarks%0Aand%20experimental%20controls%20to%20rigorously%20assess%20whether%20verbalization%20methods%0Aprovide%20meaningful%20insights%20into%20the%20operations%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Natural%2520Language%2520Descriptions%2520of%2520Model%2520Activations%2520Convey%2520Privileged%250A%2520%2520Information%253F%26entry.906535625%3DMillicent%2520Li%2520and%2520Alberto%2520Mario%2520Ceballos%2520Arroyo%2520and%2520Giordano%2520Rogers%2520and%2520Naomi%2520Saphra%2520and%2520Byron%2520C.%2520Wallace%26entry.1292438233%3D%2520%2520Recent%2520interpretability%2520methods%2520have%2520proposed%2520to%2520translate%2520LLM%2520internal%250Arepresentations%2520into%2520natural%2520language%2520descriptions%2520using%2520a%2520second%2520verbalizer%250ALLM.%2520This%2520is%2520intended%2520to%2520illuminate%2520how%2520the%2520target%2520model%2520represents%2520and%250Aoperates%2520on%2520inputs.%2520But%2520do%2520such%2520activation%2520verbalization%2520approaches%2520actually%250Aprovide%2520privileged%2520knowledge%2520about%2520the%2520internal%2520workings%2520of%2520the%2520target%2520model%252C%250Aor%2520do%2520they%2520merely%2520convey%2520information%2520about%2520its%2520inputs%253F%2520We%2520critically%2520evaluate%250Apopular%2520verbalization%2520methods%2520across%2520datasets%2520used%2520in%2520prior%2520work%2520and%2520find%2520that%250Athey%2520can%2520succeed%2520at%2520benchmarks%2520without%2520any%2520access%2520to%2520target%2520model%2520internals%252C%250Asuggesting%2520that%2520these%2520datasets%2520may%2520not%2520be%2520ideal%2520for%2520evaluating%2520verbalization%250Amethods.%2520We%2520then%2520run%2520controlled%2520experiments%2520which%2520reveal%2520that%2520verbalizations%250Aoften%2520reflect%2520the%2520parametric%2520knowledge%2520of%2520the%2520verbalizer%2520LLM%2520which%2520generated%250Athem%252C%2520rather%2520than%2520the%2520knowledge%2520of%2520the%2520target%2520LLM%2520whose%2520activations%2520are%250Adecoded.%2520Taken%2520together%252C%2520our%2520results%2520indicate%2520a%2520need%2520for%2520targeted%2520benchmarks%250Aand%2520experimental%2520controls%2520to%2520rigorously%2520assess%2520whether%2520verbalization%2520methods%250Aprovide%2520meaningful%2520insights%2520into%2520the%2520operations%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%0A%20%20Information%3F&entry.906535625=Millicent%20Li%20and%20Alberto%20Mario%20Ceballos%20Arroyo%20and%20Giordano%20Rogers%20and%20Naomi%20Saphra%20and%20Byron%20C.%20Wallace&entry.1292438233=%20%20Recent%20interpretability%20methods%20have%20proposed%20to%20translate%20LLM%20internal%0Arepresentations%20into%20natural%20language%20descriptions%20using%20a%20second%20verbalizer%0ALLM.%20This%20is%20intended%20to%20illuminate%20how%20the%20target%20model%20represents%20and%0Aoperates%20on%20inputs.%20But%20do%20such%20activation%20verbalization%20approaches%20actually%0Aprovide%20privileged%20knowledge%20about%20the%20internal%20workings%20of%20the%20target%20model%2C%0Aor%20do%20they%20merely%20convey%20information%20about%20its%20inputs%3F%20We%20critically%20evaluate%0Apopular%20verbalization%20methods%20across%20datasets%20used%20in%20prior%20work%20and%20find%20that%0Athey%20can%20succeed%20at%20benchmarks%20without%20any%20access%20to%20target%20model%20internals%2C%0Asuggesting%20that%20these%20datasets%20may%20not%20be%20ideal%20for%20evaluating%20verbalization%0Amethods.%20We%20then%20run%20controlled%20experiments%20which%20reveal%20that%20verbalizations%0Aoften%20reflect%20the%20parametric%20knowledge%20of%20the%20verbalizer%20LLM%20which%20generated%0Athem%2C%20rather%20than%20the%20knowledge%20of%20the%20target%20LLM%20whose%20activations%20are%0Adecoded.%20Taken%20together%2C%20our%20results%20indicate%20a%20need%20for%20targeted%20benchmarks%0Aand%20experimental%20controls%20to%20rigorously%20assess%20whether%20verbalization%20methods%0Aprovide%20meaningful%20insights%20into%20the%20operations%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13316v2&entry.124074799=Read"},
{"title": "On Spectral Learning for Odeco Tensors: Perturbation, Initialization,\n  and Algorithms", "author": "Arnab Auddy and Ming Yuan", "abstract": "  We study spectral learning for orthogonally decomposable (odeco) tensors,\nemphasizing the interplay between statistical limits, optimization geometry,\nand initialization. Unlike matrices, recovery for odeco tensors does not hinge\non eigengaps, yielding improved robustness under noise. While iterative methods\nsuch as tensor power iterations can be statistically efficient, initialization\nemerges as the main computational bottleneck. We investigate perturbation\nbounds, non-convex optimization analysis, and initialization strategies,\nclarifying when efficient algorithms attain statistical limits and when\nfundamental barriers remain.\n", "link": "http://arxiv.org/abs/2509.25126v1", "date": "2025-09-29", "relevancy": 2.1964, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4426}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4387}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Spectral%20Learning%20for%20Odeco%20Tensors%3A%20Perturbation%2C%20Initialization%2C%0A%20%20and%20Algorithms&body=Title%3A%20On%20Spectral%20Learning%20for%20Odeco%20Tensors%3A%20Perturbation%2C%20Initialization%2C%0A%20%20and%20Algorithms%0AAuthor%3A%20Arnab%20Auddy%20and%20Ming%20Yuan%0AAbstract%3A%20%20%20We%20study%20spectral%20learning%20for%20orthogonally%20decomposable%20%28odeco%29%20tensors%2C%0Aemphasizing%20the%20interplay%20between%20statistical%20limits%2C%20optimization%20geometry%2C%0Aand%20initialization.%20Unlike%20matrices%2C%20recovery%20for%20odeco%20tensors%20does%20not%20hinge%0Aon%20eigengaps%2C%20yielding%20improved%20robustness%20under%20noise.%20While%20iterative%20methods%0Asuch%20as%20tensor%20power%20iterations%20can%20be%20statistically%20efficient%2C%20initialization%0Aemerges%20as%20the%20main%20computational%20bottleneck.%20We%20investigate%20perturbation%0Abounds%2C%20non-convex%20optimization%20analysis%2C%20and%20initialization%20strategies%2C%0Aclarifying%20when%20efficient%20algorithms%20attain%20statistical%20limits%20and%20when%0Afundamental%20barriers%20remain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Spectral%2520Learning%2520for%2520Odeco%2520Tensors%253A%2520Perturbation%252C%2520Initialization%252C%250A%2520%2520and%2520Algorithms%26entry.906535625%3DArnab%2520Auddy%2520and%2520Ming%2520Yuan%26entry.1292438233%3D%2520%2520We%2520study%2520spectral%2520learning%2520for%2520orthogonally%2520decomposable%2520%2528odeco%2529%2520tensors%252C%250Aemphasizing%2520the%2520interplay%2520between%2520statistical%2520limits%252C%2520optimization%2520geometry%252C%250Aand%2520initialization.%2520Unlike%2520matrices%252C%2520recovery%2520for%2520odeco%2520tensors%2520does%2520not%2520hinge%250Aon%2520eigengaps%252C%2520yielding%2520improved%2520robustness%2520under%2520noise.%2520While%2520iterative%2520methods%250Asuch%2520as%2520tensor%2520power%2520iterations%2520can%2520be%2520statistically%2520efficient%252C%2520initialization%250Aemerges%2520as%2520the%2520main%2520computational%2520bottleneck.%2520We%2520investigate%2520perturbation%250Abounds%252C%2520non-convex%2520optimization%2520analysis%252C%2520and%2520initialization%2520strategies%252C%250Aclarifying%2520when%2520efficient%2520algorithms%2520attain%2520statistical%2520limits%2520and%2520when%250Afundamental%2520barriers%2520remain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Spectral%20Learning%20for%20Odeco%20Tensors%3A%20Perturbation%2C%20Initialization%2C%0A%20%20and%20Algorithms&entry.906535625=Arnab%20Auddy%20and%20Ming%20Yuan&entry.1292438233=%20%20We%20study%20spectral%20learning%20for%20orthogonally%20decomposable%20%28odeco%29%20tensors%2C%0Aemphasizing%20the%20interplay%20between%20statistical%20limits%2C%20optimization%20geometry%2C%0Aand%20initialization.%20Unlike%20matrices%2C%20recovery%20for%20odeco%20tensors%20does%20not%20hinge%0Aon%20eigengaps%2C%20yielding%20improved%20robustness%20under%20noise.%20While%20iterative%20methods%0Asuch%20as%20tensor%20power%20iterations%20can%20be%20statistically%20efficient%2C%20initialization%0Aemerges%20as%20the%20main%20computational%20bottleneck.%20We%20investigate%20perturbation%0Abounds%2C%20non-convex%20optimization%20analysis%2C%20and%20initialization%20strategies%2C%0Aclarifying%20when%20efficient%20algorithms%20attain%20statistical%20limits%20and%20when%0Afundamental%20barriers%20remain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25126v1&entry.124074799=Read"},
{"title": "A Scalable Distributed Framework for Multimodal GigaVoxel Image\n  Registration", "author": "Rohit Jena and Vedant Zope and Pratik Chaudhari and James C. Gee", "abstract": "  In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels\nsupplemented with a distributed framework for image registration at\nunprecedented scales. Image registration is an inverse problem fundamental to\nbiomedical and life sciences, but algorithms have not scaled in tandem with\nimage acquisition capabilities. Our framework complements existing model\nparallelism techniques proposed for large-scale transformer training by\noptimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding.\nWe demonstrate unprecedented capabilities by performing multimodal registration\nof a 100 micron ex-vivo human brain MRI volume at native resolution - an\ninverse problem more than 570x larger than a standard clinical datum in about a\nminute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art\noptimization and deep learning registration pipelines by upto 6 - 7x while\nreducing peak memory consumption by 20 - 59%. Comparative analysis on a 250\nmicron dataset shows that FFDP can fit upto 64x larger problems than existing\nSOTA on a single GPU, and highlights both the performance and efficiency gains\nof FFDP compared to SOTA image registration methods.\n", "link": "http://arxiv.org/abs/2509.25044v1", "date": "2025-09-29", "relevancy": 2.1851, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5593}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5482}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Distributed%20Framework%20for%20Multimodal%20GigaVoxel%20Image%0A%20%20Registration&body=Title%3A%20A%20Scalable%20Distributed%20Framework%20for%20Multimodal%20GigaVoxel%20Image%0A%20%20Registration%0AAuthor%3A%20Rohit%20Jena%20and%20Vedant%20Zope%20and%20Pratik%20Chaudhari%20and%20James%20C.%20Gee%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20FFDP%2C%20a%20set%20of%20IO-aware%20non-GEMM%20fused%20kernels%0Asupplemented%20with%20a%20distributed%20framework%20for%20image%20registration%20at%0Aunprecedented%20scales.%20Image%20registration%20is%20an%20inverse%20problem%20fundamental%20to%0Abiomedical%20and%20life%20sciences%2C%20but%20algorithms%20have%20not%20scaled%20in%20tandem%20with%0Aimage%20acquisition%20capabilities.%20Our%20framework%20complements%20existing%20model%0Aparallelism%20techniques%20proposed%20for%20large-scale%20transformer%20training%20by%0Aoptimizing%20non-GEMM%20bottlenecks%20and%20enabling%20convolution-aware%20tensor%20sharding.%0AWe%20demonstrate%20unprecedented%20capabilities%20by%20performing%20multimodal%20registration%0Aof%20a%20100%20micron%20ex-vivo%20human%20brain%20MRI%20volume%20at%20native%20resolution%20-%20an%0Ainverse%20problem%20more%20than%20570x%20larger%20than%20a%20standard%20clinical%20datum%20in%20about%20a%0Aminute%20using%20only%208%20A6000%20GPUs.%20FFDP%20accelerates%20existing%20state-of-the-art%0Aoptimization%20and%20deep%20learning%20registration%20pipelines%20by%20upto%206%20-%207x%20while%0Areducing%20peak%20memory%20consumption%20by%2020%20-%2059%25.%20Comparative%20analysis%20on%20a%20250%0Amicron%20dataset%20shows%20that%20FFDP%20can%20fit%20upto%2064x%20larger%20problems%20than%20existing%0ASOTA%20on%20a%20single%20GPU%2C%20and%20highlights%20both%20the%20performance%20and%20efficiency%20gains%0Aof%20FFDP%20compared%20to%20SOTA%20image%20registration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Distributed%2520Framework%2520for%2520Multimodal%2520GigaVoxel%2520Image%250A%2520%2520Registration%26entry.906535625%3DRohit%2520Jena%2520and%2520Vedant%2520Zope%2520and%2520Pratik%2520Chaudhari%2520and%2520James%2520C.%2520Gee%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520FFDP%252C%2520a%2520set%2520of%2520IO-aware%2520non-GEMM%2520fused%2520kernels%250Asupplemented%2520with%2520a%2520distributed%2520framework%2520for%2520image%2520registration%2520at%250Aunprecedented%2520scales.%2520Image%2520registration%2520is%2520an%2520inverse%2520problem%2520fundamental%2520to%250Abiomedical%2520and%2520life%2520sciences%252C%2520but%2520algorithms%2520have%2520not%2520scaled%2520in%2520tandem%2520with%250Aimage%2520acquisition%2520capabilities.%2520Our%2520framework%2520complements%2520existing%2520model%250Aparallelism%2520techniques%2520proposed%2520for%2520large-scale%2520transformer%2520training%2520by%250Aoptimizing%2520non-GEMM%2520bottlenecks%2520and%2520enabling%2520convolution-aware%2520tensor%2520sharding.%250AWe%2520demonstrate%2520unprecedented%2520capabilities%2520by%2520performing%2520multimodal%2520registration%250Aof%2520a%2520100%2520micron%2520ex-vivo%2520human%2520brain%2520MRI%2520volume%2520at%2520native%2520resolution%2520-%2520an%250Ainverse%2520problem%2520more%2520than%2520570x%2520larger%2520than%2520a%2520standard%2520clinical%2520datum%2520in%2520about%2520a%250Aminute%2520using%2520only%25208%2520A6000%2520GPUs.%2520FFDP%2520accelerates%2520existing%2520state-of-the-art%250Aoptimization%2520and%2520deep%2520learning%2520registration%2520pipelines%2520by%2520upto%25206%2520-%25207x%2520while%250Areducing%2520peak%2520memory%2520consumption%2520by%252020%2520-%252059%2525.%2520Comparative%2520analysis%2520on%2520a%2520250%250Amicron%2520dataset%2520shows%2520that%2520FFDP%2520can%2520fit%2520upto%252064x%2520larger%2520problems%2520than%2520existing%250ASOTA%2520on%2520a%2520single%2520GPU%252C%2520and%2520highlights%2520both%2520the%2520performance%2520and%2520efficiency%2520gains%250Aof%2520FFDP%2520compared%2520to%2520SOTA%2520image%2520registration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Distributed%20Framework%20for%20Multimodal%20GigaVoxel%20Image%0A%20%20Registration&entry.906535625=Rohit%20Jena%20and%20Vedant%20Zope%20and%20Pratik%20Chaudhari%20and%20James%20C.%20Gee&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20FFDP%2C%20a%20set%20of%20IO-aware%20non-GEMM%20fused%20kernels%0Asupplemented%20with%20a%20distributed%20framework%20for%20image%20registration%20at%0Aunprecedented%20scales.%20Image%20registration%20is%20an%20inverse%20problem%20fundamental%20to%0Abiomedical%20and%20life%20sciences%2C%20but%20algorithms%20have%20not%20scaled%20in%20tandem%20with%0Aimage%20acquisition%20capabilities.%20Our%20framework%20complements%20existing%20model%0Aparallelism%20techniques%20proposed%20for%20large-scale%20transformer%20training%20by%0Aoptimizing%20non-GEMM%20bottlenecks%20and%20enabling%20convolution-aware%20tensor%20sharding.%0AWe%20demonstrate%20unprecedented%20capabilities%20by%20performing%20multimodal%20registration%0Aof%20a%20100%20micron%20ex-vivo%20human%20brain%20MRI%20volume%20at%20native%20resolution%20-%20an%0Ainverse%20problem%20more%20than%20570x%20larger%20than%20a%20standard%20clinical%20datum%20in%20about%20a%0Aminute%20using%20only%208%20A6000%20GPUs.%20FFDP%20accelerates%20existing%20state-of-the-art%0Aoptimization%20and%20deep%20learning%20registration%20pipelines%20by%20upto%206%20-%207x%20while%0Areducing%20peak%20memory%20consumption%20by%2020%20-%2059%25.%20Comparative%20analysis%20on%20a%20250%0Amicron%20dataset%20shows%20that%20FFDP%20can%20fit%20upto%2064x%20larger%20problems%20than%20existing%0ASOTA%20on%20a%20single%20GPU%2C%20and%20highlights%20both%20the%20performance%20and%20efficiency%20gains%0Aof%20FFDP%20compared%20to%20SOTA%20image%20registration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25044v1&entry.124074799=Read"},
{"title": "VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative\n  Pipelines", "author": "Mostafa Mohaimen Akand Faisal and Rabeya Amin Jhuma", "abstract": "  Generative models such as GANs and diffusion models are widely used to\nsynthesize photorealistic images and to support downstream creative and editing\ntasks. While adversarial attacks on discriminative models are well studied,\nattacks targeting generative pipelines where small, stealthy perturbations in\ninputs lead to controlled changes in outputs are less explored. This study\nintroduces VagueGAN, an attack pipeline combining a modular perturbation\nnetwork PoisonerNet with a Generator Discriminator pair to craft stealthy\ntriggers that cause targeted changes in generated images. Attack efficacy is\nevaluated using a custom proxy metric, while stealth is analyzed through\nperceptual and frequency domain measures. The transferability of the method to\na modern diffusion based pipeline is further examined through ControlNet guided\nediting. Interestingly, the experiments show that poisoned outputs can display\nhigher visual quality compared to clean counterparts, challenging the\nassumption that poisoning necessarily reduces fidelity. Unlike conventional\npixel level perturbations, latent space poisoning in GANs and diffusion\npipelines can retain or even enhance output aesthetics, exposing a blind spot\nin pixel level defenses. Moreover, carefully optimized perturbations can\nproduce consistent, stealthy effects on generator outputs while remaining\nvisually inconspicuous, raising concerns for the integrity of image generation\npipelines.\n", "link": "http://arxiv.org/abs/2509.24891v1", "date": "2025-09-29", "relevancy": 2.1849, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5765}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAGUEGAN%3A%20Stealthy%20Poisoning%20and%20Backdoor%20Attacks%20on%20Image%20Generative%0A%20%20Pipelines&body=Title%3A%20VAGUEGAN%3A%20Stealthy%20Poisoning%20and%20Backdoor%20Attacks%20on%20Image%20Generative%0A%20%20Pipelines%0AAuthor%3A%20Mostafa%20Mohaimen%20Akand%20Faisal%20and%20Rabeya%20Amin%20Jhuma%0AAbstract%3A%20%20%20Generative%20models%20such%20as%20GANs%20and%20diffusion%20models%20are%20widely%20used%20to%0Asynthesize%20photorealistic%20images%20and%20to%20support%20downstream%20creative%20and%20editing%0Atasks.%20While%20adversarial%20attacks%20on%20discriminative%20models%20are%20well%20studied%2C%0Aattacks%20targeting%20generative%20pipelines%20where%20small%2C%20stealthy%20perturbations%20in%0Ainputs%20lead%20to%20controlled%20changes%20in%20outputs%20are%20less%20explored.%20This%20study%0Aintroduces%20VagueGAN%2C%20an%20attack%20pipeline%20combining%20a%20modular%20perturbation%0Anetwork%20PoisonerNet%20with%20a%20Generator%20Discriminator%20pair%20to%20craft%20stealthy%0Atriggers%20that%20cause%20targeted%20changes%20in%20generated%20images.%20Attack%20efficacy%20is%0Aevaluated%20using%20a%20custom%20proxy%20metric%2C%20while%20stealth%20is%20analyzed%20through%0Aperceptual%20and%20frequency%20domain%20measures.%20The%20transferability%20of%20the%20method%20to%0Aa%20modern%20diffusion%20based%20pipeline%20is%20further%20examined%20through%20ControlNet%20guided%0Aediting.%20Interestingly%2C%20the%20experiments%20show%20that%20poisoned%20outputs%20can%20display%0Ahigher%20visual%20quality%20compared%20to%20clean%20counterparts%2C%20challenging%20the%0Aassumption%20that%20poisoning%20necessarily%20reduces%20fidelity.%20Unlike%20conventional%0Apixel%20level%20perturbations%2C%20latent%20space%20poisoning%20in%20GANs%20and%20diffusion%0Apipelines%20can%20retain%20or%20even%20enhance%20output%20aesthetics%2C%20exposing%20a%20blind%20spot%0Ain%20pixel%20level%20defenses.%20Moreover%2C%20carefully%20optimized%20perturbations%20can%0Aproduce%20consistent%2C%20stealthy%20effects%20on%20generator%20outputs%20while%20remaining%0Avisually%20inconspicuous%2C%20raising%20concerns%20for%20the%20integrity%20of%20image%20generation%0Apipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAGUEGAN%253A%2520Stealthy%2520Poisoning%2520and%2520Backdoor%2520Attacks%2520on%2520Image%2520Generative%250A%2520%2520Pipelines%26entry.906535625%3DMostafa%2520Mohaimen%2520Akand%2520Faisal%2520and%2520Rabeya%2520Amin%2520Jhuma%26entry.1292438233%3D%2520%2520Generative%2520models%2520such%2520as%2520GANs%2520and%2520diffusion%2520models%2520are%2520widely%2520used%2520to%250Asynthesize%2520photorealistic%2520images%2520and%2520to%2520support%2520downstream%2520creative%2520and%2520editing%250Atasks.%2520While%2520adversarial%2520attacks%2520on%2520discriminative%2520models%2520are%2520well%2520studied%252C%250Aattacks%2520targeting%2520generative%2520pipelines%2520where%2520small%252C%2520stealthy%2520perturbations%2520in%250Ainputs%2520lead%2520to%2520controlled%2520changes%2520in%2520outputs%2520are%2520less%2520explored.%2520This%2520study%250Aintroduces%2520VagueGAN%252C%2520an%2520attack%2520pipeline%2520combining%2520a%2520modular%2520perturbation%250Anetwork%2520PoisonerNet%2520with%2520a%2520Generator%2520Discriminator%2520pair%2520to%2520craft%2520stealthy%250Atriggers%2520that%2520cause%2520targeted%2520changes%2520in%2520generated%2520images.%2520Attack%2520efficacy%2520is%250Aevaluated%2520using%2520a%2520custom%2520proxy%2520metric%252C%2520while%2520stealth%2520is%2520analyzed%2520through%250Aperceptual%2520and%2520frequency%2520domain%2520measures.%2520The%2520transferability%2520of%2520the%2520method%2520to%250Aa%2520modern%2520diffusion%2520based%2520pipeline%2520is%2520further%2520examined%2520through%2520ControlNet%2520guided%250Aediting.%2520Interestingly%252C%2520the%2520experiments%2520show%2520that%2520poisoned%2520outputs%2520can%2520display%250Ahigher%2520visual%2520quality%2520compared%2520to%2520clean%2520counterparts%252C%2520challenging%2520the%250Aassumption%2520that%2520poisoning%2520necessarily%2520reduces%2520fidelity.%2520Unlike%2520conventional%250Apixel%2520level%2520perturbations%252C%2520latent%2520space%2520poisoning%2520in%2520GANs%2520and%2520diffusion%250Apipelines%2520can%2520retain%2520or%2520even%2520enhance%2520output%2520aesthetics%252C%2520exposing%2520a%2520blind%2520spot%250Ain%2520pixel%2520level%2520defenses.%2520Moreover%252C%2520carefully%2520optimized%2520perturbations%2520can%250Aproduce%2520consistent%252C%2520stealthy%2520effects%2520on%2520generator%2520outputs%2520while%2520remaining%250Avisually%2520inconspicuous%252C%2520raising%2520concerns%2520for%2520the%2520integrity%2520of%2520image%2520generation%250Apipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAGUEGAN%3A%20Stealthy%20Poisoning%20and%20Backdoor%20Attacks%20on%20Image%20Generative%0A%20%20Pipelines&entry.906535625=Mostafa%20Mohaimen%20Akand%20Faisal%20and%20Rabeya%20Amin%20Jhuma&entry.1292438233=%20%20Generative%20models%20such%20as%20GANs%20and%20diffusion%20models%20are%20widely%20used%20to%0Asynthesize%20photorealistic%20images%20and%20to%20support%20downstream%20creative%20and%20editing%0Atasks.%20While%20adversarial%20attacks%20on%20discriminative%20models%20are%20well%20studied%2C%0Aattacks%20targeting%20generative%20pipelines%20where%20small%2C%20stealthy%20perturbations%20in%0Ainputs%20lead%20to%20controlled%20changes%20in%20outputs%20are%20less%20explored.%20This%20study%0Aintroduces%20VagueGAN%2C%20an%20attack%20pipeline%20combining%20a%20modular%20perturbation%0Anetwork%20PoisonerNet%20with%20a%20Generator%20Discriminator%20pair%20to%20craft%20stealthy%0Atriggers%20that%20cause%20targeted%20changes%20in%20generated%20images.%20Attack%20efficacy%20is%0Aevaluated%20using%20a%20custom%20proxy%20metric%2C%20while%20stealth%20is%20analyzed%20through%0Aperceptual%20and%20frequency%20domain%20measures.%20The%20transferability%20of%20the%20method%20to%0Aa%20modern%20diffusion%20based%20pipeline%20is%20further%20examined%20through%20ControlNet%20guided%0Aediting.%20Interestingly%2C%20the%20experiments%20show%20that%20poisoned%20outputs%20can%20display%0Ahigher%20visual%20quality%20compared%20to%20clean%20counterparts%2C%20challenging%20the%0Aassumption%20that%20poisoning%20necessarily%20reduces%20fidelity.%20Unlike%20conventional%0Apixel%20level%20perturbations%2C%20latent%20space%20poisoning%20in%20GANs%20and%20diffusion%0Apipelines%20can%20retain%20or%20even%20enhance%20output%20aesthetics%2C%20exposing%20a%20blind%20spot%0Ain%20pixel%20level%20defenses.%20Moreover%2C%20carefully%20optimized%20perturbations%20can%0Aproduce%20consistent%2C%20stealthy%20effects%20on%20generator%20outputs%20while%20remaining%0Avisually%20inconspicuous%2C%20raising%20concerns%20for%20the%20integrity%20of%20image%20generation%0Apipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24891v1&entry.124074799=Read"},
{"title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis", "author": "Jos\u00e9 Morano and Botond Fazekas and Emese S\u00fckei and Ronald Fecso and Taha Emre and Markus Gumpinger and Georg Faustmann and Marzieh Oghbaie and Ursula Schmidt-Erfurth and Hrvoje Bogunovi\u0107", "abstract": "  Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.\n", "link": "http://arxiv.org/abs/2506.08900v3", "date": "2025-09-29", "relevancy": 2.184, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIRAGE%3A%20Multimodal%20foundation%20model%20and%20benchmark%20for%20comprehensive%0A%20%20retinal%20OCT%20image%20analysis&body=Title%3A%20MIRAGE%3A%20Multimodal%20foundation%20model%20and%20benchmark%20for%20comprehensive%0A%20%20retinal%20OCT%20image%20analysis%0AAuthor%3A%20Jos%C3%A9%20Morano%20and%20Botond%20Fazekas%20and%20Emese%20S%C3%BCkei%20and%20Ronald%20Fecso%20and%20Taha%20Emre%20and%20Markus%20Gumpinger%20and%20Georg%20Faustmann%20and%20Marzieh%20Oghbaie%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20become%20a%20fundamental%20tool%20for%20assisting%0Aclinicians%20in%20analyzing%20ophthalmic%20images%2C%20such%20as%20optical%20coherence%20tomography%0A%28OCT%29.%20However%2C%20developing%20AI%20models%20often%20requires%20extensive%20annotation%2C%20and%0Aexisting%20models%20tend%20to%20underperform%20on%20independent%2C%20unseen%20data.%20Foundation%0Amodels%20%28FMs%29%2C%20large%20AI%20models%20trained%20on%20vast%20unlabeled%20datasets%2C%20have%20shown%0Apromise%20in%20overcoming%20these%20challenges.%20Nonetheless%2C%20available%20FMs%20for%0Aophthalmology%20lack%20extensive%20validation%2C%20especially%20for%20segmentation%20tasks%2C%20and%0Afocus%20on%20a%20single%20imaging%20modality.%20In%20this%20context%2C%20we%20propose%20MIRAGE%2C%20a%20novel%0Amultimodal%20FM%20for%20the%20analysis%20of%20OCT%20and%20scanning%20laser%20ophthalmoscopy%20%28SLO%29%0Aimages.%20Additionally%2C%20we%20propose%20a%20new%20evaluation%20benchmark%20with%20OCT/SLO%0Aclassification%20and%20segmentation%20tasks.%20The%20comparison%20with%20general%20and%0Aspecialized%20FMs%20and%20segmentation%20methods%20shows%20the%20superiority%20of%20MIRAGE%20in%0Aboth%20types%20of%20tasks%2C%20highlighting%20its%20suitability%20as%20a%20basis%20for%20the%0Adevelopment%20of%20robust%20AI%20systems%20for%20retinal%20OCT%20image%20analysis.%20Both%20MIRAGE%0Aand%20the%20evaluation%20benchmark%20are%20publicly%20available%3A%0Ahttps%3A//github.com/j-morano/MIRAGE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08900v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIRAGE%253A%2520Multimodal%2520foundation%2520model%2520and%2520benchmark%2520for%2520comprehensive%250A%2520%2520retinal%2520OCT%2520image%2520analysis%26entry.906535625%3DJos%25C3%25A9%2520Morano%2520and%2520Botond%2520Fazekas%2520and%2520Emese%2520S%25C3%25BCkei%2520and%2520Ronald%2520Fecso%2520and%2520Taha%2520Emre%2520and%2520Markus%2520Gumpinger%2520and%2520Georg%2520Faustmann%2520and%2520Marzieh%2520Oghbaie%2520and%2520Ursula%2520Schmidt-Erfurth%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520become%2520a%2520fundamental%2520tool%2520for%2520assisting%250Aclinicians%2520in%2520analyzing%2520ophthalmic%2520images%252C%2520such%2520as%2520optical%2520coherence%2520tomography%250A%2528OCT%2529.%2520However%252C%2520developing%2520AI%2520models%2520often%2520requires%2520extensive%2520annotation%252C%2520and%250Aexisting%2520models%2520tend%2520to%2520underperform%2520on%2520independent%252C%2520unseen%2520data.%2520Foundation%250Amodels%2520%2528FMs%2529%252C%2520large%2520AI%2520models%2520trained%2520on%2520vast%2520unlabeled%2520datasets%252C%2520have%2520shown%250Apromise%2520in%2520overcoming%2520these%2520challenges.%2520Nonetheless%252C%2520available%2520FMs%2520for%250Aophthalmology%2520lack%2520extensive%2520validation%252C%2520especially%2520for%2520segmentation%2520tasks%252C%2520and%250Afocus%2520on%2520a%2520single%2520imaging%2520modality.%2520In%2520this%2520context%252C%2520we%2520propose%2520MIRAGE%252C%2520a%2520novel%250Amultimodal%2520FM%2520for%2520the%2520analysis%2520of%2520OCT%2520and%2520scanning%2520laser%2520ophthalmoscopy%2520%2528SLO%2529%250Aimages.%2520Additionally%252C%2520we%2520propose%2520a%2520new%2520evaluation%2520benchmark%2520with%2520OCT/SLO%250Aclassification%2520and%2520segmentation%2520tasks.%2520The%2520comparison%2520with%2520general%2520and%250Aspecialized%2520FMs%2520and%2520segmentation%2520methods%2520shows%2520the%2520superiority%2520of%2520MIRAGE%2520in%250Aboth%2520types%2520of%2520tasks%252C%2520highlighting%2520its%2520suitability%2520as%2520a%2520basis%2520for%2520the%250Adevelopment%2520of%2520robust%2520AI%2520systems%2520for%2520retinal%2520OCT%2520image%2520analysis.%2520Both%2520MIRAGE%250Aand%2520the%2520evaluation%2520benchmark%2520are%2520publicly%2520available%253A%250Ahttps%253A//github.com/j-morano/MIRAGE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08900v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIRAGE%3A%20Multimodal%20foundation%20model%20and%20benchmark%20for%20comprehensive%0A%20%20retinal%20OCT%20image%20analysis&entry.906535625=Jos%C3%A9%20Morano%20and%20Botond%20Fazekas%20and%20Emese%20S%C3%BCkei%20and%20Ronald%20Fecso%20and%20Taha%20Emre%20and%20Markus%20Gumpinger%20and%20Georg%20Faustmann%20and%20Marzieh%20Oghbaie%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20become%20a%20fundamental%20tool%20for%20assisting%0Aclinicians%20in%20analyzing%20ophthalmic%20images%2C%20such%20as%20optical%20coherence%20tomography%0A%28OCT%29.%20However%2C%20developing%20AI%20models%20often%20requires%20extensive%20annotation%2C%20and%0Aexisting%20models%20tend%20to%20underperform%20on%20independent%2C%20unseen%20data.%20Foundation%0Amodels%20%28FMs%29%2C%20large%20AI%20models%20trained%20on%20vast%20unlabeled%20datasets%2C%20have%20shown%0Apromise%20in%20overcoming%20these%20challenges.%20Nonetheless%2C%20available%20FMs%20for%0Aophthalmology%20lack%20extensive%20validation%2C%20especially%20for%20segmentation%20tasks%2C%20and%0Afocus%20on%20a%20single%20imaging%20modality.%20In%20this%20context%2C%20we%20propose%20MIRAGE%2C%20a%20novel%0Amultimodal%20FM%20for%20the%20analysis%20of%20OCT%20and%20scanning%20laser%20ophthalmoscopy%20%28SLO%29%0Aimages.%20Additionally%2C%20we%20propose%20a%20new%20evaluation%20benchmark%20with%20OCT/SLO%0Aclassification%20and%20segmentation%20tasks.%20The%20comparison%20with%20general%20and%0Aspecialized%20FMs%20and%20segmentation%20methods%20shows%20the%20superiority%20of%20MIRAGE%20in%0Aboth%20types%20of%20tasks%2C%20highlighting%20its%20suitability%20as%20a%20basis%20for%20the%0Adevelopment%20of%20robust%20AI%20systems%20for%20retinal%20OCT%20image%20analysis.%20Both%20MIRAGE%0Aand%20the%20evaluation%20benchmark%20are%20publicly%20available%3A%0Ahttps%3A//github.com/j-morano/MIRAGE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08900v3&entry.124074799=Read"},
{"title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are\n  We?", "author": "An Guo and Shuoxiao Zhang and Enyi Tang and Xinyu Gao and Haomin Pang and Haoxiang Tian and Yanzhou Mu and Wu Wen and Chunrong Fang and Zhenyu Chen", "abstract": "  With the tremendous advancement of deep learning and communication\ntechnology, Vehicle-to-Everything (V2X) cooperative perception has the\npotential to address limitations in sensing distant objects and occlusion for a\nsingle-agent perception system. V2X cooperative perception systems are software\nsystems characterized by diverse sensor types and cooperative agents, varying\nfusion schemes, and operation under different communication conditions.\nTherefore, their complex composition gives rise to numerous operational\nchallenges. Furthermore, when cooperative perception systems produce erroneous\npredictions, the types of errors and their underlying causes remain\ninsufficiently explored. To bridge this gap, we take an initial step by\nconducting an empirical study of V2X cooperative perception. To systematically\nevaluate the impact of cooperative perception on the ego vehicle's perception\nperformance, we identify and analyze six prevalent error patterns in\ncooperative perception systems. We further conduct a systematic evaluation of\nthe critical components of these systems through our large-scale study and\nidentify the following key findings: (1) The LiDAR-based cooperation\nconfiguration exhibits the highest perception performance; (2)\nVehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication\nexhibit distinct cooperative perception performance under different fusion\nschemes; (3) Increased cooperative perception errors may result in a higher\nfrequency of driving violations; (4) Cooperative perception systems are not\nrobust against communication interference when running online. Our results\nreveal potential risks and vulnerabilities in critical components of\ncooperative perception systems. We hope that our findings can better promote\nthe design and repair of cooperative perception systems.\n", "link": "http://arxiv.org/abs/2509.24927v1", "date": "2025-09-29", "relevancy": 2.1805, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5653}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Autonomous%20Vehicle%20Meets%20V2X%20Cooperative%20Perception%3A%20How%20Far%20Are%0A%20%20We%3F&body=Title%3A%20When%20Autonomous%20Vehicle%20Meets%20V2X%20Cooperative%20Perception%3A%20How%20Far%20Are%0A%20%20We%3F%0AAuthor%3A%20An%20Guo%20and%20Shuoxiao%20Zhang%20and%20Enyi%20Tang%20and%20Xinyu%20Gao%20and%20Haomin%20Pang%20and%20Haoxiang%20Tian%20and%20Yanzhou%20Mu%20and%20Wu%20Wen%20and%20Chunrong%20Fang%20and%20Zhenyu%20Chen%0AAbstract%3A%20%20%20With%20the%20tremendous%20advancement%20of%20deep%20learning%20and%20communication%0Atechnology%2C%20Vehicle-to-Everything%20%28V2X%29%20cooperative%20perception%20has%20the%0Apotential%20to%20address%20limitations%20in%20sensing%20distant%20objects%20and%20occlusion%20for%20a%0Asingle-agent%20perception%20system.%20V2X%20cooperative%20perception%20systems%20are%20software%0Asystems%20characterized%20by%20diverse%20sensor%20types%20and%20cooperative%20agents%2C%20varying%0Afusion%20schemes%2C%20and%20operation%20under%20different%20communication%20conditions.%0ATherefore%2C%20their%20complex%20composition%20gives%20rise%20to%20numerous%20operational%0Achallenges.%20Furthermore%2C%20when%20cooperative%20perception%20systems%20produce%20erroneous%0Apredictions%2C%20the%20types%20of%20errors%20and%20their%20underlying%20causes%20remain%0Ainsufficiently%20explored.%20To%20bridge%20this%20gap%2C%20we%20take%20an%20initial%20step%20by%0Aconducting%20an%20empirical%20study%20of%20V2X%20cooperative%20perception.%20To%20systematically%0Aevaluate%20the%20impact%20of%20cooperative%20perception%20on%20the%20ego%20vehicle%27s%20perception%0Aperformance%2C%20we%20identify%20and%20analyze%20six%20prevalent%20error%20patterns%20in%0Acooperative%20perception%20systems.%20We%20further%20conduct%20a%20systematic%20evaluation%20of%0Athe%20critical%20components%20of%20these%20systems%20through%20our%20large-scale%20study%20and%0Aidentify%20the%20following%20key%20findings%3A%20%281%29%20The%20LiDAR-based%20cooperation%0Aconfiguration%20exhibits%20the%20highest%20perception%20performance%3B%20%282%29%0AVehicle-to-infrastructure%20%28V2I%29%20and%20vehicle-to-vehicle%20%28V2V%29%20communication%0Aexhibit%20distinct%20cooperative%20perception%20performance%20under%20different%20fusion%0Aschemes%3B%20%283%29%20Increased%20cooperative%20perception%20errors%20may%20result%20in%20a%20higher%0Afrequency%20of%20driving%20violations%3B%20%284%29%20Cooperative%20perception%20systems%20are%20not%0Arobust%20against%20communication%20interference%20when%20running%20online.%20Our%20results%0Areveal%20potential%20risks%20and%20vulnerabilities%20in%20critical%20components%20of%0Acooperative%20perception%20systems.%20We%20hope%20that%20our%20findings%20can%20better%20promote%0Athe%20design%20and%20repair%20of%20cooperative%20perception%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Autonomous%2520Vehicle%2520Meets%2520V2X%2520Cooperative%2520Perception%253A%2520How%2520Far%2520Are%250A%2520%2520We%253F%26entry.906535625%3DAn%2520Guo%2520and%2520Shuoxiao%2520Zhang%2520and%2520Enyi%2520Tang%2520and%2520Xinyu%2520Gao%2520and%2520Haomin%2520Pang%2520and%2520Haoxiang%2520Tian%2520and%2520Yanzhou%2520Mu%2520and%2520Wu%2520Wen%2520and%2520Chunrong%2520Fang%2520and%2520Zhenyu%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520tremendous%2520advancement%2520of%2520deep%2520learning%2520and%2520communication%250Atechnology%252C%2520Vehicle-to-Everything%2520%2528V2X%2529%2520cooperative%2520perception%2520has%2520the%250Apotential%2520to%2520address%2520limitations%2520in%2520sensing%2520distant%2520objects%2520and%2520occlusion%2520for%2520a%250Asingle-agent%2520perception%2520system.%2520V2X%2520cooperative%2520perception%2520systems%2520are%2520software%250Asystems%2520characterized%2520by%2520diverse%2520sensor%2520types%2520and%2520cooperative%2520agents%252C%2520varying%250Afusion%2520schemes%252C%2520and%2520operation%2520under%2520different%2520communication%2520conditions.%250ATherefore%252C%2520their%2520complex%2520composition%2520gives%2520rise%2520to%2520numerous%2520operational%250Achallenges.%2520Furthermore%252C%2520when%2520cooperative%2520perception%2520systems%2520produce%2520erroneous%250Apredictions%252C%2520the%2520types%2520of%2520errors%2520and%2520their%2520underlying%2520causes%2520remain%250Ainsufficiently%2520explored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520take%2520an%2520initial%2520step%2520by%250Aconducting%2520an%2520empirical%2520study%2520of%2520V2X%2520cooperative%2520perception.%2520To%2520systematically%250Aevaluate%2520the%2520impact%2520of%2520cooperative%2520perception%2520on%2520the%2520ego%2520vehicle%2527s%2520perception%250Aperformance%252C%2520we%2520identify%2520and%2520analyze%2520six%2520prevalent%2520error%2520patterns%2520in%250Acooperative%2520perception%2520systems.%2520We%2520further%2520conduct%2520a%2520systematic%2520evaluation%2520of%250Athe%2520critical%2520components%2520of%2520these%2520systems%2520through%2520our%2520large-scale%2520study%2520and%250Aidentify%2520the%2520following%2520key%2520findings%253A%2520%25281%2529%2520The%2520LiDAR-based%2520cooperation%250Aconfiguration%2520exhibits%2520the%2520highest%2520perception%2520performance%253B%2520%25282%2529%250AVehicle-to-infrastructure%2520%2528V2I%2529%2520and%2520vehicle-to-vehicle%2520%2528V2V%2529%2520communication%250Aexhibit%2520distinct%2520cooperative%2520perception%2520performance%2520under%2520different%2520fusion%250Aschemes%253B%2520%25283%2529%2520Increased%2520cooperative%2520perception%2520errors%2520may%2520result%2520in%2520a%2520higher%250Afrequency%2520of%2520driving%2520violations%253B%2520%25284%2529%2520Cooperative%2520perception%2520systems%2520are%2520not%250Arobust%2520against%2520communication%2520interference%2520when%2520running%2520online.%2520Our%2520results%250Areveal%2520potential%2520risks%2520and%2520vulnerabilities%2520in%2520critical%2520components%2520of%250Acooperative%2520perception%2520systems.%2520We%2520hope%2520that%2520our%2520findings%2520can%2520better%2520promote%250Athe%2520design%2520and%2520repair%2520of%2520cooperative%2520perception%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Autonomous%20Vehicle%20Meets%20V2X%20Cooperative%20Perception%3A%20How%20Far%20Are%0A%20%20We%3F&entry.906535625=An%20Guo%20and%20Shuoxiao%20Zhang%20and%20Enyi%20Tang%20and%20Xinyu%20Gao%20and%20Haomin%20Pang%20and%20Haoxiang%20Tian%20and%20Yanzhou%20Mu%20and%20Wu%20Wen%20and%20Chunrong%20Fang%20and%20Zhenyu%20Chen&entry.1292438233=%20%20With%20the%20tremendous%20advancement%20of%20deep%20learning%20and%20communication%0Atechnology%2C%20Vehicle-to-Everything%20%28V2X%29%20cooperative%20perception%20has%20the%0Apotential%20to%20address%20limitations%20in%20sensing%20distant%20objects%20and%20occlusion%20for%20a%0Asingle-agent%20perception%20system.%20V2X%20cooperative%20perception%20systems%20are%20software%0Asystems%20characterized%20by%20diverse%20sensor%20types%20and%20cooperative%20agents%2C%20varying%0Afusion%20schemes%2C%20and%20operation%20under%20different%20communication%20conditions.%0ATherefore%2C%20their%20complex%20composition%20gives%20rise%20to%20numerous%20operational%0Achallenges.%20Furthermore%2C%20when%20cooperative%20perception%20systems%20produce%20erroneous%0Apredictions%2C%20the%20types%20of%20errors%20and%20their%20underlying%20causes%20remain%0Ainsufficiently%20explored.%20To%20bridge%20this%20gap%2C%20we%20take%20an%20initial%20step%20by%0Aconducting%20an%20empirical%20study%20of%20V2X%20cooperative%20perception.%20To%20systematically%0Aevaluate%20the%20impact%20of%20cooperative%20perception%20on%20the%20ego%20vehicle%27s%20perception%0Aperformance%2C%20we%20identify%20and%20analyze%20six%20prevalent%20error%20patterns%20in%0Acooperative%20perception%20systems.%20We%20further%20conduct%20a%20systematic%20evaluation%20of%0Athe%20critical%20components%20of%20these%20systems%20through%20our%20large-scale%20study%20and%0Aidentify%20the%20following%20key%20findings%3A%20%281%29%20The%20LiDAR-based%20cooperation%0Aconfiguration%20exhibits%20the%20highest%20perception%20performance%3B%20%282%29%0AVehicle-to-infrastructure%20%28V2I%29%20and%20vehicle-to-vehicle%20%28V2V%29%20communication%0Aexhibit%20distinct%20cooperative%20perception%20performance%20under%20different%20fusion%0Aschemes%3B%20%283%29%20Increased%20cooperative%20perception%20errors%20may%20result%20in%20a%20higher%0Afrequency%20of%20driving%20violations%3B%20%284%29%20Cooperative%20perception%20systems%20are%20not%0Arobust%20against%20communication%20interference%20when%20running%20online.%20Our%20results%0Areveal%20potential%20risks%20and%20vulnerabilities%20in%20critical%20components%20of%0Acooperative%20perception%20systems.%20We%20hope%20that%20our%20findings%20can%20better%20promote%0Athe%20design%20and%20repair%20of%20cooperative%20perception%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24927v1&entry.124074799=Read"},
{"title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning", "author": "Sai Wang and Yu Wu and Zhongwen Xu", "abstract": "  The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.\n", "link": "http://arxiv.org/abs/2509.25052v1", "date": "2025-09-29", "relevancy": 2.1768, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.613}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5307}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cogito%2C%20Ergo%20Ludo%3A%20An%20Agent%20that%20Learns%20to%20Play%20by%20Reasoning%20and%0A%20%20Planning&body=Title%3A%20Cogito%2C%20Ergo%20Ludo%3A%20An%20Agent%20that%20Learns%20to%20Play%20by%20Reasoning%20and%0A%20%20Planning%0AAuthor%3A%20Sai%20Wang%20and%20Yu%20Wu%20and%20Zhongwen%20Xu%0AAbstract%3A%20%20%20The%20pursuit%20of%20artificial%20agents%20that%20can%20learn%20to%20master%20complex%0Aenvironments%20has%20led%20to%20remarkable%20successes%2C%20yet%20prevailing%20deep%20reinforcement%0Alearning%20methods%20often%20rely%20on%20immense%20experience%2C%20encoding%20their%20knowledge%0Aopaquely%20within%20neural%20network%20weights.%20We%20propose%20a%20different%20paradigm%2C%20one%20in%0Awhich%20an%20agent%20learns%20to%20play%20by%20reasoning%20and%20planning.%20We%20introduce%20Cogito%2C%0Aergo%20ludo%20%28CEL%29%2C%20a%20novel%20agent%20architecture%20that%20leverages%20a%20Large%20Language%0AModel%20%28LLM%29%20to%20build%20an%20explicit%2C%20language-based%20understanding%20of%20its%0Aenvironment%27s%20mechanics%20and%20its%20own%20strategy.%20Starting%20from%20a%20tabula%20rasa%20state%0Awith%20no%20prior%20knowledge%20%28except%20action%20set%29%2C%20CEL%20operates%20on%20a%20cycle%20of%0Ainteraction%20and%20reflection.%20After%20each%20episode%2C%20the%20agent%20analyzes%20its%20complete%0Atrajectory%20to%20perform%20two%20concurrent%20learning%20processes%3A%20Rule%20Induction%2C%20where%0Ait%20refines%20its%20explicit%20model%20of%20the%20environment%27s%20dynamics%2C%20and%20Strategy%20and%0APlaybook%20Summarization%2C%20where%20it%20distills%20experiences%20into%20an%20actionable%0Astrategic%20playbook.%20We%20evaluate%20CEL%20on%20diverse%20grid-world%20tasks%20%28i.e.%2C%0AMinesweeper%2C%20Frozen%20Lake%2C%20and%20Sokoban%29%2C%20and%20show%20that%20the%20CEL%20agent%0Asuccessfully%20learns%20to%20master%20these%20games%20by%20autonomously%20discovering%20their%0Arules%20and%20developing%20effective%20policies%20from%20sparse%20rewards.%20Ablation%20studies%0Aconfirm%20that%20the%20iterative%20process%20is%20critical%20for%20sustained%20learning.%20Our%20work%0Ademonstrates%20a%20path%20toward%20more%20general%20and%20interpretable%20agents%20that%20not%20only%0Aact%20effectively%20but%20also%20build%20a%20transparent%20and%20improving%20model%20of%20their%20world%0Athrough%20explicit%20reasoning%20on%20raw%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogito%252C%2520Ergo%2520Ludo%253A%2520An%2520Agent%2520that%2520Learns%2520to%2520Play%2520by%2520Reasoning%2520and%250A%2520%2520Planning%26entry.906535625%3DSai%2520Wang%2520and%2520Yu%2520Wu%2520and%2520Zhongwen%2520Xu%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520artificial%2520agents%2520that%2520can%2520learn%2520to%2520master%2520complex%250Aenvironments%2520has%2520led%2520to%2520remarkable%2520successes%252C%2520yet%2520prevailing%2520deep%2520reinforcement%250Alearning%2520methods%2520often%2520rely%2520on%2520immense%2520experience%252C%2520encoding%2520their%2520knowledge%250Aopaquely%2520within%2520neural%2520network%2520weights.%2520We%2520propose%2520a%2520different%2520paradigm%252C%2520one%2520in%250Awhich%2520an%2520agent%2520learns%2520to%2520play%2520by%2520reasoning%2520and%2520planning.%2520We%2520introduce%2520Cogito%252C%250Aergo%2520ludo%2520%2528CEL%2529%252C%2520a%2520novel%2520agent%2520architecture%2520that%2520leverages%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520to%2520build%2520an%2520explicit%252C%2520language-based%2520understanding%2520of%2520its%250Aenvironment%2527s%2520mechanics%2520and%2520its%2520own%2520strategy.%2520Starting%2520from%2520a%2520tabula%2520rasa%2520state%250Awith%2520no%2520prior%2520knowledge%2520%2528except%2520action%2520set%2529%252C%2520CEL%2520operates%2520on%2520a%2520cycle%2520of%250Ainteraction%2520and%2520reflection.%2520After%2520each%2520episode%252C%2520the%2520agent%2520analyzes%2520its%2520complete%250Atrajectory%2520to%2520perform%2520two%2520concurrent%2520learning%2520processes%253A%2520Rule%2520Induction%252C%2520where%250Ait%2520refines%2520its%2520explicit%2520model%2520of%2520the%2520environment%2527s%2520dynamics%252C%2520and%2520Strategy%2520and%250APlaybook%2520Summarization%252C%2520where%2520it%2520distills%2520experiences%2520into%2520an%2520actionable%250Astrategic%2520playbook.%2520We%2520evaluate%2520CEL%2520on%2520diverse%2520grid-world%2520tasks%2520%2528i.e.%252C%250AMinesweeper%252C%2520Frozen%2520Lake%252C%2520and%2520Sokoban%2529%252C%2520and%2520show%2520that%2520the%2520CEL%2520agent%250Asuccessfully%2520learns%2520to%2520master%2520these%2520games%2520by%2520autonomously%2520discovering%2520their%250Arules%2520and%2520developing%2520effective%2520policies%2520from%2520sparse%2520rewards.%2520Ablation%2520studies%250Aconfirm%2520that%2520the%2520iterative%2520process%2520is%2520critical%2520for%2520sustained%2520learning.%2520Our%2520work%250Ademonstrates%2520a%2520path%2520toward%2520more%2520general%2520and%2520interpretable%2520agents%2520that%2520not%2520only%250Aact%2520effectively%2520but%2520also%2520build%2520a%2520transparent%2520and%2520improving%2520model%2520of%2520their%2520world%250Athrough%2520explicit%2520reasoning%2520on%2520raw%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cogito%2C%20Ergo%20Ludo%3A%20An%20Agent%20that%20Learns%20to%20Play%20by%20Reasoning%20and%0A%20%20Planning&entry.906535625=Sai%20Wang%20and%20Yu%20Wu%20and%20Zhongwen%20Xu&entry.1292438233=%20%20The%20pursuit%20of%20artificial%20agents%20that%20can%20learn%20to%20master%20complex%0Aenvironments%20has%20led%20to%20remarkable%20successes%2C%20yet%20prevailing%20deep%20reinforcement%0Alearning%20methods%20often%20rely%20on%20immense%20experience%2C%20encoding%20their%20knowledge%0Aopaquely%20within%20neural%20network%20weights.%20We%20propose%20a%20different%20paradigm%2C%20one%20in%0Awhich%20an%20agent%20learns%20to%20play%20by%20reasoning%20and%20planning.%20We%20introduce%20Cogito%2C%0Aergo%20ludo%20%28CEL%29%2C%20a%20novel%20agent%20architecture%20that%20leverages%20a%20Large%20Language%0AModel%20%28LLM%29%20to%20build%20an%20explicit%2C%20language-based%20understanding%20of%20its%0Aenvironment%27s%20mechanics%20and%20its%20own%20strategy.%20Starting%20from%20a%20tabula%20rasa%20state%0Awith%20no%20prior%20knowledge%20%28except%20action%20set%29%2C%20CEL%20operates%20on%20a%20cycle%20of%0Ainteraction%20and%20reflection.%20After%20each%20episode%2C%20the%20agent%20analyzes%20its%20complete%0Atrajectory%20to%20perform%20two%20concurrent%20learning%20processes%3A%20Rule%20Induction%2C%20where%0Ait%20refines%20its%20explicit%20model%20of%20the%20environment%27s%20dynamics%2C%20and%20Strategy%20and%0APlaybook%20Summarization%2C%20where%20it%20distills%20experiences%20into%20an%20actionable%0Astrategic%20playbook.%20We%20evaluate%20CEL%20on%20diverse%20grid-world%20tasks%20%28i.e.%2C%0AMinesweeper%2C%20Frozen%20Lake%2C%20and%20Sokoban%29%2C%20and%20show%20that%20the%20CEL%20agent%0Asuccessfully%20learns%20to%20master%20these%20games%20by%20autonomously%20discovering%20their%0Arules%20and%20developing%20effective%20policies%20from%20sparse%20rewards.%20Ablation%20studies%0Aconfirm%20that%20the%20iterative%20process%20is%20critical%20for%20sustained%20learning.%20Our%20work%0Ademonstrates%20a%20path%20toward%20more%20general%20and%20interpretable%20agents%20that%20not%20only%0Aact%20effectively%20but%20also%20build%20a%20transparent%20and%20improving%20model%20of%20their%20world%0Athrough%20explicit%20reasoning%20on%20raw%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25052v1&entry.124074799=Read"},
{"title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing", "author": "Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He", "abstract": "  We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language\nmodel that achieves state-of-the-art recognition accuracy while maintaining\nexceptional computational efficiency. Our approach employs a coarse-to-fine,\ntwo-stage parsing strategy that decouples global layout analysis from local\ncontent recognition. In the first stage, the model performs efficient layout\nanalysis on downsampled images to identify structural elements, circumventing\nthe computational overhead of processing high-resolution inputs. In the second\nstage, guided by the global layout, it performs targeted content recognition on\nnative-resolution crops extracted from the original image, preserving\nfine-grained details in dense text, complex formulas, and tables. To support\nthis strategy, we developed a comprehensive data engine that generates diverse,\nlarge-scale training corpora for both pretraining and fine-tuning. Ultimately,\nMinerU2.5 demonstrates strong document parsing ability, achieving\nstate-of-the-art performance on multiple benchmarks, surpassing both\ngeneral-purpose and domain-specific models across various recognition tasks,\nwhile maintaining significantly lower computational overhead.\n", "link": "http://arxiv.org/abs/2509.22186v2", "date": "2025-09-29", "relevancy": 2.171, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MinerU2.5%3A%20A%20Decoupled%20Vision-Language%20Model%20for%20Efficient%0A%20%20High-Resolution%20Document%20Parsing&body=Title%3A%20MinerU2.5%3A%20A%20Decoupled%20Vision-Language%20Model%20for%20Efficient%0A%20%20High-Resolution%20Document%20Parsing%0AAuthor%3A%20Junbo%20Niu%20and%20Zheng%20Liu%20and%20Zhuangcheng%20Gu%20and%20Bin%20Wang%20and%20Linke%20Ouyang%20and%20Zhiyuan%20Zhao%20and%20Tao%20Chu%20and%20Tianyao%20He%20and%20Fan%20Wu%20and%20Qintong%20Zhang%20and%20Zhenjiang%20Jin%20and%20Guang%20Liang%20and%20Rui%20Zhang%20and%20Wenzheng%20Zhang%20and%20Yuan%20Qu%20and%20Zhifei%20Ren%20and%20Yuefeng%20Sun%20and%20Yuanhong%20Zheng%20and%20Dongsheng%20Ma%20and%20Zirui%20Tang%20and%20Boyu%20Niu%20and%20Ziyang%20Miao%20and%20Hejun%20Dong%20and%20Siyi%20Qian%20and%20Junyuan%20Zhang%20and%20Jingzhou%20Chen%20and%20Fangdong%20Wang%20and%20Xiaomeng%20Zhao%20and%20Liqun%20Wei%20and%20Wei%20Li%20and%20Shasha%20Wang%20and%20Ruiliang%20Xu%20and%20Yuanyuan%20Cao%20and%20Lu%20Chen%20and%20Qianqian%20Wu%20and%20Huaiyu%20Gu%20and%20Lindong%20Lu%20and%20Keming%20Wang%20and%20Dechen%20Lin%20and%20Guanlin%20Shen%20and%20Xuanhe%20Zhou%20and%20Linfeng%20Zhang%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Jiaqi%20Wang%20and%20Bo%20Zhang%20and%20Lei%20Bai%20and%20Pei%20Chu%20and%20Weijia%20Li%20and%20Jiang%20Wu%20and%20Lijun%20Wu%20and%20Zhenxiang%20Li%20and%20Guangyu%20Wang%20and%20Zhongying%20Tu%20and%20Chao%20Xu%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Bowen%20Zhou%20and%20Dahua%20Lin%20and%20Wentao%20Zhang%20and%20Conghui%20He%0AAbstract%3A%20%20%20We%20introduce%20MinerU2.5%2C%20a%201.2B-parameter%20document%20parsing%20vision-language%0Amodel%20that%20achieves%20state-of-the-art%20recognition%20accuracy%20while%20maintaining%0Aexceptional%20computational%20efficiency.%20Our%20approach%20employs%20a%20coarse-to-fine%2C%0Atwo-stage%20parsing%20strategy%20that%20decouples%20global%20layout%20analysis%20from%20local%0Acontent%20recognition.%20In%20the%20first%20stage%2C%20the%20model%20performs%20efficient%20layout%0Aanalysis%20on%20downsampled%20images%20to%20identify%20structural%20elements%2C%20circumventing%0Athe%20computational%20overhead%20of%20processing%20high-resolution%20inputs.%20In%20the%20second%0Astage%2C%20guided%20by%20the%20global%20layout%2C%20it%20performs%20targeted%20content%20recognition%20on%0Anative-resolution%20crops%20extracted%20from%20the%20original%20image%2C%20preserving%0Afine-grained%20details%20in%20dense%20text%2C%20complex%20formulas%2C%20and%20tables.%20To%20support%0Athis%20strategy%2C%20we%20developed%20a%20comprehensive%20data%20engine%20that%20generates%20diverse%2C%0Alarge-scale%20training%20corpora%20for%20both%20pretraining%20and%20fine-tuning.%20Ultimately%2C%0AMinerU2.5%20demonstrates%20strong%20document%20parsing%20ability%2C%20achieving%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20surpassing%20both%0Ageneral-purpose%20and%20domain-specific%20models%20across%20various%20recognition%20tasks%2C%0Awhile%20maintaining%20significantly%20lower%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22186v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinerU2.5%253A%2520A%2520Decoupled%2520Vision-Language%2520Model%2520for%2520Efficient%250A%2520%2520High-Resolution%2520Document%2520Parsing%26entry.906535625%3DJunbo%2520Niu%2520and%2520Zheng%2520Liu%2520and%2520Zhuangcheng%2520Gu%2520and%2520Bin%2520Wang%2520and%2520Linke%2520Ouyang%2520and%2520Zhiyuan%2520Zhao%2520and%2520Tao%2520Chu%2520and%2520Tianyao%2520He%2520and%2520Fan%2520Wu%2520and%2520Qintong%2520Zhang%2520and%2520Zhenjiang%2520Jin%2520and%2520Guang%2520Liang%2520and%2520Rui%2520Zhang%2520and%2520Wenzheng%2520Zhang%2520and%2520Yuan%2520Qu%2520and%2520Zhifei%2520Ren%2520and%2520Yuefeng%2520Sun%2520and%2520Yuanhong%2520Zheng%2520and%2520Dongsheng%2520Ma%2520and%2520Zirui%2520Tang%2520and%2520Boyu%2520Niu%2520and%2520Ziyang%2520Miao%2520and%2520Hejun%2520Dong%2520and%2520Siyi%2520Qian%2520and%2520Junyuan%2520Zhang%2520and%2520Jingzhou%2520Chen%2520and%2520Fangdong%2520Wang%2520and%2520Xiaomeng%2520Zhao%2520and%2520Liqun%2520Wei%2520and%2520Wei%2520Li%2520and%2520Shasha%2520Wang%2520and%2520Ruiliang%2520Xu%2520and%2520Yuanyuan%2520Cao%2520and%2520Lu%2520Chen%2520and%2520Qianqian%2520Wu%2520and%2520Huaiyu%2520Gu%2520and%2520Lindong%2520Lu%2520and%2520Keming%2520Wang%2520and%2520Dechen%2520Lin%2520and%2520Guanlin%2520Shen%2520and%2520Xuanhe%2520Zhou%2520and%2520Linfeng%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Jiaqi%2520Wang%2520and%2520Bo%2520Zhang%2520and%2520Lei%2520Bai%2520and%2520Pei%2520Chu%2520and%2520Weijia%2520Li%2520and%2520Jiang%2520Wu%2520and%2520Lijun%2520Wu%2520and%2520Zhenxiang%2520Li%2520and%2520Guangyu%2520Wang%2520and%2520Zhongying%2520Tu%2520and%2520Chao%2520Xu%2520and%2520Kai%2520Chen%2520and%2520Yu%2520Qiao%2520and%2520Bowen%2520Zhou%2520and%2520Dahua%2520Lin%2520and%2520Wentao%2520Zhang%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520We%2520introduce%2520MinerU2.5%252C%2520a%25201.2B-parameter%2520document%2520parsing%2520vision-language%250Amodel%2520that%2520achieves%2520state-of-the-art%2520recognition%2520accuracy%2520while%2520maintaining%250Aexceptional%2520computational%2520efficiency.%2520Our%2520approach%2520employs%2520a%2520coarse-to-fine%252C%250Atwo-stage%2520parsing%2520strategy%2520that%2520decouples%2520global%2520layout%2520analysis%2520from%2520local%250Acontent%2520recognition.%2520In%2520the%2520first%2520stage%252C%2520the%2520model%2520performs%2520efficient%2520layout%250Aanalysis%2520on%2520downsampled%2520images%2520to%2520identify%2520structural%2520elements%252C%2520circumventing%250Athe%2520computational%2520overhead%2520of%2520processing%2520high-resolution%2520inputs.%2520In%2520the%2520second%250Astage%252C%2520guided%2520by%2520the%2520global%2520layout%252C%2520it%2520performs%2520targeted%2520content%2520recognition%2520on%250Anative-resolution%2520crops%2520extracted%2520from%2520the%2520original%2520image%252C%2520preserving%250Afine-grained%2520details%2520in%2520dense%2520text%252C%2520complex%2520formulas%252C%2520and%2520tables.%2520To%2520support%250Athis%2520strategy%252C%2520we%2520developed%2520a%2520comprehensive%2520data%2520engine%2520that%2520generates%2520diverse%252C%250Alarge-scale%2520training%2520corpora%2520for%2520both%2520pretraining%2520and%2520fine-tuning.%2520Ultimately%252C%250AMinerU2.5%2520demonstrates%2520strong%2520document%2520parsing%2520ability%252C%2520achieving%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%252C%2520surpassing%2520both%250Ageneral-purpose%2520and%2520domain-specific%2520models%2520across%2520various%2520recognition%2520tasks%252C%250Awhile%2520maintaining%2520significantly%2520lower%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22186v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MinerU2.5%3A%20A%20Decoupled%20Vision-Language%20Model%20for%20Efficient%0A%20%20High-Resolution%20Document%20Parsing&entry.906535625=Junbo%20Niu%20and%20Zheng%20Liu%20and%20Zhuangcheng%20Gu%20and%20Bin%20Wang%20and%20Linke%20Ouyang%20and%20Zhiyuan%20Zhao%20and%20Tao%20Chu%20and%20Tianyao%20He%20and%20Fan%20Wu%20and%20Qintong%20Zhang%20and%20Zhenjiang%20Jin%20and%20Guang%20Liang%20and%20Rui%20Zhang%20and%20Wenzheng%20Zhang%20and%20Yuan%20Qu%20and%20Zhifei%20Ren%20and%20Yuefeng%20Sun%20and%20Yuanhong%20Zheng%20and%20Dongsheng%20Ma%20and%20Zirui%20Tang%20and%20Boyu%20Niu%20and%20Ziyang%20Miao%20and%20Hejun%20Dong%20and%20Siyi%20Qian%20and%20Junyuan%20Zhang%20and%20Jingzhou%20Chen%20and%20Fangdong%20Wang%20and%20Xiaomeng%20Zhao%20and%20Liqun%20Wei%20and%20Wei%20Li%20and%20Shasha%20Wang%20and%20Ruiliang%20Xu%20and%20Yuanyuan%20Cao%20and%20Lu%20Chen%20and%20Qianqian%20Wu%20and%20Huaiyu%20Gu%20and%20Lindong%20Lu%20and%20Keming%20Wang%20and%20Dechen%20Lin%20and%20Guanlin%20Shen%20and%20Xuanhe%20Zhou%20and%20Linfeng%20Zhang%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Jiaqi%20Wang%20and%20Bo%20Zhang%20and%20Lei%20Bai%20and%20Pei%20Chu%20and%20Weijia%20Li%20and%20Jiang%20Wu%20and%20Lijun%20Wu%20and%20Zhenxiang%20Li%20and%20Guangyu%20Wang%20and%20Zhongying%20Tu%20and%20Chao%20Xu%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Bowen%20Zhou%20and%20Dahua%20Lin%20and%20Wentao%20Zhang%20and%20Conghui%20He&entry.1292438233=%20%20We%20introduce%20MinerU2.5%2C%20a%201.2B-parameter%20document%20parsing%20vision-language%0Amodel%20that%20achieves%20state-of-the-art%20recognition%20accuracy%20while%20maintaining%0Aexceptional%20computational%20efficiency.%20Our%20approach%20employs%20a%20coarse-to-fine%2C%0Atwo-stage%20parsing%20strategy%20that%20decouples%20global%20layout%20analysis%20from%20local%0Acontent%20recognition.%20In%20the%20first%20stage%2C%20the%20model%20performs%20efficient%20layout%0Aanalysis%20on%20downsampled%20images%20to%20identify%20structural%20elements%2C%20circumventing%0Athe%20computational%20overhead%20of%20processing%20high-resolution%20inputs.%20In%20the%20second%0Astage%2C%20guided%20by%20the%20global%20layout%2C%20it%20performs%20targeted%20content%20recognition%20on%0Anative-resolution%20crops%20extracted%20from%20the%20original%20image%2C%20preserving%0Afine-grained%20details%20in%20dense%20text%2C%20complex%20formulas%2C%20and%20tables.%20To%20support%0Athis%20strategy%2C%20we%20developed%20a%20comprehensive%20data%20engine%20that%20generates%20diverse%2C%0Alarge-scale%20training%20corpora%20for%20both%20pretraining%20and%20fine-tuning.%20Ultimately%2C%0AMinerU2.5%20demonstrates%20strong%20document%20parsing%20ability%2C%20achieving%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20surpassing%20both%0Ageneral-purpose%20and%20domain-specific%20models%20across%20various%20recognition%20tasks%2C%0Awhile%20maintaining%20significantly%20lower%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22186v2&entry.124074799=Read"},
{"title": "The Geometry of Cortical Computation: Manifold Disentanglement and\n  Predictive Dynamics in VCNet", "author": "Brennen A. Hill and Zhang Xinyu and Timothy Putra Prasetio", "abstract": "  Despite their success, modern convolutional neural networks (CNNs) exhibit\nfundamental limitations, including data inefficiency, poor out-of-distribution\ngeneralization, and vulnerability to adversarial perturbations. These\nshortcomings can be traced to a lack of inductive biases that reflect the\ninherent geometric structure of the visual world. The primate visual system, in\ncontrast, demonstrates superior efficiency and robustness, suggesting that its\narchitectural and computational principles,which evolved to internalize these\nstructures,may offer a blueprint for more capable artificial vision. This paper\nintroduces Visual Cortex Network (VCNet), a novel neural network architecture\nwhose design is informed by the macro-scale organization of the primate visual\ncortex. VCNet is framed as a geometric framework that emulates key biological\nmechanisms, including hierarchical processing across distinct cortical areas,\ndual-stream information segregation for learning disentangled representations,\nand top-down predictive feedback for representation refinement. We interpret\nthese mechanisms through the lens of geometry and dynamical systems, positing\nthat they guide the learning of structured, low-dimensional neural manifolds.\nWe evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern\ndataset, which probes sensitivity to natural textures, and a light field image\nclassification task, which requires processing higher-dimensional visual data.\nOur results show that VCNet achieves state-of-the-art accuracy of 92.1\\% on\nSpots-10 and 74.4\\% on the light field dataset, surpassing contemporary models\nof comparable size. This work demonstrates that integrating high-level\nneuroscientific principles, viewed through a geometric lens, can lead to more\nefficient and robust models, providing a promising direction for addressing\nlong-standing challenges in machine learning.\n", "link": "http://arxiv.org/abs/2508.02995v2", "date": "2025-09-29", "relevancy": 2.1662, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5466}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Geometry%20of%20Cortical%20Computation%3A%20Manifold%20Disentanglement%20and%0A%20%20Predictive%20Dynamics%20in%20VCNet&body=Title%3A%20The%20Geometry%20of%20Cortical%20Computation%3A%20Manifold%20Disentanglement%20and%0A%20%20Predictive%20Dynamics%20in%20VCNet%0AAuthor%3A%20Brennen%20A.%20Hill%20and%20Zhang%20Xinyu%20and%20Timothy%20Putra%20Prasetio%0AAbstract%3A%20%20%20Despite%20their%20success%2C%20modern%20convolutional%20neural%20networks%20%28CNNs%29%20exhibit%0Afundamental%20limitations%2C%20including%20data%20inefficiency%2C%20poor%20out-of-distribution%0Ageneralization%2C%20and%20vulnerability%20to%20adversarial%20perturbations.%20These%0Ashortcomings%20can%20be%20traced%20to%20a%20lack%20of%20inductive%20biases%20that%20reflect%20the%0Ainherent%20geometric%20structure%20of%20the%20visual%20world.%20The%20primate%20visual%20system%2C%20in%0Acontrast%2C%20demonstrates%20superior%20efficiency%20and%20robustness%2C%20suggesting%20that%20its%0Aarchitectural%20and%20computational%20principles%2Cwhich%20evolved%20to%20internalize%20these%0Astructures%2Cmay%20offer%20a%20blueprint%20for%20more%20capable%20artificial%20vision.%20This%20paper%0Aintroduces%20Visual%20Cortex%20Network%20%28VCNet%29%2C%20a%20novel%20neural%20network%20architecture%0Awhose%20design%20is%20informed%20by%20the%20macro-scale%20organization%20of%20the%20primate%20visual%0Acortex.%20VCNet%20is%20framed%20as%20a%20geometric%20framework%20that%20emulates%20key%20biological%0Amechanisms%2C%20including%20hierarchical%20processing%20across%20distinct%20cortical%20areas%2C%0Adual-stream%20information%20segregation%20for%20learning%20disentangled%20representations%2C%0Aand%20top-down%20predictive%20feedback%20for%20representation%20refinement.%20We%20interpret%0Athese%20mechanisms%20through%20the%20lens%20of%20geometry%20and%20dynamical%20systems%2C%20positing%0Athat%20they%20guide%20the%20learning%20of%20structured%2C%20low-dimensional%20neural%20manifolds.%0AWe%20evaluate%20VCNet%20on%20two%20specialized%20benchmarks%3A%20the%20Spots-10%20animal%20pattern%0Adataset%2C%20which%20probes%20sensitivity%20to%20natural%20textures%2C%20and%20a%20light%20field%20image%0Aclassification%20task%2C%20which%20requires%20processing%20higher-dimensional%20visual%20data.%0AOur%20results%20show%20that%20VCNet%20achieves%20state-of-the-art%20accuracy%20of%2092.1%5C%25%20on%0ASpots-10%20and%2074.4%5C%25%20on%20the%20light%20field%20dataset%2C%20surpassing%20contemporary%20models%0Aof%20comparable%20size.%20This%20work%20demonstrates%20that%20integrating%20high-level%0Aneuroscientific%20principles%2C%20viewed%20through%20a%20geometric%20lens%2C%20can%20lead%20to%20more%0Aefficient%20and%20robust%20models%2C%20providing%20a%20promising%20direction%20for%20addressing%0Along-standing%20challenges%20in%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Geometry%2520of%2520Cortical%2520Computation%253A%2520Manifold%2520Disentanglement%2520and%250A%2520%2520Predictive%2520Dynamics%2520in%2520VCNet%26entry.906535625%3DBrennen%2520A.%2520Hill%2520and%2520Zhang%2520Xinyu%2520and%2520Timothy%2520Putra%2520Prasetio%26entry.1292438233%3D%2520%2520Despite%2520their%2520success%252C%2520modern%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520exhibit%250Afundamental%2520limitations%252C%2520including%2520data%2520inefficiency%252C%2520poor%2520out-of-distribution%250Ageneralization%252C%2520and%2520vulnerability%2520to%2520adversarial%2520perturbations.%2520These%250Ashortcomings%2520can%2520be%2520traced%2520to%2520a%2520lack%2520of%2520inductive%2520biases%2520that%2520reflect%2520the%250Ainherent%2520geometric%2520structure%2520of%2520the%2520visual%2520world.%2520The%2520primate%2520visual%2520system%252C%2520in%250Acontrast%252C%2520demonstrates%2520superior%2520efficiency%2520and%2520robustness%252C%2520suggesting%2520that%2520its%250Aarchitectural%2520and%2520computational%2520principles%252Cwhich%2520evolved%2520to%2520internalize%2520these%250Astructures%252Cmay%2520offer%2520a%2520blueprint%2520for%2520more%2520capable%2520artificial%2520vision.%2520This%2520paper%250Aintroduces%2520Visual%2520Cortex%2520Network%2520%2528VCNet%2529%252C%2520a%2520novel%2520neural%2520network%2520architecture%250Awhose%2520design%2520is%2520informed%2520by%2520the%2520macro-scale%2520organization%2520of%2520the%2520primate%2520visual%250Acortex.%2520VCNet%2520is%2520framed%2520as%2520a%2520geometric%2520framework%2520that%2520emulates%2520key%2520biological%250Amechanisms%252C%2520including%2520hierarchical%2520processing%2520across%2520distinct%2520cortical%2520areas%252C%250Adual-stream%2520information%2520segregation%2520for%2520learning%2520disentangled%2520representations%252C%250Aand%2520top-down%2520predictive%2520feedback%2520for%2520representation%2520refinement.%2520We%2520interpret%250Athese%2520mechanisms%2520through%2520the%2520lens%2520of%2520geometry%2520and%2520dynamical%2520systems%252C%2520positing%250Athat%2520they%2520guide%2520the%2520learning%2520of%2520structured%252C%2520low-dimensional%2520neural%2520manifolds.%250AWe%2520evaluate%2520VCNet%2520on%2520two%2520specialized%2520benchmarks%253A%2520the%2520Spots-10%2520animal%2520pattern%250Adataset%252C%2520which%2520probes%2520sensitivity%2520to%2520natural%2520textures%252C%2520and%2520a%2520light%2520field%2520image%250Aclassification%2520task%252C%2520which%2520requires%2520processing%2520higher-dimensional%2520visual%2520data.%250AOur%2520results%2520show%2520that%2520VCNet%2520achieves%2520state-of-the-art%2520accuracy%2520of%252092.1%255C%2525%2520on%250ASpots-10%2520and%252074.4%255C%2525%2520on%2520the%2520light%2520field%2520dataset%252C%2520surpassing%2520contemporary%2520models%250Aof%2520comparable%2520size.%2520This%2520work%2520demonstrates%2520that%2520integrating%2520high-level%250Aneuroscientific%2520principles%252C%2520viewed%2520through%2520a%2520geometric%2520lens%252C%2520can%2520lead%2520to%2520more%250Aefficient%2520and%2520robust%2520models%252C%2520providing%2520a%2520promising%2520direction%2520for%2520addressing%250Along-standing%2520challenges%2520in%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Geometry%20of%20Cortical%20Computation%3A%20Manifold%20Disentanglement%20and%0A%20%20Predictive%20Dynamics%20in%20VCNet&entry.906535625=Brennen%20A.%20Hill%20and%20Zhang%20Xinyu%20and%20Timothy%20Putra%20Prasetio&entry.1292438233=%20%20Despite%20their%20success%2C%20modern%20convolutional%20neural%20networks%20%28CNNs%29%20exhibit%0Afundamental%20limitations%2C%20including%20data%20inefficiency%2C%20poor%20out-of-distribution%0Ageneralization%2C%20and%20vulnerability%20to%20adversarial%20perturbations.%20These%0Ashortcomings%20can%20be%20traced%20to%20a%20lack%20of%20inductive%20biases%20that%20reflect%20the%0Ainherent%20geometric%20structure%20of%20the%20visual%20world.%20The%20primate%20visual%20system%2C%20in%0Acontrast%2C%20demonstrates%20superior%20efficiency%20and%20robustness%2C%20suggesting%20that%20its%0Aarchitectural%20and%20computational%20principles%2Cwhich%20evolved%20to%20internalize%20these%0Astructures%2Cmay%20offer%20a%20blueprint%20for%20more%20capable%20artificial%20vision.%20This%20paper%0Aintroduces%20Visual%20Cortex%20Network%20%28VCNet%29%2C%20a%20novel%20neural%20network%20architecture%0Awhose%20design%20is%20informed%20by%20the%20macro-scale%20organization%20of%20the%20primate%20visual%0Acortex.%20VCNet%20is%20framed%20as%20a%20geometric%20framework%20that%20emulates%20key%20biological%0Amechanisms%2C%20including%20hierarchical%20processing%20across%20distinct%20cortical%20areas%2C%0Adual-stream%20information%20segregation%20for%20learning%20disentangled%20representations%2C%0Aand%20top-down%20predictive%20feedback%20for%20representation%20refinement.%20We%20interpret%0Athese%20mechanisms%20through%20the%20lens%20of%20geometry%20and%20dynamical%20systems%2C%20positing%0Athat%20they%20guide%20the%20learning%20of%20structured%2C%20low-dimensional%20neural%20manifolds.%0AWe%20evaluate%20VCNet%20on%20two%20specialized%20benchmarks%3A%20the%20Spots-10%20animal%20pattern%0Adataset%2C%20which%20probes%20sensitivity%20to%20natural%20textures%2C%20and%20a%20light%20field%20image%0Aclassification%20task%2C%20which%20requires%20processing%20higher-dimensional%20visual%20data.%0AOur%20results%20show%20that%20VCNet%20achieves%20state-of-the-art%20accuracy%20of%2092.1%5C%25%20on%0ASpots-10%20and%2074.4%5C%25%20on%20the%20light%20field%20dataset%2C%20surpassing%20contemporary%20models%0Aof%20comparable%20size.%20This%20work%20demonstrates%20that%20integrating%20high-level%0Aneuroscientific%20principles%2C%20viewed%20through%20a%20geometric%20lens%2C%20can%20lead%20to%20more%0Aefficient%20and%20robust%20models%2C%20providing%20a%20promising%20direction%20for%20addressing%0Along-standing%20challenges%20in%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02995v2&entry.124074799=Read"},
{"title": "Co-Evolving Complexity: An Adversarial Framework for Automatic MARL\n  Curricula", "author": "Brennen Hill", "abstract": "  The advancement of general-purpose intelligent agents is intrinsically linked\nto the environments in which they are trained. While scaling models and\ndatasets has yielded remarkable capabilities, scaling the complexity,\ndiversity, and interactivity of environments remains a crucial bottleneck.\nHand-crafted environments are finite and often contain implicit biases,\nlimiting the potential for agents to develop truly generalizable and robust\nskills. In this work, we propose a paradigm for generating a boundless and\nadaptive curriculum of challenges by framing the environment generation process\nas an adversarial game. We introduce a system where a team of cooperative\nmulti-agent defenders learns to survive against a procedurally generative\nattacker. The attacker agent learns to produce increasingly challenging\nconfigurations of enemy units, dynamically creating novel worlds tailored to\nexploit the defenders' current weaknesses. Concurrently, the defender team\nlearns cooperative strategies to overcome these generated threats. This\nco-evolutionary dynamic creates a self-scaling environment where complexity\narises organically from the adversarial interaction, providing an effectively\ninfinite stream of novel and relevant training data. We demonstrate that with\nminimal training, this approach leads to the emergence of complex, intelligent\nbehaviors, such as flanking and shielding by the attacker, and focus-fire and\nspreading by the defenders. Our findings suggest that adversarial co-evolution\nis a powerful mechanism for automatically scaling environmental complexity,\ndriving agents towards greater robustness and strategic depth.\n", "link": "http://arxiv.org/abs/2509.03771v2", "date": "2025-09-29", "relevancy": 2.1568, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5649}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5495}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Evolving%20Complexity%3A%20An%20Adversarial%20Framework%20for%20Automatic%20MARL%0A%20%20Curricula&body=Title%3A%20Co-Evolving%20Complexity%3A%20An%20Adversarial%20Framework%20for%20Automatic%20MARL%0A%20%20Curricula%0AAuthor%3A%20Brennen%20Hill%0AAbstract%3A%20%20%20The%20advancement%20of%20general-purpose%20intelligent%20agents%20is%20intrinsically%20linked%0Ato%20the%20environments%20in%20which%20they%20are%20trained.%20While%20scaling%20models%20and%0Adatasets%20has%20yielded%20remarkable%20capabilities%2C%20scaling%20the%20complexity%2C%0Adiversity%2C%20and%20interactivity%20of%20environments%20remains%20a%20crucial%20bottleneck.%0AHand-crafted%20environments%20are%20finite%20and%20often%20contain%20implicit%20biases%2C%0Alimiting%20the%20potential%20for%20agents%20to%20develop%20truly%20generalizable%20and%20robust%0Askills.%20In%20this%20work%2C%20we%20propose%20a%20paradigm%20for%20generating%20a%20boundless%20and%0Aadaptive%20curriculum%20of%20challenges%20by%20framing%20the%20environment%20generation%20process%0Aas%20an%20adversarial%20game.%20We%20introduce%20a%20system%20where%20a%20team%20of%20cooperative%0Amulti-agent%20defenders%20learns%20to%20survive%20against%20a%20procedurally%20generative%0Aattacker.%20The%20attacker%20agent%20learns%20to%20produce%20increasingly%20challenging%0Aconfigurations%20of%20enemy%20units%2C%20dynamically%20creating%20novel%20worlds%20tailored%20to%0Aexploit%20the%20defenders%27%20current%20weaknesses.%20Concurrently%2C%20the%20defender%20team%0Alearns%20cooperative%20strategies%20to%20overcome%20these%20generated%20threats.%20This%0Aco-evolutionary%20dynamic%20creates%20a%20self-scaling%20environment%20where%20complexity%0Aarises%20organically%20from%20the%20adversarial%20interaction%2C%20providing%20an%20effectively%0Ainfinite%20stream%20of%20novel%20and%20relevant%20training%20data.%20We%20demonstrate%20that%20with%0Aminimal%20training%2C%20this%20approach%20leads%20to%20the%20emergence%20of%20complex%2C%20intelligent%0Abehaviors%2C%20such%20as%20flanking%20and%20shielding%20by%20the%20attacker%2C%20and%20focus-fire%20and%0Aspreading%20by%20the%20defenders.%20Our%20findings%20suggest%20that%20adversarial%20co-evolution%0Ais%20a%20powerful%20mechanism%20for%20automatically%20scaling%20environmental%20complexity%2C%0Adriving%20agents%20towards%20greater%20robustness%20and%20strategic%20depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Evolving%2520Complexity%253A%2520An%2520Adversarial%2520Framework%2520for%2520Automatic%2520MARL%250A%2520%2520Curricula%26entry.906535625%3DBrennen%2520Hill%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520general-purpose%2520intelligent%2520agents%2520is%2520intrinsically%2520linked%250Ato%2520the%2520environments%2520in%2520which%2520they%2520are%2520trained.%2520While%2520scaling%2520models%2520and%250Adatasets%2520has%2520yielded%2520remarkable%2520capabilities%252C%2520scaling%2520the%2520complexity%252C%250Adiversity%252C%2520and%2520interactivity%2520of%2520environments%2520remains%2520a%2520crucial%2520bottleneck.%250AHand-crafted%2520environments%2520are%2520finite%2520and%2520often%2520contain%2520implicit%2520biases%252C%250Alimiting%2520the%2520potential%2520for%2520agents%2520to%2520develop%2520truly%2520generalizable%2520and%2520robust%250Askills.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520paradigm%2520for%2520generating%2520a%2520boundless%2520and%250Aadaptive%2520curriculum%2520of%2520challenges%2520by%2520framing%2520the%2520environment%2520generation%2520process%250Aas%2520an%2520adversarial%2520game.%2520We%2520introduce%2520a%2520system%2520where%2520a%2520team%2520of%2520cooperative%250Amulti-agent%2520defenders%2520learns%2520to%2520survive%2520against%2520a%2520procedurally%2520generative%250Aattacker.%2520The%2520attacker%2520agent%2520learns%2520to%2520produce%2520increasingly%2520challenging%250Aconfigurations%2520of%2520enemy%2520units%252C%2520dynamically%2520creating%2520novel%2520worlds%2520tailored%2520to%250Aexploit%2520the%2520defenders%2527%2520current%2520weaknesses.%2520Concurrently%252C%2520the%2520defender%2520team%250Alearns%2520cooperative%2520strategies%2520to%2520overcome%2520these%2520generated%2520threats.%2520This%250Aco-evolutionary%2520dynamic%2520creates%2520a%2520self-scaling%2520environment%2520where%2520complexity%250Aarises%2520organically%2520from%2520the%2520adversarial%2520interaction%252C%2520providing%2520an%2520effectively%250Ainfinite%2520stream%2520of%2520novel%2520and%2520relevant%2520training%2520data.%2520We%2520demonstrate%2520that%2520with%250Aminimal%2520training%252C%2520this%2520approach%2520leads%2520to%2520the%2520emergence%2520of%2520complex%252C%2520intelligent%250Abehaviors%252C%2520such%2520as%2520flanking%2520and%2520shielding%2520by%2520the%2520attacker%252C%2520and%2520focus-fire%2520and%250Aspreading%2520by%2520the%2520defenders.%2520Our%2520findings%2520suggest%2520that%2520adversarial%2520co-evolution%250Ais%2520a%2520powerful%2520mechanism%2520for%2520automatically%2520scaling%2520environmental%2520complexity%252C%250Adriving%2520agents%2520towards%2520greater%2520robustness%2520and%2520strategic%2520depth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Evolving%20Complexity%3A%20An%20Adversarial%20Framework%20for%20Automatic%20MARL%0A%20%20Curricula&entry.906535625=Brennen%20Hill&entry.1292438233=%20%20The%20advancement%20of%20general-purpose%20intelligent%20agents%20is%20intrinsically%20linked%0Ato%20the%20environments%20in%20which%20they%20are%20trained.%20While%20scaling%20models%20and%0Adatasets%20has%20yielded%20remarkable%20capabilities%2C%20scaling%20the%20complexity%2C%0Adiversity%2C%20and%20interactivity%20of%20environments%20remains%20a%20crucial%20bottleneck.%0AHand-crafted%20environments%20are%20finite%20and%20often%20contain%20implicit%20biases%2C%0Alimiting%20the%20potential%20for%20agents%20to%20develop%20truly%20generalizable%20and%20robust%0Askills.%20In%20this%20work%2C%20we%20propose%20a%20paradigm%20for%20generating%20a%20boundless%20and%0Aadaptive%20curriculum%20of%20challenges%20by%20framing%20the%20environment%20generation%20process%0Aas%20an%20adversarial%20game.%20We%20introduce%20a%20system%20where%20a%20team%20of%20cooperative%0Amulti-agent%20defenders%20learns%20to%20survive%20against%20a%20procedurally%20generative%0Aattacker.%20The%20attacker%20agent%20learns%20to%20produce%20increasingly%20challenging%0Aconfigurations%20of%20enemy%20units%2C%20dynamically%20creating%20novel%20worlds%20tailored%20to%0Aexploit%20the%20defenders%27%20current%20weaknesses.%20Concurrently%2C%20the%20defender%20team%0Alearns%20cooperative%20strategies%20to%20overcome%20these%20generated%20threats.%20This%0Aco-evolutionary%20dynamic%20creates%20a%20self-scaling%20environment%20where%20complexity%0Aarises%20organically%20from%20the%20adversarial%20interaction%2C%20providing%20an%20effectively%0Ainfinite%20stream%20of%20novel%20and%20relevant%20training%20data.%20We%20demonstrate%20that%20with%0Aminimal%20training%2C%20this%20approach%20leads%20to%20the%20emergence%20of%20complex%2C%20intelligent%0Abehaviors%2C%20such%20as%20flanking%20and%20shielding%20by%20the%20attacker%2C%20and%20focus-fire%20and%0Aspreading%20by%20the%20defenders.%20Our%20findings%20suggest%20that%20adversarial%20co-evolution%0Ais%20a%20powerful%20mechanism%20for%20automatically%20scaling%20environmental%20complexity%2C%0Adriving%20agents%20towards%20greater%20robustness%20and%20strategic%20depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03771v2&entry.124074799=Read"},
{"title": "UniAPL: A Unified Adversarial Preference Learning Framework for\n  Instruct-Following", "author": "FaQiang Qian and WeiKun Zhang and Ziliang Wang and Kang An and Xuhui Zheng and Liangjian Wen and Mengya Gao and Yong Dai and Yichao Wu", "abstract": "  Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.\n", "link": "http://arxiv.org/abs/2509.25148v1", "date": "2025-09-29", "relevancy": 2.1431, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5508}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5329}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniAPL%3A%20A%20Unified%20Adversarial%20Preference%20Learning%20Framework%20for%0A%20%20Instruct-Following&body=Title%3A%20UniAPL%3A%20A%20Unified%20Adversarial%20Preference%20Learning%20Framework%20for%0A%20%20Instruct-Following%0AAuthor%3A%20FaQiang%20Qian%20and%20WeiKun%20Zhang%20and%20Ziliang%20Wang%20and%20Kang%20An%20and%20Xuhui%20Zheng%20and%20Liangjian%20Wen%20and%20Mengya%20Gao%20and%20Yong%20Dai%20and%20Yichao%20Wu%0AAbstract%3A%20%20%20Shaping%20powerful%20LLMs%20to%20be%20beneficial%20and%20safe%20is%20central%20to%20AI%20alignment.%0AWe%20argue%20that%20post-training%20alignment%20is%20fundamentally%20a%20unified%20Preference%0ALearning%20problem%2C%20involving%20two%20modalities%3A%20demonstrated%20preferences%20%28e.g.%2C%0ASupervised%20Fine-Tuning%2C%20SFT%29%20and%20comparative%20preferences%20%28e.g.%2C%20Reinforcement%0ALearning%2C%20RL%29.The%20standard%20sequential%20pipeline-SFT%20followed%20by%20RL-is%20flawed%20due%0Ato%20a%20critical%20distributional%20mismatch%3A%20SFT%20uses%20static%20expert%20data%2C%20but%20as%20the%0Apolicy%20evolves%2C%20its%20generation%20distribution%20drifts%2C%20making%20SFT%20knowledge%0Abrittle.%20Subsequent%20RL%20then%20explores%20without%20direct%20access%20to%20the%20rich%2C%0Aground-truth%20knowledge%20in%20expert%20demonstrations%2C%20leading%20to%20inefficient%2C%0Aungrounded%20updates.%20This%20separation%20prevents%20mutual%20regularization%20between%20data%0Asources.%20To%20address%20this%2C%20we%20reframe%20alignment%20as%20a%20constrained%20optimization%0Aproblem%20and%20propose%20Unified%20Adversarial%20Preference%20Learning%20%28UniAPL%29%2Ca%20novel%0Aframework%20that%20dynamically%20aligns%20the%20policy%27s%20distribution%20with%20the%20expert%27s.%0AUniAPL%20implements%20a%20single-stage%20unified%20training%20objective%2C%20jointly%20learning%0Afrom%20mixed%20batches%20of%20SFT%20and%20preference%20data.%20In%20every%20gradient%20step%2C%20dense%0Aexpert%20demonstrations%20directly%20ground%20and%20regularize%20online%20exploration%2C%0Ainherently%20resolving%20distributional%20mismatch%20and%20maximizing%20data%20synergy.We%0Aevaluate%20UniAPL%20on%20instruction-following%20tasks%20using%20Qwen3-235B-Instruct-2507%0Aas%20the%20teacher.%20Our%20models%20match%20or%20exceed%20strong%20GRPO%20baselines%3A%20%2B5.77%25%20on%0AQwen3-0.6B%20%28matching%20a%2032B%20model%29%20and%20%2B3.75%25%20on%20Qwen3-4B%2Ceven%20outperforming%20the%0Ateacher.%20Analyses%20of%20response%20length%20and%20log-probability%20distributions%20confirm%0Athat%20UniAPL%20outputs%20closely%20mimic%20expert%20demonstrations%2C%20achieving%20both%0Astronger%20performance%20and%20better%20behavioral%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniAPL%253A%2520A%2520Unified%2520Adversarial%2520Preference%2520Learning%2520Framework%2520for%250A%2520%2520Instruct-Following%26entry.906535625%3DFaQiang%2520Qian%2520and%2520WeiKun%2520Zhang%2520and%2520Ziliang%2520Wang%2520and%2520Kang%2520An%2520and%2520Xuhui%2520Zheng%2520and%2520Liangjian%2520Wen%2520and%2520Mengya%2520Gao%2520and%2520Yong%2520Dai%2520and%2520Yichao%2520Wu%26entry.1292438233%3D%2520%2520Shaping%2520powerful%2520LLMs%2520to%2520be%2520beneficial%2520and%2520safe%2520is%2520central%2520to%2520AI%2520alignment.%250AWe%2520argue%2520that%2520post-training%2520alignment%2520is%2520fundamentally%2520a%2520unified%2520Preference%250ALearning%2520problem%252C%2520involving%2520two%2520modalities%253A%2520demonstrated%2520preferences%2520%2528e.g.%252C%250ASupervised%2520Fine-Tuning%252C%2520SFT%2529%2520and%2520comparative%2520preferences%2520%2528e.g.%252C%2520Reinforcement%250ALearning%252C%2520RL%2529.The%2520standard%2520sequential%2520pipeline-SFT%2520followed%2520by%2520RL-is%2520flawed%2520due%250Ato%2520a%2520critical%2520distributional%2520mismatch%253A%2520SFT%2520uses%2520static%2520expert%2520data%252C%2520but%2520as%2520the%250Apolicy%2520evolves%252C%2520its%2520generation%2520distribution%2520drifts%252C%2520making%2520SFT%2520knowledge%250Abrittle.%2520Subsequent%2520RL%2520then%2520explores%2520without%2520direct%2520access%2520to%2520the%2520rich%252C%250Aground-truth%2520knowledge%2520in%2520expert%2520demonstrations%252C%2520leading%2520to%2520inefficient%252C%250Aungrounded%2520updates.%2520This%2520separation%2520prevents%2520mutual%2520regularization%2520between%2520data%250Asources.%2520To%2520address%2520this%252C%2520we%2520reframe%2520alignment%2520as%2520a%2520constrained%2520optimization%250Aproblem%2520and%2520propose%2520Unified%2520Adversarial%2520Preference%2520Learning%2520%2528UniAPL%2529%252Ca%2520novel%250Aframework%2520that%2520dynamically%2520aligns%2520the%2520policy%2527s%2520distribution%2520with%2520the%2520expert%2527s.%250AUniAPL%2520implements%2520a%2520single-stage%2520unified%2520training%2520objective%252C%2520jointly%2520learning%250Afrom%2520mixed%2520batches%2520of%2520SFT%2520and%2520preference%2520data.%2520In%2520every%2520gradient%2520step%252C%2520dense%250Aexpert%2520demonstrations%2520directly%2520ground%2520and%2520regularize%2520online%2520exploration%252C%250Ainherently%2520resolving%2520distributional%2520mismatch%2520and%2520maximizing%2520data%2520synergy.We%250Aevaluate%2520UniAPL%2520on%2520instruction-following%2520tasks%2520using%2520Qwen3-235B-Instruct-2507%250Aas%2520the%2520teacher.%2520Our%2520models%2520match%2520or%2520exceed%2520strong%2520GRPO%2520baselines%253A%2520%252B5.77%2525%2520on%250AQwen3-0.6B%2520%2528matching%2520a%252032B%2520model%2529%2520and%2520%252B3.75%2525%2520on%2520Qwen3-4B%252Ceven%2520outperforming%2520the%250Ateacher.%2520Analyses%2520of%2520response%2520length%2520and%2520log-probability%2520distributions%2520confirm%250Athat%2520UniAPL%2520outputs%2520closely%2520mimic%2520expert%2520demonstrations%252C%2520achieving%2520both%250Astronger%2520performance%2520and%2520better%2520behavioral%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniAPL%3A%20A%20Unified%20Adversarial%20Preference%20Learning%20Framework%20for%0A%20%20Instruct-Following&entry.906535625=FaQiang%20Qian%20and%20WeiKun%20Zhang%20and%20Ziliang%20Wang%20and%20Kang%20An%20and%20Xuhui%20Zheng%20and%20Liangjian%20Wen%20and%20Mengya%20Gao%20and%20Yong%20Dai%20and%20Yichao%20Wu&entry.1292438233=%20%20Shaping%20powerful%20LLMs%20to%20be%20beneficial%20and%20safe%20is%20central%20to%20AI%20alignment.%0AWe%20argue%20that%20post-training%20alignment%20is%20fundamentally%20a%20unified%20Preference%0ALearning%20problem%2C%20involving%20two%20modalities%3A%20demonstrated%20preferences%20%28e.g.%2C%0ASupervised%20Fine-Tuning%2C%20SFT%29%20and%20comparative%20preferences%20%28e.g.%2C%20Reinforcement%0ALearning%2C%20RL%29.The%20standard%20sequential%20pipeline-SFT%20followed%20by%20RL-is%20flawed%20due%0Ato%20a%20critical%20distributional%20mismatch%3A%20SFT%20uses%20static%20expert%20data%2C%20but%20as%20the%0Apolicy%20evolves%2C%20its%20generation%20distribution%20drifts%2C%20making%20SFT%20knowledge%0Abrittle.%20Subsequent%20RL%20then%20explores%20without%20direct%20access%20to%20the%20rich%2C%0Aground-truth%20knowledge%20in%20expert%20demonstrations%2C%20leading%20to%20inefficient%2C%0Aungrounded%20updates.%20This%20separation%20prevents%20mutual%20regularization%20between%20data%0Asources.%20To%20address%20this%2C%20we%20reframe%20alignment%20as%20a%20constrained%20optimization%0Aproblem%20and%20propose%20Unified%20Adversarial%20Preference%20Learning%20%28UniAPL%29%2Ca%20novel%0Aframework%20that%20dynamically%20aligns%20the%20policy%27s%20distribution%20with%20the%20expert%27s.%0AUniAPL%20implements%20a%20single-stage%20unified%20training%20objective%2C%20jointly%20learning%0Afrom%20mixed%20batches%20of%20SFT%20and%20preference%20data.%20In%20every%20gradient%20step%2C%20dense%0Aexpert%20demonstrations%20directly%20ground%20and%20regularize%20online%20exploration%2C%0Ainherently%20resolving%20distributional%20mismatch%20and%20maximizing%20data%20synergy.We%0Aevaluate%20UniAPL%20on%20instruction-following%20tasks%20using%20Qwen3-235B-Instruct-2507%0Aas%20the%20teacher.%20Our%20models%20match%20or%20exceed%20strong%20GRPO%20baselines%3A%20%2B5.77%25%20on%0AQwen3-0.6B%20%28matching%20a%2032B%20model%29%20and%20%2B3.75%25%20on%20Qwen3-4B%2Ceven%20outperforming%20the%0Ateacher.%20Analyses%20of%20response%20length%20and%20log-probability%20distributions%20confirm%0Athat%20UniAPL%20outputs%20closely%20mimic%20expert%20demonstrations%2C%20achieving%20both%0Astronger%20performance%20and%20better%20behavioral%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25148v1&entry.124074799=Read"},
{"title": "PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient\n  Remote Photoplethysmography Measurement", "author": "Bo Zhao and Dan Guo and Junzhe Cao and Yong Xu and Tao Tan and Yue Sun and Bochao Zou and Jie Zhang and Zitong Yu", "abstract": "  Remote photoplethysmography (rPPG) measurement enables non-contact\nphysiological monitoring but suffers from accuracy degradation under head\nmotion and illumination changes. Existing deep learning methods are mostly\nheuristic and lack theoretical grounding, which limits robustness and\ninterpretability. In this work, we propose a physics-informed rPPG paradigm\nderived from the Navier-Stokes equations of hemodynamics, showing that the\npulse signal follows a second-order dynamical system whose discrete solution\nnaturally leads to a causal convolution. This provides a theoretical\njustification for using a Temporal Convolutional Network (TCN). Based on this\nprinciple, we design PHASE-Net, a lightweight model with three key components:\n(1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial\nchannels to mix distant facial regions and enhance cross-region feature\ninteraction without breaking temporal order; (2) Adaptive Spatial Filter, which\nlearns a soft spatial mask per frame to highlight signal-rich areas and\nsuppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models\nlong-range temporal dynamics for accurate pulse recovery. Extensive experiments\ndemonstrate that PHASE-Net achieves state-of-the-art performance with strong\nefficiency, offering a theoretically grounded and deployment-ready rPPG\nsolution.\n", "link": "http://arxiv.org/abs/2509.24850v1", "date": "2025-09-29", "relevancy": 2.1162, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5508}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5144}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PHASE-Net%3A%20Physics-Grounded%20Harmonic%20Attention%20System%20for%20Efficient%0A%20%20Remote%20Photoplethysmography%20Measurement&body=Title%3A%20PHASE-Net%3A%20Physics-Grounded%20Harmonic%20Attention%20System%20for%20Efficient%0A%20%20Remote%20Photoplethysmography%20Measurement%0AAuthor%3A%20Bo%20Zhao%20and%20Dan%20Guo%20and%20Junzhe%20Cao%20and%20Yong%20Xu%20and%20Tao%20Tan%20and%20Yue%20Sun%20and%20Bochao%20Zou%20and%20Jie%20Zhang%20and%20Zitong%20Yu%0AAbstract%3A%20%20%20Remote%20photoplethysmography%20%28rPPG%29%20measurement%20enables%20non-contact%0Aphysiological%20monitoring%20but%20suffers%20from%20accuracy%20degradation%20under%20head%0Amotion%20and%20illumination%20changes.%20Existing%20deep%20learning%20methods%20are%20mostly%0Aheuristic%20and%20lack%20theoretical%20grounding%2C%20which%20limits%20robustness%20and%0Ainterpretability.%20In%20this%20work%2C%20we%20propose%20a%20physics-informed%20rPPG%20paradigm%0Aderived%20from%20the%20Navier-Stokes%20equations%20of%20hemodynamics%2C%20showing%20that%20the%0Apulse%20signal%20follows%20a%20second-order%20dynamical%20system%20whose%20discrete%20solution%0Anaturally%20leads%20to%20a%20causal%20convolution.%20This%20provides%20a%20theoretical%0Ajustification%20for%20using%20a%20Temporal%20Convolutional%20Network%20%28TCN%29.%20Based%20on%20this%0Aprinciple%2C%20we%20design%20PHASE-Net%2C%20a%20lightweight%20model%20with%20three%20key%20components%3A%0A%281%29%20Zero-FLOPs%20Axial%20Swapper%20module%2C%20which%20swaps%20or%20transposes%20a%20few%20spatial%0Achannels%20to%20mix%20distant%20facial%20regions%20and%20enhance%20cross-region%20feature%0Ainteraction%20without%20breaking%20temporal%20order%3B%20%282%29%20Adaptive%20Spatial%20Filter%2C%20which%0Alearns%20a%20soft%20spatial%20mask%20per%20frame%20to%20highlight%20signal-rich%20areas%20and%0Asuppress%20noise%3B%20and%20%283%29%20Gated%20TCN%2C%20a%20causal%20dilated%20TCN%20with%20gating%20that%20models%0Along-range%20temporal%20dynamics%20for%20accurate%20pulse%20recovery.%20Extensive%20experiments%0Ademonstrate%20that%20PHASE-Net%20achieves%20state-of-the-art%20performance%20with%20strong%0Aefficiency%2C%20offering%20a%20theoretically%20grounded%20and%20deployment-ready%20rPPG%0Asolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPHASE-Net%253A%2520Physics-Grounded%2520Harmonic%2520Attention%2520System%2520for%2520Efficient%250A%2520%2520Remote%2520Photoplethysmography%2520Measurement%26entry.906535625%3DBo%2520Zhao%2520and%2520Dan%2520Guo%2520and%2520Junzhe%2520Cao%2520and%2520Yong%2520Xu%2520and%2520Tao%2520Tan%2520and%2520Yue%2520Sun%2520and%2520Bochao%2520Zou%2520and%2520Jie%2520Zhang%2520and%2520Zitong%2520Yu%26entry.1292438233%3D%2520%2520Remote%2520photoplethysmography%2520%2528rPPG%2529%2520measurement%2520enables%2520non-contact%250Aphysiological%2520monitoring%2520but%2520suffers%2520from%2520accuracy%2520degradation%2520under%2520head%250Amotion%2520and%2520illumination%2520changes.%2520Existing%2520deep%2520learning%2520methods%2520are%2520mostly%250Aheuristic%2520and%2520lack%2520theoretical%2520grounding%252C%2520which%2520limits%2520robustness%2520and%250Ainterpretability.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520physics-informed%2520rPPG%2520paradigm%250Aderived%2520from%2520the%2520Navier-Stokes%2520equations%2520of%2520hemodynamics%252C%2520showing%2520that%2520the%250Apulse%2520signal%2520follows%2520a%2520second-order%2520dynamical%2520system%2520whose%2520discrete%2520solution%250Anaturally%2520leads%2520to%2520a%2520causal%2520convolution.%2520This%2520provides%2520a%2520theoretical%250Ajustification%2520for%2520using%2520a%2520Temporal%2520Convolutional%2520Network%2520%2528TCN%2529.%2520Based%2520on%2520this%250Aprinciple%252C%2520we%2520design%2520PHASE-Net%252C%2520a%2520lightweight%2520model%2520with%2520three%2520key%2520components%253A%250A%25281%2529%2520Zero-FLOPs%2520Axial%2520Swapper%2520module%252C%2520which%2520swaps%2520or%2520transposes%2520a%2520few%2520spatial%250Achannels%2520to%2520mix%2520distant%2520facial%2520regions%2520and%2520enhance%2520cross-region%2520feature%250Ainteraction%2520without%2520breaking%2520temporal%2520order%253B%2520%25282%2529%2520Adaptive%2520Spatial%2520Filter%252C%2520which%250Alearns%2520a%2520soft%2520spatial%2520mask%2520per%2520frame%2520to%2520highlight%2520signal-rich%2520areas%2520and%250Asuppress%2520noise%253B%2520and%2520%25283%2529%2520Gated%2520TCN%252C%2520a%2520causal%2520dilated%2520TCN%2520with%2520gating%2520that%2520models%250Along-range%2520temporal%2520dynamics%2520for%2520accurate%2520pulse%2520recovery.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520PHASE-Net%2520achieves%2520state-of-the-art%2520performance%2520with%2520strong%250Aefficiency%252C%2520offering%2520a%2520theoretically%2520grounded%2520and%2520deployment-ready%2520rPPG%250Asolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHASE-Net%3A%20Physics-Grounded%20Harmonic%20Attention%20System%20for%20Efficient%0A%20%20Remote%20Photoplethysmography%20Measurement&entry.906535625=Bo%20Zhao%20and%20Dan%20Guo%20and%20Junzhe%20Cao%20and%20Yong%20Xu%20and%20Tao%20Tan%20and%20Yue%20Sun%20and%20Bochao%20Zou%20and%20Jie%20Zhang%20and%20Zitong%20Yu&entry.1292438233=%20%20Remote%20photoplethysmography%20%28rPPG%29%20measurement%20enables%20non-contact%0Aphysiological%20monitoring%20but%20suffers%20from%20accuracy%20degradation%20under%20head%0Amotion%20and%20illumination%20changes.%20Existing%20deep%20learning%20methods%20are%20mostly%0Aheuristic%20and%20lack%20theoretical%20grounding%2C%20which%20limits%20robustness%20and%0Ainterpretability.%20In%20this%20work%2C%20we%20propose%20a%20physics-informed%20rPPG%20paradigm%0Aderived%20from%20the%20Navier-Stokes%20equations%20of%20hemodynamics%2C%20showing%20that%20the%0Apulse%20signal%20follows%20a%20second-order%20dynamical%20system%20whose%20discrete%20solution%0Anaturally%20leads%20to%20a%20causal%20convolution.%20This%20provides%20a%20theoretical%0Ajustification%20for%20using%20a%20Temporal%20Convolutional%20Network%20%28TCN%29.%20Based%20on%20this%0Aprinciple%2C%20we%20design%20PHASE-Net%2C%20a%20lightweight%20model%20with%20three%20key%20components%3A%0A%281%29%20Zero-FLOPs%20Axial%20Swapper%20module%2C%20which%20swaps%20or%20transposes%20a%20few%20spatial%0Achannels%20to%20mix%20distant%20facial%20regions%20and%20enhance%20cross-region%20feature%0Ainteraction%20without%20breaking%20temporal%20order%3B%20%282%29%20Adaptive%20Spatial%20Filter%2C%20which%0Alearns%20a%20soft%20spatial%20mask%20per%20frame%20to%20highlight%20signal-rich%20areas%20and%0Asuppress%20noise%3B%20and%20%283%29%20Gated%20TCN%2C%20a%20causal%20dilated%20TCN%20with%20gating%20that%20models%0Along-range%20temporal%20dynamics%20for%20accurate%20pulse%20recovery.%20Extensive%20experiments%0Ademonstrate%20that%20PHASE-Net%20achieves%20state-of-the-art%20performance%20with%20strong%0Aefficiency%2C%20offering%20a%20theoretically%20grounded%20and%20deployment-ready%20rPPG%0Asolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24850v1&entry.124074799=Read"},
{"title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition", "author": "Mil\u00e1n Zsolt Bagladi and L\u00e1szl\u00f3 Guly\u00e1s and Gerg\u0151 Szalay", "abstract": "  This paper presents a real-time pipeline for dynamic arm gesture recognition\nbased on OpenPose keypoint estimation, keypoint normalization, and a recurrent\nneural network classifier. The 1 x 1 normalization scheme and two feature\nrepresentations (coordinate- and angle-based) are presented for the pipeline.\nIn addition, an efficient method to improve robustness against camera angle\nvariations is also introduced by using artificially rotated training data.\nExperiments on a custom traffic-control gesture dataset demonstrate high\naccuracy across varying viewing angles and speeds. Finally, an approach to\ncalculate the speed of the arm signal (if necessary) is also presented.\n", "link": "http://arxiv.org/abs/2509.25042v1", "date": "2025-09-29", "relevancy": 2.1159, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5174}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Real-Time%20Pipeline%20for%20Robust%20Arm%20Gesture%20Recognition&body=Title%3A%20Fast%20Real-Time%20Pipeline%20for%20Robust%20Arm%20Gesture%20Recognition%0AAuthor%3A%20Mil%C3%A1n%20Zsolt%20Bagladi%20and%20L%C3%A1szl%C3%B3%20Guly%C3%A1s%20and%20Gerg%C5%91%20Szalay%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20real-time%20pipeline%20for%20dynamic%20arm%20gesture%20recognition%0Abased%20on%20OpenPose%20keypoint%20estimation%2C%20keypoint%20normalization%2C%20and%20a%20recurrent%0Aneural%20network%20classifier.%20The%201%20x%201%20normalization%20scheme%20and%20two%20feature%0Arepresentations%20%28coordinate-%20and%20angle-based%29%20are%20presented%20for%20the%20pipeline.%0AIn%20addition%2C%20an%20efficient%20method%20to%20improve%20robustness%20against%20camera%20angle%0Avariations%20is%20also%20introduced%20by%20using%20artificially%20rotated%20training%20data.%0AExperiments%20on%20a%20custom%20traffic-control%20gesture%20dataset%20demonstrate%20high%0Aaccuracy%20across%20varying%20viewing%20angles%20and%20speeds.%20Finally%2C%20an%20approach%20to%0Acalculate%20the%20speed%20of%20the%20arm%20signal%20%28if%20necessary%29%20is%20also%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Real-Time%2520Pipeline%2520for%2520Robust%2520Arm%2520Gesture%2520Recognition%26entry.906535625%3DMil%25C3%25A1n%2520Zsolt%2520Bagladi%2520and%2520L%25C3%25A1szl%25C3%25B3%2520Guly%25C3%25A1s%2520and%2520Gerg%25C5%2591%2520Szalay%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520real-time%2520pipeline%2520for%2520dynamic%2520arm%2520gesture%2520recognition%250Abased%2520on%2520OpenPose%2520keypoint%2520estimation%252C%2520keypoint%2520normalization%252C%2520and%2520a%2520recurrent%250Aneural%2520network%2520classifier.%2520The%25201%2520x%25201%2520normalization%2520scheme%2520and%2520two%2520feature%250Arepresentations%2520%2528coordinate-%2520and%2520angle-based%2529%2520are%2520presented%2520for%2520the%2520pipeline.%250AIn%2520addition%252C%2520an%2520efficient%2520method%2520to%2520improve%2520robustness%2520against%2520camera%2520angle%250Avariations%2520is%2520also%2520introduced%2520by%2520using%2520artificially%2520rotated%2520training%2520data.%250AExperiments%2520on%2520a%2520custom%2520traffic-control%2520gesture%2520dataset%2520demonstrate%2520high%250Aaccuracy%2520across%2520varying%2520viewing%2520angles%2520and%2520speeds.%2520Finally%252C%2520an%2520approach%2520to%250Acalculate%2520the%2520speed%2520of%2520the%2520arm%2520signal%2520%2528if%2520necessary%2529%2520is%2520also%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Real-Time%20Pipeline%20for%20Robust%20Arm%20Gesture%20Recognition&entry.906535625=Mil%C3%A1n%20Zsolt%20Bagladi%20and%20L%C3%A1szl%C3%B3%20Guly%C3%A1s%20and%20Gerg%C5%91%20Szalay&entry.1292438233=%20%20This%20paper%20presents%20a%20real-time%20pipeline%20for%20dynamic%20arm%20gesture%20recognition%0Abased%20on%20OpenPose%20keypoint%20estimation%2C%20keypoint%20normalization%2C%20and%20a%20recurrent%0Aneural%20network%20classifier.%20The%201%20x%201%20normalization%20scheme%20and%20two%20feature%0Arepresentations%20%28coordinate-%20and%20angle-based%29%20are%20presented%20for%20the%20pipeline.%0AIn%20addition%2C%20an%20efficient%20method%20to%20improve%20robustness%20against%20camera%20angle%0Avariations%20is%20also%20introduced%20by%20using%20artificially%20rotated%20training%20data.%0AExperiments%20on%20a%20custom%20traffic-control%20gesture%20dataset%20demonstrate%20high%0Aaccuracy%20across%20varying%20viewing%20angles%20and%20speeds.%20Finally%2C%20an%20approach%20to%0Acalculate%20the%20speed%20of%20the%20arm%20signal%20%28if%20necessary%29%20is%20also%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25042v1&entry.124074799=Read"},
{"title": "Towards Better Generalization via Distributional Input Projection\n  Network", "author": "Yifan Hao and Yanxin Lu and Hanning Zhang and Xinwei Shen and Tong Zhang", "abstract": "  As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning.\n", "link": "http://arxiv.org/abs/2506.04690v2", "date": "2025-09-29", "relevancy": 2.1127, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5375}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5367}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Better%20Generalization%20via%20Distributional%20Input%20Projection%0A%20%20Network&body=Title%3A%20Towards%20Better%20Generalization%20via%20Distributional%20Input%20Projection%0A%20%20Network%0AAuthor%3A%20Yifan%20Hao%20and%20Yanxin%20Lu%20and%20Hanning%20Zhang%20and%20Xinwei%20Shen%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20As%20overparameterized%20models%20become%20increasingly%20prevalent%2C%20training%20loss%0Aalone%20offers%20limited%20insight%20into%20generalization%20performance.%20While%20smoothness%0Ahas%20been%20linked%20to%20improved%20generalization%20across%20various%20settings%2C%20directly%0Aenforcing%20smoothness%20in%20neural%20networks%20remains%20challenging.%20To%20address%20this%2C%0Awe%20introduce%20Distributional%20Input%20Projection%20Networks%20%28DIPNet%29%2C%20a%20novel%0Aframework%20that%20projects%20inputs%20into%20learnable%20distributions%20at%20each%20layer.%20This%0Adistributional%20representation%20induces%20a%20smoother%20loss%20landscape%20with%20respect%20to%0Athe%20input%2C%20promoting%20better%20generalization.%20We%20provide%20theoretical%20analysis%0Ashowing%20that%20DIPNet%20reduces%20both%20local%20smoothness%20measures%20and%20the%20Lipschitz%0Aconstant%20of%20the%20network%2C%20contributing%20to%20improved%20generalization%20performance.%0AEmpirically%2C%20we%20validate%20DIPNet%20across%20a%20wide%20range%20of%20architectures%20and%20tasks%2C%0Aincluding%20Vision%20Transformers%20%28ViTs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20ResNet%20and%0AMLPs.%20Our%20method%20consistently%20enhances%20test%20performance%20under%20standard%0Asettings%2C%20adversarial%20attacks%2C%20out-of-distribution%20inputs%2C%20and%20reasoning%0Abenchmarks.%20We%20demonstrate%20that%20the%20proposed%20input%20projection%20strategy%20can%20be%0Aseamlessly%20integrated%20into%20existing%20models%2C%20providing%20a%20general%20and%20effective%0Aapproach%20for%20boosting%20generalization%20performance%20in%20modern%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Better%2520Generalization%2520via%2520Distributional%2520Input%2520Projection%250A%2520%2520Network%26entry.906535625%3DYifan%2520Hao%2520and%2520Yanxin%2520Lu%2520and%2520Hanning%2520Zhang%2520and%2520Xinwei%2520Shen%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520As%2520overparameterized%2520models%2520become%2520increasingly%2520prevalent%252C%2520training%2520loss%250Aalone%2520offers%2520limited%2520insight%2520into%2520generalization%2520performance.%2520While%2520smoothness%250Ahas%2520been%2520linked%2520to%2520improved%2520generalization%2520across%2520various%2520settings%252C%2520directly%250Aenforcing%2520smoothness%2520in%2520neural%2520networks%2520remains%2520challenging.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520Distributional%2520Input%2520Projection%2520Networks%2520%2528DIPNet%2529%252C%2520a%2520novel%250Aframework%2520that%2520projects%2520inputs%2520into%2520learnable%2520distributions%2520at%2520each%2520layer.%2520This%250Adistributional%2520representation%2520induces%2520a%2520smoother%2520loss%2520landscape%2520with%2520respect%2520to%250Athe%2520input%252C%2520promoting%2520better%2520generalization.%2520We%2520provide%2520theoretical%2520analysis%250Ashowing%2520that%2520DIPNet%2520reduces%2520both%2520local%2520smoothness%2520measures%2520and%2520the%2520Lipschitz%250Aconstant%2520of%2520the%2520network%252C%2520contributing%2520to%2520improved%2520generalization%2520performance.%250AEmpirically%252C%2520we%2520validate%2520DIPNet%2520across%2520a%2520wide%2520range%2520of%2520architectures%2520and%2520tasks%252C%250Aincluding%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520ResNet%2520and%250AMLPs.%2520Our%2520method%2520consistently%2520enhances%2520test%2520performance%2520under%2520standard%250Asettings%252C%2520adversarial%2520attacks%252C%2520out-of-distribution%2520inputs%252C%2520and%2520reasoning%250Abenchmarks.%2520We%2520demonstrate%2520that%2520the%2520proposed%2520input%2520projection%2520strategy%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520existing%2520models%252C%2520providing%2520a%2520general%2520and%2520effective%250Aapproach%2520for%2520boosting%2520generalization%2520performance%2520in%2520modern%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Better%20Generalization%20via%20Distributional%20Input%20Projection%0A%20%20Network&entry.906535625=Yifan%20Hao%20and%20Yanxin%20Lu%20and%20Hanning%20Zhang%20and%20Xinwei%20Shen%20and%20Tong%20Zhang&entry.1292438233=%20%20As%20overparameterized%20models%20become%20increasingly%20prevalent%2C%20training%20loss%0Aalone%20offers%20limited%20insight%20into%20generalization%20performance.%20While%20smoothness%0Ahas%20been%20linked%20to%20improved%20generalization%20across%20various%20settings%2C%20directly%0Aenforcing%20smoothness%20in%20neural%20networks%20remains%20challenging.%20To%20address%20this%2C%0Awe%20introduce%20Distributional%20Input%20Projection%20Networks%20%28DIPNet%29%2C%20a%20novel%0Aframework%20that%20projects%20inputs%20into%20learnable%20distributions%20at%20each%20layer.%20This%0Adistributional%20representation%20induces%20a%20smoother%20loss%20landscape%20with%20respect%20to%0Athe%20input%2C%20promoting%20better%20generalization.%20We%20provide%20theoretical%20analysis%0Ashowing%20that%20DIPNet%20reduces%20both%20local%20smoothness%20measures%20and%20the%20Lipschitz%0Aconstant%20of%20the%20network%2C%20contributing%20to%20improved%20generalization%20performance.%0AEmpirically%2C%20we%20validate%20DIPNet%20across%20a%20wide%20range%20of%20architectures%20and%20tasks%2C%0Aincluding%20Vision%20Transformers%20%28ViTs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20ResNet%20and%0AMLPs.%20Our%20method%20consistently%20enhances%20test%20performance%20under%20standard%0Asettings%2C%20adversarial%20attacks%2C%20out-of-distribution%20inputs%2C%20and%20reasoning%0Abenchmarks.%20We%20demonstrate%20that%20the%20proposed%20input%20projection%20strategy%20can%20be%0Aseamlessly%20integrated%20into%20existing%20models%2C%20providing%20a%20general%20and%20effective%0Aapproach%20for%20boosting%20generalization%20performance%20in%20modern%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04690v2&entry.124074799=Read"},
{"title": "FRABench and UFEval: Unified Fine-grained Evaluation with Task and\n  Aspect Generalization", "author": "Shibo Hong and Jiahao Ying and Haiyuan Liang and Mengdi Zhang and Jun Kuang and Jiazheng Zhang and Yixin Cao", "abstract": "  Evaluating open-ended outputs of Multimodal Large Language Models has become\na bottleneck as model capabilities, task diversity, and modality rapidly\nexpand. Existing ``MLLM-as-a-Judge'' evaluators, though promising, remain\nconstrained to specific tasks and aspects. In this paper, we argue that, on one\nhand, based on the interconnected nature of aspects, learning specific aspects\ncan generalize to unseen aspects; on the other hand, jointly learning to assess\nmultiple visual aspects and tasks may foster a synergistic effect. To this end,\nwe propose UFEval, the first unified fine-grained evaluator with task and\naspect generalization for four evaluation tasks -- Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. However, training such a unified evaluator is hindered by the lack\nof a large-scale, multi-modal, and aspect-level resource. To address this gap,\nwe introduce FRABench, a comprehensive fine-grained evaluation dataset.\nSpecifically, (1) We first construct a hierarchical aspect taxonomy\nencompassing 112 distinct aspects across the aforementioned four tasks. (2)\nBased on this taxonomy, we create FRABench, comprising 60.4k pairwise samples\nwith 325k evaluation labels obtained from a combination of human and GPT-4o\nannotations. (3) Finally, leveraging FRABench, we develop UFEval, a unified\nfine-grained evaluator. Experiments show that learning on specific aspects\nenables UFEval to generalize to unseen aspects, and joint learning to assess\ndiverse visual tasks and aspects can lead to substantial mutual benefits.\n", "link": "http://arxiv.org/abs/2505.12795v4", "date": "2025-09-29", "relevancy": 2.1114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRABench%20and%20UFEval%3A%20Unified%20Fine-grained%20Evaluation%20with%20Task%20and%0A%20%20Aspect%20Generalization&body=Title%3A%20FRABench%20and%20UFEval%3A%20Unified%20Fine-grained%20Evaluation%20with%20Task%20and%0A%20%20Aspect%20Generalization%0AAuthor%3A%20Shibo%20Hong%20and%20Jiahao%20Ying%20and%20Haiyuan%20Liang%20and%20Mengdi%20Zhang%20and%20Jun%20Kuang%20and%20Jiazheng%20Zhang%20and%20Yixin%20Cao%0AAbstract%3A%20%20%20Evaluating%20open-ended%20outputs%20of%20Multimodal%20Large%20Language%20Models%20has%20become%0Aa%20bottleneck%20as%20model%20capabilities%2C%20task%20diversity%2C%20and%20modality%20rapidly%0Aexpand.%20Existing%20%60%60MLLM-as-a-Judge%27%27%20evaluators%2C%20though%20promising%2C%20remain%0Aconstrained%20to%20specific%20tasks%20and%20aspects.%20In%20this%20paper%2C%20we%20argue%20that%2C%20on%20one%0Ahand%2C%20based%20on%20the%20interconnected%20nature%20of%20aspects%2C%20learning%20specific%20aspects%0Acan%20generalize%20to%20unseen%20aspects%3B%20on%20the%20other%20hand%2C%20jointly%20learning%20to%20assess%0Amultiple%20visual%20aspects%20and%20tasks%20may%20foster%20a%20synergistic%20effect.%20To%20this%20end%2C%0Awe%20propose%20UFEval%2C%20the%20first%20unified%20fine-grained%20evaluator%20with%20task%20and%0Aaspect%20generalization%20for%20four%20evaluation%20tasks%20--%20Natural%20Language%20Generation%2C%0AImage%20Understanding%2C%20Image%20Generation%2C%20and%20Interleaved%20Text-and-Image%0AGeneration.%20However%2C%20training%20such%20a%20unified%20evaluator%20is%20hindered%20by%20the%20lack%0Aof%20a%20large-scale%2C%20multi-modal%2C%20and%20aspect-level%20resource.%20To%20address%20this%20gap%2C%0Awe%20introduce%20FRABench%2C%20a%20comprehensive%20fine-grained%20evaluation%20dataset.%0ASpecifically%2C%20%281%29%20We%20first%20construct%20a%20hierarchical%20aspect%20taxonomy%0Aencompassing%20112%20distinct%20aspects%20across%20the%20aforementioned%20four%20tasks.%20%282%29%0ABased%20on%20this%20taxonomy%2C%20we%20create%20FRABench%2C%20comprising%2060.4k%20pairwise%20samples%0Awith%20325k%20evaluation%20labels%20obtained%20from%20a%20combination%20of%20human%20and%20GPT-4o%0Aannotations.%20%283%29%20Finally%2C%20leveraging%20FRABench%2C%20we%20develop%20UFEval%2C%20a%20unified%0Afine-grained%20evaluator.%20Experiments%20show%20that%20learning%20on%20specific%20aspects%0Aenables%20UFEval%20to%20generalize%20to%20unseen%20aspects%2C%20and%20joint%20learning%20to%20assess%0Adiverse%20visual%20tasks%20and%20aspects%20can%20lead%20to%20substantial%20mutual%20benefits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12795v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRABench%2520and%2520UFEval%253A%2520Unified%2520Fine-grained%2520Evaluation%2520with%2520Task%2520and%250A%2520%2520Aspect%2520Generalization%26entry.906535625%3DShibo%2520Hong%2520and%2520Jiahao%2520Ying%2520and%2520Haiyuan%2520Liang%2520and%2520Mengdi%2520Zhang%2520and%2520Jun%2520Kuang%2520and%2520Jiazheng%2520Zhang%2520and%2520Yixin%2520Cao%26entry.1292438233%3D%2520%2520Evaluating%2520open-ended%2520outputs%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520has%2520become%250Aa%2520bottleneck%2520as%2520model%2520capabilities%252C%2520task%2520diversity%252C%2520and%2520modality%2520rapidly%250Aexpand.%2520Existing%2520%2560%2560MLLM-as-a-Judge%2527%2527%2520evaluators%252C%2520though%2520promising%252C%2520remain%250Aconstrained%2520to%2520specific%2520tasks%2520and%2520aspects.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%252C%2520on%2520one%250Ahand%252C%2520based%2520on%2520the%2520interconnected%2520nature%2520of%2520aspects%252C%2520learning%2520specific%2520aspects%250Acan%2520generalize%2520to%2520unseen%2520aspects%253B%2520on%2520the%2520other%2520hand%252C%2520jointly%2520learning%2520to%2520assess%250Amultiple%2520visual%2520aspects%2520and%2520tasks%2520may%2520foster%2520a%2520synergistic%2520effect.%2520To%2520this%2520end%252C%250Awe%2520propose%2520UFEval%252C%2520the%2520first%2520unified%2520fine-grained%2520evaluator%2520with%2520task%2520and%250Aaspect%2520generalization%2520for%2520four%2520evaluation%2520tasks%2520--%2520Natural%2520Language%2520Generation%252C%250AImage%2520Understanding%252C%2520Image%2520Generation%252C%2520and%2520Interleaved%2520Text-and-Image%250AGeneration.%2520However%252C%2520training%2520such%2520a%2520unified%2520evaluator%2520is%2520hindered%2520by%2520the%2520lack%250Aof%2520a%2520large-scale%252C%2520multi-modal%252C%2520and%2520aspect-level%2520resource.%2520To%2520address%2520this%2520gap%252C%250Awe%2520introduce%2520FRABench%252C%2520a%2520comprehensive%2520fine-grained%2520evaluation%2520dataset.%250ASpecifically%252C%2520%25281%2529%2520We%2520first%2520construct%2520a%2520hierarchical%2520aspect%2520taxonomy%250Aencompassing%2520112%2520distinct%2520aspects%2520across%2520the%2520aforementioned%2520four%2520tasks.%2520%25282%2529%250ABased%2520on%2520this%2520taxonomy%252C%2520we%2520create%2520FRABench%252C%2520comprising%252060.4k%2520pairwise%2520samples%250Awith%2520325k%2520evaluation%2520labels%2520obtained%2520from%2520a%2520combination%2520of%2520human%2520and%2520GPT-4o%250Aannotations.%2520%25283%2529%2520Finally%252C%2520leveraging%2520FRABench%252C%2520we%2520develop%2520UFEval%252C%2520a%2520unified%250Afine-grained%2520evaluator.%2520Experiments%2520show%2520that%2520learning%2520on%2520specific%2520aspects%250Aenables%2520UFEval%2520to%2520generalize%2520to%2520unseen%2520aspects%252C%2520and%2520joint%2520learning%2520to%2520assess%250Adiverse%2520visual%2520tasks%2520and%2520aspects%2520can%2520lead%2520to%2520substantial%2520mutual%2520benefits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12795v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRABench%20and%20UFEval%3A%20Unified%20Fine-grained%20Evaluation%20with%20Task%20and%0A%20%20Aspect%20Generalization&entry.906535625=Shibo%20Hong%20and%20Jiahao%20Ying%20and%20Haiyuan%20Liang%20and%20Mengdi%20Zhang%20and%20Jun%20Kuang%20and%20Jiazheng%20Zhang%20and%20Yixin%20Cao&entry.1292438233=%20%20Evaluating%20open-ended%20outputs%20of%20Multimodal%20Large%20Language%20Models%20has%20become%0Aa%20bottleneck%20as%20model%20capabilities%2C%20task%20diversity%2C%20and%20modality%20rapidly%0Aexpand.%20Existing%20%60%60MLLM-as-a-Judge%27%27%20evaluators%2C%20though%20promising%2C%20remain%0Aconstrained%20to%20specific%20tasks%20and%20aspects.%20In%20this%20paper%2C%20we%20argue%20that%2C%20on%20one%0Ahand%2C%20based%20on%20the%20interconnected%20nature%20of%20aspects%2C%20learning%20specific%20aspects%0Acan%20generalize%20to%20unseen%20aspects%3B%20on%20the%20other%20hand%2C%20jointly%20learning%20to%20assess%0Amultiple%20visual%20aspects%20and%20tasks%20may%20foster%20a%20synergistic%20effect.%20To%20this%20end%2C%0Awe%20propose%20UFEval%2C%20the%20first%20unified%20fine-grained%20evaluator%20with%20task%20and%0Aaspect%20generalization%20for%20four%20evaluation%20tasks%20--%20Natural%20Language%20Generation%2C%0AImage%20Understanding%2C%20Image%20Generation%2C%20and%20Interleaved%20Text-and-Image%0AGeneration.%20However%2C%20training%20such%20a%20unified%20evaluator%20is%20hindered%20by%20the%20lack%0Aof%20a%20large-scale%2C%20multi-modal%2C%20and%20aspect-level%20resource.%20To%20address%20this%20gap%2C%0Awe%20introduce%20FRABench%2C%20a%20comprehensive%20fine-grained%20evaluation%20dataset.%0ASpecifically%2C%20%281%29%20We%20first%20construct%20a%20hierarchical%20aspect%20taxonomy%0Aencompassing%20112%20distinct%20aspects%20across%20the%20aforementioned%20four%20tasks.%20%282%29%0ABased%20on%20this%20taxonomy%2C%20we%20create%20FRABench%2C%20comprising%2060.4k%20pairwise%20samples%0Awith%20325k%20evaluation%20labels%20obtained%20from%20a%20combination%20of%20human%20and%20GPT-4o%0Aannotations.%20%283%29%20Finally%2C%20leveraging%20FRABench%2C%20we%20develop%20UFEval%2C%20a%20unified%0Afine-grained%20evaluator.%20Experiments%20show%20that%20learning%20on%20specific%20aspects%0Aenables%20UFEval%20to%20generalize%20to%20unseen%20aspects%2C%20and%20joint%20learning%20to%20assess%0Adiverse%20visual%20tasks%20and%20aspects%20can%20lead%20to%20substantial%20mutual%20benefits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12795v4&entry.124074799=Read"},
{"title": "MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality\n  Assessment", "author": "Fankai Jia and Daisong Gan and Zhe Zhang and Zhaochi Wen and Chenchen Dan and Dong Liang and Haifeng Wang", "abstract": "  Magnetic resonance imaging (MRI) quality assessment is crucial for clinical\ndecision-making, yet remains challenging due to data scarcity and protocol\nvariability. Traditional approaches face fundamental trade-offs: signal-based\nmethods like MRIQC provide quantitative metrics but lack semantic\nunderstanding, while deep learning approaches achieve high accuracy but\nsacrifice interpretability. To address these limitations, we introduce the\nMultimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration\nof multimodal large language models (MLLMs) with acquisition-aware signal\nprocessing. MMRQA combines three key innovations: robust metric extraction via\nMRQy augmented with simulated artifacts, structured transformation of metrics\ninto question-answer pairs using Qwen, and parameter-efficient fusion through\nLow-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI,\nand MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with\nstrong zero-shot generalization, as validated by comprehensive ablation\nstudies. By bridging quantitative analysis with semantic reasoning, our\nframework generates clinically interpretable outputs that enhance quality\ncontrol in dynamic medical settings.\n", "link": "http://arxiv.org/abs/2509.24888v1", "date": "2025-09-29", "relevancy": 2.1081, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMRQA%3A%20Signal-Enhanced%20Multimodal%20Large%20Language%20Models%20for%20MRI%20Quality%0A%20%20Assessment&body=Title%3A%20MMRQA%3A%20Signal-Enhanced%20Multimodal%20Large%20Language%20Models%20for%20MRI%20Quality%0A%20%20Assessment%0AAuthor%3A%20Fankai%20Jia%20and%20Daisong%20Gan%20and%20Zhe%20Zhang%20and%20Zhaochi%20Wen%20and%20Chenchen%20Dan%20and%20Dong%20Liang%20and%20Haifeng%20Wang%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20quality%20assessment%20is%20crucial%20for%20clinical%0Adecision-making%2C%20yet%20remains%20challenging%20due%20to%20data%20scarcity%20and%20protocol%0Avariability.%20Traditional%20approaches%20face%20fundamental%20trade-offs%3A%20signal-based%0Amethods%20like%20MRIQC%20provide%20quantitative%20metrics%20but%20lack%20semantic%0Aunderstanding%2C%20while%20deep%20learning%20approaches%20achieve%20high%20accuracy%20but%0Asacrifice%20interpretability.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%0AMultimodal%20MRI%20Quality%20Assessment%20%28MMRQA%29%20framework%2C%20pioneering%20the%20integration%0Aof%20multimodal%20large%20language%20models%20%28MLLMs%29%20with%20acquisition-aware%20signal%0Aprocessing.%20MMRQA%20combines%20three%20key%20innovations%3A%20robust%20metric%20extraction%20via%0AMRQy%20augmented%20with%20simulated%20artifacts%2C%20structured%20transformation%20of%20metrics%0Ainto%20question-answer%20pairs%20using%20Qwen%2C%20and%20parameter-efficient%20fusion%20through%0ALow-Rank%20Adaptation%20%28LoRA%29%20of%20LLaVA-OneVision.%20Evaluated%20on%20MR-ART%2C%20FastMRI%2C%0Aand%20MyConnectome%20benchmarks%2C%20MMRQA%20achieves%20state-of-the-art%20performance%20with%0Astrong%20zero-shot%20generalization%2C%20as%20validated%20by%20comprehensive%20ablation%0Astudies.%20By%20bridging%20quantitative%20analysis%20with%20semantic%20reasoning%2C%20our%0Aframework%20generates%20clinically%20interpretable%20outputs%20that%20enhance%20quality%0Acontrol%20in%20dynamic%20medical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMRQA%253A%2520Signal-Enhanced%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520MRI%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DFankai%2520Jia%2520and%2520Daisong%2520Gan%2520and%2520Zhe%2520Zhang%2520and%2520Zhaochi%2520Wen%2520and%2520Chenchen%2520Dan%2520and%2520Dong%2520Liang%2520and%2520Haifeng%2520Wang%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520quality%2520assessment%2520is%2520crucial%2520for%2520clinical%250Adecision-making%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520data%2520scarcity%2520and%2520protocol%250Avariability.%2520Traditional%2520approaches%2520face%2520fundamental%2520trade-offs%253A%2520signal-based%250Amethods%2520like%2520MRIQC%2520provide%2520quantitative%2520metrics%2520but%2520lack%2520semantic%250Aunderstanding%252C%2520while%2520deep%2520learning%2520approaches%2520achieve%2520high%2520accuracy%2520but%250Asacrifice%2520interpretability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%250AMultimodal%2520MRI%2520Quality%2520Assessment%2520%2528MMRQA%2529%2520framework%252C%2520pioneering%2520the%2520integration%250Aof%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520with%2520acquisition-aware%2520signal%250Aprocessing.%2520MMRQA%2520combines%2520three%2520key%2520innovations%253A%2520robust%2520metric%2520extraction%2520via%250AMRQy%2520augmented%2520with%2520simulated%2520artifacts%252C%2520structured%2520transformation%2520of%2520metrics%250Ainto%2520question-answer%2520pairs%2520using%2520Qwen%252C%2520and%2520parameter-efficient%2520fusion%2520through%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520of%2520LLaVA-OneVision.%2520Evaluated%2520on%2520MR-ART%252C%2520FastMRI%252C%250Aand%2520MyConnectome%2520benchmarks%252C%2520MMRQA%2520achieves%2520state-of-the-art%2520performance%2520with%250Astrong%2520zero-shot%2520generalization%252C%2520as%2520validated%2520by%2520comprehensive%2520ablation%250Astudies.%2520By%2520bridging%2520quantitative%2520analysis%2520with%2520semantic%2520reasoning%252C%2520our%250Aframework%2520generates%2520clinically%2520interpretable%2520outputs%2520that%2520enhance%2520quality%250Acontrol%2520in%2520dynamic%2520medical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMRQA%3A%20Signal-Enhanced%20Multimodal%20Large%20Language%20Models%20for%20MRI%20Quality%0A%20%20Assessment&entry.906535625=Fankai%20Jia%20and%20Daisong%20Gan%20and%20Zhe%20Zhang%20and%20Zhaochi%20Wen%20and%20Chenchen%20Dan%20and%20Dong%20Liang%20and%20Haifeng%20Wang&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20quality%20assessment%20is%20crucial%20for%20clinical%0Adecision-making%2C%20yet%20remains%20challenging%20due%20to%20data%20scarcity%20and%20protocol%0Avariability.%20Traditional%20approaches%20face%20fundamental%20trade-offs%3A%20signal-based%0Amethods%20like%20MRIQC%20provide%20quantitative%20metrics%20but%20lack%20semantic%0Aunderstanding%2C%20while%20deep%20learning%20approaches%20achieve%20high%20accuracy%20but%0Asacrifice%20interpretability.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%0AMultimodal%20MRI%20Quality%20Assessment%20%28MMRQA%29%20framework%2C%20pioneering%20the%20integration%0Aof%20multimodal%20large%20language%20models%20%28MLLMs%29%20with%20acquisition-aware%20signal%0Aprocessing.%20MMRQA%20combines%20three%20key%20innovations%3A%20robust%20metric%20extraction%20via%0AMRQy%20augmented%20with%20simulated%20artifacts%2C%20structured%20transformation%20of%20metrics%0Ainto%20question-answer%20pairs%20using%20Qwen%2C%20and%20parameter-efficient%20fusion%20through%0ALow-Rank%20Adaptation%20%28LoRA%29%20of%20LLaVA-OneVision.%20Evaluated%20on%20MR-ART%2C%20FastMRI%2C%0Aand%20MyConnectome%20benchmarks%2C%20MMRQA%20achieves%20state-of-the-art%20performance%20with%0Astrong%20zero-shot%20generalization%2C%20as%20validated%20by%20comprehensive%20ablation%0Astudies.%20By%20bridging%20quantitative%20analysis%20with%20semantic%20reasoning%2C%20our%0Aframework%20generates%20clinically%20interpretable%20outputs%20that%20enhance%20quality%0Acontrol%20in%20dynamic%20medical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24888v1&entry.124074799=Read"},
{"title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events", "author": "Richeek Das and Kostas Daniilidis and Pratik Chaudhari", "abstract": "  This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.\n", "link": "http://arxiv.org/abs/2509.25146v1", "date": "2025-09-29", "relevancy": 2.1029, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5292}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5235}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Feature%20Field%20%28%24%5Ctext%7BF%7D%5E3%24%29%3A%20A%20Predictive%20Representation%20of%20Events&body=Title%3A%20Fast%20Feature%20Field%20%28%24%5Ctext%7BF%7D%5E3%24%29%3A%20A%20Predictive%20Representation%20of%20Events%0AAuthor%3A%20Richeek%20Das%20and%20Kostas%20Daniilidis%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20This%20paper%20develops%20a%20mathematical%20argument%20and%20algorithms%20for%20building%0Arepresentations%20of%20data%20from%20event-based%20cameras%2C%20that%20we%20call%20Fast%20Feature%0AField%20%28%24%5Ctext%7BF%7D%5E3%24%29.%20We%20learn%20this%20representation%20by%20predicting%20future%20events%0Afrom%20past%20events%20and%20show%20that%20it%20preserves%20scene%20structure%20and%20motion%0Ainformation.%20%24%5Ctext%7BF%7D%5E3%24%20exploits%20the%20sparsity%20of%20event%20data%20and%20is%20robust%20to%0Anoise%20and%20variations%20in%20event%20rates.%20It%20can%20be%20computed%20efficiently%20using%20ideas%0Afrom%20multi-resolution%20hash%20encoding%20and%20deep%20sets%20-%20achieving%20120%20Hz%20at%20HD%20and%0A440%20Hz%20at%20VGA%20resolutions.%20%24%5Ctext%7BF%7D%5E3%24%20represents%20events%20within%20a%20contiguous%0Aspatiotemporal%20volume%20as%20a%20multi-channel%20image%2C%20enabling%20a%20range%20of%20downstream%0Atasks.%20We%20obtain%20state-of-the-art%20performance%20on%20optical%20flow%20estimation%2C%0Asemantic%20segmentation%2C%20and%20monocular%20metric%20depth%20estimation%2C%20on%20data%20from%0Athree%20robotic%20platforms%20%28a%20car%2C%20a%20quadruped%20robot%20and%20a%20flying%20platform%29%2C%0Aacross%20different%20lighting%20conditions%20%28daytime%2C%20nighttime%29%2C%20environments%0A%28indoors%2C%20outdoors%2C%20urban%2C%20as%20well%20as%20off-road%29%20and%20dynamic%20vision%20sensors%0A%28resolutions%20and%20event%20rates%29.%20Our%20implementations%20can%20predict%20these%20tasks%20at%0A25-75%20Hz%20at%20HD%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Feature%2520Field%2520%2528%2524%255Ctext%257BF%257D%255E3%2524%2529%253A%2520A%2520Predictive%2520Representation%2520of%2520Events%26entry.906535625%3DRicheek%2520Das%2520and%2520Kostas%2520Daniilidis%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3D%2520%2520This%2520paper%2520develops%2520a%2520mathematical%2520argument%2520and%2520algorithms%2520for%2520building%250Arepresentations%2520of%2520data%2520from%2520event-based%2520cameras%252C%2520that%2520we%2520call%2520Fast%2520Feature%250AField%2520%2528%2524%255Ctext%257BF%257D%255E3%2524%2529.%2520We%2520learn%2520this%2520representation%2520by%2520predicting%2520future%2520events%250Afrom%2520past%2520events%2520and%2520show%2520that%2520it%2520preserves%2520scene%2520structure%2520and%2520motion%250Ainformation.%2520%2524%255Ctext%257BF%257D%255E3%2524%2520exploits%2520the%2520sparsity%2520of%2520event%2520data%2520and%2520is%2520robust%2520to%250Anoise%2520and%2520variations%2520in%2520event%2520rates.%2520It%2520can%2520be%2520computed%2520efficiently%2520using%2520ideas%250Afrom%2520multi-resolution%2520hash%2520encoding%2520and%2520deep%2520sets%2520-%2520achieving%2520120%2520Hz%2520at%2520HD%2520and%250A440%2520Hz%2520at%2520VGA%2520resolutions.%2520%2524%255Ctext%257BF%257D%255E3%2524%2520represents%2520events%2520within%2520a%2520contiguous%250Aspatiotemporal%2520volume%2520as%2520a%2520multi-channel%2520image%252C%2520enabling%2520a%2520range%2520of%2520downstream%250Atasks.%2520We%2520obtain%2520state-of-the-art%2520performance%2520on%2520optical%2520flow%2520estimation%252C%250Asemantic%2520segmentation%252C%2520and%2520monocular%2520metric%2520depth%2520estimation%252C%2520on%2520data%2520from%250Athree%2520robotic%2520platforms%2520%2528a%2520car%252C%2520a%2520quadruped%2520robot%2520and%2520a%2520flying%2520platform%2529%252C%250Aacross%2520different%2520lighting%2520conditions%2520%2528daytime%252C%2520nighttime%2529%252C%2520environments%250A%2528indoors%252C%2520outdoors%252C%2520urban%252C%2520as%2520well%2520as%2520off-road%2529%2520and%2520dynamic%2520vision%2520sensors%250A%2528resolutions%2520and%2520event%2520rates%2529.%2520Our%2520implementations%2520can%2520predict%2520these%2520tasks%2520at%250A25-75%2520Hz%2520at%2520HD%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Feature%20Field%20%28%24%5Ctext%7BF%7D%5E3%24%29%3A%20A%20Predictive%20Representation%20of%20Events&entry.906535625=Richeek%20Das%20and%20Kostas%20Daniilidis%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20This%20paper%20develops%20a%20mathematical%20argument%20and%20algorithms%20for%20building%0Arepresentations%20of%20data%20from%20event-based%20cameras%2C%20that%20we%20call%20Fast%20Feature%0AField%20%28%24%5Ctext%7BF%7D%5E3%24%29.%20We%20learn%20this%20representation%20by%20predicting%20future%20events%0Afrom%20past%20events%20and%20show%20that%20it%20preserves%20scene%20structure%20and%20motion%0Ainformation.%20%24%5Ctext%7BF%7D%5E3%24%20exploits%20the%20sparsity%20of%20event%20data%20and%20is%20robust%20to%0Anoise%20and%20variations%20in%20event%20rates.%20It%20can%20be%20computed%20efficiently%20using%20ideas%0Afrom%20multi-resolution%20hash%20encoding%20and%20deep%20sets%20-%20achieving%20120%20Hz%20at%20HD%20and%0A440%20Hz%20at%20VGA%20resolutions.%20%24%5Ctext%7BF%7D%5E3%24%20represents%20events%20within%20a%20contiguous%0Aspatiotemporal%20volume%20as%20a%20multi-channel%20image%2C%20enabling%20a%20range%20of%20downstream%0Atasks.%20We%20obtain%20state-of-the-art%20performance%20on%20optical%20flow%20estimation%2C%0Asemantic%20segmentation%2C%20and%20monocular%20metric%20depth%20estimation%2C%20on%20data%20from%0Athree%20robotic%20platforms%20%28a%20car%2C%20a%20quadruped%20robot%20and%20a%20flying%20platform%29%2C%0Aacross%20different%20lighting%20conditions%20%28daytime%2C%20nighttime%29%2C%20environments%0A%28indoors%2C%20outdoors%2C%20urban%2C%20as%20well%20as%20off-road%29%20and%20dynamic%20vision%20sensors%0A%28resolutions%20and%20event%20rates%29.%20Our%20implementations%20can%20predict%20these%20tasks%20at%0A25-75%20Hz%20at%20HD%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25146v1&entry.124074799=Read"},
{"title": "Of-SemWat: High-payload text embedding for semantic watermarking of\n  AI-generated images with arbitrary size", "author": "Benedetta Tondi and Andrea Costanzo and Mauro Barni", "abstract": "  We propose a high-payload image watermarking method for textual embedding,\nwhere a semantic description of the image - which may also correspond to the\ninput text prompt-, is embedded inside the image. In order to be able to\nrobustly embed high payloads in large-scale images - such as those produced by\nmodern AI generators - the proposed approach builds upon a traditional\nwatermarking scheme that exploits orthogonal and turbo codes for improved\nrobustness, and integrates frequency-domain embedding and perceptual masking\ntechniques to enhance watermark imperceptibility. Experiments show that the\nproposed method is extremely robust against a wide variety of image processing,\nand the embedded text can be retrieved also after traditional and AI\ninpainting, permitting to unveil the semantic modification the image has\nundergone via image-text mismatch analysis.\n", "link": "http://arxiv.org/abs/2509.24823v1", "date": "2025-09-29", "relevancy": 2.1004, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5358}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5199}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Of-SemWat%3A%20High-payload%20text%20embedding%20for%20semantic%20watermarking%20of%0A%20%20AI-generated%20images%20with%20arbitrary%20size&body=Title%3A%20Of-SemWat%3A%20High-payload%20text%20embedding%20for%20semantic%20watermarking%20of%0A%20%20AI-generated%20images%20with%20arbitrary%20size%0AAuthor%3A%20Benedetta%20Tondi%20and%20Andrea%20Costanzo%20and%20Mauro%20Barni%0AAbstract%3A%20%20%20We%20propose%20a%20high-payload%20image%20watermarking%20method%20for%20textual%20embedding%2C%0Awhere%20a%20semantic%20description%20of%20the%20image%20-%20which%20may%20also%20correspond%20to%20the%0Ainput%20text%20prompt-%2C%20is%20embedded%20inside%20the%20image.%20In%20order%20to%20be%20able%20to%0Arobustly%20embed%20high%20payloads%20in%20large-scale%20images%20-%20such%20as%20those%20produced%20by%0Amodern%20AI%20generators%20-%20the%20proposed%20approach%20builds%20upon%20a%20traditional%0Awatermarking%20scheme%20that%20exploits%20orthogonal%20and%20turbo%20codes%20for%20improved%0Arobustness%2C%20and%20integrates%20frequency-domain%20embedding%20and%20perceptual%20masking%0Atechniques%20to%20enhance%20watermark%20imperceptibility.%20Experiments%20show%20that%20the%0Aproposed%20method%20is%20extremely%20robust%20against%20a%20wide%20variety%20of%20image%20processing%2C%0Aand%20the%20embedded%20text%20can%20be%20retrieved%20also%20after%20traditional%20and%20AI%0Ainpainting%2C%20permitting%20to%20unveil%20the%20semantic%20modification%20the%20image%20has%0Aundergone%20via%20image-text%20mismatch%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOf-SemWat%253A%2520High-payload%2520text%2520embedding%2520for%2520semantic%2520watermarking%2520of%250A%2520%2520AI-generated%2520images%2520with%2520arbitrary%2520size%26entry.906535625%3DBenedetta%2520Tondi%2520and%2520Andrea%2520Costanzo%2520and%2520Mauro%2520Barni%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520high-payload%2520image%2520watermarking%2520method%2520for%2520textual%2520embedding%252C%250Awhere%2520a%2520semantic%2520description%2520of%2520the%2520image%2520-%2520which%2520may%2520also%2520correspond%2520to%2520the%250Ainput%2520text%2520prompt-%252C%2520is%2520embedded%2520inside%2520the%2520image.%2520In%2520order%2520to%2520be%2520able%2520to%250Arobustly%2520embed%2520high%2520payloads%2520in%2520large-scale%2520images%2520-%2520such%2520as%2520those%2520produced%2520by%250Amodern%2520AI%2520generators%2520-%2520the%2520proposed%2520approach%2520builds%2520upon%2520a%2520traditional%250Awatermarking%2520scheme%2520that%2520exploits%2520orthogonal%2520and%2520turbo%2520codes%2520for%2520improved%250Arobustness%252C%2520and%2520integrates%2520frequency-domain%2520embedding%2520and%2520perceptual%2520masking%250Atechniques%2520to%2520enhance%2520watermark%2520imperceptibility.%2520Experiments%2520show%2520that%2520the%250Aproposed%2520method%2520is%2520extremely%2520robust%2520against%2520a%2520wide%2520variety%2520of%2520image%2520processing%252C%250Aand%2520the%2520embedded%2520text%2520can%2520be%2520retrieved%2520also%2520after%2520traditional%2520and%2520AI%250Ainpainting%252C%2520permitting%2520to%2520unveil%2520the%2520semantic%2520modification%2520the%2520image%2520has%250Aundergone%2520via%2520image-text%2520mismatch%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Of-SemWat%3A%20High-payload%20text%20embedding%20for%20semantic%20watermarking%20of%0A%20%20AI-generated%20images%20with%20arbitrary%20size&entry.906535625=Benedetta%20Tondi%20and%20Andrea%20Costanzo%20and%20Mauro%20Barni&entry.1292438233=%20%20We%20propose%20a%20high-payload%20image%20watermarking%20method%20for%20textual%20embedding%2C%0Awhere%20a%20semantic%20description%20of%20the%20image%20-%20which%20may%20also%20correspond%20to%20the%0Ainput%20text%20prompt-%2C%20is%20embedded%20inside%20the%20image.%20In%20order%20to%20be%20able%20to%0Arobustly%20embed%20high%20payloads%20in%20large-scale%20images%20-%20such%20as%20those%20produced%20by%0Amodern%20AI%20generators%20-%20the%20proposed%20approach%20builds%20upon%20a%20traditional%0Awatermarking%20scheme%20that%20exploits%20orthogonal%20and%20turbo%20codes%20for%20improved%0Arobustness%2C%20and%20integrates%20frequency-domain%20embedding%20and%20perceptual%20masking%0Atechniques%20to%20enhance%20watermark%20imperceptibility.%20Experiments%20show%20that%20the%0Aproposed%20method%20is%20extremely%20robust%20against%20a%20wide%20variety%20of%20image%20processing%2C%0Aand%20the%20embedded%20text%20can%20be%20retrieved%20also%20after%20traditional%20and%20AI%0Ainpainting%2C%20permitting%20to%20unveil%20the%20semantic%20modification%20the%20image%20has%0Aundergone%20via%20image-text%20mismatch%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24823v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting", "author": "Spyros Kondylatos and Gustau Camps-Valls and Ioannis Papoutsis", "abstract": "  Wildfires are among the most severe natural hazards, posing a significant\nthreat to both humans and natural ecosystems. The growing risk of wildfires\nincreases the demand for forecasting models that are not only accurate but also\nreliable. Deep Learning (DL) has shown promise in predicting wildfire danger;\nhowever, its adoption is hindered by concerns over the reliability of its\npredictions, some of which stem from the lack of uncertainty quantification. To\naddress this challenge, we present an uncertainty-aware DL framework that\njointly captures epistemic (model) and aleatoric (data) uncertainty to enhance\nshort-term wildfire danger forecasting. In the next-day forecasting, our\nbest-performing model improves the F1 Score by 2.3% and reduces the Expected\nCalibration Error by 2.1% compared to a deterministic baseline, enhancing both\npredictive skill and calibration. Our experiments confirm the reliability of\nthe uncertainty estimates and illustrate their practical utility for decision\nsupport, including the identification of uncertainty thresholds for rejecting\nlow-confidence predictions and the generation of well-calibrated wildfire\ndanger maps with accompanying uncertainty layers. Extending the forecast\nhorizon up to ten days, we observe that aleatoric uncertainty increases with\ntime, showing greater variability in environmental conditions, while epistemic\nuncertainty remains stable. Finally, we show that although the two uncertainty\ntypes may be redundant in low-uncertainty cases, they provide complementary\ninsights under more challenging conditions, underscoring the value of their\njoint modeling for robust wildfire danger prediction. In summary, our approach\nsignificantly improves the accuracy and reliability of wildfire danger\nforecasting, advancing the development of trustworthy wildfire DL systems.\n", "link": "http://arxiv.org/abs/2509.25017v1", "date": "2025-09-29", "relevancy": 2.0993, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5751}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Deep%20Learning%20for%20Wildfire%20Danger%20Forecasting&body=Title%3A%20Uncertainty-Aware%20Deep%20Learning%20for%20Wildfire%20Danger%20Forecasting%0AAuthor%3A%20Spyros%20Kondylatos%20and%20Gustau%20Camps-Valls%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20%20%20Wildfires%20are%20among%20the%20most%20severe%20natural%20hazards%2C%20posing%20a%20significant%0Athreat%20to%20both%20humans%20and%20natural%20ecosystems.%20The%20growing%20risk%20of%20wildfires%0Aincreases%20the%20demand%20for%20forecasting%20models%20that%20are%20not%20only%20accurate%20but%20also%0Areliable.%20Deep%20Learning%20%28DL%29%20has%20shown%20promise%20in%20predicting%20wildfire%20danger%3B%0Ahowever%2C%20its%20adoption%20is%20hindered%20by%20concerns%20over%20the%20reliability%20of%20its%0Apredictions%2C%20some%20of%20which%20stem%20from%20the%20lack%20of%20uncertainty%20quantification.%20To%0Aaddress%20this%20challenge%2C%20we%20present%20an%20uncertainty-aware%20DL%20framework%20that%0Ajointly%20captures%20epistemic%20%28model%29%20and%20aleatoric%20%28data%29%20uncertainty%20to%20enhance%0Ashort-term%20wildfire%20danger%20forecasting.%20In%20the%20next-day%20forecasting%2C%20our%0Abest-performing%20model%20improves%20the%20F1%20Score%20by%202.3%25%20and%20reduces%20the%20Expected%0ACalibration%20Error%20by%202.1%25%20compared%20to%20a%20deterministic%20baseline%2C%20enhancing%20both%0Apredictive%20skill%20and%20calibration.%20Our%20experiments%20confirm%20the%20reliability%20of%0Athe%20uncertainty%20estimates%20and%20illustrate%20their%20practical%20utility%20for%20decision%0Asupport%2C%20including%20the%20identification%20of%20uncertainty%20thresholds%20for%20rejecting%0Alow-confidence%20predictions%20and%20the%20generation%20of%20well-calibrated%20wildfire%0Adanger%20maps%20with%20accompanying%20uncertainty%20layers.%20Extending%20the%20forecast%0Ahorizon%20up%20to%20ten%20days%2C%20we%20observe%20that%20aleatoric%20uncertainty%20increases%20with%0Atime%2C%20showing%20greater%20variability%20in%20environmental%20conditions%2C%20while%20epistemic%0Auncertainty%20remains%20stable.%20Finally%2C%20we%20show%20that%20although%20the%20two%20uncertainty%0Atypes%20may%20be%20redundant%20in%20low-uncertainty%20cases%2C%20they%20provide%20complementary%0Ainsights%20under%20more%20challenging%20conditions%2C%20underscoring%20the%20value%20of%20their%0Ajoint%20modeling%20for%20robust%20wildfire%20danger%20prediction.%20In%20summary%2C%20our%20approach%0Asignificantly%20improves%20the%20accuracy%20and%20reliability%20of%20wildfire%20danger%0Aforecasting%2C%20advancing%20the%20development%20of%20trustworthy%20wildfire%20DL%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Deep%2520Learning%2520for%2520Wildfire%2520Danger%2520Forecasting%26entry.906535625%3DSpyros%2520Kondylatos%2520and%2520Gustau%2520Camps-Valls%2520and%2520Ioannis%2520Papoutsis%26entry.1292438233%3D%2520%2520Wildfires%2520are%2520among%2520the%2520most%2520severe%2520natural%2520hazards%252C%2520posing%2520a%2520significant%250Athreat%2520to%2520both%2520humans%2520and%2520natural%2520ecosystems.%2520The%2520growing%2520risk%2520of%2520wildfires%250Aincreases%2520the%2520demand%2520for%2520forecasting%2520models%2520that%2520are%2520not%2520only%2520accurate%2520but%2520also%250Areliable.%2520Deep%2520Learning%2520%2528DL%2529%2520has%2520shown%2520promise%2520in%2520predicting%2520wildfire%2520danger%253B%250Ahowever%252C%2520its%2520adoption%2520is%2520hindered%2520by%2520concerns%2520over%2520the%2520reliability%2520of%2520its%250Apredictions%252C%2520some%2520of%2520which%2520stem%2520from%2520the%2520lack%2520of%2520uncertainty%2520quantification.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520present%2520an%2520uncertainty-aware%2520DL%2520framework%2520that%250Ajointly%2520captures%2520epistemic%2520%2528model%2529%2520and%2520aleatoric%2520%2528data%2529%2520uncertainty%2520to%2520enhance%250Ashort-term%2520wildfire%2520danger%2520forecasting.%2520In%2520the%2520next-day%2520forecasting%252C%2520our%250Abest-performing%2520model%2520improves%2520the%2520F1%2520Score%2520by%25202.3%2525%2520and%2520reduces%2520the%2520Expected%250ACalibration%2520Error%2520by%25202.1%2525%2520compared%2520to%2520a%2520deterministic%2520baseline%252C%2520enhancing%2520both%250Apredictive%2520skill%2520and%2520calibration.%2520Our%2520experiments%2520confirm%2520the%2520reliability%2520of%250Athe%2520uncertainty%2520estimates%2520and%2520illustrate%2520their%2520practical%2520utility%2520for%2520decision%250Asupport%252C%2520including%2520the%2520identification%2520of%2520uncertainty%2520thresholds%2520for%2520rejecting%250Alow-confidence%2520predictions%2520and%2520the%2520generation%2520of%2520well-calibrated%2520wildfire%250Adanger%2520maps%2520with%2520accompanying%2520uncertainty%2520layers.%2520Extending%2520the%2520forecast%250Ahorizon%2520up%2520to%2520ten%2520days%252C%2520we%2520observe%2520that%2520aleatoric%2520uncertainty%2520increases%2520with%250Atime%252C%2520showing%2520greater%2520variability%2520in%2520environmental%2520conditions%252C%2520while%2520epistemic%250Auncertainty%2520remains%2520stable.%2520Finally%252C%2520we%2520show%2520that%2520although%2520the%2520two%2520uncertainty%250Atypes%2520may%2520be%2520redundant%2520in%2520low-uncertainty%2520cases%252C%2520they%2520provide%2520complementary%250Ainsights%2520under%2520more%2520challenging%2520conditions%252C%2520underscoring%2520the%2520value%2520of%2520their%250Ajoint%2520modeling%2520for%2520robust%2520wildfire%2520danger%2520prediction.%2520In%2520summary%252C%2520our%2520approach%250Asignificantly%2520improves%2520the%2520accuracy%2520and%2520reliability%2520of%2520wildfire%2520danger%250Aforecasting%252C%2520advancing%2520the%2520development%2520of%2520trustworthy%2520wildfire%2520DL%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Deep%20Learning%20for%20Wildfire%20Danger%20Forecasting&entry.906535625=Spyros%20Kondylatos%20and%20Gustau%20Camps-Valls%20and%20Ioannis%20Papoutsis&entry.1292438233=%20%20Wildfires%20are%20among%20the%20most%20severe%20natural%20hazards%2C%20posing%20a%20significant%0Athreat%20to%20both%20humans%20and%20natural%20ecosystems.%20The%20growing%20risk%20of%20wildfires%0Aincreases%20the%20demand%20for%20forecasting%20models%20that%20are%20not%20only%20accurate%20but%20also%0Areliable.%20Deep%20Learning%20%28DL%29%20has%20shown%20promise%20in%20predicting%20wildfire%20danger%3B%0Ahowever%2C%20its%20adoption%20is%20hindered%20by%20concerns%20over%20the%20reliability%20of%20its%0Apredictions%2C%20some%20of%20which%20stem%20from%20the%20lack%20of%20uncertainty%20quantification.%20To%0Aaddress%20this%20challenge%2C%20we%20present%20an%20uncertainty-aware%20DL%20framework%20that%0Ajointly%20captures%20epistemic%20%28model%29%20and%20aleatoric%20%28data%29%20uncertainty%20to%20enhance%0Ashort-term%20wildfire%20danger%20forecasting.%20In%20the%20next-day%20forecasting%2C%20our%0Abest-performing%20model%20improves%20the%20F1%20Score%20by%202.3%25%20and%20reduces%20the%20Expected%0ACalibration%20Error%20by%202.1%25%20compared%20to%20a%20deterministic%20baseline%2C%20enhancing%20both%0Apredictive%20skill%20and%20calibration.%20Our%20experiments%20confirm%20the%20reliability%20of%0Athe%20uncertainty%20estimates%20and%20illustrate%20their%20practical%20utility%20for%20decision%0Asupport%2C%20including%20the%20identification%20of%20uncertainty%20thresholds%20for%20rejecting%0Alow-confidence%20predictions%20and%20the%20generation%20of%20well-calibrated%20wildfire%0Adanger%20maps%20with%20accompanying%20uncertainty%20layers.%20Extending%20the%20forecast%0Ahorizon%20up%20to%20ten%20days%2C%20we%20observe%20that%20aleatoric%20uncertainty%20increases%20with%0Atime%2C%20showing%20greater%20variability%20in%20environmental%20conditions%2C%20while%20epistemic%0Auncertainty%20remains%20stable.%20Finally%2C%20we%20show%20that%20although%20the%20two%20uncertainty%0Atypes%20may%20be%20redundant%20in%20low-uncertainty%20cases%2C%20they%20provide%20complementary%0Ainsights%20under%20more%20challenging%20conditions%2C%20underscoring%20the%20value%20of%20their%0Ajoint%20modeling%20for%20robust%20wildfire%20danger%20prediction.%20In%20summary%2C%20our%20approach%0Asignificantly%20improves%20the%20accuracy%20and%20reliability%20of%20wildfire%20danger%0Aforecasting%2C%20advancing%20the%20development%20of%20trustworthy%20wildfire%20DL%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25017v1&entry.124074799=Read"},
{"title": "Hierarchical Error Correction for Large Language Models: A Systematic\n  Framework for Domain-Specific AI Quality Enhancement", "author": "Zhilong Zhao and Yindi Liu", "abstract": "  Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment.\n", "link": "http://arxiv.org/abs/2509.24841v1", "date": "2025-09-29", "relevancy": 2.0887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5255}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Error%20Correction%20for%20Large%20Language%20Models%3A%20A%20Systematic%0A%20%20Framework%20for%20Domain-Specific%20AI%20Quality%20Enhancement&body=Title%3A%20Hierarchical%20Error%20Correction%20for%20Large%20Language%20Models%3A%20A%20Systematic%0A%20%20Framework%20for%20Domain-Specific%20AI%20Quality%20Enhancement%0AAuthor%3A%20Zhilong%20Zhao%20and%20Yindi%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20face%20significant%20performance%20challenges%20in%20specialized%0Adomains%2C%20with%20state-of-the-art%20models%20achieving%20only%2045.9%25%20accuracy%20on%20medical%0Acoding%20tasks.%20This%20study%20proposes%20a%20Hierarchical%20Error%20Correction%20%28HEC%29%0Aframework%20that%20addresses%20domain-specific%20AI%20limitations%20through%20systematic%0Aerror%20analysis%20and%20targeted%20intervention%20strategies.%0A%20%20We%20analyze%20error%20patterns%20across%20four%20specialized%20domains%20and%20find%20that%20AI%0Aerrors%20follow%20consistent%20hierarchical%20structures%3A%20Knowledge-layer%20errors%0A%2858.4%25%29%2C%20Reasoning-layer%20errors%20%2839.6%25%29%2C%20and%20Complexity-layer%20errors%20%282.0%25%29.%0ABased%20on%20these%20patterns%2C%20we%20develop%20a%20three-stage%20correction%20framework%20that%0Aaddresses%20errors%20according%20to%20their%20hierarchical%20importance%20and%20demonstrates%0Athat%20framework%20effectiveness%20correlates%20inversely%20with%20baseline%20task%0Aperformance.%0A%20%20Experimental%20validation%20across%20medical%20transcription%20%284%2C921%20cases%29%2C%20legal%0Adocument%20classification%20%281%2C000%20cases%29%2C%20political%20bias%20detection%20%28645%20cases%29%2C%0Aand%20legal%20reasoning%20%281%2C000%20cases%29%20shows%20consistent%20improvements.%20Cross-model%0Avalidation%20across%20five%20LLM%20architectures%20demonstrates%20average%20improvements%20of%0A11.2%20percentage%20points%20%28p%20%3C%200.001%29.%20However%2C%20analysis%20reveals%20framework%0Alimitations%20in%20high-baseline%20tasks%20%28%3E75%25%20accuracy%29%2C%20where%20hierarchical%0Aintervention%20may%20interfere%20with%20effective%20reasoning%20processes.%0A%20%20The%20results%20suggest%20that%20systematic%20error%20analysis%20can%20guide%20effective%20AI%0Aenhancement%20strategies%20in%20specialized%20domains%2C%20particularly%20for%0Amoderate-baseline%20tasks%2C%20while%20highlighting%20the%20importance%20of%20understanding%0Aframework%20boundaries%20for%20optimal%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Error%2520Correction%2520for%2520Large%2520Language%2520Models%253A%2520A%2520Systematic%250A%2520%2520Framework%2520for%2520Domain-Specific%2520AI%2520Quality%2520Enhancement%26entry.906535625%3DZhilong%2520Zhao%2520and%2520Yindi%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520face%2520significant%2520performance%2520challenges%2520in%2520specialized%250Adomains%252C%2520with%2520state-of-the-art%2520models%2520achieving%2520only%252045.9%2525%2520accuracy%2520on%2520medical%250Acoding%2520tasks.%2520This%2520study%2520proposes%2520a%2520Hierarchical%2520Error%2520Correction%2520%2528HEC%2529%250Aframework%2520that%2520addresses%2520domain-specific%2520AI%2520limitations%2520through%2520systematic%250Aerror%2520analysis%2520and%2520targeted%2520intervention%2520strategies.%250A%2520%2520We%2520analyze%2520error%2520patterns%2520across%2520four%2520specialized%2520domains%2520and%2520find%2520that%2520AI%250Aerrors%2520follow%2520consistent%2520hierarchical%2520structures%253A%2520Knowledge-layer%2520errors%250A%252858.4%2525%2529%252C%2520Reasoning-layer%2520errors%2520%252839.6%2525%2529%252C%2520and%2520Complexity-layer%2520errors%2520%25282.0%2525%2529.%250ABased%2520on%2520these%2520patterns%252C%2520we%2520develop%2520a%2520three-stage%2520correction%2520framework%2520that%250Aaddresses%2520errors%2520according%2520to%2520their%2520hierarchical%2520importance%2520and%2520demonstrates%250Athat%2520framework%2520effectiveness%2520correlates%2520inversely%2520with%2520baseline%2520task%250Aperformance.%250A%2520%2520Experimental%2520validation%2520across%2520medical%2520transcription%2520%25284%252C921%2520cases%2529%252C%2520legal%250Adocument%2520classification%2520%25281%252C000%2520cases%2529%252C%2520political%2520bias%2520detection%2520%2528645%2520cases%2529%252C%250Aand%2520legal%2520reasoning%2520%25281%252C000%2520cases%2529%2520shows%2520consistent%2520improvements.%2520Cross-model%250Avalidation%2520across%2520five%2520LLM%2520architectures%2520demonstrates%2520average%2520improvements%2520of%250A11.2%2520percentage%2520points%2520%2528p%2520%253C%25200.001%2529.%2520However%252C%2520analysis%2520reveals%2520framework%250Alimitations%2520in%2520high-baseline%2520tasks%2520%2528%253E75%2525%2520accuracy%2529%252C%2520where%2520hierarchical%250Aintervention%2520may%2520interfere%2520with%2520effective%2520reasoning%2520processes.%250A%2520%2520The%2520results%2520suggest%2520that%2520systematic%2520error%2520analysis%2520can%2520guide%2520effective%2520AI%250Aenhancement%2520strategies%2520in%2520specialized%2520domains%252C%2520particularly%2520for%250Amoderate-baseline%2520tasks%252C%2520while%2520highlighting%2520the%2520importance%2520of%2520understanding%250Aframework%2520boundaries%2520for%2520optimal%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Error%20Correction%20for%20Large%20Language%20Models%3A%20A%20Systematic%0A%20%20Framework%20for%20Domain-Specific%20AI%20Quality%20Enhancement&entry.906535625=Zhilong%20Zhao%20and%20Yindi%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20face%20significant%20performance%20challenges%20in%20specialized%0Adomains%2C%20with%20state-of-the-art%20models%20achieving%20only%2045.9%25%20accuracy%20on%20medical%0Acoding%20tasks.%20This%20study%20proposes%20a%20Hierarchical%20Error%20Correction%20%28HEC%29%0Aframework%20that%20addresses%20domain-specific%20AI%20limitations%20through%20systematic%0Aerror%20analysis%20and%20targeted%20intervention%20strategies.%0A%20%20We%20analyze%20error%20patterns%20across%20four%20specialized%20domains%20and%20find%20that%20AI%0Aerrors%20follow%20consistent%20hierarchical%20structures%3A%20Knowledge-layer%20errors%0A%2858.4%25%29%2C%20Reasoning-layer%20errors%20%2839.6%25%29%2C%20and%20Complexity-layer%20errors%20%282.0%25%29.%0ABased%20on%20these%20patterns%2C%20we%20develop%20a%20three-stage%20correction%20framework%20that%0Aaddresses%20errors%20according%20to%20their%20hierarchical%20importance%20and%20demonstrates%0Athat%20framework%20effectiveness%20correlates%20inversely%20with%20baseline%20task%0Aperformance.%0A%20%20Experimental%20validation%20across%20medical%20transcription%20%284%2C921%20cases%29%2C%20legal%0Adocument%20classification%20%281%2C000%20cases%29%2C%20political%20bias%20detection%20%28645%20cases%29%2C%0Aand%20legal%20reasoning%20%281%2C000%20cases%29%20shows%20consistent%20improvements.%20Cross-model%0Avalidation%20across%20five%20LLM%20architectures%20demonstrates%20average%20improvements%20of%0A11.2%20percentage%20points%20%28p%20%3C%200.001%29.%20However%2C%20analysis%20reveals%20framework%0Alimitations%20in%20high-baseline%20tasks%20%28%3E75%25%20accuracy%29%2C%20where%20hierarchical%0Aintervention%20may%20interfere%20with%20effective%20reasoning%20processes.%0A%20%20The%20results%20suggest%20that%20systematic%20error%20analysis%20can%20guide%20effective%20AI%0Aenhancement%20strategies%20in%20specialized%20domains%2C%20particularly%20for%0Amoderate-baseline%20tasks%2C%20while%20highlighting%20the%20importance%20of%20understanding%0Aframework%20boundaries%20for%20optimal%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24841v1&entry.124074799=Read"},
{"title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data\n  Reasoning Intensity", "author": "Zhen Bi and Zhenlin Hu and Jinnan Yang and Mingyang Chen and Cheng Deng and Yida Xue and Zeyu Yang and Qing Shen and Zhenfang Liu and Kang Zhao and Ningyu Zhang and Jungang Lou", "abstract": "  Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data.Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential.\n", "link": "http://arxiv.org/abs/2509.24836v1", "date": "2025-09-29", "relevancy": 2.086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20LLMs%20to%20Their%20Logical%20Reasoning%20Bound%3A%20The%20Role%20of%20Data%0A%20%20Reasoning%20Intensity&body=Title%3A%20Pushing%20LLMs%20to%20Their%20Logical%20Reasoning%20Bound%3A%20The%20Role%20of%20Data%0A%20%20Reasoning%20Intensity%0AAuthor%3A%20Zhen%20Bi%20and%20Zhenlin%20Hu%20and%20Jinnan%20Yang%20and%20Mingyang%20Chen%20and%20Cheng%20Deng%20and%20Yida%20Xue%20and%20Zeyu%20Yang%20and%20Qing%20Shen%20and%20Zhenfang%20Liu%20and%20Kang%20Zhao%20and%20Ningyu%20Zhang%20and%20Jungang%20Lou%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20highlight%20the%20importance%20of%0Atraining%20data%20structure%20and%20quality%20in%20shaping%20reasoning%20behavior.%20However%2C%0Amost%20existing%20approaches%20focus%20on%20transforming%20data%20formats%20while%20neglecting%0Athe%20internal%20reasoning%20complexity%20of%20training%20samples%2C%20leaving%20the%20reasoning%0Apotential%20of%20data%20under-explored%20and%20underutilized.%20In%20this%20work%2C%20we%20posit%20that%0ALLM%20logical%20reasoning%20performance%20is%20jointly%20constrained%20by%20the%20potential%20of%0Athe%20training%20data%20and%20the%20cognitive%20capacity%20of%20the%20model.%20To%20make%20this%0Arelationship%20measurable%2C%20we%20introduce%20Data%20Reasoning%20Intensity%20%28DRI%29%2C%20a%20novel%0Ametric%20that%20quantifies%20the%20latent%20logical%20reasoning%20complexity%20of%20samples%20by%0Adecomposing%20and%20aggregating%20their%20logical%20structures.%20This%20allows%20us%20to%20analyze%0Ahow%20well%20current%20LLMs%20utilize%20logical%20reasoning%20signals%20and%20identify%0Aperformance%20gaps%20relative%20to%20data%20potential.%20Based%20on%20this%20insight%2C%20we%0Aintroduce%20a%20re-cognizing%20optimization%20strategy%20that%20systematically%20enhances%20the%0Alogical%20reasoning%20intensity%20of%20training%20data.Rather%20than%20increasing%20data%0Avolume%2C%20our%20method%20re-optimizes%20existing%20samples%20to%20better%20align%20with%20the%20LLM%27s%0Alogical%20reasoning%20boundary.%20Extensive%20experiments%20show%20that%20our%20approach%0Asignificantly%20improves%20performance%20and%20generalization%20over%20data-centric%0Astrategies.%20We%20further%20validate%20our%20method%20under%20a%20reinforcement%20learning%0Aframework.%20Our%20results%20indicate%20that%20prioritizing%20reasoning%20complexity%20in%20data%0Arather%20than%20sheer%20scale%20or%20superficial%20form%20is%20essential%20to%20realizing%20LLMs%27%0Afull%20cognitive%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520LLMs%2520to%2520Their%2520Logical%2520Reasoning%2520Bound%253A%2520The%2520Role%2520of%2520Data%250A%2520%2520Reasoning%2520Intensity%26entry.906535625%3DZhen%2520Bi%2520and%2520Zhenlin%2520Hu%2520and%2520Jinnan%2520Yang%2520and%2520Mingyang%2520Chen%2520and%2520Cheng%2520Deng%2520and%2520Yida%2520Xue%2520and%2520Zeyu%2520Yang%2520and%2520Qing%2520Shen%2520and%2520Zhenfang%2520Liu%2520and%2520Kang%2520Zhao%2520and%2520Ningyu%2520Zhang%2520and%2520Jungang%2520Lou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520highlight%2520the%2520importance%2520of%250Atraining%2520data%2520structure%2520and%2520quality%2520in%2520shaping%2520reasoning%2520behavior.%2520However%252C%250Amost%2520existing%2520approaches%2520focus%2520on%2520transforming%2520data%2520formats%2520while%2520neglecting%250Athe%2520internal%2520reasoning%2520complexity%2520of%2520training%2520samples%252C%2520leaving%2520the%2520reasoning%250Apotential%2520of%2520data%2520under-explored%2520and%2520underutilized.%2520In%2520this%2520work%252C%2520we%2520posit%2520that%250ALLM%2520logical%2520reasoning%2520performance%2520is%2520jointly%2520constrained%2520by%2520the%2520potential%2520of%250Athe%2520training%2520data%2520and%2520the%2520cognitive%2520capacity%2520of%2520the%2520model.%2520To%2520make%2520this%250Arelationship%2520measurable%252C%2520we%2520introduce%2520Data%2520Reasoning%2520Intensity%2520%2528DRI%2529%252C%2520a%2520novel%250Ametric%2520that%2520quantifies%2520the%2520latent%2520logical%2520reasoning%2520complexity%2520of%2520samples%2520by%250Adecomposing%2520and%2520aggregating%2520their%2520logical%2520structures.%2520This%2520allows%2520us%2520to%2520analyze%250Ahow%2520well%2520current%2520LLMs%2520utilize%2520logical%2520reasoning%2520signals%2520and%2520identify%250Aperformance%2520gaps%2520relative%2520to%2520data%2520potential.%2520Based%2520on%2520this%2520insight%252C%2520we%250Aintroduce%2520a%2520re-cognizing%2520optimization%2520strategy%2520that%2520systematically%2520enhances%2520the%250Alogical%2520reasoning%2520intensity%2520of%2520training%2520data.Rather%2520than%2520increasing%2520data%250Avolume%252C%2520our%2520method%2520re-optimizes%2520existing%2520samples%2520to%2520better%2520align%2520with%2520the%2520LLM%2527s%250Alogical%2520reasoning%2520boundary.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%250Asignificantly%2520improves%2520performance%2520and%2520generalization%2520over%2520data-centric%250Astrategies.%2520We%2520further%2520validate%2520our%2520method%2520under%2520a%2520reinforcement%2520learning%250Aframework.%2520Our%2520results%2520indicate%2520that%2520prioritizing%2520reasoning%2520complexity%2520in%2520data%250Arather%2520than%2520sheer%2520scale%2520or%2520superficial%2520form%2520is%2520essential%2520to%2520realizing%2520LLMs%2527%250Afull%2520cognitive%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20LLMs%20to%20Their%20Logical%20Reasoning%20Bound%3A%20The%20Role%20of%20Data%0A%20%20Reasoning%20Intensity&entry.906535625=Zhen%20Bi%20and%20Zhenlin%20Hu%20and%20Jinnan%20Yang%20and%20Mingyang%20Chen%20and%20Cheng%20Deng%20and%20Yida%20Xue%20and%20Zeyu%20Yang%20and%20Qing%20Shen%20and%20Zhenfang%20Liu%20and%20Kang%20Zhao%20and%20Ningyu%20Zhang%20and%20Jungang%20Lou&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20highlight%20the%20importance%20of%0Atraining%20data%20structure%20and%20quality%20in%20shaping%20reasoning%20behavior.%20However%2C%0Amost%20existing%20approaches%20focus%20on%20transforming%20data%20formats%20while%20neglecting%0Athe%20internal%20reasoning%20complexity%20of%20training%20samples%2C%20leaving%20the%20reasoning%0Apotential%20of%20data%20under-explored%20and%20underutilized.%20In%20this%20work%2C%20we%20posit%20that%0ALLM%20logical%20reasoning%20performance%20is%20jointly%20constrained%20by%20the%20potential%20of%0Athe%20training%20data%20and%20the%20cognitive%20capacity%20of%20the%20model.%20To%20make%20this%0Arelationship%20measurable%2C%20we%20introduce%20Data%20Reasoning%20Intensity%20%28DRI%29%2C%20a%20novel%0Ametric%20that%20quantifies%20the%20latent%20logical%20reasoning%20complexity%20of%20samples%20by%0Adecomposing%20and%20aggregating%20their%20logical%20structures.%20This%20allows%20us%20to%20analyze%0Ahow%20well%20current%20LLMs%20utilize%20logical%20reasoning%20signals%20and%20identify%0Aperformance%20gaps%20relative%20to%20data%20potential.%20Based%20on%20this%20insight%2C%20we%0Aintroduce%20a%20re-cognizing%20optimization%20strategy%20that%20systematically%20enhances%20the%0Alogical%20reasoning%20intensity%20of%20training%20data.Rather%20than%20increasing%20data%0Avolume%2C%20our%20method%20re-optimizes%20existing%20samples%20to%20better%20align%20with%20the%20LLM%27s%0Alogical%20reasoning%20boundary.%20Extensive%20experiments%20show%20that%20our%20approach%0Asignificantly%20improves%20performance%20and%20generalization%20over%20data-centric%0Astrategies.%20We%20further%20validate%20our%20method%20under%20a%20reinforcement%20learning%0Aframework.%20Our%20results%20indicate%20that%20prioritizing%20reasoning%20complexity%20in%20data%0Arather%20than%20sheer%20scale%20or%20superficial%20form%20is%20essential%20to%20realizing%20LLMs%27%0Afull%20cognitive%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24836v1&entry.124074799=Read"},
{"title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model\n  Reasoners with Open Training Recipes", "author": "Changsheng Zhao and Ernie Chang and Zechun Liu and Chia-Jung Chang and Wei Wen and Chen Lai and Rick Cao and Yuandong Tian and Raghuraman Krishnamoorthi and Yangyang Shi and Vikas Chandra", "abstract": "  The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study.\n", "link": "http://arxiv.org/abs/2509.24945v1", "date": "2025-09-29", "relevancy": 2.082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MobileLLM-R1%3A%20Exploring%20the%20Limits%20of%20Sub-Billion%20Language%20Model%0A%20%20Reasoners%20with%20Open%20Training%20Recipes&body=Title%3A%20MobileLLM-R1%3A%20Exploring%20the%20Limits%20of%20Sub-Billion%20Language%20Model%0A%20%20Reasoners%20with%20Open%20Training%20Recipes%0AAuthor%3A%20Changsheng%20Zhao%20and%20Ernie%20Chang%20and%20Zechun%20Liu%20and%20Chia-Jung%20Chang%20and%20Wei%20Wen%20and%20Chen%20Lai%20and%20Rick%20Cao%20and%20Yuandong%20Tian%20and%20Raghuraman%20Krishnamoorthi%20and%20Yangyang%20Shi%20and%20Vikas%20Chandra%0AAbstract%3A%20%20%20The%20paradigm%20shift%20in%20large%20language%20models%20%28LLMs%29%20from%20instinctive%20responses%0Ato%20chain-of-thought%20%28CoT%29%20reasoning%20has%20fueled%20two%20prevailing%20assumptions%3A%20%281%29%0Areasoning%20capabilities%20only%20emerge%20in%20sufficiently%20large%20models%2C%20and%20%282%29%20such%0Acapabilities%20require%20training%20on%20massive%20datasets.%20While%20the%20first%20assumption%0Ahas%20already%20been%20challenged%20by%20recent%20sub-billion-parameter%20reasoning%20models%0Asuch%20as%20Qwen3-0.6B%20and%20DeepSeek%20distilled%20variants%2C%20the%20second%20remains%20largely%0Aunquestioned.%20In%20this%20work%2C%20we%20revisit%20the%20necessity%20of%20scaling%20to%20extremely%0Alarge%20corpora%20%28%3E10T%20tokens%29%20for%20reasoning%20emergence.%20By%20carefully%20curating%20and%0Aresampling%20open-source%20datasets%20that%20we%20identify%20as%20beneficial%20under%20our%0Adesigned%20metrics%2C%20we%20demonstrate%20that%20strong%20reasoning%20abilities%20can%20emerge%0Awith%20far%20less%20data.%20Specifically%2C%20we%20show%20that%20only%20~2T%20tokens%20of%20high-quality%0Adata%20are%20sufficient%2C%20and%20pre-training%20with%204.2T%20tokens%20on%20the%20dataset%20resampled%0Afrom%20these%20~2T%20tokens%2C%20followed%20by%20a%20established%20post-training%20procedure%2C%0Aenables%20the%20development%20of%20MobileLLM-R1%2C%20a%20series%20of%20sub-billion-parameter%0Areasoning%20models%20that%20substantially%20outperform%20prior%20models%20trained%20on%20fully%0Aopen-sourced%20data.%20For%20example%2C%20MobileLLM-R1-950M%20achieves%20an%20AIME%20score%20of%0A15.5%2C%20compared%20to%20just%200.6%20for%20OLMo-2-1.48B%20and%200.3%20for%20SmolLM-2-1.7B.%0ARemarkably%2C%20despite%20being%20trained%20on%20only%2011.7%25%20of%20the%20tokens%20compared%20to%0AQwen3%27s%20proprietary%2036T-token%20corpus%20for%20pretraining%2C%20MobileLLM-R1-950M%20matches%0Aor%20surpasses%20Qwen3-0.6B%20across%20multiple%20reasoning%20benchmarks.%20To%20facilitate%0Afurther%20research%20in%20this%20direction%2C%20we%20have%20released%20the%20complete%20training%0Arecipe%2C%20data%20sources%2C%20data%20mixing%20ratio%2C%20and%20model%20checkpoints%2C%20together%20with%0Athe%20key%20insights%20obtained%20throughout%20this%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobileLLM-R1%253A%2520Exploring%2520the%2520Limits%2520of%2520Sub-Billion%2520Language%2520Model%250A%2520%2520Reasoners%2520with%2520Open%2520Training%2520Recipes%26entry.906535625%3DChangsheng%2520Zhao%2520and%2520Ernie%2520Chang%2520and%2520Zechun%2520Liu%2520and%2520Chia-Jung%2520Chang%2520and%2520Wei%2520Wen%2520and%2520Chen%2520Lai%2520and%2520Rick%2520Cao%2520and%2520Yuandong%2520Tian%2520and%2520Raghuraman%2520Krishnamoorthi%2520and%2520Yangyang%2520Shi%2520and%2520Vikas%2520Chandra%26entry.1292438233%3D%2520%2520The%2520paradigm%2520shift%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520from%2520instinctive%2520responses%250Ato%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520has%2520fueled%2520two%2520prevailing%2520assumptions%253A%2520%25281%2529%250Areasoning%2520capabilities%2520only%2520emerge%2520in%2520sufficiently%2520large%2520models%252C%2520and%2520%25282%2529%2520such%250Acapabilities%2520require%2520training%2520on%2520massive%2520datasets.%2520While%2520the%2520first%2520assumption%250Ahas%2520already%2520been%2520challenged%2520by%2520recent%2520sub-billion-parameter%2520reasoning%2520models%250Asuch%2520as%2520Qwen3-0.6B%2520and%2520DeepSeek%2520distilled%2520variants%252C%2520the%2520second%2520remains%2520largely%250Aunquestioned.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520necessity%2520of%2520scaling%2520to%2520extremely%250Alarge%2520corpora%2520%2528%253E10T%2520tokens%2529%2520for%2520reasoning%2520emergence.%2520By%2520carefully%2520curating%2520and%250Aresampling%2520open-source%2520datasets%2520that%2520we%2520identify%2520as%2520beneficial%2520under%2520our%250Adesigned%2520metrics%252C%2520we%2520demonstrate%2520that%2520strong%2520reasoning%2520abilities%2520can%2520emerge%250Awith%2520far%2520less%2520data.%2520Specifically%252C%2520we%2520show%2520that%2520only%2520~2T%2520tokens%2520of%2520high-quality%250Adata%2520are%2520sufficient%252C%2520and%2520pre-training%2520with%25204.2T%2520tokens%2520on%2520the%2520dataset%2520resampled%250Afrom%2520these%2520~2T%2520tokens%252C%2520followed%2520by%2520a%2520established%2520post-training%2520procedure%252C%250Aenables%2520the%2520development%2520of%2520MobileLLM-R1%252C%2520a%2520series%2520of%2520sub-billion-parameter%250Areasoning%2520models%2520that%2520substantially%2520outperform%2520prior%2520models%2520trained%2520on%2520fully%250Aopen-sourced%2520data.%2520For%2520example%252C%2520MobileLLM-R1-950M%2520achieves%2520an%2520AIME%2520score%2520of%250A15.5%252C%2520compared%2520to%2520just%25200.6%2520for%2520OLMo-2-1.48B%2520and%25200.3%2520for%2520SmolLM-2-1.7B.%250ARemarkably%252C%2520despite%2520being%2520trained%2520on%2520only%252011.7%2525%2520of%2520the%2520tokens%2520compared%2520to%250AQwen3%2527s%2520proprietary%252036T-token%2520corpus%2520for%2520pretraining%252C%2520MobileLLM-R1-950M%2520matches%250Aor%2520surpasses%2520Qwen3-0.6B%2520across%2520multiple%2520reasoning%2520benchmarks.%2520To%2520facilitate%250Afurther%2520research%2520in%2520this%2520direction%252C%2520we%2520have%2520released%2520the%2520complete%2520training%250Arecipe%252C%2520data%2520sources%252C%2520data%2520mixing%2520ratio%252C%2520and%2520model%2520checkpoints%252C%2520together%2520with%250Athe%2520key%2520insights%2520obtained%2520throughout%2520this%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MobileLLM-R1%3A%20Exploring%20the%20Limits%20of%20Sub-Billion%20Language%20Model%0A%20%20Reasoners%20with%20Open%20Training%20Recipes&entry.906535625=Changsheng%20Zhao%20and%20Ernie%20Chang%20and%20Zechun%20Liu%20and%20Chia-Jung%20Chang%20and%20Wei%20Wen%20and%20Chen%20Lai%20and%20Rick%20Cao%20and%20Yuandong%20Tian%20and%20Raghuraman%20Krishnamoorthi%20and%20Yangyang%20Shi%20and%20Vikas%20Chandra&entry.1292438233=%20%20The%20paradigm%20shift%20in%20large%20language%20models%20%28LLMs%29%20from%20instinctive%20responses%0Ato%20chain-of-thought%20%28CoT%29%20reasoning%20has%20fueled%20two%20prevailing%20assumptions%3A%20%281%29%0Areasoning%20capabilities%20only%20emerge%20in%20sufficiently%20large%20models%2C%20and%20%282%29%20such%0Acapabilities%20require%20training%20on%20massive%20datasets.%20While%20the%20first%20assumption%0Ahas%20already%20been%20challenged%20by%20recent%20sub-billion-parameter%20reasoning%20models%0Asuch%20as%20Qwen3-0.6B%20and%20DeepSeek%20distilled%20variants%2C%20the%20second%20remains%20largely%0Aunquestioned.%20In%20this%20work%2C%20we%20revisit%20the%20necessity%20of%20scaling%20to%20extremely%0Alarge%20corpora%20%28%3E10T%20tokens%29%20for%20reasoning%20emergence.%20By%20carefully%20curating%20and%0Aresampling%20open-source%20datasets%20that%20we%20identify%20as%20beneficial%20under%20our%0Adesigned%20metrics%2C%20we%20demonstrate%20that%20strong%20reasoning%20abilities%20can%20emerge%0Awith%20far%20less%20data.%20Specifically%2C%20we%20show%20that%20only%20~2T%20tokens%20of%20high-quality%0Adata%20are%20sufficient%2C%20and%20pre-training%20with%204.2T%20tokens%20on%20the%20dataset%20resampled%0Afrom%20these%20~2T%20tokens%2C%20followed%20by%20a%20established%20post-training%20procedure%2C%0Aenables%20the%20development%20of%20MobileLLM-R1%2C%20a%20series%20of%20sub-billion-parameter%0Areasoning%20models%20that%20substantially%20outperform%20prior%20models%20trained%20on%20fully%0Aopen-sourced%20data.%20For%20example%2C%20MobileLLM-R1-950M%20achieves%20an%20AIME%20score%20of%0A15.5%2C%20compared%20to%20just%200.6%20for%20OLMo-2-1.48B%20and%200.3%20for%20SmolLM-2-1.7B.%0ARemarkably%2C%20despite%20being%20trained%20on%20only%2011.7%25%20of%20the%20tokens%20compared%20to%0AQwen3%27s%20proprietary%2036T-token%20corpus%20for%20pretraining%2C%20MobileLLM-R1-950M%20matches%0Aor%20surpasses%20Qwen3-0.6B%20across%20multiple%20reasoning%20benchmarks.%20To%20facilitate%0Afurther%20research%20in%20this%20direction%2C%20we%20have%20released%20the%20complete%20training%0Arecipe%2C%20data%20sources%2C%20data%20mixing%20ratio%2C%20and%20model%20checkpoints%2C%20together%20with%0Athe%20key%20insights%20obtained%20throughout%20this%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24945v1&entry.124074799=Read"},
{"title": "Certified Neural Approximations of Nonlinear Dynamics", "author": "Frederik Baymler Mathiesen and Nikolaus Vertovec and Francesco Fabiano and Luca Laurenti and Alessandro Abate", "abstract": "  Neural networks hold great potential to act as approximate models of\nnonlinear dynamical systems, with the resulting neural approximations enabling\nverification and control of such systems. However, in safety-critical contexts,\nthe use of neural approximations requires formal bounds on their closeness to\nthe underlying system. To address this fundamental challenge, we propose a\nnovel, adaptive, and parallelizable verification method based on certified\nfirst-order models. Our approach provides formal error bounds on the neural\napproximations of dynamical systems, allowing them to be safely employed as\nsurrogates by interpreting the error bound as bounded disturbances acting on\nthe approximated dynamics. We demonstrate the effectiveness and scalability of\nour method on a range of established benchmarks from the literature, showing\nthat it significantly outperforms the state-of-the-art. Furthermore, we show\nthat our framework can successfully address additional scenarios previously\nintractable for existing methods - neural network compression and an\nautoencoder-based deep learning architecture for learning Koopman operators for\nthe purpose of trajectory prediction.\n", "link": "http://arxiv.org/abs/2505.15497v2", "date": "2025-09-29", "relevancy": 2.0628, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5444}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5158}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Neural%20Approximations%20of%20Nonlinear%20Dynamics&body=Title%3A%20Certified%20Neural%20Approximations%20of%20Nonlinear%20Dynamics%0AAuthor%3A%20Frederik%20Baymler%20Mathiesen%20and%20Nikolaus%20Vertovec%20and%20Francesco%20Fabiano%20and%20Luca%20Laurenti%20and%20Alessandro%20Abate%0AAbstract%3A%20%20%20Neural%20networks%20hold%20great%20potential%20to%20act%20as%20approximate%20models%20of%0Anonlinear%20dynamical%20systems%2C%20with%20the%20resulting%20neural%20approximations%20enabling%0Averification%20and%20control%20of%20such%20systems.%20However%2C%20in%20safety-critical%20contexts%2C%0Athe%20use%20of%20neural%20approximations%20requires%20formal%20bounds%20on%20their%20closeness%20to%0Athe%20underlying%20system.%20To%20address%20this%20fundamental%20challenge%2C%20we%20propose%20a%0Anovel%2C%20adaptive%2C%20and%20parallelizable%20verification%20method%20based%20on%20certified%0Afirst-order%20models.%20Our%20approach%20provides%20formal%20error%20bounds%20on%20the%20neural%0Aapproximations%20of%20dynamical%20systems%2C%20allowing%20them%20to%20be%20safely%20employed%20as%0Asurrogates%20by%20interpreting%20the%20error%20bound%20as%20bounded%20disturbances%20acting%20on%0Athe%20approximated%20dynamics.%20We%20demonstrate%20the%20effectiveness%20and%20scalability%20of%0Aour%20method%20on%20a%20range%20of%20established%20benchmarks%20from%20the%20literature%2C%20showing%0Athat%20it%20significantly%20outperforms%20the%20state-of-the-art.%20Furthermore%2C%20we%20show%0Athat%20our%20framework%20can%20successfully%20address%20additional%20scenarios%20previously%0Aintractable%20for%20existing%20methods%20-%20neural%20network%20compression%20and%20an%0Aautoencoder-based%20deep%20learning%20architecture%20for%20learning%20Koopman%20operators%20for%0Athe%20purpose%20of%20trajectory%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Neural%2520Approximations%2520of%2520Nonlinear%2520Dynamics%26entry.906535625%3DFrederik%2520Baymler%2520Mathiesen%2520and%2520Nikolaus%2520Vertovec%2520and%2520Francesco%2520Fabiano%2520and%2520Luca%2520Laurenti%2520and%2520Alessandro%2520Abate%26entry.1292438233%3D%2520%2520Neural%2520networks%2520hold%2520great%2520potential%2520to%2520act%2520as%2520approximate%2520models%2520of%250Anonlinear%2520dynamical%2520systems%252C%2520with%2520the%2520resulting%2520neural%2520approximations%2520enabling%250Averification%2520and%2520control%2520of%2520such%2520systems.%2520However%252C%2520in%2520safety-critical%2520contexts%252C%250Athe%2520use%2520of%2520neural%2520approximations%2520requires%2520formal%2520bounds%2520on%2520their%2520closeness%2520to%250Athe%2520underlying%2520system.%2520To%2520address%2520this%2520fundamental%2520challenge%252C%2520we%2520propose%2520a%250Anovel%252C%2520adaptive%252C%2520and%2520parallelizable%2520verification%2520method%2520based%2520on%2520certified%250Afirst-order%2520models.%2520Our%2520approach%2520provides%2520formal%2520error%2520bounds%2520on%2520the%2520neural%250Aapproximations%2520of%2520dynamical%2520systems%252C%2520allowing%2520them%2520to%2520be%2520safely%2520employed%2520as%250Asurrogates%2520by%2520interpreting%2520the%2520error%2520bound%2520as%2520bounded%2520disturbances%2520acting%2520on%250Athe%2520approximated%2520dynamics.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520scalability%2520of%250Aour%2520method%2520on%2520a%2520range%2520of%2520established%2520benchmarks%2520from%2520the%2520literature%252C%2520showing%250Athat%2520it%2520significantly%2520outperforms%2520the%2520state-of-the-art.%2520Furthermore%252C%2520we%2520show%250Athat%2520our%2520framework%2520can%2520successfully%2520address%2520additional%2520scenarios%2520previously%250Aintractable%2520for%2520existing%2520methods%2520-%2520neural%2520network%2520compression%2520and%2520an%250Aautoencoder-based%2520deep%2520learning%2520architecture%2520for%2520learning%2520Koopman%2520operators%2520for%250Athe%2520purpose%2520of%2520trajectory%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Neural%20Approximations%20of%20Nonlinear%20Dynamics&entry.906535625=Frederik%20Baymler%20Mathiesen%20and%20Nikolaus%20Vertovec%20and%20Francesco%20Fabiano%20and%20Luca%20Laurenti%20and%20Alessandro%20Abate&entry.1292438233=%20%20Neural%20networks%20hold%20great%20potential%20to%20act%20as%20approximate%20models%20of%0Anonlinear%20dynamical%20systems%2C%20with%20the%20resulting%20neural%20approximations%20enabling%0Averification%20and%20control%20of%20such%20systems.%20However%2C%20in%20safety-critical%20contexts%2C%0Athe%20use%20of%20neural%20approximations%20requires%20formal%20bounds%20on%20their%20closeness%20to%0Athe%20underlying%20system.%20To%20address%20this%20fundamental%20challenge%2C%20we%20propose%20a%0Anovel%2C%20adaptive%2C%20and%20parallelizable%20verification%20method%20based%20on%20certified%0Afirst-order%20models.%20Our%20approach%20provides%20formal%20error%20bounds%20on%20the%20neural%0Aapproximations%20of%20dynamical%20systems%2C%20allowing%20them%20to%20be%20safely%20employed%20as%0Asurrogates%20by%20interpreting%20the%20error%20bound%20as%20bounded%20disturbances%20acting%20on%0Athe%20approximated%20dynamics.%20We%20demonstrate%20the%20effectiveness%20and%20scalability%20of%0Aour%20method%20on%20a%20range%20of%20established%20benchmarks%20from%20the%20literature%2C%20showing%0Athat%20it%20significantly%20outperforms%20the%20state-of-the-art.%20Furthermore%2C%20we%20show%0Athat%20our%20framework%20can%20successfully%20address%20additional%20scenarios%20previously%0Aintractable%20for%20existing%20methods%20-%20neural%20network%20compression%20and%20an%0Aautoencoder-based%20deep%20learning%20architecture%20for%20learning%20Koopman%20operators%20for%0Athe%20purpose%20of%20trajectory%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15497v2&entry.124074799=Read"},
{"title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning\n  via Steering Vectors", "author": "Viacheslav Sinii and Nikita Balagansky and Gleb Gerasimov and Daniil Laptev and Yaroslav Aksenov and Vadim Kurochkin and Alexey Gorbatovski and Boris Shaposhnikov and Daniil Gavrilov", "abstract": "  The mechanisms by which reasoning training reshapes LLMs' internal\ncomputations remain unclear. We study lightweight steering vectors inserted\ninto the base model's residual stream and trained with a reinforcement-learning\nobjective. These vectors match full fine-tuning performance while preserving\nthe interpretability of small, additive interventions. Using logit-lens\nreadouts and path-patching analyses on two models, we find that (i) the\nlast-layer steering vector acts like a token-substitution bias concentrated on\nthe first generated token, consistently boosting tokens such as \"To\" and\n\"Step\"; (ii) the penultimate-layer vector leaves attention patterns largely\nintact and instead operates through the MLP and unembedding, preferentially\nup-weighting process words and structure symbols; and (iii) middle layers\nde-emphasize non-English tokens. Next, we show that a SAE isolates features\nassociated with correct generations. We also show that steering vectors (i)\ntransfer to other models, (ii) combine across layers when trained in isolation,\nand (iii) concentrate magnitude on meaningful prompt segments under adaptive\ntoken-wise scaling. Taken together, these results deepen understanding of how\ntrained steering vectors shape computation and should inform future work in\nactivation engineering and the study of reasoning models.\n", "link": "http://arxiv.org/abs/2509.06608v2", "date": "2025-09-29", "relevancy": 2.059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5196}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20Vectors%2C%20Big%20Effects%3A%20A%20Mechanistic%20Study%20of%20RL-Induced%20Reasoning%0A%20%20via%20Steering%20Vectors&body=Title%3A%20Small%20Vectors%2C%20Big%20Effects%3A%20A%20Mechanistic%20Study%20of%20RL-Induced%20Reasoning%0A%20%20via%20Steering%20Vectors%0AAuthor%3A%20Viacheslav%20Sinii%20and%20Nikita%20Balagansky%20and%20Gleb%20Gerasimov%20and%20Daniil%20Laptev%20and%20Yaroslav%20Aksenov%20and%20Vadim%20Kurochkin%20and%20Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20The%20mechanisms%20by%20which%20reasoning%20training%20reshapes%20LLMs%27%20internal%0Acomputations%20remain%20unclear.%20We%20study%20lightweight%20steering%20vectors%20inserted%0Ainto%20the%20base%20model%27s%20residual%20stream%20and%20trained%20with%20a%20reinforcement-learning%0Aobjective.%20These%20vectors%20match%20full%20fine-tuning%20performance%20while%20preserving%0Athe%20interpretability%20of%20small%2C%20additive%20interventions.%20Using%20logit-lens%0Areadouts%20and%20path-patching%20analyses%20on%20two%20models%2C%20we%20find%20that%20%28i%29%20the%0Alast-layer%20steering%20vector%20acts%20like%20a%20token-substitution%20bias%20concentrated%20on%0Athe%20first%20generated%20token%2C%20consistently%20boosting%20tokens%20such%20as%20%22To%22%20and%0A%22Step%22%3B%20%28ii%29%20the%20penultimate-layer%20vector%20leaves%20attention%20patterns%20largely%0Aintact%20and%20instead%20operates%20through%20the%20MLP%20and%20unembedding%2C%20preferentially%0Aup-weighting%20process%20words%20and%20structure%20symbols%3B%20and%20%28iii%29%20middle%20layers%0Ade-emphasize%20non-English%20tokens.%20Next%2C%20we%20show%20that%20a%20SAE%20isolates%20features%0Aassociated%20with%20correct%20generations.%20We%20also%20show%20that%20steering%20vectors%20%28i%29%0Atransfer%20to%20other%20models%2C%20%28ii%29%20combine%20across%20layers%20when%20trained%20in%20isolation%2C%0Aand%20%28iii%29%20concentrate%20magnitude%20on%20meaningful%20prompt%20segments%20under%20adaptive%0Atoken-wise%20scaling.%20Taken%20together%2C%20these%20results%20deepen%20understanding%20of%20how%0Atrained%20steering%20vectors%20shape%20computation%20and%20should%20inform%20future%20work%20in%0Aactivation%20engineering%20and%20the%20study%20of%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520Vectors%252C%2520Big%2520Effects%253A%2520A%2520Mechanistic%2520Study%2520of%2520RL-Induced%2520Reasoning%250A%2520%2520via%2520Steering%2520Vectors%26entry.906535625%3DViacheslav%2520Sinii%2520and%2520Nikita%2520Balagansky%2520and%2520Gleb%2520Gerasimov%2520and%2520Daniil%2520Laptev%2520and%2520Yaroslav%2520Aksenov%2520and%2520Vadim%2520Kurochkin%2520and%2520Alexey%2520Gorbatovski%2520and%2520Boris%2520Shaposhnikov%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520The%2520mechanisms%2520by%2520which%2520reasoning%2520training%2520reshapes%2520LLMs%2527%2520internal%250Acomputations%2520remain%2520unclear.%2520We%2520study%2520lightweight%2520steering%2520vectors%2520inserted%250Ainto%2520the%2520base%2520model%2527s%2520residual%2520stream%2520and%2520trained%2520with%2520a%2520reinforcement-learning%250Aobjective.%2520These%2520vectors%2520match%2520full%2520fine-tuning%2520performance%2520while%2520preserving%250Athe%2520interpretability%2520of%2520small%252C%2520additive%2520interventions.%2520Using%2520logit-lens%250Areadouts%2520and%2520path-patching%2520analyses%2520on%2520two%2520models%252C%2520we%2520find%2520that%2520%2528i%2529%2520the%250Alast-layer%2520steering%2520vector%2520acts%2520like%2520a%2520token-substitution%2520bias%2520concentrated%2520on%250Athe%2520first%2520generated%2520token%252C%2520consistently%2520boosting%2520tokens%2520such%2520as%2520%2522To%2522%2520and%250A%2522Step%2522%253B%2520%2528ii%2529%2520the%2520penultimate-layer%2520vector%2520leaves%2520attention%2520patterns%2520largely%250Aintact%2520and%2520instead%2520operates%2520through%2520the%2520MLP%2520and%2520unembedding%252C%2520preferentially%250Aup-weighting%2520process%2520words%2520and%2520structure%2520symbols%253B%2520and%2520%2528iii%2529%2520middle%2520layers%250Ade-emphasize%2520non-English%2520tokens.%2520Next%252C%2520we%2520show%2520that%2520a%2520SAE%2520isolates%2520features%250Aassociated%2520with%2520correct%2520generations.%2520We%2520also%2520show%2520that%2520steering%2520vectors%2520%2528i%2529%250Atransfer%2520to%2520other%2520models%252C%2520%2528ii%2529%2520combine%2520across%2520layers%2520when%2520trained%2520in%2520isolation%252C%250Aand%2520%2528iii%2529%2520concentrate%2520magnitude%2520on%2520meaningful%2520prompt%2520segments%2520under%2520adaptive%250Atoken-wise%2520scaling.%2520Taken%2520together%252C%2520these%2520results%2520deepen%2520understanding%2520of%2520how%250Atrained%2520steering%2520vectors%2520shape%2520computation%2520and%2520should%2520inform%2520future%2520work%2520in%250Aactivation%2520engineering%2520and%2520the%2520study%2520of%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Vectors%2C%20Big%20Effects%3A%20A%20Mechanistic%20Study%20of%20RL-Induced%20Reasoning%0A%20%20via%20Steering%20Vectors&entry.906535625=Viacheslav%20Sinii%20and%20Nikita%20Balagansky%20and%20Gleb%20Gerasimov%20and%20Daniil%20Laptev%20and%20Yaroslav%20Aksenov%20and%20Vadim%20Kurochkin%20and%20Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20The%20mechanisms%20by%20which%20reasoning%20training%20reshapes%20LLMs%27%20internal%0Acomputations%20remain%20unclear.%20We%20study%20lightweight%20steering%20vectors%20inserted%0Ainto%20the%20base%20model%27s%20residual%20stream%20and%20trained%20with%20a%20reinforcement-learning%0Aobjective.%20These%20vectors%20match%20full%20fine-tuning%20performance%20while%20preserving%0Athe%20interpretability%20of%20small%2C%20additive%20interventions.%20Using%20logit-lens%0Areadouts%20and%20path-patching%20analyses%20on%20two%20models%2C%20we%20find%20that%20%28i%29%20the%0Alast-layer%20steering%20vector%20acts%20like%20a%20token-substitution%20bias%20concentrated%20on%0Athe%20first%20generated%20token%2C%20consistently%20boosting%20tokens%20such%20as%20%22To%22%20and%0A%22Step%22%3B%20%28ii%29%20the%20penultimate-layer%20vector%20leaves%20attention%20patterns%20largely%0Aintact%20and%20instead%20operates%20through%20the%20MLP%20and%20unembedding%2C%20preferentially%0Aup-weighting%20process%20words%20and%20structure%20symbols%3B%20and%20%28iii%29%20middle%20layers%0Ade-emphasize%20non-English%20tokens.%20Next%2C%20we%20show%20that%20a%20SAE%20isolates%20features%0Aassociated%20with%20correct%20generations.%20We%20also%20show%20that%20steering%20vectors%20%28i%29%0Atransfer%20to%20other%20models%2C%20%28ii%29%20combine%20across%20layers%20when%20trained%20in%20isolation%2C%0Aand%20%28iii%29%20concentrate%20magnitude%20on%20meaningful%20prompt%20segments%20under%20adaptive%0Atoken-wise%20scaling.%20Taken%20together%2C%20these%20results%20deepen%20understanding%20of%20how%0Atrained%20steering%20vectors%20shape%20computation%20and%20should%20inform%20future%20work%20in%0Aactivation%20engineering%20and%20the%20study%20of%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06608v2&entry.124074799=Read"},
{"title": "Towards generalizable deep ptychography neural networks", "author": "Albert Vong and Steven Henke and Oliver Hoidn and Hanna Ruth and Junjing Deng and Alexander Hexemer and Apurva Mehta and Arianna Gleason and Levi Hancock and Nicholas Schwarz", "abstract": "  X-ray ptychography is a data-intensive imaging technique expected to become\nubiquitous at next-generation light sources delivering many-fold increases in\ncoherent flux. The need for real-time feedback under accelerated acquisition\nrates motivates surrogate reconstruction models like deep neural networks,\nwhich offer orders-of-magnitude speedup over conventional methods. However,\nexisting deep learning approaches lack robustness across diverse experimental\nconditions. We propose an unsupervised training workflow emphasizing probe\nlearning by combining experimentally-measured probes with synthetic,\nprocedurally generated objects. This probe-centric approach enables a single\nphysics-informed neural network to reconstruct unseen experiments across\nmultiple beamlines; among the first demonstrations of multi-probe\ngeneralization. We find probe learning is equally important as in-distribution\nlearning; models trained using this synthetic workflow achieve reconstruction\nfidelity comparable to those trained exclusively on experimental data, even\nwhen changing the type of synthetic training object. The proposed approach\nenables training of experiment-steering models that provide real-time feedback\nunder dynamic experimental conditions.\n", "link": "http://arxiv.org/abs/2509.25104v1", "date": "2025-09-29", "relevancy": 2.0578, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5138}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20generalizable%20deep%20ptychography%20neural%20networks&body=Title%3A%20Towards%20generalizable%20deep%20ptychography%20neural%20networks%0AAuthor%3A%20Albert%20Vong%20and%20Steven%20Henke%20and%20Oliver%20Hoidn%20and%20Hanna%20Ruth%20and%20Junjing%20Deng%20and%20Alexander%20Hexemer%20and%20Apurva%20Mehta%20and%20Arianna%20Gleason%20and%20Levi%20Hancock%20and%20Nicholas%20Schwarz%0AAbstract%3A%20%20%20X-ray%20ptychography%20is%20a%20data-intensive%20imaging%20technique%20expected%20to%20become%0Aubiquitous%20at%20next-generation%20light%20sources%20delivering%20many-fold%20increases%20in%0Acoherent%20flux.%20The%20need%20for%20real-time%20feedback%20under%20accelerated%20acquisition%0Arates%20motivates%20surrogate%20reconstruction%20models%20like%20deep%20neural%20networks%2C%0Awhich%20offer%20orders-of-magnitude%20speedup%20over%20conventional%20methods.%20However%2C%0Aexisting%20deep%20learning%20approaches%20lack%20robustness%20across%20diverse%20experimental%0Aconditions.%20We%20propose%20an%20unsupervised%20training%20workflow%20emphasizing%20probe%0Alearning%20by%20combining%20experimentally-measured%20probes%20with%20synthetic%2C%0Aprocedurally%20generated%20objects.%20This%20probe-centric%20approach%20enables%20a%20single%0Aphysics-informed%20neural%20network%20to%20reconstruct%20unseen%20experiments%20across%0Amultiple%20beamlines%3B%20among%20the%20first%20demonstrations%20of%20multi-probe%0Ageneralization.%20We%20find%20probe%20learning%20is%20equally%20important%20as%20in-distribution%0Alearning%3B%20models%20trained%20using%20this%20synthetic%20workflow%20achieve%20reconstruction%0Afidelity%20comparable%20to%20those%20trained%20exclusively%20on%20experimental%20data%2C%20even%0Awhen%20changing%20the%20type%20of%20synthetic%20training%20object.%20The%20proposed%20approach%0Aenables%20training%20of%20experiment-steering%20models%20that%20provide%20real-time%20feedback%0Aunder%20dynamic%20experimental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520generalizable%2520deep%2520ptychography%2520neural%2520networks%26entry.906535625%3DAlbert%2520Vong%2520and%2520Steven%2520Henke%2520and%2520Oliver%2520Hoidn%2520and%2520Hanna%2520Ruth%2520and%2520Junjing%2520Deng%2520and%2520Alexander%2520Hexemer%2520and%2520Apurva%2520Mehta%2520and%2520Arianna%2520Gleason%2520and%2520Levi%2520Hancock%2520and%2520Nicholas%2520Schwarz%26entry.1292438233%3D%2520%2520X-ray%2520ptychography%2520is%2520a%2520data-intensive%2520imaging%2520technique%2520expected%2520to%2520become%250Aubiquitous%2520at%2520next-generation%2520light%2520sources%2520delivering%2520many-fold%2520increases%2520in%250Acoherent%2520flux.%2520The%2520need%2520for%2520real-time%2520feedback%2520under%2520accelerated%2520acquisition%250Arates%2520motivates%2520surrogate%2520reconstruction%2520models%2520like%2520deep%2520neural%2520networks%252C%250Awhich%2520offer%2520orders-of-magnitude%2520speedup%2520over%2520conventional%2520methods.%2520However%252C%250Aexisting%2520deep%2520learning%2520approaches%2520lack%2520robustness%2520across%2520diverse%2520experimental%250Aconditions.%2520We%2520propose%2520an%2520unsupervised%2520training%2520workflow%2520emphasizing%2520probe%250Alearning%2520by%2520combining%2520experimentally-measured%2520probes%2520with%2520synthetic%252C%250Aprocedurally%2520generated%2520objects.%2520This%2520probe-centric%2520approach%2520enables%2520a%2520single%250Aphysics-informed%2520neural%2520network%2520to%2520reconstruct%2520unseen%2520experiments%2520across%250Amultiple%2520beamlines%253B%2520among%2520the%2520first%2520demonstrations%2520of%2520multi-probe%250Ageneralization.%2520We%2520find%2520probe%2520learning%2520is%2520equally%2520important%2520as%2520in-distribution%250Alearning%253B%2520models%2520trained%2520using%2520this%2520synthetic%2520workflow%2520achieve%2520reconstruction%250Afidelity%2520comparable%2520to%2520those%2520trained%2520exclusively%2520on%2520experimental%2520data%252C%2520even%250Awhen%2520changing%2520the%2520type%2520of%2520synthetic%2520training%2520object.%2520The%2520proposed%2520approach%250Aenables%2520training%2520of%2520experiment-steering%2520models%2520that%2520provide%2520real-time%2520feedback%250Aunder%2520dynamic%2520experimental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20generalizable%20deep%20ptychography%20neural%20networks&entry.906535625=Albert%20Vong%20and%20Steven%20Henke%20and%20Oliver%20Hoidn%20and%20Hanna%20Ruth%20and%20Junjing%20Deng%20and%20Alexander%20Hexemer%20and%20Apurva%20Mehta%20and%20Arianna%20Gleason%20and%20Levi%20Hancock%20and%20Nicholas%20Schwarz&entry.1292438233=%20%20X-ray%20ptychography%20is%20a%20data-intensive%20imaging%20technique%20expected%20to%20become%0Aubiquitous%20at%20next-generation%20light%20sources%20delivering%20many-fold%20increases%20in%0Acoherent%20flux.%20The%20need%20for%20real-time%20feedback%20under%20accelerated%20acquisition%0Arates%20motivates%20surrogate%20reconstruction%20models%20like%20deep%20neural%20networks%2C%0Awhich%20offer%20orders-of-magnitude%20speedup%20over%20conventional%20methods.%20However%2C%0Aexisting%20deep%20learning%20approaches%20lack%20robustness%20across%20diverse%20experimental%0Aconditions.%20We%20propose%20an%20unsupervised%20training%20workflow%20emphasizing%20probe%0Alearning%20by%20combining%20experimentally-measured%20probes%20with%20synthetic%2C%0Aprocedurally%20generated%20objects.%20This%20probe-centric%20approach%20enables%20a%20single%0Aphysics-informed%20neural%20network%20to%20reconstruct%20unseen%20experiments%20across%0Amultiple%20beamlines%3B%20among%20the%20first%20demonstrations%20of%20multi-probe%0Ageneralization.%20We%20find%20probe%20learning%20is%20equally%20important%20as%20in-distribution%0Alearning%3B%20models%20trained%20using%20this%20synthetic%20workflow%20achieve%20reconstruction%0Afidelity%20comparable%20to%20those%20trained%20exclusively%20on%20experimental%20data%2C%20even%0Awhen%20changing%20the%20type%20of%20synthetic%20training%20object.%20The%20proposed%20approach%0Aenables%20training%20of%20experiment-steering%20models%20that%20provide%20real-time%20feedback%0Aunder%20dynamic%20experimental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25104v1&entry.124074799=Read"},
{"title": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning", "author": "Matteo Fuoli and Weihang Huang and Jeannette Littlemore and Sarah Turner and Ellen Wilding", "abstract": "  Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.\n", "link": "http://arxiv.org/abs/2509.24866v1", "date": "2025-09-29", "relevancy": 2.0557, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metaphor%20identification%20using%20large%20language%20models%3A%20A%20comparison%20of%0A%20%20RAG%2C%20prompt%20engineering%2C%20and%20fine-tuning&body=Title%3A%20Metaphor%20identification%20using%20large%20language%20models%3A%20A%20comparison%20of%0A%20%20RAG%2C%20prompt%20engineering%2C%20and%20fine-tuning%0AAuthor%3A%20Matteo%20Fuoli%20and%20Weihang%20Huang%20and%20Jeannette%20Littlemore%20and%20Sarah%20Turner%20and%20Ellen%20Wilding%0AAbstract%3A%20%20%20Metaphor%20is%20a%20pervasive%20feature%20of%20discourse%20and%20a%20powerful%20lens%20for%0Aexamining%20cognition%2C%20emotion%2C%20and%20ideology.%20Large-scale%20analysis%2C%20however%2C%20has%0Abeen%20constrained%20by%20the%20need%20for%20manual%20annotation%20due%20to%20the%20context-sensitive%0Anature%20of%20metaphor.%20This%20study%20investigates%20the%20potential%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20metaphor%20identification%20in%20full%20texts.%20We%20compare%0Athree%20methods%3A%20%28i%29%20retrieval-augmented%20generation%20%28RAG%29%2C%20where%20the%20model%20is%0Aprovided%20with%20a%20codebook%20and%20instructed%20to%20annotate%20texts%20based%20on%20its%20rules%0Aand%20examples%3B%20%28ii%29%20prompt%20engineering%2C%20where%20we%20design%20task-specific%20verbal%0Ainstructions%3B%20and%20%28iii%29%20fine-tuning%2C%20where%20the%20model%20is%20trained%20on%20hand-coded%0Atexts%20to%20optimize%20performance.%20Within%20prompt%20engineering%2C%20we%20test%20zero-shot%2C%0Afew-shot%2C%20and%20chain-of-thought%20strategies.%20Our%20results%20show%20that%0Astate-of-the-art%20closed-source%20LLMs%20can%20achieve%20high%20accuracy%2C%20with%20fine-tuning%0Ayielding%20a%20median%20F1%20score%20of%200.79.%20A%20comparison%20of%20human%20and%20LLM%20outputs%0Areveals%20that%20most%20discrepancies%20are%20systematic%2C%20reflecting%20well-known%20grey%0Aareas%20and%20conceptual%20challenges%20in%20metaphor%20theory.%20We%20propose%20that%20LLMs%20can%20be%0Aused%20to%20at%20least%20partly%20automate%20metaphor%20identification%20and%20can%20serve%20as%20a%0Atestbed%20for%20developing%20and%20refining%20metaphor%20identification%20protocols%20and%20the%0Atheory%20that%20underpins%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaphor%2520identification%2520using%2520large%2520language%2520models%253A%2520A%2520comparison%2520of%250A%2520%2520RAG%252C%2520prompt%2520engineering%252C%2520and%2520fine-tuning%26entry.906535625%3DMatteo%2520Fuoli%2520and%2520Weihang%2520Huang%2520and%2520Jeannette%2520Littlemore%2520and%2520Sarah%2520Turner%2520and%2520Ellen%2520Wilding%26entry.1292438233%3D%2520%2520Metaphor%2520is%2520a%2520pervasive%2520feature%2520of%2520discourse%2520and%2520a%2520powerful%2520lens%2520for%250Aexamining%2520cognition%252C%2520emotion%252C%2520and%2520ideology.%2520Large-scale%2520analysis%252C%2520however%252C%2520has%250Abeen%2520constrained%2520by%2520the%2520need%2520for%2520manual%2520annotation%2520due%2520to%2520the%2520context-sensitive%250Anature%2520of%2520metaphor.%2520This%2520study%2520investigates%2520the%2520potential%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520automate%2520metaphor%2520identification%2520in%2520full%2520texts.%2520We%2520compare%250Athree%2520methods%253A%2520%2528i%2529%2520retrieval-augmented%2520generation%2520%2528RAG%2529%252C%2520where%2520the%2520model%2520is%250Aprovided%2520with%2520a%2520codebook%2520and%2520instructed%2520to%2520annotate%2520texts%2520based%2520on%2520its%2520rules%250Aand%2520examples%253B%2520%2528ii%2529%2520prompt%2520engineering%252C%2520where%2520we%2520design%2520task-specific%2520verbal%250Ainstructions%253B%2520and%2520%2528iii%2529%2520fine-tuning%252C%2520where%2520the%2520model%2520is%2520trained%2520on%2520hand-coded%250Atexts%2520to%2520optimize%2520performance.%2520Within%2520prompt%2520engineering%252C%2520we%2520test%2520zero-shot%252C%250Afew-shot%252C%2520and%2520chain-of-thought%2520strategies.%2520Our%2520results%2520show%2520that%250Astate-of-the-art%2520closed-source%2520LLMs%2520can%2520achieve%2520high%2520accuracy%252C%2520with%2520fine-tuning%250Ayielding%2520a%2520median%2520F1%2520score%2520of%25200.79.%2520A%2520comparison%2520of%2520human%2520and%2520LLM%2520outputs%250Areveals%2520that%2520most%2520discrepancies%2520are%2520systematic%252C%2520reflecting%2520well-known%2520grey%250Aareas%2520and%2520conceptual%2520challenges%2520in%2520metaphor%2520theory.%2520We%2520propose%2520that%2520LLMs%2520can%2520be%250Aused%2520to%2520at%2520least%2520partly%2520automate%2520metaphor%2520identification%2520and%2520can%2520serve%2520as%2520a%250Atestbed%2520for%2520developing%2520and%2520refining%2520metaphor%2520identification%2520protocols%2520and%2520the%250Atheory%2520that%2520underpins%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metaphor%20identification%20using%20large%20language%20models%3A%20A%20comparison%20of%0A%20%20RAG%2C%20prompt%20engineering%2C%20and%20fine-tuning&entry.906535625=Matteo%20Fuoli%20and%20Weihang%20Huang%20and%20Jeannette%20Littlemore%20and%20Sarah%20Turner%20and%20Ellen%20Wilding&entry.1292438233=%20%20Metaphor%20is%20a%20pervasive%20feature%20of%20discourse%20and%20a%20powerful%20lens%20for%0Aexamining%20cognition%2C%20emotion%2C%20and%20ideology.%20Large-scale%20analysis%2C%20however%2C%20has%0Abeen%20constrained%20by%20the%20need%20for%20manual%20annotation%20due%20to%20the%20context-sensitive%0Anature%20of%20metaphor.%20This%20study%20investigates%20the%20potential%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20metaphor%20identification%20in%20full%20texts.%20We%20compare%0Athree%20methods%3A%20%28i%29%20retrieval-augmented%20generation%20%28RAG%29%2C%20where%20the%20model%20is%0Aprovided%20with%20a%20codebook%20and%20instructed%20to%20annotate%20texts%20based%20on%20its%20rules%0Aand%20examples%3B%20%28ii%29%20prompt%20engineering%2C%20where%20we%20design%20task-specific%20verbal%0Ainstructions%3B%20and%20%28iii%29%20fine-tuning%2C%20where%20the%20model%20is%20trained%20on%20hand-coded%0Atexts%20to%20optimize%20performance.%20Within%20prompt%20engineering%2C%20we%20test%20zero-shot%2C%0Afew-shot%2C%20and%20chain-of-thought%20strategies.%20Our%20results%20show%20that%0Astate-of-the-art%20closed-source%20LLMs%20can%20achieve%20high%20accuracy%2C%20with%20fine-tuning%0Ayielding%20a%20median%20F1%20score%20of%200.79.%20A%20comparison%20of%20human%20and%20LLM%20outputs%0Areveals%20that%20most%20discrepancies%20are%20systematic%2C%20reflecting%20well-known%20grey%0Aareas%20and%20conceptual%20challenges%20in%20metaphor%20theory.%20We%20propose%20that%20LLMs%20can%20be%0Aused%20to%20at%20least%20partly%20automate%20metaphor%20identification%20and%20can%20serve%20as%20a%0Atestbed%20for%20developing%20and%20refining%20metaphor%20identification%20protocols%20and%20the%0Atheory%20that%20underpins%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24866v1&entry.124074799=Read"},
{"title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification", "author": "Xiaoyi Huang and Junwei Wu and Kejia Zhang and Carl Yang and Zhiming Luo", "abstract": "  Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method.\n", "link": "http://arxiv.org/abs/2509.25082v1", "date": "2025-09-29", "relevancy": 2.0556, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4907}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MANI-Pure%3A%20Magnitude-Adaptive%20Noise%20Injection%20for%20Adversarial%0A%20%20Purification&body=Title%3A%20MANI-Pure%3A%20Magnitude-Adaptive%20Noise%20Injection%20for%20Adversarial%0A%20%20Purification%0AAuthor%3A%20Xiaoyi%20Huang%20and%20Junwei%20Wu%20and%20Kejia%20Zhang%20and%20Carl%20Yang%20and%20Zhiming%20Luo%0AAbstract%3A%20%20%20Adversarial%20purification%20with%20diffusion%20models%20has%20emerged%20as%20a%20promising%0Adefense%20strategy%2C%20but%20existing%20methods%20typically%20rely%20on%20uniform%20noise%0Ainjection%2C%20which%20indiscriminately%20perturbs%20all%20frequencies%2C%20corrupting%20semantic%0Astructures%20and%20undermining%20robustness.%20Our%20empirical%20study%20reveals%20that%0Aadversarial%20perturbations%20are%20not%20uniformly%20distributed%3A%20they%20are%20predominantly%0Aconcentrated%20in%20high-frequency%20regions%2C%20with%20heterogeneous%20magnitude%20intensity%0Apatterns%20that%20vary%20across%20frequencies%20and%20attack%20types.%20Motivated%20by%20this%0Aobservation%2C%20we%20introduce%20MANI-Pure%2C%20a%20magnitude-adaptive%20purification%0Aframework%20that%20leverages%20the%20magnitude%20spectrum%20of%20inputs%20to%20guide%20the%0Apurification%20process.%20Instead%20of%20injecting%20homogeneous%20noise%2C%20MANI-Pure%0Aadaptively%20applies%20heterogeneous%2C%20frequency-targeted%20noise%2C%20effectively%0Asuppressing%20adversarial%20perturbations%20in%20fragile%20high-frequency%2C%20low-magnitude%0Abands%20while%20preserving%20semantically%20critical%20low-frequency%20content.%20Extensive%0Aexperiments%20on%20CIFAR-10%20and%20ImageNet-1K%20validate%20the%20effectiveness%20of%0AMANI-Pure.%20It%20narrows%20the%20clean%20accuracy%20gap%20to%20within%200.59%20of%20the%20original%0Aclassifier%2C%20while%20boosting%20robust%20accuracy%20by%202.15%2C%20and%20achieves%20the%20top-1%0Arobust%20accuracy%20on%20the%20RobustBench%20leaderboard%2C%20surpassing%20the%20previous%0Astate-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMANI-Pure%253A%2520Magnitude-Adaptive%2520Noise%2520Injection%2520for%2520Adversarial%250A%2520%2520Purification%26entry.906535625%3DXiaoyi%2520Huang%2520and%2520Junwei%2520Wu%2520and%2520Kejia%2520Zhang%2520and%2520Carl%2520Yang%2520and%2520Zhiming%2520Luo%26entry.1292438233%3D%2520%2520Adversarial%2520purification%2520with%2520diffusion%2520models%2520has%2520emerged%2520as%2520a%2520promising%250Adefense%2520strategy%252C%2520but%2520existing%2520methods%2520typically%2520rely%2520on%2520uniform%2520noise%250Ainjection%252C%2520which%2520indiscriminately%2520perturbs%2520all%2520frequencies%252C%2520corrupting%2520semantic%250Astructures%2520and%2520undermining%2520robustness.%2520Our%2520empirical%2520study%2520reveals%2520that%250Aadversarial%2520perturbations%2520are%2520not%2520uniformly%2520distributed%253A%2520they%2520are%2520predominantly%250Aconcentrated%2520in%2520high-frequency%2520regions%252C%2520with%2520heterogeneous%2520magnitude%2520intensity%250Apatterns%2520that%2520vary%2520across%2520frequencies%2520and%2520attack%2520types.%2520Motivated%2520by%2520this%250Aobservation%252C%2520we%2520introduce%2520MANI-Pure%252C%2520a%2520magnitude-adaptive%2520purification%250Aframework%2520that%2520leverages%2520the%2520magnitude%2520spectrum%2520of%2520inputs%2520to%2520guide%2520the%250Apurification%2520process.%2520Instead%2520of%2520injecting%2520homogeneous%2520noise%252C%2520MANI-Pure%250Aadaptively%2520applies%2520heterogeneous%252C%2520frequency-targeted%2520noise%252C%2520effectively%250Asuppressing%2520adversarial%2520perturbations%2520in%2520fragile%2520high-frequency%252C%2520low-magnitude%250Abands%2520while%2520preserving%2520semantically%2520critical%2520low-frequency%2520content.%2520Extensive%250Aexperiments%2520on%2520CIFAR-10%2520and%2520ImageNet-1K%2520validate%2520the%2520effectiveness%2520of%250AMANI-Pure.%2520It%2520narrows%2520the%2520clean%2520accuracy%2520gap%2520to%2520within%25200.59%2520of%2520the%2520original%250Aclassifier%252C%2520while%2520boosting%2520robust%2520accuracy%2520by%25202.15%252C%2520and%2520achieves%2520the%2520top-1%250Arobust%2520accuracy%2520on%2520the%2520RobustBench%2520leaderboard%252C%2520surpassing%2520the%2520previous%250Astate-of-the-art%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MANI-Pure%3A%20Magnitude-Adaptive%20Noise%20Injection%20for%20Adversarial%0A%20%20Purification&entry.906535625=Xiaoyi%20Huang%20and%20Junwei%20Wu%20and%20Kejia%20Zhang%20and%20Carl%20Yang%20and%20Zhiming%20Luo&entry.1292438233=%20%20Adversarial%20purification%20with%20diffusion%20models%20has%20emerged%20as%20a%20promising%0Adefense%20strategy%2C%20but%20existing%20methods%20typically%20rely%20on%20uniform%20noise%0Ainjection%2C%20which%20indiscriminately%20perturbs%20all%20frequencies%2C%20corrupting%20semantic%0Astructures%20and%20undermining%20robustness.%20Our%20empirical%20study%20reveals%20that%0Aadversarial%20perturbations%20are%20not%20uniformly%20distributed%3A%20they%20are%20predominantly%0Aconcentrated%20in%20high-frequency%20regions%2C%20with%20heterogeneous%20magnitude%20intensity%0Apatterns%20that%20vary%20across%20frequencies%20and%20attack%20types.%20Motivated%20by%20this%0Aobservation%2C%20we%20introduce%20MANI-Pure%2C%20a%20magnitude-adaptive%20purification%0Aframework%20that%20leverages%20the%20magnitude%20spectrum%20of%20inputs%20to%20guide%20the%0Apurification%20process.%20Instead%20of%20injecting%20homogeneous%20noise%2C%20MANI-Pure%0Aadaptively%20applies%20heterogeneous%2C%20frequency-targeted%20noise%2C%20effectively%0Asuppressing%20adversarial%20perturbations%20in%20fragile%20high-frequency%2C%20low-magnitude%0Abands%20while%20preserving%20semantically%20critical%20low-frequency%20content.%20Extensive%0Aexperiments%20on%20CIFAR-10%20and%20ImageNet-1K%20validate%20the%20effectiveness%20of%0AMANI-Pure.%20It%20narrows%20the%20clean%20accuracy%20gap%20to%20within%200.59%20of%20the%20original%0Aclassifier%2C%20while%20boosting%20robust%20accuracy%20by%202.15%2C%20and%20achieves%20the%20top-1%0Arobust%20accuracy%20on%20the%20RobustBench%20leaderboard%2C%20surpassing%20the%20previous%0Astate-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25082v1&entry.124074799=Read"},
{"title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic\n  Correctness Predictors from Historical Patterns", "author": "Hanqi Xiao and Vaidehi Patil and Hyunji Lee and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection.\n", "link": "http://arxiv.org/abs/2509.24988v1", "date": "2025-09-29", "relevancy": 1.4794, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5065}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Correctness%20Models%3A%20Learning%20Calibrated%20and%20Model-Agnostic%0A%20%20Correctness%20Predictors%20from%20Historical%20Patterns&body=Title%3A%20Generalized%20Correctness%20Models%3A%20Learning%20Calibrated%20and%20Model-Agnostic%0A%20%20Correctness%20Predictors%20from%20Historical%20Patterns%0AAuthor%3A%20Hanqi%20Xiao%20and%20Vaidehi%20Patil%20and%20Hyunji%20Lee%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Generating%20accurate%20and%20calibrated%20confidence%20estimates%20is%20critical%20for%0Adeploying%20LLMs%20in%20high-stakes%20or%20user-facing%20applications%2C%20and%20remains%20an%20open%0Achallenge.%20Prior%20research%20has%20often%20framed%20confidence%20as%20a%20problem%20of%20eliciting%0Aa%20model%27s%20%22self-knowledge%22%2C%20i.e.%2C%20the%20ability%20of%20an%20LLM%20to%20judge%20whether%20its%0Aown%20answers%20are%20correct%3B%20this%20approach%20implicitly%20assumes%20that%20there%20is%20some%0Aprivileged%20information%20about%20the%20answer%27s%20correctness%20that%20is%20accessible%20to%20the%0Amodel%20itself.%20However%2C%20our%20experiments%20reveal%20that%20an%20LLM%20attempting%20to%20predict%0Athe%20correctness%20of%20its%20own%20outputs%20generally%20performs%20no%20better%20than%20an%0Aunrelated%20LLM.%20Moreover%2C%20we%20hypothesize%20that%20a%20key%20factor%20in%20building%20a%0A%22Correctness%20Model%22%20%28CM%29%20is%20exposure%20to%20a%20target%20model%27s%20historical%0Apredictions.%20We%20propose%20multiple%20methods%20to%20inject%20this%20historical%20correctness%0Ainformation%2C%20creating%20a%20Generalized%20Correctness%20Model%20%28GCM%29.%20We%20first%20show%20that%0AGCMs%20can%20be%20trained%20on%20the%20correctness%20data%20from%20many%20LLMs%20and%20learn%20patterns%0Afor%20correctness%20prediction%20applicable%20across%20datasets%20and%20models.%20We%20then%20use%0ACMs%20as%20a%20lens%20for%20studying%20the%20source%20of%20correctness%20prediction%20ability%20and%20its%0Ageneralization%2C%20systematically%20controlling%20their%20training%20data%20and%20finding%20that%0Aanswer%20phrasing%20is%20a%20strong%20predictor%20for%20correctness.%20We%20further%20explore%0Aalternative%20methods%20of%20injecting%20history%20without%20training%20an%20LLM%2C%20finding%20that%0Aincluding%20history%20as%20in-context%20examples%20can%20help%20improve%20correctness%0Aprediction%2C%20and%20post-hoc%20calibration%20can%20provide%20complementary%20reductions%20in%0Acalibration%20error.%20We%20evaluate%20GCMs%20based%20on%20Qwen3-8B%20across%205%20model%20families%0Aand%20the%20MMLU%20and%20TriviaQA%20datasets%2C%20as%20well%20as%20on%20a%20downstream%20selective%0Aprediction%20task%2C%20finding%20that%20reliable%20LLM%20confidence%20estimation%20is%20a%0Ageneralizable%20and%20model-agnostic%20skill%20learned%20by%20systematically%20encoding%0Acorrectness%20history%20rather%20than%20a%20model-specific%20skill%20reliant%20on%0Aself-introspection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Correctness%2520Models%253A%2520Learning%2520Calibrated%2520and%2520Model-Agnostic%250A%2520%2520Correctness%2520Predictors%2520from%2520Historical%2520Patterns%26entry.906535625%3DHanqi%2520Xiao%2520and%2520Vaidehi%2520Patil%2520and%2520Hyunji%2520Lee%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Generating%2520accurate%2520and%2520calibrated%2520confidence%2520estimates%2520is%2520critical%2520for%250Adeploying%2520LLMs%2520in%2520high-stakes%2520or%2520user-facing%2520applications%252C%2520and%2520remains%2520an%2520open%250Achallenge.%2520Prior%2520research%2520has%2520often%2520framed%2520confidence%2520as%2520a%2520problem%2520of%2520eliciting%250Aa%2520model%2527s%2520%2522self-knowledge%2522%252C%2520i.e.%252C%2520the%2520ability%2520of%2520an%2520LLM%2520to%2520judge%2520whether%2520its%250Aown%2520answers%2520are%2520correct%253B%2520this%2520approach%2520implicitly%2520assumes%2520that%2520there%2520is%2520some%250Aprivileged%2520information%2520about%2520the%2520answer%2527s%2520correctness%2520that%2520is%2520accessible%2520to%2520the%250Amodel%2520itself.%2520However%252C%2520our%2520experiments%2520reveal%2520that%2520an%2520LLM%2520attempting%2520to%2520predict%250Athe%2520correctness%2520of%2520its%2520own%2520outputs%2520generally%2520performs%2520no%2520better%2520than%2520an%250Aunrelated%2520LLM.%2520Moreover%252C%2520we%2520hypothesize%2520that%2520a%2520key%2520factor%2520in%2520building%2520a%250A%2522Correctness%2520Model%2522%2520%2528CM%2529%2520is%2520exposure%2520to%2520a%2520target%2520model%2527s%2520historical%250Apredictions.%2520We%2520propose%2520multiple%2520methods%2520to%2520inject%2520this%2520historical%2520correctness%250Ainformation%252C%2520creating%2520a%2520Generalized%2520Correctness%2520Model%2520%2528GCM%2529.%2520We%2520first%2520show%2520that%250AGCMs%2520can%2520be%2520trained%2520on%2520the%2520correctness%2520data%2520from%2520many%2520LLMs%2520and%2520learn%2520patterns%250Afor%2520correctness%2520prediction%2520applicable%2520across%2520datasets%2520and%2520models.%2520We%2520then%2520use%250ACMs%2520as%2520a%2520lens%2520for%2520studying%2520the%2520source%2520of%2520correctness%2520prediction%2520ability%2520and%2520its%250Ageneralization%252C%2520systematically%2520controlling%2520their%2520training%2520data%2520and%2520finding%2520that%250Aanswer%2520phrasing%2520is%2520a%2520strong%2520predictor%2520for%2520correctness.%2520We%2520further%2520explore%250Aalternative%2520methods%2520of%2520injecting%2520history%2520without%2520training%2520an%2520LLM%252C%2520finding%2520that%250Aincluding%2520history%2520as%2520in-context%2520examples%2520can%2520help%2520improve%2520correctness%250Aprediction%252C%2520and%2520post-hoc%2520calibration%2520can%2520provide%2520complementary%2520reductions%2520in%250Acalibration%2520error.%2520We%2520evaluate%2520GCMs%2520based%2520on%2520Qwen3-8B%2520across%25205%2520model%2520families%250Aand%2520the%2520MMLU%2520and%2520TriviaQA%2520datasets%252C%2520as%2520well%2520as%2520on%2520a%2520downstream%2520selective%250Aprediction%2520task%252C%2520finding%2520that%2520reliable%2520LLM%2520confidence%2520estimation%2520is%2520a%250Ageneralizable%2520and%2520model-agnostic%2520skill%2520learned%2520by%2520systematically%2520encoding%250Acorrectness%2520history%2520rather%2520than%2520a%2520model-specific%2520skill%2520reliant%2520on%250Aself-introspection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Correctness%20Models%3A%20Learning%20Calibrated%20and%20Model-Agnostic%0A%20%20Correctness%20Predictors%20from%20Historical%20Patterns&entry.906535625=Hanqi%20Xiao%20and%20Vaidehi%20Patil%20and%20Hyunji%20Lee%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20Generating%20accurate%20and%20calibrated%20confidence%20estimates%20is%20critical%20for%0Adeploying%20LLMs%20in%20high-stakes%20or%20user-facing%20applications%2C%20and%20remains%20an%20open%0Achallenge.%20Prior%20research%20has%20often%20framed%20confidence%20as%20a%20problem%20of%20eliciting%0Aa%20model%27s%20%22self-knowledge%22%2C%20i.e.%2C%20the%20ability%20of%20an%20LLM%20to%20judge%20whether%20its%0Aown%20answers%20are%20correct%3B%20this%20approach%20implicitly%20assumes%20that%20there%20is%20some%0Aprivileged%20information%20about%20the%20answer%27s%20correctness%20that%20is%20accessible%20to%20the%0Amodel%20itself.%20However%2C%20our%20experiments%20reveal%20that%20an%20LLM%20attempting%20to%20predict%0Athe%20correctness%20of%20its%20own%20outputs%20generally%20performs%20no%20better%20than%20an%0Aunrelated%20LLM.%20Moreover%2C%20we%20hypothesize%20that%20a%20key%20factor%20in%20building%20a%0A%22Correctness%20Model%22%20%28CM%29%20is%20exposure%20to%20a%20target%20model%27s%20historical%0Apredictions.%20We%20propose%20multiple%20methods%20to%20inject%20this%20historical%20correctness%0Ainformation%2C%20creating%20a%20Generalized%20Correctness%20Model%20%28GCM%29.%20We%20first%20show%20that%0AGCMs%20can%20be%20trained%20on%20the%20correctness%20data%20from%20many%20LLMs%20and%20learn%20patterns%0Afor%20correctness%20prediction%20applicable%20across%20datasets%20and%20models.%20We%20then%20use%0ACMs%20as%20a%20lens%20for%20studying%20the%20source%20of%20correctness%20prediction%20ability%20and%20its%0Ageneralization%2C%20systematically%20controlling%20their%20training%20data%20and%20finding%20that%0Aanswer%20phrasing%20is%20a%20strong%20predictor%20for%20correctness.%20We%20further%20explore%0Aalternative%20methods%20of%20injecting%20history%20without%20training%20an%20LLM%2C%20finding%20that%0Aincluding%20history%20as%20in-context%20examples%20can%20help%20improve%20correctness%0Aprediction%2C%20and%20post-hoc%20calibration%20can%20provide%20complementary%20reductions%20in%0Acalibration%20error.%20We%20evaluate%20GCMs%20based%20on%20Qwen3-8B%20across%205%20model%20families%0Aand%20the%20MMLU%20and%20TriviaQA%20datasets%2C%20as%20well%20as%20on%20a%20downstream%20selective%0Aprediction%20task%2C%20finding%20that%20reliable%20LLM%20confidence%20estimation%20is%20a%0Ageneralizable%20and%20model-agnostic%20skill%20learned%20by%20systematically%20encoding%0Acorrectness%20history%20rather%20than%20a%20model-specific%20skill%20reliant%20on%0Aself-introspection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24988v1&entry.124074799=Read"},
{"title": "Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific\n  AI", "author": "Bogdan Raoni\u0107 and Siddhartha Mishra and Samuel Lanthaler", "abstract": "  Data-driven models are increasingly adopted in critical scientific fields\nlike weather forecasting and fluid dynamics. These methods can fail on\nout-of-distribution (OOD) data, but detecting such failures in regression tasks\nis an open challenge. We propose a new OOD detection method based on estimating\njoint likelihoods using a score-based diffusion model. This approach considers\nnot just the input but also the regression model's prediction, providing a\ntask-aware reliability score. Across numerous scientific datasets, including\nPDE datasets, satellite imagery and brain tumor segmentation, we show that this\nlikelihood strongly correlates with prediction error. Our work provides a\nfoundational step towards building a verifiable 'certificate of trust', thereby\noffering a practical tool for assessing the trustworthiness of AI-based\nscientific predictions. Our code is publicly available at\nhttps://github.com/bogdanraonic3/OOD_Detection_ScientificML\n", "link": "http://arxiv.org/abs/2509.25080v1", "date": "2025-09-29", "relevancy": 1.6069, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Certificate%20of%20Trust%3A%20Task-Aware%20OOD%20Detection%20for%20Scientific%0A%20%20AI&body=Title%3A%20Towards%20a%20Certificate%20of%20Trust%3A%20Task-Aware%20OOD%20Detection%20for%20Scientific%0A%20%20AI%0AAuthor%3A%20Bogdan%20Raoni%C4%87%20and%20Siddhartha%20Mishra%20and%20Samuel%20Lanthaler%0AAbstract%3A%20%20%20Data-driven%20models%20are%20increasingly%20adopted%20in%20critical%20scientific%20fields%0Alike%20weather%20forecasting%20and%20fluid%20dynamics.%20These%20methods%20can%20fail%20on%0Aout-of-distribution%20%28OOD%29%20data%2C%20but%20detecting%20such%20failures%20in%20regression%20tasks%0Ais%20an%20open%20challenge.%20We%20propose%20a%20new%20OOD%20detection%20method%20based%20on%20estimating%0Ajoint%20likelihoods%20using%20a%20score-based%20diffusion%20model.%20This%20approach%20considers%0Anot%20just%20the%20input%20but%20also%20the%20regression%20model%27s%20prediction%2C%20providing%20a%0Atask-aware%20reliability%20score.%20Across%20numerous%20scientific%20datasets%2C%20including%0APDE%20datasets%2C%20satellite%20imagery%20and%20brain%20tumor%20segmentation%2C%20we%20show%20that%20this%0Alikelihood%20strongly%20correlates%20with%20prediction%20error.%20Our%20work%20provides%20a%0Afoundational%20step%20towards%20building%20a%20verifiable%20%27certificate%20of%20trust%27%2C%20thereby%0Aoffering%20a%20practical%20tool%20for%20assessing%20the%20trustworthiness%20of%20AI-based%0Ascientific%20predictions.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/bogdanraonic3/OOD_Detection_ScientificML%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Certificate%2520of%2520Trust%253A%2520Task-Aware%2520OOD%2520Detection%2520for%2520Scientific%250A%2520%2520AI%26entry.906535625%3DBogdan%2520Raoni%25C4%2587%2520and%2520Siddhartha%2520Mishra%2520and%2520Samuel%2520Lanthaler%26entry.1292438233%3D%2520%2520Data-driven%2520models%2520are%2520increasingly%2520adopted%2520in%2520critical%2520scientific%2520fields%250Alike%2520weather%2520forecasting%2520and%2520fluid%2520dynamics.%2520These%2520methods%2520can%2520fail%2520on%250Aout-of-distribution%2520%2528OOD%2529%2520data%252C%2520but%2520detecting%2520such%2520failures%2520in%2520regression%2520tasks%250Ais%2520an%2520open%2520challenge.%2520We%2520propose%2520a%2520new%2520OOD%2520detection%2520method%2520based%2520on%2520estimating%250Ajoint%2520likelihoods%2520using%2520a%2520score-based%2520diffusion%2520model.%2520This%2520approach%2520considers%250Anot%2520just%2520the%2520input%2520but%2520also%2520the%2520regression%2520model%2527s%2520prediction%252C%2520providing%2520a%250Atask-aware%2520reliability%2520score.%2520Across%2520numerous%2520scientific%2520datasets%252C%2520including%250APDE%2520datasets%252C%2520satellite%2520imagery%2520and%2520brain%2520tumor%2520segmentation%252C%2520we%2520show%2520that%2520this%250Alikelihood%2520strongly%2520correlates%2520with%2520prediction%2520error.%2520Our%2520work%2520provides%2520a%250Afoundational%2520step%2520towards%2520building%2520a%2520verifiable%2520%2527certificate%2520of%2520trust%2527%252C%2520thereby%250Aoffering%2520a%2520practical%2520tool%2520for%2520assessing%2520the%2520trustworthiness%2520of%2520AI-based%250Ascientific%2520predictions.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/bogdanraonic3/OOD_Detection_ScientificML%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Certificate%20of%20Trust%3A%20Task-Aware%20OOD%20Detection%20for%20Scientific%0A%20%20AI&entry.906535625=Bogdan%20Raoni%C4%87%20and%20Siddhartha%20Mishra%20and%20Samuel%20Lanthaler&entry.1292438233=%20%20Data-driven%20models%20are%20increasingly%20adopted%20in%20critical%20scientific%20fields%0Alike%20weather%20forecasting%20and%20fluid%20dynamics.%20These%20methods%20can%20fail%20on%0Aout-of-distribution%20%28OOD%29%20data%2C%20but%20detecting%20such%20failures%20in%20regression%20tasks%0Ais%20an%20open%20challenge.%20We%20propose%20a%20new%20OOD%20detection%20method%20based%20on%20estimating%0Ajoint%20likelihoods%20using%20a%20score-based%20diffusion%20model.%20This%20approach%20considers%0Anot%20just%20the%20input%20but%20also%20the%20regression%20model%27s%20prediction%2C%20providing%20a%0Atask-aware%20reliability%20score.%20Across%20numerous%20scientific%20datasets%2C%20including%0APDE%20datasets%2C%20satellite%20imagery%20and%20brain%20tumor%20segmentation%2C%20we%20show%20that%20this%0Alikelihood%20strongly%20correlates%20with%20prediction%20error.%20Our%20work%20provides%20a%0Afoundational%20step%20towards%20building%20a%20verifiable%20%27certificate%20of%20trust%27%2C%20thereby%0Aoffering%20a%20practical%20tool%20for%20assessing%20the%20trustworthiness%20of%20AI-based%0Ascientific%20predictions.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/bogdanraonic3/OOD_Detection_ScientificML%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25080v1&entry.124074799=Read"},
{"title": "Towards Personalized Deep Research: Benchmarks and Evaluations", "author": "Yuan Liang and Jiaxian Li and Yuqing Wang and Piaohong Wang and Motong Tian and Pai Liu and Shuofei Qiao and Runnan Fang and He Zhu and Ge Zhang and Minghao Liu and Yuchen Eleanor Jiang and Ningyu Zhang and Wangchunshu Zhou", "abstract": "  Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.\n", "link": "http://arxiv.org/abs/2509.25106v1", "date": "2025-09-29", "relevancy": 1.8254, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Personalized%20Deep%20Research%3A%20Benchmarks%20and%20Evaluations&body=Title%3A%20Towards%20Personalized%20Deep%20Research%3A%20Benchmarks%20and%20Evaluations%0AAuthor%3A%20Yuan%20Liang%20and%20Jiaxian%20Li%20and%20Yuqing%20Wang%20and%20Piaohong%20Wang%20and%20Motong%20Tian%20and%20Pai%20Liu%20and%20Shuofei%20Qiao%20and%20Runnan%20Fang%20and%20He%20Zhu%20and%20Ge%20Zhang%20and%20Minghao%20Liu%20and%20Yuchen%20Eleanor%20Jiang%20and%20Ningyu%20Zhang%20and%20Wangchunshu%20Zhou%0AAbstract%3A%20%20%20Deep%20Research%20Agents%20%28DRAs%29%20can%20autonomously%20conduct%20complex%20investigations%0Aand%20generate%20comprehensive%20reports%2C%20demonstrating%20strong%20real-world%20potential.%0AHowever%2C%20existing%20evaluations%20mostly%20rely%20on%20close-ended%20benchmarks%2C%20while%0Aopen-ended%20deep%20research%20benchmarks%20remain%20scarce%20and%20typically%20neglect%0Apersonalized%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Personalized%20Deep%0AResearch%20Bench%2C%20the%20first%20benchmark%20for%20evaluating%20personalization%20in%20DRAs.%20It%0Apairs%2050%20diverse%20research%20tasks%20across%2010%20domains%20with%2025%20authentic%20user%0Aprofiles%20that%20combine%20structured%20persona%20attributes%20with%20dynamic%20real-world%0Acontexts%2C%20yielding%20250%20realistic%20user-task%20queries.%20To%20assess%20system%0Aperformance%2C%20we%20propose%20the%20PQR%20Evaluation%20Framework%2C%20which%20jointly%20measures%0A%28P%29%20Personalization%20Alignment%2C%20%28Q%29%20Content%20Quality%2C%20and%20%28R%29%20Factual%0AReliability.%20Our%20experiments%20on%20a%20range%20of%20systems%20highlight%20current%0Acapabilities%20and%20limitations%20in%20handling%20personalized%20deep%20research.%20This%20work%0Aestablishes%20a%20rigorous%20foundation%20for%20developing%20and%20evaluating%20the%20next%0Ageneration%20of%20truly%20personalized%20AI%20research%20assistants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Personalized%2520Deep%2520Research%253A%2520Benchmarks%2520and%2520Evaluations%26entry.906535625%3DYuan%2520Liang%2520and%2520Jiaxian%2520Li%2520and%2520Yuqing%2520Wang%2520and%2520Piaohong%2520Wang%2520and%2520Motong%2520Tian%2520and%2520Pai%2520Liu%2520and%2520Shuofei%2520Qiao%2520and%2520Runnan%2520Fang%2520and%2520He%2520Zhu%2520and%2520Ge%2520Zhang%2520and%2520Minghao%2520Liu%2520and%2520Yuchen%2520Eleanor%2520Jiang%2520and%2520Ningyu%2520Zhang%2520and%2520Wangchunshu%2520Zhou%26entry.1292438233%3D%2520%2520Deep%2520Research%2520Agents%2520%2528DRAs%2529%2520can%2520autonomously%2520conduct%2520complex%2520investigations%250Aand%2520generate%2520comprehensive%2520reports%252C%2520demonstrating%2520strong%2520real-world%2520potential.%250AHowever%252C%2520existing%2520evaluations%2520mostly%2520rely%2520on%2520close-ended%2520benchmarks%252C%2520while%250Aopen-ended%2520deep%2520research%2520benchmarks%2520remain%2520scarce%2520and%2520typically%2520neglect%250Apersonalized%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Personalized%2520Deep%250AResearch%2520Bench%252C%2520the%2520first%2520benchmark%2520for%2520evaluating%2520personalization%2520in%2520DRAs.%2520It%250Apairs%252050%2520diverse%2520research%2520tasks%2520across%252010%2520domains%2520with%252025%2520authentic%2520user%250Aprofiles%2520that%2520combine%2520structured%2520persona%2520attributes%2520with%2520dynamic%2520real-world%250Acontexts%252C%2520yielding%2520250%2520realistic%2520user-task%2520queries.%2520To%2520assess%2520system%250Aperformance%252C%2520we%2520propose%2520the%2520PQR%2520Evaluation%2520Framework%252C%2520which%2520jointly%2520measures%250A%2528P%2529%2520Personalization%2520Alignment%252C%2520%2528Q%2529%2520Content%2520Quality%252C%2520and%2520%2528R%2529%2520Factual%250AReliability.%2520Our%2520experiments%2520on%2520a%2520range%2520of%2520systems%2520highlight%2520current%250Acapabilities%2520and%2520limitations%2520in%2520handling%2520personalized%2520deep%2520research.%2520This%2520work%250Aestablishes%2520a%2520rigorous%2520foundation%2520for%2520developing%2520and%2520evaluating%2520the%2520next%250Ageneration%2520of%2520truly%2520personalized%2520AI%2520research%2520assistants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Personalized%20Deep%20Research%3A%20Benchmarks%20and%20Evaluations&entry.906535625=Yuan%20Liang%20and%20Jiaxian%20Li%20and%20Yuqing%20Wang%20and%20Piaohong%20Wang%20and%20Motong%20Tian%20and%20Pai%20Liu%20and%20Shuofei%20Qiao%20and%20Runnan%20Fang%20and%20He%20Zhu%20and%20Ge%20Zhang%20and%20Minghao%20Liu%20and%20Yuchen%20Eleanor%20Jiang%20and%20Ningyu%20Zhang%20and%20Wangchunshu%20Zhou&entry.1292438233=%20%20Deep%20Research%20Agents%20%28DRAs%29%20can%20autonomously%20conduct%20complex%20investigations%0Aand%20generate%20comprehensive%20reports%2C%20demonstrating%20strong%20real-world%20potential.%0AHowever%2C%20existing%20evaluations%20mostly%20rely%20on%20close-ended%20benchmarks%2C%20while%0Aopen-ended%20deep%20research%20benchmarks%20remain%20scarce%20and%20typically%20neglect%0Apersonalized%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Personalized%20Deep%0AResearch%20Bench%2C%20the%20first%20benchmark%20for%20evaluating%20personalization%20in%20DRAs.%20It%0Apairs%2050%20diverse%20research%20tasks%20across%2010%20domains%20with%2025%20authentic%20user%0Aprofiles%20that%20combine%20structured%20persona%20attributes%20with%20dynamic%20real-world%0Acontexts%2C%20yielding%20250%20realistic%20user-task%20queries.%20To%20assess%20system%0Aperformance%2C%20we%20propose%20the%20PQR%20Evaluation%20Framework%2C%20which%20jointly%20measures%0A%28P%29%20Personalization%20Alignment%2C%20%28Q%29%20Content%20Quality%2C%20and%20%28R%29%20Factual%0AReliability.%20Our%20experiments%20on%20a%20range%20of%20systems%20highlight%20current%0Acapabilities%20and%20limitations%20in%20handling%20personalized%20deep%20research.%20This%20work%0Aestablishes%20a%20rigorous%20foundation%20for%20developing%20and%20evaluating%20the%20next%0Ageneration%20of%20truly%20personalized%20AI%20research%20assistants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25106v1&entry.124074799=Read"},
{"title": "Confidence Improves Self-Consistency in LLMs", "author": "Amir Taubenfeld and Tom Sheffer and Eran Ofek and Amir Feder and Ariel Goldstein and Zorik Gekhman and Gal Yona", "abstract": "  Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.\n", "link": "http://arxiv.org/abs/2502.06233v2", "date": "2025-09-29", "relevancy": 1.9737, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.506}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Improves%20Self-Consistency%20in%20LLMs&body=Title%3A%20Confidence%20Improves%20Self-Consistency%20in%20LLMs%0AAuthor%3A%20Amir%20Taubenfeld%20and%20Tom%20Sheffer%20and%20Eran%20Ofek%20and%20Amir%20Feder%20and%20Ariel%20Goldstein%20and%20Zorik%20Gekhman%20and%20Gal%20Yona%0AAbstract%3A%20%20%20Self-consistency%20decoding%20enhances%20LLMs%27%20performance%20on%20reasoning%20tasks%20by%0Asampling%20diverse%20reasoning%20paths%20and%20selecting%20the%20most%20frequent%20answer.%0AHowever%2C%20it%20is%20computationally%20expensive%2C%20as%20sampling%20many%20of%20these%20%28lengthy%29%0Apaths%20is%20required%20to%20increase%20the%20chances%20that%20the%20correct%20answer%20emerges%20as%0Athe%20most%20frequent%20one.%20To%20address%20this%2C%20we%20introduce%20Confidence-Informed%0ASelf-Consistency%20%28CISC%29.%20CISC%20performs%20a%20weighted%20majority%20vote%20based%20on%0Aconfidence%20scores%20obtained%20directly%20from%20the%20model.%20By%20prioritizing%0Ahigh-confidence%20paths%2C%20it%20can%20identify%20the%20correct%20answer%20with%20a%20significantly%0Asmaller%20sample%20size.%20When%20tested%20on%20nine%20models%20and%20four%20datasets%2C%20CISC%0Aoutperforms%20self-consistency%20in%20nearly%20all%20configurations%2C%20reducing%20the%0Arequired%20number%20of%20reasoning%20paths%20by%20over%2040%25%20on%20average.%20In%20addition%2C%20we%0Aintroduce%20the%20notion%20of%20within-question%20confidence%20evaluation%2C%20after%20showing%0Athat%20standard%20evaluation%20methods%20are%20poor%20predictors%20of%20success%20in%0Adistinguishing%20correct%20and%20incorrect%20answers%20to%20the%20same%20question.%20In%20fact%2C%20the%0Amost%20calibrated%20confidence%20method%20proved%20to%20be%20the%20least%20effective%20for%20CISC.%0ALastly%2C%20beyond%20these%20practical%20implications%2C%20our%20results%20and%20analyses%20show%20that%0ALLMs%20can%20effectively%20judge%20the%20correctness%20of%20their%20own%20outputs%2C%20contributing%0Ato%20the%20ongoing%20debate%20on%20this%20topic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Improves%2520Self-Consistency%2520in%2520LLMs%26entry.906535625%3DAmir%2520Taubenfeld%2520and%2520Tom%2520Sheffer%2520and%2520Eran%2520Ofek%2520and%2520Amir%2520Feder%2520and%2520Ariel%2520Goldstein%2520and%2520Zorik%2520Gekhman%2520and%2520Gal%2520Yona%26entry.1292438233%3D%2520%2520Self-consistency%2520decoding%2520enhances%2520LLMs%2527%2520performance%2520on%2520reasoning%2520tasks%2520by%250Asampling%2520diverse%2520reasoning%2520paths%2520and%2520selecting%2520the%2520most%2520frequent%2520answer.%250AHowever%252C%2520it%2520is%2520computationally%2520expensive%252C%2520as%2520sampling%2520many%2520of%2520these%2520%2528lengthy%2529%250Apaths%2520is%2520required%2520to%2520increase%2520the%2520chances%2520that%2520the%2520correct%2520answer%2520emerges%2520as%250Athe%2520most%2520frequent%2520one.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Confidence-Informed%250ASelf-Consistency%2520%2528CISC%2529.%2520CISC%2520performs%2520a%2520weighted%2520majority%2520vote%2520based%2520on%250Aconfidence%2520scores%2520obtained%2520directly%2520from%2520the%2520model.%2520By%2520prioritizing%250Ahigh-confidence%2520paths%252C%2520it%2520can%2520identify%2520the%2520correct%2520answer%2520with%2520a%2520significantly%250Asmaller%2520sample%2520size.%2520When%2520tested%2520on%2520nine%2520models%2520and%2520four%2520datasets%252C%2520CISC%250Aoutperforms%2520self-consistency%2520in%2520nearly%2520all%2520configurations%252C%2520reducing%2520the%250Arequired%2520number%2520of%2520reasoning%2520paths%2520by%2520over%252040%2525%2520on%2520average.%2520In%2520addition%252C%2520we%250Aintroduce%2520the%2520notion%2520of%2520within-question%2520confidence%2520evaluation%252C%2520after%2520showing%250Athat%2520standard%2520evaluation%2520methods%2520are%2520poor%2520predictors%2520of%2520success%2520in%250Adistinguishing%2520correct%2520and%2520incorrect%2520answers%2520to%2520the%2520same%2520question.%2520In%2520fact%252C%2520the%250Amost%2520calibrated%2520confidence%2520method%2520proved%2520to%2520be%2520the%2520least%2520effective%2520for%2520CISC.%250ALastly%252C%2520beyond%2520these%2520practical%2520implications%252C%2520our%2520results%2520and%2520analyses%2520show%2520that%250ALLMs%2520can%2520effectively%2520judge%2520the%2520correctness%2520of%2520their%2520own%2520outputs%252C%2520contributing%250Ato%2520the%2520ongoing%2520debate%2520on%2520this%2520topic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Improves%20Self-Consistency%20in%20LLMs&entry.906535625=Amir%20Taubenfeld%20and%20Tom%20Sheffer%20and%20Eran%20Ofek%20and%20Amir%20Feder%20and%20Ariel%20Goldstein%20and%20Zorik%20Gekhman%20and%20Gal%20Yona&entry.1292438233=%20%20Self-consistency%20decoding%20enhances%20LLMs%27%20performance%20on%20reasoning%20tasks%20by%0Asampling%20diverse%20reasoning%20paths%20and%20selecting%20the%20most%20frequent%20answer.%0AHowever%2C%20it%20is%20computationally%20expensive%2C%20as%20sampling%20many%20of%20these%20%28lengthy%29%0Apaths%20is%20required%20to%20increase%20the%20chances%20that%20the%20correct%20answer%20emerges%20as%0Athe%20most%20frequent%20one.%20To%20address%20this%2C%20we%20introduce%20Confidence-Informed%0ASelf-Consistency%20%28CISC%29.%20CISC%20performs%20a%20weighted%20majority%20vote%20based%20on%0Aconfidence%20scores%20obtained%20directly%20from%20the%20model.%20By%20prioritizing%0Ahigh-confidence%20paths%2C%20it%20can%20identify%20the%20correct%20answer%20with%20a%20significantly%0Asmaller%20sample%20size.%20When%20tested%20on%20nine%20models%20and%20four%20datasets%2C%20CISC%0Aoutperforms%20self-consistency%20in%20nearly%20all%20configurations%2C%20reducing%20the%0Arequired%20number%20of%20reasoning%20paths%20by%20over%2040%25%20on%20average.%20In%20addition%2C%20we%0Aintroduce%20the%20notion%20of%20within-question%20confidence%20evaluation%2C%20after%20showing%0Athat%20standard%20evaluation%20methods%20are%20poor%20predictors%20of%20success%20in%0Adistinguishing%20correct%20and%20incorrect%20answers%20to%20the%20same%20question.%20In%20fact%2C%20the%0Amost%20calibrated%20confidence%20method%20proved%20to%20be%20the%20least%20effective%20for%20CISC.%0ALastly%2C%20beyond%20these%20practical%20implications%2C%20our%20results%20and%20analyses%20show%20that%0ALLMs%20can%20effectively%20judge%20the%20correctness%20of%20their%20own%20outputs%2C%20contributing%0Ato%20the%20ongoing%20debate%20on%20this%20topic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06233v2&entry.124074799=Read"},
{"title": "Fidelity-Aware Data Composition for Robust Robot Generalization", "author": "Zizhao Tong and Di Chen and Sicheng Hu and Hongwei Fan and Liliang Chen and Guanghui Ren and Hao Tang and Hao Dong and Ling Shao", "abstract": "  Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots.\n", "link": "http://arxiv.org/abs/2509.24797v1", "date": "2025-09-29", "relevancy": 1.6798, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5636}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fidelity-Aware%20Data%20Composition%20for%20Robust%20Robot%20Generalization&body=Title%3A%20Fidelity-Aware%20Data%20Composition%20for%20Robust%20Robot%20Generalization%0AAuthor%3A%20Zizhao%20Tong%20and%20Di%20Chen%20and%20Sicheng%20Hu%20and%20Hongwei%20Fan%20and%20Liliang%20Chen%20and%20Guanghui%20Ren%20and%20Hao%20Tang%20and%20Hao%20Dong%20and%20Ling%20Shao%0AAbstract%3A%20%20%20Generalist%20robot%20policies%20trained%20on%20large-scale%2C%20visually%20homogeneous%0Adatasets%20can%20be%20susceptible%20to%20shortcut%20learning%2C%20which%20impairs%20their%0Aout-of-distribution%20%28OOD%29%20generalization.%20While%20generative%20data%20augmentation%20is%0Aa%20common%20approach%20to%20introduce%20diversity%2C%20it%20presents%20a%20subtle%20challenge%3A%20data%0Acomposition.%20Naively%20mixing%20real%20and%20synthetic%20data%20can%20corrupt%20the%20learning%0Asignal%2C%20as%20this%20process%20often%20prioritizes%20visual%20diversity%20at%20the%20expense%20of%0Ainformation%20fidelity.%20This%20paper%20suggests%20that%20robust%20generalization%20depends%20on%0Aprincipled%2C%20fidelity-aware%20data%20composition.%20We%20introduce%20Coherent%20Information%0AFidelity%20Tuning%20%28CIFT%29%2C%20a%20framework%20that%20treats%20data%20composition%20as%20an%0Aoptimization%20problem.%20CIFT%20uses%20a%20practical%20proxy%20for%20Information%20Fidelity%0Abased%20on%20the%20feature-space%20geometry%20of%20a%20dataset.%20This%20enables%20the%0Aidentification%20of%20a%20phase%20transition%2C%20termed%20the%20Decoherence%20Point%2C%20where%0Atraining%20stability%20degrades.%20The%20framework%20includes%20a%20generative%20engine%2C%0AMulti-View%20Video%20Augmentation%20%28MVAug%29%2C%20to%20synthesize%20a%20causally%20disentangled%0Adata%20spectrum%20for%20this%20tuning%20process.%20Applying%20CIFT%20to%20policy%20architectures%0Asuch%20as%20%24%5Cpi_0%24%20and%20Diffusion%20Policy%20improves%20OOD%20success%20rates%20by%20over%2054%5C%25.%0AThese%20results%20indicate%20that%20fidelity-aware%20composition%2C%20beyond%20data%20synthesis%0Aalone%2C%20is%20an%20important%20component%20for%20developing%20robust%2C%20general-purpose%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFidelity-Aware%2520Data%2520Composition%2520for%2520Robust%2520Robot%2520Generalization%26entry.906535625%3DZizhao%2520Tong%2520and%2520Di%2520Chen%2520and%2520Sicheng%2520Hu%2520and%2520Hongwei%2520Fan%2520and%2520Liliang%2520Chen%2520and%2520Guanghui%2520Ren%2520and%2520Hao%2520Tang%2520and%2520Hao%2520Dong%2520and%2520Ling%2520Shao%26entry.1292438233%3D%2520%2520Generalist%2520robot%2520policies%2520trained%2520on%2520large-scale%252C%2520visually%2520homogeneous%250Adatasets%2520can%2520be%2520susceptible%2520to%2520shortcut%2520learning%252C%2520which%2520impairs%2520their%250Aout-of-distribution%2520%2528OOD%2529%2520generalization.%2520While%2520generative%2520data%2520augmentation%2520is%250Aa%2520common%2520approach%2520to%2520introduce%2520diversity%252C%2520it%2520presents%2520a%2520subtle%2520challenge%253A%2520data%250Acomposition.%2520Naively%2520mixing%2520real%2520and%2520synthetic%2520data%2520can%2520corrupt%2520the%2520learning%250Asignal%252C%2520as%2520this%2520process%2520often%2520prioritizes%2520visual%2520diversity%2520at%2520the%2520expense%2520of%250Ainformation%2520fidelity.%2520This%2520paper%2520suggests%2520that%2520robust%2520generalization%2520depends%2520on%250Aprincipled%252C%2520fidelity-aware%2520data%2520composition.%2520We%2520introduce%2520Coherent%2520Information%250AFidelity%2520Tuning%2520%2528CIFT%2529%252C%2520a%2520framework%2520that%2520treats%2520data%2520composition%2520as%2520an%250Aoptimization%2520problem.%2520CIFT%2520uses%2520a%2520practical%2520proxy%2520for%2520Information%2520Fidelity%250Abased%2520on%2520the%2520feature-space%2520geometry%2520of%2520a%2520dataset.%2520This%2520enables%2520the%250Aidentification%2520of%2520a%2520phase%2520transition%252C%2520termed%2520the%2520Decoherence%2520Point%252C%2520where%250Atraining%2520stability%2520degrades.%2520The%2520framework%2520includes%2520a%2520generative%2520engine%252C%250AMulti-View%2520Video%2520Augmentation%2520%2528MVAug%2529%252C%2520to%2520synthesize%2520a%2520causally%2520disentangled%250Adata%2520spectrum%2520for%2520this%2520tuning%2520process.%2520Applying%2520CIFT%2520to%2520policy%2520architectures%250Asuch%2520as%2520%2524%255Cpi_0%2524%2520and%2520Diffusion%2520Policy%2520improves%2520OOD%2520success%2520rates%2520by%2520over%252054%255C%2525.%250AThese%2520results%2520indicate%2520that%2520fidelity-aware%2520composition%252C%2520beyond%2520data%2520synthesis%250Aalone%252C%2520is%2520an%2520important%2520component%2520for%2520developing%2520robust%252C%2520general-purpose%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fidelity-Aware%20Data%20Composition%20for%20Robust%20Robot%20Generalization&entry.906535625=Zizhao%20Tong%20and%20Di%20Chen%20and%20Sicheng%20Hu%20and%20Hongwei%20Fan%20and%20Liliang%20Chen%20and%20Guanghui%20Ren%20and%20Hao%20Tang%20and%20Hao%20Dong%20and%20Ling%20Shao&entry.1292438233=%20%20Generalist%20robot%20policies%20trained%20on%20large-scale%2C%20visually%20homogeneous%0Adatasets%20can%20be%20susceptible%20to%20shortcut%20learning%2C%20which%20impairs%20their%0Aout-of-distribution%20%28OOD%29%20generalization.%20While%20generative%20data%20augmentation%20is%0Aa%20common%20approach%20to%20introduce%20diversity%2C%20it%20presents%20a%20subtle%20challenge%3A%20data%0Acomposition.%20Naively%20mixing%20real%20and%20synthetic%20data%20can%20corrupt%20the%20learning%0Asignal%2C%20as%20this%20process%20often%20prioritizes%20visual%20diversity%20at%20the%20expense%20of%0Ainformation%20fidelity.%20This%20paper%20suggests%20that%20robust%20generalization%20depends%20on%0Aprincipled%2C%20fidelity-aware%20data%20composition.%20We%20introduce%20Coherent%20Information%0AFidelity%20Tuning%20%28CIFT%29%2C%20a%20framework%20that%20treats%20data%20composition%20as%20an%0Aoptimization%20problem.%20CIFT%20uses%20a%20practical%20proxy%20for%20Information%20Fidelity%0Abased%20on%20the%20feature-space%20geometry%20of%20a%20dataset.%20This%20enables%20the%0Aidentification%20of%20a%20phase%20transition%2C%20termed%20the%20Decoherence%20Point%2C%20where%0Atraining%20stability%20degrades.%20The%20framework%20includes%20a%20generative%20engine%2C%0AMulti-View%20Video%20Augmentation%20%28MVAug%29%2C%20to%20synthesize%20a%20causally%20disentangled%0Adata%20spectrum%20for%20this%20tuning%20process.%20Applying%20CIFT%20to%20policy%20architectures%0Asuch%20as%20%24%5Cpi_0%24%20and%20Diffusion%20Policy%20improves%20OOD%20success%20rates%20by%20over%2054%5C%25.%0AThese%20results%20indicate%20that%20fidelity-aware%20composition%2C%20beyond%20data%20synthesis%0Aalone%2C%20is%20an%20important%20component%20for%20developing%20robust%2C%20general-purpose%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24797v1&entry.124074799=Read"},
{"title": "Sampling Complexity of TD and PPO in RKHS", "author": "Lu Zou and Wendi Ren and Weizhong Zhang and Liang Ding and Shuang Li", "abstract": "  We revisit Proximal Policy Optimization (PPO) from a function-space\nperspective. Our analysis decouples policy evaluation and improvement in a\nreproducing kernel Hilbert space (RKHS): (i) A kernelized temporal-difference\n(TD) critic performs efficient RKHS-gradient updates using only one-step\nstate-action transition samples; (ii) a KL-regularized, natural-gradient policy\nstep exponentiates the evaluated action-value, recovering a PPO/TRPO-style\nproximal update in continuous state-action spaces. We provide non-asymptotic,\ninstance-adaptive guarantees whose rates depend on RKHS entropy, unifying\ntabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes,\nand we derive a sampling rule for the proximal update that ensures the optimal\n$k^{-1/2}$ convergence rate for stochastic optimization. Empirically, the\ntheory-aligned schedule improves stability and sample efficiency on common\ncontrol tasks (e.g., CartPole, Acrobot), while our TD-based critic attains\nfavorable throughput versus a GAE baseline. Altogether, our results place PPO\non a firmer theoretical footing beyond finite-dimensional assumptions and\nclarify when RKHS-proximal updates with kernel-TD critics yield global policy\nimprovement with practical efficiency.\n", "link": "http://arxiv.org/abs/2509.24991v1", "date": "2025-09-29", "relevancy": 1.7194, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4273}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20Complexity%20of%20TD%20and%20PPO%20in%20RKHS&body=Title%3A%20Sampling%20Complexity%20of%20TD%20and%20PPO%20in%20RKHS%0AAuthor%3A%20Lu%20Zou%20and%20Wendi%20Ren%20and%20Weizhong%20Zhang%20and%20Liang%20Ding%20and%20Shuang%20Li%0AAbstract%3A%20%20%20We%20revisit%20Proximal%20Policy%20Optimization%20%28PPO%29%20from%20a%20function-space%0Aperspective.%20Our%20analysis%20decouples%20policy%20evaluation%20and%20improvement%20in%20a%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%3A%20%28i%29%20A%20kernelized%20temporal-difference%0A%28TD%29%20critic%20performs%20efficient%20RKHS-gradient%20updates%20using%20only%20one-step%0Astate-action%20transition%20samples%3B%20%28ii%29%20a%20KL-regularized%2C%20natural-gradient%20policy%0Astep%20exponentiates%20the%20evaluated%20action-value%2C%20recovering%20a%20PPO/TRPO-style%0Aproximal%20update%20in%20continuous%20state-action%20spaces.%20We%20provide%20non-asymptotic%2C%0Ainstance-adaptive%20guarantees%20whose%20rates%20depend%20on%20RKHS%20entropy%2C%20unifying%0Atabular%2C%20linear%2C%20Sobolev%2C%20Gaussian%2C%20and%20Neural%20Tangent%20Kernel%20%28NTK%29%20regimes%2C%0Aand%20we%20derive%20a%20sampling%20rule%20for%20the%20proximal%20update%20that%20ensures%20the%20optimal%0A%24k%5E%7B-1/2%7D%24%20convergence%20rate%20for%20stochastic%20optimization.%20Empirically%2C%20the%0Atheory-aligned%20schedule%20improves%20stability%20and%20sample%20efficiency%20on%20common%0Acontrol%20tasks%20%28e.g.%2C%20CartPole%2C%20Acrobot%29%2C%20while%20our%20TD-based%20critic%20attains%0Afavorable%20throughput%20versus%20a%20GAE%20baseline.%20Altogether%2C%20our%20results%20place%20PPO%0Aon%20a%20firmer%20theoretical%20footing%20beyond%20finite-dimensional%20assumptions%20and%0Aclarify%20when%20RKHS-proximal%20updates%20with%20kernel-TD%20critics%20yield%20global%20policy%0Aimprovement%20with%20practical%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520Complexity%2520of%2520TD%2520and%2520PPO%2520in%2520RKHS%26entry.906535625%3DLu%2520Zou%2520and%2520Wendi%2520Ren%2520and%2520Weizhong%2520Zhang%2520and%2520Liang%2520Ding%2520and%2520Shuang%2520Li%26entry.1292438233%3D%2520%2520We%2520revisit%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520from%2520a%2520function-space%250Aperspective.%2520Our%2520analysis%2520decouples%2520policy%2520evaluation%2520and%2520improvement%2520in%2520a%250Areproducing%2520kernel%2520Hilbert%2520space%2520%2528RKHS%2529%253A%2520%2528i%2529%2520A%2520kernelized%2520temporal-difference%250A%2528TD%2529%2520critic%2520performs%2520efficient%2520RKHS-gradient%2520updates%2520using%2520only%2520one-step%250Astate-action%2520transition%2520samples%253B%2520%2528ii%2529%2520a%2520KL-regularized%252C%2520natural-gradient%2520policy%250Astep%2520exponentiates%2520the%2520evaluated%2520action-value%252C%2520recovering%2520a%2520PPO/TRPO-style%250Aproximal%2520update%2520in%2520continuous%2520state-action%2520spaces.%2520We%2520provide%2520non-asymptotic%252C%250Ainstance-adaptive%2520guarantees%2520whose%2520rates%2520depend%2520on%2520RKHS%2520entropy%252C%2520unifying%250Atabular%252C%2520linear%252C%2520Sobolev%252C%2520Gaussian%252C%2520and%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520regimes%252C%250Aand%2520we%2520derive%2520a%2520sampling%2520rule%2520for%2520the%2520proximal%2520update%2520that%2520ensures%2520the%2520optimal%250A%2524k%255E%257B-1/2%257D%2524%2520convergence%2520rate%2520for%2520stochastic%2520optimization.%2520Empirically%252C%2520the%250Atheory-aligned%2520schedule%2520improves%2520stability%2520and%2520sample%2520efficiency%2520on%2520common%250Acontrol%2520tasks%2520%2528e.g.%252C%2520CartPole%252C%2520Acrobot%2529%252C%2520while%2520our%2520TD-based%2520critic%2520attains%250Afavorable%2520throughput%2520versus%2520a%2520GAE%2520baseline.%2520Altogether%252C%2520our%2520results%2520place%2520PPO%250Aon%2520a%2520firmer%2520theoretical%2520footing%2520beyond%2520finite-dimensional%2520assumptions%2520and%250Aclarify%2520when%2520RKHS-proximal%2520updates%2520with%2520kernel-TD%2520critics%2520yield%2520global%2520policy%250Aimprovement%2520with%2520practical%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20Complexity%20of%20TD%20and%20PPO%20in%20RKHS&entry.906535625=Lu%20Zou%20and%20Wendi%20Ren%20and%20Weizhong%20Zhang%20and%20Liang%20Ding%20and%20Shuang%20Li&entry.1292438233=%20%20We%20revisit%20Proximal%20Policy%20Optimization%20%28PPO%29%20from%20a%20function-space%0Aperspective.%20Our%20analysis%20decouples%20policy%20evaluation%20and%20improvement%20in%20a%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%3A%20%28i%29%20A%20kernelized%20temporal-difference%0A%28TD%29%20critic%20performs%20efficient%20RKHS-gradient%20updates%20using%20only%20one-step%0Astate-action%20transition%20samples%3B%20%28ii%29%20a%20KL-regularized%2C%20natural-gradient%20policy%0Astep%20exponentiates%20the%20evaluated%20action-value%2C%20recovering%20a%20PPO/TRPO-style%0Aproximal%20update%20in%20continuous%20state-action%20spaces.%20We%20provide%20non-asymptotic%2C%0Ainstance-adaptive%20guarantees%20whose%20rates%20depend%20on%20RKHS%20entropy%2C%20unifying%0Atabular%2C%20linear%2C%20Sobolev%2C%20Gaussian%2C%20and%20Neural%20Tangent%20Kernel%20%28NTK%29%20regimes%2C%0Aand%20we%20derive%20a%20sampling%20rule%20for%20the%20proximal%20update%20that%20ensures%20the%20optimal%0A%24k%5E%7B-1/2%7D%24%20convergence%20rate%20for%20stochastic%20optimization.%20Empirically%2C%20the%0Atheory-aligned%20schedule%20improves%20stability%20and%20sample%20efficiency%20on%20common%0Acontrol%20tasks%20%28e.g.%2C%20CartPole%2C%20Acrobot%29%2C%20while%20our%20TD-based%20critic%20attains%0Afavorable%20throughput%20versus%20a%20GAE%20baseline.%20Altogether%2C%20our%20results%20place%20PPO%0Aon%20a%20firmer%20theoretical%20footing%20beyond%20finite-dimensional%20assumptions%20and%0Aclarify%20when%20RKHS-proximal%20updates%20with%20kernel-TD%20critics%20yield%20global%20policy%0Aimprovement%20with%20practical%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24991v1&entry.124074799=Read"},
{"title": "Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon\n  Annotation", "author": "Teodor Chiaburu and Vipin Singh and Frank Hau\u00dfer and Felix Bie\u00dfmann", "abstract": "  Uncertainty quantification is essential in human-machine collaboration, as\nhuman agents tend to adjust their decisions based on the confidence of the\nmachine counterpart. Reliably calibrated model uncertainties, hence, enable\nmore effective collaboration, targeted expert intervention and more responsible\nusage of Machine Learning (ML) systems. Conformal prediction has become a well\nestablished model-agnostic framework for uncertainty calibration of ML models,\noffering statistically valid confidence estimates for both regression and\nclassification tasks. In this work, we apply conformal prediction to\n$\\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.\nWe design a simulated human-in-the-loop (HIL) annotation pipeline, where a\nlimited budget for obtaining ground truth annotations from domain experts is\navailable when model uncertainty is high. Our experiments show that\nconformalizing SoilNet leads to more efficient annotation in regression tasks\nand comparable performance scores in classification tasks under the same\nannotation budget when tested against its non-conformal counterpart. All code\nand experiments can be found in our repository:\nhttps://github.com/calgo-lab/BGR\n", "link": "http://arxiv.org/abs/2509.24873v1", "date": "2025-09-29", "relevancy": 1.7526, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6064}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.583}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Guided%20Expert-AI%20Collaboration%20for%20Efficient%20Soil%20Horizon%0A%20%20Annotation&body=Title%3A%20Uncertainty-Guided%20Expert-AI%20Collaboration%20for%20Efficient%20Soil%20Horizon%0A%20%20Annotation%0AAuthor%3A%20Teodor%20Chiaburu%20and%20Vipin%20Singh%20and%20Frank%20Hau%C3%9Fer%20and%20Felix%20Bie%C3%9Fmann%0AAbstract%3A%20%20%20Uncertainty%20quantification%20is%20essential%20in%20human-machine%20collaboration%2C%20as%0Ahuman%20agents%20tend%20to%20adjust%20their%20decisions%20based%20on%20the%20confidence%20of%20the%0Amachine%20counterpart.%20Reliably%20calibrated%20model%20uncertainties%2C%20hence%2C%20enable%0Amore%20effective%20collaboration%2C%20targeted%20expert%20intervention%20and%20more%20responsible%0Ausage%20of%20Machine%20Learning%20%28ML%29%20systems.%20Conformal%20prediction%20has%20become%20a%20well%0Aestablished%20model-agnostic%20framework%20for%20uncertainty%20calibration%20of%20ML%20models%2C%0Aoffering%20statistically%20valid%20confidence%20estimates%20for%20both%20regression%20and%0Aclassification%20tasks.%20In%20this%20work%2C%20we%20apply%20conformal%20prediction%20to%0A%24%5Ctextit%7BSoilNet%7D%24%2C%20a%20multimodal%20multitask%20model%20for%20describing%20soil%20profiles.%0AWe%20design%20a%20simulated%20human-in-the-loop%20%28HIL%29%20annotation%20pipeline%2C%20where%20a%0Alimited%20budget%20for%20obtaining%20ground%20truth%20annotations%20from%20domain%20experts%20is%0Aavailable%20when%20model%20uncertainty%20is%20high.%20Our%20experiments%20show%20that%0Aconformalizing%20SoilNet%20leads%20to%20more%20efficient%20annotation%20in%20regression%20tasks%0Aand%20comparable%20performance%20scores%20in%20classification%20tasks%20under%20the%20same%0Aannotation%20budget%20when%20tested%20against%20its%20non-conformal%20counterpart.%20All%20code%0Aand%20experiments%20can%20be%20found%20in%20our%20repository%3A%0Ahttps%3A//github.com/calgo-lab/BGR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Guided%2520Expert-AI%2520Collaboration%2520for%2520Efficient%2520Soil%2520Horizon%250A%2520%2520Annotation%26entry.906535625%3DTeodor%2520Chiaburu%2520and%2520Vipin%2520Singh%2520and%2520Frank%2520Hau%25C3%259Fer%2520and%2520Felix%2520Bie%25C3%259Fmann%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520is%2520essential%2520in%2520human-machine%2520collaboration%252C%2520as%250Ahuman%2520agents%2520tend%2520to%2520adjust%2520their%2520decisions%2520based%2520on%2520the%2520confidence%2520of%2520the%250Amachine%2520counterpart.%2520Reliably%2520calibrated%2520model%2520uncertainties%252C%2520hence%252C%2520enable%250Amore%2520effective%2520collaboration%252C%2520targeted%2520expert%2520intervention%2520and%2520more%2520responsible%250Ausage%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520systems.%2520Conformal%2520prediction%2520has%2520become%2520a%2520well%250Aestablished%2520model-agnostic%2520framework%2520for%2520uncertainty%2520calibration%2520of%2520ML%2520models%252C%250Aoffering%2520statistically%2520valid%2520confidence%2520estimates%2520for%2520both%2520regression%2520and%250Aclassification%2520tasks.%2520In%2520this%2520work%252C%2520we%2520apply%2520conformal%2520prediction%2520to%250A%2524%255Ctextit%257BSoilNet%257D%2524%252C%2520a%2520multimodal%2520multitask%2520model%2520for%2520describing%2520soil%2520profiles.%250AWe%2520design%2520a%2520simulated%2520human-in-the-loop%2520%2528HIL%2529%2520annotation%2520pipeline%252C%2520where%2520a%250Alimited%2520budget%2520for%2520obtaining%2520ground%2520truth%2520annotations%2520from%2520domain%2520experts%2520is%250Aavailable%2520when%2520model%2520uncertainty%2520is%2520high.%2520Our%2520experiments%2520show%2520that%250Aconformalizing%2520SoilNet%2520leads%2520to%2520more%2520efficient%2520annotation%2520in%2520regression%2520tasks%250Aand%2520comparable%2520performance%2520scores%2520in%2520classification%2520tasks%2520under%2520the%2520same%250Aannotation%2520budget%2520when%2520tested%2520against%2520its%2520non-conformal%2520counterpart.%2520All%2520code%250Aand%2520experiments%2520can%2520be%2520found%2520in%2520our%2520repository%253A%250Ahttps%253A//github.com/calgo-lab/BGR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Guided%20Expert-AI%20Collaboration%20for%20Efficient%20Soil%20Horizon%0A%20%20Annotation&entry.906535625=Teodor%20Chiaburu%20and%20Vipin%20Singh%20and%20Frank%20Hau%C3%9Fer%20and%20Felix%20Bie%C3%9Fmann&entry.1292438233=%20%20Uncertainty%20quantification%20is%20essential%20in%20human-machine%20collaboration%2C%20as%0Ahuman%20agents%20tend%20to%20adjust%20their%20decisions%20based%20on%20the%20confidence%20of%20the%0Amachine%20counterpart.%20Reliably%20calibrated%20model%20uncertainties%2C%20hence%2C%20enable%0Amore%20effective%20collaboration%2C%20targeted%20expert%20intervention%20and%20more%20responsible%0Ausage%20of%20Machine%20Learning%20%28ML%29%20systems.%20Conformal%20prediction%20has%20become%20a%20well%0Aestablished%20model-agnostic%20framework%20for%20uncertainty%20calibration%20of%20ML%20models%2C%0Aoffering%20statistically%20valid%20confidence%20estimates%20for%20both%20regression%20and%0Aclassification%20tasks.%20In%20this%20work%2C%20we%20apply%20conformal%20prediction%20to%0A%24%5Ctextit%7BSoilNet%7D%24%2C%20a%20multimodal%20multitask%20model%20for%20describing%20soil%20profiles.%0AWe%20design%20a%20simulated%20human-in-the-loop%20%28HIL%29%20annotation%20pipeline%2C%20where%20a%0Alimited%20budget%20for%20obtaining%20ground%20truth%20annotations%20from%20domain%20experts%20is%0Aavailable%20when%20model%20uncertainty%20is%20high.%20Our%20experiments%20show%20that%0Aconformalizing%20SoilNet%20leads%20to%20more%20efficient%20annotation%20in%20regression%20tasks%0Aand%20comparable%20performance%20scores%20in%20classification%20tasks%20under%20the%20same%0Aannotation%20budget%20when%20tested%20against%20its%20non-conformal%20counterpart.%20All%20code%0Aand%20experiments%20can%20be%20found%20in%20our%20repository%3A%0Ahttps%3A//github.com/calgo-lab/BGR%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24873v1&entry.124074799=Read"},
{"title": "Spectral Collapse Drives Loss of Plasticity in Deep Continual Learning", "author": "Naicheng He and Kaicheng Guo and Arjun Prakash and Saket Tiwari and Ruo Yu Tao and Tyrone Serapio and Amy Greenwald and George Konidaris", "abstract": "  We investigate why deep neural networks suffer from loss of plasticity in\ndeep continual learning, failing to learn new tasks without reinitializing\nparameters. We show that this failure is preceded by Hessian spectral collapse\nat new-task initialization, where meaningful curvature directions vanish and\ngradient descent becomes ineffective. To characterize the necessary condition\nfor successful training, we introduce the notion of $\\tau$-trainability and\nshow that current plasticity preserving algorithms can be unified under this\nframework. Targeting spectral collapse directly, we then discuss the Kronecker\nfactored approximation of the Hessian, which motivates two regularization\nenhancements: maintaining high effective feature rank and applying L2\npenalties. Experiments on continual supervised and reinforcement learning tasks\nconfirm that combining these two regularizers effectively preserves plasticity.\n", "link": "http://arxiv.org/abs/2509.22335v2", "date": "2025-09-29", "relevancy": 1.3252, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.435}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Collapse%20Drives%20Loss%20of%20Plasticity%20in%20Deep%20Continual%20Learning&body=Title%3A%20Spectral%20Collapse%20Drives%20Loss%20of%20Plasticity%20in%20Deep%20Continual%20Learning%0AAuthor%3A%20Naicheng%20He%20and%20Kaicheng%20Guo%20and%20Arjun%20Prakash%20and%20Saket%20Tiwari%20and%20Ruo%20Yu%20Tao%20and%20Tyrone%20Serapio%20and%20Amy%20Greenwald%20and%20George%20Konidaris%0AAbstract%3A%20%20%20We%20investigate%20why%20deep%20neural%20networks%20suffer%20from%20loss%20of%20plasticity%20in%0Adeep%20continual%20learning%2C%20failing%20to%20learn%20new%20tasks%20without%20reinitializing%0Aparameters.%20We%20show%20that%20this%20failure%20is%20preceded%20by%20Hessian%20spectral%20collapse%0Aat%20new-task%20initialization%2C%20where%20meaningful%20curvature%20directions%20vanish%20and%0Agradient%20descent%20becomes%20ineffective.%20To%20characterize%20the%20necessary%20condition%0Afor%20successful%20training%2C%20we%20introduce%20the%20notion%20of%20%24%5Ctau%24-trainability%20and%0Ashow%20that%20current%20plasticity%20preserving%20algorithms%20can%20be%20unified%20under%20this%0Aframework.%20Targeting%20spectral%20collapse%20directly%2C%20we%20then%20discuss%20the%20Kronecker%0Afactored%20approximation%20of%20the%20Hessian%2C%20which%20motivates%20two%20regularization%0Aenhancements%3A%20maintaining%20high%20effective%20feature%20rank%20and%20applying%20L2%0Apenalties.%20Experiments%20on%20continual%20supervised%20and%20reinforcement%20learning%20tasks%0Aconfirm%20that%20combining%20these%20two%20regularizers%20effectively%20preserves%20plasticity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Collapse%2520Drives%2520Loss%2520of%2520Plasticity%2520in%2520Deep%2520Continual%2520Learning%26entry.906535625%3DNaicheng%2520He%2520and%2520Kaicheng%2520Guo%2520and%2520Arjun%2520Prakash%2520and%2520Saket%2520Tiwari%2520and%2520Ruo%2520Yu%2520Tao%2520and%2520Tyrone%2520Serapio%2520and%2520Amy%2520Greenwald%2520and%2520George%2520Konidaris%26entry.1292438233%3D%2520%2520We%2520investigate%2520why%2520deep%2520neural%2520networks%2520suffer%2520from%2520loss%2520of%2520plasticity%2520in%250Adeep%2520continual%2520learning%252C%2520failing%2520to%2520learn%2520new%2520tasks%2520without%2520reinitializing%250Aparameters.%2520We%2520show%2520that%2520this%2520failure%2520is%2520preceded%2520by%2520Hessian%2520spectral%2520collapse%250Aat%2520new-task%2520initialization%252C%2520where%2520meaningful%2520curvature%2520directions%2520vanish%2520and%250Agradient%2520descent%2520becomes%2520ineffective.%2520To%2520characterize%2520the%2520necessary%2520condition%250Afor%2520successful%2520training%252C%2520we%2520introduce%2520the%2520notion%2520of%2520%2524%255Ctau%2524-trainability%2520and%250Ashow%2520that%2520current%2520plasticity%2520preserving%2520algorithms%2520can%2520be%2520unified%2520under%2520this%250Aframework.%2520Targeting%2520spectral%2520collapse%2520directly%252C%2520we%2520then%2520discuss%2520the%2520Kronecker%250Afactored%2520approximation%2520of%2520the%2520Hessian%252C%2520which%2520motivates%2520two%2520regularization%250Aenhancements%253A%2520maintaining%2520high%2520effective%2520feature%2520rank%2520and%2520applying%2520L2%250Apenalties.%2520Experiments%2520on%2520continual%2520supervised%2520and%2520reinforcement%2520learning%2520tasks%250Aconfirm%2520that%2520combining%2520these%2520two%2520regularizers%2520effectively%2520preserves%2520plasticity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Collapse%20Drives%20Loss%20of%20Plasticity%20in%20Deep%20Continual%20Learning&entry.906535625=Naicheng%20He%20and%20Kaicheng%20Guo%20and%20Arjun%20Prakash%20and%20Saket%20Tiwari%20and%20Ruo%20Yu%20Tao%20and%20Tyrone%20Serapio%20and%20Amy%20Greenwald%20and%20George%20Konidaris&entry.1292438233=%20%20We%20investigate%20why%20deep%20neural%20networks%20suffer%20from%20loss%20of%20plasticity%20in%0Adeep%20continual%20learning%2C%20failing%20to%20learn%20new%20tasks%20without%20reinitializing%0Aparameters.%20We%20show%20that%20this%20failure%20is%20preceded%20by%20Hessian%20spectral%20collapse%0Aat%20new-task%20initialization%2C%20where%20meaningful%20curvature%20directions%20vanish%20and%0Agradient%20descent%20becomes%20ineffective.%20To%20characterize%20the%20necessary%20condition%0Afor%20successful%20training%2C%20we%20introduce%20the%20notion%20of%20%24%5Ctau%24-trainability%20and%0Ashow%20that%20current%20plasticity%20preserving%20algorithms%20can%20be%20unified%20under%20this%0Aframework.%20Targeting%20spectral%20collapse%20directly%2C%20we%20then%20discuss%20the%20Kronecker%0Afactored%20approximation%20of%20the%20Hessian%2C%20which%20motivates%20two%20regularization%0Aenhancements%3A%20maintaining%20high%20effective%20feature%20rank%20and%20applying%20L2%0Apenalties.%20Experiments%20on%20continual%20supervised%20and%20reinforcement%20learning%20tasks%0Aconfirm%20that%20combining%20these%20two%20regularizers%20effectively%20preserves%20plasticity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22335v2&entry.124074799=Read"},
{"title": "Score-based Membership Inference on Diffusion Models", "author": "Mingxing Rao and Bowen Qu and Daniel Moyer", "abstract": "  Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess\n", "link": "http://arxiv.org/abs/2509.25003v1", "date": "2025-09-29", "relevancy": 1.5574, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5707}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5067}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Score-based%20Membership%20Inference%20on%20Diffusion%20Models&body=Title%3A%20Score-based%20Membership%20Inference%20on%20Diffusion%20Models%0AAuthor%3A%20Mingxing%20Rao%20and%20Bowen%20Qu%20and%20Daniel%20Moyer%0AAbstract%3A%20%20%20Membership%20inference%20attacks%20%28MIAs%29%20against%20diffusion%20models%20have%20emerged%20as%0Aa%20pressing%20privacy%20concern%2C%20as%20these%20models%20may%20inadvertently%20reveal%20whether%20a%0Agiven%20sample%20was%20part%20of%20their%20training%20set.%20We%20present%20a%20theoretical%20and%0Aempirical%20study%20of%20score-based%20MIAs%2C%20focusing%20on%20the%20predicted%20noise%20vectors%0Athat%20diffusion%20models%20learn%20to%20approximate.%20We%20show%20that%20the%20expected%20denoiser%0Aoutput%20points%20toward%20a%20kernel-weighted%20local%20mean%20of%20nearby%20training%20samples%2C%0Asuch%20that%20its%20norm%20encodes%20proximity%20to%20the%20training%20set%20and%20thereby%20reveals%0Amembership.%20Building%20on%20this%20observation%2C%20we%20propose%20SimA%2C%20a%20single-query%0Aattack%20that%20provides%20a%20principled%2C%20efficient%20alternative%20to%20existing%0Amulti-query%20methods.%20SimA%20achieves%20consistently%20strong%20performance%20across%0Avariants%20of%20DDPM%2C%20Latent%20Diffusion%20Model%20%28LDM%29.%20Notably%2C%20we%20find%20that%20Latent%0ADiffusion%20Models%20are%20surprisingly%20less%20vulnerable%20than%20pixel-space%20models%2C%20due%0Ato%20the%20strong%20information%20bottleneck%20imposed%20by%20their%20latent%20auto-encoder.%20We%0Afurther%20investigate%20this%20by%20differing%20the%20regularization%20hyperparameters%0A%28%24%5Cbeta%24%20in%20%24%5Cbeta%24-VAE%29%20in%20latent%20channel%20and%20suggest%20a%20strategy%20to%20make%20LDM%0Atraining%20more%20robust%20to%20MIA.%20Our%20results%20solidify%20the%20theory%20of%20score-based%0AMIAs%2C%20while%20highlighting%20that%20Latent%20Diffusion%20class%20of%20methods%20requires%20better%0Aunderstanding%20of%20inversion%20for%20VAE%2C%20and%20not%20simply%20inversion%20of%20the%20Diffusion%0Aprocess%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScore-based%2520Membership%2520Inference%2520on%2520Diffusion%2520Models%26entry.906535625%3DMingxing%2520Rao%2520and%2520Bowen%2520Qu%2520and%2520Daniel%2520Moyer%26entry.1292438233%3D%2520%2520Membership%2520inference%2520attacks%2520%2528MIAs%2529%2520against%2520diffusion%2520models%2520have%2520emerged%2520as%250Aa%2520pressing%2520privacy%2520concern%252C%2520as%2520these%2520models%2520may%2520inadvertently%2520reveal%2520whether%2520a%250Agiven%2520sample%2520was%2520part%2520of%2520their%2520training%2520set.%2520We%2520present%2520a%2520theoretical%2520and%250Aempirical%2520study%2520of%2520score-based%2520MIAs%252C%2520focusing%2520on%2520the%2520predicted%2520noise%2520vectors%250Athat%2520diffusion%2520models%2520learn%2520to%2520approximate.%2520We%2520show%2520that%2520the%2520expected%2520denoiser%250Aoutput%2520points%2520toward%2520a%2520kernel-weighted%2520local%2520mean%2520of%2520nearby%2520training%2520samples%252C%250Asuch%2520that%2520its%2520norm%2520encodes%2520proximity%2520to%2520the%2520training%2520set%2520and%2520thereby%2520reveals%250Amembership.%2520Building%2520on%2520this%2520observation%252C%2520we%2520propose%2520SimA%252C%2520a%2520single-query%250Aattack%2520that%2520provides%2520a%2520principled%252C%2520efficient%2520alternative%2520to%2520existing%250Amulti-query%2520methods.%2520SimA%2520achieves%2520consistently%2520strong%2520performance%2520across%250Avariants%2520of%2520DDPM%252C%2520Latent%2520Diffusion%2520Model%2520%2528LDM%2529.%2520Notably%252C%2520we%2520find%2520that%2520Latent%250ADiffusion%2520Models%2520are%2520surprisingly%2520less%2520vulnerable%2520than%2520pixel-space%2520models%252C%2520due%250Ato%2520the%2520strong%2520information%2520bottleneck%2520imposed%2520by%2520their%2520latent%2520auto-encoder.%2520We%250Afurther%2520investigate%2520this%2520by%2520differing%2520the%2520regularization%2520hyperparameters%250A%2528%2524%255Cbeta%2524%2520in%2520%2524%255Cbeta%2524-VAE%2529%2520in%2520latent%2520channel%2520and%2520suggest%2520a%2520strategy%2520to%2520make%2520LDM%250Atraining%2520more%2520robust%2520to%2520MIA.%2520Our%2520results%2520solidify%2520the%2520theory%2520of%2520score-based%250AMIAs%252C%2520while%2520highlighting%2520that%2520Latent%2520Diffusion%2520class%2520of%2520methods%2520requires%2520better%250Aunderstanding%2520of%2520inversion%2520for%2520VAE%252C%2520and%2520not%2520simply%2520inversion%2520of%2520the%2520Diffusion%250Aprocess%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-based%20Membership%20Inference%20on%20Diffusion%20Models&entry.906535625=Mingxing%20Rao%20and%20Bowen%20Qu%20and%20Daniel%20Moyer&entry.1292438233=%20%20Membership%20inference%20attacks%20%28MIAs%29%20against%20diffusion%20models%20have%20emerged%20as%0Aa%20pressing%20privacy%20concern%2C%20as%20these%20models%20may%20inadvertently%20reveal%20whether%20a%0Agiven%20sample%20was%20part%20of%20their%20training%20set.%20We%20present%20a%20theoretical%20and%0Aempirical%20study%20of%20score-based%20MIAs%2C%20focusing%20on%20the%20predicted%20noise%20vectors%0Athat%20diffusion%20models%20learn%20to%20approximate.%20We%20show%20that%20the%20expected%20denoiser%0Aoutput%20points%20toward%20a%20kernel-weighted%20local%20mean%20of%20nearby%20training%20samples%2C%0Asuch%20that%20its%20norm%20encodes%20proximity%20to%20the%20training%20set%20and%20thereby%20reveals%0Amembership.%20Building%20on%20this%20observation%2C%20we%20propose%20SimA%2C%20a%20single-query%0Aattack%20that%20provides%20a%20principled%2C%20efficient%20alternative%20to%20existing%0Amulti-query%20methods.%20SimA%20achieves%20consistently%20strong%20performance%20across%0Avariants%20of%20DDPM%2C%20Latent%20Diffusion%20Model%20%28LDM%29.%20Notably%2C%20we%20find%20that%20Latent%0ADiffusion%20Models%20are%20surprisingly%20less%20vulnerable%20than%20pixel-space%20models%2C%20due%0Ato%20the%20strong%20information%20bottleneck%20imposed%20by%20their%20latent%20auto-encoder.%20We%0Afurther%20investigate%20this%20by%20differing%20the%20regularization%20hyperparameters%0A%28%24%5Cbeta%24%20in%20%24%5Cbeta%24-VAE%29%20in%20latent%20channel%20and%20suggest%20a%20strategy%20to%20make%20LDM%0Atraining%20more%20robust%20to%20MIA.%20Our%20results%20solidify%20the%20theory%20of%20score-based%0AMIAs%2C%20while%20highlighting%20that%20Latent%20Diffusion%20class%20of%20methods%20requires%20better%0Aunderstanding%20of%20inversion%20for%20VAE%2C%20and%20not%20simply%20inversion%20of%20the%20Diffusion%0Aprocess%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25003v1&entry.124074799=Read"},
{"title": "ELPG-DTFS: Prior-Guided Adaptive Time-Frequency Graph Neural Network for\n  EEG Depression Diagnosis", "author": "Jingru Qiu and Jiale Liang and Xuanhan Fan and Mingda Zhang and Zhenli He", "abstract": "  Timely and objective screening of major depressive disorder (MDD) is vital,\nyet diagnosis still relies on subjective scales. Electroencephalography (EEG)\nprovides a low-cost biomarker, but existing deep models treat spectra as static\nimages, fix inter-channel graphs, and ignore prior knowledge, limiting accuracy\nand interpretability. We propose ELPG-DTFS, a prior-guided adaptive\ntime-frequency graph neural network that introduces: (1) channel-band attention\nwith cross-band mutual information, (2) a learnable adjacency matrix for\ndynamic functional links, and (3) a residual knowledge-graph pathway injecting\nneuroscience priors. On the 128-channel MODMA dataset (53 subjects), ELPG-DTFS\nachieves 97.63% accuracy and 97.33% F1, surpassing the 2025 state-of-the-art\nACM-GNN. Ablation shows that removing any module lowers F1 by up to 4.35,\nconfirming their complementary value. ELPG-DTFS thus offers a robust and\ninterpretable framework for next-generation EEG-based MDD diagnostics.\n", "link": "http://arxiv.org/abs/2509.24860v1", "date": "2025-09-29", "relevancy": 1.9329, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5117}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4634}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELPG-DTFS%3A%20Prior-Guided%20Adaptive%20Time-Frequency%20Graph%20Neural%20Network%20for%0A%20%20EEG%20Depression%20Diagnosis&body=Title%3A%20ELPG-DTFS%3A%20Prior-Guided%20Adaptive%20Time-Frequency%20Graph%20Neural%20Network%20for%0A%20%20EEG%20Depression%20Diagnosis%0AAuthor%3A%20Jingru%20Qiu%20and%20Jiale%20Liang%20and%20Xuanhan%20Fan%20and%20Mingda%20Zhang%20and%20Zhenli%20He%0AAbstract%3A%20%20%20Timely%20and%20objective%20screening%20of%20major%20depressive%20disorder%20%28MDD%29%20is%20vital%2C%0Ayet%20diagnosis%20still%20relies%20on%20subjective%20scales.%20Electroencephalography%20%28EEG%29%0Aprovides%20a%20low-cost%20biomarker%2C%20but%20existing%20deep%20models%20treat%20spectra%20as%20static%0Aimages%2C%20fix%20inter-channel%20graphs%2C%20and%20ignore%20prior%20knowledge%2C%20limiting%20accuracy%0Aand%20interpretability.%20We%20propose%20ELPG-DTFS%2C%20a%20prior-guided%20adaptive%0Atime-frequency%20graph%20neural%20network%20that%20introduces%3A%20%281%29%20channel-band%20attention%0Awith%20cross-band%20mutual%20information%2C%20%282%29%20a%20learnable%20adjacency%20matrix%20for%0Adynamic%20functional%20links%2C%20and%20%283%29%20a%20residual%20knowledge-graph%20pathway%20injecting%0Aneuroscience%20priors.%20On%20the%20128-channel%20MODMA%20dataset%20%2853%20subjects%29%2C%20ELPG-DTFS%0Aachieves%2097.63%25%20accuracy%20and%2097.33%25%20F1%2C%20surpassing%20the%202025%20state-of-the-art%0AACM-GNN.%20Ablation%20shows%20that%20removing%20any%20module%20lowers%20F1%20by%20up%20to%204.35%2C%0Aconfirming%20their%20complementary%20value.%20ELPG-DTFS%20thus%20offers%20a%20robust%20and%0Ainterpretable%20framework%20for%20next-generation%20EEG-based%20MDD%20diagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELPG-DTFS%253A%2520Prior-Guided%2520Adaptive%2520Time-Frequency%2520Graph%2520Neural%2520Network%2520for%250A%2520%2520EEG%2520Depression%2520Diagnosis%26entry.906535625%3DJingru%2520Qiu%2520and%2520Jiale%2520Liang%2520and%2520Xuanhan%2520Fan%2520and%2520Mingda%2520Zhang%2520and%2520Zhenli%2520He%26entry.1292438233%3D%2520%2520Timely%2520and%2520objective%2520screening%2520of%2520major%2520depressive%2520disorder%2520%2528MDD%2529%2520is%2520vital%252C%250Ayet%2520diagnosis%2520still%2520relies%2520on%2520subjective%2520scales.%2520Electroencephalography%2520%2528EEG%2529%250Aprovides%2520a%2520low-cost%2520biomarker%252C%2520but%2520existing%2520deep%2520models%2520treat%2520spectra%2520as%2520static%250Aimages%252C%2520fix%2520inter-channel%2520graphs%252C%2520and%2520ignore%2520prior%2520knowledge%252C%2520limiting%2520accuracy%250Aand%2520interpretability.%2520We%2520propose%2520ELPG-DTFS%252C%2520a%2520prior-guided%2520adaptive%250Atime-frequency%2520graph%2520neural%2520network%2520that%2520introduces%253A%2520%25281%2529%2520channel-band%2520attention%250Awith%2520cross-band%2520mutual%2520information%252C%2520%25282%2529%2520a%2520learnable%2520adjacency%2520matrix%2520for%250Adynamic%2520functional%2520links%252C%2520and%2520%25283%2529%2520a%2520residual%2520knowledge-graph%2520pathway%2520injecting%250Aneuroscience%2520priors.%2520On%2520the%2520128-channel%2520MODMA%2520dataset%2520%252853%2520subjects%2529%252C%2520ELPG-DTFS%250Aachieves%252097.63%2525%2520accuracy%2520and%252097.33%2525%2520F1%252C%2520surpassing%2520the%25202025%2520state-of-the-art%250AACM-GNN.%2520Ablation%2520shows%2520that%2520removing%2520any%2520module%2520lowers%2520F1%2520by%2520up%2520to%25204.35%252C%250Aconfirming%2520their%2520complementary%2520value.%2520ELPG-DTFS%2520thus%2520offers%2520a%2520robust%2520and%250Ainterpretable%2520framework%2520for%2520next-generation%2520EEG-based%2520MDD%2520diagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELPG-DTFS%3A%20Prior-Guided%20Adaptive%20Time-Frequency%20Graph%20Neural%20Network%20for%0A%20%20EEG%20Depression%20Diagnosis&entry.906535625=Jingru%20Qiu%20and%20Jiale%20Liang%20and%20Xuanhan%20Fan%20and%20Mingda%20Zhang%20and%20Zhenli%20He&entry.1292438233=%20%20Timely%20and%20objective%20screening%20of%20major%20depressive%20disorder%20%28MDD%29%20is%20vital%2C%0Ayet%20diagnosis%20still%20relies%20on%20subjective%20scales.%20Electroencephalography%20%28EEG%29%0Aprovides%20a%20low-cost%20biomarker%2C%20but%20existing%20deep%20models%20treat%20spectra%20as%20static%0Aimages%2C%20fix%20inter-channel%20graphs%2C%20and%20ignore%20prior%20knowledge%2C%20limiting%20accuracy%0Aand%20interpretability.%20We%20propose%20ELPG-DTFS%2C%20a%20prior-guided%20adaptive%0Atime-frequency%20graph%20neural%20network%20that%20introduces%3A%20%281%29%20channel-band%20attention%0Awith%20cross-band%20mutual%20information%2C%20%282%29%20a%20learnable%20adjacency%20matrix%20for%0Adynamic%20functional%20links%2C%20and%20%283%29%20a%20residual%20knowledge-graph%20pathway%20injecting%0Aneuroscience%20priors.%20On%20the%20128-channel%20MODMA%20dataset%20%2853%20subjects%29%2C%20ELPG-DTFS%0Aachieves%2097.63%25%20accuracy%20and%2097.33%25%20F1%2C%20surpassing%20the%202025%20state-of-the-art%0AACM-GNN.%20Ablation%20shows%20that%20removing%20any%20module%20lowers%20F1%20by%20up%20to%204.35%2C%0Aconfirming%20their%20complementary%20value.%20ELPG-DTFS%20thus%20offers%20a%20robust%20and%0Ainterpretable%20framework%20for%20next-generation%20EEG-based%20MDD%20diagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24860v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


