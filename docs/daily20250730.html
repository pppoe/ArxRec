<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250729.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos", "author": "Ziren Gong and Xiaohan Li and Fabio Tosi and Jiawei Han and Stefano Mattoccia and Jianfei Cai and Matteo Poggi", "abstract": "  We present Ov3R, a novel framework for open-vocabulary semantic 3D\nreconstruction from RGB video streams, designed to advance Spatial AI. The\nsystem features two key components: CLIP3R, a CLIP-informed 3D reconstruction\nmodule that predicts dense point maps from overlapping clips while embedding\nobject-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module\nthat lifts 2D features into 3D by learning fused descriptors integrating\nspatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates\nCLIP semantics directly into the reconstruction process, enabling globally\nconsistent geometry and fine-grained semantic alignment. Our framework achieves\nstate-of-the-art performance in both dense 3D reconstruction and\nopen-vocabulary 3D segmentation, marking a step forward toward real-time,\nsemantics-aware Spatial AI.\n", "link": "http://arxiv.org/abs/2507.22052v1", "date": "2025-07-29", "relevancy": 3.2379, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ov3R%3A%20Open-Vocabulary%20Semantic%203D%20Reconstruction%20from%20RGB%20Videos&body=Title%3A%20Ov3R%3A%20Open-Vocabulary%20Semantic%203D%20Reconstruction%20from%20RGB%20Videos%0AAuthor%3A%20Ziren%20Gong%20and%20Xiaohan%20Li%20and%20Fabio%20Tosi%20and%20Jiawei%20Han%20and%20Stefano%20Mattoccia%20and%20Jianfei%20Cai%20and%20Matteo%20Poggi%0AAbstract%3A%20%20%20We%20present%20Ov3R%2C%20a%20novel%20framework%20for%20open-vocabulary%20semantic%203D%0Areconstruction%20from%20RGB%20video%20streams%2C%20designed%20to%20advance%20Spatial%20AI.%20The%0Asystem%20features%20two%20key%20components%3A%20CLIP3R%2C%20a%20CLIP-informed%203D%20reconstruction%0Amodule%20that%20predicts%20dense%20point%20maps%20from%20overlapping%20clips%20while%20embedding%0Aobject-level%20semantics%3B%20and%202D-3D%20OVS%2C%20a%202D-3D%20open-vocabulary%20semantic%20module%0Athat%20lifts%202D%20features%20into%203D%20by%20learning%20fused%20descriptors%20integrating%0Aspatial%2C%20geometric%2C%20and%20semantic%20cues.%20Unlike%20prior%20methods%2C%20Ov3R%20incorporates%0ACLIP%20semantics%20directly%20into%20the%20reconstruction%20process%2C%20enabling%20globally%0Aconsistent%20geometry%20and%20fine-grained%20semantic%20alignment.%20Our%20framework%20achieves%0Astate-of-the-art%20performance%20in%20both%20dense%203D%20reconstruction%20and%0Aopen-vocabulary%203D%20segmentation%2C%20marking%20a%20step%20forward%20toward%20real-time%2C%0Asemantics-aware%20Spatial%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOv3R%253A%2520Open-Vocabulary%2520Semantic%25203D%2520Reconstruction%2520from%2520RGB%2520Videos%26entry.906535625%3DZiren%2520Gong%2520and%2520Xiaohan%2520Li%2520and%2520Fabio%2520Tosi%2520and%2520Jiawei%2520Han%2520and%2520Stefano%2520Mattoccia%2520and%2520Jianfei%2520Cai%2520and%2520Matteo%2520Poggi%26entry.1292438233%3D%2520%2520We%2520present%2520Ov3R%252C%2520a%2520novel%2520framework%2520for%2520open-vocabulary%2520semantic%25203D%250Areconstruction%2520from%2520RGB%2520video%2520streams%252C%2520designed%2520to%2520advance%2520Spatial%2520AI.%2520The%250Asystem%2520features%2520two%2520key%2520components%253A%2520CLIP3R%252C%2520a%2520CLIP-informed%25203D%2520reconstruction%250Amodule%2520that%2520predicts%2520dense%2520point%2520maps%2520from%2520overlapping%2520clips%2520while%2520embedding%250Aobject-level%2520semantics%253B%2520and%25202D-3D%2520OVS%252C%2520a%25202D-3D%2520open-vocabulary%2520semantic%2520module%250Athat%2520lifts%25202D%2520features%2520into%25203D%2520by%2520learning%2520fused%2520descriptors%2520integrating%250Aspatial%252C%2520geometric%252C%2520and%2520semantic%2520cues.%2520Unlike%2520prior%2520methods%252C%2520Ov3R%2520incorporates%250ACLIP%2520semantics%2520directly%2520into%2520the%2520reconstruction%2520process%252C%2520enabling%2520globally%250Aconsistent%2520geometry%2520and%2520fine-grained%2520semantic%2520alignment.%2520Our%2520framework%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520dense%25203D%2520reconstruction%2520and%250Aopen-vocabulary%25203D%2520segmentation%252C%2520marking%2520a%2520step%2520forward%2520toward%2520real-time%252C%250Asemantics-aware%2520Spatial%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ov3R%3A%20Open-Vocabulary%20Semantic%203D%20Reconstruction%20from%20RGB%20Videos&entry.906535625=Ziren%20Gong%20and%20Xiaohan%20Li%20and%20Fabio%20Tosi%20and%20Jiawei%20Han%20and%20Stefano%20Mattoccia%20and%20Jianfei%20Cai%20and%20Matteo%20Poggi&entry.1292438233=%20%20We%20present%20Ov3R%2C%20a%20novel%20framework%20for%20open-vocabulary%20semantic%203D%0Areconstruction%20from%20RGB%20video%20streams%2C%20designed%20to%20advance%20Spatial%20AI.%20The%0Asystem%20features%20two%20key%20components%3A%20CLIP3R%2C%20a%20CLIP-informed%203D%20reconstruction%0Amodule%20that%20predicts%20dense%20point%20maps%20from%20overlapping%20clips%20while%20embedding%0Aobject-level%20semantics%3B%20and%202D-3D%20OVS%2C%20a%202D-3D%20open-vocabulary%20semantic%20module%0Athat%20lifts%202D%20features%20into%203D%20by%20learning%20fused%20descriptors%20integrating%0Aspatial%2C%20geometric%2C%20and%20semantic%20cues.%20Unlike%20prior%20methods%2C%20Ov3R%20incorporates%0ACLIP%20semantics%20directly%20into%20the%20reconstruction%20process%2C%20enabling%20globally%0Aconsistent%20geometry%20and%20fine-grained%20semantic%20alignment.%20Our%20framework%20achieves%0Astate-of-the-art%20performance%20in%20both%20dense%203D%20reconstruction%20and%0Aopen-vocabulary%203D%20segmentation%2C%20marking%20a%20step%20forward%20toward%20real-time%2C%0Asemantics-aware%20Spatial%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22052v1&entry.124074799=Read"},
{"title": "Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT\n  Images", "author": "Yutao Hu and Ying Zheng and Shumei Miao and Xiaolei Zhang and Jiahao Xia and Yaolei Qi and Yiyang Zhang and Yuting He and Qian Chen and Jing Ye and Hongyan Qiao and Xiuhua Hu and Lei Xu and Jiayin Zhang and Hui Liu and Minwen Zheng and Yining Wang and Daimin Zhang and Ji Zhang and Wenqi Shao and Yun Liu and Longjiang Zhang and Guanyu Yang", "abstract": "  Foundation models have demonstrated remarkable potential in medical domain.\nHowever, their application to complex cardiovascular diagnostics remains\nunderexplored. In this paper, we present Cardiac-CLIP, a multi-modal foundation\nmodel designed for 3D cardiac CT images. Cardiac-CLIP is developed through a\ntwo-stage pre-training strategy. The first stage employs a 3D masked\nautoencoder (MAE) to perform self-supervised representation learning from\nlarge-scale unlabeled volumetric data, enabling the visual encoder to capture\nrich anatomical and contextual features. In the second stage, contrastive\nlearning is introduced to align visual and textual representations,\nfacilitating cross-modal understanding. To support the pre-training, we collect\n16641 real clinical CT scans, supplemented by 114k publicly available data.\nMeanwhile, we standardize free-text radiology reports into unified templates\nand construct the pathology vectors according to diagnostic attributes, based\non which the soft-label matrix is generated to supervise the contrastive\nlearning process. On the other hand, to comprehensively evaluate the\neffectiveness of Cardiac-CLIP, we collect 6,722 real-clinical data from 12\nindependent institutions, along with the open-source data to construct the\nevaluation dataset. Specifically, Cardiac-CLIP is comprehensively evaluated\nacross multiple tasks, including cardiovascular abnormality classification,\ninformation retrieval and clinical analysis. Experimental results demonstrate\nthat Cardiac-CLIP achieves state-of-the-art performance across various\ndownstream tasks in both internal and external data. Particularly, Cardiac-CLIP\nexhibits great effectiveness in supporting complex clinical tasks such as the\nprospective prediction of acute coronary syndrome, which is notoriously\ndifficult in real-world scenarios.\n", "link": "http://arxiv.org/abs/2507.22024v1", "date": "2025-07-29", "relevancy": 3.2279, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cardiac-CLIP%3A%20A%20Vision-Language%20Foundation%20Model%20for%203D%20Cardiac%20CT%0A%20%20Images&body=Title%3A%20Cardiac-CLIP%3A%20A%20Vision-Language%20Foundation%20Model%20for%203D%20Cardiac%20CT%0A%20%20Images%0AAuthor%3A%20Yutao%20Hu%20and%20Ying%20Zheng%20and%20Shumei%20Miao%20and%20Xiaolei%20Zhang%20and%20Jiahao%20Xia%20and%20Yaolei%20Qi%20and%20Yiyang%20Zhang%20and%20Yuting%20He%20and%20Qian%20Chen%20and%20Jing%20Ye%20and%20Hongyan%20Qiao%20and%20Xiuhua%20Hu%20and%20Lei%20Xu%20and%20Jiayin%20Zhang%20and%20Hui%20Liu%20and%20Minwen%20Zheng%20and%20Yining%20Wang%20and%20Daimin%20Zhang%20and%20Ji%20Zhang%20and%20Wenqi%20Shao%20and%20Yun%20Liu%20and%20Longjiang%20Zhang%20and%20Guanyu%20Yang%0AAbstract%3A%20%20%20Foundation%20models%20have%20demonstrated%20remarkable%20potential%20in%20medical%20domain.%0AHowever%2C%20their%20application%20to%20complex%20cardiovascular%20diagnostics%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20present%20Cardiac-CLIP%2C%20a%20multi-modal%20foundation%0Amodel%20designed%20for%203D%20cardiac%20CT%20images.%20Cardiac-CLIP%20is%20developed%20through%20a%0Atwo-stage%20pre-training%20strategy.%20The%20first%20stage%20employs%20a%203D%20masked%0Aautoencoder%20%28MAE%29%20to%20perform%20self-supervised%20representation%20learning%20from%0Alarge-scale%20unlabeled%20volumetric%20data%2C%20enabling%20the%20visual%20encoder%20to%20capture%0Arich%20anatomical%20and%20contextual%20features.%20In%20the%20second%20stage%2C%20contrastive%0Alearning%20is%20introduced%20to%20align%20visual%20and%20textual%20representations%2C%0Afacilitating%20cross-modal%20understanding.%20To%20support%20the%20pre-training%2C%20we%20collect%0A16641%20real%20clinical%20CT%20scans%2C%20supplemented%20by%20114k%20publicly%20available%20data.%0AMeanwhile%2C%20we%20standardize%20free-text%20radiology%20reports%20into%20unified%20templates%0Aand%20construct%20the%20pathology%20vectors%20according%20to%20diagnostic%20attributes%2C%20based%0Aon%20which%20the%20soft-label%20matrix%20is%20generated%20to%20supervise%20the%20contrastive%0Alearning%20process.%20On%20the%20other%20hand%2C%20to%20comprehensively%20evaluate%20the%0Aeffectiveness%20of%20Cardiac-CLIP%2C%20we%20collect%206%2C722%20real-clinical%20data%20from%2012%0Aindependent%20institutions%2C%20along%20with%20the%20open-source%20data%20to%20construct%20the%0Aevaluation%20dataset.%20Specifically%2C%20Cardiac-CLIP%20is%20comprehensively%20evaluated%0Aacross%20multiple%20tasks%2C%20including%20cardiovascular%20abnormality%20classification%2C%0Ainformation%20retrieval%20and%20clinical%20analysis.%20Experimental%20results%20demonstrate%0Athat%20Cardiac-CLIP%20achieves%20state-of-the-art%20performance%20across%20various%0Adownstream%20tasks%20in%20both%20internal%20and%20external%20data.%20Particularly%2C%20Cardiac-CLIP%0Aexhibits%20great%20effectiveness%20in%20supporting%20complex%20clinical%20tasks%20such%20as%20the%0Aprospective%20prediction%20of%20acute%20coronary%20syndrome%2C%20which%20is%20notoriously%0Adifficult%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardiac-CLIP%253A%2520A%2520Vision-Language%2520Foundation%2520Model%2520for%25203D%2520Cardiac%2520CT%250A%2520%2520Images%26entry.906535625%3DYutao%2520Hu%2520and%2520Ying%2520Zheng%2520and%2520Shumei%2520Miao%2520and%2520Xiaolei%2520Zhang%2520and%2520Jiahao%2520Xia%2520and%2520Yaolei%2520Qi%2520and%2520Yiyang%2520Zhang%2520and%2520Yuting%2520He%2520and%2520Qian%2520Chen%2520and%2520Jing%2520Ye%2520and%2520Hongyan%2520Qiao%2520and%2520Xiuhua%2520Hu%2520and%2520Lei%2520Xu%2520and%2520Jiayin%2520Zhang%2520and%2520Hui%2520Liu%2520and%2520Minwen%2520Zheng%2520and%2520Yining%2520Wang%2520and%2520Daimin%2520Zhang%2520and%2520Ji%2520Zhang%2520and%2520Wenqi%2520Shao%2520and%2520Yun%2520Liu%2520and%2520Longjiang%2520Zhang%2520and%2520Guanyu%2520Yang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520demonstrated%2520remarkable%2520potential%2520in%2520medical%2520domain.%250AHowever%252C%2520their%2520application%2520to%2520complex%2520cardiovascular%2520diagnostics%2520remains%250Aunderexplored.%2520In%2520this%2520paper%252C%2520we%2520present%2520Cardiac-CLIP%252C%2520a%2520multi-modal%2520foundation%250Amodel%2520designed%2520for%25203D%2520cardiac%2520CT%2520images.%2520Cardiac-CLIP%2520is%2520developed%2520through%2520a%250Atwo-stage%2520pre-training%2520strategy.%2520The%2520first%2520stage%2520employs%2520a%25203D%2520masked%250Aautoencoder%2520%2528MAE%2529%2520to%2520perform%2520self-supervised%2520representation%2520learning%2520from%250Alarge-scale%2520unlabeled%2520volumetric%2520data%252C%2520enabling%2520the%2520visual%2520encoder%2520to%2520capture%250Arich%2520anatomical%2520and%2520contextual%2520features.%2520In%2520the%2520second%2520stage%252C%2520contrastive%250Alearning%2520is%2520introduced%2520to%2520align%2520visual%2520and%2520textual%2520representations%252C%250Afacilitating%2520cross-modal%2520understanding.%2520To%2520support%2520the%2520pre-training%252C%2520we%2520collect%250A16641%2520real%2520clinical%2520CT%2520scans%252C%2520supplemented%2520by%2520114k%2520publicly%2520available%2520data.%250AMeanwhile%252C%2520we%2520standardize%2520free-text%2520radiology%2520reports%2520into%2520unified%2520templates%250Aand%2520construct%2520the%2520pathology%2520vectors%2520according%2520to%2520diagnostic%2520attributes%252C%2520based%250Aon%2520which%2520the%2520soft-label%2520matrix%2520is%2520generated%2520to%2520supervise%2520the%2520contrastive%250Alearning%2520process.%2520On%2520the%2520other%2520hand%252C%2520to%2520comprehensively%2520evaluate%2520the%250Aeffectiveness%2520of%2520Cardiac-CLIP%252C%2520we%2520collect%25206%252C722%2520real-clinical%2520data%2520from%252012%250Aindependent%2520institutions%252C%2520along%2520with%2520the%2520open-source%2520data%2520to%2520construct%2520the%250Aevaluation%2520dataset.%2520Specifically%252C%2520Cardiac-CLIP%2520is%2520comprehensively%2520evaluated%250Aacross%2520multiple%2520tasks%252C%2520including%2520cardiovascular%2520abnormality%2520classification%252C%250Ainformation%2520retrieval%2520and%2520clinical%2520analysis.%2520Experimental%2520results%2520demonstrate%250Athat%2520Cardiac-CLIP%2520achieves%2520state-of-the-art%2520performance%2520across%2520various%250Adownstream%2520tasks%2520in%2520both%2520internal%2520and%2520external%2520data.%2520Particularly%252C%2520Cardiac-CLIP%250Aexhibits%2520great%2520effectiveness%2520in%2520supporting%2520complex%2520clinical%2520tasks%2520such%2520as%2520the%250Aprospective%2520prediction%2520of%2520acute%2520coronary%2520syndrome%252C%2520which%2520is%2520notoriously%250Adifficult%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cardiac-CLIP%3A%20A%20Vision-Language%20Foundation%20Model%20for%203D%20Cardiac%20CT%0A%20%20Images&entry.906535625=Yutao%20Hu%20and%20Ying%20Zheng%20and%20Shumei%20Miao%20and%20Xiaolei%20Zhang%20and%20Jiahao%20Xia%20and%20Yaolei%20Qi%20and%20Yiyang%20Zhang%20and%20Yuting%20He%20and%20Qian%20Chen%20and%20Jing%20Ye%20and%20Hongyan%20Qiao%20and%20Xiuhua%20Hu%20and%20Lei%20Xu%20and%20Jiayin%20Zhang%20and%20Hui%20Liu%20and%20Minwen%20Zheng%20and%20Yining%20Wang%20and%20Daimin%20Zhang%20and%20Ji%20Zhang%20and%20Wenqi%20Shao%20and%20Yun%20Liu%20and%20Longjiang%20Zhang%20and%20Guanyu%20Yang&entry.1292438233=%20%20Foundation%20models%20have%20demonstrated%20remarkable%20potential%20in%20medical%20domain.%0AHowever%2C%20their%20application%20to%20complex%20cardiovascular%20diagnostics%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20present%20Cardiac-CLIP%2C%20a%20multi-modal%20foundation%0Amodel%20designed%20for%203D%20cardiac%20CT%20images.%20Cardiac-CLIP%20is%20developed%20through%20a%0Atwo-stage%20pre-training%20strategy.%20The%20first%20stage%20employs%20a%203D%20masked%0Aautoencoder%20%28MAE%29%20to%20perform%20self-supervised%20representation%20learning%20from%0Alarge-scale%20unlabeled%20volumetric%20data%2C%20enabling%20the%20visual%20encoder%20to%20capture%0Arich%20anatomical%20and%20contextual%20features.%20In%20the%20second%20stage%2C%20contrastive%0Alearning%20is%20introduced%20to%20align%20visual%20and%20textual%20representations%2C%0Afacilitating%20cross-modal%20understanding.%20To%20support%20the%20pre-training%2C%20we%20collect%0A16641%20real%20clinical%20CT%20scans%2C%20supplemented%20by%20114k%20publicly%20available%20data.%0AMeanwhile%2C%20we%20standardize%20free-text%20radiology%20reports%20into%20unified%20templates%0Aand%20construct%20the%20pathology%20vectors%20according%20to%20diagnostic%20attributes%2C%20based%0Aon%20which%20the%20soft-label%20matrix%20is%20generated%20to%20supervise%20the%20contrastive%0Alearning%20process.%20On%20the%20other%20hand%2C%20to%20comprehensively%20evaluate%20the%0Aeffectiveness%20of%20Cardiac-CLIP%2C%20we%20collect%206%2C722%20real-clinical%20data%20from%2012%0Aindependent%20institutions%2C%20along%20with%20the%20open-source%20data%20to%20construct%20the%0Aevaluation%20dataset.%20Specifically%2C%20Cardiac-CLIP%20is%20comprehensively%20evaluated%0Aacross%20multiple%20tasks%2C%20including%20cardiovascular%20abnormality%20classification%2C%0Ainformation%20retrieval%20and%20clinical%20analysis.%20Experimental%20results%20demonstrate%0Athat%20Cardiac-CLIP%20achieves%20state-of-the-art%20performance%20across%20various%0Adownstream%20tasks%20in%20both%20internal%20and%20external%20data.%20Particularly%2C%20Cardiac-CLIP%0Aexhibits%20great%20effectiveness%20in%20supporting%20complex%20clinical%20tasks%20such%20as%20the%0Aprospective%20prediction%20of%20acute%20coronary%20syndrome%2C%20which%20is%20notoriously%0Adifficult%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22024v1&entry.124074799=Read"},
{"title": "VeS: Teaching Pixels to Listen Without Supervision", "author": "Sajay Raj", "abstract": "  Recent dense audio-visual (AV) models achieve impressive retrieval and\nemergent localization, but almost all evidence comes from English-centric,\ncaption-rich web video. It is unclear whether these objectives survive in\nlow-resource, code-switched, and noisy multilingual settings that typify\ndeveloping regions. We show they do**-**and that the choice of aggregation\nfunction becomes even more critical. Using a multilingual subset of Project\nVaani spanning dozens of Indian languages and dialectal variants, we compare\nthree contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)\na dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid\n(motivated by frozen-vision alignment strategies). The dense objective delivers\na +59% relative R@1 (Audio Visual) improvement over global pooling and\nsubstantially lower mean/median ranks, while consistently producing sharp\nzero-shot localization heatmaps of spoken objects-despite keeping the vision\nbackbone entirely frozen (no LoRA / partial fine-tuning). Our results\ndemonstrate that dense token routing is not a luxury of high-resource English\ncorpora; it is more decisive when annotations and acoustic cleanliness are\nscarce. We release the codebase and trained models.\n", "link": "http://arxiv.org/abs/2507.22008v1", "date": "2025-07-29", "relevancy": 2.9759, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6103}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeS%3A%20Teaching%20Pixels%20to%20Listen%20Without%20Supervision&body=Title%3A%20VeS%3A%20Teaching%20Pixels%20to%20Listen%20Without%20Supervision%0AAuthor%3A%20Sajay%20Raj%0AAbstract%3A%20%20%20Recent%20dense%20audio-visual%20%28AV%29%20models%20achieve%20impressive%20retrieval%20and%0Aemergent%20localization%2C%20but%20almost%20all%20evidence%20comes%20from%20English-centric%2C%0Acaption-rich%20web%20video.%20It%20is%20unclear%20whether%20these%20objectives%20survive%20in%0Alow-resource%2C%20code-switched%2C%20and%20noisy%20multilingual%20settings%20that%20typify%0Adeveloping%20regions.%20We%20show%20they%20do%2A%2A-%2A%2Aand%20that%20the%20choice%20of%20aggregation%0Afunction%20becomes%20even%20more%20critical.%20Using%20a%20multilingual%20subset%20of%20Project%0AVaani%20spanning%20dozens%20of%20Indian%20languages%20and%20dialectal%20variants%2C%20we%20compare%0Athree%20contrastive%20objectives%3A%20%28i%29%20a%20global%20mean-pooled%20loss%20%28CLIP-style%29%2C%20%28ii%29%0Aa%20dense%20max-mean%20token%20matcher%20%28DenseAV-style%29%2C%20and%20%28iii%29%20a%20simple%20hybrid%0A%28motivated%20by%20frozen-vision%20alignment%20strategies%29.%20The%20dense%20objective%20delivers%0Aa%20%2B59%25%20relative%20R%401%20%28Audio%20Visual%29%20improvement%20over%20global%20pooling%20and%0Asubstantially%20lower%20mean/median%20ranks%2C%20while%20consistently%20producing%20sharp%0Azero-shot%20localization%20heatmaps%20of%20spoken%20objects-despite%20keeping%20the%20vision%0Abackbone%20entirely%20frozen%20%28no%20LoRA%20/%20partial%20fine-tuning%29.%20Our%20results%0Ademonstrate%20that%20dense%20token%20routing%20is%20not%20a%20luxury%20of%20high-resource%20English%0Acorpora%3B%20it%20is%20more%20decisive%20when%20annotations%20and%20acoustic%20cleanliness%20are%0Ascarce.%20We%20release%20the%20codebase%20and%20trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeS%253A%2520Teaching%2520Pixels%2520to%2520Listen%2520Without%2520Supervision%26entry.906535625%3DSajay%2520Raj%26entry.1292438233%3D%2520%2520Recent%2520dense%2520audio-visual%2520%2528AV%2529%2520models%2520achieve%2520impressive%2520retrieval%2520and%250Aemergent%2520localization%252C%2520but%2520almost%2520all%2520evidence%2520comes%2520from%2520English-centric%252C%250Acaption-rich%2520web%2520video.%2520It%2520is%2520unclear%2520whether%2520these%2520objectives%2520survive%2520in%250Alow-resource%252C%2520code-switched%252C%2520and%2520noisy%2520multilingual%2520settings%2520that%2520typify%250Adeveloping%2520regions.%2520We%2520show%2520they%2520do%252A%252A-%252A%252Aand%2520that%2520the%2520choice%2520of%2520aggregation%250Afunction%2520becomes%2520even%2520more%2520critical.%2520Using%2520a%2520multilingual%2520subset%2520of%2520Project%250AVaani%2520spanning%2520dozens%2520of%2520Indian%2520languages%2520and%2520dialectal%2520variants%252C%2520we%2520compare%250Athree%2520contrastive%2520objectives%253A%2520%2528i%2529%2520a%2520global%2520mean-pooled%2520loss%2520%2528CLIP-style%2529%252C%2520%2528ii%2529%250Aa%2520dense%2520max-mean%2520token%2520matcher%2520%2528DenseAV-style%2529%252C%2520and%2520%2528iii%2529%2520a%2520simple%2520hybrid%250A%2528motivated%2520by%2520frozen-vision%2520alignment%2520strategies%2529.%2520The%2520dense%2520objective%2520delivers%250Aa%2520%252B59%2525%2520relative%2520R%25401%2520%2528Audio%2520Visual%2529%2520improvement%2520over%2520global%2520pooling%2520and%250Asubstantially%2520lower%2520mean/median%2520ranks%252C%2520while%2520consistently%2520producing%2520sharp%250Azero-shot%2520localization%2520heatmaps%2520of%2520spoken%2520objects-despite%2520keeping%2520the%2520vision%250Abackbone%2520entirely%2520frozen%2520%2528no%2520LoRA%2520/%2520partial%2520fine-tuning%2529.%2520Our%2520results%250Ademonstrate%2520that%2520dense%2520token%2520routing%2520is%2520not%2520a%2520luxury%2520of%2520high-resource%2520English%250Acorpora%253B%2520it%2520is%2520more%2520decisive%2520when%2520annotations%2520and%2520acoustic%2520cleanliness%2520are%250Ascarce.%2520We%2520release%2520the%2520codebase%2520and%2520trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeS%3A%20Teaching%20Pixels%20to%20Listen%20Without%20Supervision&entry.906535625=Sajay%20Raj&entry.1292438233=%20%20Recent%20dense%20audio-visual%20%28AV%29%20models%20achieve%20impressive%20retrieval%20and%0Aemergent%20localization%2C%20but%20almost%20all%20evidence%20comes%20from%20English-centric%2C%0Acaption-rich%20web%20video.%20It%20is%20unclear%20whether%20these%20objectives%20survive%20in%0Alow-resource%2C%20code-switched%2C%20and%20noisy%20multilingual%20settings%20that%20typify%0Adeveloping%20regions.%20We%20show%20they%20do%2A%2A-%2A%2Aand%20that%20the%20choice%20of%20aggregation%0Afunction%20becomes%20even%20more%20critical.%20Using%20a%20multilingual%20subset%20of%20Project%0AVaani%20spanning%20dozens%20of%20Indian%20languages%20and%20dialectal%20variants%2C%20we%20compare%0Athree%20contrastive%20objectives%3A%20%28i%29%20a%20global%20mean-pooled%20loss%20%28CLIP-style%29%2C%20%28ii%29%0Aa%20dense%20max-mean%20token%20matcher%20%28DenseAV-style%29%2C%20and%20%28iii%29%20a%20simple%20hybrid%0A%28motivated%20by%20frozen-vision%20alignment%20strategies%29.%20The%20dense%20objective%20delivers%0Aa%20%2B59%25%20relative%20R%401%20%28Audio%20Visual%29%20improvement%20over%20global%20pooling%20and%0Asubstantially%20lower%20mean/median%20ranks%2C%20while%20consistently%20producing%20sharp%0Azero-shot%20localization%20heatmaps%20of%20spoken%20objects-despite%20keeping%20the%20vision%0Abackbone%20entirely%20frozen%20%28no%20LoRA%20/%20partial%20fine-tuning%29.%20Our%20results%0Ademonstrate%20that%20dense%20token%20routing%20is%20not%20a%20luxury%20of%20high-resource%20English%0Acorpora%3B%20it%20is%20more%20decisive%20when%20annotations%20and%20acoustic%20cleanliness%20are%0Ascarce.%20We%20release%20the%20codebase%20and%20trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22008v1&entry.124074799=Read"},
{"title": "GLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language\n  Models", "author": "Guanxi Shen", "abstract": "  Recent large vision-language models (LVLMs) have advanced capabilities in\nvisual question answering (VQA). However, interpreting where LVLMs direct their\nvisual attention remains a significant challenge, yet is essential for\nunderstanding model behavior. We introduce GLIMPSE (Gradient-Layer Importance\nMapping for Prompted Visual Saliency Explanation), a lightweight,\nmodel-agnostic framework that jointly attributes LVLM outputs to the most\nrelevant visual evidence and textual signals that support open-ended\ngeneration. GLIMPSE fuses gradient-weighted attention, adaptive layer\npropagation, and relevance-weighted token aggregation to produce holistic\nresponse-level heat maps for interpreting cross-modal reasoning, outperforming\nprior methods in faithfulness and pushing the state-of-the-art in\nhuman-attention alignment. We demonstrate an analytic approach to uncover\nfine-grained insights into LVLM cross-modal attribution, trace reasoning\ndynamics, analyze systematic misalignment, diagnose hallucination and bias, and\nensure transparency.\n", "link": "http://arxiv.org/abs/2506.18985v3", "date": "2025-07-29", "relevancy": 2.927, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLIMPSE%3A%20Holistic%20Cross-Modal%20Explainability%20for%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20GLIMPSE%3A%20Holistic%20Cross-Modal%20Explainability%20for%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Guanxi%20Shen%0AAbstract%3A%20%20%20Recent%20large%20vision-language%20models%20%28LVLMs%29%20have%20advanced%20capabilities%20in%0Avisual%20question%20answering%20%28VQA%29.%20However%2C%20interpreting%20where%20LVLMs%20direct%20their%0Avisual%20attention%20remains%20a%20significant%20challenge%2C%20yet%20is%20essential%20for%0Aunderstanding%20model%20behavior.%20We%20introduce%20GLIMPSE%20%28Gradient-Layer%20Importance%0AMapping%20for%20Prompted%20Visual%20Saliency%20Explanation%29%2C%20a%20lightweight%2C%0Amodel-agnostic%20framework%20that%20jointly%20attributes%20LVLM%20outputs%20to%20the%20most%0Arelevant%20visual%20evidence%20and%20textual%20signals%20that%20support%20open-ended%0Ageneration.%20GLIMPSE%20fuses%20gradient-weighted%20attention%2C%20adaptive%20layer%0Apropagation%2C%20and%20relevance-weighted%20token%20aggregation%20to%20produce%20holistic%0Aresponse-level%20heat%20maps%20for%20interpreting%20cross-modal%20reasoning%2C%20outperforming%0Aprior%20methods%20in%20faithfulness%20and%20pushing%20the%20state-of-the-art%20in%0Ahuman-attention%20alignment.%20We%20demonstrate%20an%20analytic%20approach%20to%20uncover%0Afine-grained%20insights%20into%20LVLM%20cross-modal%20attribution%2C%20trace%20reasoning%0Adynamics%2C%20analyze%20systematic%20misalignment%2C%20diagnose%20hallucination%20and%20bias%2C%20and%0Aensure%20transparency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18985v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLIMPSE%253A%2520Holistic%2520Cross-Modal%2520Explainability%2520for%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DGuanxi%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520advanced%2520capabilities%2520in%250Avisual%2520question%2520answering%2520%2528VQA%2529.%2520However%252C%2520interpreting%2520where%2520LVLMs%2520direct%2520their%250Avisual%2520attention%2520remains%2520a%2520significant%2520challenge%252C%2520yet%2520is%2520essential%2520for%250Aunderstanding%2520model%2520behavior.%2520We%2520introduce%2520GLIMPSE%2520%2528Gradient-Layer%2520Importance%250AMapping%2520for%2520Prompted%2520Visual%2520Saliency%2520Explanation%2529%252C%2520a%2520lightweight%252C%250Amodel-agnostic%2520framework%2520that%2520jointly%2520attributes%2520LVLM%2520outputs%2520to%2520the%2520most%250Arelevant%2520visual%2520evidence%2520and%2520textual%2520signals%2520that%2520support%2520open-ended%250Ageneration.%2520GLIMPSE%2520fuses%2520gradient-weighted%2520attention%252C%2520adaptive%2520layer%250Apropagation%252C%2520and%2520relevance-weighted%2520token%2520aggregation%2520to%2520produce%2520holistic%250Aresponse-level%2520heat%2520maps%2520for%2520interpreting%2520cross-modal%2520reasoning%252C%2520outperforming%250Aprior%2520methods%2520in%2520faithfulness%2520and%2520pushing%2520the%2520state-of-the-art%2520in%250Ahuman-attention%2520alignment.%2520We%2520demonstrate%2520an%2520analytic%2520approach%2520to%2520uncover%250Afine-grained%2520insights%2520into%2520LVLM%2520cross-modal%2520attribution%252C%2520trace%2520reasoning%250Adynamics%252C%2520analyze%2520systematic%2520misalignment%252C%2520diagnose%2520hallucination%2520and%2520bias%252C%2520and%250Aensure%2520transparency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18985v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLIMPSE%3A%20Holistic%20Cross-Modal%20Explainability%20for%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Guanxi%20Shen&entry.1292438233=%20%20Recent%20large%20vision-language%20models%20%28LVLMs%29%20have%20advanced%20capabilities%20in%0Avisual%20question%20answering%20%28VQA%29.%20However%2C%20interpreting%20where%20LVLMs%20direct%20their%0Avisual%20attention%20remains%20a%20significant%20challenge%2C%20yet%20is%20essential%20for%0Aunderstanding%20model%20behavior.%20We%20introduce%20GLIMPSE%20%28Gradient-Layer%20Importance%0AMapping%20for%20Prompted%20Visual%20Saliency%20Explanation%29%2C%20a%20lightweight%2C%0Amodel-agnostic%20framework%20that%20jointly%20attributes%20LVLM%20outputs%20to%20the%20most%0Arelevant%20visual%20evidence%20and%20textual%20signals%20that%20support%20open-ended%0Ageneration.%20GLIMPSE%20fuses%20gradient-weighted%20attention%2C%20adaptive%20layer%0Apropagation%2C%20and%20relevance-weighted%20token%20aggregation%20to%20produce%20holistic%0Aresponse-level%20heat%20maps%20for%20interpreting%20cross-modal%20reasoning%2C%20outperforming%0Aprior%20methods%20in%20faithfulness%20and%20pushing%20the%20state-of-the-art%20in%0Ahuman-attention%20alignment.%20We%20demonstrate%20an%20analytic%20approach%20to%20uncover%0Afine-grained%20insights%20into%20LVLM%20cross-modal%20attribution%2C%20trace%20reasoning%0Adynamics%2C%20analyze%20systematic%20misalignment%2C%20diagnose%20hallucination%20and%20bias%2C%20and%0Aensure%20transparency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18985v3&entry.124074799=Read"},
{"title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long\n  Video Generation Models", "author": "X. Feng and H. Yu and M. Wu and S. Hu and J. Chen and C. Zhu and J. Wu and X. Chu and K. Huang", "abstract": "  With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.\n", "link": "http://arxiv.org/abs/2507.11245v2", "date": "2025-07-29", "relevancy": 2.7232, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NarrLV%3A%20Towards%20a%20Comprehensive%20Narrative-Centric%20Evaluation%20for%20Long%0A%20%20Video%20Generation%20Models&body=Title%3A%20NarrLV%3A%20Towards%20a%20Comprehensive%20Narrative-Centric%20Evaluation%20for%20Long%0A%20%20Video%20Generation%20Models%0AAuthor%3A%20X.%20Feng%20and%20H.%20Yu%20and%20M.%20Wu%20and%20S.%20Hu%20and%20J.%20Chen%20and%20C.%20Zhu%20and%20J.%20Wu%20and%20X.%20Chu%20and%20K.%20Huang%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20foundation%20video%20generation%20technologies%2C%20long%0Avideo%20generation%20models%20have%20exhibited%20promising%20research%20potential%20thanks%20to%0Aexpanded%20content%20creation%20space.%20Recent%20studies%20reveal%20that%20the%20goal%20of%20long%0Avideo%20generation%20tasks%20is%20not%20only%20to%20extend%20video%20duration%20but%20also%20to%0Aaccurately%20express%20richer%20narrative%20content%20within%20longer%20videos.%20However%2C%20due%0Ato%20the%20lack%20of%20evaluation%20benchmarks%20specifically%20designed%20for%20long%20video%0Ageneration%20models%2C%20the%20current%20assessment%20of%20these%20models%20primarily%20relies%20on%0Abenchmarks%20with%20simple%20narrative%20prompts%20%28e.g.%2C%20VBench%29.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20proposed%20NarrLV%20is%20the%20first%20benchmark%20to%20comprehensively%0Aevaluate%20the%20Narrative%20expression%20capabilities%20of%20Long%20Video%20generation%20models.%0AInspired%20by%20film%20narrative%20theory%2C%20%28i%29%20we%20first%20introduce%20the%20basic%20narrative%0Aunit%20maintaining%20continuous%20visual%20presentation%20in%20videos%20as%20Temporal%20Narrative%0AAtom%20%28TNA%29%2C%20and%20use%20its%20count%20to%20quantitatively%20measure%20narrative%20richness.%0AGuided%20by%20three%20key%20film%20narrative%20elements%20influencing%20TNA%20changes%2C%20we%0Aconstruct%20an%20automatic%20prompt%20generation%20pipeline%20capable%20of%20producing%0Aevaluation%20prompts%20with%20a%20flexibly%20expandable%20number%20of%20TNAs.%20%28ii%29%20Then%2C%20based%0Aon%20the%20three%20progressive%20levels%20of%20narrative%20content%20expression%2C%20we%20design%20an%0Aeffective%20evaluation%20metric%20using%20the%20MLLM-based%20question%20generation%20and%0Aanswering%20framework.%20%28iii%29%20Finally%2C%20we%20conduct%20extensive%20evaluations%20on%0Aexisting%20long%20video%20generation%20models%20and%20the%20foundation%20generation%20models.%0AExperimental%20results%20demonstrate%20that%20our%20metric%20aligns%20closely%20with%20human%0Ajudgments.%20The%20derived%20evaluation%20outcomes%20reveal%20the%20detailed%20capability%0Aboundaries%20of%20current%20video%20generation%20models%20in%20narrative%20content%20expression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNarrLV%253A%2520Towards%2520a%2520Comprehensive%2520Narrative-Centric%2520Evaluation%2520for%2520Long%250A%2520%2520Video%2520Generation%2520Models%26entry.906535625%3DX.%2520Feng%2520and%2520H.%2520Yu%2520and%2520M.%2520Wu%2520and%2520S.%2520Hu%2520and%2520J.%2520Chen%2520and%2520C.%2520Zhu%2520and%2520J.%2520Wu%2520and%2520X.%2520Chu%2520and%2520K.%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520foundation%2520video%2520generation%2520technologies%252C%2520long%250Avideo%2520generation%2520models%2520have%2520exhibited%2520promising%2520research%2520potential%2520thanks%2520to%250Aexpanded%2520content%2520creation%2520space.%2520Recent%2520studies%2520reveal%2520that%2520the%2520goal%2520of%2520long%250Avideo%2520generation%2520tasks%2520is%2520not%2520only%2520to%2520extend%2520video%2520duration%2520but%2520also%2520to%250Aaccurately%2520express%2520richer%2520narrative%2520content%2520within%2520longer%2520videos.%2520However%252C%2520due%250Ato%2520the%2520lack%2520of%2520evaluation%2520benchmarks%2520specifically%2520designed%2520for%2520long%2520video%250Ageneration%2520models%252C%2520the%2520current%2520assessment%2520of%2520these%2520models%2520primarily%2520relies%2520on%250Abenchmarks%2520with%2520simple%2520narrative%2520prompts%2520%2528e.g.%252C%2520VBench%2529.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520our%2520proposed%2520NarrLV%2520is%2520the%2520first%2520benchmark%2520to%2520comprehensively%250Aevaluate%2520the%2520Narrative%2520expression%2520capabilities%2520of%2520Long%2520Video%2520generation%2520models.%250AInspired%2520by%2520film%2520narrative%2520theory%252C%2520%2528i%2529%2520we%2520first%2520introduce%2520the%2520basic%2520narrative%250Aunit%2520maintaining%2520continuous%2520visual%2520presentation%2520in%2520videos%2520as%2520Temporal%2520Narrative%250AAtom%2520%2528TNA%2529%252C%2520and%2520use%2520its%2520count%2520to%2520quantitatively%2520measure%2520narrative%2520richness.%250AGuided%2520by%2520three%2520key%2520film%2520narrative%2520elements%2520influencing%2520TNA%2520changes%252C%2520we%250Aconstruct%2520an%2520automatic%2520prompt%2520generation%2520pipeline%2520capable%2520of%2520producing%250Aevaluation%2520prompts%2520with%2520a%2520flexibly%2520expandable%2520number%2520of%2520TNAs.%2520%2528ii%2529%2520Then%252C%2520based%250Aon%2520the%2520three%2520progressive%2520levels%2520of%2520narrative%2520content%2520expression%252C%2520we%2520design%2520an%250Aeffective%2520evaluation%2520metric%2520using%2520the%2520MLLM-based%2520question%2520generation%2520and%250Aanswering%2520framework.%2520%2528iii%2529%2520Finally%252C%2520we%2520conduct%2520extensive%2520evaluations%2520on%250Aexisting%2520long%2520video%2520generation%2520models%2520and%2520the%2520foundation%2520generation%2520models.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520metric%2520aligns%2520closely%2520with%2520human%250Ajudgments.%2520The%2520derived%2520evaluation%2520outcomes%2520reveal%2520the%2520detailed%2520capability%250Aboundaries%2520of%2520current%2520video%2520generation%2520models%2520in%2520narrative%2520content%2520expression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NarrLV%3A%20Towards%20a%20Comprehensive%20Narrative-Centric%20Evaluation%20for%20Long%0A%20%20Video%20Generation%20Models&entry.906535625=X.%20Feng%20and%20H.%20Yu%20and%20M.%20Wu%20and%20S.%20Hu%20and%20J.%20Chen%20and%20C.%20Zhu%20and%20J.%20Wu%20and%20X.%20Chu%20and%20K.%20Huang&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20foundation%20video%20generation%20technologies%2C%20long%0Avideo%20generation%20models%20have%20exhibited%20promising%20research%20potential%20thanks%20to%0Aexpanded%20content%20creation%20space.%20Recent%20studies%20reveal%20that%20the%20goal%20of%20long%0Avideo%20generation%20tasks%20is%20not%20only%20to%20extend%20video%20duration%20but%20also%20to%0Aaccurately%20express%20richer%20narrative%20content%20within%20longer%20videos.%20However%2C%20due%0Ato%20the%20lack%20of%20evaluation%20benchmarks%20specifically%20designed%20for%20long%20video%0Ageneration%20models%2C%20the%20current%20assessment%20of%20these%20models%20primarily%20relies%20on%0Abenchmarks%20with%20simple%20narrative%20prompts%20%28e.g.%2C%20VBench%29.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20proposed%20NarrLV%20is%20the%20first%20benchmark%20to%20comprehensively%0Aevaluate%20the%20Narrative%20expression%20capabilities%20of%20Long%20Video%20generation%20models.%0AInspired%20by%20film%20narrative%20theory%2C%20%28i%29%20we%20first%20introduce%20the%20basic%20narrative%0Aunit%20maintaining%20continuous%20visual%20presentation%20in%20videos%20as%20Temporal%20Narrative%0AAtom%20%28TNA%29%2C%20and%20use%20its%20count%20to%20quantitatively%20measure%20narrative%20richness.%0AGuided%20by%20three%20key%20film%20narrative%20elements%20influencing%20TNA%20changes%2C%20we%0Aconstruct%20an%20automatic%20prompt%20generation%20pipeline%20capable%20of%20producing%0Aevaluation%20prompts%20with%20a%20flexibly%20expandable%20number%20of%20TNAs.%20%28ii%29%20Then%2C%20based%0Aon%20the%20three%20progressive%20levels%20of%20narrative%20content%20expression%2C%20we%20design%20an%0Aeffective%20evaluation%20metric%20using%20the%20MLLM-based%20question%20generation%20and%0Aanswering%20framework.%20%28iii%29%20Finally%2C%20we%20conduct%20extensive%20evaluations%20on%0Aexisting%20long%20video%20generation%20models%20and%20the%20foundation%20generation%20models.%0AExperimental%20results%20demonstrate%20that%20our%20metric%20aligns%20closely%20with%20human%0Ajudgments.%20The%20derived%20evaluation%20outcomes%20reveal%20the%20detailed%20capability%0Aboundaries%20of%20current%20video%20generation%20models%20in%20narrative%20content%20expression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11245v2&entry.124074799=Read"},
{"title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding", "author": "Shuquan Lian and Yuhang Wu and Jia Ma and Zihan Song and Bingqi Chen and Xiawu Zheng and Hui Li", "abstract": "  The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.\n", "link": "http://arxiv.org/abs/2507.22025v1", "date": "2025-07-29", "relevancy": 2.6924, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5433}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5418}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-AGILE%3A%20Advancing%20GUI%20Agents%20with%20Effective%20Reinforcement%20Learning%20and%0A%20%20Precise%20Inference-Time%20Grounding&body=Title%3A%20UI-AGILE%3A%20Advancing%20GUI%20Agents%20with%20Effective%20Reinforcement%20Learning%20and%0A%20%20Precise%20Inference-Time%20Grounding%0AAuthor%3A%20Shuquan%20Lian%20and%20Yuhang%20Wu%20and%20Jia%20Ma%20and%20Zihan%20Song%20and%20Bingqi%20Chen%20and%20Xiawu%20Zheng%20and%20Hui%20Li%0AAbstract%3A%20%20%20The%20emergence%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20driven%0Asignificant%20advances%20in%20Graphical%20User%20Interface%20%28GUI%29%20agent%20capabilities.%0ANevertheless%2C%20existing%20GUI%20agent%20training%20and%20inference%20techniques%20still%20suffer%0Afrom%20a%20dilemma%20for%20reasoning%20designs%2C%20ineffective%20reward%2C%20and%20visual%20noise.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20UI-AGILE%2C%20a%20comprehensive%20framework%0Aenhancing%20GUI%20agents%20at%20both%20the%20training%20and%20inference%20stages.%20For%20training%2C%0Awe%20propose%20a%20suite%20of%20improvements%20to%20the%20Supervised%20Fine-Tuning%20%28SFT%29%20process%3A%0A1%29%20a%20Continuous%20Reward%20function%20to%20incentivize%20high-precision%20grounding%3B%202%29%20a%0A%22Simple%20Thinking%22%20reward%20to%20balance%20planning%20with%20speed%20and%20grounding%20accuracy%3B%0Aand%203%29%20a%20Cropping-based%20Resampling%20strategy%20to%20mitigate%20the%20sparse%20reward%0Aproblem%20and%20improve%20learning%20on%20complex%20tasks.%20For%20inference%2C%20we%20present%0ADecomposed%20Grounding%20with%20Selection%2C%20a%20novel%20method%20that%20dramatically%20improves%0Agrounding%20accuracy%20on%20high-resolution%20displays%20by%20breaking%20the%20image%20into%0Asmaller%2C%20manageable%20parts.%20Experiments%20show%20that%20UI-AGILE%20achieves%20the%0Astate-of-the-art%20performance%20on%20two%20benchmarks%20ScreenSpot-Pro%20and%0AScreenSpot-v2.%20For%20instance%2C%20using%20both%20our%20proposed%20training%20and%20inference%0Aenhancement%20methods%20brings%2023%25%20grounding%20accuracy%20improvement%20over%20the%20best%0Abaseline%20on%20ScreenSpot-Pro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-AGILE%253A%2520Advancing%2520GUI%2520Agents%2520with%2520Effective%2520Reinforcement%2520Learning%2520and%250A%2520%2520Precise%2520Inference-Time%2520Grounding%26entry.906535625%3DShuquan%2520Lian%2520and%2520Yuhang%2520Wu%2520and%2520Jia%2520Ma%2520and%2520Zihan%2520Song%2520and%2520Bingqi%2520Chen%2520and%2520Xiawu%2520Zheng%2520and%2520Hui%2520Li%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520driven%250Asignificant%2520advances%2520in%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agent%2520capabilities.%250ANevertheless%252C%2520existing%2520GUI%2520agent%2520training%2520and%2520inference%2520techniques%2520still%2520suffer%250Afrom%2520a%2520dilemma%2520for%2520reasoning%2520designs%252C%2520ineffective%2520reward%252C%2520and%2520visual%2520noise.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520UI-AGILE%252C%2520a%2520comprehensive%2520framework%250Aenhancing%2520GUI%2520agents%2520at%2520both%2520the%2520training%2520and%2520inference%2520stages.%2520For%2520training%252C%250Awe%2520propose%2520a%2520suite%2520of%2520improvements%2520to%2520the%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520process%253A%250A1%2529%2520a%2520Continuous%2520Reward%2520function%2520to%2520incentivize%2520high-precision%2520grounding%253B%25202%2529%2520a%250A%2522Simple%2520Thinking%2522%2520reward%2520to%2520balance%2520planning%2520with%2520speed%2520and%2520grounding%2520accuracy%253B%250Aand%25203%2529%2520a%2520Cropping-based%2520Resampling%2520strategy%2520to%2520mitigate%2520the%2520sparse%2520reward%250Aproblem%2520and%2520improve%2520learning%2520on%2520complex%2520tasks.%2520For%2520inference%252C%2520we%2520present%250ADecomposed%2520Grounding%2520with%2520Selection%252C%2520a%2520novel%2520method%2520that%2520dramatically%2520improves%250Agrounding%2520accuracy%2520on%2520high-resolution%2520displays%2520by%2520breaking%2520the%2520image%2520into%250Asmaller%252C%2520manageable%2520parts.%2520Experiments%2520show%2520that%2520UI-AGILE%2520achieves%2520the%250Astate-of-the-art%2520performance%2520on%2520two%2520benchmarks%2520ScreenSpot-Pro%2520and%250AScreenSpot-v2.%2520For%2520instance%252C%2520using%2520both%2520our%2520proposed%2520training%2520and%2520inference%250Aenhancement%2520methods%2520brings%252023%2525%2520grounding%2520accuracy%2520improvement%2520over%2520the%2520best%250Abaseline%2520on%2520ScreenSpot-Pro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-AGILE%3A%20Advancing%20GUI%20Agents%20with%20Effective%20Reinforcement%20Learning%20and%0A%20%20Precise%20Inference-Time%20Grounding&entry.906535625=Shuquan%20Lian%20and%20Yuhang%20Wu%20and%20Jia%20Ma%20and%20Zihan%20Song%20and%20Bingqi%20Chen%20and%20Xiawu%20Zheng%20and%20Hui%20Li&entry.1292438233=%20%20The%20emergence%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20driven%0Asignificant%20advances%20in%20Graphical%20User%20Interface%20%28GUI%29%20agent%20capabilities.%0ANevertheless%2C%20existing%20GUI%20agent%20training%20and%20inference%20techniques%20still%20suffer%0Afrom%20a%20dilemma%20for%20reasoning%20designs%2C%20ineffective%20reward%2C%20and%20visual%20noise.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20UI-AGILE%2C%20a%20comprehensive%20framework%0Aenhancing%20GUI%20agents%20at%20both%20the%20training%20and%20inference%20stages.%20For%20training%2C%0Awe%20propose%20a%20suite%20of%20improvements%20to%20the%20Supervised%20Fine-Tuning%20%28SFT%29%20process%3A%0A1%29%20a%20Continuous%20Reward%20function%20to%20incentivize%20high-precision%20grounding%3B%202%29%20a%0A%22Simple%20Thinking%22%20reward%20to%20balance%20planning%20with%20speed%20and%20grounding%20accuracy%3B%0Aand%203%29%20a%20Cropping-based%20Resampling%20strategy%20to%20mitigate%20the%20sparse%20reward%0Aproblem%20and%20improve%20learning%20on%20complex%20tasks.%20For%20inference%2C%20we%20present%0ADecomposed%20Grounding%20with%20Selection%2C%20a%20novel%20method%20that%20dramatically%20improves%0Agrounding%20accuracy%20on%20high-resolution%20displays%20by%20breaking%20the%20image%20into%0Asmaller%2C%20manageable%20parts.%20Experiments%20show%20that%20UI-AGILE%20achieves%20the%0Astate-of-the-art%20performance%20on%20two%20benchmarks%20ScreenSpot-Pro%20and%0AScreenSpot-v2.%20For%20instance%2C%20using%20both%20our%20proposed%20training%20and%20inference%0Aenhancement%20methods%20brings%2023%25%20grounding%20accuracy%20improvement%20over%20the%20best%0Abaseline%20on%20ScreenSpot-Pro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22025v1&entry.124074799=Read"},
{"title": "MetaCLIP 2: A Worldwide Scaling Recipe", "author": "Yung-Sung Chuang and Yang Li and Dong Wang and Ching-Feng Yeh and Kehan Lyu and Ramya Raghavendra and James Glass and Lifei Huang and Jason Weston and Luke Zettlemoyer and Xinlei Chen and Zhuang Liu and Saining Xie and Wen-tau Yih and Shang-Wen Li and Hu Xu", "abstract": "  Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.\n", "link": "http://arxiv.org/abs/2507.22062v1", "date": "2025-07-29", "relevancy": 2.6798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaCLIP%202%3A%20A%20Worldwide%20Scaling%20Recipe&body=Title%3A%20MetaCLIP%202%3A%20A%20Worldwide%20Scaling%20Recipe%0AAuthor%3A%20Yung-Sung%20Chuang%20and%20Yang%20Li%20and%20Dong%20Wang%20and%20Ching-Feng%20Yeh%20and%20Kehan%20Lyu%20and%20Ramya%20Raghavendra%20and%20James%20Glass%20and%20Lifei%20Huang%20and%20Jason%20Weston%20and%20Luke%20Zettlemoyer%20and%20Xinlei%20Chen%20and%20Zhuang%20Liu%20and%20Saining%20Xie%20and%20Wen-tau%20Yih%20and%20Shang-Wen%20Li%20and%20Hu%20Xu%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20is%20a%20popular%20foundation%20model%2C%0Asupporting%20from%20zero-shot%20classification%2C%20retrieval%20to%20encoders%20for%20multimodal%0Alarge%20language%20models%20%28MLLMs%29.%20Although%20CLIP%20is%20successfully%20trained%20on%0Abillion-scale%20image-text%20pairs%20from%20the%20English%20world%2C%20scaling%20CLIP%27s%20training%0Afurther%20to%20learning%20from%20the%20worldwide%20web%20data%20is%20still%20challenging%3A%20%281%29%20no%0Acuration%20method%20is%20available%20to%20handle%20data%20points%20from%20non-English%20world%3B%20%282%29%0Athe%20English%20performance%20from%20existing%20multilingual%20CLIP%20is%20worse%20than%20its%0AEnglish-only%20counterpart%2C%20i.e.%2C%20%22curse%20of%20multilinguality%22%20that%20is%20common%20in%0ALLMs.%20Here%2C%20we%20present%20MetaCLIP%202%2C%20the%20first%20recipe%20training%20CLIP%20from%20scratch%0Aon%20worldwide%20web-scale%20image-text%20pairs.%20To%20generalize%20our%20findings%2C%20we%20conduct%0Arigorous%20ablations%20with%20minimal%20changes%20that%20are%20necessary%20to%20address%20the%20above%0Achallenges%20and%20present%20a%20recipe%20enabling%20mutual%20benefits%20from%20English%20and%0Anon-English%20world%20data.%20In%20zero-shot%20ImageNet%20classification%2C%20MetaCLIP%202%0AViT-H/14%20surpasses%20its%20English-only%20counterpart%20by%200.8%25%20and%20mSigLIP%20by%200.7%25%2C%0Aand%20surprisingly%20sets%20new%20state-of-the-art%20without%20system-level%20confounding%0Afactors%20%28e.g.%2C%20translation%2C%20bespoke%20architecture%20changes%29%20on%20multilingual%0Abenchmarks%2C%20such%20as%20CVQA%20with%2057.4%25%2C%20Babel-ImageNet%20with%2050.2%25%20and%20XM3600%20with%0A64.3%25%20on%20image-to-text%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaCLIP%25202%253A%2520A%2520Worldwide%2520Scaling%2520Recipe%26entry.906535625%3DYung-Sung%2520Chuang%2520and%2520Yang%2520Li%2520and%2520Dong%2520Wang%2520and%2520Ching-Feng%2520Yeh%2520and%2520Kehan%2520Lyu%2520and%2520Ramya%2520Raghavendra%2520and%2520James%2520Glass%2520and%2520Lifei%2520Huang%2520and%2520Jason%2520Weston%2520and%2520Luke%2520Zettlemoyer%2520and%2520Xinlei%2520Chen%2520and%2520Zhuang%2520Liu%2520and%2520Saining%2520Xie%2520and%2520Wen-tau%2520Yih%2520and%2520Shang-Wen%2520Li%2520and%2520Hu%2520Xu%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520is%2520a%2520popular%2520foundation%2520model%252C%250Asupporting%2520from%2520zero-shot%2520classification%252C%2520retrieval%2520to%2520encoders%2520for%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529.%2520Although%2520CLIP%2520is%2520successfully%2520trained%2520on%250Abillion-scale%2520image-text%2520pairs%2520from%2520the%2520English%2520world%252C%2520scaling%2520CLIP%2527s%2520training%250Afurther%2520to%2520learning%2520from%2520the%2520worldwide%2520web%2520data%2520is%2520still%2520challenging%253A%2520%25281%2529%2520no%250Acuration%2520method%2520is%2520available%2520to%2520handle%2520data%2520points%2520from%2520non-English%2520world%253B%2520%25282%2529%250Athe%2520English%2520performance%2520from%2520existing%2520multilingual%2520CLIP%2520is%2520worse%2520than%2520its%250AEnglish-only%2520counterpart%252C%2520i.e.%252C%2520%2522curse%2520of%2520multilinguality%2522%2520that%2520is%2520common%2520in%250ALLMs.%2520Here%252C%2520we%2520present%2520MetaCLIP%25202%252C%2520the%2520first%2520recipe%2520training%2520CLIP%2520from%2520scratch%250Aon%2520worldwide%2520web-scale%2520image-text%2520pairs.%2520To%2520generalize%2520our%2520findings%252C%2520we%2520conduct%250Arigorous%2520ablations%2520with%2520minimal%2520changes%2520that%2520are%2520necessary%2520to%2520address%2520the%2520above%250Achallenges%2520and%2520present%2520a%2520recipe%2520enabling%2520mutual%2520benefits%2520from%2520English%2520and%250Anon-English%2520world%2520data.%2520In%2520zero-shot%2520ImageNet%2520classification%252C%2520MetaCLIP%25202%250AViT-H/14%2520surpasses%2520its%2520English-only%2520counterpart%2520by%25200.8%2525%2520and%2520mSigLIP%2520by%25200.7%2525%252C%250Aand%2520surprisingly%2520sets%2520new%2520state-of-the-art%2520without%2520system-level%2520confounding%250Afactors%2520%2528e.g.%252C%2520translation%252C%2520bespoke%2520architecture%2520changes%2529%2520on%2520multilingual%250Abenchmarks%252C%2520such%2520as%2520CVQA%2520with%252057.4%2525%252C%2520Babel-ImageNet%2520with%252050.2%2525%2520and%2520XM3600%2520with%250A64.3%2525%2520on%2520image-to-text%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaCLIP%202%3A%20A%20Worldwide%20Scaling%20Recipe&entry.906535625=Yung-Sung%20Chuang%20and%20Yang%20Li%20and%20Dong%20Wang%20and%20Ching-Feng%20Yeh%20and%20Kehan%20Lyu%20and%20Ramya%20Raghavendra%20and%20James%20Glass%20and%20Lifei%20Huang%20and%20Jason%20Weston%20and%20Luke%20Zettlemoyer%20and%20Xinlei%20Chen%20and%20Zhuang%20Liu%20and%20Saining%20Xie%20and%20Wen-tau%20Yih%20and%20Shang-Wen%20Li%20and%20Hu%20Xu&entry.1292438233=%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20is%20a%20popular%20foundation%20model%2C%0Asupporting%20from%20zero-shot%20classification%2C%20retrieval%20to%20encoders%20for%20multimodal%0Alarge%20language%20models%20%28MLLMs%29.%20Although%20CLIP%20is%20successfully%20trained%20on%0Abillion-scale%20image-text%20pairs%20from%20the%20English%20world%2C%20scaling%20CLIP%27s%20training%0Afurther%20to%20learning%20from%20the%20worldwide%20web%20data%20is%20still%20challenging%3A%20%281%29%20no%0Acuration%20method%20is%20available%20to%20handle%20data%20points%20from%20non-English%20world%3B%20%282%29%0Athe%20English%20performance%20from%20existing%20multilingual%20CLIP%20is%20worse%20than%20its%0AEnglish-only%20counterpart%2C%20i.e.%2C%20%22curse%20of%20multilinguality%22%20that%20is%20common%20in%0ALLMs.%20Here%2C%20we%20present%20MetaCLIP%202%2C%20the%20first%20recipe%20training%20CLIP%20from%20scratch%0Aon%20worldwide%20web-scale%20image-text%20pairs.%20To%20generalize%20our%20findings%2C%20we%20conduct%0Arigorous%20ablations%20with%20minimal%20changes%20that%20are%20necessary%20to%20address%20the%20above%0Achallenges%20and%20present%20a%20recipe%20enabling%20mutual%20benefits%20from%20English%20and%0Anon-English%20world%20data.%20In%20zero-shot%20ImageNet%20classification%2C%20MetaCLIP%202%0AViT-H/14%20surpasses%20its%20English-only%20counterpart%20by%200.8%25%20and%20mSigLIP%20by%200.7%25%2C%0Aand%20surprisingly%20sets%20new%20state-of-the-art%20without%20system-level%20confounding%0Afactors%20%28e.g.%2C%20translation%2C%20bespoke%20architecture%20changes%29%20on%20multilingual%0Abenchmarks%2C%20such%20as%20CVQA%20with%2057.4%25%2C%20Babel-ImageNet%20with%2050.2%25%20and%20XM3600%20with%0A64.3%25%20on%20image-to-text%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22062v1&entry.124074799=Read"},
{"title": "Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning", "author": "Chaofei Qi and Chao Ye and Zhitai Liu and Weiyang Lin and Jianbin Qiu", "abstract": "  Deep learning has witnessed the extensive utilization across a wide spectrum\nof domains, including fine-grained few-shot learning (FGFSL) which heavily\ndepends on deep backbones. Nonetheless, shallower deep backbones such as\nConvNet-4, are not commonly preferred because they're prone to extract a larger\nquantity of non-abstract visual attributes. In this paper, we initially\nre-evaluate the relationship between network depth and the ability to fully\nencode few-shot instances, and delve into whether shallow deep architecture\ncould effectuate comparable or superior performance to mainstream deep\nbackbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a\nlocation-aware constellation network (LCN-4), equipped with a cutting-edge\nlocation-aware feature clustering module. This module can proficiently encoder\nand integrate spatial feature fusion, feature clustering, and recessive feature\nlocation, thereby significantly minimizing the overall loss. Specifically, we\ninnovatively put forward a general grid position encoding compensation to\neffectively address the issue of positional information missing during the\nfeature extraction process of specific ordinary convolutions. Additionally, we\nfurther propose a general frequency domain location embedding technique to\noffset for the location loss in clustering features. We have carried out\nvalidation procedures on three representative fine-grained few-shot benchmarks.\nRelevant experiments have established that LCN-4 notably outperforms the\nConvNet-4 based State-of-the-Arts and achieves performance that is on par with\nor superior to most ResNet12-based methods, confirming the correctness of our\nconjecture.\n", "link": "http://arxiv.org/abs/2507.22041v1", "date": "2025-07-29", "relevancy": 2.577, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shallow%20Deep%20Learning%20Can%20Still%20Excel%20in%20Fine-Grained%20Few-Shot%20Learning&body=Title%3A%20Shallow%20Deep%20Learning%20Can%20Still%20Excel%20in%20Fine-Grained%20Few-Shot%20Learning%0AAuthor%3A%20Chaofei%20Qi%20and%20Chao%20Ye%20and%20Zhitai%20Liu%20and%20Weiyang%20Lin%20and%20Jianbin%20Qiu%0AAbstract%3A%20%20%20Deep%20learning%20has%20witnessed%20the%20extensive%20utilization%20across%20a%20wide%20spectrum%0Aof%20domains%2C%20including%20fine-grained%20few-shot%20learning%20%28FGFSL%29%20which%20heavily%0Adepends%20on%20deep%20backbones.%20Nonetheless%2C%20shallower%20deep%20backbones%20such%20as%0AConvNet-4%2C%20are%20not%20commonly%20preferred%20because%20they%27re%20prone%20to%20extract%20a%20larger%0Aquantity%20of%20non-abstract%20visual%20attributes.%20In%20this%20paper%2C%20we%20initially%0Are-evaluate%20the%20relationship%20between%20network%20depth%20and%20the%20ability%20to%20fully%0Aencode%20few-shot%20instances%2C%20and%20delve%20into%20whether%20shallow%20deep%20architecture%0Acould%20effectuate%20comparable%20or%20superior%20performance%20to%20mainstream%20deep%0Abackbone.%20Fueled%20by%20the%20inspiration%20from%20vanilla%20ConvNet-4%2C%20we%20introduce%20a%0Alocation-aware%20constellation%20network%20%28LCN-4%29%2C%20equipped%20with%20a%20cutting-edge%0Alocation-aware%20feature%20clustering%20module.%20This%20module%20can%20proficiently%20encoder%0Aand%20integrate%20spatial%20feature%20fusion%2C%20feature%20clustering%2C%20and%20recessive%20feature%0Alocation%2C%20thereby%20significantly%20minimizing%20the%20overall%20loss.%20Specifically%2C%20we%0Ainnovatively%20put%20forward%20a%20general%20grid%20position%20encoding%20compensation%20to%0Aeffectively%20address%20the%20issue%20of%20positional%20information%20missing%20during%20the%0Afeature%20extraction%20process%20of%20specific%20ordinary%20convolutions.%20Additionally%2C%20we%0Afurther%20propose%20a%20general%20frequency%20domain%20location%20embedding%20technique%20to%0Aoffset%20for%20the%20location%20loss%20in%20clustering%20features.%20We%20have%20carried%20out%0Avalidation%20procedures%20on%20three%20representative%20fine-grained%20few-shot%20benchmarks.%0ARelevant%20experiments%20have%20established%20that%20LCN-4%20notably%20outperforms%20the%0AConvNet-4%20based%20State-of-the-Arts%20and%20achieves%20performance%20that%20is%20on%20par%20with%0Aor%20superior%20to%20most%20ResNet12-based%20methods%2C%20confirming%20the%20correctness%20of%20our%0Aconjecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShallow%2520Deep%2520Learning%2520Can%2520Still%2520Excel%2520in%2520Fine-Grained%2520Few-Shot%2520Learning%26entry.906535625%3DChaofei%2520Qi%2520and%2520Chao%2520Ye%2520and%2520Zhitai%2520Liu%2520and%2520Weiyang%2520Lin%2520and%2520Jianbin%2520Qiu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520witnessed%2520the%2520extensive%2520utilization%2520across%2520a%2520wide%2520spectrum%250Aof%2520domains%252C%2520including%2520fine-grained%2520few-shot%2520learning%2520%2528FGFSL%2529%2520which%2520heavily%250Adepends%2520on%2520deep%2520backbones.%2520Nonetheless%252C%2520shallower%2520deep%2520backbones%2520such%2520as%250AConvNet-4%252C%2520are%2520not%2520commonly%2520preferred%2520because%2520they%2527re%2520prone%2520to%2520extract%2520a%2520larger%250Aquantity%2520of%2520non-abstract%2520visual%2520attributes.%2520In%2520this%2520paper%252C%2520we%2520initially%250Are-evaluate%2520the%2520relationship%2520between%2520network%2520depth%2520and%2520the%2520ability%2520to%2520fully%250Aencode%2520few-shot%2520instances%252C%2520and%2520delve%2520into%2520whether%2520shallow%2520deep%2520architecture%250Acould%2520effectuate%2520comparable%2520or%2520superior%2520performance%2520to%2520mainstream%2520deep%250Abackbone.%2520Fueled%2520by%2520the%2520inspiration%2520from%2520vanilla%2520ConvNet-4%252C%2520we%2520introduce%2520a%250Alocation-aware%2520constellation%2520network%2520%2528LCN-4%2529%252C%2520equipped%2520with%2520a%2520cutting-edge%250Alocation-aware%2520feature%2520clustering%2520module.%2520This%2520module%2520can%2520proficiently%2520encoder%250Aand%2520integrate%2520spatial%2520feature%2520fusion%252C%2520feature%2520clustering%252C%2520and%2520recessive%2520feature%250Alocation%252C%2520thereby%2520significantly%2520minimizing%2520the%2520overall%2520loss.%2520Specifically%252C%2520we%250Ainnovatively%2520put%2520forward%2520a%2520general%2520grid%2520position%2520encoding%2520compensation%2520to%250Aeffectively%2520address%2520the%2520issue%2520of%2520positional%2520information%2520missing%2520during%2520the%250Afeature%2520extraction%2520process%2520of%2520specific%2520ordinary%2520convolutions.%2520Additionally%252C%2520we%250Afurther%2520propose%2520a%2520general%2520frequency%2520domain%2520location%2520embedding%2520technique%2520to%250Aoffset%2520for%2520the%2520location%2520loss%2520in%2520clustering%2520features.%2520We%2520have%2520carried%2520out%250Avalidation%2520procedures%2520on%2520three%2520representative%2520fine-grained%2520few-shot%2520benchmarks.%250ARelevant%2520experiments%2520have%2520established%2520that%2520LCN-4%2520notably%2520outperforms%2520the%250AConvNet-4%2520based%2520State-of-the-Arts%2520and%2520achieves%2520performance%2520that%2520is%2520on%2520par%2520with%250Aor%2520superior%2520to%2520most%2520ResNet12-based%2520methods%252C%2520confirming%2520the%2520correctness%2520of%2520our%250Aconjecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shallow%20Deep%20Learning%20Can%20Still%20Excel%20in%20Fine-Grained%20Few-Shot%20Learning&entry.906535625=Chaofei%20Qi%20and%20Chao%20Ye%20and%20Zhitai%20Liu%20and%20Weiyang%20Lin%20and%20Jianbin%20Qiu&entry.1292438233=%20%20Deep%20learning%20has%20witnessed%20the%20extensive%20utilization%20across%20a%20wide%20spectrum%0Aof%20domains%2C%20including%20fine-grained%20few-shot%20learning%20%28FGFSL%29%20which%20heavily%0Adepends%20on%20deep%20backbones.%20Nonetheless%2C%20shallower%20deep%20backbones%20such%20as%0AConvNet-4%2C%20are%20not%20commonly%20preferred%20because%20they%27re%20prone%20to%20extract%20a%20larger%0Aquantity%20of%20non-abstract%20visual%20attributes.%20In%20this%20paper%2C%20we%20initially%0Are-evaluate%20the%20relationship%20between%20network%20depth%20and%20the%20ability%20to%20fully%0Aencode%20few-shot%20instances%2C%20and%20delve%20into%20whether%20shallow%20deep%20architecture%0Acould%20effectuate%20comparable%20or%20superior%20performance%20to%20mainstream%20deep%0Abackbone.%20Fueled%20by%20the%20inspiration%20from%20vanilla%20ConvNet-4%2C%20we%20introduce%20a%0Alocation-aware%20constellation%20network%20%28LCN-4%29%2C%20equipped%20with%20a%20cutting-edge%0Alocation-aware%20feature%20clustering%20module.%20This%20module%20can%20proficiently%20encoder%0Aand%20integrate%20spatial%20feature%20fusion%2C%20feature%20clustering%2C%20and%20recessive%20feature%0Alocation%2C%20thereby%20significantly%20minimizing%20the%20overall%20loss.%20Specifically%2C%20we%0Ainnovatively%20put%20forward%20a%20general%20grid%20position%20encoding%20compensation%20to%0Aeffectively%20address%20the%20issue%20of%20positional%20information%20missing%20during%20the%0Afeature%20extraction%20process%20of%20specific%20ordinary%20convolutions.%20Additionally%2C%20we%0Afurther%20propose%20a%20general%20frequency%20domain%20location%20embedding%20technique%20to%0Aoffset%20for%20the%20location%20loss%20in%20clustering%20features.%20We%20have%20carried%20out%0Avalidation%20procedures%20on%20three%20representative%20fine-grained%20few-shot%20benchmarks.%0ARelevant%20experiments%20have%20established%20that%20LCN-4%20notably%20outperforms%20the%0AConvNet-4%20based%20State-of-the-Arts%20and%20achieves%20performance%20that%20is%20on%20par%20with%0Aor%20superior%20to%20most%20ResNet12-based%20methods%2C%20confirming%20the%20correctness%20of%20our%0Aconjecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22041v1&entry.124074799=Read"},
{"title": "LIMO: Less is More for Reasoning", "author": "Yixin Ye and Zhen Huang and Yang Xiao and Ethan Chern and Shijie Xia and Pengfei Liu", "abstract": "  We challenge the prevailing assumption that complex reasoning in large\nlanguage models (LLMs) necessitates massive training data. We demonstrate that\nsophisticated mathematical reasoning can emerge with only a few examples.\nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves\n63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned\nmodels (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the\ntraining data required by prior approaches. Furthermore, LIMO exhibits strong\nout-of-distribution generalization, achieving a 45.8\\% absolute improvement\nacross diverse benchmarks, outperforming models trained on 100x more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis\n(LIMO Hypothesis): In foundation models where domain knowledge has been\ncomprehensively encoded during pre-training, sophisticated reasoning can emerge\nthrough minimal but strategically designed demonstrations of cognitive\nprocesses. This hypothesis suggests that the threshold for eliciting complex\nreasoning is not dictated by task complexity but rather by two key factors: (1)\nthe completeness of the model's pre-trained knowledge base and (2) the\neffectiveness of post-training examples in serving as \"cognitive templates\"\nthat guide reasoning.\n", "link": "http://arxiv.org/abs/2502.03387v3", "date": "2025-07-29", "relevancy": 2.5726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIMO%3A%20Less%20is%20More%20for%20Reasoning&body=Title%3A%20LIMO%3A%20Less%20is%20More%20for%20Reasoning%0AAuthor%3A%20Yixin%20Ye%20and%20Zhen%20Huang%20and%20Yang%20Xiao%20and%20Ethan%20Chern%20and%20Shijie%20Xia%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20We%20challenge%20the%20prevailing%20assumption%20that%20complex%20reasoning%20in%20large%0Alanguage%20models%20%28LLMs%29%20necessitates%20massive%20training%20data.%20We%20demonstrate%20that%0Asophisticated%20mathematical%20reasoning%20can%20emerge%20with%20only%20a%20few%20examples.%0ASpecifically%2C%20through%20simple%20supervised%20fine-tuning%2C%20our%20model%2C%20LIMO%2C%20achieves%0A63.3%5C%25%20accuracy%20on%20AIME24%20and%2095.6%5C%25%20on%20MATH500%2C%20surpassing%20previous%20fine-tuned%0Amodels%20%286.5%5C%25%20on%20AIME24%2C%2059.2%5C%25%20on%20MATH500%29%20while%20using%20only%201%5C%25%20of%20the%0Atraining%20data%20required%20by%20prior%20approaches.%20Furthermore%2C%20LIMO%20exhibits%20strong%0Aout-of-distribution%20generalization%2C%20achieving%20a%2045.8%5C%25%20absolute%20improvement%0Aacross%20diverse%20benchmarks%2C%20outperforming%20models%20trained%20on%20100x%20more%20data.%0ASynthesizing%20these%20findings%2C%20we%20propose%20the%20Less-Is-More%20Reasoning%20Hypothesis%0A%28LIMO%20Hypothesis%29%3A%20In%20foundation%20models%20where%20domain%20knowledge%20has%20been%0Acomprehensively%20encoded%20during%20pre-training%2C%20sophisticated%20reasoning%20can%20emerge%0Athrough%20minimal%20but%20strategically%20designed%20demonstrations%20of%20cognitive%0Aprocesses.%20This%20hypothesis%20suggests%20that%20the%20threshold%20for%20eliciting%20complex%0Areasoning%20is%20not%20dictated%20by%20task%20complexity%20but%20rather%20by%20two%20key%20factors%3A%20%281%29%0Athe%20completeness%20of%20the%20model%27s%20pre-trained%20knowledge%20base%20and%20%282%29%20the%0Aeffectiveness%20of%20post-training%20examples%20in%20serving%20as%20%22cognitive%20templates%22%0Athat%20guide%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIMO%253A%2520Less%2520is%2520More%2520for%2520Reasoning%26entry.906535625%3DYixin%2520Ye%2520and%2520Zhen%2520Huang%2520and%2520Yang%2520Xiao%2520and%2520Ethan%2520Chern%2520and%2520Shijie%2520Xia%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520We%2520challenge%2520the%2520prevailing%2520assumption%2520that%2520complex%2520reasoning%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520necessitates%2520massive%2520training%2520data.%2520We%2520demonstrate%2520that%250Asophisticated%2520mathematical%2520reasoning%2520can%2520emerge%2520with%2520only%2520a%2520few%2520examples.%250ASpecifically%252C%2520through%2520simple%2520supervised%2520fine-tuning%252C%2520our%2520model%252C%2520LIMO%252C%2520achieves%250A63.3%255C%2525%2520accuracy%2520on%2520AIME24%2520and%252095.6%255C%2525%2520on%2520MATH500%252C%2520surpassing%2520previous%2520fine-tuned%250Amodels%2520%25286.5%255C%2525%2520on%2520AIME24%252C%252059.2%255C%2525%2520on%2520MATH500%2529%2520while%2520using%2520only%25201%255C%2525%2520of%2520the%250Atraining%2520data%2520required%2520by%2520prior%2520approaches.%2520Furthermore%252C%2520LIMO%2520exhibits%2520strong%250Aout-of-distribution%2520generalization%252C%2520achieving%2520a%252045.8%255C%2525%2520absolute%2520improvement%250Aacross%2520diverse%2520benchmarks%252C%2520outperforming%2520models%2520trained%2520on%2520100x%2520more%2520data.%250ASynthesizing%2520these%2520findings%252C%2520we%2520propose%2520the%2520Less-Is-More%2520Reasoning%2520Hypothesis%250A%2528LIMO%2520Hypothesis%2529%253A%2520In%2520foundation%2520models%2520where%2520domain%2520knowledge%2520has%2520been%250Acomprehensively%2520encoded%2520during%2520pre-training%252C%2520sophisticated%2520reasoning%2520can%2520emerge%250Athrough%2520minimal%2520but%2520strategically%2520designed%2520demonstrations%2520of%2520cognitive%250Aprocesses.%2520This%2520hypothesis%2520suggests%2520that%2520the%2520threshold%2520for%2520eliciting%2520complex%250Areasoning%2520is%2520not%2520dictated%2520by%2520task%2520complexity%2520but%2520rather%2520by%2520two%2520key%2520factors%253A%2520%25281%2529%250Athe%2520completeness%2520of%2520the%2520model%2527s%2520pre-trained%2520knowledge%2520base%2520and%2520%25282%2529%2520the%250Aeffectiveness%2520of%2520post-training%2520examples%2520in%2520serving%2520as%2520%2522cognitive%2520templates%2522%250Athat%2520guide%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIMO%3A%20Less%20is%20More%20for%20Reasoning&entry.906535625=Yixin%20Ye%20and%20Zhen%20Huang%20and%20Yang%20Xiao%20and%20Ethan%20Chern%20and%20Shijie%20Xia%20and%20Pengfei%20Liu&entry.1292438233=%20%20We%20challenge%20the%20prevailing%20assumption%20that%20complex%20reasoning%20in%20large%0Alanguage%20models%20%28LLMs%29%20necessitates%20massive%20training%20data.%20We%20demonstrate%20that%0Asophisticated%20mathematical%20reasoning%20can%20emerge%20with%20only%20a%20few%20examples.%0ASpecifically%2C%20through%20simple%20supervised%20fine-tuning%2C%20our%20model%2C%20LIMO%2C%20achieves%0A63.3%5C%25%20accuracy%20on%20AIME24%20and%2095.6%5C%25%20on%20MATH500%2C%20surpassing%20previous%20fine-tuned%0Amodels%20%286.5%5C%25%20on%20AIME24%2C%2059.2%5C%25%20on%20MATH500%29%20while%20using%20only%201%5C%25%20of%20the%0Atraining%20data%20required%20by%20prior%20approaches.%20Furthermore%2C%20LIMO%20exhibits%20strong%0Aout-of-distribution%20generalization%2C%20achieving%20a%2045.8%5C%25%20absolute%20improvement%0Aacross%20diverse%20benchmarks%2C%20outperforming%20models%20trained%20on%20100x%20more%20data.%0ASynthesizing%20these%20findings%2C%20we%20propose%20the%20Less-Is-More%20Reasoning%20Hypothesis%0A%28LIMO%20Hypothesis%29%3A%20In%20foundation%20models%20where%20domain%20knowledge%20has%20been%0Acomprehensively%20encoded%20during%20pre-training%2C%20sophisticated%20reasoning%20can%20emerge%0Athrough%20minimal%20but%20strategically%20designed%20demonstrations%20of%20cognitive%0Aprocesses.%20This%20hypothesis%20suggests%20that%20the%20threshold%20for%20eliciting%20complex%0Areasoning%20is%20not%20dictated%20by%20task%20complexity%20but%20rather%20by%20two%20key%20factors%3A%20%281%29%0Athe%20completeness%20of%20the%20model%27s%20pre-trained%20knowledge%20base%20and%20%282%29%20the%0Aeffectiveness%20of%20post-training%20examples%20in%20serving%20as%20%22cognitive%20templates%22%0Athat%20guide%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03387v3&entry.124074799=Read"},
{"title": "ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from\n  Free-Text Reports", "author": "Mohammed Baharoon and Luyang Luo and Michael Moritz and Abhinav Kumar and Sung Eun Kim and Xiaoman Zhang and Miao Zhu and Mahmoud Hussain Alabbad and Maha Sbayel Alhazmi and Neel P. Mistry and Kent Ryan Kleinschmidt and Brady Chrisler and Sathvik Suryadevara and Sri Sai Dinesh Jaliparthi and Noah Michael Prudlo and Mark David Marino and Jeremy Palacio and Rithvik Akula and Hong-Yu Zhou and Ibrahim Ethem Hamamci and Scott J. Adams and Hassan Rayhan AlOmaish and Pranav Rajpurkar", "abstract": "  We present ReXGroundingCT, the first publicly available dataset to link\nfree-text radiology findings with pixel-level segmentations in 3D chest CT\nscans that is manually annotated. While prior datasets have relied on\nstructured labels or predefined categories, ReXGroundingCT captures the full\nexpressiveness of clinical language represented in free text and grounds it to\nspatially localized 3D segmentation annotations in volumetric imaging. This\naddresses a critical gap in medical AI: the ability to connect complex,\ndescriptive text, such as \"3 mm nodule in the left lower lobe\", to its precise\nanatomical location in three-dimensional space, a capability essential for\ngrounded radiology report generation systems. The dataset comprises 3,142\nnon-contrast chest CT scans paired with standardized radiology reports from the\nCT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to\nextract positive lung and pleural findings, which were then manually segmented\nby expert annotators. A total of 8,028 findings across 16,301 entities were\nannotated, with quality control performed by board-certified radiologists.\nApproximately 79% of findings are focal abnormalities, while 21% are non-focal.\nThe training set includes up to three representative segmentations per finding,\nwhile the validation and test sets contain exhaustive labels for each finding\nentity. ReXGroundingCT establishes a new benchmark for developing and\nevaluating sentence-level grounding and free-text medical segmentation models\nin chest CT. The dataset can be accessed at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.\n", "link": "http://arxiv.org/abs/2507.22030v1", "date": "2025-07-29", "relevancy": 2.4676, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReXGroundingCT%3A%20A%203D%20Chest%20CT%20Dataset%20for%20Segmentation%20of%20Findings%20from%0A%20%20Free-Text%20Reports&body=Title%3A%20ReXGroundingCT%3A%20A%203D%20Chest%20CT%20Dataset%20for%20Segmentation%20of%20Findings%20from%0A%20%20Free-Text%20Reports%0AAuthor%3A%20Mohammed%20Baharoon%20and%20Luyang%20Luo%20and%20Michael%20Moritz%20and%20Abhinav%20Kumar%20and%20Sung%20Eun%20Kim%20and%20Xiaoman%20Zhang%20and%20Miao%20Zhu%20and%20Mahmoud%20Hussain%20Alabbad%20and%20Maha%20Sbayel%20Alhazmi%20and%20Neel%20P.%20Mistry%20and%20Kent%20Ryan%20Kleinschmidt%20and%20Brady%20Chrisler%20and%20Sathvik%20Suryadevara%20and%20Sri%20Sai%20Dinesh%20Jaliparthi%20and%20Noah%20Michael%20Prudlo%20and%20Mark%20David%20Marino%20and%20Jeremy%20Palacio%20and%20Rithvik%20Akula%20and%20Hong-Yu%20Zhou%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Scott%20J.%20Adams%20and%20Hassan%20Rayhan%20AlOmaish%20and%20Pranav%20Rajpurkar%0AAbstract%3A%20%20%20We%20present%20ReXGroundingCT%2C%20the%20first%20publicly%20available%20dataset%20to%20link%0Afree-text%20radiology%20findings%20with%20pixel-level%20segmentations%20in%203D%20chest%20CT%0Ascans%20that%20is%20manually%20annotated.%20While%20prior%20datasets%20have%20relied%20on%0Astructured%20labels%20or%20predefined%20categories%2C%20ReXGroundingCT%20captures%20the%20full%0Aexpressiveness%20of%20clinical%20language%20represented%20in%20free%20text%20and%20grounds%20it%20to%0Aspatially%20localized%203D%20segmentation%20annotations%20in%20volumetric%20imaging.%20This%0Aaddresses%20a%20critical%20gap%20in%20medical%20AI%3A%20the%20ability%20to%20connect%20complex%2C%0Adescriptive%20text%2C%20such%20as%20%223%20mm%20nodule%20in%20the%20left%20lower%20lobe%22%2C%20to%20its%20precise%0Aanatomical%20location%20in%20three-dimensional%20space%2C%20a%20capability%20essential%20for%0Agrounded%20radiology%20report%20generation%20systems.%20The%20dataset%20comprises%203%2C142%0Anon-contrast%20chest%20CT%20scans%20paired%20with%20standardized%20radiology%20reports%20from%20the%0ACT-RATE%20dataset.%20Using%20a%20systematic%20three-stage%20pipeline%2C%20GPT-4%20was%20used%20to%0Aextract%20positive%20lung%20and%20pleural%20findings%2C%20which%20were%20then%20manually%20segmented%0Aby%20expert%20annotators.%20A%20total%20of%208%2C028%20findings%20across%2016%2C301%20entities%20were%0Aannotated%2C%20with%20quality%20control%20performed%20by%20board-certified%20radiologists.%0AApproximately%2079%25%20of%20findings%20are%20focal%20abnormalities%2C%20while%2021%25%20are%20non-focal.%0AThe%20training%20set%20includes%20up%20to%20three%20representative%20segmentations%20per%20finding%2C%0Awhile%20the%20validation%20and%20test%20sets%20contain%20exhaustive%20labels%20for%20each%20finding%0Aentity.%20ReXGroundingCT%20establishes%20a%20new%20benchmark%20for%20developing%20and%0Aevaluating%20sentence-level%20grounding%20and%20free-text%20medical%20segmentation%20models%0Ain%20chest%20CT.%20The%20dataset%20can%20be%20accessed%20at%0Ahttps%3A//huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReXGroundingCT%253A%2520A%25203D%2520Chest%2520CT%2520Dataset%2520for%2520Segmentation%2520of%2520Findings%2520from%250A%2520%2520Free-Text%2520Reports%26entry.906535625%3DMohammed%2520Baharoon%2520and%2520Luyang%2520Luo%2520and%2520Michael%2520Moritz%2520and%2520Abhinav%2520Kumar%2520and%2520Sung%2520Eun%2520Kim%2520and%2520Xiaoman%2520Zhang%2520and%2520Miao%2520Zhu%2520and%2520Mahmoud%2520Hussain%2520Alabbad%2520and%2520Maha%2520Sbayel%2520Alhazmi%2520and%2520Neel%2520P.%2520Mistry%2520and%2520Kent%2520Ryan%2520Kleinschmidt%2520and%2520Brady%2520Chrisler%2520and%2520Sathvik%2520Suryadevara%2520and%2520Sri%2520Sai%2520Dinesh%2520Jaliparthi%2520and%2520Noah%2520Michael%2520Prudlo%2520and%2520Mark%2520David%2520Marino%2520and%2520Jeremy%2520Palacio%2520and%2520Rithvik%2520Akula%2520and%2520Hong-Yu%2520Zhou%2520and%2520Ibrahim%2520Ethem%2520Hamamci%2520and%2520Scott%2520J.%2520Adams%2520and%2520Hassan%2520Rayhan%2520AlOmaish%2520and%2520Pranav%2520Rajpurkar%26entry.1292438233%3D%2520%2520We%2520present%2520ReXGroundingCT%252C%2520the%2520first%2520publicly%2520available%2520dataset%2520to%2520link%250Afree-text%2520radiology%2520findings%2520with%2520pixel-level%2520segmentations%2520in%25203D%2520chest%2520CT%250Ascans%2520that%2520is%2520manually%2520annotated.%2520While%2520prior%2520datasets%2520have%2520relied%2520on%250Astructured%2520labels%2520or%2520predefined%2520categories%252C%2520ReXGroundingCT%2520captures%2520the%2520full%250Aexpressiveness%2520of%2520clinical%2520language%2520represented%2520in%2520free%2520text%2520and%2520grounds%2520it%2520to%250Aspatially%2520localized%25203D%2520segmentation%2520annotations%2520in%2520volumetric%2520imaging.%2520This%250Aaddresses%2520a%2520critical%2520gap%2520in%2520medical%2520AI%253A%2520the%2520ability%2520to%2520connect%2520complex%252C%250Adescriptive%2520text%252C%2520such%2520as%2520%25223%2520mm%2520nodule%2520in%2520the%2520left%2520lower%2520lobe%2522%252C%2520to%2520its%2520precise%250Aanatomical%2520location%2520in%2520three-dimensional%2520space%252C%2520a%2520capability%2520essential%2520for%250Agrounded%2520radiology%2520report%2520generation%2520systems.%2520The%2520dataset%2520comprises%25203%252C142%250Anon-contrast%2520chest%2520CT%2520scans%2520paired%2520with%2520standardized%2520radiology%2520reports%2520from%2520the%250ACT-RATE%2520dataset.%2520Using%2520a%2520systematic%2520three-stage%2520pipeline%252C%2520GPT-4%2520was%2520used%2520to%250Aextract%2520positive%2520lung%2520and%2520pleural%2520findings%252C%2520which%2520were%2520then%2520manually%2520segmented%250Aby%2520expert%2520annotators.%2520A%2520total%2520of%25208%252C028%2520findings%2520across%252016%252C301%2520entities%2520were%250Aannotated%252C%2520with%2520quality%2520control%2520performed%2520by%2520board-certified%2520radiologists.%250AApproximately%252079%2525%2520of%2520findings%2520are%2520focal%2520abnormalities%252C%2520while%252021%2525%2520are%2520non-focal.%250AThe%2520training%2520set%2520includes%2520up%2520to%2520three%2520representative%2520segmentations%2520per%2520finding%252C%250Awhile%2520the%2520validation%2520and%2520test%2520sets%2520contain%2520exhaustive%2520labels%2520for%2520each%2520finding%250Aentity.%2520ReXGroundingCT%2520establishes%2520a%2520new%2520benchmark%2520for%2520developing%2520and%250Aevaluating%2520sentence-level%2520grounding%2520and%2520free-text%2520medical%2520segmentation%2520models%250Ain%2520chest%2520CT.%2520The%2520dataset%2520can%2520be%2520accessed%2520at%250Ahttps%253A//huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReXGroundingCT%3A%20A%203D%20Chest%20CT%20Dataset%20for%20Segmentation%20of%20Findings%20from%0A%20%20Free-Text%20Reports&entry.906535625=Mohammed%20Baharoon%20and%20Luyang%20Luo%20and%20Michael%20Moritz%20and%20Abhinav%20Kumar%20and%20Sung%20Eun%20Kim%20and%20Xiaoman%20Zhang%20and%20Miao%20Zhu%20and%20Mahmoud%20Hussain%20Alabbad%20and%20Maha%20Sbayel%20Alhazmi%20and%20Neel%20P.%20Mistry%20and%20Kent%20Ryan%20Kleinschmidt%20and%20Brady%20Chrisler%20and%20Sathvik%20Suryadevara%20and%20Sri%20Sai%20Dinesh%20Jaliparthi%20and%20Noah%20Michael%20Prudlo%20and%20Mark%20David%20Marino%20and%20Jeremy%20Palacio%20and%20Rithvik%20Akula%20and%20Hong-Yu%20Zhou%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Scott%20J.%20Adams%20and%20Hassan%20Rayhan%20AlOmaish%20and%20Pranav%20Rajpurkar&entry.1292438233=%20%20We%20present%20ReXGroundingCT%2C%20the%20first%20publicly%20available%20dataset%20to%20link%0Afree-text%20radiology%20findings%20with%20pixel-level%20segmentations%20in%203D%20chest%20CT%0Ascans%20that%20is%20manually%20annotated.%20While%20prior%20datasets%20have%20relied%20on%0Astructured%20labels%20or%20predefined%20categories%2C%20ReXGroundingCT%20captures%20the%20full%0Aexpressiveness%20of%20clinical%20language%20represented%20in%20free%20text%20and%20grounds%20it%20to%0Aspatially%20localized%203D%20segmentation%20annotations%20in%20volumetric%20imaging.%20This%0Aaddresses%20a%20critical%20gap%20in%20medical%20AI%3A%20the%20ability%20to%20connect%20complex%2C%0Adescriptive%20text%2C%20such%20as%20%223%20mm%20nodule%20in%20the%20left%20lower%20lobe%22%2C%20to%20its%20precise%0Aanatomical%20location%20in%20three-dimensional%20space%2C%20a%20capability%20essential%20for%0Agrounded%20radiology%20report%20generation%20systems.%20The%20dataset%20comprises%203%2C142%0Anon-contrast%20chest%20CT%20scans%20paired%20with%20standardized%20radiology%20reports%20from%20the%0ACT-RATE%20dataset.%20Using%20a%20systematic%20three-stage%20pipeline%2C%20GPT-4%20was%20used%20to%0Aextract%20positive%20lung%20and%20pleural%20findings%2C%20which%20were%20then%20manually%20segmented%0Aby%20expert%20annotators.%20A%20total%20of%208%2C028%20findings%20across%2016%2C301%20entities%20were%0Aannotated%2C%20with%20quality%20control%20performed%20by%20board-certified%20radiologists.%0AApproximately%2079%25%20of%20findings%20are%20focal%20abnormalities%2C%20while%2021%25%20are%20non-focal.%0AThe%20training%20set%20includes%20up%20to%20three%20representative%20segmentations%20per%20finding%2C%0Awhile%20the%20validation%20and%20test%20sets%20contain%20exhaustive%20labels%20for%20each%20finding%0Aentity.%20ReXGroundingCT%20establishes%20a%20new%20benchmark%20for%20developing%20and%0Aevaluating%20sentence-level%20grounding%20and%20free-text%20medical%20segmentation%20models%0Ain%20chest%20CT.%20The%20dataset%20can%20be%20accessed%20at%0Ahttps%3A//huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22030v1&entry.124074799=Read"},
{"title": "Persistent Backdoor Attacks in Continual Learning", "author": "Zhen Guo and Abhinav Kumar and Reza Tourani", "abstract": "  Backdoor attacks pose a significant threat to neural networks, enabling\nadversaries to manipulate model outputs on specific inputs, often with\ndevastating consequences, especially in critical applications. While backdoor\nattacks have been studied in various contexts, little attention has been given\nto their practicality and persistence in continual learning, particularly in\nunderstanding how the continual updates to model parameters, as new data\ndistributions are learned and integrated, impact the effectiveness of these\nattacks over time. To address this gap, we introduce two persistent backdoor\nattacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal\nadversarial influence. Our blind task backdoor subtly alters the loss\ncomputation without direct control over the training process, while the latent\ntask backdoor influences only a single task's training, with all other tasks\ntrained benignly. We evaluate these attacks under various configurations,\ndemonstrating their efficacy with static, dynamic, physical, and semantic\ntriggers. Our results show that both attacks consistently achieve high success\nrates across different continual learning algorithms, while effectively evading\nstate-of-the-art defenses, such as SentiNet and I-BAU.\n", "link": "http://arxiv.org/abs/2409.13864v3", "date": "2025-07-29", "relevancy": 2.4641, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.507}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Backdoor%20Attacks%20in%20Continual%20Learning&body=Title%3A%20Persistent%20Backdoor%20Attacks%20in%20Continual%20Learning%0AAuthor%3A%20Zhen%20Guo%20and%20Abhinav%20Kumar%20and%20Reza%20Tourani%0AAbstract%3A%20%20%20Backdoor%20attacks%20pose%20a%20significant%20threat%20to%20neural%20networks%2C%20enabling%0Aadversaries%20to%20manipulate%20model%20outputs%20on%20specific%20inputs%2C%20often%20with%0Adevastating%20consequences%2C%20especially%20in%20critical%20applications.%20While%20backdoor%0Aattacks%20have%20been%20studied%20in%20various%20contexts%2C%20little%20attention%20has%20been%20given%0Ato%20their%20practicality%20and%20persistence%20in%20continual%20learning%2C%20particularly%20in%0Aunderstanding%20how%20the%20continual%20updates%20to%20model%20parameters%2C%20as%20new%20data%0Adistributions%20are%20learned%20and%20integrated%2C%20impact%20the%20effectiveness%20of%20these%0Aattacks%20over%20time.%20To%20address%20this%20gap%2C%20we%20introduce%20two%20persistent%20backdoor%0Aattacks-Blind%20Task%20Backdoor%20and%20Latent%20Task%20Backdoor-each%20leveraging%20minimal%0Aadversarial%20influence.%20Our%20blind%20task%20backdoor%20subtly%20alters%20the%20loss%0Acomputation%20without%20direct%20control%20over%20the%20training%20process%2C%20while%20the%20latent%0Atask%20backdoor%20influences%20only%20a%20single%20task%27s%20training%2C%20with%20all%20other%20tasks%0Atrained%20benignly.%20We%20evaluate%20these%20attacks%20under%20various%20configurations%2C%0Ademonstrating%20their%20efficacy%20with%20static%2C%20dynamic%2C%20physical%2C%20and%20semantic%0Atriggers.%20Our%20results%20show%20that%20both%20attacks%20consistently%20achieve%20high%20success%0Arates%20across%20different%20continual%20learning%20algorithms%2C%20while%20effectively%20evading%0Astate-of-the-art%20defenses%2C%20such%20as%20SentiNet%20and%20I-BAU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13864v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Backdoor%2520Attacks%2520in%2520Continual%2520Learning%26entry.906535625%3DZhen%2520Guo%2520and%2520Abhinav%2520Kumar%2520and%2520Reza%2520Tourani%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520pose%2520a%2520significant%2520threat%2520to%2520neural%2520networks%252C%2520enabling%250Aadversaries%2520to%2520manipulate%2520model%2520outputs%2520on%2520specific%2520inputs%252C%2520often%2520with%250Adevastating%2520consequences%252C%2520especially%2520in%2520critical%2520applications.%2520While%2520backdoor%250Aattacks%2520have%2520been%2520studied%2520in%2520various%2520contexts%252C%2520little%2520attention%2520has%2520been%2520given%250Ato%2520their%2520practicality%2520and%2520persistence%2520in%2520continual%2520learning%252C%2520particularly%2520in%250Aunderstanding%2520how%2520the%2520continual%2520updates%2520to%2520model%2520parameters%252C%2520as%2520new%2520data%250Adistributions%2520are%2520learned%2520and%2520integrated%252C%2520impact%2520the%2520effectiveness%2520of%2520these%250Aattacks%2520over%2520time.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520two%2520persistent%2520backdoor%250Aattacks-Blind%2520Task%2520Backdoor%2520and%2520Latent%2520Task%2520Backdoor-each%2520leveraging%2520minimal%250Aadversarial%2520influence.%2520Our%2520blind%2520task%2520backdoor%2520subtly%2520alters%2520the%2520loss%250Acomputation%2520without%2520direct%2520control%2520over%2520the%2520training%2520process%252C%2520while%2520the%2520latent%250Atask%2520backdoor%2520influences%2520only%2520a%2520single%2520task%2527s%2520training%252C%2520with%2520all%2520other%2520tasks%250Atrained%2520benignly.%2520We%2520evaluate%2520these%2520attacks%2520under%2520various%2520configurations%252C%250Ademonstrating%2520their%2520efficacy%2520with%2520static%252C%2520dynamic%252C%2520physical%252C%2520and%2520semantic%250Atriggers.%2520Our%2520results%2520show%2520that%2520both%2520attacks%2520consistently%2520achieve%2520high%2520success%250Arates%2520across%2520different%2520continual%2520learning%2520algorithms%252C%2520while%2520effectively%2520evading%250Astate-of-the-art%2520defenses%252C%2520such%2520as%2520SentiNet%2520and%2520I-BAU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13864v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Backdoor%20Attacks%20in%20Continual%20Learning&entry.906535625=Zhen%20Guo%20and%20Abhinav%20Kumar%20and%20Reza%20Tourani&entry.1292438233=%20%20Backdoor%20attacks%20pose%20a%20significant%20threat%20to%20neural%20networks%2C%20enabling%0Aadversaries%20to%20manipulate%20model%20outputs%20on%20specific%20inputs%2C%20often%20with%0Adevastating%20consequences%2C%20especially%20in%20critical%20applications.%20While%20backdoor%0Aattacks%20have%20been%20studied%20in%20various%20contexts%2C%20little%20attention%20has%20been%20given%0Ato%20their%20practicality%20and%20persistence%20in%20continual%20learning%2C%20particularly%20in%0Aunderstanding%20how%20the%20continual%20updates%20to%20model%20parameters%2C%20as%20new%20data%0Adistributions%20are%20learned%20and%20integrated%2C%20impact%20the%20effectiveness%20of%20these%0Aattacks%20over%20time.%20To%20address%20this%20gap%2C%20we%20introduce%20two%20persistent%20backdoor%0Aattacks-Blind%20Task%20Backdoor%20and%20Latent%20Task%20Backdoor-each%20leveraging%20minimal%0Aadversarial%20influence.%20Our%20blind%20task%20backdoor%20subtly%20alters%20the%20loss%0Acomputation%20without%20direct%20control%20over%20the%20training%20process%2C%20while%20the%20latent%0Atask%20backdoor%20influences%20only%20a%20single%20task%27s%20training%2C%20with%20all%20other%20tasks%0Atrained%20benignly.%20We%20evaluate%20these%20attacks%20under%20various%20configurations%2C%0Ademonstrating%20their%20efficacy%20with%20static%2C%20dynamic%2C%20physical%2C%20and%20semantic%0Atriggers.%20Our%20results%20show%20that%20both%20attacks%20consistently%20achieve%20high%20success%0Arates%20across%20different%20continual%20learning%20algorithms%2C%20while%20effectively%20evading%0Astate-of-the-art%20defenses%2C%20such%20as%20SentiNet%20and%20I-BAU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13864v3&entry.124074799=Read"},
{"title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model\n  for Open-vocabulary Situation Recognition", "author": "Chen Cai and Tianyi Liu and Jianjun Gao and Wenyang Liu and Kejun Wu and Ruoyu Wang and Yi Wang and Soo Chin Liew", "abstract": "  Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.\n", "link": "http://arxiv.org/abs/2507.14686v2", "date": "2025-07-29", "relevancy": 2.3848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6025}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Semantics%2C%20Scene%20to%20Instance-awareness%3A%20Distilling%20Foundation%20Model%0A%20%20for%20Open-vocabulary%20Situation%20Recognition&body=Title%3A%20From%20Semantics%2C%20Scene%20to%20Instance-awareness%3A%20Distilling%20Foundation%20Model%0A%20%20for%20Open-vocabulary%20Situation%20Recognition%0AAuthor%3A%20Chen%20Cai%20and%20Tianyi%20Liu%20and%20Jianjun%20Gao%20and%20Wenyang%20Liu%20and%20Kejun%20Wu%20and%20Ruoyu%20Wang%20and%20Yi%20Wang%20and%20Soo%20Chin%20Liew%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20exhibit%20strong%20zero-shot%0Aabilities%20but%20struggle%20with%20complex%20Grounded%20Situation%20Recognition%20%28GSR%29%20and%0Aare%20resource-intensive%20for%20edge%20device%20deployment.%20Meanwhile%2C%20conventional%20GSR%0Amodels%20often%20lack%20generalization%20ability%2C%20falling%20short%20in%20recognizing%20unseen%0Aand%20rare%20situations.%20In%20this%20paper%2C%20we%20exploit%20transferring%20knowledge%20from%20a%0Ateacher%20MLLM%20to%20a%20small%20GSR%20model%20to%20enhance%20its%20generalization%20and%20zero-shot%0Aabilities%2C%20thereby%20introducing%20the%20task%20of%20Open-vocabulary%20Grounded%20Situation%0ARecognition%20%28Ov-GSR%29.%20To%20achieve%20this%2C%20we%20propose%20Multimodal%20Interactive%20Prompt%0ADistillation%20%28MIPD%29%2C%20a%20novel%20framework%20that%20distills%20enriched%20multimodal%0Aknowledge%20from%20the%20foundation%20model%2C%20enabling%20the%20student%20Ov-GSR%20model%20to%0Arecognize%20unseen%20situations%20and%20be%20better%20aware%20of%20rare%20situations.%0ASpecifically%2C%20the%20MIPD%20framework%20first%20leverages%20the%20LLM-based%20Judgmental%0ARationales%20Generator%20%28JRG%29%20to%20construct%20positive%20and%20negative%20glimpse%20and%20gaze%0Arationales%20enriched%20with%20contextual%20semantic%20information.%20The%20proposed%0Ascene-aware%20and%20instance-perception%20prompts%20are%20then%20introduced%20to%20align%0Arationales%20with%20visual%20information%20from%20the%20MLLM%20teacher%20via%20the%0ANegative-Guided%20Multimodal%20Prompting%20Alignment%20%28NMPA%29%20module%2C%20effectively%0Acapturing%20holistic%20and%20perceptual%20multimodal%20knowledge.%20Finally%2C%20the%20aligned%0Amultimodal%20knowledge%20is%20distilled%20into%20the%20student%20Ov-GSR%20model%2C%20providing%20a%0Astronger%20foundation%20for%20generalization%20that%20enhances%20situation%20understanding%2C%0Abridges%20the%20gap%20between%20seen%20and%20unseen%20scenarios%2C%20and%20mitigates%20prediction%0Abias%20in%20rare%20cases.%20We%20evaluate%20MIPD%20on%20the%20refined%20Ov-SWiG%20dataset%2C%20achieving%0Asuperior%20performance%20on%20seen%2C%20rare%2C%20and%20unseen%20situations%2C%20and%20further%0Ademonstrate%20improved%20unseen%20detection%20on%20the%20HICO-DET%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Semantics%252C%2520Scene%2520to%2520Instance-awareness%253A%2520Distilling%2520Foundation%2520Model%250A%2520%2520for%2520Open-vocabulary%2520Situation%2520Recognition%26entry.906535625%3DChen%2520Cai%2520and%2520Tianyi%2520Liu%2520and%2520Jianjun%2520Gao%2520and%2520Wenyang%2520Liu%2520and%2520Kejun%2520Wu%2520and%2520Ruoyu%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Soo%2520Chin%2520Liew%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520exhibit%2520strong%2520zero-shot%250Aabilities%2520but%2520struggle%2520with%2520complex%2520Grounded%2520Situation%2520Recognition%2520%2528GSR%2529%2520and%250Aare%2520resource-intensive%2520for%2520edge%2520device%2520deployment.%2520Meanwhile%252C%2520conventional%2520GSR%250Amodels%2520often%2520lack%2520generalization%2520ability%252C%2520falling%2520short%2520in%2520recognizing%2520unseen%250Aand%2520rare%2520situations.%2520In%2520this%2520paper%252C%2520we%2520exploit%2520transferring%2520knowledge%2520from%2520a%250Ateacher%2520MLLM%2520to%2520a%2520small%2520GSR%2520model%2520to%2520enhance%2520its%2520generalization%2520and%2520zero-shot%250Aabilities%252C%2520thereby%2520introducing%2520the%2520task%2520of%2520Open-vocabulary%2520Grounded%2520Situation%250ARecognition%2520%2528Ov-GSR%2529.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520Multimodal%2520Interactive%2520Prompt%250ADistillation%2520%2528MIPD%2529%252C%2520a%2520novel%2520framework%2520that%2520distills%2520enriched%2520multimodal%250Aknowledge%2520from%2520the%2520foundation%2520model%252C%2520enabling%2520the%2520student%2520Ov-GSR%2520model%2520to%250Arecognize%2520unseen%2520situations%2520and%2520be%2520better%2520aware%2520of%2520rare%2520situations.%250ASpecifically%252C%2520the%2520MIPD%2520framework%2520first%2520leverages%2520the%2520LLM-based%2520Judgmental%250ARationales%2520Generator%2520%2528JRG%2529%2520to%2520construct%2520positive%2520and%2520negative%2520glimpse%2520and%2520gaze%250Arationales%2520enriched%2520with%2520contextual%2520semantic%2520information.%2520The%2520proposed%250Ascene-aware%2520and%2520instance-perception%2520prompts%2520are%2520then%2520introduced%2520to%2520align%250Arationales%2520with%2520visual%2520information%2520from%2520the%2520MLLM%2520teacher%2520via%2520the%250ANegative-Guided%2520Multimodal%2520Prompting%2520Alignment%2520%2528NMPA%2529%2520module%252C%2520effectively%250Acapturing%2520holistic%2520and%2520perceptual%2520multimodal%2520knowledge.%2520Finally%252C%2520the%2520aligned%250Amultimodal%2520knowledge%2520is%2520distilled%2520into%2520the%2520student%2520Ov-GSR%2520model%252C%2520providing%2520a%250Astronger%2520foundation%2520for%2520generalization%2520that%2520enhances%2520situation%2520understanding%252C%250Abridges%2520the%2520gap%2520between%2520seen%2520and%2520unseen%2520scenarios%252C%2520and%2520mitigates%2520prediction%250Abias%2520in%2520rare%2520cases.%2520We%2520evaluate%2520MIPD%2520on%2520the%2520refined%2520Ov-SWiG%2520dataset%252C%2520achieving%250Asuperior%2520performance%2520on%2520seen%252C%2520rare%252C%2520and%2520unseen%2520situations%252C%2520and%2520further%250Ademonstrate%2520improved%2520unseen%2520detection%2520on%2520the%2520HICO-DET%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Semantics%2C%20Scene%20to%20Instance-awareness%3A%20Distilling%20Foundation%20Model%0A%20%20for%20Open-vocabulary%20Situation%20Recognition&entry.906535625=Chen%20Cai%20and%20Tianyi%20Liu%20and%20Jianjun%20Gao%20and%20Wenyang%20Liu%20and%20Kejun%20Wu%20and%20Ruoyu%20Wang%20and%20Yi%20Wang%20and%20Soo%20Chin%20Liew&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20exhibit%20strong%20zero-shot%0Aabilities%20but%20struggle%20with%20complex%20Grounded%20Situation%20Recognition%20%28GSR%29%20and%0Aare%20resource-intensive%20for%20edge%20device%20deployment.%20Meanwhile%2C%20conventional%20GSR%0Amodels%20often%20lack%20generalization%20ability%2C%20falling%20short%20in%20recognizing%20unseen%0Aand%20rare%20situations.%20In%20this%20paper%2C%20we%20exploit%20transferring%20knowledge%20from%20a%0Ateacher%20MLLM%20to%20a%20small%20GSR%20model%20to%20enhance%20its%20generalization%20and%20zero-shot%0Aabilities%2C%20thereby%20introducing%20the%20task%20of%20Open-vocabulary%20Grounded%20Situation%0ARecognition%20%28Ov-GSR%29.%20To%20achieve%20this%2C%20we%20propose%20Multimodal%20Interactive%20Prompt%0ADistillation%20%28MIPD%29%2C%20a%20novel%20framework%20that%20distills%20enriched%20multimodal%0Aknowledge%20from%20the%20foundation%20model%2C%20enabling%20the%20student%20Ov-GSR%20model%20to%0Arecognize%20unseen%20situations%20and%20be%20better%20aware%20of%20rare%20situations.%0ASpecifically%2C%20the%20MIPD%20framework%20first%20leverages%20the%20LLM-based%20Judgmental%0ARationales%20Generator%20%28JRG%29%20to%20construct%20positive%20and%20negative%20glimpse%20and%20gaze%0Arationales%20enriched%20with%20contextual%20semantic%20information.%20The%20proposed%0Ascene-aware%20and%20instance-perception%20prompts%20are%20then%20introduced%20to%20align%0Arationales%20with%20visual%20information%20from%20the%20MLLM%20teacher%20via%20the%0ANegative-Guided%20Multimodal%20Prompting%20Alignment%20%28NMPA%29%20module%2C%20effectively%0Acapturing%20holistic%20and%20perceptual%20multimodal%20knowledge.%20Finally%2C%20the%20aligned%0Amultimodal%20knowledge%20is%20distilled%20into%20the%20student%20Ov-GSR%20model%2C%20providing%20a%0Astronger%20foundation%20for%20generalization%20that%20enhances%20situation%20understanding%2C%0Abridges%20the%20gap%20between%20seen%20and%20unseen%20scenarios%2C%20and%20mitigates%20prediction%0Abias%20in%20rare%20cases.%20We%20evaluate%20MIPD%20on%20the%20refined%20Ov-SWiG%20dataset%2C%20achieving%0Asuperior%20performance%20on%20seen%2C%20rare%2C%20and%20unseen%20situations%2C%20and%20further%0Ademonstrate%20improved%20unseen%20detection%20on%20the%20HICO-DET%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14686v2&entry.124074799=Read"},
{"title": "Foundation Models for Demand Forecasting via Dual-Strategy Ensembling", "author": "Wei Yang and Defu Cao and Yan Liu", "abstract": "  Accurate demand forecasting is critical for supply chain optimization, yet\nremains difficult in practice due to hierarchical complexity, domain shifts,\nand evolving external factors. While recent foundation models offer strong\npotential for time series forecasting, they often suffer from architectural\nrigidity and limited robustness under distributional change. In this paper, we\npropose a unified ensemble framework that enhances the performance of\nfoundation models for sales forecasting in real-world supply chains. Our method\ncombines two complementary strategies: (1) Hierarchical Ensemble (HE), which\npartitions training and inference by semantic levels (e.g., store, category,\ndepartment) to capture localized patterns; and (2) Architectural Ensemble (AE),\nwhich integrates predictions from diverse model backbones to mitigate bias and\nimprove stability. We conduct extensive experiments on the M5 benchmark and\nthree external sales datasets, covering both in-domain and zero-shot\nforecasting. Results show that our approach consistently outperforms strong\nbaselines, improves accuracy across hierarchical levels, and provides a simple\nyet effective mechanism for boosting generalization in complex forecasting\nenvironments.\n", "link": "http://arxiv.org/abs/2507.22053v1", "date": "2025-07-29", "relevancy": 2.3526, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20for%20Demand%20Forecasting%20via%20Dual-Strategy%20Ensembling&body=Title%3A%20Foundation%20Models%20for%20Demand%20Forecasting%20via%20Dual-Strategy%20Ensembling%0AAuthor%3A%20Wei%20Yang%20and%20Defu%20Cao%20and%20Yan%20Liu%0AAbstract%3A%20%20%20Accurate%20demand%20forecasting%20is%20critical%20for%20supply%20chain%20optimization%2C%20yet%0Aremains%20difficult%20in%20practice%20due%20to%20hierarchical%20complexity%2C%20domain%20shifts%2C%0Aand%20evolving%20external%20factors.%20While%20recent%20foundation%20models%20offer%20strong%0Apotential%20for%20time%20series%20forecasting%2C%20they%20often%20suffer%20from%20architectural%0Arigidity%20and%20limited%20robustness%20under%20distributional%20change.%20In%20this%20paper%2C%20we%0Apropose%20a%20unified%20ensemble%20framework%20that%20enhances%20the%20performance%20of%0Afoundation%20models%20for%20sales%20forecasting%20in%20real-world%20supply%20chains.%20Our%20method%0Acombines%20two%20complementary%20strategies%3A%20%281%29%20Hierarchical%20Ensemble%20%28HE%29%2C%20which%0Apartitions%20training%20and%20inference%20by%20semantic%20levels%20%28e.g.%2C%20store%2C%20category%2C%0Adepartment%29%20to%20capture%20localized%20patterns%3B%20and%20%282%29%20Architectural%20Ensemble%20%28AE%29%2C%0Awhich%20integrates%20predictions%20from%20diverse%20model%20backbones%20to%20mitigate%20bias%20and%0Aimprove%20stability.%20We%20conduct%20extensive%20experiments%20on%20the%20M5%20benchmark%20and%0Athree%20external%20sales%20datasets%2C%20covering%20both%20in-domain%20and%20zero-shot%0Aforecasting.%20Results%20show%20that%20our%20approach%20consistently%20outperforms%20strong%0Abaselines%2C%20improves%20accuracy%20across%20hierarchical%20levels%2C%20and%20provides%20a%20simple%0Ayet%20effective%20mechanism%20for%20boosting%20generalization%20in%20complex%20forecasting%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520for%2520Demand%2520Forecasting%2520via%2520Dual-Strategy%2520Ensembling%26entry.906535625%3DWei%2520Yang%2520and%2520Defu%2520Cao%2520and%2520Yan%2520Liu%26entry.1292438233%3D%2520%2520Accurate%2520demand%2520forecasting%2520is%2520critical%2520for%2520supply%2520chain%2520optimization%252C%2520yet%250Aremains%2520difficult%2520in%2520practice%2520due%2520to%2520hierarchical%2520complexity%252C%2520domain%2520shifts%252C%250Aand%2520evolving%2520external%2520factors.%2520While%2520recent%2520foundation%2520models%2520offer%2520strong%250Apotential%2520for%2520time%2520series%2520forecasting%252C%2520they%2520often%2520suffer%2520from%2520architectural%250Arigidity%2520and%2520limited%2520robustness%2520under%2520distributional%2520change.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520unified%2520ensemble%2520framework%2520that%2520enhances%2520the%2520performance%2520of%250Afoundation%2520models%2520for%2520sales%2520forecasting%2520in%2520real-world%2520supply%2520chains.%2520Our%2520method%250Acombines%2520two%2520complementary%2520strategies%253A%2520%25281%2529%2520Hierarchical%2520Ensemble%2520%2528HE%2529%252C%2520which%250Apartitions%2520training%2520and%2520inference%2520by%2520semantic%2520levels%2520%2528e.g.%252C%2520store%252C%2520category%252C%250Adepartment%2529%2520to%2520capture%2520localized%2520patterns%253B%2520and%2520%25282%2529%2520Architectural%2520Ensemble%2520%2528AE%2529%252C%250Awhich%2520integrates%2520predictions%2520from%2520diverse%2520model%2520backbones%2520to%2520mitigate%2520bias%2520and%250Aimprove%2520stability.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520M5%2520benchmark%2520and%250Athree%2520external%2520sales%2520datasets%252C%2520covering%2520both%2520in-domain%2520and%2520zero-shot%250Aforecasting.%2520Results%2520show%2520that%2520our%2520approach%2520consistently%2520outperforms%2520strong%250Abaselines%252C%2520improves%2520accuracy%2520across%2520hierarchical%2520levels%252C%2520and%2520provides%2520a%2520simple%250Ayet%2520effective%2520mechanism%2520for%2520boosting%2520generalization%2520in%2520complex%2520forecasting%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20for%20Demand%20Forecasting%20via%20Dual-Strategy%20Ensembling&entry.906535625=Wei%20Yang%20and%20Defu%20Cao%20and%20Yan%20Liu&entry.1292438233=%20%20Accurate%20demand%20forecasting%20is%20critical%20for%20supply%20chain%20optimization%2C%20yet%0Aremains%20difficult%20in%20practice%20due%20to%20hierarchical%20complexity%2C%20domain%20shifts%2C%0Aand%20evolving%20external%20factors.%20While%20recent%20foundation%20models%20offer%20strong%0Apotential%20for%20time%20series%20forecasting%2C%20they%20often%20suffer%20from%20architectural%0Arigidity%20and%20limited%20robustness%20under%20distributional%20change.%20In%20this%20paper%2C%20we%0Apropose%20a%20unified%20ensemble%20framework%20that%20enhances%20the%20performance%20of%0Afoundation%20models%20for%20sales%20forecasting%20in%20real-world%20supply%20chains.%20Our%20method%0Acombines%20two%20complementary%20strategies%3A%20%281%29%20Hierarchical%20Ensemble%20%28HE%29%2C%20which%0Apartitions%20training%20and%20inference%20by%20semantic%20levels%20%28e.g.%2C%20store%2C%20category%2C%0Adepartment%29%20to%20capture%20localized%20patterns%3B%20and%20%282%29%20Architectural%20Ensemble%20%28AE%29%2C%0Awhich%20integrates%20predictions%20from%20diverse%20model%20backbones%20to%20mitigate%20bias%20and%0Aimprove%20stability.%20We%20conduct%20extensive%20experiments%20on%20the%20M5%20benchmark%20and%0Athree%20external%20sales%20datasets%2C%20covering%20both%20in-domain%20and%20zero-shot%0Aforecasting.%20Results%20show%20that%20our%20approach%20consistently%20outperforms%20strong%0Abaselines%2C%20improves%20accuracy%20across%20hierarchical%20levels%2C%20and%20provides%20a%20simple%0Ayet%20effective%20mechanism%20for%20boosting%20generalization%20in%20complex%20forecasting%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22053v1&entry.124074799=Read"},
{"title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks", "author": "Mohamed Sana and Nicola Piovesan and Antonio De Domenico and Yibin Kang and Haozhe Zhang and Merouane Debbah and Fadhel Ayed", "abstract": "  Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.\n", "link": "http://arxiv.org/abs/2507.21974v1", "date": "2025-07-29", "relevancy": 2.3056, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Language%20Models%20for%20Root%20Cause%20Analysis%20in%205G%20Wireless%0A%20%20Networks&body=Title%3A%20Reasoning%20Language%20Models%20for%20Root%20Cause%20Analysis%20in%205G%20Wireless%0A%20%20Networks%0AAuthor%3A%20Mohamed%20Sana%20and%20Nicola%20Piovesan%20and%20Antonio%20De%20Domenico%20and%20Yibin%20Kang%20and%20Haozhe%20Zhang%20and%20Merouane%20Debbah%20and%20Fadhel%20Ayed%0AAbstract%3A%20%20%20Root%20Cause%20Analysis%20%28RCA%29%20in%20mobile%20networks%20remains%20a%20challenging%20task%20due%0Ato%20the%20need%20for%20interpretability%2C%20domain%20expertise%2C%20and%20causal%20reasoning.%20In%0Athis%20work%2C%20we%20propose%20a%20lightweight%20framework%20that%20leverages%20Large%20Language%0AModels%20%28LLMs%29%20for%20RCA.%20To%20do%20so%2C%20we%20introduce%20TeleLogs%2C%20a%20curated%20dataset%20of%0Aannotated%20troubleshooting%20problems%20designed%20to%20benchmark%20RCA%20capabilities.%20Our%0Aevaluation%20reveals%20that%20existing%20open-source%20reasoning%20LLMs%20struggle%20with%20these%0Aproblems%2C%20underscoring%20the%20need%20for%20domain-specific%20adaptation.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20two-stage%20training%20methodology%20that%20combines%20supervised%0Afine-tuning%20with%20reinforcement%20learning%20to%20improve%20the%20accuracy%20and%20reasoning%0Aquality%20of%20LLMs.%20The%20proposed%20approach%20fine-tunes%20a%20series%20of%20RCA%20models%20to%0Aintegrate%20domain%20knowledge%20and%20generate%20structured%2C%20multi-step%20diagnostic%0Aexplanations%2C%20improving%20both%20interpretability%20and%20effectiveness.%20Extensive%0Aexperiments%20across%20multiple%20LLM%20sizes%20show%20significant%20performance%20gains%20over%0Astate-of-the-art%20reasoning%20and%20non-reasoning%20models%2C%20including%20strong%0Ageneralization%20to%20randomized%20test%20variants.%20These%20results%20demonstrate%20the%0Apromise%20of%20domain-adapted%2C%20reasoning-enhanced%20LLMs%20for%20practical%20and%0Aexplainable%20RCA%20in%20network%20operation%20and%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Language%2520Models%2520for%2520Root%2520Cause%2520Analysis%2520in%25205G%2520Wireless%250A%2520%2520Networks%26entry.906535625%3DMohamed%2520Sana%2520and%2520Nicola%2520Piovesan%2520and%2520Antonio%2520De%2520Domenico%2520and%2520Yibin%2520Kang%2520and%2520Haozhe%2520Zhang%2520and%2520Merouane%2520Debbah%2520and%2520Fadhel%2520Ayed%26entry.1292438233%3D%2520%2520Root%2520Cause%2520Analysis%2520%2528RCA%2529%2520in%2520mobile%2520networks%2520remains%2520a%2520challenging%2520task%2520due%250Ato%2520the%2520need%2520for%2520interpretability%252C%2520domain%2520expertise%252C%2520and%2520causal%2520reasoning.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520lightweight%2520framework%2520that%2520leverages%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520for%2520RCA.%2520To%2520do%2520so%252C%2520we%2520introduce%2520TeleLogs%252C%2520a%2520curated%2520dataset%2520of%250Aannotated%2520troubleshooting%2520problems%2520designed%2520to%2520benchmark%2520RCA%2520capabilities.%2520Our%250Aevaluation%2520reveals%2520that%2520existing%2520open-source%2520reasoning%2520LLMs%2520struggle%2520with%2520these%250Aproblems%252C%2520underscoring%2520the%2520need%2520for%2520domain-specific%2520adaptation.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520two-stage%2520training%2520methodology%2520that%2520combines%2520supervised%250Afine-tuning%2520with%2520reinforcement%2520learning%2520to%2520improve%2520the%2520accuracy%2520and%2520reasoning%250Aquality%2520of%2520LLMs.%2520The%2520proposed%2520approach%2520fine-tunes%2520a%2520series%2520of%2520RCA%2520models%2520to%250Aintegrate%2520domain%2520knowledge%2520and%2520generate%2520structured%252C%2520multi-step%2520diagnostic%250Aexplanations%252C%2520improving%2520both%2520interpretability%2520and%2520effectiveness.%2520Extensive%250Aexperiments%2520across%2520multiple%2520LLM%2520sizes%2520show%2520significant%2520performance%2520gains%2520over%250Astate-of-the-art%2520reasoning%2520and%2520non-reasoning%2520models%252C%2520including%2520strong%250Ageneralization%2520to%2520randomized%2520test%2520variants.%2520These%2520results%2520demonstrate%2520the%250Apromise%2520of%2520domain-adapted%252C%2520reasoning-enhanced%2520LLMs%2520for%2520practical%2520and%250Aexplainable%2520RCA%2520in%2520network%2520operation%2520and%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Language%20Models%20for%20Root%20Cause%20Analysis%20in%205G%20Wireless%0A%20%20Networks&entry.906535625=Mohamed%20Sana%20and%20Nicola%20Piovesan%20and%20Antonio%20De%20Domenico%20and%20Yibin%20Kang%20and%20Haozhe%20Zhang%20and%20Merouane%20Debbah%20and%20Fadhel%20Ayed&entry.1292438233=%20%20Root%20Cause%20Analysis%20%28RCA%29%20in%20mobile%20networks%20remains%20a%20challenging%20task%20due%0Ato%20the%20need%20for%20interpretability%2C%20domain%20expertise%2C%20and%20causal%20reasoning.%20In%0Athis%20work%2C%20we%20propose%20a%20lightweight%20framework%20that%20leverages%20Large%20Language%0AModels%20%28LLMs%29%20for%20RCA.%20To%20do%20so%2C%20we%20introduce%20TeleLogs%2C%20a%20curated%20dataset%20of%0Aannotated%20troubleshooting%20problems%20designed%20to%20benchmark%20RCA%20capabilities.%20Our%0Aevaluation%20reveals%20that%20existing%20open-source%20reasoning%20LLMs%20struggle%20with%20these%0Aproblems%2C%20underscoring%20the%20need%20for%20domain-specific%20adaptation.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20two-stage%20training%20methodology%20that%20combines%20supervised%0Afine-tuning%20with%20reinforcement%20learning%20to%20improve%20the%20accuracy%20and%20reasoning%0Aquality%20of%20LLMs.%20The%20proposed%20approach%20fine-tunes%20a%20series%20of%20RCA%20models%20to%0Aintegrate%20domain%20knowledge%20and%20generate%20structured%2C%20multi-step%20diagnostic%0Aexplanations%2C%20improving%20both%20interpretability%20and%20effectiveness.%20Extensive%0Aexperiments%20across%20multiple%20LLM%20sizes%20show%20significant%20performance%20gains%20over%0Astate-of-the-art%20reasoning%20and%20non-reasoning%20models%2C%20including%20strong%0Ageneralization%20to%20randomized%20test%20variants.%20These%20results%20demonstrate%20the%0Apromise%20of%20domain-adapted%2C%20reasoning-enhanced%20LLMs%20for%20practical%20and%0Aexplainable%20RCA%20in%20network%20operation%20and%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21974v1&entry.124074799=Read"},
{"title": "See Different, Think Better: Visual Variations Mitigating Hallucinations\n  in LVLMs", "author": "Ziyun Dai and Xiaoqiang Li and Shaohua Zhang and Yuanchen Wu and Jide Li", "abstract": "  Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in visual understanding and multimodal reasoning. However, LVLMs\nfrequently exhibit hallucination phenomena, manifesting as the generated\ntextual responses that demonstrate inconsistencies with the provided visual\ncontent. Existing hallucination mitigation methods are predominantly\ntext-centric, the challenges of visual-semantic alignment significantly limit\ntheir effectiveness, especially when confronted with fine-grained visual\nunderstanding scenarios. To this end, this paper presents ViHallu, a\nVision-Centric Hallucination mitigation framework that enhances visual-semantic\nalignment through Visual Variation Image Generation and Visual Instruction\nConstruction. ViHallu introduces \\textbf{\\textit{visual variation images}} with\ncontrollable visual alterations while maintaining the overall image structure.\nThese images, combined with carefully constructed visual instructions, enable\nLVLMs to better understand fine-grained visual content through fine-tuning,\nallowing models to more precisely capture the correspondence between visual\ncontent and text, thereby enhancing visual-semantic alignment. Extensive\nexperiments on multiple benchmarks show that ViHallu effectively enhances\nmodels' fine-grained visual understanding while significantly reducing\nhallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual\ninstruction dataset specifically designed for hallucination mitigation and\nvisual-semantic alignment. Code is available at\nhttps://github.com/oliviadzy/ViHallu.\n", "link": "http://arxiv.org/abs/2507.22003v1", "date": "2025-07-29", "relevancy": 2.2304, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5627}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20Different%2C%20Think%20Better%3A%20Visual%20Variations%20Mitigating%20Hallucinations%0A%20%20in%20LVLMs&body=Title%3A%20See%20Different%2C%20Think%20Better%3A%20Visual%20Variations%20Mitigating%20Hallucinations%0A%20%20in%20LVLMs%0AAuthor%3A%20Ziyun%20Dai%20and%20Xiaoqiang%20Li%20and%20Shaohua%20Zhang%20and%20Yuanchen%20Wu%20and%20Jide%20Li%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20visual%20understanding%20and%20multimodal%20reasoning.%20However%2C%20LVLMs%0Afrequently%20exhibit%20hallucination%20phenomena%2C%20manifesting%20as%20the%20generated%0Atextual%20responses%20that%20demonstrate%20inconsistencies%20with%20the%20provided%20visual%0Acontent.%20Existing%20hallucination%20mitigation%20methods%20are%20predominantly%0Atext-centric%2C%20the%20challenges%20of%20visual-semantic%20alignment%20significantly%20limit%0Atheir%20effectiveness%2C%20especially%20when%20confronted%20with%20fine-grained%20visual%0Aunderstanding%20scenarios.%20To%20this%20end%2C%20this%20paper%20presents%20ViHallu%2C%20a%0AVision-Centric%20Hallucination%20mitigation%20framework%20that%20enhances%20visual-semantic%0Aalignment%20through%20Visual%20Variation%20Image%20Generation%20and%20Visual%20Instruction%0AConstruction.%20ViHallu%20introduces%20%5Ctextbf%7B%5Ctextit%7Bvisual%20variation%20images%7D%7D%20with%0Acontrollable%20visual%20alterations%20while%20maintaining%20the%20overall%20image%20structure.%0AThese%20images%2C%20combined%20with%20carefully%20constructed%20visual%20instructions%2C%20enable%0ALVLMs%20to%20better%20understand%20fine-grained%20visual%20content%20through%20fine-tuning%2C%0Aallowing%20models%20to%20more%20precisely%20capture%20the%20correspondence%20between%20visual%0Acontent%20and%20text%2C%20thereby%20enhancing%20visual-semantic%20alignment.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%20show%20that%20ViHallu%20effectively%20enhances%0Amodels%27%20fine-grained%20visual%20understanding%20while%20significantly%20reducing%0Ahallucination%20tendencies.%20Furthermore%2C%20we%20release%20ViHallu-Instruction%2C%20a%20visual%0Ainstruction%20dataset%20specifically%20designed%20for%20hallucination%20mitigation%20and%0Avisual-semantic%20alignment.%20Code%20is%20available%20at%0Ahttps%3A//github.com/oliviadzy/ViHallu.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520Different%252C%2520Think%2520Better%253A%2520Visual%2520Variations%2520Mitigating%2520Hallucinations%250A%2520%2520in%2520LVLMs%26entry.906535625%3DZiyun%2520Dai%2520and%2520Xiaoqiang%2520Li%2520and%2520Shaohua%2520Zhang%2520and%2520Yuanchen%2520Wu%2520and%2520Jide%2520Li%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520visual%2520understanding%2520and%2520multimodal%2520reasoning.%2520However%252C%2520LVLMs%250Afrequently%2520exhibit%2520hallucination%2520phenomena%252C%2520manifesting%2520as%2520the%2520generated%250Atextual%2520responses%2520that%2520demonstrate%2520inconsistencies%2520with%2520the%2520provided%2520visual%250Acontent.%2520Existing%2520hallucination%2520mitigation%2520methods%2520are%2520predominantly%250Atext-centric%252C%2520the%2520challenges%2520of%2520visual-semantic%2520alignment%2520significantly%2520limit%250Atheir%2520effectiveness%252C%2520especially%2520when%2520confronted%2520with%2520fine-grained%2520visual%250Aunderstanding%2520scenarios.%2520To%2520this%2520end%252C%2520this%2520paper%2520presents%2520ViHallu%252C%2520a%250AVision-Centric%2520Hallucination%2520mitigation%2520framework%2520that%2520enhances%2520visual-semantic%250Aalignment%2520through%2520Visual%2520Variation%2520Image%2520Generation%2520and%2520Visual%2520Instruction%250AConstruction.%2520ViHallu%2520introduces%2520%255Ctextbf%257B%255Ctextit%257Bvisual%2520variation%2520images%257D%257D%2520with%250Acontrollable%2520visual%2520alterations%2520while%2520maintaining%2520the%2520overall%2520image%2520structure.%250AThese%2520images%252C%2520combined%2520with%2520carefully%2520constructed%2520visual%2520instructions%252C%2520enable%250ALVLMs%2520to%2520better%2520understand%2520fine-grained%2520visual%2520content%2520through%2520fine-tuning%252C%250Aallowing%2520models%2520to%2520more%2520precisely%2520capture%2520the%2520correspondence%2520between%2520visual%250Acontent%2520and%2520text%252C%2520thereby%2520enhancing%2520visual-semantic%2520alignment.%2520Extensive%250Aexperiments%2520on%2520multiple%2520benchmarks%2520show%2520that%2520ViHallu%2520effectively%2520enhances%250Amodels%2527%2520fine-grained%2520visual%2520understanding%2520while%2520significantly%2520reducing%250Ahallucination%2520tendencies.%2520Furthermore%252C%2520we%2520release%2520ViHallu-Instruction%252C%2520a%2520visual%250Ainstruction%2520dataset%2520specifically%2520designed%2520for%2520hallucination%2520mitigation%2520and%250Avisual-semantic%2520alignment.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/oliviadzy/ViHallu.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20Different%2C%20Think%20Better%3A%20Visual%20Variations%20Mitigating%20Hallucinations%0A%20%20in%20LVLMs&entry.906535625=Ziyun%20Dai%20and%20Xiaoqiang%20Li%20and%20Shaohua%20Zhang%20and%20Yuanchen%20Wu%20and%20Jide%20Li&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20visual%20understanding%20and%20multimodal%20reasoning.%20However%2C%20LVLMs%0Afrequently%20exhibit%20hallucination%20phenomena%2C%20manifesting%20as%20the%20generated%0Atextual%20responses%20that%20demonstrate%20inconsistencies%20with%20the%20provided%20visual%0Acontent.%20Existing%20hallucination%20mitigation%20methods%20are%20predominantly%0Atext-centric%2C%20the%20challenges%20of%20visual-semantic%20alignment%20significantly%20limit%0Atheir%20effectiveness%2C%20especially%20when%20confronted%20with%20fine-grained%20visual%0Aunderstanding%20scenarios.%20To%20this%20end%2C%20this%20paper%20presents%20ViHallu%2C%20a%0AVision-Centric%20Hallucination%20mitigation%20framework%20that%20enhances%20visual-semantic%0Aalignment%20through%20Visual%20Variation%20Image%20Generation%20and%20Visual%20Instruction%0AConstruction.%20ViHallu%20introduces%20%5Ctextbf%7B%5Ctextit%7Bvisual%20variation%20images%7D%7D%20with%0Acontrollable%20visual%20alterations%20while%20maintaining%20the%20overall%20image%20structure.%0AThese%20images%2C%20combined%20with%20carefully%20constructed%20visual%20instructions%2C%20enable%0ALVLMs%20to%20better%20understand%20fine-grained%20visual%20content%20through%20fine-tuning%2C%0Aallowing%20models%20to%20more%20precisely%20capture%20the%20correspondence%20between%20visual%0Acontent%20and%20text%2C%20thereby%20enhancing%20visual-semantic%20alignment.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%20show%20that%20ViHallu%20effectively%20enhances%0Amodels%27%20fine-grained%20visual%20understanding%20while%20significantly%20reducing%0Ahallucination%20tendencies.%20Furthermore%2C%20we%20release%20ViHallu-Instruction%2C%20a%20visual%0Ainstruction%20dataset%20specifically%20designed%20for%20hallucination%20mitigation%20and%0Avisual-semantic%20alignment.%20Code%20is%20available%20at%0Ahttps%3A//github.com/oliviadzy/ViHallu.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22003v1&entry.124074799=Read"},
{"title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with\n  Reinforcement Learning", "author": "Honglin He and Yukai Ma and Wayne Wu and Bolei Zhou", "abstract": "  Navigation foundation models trained on massive webscale data enable agents\nto generalize across diverse environments and embodiments. However, these\nmodels trained solely on offline data, often lack the capacity to reason about\nthe consequences of their actions or adapt through counterfactual\nunderstanding. They thus face significant limitations in the real-world urban\nnavigation where interactive and safe behaviors, such as avoiding obstacles and\nmoving pedestrians, are critical. To tackle these challenges, we introduce the\nSeeing-to-Experiencing framework to scale the capability of navigation\nfoundation models with reinforcement learning. S2E combines the strengths of\npre-training on videos and post-training through RL. It maintains the\ngeneralizability acquired from large-scale real-world videos while enhancing\nits interactivity through RL in simulation environments. Specifically, we\nintroduce two innovations: an Anchor-Guided Distribution Matching strategy,\nwhich stabilizes learning and models diverse motion patterns through\nanchor-based supervision; and a Residual-Attention Module, which obtains\nreactive behaviors from simulation environments without erasing the model's\npretrained knowledge. Moreover, we establish a comprehensive end-to-end\nevaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions\nof real-world scenes that incorporate physical interactions. It can\nsystematically assess the generalizability and safety of navigation foundation\nmodels. Extensive experiments show that S2E mitigates the diminishing returns\noften seen when scaling with offline data alone. We perform a thorough analysis\nof the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in\nthe context of post-training for robot learning. Our findings emphasize the\ncrucial role of integrating interactive online experiences to effectively scale\nfoundation models in Robotics.\n", "link": "http://arxiv.org/abs/2507.22028v1", "date": "2025-07-29", "relevancy": 2.2021, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Seeing%20to%20Experiencing%3A%20Scaling%20Navigation%20Foundation%20Models%20with%0A%20%20Reinforcement%20Learning&body=Title%3A%20From%20Seeing%20to%20Experiencing%3A%20Scaling%20Navigation%20Foundation%20Models%20with%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Honglin%20He%20and%20Yukai%20Ma%20and%20Wayne%20Wu%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Navigation%20foundation%20models%20trained%20on%20massive%20webscale%20data%20enable%20agents%0Ato%20generalize%20across%20diverse%20environments%20and%20embodiments.%20However%2C%20these%0Amodels%20trained%20solely%20on%20offline%20data%2C%20often%20lack%20the%20capacity%20to%20reason%20about%0Athe%20consequences%20of%20their%20actions%20or%20adapt%20through%20counterfactual%0Aunderstanding.%20They%20thus%20face%20significant%20limitations%20in%20the%20real-world%20urban%0Anavigation%20where%20interactive%20and%20safe%20behaviors%2C%20such%20as%20avoiding%20obstacles%20and%0Amoving%20pedestrians%2C%20are%20critical.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20the%0ASeeing-to-Experiencing%20framework%20to%20scale%20the%20capability%20of%20navigation%0Afoundation%20models%20with%20reinforcement%20learning.%20S2E%20combines%20the%20strengths%20of%0Apre-training%20on%20videos%20and%20post-training%20through%20RL.%20It%20maintains%20the%0Ageneralizability%20acquired%20from%20large-scale%20real-world%20videos%20while%20enhancing%0Aits%20interactivity%20through%20RL%20in%20simulation%20environments.%20Specifically%2C%20we%0Aintroduce%20two%20innovations%3A%20an%20Anchor-Guided%20Distribution%20Matching%20strategy%2C%0Awhich%20stabilizes%20learning%20and%20models%20diverse%20motion%20patterns%20through%0Aanchor-based%20supervision%3B%20and%20a%20Residual-Attention%20Module%2C%20which%20obtains%0Areactive%20behaviors%20from%20simulation%20environments%20without%20erasing%20the%20model%27s%0Apretrained%20knowledge.%20Moreover%2C%20we%20establish%20a%20comprehensive%20end-to-end%0Aevaluation%20benchmark%2C%20NavBench-GS%2C%20built%20on%20photorealistic%203DGS%20reconstructions%0Aof%20real-world%20scenes%20that%20incorporate%20physical%20interactions.%20It%20can%0Asystematically%20assess%20the%20generalizability%20and%20safety%20of%20navigation%20foundation%0Amodels.%20Extensive%20experiments%20show%20that%20S2E%20mitigates%20the%20diminishing%20returns%0Aoften%20seen%20when%20scaling%20with%20offline%20data%20alone.%20We%20perform%20a%20thorough%20analysis%0Aof%20the%20benefits%20of%20Reinforcement%20Learning%20compared%20to%20Supervised%20Fine-Tuning%20in%0Athe%20context%20of%20post-training%20for%20robot%20learning.%20Our%20findings%20emphasize%20the%0Acrucial%20role%20of%20integrating%20interactive%20online%20experiences%20to%20effectively%20scale%0Afoundation%20models%20in%20Robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Seeing%2520to%2520Experiencing%253A%2520Scaling%2520Navigation%2520Foundation%2520Models%2520with%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DHonglin%2520He%2520and%2520Yukai%2520Ma%2520and%2520Wayne%2520Wu%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520Navigation%2520foundation%2520models%2520trained%2520on%2520massive%2520webscale%2520data%2520enable%2520agents%250Ato%2520generalize%2520across%2520diverse%2520environments%2520and%2520embodiments.%2520However%252C%2520these%250Amodels%2520trained%2520solely%2520on%2520offline%2520data%252C%2520often%2520lack%2520the%2520capacity%2520to%2520reason%2520about%250Athe%2520consequences%2520of%2520their%2520actions%2520or%2520adapt%2520through%2520counterfactual%250Aunderstanding.%2520They%2520thus%2520face%2520significant%2520limitations%2520in%2520the%2520real-world%2520urban%250Anavigation%2520where%2520interactive%2520and%2520safe%2520behaviors%252C%2520such%2520as%2520avoiding%2520obstacles%2520and%250Amoving%2520pedestrians%252C%2520are%2520critical.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520introduce%2520the%250ASeeing-to-Experiencing%2520framework%2520to%2520scale%2520the%2520capability%2520of%2520navigation%250Afoundation%2520models%2520with%2520reinforcement%2520learning.%2520S2E%2520combines%2520the%2520strengths%2520of%250Apre-training%2520on%2520videos%2520and%2520post-training%2520through%2520RL.%2520It%2520maintains%2520the%250Ageneralizability%2520acquired%2520from%2520large-scale%2520real-world%2520videos%2520while%2520enhancing%250Aits%2520interactivity%2520through%2520RL%2520in%2520simulation%2520environments.%2520Specifically%252C%2520we%250Aintroduce%2520two%2520innovations%253A%2520an%2520Anchor-Guided%2520Distribution%2520Matching%2520strategy%252C%250Awhich%2520stabilizes%2520learning%2520and%2520models%2520diverse%2520motion%2520patterns%2520through%250Aanchor-based%2520supervision%253B%2520and%2520a%2520Residual-Attention%2520Module%252C%2520which%2520obtains%250Areactive%2520behaviors%2520from%2520simulation%2520environments%2520without%2520erasing%2520the%2520model%2527s%250Apretrained%2520knowledge.%2520Moreover%252C%2520we%2520establish%2520a%2520comprehensive%2520end-to-end%250Aevaluation%2520benchmark%252C%2520NavBench-GS%252C%2520built%2520on%2520photorealistic%25203DGS%2520reconstructions%250Aof%2520real-world%2520scenes%2520that%2520incorporate%2520physical%2520interactions.%2520It%2520can%250Asystematically%2520assess%2520the%2520generalizability%2520and%2520safety%2520of%2520navigation%2520foundation%250Amodels.%2520Extensive%2520experiments%2520show%2520that%2520S2E%2520mitigates%2520the%2520diminishing%2520returns%250Aoften%2520seen%2520when%2520scaling%2520with%2520offline%2520data%2520alone.%2520We%2520perform%2520a%2520thorough%2520analysis%250Aof%2520the%2520benefits%2520of%2520Reinforcement%2520Learning%2520compared%2520to%2520Supervised%2520Fine-Tuning%2520in%250Athe%2520context%2520of%2520post-training%2520for%2520robot%2520learning.%2520Our%2520findings%2520emphasize%2520the%250Acrucial%2520role%2520of%2520integrating%2520interactive%2520online%2520experiences%2520to%2520effectively%2520scale%250Afoundation%2520models%2520in%2520Robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Seeing%20to%20Experiencing%3A%20Scaling%20Navigation%20Foundation%20Models%20with%0A%20%20Reinforcement%20Learning&entry.906535625=Honglin%20He%20and%20Yukai%20Ma%20and%20Wayne%20Wu%20and%20Bolei%20Zhou&entry.1292438233=%20%20Navigation%20foundation%20models%20trained%20on%20massive%20webscale%20data%20enable%20agents%0Ato%20generalize%20across%20diverse%20environments%20and%20embodiments.%20However%2C%20these%0Amodels%20trained%20solely%20on%20offline%20data%2C%20often%20lack%20the%20capacity%20to%20reason%20about%0Athe%20consequences%20of%20their%20actions%20or%20adapt%20through%20counterfactual%0Aunderstanding.%20They%20thus%20face%20significant%20limitations%20in%20the%20real-world%20urban%0Anavigation%20where%20interactive%20and%20safe%20behaviors%2C%20such%20as%20avoiding%20obstacles%20and%0Amoving%20pedestrians%2C%20are%20critical.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20the%0ASeeing-to-Experiencing%20framework%20to%20scale%20the%20capability%20of%20navigation%0Afoundation%20models%20with%20reinforcement%20learning.%20S2E%20combines%20the%20strengths%20of%0Apre-training%20on%20videos%20and%20post-training%20through%20RL.%20It%20maintains%20the%0Ageneralizability%20acquired%20from%20large-scale%20real-world%20videos%20while%20enhancing%0Aits%20interactivity%20through%20RL%20in%20simulation%20environments.%20Specifically%2C%20we%0Aintroduce%20two%20innovations%3A%20an%20Anchor-Guided%20Distribution%20Matching%20strategy%2C%0Awhich%20stabilizes%20learning%20and%20models%20diverse%20motion%20patterns%20through%0Aanchor-based%20supervision%3B%20and%20a%20Residual-Attention%20Module%2C%20which%20obtains%0Areactive%20behaviors%20from%20simulation%20environments%20without%20erasing%20the%20model%27s%0Apretrained%20knowledge.%20Moreover%2C%20we%20establish%20a%20comprehensive%20end-to-end%0Aevaluation%20benchmark%2C%20NavBench-GS%2C%20built%20on%20photorealistic%203DGS%20reconstructions%0Aof%20real-world%20scenes%20that%20incorporate%20physical%20interactions.%20It%20can%0Asystematically%20assess%20the%20generalizability%20and%20safety%20of%20navigation%20foundation%0Amodels.%20Extensive%20experiments%20show%20that%20S2E%20mitigates%20the%20diminishing%20returns%0Aoften%20seen%20when%20scaling%20with%20offline%20data%20alone.%20We%20perform%20a%20thorough%20analysis%0Aof%20the%20benefits%20of%20Reinforcement%20Learning%20compared%20to%20Supervised%20Fine-Tuning%20in%0Athe%20context%20of%20post-training%20for%20robot%20learning.%20Our%20findings%20emphasize%20the%0Acrucial%20role%20of%20integrating%20interactive%20online%20experiences%20to%20effectively%20scale%0Afoundation%20models%20in%20Robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22028v1&entry.124074799=Read"},
{"title": "MetaLab: Few-Shot Game Changer for Image Recognition", "author": "Chaofei Qi and Zhitai Liu and Jianbin Qiu", "abstract": "  Difficult few-shot image recognition has significant application prospects,\nyet remaining the substantial technical gaps with the conventional large-scale\nimage recognition. In this paper, we have proposed an efficient original method\nfor few-shot image recognition, called CIELab-Guided Coherent Meta-Learning\n(MetaLab). Structurally, our MetaLab comprises two collaborative neural\nnetworks: LabNet, which can perform domain transformation for the CIELab color\nspace and extract rich grouped features, and coherent LabGNN, which can\nfacilitate mutual learning between lightness graph and color graph. For\nsufficient certification, we have implemented extensive comparative studies on\nfour coarse-grained benchmarks, four fine-grained benchmarks, and four\ncross-domain few-shot benchmarks. Specifically, our method can achieve high\naccuracy, robust performance, and effective generalization capability with\none-shot sample per class. Overall, all experiments have demonstrated that our\nMetaLab can approach 99\\% $\\uparrow\\downarrow$ accuracy, reaching the human\nrecognition ceiling with little visual deviation.\n", "link": "http://arxiv.org/abs/2507.22057v1", "date": "2025-07-29", "relevancy": 2.1817, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5493}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5436}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaLab%3A%20Few-Shot%20Game%20Changer%20for%20Image%20Recognition&body=Title%3A%20MetaLab%3A%20Few-Shot%20Game%20Changer%20for%20Image%20Recognition%0AAuthor%3A%20Chaofei%20Qi%20and%20Zhitai%20Liu%20and%20Jianbin%20Qiu%0AAbstract%3A%20%20%20Difficult%20few-shot%20image%20recognition%20has%20significant%20application%20prospects%2C%0Ayet%20remaining%20the%20substantial%20technical%20gaps%20with%20the%20conventional%20large-scale%0Aimage%20recognition.%20In%20this%20paper%2C%20we%20have%20proposed%20an%20efficient%20original%20method%0Afor%20few-shot%20image%20recognition%2C%20called%20CIELab-Guided%20Coherent%20Meta-Learning%0A%28MetaLab%29.%20Structurally%2C%20our%20MetaLab%20comprises%20two%20collaborative%20neural%0Anetworks%3A%20LabNet%2C%20which%20can%20perform%20domain%20transformation%20for%20the%20CIELab%20color%0Aspace%20and%20extract%20rich%20grouped%20features%2C%20and%20coherent%20LabGNN%2C%20which%20can%0Afacilitate%20mutual%20learning%20between%20lightness%20graph%20and%20color%20graph.%20For%0Asufficient%20certification%2C%20we%20have%20implemented%20extensive%20comparative%20studies%20on%0Afour%20coarse-grained%20benchmarks%2C%20four%20fine-grained%20benchmarks%2C%20and%20four%0Across-domain%20few-shot%20benchmarks.%20Specifically%2C%20our%20method%20can%20achieve%20high%0Aaccuracy%2C%20robust%20performance%2C%20and%20effective%20generalization%20capability%20with%0Aone-shot%20sample%20per%20class.%20Overall%2C%20all%20experiments%20have%20demonstrated%20that%20our%0AMetaLab%20can%20approach%2099%5C%25%20%24%5Cuparrow%5Cdownarrow%24%20accuracy%2C%20reaching%20the%20human%0Arecognition%20ceiling%20with%20little%20visual%20deviation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaLab%253A%2520Few-Shot%2520Game%2520Changer%2520for%2520Image%2520Recognition%26entry.906535625%3DChaofei%2520Qi%2520and%2520Zhitai%2520Liu%2520and%2520Jianbin%2520Qiu%26entry.1292438233%3D%2520%2520Difficult%2520few-shot%2520image%2520recognition%2520has%2520significant%2520application%2520prospects%252C%250Ayet%2520remaining%2520the%2520substantial%2520technical%2520gaps%2520with%2520the%2520conventional%2520large-scale%250Aimage%2520recognition.%2520In%2520this%2520paper%252C%2520we%2520have%2520proposed%2520an%2520efficient%2520original%2520method%250Afor%2520few-shot%2520image%2520recognition%252C%2520called%2520CIELab-Guided%2520Coherent%2520Meta-Learning%250A%2528MetaLab%2529.%2520Structurally%252C%2520our%2520MetaLab%2520comprises%2520two%2520collaborative%2520neural%250Anetworks%253A%2520LabNet%252C%2520which%2520can%2520perform%2520domain%2520transformation%2520for%2520the%2520CIELab%2520color%250Aspace%2520and%2520extract%2520rich%2520grouped%2520features%252C%2520and%2520coherent%2520LabGNN%252C%2520which%2520can%250Afacilitate%2520mutual%2520learning%2520between%2520lightness%2520graph%2520and%2520color%2520graph.%2520For%250Asufficient%2520certification%252C%2520we%2520have%2520implemented%2520extensive%2520comparative%2520studies%2520on%250Afour%2520coarse-grained%2520benchmarks%252C%2520four%2520fine-grained%2520benchmarks%252C%2520and%2520four%250Across-domain%2520few-shot%2520benchmarks.%2520Specifically%252C%2520our%2520method%2520can%2520achieve%2520high%250Aaccuracy%252C%2520robust%2520performance%252C%2520and%2520effective%2520generalization%2520capability%2520with%250Aone-shot%2520sample%2520per%2520class.%2520Overall%252C%2520all%2520experiments%2520have%2520demonstrated%2520that%2520our%250AMetaLab%2520can%2520approach%252099%255C%2525%2520%2524%255Cuparrow%255Cdownarrow%2524%2520accuracy%252C%2520reaching%2520the%2520human%250Arecognition%2520ceiling%2520with%2520little%2520visual%2520deviation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaLab%3A%20Few-Shot%20Game%20Changer%20for%20Image%20Recognition&entry.906535625=Chaofei%20Qi%20and%20Zhitai%20Liu%20and%20Jianbin%20Qiu&entry.1292438233=%20%20Difficult%20few-shot%20image%20recognition%20has%20significant%20application%20prospects%2C%0Ayet%20remaining%20the%20substantial%20technical%20gaps%20with%20the%20conventional%20large-scale%0Aimage%20recognition.%20In%20this%20paper%2C%20we%20have%20proposed%20an%20efficient%20original%20method%0Afor%20few-shot%20image%20recognition%2C%20called%20CIELab-Guided%20Coherent%20Meta-Learning%0A%28MetaLab%29.%20Structurally%2C%20our%20MetaLab%20comprises%20two%20collaborative%20neural%0Anetworks%3A%20LabNet%2C%20which%20can%20perform%20domain%20transformation%20for%20the%20CIELab%20color%0Aspace%20and%20extract%20rich%20grouped%20features%2C%20and%20coherent%20LabGNN%2C%20which%20can%0Afacilitate%20mutual%20learning%20between%20lightness%20graph%20and%20color%20graph.%20For%0Asufficient%20certification%2C%20we%20have%20implemented%20extensive%20comparative%20studies%20on%0Afour%20coarse-grained%20benchmarks%2C%20four%20fine-grained%20benchmarks%2C%20and%20four%0Across-domain%20few-shot%20benchmarks.%20Specifically%2C%20our%20method%20can%20achieve%20high%0Aaccuracy%2C%20robust%20performance%2C%20and%20effective%20generalization%20capability%20with%0Aone-shot%20sample%20per%20class.%20Overall%2C%20all%20experiments%20have%20demonstrated%20that%20our%0AMetaLab%20can%20approach%2099%5C%25%20%24%5Cuparrow%5Cdownarrow%24%20accuracy%2C%20reaching%20the%20human%0Arecognition%20ceiling%20with%20little%20visual%20deviation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22057v1&entry.124074799=Read"},
{"title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation", "author": "Kaining Ying and Hengrui Hu and Henghui Ding", "abstract": "  This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.\n", "link": "http://arxiv.org/abs/2507.22061v1", "date": "2025-07-29", "relevancy": 2.1558, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5542}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOVE%3A%20Motion-Guided%20Few-Shot%20Video%20Object%20Segmentation&body=Title%3A%20MOVE%3A%20Motion-Guided%20Few-Shot%20Video%20Object%20Segmentation%0AAuthor%3A%20Kaining%20Ying%20and%20Hengrui%20Hu%20and%20Henghui%20Ding%0AAbstract%3A%20%20%20This%20work%20addresses%20motion-guided%20few-shot%20video%20object%20segmentation%20%28FSVOS%29%2C%0Awhich%20aims%20to%20segment%20dynamic%20objects%20in%20videos%20based%20on%20a%20few%20annotated%0Aexamples%20with%20the%20same%20motion%20patterns.%20Existing%20FSVOS%20datasets%20and%20methods%0Atypically%20focus%20on%20object%20categories%2C%20which%20are%20static%20attributes%20that%20ignore%0Athe%20rich%20temporal%20dynamics%20in%20videos%2C%20limiting%20their%20application%20in%20scenarios%0Arequiring%20motion%20understanding.%20To%20fill%20this%20gap%2C%20we%20introduce%20MOVE%2C%20a%0Alarge-scale%20dataset%20specifically%20designed%20for%20motion-guided%20FSVOS.%20Based%20on%0AMOVE%2C%20we%20comprehensively%20evaluate%206%20state-of-the-art%20methods%20from%203%20different%0Arelated%20tasks%20across%202%20experimental%20settings.%20Our%20results%20reveal%20that%20current%0Amethods%20struggle%20to%20address%20motion-guided%20FSVOS%2C%20prompting%20us%20to%20analyze%20the%0Aassociated%20challenges%20and%20propose%20a%20baseline%20method%2C%20Decoupled%20Motion%0AAppearance%20Network%20%28DMA%29.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20performance%20in%20few%20shot%20motion%20understanding%2C%20establishing%20a%20solid%0Afoundation%20for%20future%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOVE%253A%2520Motion-Guided%2520Few-Shot%2520Video%2520Object%2520Segmentation%26entry.906535625%3DKaining%2520Ying%2520and%2520Hengrui%2520Hu%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520motion-guided%2520few-shot%2520video%2520object%2520segmentation%2520%2528FSVOS%2529%252C%250Awhich%2520aims%2520to%2520segment%2520dynamic%2520objects%2520in%2520videos%2520based%2520on%2520a%2520few%2520annotated%250Aexamples%2520with%2520the%2520same%2520motion%2520patterns.%2520Existing%2520FSVOS%2520datasets%2520and%2520methods%250Atypically%2520focus%2520on%2520object%2520categories%252C%2520which%2520are%2520static%2520attributes%2520that%2520ignore%250Athe%2520rich%2520temporal%2520dynamics%2520in%2520videos%252C%2520limiting%2520their%2520application%2520in%2520scenarios%250Arequiring%2520motion%2520understanding.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520MOVE%252C%2520a%250Alarge-scale%2520dataset%2520specifically%2520designed%2520for%2520motion-guided%2520FSVOS.%2520Based%2520on%250AMOVE%252C%2520we%2520comprehensively%2520evaluate%25206%2520state-of-the-art%2520methods%2520from%25203%2520different%250Arelated%2520tasks%2520across%25202%2520experimental%2520settings.%2520Our%2520results%2520reveal%2520that%2520current%250Amethods%2520struggle%2520to%2520address%2520motion-guided%2520FSVOS%252C%2520prompting%2520us%2520to%2520analyze%2520the%250Aassociated%2520challenges%2520and%2520propose%2520a%2520baseline%2520method%252C%2520Decoupled%2520Motion%250AAppearance%2520Network%2520%2528DMA%2529.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Asuperior%2520performance%2520in%2520few%2520shot%2520motion%2520understanding%252C%2520establishing%2520a%2520solid%250Afoundation%2520for%2520future%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOVE%3A%20Motion-Guided%20Few-Shot%20Video%20Object%20Segmentation&entry.906535625=Kaining%20Ying%20and%20Hengrui%20Hu%20and%20Henghui%20Ding&entry.1292438233=%20%20This%20work%20addresses%20motion-guided%20few-shot%20video%20object%20segmentation%20%28FSVOS%29%2C%0Awhich%20aims%20to%20segment%20dynamic%20objects%20in%20videos%20based%20on%20a%20few%20annotated%0Aexamples%20with%20the%20same%20motion%20patterns.%20Existing%20FSVOS%20datasets%20and%20methods%0Atypically%20focus%20on%20object%20categories%2C%20which%20are%20static%20attributes%20that%20ignore%0Athe%20rich%20temporal%20dynamics%20in%20videos%2C%20limiting%20their%20application%20in%20scenarios%0Arequiring%20motion%20understanding.%20To%20fill%20this%20gap%2C%20we%20introduce%20MOVE%2C%20a%0Alarge-scale%20dataset%20specifically%20designed%20for%20motion-guided%20FSVOS.%20Based%20on%0AMOVE%2C%20we%20comprehensively%20evaluate%206%20state-of-the-art%20methods%20from%203%20different%0Arelated%20tasks%20across%202%20experimental%20settings.%20Our%20results%20reveal%20that%20current%0Amethods%20struggle%20to%20address%20motion-guided%20FSVOS%2C%20prompting%20us%20to%20analyze%20the%0Aassociated%20challenges%20and%20propose%20a%20baseline%20method%2C%20Decoupled%20Motion%0AAppearance%20Network%20%28DMA%29.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20performance%20in%20few%20shot%20motion%20understanding%2C%20establishing%20a%20solid%0Afoundation%20for%20future%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22061v1&entry.124074799=Read"},
{"title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop\n  Weakly-Supervised Framework for Industrial Toxic Emission Segmentation", "author": "Yida Tao and Yen-Chia Hsu", "abstract": "  Industrial smoke segmentation is critical for air-quality monitoring and\nenvironmental protection but is often hampered by the high cost and scarcity of\npixel-level annotations in real-world settings. We introduce CEDANet, a\nhuman-in-the-loop, class-aware domain adaptation framework that uniquely\nintegrates weak, citizen-provided video-level labels with adversarial feature\nalignment. Specifically, we refine pseudo-labels generated by a source-trained\nsegmentation model using citizen votes, and employ class-specific domain\ndiscriminators to transfer rich source-domain representations to the industrial\ndomain. Comprehensive experiments on SMOKE5K and custom IJmond datasets\ndemonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of\n0.261 with citizen feedback, vastly outperforming the baseline model, which\nscored 0.083 and 0.043 respectively. This represents a five-fold increase in\nF1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with\ncitizen-constrained pseudo-labels achieves performance comparable to the same\narchitecture trained on limited 100 fully annotated images with F1-score of\n0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully\nsupervised-level accuracy without target-domain annotations. Our research\nvalidates the scalability and cost-efficiency of combining citizen science with\nweakly supervised domain adaptation, offering a practical solution for complex,\ndata-scarce environmental monitoring applications.\n", "link": "http://arxiv.org/abs/2507.22002v1", "date": "2025-07-29", "relevancy": 2.128, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5698}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5353}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Synthetic%20and%20Real-World%20Domains%3A%20A%20Human-in-the-Loop%0A%20%20Weakly-Supervised%20Framework%20for%20Industrial%20Toxic%20Emission%20Segmentation&body=Title%3A%20Bridging%20Synthetic%20and%20Real-World%20Domains%3A%20A%20Human-in-the-Loop%0A%20%20Weakly-Supervised%20Framework%20for%20Industrial%20Toxic%20Emission%20Segmentation%0AAuthor%3A%20Yida%20Tao%20and%20Yen-Chia%20Hsu%0AAbstract%3A%20%20%20Industrial%20smoke%20segmentation%20is%20critical%20for%20air-quality%20monitoring%20and%0Aenvironmental%20protection%20but%20is%20often%20hampered%20by%20the%20high%20cost%20and%20scarcity%20of%0Apixel-level%20annotations%20in%20real-world%20settings.%20We%20introduce%20CEDANet%2C%20a%0Ahuman-in-the-loop%2C%20class-aware%20domain%20adaptation%20framework%20that%20uniquely%0Aintegrates%20weak%2C%20citizen-provided%20video-level%20labels%20with%20adversarial%20feature%0Aalignment.%20Specifically%2C%20we%20refine%20pseudo-labels%20generated%20by%20a%20source-trained%0Asegmentation%20model%20using%20citizen%20votes%2C%20and%20employ%20class-specific%20domain%0Adiscriminators%20to%20transfer%20rich%20source-domain%20representations%20to%20the%20industrial%0Adomain.%20Comprehensive%20experiments%20on%20SMOKE5K%20and%20custom%20IJmond%20datasets%0Ademonstrate%20that%20CEDANet%20achieves%20an%20F1-score%20of%200.414%20and%20a%20smoke-class%20IoU%20of%0A0.261%20with%20citizen%20feedback%2C%20vastly%20outperforming%20the%20baseline%20model%2C%20which%0Ascored%200.083%20and%200.043%20respectively.%20This%20represents%20a%20five-fold%20increase%20in%0AF1-score%20and%20a%20six-fold%20increase%20in%20smoke-class%20IoU.%20Notably%2C%20CEDANet%20with%0Acitizen-constrained%20pseudo-labels%20achieves%20performance%20comparable%20to%20the%20same%0Aarchitecture%20trained%20on%20limited%20100%20fully%20annotated%20images%20with%20F1-score%20of%0A0.418%20and%20IoU%20of%200.264%2C%20demonstrating%20its%20ability%20to%20reach%20small-sampled%20fully%0Asupervised-level%20accuracy%20without%20target-domain%20annotations.%20Our%20research%0Avalidates%20the%20scalability%20and%20cost-efficiency%20of%20combining%20citizen%20science%20with%0Aweakly%20supervised%20domain%20adaptation%2C%20offering%20a%20practical%20solution%20for%20complex%2C%0Adata-scarce%20environmental%20monitoring%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Synthetic%2520and%2520Real-World%2520Domains%253A%2520A%2520Human-in-the-Loop%250A%2520%2520Weakly-Supervised%2520Framework%2520for%2520Industrial%2520Toxic%2520Emission%2520Segmentation%26entry.906535625%3DYida%2520Tao%2520and%2520Yen-Chia%2520Hsu%26entry.1292438233%3D%2520%2520Industrial%2520smoke%2520segmentation%2520is%2520critical%2520for%2520air-quality%2520monitoring%2520and%250Aenvironmental%2520protection%2520but%2520is%2520often%2520hampered%2520by%2520the%2520high%2520cost%2520and%2520scarcity%2520of%250Apixel-level%2520annotations%2520in%2520real-world%2520settings.%2520We%2520introduce%2520CEDANet%252C%2520a%250Ahuman-in-the-loop%252C%2520class-aware%2520domain%2520adaptation%2520framework%2520that%2520uniquely%250Aintegrates%2520weak%252C%2520citizen-provided%2520video-level%2520labels%2520with%2520adversarial%2520feature%250Aalignment.%2520Specifically%252C%2520we%2520refine%2520pseudo-labels%2520generated%2520by%2520a%2520source-trained%250Asegmentation%2520model%2520using%2520citizen%2520votes%252C%2520and%2520employ%2520class-specific%2520domain%250Adiscriminators%2520to%2520transfer%2520rich%2520source-domain%2520representations%2520to%2520the%2520industrial%250Adomain.%2520Comprehensive%2520experiments%2520on%2520SMOKE5K%2520and%2520custom%2520IJmond%2520datasets%250Ademonstrate%2520that%2520CEDANet%2520achieves%2520an%2520F1-score%2520of%25200.414%2520and%2520a%2520smoke-class%2520IoU%2520of%250A0.261%2520with%2520citizen%2520feedback%252C%2520vastly%2520outperforming%2520the%2520baseline%2520model%252C%2520which%250Ascored%25200.083%2520and%25200.043%2520respectively.%2520This%2520represents%2520a%2520five-fold%2520increase%2520in%250AF1-score%2520and%2520a%2520six-fold%2520increase%2520in%2520smoke-class%2520IoU.%2520Notably%252C%2520CEDANet%2520with%250Acitizen-constrained%2520pseudo-labels%2520achieves%2520performance%2520comparable%2520to%2520the%2520same%250Aarchitecture%2520trained%2520on%2520limited%2520100%2520fully%2520annotated%2520images%2520with%2520F1-score%2520of%250A0.418%2520and%2520IoU%2520of%25200.264%252C%2520demonstrating%2520its%2520ability%2520to%2520reach%2520small-sampled%2520fully%250Asupervised-level%2520accuracy%2520without%2520target-domain%2520annotations.%2520Our%2520research%250Avalidates%2520the%2520scalability%2520and%2520cost-efficiency%2520of%2520combining%2520citizen%2520science%2520with%250Aweakly%2520supervised%2520domain%2520adaptation%252C%2520offering%2520a%2520practical%2520solution%2520for%2520complex%252C%250Adata-scarce%2520environmental%2520monitoring%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Synthetic%20and%20Real-World%20Domains%3A%20A%20Human-in-the-Loop%0A%20%20Weakly-Supervised%20Framework%20for%20Industrial%20Toxic%20Emission%20Segmentation&entry.906535625=Yida%20Tao%20and%20Yen-Chia%20Hsu&entry.1292438233=%20%20Industrial%20smoke%20segmentation%20is%20critical%20for%20air-quality%20monitoring%20and%0Aenvironmental%20protection%20but%20is%20often%20hampered%20by%20the%20high%20cost%20and%20scarcity%20of%0Apixel-level%20annotations%20in%20real-world%20settings.%20We%20introduce%20CEDANet%2C%20a%0Ahuman-in-the-loop%2C%20class-aware%20domain%20adaptation%20framework%20that%20uniquely%0Aintegrates%20weak%2C%20citizen-provided%20video-level%20labels%20with%20adversarial%20feature%0Aalignment.%20Specifically%2C%20we%20refine%20pseudo-labels%20generated%20by%20a%20source-trained%0Asegmentation%20model%20using%20citizen%20votes%2C%20and%20employ%20class-specific%20domain%0Adiscriminators%20to%20transfer%20rich%20source-domain%20representations%20to%20the%20industrial%0Adomain.%20Comprehensive%20experiments%20on%20SMOKE5K%20and%20custom%20IJmond%20datasets%0Ademonstrate%20that%20CEDANet%20achieves%20an%20F1-score%20of%200.414%20and%20a%20smoke-class%20IoU%20of%0A0.261%20with%20citizen%20feedback%2C%20vastly%20outperforming%20the%20baseline%20model%2C%20which%0Ascored%200.083%20and%200.043%20respectively.%20This%20represents%20a%20five-fold%20increase%20in%0AF1-score%20and%20a%20six-fold%20increase%20in%20smoke-class%20IoU.%20Notably%2C%20CEDANet%20with%0Acitizen-constrained%20pseudo-labels%20achieves%20performance%20comparable%20to%20the%20same%0Aarchitecture%20trained%20on%20limited%20100%20fully%20annotated%20images%20with%20F1-score%20of%0A0.418%20and%20IoU%20of%200.264%2C%20demonstrating%20its%20ability%20to%20reach%20small-sampled%20fully%0Asupervised-level%20accuracy%20without%20target-domain%20annotations.%20Our%20research%0Avalidates%20the%20scalability%20and%20cost-efficiency%20of%20combining%20citizen%20science%20with%0Aweakly%20supervised%20domain%20adaptation%2C%20offering%20a%20practical%20solution%20for%20complex%2C%0Adata-scarce%20environmental%20monitoring%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22002v1&entry.124074799=Read"},
{"title": "XAI for Point Cloud Data using Perturbations based on Meaningful\n  Segmentation", "author": "Raju Ningappa Mulawade and Christoph Garth and Alexander Wiebel", "abstract": "  We propose a novel segmentation-based explainable artificial intelligence\n(XAI) method for neural networks working on point cloud classification. As one\nbuilding block of this method, we propose a novel point-shifting mechanism to\nintroduce perturbations in point cloud data. Recently, AI has seen an\nexponential growth. Hence, it is important to understand the decision-making\nprocess of AI algorithms when they are applied in critical areas. Our work\nfocuses on explaining AI algorithms that classify point cloud data. An\nimportant aspect of the methods used for explaining AI algorithms is their\nability to produce explanations that are easy for humans to understand. This\nallows them to analyze the AI algorithms better and make appropriate decisions\nbased on that analysis. Therefore, in this work, we intend to generate\nmeaningful explanations that can be easily interpreted by humans. The point\ncloud data we consider represents 3D objects such as cars, guitars, and\nlaptops. We make use of point cloud segmentation models to generate\nexplanations for the working of classification models. The segments are used to\nintroduce perturbations into the input point cloud data and generate saliency\nmaps. The perturbations are introduced using the novel point-shifting mechanism\nproposed in this work which ensures that the shifted points no longer influence\nthe output of the classification algorithm. In contrast to previous methods,\nthe segments used by our method are meaningful, i.e. humans can easily\ninterpret the meaning of the segments. Thus, the benefit of our method over\nother methods is its ability to produce more meaningful saliency maps. We\ncompare our method with the use of classical clustering algorithms to generate\nexplanations. We also analyze the saliency maps generated for example inputs\nusing our method to demonstrate the usefulness of the method in generating\nmeaningful explanations.\n", "link": "http://arxiv.org/abs/2507.22020v1", "date": "2025-07-29", "relevancy": 2.0903, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5414}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5155}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XAI%20for%20Point%20Cloud%20Data%20using%20Perturbations%20based%20on%20Meaningful%0A%20%20Segmentation&body=Title%3A%20XAI%20for%20Point%20Cloud%20Data%20using%20Perturbations%20based%20on%20Meaningful%0A%20%20Segmentation%0AAuthor%3A%20Raju%20Ningappa%20Mulawade%20and%20Christoph%20Garth%20and%20Alexander%20Wiebel%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20segmentation-based%20explainable%20artificial%20intelligence%0A%28XAI%29%20method%20for%20neural%20networks%20working%20on%20point%20cloud%20classification.%20As%20one%0Abuilding%20block%20of%20this%20method%2C%20we%20propose%20a%20novel%20point-shifting%20mechanism%20to%0Aintroduce%20perturbations%20in%20point%20cloud%20data.%20Recently%2C%20AI%20has%20seen%20an%0Aexponential%20growth.%20Hence%2C%20it%20is%20important%20to%20understand%20the%20decision-making%0Aprocess%20of%20AI%20algorithms%20when%20they%20are%20applied%20in%20critical%20areas.%20Our%20work%0Afocuses%20on%20explaining%20AI%20algorithms%20that%20classify%20point%20cloud%20data.%20An%0Aimportant%20aspect%20of%20the%20methods%20used%20for%20explaining%20AI%20algorithms%20is%20their%0Aability%20to%20produce%20explanations%20that%20are%20easy%20for%20humans%20to%20understand.%20This%0Aallows%20them%20to%20analyze%20the%20AI%20algorithms%20better%20and%20make%20appropriate%20decisions%0Abased%20on%20that%20analysis.%20Therefore%2C%20in%20this%20work%2C%20we%20intend%20to%20generate%0Ameaningful%20explanations%20that%20can%20be%20easily%20interpreted%20by%20humans.%20The%20point%0Acloud%20data%20we%20consider%20represents%203D%20objects%20such%20as%20cars%2C%20guitars%2C%20and%0Alaptops.%20We%20make%20use%20of%20point%20cloud%20segmentation%20models%20to%20generate%0Aexplanations%20for%20the%20working%20of%20classification%20models.%20The%20segments%20are%20used%20to%0Aintroduce%20perturbations%20into%20the%20input%20point%20cloud%20data%20and%20generate%20saliency%0Amaps.%20The%20perturbations%20are%20introduced%20using%20the%20novel%20point-shifting%20mechanism%0Aproposed%20in%20this%20work%20which%20ensures%20that%20the%20shifted%20points%20no%20longer%20influence%0Athe%20output%20of%20the%20classification%20algorithm.%20In%20contrast%20to%20previous%20methods%2C%0Athe%20segments%20used%20by%20our%20method%20are%20meaningful%2C%20i.e.%20humans%20can%20easily%0Ainterpret%20the%20meaning%20of%20the%20segments.%20Thus%2C%20the%20benefit%20of%20our%20method%20over%0Aother%20methods%20is%20its%20ability%20to%20produce%20more%20meaningful%20saliency%20maps.%20We%0Acompare%20our%20method%20with%20the%20use%20of%20classical%20clustering%20algorithms%20to%20generate%0Aexplanations.%20We%20also%20analyze%20the%20saliency%20maps%20generated%20for%20example%20inputs%0Ausing%20our%20method%20to%20demonstrate%20the%20usefulness%20of%20the%20method%20in%20generating%0Ameaningful%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXAI%2520for%2520Point%2520Cloud%2520Data%2520using%2520Perturbations%2520based%2520on%2520Meaningful%250A%2520%2520Segmentation%26entry.906535625%3DRaju%2520Ningappa%2520Mulawade%2520and%2520Christoph%2520Garth%2520and%2520Alexander%2520Wiebel%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520segmentation-based%2520explainable%2520artificial%2520intelligence%250A%2528XAI%2529%2520method%2520for%2520neural%2520networks%2520working%2520on%2520point%2520cloud%2520classification.%2520As%2520one%250Abuilding%2520block%2520of%2520this%2520method%252C%2520we%2520propose%2520a%2520novel%2520point-shifting%2520mechanism%2520to%250Aintroduce%2520perturbations%2520in%2520point%2520cloud%2520data.%2520Recently%252C%2520AI%2520has%2520seen%2520an%250Aexponential%2520growth.%2520Hence%252C%2520it%2520is%2520important%2520to%2520understand%2520the%2520decision-making%250Aprocess%2520of%2520AI%2520algorithms%2520when%2520they%2520are%2520applied%2520in%2520critical%2520areas.%2520Our%2520work%250Afocuses%2520on%2520explaining%2520AI%2520algorithms%2520that%2520classify%2520point%2520cloud%2520data.%2520An%250Aimportant%2520aspect%2520of%2520the%2520methods%2520used%2520for%2520explaining%2520AI%2520algorithms%2520is%2520their%250Aability%2520to%2520produce%2520explanations%2520that%2520are%2520easy%2520for%2520humans%2520to%2520understand.%2520This%250Aallows%2520them%2520to%2520analyze%2520the%2520AI%2520algorithms%2520better%2520and%2520make%2520appropriate%2520decisions%250Abased%2520on%2520that%2520analysis.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520intend%2520to%2520generate%250Ameaningful%2520explanations%2520that%2520can%2520be%2520easily%2520interpreted%2520by%2520humans.%2520The%2520point%250Acloud%2520data%2520we%2520consider%2520represents%25203D%2520objects%2520such%2520as%2520cars%252C%2520guitars%252C%2520and%250Alaptops.%2520We%2520make%2520use%2520of%2520point%2520cloud%2520segmentation%2520models%2520to%2520generate%250Aexplanations%2520for%2520the%2520working%2520of%2520classification%2520models.%2520The%2520segments%2520are%2520used%2520to%250Aintroduce%2520perturbations%2520into%2520the%2520input%2520point%2520cloud%2520data%2520and%2520generate%2520saliency%250Amaps.%2520The%2520perturbations%2520are%2520introduced%2520using%2520the%2520novel%2520point-shifting%2520mechanism%250Aproposed%2520in%2520this%2520work%2520which%2520ensures%2520that%2520the%2520shifted%2520points%2520no%2520longer%2520influence%250Athe%2520output%2520of%2520the%2520classification%2520algorithm.%2520In%2520contrast%2520to%2520previous%2520methods%252C%250Athe%2520segments%2520used%2520by%2520our%2520method%2520are%2520meaningful%252C%2520i.e.%2520humans%2520can%2520easily%250Ainterpret%2520the%2520meaning%2520of%2520the%2520segments.%2520Thus%252C%2520the%2520benefit%2520of%2520our%2520method%2520over%250Aother%2520methods%2520is%2520its%2520ability%2520to%2520produce%2520more%2520meaningful%2520saliency%2520maps.%2520We%250Acompare%2520our%2520method%2520with%2520the%2520use%2520of%2520classical%2520clustering%2520algorithms%2520to%2520generate%250Aexplanations.%2520We%2520also%2520analyze%2520the%2520saliency%2520maps%2520generated%2520for%2520example%2520inputs%250Ausing%2520our%2520method%2520to%2520demonstrate%2520the%2520usefulness%2520of%2520the%2520method%2520in%2520generating%250Ameaningful%2520explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XAI%20for%20Point%20Cloud%20Data%20using%20Perturbations%20based%20on%20Meaningful%0A%20%20Segmentation&entry.906535625=Raju%20Ningappa%20Mulawade%20and%20Christoph%20Garth%20and%20Alexander%20Wiebel&entry.1292438233=%20%20We%20propose%20a%20novel%20segmentation-based%20explainable%20artificial%20intelligence%0A%28XAI%29%20method%20for%20neural%20networks%20working%20on%20point%20cloud%20classification.%20As%20one%0Abuilding%20block%20of%20this%20method%2C%20we%20propose%20a%20novel%20point-shifting%20mechanism%20to%0Aintroduce%20perturbations%20in%20point%20cloud%20data.%20Recently%2C%20AI%20has%20seen%20an%0Aexponential%20growth.%20Hence%2C%20it%20is%20important%20to%20understand%20the%20decision-making%0Aprocess%20of%20AI%20algorithms%20when%20they%20are%20applied%20in%20critical%20areas.%20Our%20work%0Afocuses%20on%20explaining%20AI%20algorithms%20that%20classify%20point%20cloud%20data.%20An%0Aimportant%20aspect%20of%20the%20methods%20used%20for%20explaining%20AI%20algorithms%20is%20their%0Aability%20to%20produce%20explanations%20that%20are%20easy%20for%20humans%20to%20understand.%20This%0Aallows%20them%20to%20analyze%20the%20AI%20algorithms%20better%20and%20make%20appropriate%20decisions%0Abased%20on%20that%20analysis.%20Therefore%2C%20in%20this%20work%2C%20we%20intend%20to%20generate%0Ameaningful%20explanations%20that%20can%20be%20easily%20interpreted%20by%20humans.%20The%20point%0Acloud%20data%20we%20consider%20represents%203D%20objects%20such%20as%20cars%2C%20guitars%2C%20and%0Alaptops.%20We%20make%20use%20of%20point%20cloud%20segmentation%20models%20to%20generate%0Aexplanations%20for%20the%20working%20of%20classification%20models.%20The%20segments%20are%20used%20to%0Aintroduce%20perturbations%20into%20the%20input%20point%20cloud%20data%20and%20generate%20saliency%0Amaps.%20The%20perturbations%20are%20introduced%20using%20the%20novel%20point-shifting%20mechanism%0Aproposed%20in%20this%20work%20which%20ensures%20that%20the%20shifted%20points%20no%20longer%20influence%0Athe%20output%20of%20the%20classification%20algorithm.%20In%20contrast%20to%20previous%20methods%2C%0Athe%20segments%20used%20by%20our%20method%20are%20meaningful%2C%20i.e.%20humans%20can%20easily%0Ainterpret%20the%20meaning%20of%20the%20segments.%20Thus%2C%20the%20benefit%20of%20our%20method%20over%0Aother%20methods%20is%20its%20ability%20to%20produce%20more%20meaningful%20saliency%20maps.%20We%0Acompare%20our%20method%20with%20the%20use%20of%20classical%20clustering%20algorithms%20to%20generate%0Aexplanations.%20We%20also%20analyze%20the%20saliency%20maps%20generated%20for%20example%20inputs%0Ausing%20our%20method%20to%20demonstrate%20the%20usefulness%20of%20the%20method%20in%20generating%0Ameaningful%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22020v1&entry.124074799=Read"},
{"title": "The Effect of Compression Techniques on Large Multimodal Language Models\n  in the Medical Domain", "author": "Tanvir Ahmed Khan and Aranya Saha and Ismam Nur Swapnil and Mohammad Ariful Haque", "abstract": "  Multimodal Large Language Models (MLLMs) hold huge potential for usage in the\nmedical domain, but their computational costs necessitate efficient compression\ntechniques. This paper evaluates the impact of structural pruning and\nactivation-aware quantization on a fine-tuned LLAVA model for medical\napplications. We propose a novel layer selection method for pruning, analyze\ndifferent quantization techniques, and assess the performance trade-offs in a\nprune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B\nparameters to run within 4 GB of VRAM, reducing memory usage by 70% while\nachieving 4% higher model performance compared to traditional pruning and\nquantization techniques in the same compression ratio.\n", "link": "http://arxiv.org/abs/2507.21976v1", "date": "2025-07-29", "relevancy": 2.0779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Effect%20of%20Compression%20Techniques%20on%20Large%20Multimodal%20Language%20Models%0A%20%20in%20the%20Medical%20Domain&body=Title%3A%20The%20Effect%20of%20Compression%20Techniques%20on%20Large%20Multimodal%20Language%20Models%0A%20%20in%20the%20Medical%20Domain%0AAuthor%3A%20Tanvir%20Ahmed%20Khan%20and%20Aranya%20Saha%20and%20Ismam%20Nur%20Swapnil%20and%20Mohammad%20Ariful%20Haque%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20huge%20potential%20for%20usage%20in%20the%0Amedical%20domain%2C%20but%20their%20computational%20costs%20necessitate%20efficient%20compression%0Atechniques.%20This%20paper%20evaluates%20the%20impact%20of%20structural%20pruning%20and%0Aactivation-aware%20quantization%20on%20a%20fine-tuned%20LLAVA%20model%20for%20medical%0Aapplications.%20We%20propose%20a%20novel%20layer%20selection%20method%20for%20pruning%2C%20analyze%0Adifferent%20quantization%20techniques%2C%20and%20assess%20the%20performance%20trade-offs%20in%20a%0Aprune-SFT-quantize%20pipeline.%20Our%20proposed%20method%20enables%20MLLMs%20with%207B%0Aparameters%20to%20run%20within%204%20GB%20of%20VRAM%2C%20reducing%20memory%20usage%20by%2070%25%20while%0Aachieving%204%25%20higher%20model%20performance%20compared%20to%20traditional%20pruning%20and%0Aquantization%20techniques%20in%20the%20same%20compression%20ratio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Effect%2520of%2520Compression%2520Techniques%2520on%2520Large%2520Multimodal%2520Language%2520Models%250A%2520%2520in%2520the%2520Medical%2520Domain%26entry.906535625%3DTanvir%2520Ahmed%2520Khan%2520and%2520Aranya%2520Saha%2520and%2520Ismam%2520Nur%2520Swapnil%2520and%2520Mohammad%2520Ariful%2520Haque%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520hold%2520huge%2520potential%2520for%2520usage%2520in%2520the%250Amedical%2520domain%252C%2520but%2520their%2520computational%2520costs%2520necessitate%2520efficient%2520compression%250Atechniques.%2520This%2520paper%2520evaluates%2520the%2520impact%2520of%2520structural%2520pruning%2520and%250Aactivation-aware%2520quantization%2520on%2520a%2520fine-tuned%2520LLAVA%2520model%2520for%2520medical%250Aapplications.%2520We%2520propose%2520a%2520novel%2520layer%2520selection%2520method%2520for%2520pruning%252C%2520analyze%250Adifferent%2520quantization%2520techniques%252C%2520and%2520assess%2520the%2520performance%2520trade-offs%2520in%2520a%250Aprune-SFT-quantize%2520pipeline.%2520Our%2520proposed%2520method%2520enables%2520MLLMs%2520with%25207B%250Aparameters%2520to%2520run%2520within%25204%2520GB%2520of%2520VRAM%252C%2520reducing%2520memory%2520usage%2520by%252070%2525%2520while%250Aachieving%25204%2525%2520higher%2520model%2520performance%2520compared%2520to%2520traditional%2520pruning%2520and%250Aquantization%2520techniques%2520in%2520the%2520same%2520compression%2520ratio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Effect%20of%20Compression%20Techniques%20on%20Large%20Multimodal%20Language%20Models%0A%20%20in%20the%20Medical%20Domain&entry.906535625=Tanvir%20Ahmed%20Khan%20and%20Aranya%20Saha%20and%20Ismam%20Nur%20Swapnil%20and%20Mohammad%20Ariful%20Haque&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20huge%20potential%20for%20usage%20in%20the%0Amedical%20domain%2C%20but%20their%20computational%20costs%20necessitate%20efficient%20compression%0Atechniques.%20This%20paper%20evaluates%20the%20impact%20of%20structural%20pruning%20and%0Aactivation-aware%20quantization%20on%20a%20fine-tuned%20LLAVA%20model%20for%20medical%0Aapplications.%20We%20propose%20a%20novel%20layer%20selection%20method%20for%20pruning%2C%20analyze%0Adifferent%20quantization%20techniques%2C%20and%20assess%20the%20performance%20trade-offs%20in%20a%0Aprune-SFT-quantize%20pipeline.%20Our%20proposed%20method%20enables%20MLLMs%20with%207B%0Aparameters%20to%20run%20within%204%20GB%20of%20VRAM%2C%20reducing%20memory%20usage%20by%2070%25%20while%0Aachieving%204%25%20higher%20model%20performance%20compared%20to%20traditional%20pruning%20and%0Aquantization%20techniques%20in%20the%20same%20compression%20ratio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21976v1&entry.124074799=Read"},
{"title": "EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation", "author": "Zhijiang Li and Haoran He", "abstract": "  Event-based semantic segmentation explores the potential of event cameras,\nwhich offer high dynamic range and fine temporal resolution, to achieve robust\nscene understanding in challenging environments. Despite these advantages, the\ntask remains difficult due to two main challenges: extracting reliable features\nfrom sparse and noisy event streams, and effectively fusing them with dense,\nsemantically rich image data that differ in structure and representation. To\naddress these issues, we propose EIFNet, a multi-modal fusion network that\ncombines the strengths of both event and frame-based inputs. The network\nincludes an Adaptive Event Feature Refinement Module (AEFRM), which improves\nevent representations through multi-scale activity modeling and spatial\nattention. In addition, we introduce a Modality-Adaptive Recalibration Module\n(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and\nintegrate features across modalities using attention mechanisms and gated\nfusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets\nshow that EIFNet achieves state-of-the-art performance, demonstrating its\neffectiveness in event-based semantic segmentation.\n", "link": "http://arxiv.org/abs/2507.21971v1", "date": "2025-07-29", "relevancy": 2.0631, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EIFNet%3A%20Leveraging%20Event-Image%20Fusion%20for%20Robust%20Semantic%20Segmentation&body=Title%3A%20EIFNet%3A%20Leveraging%20Event-Image%20Fusion%20for%20Robust%20Semantic%20Segmentation%0AAuthor%3A%20Zhijiang%20Li%20and%20Haoran%20He%0AAbstract%3A%20%20%20Event-based%20semantic%20segmentation%20explores%20the%20potential%20of%20event%20cameras%2C%0Awhich%20offer%20high%20dynamic%20range%20and%20fine%20temporal%20resolution%2C%20to%20achieve%20robust%0Ascene%20understanding%20in%20challenging%20environments.%20Despite%20these%20advantages%2C%20the%0Atask%20remains%20difficult%20due%20to%20two%20main%20challenges%3A%20extracting%20reliable%20features%0Afrom%20sparse%20and%20noisy%20event%20streams%2C%20and%20effectively%20fusing%20them%20with%20dense%2C%0Asemantically%20rich%20image%20data%20that%20differ%20in%20structure%20and%20representation.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20EIFNet%2C%20a%20multi-modal%20fusion%20network%20that%0Acombines%20the%20strengths%20of%20both%20event%20and%20frame-based%20inputs.%20The%20network%0Aincludes%20an%20Adaptive%20Event%20Feature%20Refinement%20Module%20%28AEFRM%29%2C%20which%20improves%0Aevent%20representations%20through%20multi-scale%20activity%20modeling%20and%20spatial%0Aattention.%20In%20addition%2C%20we%20introduce%20a%20Modality-Adaptive%20Recalibration%20Module%0A%28MARM%29%20and%20a%20Multi-Head%20Attention%20Gated%20Fusion%20Module%20%28MGFM%29%2C%20which%20align%20and%0Aintegrate%20features%20across%20modalities%20using%20attention%20mechanisms%20and%20gated%0Afusion%20strategies.%20Experiments%20on%20DDD17-Semantic%20and%20DSEC-Semantic%20datasets%0Ashow%20that%20EIFNet%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20its%0Aeffectiveness%20in%20event-based%20semantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEIFNet%253A%2520Leveraging%2520Event-Image%2520Fusion%2520for%2520Robust%2520Semantic%2520Segmentation%26entry.906535625%3DZhijiang%2520Li%2520and%2520Haoran%2520He%26entry.1292438233%3D%2520%2520Event-based%2520semantic%2520segmentation%2520explores%2520the%2520potential%2520of%2520event%2520cameras%252C%250Awhich%2520offer%2520high%2520dynamic%2520range%2520and%2520fine%2520temporal%2520resolution%252C%2520to%2520achieve%2520robust%250Ascene%2520understanding%2520in%2520challenging%2520environments.%2520Despite%2520these%2520advantages%252C%2520the%250Atask%2520remains%2520difficult%2520due%2520to%2520two%2520main%2520challenges%253A%2520extracting%2520reliable%2520features%250Afrom%2520sparse%2520and%2520noisy%2520event%2520streams%252C%2520and%2520effectively%2520fusing%2520them%2520with%2520dense%252C%250Asemantically%2520rich%2520image%2520data%2520that%2520differ%2520in%2520structure%2520and%2520representation.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520EIFNet%252C%2520a%2520multi-modal%2520fusion%2520network%2520that%250Acombines%2520the%2520strengths%2520of%2520both%2520event%2520and%2520frame-based%2520inputs.%2520The%2520network%250Aincludes%2520an%2520Adaptive%2520Event%2520Feature%2520Refinement%2520Module%2520%2528AEFRM%2529%252C%2520which%2520improves%250Aevent%2520representations%2520through%2520multi-scale%2520activity%2520modeling%2520and%2520spatial%250Aattention.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520Modality-Adaptive%2520Recalibration%2520Module%250A%2528MARM%2529%2520and%2520a%2520Multi-Head%2520Attention%2520Gated%2520Fusion%2520Module%2520%2528MGFM%2529%252C%2520which%2520align%2520and%250Aintegrate%2520features%2520across%2520modalities%2520using%2520attention%2520mechanisms%2520and%2520gated%250Afusion%2520strategies.%2520Experiments%2520on%2520DDD17-Semantic%2520and%2520DSEC-Semantic%2520datasets%250Ashow%2520that%2520EIFNet%2520achieves%2520state-of-the-art%2520performance%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520event-based%2520semantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EIFNet%3A%20Leveraging%20Event-Image%20Fusion%20for%20Robust%20Semantic%20Segmentation&entry.906535625=Zhijiang%20Li%20and%20Haoran%20He&entry.1292438233=%20%20Event-based%20semantic%20segmentation%20explores%20the%20potential%20of%20event%20cameras%2C%0Awhich%20offer%20high%20dynamic%20range%20and%20fine%20temporal%20resolution%2C%20to%20achieve%20robust%0Ascene%20understanding%20in%20challenging%20environments.%20Despite%20these%20advantages%2C%20the%0Atask%20remains%20difficult%20due%20to%20two%20main%20challenges%3A%20extracting%20reliable%20features%0Afrom%20sparse%20and%20noisy%20event%20streams%2C%20and%20effectively%20fusing%20them%20with%20dense%2C%0Asemantically%20rich%20image%20data%20that%20differ%20in%20structure%20and%20representation.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20EIFNet%2C%20a%20multi-modal%20fusion%20network%20that%0Acombines%20the%20strengths%20of%20both%20event%20and%20frame-based%20inputs.%20The%20network%0Aincludes%20an%20Adaptive%20Event%20Feature%20Refinement%20Module%20%28AEFRM%29%2C%20which%20improves%0Aevent%20representations%20through%20multi-scale%20activity%20modeling%20and%20spatial%0Aattention.%20In%20addition%2C%20we%20introduce%20a%20Modality-Adaptive%20Recalibration%20Module%0A%28MARM%29%20and%20a%20Multi-Head%20Attention%20Gated%20Fusion%20Module%20%28MGFM%29%2C%20which%20align%20and%0Aintegrate%20features%20across%20modalities%20using%20attention%20mechanisms%20and%20gated%0Afusion%20strategies.%20Experiments%20on%20DDD17-Semantic%20and%20DSEC-Semantic%20datasets%0Ashow%20that%20EIFNet%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20its%0Aeffectiveness%20in%20event-based%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21971v1&entry.124074799=Read"},
{"title": "Supervised Quantum Image Processing", "author": "Marco Parigi and Mehran Khosrojerdi and Filippo Caruso and Leonardo Banchi", "abstract": "  In the era of big data and artificial intelligence, the increasing volume of\ndata and the demand to solve more and more complex computational challenges are\ntwo driving forces for improving the efficiency of data storage, processing and\nanalysis. Quantum image processing (QIP) is an interdisciplinary field between\nquantum information science and image processing, which has the potential to\nalleviate some of these challenges by leveraging the power of quantum\ncomputing. In this work, we compare and examine the compression properties of\nfour different Quantum Image Representations (QImRs): namely, Tensor Network\nRepresentation (TNR), Flexible Representation of Quantum Image (FRQI), Novel\nEnhanced Quantum Representation NEQR, and Quantum Probability Image Encoding\n(QPIE). Our simulations show that FRQI performs a higher compression of image\ninformation than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off\nbetween accuracy and memory in binary classification problems, evaluating the\nperformance of quantum kernels based on QImRs compared to the classical linear\nkernel. Our results indicate that quantum kernels provide comparable\nclassification average accuracy but require exponentially fewer resources for\nimage storage.\n", "link": "http://arxiv.org/abs/2507.22039v1", "date": "2025-07-29", "relevancy": 2.0272, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5176}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5075}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Quantum%20Image%20Processing&body=Title%3A%20Supervised%20Quantum%20Image%20Processing%0AAuthor%3A%20Marco%20Parigi%20and%20Mehran%20Khosrojerdi%20and%20Filippo%20Caruso%20and%20Leonardo%20Banchi%0AAbstract%3A%20%20%20In%20the%20era%20of%20big%20data%20and%20artificial%20intelligence%2C%20the%20increasing%20volume%20of%0Adata%20and%20the%20demand%20to%20solve%20more%20and%20more%20complex%20computational%20challenges%20are%0Atwo%20driving%20forces%20for%20improving%20the%20efficiency%20of%20data%20storage%2C%20processing%20and%0Aanalysis.%20Quantum%20image%20processing%20%28QIP%29%20is%20an%20interdisciplinary%20field%20between%0Aquantum%20information%20science%20and%20image%20processing%2C%20which%20has%20the%20potential%20to%0Aalleviate%20some%20of%20these%20challenges%20by%20leveraging%20the%20power%20of%20quantum%0Acomputing.%20In%20this%20work%2C%20we%20compare%20and%20examine%20the%20compression%20properties%20of%0Afour%20different%20Quantum%20Image%20Representations%20%28QImRs%29%3A%20namely%2C%20Tensor%20Network%0ARepresentation%20%28TNR%29%2C%20Flexible%20Representation%20of%20Quantum%20Image%20%28FRQI%29%2C%20Novel%0AEnhanced%20Quantum%20Representation%20NEQR%2C%20and%20Quantum%20Probability%20Image%20Encoding%0A%28QPIE%29.%20Our%20simulations%20show%20that%20FRQI%20performs%20a%20higher%20compression%20of%20image%0Ainformation%20than%20TNR%2C%20NEQR%2C%20and%20QPIE.%20Furthermore%2C%20we%20investigate%20the%20trade-off%0Abetween%20accuracy%20and%20memory%20in%20binary%20classification%20problems%2C%20evaluating%20the%0Aperformance%20of%20quantum%20kernels%20based%20on%20QImRs%20compared%20to%20the%20classical%20linear%0Akernel.%20Our%20results%20indicate%20that%20quantum%20kernels%20provide%20comparable%0Aclassification%20average%20accuracy%20but%20require%20exponentially%20fewer%20resources%20for%0Aimage%20storage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Quantum%2520Image%2520Processing%26entry.906535625%3DMarco%2520Parigi%2520and%2520Mehran%2520Khosrojerdi%2520and%2520Filippo%2520Caruso%2520and%2520Leonardo%2520Banchi%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520big%2520data%2520and%2520artificial%2520intelligence%252C%2520the%2520increasing%2520volume%2520of%250Adata%2520and%2520the%2520demand%2520to%2520solve%2520more%2520and%2520more%2520complex%2520computational%2520challenges%2520are%250Atwo%2520driving%2520forces%2520for%2520improving%2520the%2520efficiency%2520of%2520data%2520storage%252C%2520processing%2520and%250Aanalysis.%2520Quantum%2520image%2520processing%2520%2528QIP%2529%2520is%2520an%2520interdisciplinary%2520field%2520between%250Aquantum%2520information%2520science%2520and%2520image%2520processing%252C%2520which%2520has%2520the%2520potential%2520to%250Aalleviate%2520some%2520of%2520these%2520challenges%2520by%2520leveraging%2520the%2520power%2520of%2520quantum%250Acomputing.%2520In%2520this%2520work%252C%2520we%2520compare%2520and%2520examine%2520the%2520compression%2520properties%2520of%250Afour%2520different%2520Quantum%2520Image%2520Representations%2520%2528QImRs%2529%253A%2520namely%252C%2520Tensor%2520Network%250ARepresentation%2520%2528TNR%2529%252C%2520Flexible%2520Representation%2520of%2520Quantum%2520Image%2520%2528FRQI%2529%252C%2520Novel%250AEnhanced%2520Quantum%2520Representation%2520NEQR%252C%2520and%2520Quantum%2520Probability%2520Image%2520Encoding%250A%2528QPIE%2529.%2520Our%2520simulations%2520show%2520that%2520FRQI%2520performs%2520a%2520higher%2520compression%2520of%2520image%250Ainformation%2520than%2520TNR%252C%2520NEQR%252C%2520and%2520QPIE.%2520Furthermore%252C%2520we%2520investigate%2520the%2520trade-off%250Abetween%2520accuracy%2520and%2520memory%2520in%2520binary%2520classification%2520problems%252C%2520evaluating%2520the%250Aperformance%2520of%2520quantum%2520kernels%2520based%2520on%2520QImRs%2520compared%2520to%2520the%2520classical%2520linear%250Akernel.%2520Our%2520results%2520indicate%2520that%2520quantum%2520kernels%2520provide%2520comparable%250Aclassification%2520average%2520accuracy%2520but%2520require%2520exponentially%2520fewer%2520resources%2520for%250Aimage%2520storage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Quantum%20Image%20Processing&entry.906535625=Marco%20Parigi%20and%20Mehran%20Khosrojerdi%20and%20Filippo%20Caruso%20and%20Leonardo%20Banchi&entry.1292438233=%20%20In%20the%20era%20of%20big%20data%20and%20artificial%20intelligence%2C%20the%20increasing%20volume%20of%0Adata%20and%20the%20demand%20to%20solve%20more%20and%20more%20complex%20computational%20challenges%20are%0Atwo%20driving%20forces%20for%20improving%20the%20efficiency%20of%20data%20storage%2C%20processing%20and%0Aanalysis.%20Quantum%20image%20processing%20%28QIP%29%20is%20an%20interdisciplinary%20field%20between%0Aquantum%20information%20science%20and%20image%20processing%2C%20which%20has%20the%20potential%20to%0Aalleviate%20some%20of%20these%20challenges%20by%20leveraging%20the%20power%20of%20quantum%0Acomputing.%20In%20this%20work%2C%20we%20compare%20and%20examine%20the%20compression%20properties%20of%0Afour%20different%20Quantum%20Image%20Representations%20%28QImRs%29%3A%20namely%2C%20Tensor%20Network%0ARepresentation%20%28TNR%29%2C%20Flexible%20Representation%20of%20Quantum%20Image%20%28FRQI%29%2C%20Novel%0AEnhanced%20Quantum%20Representation%20NEQR%2C%20and%20Quantum%20Probability%20Image%20Encoding%0A%28QPIE%29.%20Our%20simulations%20show%20that%20FRQI%20performs%20a%20higher%20compression%20of%20image%0Ainformation%20than%20TNR%2C%20NEQR%2C%20and%20QPIE.%20Furthermore%2C%20we%20investigate%20the%20trade-off%0Abetween%20accuracy%20and%20memory%20in%20binary%20classification%20problems%2C%20evaluating%20the%0Aperformance%20of%20quantum%20kernels%20based%20on%20QImRs%20compared%20to%20the%20classical%20linear%0Akernel.%20Our%20results%20indicate%20that%20quantum%20kernels%20provide%20comparable%0Aclassification%20average%20accuracy%20but%20require%20exponentially%20fewer%20resources%20for%0Aimage%20storage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22039v1&entry.124074799=Read"},
{"title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with\n  Reinforcement Learning for Multimodal Model Security", "author": "Muzhi Dai and Shixuan Liu and Zhiyuan Zhao and Junyu Gao and Hao Sun and Xuelong Li", "abstract": "  The rapid advancement of multimodal large language models (MLLMs) has led to\nbreakthroughs in various applications, yet their security remains a critical\nchallenge. One pressing issue involves unsafe image-query pairs--jailbreak\ninputs specifically designed to bypass security constraints and elicit\nunintended responses from MLLMs. Compared to general multimodal data, such\nunsafe inputs are relatively sparse, which limits the diversity and richness of\ntraining samples available for developing robust defense models. Meanwhile,\nexisting guardrail-type methods rely on external modules to enforce security\nconstraints but fail to address intrinsic vulnerabilities within MLLMs.\nTraditional supervised fine-tuning (SFT), on the other hand, often over-refuses\nharmless inputs, compromising general performance. Given these challenges, we\npropose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack\ntraining method to enhance the security of MLLMs. SecTOW consists of two\nmodules: a defender and an auxiliary attacker, both trained iteratively using\nreinforcement learning (GRPO). During the iterative process, the attacker\nidentifies security vulnerabilities in the defense model and expands jailbreak\ndata. The expanded data are then used to train the defender, enabling it to\naddress identified security vulnerabilities. We also design reward mechanisms\nused for GRPO to simplify the use of response labels, reducing dependence on\ncomplex generative labels and enabling the efficient use of synthetic data.\nAdditionally, a quality monitoring mechanism is used to mitigate the defender's\nover-refusal of harmless inputs and ensure the diversity of the jailbreak data\ngenerated by the attacker. Experimental results on safety-specific and general\nbenchmarks demonstrate that SecTOW significantly improves security while\npreserving general performance.\n", "link": "http://arxiv.org/abs/2507.22037v1", "date": "2025-07-29", "relevancy": 2.0185, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5051}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secure%20Tug-of-War%20%28SecTOW%29%3A%20Iterative%20Defense-Attack%20Training%20with%0A%20%20Reinforcement%20Learning%20for%20Multimodal%20Model%20Security&body=Title%3A%20Secure%20Tug-of-War%20%28SecTOW%29%3A%20Iterative%20Defense-Attack%20Training%20with%0A%20%20Reinforcement%20Learning%20for%20Multimodal%20Model%20Security%0AAuthor%3A%20Muzhi%20Dai%20and%20Shixuan%20Liu%20and%20Zhiyuan%20Zhao%20and%20Junyu%20Gao%20and%20Hao%20Sun%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20has%20led%20to%0Abreakthroughs%20in%20various%20applications%2C%20yet%20their%20security%20remains%20a%20critical%0Achallenge.%20One%20pressing%20issue%20involves%20unsafe%20image-query%20pairs--jailbreak%0Ainputs%20specifically%20designed%20to%20bypass%20security%20constraints%20and%20elicit%0Aunintended%20responses%20from%20MLLMs.%20Compared%20to%20general%20multimodal%20data%2C%20such%0Aunsafe%20inputs%20are%20relatively%20sparse%2C%20which%20limits%20the%20diversity%20and%20richness%20of%0Atraining%20samples%20available%20for%20developing%20robust%20defense%20models.%20Meanwhile%2C%0Aexisting%20guardrail-type%20methods%20rely%20on%20external%20modules%20to%20enforce%20security%0Aconstraints%20but%20fail%20to%20address%20intrinsic%20vulnerabilities%20within%20MLLMs.%0ATraditional%20supervised%20fine-tuning%20%28SFT%29%2C%20on%20the%20other%20hand%2C%20often%20over-refuses%0Aharmless%20inputs%2C%20compromising%20general%20performance.%20Given%20these%20challenges%2C%20we%0Apropose%20Secure%20Tug-of-War%20%28SecTOW%29%2C%20an%20innovative%20iterative%20defense-attack%0Atraining%20method%20to%20enhance%20the%20security%20of%20MLLMs.%20SecTOW%20consists%20of%20two%0Amodules%3A%20a%20defender%20and%20an%20auxiliary%20attacker%2C%20both%20trained%20iteratively%20using%0Areinforcement%20learning%20%28GRPO%29.%20During%20the%20iterative%20process%2C%20the%20attacker%0Aidentifies%20security%20vulnerabilities%20in%20the%20defense%20model%20and%20expands%20jailbreak%0Adata.%20The%20expanded%20data%20are%20then%20used%20to%20train%20the%20defender%2C%20enabling%20it%20to%0Aaddress%20identified%20security%20vulnerabilities.%20We%20also%20design%20reward%20mechanisms%0Aused%20for%20GRPO%20to%20simplify%20the%20use%20of%20response%20labels%2C%20reducing%20dependence%20on%0Acomplex%20generative%20labels%20and%20enabling%20the%20efficient%20use%20of%20synthetic%20data.%0AAdditionally%2C%20a%20quality%20monitoring%20mechanism%20is%20used%20to%20mitigate%20the%20defender%27s%0Aover-refusal%20of%20harmless%20inputs%20and%20ensure%20the%20diversity%20of%20the%20jailbreak%20data%0Agenerated%20by%20the%20attacker.%20Experimental%20results%20on%20safety-specific%20and%20general%0Abenchmarks%20demonstrate%20that%20SecTOW%20significantly%20improves%20security%20while%0Apreserving%20general%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecure%2520Tug-of-War%2520%2528SecTOW%2529%253A%2520Iterative%2520Defense-Attack%2520Training%2520with%250A%2520%2520Reinforcement%2520Learning%2520for%2520Multimodal%2520Model%2520Security%26entry.906535625%3DMuzhi%2520Dai%2520and%2520Shixuan%2520Liu%2520and%2520Zhiyuan%2520Zhao%2520and%2520Junyu%2520Gao%2520and%2520Hao%2520Sun%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520has%2520led%2520to%250Abreakthroughs%2520in%2520various%2520applications%252C%2520yet%2520their%2520security%2520remains%2520a%2520critical%250Achallenge.%2520One%2520pressing%2520issue%2520involves%2520unsafe%2520image-query%2520pairs--jailbreak%250Ainputs%2520specifically%2520designed%2520to%2520bypass%2520security%2520constraints%2520and%2520elicit%250Aunintended%2520responses%2520from%2520MLLMs.%2520Compared%2520to%2520general%2520multimodal%2520data%252C%2520such%250Aunsafe%2520inputs%2520are%2520relatively%2520sparse%252C%2520which%2520limits%2520the%2520diversity%2520and%2520richness%2520of%250Atraining%2520samples%2520available%2520for%2520developing%2520robust%2520defense%2520models.%2520Meanwhile%252C%250Aexisting%2520guardrail-type%2520methods%2520rely%2520on%2520external%2520modules%2520to%2520enforce%2520security%250Aconstraints%2520but%2520fail%2520to%2520address%2520intrinsic%2520vulnerabilities%2520within%2520MLLMs.%250ATraditional%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520on%2520the%2520other%2520hand%252C%2520often%2520over-refuses%250Aharmless%2520inputs%252C%2520compromising%2520general%2520performance.%2520Given%2520these%2520challenges%252C%2520we%250Apropose%2520Secure%2520Tug-of-War%2520%2528SecTOW%2529%252C%2520an%2520innovative%2520iterative%2520defense-attack%250Atraining%2520method%2520to%2520enhance%2520the%2520security%2520of%2520MLLMs.%2520SecTOW%2520consists%2520of%2520two%250Amodules%253A%2520a%2520defender%2520and%2520an%2520auxiliary%2520attacker%252C%2520both%2520trained%2520iteratively%2520using%250Areinforcement%2520learning%2520%2528GRPO%2529.%2520During%2520the%2520iterative%2520process%252C%2520the%2520attacker%250Aidentifies%2520security%2520vulnerabilities%2520in%2520the%2520defense%2520model%2520and%2520expands%2520jailbreak%250Adata.%2520The%2520expanded%2520data%2520are%2520then%2520used%2520to%2520train%2520the%2520defender%252C%2520enabling%2520it%2520to%250Aaddress%2520identified%2520security%2520vulnerabilities.%2520We%2520also%2520design%2520reward%2520mechanisms%250Aused%2520for%2520GRPO%2520to%2520simplify%2520the%2520use%2520of%2520response%2520labels%252C%2520reducing%2520dependence%2520on%250Acomplex%2520generative%2520labels%2520and%2520enabling%2520the%2520efficient%2520use%2520of%2520synthetic%2520data.%250AAdditionally%252C%2520a%2520quality%2520monitoring%2520mechanism%2520is%2520used%2520to%2520mitigate%2520the%2520defender%2527s%250Aover-refusal%2520of%2520harmless%2520inputs%2520and%2520ensure%2520the%2520diversity%2520of%2520the%2520jailbreak%2520data%250Agenerated%2520by%2520the%2520attacker.%2520Experimental%2520results%2520on%2520safety-specific%2520and%2520general%250Abenchmarks%2520demonstrate%2520that%2520SecTOW%2520significantly%2520improves%2520security%2520while%250Apreserving%2520general%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secure%20Tug-of-War%20%28SecTOW%29%3A%20Iterative%20Defense-Attack%20Training%20with%0A%20%20Reinforcement%20Learning%20for%20Multimodal%20Model%20Security&entry.906535625=Muzhi%20Dai%20and%20Shixuan%20Liu%20and%20Zhiyuan%20Zhao%20and%20Junyu%20Gao%20and%20Hao%20Sun%20and%20Xuelong%20Li&entry.1292438233=%20%20The%20rapid%20advancement%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20has%20led%20to%0Abreakthroughs%20in%20various%20applications%2C%20yet%20their%20security%20remains%20a%20critical%0Achallenge.%20One%20pressing%20issue%20involves%20unsafe%20image-query%20pairs--jailbreak%0Ainputs%20specifically%20designed%20to%20bypass%20security%20constraints%20and%20elicit%0Aunintended%20responses%20from%20MLLMs.%20Compared%20to%20general%20multimodal%20data%2C%20such%0Aunsafe%20inputs%20are%20relatively%20sparse%2C%20which%20limits%20the%20diversity%20and%20richness%20of%0Atraining%20samples%20available%20for%20developing%20robust%20defense%20models.%20Meanwhile%2C%0Aexisting%20guardrail-type%20methods%20rely%20on%20external%20modules%20to%20enforce%20security%0Aconstraints%20but%20fail%20to%20address%20intrinsic%20vulnerabilities%20within%20MLLMs.%0ATraditional%20supervised%20fine-tuning%20%28SFT%29%2C%20on%20the%20other%20hand%2C%20often%20over-refuses%0Aharmless%20inputs%2C%20compromising%20general%20performance.%20Given%20these%20challenges%2C%20we%0Apropose%20Secure%20Tug-of-War%20%28SecTOW%29%2C%20an%20innovative%20iterative%20defense-attack%0Atraining%20method%20to%20enhance%20the%20security%20of%20MLLMs.%20SecTOW%20consists%20of%20two%0Amodules%3A%20a%20defender%20and%20an%20auxiliary%20attacker%2C%20both%20trained%20iteratively%20using%0Areinforcement%20learning%20%28GRPO%29.%20During%20the%20iterative%20process%2C%20the%20attacker%0Aidentifies%20security%20vulnerabilities%20in%20the%20defense%20model%20and%20expands%20jailbreak%0Adata.%20The%20expanded%20data%20are%20then%20used%20to%20train%20the%20defender%2C%20enabling%20it%20to%0Aaddress%20identified%20security%20vulnerabilities.%20We%20also%20design%20reward%20mechanisms%0Aused%20for%20GRPO%20to%20simplify%20the%20use%20of%20response%20labels%2C%20reducing%20dependence%20on%0Acomplex%20generative%20labels%20and%20enabling%20the%20efficient%20use%20of%20synthetic%20data.%0AAdditionally%2C%20a%20quality%20monitoring%20mechanism%20is%20used%20to%20mitigate%20the%20defender%27s%0Aover-refusal%20of%20harmless%20inputs%20and%20ensure%20the%20diversity%20of%20the%20jailbreak%20data%0Agenerated%20by%20the%20attacker.%20Experimental%20results%20on%20safety-specific%20and%20general%0Abenchmarks%20demonstrate%20that%20SecTOW%20significantly%20improves%20security%20while%0Apreserving%20general%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22037v1&entry.124074799=Read"},
{"title": "An $\\tilde{O}$ptimal Differentially Private Learner for Concept Classes\n  with VC Dimension 1", "author": "Chao Yan", "abstract": "  We present the first nearly optimal differentially private PAC learner for\nany concept class with VC dimension 1 and Littlestone dimension $d$. Our\nalgorithm achieves the sample complexity of\n$\\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$, nearly matching the\nlower bound of $\\Omega(\\log^* d)$ proved by Alon et al. [STOC19]. Prior to our\nwork, the best known upper bound is $\\tilde{O}(VC\\cdot d^5)$ for general VC\nclasses, as shown by Ghazi et al. [STOC21].\n", "link": "http://arxiv.org/abs/2505.06581v2", "date": "2025-07-29", "relevancy": 1.9861, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.395}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20%24%5Ctilde%7BO%7D%24ptimal%20Differentially%20Private%20Learner%20for%20Concept%20Classes%0A%20%20with%20VC%20Dimension%201&body=Title%3A%20An%20%24%5Ctilde%7BO%7D%24ptimal%20Differentially%20Private%20Learner%20for%20Concept%20Classes%0A%20%20with%20VC%20Dimension%201%0AAuthor%3A%20Chao%20Yan%0AAbstract%3A%20%20%20We%20present%20the%20first%20nearly%20optimal%20differentially%20private%20PAC%20learner%20for%0Aany%20concept%20class%20with%20VC%20dimension%201%20and%20Littlestone%20dimension%20%24d%24.%20Our%0Aalgorithm%20achieves%20the%20sample%20complexity%20of%0A%24%5Ctilde%7BO%7D_%7B%5Cvarepsilon%2C%5Cdelta%2C%5Calpha%2C%5Cdelta%7D%28%5Clog%5E%2A%20d%29%24%2C%20nearly%20matching%20the%0Alower%20bound%20of%20%24%5COmega%28%5Clog%5E%2A%20d%29%24%20proved%20by%20Alon%20et%20al.%20%5BSTOC19%5D.%20Prior%20to%20our%0Awork%2C%20the%20best%20known%20upper%20bound%20is%20%24%5Ctilde%7BO%7D%28VC%5Ccdot%20d%5E5%29%24%20for%20general%20VC%0Aclasses%2C%20as%20shown%20by%20Ghazi%20et%20al.%20%5BSTOC21%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520%2524%255Ctilde%257BO%257D%2524ptimal%2520Differentially%2520Private%2520Learner%2520for%2520Concept%2520Classes%250A%2520%2520with%2520VC%2520Dimension%25201%26entry.906535625%3DChao%2520Yan%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520nearly%2520optimal%2520differentially%2520private%2520PAC%2520learner%2520for%250Aany%2520concept%2520class%2520with%2520VC%2520dimension%25201%2520and%2520Littlestone%2520dimension%2520%2524d%2524.%2520Our%250Aalgorithm%2520achieves%2520the%2520sample%2520complexity%2520of%250A%2524%255Ctilde%257BO%257D_%257B%255Cvarepsilon%252C%255Cdelta%252C%255Calpha%252C%255Cdelta%257D%2528%255Clog%255E%252A%2520d%2529%2524%252C%2520nearly%2520matching%2520the%250Alower%2520bound%2520of%2520%2524%255COmega%2528%255Clog%255E%252A%2520d%2529%2524%2520proved%2520by%2520Alon%2520et%2520al.%2520%255BSTOC19%255D.%2520Prior%2520to%2520our%250Awork%252C%2520the%2520best%2520known%2520upper%2520bound%2520is%2520%2524%255Ctilde%257BO%257D%2528VC%255Ccdot%2520d%255E5%2529%2524%2520for%2520general%2520VC%250Aclasses%252C%2520as%2520shown%2520by%2520Ghazi%2520et%2520al.%2520%255BSTOC21%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20%24%5Ctilde%7BO%7D%24ptimal%20Differentially%20Private%20Learner%20for%20Concept%20Classes%0A%20%20with%20VC%20Dimension%201&entry.906535625=Chao%20Yan&entry.1292438233=%20%20We%20present%20the%20first%20nearly%20optimal%20differentially%20private%20PAC%20learner%20for%0Aany%20concept%20class%20with%20VC%20dimension%201%20and%20Littlestone%20dimension%20%24d%24.%20Our%0Aalgorithm%20achieves%20the%20sample%20complexity%20of%0A%24%5Ctilde%7BO%7D_%7B%5Cvarepsilon%2C%5Cdelta%2C%5Calpha%2C%5Cdelta%7D%28%5Clog%5E%2A%20d%29%24%2C%20nearly%20matching%20the%0Alower%20bound%20of%20%24%5COmega%28%5Clog%5E%2A%20d%29%24%20proved%20by%20Alon%20et%20al.%20%5BSTOC19%5D.%20Prior%20to%20our%0Awork%2C%20the%20best%20known%20upper%20bound%20is%20%24%5Ctilde%7BO%7D%28VC%5Ccdot%20d%5E5%29%24%20for%20general%20VC%0Aclasses%2C%20as%20shown%20by%20Ghazi%20et%20al.%20%5BSTOC21%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06581v2&entry.124074799=Read"},
{"title": "Staining and locking computer vision models without retraining", "author": "Oliver J. Sutton and Qinghua Zhou and George Leete and Alexander N. Gorban and Ivan Y. Tyukin", "abstract": "  We introduce new methods of staining and locking computer vision models, to\nprotect their owners' intellectual property. Staining, also known as\nwatermarking, embeds secret behaviour into a model which can later be used to\nidentify it, while locking aims to make a model unusable unless a secret\ntrigger is inserted into input images. Unlike existing methods, our algorithms\ncan be used to stain and lock pre-trained models without requiring fine-tuning\nor retraining, and come with provable, computable guarantees bounding their\nworst-case false positive rates. The stain and lock are implemented by directly\nmodifying a small number of the model's weights and have minimal impact on the\n(unlocked) model's performance. Locked models are unlocked by inserting a small\n`trigger patch' into the corner of the input image. We present experimental\nresults showing the efficacy of our methods and demonstrating their practical\nperformance on a variety of computer vision models.\n", "link": "http://arxiv.org/abs/2507.22000v1", "date": "2025-07-29", "relevancy": 1.9659, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4918}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Staining%20and%20locking%20computer%20vision%20models%20without%20retraining&body=Title%3A%20Staining%20and%20locking%20computer%20vision%20models%20without%20retraining%0AAuthor%3A%20Oliver%20J.%20Sutton%20and%20Qinghua%20Zhou%20and%20George%20Leete%20and%20Alexander%20N.%20Gorban%20and%20Ivan%20Y.%20Tyukin%0AAbstract%3A%20%20%20We%20introduce%20new%20methods%20of%20staining%20and%20locking%20computer%20vision%20models%2C%20to%0Aprotect%20their%20owners%27%20intellectual%20property.%20Staining%2C%20also%20known%20as%0Awatermarking%2C%20embeds%20secret%20behaviour%20into%20a%20model%20which%20can%20later%20be%20used%20to%0Aidentify%20it%2C%20while%20locking%20aims%20to%20make%20a%20model%20unusable%20unless%20a%20secret%0Atrigger%20is%20inserted%20into%20input%20images.%20Unlike%20existing%20methods%2C%20our%20algorithms%0Acan%20be%20used%20to%20stain%20and%20lock%20pre-trained%20models%20without%20requiring%20fine-tuning%0Aor%20retraining%2C%20and%20come%20with%20provable%2C%20computable%20guarantees%20bounding%20their%0Aworst-case%20false%20positive%20rates.%20The%20stain%20and%20lock%20are%20implemented%20by%20directly%0Amodifying%20a%20small%20number%20of%20the%20model%27s%20weights%20and%20have%20minimal%20impact%20on%20the%0A%28unlocked%29%20model%27s%20performance.%20Locked%20models%20are%20unlocked%20by%20inserting%20a%20small%0A%60trigger%20patch%27%20into%20the%20corner%20of%20the%20input%20image.%20We%20present%20experimental%0Aresults%20showing%20the%20efficacy%20of%20our%20methods%20and%20demonstrating%20their%20practical%0Aperformance%20on%20a%20variety%20of%20computer%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStaining%2520and%2520locking%2520computer%2520vision%2520models%2520without%2520retraining%26entry.906535625%3DOliver%2520J.%2520Sutton%2520and%2520Qinghua%2520Zhou%2520and%2520George%2520Leete%2520and%2520Alexander%2520N.%2520Gorban%2520and%2520Ivan%2520Y.%2520Tyukin%26entry.1292438233%3D%2520%2520We%2520introduce%2520new%2520methods%2520of%2520staining%2520and%2520locking%2520computer%2520vision%2520models%252C%2520to%250Aprotect%2520their%2520owners%2527%2520intellectual%2520property.%2520Staining%252C%2520also%2520known%2520as%250Awatermarking%252C%2520embeds%2520secret%2520behaviour%2520into%2520a%2520model%2520which%2520can%2520later%2520be%2520used%2520to%250Aidentify%2520it%252C%2520while%2520locking%2520aims%2520to%2520make%2520a%2520model%2520unusable%2520unless%2520a%2520secret%250Atrigger%2520is%2520inserted%2520into%2520input%2520images.%2520Unlike%2520existing%2520methods%252C%2520our%2520algorithms%250Acan%2520be%2520used%2520to%2520stain%2520and%2520lock%2520pre-trained%2520models%2520without%2520requiring%2520fine-tuning%250Aor%2520retraining%252C%2520and%2520come%2520with%2520provable%252C%2520computable%2520guarantees%2520bounding%2520their%250Aworst-case%2520false%2520positive%2520rates.%2520The%2520stain%2520and%2520lock%2520are%2520implemented%2520by%2520directly%250Amodifying%2520a%2520small%2520number%2520of%2520the%2520model%2527s%2520weights%2520and%2520have%2520minimal%2520impact%2520on%2520the%250A%2528unlocked%2529%2520model%2527s%2520performance.%2520Locked%2520models%2520are%2520unlocked%2520by%2520inserting%2520a%2520small%250A%2560trigger%2520patch%2527%2520into%2520the%2520corner%2520of%2520the%2520input%2520image.%2520We%2520present%2520experimental%250Aresults%2520showing%2520the%2520efficacy%2520of%2520our%2520methods%2520and%2520demonstrating%2520their%2520practical%250Aperformance%2520on%2520a%2520variety%2520of%2520computer%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Staining%20and%20locking%20computer%20vision%20models%20without%20retraining&entry.906535625=Oliver%20J.%20Sutton%20and%20Qinghua%20Zhou%20and%20George%20Leete%20and%20Alexander%20N.%20Gorban%20and%20Ivan%20Y.%20Tyukin&entry.1292438233=%20%20We%20introduce%20new%20methods%20of%20staining%20and%20locking%20computer%20vision%20models%2C%20to%0Aprotect%20their%20owners%27%20intellectual%20property.%20Staining%2C%20also%20known%20as%0Awatermarking%2C%20embeds%20secret%20behaviour%20into%20a%20model%20which%20can%20later%20be%20used%20to%0Aidentify%20it%2C%20while%20locking%20aims%20to%20make%20a%20model%20unusable%20unless%20a%20secret%0Atrigger%20is%20inserted%20into%20input%20images.%20Unlike%20existing%20methods%2C%20our%20algorithms%0Acan%20be%20used%20to%20stain%20and%20lock%20pre-trained%20models%20without%20requiring%20fine-tuning%0Aor%20retraining%2C%20and%20come%20with%20provable%2C%20computable%20guarantees%20bounding%20their%0Aworst-case%20false%20positive%20rates.%20The%20stain%20and%20lock%20are%20implemented%20by%20directly%0Amodifying%20a%20small%20number%20of%20the%20model%27s%20weights%20and%20have%20minimal%20impact%20on%20the%0A%28unlocked%29%20model%27s%20performance.%20Locked%20models%20are%20unlocked%20by%20inserting%20a%20small%0A%60trigger%20patch%27%20into%20the%20corner%20of%20the%20input%20image.%20We%20present%20experimental%0Aresults%20showing%20the%20efficacy%20of%20our%20methods%20and%20demonstrating%20their%20practical%0Aperformance%20on%20a%20variety%20of%20computer%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22000v1&entry.124074799=Read"},
{"title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge\n  Distillation", "author": "Siddhartha Pradhan and Shikshya Shiwakoti and Neha Bathuri", "abstract": "  We investigate whether knowledge distillation (KD) from multiple\nheterogeneous teacher models can enhance the generation of transferable\nadversarial examples. A lightweight student model is trained using two KD\nstrategies: curriculum-based switching and joint optimization, with ResNet50\nand DenseNet-161 as teachers. The trained student is then used to generate\nadversarial examples using FG, FGS, and PGD attacks, which are evaluated\nagainst a black-box target model (GoogLeNet). Our results show that student\nmodels distilled from multiple teachers achieve attack success rates comparable\nto ensemble-based baselines, while reducing adversarial example generation time\nby up to a factor of six. An ablation study further reveals that lower\ntemperature settings and the inclusion of hard-label supervision significantly\nenhance transferability. These findings suggest that KD can serve not only as a\nmodel compression technique but also as a powerful tool for improving the\nefficiency and effectiveness of black-box adversarial attacks.\n", "link": "http://arxiv.org/abs/2507.21992v1", "date": "2025-07-29", "relevancy": 1.9483, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4963}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4849}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teach%20Me%20to%20Trick%3A%20Exploring%20Adversarial%20Transferability%20via%20Knowledge%0A%20%20Distillation&body=Title%3A%20Teach%20Me%20to%20Trick%3A%20Exploring%20Adversarial%20Transferability%20via%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Siddhartha%20Pradhan%20and%20Shikshya%20Shiwakoti%20and%20Neha%20Bathuri%0AAbstract%3A%20%20%20We%20investigate%20whether%20knowledge%20distillation%20%28KD%29%20from%20multiple%0Aheterogeneous%20teacher%20models%20can%20enhance%20the%20generation%20of%20transferable%0Aadversarial%20examples.%20A%20lightweight%20student%20model%20is%20trained%20using%20two%20KD%0Astrategies%3A%20curriculum-based%20switching%20and%20joint%20optimization%2C%20with%20ResNet50%0Aand%20DenseNet-161%20as%20teachers.%20The%20trained%20student%20is%20then%20used%20to%20generate%0Aadversarial%20examples%20using%20FG%2C%20FGS%2C%20and%20PGD%20attacks%2C%20which%20are%20evaluated%0Aagainst%20a%20black-box%20target%20model%20%28GoogLeNet%29.%20Our%20results%20show%20that%20student%0Amodels%20distilled%20from%20multiple%20teachers%20achieve%20attack%20success%20rates%20comparable%0Ato%20ensemble-based%20baselines%2C%20while%20reducing%20adversarial%20example%20generation%20time%0Aby%20up%20to%20a%20factor%20of%20six.%20An%20ablation%20study%20further%20reveals%20that%20lower%0Atemperature%20settings%20and%20the%20inclusion%20of%20hard-label%20supervision%20significantly%0Aenhance%20transferability.%20These%20findings%20suggest%20that%20KD%20can%20serve%20not%20only%20as%20a%0Amodel%20compression%20technique%20but%20also%20as%20a%20powerful%20tool%20for%20improving%20the%0Aefficiency%20and%20effectiveness%20of%20black-box%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeach%2520Me%2520to%2520Trick%253A%2520Exploring%2520Adversarial%2520Transferability%2520via%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DSiddhartha%2520Pradhan%2520and%2520Shikshya%2520Shiwakoti%2520and%2520Neha%2520Bathuri%26entry.1292438233%3D%2520%2520We%2520investigate%2520whether%2520knowledge%2520distillation%2520%2528KD%2529%2520from%2520multiple%250Aheterogeneous%2520teacher%2520models%2520can%2520enhance%2520the%2520generation%2520of%2520transferable%250Aadversarial%2520examples.%2520A%2520lightweight%2520student%2520model%2520is%2520trained%2520using%2520two%2520KD%250Astrategies%253A%2520curriculum-based%2520switching%2520and%2520joint%2520optimization%252C%2520with%2520ResNet50%250Aand%2520DenseNet-161%2520as%2520teachers.%2520The%2520trained%2520student%2520is%2520then%2520used%2520to%2520generate%250Aadversarial%2520examples%2520using%2520FG%252C%2520FGS%252C%2520and%2520PGD%2520attacks%252C%2520which%2520are%2520evaluated%250Aagainst%2520a%2520black-box%2520target%2520model%2520%2528GoogLeNet%2529.%2520Our%2520results%2520show%2520that%2520student%250Amodels%2520distilled%2520from%2520multiple%2520teachers%2520achieve%2520attack%2520success%2520rates%2520comparable%250Ato%2520ensemble-based%2520baselines%252C%2520while%2520reducing%2520adversarial%2520example%2520generation%2520time%250Aby%2520up%2520to%2520a%2520factor%2520of%2520six.%2520An%2520ablation%2520study%2520further%2520reveals%2520that%2520lower%250Atemperature%2520settings%2520and%2520the%2520inclusion%2520of%2520hard-label%2520supervision%2520significantly%250Aenhance%2520transferability.%2520These%2520findings%2520suggest%2520that%2520KD%2520can%2520serve%2520not%2520only%2520as%2520a%250Amodel%2520compression%2520technique%2520but%2520also%2520as%2520a%2520powerful%2520tool%2520for%2520improving%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520black-box%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teach%20Me%20to%20Trick%3A%20Exploring%20Adversarial%20Transferability%20via%20Knowledge%0A%20%20Distillation&entry.906535625=Siddhartha%20Pradhan%20and%20Shikshya%20Shiwakoti%20and%20Neha%20Bathuri&entry.1292438233=%20%20We%20investigate%20whether%20knowledge%20distillation%20%28KD%29%20from%20multiple%0Aheterogeneous%20teacher%20models%20can%20enhance%20the%20generation%20of%20transferable%0Aadversarial%20examples.%20A%20lightweight%20student%20model%20is%20trained%20using%20two%20KD%0Astrategies%3A%20curriculum-based%20switching%20and%20joint%20optimization%2C%20with%20ResNet50%0Aand%20DenseNet-161%20as%20teachers.%20The%20trained%20student%20is%20then%20used%20to%20generate%0Aadversarial%20examples%20using%20FG%2C%20FGS%2C%20and%20PGD%20attacks%2C%20which%20are%20evaluated%0Aagainst%20a%20black-box%20target%20model%20%28GoogLeNet%29.%20Our%20results%20show%20that%20student%0Amodels%20distilled%20from%20multiple%20teachers%20achieve%20attack%20success%20rates%20comparable%0Ato%20ensemble-based%20baselines%2C%20while%20reducing%20adversarial%20example%20generation%20time%0Aby%20up%20to%20a%20factor%20of%20six.%20An%20ablation%20study%20further%20reveals%20that%20lower%0Atemperature%20settings%20and%20the%20inclusion%20of%20hard-label%20supervision%20significantly%0Aenhance%20transferability.%20These%20findings%20suggest%20that%20KD%20can%20serve%20not%20only%20as%20a%0Amodel%20compression%20technique%20but%20also%20as%20a%20powerful%20tool%20for%20improving%20the%0Aefficiency%20and%20effectiveness%20of%20black-box%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21992v1&entry.124074799=Read"},
{"title": "SAKE: Steering Activations for Knowledge Editing", "author": "Marco Scialanga and Thibault Laugel and Vincent Grari and Marcin Detyniecki", "abstract": "  As Large Langue Models have been shown to memorize real-world facts, the need\nto update this knowledge in a controlled and efficient manner arises. Designed\nwith these constraints in mind, Knowledge Editing (KE) approaches propose to\nalter specific facts in pretrained models. However, they have been shown to\nsuffer from several limitations, including their lack of contextual robustness\nand their failure to generalize to logical implications related to the fact. To\novercome these issues, we propose SAKE, a steering activation method that\nmodels a fact to be edited as a distribution rather than a single prompt.\nLeveraging Optimal Transport, SAKE alters the LLM behavior over a whole\nfact-related distribution, defined as paraphrases and logical implications.\nSeveral numerical experiments demonstrate the effectiveness of this method:\nSAKE is thus able to perform more robust edits than its existing counterparts.\n", "link": "http://arxiv.org/abs/2503.01751v2", "date": "2025-07-29", "relevancy": 1.8919, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAKE%3A%20Steering%20Activations%20for%20Knowledge%20Editing&body=Title%3A%20SAKE%3A%20Steering%20Activations%20for%20Knowledge%20Editing%0AAuthor%3A%20Marco%20Scialanga%20and%20Thibault%20Laugel%20and%20Vincent%20Grari%20and%20Marcin%20Detyniecki%0AAbstract%3A%20%20%20As%20Large%20Langue%20Models%20have%20been%20shown%20to%20memorize%20real-world%20facts%2C%20the%20need%0Ato%20update%20this%20knowledge%20in%20a%20controlled%20and%20efficient%20manner%20arises.%20Designed%0Awith%20these%20constraints%20in%20mind%2C%20Knowledge%20Editing%20%28KE%29%20approaches%20propose%20to%0Aalter%20specific%20facts%20in%20pretrained%20models.%20However%2C%20they%20have%20been%20shown%20to%0Asuffer%20from%20several%20limitations%2C%20including%20their%20lack%20of%20contextual%20robustness%0Aand%20their%20failure%20to%20generalize%20to%20logical%20implications%20related%20to%20the%20fact.%20To%0Aovercome%20these%20issues%2C%20we%20propose%20SAKE%2C%20a%20steering%20activation%20method%20that%0Amodels%20a%20fact%20to%20be%20edited%20as%20a%20distribution%20rather%20than%20a%20single%20prompt.%0ALeveraging%20Optimal%20Transport%2C%20SAKE%20alters%20the%20LLM%20behavior%20over%20a%20whole%0Afact-related%20distribution%2C%20defined%20as%20paraphrases%20and%20logical%20implications.%0ASeveral%20numerical%20experiments%20demonstrate%20the%20effectiveness%20of%20this%20method%3A%0ASAKE%20is%20thus%20able%20to%20perform%20more%20robust%20edits%20than%20its%20existing%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAKE%253A%2520Steering%2520Activations%2520for%2520Knowledge%2520Editing%26entry.906535625%3DMarco%2520Scialanga%2520and%2520Thibault%2520Laugel%2520and%2520Vincent%2520Grari%2520and%2520Marcin%2520Detyniecki%26entry.1292438233%3D%2520%2520As%2520Large%2520Langue%2520Models%2520have%2520been%2520shown%2520to%2520memorize%2520real-world%2520facts%252C%2520the%2520need%250Ato%2520update%2520this%2520knowledge%2520in%2520a%2520controlled%2520and%2520efficient%2520manner%2520arises.%2520Designed%250Awith%2520these%2520constraints%2520in%2520mind%252C%2520Knowledge%2520Editing%2520%2528KE%2529%2520approaches%2520propose%2520to%250Aalter%2520specific%2520facts%2520in%2520pretrained%2520models.%2520However%252C%2520they%2520have%2520been%2520shown%2520to%250Asuffer%2520from%2520several%2520limitations%252C%2520including%2520their%2520lack%2520of%2520contextual%2520robustness%250Aand%2520their%2520failure%2520to%2520generalize%2520to%2520logical%2520implications%2520related%2520to%2520the%2520fact.%2520To%250Aovercome%2520these%2520issues%252C%2520we%2520propose%2520SAKE%252C%2520a%2520steering%2520activation%2520method%2520that%250Amodels%2520a%2520fact%2520to%2520be%2520edited%2520as%2520a%2520distribution%2520rather%2520than%2520a%2520single%2520prompt.%250ALeveraging%2520Optimal%2520Transport%252C%2520SAKE%2520alters%2520the%2520LLM%2520behavior%2520over%2520a%2520whole%250Afact-related%2520distribution%252C%2520defined%2520as%2520paraphrases%2520and%2520logical%2520implications.%250ASeveral%2520numerical%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520method%253A%250ASAKE%2520is%2520thus%2520able%2520to%2520perform%2520more%2520robust%2520edits%2520than%2520its%2520existing%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAKE%3A%20Steering%20Activations%20for%20Knowledge%20Editing&entry.906535625=Marco%20Scialanga%20and%20Thibault%20Laugel%20and%20Vincent%20Grari%20and%20Marcin%20Detyniecki&entry.1292438233=%20%20As%20Large%20Langue%20Models%20have%20been%20shown%20to%20memorize%20real-world%20facts%2C%20the%20need%0Ato%20update%20this%20knowledge%20in%20a%20controlled%20and%20efficient%20manner%20arises.%20Designed%0Awith%20these%20constraints%20in%20mind%2C%20Knowledge%20Editing%20%28KE%29%20approaches%20propose%20to%0Aalter%20specific%20facts%20in%20pretrained%20models.%20However%2C%20they%20have%20been%20shown%20to%0Asuffer%20from%20several%20limitations%2C%20including%20their%20lack%20of%20contextual%20robustness%0Aand%20their%20failure%20to%20generalize%20to%20logical%20implications%20related%20to%20the%20fact.%20To%0Aovercome%20these%20issues%2C%20we%20propose%20SAKE%2C%20a%20steering%20activation%20method%20that%0Amodels%20a%20fact%20to%20be%20edited%20as%20a%20distribution%20rather%20than%20a%20single%20prompt.%0ALeveraging%20Optimal%20Transport%2C%20SAKE%20alters%20the%20LLM%20behavior%20over%20a%20whole%0Afact-related%20distribution%2C%20defined%20as%20paraphrases%20and%20logical%20implications.%0ASeveral%20numerical%20experiments%20demonstrate%20the%20effectiveness%20of%20this%20method%3A%0ASAKE%20is%20thus%20able%20to%20perform%20more%20robust%20edits%20than%20its%20existing%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01751v2&entry.124074799=Read"},
{"title": "Extracting Interpretable Models from Tree Ensembles: Computational and\n  Statistical Perspectives", "author": "Brian Liu and Rahul Mazumder and Peter Radchenko", "abstract": "  Tree ensembles are non-parametric methods widely recognized for their\naccuracy and ability to capture complex interactions. While these models excel\nat prediction, they are difficult to interpret and may fail to uncover useful\nrelationships in the data. We propose an estimator to extract compact sets of\ndecision rules from tree ensembles. The extracted models are accurate and can\nbe manually examined to reveal relationships between the predictors and the\nresponse. A key novelty of our estimator is the flexibility to jointly control\nthe number of rules extracted and the interaction depth of each rule, which\nimproves accuracy. We develop a tailored exact algorithm to efficiently solve\noptimization problems underlying our estimator and an approximate algorithm for\ncomputing regularization paths, sequences of solutions that correspond to\nvarying model sizes. We also establish novel non-asymptotic prediction error\nbounds for our proposed approach, comparing it to an oracle that chooses the\nbest data-dependent linear combination of the rules in the ensemble subject to\nthe same complexity constraint as our estimator. The bounds illustrate that the\nlarge-sample predictive performance of our estimator is on par with that of the\noracle. Through experiments, we demonstrate that our estimator outperforms\nexisting algorithms for rule extraction.\n", "link": "http://arxiv.org/abs/2506.20114v3", "date": "2025-07-29", "relevancy": 1.8898, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Interpretable%20Models%20from%20Tree%20Ensembles%3A%20Computational%20and%0A%20%20Statistical%20Perspectives&body=Title%3A%20Extracting%20Interpretable%20Models%20from%20Tree%20Ensembles%3A%20Computational%20and%0A%20%20Statistical%20Perspectives%0AAuthor%3A%20Brian%20Liu%20and%20Rahul%20Mazumder%20and%20Peter%20Radchenko%0AAbstract%3A%20%20%20Tree%20ensembles%20are%20non-parametric%20methods%20widely%20recognized%20for%20their%0Aaccuracy%20and%20ability%20to%20capture%20complex%20interactions.%20While%20these%20models%20excel%0Aat%20prediction%2C%20they%20are%20difficult%20to%20interpret%20and%20may%20fail%20to%20uncover%20useful%0Arelationships%20in%20the%20data.%20We%20propose%20an%20estimator%20to%20extract%20compact%20sets%20of%0Adecision%20rules%20from%20tree%20ensembles.%20The%20extracted%20models%20are%20accurate%20and%20can%0Abe%20manually%20examined%20to%20reveal%20relationships%20between%20the%20predictors%20and%20the%0Aresponse.%20A%20key%20novelty%20of%20our%20estimator%20is%20the%20flexibility%20to%20jointly%20control%0Athe%20number%20of%20rules%20extracted%20and%20the%20interaction%20depth%20of%20each%20rule%2C%20which%0Aimproves%20accuracy.%20We%20develop%20a%20tailored%20exact%20algorithm%20to%20efficiently%20solve%0Aoptimization%20problems%20underlying%20our%20estimator%20and%20an%20approximate%20algorithm%20for%0Acomputing%20regularization%20paths%2C%20sequences%20of%20solutions%20that%20correspond%20to%0Avarying%20model%20sizes.%20We%20also%20establish%20novel%20non-asymptotic%20prediction%20error%0Abounds%20for%20our%20proposed%20approach%2C%20comparing%20it%20to%20an%20oracle%20that%20chooses%20the%0Abest%20data-dependent%20linear%20combination%20of%20the%20rules%20in%20the%20ensemble%20subject%20to%0Athe%20same%20complexity%20constraint%20as%20our%20estimator.%20The%20bounds%20illustrate%20that%20the%0Alarge-sample%20predictive%20performance%20of%20our%20estimator%20is%20on%20par%20with%20that%20of%20the%0Aoracle.%20Through%20experiments%2C%20we%20demonstrate%20that%20our%20estimator%20outperforms%0Aexisting%20algorithms%20for%20rule%20extraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20114v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Interpretable%2520Models%2520from%2520Tree%2520Ensembles%253A%2520Computational%2520and%250A%2520%2520Statistical%2520Perspectives%26entry.906535625%3DBrian%2520Liu%2520and%2520Rahul%2520Mazumder%2520and%2520Peter%2520Radchenko%26entry.1292438233%3D%2520%2520Tree%2520ensembles%2520are%2520non-parametric%2520methods%2520widely%2520recognized%2520for%2520their%250Aaccuracy%2520and%2520ability%2520to%2520capture%2520complex%2520interactions.%2520While%2520these%2520models%2520excel%250Aat%2520prediction%252C%2520they%2520are%2520difficult%2520to%2520interpret%2520and%2520may%2520fail%2520to%2520uncover%2520useful%250Arelationships%2520in%2520the%2520data.%2520We%2520propose%2520an%2520estimator%2520to%2520extract%2520compact%2520sets%2520of%250Adecision%2520rules%2520from%2520tree%2520ensembles.%2520The%2520extracted%2520models%2520are%2520accurate%2520and%2520can%250Abe%2520manually%2520examined%2520to%2520reveal%2520relationships%2520between%2520the%2520predictors%2520and%2520the%250Aresponse.%2520A%2520key%2520novelty%2520of%2520our%2520estimator%2520is%2520the%2520flexibility%2520to%2520jointly%2520control%250Athe%2520number%2520of%2520rules%2520extracted%2520and%2520the%2520interaction%2520depth%2520of%2520each%2520rule%252C%2520which%250Aimproves%2520accuracy.%2520We%2520develop%2520a%2520tailored%2520exact%2520algorithm%2520to%2520efficiently%2520solve%250Aoptimization%2520problems%2520underlying%2520our%2520estimator%2520and%2520an%2520approximate%2520algorithm%2520for%250Acomputing%2520regularization%2520paths%252C%2520sequences%2520of%2520solutions%2520that%2520correspond%2520to%250Avarying%2520model%2520sizes.%2520We%2520also%2520establish%2520novel%2520non-asymptotic%2520prediction%2520error%250Abounds%2520for%2520our%2520proposed%2520approach%252C%2520comparing%2520it%2520to%2520an%2520oracle%2520that%2520chooses%2520the%250Abest%2520data-dependent%2520linear%2520combination%2520of%2520the%2520rules%2520in%2520the%2520ensemble%2520subject%2520to%250Athe%2520same%2520complexity%2520constraint%2520as%2520our%2520estimator.%2520The%2520bounds%2520illustrate%2520that%2520the%250Alarge-sample%2520predictive%2520performance%2520of%2520our%2520estimator%2520is%2520on%2520par%2520with%2520that%2520of%2520the%250Aoracle.%2520Through%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520estimator%2520outperforms%250Aexisting%2520algorithms%2520for%2520rule%2520extraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20114v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Interpretable%20Models%20from%20Tree%20Ensembles%3A%20Computational%20and%0A%20%20Statistical%20Perspectives&entry.906535625=Brian%20Liu%20and%20Rahul%20Mazumder%20and%20Peter%20Radchenko&entry.1292438233=%20%20Tree%20ensembles%20are%20non-parametric%20methods%20widely%20recognized%20for%20their%0Aaccuracy%20and%20ability%20to%20capture%20complex%20interactions.%20While%20these%20models%20excel%0Aat%20prediction%2C%20they%20are%20difficult%20to%20interpret%20and%20may%20fail%20to%20uncover%20useful%0Arelationships%20in%20the%20data.%20We%20propose%20an%20estimator%20to%20extract%20compact%20sets%20of%0Adecision%20rules%20from%20tree%20ensembles.%20The%20extracted%20models%20are%20accurate%20and%20can%0Abe%20manually%20examined%20to%20reveal%20relationships%20between%20the%20predictors%20and%20the%0Aresponse.%20A%20key%20novelty%20of%20our%20estimator%20is%20the%20flexibility%20to%20jointly%20control%0Athe%20number%20of%20rules%20extracted%20and%20the%20interaction%20depth%20of%20each%20rule%2C%20which%0Aimproves%20accuracy.%20We%20develop%20a%20tailored%20exact%20algorithm%20to%20efficiently%20solve%0Aoptimization%20problems%20underlying%20our%20estimator%20and%20an%20approximate%20algorithm%20for%0Acomputing%20regularization%20paths%2C%20sequences%20of%20solutions%20that%20correspond%20to%0Avarying%20model%20sizes.%20We%20also%20establish%20novel%20non-asymptotic%20prediction%20error%0Abounds%20for%20our%20proposed%20approach%2C%20comparing%20it%20to%20an%20oracle%20that%20chooses%20the%0Abest%20data-dependent%20linear%20combination%20of%20the%20rules%20in%20the%20ensemble%20subject%20to%0Athe%20same%20complexity%20constraint%20as%20our%20estimator.%20The%20bounds%20illustrate%20that%20the%0Alarge-sample%20predictive%20performance%20of%20our%20estimator%20is%20on%20par%20with%20that%20of%20the%0Aoracle.%20Through%20experiments%2C%20we%20demonstrate%20that%20our%20estimator%20outperforms%0Aexisting%20algorithms%20for%20rule%20extraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20114v3&entry.124074799=Read"},
{"title": "Compton Form Factor Extraction using Quantum Deep Neural Networks", "author": "Brandon B. Le and Dustin Keller", "abstract": "  We present an extraction of Compton Form Factors (CFFs) from Deeply Virtual\nCompton Scattering (DVCS) experiments conducted at Thomas Jefferson National\nAccelerator Facility, utilizing Quantum Deep Neural Networks (QDNNs). The\nanalysis employs the standard Belitsky, Kirchner, and M\\\"uller formalism at\ntwist-two, complemented by a fitting procedure designed to minimize model\ndependence in a manner analogous to conventional local fits. A pseudodata\nextraction test of the CFFs is performed using both Classical Deep Neural\nNetworks (CDNNs) and QDNNs, with a detailed comparative analysis. Results\nindicate that QDNNs can outperform CDNNs in particular cases, offering enhanced\npredictive accuracy and precision even with limited model complexity. Motivated\nby this, we develop a metric to quantify the extent of the quantum advantage\nbased on characteristics of DVCS experimental data. These findings underscore\nthe promising role of QDNNs in advancing future investigations into\nmultidimensional parton distributions and hadronic physics.\n", "link": "http://arxiv.org/abs/2504.15458v2", "date": "2025-07-29", "relevancy": 1.887, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compton%20Form%20Factor%20Extraction%20using%20Quantum%20Deep%20Neural%20Networks&body=Title%3A%20Compton%20Form%20Factor%20Extraction%20using%20Quantum%20Deep%20Neural%20Networks%0AAuthor%3A%20Brandon%20B.%20Le%20and%20Dustin%20Keller%0AAbstract%3A%20%20%20We%20present%20an%20extraction%20of%20Compton%20Form%20Factors%20%28CFFs%29%20from%20Deeply%20Virtual%0ACompton%20Scattering%20%28DVCS%29%20experiments%20conducted%20at%20Thomas%20Jefferson%20National%0AAccelerator%20Facility%2C%20utilizing%20Quantum%20Deep%20Neural%20Networks%20%28QDNNs%29.%20The%0Aanalysis%20employs%20the%20standard%20Belitsky%2C%20Kirchner%2C%20and%20M%5C%22uller%20formalism%20at%0Atwist-two%2C%20complemented%20by%20a%20fitting%20procedure%20designed%20to%20minimize%20model%0Adependence%20in%20a%20manner%20analogous%20to%20conventional%20local%20fits.%20A%20pseudodata%0Aextraction%20test%20of%20the%20CFFs%20is%20performed%20using%20both%20Classical%20Deep%20Neural%0ANetworks%20%28CDNNs%29%20and%20QDNNs%2C%20with%20a%20detailed%20comparative%20analysis.%20Results%0Aindicate%20that%20QDNNs%20can%20outperform%20CDNNs%20in%20particular%20cases%2C%20offering%20enhanced%0Apredictive%20accuracy%20and%20precision%20even%20with%20limited%20model%20complexity.%20Motivated%0Aby%20this%2C%20we%20develop%20a%20metric%20to%20quantify%20the%20extent%20of%20the%20quantum%20advantage%0Abased%20on%20characteristics%20of%20DVCS%20experimental%20data.%20These%20findings%20underscore%0Athe%20promising%20role%20of%20QDNNs%20in%20advancing%20future%20investigations%20into%0Amultidimensional%20parton%20distributions%20and%20hadronic%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompton%2520Form%2520Factor%2520Extraction%2520using%2520Quantum%2520Deep%2520Neural%2520Networks%26entry.906535625%3DBrandon%2520B.%2520Le%2520and%2520Dustin%2520Keller%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520extraction%2520of%2520Compton%2520Form%2520Factors%2520%2528CFFs%2529%2520from%2520Deeply%2520Virtual%250ACompton%2520Scattering%2520%2528DVCS%2529%2520experiments%2520conducted%2520at%2520Thomas%2520Jefferson%2520National%250AAccelerator%2520Facility%252C%2520utilizing%2520Quantum%2520Deep%2520Neural%2520Networks%2520%2528QDNNs%2529.%2520The%250Aanalysis%2520employs%2520the%2520standard%2520Belitsky%252C%2520Kirchner%252C%2520and%2520M%255C%2522uller%2520formalism%2520at%250Atwist-two%252C%2520complemented%2520by%2520a%2520fitting%2520procedure%2520designed%2520to%2520minimize%2520model%250Adependence%2520in%2520a%2520manner%2520analogous%2520to%2520conventional%2520local%2520fits.%2520A%2520pseudodata%250Aextraction%2520test%2520of%2520the%2520CFFs%2520is%2520performed%2520using%2520both%2520Classical%2520Deep%2520Neural%250ANetworks%2520%2528CDNNs%2529%2520and%2520QDNNs%252C%2520with%2520a%2520detailed%2520comparative%2520analysis.%2520Results%250Aindicate%2520that%2520QDNNs%2520can%2520outperform%2520CDNNs%2520in%2520particular%2520cases%252C%2520offering%2520enhanced%250Apredictive%2520accuracy%2520and%2520precision%2520even%2520with%2520limited%2520model%2520complexity.%2520Motivated%250Aby%2520this%252C%2520we%2520develop%2520a%2520metric%2520to%2520quantify%2520the%2520extent%2520of%2520the%2520quantum%2520advantage%250Abased%2520on%2520characteristics%2520of%2520DVCS%2520experimental%2520data.%2520These%2520findings%2520underscore%250Athe%2520promising%2520role%2520of%2520QDNNs%2520in%2520advancing%2520future%2520investigations%2520into%250Amultidimensional%2520parton%2520distributions%2520and%2520hadronic%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compton%20Form%20Factor%20Extraction%20using%20Quantum%20Deep%20Neural%20Networks&entry.906535625=Brandon%20B.%20Le%20and%20Dustin%20Keller&entry.1292438233=%20%20We%20present%20an%20extraction%20of%20Compton%20Form%20Factors%20%28CFFs%29%20from%20Deeply%20Virtual%0ACompton%20Scattering%20%28DVCS%29%20experiments%20conducted%20at%20Thomas%20Jefferson%20National%0AAccelerator%20Facility%2C%20utilizing%20Quantum%20Deep%20Neural%20Networks%20%28QDNNs%29.%20The%0Aanalysis%20employs%20the%20standard%20Belitsky%2C%20Kirchner%2C%20and%20M%5C%22uller%20formalism%20at%0Atwist-two%2C%20complemented%20by%20a%20fitting%20procedure%20designed%20to%20minimize%20model%0Adependence%20in%20a%20manner%20analogous%20to%20conventional%20local%20fits.%20A%20pseudodata%0Aextraction%20test%20of%20the%20CFFs%20is%20performed%20using%20both%20Classical%20Deep%20Neural%0ANetworks%20%28CDNNs%29%20and%20QDNNs%2C%20with%20a%20detailed%20comparative%20analysis.%20Results%0Aindicate%20that%20QDNNs%20can%20outperform%20CDNNs%20in%20particular%20cases%2C%20offering%20enhanced%0Apredictive%20accuracy%20and%20precision%20even%20with%20limited%20model%20complexity.%20Motivated%0Aby%20this%2C%20we%20develop%20a%20metric%20to%20quantify%20the%20extent%20of%20the%20quantum%20advantage%0Abased%20on%20characteristics%20of%20DVCS%20experimental%20data.%20These%20findings%20underscore%0Athe%20promising%20role%20of%20QDNNs%20in%20advancing%20future%20investigations%20into%0Amultidimensional%20parton%20distributions%20and%20hadronic%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15458v2&entry.124074799=Read"},
{"title": "Cyst-X: AI-Powered Pancreatic Cancer Risk Prediction from Multicenter\n  MRI in Centralized and Federated Learning", "author": "Hongyi Pan and Gorkem Durak and Elif Keles and Deniz Seyithanoglu and Zheyuan Zhang and Alpay Medetalibeyoglu and Halil Ertugrul Aktas and Andrea Mia Bejar and Ziliang Hong and Yavuz Taktak and Gulbiz Dagoglu Kartal and Mehmet Sukru Erturk and Timurhan Cebeci and Maria Jaramillo Gonzalez and Yury Velichko and Lili Zhao and Emil Agarunov and Federica Proietto Salanitri and Concetto Spampinato and Pallavi Tiwari and Ziyue Xu and Sachin Jambawalikar and Ivo G. Schoots and Marco J. Bruno and Chenchang Huang and Candice Bolan and Tamas Gonda and Frank H. Miller and Rajesh N. Keswani and Michael B. Wallace and Ulas Bagci", "abstract": "  Pancreatic cancer is projected to become the second-deadliest malignancy in\nWestern countries by 2030, highlighting the urgent need for better early\ndetection. Intraductal papillary mucinous neoplasms (IPMNs), key precursors to\npancreatic cancer, are challenging to assess with current guidelines, often\nleading to unnecessary surgeries or missed malignancies. We present Cyst-X, an\nAI framework that predicts IPMN malignancy using multicenter MRI data,\nleveraging MRI's superior soft tissue contrast over CT. Trained on 723 T1- and\n738 T2-weighted scans from 764 patients across seven institutions, our models\n(AUC=0.82) significantly outperform both Kyoto guidelines (AUC=0.75) and expert\nradiologists. The AI-derived imaging features align with known clinical markers\nand offer biologically meaningful insights. We also demonstrate strong\nperformance in a federated learning setting, enabling collaborative training\nwithout sharing patient data. To promote privacy-preserving AI development and\nimprove IPMN risk stratification, the Cyst-X dataset is released as the first\nlarge-scale, multi-center pancreatic cysts MRI dataset.\n", "link": "http://arxiv.org/abs/2507.22017v1", "date": "2025-07-29", "relevancy": 1.8864, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cyst-X%3A%20AI-Powered%20Pancreatic%20Cancer%20Risk%20Prediction%20from%20Multicenter%0A%20%20MRI%20in%20Centralized%20and%20Federated%20Learning&body=Title%3A%20Cyst-X%3A%20AI-Powered%20Pancreatic%20Cancer%20Risk%20Prediction%20from%20Multicenter%0A%20%20MRI%20in%20Centralized%20and%20Federated%20Learning%0AAuthor%3A%20Hongyi%20Pan%20and%20Gorkem%20Durak%20and%20Elif%20Keles%20and%20Deniz%20Seyithanoglu%20and%20Zheyuan%20Zhang%20and%20Alpay%20Medetalibeyoglu%20and%20Halil%20Ertugrul%20Aktas%20and%20Andrea%20Mia%20Bejar%20and%20Ziliang%20Hong%20and%20Yavuz%20Taktak%20and%20Gulbiz%20Dagoglu%20Kartal%20and%20Mehmet%20Sukru%20Erturk%20and%20Timurhan%20Cebeci%20and%20Maria%20Jaramillo%20Gonzalez%20and%20Yury%20Velichko%20and%20Lili%20Zhao%20and%20Emil%20Agarunov%20and%20Federica%20Proietto%20Salanitri%20and%20Concetto%20Spampinato%20and%20Pallavi%20Tiwari%20and%20Ziyue%20Xu%20and%20Sachin%20Jambawalikar%20and%20Ivo%20G.%20Schoots%20and%20Marco%20J.%20Bruno%20and%20Chenchang%20Huang%20and%20Candice%20Bolan%20and%20Tamas%20Gonda%20and%20Frank%20H.%20Miller%20and%20Rajesh%20N.%20Keswani%20and%20Michael%20B.%20Wallace%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Pancreatic%20cancer%20is%20projected%20to%20become%20the%20second-deadliest%20malignancy%20in%0AWestern%20countries%20by%202030%2C%20highlighting%20the%20urgent%20need%20for%20better%20early%0Adetection.%20Intraductal%20papillary%20mucinous%20neoplasms%20%28IPMNs%29%2C%20key%20precursors%20to%0Apancreatic%20cancer%2C%20are%20challenging%20to%20assess%20with%20current%20guidelines%2C%20often%0Aleading%20to%20unnecessary%20surgeries%20or%20missed%20malignancies.%20We%20present%20Cyst-X%2C%20an%0AAI%20framework%20that%20predicts%20IPMN%20malignancy%20using%20multicenter%20MRI%20data%2C%0Aleveraging%20MRI%27s%20superior%20soft%20tissue%20contrast%20over%20CT.%20Trained%20on%20723%20T1-%20and%0A738%20T2-weighted%20scans%20from%20764%20patients%20across%20seven%20institutions%2C%20our%20models%0A%28AUC%3D0.82%29%20significantly%20outperform%20both%20Kyoto%20guidelines%20%28AUC%3D0.75%29%20and%20expert%0Aradiologists.%20The%20AI-derived%20imaging%20features%20align%20with%20known%20clinical%20markers%0Aand%20offer%20biologically%20meaningful%20insights.%20We%20also%20demonstrate%20strong%0Aperformance%20in%20a%20federated%20learning%20setting%2C%20enabling%20collaborative%20training%0Awithout%20sharing%20patient%20data.%20To%20promote%20privacy-preserving%20AI%20development%20and%0Aimprove%20IPMN%20risk%20stratification%2C%20the%20Cyst-X%20dataset%20is%20released%20as%20the%20first%0Alarge-scale%2C%20multi-center%20pancreatic%20cysts%20MRI%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyst-X%253A%2520AI-Powered%2520Pancreatic%2520Cancer%2520Risk%2520Prediction%2520from%2520Multicenter%250A%2520%2520MRI%2520in%2520Centralized%2520and%2520Federated%2520Learning%26entry.906535625%3DHongyi%2520Pan%2520and%2520Gorkem%2520Durak%2520and%2520Elif%2520Keles%2520and%2520Deniz%2520Seyithanoglu%2520and%2520Zheyuan%2520Zhang%2520and%2520Alpay%2520Medetalibeyoglu%2520and%2520Halil%2520Ertugrul%2520Aktas%2520and%2520Andrea%2520Mia%2520Bejar%2520and%2520Ziliang%2520Hong%2520and%2520Yavuz%2520Taktak%2520and%2520Gulbiz%2520Dagoglu%2520Kartal%2520and%2520Mehmet%2520Sukru%2520Erturk%2520and%2520Timurhan%2520Cebeci%2520and%2520Maria%2520Jaramillo%2520Gonzalez%2520and%2520Yury%2520Velichko%2520and%2520Lili%2520Zhao%2520and%2520Emil%2520Agarunov%2520and%2520Federica%2520Proietto%2520Salanitri%2520and%2520Concetto%2520Spampinato%2520and%2520Pallavi%2520Tiwari%2520and%2520Ziyue%2520Xu%2520and%2520Sachin%2520Jambawalikar%2520and%2520Ivo%2520G.%2520Schoots%2520and%2520Marco%2520J.%2520Bruno%2520and%2520Chenchang%2520Huang%2520and%2520Candice%2520Bolan%2520and%2520Tamas%2520Gonda%2520and%2520Frank%2520H.%2520Miller%2520and%2520Rajesh%2520N.%2520Keswani%2520and%2520Michael%2520B.%2520Wallace%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Pancreatic%2520cancer%2520is%2520projected%2520to%2520become%2520the%2520second-deadliest%2520malignancy%2520in%250AWestern%2520countries%2520by%25202030%252C%2520highlighting%2520the%2520urgent%2520need%2520for%2520better%2520early%250Adetection.%2520Intraductal%2520papillary%2520mucinous%2520neoplasms%2520%2528IPMNs%2529%252C%2520key%2520precursors%2520to%250Apancreatic%2520cancer%252C%2520are%2520challenging%2520to%2520assess%2520with%2520current%2520guidelines%252C%2520often%250Aleading%2520to%2520unnecessary%2520surgeries%2520or%2520missed%2520malignancies.%2520We%2520present%2520Cyst-X%252C%2520an%250AAI%2520framework%2520that%2520predicts%2520IPMN%2520malignancy%2520using%2520multicenter%2520MRI%2520data%252C%250Aleveraging%2520MRI%2527s%2520superior%2520soft%2520tissue%2520contrast%2520over%2520CT.%2520Trained%2520on%2520723%2520T1-%2520and%250A738%2520T2-weighted%2520scans%2520from%2520764%2520patients%2520across%2520seven%2520institutions%252C%2520our%2520models%250A%2528AUC%253D0.82%2529%2520significantly%2520outperform%2520both%2520Kyoto%2520guidelines%2520%2528AUC%253D0.75%2529%2520and%2520expert%250Aradiologists.%2520The%2520AI-derived%2520imaging%2520features%2520align%2520with%2520known%2520clinical%2520markers%250Aand%2520offer%2520biologically%2520meaningful%2520insights.%2520We%2520also%2520demonstrate%2520strong%250Aperformance%2520in%2520a%2520federated%2520learning%2520setting%252C%2520enabling%2520collaborative%2520training%250Awithout%2520sharing%2520patient%2520data.%2520To%2520promote%2520privacy-preserving%2520AI%2520development%2520and%250Aimprove%2520IPMN%2520risk%2520stratification%252C%2520the%2520Cyst-X%2520dataset%2520is%2520released%2520as%2520the%2520first%250Alarge-scale%252C%2520multi-center%2520pancreatic%2520cysts%2520MRI%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyst-X%3A%20AI-Powered%20Pancreatic%20Cancer%20Risk%20Prediction%20from%20Multicenter%0A%20%20MRI%20in%20Centralized%20and%20Federated%20Learning&entry.906535625=Hongyi%20Pan%20and%20Gorkem%20Durak%20and%20Elif%20Keles%20and%20Deniz%20Seyithanoglu%20and%20Zheyuan%20Zhang%20and%20Alpay%20Medetalibeyoglu%20and%20Halil%20Ertugrul%20Aktas%20and%20Andrea%20Mia%20Bejar%20and%20Ziliang%20Hong%20and%20Yavuz%20Taktak%20and%20Gulbiz%20Dagoglu%20Kartal%20and%20Mehmet%20Sukru%20Erturk%20and%20Timurhan%20Cebeci%20and%20Maria%20Jaramillo%20Gonzalez%20and%20Yury%20Velichko%20and%20Lili%20Zhao%20and%20Emil%20Agarunov%20and%20Federica%20Proietto%20Salanitri%20and%20Concetto%20Spampinato%20and%20Pallavi%20Tiwari%20and%20Ziyue%20Xu%20and%20Sachin%20Jambawalikar%20and%20Ivo%20G.%20Schoots%20and%20Marco%20J.%20Bruno%20and%20Chenchang%20Huang%20and%20Candice%20Bolan%20and%20Tamas%20Gonda%20and%20Frank%20H.%20Miller%20and%20Rajesh%20N.%20Keswani%20and%20Michael%20B.%20Wallace%20and%20Ulas%20Bagci&entry.1292438233=%20%20Pancreatic%20cancer%20is%20projected%20to%20become%20the%20second-deadliest%20malignancy%20in%0AWestern%20countries%20by%202030%2C%20highlighting%20the%20urgent%20need%20for%20better%20early%0Adetection.%20Intraductal%20papillary%20mucinous%20neoplasms%20%28IPMNs%29%2C%20key%20precursors%20to%0Apancreatic%20cancer%2C%20are%20challenging%20to%20assess%20with%20current%20guidelines%2C%20often%0Aleading%20to%20unnecessary%20surgeries%20or%20missed%20malignancies.%20We%20present%20Cyst-X%2C%20an%0AAI%20framework%20that%20predicts%20IPMN%20malignancy%20using%20multicenter%20MRI%20data%2C%0Aleveraging%20MRI%27s%20superior%20soft%20tissue%20contrast%20over%20CT.%20Trained%20on%20723%20T1-%20and%0A738%20T2-weighted%20scans%20from%20764%20patients%20across%20seven%20institutions%2C%20our%20models%0A%28AUC%3D0.82%29%20significantly%20outperform%20both%20Kyoto%20guidelines%20%28AUC%3D0.75%29%20and%20expert%0Aradiologists.%20The%20AI-derived%20imaging%20features%20align%20with%20known%20clinical%20markers%0Aand%20offer%20biologically%20meaningful%20insights.%20We%20also%20demonstrate%20strong%0Aperformance%20in%20a%20federated%20learning%20setting%2C%20enabling%20collaborative%20training%0Awithout%20sharing%20patient%20data.%20To%20promote%20privacy-preserving%20AI%20development%20and%0Aimprove%20IPMN%20risk%20stratification%2C%20the%20Cyst-X%20dataset%20is%20released%20as%20the%20first%0Alarge-scale%2C%20multi-center%20pancreatic%20cysts%20MRI%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22017v1&entry.124074799=Read"},
{"title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image\n  Generative Models Great Again", "author": "Zigang Geng and Yibing Wang and Yeyao Ma and Chen Li and Yongming Rao and Shuyang Gu and Zhao Zhong and Qinglin Lu and Han Hu and Xiaosong Zhang and  Linus and Di Wang and Jie Jiang", "abstract": "  Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.\n", "link": "http://arxiv.org/abs/2507.22058v1", "date": "2025-07-29", "relevancy": 1.8821, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6313}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6306}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Omni%3A%20Reinforcement%20Learning%20Makes%20Discrete%20Autoregressive%20Image%0A%20%20Generative%20Models%20Great%20Again&body=Title%3A%20X-Omni%3A%20Reinforcement%20Learning%20Makes%20Discrete%20Autoregressive%20Image%0A%20%20Generative%20Models%20Great%20Again%0AAuthor%3A%20Zigang%20Geng%20and%20Yibing%20Wang%20and%20Yeyao%20Ma%20and%20Chen%20Li%20and%20Yongming%20Rao%20and%20Shuyang%20Gu%20and%20Zhao%20Zhong%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Xiaosong%20Zhang%20and%20%20Linus%20and%20Di%20Wang%20and%20Jie%20Jiang%0AAbstract%3A%20%20%20Numerous%20efforts%20have%20been%20made%20to%20extend%20the%20%60%60next%20token%20prediction%27%27%0Aparadigm%20to%20visual%20contents%2C%20aiming%20to%20create%20a%20unified%20approach%20for%20both%20image%0Ageneration%20and%20understanding.%20Nevertheless%2C%20attempts%20to%20generate%20images%20through%0Aautoregressive%20modeling%20with%20discrete%20tokens%20have%20been%20plagued%20by%20issues%20such%0Aas%20low%20visual%20fidelity%2C%20distorted%20outputs%2C%20and%20failure%20to%20adhere%20to%20complex%0Ainstructions%20when%20rendering%20intricate%20details.%20These%20shortcomings%20are%20likely%0Aattributed%20to%20cumulative%20errors%20during%20autoregressive%20inference%20or%20information%0Aloss%20incurred%20during%20the%20discretization%20process.%20Probably%20due%20to%20this%0Achallenge%2C%20recent%20research%20has%20increasingly%20shifted%20toward%20jointly%20training%0Aimage%20generation%20with%20diffusion%20objectives%20and%20language%20generation%20with%0Aautoregressive%20objectives%2C%20moving%20away%20from%20unified%20modeling%20approaches.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20reinforcement%20learning%20can%20effectively%20mitigate%0Aartifacts%20and%20largely%20enhance%20the%20generation%20quality%20of%20a%20discrete%0Aautoregressive%20modeling%20method%2C%20thereby%20enabling%20seamless%20integration%20of%20image%0Aand%20language%20generation.%20Our%20framework%20comprises%20a%20semantic%20image%20tokenizer%2C%20a%0Aunified%20autoregressive%20model%20for%20both%20language%20and%20images%2C%20and%20an%20offline%0Adiffusion%20decoder%20for%20image%20generation%2C%20termed%20X-Omni.%20X-Omni%20achieves%0Astate-of-the-art%20performance%20in%20image%20generation%20tasks%20using%20a%207B%20language%0Amodel%2C%20producing%20images%20with%20high%20aesthetic%20quality%20while%20exhibiting%20strong%0Acapabilities%20in%20following%20instructions%20and%20rendering%20long%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Omni%253A%2520Reinforcement%2520Learning%2520Makes%2520Discrete%2520Autoregressive%2520Image%250A%2520%2520Generative%2520Models%2520Great%2520Again%26entry.906535625%3DZigang%2520Geng%2520and%2520Yibing%2520Wang%2520and%2520Yeyao%2520Ma%2520and%2520Chen%2520Li%2520and%2520Yongming%2520Rao%2520and%2520Shuyang%2520Gu%2520and%2520Zhao%2520Zhong%2520and%2520Qinglin%2520Lu%2520and%2520Han%2520Hu%2520and%2520Xiaosong%2520Zhang%2520and%2520%2520Linus%2520and%2520Di%2520Wang%2520and%2520Jie%2520Jiang%26entry.1292438233%3D%2520%2520Numerous%2520efforts%2520have%2520been%2520made%2520to%2520extend%2520the%2520%2560%2560next%2520token%2520prediction%2527%2527%250Aparadigm%2520to%2520visual%2520contents%252C%2520aiming%2520to%2520create%2520a%2520unified%2520approach%2520for%2520both%2520image%250Ageneration%2520and%2520understanding.%2520Nevertheless%252C%2520attempts%2520to%2520generate%2520images%2520through%250Aautoregressive%2520modeling%2520with%2520discrete%2520tokens%2520have%2520been%2520plagued%2520by%2520issues%2520such%250Aas%2520low%2520visual%2520fidelity%252C%2520distorted%2520outputs%252C%2520and%2520failure%2520to%2520adhere%2520to%2520complex%250Ainstructions%2520when%2520rendering%2520intricate%2520details.%2520These%2520shortcomings%2520are%2520likely%250Aattributed%2520to%2520cumulative%2520errors%2520during%2520autoregressive%2520inference%2520or%2520information%250Aloss%2520incurred%2520during%2520the%2520discretization%2520process.%2520Probably%2520due%2520to%2520this%250Achallenge%252C%2520recent%2520research%2520has%2520increasingly%2520shifted%2520toward%2520jointly%2520training%250Aimage%2520generation%2520with%2520diffusion%2520objectives%2520and%2520language%2520generation%2520with%250Aautoregressive%2520objectives%252C%2520moving%2520away%2520from%2520unified%2520modeling%2520approaches.%2520In%250Athis%2520work%252C%2520we%2520demonstrate%2520that%2520reinforcement%2520learning%2520can%2520effectively%2520mitigate%250Aartifacts%2520and%2520largely%2520enhance%2520the%2520generation%2520quality%2520of%2520a%2520discrete%250Aautoregressive%2520modeling%2520method%252C%2520thereby%2520enabling%2520seamless%2520integration%2520of%2520image%250Aand%2520language%2520generation.%2520Our%2520framework%2520comprises%2520a%2520semantic%2520image%2520tokenizer%252C%2520a%250Aunified%2520autoregressive%2520model%2520for%2520both%2520language%2520and%2520images%252C%2520and%2520an%2520offline%250Adiffusion%2520decoder%2520for%2520image%2520generation%252C%2520termed%2520X-Omni.%2520X-Omni%2520achieves%250Astate-of-the-art%2520performance%2520in%2520image%2520generation%2520tasks%2520using%2520a%25207B%2520language%250Amodel%252C%2520producing%2520images%2520with%2520high%2520aesthetic%2520quality%2520while%2520exhibiting%2520strong%250Acapabilities%2520in%2520following%2520instructions%2520and%2520rendering%2520long%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Omni%3A%20Reinforcement%20Learning%20Makes%20Discrete%20Autoregressive%20Image%0A%20%20Generative%20Models%20Great%20Again&entry.906535625=Zigang%20Geng%20and%20Yibing%20Wang%20and%20Yeyao%20Ma%20and%20Chen%20Li%20and%20Yongming%20Rao%20and%20Shuyang%20Gu%20and%20Zhao%20Zhong%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Xiaosong%20Zhang%20and%20%20Linus%20and%20Di%20Wang%20and%20Jie%20Jiang&entry.1292438233=%20%20Numerous%20efforts%20have%20been%20made%20to%20extend%20the%20%60%60next%20token%20prediction%27%27%0Aparadigm%20to%20visual%20contents%2C%20aiming%20to%20create%20a%20unified%20approach%20for%20both%20image%0Ageneration%20and%20understanding.%20Nevertheless%2C%20attempts%20to%20generate%20images%20through%0Aautoregressive%20modeling%20with%20discrete%20tokens%20have%20been%20plagued%20by%20issues%20such%0Aas%20low%20visual%20fidelity%2C%20distorted%20outputs%2C%20and%20failure%20to%20adhere%20to%20complex%0Ainstructions%20when%20rendering%20intricate%20details.%20These%20shortcomings%20are%20likely%0Aattributed%20to%20cumulative%20errors%20during%20autoregressive%20inference%20or%20information%0Aloss%20incurred%20during%20the%20discretization%20process.%20Probably%20due%20to%20this%0Achallenge%2C%20recent%20research%20has%20increasingly%20shifted%20toward%20jointly%20training%0Aimage%20generation%20with%20diffusion%20objectives%20and%20language%20generation%20with%0Aautoregressive%20objectives%2C%20moving%20away%20from%20unified%20modeling%20approaches.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20reinforcement%20learning%20can%20effectively%20mitigate%0Aartifacts%20and%20largely%20enhance%20the%20generation%20quality%20of%20a%20discrete%0Aautoregressive%20modeling%20method%2C%20thereby%20enabling%20seamless%20integration%20of%20image%0Aand%20language%20generation.%20Our%20framework%20comprises%20a%20semantic%20image%20tokenizer%2C%20a%0Aunified%20autoregressive%20model%20for%20both%20language%20and%20images%2C%20and%20an%20offline%0Adiffusion%20decoder%20for%20image%20generation%2C%20termed%20X-Omni.%20X-Omni%20achieves%0Astate-of-the-art%20performance%20in%20image%20generation%20tasks%20using%20a%207B%20language%0Amodel%2C%20producing%20images%20with%20high%20aesthetic%20quality%20while%20exhibiting%20strong%0Acapabilities%20in%20following%20instructions%20and%20rendering%20long%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22058v1&entry.124074799=Read"},
{"title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge", "author": "Zihan Zhao and Bo Chen and Ziping Wan and Lu Chen and Xuanze Lin and Shiyang Yu and Situo Zhang and Da Ma and Zichen Zhu and Danyang Zhang and Huayang Wang and Zhongyang Dai and Liyang Wen and Xin Chen and Kai Yu", "abstract": "  While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves state-of-the-art performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.\n", "link": "http://arxiv.org/abs/2507.21990v1", "date": "2025-07-29", "relevancy": 1.8256, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChemDFM-R%3A%20An%20Chemical%20Reasoner%20LLM%20Enhanced%20with%20Atomized%20Chemical%0A%20%20Knowledge&body=Title%3A%20ChemDFM-R%3A%20An%20Chemical%20Reasoner%20LLM%20Enhanced%20with%20Atomized%20Chemical%0A%20%20Knowledge%0AAuthor%3A%20Zihan%20Zhao%20and%20Bo%20Chen%20and%20Ziping%20Wan%20and%20Lu%20Chen%20and%20Xuanze%20Lin%20and%20Shiyang%20Yu%20and%20Situo%20Zhang%20and%20Da%20Ma%20and%20Zichen%20Zhu%20and%20Danyang%20Zhang%20and%20Huayang%20Wang%20and%20Zhongyang%20Dai%20and%20Liyang%20Wen%20and%20Xin%20Chen%20and%20Kai%20Yu%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20progress%2C%20their%0Aapplication%20in%20scientific%20domains%20such%20as%20chemistry%20remains%20hindered%20by%20shallow%0Adomain%20understanding%20and%20limited%20reasoning%20capabilities.%20In%20this%20work%2C%20we%20focus%0Aon%20the%20specific%20field%20of%20chemistry%20and%20develop%20a%20Chemical%20Reasoner%20LLM%2C%0AChemDFM-R.%20We%20first%20construct%20a%20comprehensive%20dataset%20of%20atomized%20knowledge%0Apoints%20to%20enhance%20the%20model%27s%20understanding%20of%20the%20fundamental%20principles%20and%0Alogical%20structure%20of%20chemistry.%20Then%2C%20we%20propose%20a%20mix-sourced%20distillation%0Astrategy%20that%20integrates%20expert-curated%20knowledge%20with%20general-domain%20reasoning%0Askills%2C%20followed%20by%20domain-specific%20reinforcement%20learning%20to%20enhance%20chemical%0Areasoning.%20Experiments%20on%20diverse%20chemical%20benchmarks%20demonstrate%20that%0AChemDFM-R%20achieves%20state-of-the-art%20performance%20while%20providing%20interpretable%2C%0Arationale-driven%20outputs.%20Further%20case%20studies%20illustrate%20how%20explicit%0Areasoning%20chains%20significantly%20improve%20the%20reliability%2C%20transparency%2C%20and%0Apractical%20utility%20of%20the%20model%20in%20real-world%20human-AI%20collaboration%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemDFM-R%253A%2520An%2520Chemical%2520Reasoner%2520LLM%2520Enhanced%2520with%2520Atomized%2520Chemical%250A%2520%2520Knowledge%26entry.906535625%3DZihan%2520Zhao%2520and%2520Bo%2520Chen%2520and%2520Ziping%2520Wan%2520and%2520Lu%2520Chen%2520and%2520Xuanze%2520Lin%2520and%2520Shiyang%2520Yu%2520and%2520Situo%2520Zhang%2520and%2520Da%2520Ma%2520and%2520Zichen%2520Zhu%2520and%2520Danyang%2520Zhang%2520and%2520Huayang%2520Wang%2520and%2520Zhongyang%2520Dai%2520and%2520Liyang%2520Wen%2520and%2520Xin%2520Chen%2520and%2520Kai%2520Yu%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520progress%252C%2520their%250Aapplication%2520in%2520scientific%2520domains%2520such%2520as%2520chemistry%2520remains%2520hindered%2520by%2520shallow%250Adomain%2520understanding%2520and%2520limited%2520reasoning%2520capabilities.%2520In%2520this%2520work%252C%2520we%2520focus%250Aon%2520the%2520specific%2520field%2520of%2520chemistry%2520and%2520develop%2520a%2520Chemical%2520Reasoner%2520LLM%252C%250AChemDFM-R.%2520We%2520first%2520construct%2520a%2520comprehensive%2520dataset%2520of%2520atomized%2520knowledge%250Apoints%2520to%2520enhance%2520the%2520model%2527s%2520understanding%2520of%2520the%2520fundamental%2520principles%2520and%250Alogical%2520structure%2520of%2520chemistry.%2520Then%252C%2520we%2520propose%2520a%2520mix-sourced%2520distillation%250Astrategy%2520that%2520integrates%2520expert-curated%2520knowledge%2520with%2520general-domain%2520reasoning%250Askills%252C%2520followed%2520by%2520domain-specific%2520reinforcement%2520learning%2520to%2520enhance%2520chemical%250Areasoning.%2520Experiments%2520on%2520diverse%2520chemical%2520benchmarks%2520demonstrate%2520that%250AChemDFM-R%2520achieves%2520state-of-the-art%2520performance%2520while%2520providing%2520interpretable%252C%250Arationale-driven%2520outputs.%2520Further%2520case%2520studies%2520illustrate%2520how%2520explicit%250Areasoning%2520chains%2520significantly%2520improve%2520the%2520reliability%252C%2520transparency%252C%2520and%250Apractical%2520utility%2520of%2520the%2520model%2520in%2520real-world%2520human-AI%2520collaboration%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChemDFM-R%3A%20An%20Chemical%20Reasoner%20LLM%20Enhanced%20with%20Atomized%20Chemical%0A%20%20Knowledge&entry.906535625=Zihan%20Zhao%20and%20Bo%20Chen%20and%20Ziping%20Wan%20and%20Lu%20Chen%20and%20Xuanze%20Lin%20and%20Shiyang%20Yu%20and%20Situo%20Zhang%20and%20Da%20Ma%20and%20Zichen%20Zhu%20and%20Danyang%20Zhang%20and%20Huayang%20Wang%20and%20Zhongyang%20Dai%20and%20Liyang%20Wen%20and%20Xin%20Chen%20and%20Kai%20Yu&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20progress%2C%20their%0Aapplication%20in%20scientific%20domains%20such%20as%20chemistry%20remains%20hindered%20by%20shallow%0Adomain%20understanding%20and%20limited%20reasoning%20capabilities.%20In%20this%20work%2C%20we%20focus%0Aon%20the%20specific%20field%20of%20chemistry%20and%20develop%20a%20Chemical%20Reasoner%20LLM%2C%0AChemDFM-R.%20We%20first%20construct%20a%20comprehensive%20dataset%20of%20atomized%20knowledge%0Apoints%20to%20enhance%20the%20model%27s%20understanding%20of%20the%20fundamental%20principles%20and%0Alogical%20structure%20of%20chemistry.%20Then%2C%20we%20propose%20a%20mix-sourced%20distillation%0Astrategy%20that%20integrates%20expert-curated%20knowledge%20with%20general-domain%20reasoning%0Askills%2C%20followed%20by%20domain-specific%20reinforcement%20learning%20to%20enhance%20chemical%0Areasoning.%20Experiments%20on%20diverse%20chemical%20benchmarks%20demonstrate%20that%0AChemDFM-R%20achieves%20state-of-the-art%20performance%20while%20providing%20interpretable%2C%0Arationale-driven%20outputs.%20Further%20case%20studies%20illustrate%20how%20explicit%0Areasoning%20chains%20significantly%20improve%20the%20reliability%2C%20transparency%2C%20and%0Apractical%20utility%20of%20the%20model%20in%20real-world%20human-AI%20collaboration%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21990v1&entry.124074799=Read"},
{"title": "IRASim: A Fine-Grained World Model for Robot Manipulation", "author": "Fangqi Zhu and Hongtao Wu and Song Guo and Yuxiao Liu and Chilam Cheang and Tao Kong", "abstract": "  World models allow autonomous agents to plan and explore by predicting the\nvisual outcomes of different actions. However, for robot manipulation, it is\nchallenging to accurately model the fine-grained robot-object interaction\nwithin the visual space using existing methods which overlooks precise\nalignment between each action and the corresponding frame. In this paper, we\npresent IRASim, a novel world model capable of generating videos with\nfine-grained robot-object interaction details, conditioned on historical\nobservations and robot action trajectories. We train a diffusion transformer\nand introduce a novel frame-level action-conditioning module within each\ntransformer block to explicitly model and strengthen the action-frame\nalignment. Extensive experiments show that: (1) the quality of the videos\ngenerated by our method surpasses all the baseline methods and scales\neffectively with increased model size and computation; (2) policy evaluations\nusing IRASim exhibit a strong correlation with those using the ground-truth\nsimulator, highlighting its potential to accelerate real-world policy\nevaluation; (3) testing-time scaling through model-based planning with IRASim\nsignificantly enhances policy performance, as evidenced by an improvement in\nthe IoU metric on the Push-T benchmark from 0.637 to 0.961; (4) IRASim provides\nflexible action controllability, allowing virtual robotic arms in datasets to\nbe controlled via a keyboard or VR controller.\n", "link": "http://arxiv.org/abs/2406.14540v2", "date": "2025-07-29", "relevancy": 1.7833, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6319}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5887}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRASim%3A%20A%20Fine-Grained%20World%20Model%20for%20Robot%20Manipulation&body=Title%3A%20IRASim%3A%20A%20Fine-Grained%20World%20Model%20for%20Robot%20Manipulation%0AAuthor%3A%20Fangqi%20Zhu%20and%20Hongtao%20Wu%20and%20Song%20Guo%20and%20Yuxiao%20Liu%20and%20Chilam%20Cheang%20and%20Tao%20Kong%0AAbstract%3A%20%20%20World%20models%20allow%20autonomous%20agents%20to%20plan%20and%20explore%20by%20predicting%20the%0Avisual%20outcomes%20of%20different%20actions.%20However%2C%20for%20robot%20manipulation%2C%20it%20is%0Achallenging%20to%20accurately%20model%20the%20fine-grained%20robot-object%20interaction%0Awithin%20the%20visual%20space%20using%20existing%20methods%20which%20overlooks%20precise%0Aalignment%20between%20each%20action%20and%20the%20corresponding%20frame.%20In%20this%20paper%2C%20we%0Apresent%20IRASim%2C%20a%20novel%20world%20model%20capable%20of%20generating%20videos%20with%0Afine-grained%20robot-object%20interaction%20details%2C%20conditioned%20on%20historical%0Aobservations%20and%20robot%20action%20trajectories.%20We%20train%20a%20diffusion%20transformer%0Aand%20introduce%20a%20novel%20frame-level%20action-conditioning%20module%20within%20each%0Atransformer%20block%20to%20explicitly%20model%20and%20strengthen%20the%20action-frame%0Aalignment.%20Extensive%20experiments%20show%20that%3A%20%281%29%20the%20quality%20of%20the%20videos%0Agenerated%20by%20our%20method%20surpasses%20all%20the%20baseline%20methods%20and%20scales%0Aeffectively%20with%20increased%20model%20size%20and%20computation%3B%20%282%29%20policy%20evaluations%0Ausing%20IRASim%20exhibit%20a%20strong%20correlation%20with%20those%20using%20the%20ground-truth%0Asimulator%2C%20highlighting%20its%20potential%20to%20accelerate%20real-world%20policy%0Aevaluation%3B%20%283%29%20testing-time%20scaling%20through%20model-based%20planning%20with%20IRASim%0Asignificantly%20enhances%20policy%20performance%2C%20as%20evidenced%20by%20an%20improvement%20in%0Athe%20IoU%20metric%20on%20the%20Push-T%20benchmark%20from%200.637%20to%200.961%3B%20%284%29%20IRASim%20provides%0Aflexible%20action%20controllability%2C%20allowing%20virtual%20robotic%20arms%20in%20datasets%20to%0Abe%20controlled%20via%20a%20keyboard%20or%20VR%20controller.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRASim%253A%2520A%2520Fine-Grained%2520World%2520Model%2520for%2520Robot%2520Manipulation%26entry.906535625%3DFangqi%2520Zhu%2520and%2520Hongtao%2520Wu%2520and%2520Song%2520Guo%2520and%2520Yuxiao%2520Liu%2520and%2520Chilam%2520Cheang%2520and%2520Tao%2520Kong%26entry.1292438233%3D%2520%2520World%2520models%2520allow%2520autonomous%2520agents%2520to%2520plan%2520and%2520explore%2520by%2520predicting%2520the%250Avisual%2520outcomes%2520of%2520different%2520actions.%2520However%252C%2520for%2520robot%2520manipulation%252C%2520it%2520is%250Achallenging%2520to%2520accurately%2520model%2520the%2520fine-grained%2520robot-object%2520interaction%250Awithin%2520the%2520visual%2520space%2520using%2520existing%2520methods%2520which%2520overlooks%2520precise%250Aalignment%2520between%2520each%2520action%2520and%2520the%2520corresponding%2520frame.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520IRASim%252C%2520a%2520novel%2520world%2520model%2520capable%2520of%2520generating%2520videos%2520with%250Afine-grained%2520robot-object%2520interaction%2520details%252C%2520conditioned%2520on%2520historical%250Aobservations%2520and%2520robot%2520action%2520trajectories.%2520We%2520train%2520a%2520diffusion%2520transformer%250Aand%2520introduce%2520a%2520novel%2520frame-level%2520action-conditioning%2520module%2520within%2520each%250Atransformer%2520block%2520to%2520explicitly%2520model%2520and%2520strengthen%2520the%2520action-frame%250Aalignment.%2520Extensive%2520experiments%2520show%2520that%253A%2520%25281%2529%2520the%2520quality%2520of%2520the%2520videos%250Agenerated%2520by%2520our%2520method%2520surpasses%2520all%2520the%2520baseline%2520methods%2520and%2520scales%250Aeffectively%2520with%2520increased%2520model%2520size%2520and%2520computation%253B%2520%25282%2529%2520policy%2520evaluations%250Ausing%2520IRASim%2520exhibit%2520a%2520strong%2520correlation%2520with%2520those%2520using%2520the%2520ground-truth%250Asimulator%252C%2520highlighting%2520its%2520potential%2520to%2520accelerate%2520real-world%2520policy%250Aevaluation%253B%2520%25283%2529%2520testing-time%2520scaling%2520through%2520model-based%2520planning%2520with%2520IRASim%250Asignificantly%2520enhances%2520policy%2520performance%252C%2520as%2520evidenced%2520by%2520an%2520improvement%2520in%250Athe%2520IoU%2520metric%2520on%2520the%2520Push-T%2520benchmark%2520from%25200.637%2520to%25200.961%253B%2520%25284%2529%2520IRASim%2520provides%250Aflexible%2520action%2520controllability%252C%2520allowing%2520virtual%2520robotic%2520arms%2520in%2520datasets%2520to%250Abe%2520controlled%2520via%2520a%2520keyboard%2520or%2520VR%2520controller.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRASim%3A%20A%20Fine-Grained%20World%20Model%20for%20Robot%20Manipulation&entry.906535625=Fangqi%20Zhu%20and%20Hongtao%20Wu%20and%20Song%20Guo%20and%20Yuxiao%20Liu%20and%20Chilam%20Cheang%20and%20Tao%20Kong&entry.1292438233=%20%20World%20models%20allow%20autonomous%20agents%20to%20plan%20and%20explore%20by%20predicting%20the%0Avisual%20outcomes%20of%20different%20actions.%20However%2C%20for%20robot%20manipulation%2C%20it%20is%0Achallenging%20to%20accurately%20model%20the%20fine-grained%20robot-object%20interaction%0Awithin%20the%20visual%20space%20using%20existing%20methods%20which%20overlooks%20precise%0Aalignment%20between%20each%20action%20and%20the%20corresponding%20frame.%20In%20this%20paper%2C%20we%0Apresent%20IRASim%2C%20a%20novel%20world%20model%20capable%20of%20generating%20videos%20with%0Afine-grained%20robot-object%20interaction%20details%2C%20conditioned%20on%20historical%0Aobservations%20and%20robot%20action%20trajectories.%20We%20train%20a%20diffusion%20transformer%0Aand%20introduce%20a%20novel%20frame-level%20action-conditioning%20module%20within%20each%0Atransformer%20block%20to%20explicitly%20model%20and%20strengthen%20the%20action-frame%0Aalignment.%20Extensive%20experiments%20show%20that%3A%20%281%29%20the%20quality%20of%20the%20videos%0Agenerated%20by%20our%20method%20surpasses%20all%20the%20baseline%20methods%20and%20scales%0Aeffectively%20with%20increased%20model%20size%20and%20computation%3B%20%282%29%20policy%20evaluations%0Ausing%20IRASim%20exhibit%20a%20strong%20correlation%20with%20those%20using%20the%20ground-truth%0Asimulator%2C%20highlighting%20its%20potential%20to%20accelerate%20real-world%20policy%0Aevaluation%3B%20%283%29%20testing-time%20scaling%20through%20model-based%20planning%20with%20IRASim%0Asignificantly%20enhances%20policy%20performance%2C%20as%20evidenced%20by%20an%20improvement%20in%0Athe%20IoU%20metric%20on%20the%20Push-T%20benchmark%20from%200.637%20to%200.961%3B%20%284%29%20IRASim%20provides%0Aflexible%20action%20controllability%2C%20allowing%20virtual%20robotic%20arms%20in%20datasets%20to%0Abe%20controlled%20via%20a%20keyboard%20or%20VR%20controller.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14540v2&entry.124074799=Read"},
{"title": "Classification of Honey Botanical and Geographical Sources using Mineral\n  Profiles and Machine Learning", "author": "Mokhtar Al-Awadhi and Ratnadeep Deshmukh", "abstract": "  This paper proposes a machine learning-based approach for identifying honey\nfloral and geographical sources using mineral element profiles. The proposed\nmethod comprises two steps: preprocessing and classification. The preprocessing\nphase involves missing-value treatment and data normalization. In the\nclassification phase, we employ various supervised classification models for\ndiscriminating between six botanical sources and 13 geographical origins of\nhoney. We test the classifiers' performance on a publicly available honey\nmineral element dataset. The dataset contains mineral element profiles of\nhoneys from various floral and geographical origins. Results show that mineral\nelement content in honey provides discriminative information useful for\nclassifying honey botanical and geographical sources. Results also show that\nthe Random Forests (RF) classifier obtains the best performance on this\ndataset, achieving a cross-validation accuracy of 99.30% for classifying honey\nbotanical origins and 98.01% for classifying honey geographical origins.\n", "link": "http://arxiv.org/abs/2507.22032v1", "date": "2025-07-29", "relevancy": 1.7817, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Honey%20Botanical%20and%20Geographical%20Sources%20using%20Mineral%0A%20%20Profiles%20and%20Machine%20Learning&body=Title%3A%20Classification%20of%20Honey%20Botanical%20and%20Geographical%20Sources%20using%20Mineral%0A%20%20Profiles%20and%20Machine%20Learning%0AAuthor%3A%20Mokhtar%20Al-Awadhi%20and%20Ratnadeep%20Deshmukh%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20machine%20learning-based%20approach%20for%20identifying%20honey%0Afloral%20and%20geographical%20sources%20using%20mineral%20element%20profiles.%20The%20proposed%0Amethod%20comprises%20two%20steps%3A%20preprocessing%20and%20classification.%20The%20preprocessing%0Aphase%20involves%20missing-value%20treatment%20and%20data%20normalization.%20In%20the%0Aclassification%20phase%2C%20we%20employ%20various%20supervised%20classification%20models%20for%0Adiscriminating%20between%20six%20botanical%20sources%20and%2013%20geographical%20origins%20of%0Ahoney.%20We%20test%20the%20classifiers%27%20performance%20on%20a%20publicly%20available%20honey%0Amineral%20element%20dataset.%20The%20dataset%20contains%20mineral%20element%20profiles%20of%0Ahoneys%20from%20various%20floral%20and%20geographical%20origins.%20Results%20show%20that%20mineral%0Aelement%20content%20in%20honey%20provides%20discriminative%20information%20useful%20for%0Aclassifying%20honey%20botanical%20and%20geographical%20sources.%20Results%20also%20show%20that%0Athe%20Random%20Forests%20%28RF%29%20classifier%20obtains%20the%20best%20performance%20on%20this%0Adataset%2C%20achieving%20a%20cross-validation%20accuracy%20of%2099.30%25%20for%20classifying%20honey%0Abotanical%20origins%20and%2098.01%25%20for%20classifying%20honey%20geographical%20origins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Honey%2520Botanical%2520and%2520Geographical%2520Sources%2520using%2520Mineral%250A%2520%2520Profiles%2520and%2520Machine%2520Learning%26entry.906535625%3DMokhtar%2520Al-Awadhi%2520and%2520Ratnadeep%2520Deshmukh%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520machine%2520learning-based%2520approach%2520for%2520identifying%2520honey%250Afloral%2520and%2520geographical%2520sources%2520using%2520mineral%2520element%2520profiles.%2520The%2520proposed%250Amethod%2520comprises%2520two%2520steps%253A%2520preprocessing%2520and%2520classification.%2520The%2520preprocessing%250Aphase%2520involves%2520missing-value%2520treatment%2520and%2520data%2520normalization.%2520In%2520the%250Aclassification%2520phase%252C%2520we%2520employ%2520various%2520supervised%2520classification%2520models%2520for%250Adiscriminating%2520between%2520six%2520botanical%2520sources%2520and%252013%2520geographical%2520origins%2520of%250Ahoney.%2520We%2520test%2520the%2520classifiers%2527%2520performance%2520on%2520a%2520publicly%2520available%2520honey%250Amineral%2520element%2520dataset.%2520The%2520dataset%2520contains%2520mineral%2520element%2520profiles%2520of%250Ahoneys%2520from%2520various%2520floral%2520and%2520geographical%2520origins.%2520Results%2520show%2520that%2520mineral%250Aelement%2520content%2520in%2520honey%2520provides%2520discriminative%2520information%2520useful%2520for%250Aclassifying%2520honey%2520botanical%2520and%2520geographical%2520sources.%2520Results%2520also%2520show%2520that%250Athe%2520Random%2520Forests%2520%2528RF%2529%2520classifier%2520obtains%2520the%2520best%2520performance%2520on%2520this%250Adataset%252C%2520achieving%2520a%2520cross-validation%2520accuracy%2520of%252099.30%2525%2520for%2520classifying%2520honey%250Abotanical%2520origins%2520and%252098.01%2525%2520for%2520classifying%2520honey%2520geographical%2520origins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Honey%20Botanical%20and%20Geographical%20Sources%20using%20Mineral%0A%20%20Profiles%20and%20Machine%20Learning&entry.906535625=Mokhtar%20Al-Awadhi%20and%20Ratnadeep%20Deshmukh&entry.1292438233=%20%20This%20paper%20proposes%20a%20machine%20learning-based%20approach%20for%20identifying%20honey%0Afloral%20and%20geographical%20sources%20using%20mineral%20element%20profiles.%20The%20proposed%0Amethod%20comprises%20two%20steps%3A%20preprocessing%20and%20classification.%20The%20preprocessing%0Aphase%20involves%20missing-value%20treatment%20and%20data%20normalization.%20In%20the%0Aclassification%20phase%2C%20we%20employ%20various%20supervised%20classification%20models%20for%0Adiscriminating%20between%20six%20botanical%20sources%20and%2013%20geographical%20origins%20of%0Ahoney.%20We%20test%20the%20classifiers%27%20performance%20on%20a%20publicly%20available%20honey%0Amineral%20element%20dataset.%20The%20dataset%20contains%20mineral%20element%20profiles%20of%0Ahoneys%20from%20various%20floral%20and%20geographical%20origins.%20Results%20show%20that%20mineral%0Aelement%20content%20in%20honey%20provides%20discriminative%20information%20useful%20for%0Aclassifying%20honey%20botanical%20and%20geographical%20sources.%20Results%20also%20show%20that%0Athe%20Random%20Forests%20%28RF%29%20classifier%20obtains%20the%20best%20performance%20on%20this%0Adataset%2C%20achieving%20a%20cross-validation%20accuracy%20of%2099.30%25%20for%20classifying%20honey%0Abotanical%20origins%20and%2098.01%25%20for%20classifying%20honey%20geographical%20origins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22032v1&entry.124074799=Read"},
{"title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "author": "Mihir Prabhudesai and Mengning Wu and Amir Zadeh and Katerina Fragkiadaki and Deepak Pathak", "abstract": "  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.\n", "link": "http://arxiv.org/abs/2507.15857v3", "date": "2025-07-29", "relevancy": 1.765, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6217}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6164}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Beats%20Autoregressive%20in%20Data-Constrained%20Settings&body=Title%3A%20Diffusion%20Beats%20Autoregressive%20in%20Data-Constrained%20Settings%0AAuthor%3A%20Mihir%20Prabhudesai%20and%20Mengning%20Wu%20and%20Amir%20Zadeh%20and%20Katerina%20Fragkiadaki%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20Autoregressive%20%28AR%29%20models%20have%20long%20dominated%20the%20landscape%20of%20large%0Alanguage%20models%2C%20driving%20progress%20across%20a%20wide%20range%20of%20tasks.%20Recently%2C%0Adiffusion-based%20language%20models%20have%20emerged%20as%20a%20promising%20alternative%2C%20though%0Atheir%20advantages%20over%20AR%20models%20remain%20underexplored.%20In%20this%20paper%2C%20we%0Asystematically%20study%20masked%20diffusion%20models%20in%20data-constrained%20settings-where%0Atraining%20involves%20repeated%20passes%20over%20limited%20data-and%20find%20that%20they%0Asignificantly%20outperform%20AR%20models%20when%20compute%20is%20abundant%20but%20data%20is%20scarce.%0ADiffusion%20models%20make%20better%20use%20of%20repeated%20data%2C%20achieving%20lower%20validation%0Aloss%20and%20superior%20downstream%20performance.%20We%20interpret%20this%20advantage%20as%0Aimplicit%20data%20augmentation%3A%20masked%20diffusion%20exposes%20the%20model%20to%20a%20diverse%0Adistribution%20of%20token%20orderings%20and%20prediction%20tasks%2C%20unlike%20AR%27s%20fixed%0Aleft-to-right%20factorization.%20We%20find%20new%20scaling%20laws%20for%20diffusion%20models%20and%0Aderive%20a%20closed-form%20expression%20for%20the%20critical%20compute%20threshold%20at%20which%0Adiffusion%20begins%20to%20outperform%20AR.%20These%20results%20suggest%20that%20when%20data%2C%20not%0Acompute%2C%20is%20the%20bottleneck%2C%20diffusion%20models%20offer%20a%20compelling%20alternative%20to%0Athe%20standard%20AR%20paradigm.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//diffusion-scaling.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15857v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Beats%2520Autoregressive%2520in%2520Data-Constrained%2520Settings%26entry.906535625%3DMihir%2520Prabhudesai%2520and%2520Mengning%2520Wu%2520and%2520Amir%2520Zadeh%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520Autoregressive%2520%2528AR%2529%2520models%2520have%2520long%2520dominated%2520the%2520landscape%2520of%2520large%250Alanguage%2520models%252C%2520driving%2520progress%2520across%2520a%2520wide%2520range%2520of%2520tasks.%2520Recently%252C%250Adiffusion-based%2520language%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520alternative%252C%2520though%250Atheir%2520advantages%2520over%2520AR%2520models%2520remain%2520underexplored.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520study%2520masked%2520diffusion%2520models%2520in%2520data-constrained%2520settings-where%250Atraining%2520involves%2520repeated%2520passes%2520over%2520limited%2520data-and%2520find%2520that%2520they%250Asignificantly%2520outperform%2520AR%2520models%2520when%2520compute%2520is%2520abundant%2520but%2520data%2520is%2520scarce.%250ADiffusion%2520models%2520make%2520better%2520use%2520of%2520repeated%2520data%252C%2520achieving%2520lower%2520validation%250Aloss%2520and%2520superior%2520downstream%2520performance.%2520We%2520interpret%2520this%2520advantage%2520as%250Aimplicit%2520data%2520augmentation%253A%2520masked%2520diffusion%2520exposes%2520the%2520model%2520to%2520a%2520diverse%250Adistribution%2520of%2520token%2520orderings%2520and%2520prediction%2520tasks%252C%2520unlike%2520AR%2527s%2520fixed%250Aleft-to-right%2520factorization.%2520We%2520find%2520new%2520scaling%2520laws%2520for%2520diffusion%2520models%2520and%250Aderive%2520a%2520closed-form%2520expression%2520for%2520the%2520critical%2520compute%2520threshold%2520at%2520which%250Adiffusion%2520begins%2520to%2520outperform%2520AR.%2520These%2520results%2520suggest%2520that%2520when%2520data%252C%2520not%250Acompute%252C%2520is%2520the%2520bottleneck%252C%2520diffusion%2520models%2520offer%2520a%2520compelling%2520alternative%2520to%250Athe%2520standard%2520AR%2520paradigm.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//diffusion-scaling.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15857v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Beats%20Autoregressive%20in%20Data-Constrained%20Settings&entry.906535625=Mihir%20Prabhudesai%20and%20Mengning%20Wu%20and%20Amir%20Zadeh%20and%20Katerina%20Fragkiadaki%20and%20Deepak%20Pathak&entry.1292438233=%20%20Autoregressive%20%28AR%29%20models%20have%20long%20dominated%20the%20landscape%20of%20large%0Alanguage%20models%2C%20driving%20progress%20across%20a%20wide%20range%20of%20tasks.%20Recently%2C%0Adiffusion-based%20language%20models%20have%20emerged%20as%20a%20promising%20alternative%2C%20though%0Atheir%20advantages%20over%20AR%20models%20remain%20underexplored.%20In%20this%20paper%2C%20we%0Asystematically%20study%20masked%20diffusion%20models%20in%20data-constrained%20settings-where%0Atraining%20involves%20repeated%20passes%20over%20limited%20data-and%20find%20that%20they%0Asignificantly%20outperform%20AR%20models%20when%20compute%20is%20abundant%20but%20data%20is%20scarce.%0ADiffusion%20models%20make%20better%20use%20of%20repeated%20data%2C%20achieving%20lower%20validation%0Aloss%20and%20superior%20downstream%20performance.%20We%20interpret%20this%20advantage%20as%0Aimplicit%20data%20augmentation%3A%20masked%20diffusion%20exposes%20the%20model%20to%20a%20diverse%0Adistribution%20of%20token%20orderings%20and%20prediction%20tasks%2C%20unlike%20AR%27s%20fixed%0Aleft-to-right%20factorization.%20We%20find%20new%20scaling%20laws%20for%20diffusion%20models%20and%0Aderive%20a%20closed-form%20expression%20for%20the%20critical%20compute%20threshold%20at%20which%0Adiffusion%20begins%20to%20outperform%20AR.%20These%20results%20suggest%20that%20when%20data%2C%20not%0Acompute%2C%20is%20the%20bottleneck%2C%20diffusion%20models%20offer%20a%20compelling%20alternative%20to%0Athe%20standard%20AR%20paradigm.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//diffusion-scaling.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15857v3&entry.124074799=Read"},
{"title": "Motion Matters: Motion-guided Modulation Network for Skeleton-based\n  Micro-Action Recognition", "author": "Jihao Gu and Kun Li and Fei Wang and Yanyan Wei and Zhiliang Wu and Hehe Fan and Meng Wang", "abstract": "  Micro-Actions (MAs) are an important form of non-verbal communication in\nsocial interactions, with potential applications in human emotional analysis.\nHowever, existing methods in Micro-Action Recognition often overlook the\ninherent subtle changes in MAs, which limits the accuracy of distinguishing MAs\nwith subtle changes. To address this issue, we present a novel Motion-guided\nModulation Network (MMN) that implicitly captures and modulates subtle motion\ncues to enhance spatial-temporal representation learning. Specifically, we\nintroduce a Motion-guided Skeletal Modulation module (MSM) to inject motion\ncues at the skeletal level, acting as a control signal to guide spatial\nrepresentation modeling. In parallel, we design a Motion-guided Temporal\nModulation module (MTM) to incorporate motion information at the frame level,\nfacilitating the modeling of holistic motion patterns in micro-actions.\nFinally, we propose a motion consistency learning strategy to aggregate the\nmotion cues from multi-scale features for micro-action classification.\nExperimental results on the Micro-Action 52 and iMiGUE datasets demonstrate\nthat MMN achieves state-of-the-art performance in skeleton-based micro-action\nrecognition, underscoring the importance of explicitly modeling subtle motion\ncues. The code will be available at https://github.com/momiji-bit/MMN.\n", "link": "http://arxiv.org/abs/2507.21977v1", "date": "2025-07-29", "relevancy": 1.7133, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5594}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Matters%3A%20Motion-guided%20Modulation%20Network%20for%20Skeleton-based%0A%20%20Micro-Action%20Recognition&body=Title%3A%20Motion%20Matters%3A%20Motion-guided%20Modulation%20Network%20for%20Skeleton-based%0A%20%20Micro-Action%20Recognition%0AAuthor%3A%20Jihao%20Gu%20and%20Kun%20Li%20and%20Fei%20Wang%20and%20Yanyan%20Wei%20and%20Zhiliang%20Wu%20and%20Hehe%20Fan%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Micro-Actions%20%28MAs%29%20are%20an%20important%20form%20of%20non-verbal%20communication%20in%0Asocial%20interactions%2C%20with%20potential%20applications%20in%20human%20emotional%20analysis.%0AHowever%2C%20existing%20methods%20in%20Micro-Action%20Recognition%20often%20overlook%20the%0Ainherent%20subtle%20changes%20in%20MAs%2C%20which%20limits%20the%20accuracy%20of%20distinguishing%20MAs%0Awith%20subtle%20changes.%20To%20address%20this%20issue%2C%20we%20present%20a%20novel%20Motion-guided%0AModulation%20Network%20%28MMN%29%20that%20implicitly%20captures%20and%20modulates%20subtle%20motion%0Acues%20to%20enhance%20spatial-temporal%20representation%20learning.%20Specifically%2C%20we%0Aintroduce%20a%20Motion-guided%20Skeletal%20Modulation%20module%20%28MSM%29%20to%20inject%20motion%0Acues%20at%20the%20skeletal%20level%2C%20acting%20as%20a%20control%20signal%20to%20guide%20spatial%0Arepresentation%20modeling.%20In%20parallel%2C%20we%20design%20a%20Motion-guided%20Temporal%0AModulation%20module%20%28MTM%29%20to%20incorporate%20motion%20information%20at%20the%20frame%20level%2C%0Afacilitating%20the%20modeling%20of%20holistic%20motion%20patterns%20in%20micro-actions.%0AFinally%2C%20we%20propose%20a%20motion%20consistency%20learning%20strategy%20to%20aggregate%20the%0Amotion%20cues%20from%20multi-scale%20features%20for%20micro-action%20classification.%0AExperimental%20results%20on%20the%20Micro-Action%2052%20and%20iMiGUE%20datasets%20demonstrate%0Athat%20MMN%20achieves%20state-of-the-art%20performance%20in%20skeleton-based%20micro-action%0Arecognition%2C%20underscoring%20the%20importance%20of%20explicitly%20modeling%20subtle%20motion%0Acues.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/momiji-bit/MMN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Matters%253A%2520Motion-guided%2520Modulation%2520Network%2520for%2520Skeleton-based%250A%2520%2520Micro-Action%2520Recognition%26entry.906535625%3DJihao%2520Gu%2520and%2520Kun%2520Li%2520and%2520Fei%2520Wang%2520and%2520Yanyan%2520Wei%2520and%2520Zhiliang%2520Wu%2520and%2520Hehe%2520Fan%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Micro-Actions%2520%2528MAs%2529%2520are%2520an%2520important%2520form%2520of%2520non-verbal%2520communication%2520in%250Asocial%2520interactions%252C%2520with%2520potential%2520applications%2520in%2520human%2520emotional%2520analysis.%250AHowever%252C%2520existing%2520methods%2520in%2520Micro-Action%2520Recognition%2520often%2520overlook%2520the%250Ainherent%2520subtle%2520changes%2520in%2520MAs%252C%2520which%2520limits%2520the%2520accuracy%2520of%2520distinguishing%2520MAs%250Awith%2520subtle%2520changes.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520a%2520novel%2520Motion-guided%250AModulation%2520Network%2520%2528MMN%2529%2520that%2520implicitly%2520captures%2520and%2520modulates%2520subtle%2520motion%250Acues%2520to%2520enhance%2520spatial-temporal%2520representation%2520learning.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520Motion-guided%2520Skeletal%2520Modulation%2520module%2520%2528MSM%2529%2520to%2520inject%2520motion%250Acues%2520at%2520the%2520skeletal%2520level%252C%2520acting%2520as%2520a%2520control%2520signal%2520to%2520guide%2520spatial%250Arepresentation%2520modeling.%2520In%2520parallel%252C%2520we%2520design%2520a%2520Motion-guided%2520Temporal%250AModulation%2520module%2520%2528MTM%2529%2520to%2520incorporate%2520motion%2520information%2520at%2520the%2520frame%2520level%252C%250Afacilitating%2520the%2520modeling%2520of%2520holistic%2520motion%2520patterns%2520in%2520micro-actions.%250AFinally%252C%2520we%2520propose%2520a%2520motion%2520consistency%2520learning%2520strategy%2520to%2520aggregate%2520the%250Amotion%2520cues%2520from%2520multi-scale%2520features%2520for%2520micro-action%2520classification.%250AExperimental%2520results%2520on%2520the%2520Micro-Action%252052%2520and%2520iMiGUE%2520datasets%2520demonstrate%250Athat%2520MMN%2520achieves%2520state-of-the-art%2520performance%2520in%2520skeleton-based%2520micro-action%250Arecognition%252C%2520underscoring%2520the%2520importance%2520of%2520explicitly%2520modeling%2520subtle%2520motion%250Acues.%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/momiji-bit/MMN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Matters%3A%20Motion-guided%20Modulation%20Network%20for%20Skeleton-based%0A%20%20Micro-Action%20Recognition&entry.906535625=Jihao%20Gu%20and%20Kun%20Li%20and%20Fei%20Wang%20and%20Yanyan%20Wei%20and%20Zhiliang%20Wu%20and%20Hehe%20Fan%20and%20Meng%20Wang&entry.1292438233=%20%20Micro-Actions%20%28MAs%29%20are%20an%20important%20form%20of%20non-verbal%20communication%20in%0Asocial%20interactions%2C%20with%20potential%20applications%20in%20human%20emotional%20analysis.%0AHowever%2C%20existing%20methods%20in%20Micro-Action%20Recognition%20often%20overlook%20the%0Ainherent%20subtle%20changes%20in%20MAs%2C%20which%20limits%20the%20accuracy%20of%20distinguishing%20MAs%0Awith%20subtle%20changes.%20To%20address%20this%20issue%2C%20we%20present%20a%20novel%20Motion-guided%0AModulation%20Network%20%28MMN%29%20that%20implicitly%20captures%20and%20modulates%20subtle%20motion%0Acues%20to%20enhance%20spatial-temporal%20representation%20learning.%20Specifically%2C%20we%0Aintroduce%20a%20Motion-guided%20Skeletal%20Modulation%20module%20%28MSM%29%20to%20inject%20motion%0Acues%20at%20the%20skeletal%20level%2C%20acting%20as%20a%20control%20signal%20to%20guide%20spatial%0Arepresentation%20modeling.%20In%20parallel%2C%20we%20design%20a%20Motion-guided%20Temporal%0AModulation%20module%20%28MTM%29%20to%20incorporate%20motion%20information%20at%20the%20frame%20level%2C%0Afacilitating%20the%20modeling%20of%20holistic%20motion%20patterns%20in%20micro-actions.%0AFinally%2C%20we%20propose%20a%20motion%20consistency%20learning%20strategy%20to%20aggregate%20the%0Amotion%20cues%20from%20multi-scale%20features%20for%20micro-action%20classification.%0AExperimental%20results%20on%20the%20Micro-Action%2052%20and%20iMiGUE%20datasets%20demonstrate%0Athat%20MMN%20achieves%20state-of-the-art%20performance%20in%20skeleton-based%20micro-action%0Arecognition%2C%20underscoring%20the%20importance%20of%20explicitly%20modeling%20subtle%20motion%0Acues.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/momiji-bit/MMN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21977v1&entry.124074799=Read"},
{"title": "DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity\n  Environments", "author": "Yufei Jia and Guangyu Wang and Yuhang Dong and Junzhe Wu and Yupei Zeng and Haonan Lin and Zifan Wang and Haizhou Ge and Weibin Gu and Kairui Ding and Zike Yan and Yunjie Cheng and Yue Li and Ziming Wang and Chuxuan Li and Wei Sui and Lu Shi and Guanzhong Tian and Ruqi Huang and Guyue Zhou", "abstract": "  We present the first unified, modular, open-source 3DGS-based simulation\nframework for Real2Sim2Real robot learning. It features a holistic Real2Sim\npipeline that synthesizes hyper-realistic geometry and appearance of complex\nreal-world scenarios, paving the way for analyzing and bridging the Sim2Real\ngap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively\nparallel simulation of multiple sensor modalities and accurate physics, with\ninclusive supports for existing 3D assets, robot models, and ROS plugins,\nempowering large-scale robot learning and complex robotic benchmarks. Through\nextensive experiments on imitation learning, Discoverse demonstrates\nstate-of-the-art zero-shot Sim2Real transfer performance compared to existing\nsimulators. For code and demos: https://air-discoverse.github.io/.\n", "link": "http://arxiv.org/abs/2507.21981v1", "date": "2025-07-29", "relevancy": 1.7053, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.617}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5548}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISCOVERSE%3A%20Efficient%20Robot%20Simulation%20in%20Complex%20High-Fidelity%0A%20%20Environments&body=Title%3A%20DISCOVERSE%3A%20Efficient%20Robot%20Simulation%20in%20Complex%20High-Fidelity%0A%20%20Environments%0AAuthor%3A%20Yufei%20Jia%20and%20Guangyu%20Wang%20and%20Yuhang%20Dong%20and%20Junzhe%20Wu%20and%20Yupei%20Zeng%20and%20Haonan%20Lin%20and%20Zifan%20Wang%20and%20Haizhou%20Ge%20and%20Weibin%20Gu%20and%20Kairui%20Ding%20and%20Zike%20Yan%20and%20Yunjie%20Cheng%20and%20Yue%20Li%20and%20Ziming%20Wang%20and%20Chuxuan%20Li%20and%20Wei%20Sui%20and%20Lu%20Shi%20and%20Guanzhong%20Tian%20and%20Ruqi%20Huang%20and%20Guyue%20Zhou%0AAbstract%3A%20%20%20We%20present%20the%20first%20unified%2C%20modular%2C%20open-source%203DGS-based%20simulation%0Aframework%20for%20Real2Sim2Real%20robot%20learning.%20It%20features%20a%20holistic%20Real2Sim%0Apipeline%20that%20synthesizes%20hyper-realistic%20geometry%20and%20appearance%20of%20complex%0Areal-world%20scenarios%2C%20paving%20the%20way%20for%20analyzing%20and%20bridging%20the%20Sim2Real%0Agap.%20Powered%20by%20Gaussian%20Splatting%20and%20MuJoCo%2C%20Discoverse%20enables%20massively%0Aparallel%20simulation%20of%20multiple%20sensor%20modalities%20and%20accurate%20physics%2C%20with%0Ainclusive%20supports%20for%20existing%203D%20assets%2C%20robot%20models%2C%20and%20ROS%20plugins%2C%0Aempowering%20large-scale%20robot%20learning%20and%20complex%20robotic%20benchmarks.%20Through%0Aextensive%20experiments%20on%20imitation%20learning%2C%20Discoverse%20demonstrates%0Astate-of-the-art%20zero-shot%20Sim2Real%20transfer%20performance%20compared%20to%20existing%0Asimulators.%20For%20code%20and%20demos%3A%20https%3A//air-discoverse.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISCOVERSE%253A%2520Efficient%2520Robot%2520Simulation%2520in%2520Complex%2520High-Fidelity%250A%2520%2520Environments%26entry.906535625%3DYufei%2520Jia%2520and%2520Guangyu%2520Wang%2520and%2520Yuhang%2520Dong%2520and%2520Junzhe%2520Wu%2520and%2520Yupei%2520Zeng%2520and%2520Haonan%2520Lin%2520and%2520Zifan%2520Wang%2520and%2520Haizhou%2520Ge%2520and%2520Weibin%2520Gu%2520and%2520Kairui%2520Ding%2520and%2520Zike%2520Yan%2520and%2520Yunjie%2520Cheng%2520and%2520Yue%2520Li%2520and%2520Ziming%2520Wang%2520and%2520Chuxuan%2520Li%2520and%2520Wei%2520Sui%2520and%2520Lu%2520Shi%2520and%2520Guanzhong%2520Tian%2520and%2520Ruqi%2520Huang%2520and%2520Guyue%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520unified%252C%2520modular%252C%2520open-source%25203DGS-based%2520simulation%250Aframework%2520for%2520Real2Sim2Real%2520robot%2520learning.%2520It%2520features%2520a%2520holistic%2520Real2Sim%250Apipeline%2520that%2520synthesizes%2520hyper-realistic%2520geometry%2520and%2520appearance%2520of%2520complex%250Areal-world%2520scenarios%252C%2520paving%2520the%2520way%2520for%2520analyzing%2520and%2520bridging%2520the%2520Sim2Real%250Agap.%2520Powered%2520by%2520Gaussian%2520Splatting%2520and%2520MuJoCo%252C%2520Discoverse%2520enables%2520massively%250Aparallel%2520simulation%2520of%2520multiple%2520sensor%2520modalities%2520and%2520accurate%2520physics%252C%2520with%250Ainclusive%2520supports%2520for%2520existing%25203D%2520assets%252C%2520robot%2520models%252C%2520and%2520ROS%2520plugins%252C%250Aempowering%2520large-scale%2520robot%2520learning%2520and%2520complex%2520robotic%2520benchmarks.%2520Through%250Aextensive%2520experiments%2520on%2520imitation%2520learning%252C%2520Discoverse%2520demonstrates%250Astate-of-the-art%2520zero-shot%2520Sim2Real%2520transfer%2520performance%2520compared%2520to%2520existing%250Asimulators.%2520For%2520code%2520and%2520demos%253A%2520https%253A//air-discoverse.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISCOVERSE%3A%20Efficient%20Robot%20Simulation%20in%20Complex%20High-Fidelity%0A%20%20Environments&entry.906535625=Yufei%20Jia%20and%20Guangyu%20Wang%20and%20Yuhang%20Dong%20and%20Junzhe%20Wu%20and%20Yupei%20Zeng%20and%20Haonan%20Lin%20and%20Zifan%20Wang%20and%20Haizhou%20Ge%20and%20Weibin%20Gu%20and%20Kairui%20Ding%20and%20Zike%20Yan%20and%20Yunjie%20Cheng%20and%20Yue%20Li%20and%20Ziming%20Wang%20and%20Chuxuan%20Li%20and%20Wei%20Sui%20and%20Lu%20Shi%20and%20Guanzhong%20Tian%20and%20Ruqi%20Huang%20and%20Guyue%20Zhou&entry.1292438233=%20%20We%20present%20the%20first%20unified%2C%20modular%2C%20open-source%203DGS-based%20simulation%0Aframework%20for%20Real2Sim2Real%20robot%20learning.%20It%20features%20a%20holistic%20Real2Sim%0Apipeline%20that%20synthesizes%20hyper-realistic%20geometry%20and%20appearance%20of%20complex%0Areal-world%20scenarios%2C%20paving%20the%20way%20for%20analyzing%20and%20bridging%20the%20Sim2Real%0Agap.%20Powered%20by%20Gaussian%20Splatting%20and%20MuJoCo%2C%20Discoverse%20enables%20massively%0Aparallel%20simulation%20of%20multiple%20sensor%20modalities%20and%20accurate%20physics%2C%20with%0Ainclusive%20supports%20for%20existing%203D%20assets%2C%20robot%20models%2C%20and%20ROS%20plugins%2C%0Aempowering%20large-scale%20robot%20learning%20and%20complex%20robotic%20benchmarks.%20Through%0Aextensive%20experiments%20on%20imitation%20learning%2C%20Discoverse%20demonstrates%0Astate-of-the-art%20zero-shot%20Sim2Real%20transfer%20performance%20compared%20to%20existing%0Asimulators.%20For%20code%20and%20demos%3A%20https%3A//air-discoverse.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21981v1&entry.124074799=Read"},
{"title": "PHAX: A Structured Argumentation Framework for User-Centered Explainable\n  AI in Public Health and Biomedical Sciences", "author": "Bahar \u0130lgen and Akshat Dubey and Georges Hattab", "abstract": "  Ensuring transparency and trust in AI-driven public health and biomedical\nsciences systems requires more than accurate predictions-it demands\nexplanations that are clear, contextual, and socially accountable. While\nexplainable AI (XAI) has advanced in areas like feature attribution and model\ninterpretability, most methods still lack the structure and adaptability needed\nfor diverse health stakeholders, including clinicians, policymakers, and the\ngeneral public. We introduce PHAX-a Public Health Argumentation and\neXplainability framework-that leverages structured argumentation to generate\nhuman-centered explanations for AI outputs. PHAX is a multi-layer architecture\ncombining defeasible reasoning, adaptive natural language techniques, and user\nmodeling to produce context-aware, audience-specific justifications. More\nspecifically, we show how argumentation enhances explainability by supporting\nAI-driven decision-making, justifying recommendations, and enabling interactive\ndialogues across user types. We demonstrate the applicability of PHAX through\nuse cases such as medical term simplification, patient-clinician communication,\nand policy justification. In particular, we show how simplification decisions\ncan be modeled as argument chains and personalized based on user\nexpertise-enhancing both interpretability and trust. By aligning formal\nreasoning methods with communicative demands, PHAX contributes to a broader\nvision of transparent, human-centered AI in public health.\n", "link": "http://arxiv.org/abs/2507.22009v1", "date": "2025-07-29", "relevancy": 1.6943, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PHAX%3A%20A%20Structured%20Argumentation%20Framework%20for%20User-Centered%20Explainable%0A%20%20AI%20in%20Public%20Health%20and%20Biomedical%20Sciences&body=Title%3A%20PHAX%3A%20A%20Structured%20Argumentation%20Framework%20for%20User-Centered%20Explainable%0A%20%20AI%20in%20Public%20Health%20and%20Biomedical%20Sciences%0AAuthor%3A%20Bahar%20%C4%B0lgen%20and%20Akshat%20Dubey%20and%20Georges%20Hattab%0AAbstract%3A%20%20%20Ensuring%20transparency%20and%20trust%20in%20AI-driven%20public%20health%20and%20biomedical%0Asciences%20systems%20requires%20more%20than%20accurate%20predictions-it%20demands%0Aexplanations%20that%20are%20clear%2C%20contextual%2C%20and%20socially%20accountable.%20While%0Aexplainable%20AI%20%28XAI%29%20has%20advanced%20in%20areas%20like%20feature%20attribution%20and%20model%0Ainterpretability%2C%20most%20methods%20still%20lack%20the%20structure%20and%20adaptability%20needed%0Afor%20diverse%20health%20stakeholders%2C%20including%20clinicians%2C%20policymakers%2C%20and%20the%0Ageneral%20public.%20We%20introduce%20PHAX-a%20Public%20Health%20Argumentation%20and%0AeXplainability%20framework-that%20leverages%20structured%20argumentation%20to%20generate%0Ahuman-centered%20explanations%20for%20AI%20outputs.%20PHAX%20is%20a%20multi-layer%20architecture%0Acombining%20defeasible%20reasoning%2C%20adaptive%20natural%20language%20techniques%2C%20and%20user%0Amodeling%20to%20produce%20context-aware%2C%20audience-specific%20justifications.%20More%0Aspecifically%2C%20we%20show%20how%20argumentation%20enhances%20explainability%20by%20supporting%0AAI-driven%20decision-making%2C%20justifying%20recommendations%2C%20and%20enabling%20interactive%0Adialogues%20across%20user%20types.%20We%20demonstrate%20the%20applicability%20of%20PHAX%20through%0Ause%20cases%20such%20as%20medical%20term%20simplification%2C%20patient-clinician%20communication%2C%0Aand%20policy%20justification.%20In%20particular%2C%20we%20show%20how%20simplification%20decisions%0Acan%20be%20modeled%20as%20argument%20chains%20and%20personalized%20based%20on%20user%0Aexpertise-enhancing%20both%20interpretability%20and%20trust.%20By%20aligning%20formal%0Areasoning%20methods%20with%20communicative%20demands%2C%20PHAX%20contributes%20to%20a%20broader%0Avision%20of%20transparent%2C%20human-centered%20AI%20in%20public%20health.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPHAX%253A%2520A%2520Structured%2520Argumentation%2520Framework%2520for%2520User-Centered%2520Explainable%250A%2520%2520AI%2520in%2520Public%2520Health%2520and%2520Biomedical%2520Sciences%26entry.906535625%3DBahar%2520%25C4%25B0lgen%2520and%2520Akshat%2520Dubey%2520and%2520Georges%2520Hattab%26entry.1292438233%3D%2520%2520Ensuring%2520transparency%2520and%2520trust%2520in%2520AI-driven%2520public%2520health%2520and%2520biomedical%250Asciences%2520systems%2520requires%2520more%2520than%2520accurate%2520predictions-it%2520demands%250Aexplanations%2520that%2520are%2520clear%252C%2520contextual%252C%2520and%2520socially%2520accountable.%2520While%250Aexplainable%2520AI%2520%2528XAI%2529%2520has%2520advanced%2520in%2520areas%2520like%2520feature%2520attribution%2520and%2520model%250Ainterpretability%252C%2520most%2520methods%2520still%2520lack%2520the%2520structure%2520and%2520adaptability%2520needed%250Afor%2520diverse%2520health%2520stakeholders%252C%2520including%2520clinicians%252C%2520policymakers%252C%2520and%2520the%250Ageneral%2520public.%2520We%2520introduce%2520PHAX-a%2520Public%2520Health%2520Argumentation%2520and%250AeXplainability%2520framework-that%2520leverages%2520structured%2520argumentation%2520to%2520generate%250Ahuman-centered%2520explanations%2520for%2520AI%2520outputs.%2520PHAX%2520is%2520a%2520multi-layer%2520architecture%250Acombining%2520defeasible%2520reasoning%252C%2520adaptive%2520natural%2520language%2520techniques%252C%2520and%2520user%250Amodeling%2520to%2520produce%2520context-aware%252C%2520audience-specific%2520justifications.%2520More%250Aspecifically%252C%2520we%2520show%2520how%2520argumentation%2520enhances%2520explainability%2520by%2520supporting%250AAI-driven%2520decision-making%252C%2520justifying%2520recommendations%252C%2520and%2520enabling%2520interactive%250Adialogues%2520across%2520user%2520types.%2520We%2520demonstrate%2520the%2520applicability%2520of%2520PHAX%2520through%250Ause%2520cases%2520such%2520as%2520medical%2520term%2520simplification%252C%2520patient-clinician%2520communication%252C%250Aand%2520policy%2520justification.%2520In%2520particular%252C%2520we%2520show%2520how%2520simplification%2520decisions%250Acan%2520be%2520modeled%2520as%2520argument%2520chains%2520and%2520personalized%2520based%2520on%2520user%250Aexpertise-enhancing%2520both%2520interpretability%2520and%2520trust.%2520By%2520aligning%2520formal%250Areasoning%2520methods%2520with%2520communicative%2520demands%252C%2520PHAX%2520contributes%2520to%2520a%2520broader%250Avision%2520of%2520transparent%252C%2520human-centered%2520AI%2520in%2520public%2520health.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHAX%3A%20A%20Structured%20Argumentation%20Framework%20for%20User-Centered%20Explainable%0A%20%20AI%20in%20Public%20Health%20and%20Biomedical%20Sciences&entry.906535625=Bahar%20%C4%B0lgen%20and%20Akshat%20Dubey%20and%20Georges%20Hattab&entry.1292438233=%20%20Ensuring%20transparency%20and%20trust%20in%20AI-driven%20public%20health%20and%20biomedical%0Asciences%20systems%20requires%20more%20than%20accurate%20predictions-it%20demands%0Aexplanations%20that%20are%20clear%2C%20contextual%2C%20and%20socially%20accountable.%20While%0Aexplainable%20AI%20%28XAI%29%20has%20advanced%20in%20areas%20like%20feature%20attribution%20and%20model%0Ainterpretability%2C%20most%20methods%20still%20lack%20the%20structure%20and%20adaptability%20needed%0Afor%20diverse%20health%20stakeholders%2C%20including%20clinicians%2C%20policymakers%2C%20and%20the%0Ageneral%20public.%20We%20introduce%20PHAX-a%20Public%20Health%20Argumentation%20and%0AeXplainability%20framework-that%20leverages%20structured%20argumentation%20to%20generate%0Ahuman-centered%20explanations%20for%20AI%20outputs.%20PHAX%20is%20a%20multi-layer%20architecture%0Acombining%20defeasible%20reasoning%2C%20adaptive%20natural%20language%20techniques%2C%20and%20user%0Amodeling%20to%20produce%20context-aware%2C%20audience-specific%20justifications.%20More%0Aspecifically%2C%20we%20show%20how%20argumentation%20enhances%20explainability%20by%20supporting%0AAI-driven%20decision-making%2C%20justifying%20recommendations%2C%20and%20enabling%20interactive%0Adialogues%20across%20user%20types.%20We%20demonstrate%20the%20applicability%20of%20PHAX%20through%0Ause%20cases%20such%20as%20medical%20term%20simplification%2C%20patient-clinician%20communication%2C%0Aand%20policy%20justification.%20In%20particular%2C%20we%20show%20how%20simplification%20decisions%0Acan%20be%20modeled%20as%20argument%20chains%20and%20personalized%20based%20on%20user%0Aexpertise-enhancing%20both%20interpretability%20and%20trust.%20By%20aligning%20formal%0Areasoning%20methods%20with%20communicative%20demands%2C%20PHAX%20contributes%20to%20a%20broader%0Avision%20of%20transparent%2C%20human-centered%20AI%20in%20public%20health.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22009v1&entry.124074799=Read"},
{"title": "The Interspeech 2025 Speech Accessibility Project Challenge", "author": "Xiuwen Zheng and Bornali Phukon and Jonghwan Na and Ed Cutrell and Kyu Han and Mark Hasegawa-Johnson and Pan-Pan Jiang and Aadhrik Kuila and Colin Lea and Bob MacDonald and Gautam Mantena and Venkatesh Ravichandran and Leda Sari and Katrin Tomanek and Chang D. Yoo and Chris Zwilling", "abstract": "  While the last decade has witnessed significant advancements in Automatic\nSpeech Recognition (ASR) systems, performance of these systems for individuals\nwith speech disabilities remains inadequate, partly due to limited public\ntraining data. To bridge this gap, the 2025 Interspeech Speech Accessibility\nProject (SAP) Challenge was launched, utilizing over 400 hours of SAP data\ncollected and transcribed from more than 500 individuals with diverse speech\ndisabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,\nthe SAP Challenge evaluates submissions based on Word Error Rate and Semantic\nScore. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2\nbaseline in terms of WER, while 17 teams surpassed the baseline on SemScore.\nNotably, the top team achieved the lowest WER of 8.11\\%, and the highest\nSemScore of 88.44\\% at the same time, setting new benchmarks for future ASR\nsystems in recognizing impaired speech.\n", "link": "http://arxiv.org/abs/2507.22047v1", "date": "2025-07-29", "relevancy": 1.6679, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Interspeech%202025%20Speech%20Accessibility%20Project%20Challenge&body=Title%3A%20The%20Interspeech%202025%20Speech%20Accessibility%20Project%20Challenge%0AAuthor%3A%20Xiuwen%20Zheng%20and%20Bornali%20Phukon%20and%20Jonghwan%20Na%20and%20Ed%20Cutrell%20and%20Kyu%20Han%20and%20Mark%20Hasegawa-Johnson%20and%20Pan-Pan%20Jiang%20and%20Aadhrik%20Kuila%20and%20Colin%20Lea%20and%20Bob%20MacDonald%20and%20Gautam%20Mantena%20and%20Venkatesh%20Ravichandran%20and%20Leda%20Sari%20and%20Katrin%20Tomanek%20and%20Chang%20D.%20Yoo%20and%20Chris%20Zwilling%0AAbstract%3A%20%20%20While%20the%20last%20decade%20has%20witnessed%20significant%20advancements%20in%20Automatic%0ASpeech%20Recognition%20%28ASR%29%20systems%2C%20performance%20of%20these%20systems%20for%20individuals%0Awith%20speech%20disabilities%20remains%20inadequate%2C%20partly%20due%20to%20limited%20public%0Atraining%20data.%20To%20bridge%20this%20gap%2C%20the%202025%20Interspeech%20Speech%20Accessibility%0AProject%20%28SAP%29%20Challenge%20was%20launched%2C%20utilizing%20over%20400%20hours%20of%20SAP%20data%0Acollected%20and%20transcribed%20from%20more%20than%20500%20individuals%20with%20diverse%20speech%0Adisabilities.%20Hosted%20on%20EvalAI%20and%20leveraging%20the%20remote%20evaluation%20pipeline%2C%0Athe%20SAP%20Challenge%20evaluates%20submissions%20based%20on%20Word%20Error%20Rate%20and%20Semantic%0AScore.%20Consequently%2C%2012%20out%20of%2022%20valid%20teams%20outperformed%20the%20whisper-large-v2%0Abaseline%20in%20terms%20of%20WER%2C%20while%2017%20teams%20surpassed%20the%20baseline%20on%20SemScore.%0ANotably%2C%20the%20top%20team%20achieved%20the%20lowest%20WER%20of%208.11%5C%25%2C%20and%20the%20highest%0ASemScore%20of%2088.44%5C%25%20at%20the%20same%20time%2C%20setting%20new%20benchmarks%20for%20future%20ASR%0Asystems%20in%20recognizing%20impaired%20speech.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Interspeech%25202025%2520Speech%2520Accessibility%2520Project%2520Challenge%26entry.906535625%3DXiuwen%2520Zheng%2520and%2520Bornali%2520Phukon%2520and%2520Jonghwan%2520Na%2520and%2520Ed%2520Cutrell%2520and%2520Kyu%2520Han%2520and%2520Mark%2520Hasegawa-Johnson%2520and%2520Pan-Pan%2520Jiang%2520and%2520Aadhrik%2520Kuila%2520and%2520Colin%2520Lea%2520and%2520Bob%2520MacDonald%2520and%2520Gautam%2520Mantena%2520and%2520Venkatesh%2520Ravichandran%2520and%2520Leda%2520Sari%2520and%2520Katrin%2520Tomanek%2520and%2520Chang%2520D.%2520Yoo%2520and%2520Chris%2520Zwilling%26entry.1292438233%3D%2520%2520While%2520the%2520last%2520decade%2520has%2520witnessed%2520significant%2520advancements%2520in%2520Automatic%250ASpeech%2520Recognition%2520%2528ASR%2529%2520systems%252C%2520performance%2520of%2520these%2520systems%2520for%2520individuals%250Awith%2520speech%2520disabilities%2520remains%2520inadequate%252C%2520partly%2520due%2520to%2520limited%2520public%250Atraining%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520the%25202025%2520Interspeech%2520Speech%2520Accessibility%250AProject%2520%2528SAP%2529%2520Challenge%2520was%2520launched%252C%2520utilizing%2520over%2520400%2520hours%2520of%2520SAP%2520data%250Acollected%2520and%2520transcribed%2520from%2520more%2520than%2520500%2520individuals%2520with%2520diverse%2520speech%250Adisabilities.%2520Hosted%2520on%2520EvalAI%2520and%2520leveraging%2520the%2520remote%2520evaluation%2520pipeline%252C%250Athe%2520SAP%2520Challenge%2520evaluates%2520submissions%2520based%2520on%2520Word%2520Error%2520Rate%2520and%2520Semantic%250AScore.%2520Consequently%252C%252012%2520out%2520of%252022%2520valid%2520teams%2520outperformed%2520the%2520whisper-large-v2%250Abaseline%2520in%2520terms%2520of%2520WER%252C%2520while%252017%2520teams%2520surpassed%2520the%2520baseline%2520on%2520SemScore.%250ANotably%252C%2520the%2520top%2520team%2520achieved%2520the%2520lowest%2520WER%2520of%25208.11%255C%2525%252C%2520and%2520the%2520highest%250ASemScore%2520of%252088.44%255C%2525%2520at%2520the%2520same%2520time%252C%2520setting%2520new%2520benchmarks%2520for%2520future%2520ASR%250Asystems%2520in%2520recognizing%2520impaired%2520speech.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Interspeech%202025%20Speech%20Accessibility%20Project%20Challenge&entry.906535625=Xiuwen%20Zheng%20and%20Bornali%20Phukon%20and%20Jonghwan%20Na%20and%20Ed%20Cutrell%20and%20Kyu%20Han%20and%20Mark%20Hasegawa-Johnson%20and%20Pan-Pan%20Jiang%20and%20Aadhrik%20Kuila%20and%20Colin%20Lea%20and%20Bob%20MacDonald%20and%20Gautam%20Mantena%20and%20Venkatesh%20Ravichandran%20and%20Leda%20Sari%20and%20Katrin%20Tomanek%20and%20Chang%20D.%20Yoo%20and%20Chris%20Zwilling&entry.1292438233=%20%20While%20the%20last%20decade%20has%20witnessed%20significant%20advancements%20in%20Automatic%0ASpeech%20Recognition%20%28ASR%29%20systems%2C%20performance%20of%20these%20systems%20for%20individuals%0Awith%20speech%20disabilities%20remains%20inadequate%2C%20partly%20due%20to%20limited%20public%0Atraining%20data.%20To%20bridge%20this%20gap%2C%20the%202025%20Interspeech%20Speech%20Accessibility%0AProject%20%28SAP%29%20Challenge%20was%20launched%2C%20utilizing%20over%20400%20hours%20of%20SAP%20data%0Acollected%20and%20transcribed%20from%20more%20than%20500%20individuals%20with%20diverse%20speech%0Adisabilities.%20Hosted%20on%20EvalAI%20and%20leveraging%20the%20remote%20evaluation%20pipeline%2C%0Athe%20SAP%20Challenge%20evaluates%20submissions%20based%20on%20Word%20Error%20Rate%20and%20Semantic%0AScore.%20Consequently%2C%2012%20out%20of%2022%20valid%20teams%20outperformed%20the%20whisper-large-v2%0Abaseline%20in%20terms%20of%20WER%2C%20while%2017%20teams%20surpassed%20the%20baseline%20on%20SemScore.%0ANotably%2C%20the%20top%20team%20achieved%20the%20lowest%20WER%20of%208.11%5C%25%2C%20and%20the%20highest%0ASemScore%20of%2088.44%5C%25%20at%20the%20same%20time%2C%20setting%20new%20benchmarks%20for%20future%20ASR%0Asystems%20in%20recognizing%20impaired%20speech.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22047v1&entry.124074799=Read"},
{"title": "A Nonlinear MPC Framework for Loco-Manipulation of Quadrupedal Robots\n  with Non-Negligible Manipulator Dynamics", "author": "Ruturaj Sambhus and Kapi Ketan Mehta and Ali MirMohammad Sadeghi and Basit Muhammad Imran and Jeeseop Kim and Taizoon Chunawala and Vittorio Pastore and Sujith Vijayan and Kaveh Akbari Hamed", "abstract": "  Model predictive control (MPC) combined with reduced-order template models\nhas emerged as a powerful tool for trajectory optimization in dynamic legged\nlocomotion. However, loco-manipulation tasks performed by legged robots\nintroduce additional complexity, necessitating computationally efficient MPC\nalgorithms capable of handling high-degree-of-freedom (DoF) models. This letter\npresents a computationally efficient nonlinear MPC (NMPC) framework tailored\nfor loco-manipulation tasks of quadrupedal robots equipped with robotic\nmanipulators whose dynamics are non-negligible relative to those of the\nquadruped. The proposed framework adopts a decomposition strategy that couples\nlocomotion template models -- such as the single rigid body (SRB) model -- with\na full-order dynamic model of the robotic manipulator for torque-level control.\nThis decomposition enables efficient real-time solution of the NMPC problem in\na receding horizon fashion at 60 Hz. The optimal state and input trajectories\ngenerated by the NMPC for locomotion are tracked by a low-level nonlinear\nwhole-body controller (WBC) running at 500 Hz, while the optimal torque\ncommands for the manipulator are directly applied. The layered control\narchitecture is validated through extensive numerical simulations and hardware\nexperiments on a 15-kg Unitree Go2 quadrupedal robot augmented with a 4.4-kg\n4-DoF Kinova arm. Given that the Kinova arm dynamics are non-negligible\nrelative to the Go2 base, the proposed NMPC framework demonstrates robust\nstability in performing diverse loco-manipulation tasks, effectively handling\nexternal disturbances, payload variations, and uneven terrain.\n", "link": "http://arxiv.org/abs/2507.22042v1", "date": "2025-07-29", "relevancy": 1.5744, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5152}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Nonlinear%20MPC%20Framework%20for%20Loco-Manipulation%20of%20Quadrupedal%20Robots%0A%20%20with%20Non-Negligible%20Manipulator%20Dynamics&body=Title%3A%20A%20Nonlinear%20MPC%20Framework%20for%20Loco-Manipulation%20of%20Quadrupedal%20Robots%0A%20%20with%20Non-Negligible%20Manipulator%20Dynamics%0AAuthor%3A%20Ruturaj%20Sambhus%20and%20Kapi%20Ketan%20Mehta%20and%20Ali%20MirMohammad%20Sadeghi%20and%20Basit%20Muhammad%20Imran%20and%20Jeeseop%20Kim%20and%20Taizoon%20Chunawala%20and%20Vittorio%20Pastore%20and%20Sujith%20Vijayan%20and%20Kaveh%20Akbari%20Hamed%0AAbstract%3A%20%20%20Model%20predictive%20control%20%28MPC%29%20combined%20with%20reduced-order%20template%20models%0Ahas%20emerged%20as%20a%20powerful%20tool%20for%20trajectory%20optimization%20in%20dynamic%20legged%0Alocomotion.%20However%2C%20loco-manipulation%20tasks%20performed%20by%20legged%20robots%0Aintroduce%20additional%20complexity%2C%20necessitating%20computationally%20efficient%20MPC%0Aalgorithms%20capable%20of%20handling%20high-degree-of-freedom%20%28DoF%29%20models.%20This%20letter%0Apresents%20a%20computationally%20efficient%20nonlinear%20MPC%20%28NMPC%29%20framework%20tailored%0Afor%20loco-manipulation%20tasks%20of%20quadrupedal%20robots%20equipped%20with%20robotic%0Amanipulators%20whose%20dynamics%20are%20non-negligible%20relative%20to%20those%20of%20the%0Aquadruped.%20The%20proposed%20framework%20adopts%20a%20decomposition%20strategy%20that%20couples%0Alocomotion%20template%20models%20--%20such%20as%20the%20single%20rigid%20body%20%28SRB%29%20model%20--%20with%0Aa%20full-order%20dynamic%20model%20of%20the%20robotic%20manipulator%20for%20torque-level%20control.%0AThis%20decomposition%20enables%20efficient%20real-time%20solution%20of%20the%20NMPC%20problem%20in%0Aa%20receding%20horizon%20fashion%20at%2060%20Hz.%20The%20optimal%20state%20and%20input%20trajectories%0Agenerated%20by%20the%20NMPC%20for%20locomotion%20are%20tracked%20by%20a%20low-level%20nonlinear%0Awhole-body%20controller%20%28WBC%29%20running%20at%20500%20Hz%2C%20while%20the%20optimal%20torque%0Acommands%20for%20the%20manipulator%20are%20directly%20applied.%20The%20layered%20control%0Aarchitecture%20is%20validated%20through%20extensive%20numerical%20simulations%20and%20hardware%0Aexperiments%20on%20a%2015-kg%20Unitree%20Go2%20quadrupedal%20robot%20augmented%20with%20a%204.4-kg%0A4-DoF%20Kinova%20arm.%20Given%20that%20the%20Kinova%20arm%20dynamics%20are%20non-negligible%0Arelative%20to%20the%20Go2%20base%2C%20the%20proposed%20NMPC%20framework%20demonstrates%20robust%0Astability%20in%20performing%20diverse%20loco-manipulation%20tasks%2C%20effectively%20handling%0Aexternal%20disturbances%2C%20payload%20variations%2C%20and%20uneven%20terrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Nonlinear%2520MPC%2520Framework%2520for%2520Loco-Manipulation%2520of%2520Quadrupedal%2520Robots%250A%2520%2520with%2520Non-Negligible%2520Manipulator%2520Dynamics%26entry.906535625%3DRuturaj%2520Sambhus%2520and%2520Kapi%2520Ketan%2520Mehta%2520and%2520Ali%2520MirMohammad%2520Sadeghi%2520and%2520Basit%2520Muhammad%2520Imran%2520and%2520Jeeseop%2520Kim%2520and%2520Taizoon%2520Chunawala%2520and%2520Vittorio%2520Pastore%2520and%2520Sujith%2520Vijayan%2520and%2520Kaveh%2520Akbari%2520Hamed%26entry.1292438233%3D%2520%2520Model%2520predictive%2520control%2520%2528MPC%2529%2520combined%2520with%2520reduced-order%2520template%2520models%250Ahas%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520trajectory%2520optimization%2520in%2520dynamic%2520legged%250Alocomotion.%2520However%252C%2520loco-manipulation%2520tasks%2520performed%2520by%2520legged%2520robots%250Aintroduce%2520additional%2520complexity%252C%2520necessitating%2520computationally%2520efficient%2520MPC%250Aalgorithms%2520capable%2520of%2520handling%2520high-degree-of-freedom%2520%2528DoF%2529%2520models.%2520This%2520letter%250Apresents%2520a%2520computationally%2520efficient%2520nonlinear%2520MPC%2520%2528NMPC%2529%2520framework%2520tailored%250Afor%2520loco-manipulation%2520tasks%2520of%2520quadrupedal%2520robots%2520equipped%2520with%2520robotic%250Amanipulators%2520whose%2520dynamics%2520are%2520non-negligible%2520relative%2520to%2520those%2520of%2520the%250Aquadruped.%2520The%2520proposed%2520framework%2520adopts%2520a%2520decomposition%2520strategy%2520that%2520couples%250Alocomotion%2520template%2520models%2520--%2520such%2520as%2520the%2520single%2520rigid%2520body%2520%2528SRB%2529%2520model%2520--%2520with%250Aa%2520full-order%2520dynamic%2520model%2520of%2520the%2520robotic%2520manipulator%2520for%2520torque-level%2520control.%250AThis%2520decomposition%2520enables%2520efficient%2520real-time%2520solution%2520of%2520the%2520NMPC%2520problem%2520in%250Aa%2520receding%2520horizon%2520fashion%2520at%252060%2520Hz.%2520The%2520optimal%2520state%2520and%2520input%2520trajectories%250Agenerated%2520by%2520the%2520NMPC%2520for%2520locomotion%2520are%2520tracked%2520by%2520a%2520low-level%2520nonlinear%250Awhole-body%2520controller%2520%2528WBC%2529%2520running%2520at%2520500%2520Hz%252C%2520while%2520the%2520optimal%2520torque%250Acommands%2520for%2520the%2520manipulator%2520are%2520directly%2520applied.%2520The%2520layered%2520control%250Aarchitecture%2520is%2520validated%2520through%2520extensive%2520numerical%2520simulations%2520and%2520hardware%250Aexperiments%2520on%2520a%252015-kg%2520Unitree%2520Go2%2520quadrupedal%2520robot%2520augmented%2520with%2520a%25204.4-kg%250A4-DoF%2520Kinova%2520arm.%2520Given%2520that%2520the%2520Kinova%2520arm%2520dynamics%2520are%2520non-negligible%250Arelative%2520to%2520the%2520Go2%2520base%252C%2520the%2520proposed%2520NMPC%2520framework%2520demonstrates%2520robust%250Astability%2520in%2520performing%2520diverse%2520loco-manipulation%2520tasks%252C%2520effectively%2520handling%250Aexternal%2520disturbances%252C%2520payload%2520variations%252C%2520and%2520uneven%2520terrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Nonlinear%20MPC%20Framework%20for%20Loco-Manipulation%20of%20Quadrupedal%20Robots%0A%20%20with%20Non-Negligible%20Manipulator%20Dynamics&entry.906535625=Ruturaj%20Sambhus%20and%20Kapi%20Ketan%20Mehta%20and%20Ali%20MirMohammad%20Sadeghi%20and%20Basit%20Muhammad%20Imran%20and%20Jeeseop%20Kim%20and%20Taizoon%20Chunawala%20and%20Vittorio%20Pastore%20and%20Sujith%20Vijayan%20and%20Kaveh%20Akbari%20Hamed&entry.1292438233=%20%20Model%20predictive%20control%20%28MPC%29%20combined%20with%20reduced-order%20template%20models%0Ahas%20emerged%20as%20a%20powerful%20tool%20for%20trajectory%20optimization%20in%20dynamic%20legged%0Alocomotion.%20However%2C%20loco-manipulation%20tasks%20performed%20by%20legged%20robots%0Aintroduce%20additional%20complexity%2C%20necessitating%20computationally%20efficient%20MPC%0Aalgorithms%20capable%20of%20handling%20high-degree-of-freedom%20%28DoF%29%20models.%20This%20letter%0Apresents%20a%20computationally%20efficient%20nonlinear%20MPC%20%28NMPC%29%20framework%20tailored%0Afor%20loco-manipulation%20tasks%20of%20quadrupedal%20robots%20equipped%20with%20robotic%0Amanipulators%20whose%20dynamics%20are%20non-negligible%20relative%20to%20those%20of%20the%0Aquadruped.%20The%20proposed%20framework%20adopts%20a%20decomposition%20strategy%20that%20couples%0Alocomotion%20template%20models%20--%20such%20as%20the%20single%20rigid%20body%20%28SRB%29%20model%20--%20with%0Aa%20full-order%20dynamic%20model%20of%20the%20robotic%20manipulator%20for%20torque-level%20control.%0AThis%20decomposition%20enables%20efficient%20real-time%20solution%20of%20the%20NMPC%20problem%20in%0Aa%20receding%20horizon%20fashion%20at%2060%20Hz.%20The%20optimal%20state%20and%20input%20trajectories%0Agenerated%20by%20the%20NMPC%20for%20locomotion%20are%20tracked%20by%20a%20low-level%20nonlinear%0Awhole-body%20controller%20%28WBC%29%20running%20at%20500%20Hz%2C%20while%20the%20optimal%20torque%0Acommands%20for%20the%20manipulator%20are%20directly%20applied.%20The%20layered%20control%0Aarchitecture%20is%20validated%20through%20extensive%20numerical%20simulations%20and%20hardware%0Aexperiments%20on%20a%2015-kg%20Unitree%20Go2%20quadrupedal%20robot%20augmented%20with%20a%204.4-kg%0A4-DoF%20Kinova%20arm.%20Given%20that%20the%20Kinova%20arm%20dynamics%20are%20non-negligible%0Arelative%20to%20the%20Go2%20base%2C%20the%20proposed%20NMPC%20framework%20demonstrates%20robust%0Astability%20in%20performing%20diverse%20loco-manipulation%20tasks%2C%20effectively%20handling%0Aexternal%20disturbances%2C%20payload%20variations%2C%20and%20uneven%20terrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22042v1&entry.124074799=Read"},
{"title": "UserBench: An Interactive Gym Environment for User-Centric Agents", "author": "Cheng Qian and Zuxin Liu and Akshara Prabhakar and Zhiwei Liu and Jianguo Zhang and Haolin Chen and Heng Ji and Weiran Yao and Shelby Heinecke and Silvio Savarese and Caiming Xiong and Huan Wang", "abstract": "  Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.\n", "link": "http://arxiv.org/abs/2507.22034v1", "date": "2025-07-29", "relevancy": 1.5729, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5453}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5239}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UserBench%3A%20An%20Interactive%20Gym%20Environment%20for%20User-Centric%20Agents&body=Title%3A%20UserBench%3A%20An%20Interactive%20Gym%20Environment%20for%20User-Centric%20Agents%0AAuthor%3A%20Cheng%20Qian%20and%20Zuxin%20Liu%20and%20Akshara%20Prabhakar%20and%20Zhiwei%20Liu%20and%20Jianguo%20Zhang%20and%20Haolin%20Chen%20and%20Heng%20Ji%20and%20Weiran%20Yao%20and%20Shelby%20Heinecke%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Huan%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29-based%20agents%20have%20made%20impressive%20progress%20in%0Areasoning%20and%20tool%20use%2C%20enabling%20them%20to%20solve%20complex%20tasks.%20However%2C%20their%0Aability%20to%20proactively%20collaborate%20with%20users%2C%20especially%20when%20goals%20are%20vague%2C%0Aevolving%2C%20or%20indirectly%20expressed%2C%20remains%20underexplored.%20To%20address%20this%20gap%2C%0Awe%20introduce%20UserBench%2C%20a%20user-centric%20benchmark%20designed%20to%20evaluate%20agents%20in%0Amulti-turn%2C%20preference-driven%20interactions.%20UserBench%20features%20simulated%20users%0Awho%20start%20with%20underspecified%20goals%20and%20reveal%20preferences%20incrementally%2C%0Arequiring%20agents%20to%20proactively%20clarify%20intent%20and%20make%20grounded%20decisions%20with%0Atools.%20Our%20evaluation%20of%20leading%20open-%20and%20closed-source%20LLMs%20reveals%20a%0Asignificant%20disconnect%20between%20task%20completion%20and%20user%20alignment.%20For%0Ainstance%2C%20models%20provide%20answers%20that%20fully%20align%20with%20all%20user%20intents%20only%0A20%25%20of%20the%20time%20on%20average%2C%20and%20even%20the%20most%20advanced%20models%20uncover%20fewer%0Athan%2030%25%20of%20all%20user%20preferences%20through%20active%20interaction.%20These%20results%0Ahighlight%20the%20challenges%20of%20building%20agents%20that%20are%20not%20just%20capable%20task%0Aexecutors%2C%20but%20true%20collaborative%20partners.%20UserBench%20offers%20an%20interactive%0Aenvironment%20to%20measure%20and%20advance%20this%20critical%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUserBench%253A%2520An%2520Interactive%2520Gym%2520Environment%2520for%2520User-Centric%2520Agents%26entry.906535625%3DCheng%2520Qian%2520and%2520Zuxin%2520Liu%2520and%2520Akshara%2520Prabhakar%2520and%2520Zhiwei%2520Liu%2520and%2520Jianguo%2520Zhang%2520and%2520Haolin%2520Chen%2520and%2520Heng%2520Ji%2520and%2520Weiran%2520Yao%2520and%2520Shelby%2520Heinecke%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Huan%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529-based%2520agents%2520have%2520made%2520impressive%2520progress%2520in%250Areasoning%2520and%2520tool%2520use%252C%2520enabling%2520them%2520to%2520solve%2520complex%2520tasks.%2520However%252C%2520their%250Aability%2520to%2520proactively%2520collaborate%2520with%2520users%252C%2520especially%2520when%2520goals%2520are%2520vague%252C%250Aevolving%252C%2520or%2520indirectly%2520expressed%252C%2520remains%2520underexplored.%2520To%2520address%2520this%2520gap%252C%250Awe%2520introduce%2520UserBench%252C%2520a%2520user-centric%2520benchmark%2520designed%2520to%2520evaluate%2520agents%2520in%250Amulti-turn%252C%2520preference-driven%2520interactions.%2520UserBench%2520features%2520simulated%2520users%250Awho%2520start%2520with%2520underspecified%2520goals%2520and%2520reveal%2520preferences%2520incrementally%252C%250Arequiring%2520agents%2520to%2520proactively%2520clarify%2520intent%2520and%2520make%2520grounded%2520decisions%2520with%250Atools.%2520Our%2520evaluation%2520of%2520leading%2520open-%2520and%2520closed-source%2520LLMs%2520reveals%2520a%250Asignificant%2520disconnect%2520between%2520task%2520completion%2520and%2520user%2520alignment.%2520For%250Ainstance%252C%2520models%2520provide%2520answers%2520that%2520fully%2520align%2520with%2520all%2520user%2520intents%2520only%250A20%2525%2520of%2520the%2520time%2520on%2520average%252C%2520and%2520even%2520the%2520most%2520advanced%2520models%2520uncover%2520fewer%250Athan%252030%2525%2520of%2520all%2520user%2520preferences%2520through%2520active%2520interaction.%2520These%2520results%250Ahighlight%2520the%2520challenges%2520of%2520building%2520agents%2520that%2520are%2520not%2520just%2520capable%2520task%250Aexecutors%252C%2520but%2520true%2520collaborative%2520partners.%2520UserBench%2520offers%2520an%2520interactive%250Aenvironment%2520to%2520measure%2520and%2520advance%2520this%2520critical%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UserBench%3A%20An%20Interactive%20Gym%20Environment%20for%20User-Centric%20Agents&entry.906535625=Cheng%20Qian%20and%20Zuxin%20Liu%20and%20Akshara%20Prabhakar%20and%20Zhiwei%20Liu%20and%20Jianguo%20Zhang%20and%20Haolin%20Chen%20and%20Heng%20Ji%20and%20Weiran%20Yao%20and%20Shelby%20Heinecke%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Huan%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29-based%20agents%20have%20made%20impressive%20progress%20in%0Areasoning%20and%20tool%20use%2C%20enabling%20them%20to%20solve%20complex%20tasks.%20However%2C%20their%0Aability%20to%20proactively%20collaborate%20with%20users%2C%20especially%20when%20goals%20are%20vague%2C%0Aevolving%2C%20or%20indirectly%20expressed%2C%20remains%20underexplored.%20To%20address%20this%20gap%2C%0Awe%20introduce%20UserBench%2C%20a%20user-centric%20benchmark%20designed%20to%20evaluate%20agents%20in%0Amulti-turn%2C%20preference-driven%20interactions.%20UserBench%20features%20simulated%20users%0Awho%20start%20with%20underspecified%20goals%20and%20reveal%20preferences%20incrementally%2C%0Arequiring%20agents%20to%20proactively%20clarify%20intent%20and%20make%20grounded%20decisions%20with%0Atools.%20Our%20evaluation%20of%20leading%20open-%20and%20closed-source%20LLMs%20reveals%20a%0Asignificant%20disconnect%20between%20task%20completion%20and%20user%20alignment.%20For%0Ainstance%2C%20models%20provide%20answers%20that%20fully%20align%20with%20all%20user%20intents%20only%0A20%25%20of%20the%20time%20on%20average%2C%20and%20even%20the%20most%20advanced%20models%20uncover%20fewer%0Athan%2030%25%20of%20all%20user%20preferences%20through%20active%20interaction.%20These%20results%0Ahighlight%20the%20challenges%20of%20building%20agents%20that%20are%20not%20just%20capable%20task%0Aexecutors%2C%20but%20true%20collaborative%20partners.%20UserBench%20offers%20an%20interactive%0Aenvironment%20to%20measure%20and%20advance%20this%20critical%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22034v1&entry.124074799=Read"},
{"title": "Exploring the Stratified Space Structure of an RL Game with the Volume\n  Growth Transform", "author": "Justin Curry and Brennan Lagasse and Ngoc B. Lam and Gregory Cox and David Rosenbluth and Alberto Speranzon", "abstract": "  In this work, we explore the structure of the embedding space of a\ntransformer model trained for playing a particular reinforcement learning (RL)\ngame. Specifically, we investigate how a transformer-based Proximal Policy\nOptimization (PPO) model embeds visual inputs in a simple environment where an\nagent must collect \"coins\" while avoiding dynamic obstacles consisting of\n\"spotlights.\" By adapting Robinson et al.'s study of the volume growth\ntransform for LLMs to the RL setting, we find that the token embedding space\nfor our visual coin collecting game is also not a manifold, and is better\nmodeled as a stratified space, where local dimension can vary from point to\npoint. We further strengthen Robinson's method by proving that fairly general\nvolume growth curves can be realized by stratified spaces. Finally, we carry\nout an analysis that suggests that as an RL agent acts, its latent\nrepresentation alternates between periods of low local dimension, while\nfollowing a fixed sub-strategy, and bursts of high local dimension, where the\nagent achieves a sub-goal (e.g., collecting an object) or where the\nenvironmental complexity increases (e.g., more obstacles appear). Consequently,\nour work suggests that the distribution of dimensions in a stratified latent\nspace may provide a new geometric indicator of complexity for RL games.\n", "link": "http://arxiv.org/abs/2507.22010v1", "date": "2025-07-29", "relevancy": 1.5355, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5289}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Stratified%20Space%20Structure%20of%20an%20RL%20Game%20with%20the%20Volume%0A%20%20Growth%20Transform&body=Title%3A%20Exploring%20the%20Stratified%20Space%20Structure%20of%20an%20RL%20Game%20with%20the%20Volume%0A%20%20Growth%20Transform%0AAuthor%3A%20Justin%20Curry%20and%20Brennan%20Lagasse%20and%20Ngoc%20B.%20Lam%20and%20Gregory%20Cox%20and%20David%20Rosenbluth%20and%20Alberto%20Speranzon%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20explore%20the%20structure%20of%20the%20embedding%20space%20of%20a%0Atransformer%20model%20trained%20for%20playing%20a%20particular%20reinforcement%20learning%20%28RL%29%0Agame.%20Specifically%2C%20we%20investigate%20how%20a%20transformer-based%20Proximal%20Policy%0AOptimization%20%28PPO%29%20model%20embeds%20visual%20inputs%20in%20a%20simple%20environment%20where%20an%0Aagent%20must%20collect%20%22coins%22%20while%20avoiding%20dynamic%20obstacles%20consisting%20of%0A%22spotlights.%22%20By%20adapting%20Robinson%20et%20al.%27s%20study%20of%20the%20volume%20growth%0Atransform%20for%20LLMs%20to%20the%20RL%20setting%2C%20we%20find%20that%20the%20token%20embedding%20space%0Afor%20our%20visual%20coin%20collecting%20game%20is%20also%20not%20a%20manifold%2C%20and%20is%20better%0Amodeled%20as%20a%20stratified%20space%2C%20where%20local%20dimension%20can%20vary%20from%20point%20to%0Apoint.%20We%20further%20strengthen%20Robinson%27s%20method%20by%20proving%20that%20fairly%20general%0Avolume%20growth%20curves%20can%20be%20realized%20by%20stratified%20spaces.%20Finally%2C%20we%20carry%0Aout%20an%20analysis%20that%20suggests%20that%20as%20an%20RL%20agent%20acts%2C%20its%20latent%0Arepresentation%20alternates%20between%20periods%20of%20low%20local%20dimension%2C%20while%0Afollowing%20a%20fixed%20sub-strategy%2C%20and%20bursts%20of%20high%20local%20dimension%2C%20where%20the%0Aagent%20achieves%20a%20sub-goal%20%28e.g.%2C%20collecting%20an%20object%29%20or%20where%20the%0Aenvironmental%20complexity%20increases%20%28e.g.%2C%20more%20obstacles%20appear%29.%20Consequently%2C%0Aour%20work%20suggests%20that%20the%20distribution%20of%20dimensions%20in%20a%20stratified%20latent%0Aspace%20may%20provide%20a%20new%20geometric%20indicator%20of%20complexity%20for%20RL%20games.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Stratified%2520Space%2520Structure%2520of%2520an%2520RL%2520Game%2520with%2520the%2520Volume%250A%2520%2520Growth%2520Transform%26entry.906535625%3DJustin%2520Curry%2520and%2520Brennan%2520Lagasse%2520and%2520Ngoc%2520B.%2520Lam%2520and%2520Gregory%2520Cox%2520and%2520David%2520Rosenbluth%2520and%2520Alberto%2520Speranzon%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520structure%2520of%2520the%2520embedding%2520space%2520of%2520a%250Atransformer%2520model%2520trained%2520for%2520playing%2520a%2520particular%2520reinforcement%2520learning%2520%2528RL%2529%250Agame.%2520Specifically%252C%2520we%2520investigate%2520how%2520a%2520transformer-based%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529%2520model%2520embeds%2520visual%2520inputs%2520in%2520a%2520simple%2520environment%2520where%2520an%250Aagent%2520must%2520collect%2520%2522coins%2522%2520while%2520avoiding%2520dynamic%2520obstacles%2520consisting%2520of%250A%2522spotlights.%2522%2520By%2520adapting%2520Robinson%2520et%2520al.%2527s%2520study%2520of%2520the%2520volume%2520growth%250Atransform%2520for%2520LLMs%2520to%2520the%2520RL%2520setting%252C%2520we%2520find%2520that%2520the%2520token%2520embedding%2520space%250Afor%2520our%2520visual%2520coin%2520collecting%2520game%2520is%2520also%2520not%2520a%2520manifold%252C%2520and%2520is%2520better%250Amodeled%2520as%2520a%2520stratified%2520space%252C%2520where%2520local%2520dimension%2520can%2520vary%2520from%2520point%2520to%250Apoint.%2520We%2520further%2520strengthen%2520Robinson%2527s%2520method%2520by%2520proving%2520that%2520fairly%2520general%250Avolume%2520growth%2520curves%2520can%2520be%2520realized%2520by%2520stratified%2520spaces.%2520Finally%252C%2520we%2520carry%250Aout%2520an%2520analysis%2520that%2520suggests%2520that%2520as%2520an%2520RL%2520agent%2520acts%252C%2520its%2520latent%250Arepresentation%2520alternates%2520between%2520periods%2520of%2520low%2520local%2520dimension%252C%2520while%250Afollowing%2520a%2520fixed%2520sub-strategy%252C%2520and%2520bursts%2520of%2520high%2520local%2520dimension%252C%2520where%2520the%250Aagent%2520achieves%2520a%2520sub-goal%2520%2528e.g.%252C%2520collecting%2520an%2520object%2529%2520or%2520where%2520the%250Aenvironmental%2520complexity%2520increases%2520%2528e.g.%252C%2520more%2520obstacles%2520appear%2529.%2520Consequently%252C%250Aour%2520work%2520suggests%2520that%2520the%2520distribution%2520of%2520dimensions%2520in%2520a%2520stratified%2520latent%250Aspace%2520may%2520provide%2520a%2520new%2520geometric%2520indicator%2520of%2520complexity%2520for%2520RL%2520games.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Stratified%20Space%20Structure%20of%20an%20RL%20Game%20with%20the%20Volume%0A%20%20Growth%20Transform&entry.906535625=Justin%20Curry%20and%20Brennan%20Lagasse%20and%20Ngoc%20B.%20Lam%20and%20Gregory%20Cox%20and%20David%20Rosenbluth%20and%20Alberto%20Speranzon&entry.1292438233=%20%20In%20this%20work%2C%20we%20explore%20the%20structure%20of%20the%20embedding%20space%20of%20a%0Atransformer%20model%20trained%20for%20playing%20a%20particular%20reinforcement%20learning%20%28RL%29%0Agame.%20Specifically%2C%20we%20investigate%20how%20a%20transformer-based%20Proximal%20Policy%0AOptimization%20%28PPO%29%20model%20embeds%20visual%20inputs%20in%20a%20simple%20environment%20where%20an%0Aagent%20must%20collect%20%22coins%22%20while%20avoiding%20dynamic%20obstacles%20consisting%20of%0A%22spotlights.%22%20By%20adapting%20Robinson%20et%20al.%27s%20study%20of%20the%20volume%20growth%0Atransform%20for%20LLMs%20to%20the%20RL%20setting%2C%20we%20find%20that%20the%20token%20embedding%20space%0Afor%20our%20visual%20coin%20collecting%20game%20is%20also%20not%20a%20manifold%2C%20and%20is%20better%0Amodeled%20as%20a%20stratified%20space%2C%20where%20local%20dimension%20can%20vary%20from%20point%20to%0Apoint.%20We%20further%20strengthen%20Robinson%27s%20method%20by%20proving%20that%20fairly%20general%0Avolume%20growth%20curves%20can%20be%20realized%20by%20stratified%20spaces.%20Finally%2C%20we%20carry%0Aout%20an%20analysis%20that%20suggests%20that%20as%20an%20RL%20agent%20acts%2C%20its%20latent%0Arepresentation%20alternates%20between%20periods%20of%20low%20local%20dimension%2C%20while%0Afollowing%20a%20fixed%20sub-strategy%2C%20and%20bursts%20of%20high%20local%20dimension%2C%20where%20the%0Aagent%20achieves%20a%20sub-goal%20%28e.g.%2C%20collecting%20an%20object%29%20or%20where%20the%0Aenvironmental%20complexity%20increases%20%28e.g.%2C%20more%20obstacles%20appear%29.%20Consequently%2C%0Aour%20work%20suggests%20that%20the%20distribution%20of%20dimensions%20in%20a%20stratified%20latent%0Aspace%20may%20provide%20a%20new%20geometric%20indicator%20of%20complexity%20for%20RL%20games.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22010v1&entry.124074799=Read"},
{"title": "StepAL: Step-aware Active Learning for Cataract Surgical Videos", "author": "Nisarg A. Shah and Bardia Safaei and Shameema Sikder and S. Swaroop Vedula and Vishal M. Patel", "abstract": "  Active learning (AL) can reduce annotation costs in surgical video analysis\nwhile maintaining model performance. However, traditional AL methods, developed\nfor images or short video clips, are suboptimal for surgical step recognition\ndue to inter-step dependencies within long, untrimmed surgical videos. These\nmethods typically select individual frames or clips for labeling, which is\nineffective for surgical videos where annotators require the context of the\nentire video for annotation. To address this, we propose StepAL, an active\nlearning framework designed for full video selection in surgical step\nrecognition. StepAL integrates a step-aware feature representation, which\nleverages pseudo-labels to capture the distribution of predicted steps within\neach video, with an entropy-weighted clustering strategy. This combination\nprioritizes videos that are both uncertain and exhibit diverse step\ncompositions for annotation. Experiments on two cataract surgery datasets\n(Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms\nexisting active learning approaches, achieving higher accuracy in step\nrecognition with fewer labeled videos. StepAL offers an effective approach for\nefficient surgical video analysis, reducing the annotation burden in developing\ncomputer-assisted surgical systems.\n", "link": "http://arxiv.org/abs/2507.22059v1", "date": "2025-07-29", "relevancy": 1.5309, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StepAL%3A%20Step-aware%20Active%20Learning%20for%20Cataract%20Surgical%20Videos&body=Title%3A%20StepAL%3A%20Step-aware%20Active%20Learning%20for%20Cataract%20Surgical%20Videos%0AAuthor%3A%20Nisarg%20A.%20Shah%20and%20Bardia%20Safaei%20and%20Shameema%20Sikder%20and%20S.%20Swaroop%20Vedula%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20can%20reduce%20annotation%20costs%20in%20surgical%20video%20analysis%0Awhile%20maintaining%20model%20performance.%20However%2C%20traditional%20AL%20methods%2C%20developed%0Afor%20images%20or%20short%20video%20clips%2C%20are%20suboptimal%20for%20surgical%20step%20recognition%0Adue%20to%20inter-step%20dependencies%20within%20long%2C%20untrimmed%20surgical%20videos.%20These%0Amethods%20typically%20select%20individual%20frames%20or%20clips%20for%20labeling%2C%20which%20is%0Aineffective%20for%20surgical%20videos%20where%20annotators%20require%20the%20context%20of%20the%0Aentire%20video%20for%20annotation.%20To%20address%20this%2C%20we%20propose%20StepAL%2C%20an%20active%0Alearning%20framework%20designed%20for%20full%20video%20selection%20in%20surgical%20step%0Arecognition.%20StepAL%20integrates%20a%20step-aware%20feature%20representation%2C%20which%0Aleverages%20pseudo-labels%20to%20capture%20the%20distribution%20of%20predicted%20steps%20within%0Aeach%20video%2C%20with%20an%20entropy-weighted%20clustering%20strategy.%20This%20combination%0Aprioritizes%20videos%20that%20are%20both%20uncertain%20and%20exhibit%20diverse%20step%0Acompositions%20for%20annotation.%20Experiments%20on%20two%20cataract%20surgery%20datasets%0A%28Cataract-1k%20and%20Cataract-101%29%20demonstrate%20that%20StepAL%20consistently%20outperforms%0Aexisting%20active%20learning%20approaches%2C%20achieving%20higher%20accuracy%20in%20step%0Arecognition%20with%20fewer%20labeled%20videos.%20StepAL%20offers%20an%20effective%20approach%20for%0Aefficient%20surgical%20video%20analysis%2C%20reducing%20the%20annotation%20burden%20in%20developing%0Acomputer-assisted%20surgical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepAL%253A%2520Step-aware%2520Active%2520Learning%2520for%2520Cataract%2520Surgical%2520Videos%26entry.906535625%3DNisarg%2520A.%2520Shah%2520and%2520Bardia%2520Safaei%2520and%2520Shameema%2520Sikder%2520and%2520S.%2520Swaroop%2520Vedula%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520Active%2520learning%2520%2528AL%2529%2520can%2520reduce%2520annotation%2520costs%2520in%2520surgical%2520video%2520analysis%250Awhile%2520maintaining%2520model%2520performance.%2520However%252C%2520traditional%2520AL%2520methods%252C%2520developed%250Afor%2520images%2520or%2520short%2520video%2520clips%252C%2520are%2520suboptimal%2520for%2520surgical%2520step%2520recognition%250Adue%2520to%2520inter-step%2520dependencies%2520within%2520long%252C%2520untrimmed%2520surgical%2520videos.%2520These%250Amethods%2520typically%2520select%2520individual%2520frames%2520or%2520clips%2520for%2520labeling%252C%2520which%2520is%250Aineffective%2520for%2520surgical%2520videos%2520where%2520annotators%2520require%2520the%2520context%2520of%2520the%250Aentire%2520video%2520for%2520annotation.%2520To%2520address%2520this%252C%2520we%2520propose%2520StepAL%252C%2520an%2520active%250Alearning%2520framework%2520designed%2520for%2520full%2520video%2520selection%2520in%2520surgical%2520step%250Arecognition.%2520StepAL%2520integrates%2520a%2520step-aware%2520feature%2520representation%252C%2520which%250Aleverages%2520pseudo-labels%2520to%2520capture%2520the%2520distribution%2520of%2520predicted%2520steps%2520within%250Aeach%2520video%252C%2520with%2520an%2520entropy-weighted%2520clustering%2520strategy.%2520This%2520combination%250Aprioritizes%2520videos%2520that%2520are%2520both%2520uncertain%2520and%2520exhibit%2520diverse%2520step%250Acompositions%2520for%2520annotation.%2520Experiments%2520on%2520two%2520cataract%2520surgery%2520datasets%250A%2528Cataract-1k%2520and%2520Cataract-101%2529%2520demonstrate%2520that%2520StepAL%2520consistently%2520outperforms%250Aexisting%2520active%2520learning%2520approaches%252C%2520achieving%2520higher%2520accuracy%2520in%2520step%250Arecognition%2520with%2520fewer%2520labeled%2520videos.%2520StepAL%2520offers%2520an%2520effective%2520approach%2520for%250Aefficient%2520surgical%2520video%2520analysis%252C%2520reducing%2520the%2520annotation%2520burden%2520in%2520developing%250Acomputer-assisted%2520surgical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StepAL%3A%20Step-aware%20Active%20Learning%20for%20Cataract%20Surgical%20Videos&entry.906535625=Nisarg%20A.%20Shah%20and%20Bardia%20Safaei%20and%20Shameema%20Sikder%20and%20S.%20Swaroop%20Vedula%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20Active%20learning%20%28AL%29%20can%20reduce%20annotation%20costs%20in%20surgical%20video%20analysis%0Awhile%20maintaining%20model%20performance.%20However%2C%20traditional%20AL%20methods%2C%20developed%0Afor%20images%20or%20short%20video%20clips%2C%20are%20suboptimal%20for%20surgical%20step%20recognition%0Adue%20to%20inter-step%20dependencies%20within%20long%2C%20untrimmed%20surgical%20videos.%20These%0Amethods%20typically%20select%20individual%20frames%20or%20clips%20for%20labeling%2C%20which%20is%0Aineffective%20for%20surgical%20videos%20where%20annotators%20require%20the%20context%20of%20the%0Aentire%20video%20for%20annotation.%20To%20address%20this%2C%20we%20propose%20StepAL%2C%20an%20active%0Alearning%20framework%20designed%20for%20full%20video%20selection%20in%20surgical%20step%0Arecognition.%20StepAL%20integrates%20a%20step-aware%20feature%20representation%2C%20which%0Aleverages%20pseudo-labels%20to%20capture%20the%20distribution%20of%20predicted%20steps%20within%0Aeach%20video%2C%20with%20an%20entropy-weighted%20clustering%20strategy.%20This%20combination%0Aprioritizes%20videos%20that%20are%20both%20uncertain%20and%20exhibit%20diverse%20step%0Acompositions%20for%20annotation.%20Experiments%20on%20two%20cataract%20surgery%20datasets%0A%28Cataract-1k%20and%20Cataract-101%29%20demonstrate%20that%20StepAL%20consistently%20outperforms%0Aexisting%20active%20learning%20approaches%2C%20achieving%20higher%20accuracy%20in%20step%0Arecognition%20with%20fewer%20labeled%20videos.%20StepAL%20offers%20an%20effective%20approach%20for%0Aefficient%20surgical%20video%20analysis%2C%20reducing%20the%20annotation%20burden%20in%20developing%0Acomputer-assisted%20surgical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22059v1&entry.124074799=Read"},
{"title": "Improving Generative Ad Text on Facebook using Reinforcement Learning", "author": "Daniel R. Jiang and Alex Nikulkov and Yu-Chia Chen and Yang Bai and Zheqing Zhu", "abstract": "  Generative artificial intelligence (AI), in particular large language models\n(LLMs), is poised to drive transformative economic change. LLMs are pre-trained\non vast text data to learn general language patterns, but a subsequent\npost-training phase is critical to align them for specific real-world tasks.\nReinforcement learning (RL) is the leading post-training technique, yet its\neconomic impact remains largely underexplored and unquantified. We examine this\nquestion through the lens of the first deployment of an RL-trained LLM for\ngenerative advertising on Facebook. Integrated into Meta's Text Generation\nfeature, our model, \"AdLlama,\" powers an AI tool that helps advertisers create\nnew variations of human-written ad text. To train this model, we introduce\nreinforcement learning with performance feedback (RLPF), a post-training method\nthat uses historical ad performance data as a reward signal. In a large-scale\n10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad\nvariations, we find that AdLlama improves click-through rates by 6.7%\n(p=0.0296) compared to a supervised imitation model trained on curated ads.\nThis represents a substantial improvement in advertiser return on investment on\nFacebook. We also find that advertisers who used AdLlama generated more ad\nvariations, indicating higher satisfaction with the model's outputs. To our\nknowledge, this is the largest study to date on the use of generative AI in an\necologically valid setting, offering an important data point quantifying the\ntangible impact of RL post-training. Furthermore, the results show that RLPF is\na promising and generalizable approach for metric-driven post-training that\nbridges the gap between highly capable language models and tangible outcomes.\n", "link": "http://arxiv.org/abs/2507.21983v1", "date": "2025-07-29", "relevancy": 1.5126, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5258}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5046}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Generative%20Ad%20Text%20on%20Facebook%20using%20Reinforcement%20Learning&body=Title%3A%20Improving%20Generative%20Ad%20Text%20on%20Facebook%20using%20Reinforcement%20Learning%0AAuthor%3A%20Daniel%20R.%20Jiang%20and%20Alex%20Nikulkov%20and%20Yu-Chia%20Chen%20and%20Yang%20Bai%20and%20Zheqing%20Zhu%0AAbstract%3A%20%20%20Generative%20artificial%20intelligence%20%28AI%29%2C%20in%20particular%20large%20language%20models%0A%28LLMs%29%2C%20is%20poised%20to%20drive%20transformative%20economic%20change.%20LLMs%20are%20pre-trained%0Aon%20vast%20text%20data%20to%20learn%20general%20language%20patterns%2C%20but%20a%20subsequent%0Apost-training%20phase%20is%20critical%20to%20align%20them%20for%20specific%20real-world%20tasks.%0AReinforcement%20learning%20%28RL%29%20is%20the%20leading%20post-training%20technique%2C%20yet%20its%0Aeconomic%20impact%20remains%20largely%20underexplored%20and%20unquantified.%20We%20examine%20this%0Aquestion%20through%20the%20lens%20of%20the%20first%20deployment%20of%20an%20RL-trained%20LLM%20for%0Agenerative%20advertising%20on%20Facebook.%20Integrated%20into%20Meta%27s%20Text%20Generation%0Afeature%2C%20our%20model%2C%20%22AdLlama%2C%22%20powers%20an%20AI%20tool%20that%20helps%20advertisers%20create%0Anew%20variations%20of%20human-written%20ad%20text.%20To%20train%20this%20model%2C%20we%20introduce%0Areinforcement%20learning%20with%20performance%20feedback%20%28RLPF%29%2C%20a%20post-training%20method%0Athat%20uses%20historical%20ad%20performance%20data%20as%20a%20reward%20signal.%20In%20a%20large-scale%0A10-week%20A/B%20test%20on%20Facebook%20spanning%20nearly%2035%2C000%20advertisers%20and%20640%2C000%20ad%0Avariations%2C%20we%20find%20that%20AdLlama%20improves%20click-through%20rates%20by%206.7%25%0A%28p%3D0.0296%29%20compared%20to%20a%20supervised%20imitation%20model%20trained%20on%20curated%20ads.%0AThis%20represents%20a%20substantial%20improvement%20in%20advertiser%20return%20on%20investment%20on%0AFacebook.%20We%20also%20find%20that%20advertisers%20who%20used%20AdLlama%20generated%20more%20ad%0Avariations%2C%20indicating%20higher%20satisfaction%20with%20the%20model%27s%20outputs.%20To%20our%0Aknowledge%2C%20this%20is%20the%20largest%20study%20to%20date%20on%20the%20use%20of%20generative%20AI%20in%20an%0Aecologically%20valid%20setting%2C%20offering%20an%20important%20data%20point%20quantifying%20the%0Atangible%20impact%20of%20RL%20post-training.%20Furthermore%2C%20the%20results%20show%20that%20RLPF%20is%0Aa%20promising%20and%20generalizable%20approach%20for%20metric-driven%20post-training%20that%0Abridges%20the%20gap%20between%20highly%20capable%20language%20models%20and%20tangible%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Generative%2520Ad%2520Text%2520on%2520Facebook%2520using%2520Reinforcement%2520Learning%26entry.906535625%3DDaniel%2520R.%2520Jiang%2520and%2520Alex%2520Nikulkov%2520and%2520Yu-Chia%2520Chen%2520and%2520Yang%2520Bai%2520and%2520Zheqing%2520Zhu%26entry.1292438233%3D%2520%2520Generative%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520in%2520particular%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520is%2520poised%2520to%2520drive%2520transformative%2520economic%2520change.%2520LLMs%2520are%2520pre-trained%250Aon%2520vast%2520text%2520data%2520to%2520learn%2520general%2520language%2520patterns%252C%2520but%2520a%2520subsequent%250Apost-training%2520phase%2520is%2520critical%2520to%2520align%2520them%2520for%2520specific%2520real-world%2520tasks.%250AReinforcement%2520learning%2520%2528RL%2529%2520is%2520the%2520leading%2520post-training%2520technique%252C%2520yet%2520its%250Aeconomic%2520impact%2520remains%2520largely%2520underexplored%2520and%2520unquantified.%2520We%2520examine%2520this%250Aquestion%2520through%2520the%2520lens%2520of%2520the%2520first%2520deployment%2520of%2520an%2520RL-trained%2520LLM%2520for%250Agenerative%2520advertising%2520on%2520Facebook.%2520Integrated%2520into%2520Meta%2527s%2520Text%2520Generation%250Afeature%252C%2520our%2520model%252C%2520%2522AdLlama%252C%2522%2520powers%2520an%2520AI%2520tool%2520that%2520helps%2520advertisers%2520create%250Anew%2520variations%2520of%2520human-written%2520ad%2520text.%2520To%2520train%2520this%2520model%252C%2520we%2520introduce%250Areinforcement%2520learning%2520with%2520performance%2520feedback%2520%2528RLPF%2529%252C%2520a%2520post-training%2520method%250Athat%2520uses%2520historical%2520ad%2520performance%2520data%2520as%2520a%2520reward%2520signal.%2520In%2520a%2520large-scale%250A10-week%2520A/B%2520test%2520on%2520Facebook%2520spanning%2520nearly%252035%252C000%2520advertisers%2520and%2520640%252C000%2520ad%250Avariations%252C%2520we%2520find%2520that%2520AdLlama%2520improves%2520click-through%2520rates%2520by%25206.7%2525%250A%2528p%253D0.0296%2529%2520compared%2520to%2520a%2520supervised%2520imitation%2520model%2520trained%2520on%2520curated%2520ads.%250AThis%2520represents%2520a%2520substantial%2520improvement%2520in%2520advertiser%2520return%2520on%2520investment%2520on%250AFacebook.%2520We%2520also%2520find%2520that%2520advertisers%2520who%2520used%2520AdLlama%2520generated%2520more%2520ad%250Avariations%252C%2520indicating%2520higher%2520satisfaction%2520with%2520the%2520model%2527s%2520outputs.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520largest%2520study%2520to%2520date%2520on%2520the%2520use%2520of%2520generative%2520AI%2520in%2520an%250Aecologically%2520valid%2520setting%252C%2520offering%2520an%2520important%2520data%2520point%2520quantifying%2520the%250Atangible%2520impact%2520of%2520RL%2520post-training.%2520Furthermore%252C%2520the%2520results%2520show%2520that%2520RLPF%2520is%250Aa%2520promising%2520and%2520generalizable%2520approach%2520for%2520metric-driven%2520post-training%2520that%250Abridges%2520the%2520gap%2520between%2520highly%2520capable%2520language%2520models%2520and%2520tangible%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Generative%20Ad%20Text%20on%20Facebook%20using%20Reinforcement%20Learning&entry.906535625=Daniel%20R.%20Jiang%20and%20Alex%20Nikulkov%20and%20Yu-Chia%20Chen%20and%20Yang%20Bai%20and%20Zheqing%20Zhu&entry.1292438233=%20%20Generative%20artificial%20intelligence%20%28AI%29%2C%20in%20particular%20large%20language%20models%0A%28LLMs%29%2C%20is%20poised%20to%20drive%20transformative%20economic%20change.%20LLMs%20are%20pre-trained%0Aon%20vast%20text%20data%20to%20learn%20general%20language%20patterns%2C%20but%20a%20subsequent%0Apost-training%20phase%20is%20critical%20to%20align%20them%20for%20specific%20real-world%20tasks.%0AReinforcement%20learning%20%28RL%29%20is%20the%20leading%20post-training%20technique%2C%20yet%20its%0Aeconomic%20impact%20remains%20largely%20underexplored%20and%20unquantified.%20We%20examine%20this%0Aquestion%20through%20the%20lens%20of%20the%20first%20deployment%20of%20an%20RL-trained%20LLM%20for%0Agenerative%20advertising%20on%20Facebook.%20Integrated%20into%20Meta%27s%20Text%20Generation%0Afeature%2C%20our%20model%2C%20%22AdLlama%2C%22%20powers%20an%20AI%20tool%20that%20helps%20advertisers%20create%0Anew%20variations%20of%20human-written%20ad%20text.%20To%20train%20this%20model%2C%20we%20introduce%0Areinforcement%20learning%20with%20performance%20feedback%20%28RLPF%29%2C%20a%20post-training%20method%0Athat%20uses%20historical%20ad%20performance%20data%20as%20a%20reward%20signal.%20In%20a%20large-scale%0A10-week%20A/B%20test%20on%20Facebook%20spanning%20nearly%2035%2C000%20advertisers%20and%20640%2C000%20ad%0Avariations%2C%20we%20find%20that%20AdLlama%20improves%20click-through%20rates%20by%206.7%25%0A%28p%3D0.0296%29%20compared%20to%20a%20supervised%20imitation%20model%20trained%20on%20curated%20ads.%0AThis%20represents%20a%20substantial%20improvement%20in%20advertiser%20return%20on%20investment%20on%0AFacebook.%20We%20also%20find%20that%20advertisers%20who%20used%20AdLlama%20generated%20more%20ad%0Avariations%2C%20indicating%20higher%20satisfaction%20with%20the%20model%27s%20outputs.%20To%20our%0Aknowledge%2C%20this%20is%20the%20largest%20study%20to%20date%20on%20the%20use%20of%20generative%20AI%20in%20an%0Aecologically%20valid%20setting%2C%20offering%20an%20important%20data%20point%20quantifying%20the%0Atangible%20impact%20of%20RL%20post-training.%20Furthermore%2C%20the%20results%20show%20that%20RLPF%20is%0Aa%20promising%20and%20generalizable%20approach%20for%20metric-driven%20post-training%20that%0Abridges%20the%20gap%20between%20highly%20capable%20language%20models%20and%20tangible%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21983v1&entry.124074799=Read"},
{"title": "ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models", "author": "Hyun Jun Yook and Ga San Jhun and Jae Hyun Cho and Min Jeon and Donghyun Kim and Tae Hyung Kim and Youn Kyu Lee", "abstract": "  Machine unlearning (MU) removes specific data points or concepts from deep\nlearning models to enhance privacy and prevent sensitive content generation.\nAdversarial prompts can exploit unlearned models to generate content containing\nremoved concepts, posing a significant security risk. However, existing\nadversarial attack methods still face challenges in generating content that\naligns with an attacker's intent while incurring high computational costs to\nidentify successful prompts. To address these challenges, we propose ZIUM, a\nZero-shot Intent-aware adversarial attack on Unlearned Models, which enables\nthe flexible customization of target attack images to reflect an attacker's\nintent. Additionally, ZIUM supports zero-shot adversarial attacks without\nrequiring further optimization for previously attacked unlearned concepts. The\nevaluation across various MU scenarios demonstrated ZIUM's effectiveness in\nsuccessfully customizing content based on user-intent prompts while achieving a\nsuperior attack success rate compared to existing methods. Moreover, its\nzero-shot adversarial attack significantly reduces the attack time for\npreviously attacked unlearned concepts.\n", "link": "http://arxiv.org/abs/2507.21985v1", "date": "2025-07-29", "relevancy": 1.4659, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5008}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZIUM%3A%20Zero-Shot%20Intent-Aware%20Adversarial%20Attack%20on%20Unlearned%20Models&body=Title%3A%20ZIUM%3A%20Zero-Shot%20Intent-Aware%20Adversarial%20Attack%20on%20Unlearned%20Models%0AAuthor%3A%20Hyun%20Jun%20Yook%20and%20Ga%20San%20Jhun%20and%20Jae%20Hyun%20Cho%20and%20Min%20Jeon%20and%20Donghyun%20Kim%20and%20Tae%20Hyung%20Kim%20and%20Youn%20Kyu%20Lee%0AAbstract%3A%20%20%20Machine%20unlearning%20%28MU%29%20removes%20specific%20data%20points%20or%20concepts%20from%20deep%0Alearning%20models%20to%20enhance%20privacy%20and%20prevent%20sensitive%20content%20generation.%0AAdversarial%20prompts%20can%20exploit%20unlearned%20models%20to%20generate%20content%20containing%0Aremoved%20concepts%2C%20posing%20a%20significant%20security%20risk.%20However%2C%20existing%0Aadversarial%20attack%20methods%20still%20face%20challenges%20in%20generating%20content%20that%0Aaligns%20with%20an%20attacker%27s%20intent%20while%20incurring%20high%20computational%20costs%20to%0Aidentify%20successful%20prompts.%20To%20address%20these%20challenges%2C%20we%20propose%20ZIUM%2C%20a%0AZero-shot%20Intent-aware%20adversarial%20attack%20on%20Unlearned%20Models%2C%20which%20enables%0Athe%20flexible%20customization%20of%20target%20attack%20images%20to%20reflect%20an%20attacker%27s%0Aintent.%20Additionally%2C%20ZIUM%20supports%20zero-shot%20adversarial%20attacks%20without%0Arequiring%20further%20optimization%20for%20previously%20attacked%20unlearned%20concepts.%20The%0Aevaluation%20across%20various%20MU%20scenarios%20demonstrated%20ZIUM%27s%20effectiveness%20in%0Asuccessfully%20customizing%20content%20based%20on%20user-intent%20prompts%20while%20achieving%20a%0Asuperior%20attack%20success%20rate%20compared%20to%20existing%20methods.%20Moreover%2C%20its%0Azero-shot%20adversarial%20attack%20significantly%20reduces%20the%20attack%20time%20for%0Apreviously%20attacked%20unlearned%20concepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZIUM%253A%2520Zero-Shot%2520Intent-Aware%2520Adversarial%2520Attack%2520on%2520Unlearned%2520Models%26entry.906535625%3DHyun%2520Jun%2520Yook%2520and%2520Ga%2520San%2520Jhun%2520and%2520Jae%2520Hyun%2520Cho%2520and%2520Min%2520Jeon%2520and%2520Donghyun%2520Kim%2520and%2520Tae%2520Hyung%2520Kim%2520and%2520Youn%2520Kyu%2520Lee%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520%2528MU%2529%2520removes%2520specific%2520data%2520points%2520or%2520concepts%2520from%2520deep%250Alearning%2520models%2520to%2520enhance%2520privacy%2520and%2520prevent%2520sensitive%2520content%2520generation.%250AAdversarial%2520prompts%2520can%2520exploit%2520unlearned%2520models%2520to%2520generate%2520content%2520containing%250Aremoved%2520concepts%252C%2520posing%2520a%2520significant%2520security%2520risk.%2520However%252C%2520existing%250Aadversarial%2520attack%2520methods%2520still%2520face%2520challenges%2520in%2520generating%2520content%2520that%250Aaligns%2520with%2520an%2520attacker%2527s%2520intent%2520while%2520incurring%2520high%2520computational%2520costs%2520to%250Aidentify%2520successful%2520prompts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520ZIUM%252C%2520a%250AZero-shot%2520Intent-aware%2520adversarial%2520attack%2520on%2520Unlearned%2520Models%252C%2520which%2520enables%250Athe%2520flexible%2520customization%2520of%2520target%2520attack%2520images%2520to%2520reflect%2520an%2520attacker%2527s%250Aintent.%2520Additionally%252C%2520ZIUM%2520supports%2520zero-shot%2520adversarial%2520attacks%2520without%250Arequiring%2520further%2520optimization%2520for%2520previously%2520attacked%2520unlearned%2520concepts.%2520The%250Aevaluation%2520across%2520various%2520MU%2520scenarios%2520demonstrated%2520ZIUM%2527s%2520effectiveness%2520in%250Asuccessfully%2520customizing%2520content%2520based%2520on%2520user-intent%2520prompts%2520while%2520achieving%2520a%250Asuperior%2520attack%2520success%2520rate%2520compared%2520to%2520existing%2520methods.%2520Moreover%252C%2520its%250Azero-shot%2520adversarial%2520attack%2520significantly%2520reduces%2520the%2520attack%2520time%2520for%250Apreviously%2520attacked%2520unlearned%2520concepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZIUM%3A%20Zero-Shot%20Intent-Aware%20Adversarial%20Attack%20on%20Unlearned%20Models&entry.906535625=Hyun%20Jun%20Yook%20and%20Ga%20San%20Jhun%20and%20Jae%20Hyun%20Cho%20and%20Min%20Jeon%20and%20Donghyun%20Kim%20and%20Tae%20Hyung%20Kim%20and%20Youn%20Kyu%20Lee&entry.1292438233=%20%20Machine%20unlearning%20%28MU%29%20removes%20specific%20data%20points%20or%20concepts%20from%20deep%0Alearning%20models%20to%20enhance%20privacy%20and%20prevent%20sensitive%20content%20generation.%0AAdversarial%20prompts%20can%20exploit%20unlearned%20models%20to%20generate%20content%20containing%0Aremoved%20concepts%2C%20posing%20a%20significant%20security%20risk.%20However%2C%20existing%0Aadversarial%20attack%20methods%20still%20face%20challenges%20in%20generating%20content%20that%0Aaligns%20with%20an%20attacker%27s%20intent%20while%20incurring%20high%20computational%20costs%20to%0Aidentify%20successful%20prompts.%20To%20address%20these%20challenges%2C%20we%20propose%20ZIUM%2C%20a%0AZero-shot%20Intent-aware%20adversarial%20attack%20on%20Unlearned%20Models%2C%20which%20enables%0Athe%20flexible%20customization%20of%20target%20attack%20images%20to%20reflect%20an%20attacker%27s%0Aintent.%20Additionally%2C%20ZIUM%20supports%20zero-shot%20adversarial%20attacks%20without%0Arequiring%20further%20optimization%20for%20previously%20attacked%20unlearned%20concepts.%20The%0Aevaluation%20across%20various%20MU%20scenarios%20demonstrated%20ZIUM%27s%20effectiveness%20in%0Asuccessfully%20customizing%20content%20based%20on%20user-intent%20prompts%20while%20achieving%20a%0Asuperior%20attack%20success%20rate%20compared%20to%20existing%20methods.%20Moreover%2C%20its%0Azero-shot%20adversarial%20attack%20significantly%20reduces%20the%20attack%20time%20for%0Apreviously%20attacked%20unlearned%20concepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21985v1&entry.124074799=Read"},
{"title": "A Deep Learning Pipeline Using Synthetic Data to Improve Interpretation\n  of Paper ECG Images", "author": "Xiaoyu Wang and Ramesh Nadarajah and Zhiqiang Zhang and David Wong", "abstract": "  Cardiovascular diseases (CVDs) are the leading global cause of death, and\nearly detection is essential to improve patient outcomes. Electrocardiograms\n(ECGs), especially 12-lead ECGs, play a key role in the identification of CVDs.\nThese are routinely interpreted by human experts, a process that is\ntime-consuming and requires expert knowledge. Historical research in this area\nhas focused on automatic ECG interpretation from digital signals, with recent\ndeep learning approaches achieving strong results. In practice, however, most\nECG data in clinical practice are stored or shared in image form. To bridge\nthis gap, we propose a deep learning framework designed specifically to\nclassify paper-like ECG images into five main diagnostic categories. Our method\nwas the winning entry to the 2024 British Heart Foundation Open Data Science\nChallenge. It addresses two main challenges of paper ECG classification: visual\nnoise (e.g., shadows or creases) and the need to detect fine-detailed waveform\npatterns. We propose a pre-processing pipeline that reduces visual noise and a\ntwo-stage fine-tuning strategy: the model is first fine-tuned on synthetic and\nexternal ECG image datasets to learn domain-specific features, and then further\nfine-tuned on the target dataset to enhance disease-specific recognition. We\nadopt the ConvNeXt architecture as the backbone of our model. Our method\nachieved AUROC scores of 0.9688 on the public validation set and 0.9677 on the\nprivate test set of the British Heart Foundation Open Data Science Challenge,\nhighlighting its potential as a practical tool for automated ECG interpretation\nin clinical workflows.\n", "link": "http://arxiv.org/abs/2507.21968v1", "date": "2025-07-29", "relevancy": 1.4601, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4883}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Pipeline%20Using%20Synthetic%20Data%20to%20Improve%20Interpretation%0A%20%20of%20Paper%20ECG%20Images&body=Title%3A%20A%20Deep%20Learning%20Pipeline%20Using%20Synthetic%20Data%20to%20Improve%20Interpretation%0A%20%20of%20Paper%20ECG%20Images%0AAuthor%3A%20Xiaoyu%20Wang%20and%20Ramesh%20Nadarajah%20and%20Zhiqiang%20Zhang%20and%20David%20Wong%0AAbstract%3A%20%20%20Cardiovascular%20diseases%20%28CVDs%29%20are%20the%20leading%20global%20cause%20of%20death%2C%20and%0Aearly%20detection%20is%20essential%20to%20improve%20patient%20outcomes.%20Electrocardiograms%0A%28ECGs%29%2C%20especially%2012-lead%20ECGs%2C%20play%20a%20key%20role%20in%20the%20identification%20of%20CVDs.%0AThese%20are%20routinely%20interpreted%20by%20human%20experts%2C%20a%20process%20that%20is%0Atime-consuming%20and%20requires%20expert%20knowledge.%20Historical%20research%20in%20this%20area%0Ahas%20focused%20on%20automatic%20ECG%20interpretation%20from%20digital%20signals%2C%20with%20recent%0Adeep%20learning%20approaches%20achieving%20strong%20results.%20In%20practice%2C%20however%2C%20most%0AECG%20data%20in%20clinical%20practice%20are%20stored%20or%20shared%20in%20image%20form.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20a%20deep%20learning%20framework%20designed%20specifically%20to%0Aclassify%20paper-like%20ECG%20images%20into%20five%20main%20diagnostic%20categories.%20Our%20method%0Awas%20the%20winning%20entry%20to%20the%202024%20British%20Heart%20Foundation%20Open%20Data%20Science%0AChallenge.%20It%20addresses%20two%20main%20challenges%20of%20paper%20ECG%20classification%3A%20visual%0Anoise%20%28e.g.%2C%20shadows%20or%20creases%29%20and%20the%20need%20to%20detect%20fine-detailed%20waveform%0Apatterns.%20We%20propose%20a%20pre-processing%20pipeline%20that%20reduces%20visual%20noise%20and%20a%0Atwo-stage%20fine-tuning%20strategy%3A%20the%20model%20is%20first%20fine-tuned%20on%20synthetic%20and%0Aexternal%20ECG%20image%20datasets%20to%20learn%20domain-specific%20features%2C%20and%20then%20further%0Afine-tuned%20on%20the%20target%20dataset%20to%20enhance%20disease-specific%20recognition.%20We%0Aadopt%20the%20ConvNeXt%20architecture%20as%20the%20backbone%20of%20our%20model.%20Our%20method%0Aachieved%20AUROC%20scores%20of%200.9688%20on%20the%20public%20validation%20set%20and%200.9677%20on%20the%0Aprivate%20test%20set%20of%20the%20British%20Heart%20Foundation%20Open%20Data%20Science%20Challenge%2C%0Ahighlighting%20its%20potential%20as%20a%20practical%20tool%20for%20automated%20ECG%20interpretation%0Ain%20clinical%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Learning%2520Pipeline%2520Using%2520Synthetic%2520Data%2520to%2520Improve%2520Interpretation%250A%2520%2520of%2520Paper%2520ECG%2520Images%26entry.906535625%3DXiaoyu%2520Wang%2520and%2520Ramesh%2520Nadarajah%2520and%2520Zhiqiang%2520Zhang%2520and%2520David%2520Wong%26entry.1292438233%3D%2520%2520Cardiovascular%2520diseases%2520%2528CVDs%2529%2520are%2520the%2520leading%2520global%2520cause%2520of%2520death%252C%2520and%250Aearly%2520detection%2520is%2520essential%2520to%2520improve%2520patient%2520outcomes.%2520Electrocardiograms%250A%2528ECGs%2529%252C%2520especially%252012-lead%2520ECGs%252C%2520play%2520a%2520key%2520role%2520in%2520the%2520identification%2520of%2520CVDs.%250AThese%2520are%2520routinely%2520interpreted%2520by%2520human%2520experts%252C%2520a%2520process%2520that%2520is%250Atime-consuming%2520and%2520requires%2520expert%2520knowledge.%2520Historical%2520research%2520in%2520this%2520area%250Ahas%2520focused%2520on%2520automatic%2520ECG%2520interpretation%2520from%2520digital%2520signals%252C%2520with%2520recent%250Adeep%2520learning%2520approaches%2520achieving%2520strong%2520results.%2520In%2520practice%252C%2520however%252C%2520most%250AECG%2520data%2520in%2520clinical%2520practice%2520are%2520stored%2520or%2520shared%2520in%2520image%2520form.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520propose%2520a%2520deep%2520learning%2520framework%2520designed%2520specifically%2520to%250Aclassify%2520paper-like%2520ECG%2520images%2520into%2520five%2520main%2520diagnostic%2520categories.%2520Our%2520method%250Awas%2520the%2520winning%2520entry%2520to%2520the%25202024%2520British%2520Heart%2520Foundation%2520Open%2520Data%2520Science%250AChallenge.%2520It%2520addresses%2520two%2520main%2520challenges%2520of%2520paper%2520ECG%2520classification%253A%2520visual%250Anoise%2520%2528e.g.%252C%2520shadows%2520or%2520creases%2529%2520and%2520the%2520need%2520to%2520detect%2520fine-detailed%2520waveform%250Apatterns.%2520We%2520propose%2520a%2520pre-processing%2520pipeline%2520that%2520reduces%2520visual%2520noise%2520and%2520a%250Atwo-stage%2520fine-tuning%2520strategy%253A%2520the%2520model%2520is%2520first%2520fine-tuned%2520on%2520synthetic%2520and%250Aexternal%2520ECG%2520image%2520datasets%2520to%2520learn%2520domain-specific%2520features%252C%2520and%2520then%2520further%250Afine-tuned%2520on%2520the%2520target%2520dataset%2520to%2520enhance%2520disease-specific%2520recognition.%2520We%250Aadopt%2520the%2520ConvNeXt%2520architecture%2520as%2520the%2520backbone%2520of%2520our%2520model.%2520Our%2520method%250Aachieved%2520AUROC%2520scores%2520of%25200.9688%2520on%2520the%2520public%2520validation%2520set%2520and%25200.9677%2520on%2520the%250Aprivate%2520test%2520set%2520of%2520the%2520British%2520Heart%2520Foundation%2520Open%2520Data%2520Science%2520Challenge%252C%250Ahighlighting%2520its%2520potential%2520as%2520a%2520practical%2520tool%2520for%2520automated%2520ECG%2520interpretation%250Ain%2520clinical%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Pipeline%20Using%20Synthetic%20Data%20to%20Improve%20Interpretation%0A%20%20of%20Paper%20ECG%20Images&entry.906535625=Xiaoyu%20Wang%20and%20Ramesh%20Nadarajah%20and%20Zhiqiang%20Zhang%20and%20David%20Wong&entry.1292438233=%20%20Cardiovascular%20diseases%20%28CVDs%29%20are%20the%20leading%20global%20cause%20of%20death%2C%20and%0Aearly%20detection%20is%20essential%20to%20improve%20patient%20outcomes.%20Electrocardiograms%0A%28ECGs%29%2C%20especially%2012-lead%20ECGs%2C%20play%20a%20key%20role%20in%20the%20identification%20of%20CVDs.%0AThese%20are%20routinely%20interpreted%20by%20human%20experts%2C%20a%20process%20that%20is%0Atime-consuming%20and%20requires%20expert%20knowledge.%20Historical%20research%20in%20this%20area%0Ahas%20focused%20on%20automatic%20ECG%20interpretation%20from%20digital%20signals%2C%20with%20recent%0Adeep%20learning%20approaches%20achieving%20strong%20results.%20In%20practice%2C%20however%2C%20most%0AECG%20data%20in%20clinical%20practice%20are%20stored%20or%20shared%20in%20image%20form.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20a%20deep%20learning%20framework%20designed%20specifically%20to%0Aclassify%20paper-like%20ECG%20images%20into%20five%20main%20diagnostic%20categories.%20Our%20method%0Awas%20the%20winning%20entry%20to%20the%202024%20British%20Heart%20Foundation%20Open%20Data%20Science%0AChallenge.%20It%20addresses%20two%20main%20challenges%20of%20paper%20ECG%20classification%3A%20visual%0Anoise%20%28e.g.%2C%20shadows%20or%20creases%29%20and%20the%20need%20to%20detect%20fine-detailed%20waveform%0Apatterns.%20We%20propose%20a%20pre-processing%20pipeline%20that%20reduces%20visual%20noise%20and%20a%0Atwo-stage%20fine-tuning%20strategy%3A%20the%20model%20is%20first%20fine-tuned%20on%20synthetic%20and%0Aexternal%20ECG%20image%20datasets%20to%20learn%20domain-specific%20features%2C%20and%20then%20further%0Afine-tuned%20on%20the%20target%20dataset%20to%20enhance%20disease-specific%20recognition.%20We%0Aadopt%20the%20ConvNeXt%20architecture%20as%20the%20backbone%20of%20our%20model.%20Our%20method%0Aachieved%20AUROC%20scores%20of%200.9688%20on%20the%20public%20validation%20set%20and%200.9677%20on%20the%0Aprivate%20test%20set%20of%20the%20British%20Heart%20Foundation%20Open%20Data%20Science%20Challenge%2C%0Ahighlighting%20its%20potential%20as%20a%20practical%20tool%20for%20automated%20ECG%20interpretation%0Ain%20clinical%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21968v1&entry.124074799=Read"},
{"title": "Structure-Informed Deep Reinforcement Learning for Inventory Management", "author": "Alvaro Maggiar and Sohrab Andaz and Akhil Bagaria and Carson Eisenach and Dean Foster and Omer Gottesman and Dominique Perrault-Joncas", "abstract": "  This paper investigates the application of Deep Reinforcement Learning (DRL)\nto classical inventory management problems, with a focus on practical\nimplementation considerations. We apply a DRL algorithm based on DirectBackprop\nto several fundamental inventory management scenarios including multi-period\nsystems with lost sales (with and without lead times), perishable inventory\nmanagement, dual sourcing, and joint inventory procurement and removal. The DRL\napproach learns policies across products using only historical information that\nwould be available in practice, avoiding unrealistic assumptions about demand\ndistributions or access to distribution parameters. We demonstrate that our\ngeneric DRL implementation performs competitively against or outperforms\nestablished benchmarks and heuristics across these diverse settings, while\nrequiring minimal parameter tuning. Through examination of the learned\npolicies, we show that the DRL approach naturally captures many known\nstructural properties of optimal policies derived from traditional operations\nresearch methods. To further improve policy performance and interpretability,\nwe propose a Structure-Informed Policy Network technique that explicitly\nincorporates analytically-derived characteristics of optimal policies into the\nlearning process. This approach can help interpretability and add robustness to\nthe policy in out-of-sample performance, as we demonstrate in an example with\nrealistic demand data. Finally, we provide an illustrative application of DRL\nin a non-stationary setting. Our work bridges the gap between data-driven\nlearning and analytical insights in inventory management while maintaining\npractical applicability.\n", "link": "http://arxiv.org/abs/2507.22040v1", "date": "2025-07-29", "relevancy": 1.4047, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4743}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Informed%20Deep%20Reinforcement%20Learning%20for%20Inventory%20Management&body=Title%3A%20Structure-Informed%20Deep%20Reinforcement%20Learning%20for%20Inventory%20Management%0AAuthor%3A%20Alvaro%20Maggiar%20and%20Sohrab%20Andaz%20and%20Akhil%20Bagaria%20and%20Carson%20Eisenach%20and%20Dean%20Foster%20and%20Omer%20Gottesman%20and%20Dominique%20Perrault-Joncas%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20application%20of%20Deep%20Reinforcement%20Learning%20%28DRL%29%0Ato%20classical%20inventory%20management%20problems%2C%20with%20a%20focus%20on%20practical%0Aimplementation%20considerations.%20We%20apply%20a%20DRL%20algorithm%20based%20on%20DirectBackprop%0Ato%20several%20fundamental%20inventory%20management%20scenarios%20including%20multi-period%0Asystems%20with%20lost%20sales%20%28with%20and%20without%20lead%20times%29%2C%20perishable%20inventory%0Amanagement%2C%20dual%20sourcing%2C%20and%20joint%20inventory%20procurement%20and%20removal.%20The%20DRL%0Aapproach%20learns%20policies%20across%20products%20using%20only%20historical%20information%20that%0Awould%20be%20available%20in%20practice%2C%20avoiding%20unrealistic%20assumptions%20about%20demand%0Adistributions%20or%20access%20to%20distribution%20parameters.%20We%20demonstrate%20that%20our%0Ageneric%20DRL%20implementation%20performs%20competitively%20against%20or%20outperforms%0Aestablished%20benchmarks%20and%20heuristics%20across%20these%20diverse%20settings%2C%20while%0Arequiring%20minimal%20parameter%20tuning.%20Through%20examination%20of%20the%20learned%0Apolicies%2C%20we%20show%20that%20the%20DRL%20approach%20naturally%20captures%20many%20known%0Astructural%20properties%20of%20optimal%20policies%20derived%20from%20traditional%20operations%0Aresearch%20methods.%20To%20further%20improve%20policy%20performance%20and%20interpretability%2C%0Awe%20propose%20a%20Structure-Informed%20Policy%20Network%20technique%20that%20explicitly%0Aincorporates%20analytically-derived%20characteristics%20of%20optimal%20policies%20into%20the%0Alearning%20process.%20This%20approach%20can%20help%20interpretability%20and%20add%20robustness%20to%0Athe%20policy%20in%20out-of-sample%20performance%2C%20as%20we%20demonstrate%20in%20an%20example%20with%0Arealistic%20demand%20data.%20Finally%2C%20we%20provide%20an%20illustrative%20application%20of%20DRL%0Ain%20a%20non-stationary%20setting.%20Our%20work%20bridges%20the%20gap%20between%20data-driven%0Alearning%20and%20analytical%20insights%20in%20inventory%20management%20while%20maintaining%0Apractical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Informed%2520Deep%2520Reinforcement%2520Learning%2520for%2520Inventory%2520Management%26entry.906535625%3DAlvaro%2520Maggiar%2520and%2520Sohrab%2520Andaz%2520and%2520Akhil%2520Bagaria%2520and%2520Carson%2520Eisenach%2520and%2520Dean%2520Foster%2520and%2520Omer%2520Gottesman%2520and%2520Dominique%2520Perrault-Joncas%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520application%2520of%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%250Ato%2520classical%2520inventory%2520management%2520problems%252C%2520with%2520a%2520focus%2520on%2520practical%250Aimplementation%2520considerations.%2520We%2520apply%2520a%2520DRL%2520algorithm%2520based%2520on%2520DirectBackprop%250Ato%2520several%2520fundamental%2520inventory%2520management%2520scenarios%2520including%2520multi-period%250Asystems%2520with%2520lost%2520sales%2520%2528with%2520and%2520without%2520lead%2520times%2529%252C%2520perishable%2520inventory%250Amanagement%252C%2520dual%2520sourcing%252C%2520and%2520joint%2520inventory%2520procurement%2520and%2520removal.%2520The%2520DRL%250Aapproach%2520learns%2520policies%2520across%2520products%2520using%2520only%2520historical%2520information%2520that%250Awould%2520be%2520available%2520in%2520practice%252C%2520avoiding%2520unrealistic%2520assumptions%2520about%2520demand%250Adistributions%2520or%2520access%2520to%2520distribution%2520parameters.%2520We%2520demonstrate%2520that%2520our%250Ageneric%2520DRL%2520implementation%2520performs%2520competitively%2520against%2520or%2520outperforms%250Aestablished%2520benchmarks%2520and%2520heuristics%2520across%2520these%2520diverse%2520settings%252C%2520while%250Arequiring%2520minimal%2520parameter%2520tuning.%2520Through%2520examination%2520of%2520the%2520learned%250Apolicies%252C%2520we%2520show%2520that%2520the%2520DRL%2520approach%2520naturally%2520captures%2520many%2520known%250Astructural%2520properties%2520of%2520optimal%2520policies%2520derived%2520from%2520traditional%2520operations%250Aresearch%2520methods.%2520To%2520further%2520improve%2520policy%2520performance%2520and%2520interpretability%252C%250Awe%2520propose%2520a%2520Structure-Informed%2520Policy%2520Network%2520technique%2520that%2520explicitly%250Aincorporates%2520analytically-derived%2520characteristics%2520of%2520optimal%2520policies%2520into%2520the%250Alearning%2520process.%2520This%2520approach%2520can%2520help%2520interpretability%2520and%2520add%2520robustness%2520to%250Athe%2520policy%2520in%2520out-of-sample%2520performance%252C%2520as%2520we%2520demonstrate%2520in%2520an%2520example%2520with%250Arealistic%2520demand%2520data.%2520Finally%252C%2520we%2520provide%2520an%2520illustrative%2520application%2520of%2520DRL%250Ain%2520a%2520non-stationary%2520setting.%2520Our%2520work%2520bridges%2520the%2520gap%2520between%2520data-driven%250Alearning%2520and%2520analytical%2520insights%2520in%2520inventory%2520management%2520while%2520maintaining%250Apractical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Informed%20Deep%20Reinforcement%20Learning%20for%20Inventory%20Management&entry.906535625=Alvaro%20Maggiar%20and%20Sohrab%20Andaz%20and%20Akhil%20Bagaria%20and%20Carson%20Eisenach%20and%20Dean%20Foster%20and%20Omer%20Gottesman%20and%20Dominique%20Perrault-Joncas&entry.1292438233=%20%20This%20paper%20investigates%20the%20application%20of%20Deep%20Reinforcement%20Learning%20%28DRL%29%0Ato%20classical%20inventory%20management%20problems%2C%20with%20a%20focus%20on%20practical%0Aimplementation%20considerations.%20We%20apply%20a%20DRL%20algorithm%20based%20on%20DirectBackprop%0Ato%20several%20fundamental%20inventory%20management%20scenarios%20including%20multi-period%0Asystems%20with%20lost%20sales%20%28with%20and%20without%20lead%20times%29%2C%20perishable%20inventory%0Amanagement%2C%20dual%20sourcing%2C%20and%20joint%20inventory%20procurement%20and%20removal.%20The%20DRL%0Aapproach%20learns%20policies%20across%20products%20using%20only%20historical%20information%20that%0Awould%20be%20available%20in%20practice%2C%20avoiding%20unrealistic%20assumptions%20about%20demand%0Adistributions%20or%20access%20to%20distribution%20parameters.%20We%20demonstrate%20that%20our%0Ageneric%20DRL%20implementation%20performs%20competitively%20against%20or%20outperforms%0Aestablished%20benchmarks%20and%20heuristics%20across%20these%20diverse%20settings%2C%20while%0Arequiring%20minimal%20parameter%20tuning.%20Through%20examination%20of%20the%20learned%0Apolicies%2C%20we%20show%20that%20the%20DRL%20approach%20naturally%20captures%20many%20known%0Astructural%20properties%20of%20optimal%20policies%20derived%20from%20traditional%20operations%0Aresearch%20methods.%20To%20further%20improve%20policy%20performance%20and%20interpretability%2C%0Awe%20propose%20a%20Structure-Informed%20Policy%20Network%20technique%20that%20explicitly%0Aincorporates%20analytically-derived%20characteristics%20of%20optimal%20policies%20into%20the%0Alearning%20process.%20This%20approach%20can%20help%20interpretability%20and%20add%20robustness%20to%0Athe%20policy%20in%20out-of-sample%20performance%2C%20as%20we%20demonstrate%20in%20an%20example%20with%0Arealistic%20demand%20data.%20Finally%2C%20we%20provide%20an%20illustrative%20application%20of%20DRL%0Ain%20a%20non-stationary%20setting.%20Our%20work%20bridges%20the%20gap%20between%20data-driven%0Alearning%20and%20analytical%20insights%20in%20inventory%20management%20while%20maintaining%0Apractical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22040v1&entry.124074799=Read"},
{"title": "Higher-Order Kuramoto Oscillator Network for Dense Associative Memory", "author": "Jona Nagerl and Natalia G. Berloff", "abstract": "  Networks of phase oscillators can serve as dense associative memories if they\nincorporate higher-order coupling beyond the classical Kuramoto model's\npairwise interactions. Here we introduce a generalized Kuramoto model with\ncombined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling,\ninspired by dense Hopfield memory theory. Using mean-field theory and its\ndynamical approximation, we obtain a phase diagram for dense associative memory\nmodel that exhibits a tricritical point at which the continuous onset of memory\nretrieval is supplanted by a discontinuous, hysteretic transition. In the\nquartic-dominated regime, the system supports bistable phase-locked states\ncorresponding to stored memory patterns, with a sizable energy barrier between\nmemory and incoherent states. We analytically determine this bistable region\nand show that the escape time from a memory state (due to noise) grows\nexponentially with network size, indicating robust storage. Extending the\ntheory to finite memory load, we show that higher-order couplings achieve\nsuperlinear scaling of memory capacity with system size, far exceeding the\nlimit of pairwise-only oscillators. Large-scale simulations of the oscillator\nnetwork confirm our theoretical predictions, demonstrating rapid pattern\nretrieval and robust storage of many phase patterns. These results bridge the\nKuramoto synchronization with modern Hopfield memories, pointing toward\nexperimental realization of high-capacity, analog associative memory in\noscillator systems.\n", "link": "http://arxiv.org/abs/2507.21984v1", "date": "2025-07-29", "relevancy": 1.1336, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3812}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3811}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Kuramoto%20Oscillator%20Network%20for%20Dense%20Associative%20Memory&body=Title%3A%20Higher-Order%20Kuramoto%20Oscillator%20Network%20for%20Dense%20Associative%20Memory%0AAuthor%3A%20Jona%20Nagerl%20and%20Natalia%20G.%20Berloff%0AAbstract%3A%20%20%20Networks%20of%20phase%20oscillators%20can%20serve%20as%20dense%20associative%20memories%20if%20they%0Aincorporate%20higher-order%20coupling%20beyond%20the%20classical%20Kuramoto%20model%27s%0Apairwise%20interactions.%20Here%20we%20introduce%20a%20generalized%20Kuramoto%20model%20with%0Acombined%20second-harmonic%20%28pairwise%29%20and%20fourth-harmonic%20%28quartic%29%20coupling%2C%0Ainspired%20by%20dense%20Hopfield%20memory%20theory.%20Using%20mean-field%20theory%20and%20its%0Adynamical%20approximation%2C%20we%20obtain%20a%20phase%20diagram%20for%20dense%20associative%20memory%0Amodel%20that%20exhibits%20a%20tricritical%20point%20at%20which%20the%20continuous%20onset%20of%20memory%0Aretrieval%20is%20supplanted%20by%20a%20discontinuous%2C%20hysteretic%20transition.%20In%20the%0Aquartic-dominated%20regime%2C%20the%20system%20supports%20bistable%20phase-locked%20states%0Acorresponding%20to%20stored%20memory%20patterns%2C%20with%20a%20sizable%20energy%20barrier%20between%0Amemory%20and%20incoherent%20states.%20We%20analytically%20determine%20this%20bistable%20region%0Aand%20show%20that%20the%20escape%20time%20from%20a%20memory%20state%20%28due%20to%20noise%29%20grows%0Aexponentially%20with%20network%20size%2C%20indicating%20robust%20storage.%20Extending%20the%0Atheory%20to%20finite%20memory%20load%2C%20we%20show%20that%20higher-order%20couplings%20achieve%0Asuperlinear%20scaling%20of%20memory%20capacity%20with%20system%20size%2C%20far%20exceeding%20the%0Alimit%20of%20pairwise-only%20oscillators.%20Large-scale%20simulations%20of%20the%20oscillator%0Anetwork%20confirm%20our%20theoretical%20predictions%2C%20demonstrating%20rapid%20pattern%0Aretrieval%20and%20robust%20storage%20of%20many%20phase%20patterns.%20These%20results%20bridge%20the%0AKuramoto%20synchronization%20with%20modern%20Hopfield%20memories%2C%20pointing%20toward%0Aexperimental%20realization%20of%20high-capacity%2C%20analog%20associative%20memory%20in%0Aoscillator%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Kuramoto%2520Oscillator%2520Network%2520for%2520Dense%2520Associative%2520Memory%26entry.906535625%3DJona%2520Nagerl%2520and%2520Natalia%2520G.%2520Berloff%26entry.1292438233%3D%2520%2520Networks%2520of%2520phase%2520oscillators%2520can%2520serve%2520as%2520dense%2520associative%2520memories%2520if%2520they%250Aincorporate%2520higher-order%2520coupling%2520beyond%2520the%2520classical%2520Kuramoto%2520model%2527s%250Apairwise%2520interactions.%2520Here%2520we%2520introduce%2520a%2520generalized%2520Kuramoto%2520model%2520with%250Acombined%2520second-harmonic%2520%2528pairwise%2529%2520and%2520fourth-harmonic%2520%2528quartic%2529%2520coupling%252C%250Ainspired%2520by%2520dense%2520Hopfield%2520memory%2520theory.%2520Using%2520mean-field%2520theory%2520and%2520its%250Adynamical%2520approximation%252C%2520we%2520obtain%2520a%2520phase%2520diagram%2520for%2520dense%2520associative%2520memory%250Amodel%2520that%2520exhibits%2520a%2520tricritical%2520point%2520at%2520which%2520the%2520continuous%2520onset%2520of%2520memory%250Aretrieval%2520is%2520supplanted%2520by%2520a%2520discontinuous%252C%2520hysteretic%2520transition.%2520In%2520the%250Aquartic-dominated%2520regime%252C%2520the%2520system%2520supports%2520bistable%2520phase-locked%2520states%250Acorresponding%2520to%2520stored%2520memory%2520patterns%252C%2520with%2520a%2520sizable%2520energy%2520barrier%2520between%250Amemory%2520and%2520incoherent%2520states.%2520We%2520analytically%2520determine%2520this%2520bistable%2520region%250Aand%2520show%2520that%2520the%2520escape%2520time%2520from%2520a%2520memory%2520state%2520%2528due%2520to%2520noise%2529%2520grows%250Aexponentially%2520with%2520network%2520size%252C%2520indicating%2520robust%2520storage.%2520Extending%2520the%250Atheory%2520to%2520finite%2520memory%2520load%252C%2520we%2520show%2520that%2520higher-order%2520couplings%2520achieve%250Asuperlinear%2520scaling%2520of%2520memory%2520capacity%2520with%2520system%2520size%252C%2520far%2520exceeding%2520the%250Alimit%2520of%2520pairwise-only%2520oscillators.%2520Large-scale%2520simulations%2520of%2520the%2520oscillator%250Anetwork%2520confirm%2520our%2520theoretical%2520predictions%252C%2520demonstrating%2520rapid%2520pattern%250Aretrieval%2520and%2520robust%2520storage%2520of%2520many%2520phase%2520patterns.%2520These%2520results%2520bridge%2520the%250AKuramoto%2520synchronization%2520with%2520modern%2520Hopfield%2520memories%252C%2520pointing%2520toward%250Aexperimental%2520realization%2520of%2520high-capacity%252C%2520analog%2520associative%2520memory%2520in%250Aoscillator%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Kuramoto%20Oscillator%20Network%20for%20Dense%20Associative%20Memory&entry.906535625=Jona%20Nagerl%20and%20Natalia%20G.%20Berloff&entry.1292438233=%20%20Networks%20of%20phase%20oscillators%20can%20serve%20as%20dense%20associative%20memories%20if%20they%0Aincorporate%20higher-order%20coupling%20beyond%20the%20classical%20Kuramoto%20model%27s%0Apairwise%20interactions.%20Here%20we%20introduce%20a%20generalized%20Kuramoto%20model%20with%0Acombined%20second-harmonic%20%28pairwise%29%20and%20fourth-harmonic%20%28quartic%29%20coupling%2C%0Ainspired%20by%20dense%20Hopfield%20memory%20theory.%20Using%20mean-field%20theory%20and%20its%0Adynamical%20approximation%2C%20we%20obtain%20a%20phase%20diagram%20for%20dense%20associative%20memory%0Amodel%20that%20exhibits%20a%20tricritical%20point%20at%20which%20the%20continuous%20onset%20of%20memory%0Aretrieval%20is%20supplanted%20by%20a%20discontinuous%2C%20hysteretic%20transition.%20In%20the%0Aquartic-dominated%20regime%2C%20the%20system%20supports%20bistable%20phase-locked%20states%0Acorresponding%20to%20stored%20memory%20patterns%2C%20with%20a%20sizable%20energy%20barrier%20between%0Amemory%20and%20incoherent%20states.%20We%20analytically%20determine%20this%20bistable%20region%0Aand%20show%20that%20the%20escape%20time%20from%20a%20memory%20state%20%28due%20to%20noise%29%20grows%0Aexponentially%20with%20network%20size%2C%20indicating%20robust%20storage.%20Extending%20the%0Atheory%20to%20finite%20memory%20load%2C%20we%20show%20that%20higher-order%20couplings%20achieve%0Asuperlinear%20scaling%20of%20memory%20capacity%20with%20system%20size%2C%20far%20exceeding%20the%0Alimit%20of%20pairwise-only%20oscillators.%20Large-scale%20simulations%20of%20the%20oscillator%0Anetwork%20confirm%20our%20theoretical%20predictions%2C%20demonstrating%20rapid%20pattern%0Aretrieval%20and%20robust%20storage%20of%20many%20phase%20patterns.%20These%20results%20bridge%20the%0AKuramoto%20synchronization%20with%20modern%20Hopfield%20memories%2C%20pointing%20toward%0Aexperimental%20realization%20of%20high-capacity%2C%20analog%20associative%20memory%20in%0Aoscillator%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21984v1&entry.124074799=Read"},
{"title": "Weight-Parameterization in Continuous Time Deep Neural Networks for\n  Surrogate Modeling", "author": "Haley Rosso and Lars Ruthotto and Khachik Sargsyan", "abstract": "  Continuous-time deep learning models, such as neural ordinary differential\nequations (ODEs), offer a promising framework for surrogate modeling of complex\nphysical systems. A central challenge in training these models lies in learning\nexpressive yet stable time-varying weights, particularly under computational\nconstraints. This work investigates weight parameterization strategies that\nconstrain the temporal evolution of weights to a low-dimensional subspace\nspanned by polynomial basis functions. We evaluate both monomial and Legendre\npolynomial bases within neural ODE and residual network (ResNet) architectures\nunder discretize-then-optimize and optimize-then-discretize training paradigms.\nExperimental results across three high-dimensional benchmark problems show that\nLegendre parameterizations yield more stable training dynamics, reduce\ncomputational cost, and achieve accuracy comparable to or better than both\nmonomial parameterizations and unconstrained weight models. These findings\nelucidate the role of basis choice in time-dependent weight parameterization\nand demonstrate that using orthogonal polynomial bases offers a favorable\ntradeoff between model expressivity and training efficiency.\n", "link": "http://arxiv.org/abs/2507.22045v1", "date": "2025-07-29", "relevancy": 0.9647, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4732}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weight-Parameterization%20in%20Continuous%20Time%20Deep%20Neural%20Networks%20for%0A%20%20Surrogate%20Modeling&body=Title%3A%20Weight-Parameterization%20in%20Continuous%20Time%20Deep%20Neural%20Networks%20for%0A%20%20Surrogate%20Modeling%0AAuthor%3A%20Haley%20Rosso%20and%20Lars%20Ruthotto%20and%20Khachik%20Sargsyan%0AAbstract%3A%20%20%20Continuous-time%20deep%20learning%20models%2C%20such%20as%20neural%20ordinary%20differential%0Aequations%20%28ODEs%29%2C%20offer%20a%20promising%20framework%20for%20surrogate%20modeling%20of%20complex%0Aphysical%20systems.%20A%20central%20challenge%20in%20training%20these%20models%20lies%20in%20learning%0Aexpressive%20yet%20stable%20time-varying%20weights%2C%20particularly%20under%20computational%0Aconstraints.%20This%20work%20investigates%20weight%20parameterization%20strategies%20that%0Aconstrain%20the%20temporal%20evolution%20of%20weights%20to%20a%20low-dimensional%20subspace%0Aspanned%20by%20polynomial%20basis%20functions.%20We%20evaluate%20both%20monomial%20and%20Legendre%0Apolynomial%20bases%20within%20neural%20ODE%20and%20residual%20network%20%28ResNet%29%20architectures%0Aunder%20discretize-then-optimize%20and%20optimize-then-discretize%20training%20paradigms.%0AExperimental%20results%20across%20three%20high-dimensional%20benchmark%20problems%20show%20that%0ALegendre%20parameterizations%20yield%20more%20stable%20training%20dynamics%2C%20reduce%0Acomputational%20cost%2C%20and%20achieve%20accuracy%20comparable%20to%20or%20better%20than%20both%0Amonomial%20parameterizations%20and%20unconstrained%20weight%20models.%20These%20findings%0Aelucidate%20the%20role%20of%20basis%20choice%20in%20time-dependent%20weight%20parameterization%0Aand%20demonstrate%20that%20using%20orthogonal%20polynomial%20bases%20offers%20a%20favorable%0Atradeoff%20between%20model%20expressivity%20and%20training%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeight-Parameterization%2520in%2520Continuous%2520Time%2520Deep%2520Neural%2520Networks%2520for%250A%2520%2520Surrogate%2520Modeling%26entry.906535625%3DHaley%2520Rosso%2520and%2520Lars%2520Ruthotto%2520and%2520Khachik%2520Sargsyan%26entry.1292438233%3D%2520%2520Continuous-time%2520deep%2520learning%2520models%252C%2520such%2520as%2520neural%2520ordinary%2520differential%250Aequations%2520%2528ODEs%2529%252C%2520offer%2520a%2520promising%2520framework%2520for%2520surrogate%2520modeling%2520of%2520complex%250Aphysical%2520systems.%2520A%2520central%2520challenge%2520in%2520training%2520these%2520models%2520lies%2520in%2520learning%250Aexpressive%2520yet%2520stable%2520time-varying%2520weights%252C%2520particularly%2520under%2520computational%250Aconstraints.%2520This%2520work%2520investigates%2520weight%2520parameterization%2520strategies%2520that%250Aconstrain%2520the%2520temporal%2520evolution%2520of%2520weights%2520to%2520a%2520low-dimensional%2520subspace%250Aspanned%2520by%2520polynomial%2520basis%2520functions.%2520We%2520evaluate%2520both%2520monomial%2520and%2520Legendre%250Apolynomial%2520bases%2520within%2520neural%2520ODE%2520and%2520residual%2520network%2520%2528ResNet%2529%2520architectures%250Aunder%2520discretize-then-optimize%2520and%2520optimize-then-discretize%2520training%2520paradigms.%250AExperimental%2520results%2520across%2520three%2520high-dimensional%2520benchmark%2520problems%2520show%2520that%250ALegendre%2520parameterizations%2520yield%2520more%2520stable%2520training%2520dynamics%252C%2520reduce%250Acomputational%2520cost%252C%2520and%2520achieve%2520accuracy%2520comparable%2520to%2520or%2520better%2520than%2520both%250Amonomial%2520parameterizations%2520and%2520unconstrained%2520weight%2520models.%2520These%2520findings%250Aelucidate%2520the%2520role%2520of%2520basis%2520choice%2520in%2520time-dependent%2520weight%2520parameterization%250Aand%2520demonstrate%2520that%2520using%2520orthogonal%2520polynomial%2520bases%2520offers%2520a%2520favorable%250Atradeoff%2520between%2520model%2520expressivity%2520and%2520training%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weight-Parameterization%20in%20Continuous%20Time%20Deep%20Neural%20Networks%20for%0A%20%20Surrogate%20Modeling&entry.906535625=Haley%20Rosso%20and%20Lars%20Ruthotto%20and%20Khachik%20Sargsyan&entry.1292438233=%20%20Continuous-time%20deep%20learning%20models%2C%20such%20as%20neural%20ordinary%20differential%0Aequations%20%28ODEs%29%2C%20offer%20a%20promising%20framework%20for%20surrogate%20modeling%20of%20complex%0Aphysical%20systems.%20A%20central%20challenge%20in%20training%20these%20models%20lies%20in%20learning%0Aexpressive%20yet%20stable%20time-varying%20weights%2C%20particularly%20under%20computational%0Aconstraints.%20This%20work%20investigates%20weight%20parameterization%20strategies%20that%0Aconstrain%20the%20temporal%20evolution%20of%20weights%20to%20a%20low-dimensional%20subspace%0Aspanned%20by%20polynomial%20basis%20functions.%20We%20evaluate%20both%20monomial%20and%20Legendre%0Apolynomial%20bases%20within%20neural%20ODE%20and%20residual%20network%20%28ResNet%29%20architectures%0Aunder%20discretize-then-optimize%20and%20optimize-then-discretize%20training%20paradigms.%0AExperimental%20results%20across%20three%20high-dimensional%20benchmark%20problems%20show%20that%0ALegendre%20parameterizations%20yield%20more%20stable%20training%20dynamics%2C%20reduce%0Acomputational%20cost%2C%20and%20achieve%20accuracy%20comparable%20to%20or%20better%20than%20both%0Amonomial%20parameterizations%20and%20unconstrained%20weight%20models.%20These%20findings%0Aelucidate%20the%20role%20of%20basis%20choice%20in%20time-dependent%20weight%20parameterization%0Aand%20demonstrate%20that%20using%20orthogonal%20polynomial%20bases%20offers%20a%20favorable%0Atradeoff%20between%20model%20expressivity%20and%20training%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22045v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


