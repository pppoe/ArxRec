<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240909.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning", "author": "Artemis Panagopoulou and Le Xue and Ning Yu and Junnan Li and Dongxu Li and Shafiq Joty and Ran Xu and Silvio Savarese and Caiming Xiong and Juan Carlos Niebles", "abstract": "  Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.\n", "link": "http://arxiv.org/abs/2311.18799v2", "date": "2024-09-09", "relevancy": 3.2084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-InstructBLIP%3A%20A%20Framework%20for%20aligning%20X-Modal%20instruction-aware%0A%20%20representations%20to%20LLMs%20and%20Emergent%20Cross-modal%20Reasoning&body=Title%3A%20X-InstructBLIP%3A%20A%20Framework%20for%20aligning%20X-Modal%20instruction-aware%0A%20%20representations%20to%20LLMs%20and%20Emergent%20Cross-modal%20Reasoning%0AAuthor%3A%20Artemis%20Panagopoulou%20and%20Le%20Xue%20and%20Ning%20Yu%20and%20Junnan%20Li%20and%20Dongxu%20Li%20and%20Shafiq%20Joty%20and%20Ran%20Xu%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Juan%20Carlos%20Niebles%0AAbstract%3A%20%20%20Recent%20research%20has%20achieved%20significant%20advancements%20in%20visual%20reasoning%0Atasks%20through%20learning%20image-to-language%20projections%20and%20leveraging%20the%0Aimpressive%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20This%20paper%0Aintroduces%20an%20efficient%20and%20effective%20framework%20that%20integrates%20multiple%0Amodalities%20%28images%2C%203D%2C%20audio%20and%20video%29%20to%20a%20frozen%20LLM%20and%20demonstrates%20an%0Aemergent%20ability%20for%20cross-modal%20reasoning%20%282%2B%20modality%20inputs%29.%20Our%20approach%0Aexplores%20two%20distinct%20projection%20mechanisms%3A%20Q-Formers%20and%20Linear%20Projections%0A%28LPs%29.%20Through%20extensive%20experimentation%20across%20all%20four%20modalities%20on%2016%0Abenchmarks%2C%20we%20explore%20both%20methods%20and%20assess%20their%20adaptability%20in%20integrated%0Aand%20separate%20cross-modal%20reasoning.%20The%20Q-Former%20projection%20demonstrates%0Asuperior%20performance%20in%20single%20modality%20scenarios%20and%20adaptability%20in%20joint%0Aversus%20discriminative%20reasoning%20involving%20two%20or%20more%20modalities.%20However%2C%20it%0Aexhibits%20lower%20generalization%20capabilities%20than%20linear%20projection%20in%20contexts%0Awhere%20task-modality%20data%20are%20limited.%20To%20enable%20this%20framework%2C%20we%20devise%20a%0Ascalable%20pipeline%20that%20automatically%20generates%20high-quality%2C%20instruction-tuning%0Adatasets%20from%20readily%20available%20captioning%20data%20across%20different%20modalities%2C%0Aand%20contribute%2024K%20QA%20data%20for%20audio%20and%20250K%20QA%20data%20for%203D.%20To%20facilitate%0Afurther%20research%20in%20cross-modal%20reasoning%2C%20we%20introduce%20the%20DisCRn%0A%28Discriminative%20Cross-modal%20Reasoning%29%20benchmark%20comprising%209K%20audio-video%20QA%0Asamples%20and%2028K%20image-3D%20QA%20samples%20that%20require%20the%20model%20to%20reason%0Adiscriminatively%20across%20disparate%20input%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-InstructBLIP%253A%2520A%2520Framework%2520for%2520aligning%2520X-Modal%2520instruction-aware%250A%2520%2520representations%2520to%2520LLMs%2520and%2520Emergent%2520Cross-modal%2520Reasoning%26entry.906535625%3DArtemis%2520Panagopoulou%2520and%2520Le%2520Xue%2520and%2520Ning%2520Yu%2520and%2520Junnan%2520Li%2520and%2520Dongxu%2520Li%2520and%2520Shafiq%2520Joty%2520and%2520Ran%2520Xu%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Juan%2520Carlos%2520Niebles%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520achieved%2520significant%2520advancements%2520in%2520visual%2520reasoning%250Atasks%2520through%2520learning%2520image-to-language%2520projections%2520and%2520leveraging%2520the%250Aimpressive%2520reasoning%2520abilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520This%2520paper%250Aintroduces%2520an%2520efficient%2520and%2520effective%2520framework%2520that%2520integrates%2520multiple%250Amodalities%2520%2528images%252C%25203D%252C%2520audio%2520and%2520video%2529%2520to%2520a%2520frozen%2520LLM%2520and%2520demonstrates%2520an%250Aemergent%2520ability%2520for%2520cross-modal%2520reasoning%2520%25282%252B%2520modality%2520inputs%2529.%2520Our%2520approach%250Aexplores%2520two%2520distinct%2520projection%2520mechanisms%253A%2520Q-Formers%2520and%2520Linear%2520Projections%250A%2528LPs%2529.%2520Through%2520extensive%2520experimentation%2520across%2520all%2520four%2520modalities%2520on%252016%250Abenchmarks%252C%2520we%2520explore%2520both%2520methods%2520and%2520assess%2520their%2520adaptability%2520in%2520integrated%250Aand%2520separate%2520cross-modal%2520reasoning.%2520The%2520Q-Former%2520projection%2520demonstrates%250Asuperior%2520performance%2520in%2520single%2520modality%2520scenarios%2520and%2520adaptability%2520in%2520joint%250Aversus%2520discriminative%2520reasoning%2520involving%2520two%2520or%2520more%2520modalities.%2520However%252C%2520it%250Aexhibits%2520lower%2520generalization%2520capabilities%2520than%2520linear%2520projection%2520in%2520contexts%250Awhere%2520task-modality%2520data%2520are%2520limited.%2520To%2520enable%2520this%2520framework%252C%2520we%2520devise%2520a%250Ascalable%2520pipeline%2520that%2520automatically%2520generates%2520high-quality%252C%2520instruction-tuning%250Adatasets%2520from%2520readily%2520available%2520captioning%2520data%2520across%2520different%2520modalities%252C%250Aand%2520contribute%252024K%2520QA%2520data%2520for%2520audio%2520and%2520250K%2520QA%2520data%2520for%25203D.%2520To%2520facilitate%250Afurther%2520research%2520in%2520cross-modal%2520reasoning%252C%2520we%2520introduce%2520the%2520DisCRn%250A%2528Discriminative%2520Cross-modal%2520Reasoning%2529%2520benchmark%2520comprising%25209K%2520audio-video%2520QA%250Asamples%2520and%252028K%2520image-3D%2520QA%2520samples%2520that%2520require%2520the%2520model%2520to%2520reason%250Adiscriminatively%2520across%2520disparate%2520input%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-InstructBLIP%3A%20A%20Framework%20for%20aligning%20X-Modal%20instruction-aware%0A%20%20representations%20to%20LLMs%20and%20Emergent%20Cross-modal%20Reasoning&entry.906535625=Artemis%20Panagopoulou%20and%20Le%20Xue%20and%20Ning%20Yu%20and%20Junnan%20Li%20and%20Dongxu%20Li%20and%20Shafiq%20Joty%20and%20Ran%20Xu%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Juan%20Carlos%20Niebles&entry.1292438233=%20%20Recent%20research%20has%20achieved%20significant%20advancements%20in%20visual%20reasoning%0Atasks%20through%20learning%20image-to-language%20projections%20and%20leveraging%20the%0Aimpressive%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20This%20paper%0Aintroduces%20an%20efficient%20and%20effective%20framework%20that%20integrates%20multiple%0Amodalities%20%28images%2C%203D%2C%20audio%20and%20video%29%20to%20a%20frozen%20LLM%20and%20demonstrates%20an%0Aemergent%20ability%20for%20cross-modal%20reasoning%20%282%2B%20modality%20inputs%29.%20Our%20approach%0Aexplores%20two%20distinct%20projection%20mechanisms%3A%20Q-Formers%20and%20Linear%20Projections%0A%28LPs%29.%20Through%20extensive%20experimentation%20across%20all%20four%20modalities%20on%2016%0Abenchmarks%2C%20we%20explore%20both%20methods%20and%20assess%20their%20adaptability%20in%20integrated%0Aand%20separate%20cross-modal%20reasoning.%20The%20Q-Former%20projection%20demonstrates%0Asuperior%20performance%20in%20single%20modality%20scenarios%20and%20adaptability%20in%20joint%0Aversus%20discriminative%20reasoning%20involving%20two%20or%20more%20modalities.%20However%2C%20it%0Aexhibits%20lower%20generalization%20capabilities%20than%20linear%20projection%20in%20contexts%0Awhere%20task-modality%20data%20are%20limited.%20To%20enable%20this%20framework%2C%20we%20devise%20a%0Ascalable%20pipeline%20that%20automatically%20generates%20high-quality%2C%20instruction-tuning%0Adatasets%20from%20readily%20available%20captioning%20data%20across%20different%20modalities%2C%0Aand%20contribute%2024K%20QA%20data%20for%20audio%20and%20250K%20QA%20data%20for%203D.%20To%20facilitate%0Afurther%20research%20in%20cross-modal%20reasoning%2C%20we%20introduce%20the%20DisCRn%0A%28Discriminative%20Cross-modal%20Reasoning%29%20benchmark%20comprising%209K%20audio-video%20QA%0Asamples%20and%2028K%20image-3D%20QA%20samples%20that%20require%20the%20model%20to%20reason%0Adiscriminatively%20across%20disparate%20input%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18799v2&entry.124074799=Read"},
{"title": "GASP: Gaussian Splatting for Physic-Based Simulations", "author": "Piotr Borycki and Weronika Smolak and Joanna Waczy\u0144ska and Marcin Mazur and S\u0142awomir Tadeja and Przemys\u0142aw Spurek", "abstract": "  Physics simulation is paramount for modeling and utilization of 3D scenes in\nvarious real-world applications. However, its integration with state-of-the-art\n3D scene rendering techniques such as Gaussian Splatting (GS) remains\nchallenging. Existing models use additional meshing mechanisms, including\ntriangle or tetrahedron meshing, marching cubes, or cage meshes. As an\nalternative, we can modify the physics grounded Newtonian dynamics to align\nwith 3D Gaussian components. Current models take the first-order approximation\nof a deformation map, which locally approximates the dynamics by linear\ntransformations. In contrast, our Gaussian Splatting for Physics-Based\nSimulations (GASP) model uses such a map (without any modifications) and flat\nGaussian distributions, which are parameterized by three points (mesh faces).\nSubsequently, each 3D point (mesh face node) is treated as a discrete entity\nwithin a 3D space. Consequently, the problem of modeling Gaussian components is\nreduced to working with 3D points. Additionally, the information on mesh faces\ncan be used to incorporate further properties into the physics model,\nfacilitating the use of triangles. Resulting solution can be integrated into\nany physics engine that can be treated as a black box. As demonstrated in our\nstudies, the proposed model exhibits superior performance on a diverse range of\nbenchmark datasets designed for 3D object rendering.\n", "link": "http://arxiv.org/abs/2409.05819v1", "date": "2024-09-09", "relevancy": 3.106, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6293}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GASP%3A%20Gaussian%20Splatting%20for%20Physic-Based%20Simulations&body=Title%3A%20GASP%3A%20Gaussian%20Splatting%20for%20Physic-Based%20Simulations%0AAuthor%3A%20Piotr%20Borycki%20and%20Weronika%20Smolak%20and%20Joanna%20Waczy%C5%84ska%20and%20Marcin%20Mazur%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Physics%20simulation%20is%20paramount%20for%20modeling%20and%20utilization%20of%203D%20scenes%20in%0Avarious%20real-world%20applications.%20However%2C%20its%20integration%20with%20state-of-the-art%0A3D%20scene%20rendering%20techniques%20such%20as%20Gaussian%20Splatting%20%28GS%29%20remains%0Achallenging.%20Existing%20models%20use%20additional%20meshing%20mechanisms%2C%20including%0Atriangle%20or%20tetrahedron%20meshing%2C%20marching%20cubes%2C%20or%20cage%20meshes.%20As%20an%0Aalternative%2C%20we%20can%20modify%20the%20physics%20grounded%20Newtonian%20dynamics%20to%20align%0Awith%203D%20Gaussian%20components.%20Current%20models%20take%20the%20first-order%20approximation%0Aof%20a%20deformation%20map%2C%20which%20locally%20approximates%20the%20dynamics%20by%20linear%0Atransformations.%20In%20contrast%2C%20our%20Gaussian%20Splatting%20for%20Physics-Based%0ASimulations%20%28GASP%29%20model%20uses%20such%20a%20map%20%28without%20any%20modifications%29%20and%20flat%0AGaussian%20distributions%2C%20which%20are%20parameterized%20by%20three%20points%20%28mesh%20faces%29.%0ASubsequently%2C%20each%203D%20point%20%28mesh%20face%20node%29%20is%20treated%20as%20a%20discrete%20entity%0Awithin%20a%203D%20space.%20Consequently%2C%20the%20problem%20of%20modeling%20Gaussian%20components%20is%0Areduced%20to%20working%20with%203D%20points.%20Additionally%2C%20the%20information%20on%20mesh%20faces%0Acan%20be%20used%20to%20incorporate%20further%20properties%20into%20the%20physics%20model%2C%0Afacilitating%20the%20use%20of%20triangles.%20Resulting%20solution%20can%20be%20integrated%20into%0Aany%20physics%20engine%20that%20can%20be%20treated%20as%20a%20black%20box.%20As%20demonstrated%20in%20our%0Astudies%2C%20the%20proposed%20model%20exhibits%20superior%20performance%20on%20a%20diverse%20range%20of%0Abenchmark%20datasets%20designed%20for%203D%20object%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGASP%253A%2520Gaussian%2520Splatting%2520for%2520Physic-Based%2520Simulations%26entry.906535625%3DPiotr%2520Borycki%2520and%2520Weronika%2520Smolak%2520and%2520Joanna%2520Waczy%25C5%2584ska%2520and%2520Marcin%2520Mazur%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Physics%2520simulation%2520is%2520paramount%2520for%2520modeling%2520and%2520utilization%2520of%25203D%2520scenes%2520in%250Avarious%2520real-world%2520applications.%2520However%252C%2520its%2520integration%2520with%2520state-of-the-art%250A3D%2520scene%2520rendering%2520techniques%2520such%2520as%2520Gaussian%2520Splatting%2520%2528GS%2529%2520remains%250Achallenging.%2520Existing%2520models%2520use%2520additional%2520meshing%2520mechanisms%252C%2520including%250Atriangle%2520or%2520tetrahedron%2520meshing%252C%2520marching%2520cubes%252C%2520or%2520cage%2520meshes.%2520As%2520an%250Aalternative%252C%2520we%2520can%2520modify%2520the%2520physics%2520grounded%2520Newtonian%2520dynamics%2520to%2520align%250Awith%25203D%2520Gaussian%2520components.%2520Current%2520models%2520take%2520the%2520first-order%2520approximation%250Aof%2520a%2520deformation%2520map%252C%2520which%2520locally%2520approximates%2520the%2520dynamics%2520by%2520linear%250Atransformations.%2520In%2520contrast%252C%2520our%2520Gaussian%2520Splatting%2520for%2520Physics-Based%250ASimulations%2520%2528GASP%2529%2520model%2520uses%2520such%2520a%2520map%2520%2528without%2520any%2520modifications%2529%2520and%2520flat%250AGaussian%2520distributions%252C%2520which%2520are%2520parameterized%2520by%2520three%2520points%2520%2528mesh%2520faces%2529.%250ASubsequently%252C%2520each%25203D%2520point%2520%2528mesh%2520face%2520node%2529%2520is%2520treated%2520as%2520a%2520discrete%2520entity%250Awithin%2520a%25203D%2520space.%2520Consequently%252C%2520the%2520problem%2520of%2520modeling%2520Gaussian%2520components%2520is%250Areduced%2520to%2520working%2520with%25203D%2520points.%2520Additionally%252C%2520the%2520information%2520on%2520mesh%2520faces%250Acan%2520be%2520used%2520to%2520incorporate%2520further%2520properties%2520into%2520the%2520physics%2520model%252C%250Afacilitating%2520the%2520use%2520of%2520triangles.%2520Resulting%2520solution%2520can%2520be%2520integrated%2520into%250Aany%2520physics%2520engine%2520that%2520can%2520be%2520treated%2520as%2520a%2520black%2520box.%2520As%2520demonstrated%2520in%2520our%250Astudies%252C%2520the%2520proposed%2520model%2520exhibits%2520superior%2520performance%2520on%2520a%2520diverse%2520range%2520of%250Abenchmark%2520datasets%2520designed%2520for%25203D%2520object%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GASP%3A%20Gaussian%20Splatting%20for%20Physic-Based%20Simulations&entry.906535625=Piotr%20Borycki%20and%20Weronika%20Smolak%20and%20Joanna%20Waczy%C5%84ska%20and%20Marcin%20Mazur%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Physics%20simulation%20is%20paramount%20for%20modeling%20and%20utilization%20of%203D%20scenes%20in%0Avarious%20real-world%20applications.%20However%2C%20its%20integration%20with%20state-of-the-art%0A3D%20scene%20rendering%20techniques%20such%20as%20Gaussian%20Splatting%20%28GS%29%20remains%0Achallenging.%20Existing%20models%20use%20additional%20meshing%20mechanisms%2C%20including%0Atriangle%20or%20tetrahedron%20meshing%2C%20marching%20cubes%2C%20or%20cage%20meshes.%20As%20an%0Aalternative%2C%20we%20can%20modify%20the%20physics%20grounded%20Newtonian%20dynamics%20to%20align%0Awith%203D%20Gaussian%20components.%20Current%20models%20take%20the%20first-order%20approximation%0Aof%20a%20deformation%20map%2C%20which%20locally%20approximates%20the%20dynamics%20by%20linear%0Atransformations.%20In%20contrast%2C%20our%20Gaussian%20Splatting%20for%20Physics-Based%0ASimulations%20%28GASP%29%20model%20uses%20such%20a%20map%20%28without%20any%20modifications%29%20and%20flat%0AGaussian%20distributions%2C%20which%20are%20parameterized%20by%20three%20points%20%28mesh%20faces%29.%0ASubsequently%2C%20each%203D%20point%20%28mesh%20face%20node%29%20is%20treated%20as%20a%20discrete%20entity%0Awithin%20a%203D%20space.%20Consequently%2C%20the%20problem%20of%20modeling%20Gaussian%20components%20is%0Areduced%20to%20working%20with%203D%20points.%20Additionally%2C%20the%20information%20on%20mesh%20faces%0Acan%20be%20used%20to%20incorporate%20further%20properties%20into%20the%20physics%20model%2C%0Afacilitating%20the%20use%20of%20triangles.%20Resulting%20solution%20can%20be%20integrated%20into%0Aany%20physics%20engine%20that%20can%20be%20treated%20as%20a%20black%20box.%20As%20demonstrated%20in%20our%0Astudies%2C%20the%20proposed%20model%20exhibits%20superior%20performance%20on%20a%20diverse%20range%20of%0Abenchmark%20datasets%20designed%20for%203D%20object%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05819v1&entry.124074799=Read"},
{"title": "TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields", "author": "Chengyao Duan and Zhiliu Yang", "abstract": "  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. This paper proposes a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, a tracking process and a mapping process, are\nmaintained simultaneously in our framework. In the tracking process, all input\nimages are uniformly sampled and then progressively trained in a\nself-supervised paradigm. In the mapping process, we leverage motion masks to\ndistinguish dynamic objects from the static background, and sample more pixels\nfrom dynamic areas. Secondly, the parameter optimization for both processes is\ncomprised of two stages: the first stage associates time with 3D positions to\nconvert the deformation field to the canonical field. The second stage\nassociates time with the embeddings of the canonical field to obtain colors and\na Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection\nstrategy based on the overlapping rate. Our approach is evaluated on two\nsynthetic datasets and one real-world dataset, and the experiments validate\nthat our method achieves competitive results in both tracking and mapping when\ncompared to existing state-of-the-art NeRF-based dynamic SLAM systems.\n", "link": "http://arxiv.org/abs/2310.18917v5", "date": "2024-09-09", "relevancy": 3.0941, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields&body=Title%3A%20TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields%0AAuthor%3A%20Chengyao%20Duan%20and%20Zhiliu%20Yang%0AAbstract%3A%20%20%20Previous%20attempts%20to%20integrate%20Neural%20Radiance%20Fields%20%28NeRF%29%20into%20the%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20framework%20either%20rely%20on%20the%0Aassumption%20of%20static%20scenes%20or%20require%20the%20ground%20truth%20camera%20poses%2C%20which%0Aimpedes%20their%20application%20in%20real-world%20scenarios.%20This%20paper%20proposes%20a%0Atime-varying%20representation%20to%20track%20and%20reconstruct%20the%20dynamic%20scenes.%0AFirstly%2C%20two%20processes%2C%20a%20tracking%20process%20and%20a%20mapping%20process%2C%20are%0Amaintained%20simultaneously%20in%20our%20framework.%20In%20the%20tracking%20process%2C%20all%20input%0Aimages%20are%20uniformly%20sampled%20and%20then%20progressively%20trained%20in%20a%0Aself-supervised%20paradigm.%20In%20the%20mapping%20process%2C%20we%20leverage%20motion%20masks%20to%0Adistinguish%20dynamic%20objects%20from%20the%20static%20background%2C%20and%20sample%20more%20pixels%0Afrom%20dynamic%20areas.%20Secondly%2C%20the%20parameter%20optimization%20for%20both%20processes%20is%0Acomprised%20of%20two%20stages%3A%20the%20first%20stage%20associates%20time%20with%203D%20positions%20to%0Aconvert%20the%20deformation%20field%20to%20the%20canonical%20field.%20The%20second%20stage%0Aassociates%20time%20with%20the%20embeddings%20of%20the%20canonical%20field%20to%20obtain%20colors%20and%0Aa%20Signed%20Distance%20Function%20%28SDF%29.%20Lastly%2C%20we%20propose%20a%20novel%20keyframe%20selection%0Astrategy%20based%20on%20the%20overlapping%20rate.%20Our%20approach%20is%20evaluated%20on%20two%0Asynthetic%20datasets%20and%20one%20real-world%20dataset%2C%20and%20the%20experiments%20validate%0Athat%20our%20method%20achieves%20competitive%20results%20in%20both%20tracking%20and%20mapping%20when%0Acompared%20to%20existing%20state-of-the-art%20NeRF-based%20dynamic%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18917v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTivNe-SLAM%253A%2520Dynamic%2520Mapping%2520and%2520Tracking%2520via%2520Time-Varying%2520Neural%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DChengyao%2520Duan%2520and%2520Zhiliu%2520Yang%26entry.1292438233%3D%2520%2520Previous%2520attempts%2520to%2520integrate%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520into%2520the%250ASimultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520framework%2520either%2520rely%2520on%2520the%250Aassumption%2520of%2520static%2520scenes%2520or%2520require%2520the%2520ground%2520truth%2520camera%2520poses%252C%2520which%250Aimpedes%2520their%2520application%2520in%2520real-world%2520scenarios.%2520This%2520paper%2520proposes%2520a%250Atime-varying%2520representation%2520to%2520track%2520and%2520reconstruct%2520the%2520dynamic%2520scenes.%250AFirstly%252C%2520two%2520processes%252C%2520a%2520tracking%2520process%2520and%2520a%2520mapping%2520process%252C%2520are%250Amaintained%2520simultaneously%2520in%2520our%2520framework.%2520In%2520the%2520tracking%2520process%252C%2520all%2520input%250Aimages%2520are%2520uniformly%2520sampled%2520and%2520then%2520progressively%2520trained%2520in%2520a%250Aself-supervised%2520paradigm.%2520In%2520the%2520mapping%2520process%252C%2520we%2520leverage%2520motion%2520masks%2520to%250Adistinguish%2520dynamic%2520objects%2520from%2520the%2520static%2520background%252C%2520and%2520sample%2520more%2520pixels%250Afrom%2520dynamic%2520areas.%2520Secondly%252C%2520the%2520parameter%2520optimization%2520for%2520both%2520processes%2520is%250Acomprised%2520of%2520two%2520stages%253A%2520the%2520first%2520stage%2520associates%2520time%2520with%25203D%2520positions%2520to%250Aconvert%2520the%2520deformation%2520field%2520to%2520the%2520canonical%2520field.%2520The%2520second%2520stage%250Aassociates%2520time%2520with%2520the%2520embeddings%2520of%2520the%2520canonical%2520field%2520to%2520obtain%2520colors%2520and%250Aa%2520Signed%2520Distance%2520Function%2520%2528SDF%2529.%2520Lastly%252C%2520we%2520propose%2520a%2520novel%2520keyframe%2520selection%250Astrategy%2520based%2520on%2520the%2520overlapping%2520rate.%2520Our%2520approach%2520is%2520evaluated%2520on%2520two%250Asynthetic%2520datasets%2520and%2520one%2520real-world%2520dataset%252C%2520and%2520the%2520experiments%2520validate%250Athat%2520our%2520method%2520achieves%2520competitive%2520results%2520in%2520both%2520tracking%2520and%2520mapping%2520when%250Acompared%2520to%2520existing%2520state-of-the-art%2520NeRF-based%2520dynamic%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18917v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields&entry.906535625=Chengyao%20Duan%20and%20Zhiliu%20Yang&entry.1292438233=%20%20Previous%20attempts%20to%20integrate%20Neural%20Radiance%20Fields%20%28NeRF%29%20into%20the%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20framework%20either%20rely%20on%20the%0Aassumption%20of%20static%20scenes%20or%20require%20the%20ground%20truth%20camera%20poses%2C%20which%0Aimpedes%20their%20application%20in%20real-world%20scenarios.%20This%20paper%20proposes%20a%0Atime-varying%20representation%20to%20track%20and%20reconstruct%20the%20dynamic%20scenes.%0AFirstly%2C%20two%20processes%2C%20a%20tracking%20process%20and%20a%20mapping%20process%2C%20are%0Amaintained%20simultaneously%20in%20our%20framework.%20In%20the%20tracking%20process%2C%20all%20input%0Aimages%20are%20uniformly%20sampled%20and%20then%20progressively%20trained%20in%20a%0Aself-supervised%20paradigm.%20In%20the%20mapping%20process%2C%20we%20leverage%20motion%20masks%20to%0Adistinguish%20dynamic%20objects%20from%20the%20static%20background%2C%20and%20sample%20more%20pixels%0Afrom%20dynamic%20areas.%20Secondly%2C%20the%20parameter%20optimization%20for%20both%20processes%20is%0Acomprised%20of%20two%20stages%3A%20the%20first%20stage%20associates%20time%20with%203D%20positions%20to%0Aconvert%20the%20deformation%20field%20to%20the%20canonical%20field.%20The%20second%20stage%0Aassociates%20time%20with%20the%20embeddings%20of%20the%20canonical%20field%20to%20obtain%20colors%20and%0Aa%20Signed%20Distance%20Function%20%28SDF%29.%20Lastly%2C%20we%20propose%20a%20novel%20keyframe%20selection%0Astrategy%20based%20on%20the%20overlapping%20rate.%20Our%20approach%20is%20evaluated%20on%20two%0Asynthetic%20datasets%20and%20one%20real-world%20dataset%2C%20and%20the%20experiments%20validate%0Athat%20our%20method%20achieves%20competitive%20results%20in%20both%20tracking%20and%20mapping%20when%0Acompared%20to%20existing%20state-of-the-art%20NeRF-based%20dynamic%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18917v5&entry.124074799=Read"},
{"title": "Evaluating Multiview Object Consistency in Humans and Image Models", "author": "Tyler Bonnen and Stephanie Fu and Yutong Bai and Thomas O'Connell and Yoni Friedman and Nancy Kanwisher and Joshua B. Tenenbaum and Alexei A. Efros", "abstract": "  We introduce a benchmark to directly evaluate the alignment between human\nobservers and vision models on a 3D shape inference task. We leverage an\nexperimental design from the cognitive sciences which requires zero-shot visual\ninferences about object shape: given a set of images, participants identify\nwhich contain the same/different objects, despite considerable viewpoint\nvariation. We draw from a diverse range of images that include common objects\n(e.g., chairs) as well as abstract shapes (i.e., procedurally generated\n`nonsense' objects). After constructing over 2000 unique image sets, we\nadminister these tasks to human participants, collecting 35K trials of\nbehavioral data from over 500 participants. This includes explicit choice\nbehaviors as well as intermediate measures, such as reaction time and gaze\ndata. We then evaluate the performance of common vision models (e.g., DINOv2,\nMAE, CLIP). We find that humans outperform all models by a wide margin. Using a\nmulti-scale evaluation approach, we identify underlying similarities and\ndifferences between models and humans: while human-model performance is\ncorrelated, humans allocate more time/processing on challenging trials. All\nimages, data, and code can be accessed via our project page.\n", "link": "http://arxiv.org/abs/2409.05862v1", "date": "2024-09-09", "relevancy": 3.0861, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Multiview%20Object%20Consistency%20in%20Humans%20and%20Image%20Models&body=Title%3A%20Evaluating%20Multiview%20Object%20Consistency%20in%20Humans%20and%20Image%20Models%0AAuthor%3A%20Tyler%20Bonnen%20and%20Stephanie%20Fu%20and%20Yutong%20Bai%20and%20Thomas%20O%27Connell%20and%20Yoni%20Friedman%20and%20Nancy%20Kanwisher%20and%20Joshua%20B.%20Tenenbaum%20and%20Alexei%20A.%20Efros%0AAbstract%3A%20%20%20We%20introduce%20a%20benchmark%20to%20directly%20evaluate%20the%20alignment%20between%20human%0Aobservers%20and%20vision%20models%20on%20a%203D%20shape%20inference%20task.%20We%20leverage%20an%0Aexperimental%20design%20from%20the%20cognitive%20sciences%20which%20requires%20zero-shot%20visual%0Ainferences%20about%20object%20shape%3A%20given%20a%20set%20of%20images%2C%20participants%20identify%0Awhich%20contain%20the%20same/different%20objects%2C%20despite%20considerable%20viewpoint%0Avariation.%20We%20draw%20from%20a%20diverse%20range%20of%20images%20that%20include%20common%20objects%0A%28e.g.%2C%20chairs%29%20as%20well%20as%20abstract%20shapes%20%28i.e.%2C%20procedurally%20generated%0A%60nonsense%27%20objects%29.%20After%20constructing%20over%202000%20unique%20image%20sets%2C%20we%0Aadminister%20these%20tasks%20to%20human%20participants%2C%20collecting%2035K%20trials%20of%0Abehavioral%20data%20from%20over%20500%20participants.%20This%20includes%20explicit%20choice%0Abehaviors%20as%20well%20as%20intermediate%20measures%2C%20such%20as%20reaction%20time%20and%20gaze%0Adata.%20We%20then%20evaluate%20the%20performance%20of%20common%20vision%20models%20%28e.g.%2C%20DINOv2%2C%0AMAE%2C%20CLIP%29.%20We%20find%20that%20humans%20outperform%20all%20models%20by%20a%20wide%20margin.%20Using%20a%0Amulti-scale%20evaluation%20approach%2C%20we%20identify%20underlying%20similarities%20and%0Adifferences%20between%20models%20and%20humans%3A%20while%20human-model%20performance%20is%0Acorrelated%2C%20humans%20allocate%20more%20time/processing%20on%20challenging%20trials.%20All%0Aimages%2C%20data%2C%20and%20code%20can%20be%20accessed%20via%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Multiview%2520Object%2520Consistency%2520in%2520Humans%2520and%2520Image%2520Models%26entry.906535625%3DTyler%2520Bonnen%2520and%2520Stephanie%2520Fu%2520and%2520Yutong%2520Bai%2520and%2520Thomas%2520O%2527Connell%2520and%2520Yoni%2520Friedman%2520and%2520Nancy%2520Kanwisher%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Alexei%2520A.%2520Efros%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520benchmark%2520to%2520directly%2520evaluate%2520the%2520alignment%2520between%2520human%250Aobservers%2520and%2520vision%2520models%2520on%2520a%25203D%2520shape%2520inference%2520task.%2520We%2520leverage%2520an%250Aexperimental%2520design%2520from%2520the%2520cognitive%2520sciences%2520which%2520requires%2520zero-shot%2520visual%250Ainferences%2520about%2520object%2520shape%253A%2520given%2520a%2520set%2520of%2520images%252C%2520participants%2520identify%250Awhich%2520contain%2520the%2520same/different%2520objects%252C%2520despite%2520considerable%2520viewpoint%250Avariation.%2520We%2520draw%2520from%2520a%2520diverse%2520range%2520of%2520images%2520that%2520include%2520common%2520objects%250A%2528e.g.%252C%2520chairs%2529%2520as%2520well%2520as%2520abstract%2520shapes%2520%2528i.e.%252C%2520procedurally%2520generated%250A%2560nonsense%2527%2520objects%2529.%2520After%2520constructing%2520over%25202000%2520unique%2520image%2520sets%252C%2520we%250Aadminister%2520these%2520tasks%2520to%2520human%2520participants%252C%2520collecting%252035K%2520trials%2520of%250Abehavioral%2520data%2520from%2520over%2520500%2520participants.%2520This%2520includes%2520explicit%2520choice%250Abehaviors%2520as%2520well%2520as%2520intermediate%2520measures%252C%2520such%2520as%2520reaction%2520time%2520and%2520gaze%250Adata.%2520We%2520then%2520evaluate%2520the%2520performance%2520of%2520common%2520vision%2520models%2520%2528e.g.%252C%2520DINOv2%252C%250AMAE%252C%2520CLIP%2529.%2520We%2520find%2520that%2520humans%2520outperform%2520all%2520models%2520by%2520a%2520wide%2520margin.%2520Using%2520a%250Amulti-scale%2520evaluation%2520approach%252C%2520we%2520identify%2520underlying%2520similarities%2520and%250Adifferences%2520between%2520models%2520and%2520humans%253A%2520while%2520human-model%2520performance%2520is%250Acorrelated%252C%2520humans%2520allocate%2520more%2520time/processing%2520on%2520challenging%2520trials.%2520All%250Aimages%252C%2520data%252C%2520and%2520code%2520can%2520be%2520accessed%2520via%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Multiview%20Object%20Consistency%20in%20Humans%20and%20Image%20Models&entry.906535625=Tyler%20Bonnen%20and%20Stephanie%20Fu%20and%20Yutong%20Bai%20and%20Thomas%20O%27Connell%20and%20Yoni%20Friedman%20and%20Nancy%20Kanwisher%20and%20Joshua%20B.%20Tenenbaum%20and%20Alexei%20A.%20Efros&entry.1292438233=%20%20We%20introduce%20a%20benchmark%20to%20directly%20evaluate%20the%20alignment%20between%20human%0Aobservers%20and%20vision%20models%20on%20a%203D%20shape%20inference%20task.%20We%20leverage%20an%0Aexperimental%20design%20from%20the%20cognitive%20sciences%20which%20requires%20zero-shot%20visual%0Ainferences%20about%20object%20shape%3A%20given%20a%20set%20of%20images%2C%20participants%20identify%0Awhich%20contain%20the%20same/different%20objects%2C%20despite%20considerable%20viewpoint%0Avariation.%20We%20draw%20from%20a%20diverse%20range%20of%20images%20that%20include%20common%20objects%0A%28e.g.%2C%20chairs%29%20as%20well%20as%20abstract%20shapes%20%28i.e.%2C%20procedurally%20generated%0A%60nonsense%27%20objects%29.%20After%20constructing%20over%202000%20unique%20image%20sets%2C%20we%0Aadminister%20these%20tasks%20to%20human%20participants%2C%20collecting%2035K%20trials%20of%0Abehavioral%20data%20from%20over%20500%20participants.%20This%20includes%20explicit%20choice%0Abehaviors%20as%20well%20as%20intermediate%20measures%2C%20such%20as%20reaction%20time%20and%20gaze%0Adata.%20We%20then%20evaluate%20the%20performance%20of%20common%20vision%20models%20%28e.g.%2C%20DINOv2%2C%0AMAE%2C%20CLIP%29.%20We%20find%20that%20humans%20outperform%20all%20models%20by%20a%20wide%20margin.%20Using%20a%0Amulti-scale%20evaluation%20approach%2C%20we%20identify%20underlying%20similarities%20and%0Adifferences%20between%20models%20and%20humans%3A%20while%20human-model%20performance%20is%0Acorrelated%2C%20humans%20allocate%20more%20time/processing%20on%20challenging%20trials.%20All%0Aimages%2C%20data%2C%20and%20code%20can%20be%20accessed%20via%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05862v1&entry.124074799=Read"},
{"title": "G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric\n  Information", "author": "Lorenzo Montano-Oliv\u00e1n and Julio A. Placed and Luis Montano and Mar\u00eda T. L\u00e1zaro", "abstract": "  Localization in already mapped environments is a critical component in many\nrobotics and automotive applications, where previously acquired information can\nbe exploited along with sensor fusion to provide robust and accurate\nlocalization estimates. In this work, we offer a new perspective on map-based\nlocalization by reusing prior topological and metric information. Thus, we\nreformulate this long-studied problem to go beyond the mere use of metric maps.\nOur framework seamlessly integrates LiDAR, inertial and GNSS measurements, and\ncloud-to-map registrations in a sliding window graph fashion, which allows to\naccommodate the uncertainty of each observation. The modularity of our\nframework allows it to work with different sensor configurations (e.g., LiDAR\nresolutions, GNSS denial) and environmental conditions (e.g., mapless regions,\nlarge environments). We have conducted several validation experiments,\nincluding the deployment in a real-world automotive application, demonstrating\nthe accuracy, efficiency, and versatility of our system in online localization.\n", "link": "http://arxiv.org/abs/2405.05059v2", "date": "2024-09-09", "relevancy": 3.0416, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5999}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Loc%3A%20Tightly-coupled%20Graph%20Localization%20with%20Prior%20Topo-metric%0A%20%20Information&body=Title%3A%20G-Loc%3A%20Tightly-coupled%20Graph%20Localization%20with%20Prior%20Topo-metric%0A%20%20Information%0AAuthor%3A%20Lorenzo%20Montano-Oliv%C3%A1n%20and%20Julio%20A.%20Placed%20and%20Luis%20Montano%20and%20Mar%C3%ADa%20T.%20L%C3%A1zaro%0AAbstract%3A%20%20%20Localization%20in%20already%20mapped%20environments%20is%20a%20critical%20component%20in%20many%0Arobotics%20and%20automotive%20applications%2C%20where%20previously%20acquired%20information%20can%0Abe%20exploited%20along%20with%20sensor%20fusion%20to%20provide%20robust%20and%20accurate%0Alocalization%20estimates.%20In%20this%20work%2C%20we%20offer%20a%20new%20perspective%20on%20map-based%0Alocalization%20by%20reusing%20prior%20topological%20and%20metric%20information.%20Thus%2C%20we%0Areformulate%20this%20long-studied%20problem%20to%20go%20beyond%20the%20mere%20use%20of%20metric%20maps.%0AOur%20framework%20seamlessly%20integrates%20LiDAR%2C%20inertial%20and%20GNSS%20measurements%2C%20and%0Acloud-to-map%20registrations%20in%20a%20sliding%20window%20graph%20fashion%2C%20which%20allows%20to%0Aaccommodate%20the%20uncertainty%20of%20each%20observation.%20The%20modularity%20of%20our%0Aframework%20allows%20it%20to%20work%20with%20different%20sensor%20configurations%20%28e.g.%2C%20LiDAR%0Aresolutions%2C%20GNSS%20denial%29%20and%20environmental%20conditions%20%28e.g.%2C%20mapless%20regions%2C%0Alarge%20environments%29.%20We%20have%20conducted%20several%20validation%20experiments%2C%0Aincluding%20the%20deployment%20in%20a%20real-world%20automotive%20application%2C%20demonstrating%0Athe%20accuracy%2C%20efficiency%2C%20and%20versatility%20of%20our%20system%20in%20online%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Loc%253A%2520Tightly-coupled%2520Graph%2520Localization%2520with%2520Prior%2520Topo-metric%250A%2520%2520Information%26entry.906535625%3DLorenzo%2520Montano-Oliv%25C3%25A1n%2520and%2520Julio%2520A.%2520Placed%2520and%2520Luis%2520Montano%2520and%2520Mar%25C3%25ADa%2520T.%2520L%25C3%25A1zaro%26entry.1292438233%3D%2520%2520Localization%2520in%2520already%2520mapped%2520environments%2520is%2520a%2520critical%2520component%2520in%2520many%250Arobotics%2520and%2520automotive%2520applications%252C%2520where%2520previously%2520acquired%2520information%2520can%250Abe%2520exploited%2520along%2520with%2520sensor%2520fusion%2520to%2520provide%2520robust%2520and%2520accurate%250Alocalization%2520estimates.%2520In%2520this%2520work%252C%2520we%2520offer%2520a%2520new%2520perspective%2520on%2520map-based%250Alocalization%2520by%2520reusing%2520prior%2520topological%2520and%2520metric%2520information.%2520Thus%252C%2520we%250Areformulate%2520this%2520long-studied%2520problem%2520to%2520go%2520beyond%2520the%2520mere%2520use%2520of%2520metric%2520maps.%250AOur%2520framework%2520seamlessly%2520integrates%2520LiDAR%252C%2520inertial%2520and%2520GNSS%2520measurements%252C%2520and%250Acloud-to-map%2520registrations%2520in%2520a%2520sliding%2520window%2520graph%2520fashion%252C%2520which%2520allows%2520to%250Aaccommodate%2520the%2520uncertainty%2520of%2520each%2520observation.%2520The%2520modularity%2520of%2520our%250Aframework%2520allows%2520it%2520to%2520work%2520with%2520different%2520sensor%2520configurations%2520%2528e.g.%252C%2520LiDAR%250Aresolutions%252C%2520GNSS%2520denial%2529%2520and%2520environmental%2520conditions%2520%2528e.g.%252C%2520mapless%2520regions%252C%250Alarge%2520environments%2529.%2520We%2520have%2520conducted%2520several%2520validation%2520experiments%252C%250Aincluding%2520the%2520deployment%2520in%2520a%2520real-world%2520automotive%2520application%252C%2520demonstrating%250Athe%2520accuracy%252C%2520efficiency%252C%2520and%2520versatility%2520of%2520our%2520system%2520in%2520online%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Loc%3A%20Tightly-coupled%20Graph%20Localization%20with%20Prior%20Topo-metric%0A%20%20Information&entry.906535625=Lorenzo%20Montano-Oliv%C3%A1n%20and%20Julio%20A.%20Placed%20and%20Luis%20Montano%20and%20Mar%C3%ADa%20T.%20L%C3%A1zaro&entry.1292438233=%20%20Localization%20in%20already%20mapped%20environments%20is%20a%20critical%20component%20in%20many%0Arobotics%20and%20automotive%20applications%2C%20where%20previously%20acquired%20information%20can%0Abe%20exploited%20along%20with%20sensor%20fusion%20to%20provide%20robust%20and%20accurate%0Alocalization%20estimates.%20In%20this%20work%2C%20we%20offer%20a%20new%20perspective%20on%20map-based%0Alocalization%20by%20reusing%20prior%20topological%20and%20metric%20information.%20Thus%2C%20we%0Areformulate%20this%20long-studied%20problem%20to%20go%20beyond%20the%20mere%20use%20of%20metric%20maps.%0AOur%20framework%20seamlessly%20integrates%20LiDAR%2C%20inertial%20and%20GNSS%20measurements%2C%20and%0Acloud-to-map%20registrations%20in%20a%20sliding%20window%20graph%20fashion%2C%20which%20allows%20to%0Aaccommodate%20the%20uncertainty%20of%20each%20observation.%20The%20modularity%20of%20our%0Aframework%20allows%20it%20to%20work%20with%20different%20sensor%20configurations%20%28e.g.%2C%20LiDAR%0Aresolutions%2C%20GNSS%20denial%29%20and%20environmental%20conditions%20%28e.g.%2C%20mapless%20regions%2C%0Alarge%20environments%29.%20We%20have%20conducted%20several%20validation%20experiments%2C%0Aincluding%20the%20deployment%20in%20a%20real-world%20automotive%20application%2C%20demonstrating%0Athe%20accuracy%2C%20efficiency%2C%20and%20versatility%20of%20our%20system%20in%20online%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05059v2&entry.124074799=Read"},
{"title": "Vision-Driven 2D Supervised Fine-Tuning Framework for Bird's Eye View\n  Perception", "author": "Lei He and Qiaoyi Wang and Honglin Sun and Qing Xu and Bolin Gao and Shengbo Eben Li and Jianqiang Wang and Keqiang Li", "abstract": "  Visual bird's eye view (BEV) perception, due to its excellent perceptual\ncapabilities, is progressively replacing costly LiDAR-based perception systems,\nespecially in the realm of urban intelligent driving. However, this type of\nperception still relies on LiDAR data to construct ground truth databases, a\nprocess that is both cumbersome and time-consuming. Moreover, most massproduced\nautonomous driving systems are only equipped with surround camera sensors and\nlack LiDAR data for precise annotation. To tackle this challenge, we propose a\nfine-tuning method for BEV perception network based on visual 2D semantic\nperception, aimed at enhancing the model's generalization capabilities in new\nscene data. Considering the maturity and development of 2D perception\ntechnologies, our method significantly reduces the dependency on high-cost BEV\nground truths and shows promising industrial application prospects. Extensive\nexperiments and comparative analyses conducted on the nuScenes and Waymo public\ndatasets demonstrate the effectiveness of our proposed method.\n", "link": "http://arxiv.org/abs/2409.05834v1", "date": "2024-09-09", "relevancy": 2.9895, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6072}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Driven%202D%20Supervised%20Fine-Tuning%20Framework%20for%20Bird%27s%20Eye%20View%0A%20%20Perception&body=Title%3A%20Vision-Driven%202D%20Supervised%20Fine-Tuning%20Framework%20for%20Bird%27s%20Eye%20View%0A%20%20Perception%0AAuthor%3A%20Lei%20He%20and%20Qiaoyi%20Wang%20and%20Honglin%20Sun%20and%20Qing%20Xu%20and%20Bolin%20Gao%20and%20Shengbo%20Eben%20Li%20and%20Jianqiang%20Wang%20and%20Keqiang%20Li%0AAbstract%3A%20%20%20Visual%20bird%27s%20eye%20view%20%28BEV%29%20perception%2C%20due%20to%20its%20excellent%20perceptual%0Acapabilities%2C%20is%20progressively%20replacing%20costly%20LiDAR-based%20perception%20systems%2C%0Aespecially%20in%20the%20realm%20of%20urban%20intelligent%20driving.%20However%2C%20this%20type%20of%0Aperception%20still%20relies%20on%20LiDAR%20data%20to%20construct%20ground%20truth%20databases%2C%20a%0Aprocess%20that%20is%20both%20cumbersome%20and%20time-consuming.%20Moreover%2C%20most%20massproduced%0Aautonomous%20driving%20systems%20are%20only%20equipped%20with%20surround%20camera%20sensors%20and%0Alack%20LiDAR%20data%20for%20precise%20annotation.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%0Afine-tuning%20method%20for%20BEV%20perception%20network%20based%20on%20visual%202D%20semantic%0Aperception%2C%20aimed%20at%20enhancing%20the%20model%27s%20generalization%20capabilities%20in%20new%0Ascene%20data.%20Considering%20the%20maturity%20and%20development%20of%202D%20perception%0Atechnologies%2C%20our%20method%20significantly%20reduces%20the%20dependency%20on%20high-cost%20BEV%0Aground%20truths%20and%20shows%20promising%20industrial%20application%20prospects.%20Extensive%0Aexperiments%20and%20comparative%20analyses%20conducted%20on%20the%20nuScenes%20and%20Waymo%20public%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Driven%25202D%2520Supervised%2520Fine-Tuning%2520Framework%2520for%2520Bird%2527s%2520Eye%2520View%250A%2520%2520Perception%26entry.906535625%3DLei%2520He%2520and%2520Qiaoyi%2520Wang%2520and%2520Honglin%2520Sun%2520and%2520Qing%2520Xu%2520and%2520Bolin%2520Gao%2520and%2520Shengbo%2520Eben%2520Li%2520and%2520Jianqiang%2520Wang%2520and%2520Keqiang%2520Li%26entry.1292438233%3D%2520%2520Visual%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529%2520perception%252C%2520due%2520to%2520its%2520excellent%2520perceptual%250Acapabilities%252C%2520is%2520progressively%2520replacing%2520costly%2520LiDAR-based%2520perception%2520systems%252C%250Aespecially%2520in%2520the%2520realm%2520of%2520urban%2520intelligent%2520driving.%2520However%252C%2520this%2520type%2520of%250Aperception%2520still%2520relies%2520on%2520LiDAR%2520data%2520to%2520construct%2520ground%2520truth%2520databases%252C%2520a%250Aprocess%2520that%2520is%2520both%2520cumbersome%2520and%2520time-consuming.%2520Moreover%252C%2520most%2520massproduced%250Aautonomous%2520driving%2520systems%2520are%2520only%2520equipped%2520with%2520surround%2520camera%2520sensors%2520and%250Alack%2520LiDAR%2520data%2520for%2520precise%2520annotation.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%250Afine-tuning%2520method%2520for%2520BEV%2520perception%2520network%2520based%2520on%2520visual%25202D%2520semantic%250Aperception%252C%2520aimed%2520at%2520enhancing%2520the%2520model%2527s%2520generalization%2520capabilities%2520in%2520new%250Ascene%2520data.%2520Considering%2520the%2520maturity%2520and%2520development%2520of%25202D%2520perception%250Atechnologies%252C%2520our%2520method%2520significantly%2520reduces%2520the%2520dependency%2520on%2520high-cost%2520BEV%250Aground%2520truths%2520and%2520shows%2520promising%2520industrial%2520application%2520prospects.%2520Extensive%250Aexperiments%2520and%2520comparative%2520analyses%2520conducted%2520on%2520the%2520nuScenes%2520and%2520Waymo%2520public%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Driven%202D%20Supervised%20Fine-Tuning%20Framework%20for%20Bird%27s%20Eye%20View%0A%20%20Perception&entry.906535625=Lei%20He%20and%20Qiaoyi%20Wang%20and%20Honglin%20Sun%20and%20Qing%20Xu%20and%20Bolin%20Gao%20and%20Shengbo%20Eben%20Li%20and%20Jianqiang%20Wang%20and%20Keqiang%20Li&entry.1292438233=%20%20Visual%20bird%27s%20eye%20view%20%28BEV%29%20perception%2C%20due%20to%20its%20excellent%20perceptual%0Acapabilities%2C%20is%20progressively%20replacing%20costly%20LiDAR-based%20perception%20systems%2C%0Aespecially%20in%20the%20realm%20of%20urban%20intelligent%20driving.%20However%2C%20this%20type%20of%0Aperception%20still%20relies%20on%20LiDAR%20data%20to%20construct%20ground%20truth%20databases%2C%20a%0Aprocess%20that%20is%20both%20cumbersome%20and%20time-consuming.%20Moreover%2C%20most%20massproduced%0Aautonomous%20driving%20systems%20are%20only%20equipped%20with%20surround%20camera%20sensors%20and%0Alack%20LiDAR%20data%20for%20precise%20annotation.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%0Afine-tuning%20method%20for%20BEV%20perception%20network%20based%20on%20visual%202D%20semantic%0Aperception%2C%20aimed%20at%20enhancing%20the%20model%27s%20generalization%20capabilities%20in%20new%0Ascene%20data.%20Considering%20the%20maturity%20and%20development%20of%202D%20perception%0Atechnologies%2C%20our%20method%20significantly%20reduces%20the%20dependency%20on%20high-cost%20BEV%0Aground%20truths%20and%20shows%20promising%20industrial%20application%20prospects.%20Extensive%0Aexperiments%20and%20comparative%20analyses%20conducted%20on%20the%20nuScenes%20and%20Waymo%20public%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05834v1&entry.124074799=Read"},
{"title": "No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard\n  Negatives via CLIP Knowledge and LLMs", "author": "Cristian Sbrolli and Matteo Matteucci", "abstract": "  In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods.\n", "link": "http://arxiv.org/abs/2406.02202v2", "date": "2024-09-09", "relevancy": 2.9347, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6274}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Captions%2C%20No%20Problem%3A%20Captionless%203D-CLIP%20Alignment%20with%20Hard%0A%20%20Negatives%20via%20CLIP%20Knowledge%20and%20LLMs&body=Title%3A%20No%20Captions%2C%20No%20Problem%3A%20Captionless%203D-CLIP%20Alignment%20with%20Hard%0A%20%20Negatives%20via%20CLIP%20Knowledge%20and%20LLMs%0AAuthor%3A%20Cristian%20Sbrolli%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20explore%20an%20alternative%20approach%20to%20enhance%20contrastive%0Atext-image-3D%20alignment%20in%20the%20absence%20of%20textual%20descriptions%20for%203D%20objects.%0AWe%20introduce%20two%20unsupervised%20methods%2C%20%24I2I%24%20and%20%24%28I2L%29%5E2%24%2C%20which%20leverage%20CLIP%0Aknowledge%20about%20textual%20and%202D%20data%20to%20compute%20the%20neural%20perceived%20similarity%0Abetween%20two%203D%20samples.%20We%20employ%20the%20proposed%20methods%20to%20mine%203D%20hard%0Anegatives%2C%20establishing%20a%20multimodal%20contrastive%20pipeline%20with%20hard%20negative%0Aweighting%20via%20a%20custom%20loss%20function.%20We%20train%20on%20different%20configurations%20of%0Athe%20proposed%20hard%20negative%20mining%20approach%2C%20and%20we%20evaluate%20the%20accuracy%20of%20our%0Amodels%20in%203D%20classification%20and%20on%20the%20cross-modal%20retrieval%20benchmark%2C%20testing%0Aimage-to-shape%20and%20shape-to-image%20retrieval.%20Results%20demonstrate%20that%20our%0Aapproach%2C%20even%20without%20explicit%20text%20alignment%2C%20achieves%20comparable%20or%20superior%0Aperformance%20on%20zero-shot%20and%20standard%203D%20classification%2C%20while%20significantly%0Aimproving%20both%20image-to-shape%20and%20shape-to-image%20retrieval%20compared%20to%20previous%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Captions%252C%2520No%2520Problem%253A%2520Captionless%25203D-CLIP%2520Alignment%2520with%2520Hard%250A%2520%2520Negatives%2520via%2520CLIP%2520Knowledge%2520and%2520LLMs%26entry.906535625%3DCristian%2520Sbrolli%2520and%2520Matteo%2520Matteucci%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520explore%2520an%2520alternative%2520approach%2520to%2520enhance%2520contrastive%250Atext-image-3D%2520alignment%2520in%2520the%2520absence%2520of%2520textual%2520descriptions%2520for%25203D%2520objects.%250AWe%2520introduce%2520two%2520unsupervised%2520methods%252C%2520%2524I2I%2524%2520and%2520%2524%2528I2L%2529%255E2%2524%252C%2520which%2520leverage%2520CLIP%250Aknowledge%2520about%2520textual%2520and%25202D%2520data%2520to%2520compute%2520the%2520neural%2520perceived%2520similarity%250Abetween%2520two%25203D%2520samples.%2520We%2520employ%2520the%2520proposed%2520methods%2520to%2520mine%25203D%2520hard%250Anegatives%252C%2520establishing%2520a%2520multimodal%2520contrastive%2520pipeline%2520with%2520hard%2520negative%250Aweighting%2520via%2520a%2520custom%2520loss%2520function.%2520We%2520train%2520on%2520different%2520configurations%2520of%250Athe%2520proposed%2520hard%2520negative%2520mining%2520approach%252C%2520and%2520we%2520evaluate%2520the%2520accuracy%2520of%2520our%250Amodels%2520in%25203D%2520classification%2520and%2520on%2520the%2520cross-modal%2520retrieval%2520benchmark%252C%2520testing%250Aimage-to-shape%2520and%2520shape-to-image%2520retrieval.%2520Results%2520demonstrate%2520that%2520our%250Aapproach%252C%2520even%2520without%2520explicit%2520text%2520alignment%252C%2520achieves%2520comparable%2520or%2520superior%250Aperformance%2520on%2520zero-shot%2520and%2520standard%25203D%2520classification%252C%2520while%2520significantly%250Aimproving%2520both%2520image-to-shape%2520and%2520shape-to-image%2520retrieval%2520compared%2520to%2520previous%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Captions%2C%20No%20Problem%3A%20Captionless%203D-CLIP%20Alignment%20with%20Hard%0A%20%20Negatives%20via%20CLIP%20Knowledge%20and%20LLMs&entry.906535625=Cristian%20Sbrolli%20and%20Matteo%20Matteucci&entry.1292438233=%20%20In%20this%20study%2C%20we%20explore%20an%20alternative%20approach%20to%20enhance%20contrastive%0Atext-image-3D%20alignment%20in%20the%20absence%20of%20textual%20descriptions%20for%203D%20objects.%0AWe%20introduce%20two%20unsupervised%20methods%2C%20%24I2I%24%20and%20%24%28I2L%29%5E2%24%2C%20which%20leverage%20CLIP%0Aknowledge%20about%20textual%20and%202D%20data%20to%20compute%20the%20neural%20perceived%20similarity%0Abetween%20two%203D%20samples.%20We%20employ%20the%20proposed%20methods%20to%20mine%203D%20hard%0Anegatives%2C%20establishing%20a%20multimodal%20contrastive%20pipeline%20with%20hard%20negative%0Aweighting%20via%20a%20custom%20loss%20function.%20We%20train%20on%20different%20configurations%20of%0Athe%20proposed%20hard%20negative%20mining%20approach%2C%20and%20we%20evaluate%20the%20accuracy%20of%20our%0Amodels%20in%203D%20classification%20and%20on%20the%20cross-modal%20retrieval%20benchmark%2C%20testing%0Aimage-to-shape%20and%20shape-to-image%20retrieval.%20Results%20demonstrate%20that%20our%0Aapproach%2C%20even%20without%20explicit%20text%20alignment%2C%20achieves%20comparable%20or%20superior%0Aperformance%20on%20zero-shot%20and%20standard%203D%20classification%2C%20while%20significantly%0Aimproving%20both%20image-to-shape%20and%20shape-to-image%20retrieval%20compared%20to%20previous%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02202v2&entry.124074799=Read"},
{"title": "TextGaze: Gaze-Controllable Face Generation with Natural Language", "author": "Hengfei Wang and Zhongqun Zhang and Yihua Cheng and Hyung Jin Chang", "abstract": "  Generating face image with specific gaze information has attracted\nconsiderable attention. Existing approaches typically input gaze values\ndirectly for face generation, which is unnatural and requires annotated gaze\ndatasets for training, thereby limiting its application. In this paper, we\npresent a novel gaze-controllable face generation task. Our approach inputs\ntextual descriptions that describe human gaze and head behavior and generates\ncorresponding face images. Our work first introduces a text-of-gaze dataset\ncontaining over 90k text descriptions spanning a dense distribution of gaze and\nhead poses. We further propose a gaze-controllable text-to-face method. Our\nmethod contains a sketch-conditioned face diffusion module and a model-based\nsketch diffusion module. We define a face sketch based on facial landmarks and\neye segmentation map. The face diffusion module generates face images from the\nface sketch, and the sketch diffusion module employs a 3D face model to\ngenerate face sketch from text description. Experiments on the FFHQ dataset\nshow the effectiveness of our method. We will release our dataset and code for\nfuture research.\n", "link": "http://arxiv.org/abs/2404.17486v2", "date": "2024-09-09", "relevancy": 2.9148, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5843}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5843}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextGaze%3A%20Gaze-Controllable%20Face%20Generation%20with%20Natural%20Language&body=Title%3A%20TextGaze%3A%20Gaze-Controllable%20Face%20Generation%20with%20Natural%20Language%0AAuthor%3A%20Hengfei%20Wang%20and%20Zhongqun%20Zhang%20and%20Yihua%20Cheng%20and%20Hyung%20Jin%20Chang%0AAbstract%3A%20%20%20Generating%20face%20image%20with%20specific%20gaze%20information%20has%20attracted%0Aconsiderable%20attention.%20Existing%20approaches%20typically%20input%20gaze%20values%0Adirectly%20for%20face%20generation%2C%20which%20is%20unnatural%20and%20requires%20annotated%20gaze%0Adatasets%20for%20training%2C%20thereby%20limiting%20its%20application.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20gaze-controllable%20face%20generation%20task.%20Our%20approach%20inputs%0Atextual%20descriptions%20that%20describe%20human%20gaze%20and%20head%20behavior%20and%20generates%0Acorresponding%20face%20images.%20Our%20work%20first%20introduces%20a%20text-of-gaze%20dataset%0Acontaining%20over%2090k%20text%20descriptions%20spanning%20a%20dense%20distribution%20of%20gaze%20and%0Ahead%20poses.%20We%20further%20propose%20a%20gaze-controllable%20text-to-face%20method.%20Our%0Amethod%20contains%20a%20sketch-conditioned%20face%20diffusion%20module%20and%20a%20model-based%0Asketch%20diffusion%20module.%20We%20define%20a%20face%20sketch%20based%20on%20facial%20landmarks%20and%0Aeye%20segmentation%20map.%20The%20face%20diffusion%20module%20generates%20face%20images%20from%20the%0Aface%20sketch%2C%20and%20the%20sketch%20diffusion%20module%20employs%20a%203D%20face%20model%20to%0Agenerate%20face%20sketch%20from%20text%20description.%20Experiments%20on%20the%20FFHQ%20dataset%0Ashow%20the%20effectiveness%20of%20our%20method.%20We%20will%20release%20our%20dataset%20and%20code%20for%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextGaze%253A%2520Gaze-Controllable%2520Face%2520Generation%2520with%2520Natural%2520Language%26entry.906535625%3DHengfei%2520Wang%2520and%2520Zhongqun%2520Zhang%2520and%2520Yihua%2520Cheng%2520and%2520Hyung%2520Jin%2520Chang%26entry.1292438233%3D%2520%2520Generating%2520face%2520image%2520with%2520specific%2520gaze%2520information%2520has%2520attracted%250Aconsiderable%2520attention.%2520Existing%2520approaches%2520typically%2520input%2520gaze%2520values%250Adirectly%2520for%2520face%2520generation%252C%2520which%2520is%2520unnatural%2520and%2520requires%2520annotated%2520gaze%250Adatasets%2520for%2520training%252C%2520thereby%2520limiting%2520its%2520application.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520gaze-controllable%2520face%2520generation%2520task.%2520Our%2520approach%2520inputs%250Atextual%2520descriptions%2520that%2520describe%2520human%2520gaze%2520and%2520head%2520behavior%2520and%2520generates%250Acorresponding%2520face%2520images.%2520Our%2520work%2520first%2520introduces%2520a%2520text-of-gaze%2520dataset%250Acontaining%2520over%252090k%2520text%2520descriptions%2520spanning%2520a%2520dense%2520distribution%2520of%2520gaze%2520and%250Ahead%2520poses.%2520We%2520further%2520propose%2520a%2520gaze-controllable%2520text-to-face%2520method.%2520Our%250Amethod%2520contains%2520a%2520sketch-conditioned%2520face%2520diffusion%2520module%2520and%2520a%2520model-based%250Asketch%2520diffusion%2520module.%2520We%2520define%2520a%2520face%2520sketch%2520based%2520on%2520facial%2520landmarks%2520and%250Aeye%2520segmentation%2520map.%2520The%2520face%2520diffusion%2520module%2520generates%2520face%2520images%2520from%2520the%250Aface%2520sketch%252C%2520and%2520the%2520sketch%2520diffusion%2520module%2520employs%2520a%25203D%2520face%2520model%2520to%250Agenerate%2520face%2520sketch%2520from%2520text%2520description.%2520Experiments%2520on%2520the%2520FFHQ%2520dataset%250Ashow%2520the%2520effectiveness%2520of%2520our%2520method.%2520We%2520will%2520release%2520our%2520dataset%2520and%2520code%2520for%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextGaze%3A%20Gaze-Controllable%20Face%20Generation%20with%20Natural%20Language&entry.906535625=Hengfei%20Wang%20and%20Zhongqun%20Zhang%20and%20Yihua%20Cheng%20and%20Hyung%20Jin%20Chang&entry.1292438233=%20%20Generating%20face%20image%20with%20specific%20gaze%20information%20has%20attracted%0Aconsiderable%20attention.%20Existing%20approaches%20typically%20input%20gaze%20values%0Adirectly%20for%20face%20generation%2C%20which%20is%20unnatural%20and%20requires%20annotated%20gaze%0Adatasets%20for%20training%2C%20thereby%20limiting%20its%20application.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20gaze-controllable%20face%20generation%20task.%20Our%20approach%20inputs%0Atextual%20descriptions%20that%20describe%20human%20gaze%20and%20head%20behavior%20and%20generates%0Acorresponding%20face%20images.%20Our%20work%20first%20introduces%20a%20text-of-gaze%20dataset%0Acontaining%20over%2090k%20text%20descriptions%20spanning%20a%20dense%20distribution%20of%20gaze%20and%0Ahead%20poses.%20We%20further%20propose%20a%20gaze-controllable%20text-to-face%20method.%20Our%0Amethod%20contains%20a%20sketch-conditioned%20face%20diffusion%20module%20and%20a%20model-based%0Asketch%20diffusion%20module.%20We%20define%20a%20face%20sketch%20based%20on%20facial%20landmarks%20and%0Aeye%20segmentation%20map.%20The%20face%20diffusion%20module%20generates%20face%20images%20from%20the%0Aface%20sketch%2C%20and%20the%20sketch%20diffusion%20module%20employs%20a%203D%20face%20model%20to%0Agenerate%20face%20sketch%20from%20text%20description.%20Experiments%20on%20the%20FFHQ%20dataset%0Ashow%20the%20effectiveness%20of%20our%20method.%20We%20will%20release%20our%20dataset%20and%20code%20for%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17486v2&entry.124074799=Read"},
{"title": "PVP-Recon: Progressive View Planning via Warping Consistency for\n  Sparse-View Surface Reconstruction", "author": "Sheng Ye and Yuze He and Matthieu Lin and Jenny Sheng and Ruoyu Fan and Yiheng Han and Yubin Hu and Ran Yi and Yu-Hui Wen and Yong-Jin Liu and Wenping Wang", "abstract": "  Neural implicit representations have revolutionized dense multi-view surface\nreconstruction, yet their performance significantly diminishes with sparse\ninput views. A few pioneering works have sought to tackle the challenge of\nsparse-view reconstruction by leveraging additional geometric priors or\nmulti-scene generalizability. However, they are still hindered by the imperfect\nchoice of input views, using images under empirically determined viewpoints to\nprovide considerable overlap. We propose PVP-Recon, a novel and effective\nsparse-view surface reconstruction method that progressively plans the next\nbest views to form an optimal set of sparse viewpoints for image capturing.\nPVP-Recon starts initial surface reconstruction with as few as 3 views and\nprogressively adds new views which are determined based on a novel warping\nscore that reflects the information gain of each newly added view. This\nprogressive view planning progress is interleaved with a neural SDF-based\nreconstruction module that utilizes multi-resolution hash features, enhanced by\na progressive training scheme and a directional Hessian loss. Quantitative and\nqualitative experiments on three benchmark datasets show that our framework\nachieves high-quality reconstruction with a constrained input budget and\noutperforms existing baselines.\n", "link": "http://arxiv.org/abs/2409.05474v1", "date": "2024-09-09", "relevancy": 2.9004, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5902}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5751}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PVP-Recon%3A%20Progressive%20View%20Planning%20via%20Warping%20Consistency%20for%0A%20%20Sparse-View%20Surface%20Reconstruction&body=Title%3A%20PVP-Recon%3A%20Progressive%20View%20Planning%20via%20Warping%20Consistency%20for%0A%20%20Sparse-View%20Surface%20Reconstruction%0AAuthor%3A%20Sheng%20Ye%20and%20Yuze%20He%20and%20Matthieu%20Lin%20and%20Jenny%20Sheng%20and%20Ruoyu%20Fan%20and%20Yiheng%20Han%20and%20Yubin%20Hu%20and%20Ran%20Yi%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20Neural%20implicit%20representations%20have%20revolutionized%20dense%20multi-view%20surface%0Areconstruction%2C%20yet%20their%20performance%20significantly%20diminishes%20with%20sparse%0Ainput%20views.%20A%20few%20pioneering%20works%20have%20sought%20to%20tackle%20the%20challenge%20of%0Asparse-view%20reconstruction%20by%20leveraging%20additional%20geometric%20priors%20or%0Amulti-scene%20generalizability.%20However%2C%20they%20are%20still%20hindered%20by%20the%20imperfect%0Achoice%20of%20input%20views%2C%20using%20images%20under%20empirically%20determined%20viewpoints%20to%0Aprovide%20considerable%20overlap.%20We%20propose%20PVP-Recon%2C%20a%20novel%20and%20effective%0Asparse-view%20surface%20reconstruction%20method%20that%20progressively%20plans%20the%20next%0Abest%20views%20to%20form%20an%20optimal%20set%20of%20sparse%20viewpoints%20for%20image%20capturing.%0APVP-Recon%20starts%20initial%20surface%20reconstruction%20with%20as%20few%20as%203%20views%20and%0Aprogressively%20adds%20new%20views%20which%20are%20determined%20based%20on%20a%20novel%20warping%0Ascore%20that%20reflects%20the%20information%20gain%20of%20each%20newly%20added%20view.%20This%0Aprogressive%20view%20planning%20progress%20is%20interleaved%20with%20a%20neural%20SDF-based%0Areconstruction%20module%20that%20utilizes%20multi-resolution%20hash%20features%2C%20enhanced%20by%0Aa%20progressive%20training%20scheme%20and%20a%20directional%20Hessian%20loss.%20Quantitative%20and%0Aqualitative%20experiments%20on%20three%20benchmark%20datasets%20show%20that%20our%20framework%0Aachieves%20high-quality%20reconstruction%20with%20a%20constrained%20input%20budget%20and%0Aoutperforms%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPVP-Recon%253A%2520Progressive%2520View%2520Planning%2520via%2520Warping%2520Consistency%2520for%250A%2520%2520Sparse-View%2520Surface%2520Reconstruction%26entry.906535625%3DSheng%2520Ye%2520and%2520Yuze%2520He%2520and%2520Matthieu%2520Lin%2520and%2520Jenny%2520Sheng%2520and%2520Ruoyu%2520Fan%2520and%2520Yiheng%2520Han%2520and%2520Yubin%2520Hu%2520and%2520Ran%2520Yi%2520and%2520Yu-Hui%2520Wen%2520and%2520Yong-Jin%2520Liu%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representations%2520have%2520revolutionized%2520dense%2520multi-view%2520surface%250Areconstruction%252C%2520yet%2520their%2520performance%2520significantly%2520diminishes%2520with%2520sparse%250Ainput%2520views.%2520A%2520few%2520pioneering%2520works%2520have%2520sought%2520to%2520tackle%2520the%2520challenge%2520of%250Asparse-view%2520reconstruction%2520by%2520leveraging%2520additional%2520geometric%2520priors%2520or%250Amulti-scene%2520generalizability.%2520However%252C%2520they%2520are%2520still%2520hindered%2520by%2520the%2520imperfect%250Achoice%2520of%2520input%2520views%252C%2520using%2520images%2520under%2520empirically%2520determined%2520viewpoints%2520to%250Aprovide%2520considerable%2520overlap.%2520We%2520propose%2520PVP-Recon%252C%2520a%2520novel%2520and%2520effective%250Asparse-view%2520surface%2520reconstruction%2520method%2520that%2520progressively%2520plans%2520the%2520next%250Abest%2520views%2520to%2520form%2520an%2520optimal%2520set%2520of%2520sparse%2520viewpoints%2520for%2520image%2520capturing.%250APVP-Recon%2520starts%2520initial%2520surface%2520reconstruction%2520with%2520as%2520few%2520as%25203%2520views%2520and%250Aprogressively%2520adds%2520new%2520views%2520which%2520are%2520determined%2520based%2520on%2520a%2520novel%2520warping%250Ascore%2520that%2520reflects%2520the%2520information%2520gain%2520of%2520each%2520newly%2520added%2520view.%2520This%250Aprogressive%2520view%2520planning%2520progress%2520is%2520interleaved%2520with%2520a%2520neural%2520SDF-based%250Areconstruction%2520module%2520that%2520utilizes%2520multi-resolution%2520hash%2520features%252C%2520enhanced%2520by%250Aa%2520progressive%2520training%2520scheme%2520and%2520a%2520directional%2520Hessian%2520loss.%2520Quantitative%2520and%250Aqualitative%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520show%2520that%2520our%2520framework%250Aachieves%2520high-quality%2520reconstruction%2520with%2520a%2520constrained%2520input%2520budget%2520and%250Aoutperforms%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PVP-Recon%3A%20Progressive%20View%20Planning%20via%20Warping%20Consistency%20for%0A%20%20Sparse-View%20Surface%20Reconstruction&entry.906535625=Sheng%20Ye%20and%20Yuze%20He%20and%20Matthieu%20Lin%20and%20Jenny%20Sheng%20and%20Ruoyu%20Fan%20and%20Yiheng%20Han%20and%20Yubin%20Hu%20and%20Ran%20Yi%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu%20and%20Wenping%20Wang&entry.1292438233=%20%20Neural%20implicit%20representations%20have%20revolutionized%20dense%20multi-view%20surface%0Areconstruction%2C%20yet%20their%20performance%20significantly%20diminishes%20with%20sparse%0Ainput%20views.%20A%20few%20pioneering%20works%20have%20sought%20to%20tackle%20the%20challenge%20of%0Asparse-view%20reconstruction%20by%20leveraging%20additional%20geometric%20priors%20or%0Amulti-scene%20generalizability.%20However%2C%20they%20are%20still%20hindered%20by%20the%20imperfect%0Achoice%20of%20input%20views%2C%20using%20images%20under%20empirically%20determined%20viewpoints%20to%0Aprovide%20considerable%20overlap.%20We%20propose%20PVP-Recon%2C%20a%20novel%20and%20effective%0Asparse-view%20surface%20reconstruction%20method%20that%20progressively%20plans%20the%20next%0Abest%20views%20to%20form%20an%20optimal%20set%20of%20sparse%20viewpoints%20for%20image%20capturing.%0APVP-Recon%20starts%20initial%20surface%20reconstruction%20with%20as%20few%20as%203%20views%20and%0Aprogressively%20adds%20new%20views%20which%20are%20determined%20based%20on%20a%20novel%20warping%0Ascore%20that%20reflects%20the%20information%20gain%20of%20each%20newly%20added%20view.%20This%0Aprogressive%20view%20planning%20progress%20is%20interleaved%20with%20a%20neural%20SDF-based%0Areconstruction%20module%20that%20utilizes%20multi-resolution%20hash%20features%2C%20enhanced%20by%0Aa%20progressive%20training%20scheme%20and%20a%20directional%20Hessian%20loss.%20Quantitative%20and%0Aqualitative%20experiments%20on%20three%20benchmark%20datasets%20show%20that%20our%20framework%0Aachieves%20high-quality%20reconstruction%20with%20a%20constrained%20input%20budget%20and%0Aoutperforms%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05474v1&entry.124074799=Read"},
{"title": "Prediction-Feedback DETR for Temporal Action Detection", "author": "Jihwan Kim and Miso Lee and Cheol-Ho Cho and Jihyun Lee and Jae-Pil Heo", "abstract": "  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n", "link": "http://arxiv.org/abs/2408.16729v2", "date": "2024-09-09", "relevancy": 2.8419, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection&body=Title%3A%20Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection%0AAuthor%3A%20Jihwan%20Kim%20and%20Miso%20Lee%20and%20Cheol-Ho%20Cho%20and%20Jihyun%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Temporal%20Action%20Detection%20%28TAD%29%20is%20fundamental%20yet%20challenging%20for%20real-world%0Avideo%20applications.%20Leveraging%20the%20unique%20benefits%20of%20transformers%2C%20various%0ADETR-based%20approaches%20have%20been%20adopted%20in%20TAD.%20However%2C%20it%20has%20recently%20been%0Aidentified%20that%20the%20attention%20collapse%20in%20self-attention%20causes%20the%20performance%0Adegradation%20of%20DETR%20for%20TAD.%20Building%20upon%20previous%20research%2C%20this%20paper%20newly%0Aaddresses%20the%20attention%20collapse%20problem%20in%20cross-attention%20within%20DETR-based%0ATAD%20methods.%20Moreover%2C%20our%20findings%20reveal%20that%20cross-attention%20exhibits%0Apatterns%20distinct%20from%20predictions%2C%20indicating%20a%20short-cut%20phenomenon.%20To%0Aresolve%20this%2C%20we%20propose%20a%20new%20framework%2C%20Prediction-Feedback%20DETR%20%28Pred-DETR%29%2C%0Awhich%20utilizes%20predictions%20to%20restore%20the%20collapse%20and%20align%20the%20cross-%20and%0Aself-attention%20with%20predictions.%20Specifically%2C%20we%20devise%20novel%0Aprediction-feedback%20objectives%20using%20guidance%20from%20the%20relations%20of%20the%0Apredictions.%20As%20a%20result%2C%20Pred-DETR%20significantly%20alleviates%20the%20collapse%20and%0Aachieves%20state-of-the-art%20performance%20among%20DETR-based%20methods%20on%20various%0Achallenging%20benchmarks%20including%20THUMOS14%2C%20ActivityNet-v1.3%2C%20HACS%2C%20and%0AFineAction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction-Feedback%2520DETR%2520for%2520Temporal%2520Action%2520Detection%26entry.906535625%3DJihwan%2520Kim%2520and%2520Miso%2520Lee%2520and%2520Cheol-Ho%2520Cho%2520and%2520Jihyun%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Detection%2520%2528TAD%2529%2520is%2520fundamental%2520yet%2520challenging%2520for%2520real-world%250Avideo%2520applications.%2520Leveraging%2520the%2520unique%2520benefits%2520of%2520transformers%252C%2520various%250ADETR-based%2520approaches%2520have%2520been%2520adopted%2520in%2520TAD.%2520However%252C%2520it%2520has%2520recently%2520been%250Aidentified%2520that%2520the%2520attention%2520collapse%2520in%2520self-attention%2520causes%2520the%2520performance%250Adegradation%2520of%2520DETR%2520for%2520TAD.%2520Building%2520upon%2520previous%2520research%252C%2520this%2520paper%2520newly%250Aaddresses%2520the%2520attention%2520collapse%2520problem%2520in%2520cross-attention%2520within%2520DETR-based%250ATAD%2520methods.%2520Moreover%252C%2520our%2520findings%2520reveal%2520that%2520cross-attention%2520exhibits%250Apatterns%2520distinct%2520from%2520predictions%252C%2520indicating%2520a%2520short-cut%2520phenomenon.%2520To%250Aresolve%2520this%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520Prediction-Feedback%2520DETR%2520%2528Pred-DETR%2529%252C%250Awhich%2520utilizes%2520predictions%2520to%2520restore%2520the%2520collapse%2520and%2520align%2520the%2520cross-%2520and%250Aself-attention%2520with%2520predictions.%2520Specifically%252C%2520we%2520devise%2520novel%250Aprediction-feedback%2520objectives%2520using%2520guidance%2520from%2520the%2520relations%2520of%2520the%250Apredictions.%2520As%2520a%2520result%252C%2520Pred-DETR%2520significantly%2520alleviates%2520the%2520collapse%2520and%250Aachieves%2520state-of-the-art%2520performance%2520among%2520DETR-based%2520methods%2520on%2520various%250Achallenging%2520benchmarks%2520including%2520THUMOS14%252C%2520ActivityNet-v1.3%252C%2520HACS%252C%2520and%250AFineAction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection&entry.906535625=Jihwan%20Kim%20and%20Miso%20Lee%20and%20Cheol-Ho%20Cho%20and%20Jihyun%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Temporal%20Action%20Detection%20%28TAD%29%20is%20fundamental%20yet%20challenging%20for%20real-world%0Avideo%20applications.%20Leveraging%20the%20unique%20benefits%20of%20transformers%2C%20various%0ADETR-based%20approaches%20have%20been%20adopted%20in%20TAD.%20However%2C%20it%20has%20recently%20been%0Aidentified%20that%20the%20attention%20collapse%20in%20self-attention%20causes%20the%20performance%0Adegradation%20of%20DETR%20for%20TAD.%20Building%20upon%20previous%20research%2C%20this%20paper%20newly%0Aaddresses%20the%20attention%20collapse%20problem%20in%20cross-attention%20within%20DETR-based%0ATAD%20methods.%20Moreover%2C%20our%20findings%20reveal%20that%20cross-attention%20exhibits%0Apatterns%20distinct%20from%20predictions%2C%20indicating%20a%20short-cut%20phenomenon.%20To%0Aresolve%20this%2C%20we%20propose%20a%20new%20framework%2C%20Prediction-Feedback%20DETR%20%28Pred-DETR%29%2C%0Awhich%20utilizes%20predictions%20to%20restore%20the%20collapse%20and%20align%20the%20cross-%20and%0Aself-attention%20with%20predictions.%20Specifically%2C%20we%20devise%20novel%0Aprediction-feedback%20objectives%20using%20guidance%20from%20the%20relations%20of%20the%0Apredictions.%20As%20a%20result%2C%20Pred-DETR%20significantly%20alleviates%20the%20collapse%20and%0Aachieves%20state-of-the-art%20performance%20among%20DETR-based%20methods%20on%20various%0Achallenging%20benchmarks%20including%20THUMOS14%2C%20ActivityNet-v1.3%2C%20HACS%2C%20and%0AFineAction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16729v2&entry.124074799=Read"},
{"title": "Stepping Stones: A Progressive Training Strategy for Audio-Visual\n  Semantic Segmentation", "author": "Juncheng Ma and Peiwen Sun and Yaoting Wang and Di Hu", "abstract": "  Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of\nsound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an\nextension of AVS, further pursues semantic understanding of audio-visual\nscenes. However, since the AVSS task requires the establishment of audio-visual\ncorrespondence and semantic understanding simultaneously, we observe that\nprevious methods have struggled to handle this mashup of objectives in\nend-to-end training, resulting in insufficient learning and sub-optimization.\nTherefore, we propose a two-stage training strategy called \\textit{Stepping\nStones}, which decomposes the AVSS task into two simple subtasks from\nlocalization to semantic understanding, which are fully optimized in each stage\nto achieve step-by-step global optimization. This training strategy has also\nproved its generalization and effectiveness on existing methods. To further\nimprove the performance of AVS tasks, we propose a novel framework Adaptive\nAudio Visual Segmentation, in which we incorporate an adaptive audio query\ngenerator and integrate masked attention into the transformer decoder,\nfacilitating the adaptive fusion of visual and audio features. Extensive\nexperiments demonstrate that our methods achieve state-of-the-art results on\nall three AVS benchmarks. The project homepage can be accessed at\nhttps://gewu-lab.github.io/stepping_stones/.\n", "link": "http://arxiv.org/abs/2407.11820v2", "date": "2024-09-09", "relevancy": 2.8392, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepping%20Stones%3A%20A%20Progressive%20Training%20Strategy%20for%20Audio-Visual%0A%20%20Semantic%20Segmentation&body=Title%3A%20Stepping%20Stones%3A%20A%20Progressive%20Training%20Strategy%20for%20Audio-Visual%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Juncheng%20Ma%20and%20Peiwen%20Sun%20and%20Yaoting%20Wang%20and%20Di%20Hu%0AAbstract%3A%20%20%20Audio-Visual%20Segmentation%20%28AVS%29%20aims%20to%20achieve%20pixel-level%20localization%20of%0Asound%20sources%20in%20videos%2C%20while%20Audio-Visual%20Semantic%20Segmentation%20%28AVSS%29%2C%20as%20an%0Aextension%20of%20AVS%2C%20further%20pursues%20semantic%20understanding%20of%20audio-visual%0Ascenes.%20However%2C%20since%20the%20AVSS%20task%20requires%20the%20establishment%20of%20audio-visual%0Acorrespondence%20and%20semantic%20understanding%20simultaneously%2C%20we%20observe%20that%0Aprevious%20methods%20have%20struggled%20to%20handle%20this%20mashup%20of%20objectives%20in%0Aend-to-end%20training%2C%20resulting%20in%20insufficient%20learning%20and%20sub-optimization.%0ATherefore%2C%20we%20propose%20a%20two-stage%20training%20strategy%20called%20%5Ctextit%7BStepping%0AStones%7D%2C%20which%20decomposes%20the%20AVSS%20task%20into%20two%20simple%20subtasks%20from%0Alocalization%20to%20semantic%20understanding%2C%20which%20are%20fully%20optimized%20in%20each%20stage%0Ato%20achieve%20step-by-step%20global%20optimization.%20This%20training%20strategy%20has%20also%0Aproved%20its%20generalization%20and%20effectiveness%20on%20existing%20methods.%20To%20further%0Aimprove%20the%20performance%20of%20AVS%20tasks%2C%20we%20propose%20a%20novel%20framework%20Adaptive%0AAudio%20Visual%20Segmentation%2C%20in%20which%20we%20incorporate%20an%20adaptive%20audio%20query%0Agenerator%20and%20integrate%20masked%20attention%20into%20the%20transformer%20decoder%2C%0Afacilitating%20the%20adaptive%20fusion%20of%20visual%20and%20audio%20features.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20methods%20achieve%20state-of-the-art%20results%20on%0Aall%20three%20AVS%20benchmarks.%20The%20project%20homepage%20can%20be%20accessed%20at%0Ahttps%3A//gewu-lab.github.io/stepping_stones/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepping%2520Stones%253A%2520A%2520Progressive%2520Training%2520Strategy%2520for%2520Audio-Visual%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DJuncheng%2520Ma%2520and%2520Peiwen%2520Sun%2520and%2520Yaoting%2520Wang%2520and%2520Di%2520Hu%26entry.1292438233%3D%2520%2520Audio-Visual%2520Segmentation%2520%2528AVS%2529%2520aims%2520to%2520achieve%2520pixel-level%2520localization%2520of%250Asound%2520sources%2520in%2520videos%252C%2520while%2520Audio-Visual%2520Semantic%2520Segmentation%2520%2528AVSS%2529%252C%2520as%2520an%250Aextension%2520of%2520AVS%252C%2520further%2520pursues%2520semantic%2520understanding%2520of%2520audio-visual%250Ascenes.%2520However%252C%2520since%2520the%2520AVSS%2520task%2520requires%2520the%2520establishment%2520of%2520audio-visual%250Acorrespondence%2520and%2520semantic%2520understanding%2520simultaneously%252C%2520we%2520observe%2520that%250Aprevious%2520methods%2520have%2520struggled%2520to%2520handle%2520this%2520mashup%2520of%2520objectives%2520in%250Aend-to-end%2520training%252C%2520resulting%2520in%2520insufficient%2520learning%2520and%2520sub-optimization.%250ATherefore%252C%2520we%2520propose%2520a%2520two-stage%2520training%2520strategy%2520called%2520%255Ctextit%257BStepping%250AStones%257D%252C%2520which%2520decomposes%2520the%2520AVSS%2520task%2520into%2520two%2520simple%2520subtasks%2520from%250Alocalization%2520to%2520semantic%2520understanding%252C%2520which%2520are%2520fully%2520optimized%2520in%2520each%2520stage%250Ato%2520achieve%2520step-by-step%2520global%2520optimization.%2520This%2520training%2520strategy%2520has%2520also%250Aproved%2520its%2520generalization%2520and%2520effectiveness%2520on%2520existing%2520methods.%2520To%2520further%250Aimprove%2520the%2520performance%2520of%2520AVS%2520tasks%252C%2520we%2520propose%2520a%2520novel%2520framework%2520Adaptive%250AAudio%2520Visual%2520Segmentation%252C%2520in%2520which%2520we%2520incorporate%2520an%2520adaptive%2520audio%2520query%250Agenerator%2520and%2520integrate%2520masked%2520attention%2520into%2520the%2520transformer%2520decoder%252C%250Afacilitating%2520the%2520adaptive%2520fusion%2520of%2520visual%2520and%2520audio%2520features.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520methods%2520achieve%2520state-of-the-art%2520results%2520on%250Aall%2520three%2520AVS%2520benchmarks.%2520The%2520project%2520homepage%2520can%2520be%2520accessed%2520at%250Ahttps%253A//gewu-lab.github.io/stepping_stones/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepping%20Stones%3A%20A%20Progressive%20Training%20Strategy%20for%20Audio-Visual%0A%20%20Semantic%20Segmentation&entry.906535625=Juncheng%20Ma%20and%20Peiwen%20Sun%20and%20Yaoting%20Wang%20and%20Di%20Hu&entry.1292438233=%20%20Audio-Visual%20Segmentation%20%28AVS%29%20aims%20to%20achieve%20pixel-level%20localization%20of%0Asound%20sources%20in%20videos%2C%20while%20Audio-Visual%20Semantic%20Segmentation%20%28AVSS%29%2C%20as%20an%0Aextension%20of%20AVS%2C%20further%20pursues%20semantic%20understanding%20of%20audio-visual%0Ascenes.%20However%2C%20since%20the%20AVSS%20task%20requires%20the%20establishment%20of%20audio-visual%0Acorrespondence%20and%20semantic%20understanding%20simultaneously%2C%20we%20observe%20that%0Aprevious%20methods%20have%20struggled%20to%20handle%20this%20mashup%20of%20objectives%20in%0Aend-to-end%20training%2C%20resulting%20in%20insufficient%20learning%20and%20sub-optimization.%0ATherefore%2C%20we%20propose%20a%20two-stage%20training%20strategy%20called%20%5Ctextit%7BStepping%0AStones%7D%2C%20which%20decomposes%20the%20AVSS%20task%20into%20two%20simple%20subtasks%20from%0Alocalization%20to%20semantic%20understanding%2C%20which%20are%20fully%20optimized%20in%20each%20stage%0Ato%20achieve%20step-by-step%20global%20optimization.%20This%20training%20strategy%20has%20also%0Aproved%20its%20generalization%20and%20effectiveness%20on%20existing%20methods.%20To%20further%0Aimprove%20the%20performance%20of%20AVS%20tasks%2C%20we%20propose%20a%20novel%20framework%20Adaptive%0AAudio%20Visual%20Segmentation%2C%20in%20which%20we%20incorporate%20an%20adaptive%20audio%20query%0Agenerator%20and%20integrate%20masked%20attention%20into%20the%20transformer%20decoder%2C%0Afacilitating%20the%20adaptive%20fusion%20of%20visual%20and%20audio%20features.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20methods%20achieve%20state-of-the-art%20results%20on%0Aall%20three%20AVS%20benchmarks.%20The%20project%20homepage%20can%20be%20accessed%20at%0Ahttps%3A//gewu-lab.github.io/stepping_stones/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11820v2&entry.124074799=Read"},
{"title": "A Lost Opportunity for Vision-Language Models: A Comparative Study of\n  Online Test-Time Adaptation for Vision-Language Models", "author": "Mario D\u00f6bler and Robert A. Marsden and Tobias Raichle and Bin Yang", "abstract": "  In deep learning, maintaining model robustness against distribution shifts is\ncritical. This work explores a broad range of possibilities to adapt\nvision-language foundation models at test-time, with a particular emphasis on\nCLIP and its variants. The study systematically examines prompt-based\ntechniques and existing test-time adaptation methods, aiming to improve the\nrobustness under distribution shift in diverse real-world scenarios.\nSpecifically, the investigation covers various prompt engineering strategies,\nincluding handcrafted prompts, prompt ensembles, and prompt learning\ntechniques. Additionally, we introduce a vision-text-space ensemble that\nsubstantially enhances average performance compared to text-space-only\nensembles. Since online test-time adaptation has shown to be effective to\nmitigate performance drops under distribution shift, the study extends its\nscope to evaluate the effectiveness of existing test-time adaptation methods\nthat were originally designed for vision-only classification models. Through\nextensive experimental evaluations conducted across multiple datasets and\ndiverse model architectures, the research demonstrates the effectiveness of\nthese adaptation strategies. Code is available at:\nhttps://github.com/mariodoebler/test-time-adaptation\n", "link": "http://arxiv.org/abs/2405.14977v2", "date": "2024-09-09", "relevancy": 2.831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lost%20Opportunity%20for%20Vision-Language%20Models%3A%20A%20Comparative%20Study%20of%0A%20%20Online%20Test-Time%20Adaptation%20for%20Vision-Language%20Models&body=Title%3A%20A%20Lost%20Opportunity%20for%20Vision-Language%20Models%3A%20A%20Comparative%20Study%20of%0A%20%20Online%20Test-Time%20Adaptation%20for%20Vision-Language%20Models%0AAuthor%3A%20Mario%20D%C3%B6bler%20and%20Robert%20A.%20Marsden%20and%20Tobias%20Raichle%20and%20Bin%20Yang%0AAbstract%3A%20%20%20In%20deep%20learning%2C%20maintaining%20model%20robustness%20against%20distribution%20shifts%20is%0Acritical.%20This%20work%20explores%20a%20broad%20range%20of%20possibilities%20to%20adapt%0Avision-language%20foundation%20models%20at%20test-time%2C%20with%20a%20particular%20emphasis%20on%0ACLIP%20and%20its%20variants.%20The%20study%20systematically%20examines%20prompt-based%0Atechniques%20and%20existing%20test-time%20adaptation%20methods%2C%20aiming%20to%20improve%20the%0Arobustness%20under%20distribution%20shift%20in%20diverse%20real-world%20scenarios.%0ASpecifically%2C%20the%20investigation%20covers%20various%20prompt%20engineering%20strategies%2C%0Aincluding%20handcrafted%20prompts%2C%20prompt%20ensembles%2C%20and%20prompt%20learning%0Atechniques.%20Additionally%2C%20we%20introduce%20a%20vision-text-space%20ensemble%20that%0Asubstantially%20enhances%20average%20performance%20compared%20to%20text-space-only%0Aensembles.%20Since%20online%20test-time%20adaptation%20has%20shown%20to%20be%20effective%20to%0Amitigate%20performance%20drops%20under%20distribution%20shift%2C%20the%20study%20extends%20its%0Ascope%20to%20evaluate%20the%20effectiveness%20of%20existing%20test-time%20adaptation%20methods%0Athat%20were%20originally%20designed%20for%20vision-only%20classification%20models.%20Through%0Aextensive%20experimental%20evaluations%20conducted%20across%20multiple%20datasets%20and%0Adiverse%20model%20architectures%2C%20the%20research%20demonstrates%20the%20effectiveness%20of%0Athese%20adaptation%20strategies.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/mariodoebler/test-time-adaptation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lost%2520Opportunity%2520for%2520Vision-Language%2520Models%253A%2520A%2520Comparative%2520Study%2520of%250A%2520%2520Online%2520Test-Time%2520Adaptation%2520for%2520Vision-Language%2520Models%26entry.906535625%3DMario%2520D%25C3%25B6bler%2520and%2520Robert%2520A.%2520Marsden%2520and%2520Tobias%2520Raichle%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520In%2520deep%2520learning%252C%2520maintaining%2520model%2520robustness%2520against%2520distribution%2520shifts%2520is%250Acritical.%2520This%2520work%2520explores%2520a%2520broad%2520range%2520of%2520possibilities%2520to%2520adapt%250Avision-language%2520foundation%2520models%2520at%2520test-time%252C%2520with%2520a%2520particular%2520emphasis%2520on%250ACLIP%2520and%2520its%2520variants.%2520The%2520study%2520systematically%2520examines%2520prompt-based%250Atechniques%2520and%2520existing%2520test-time%2520adaptation%2520methods%252C%2520aiming%2520to%2520improve%2520the%250Arobustness%2520under%2520distribution%2520shift%2520in%2520diverse%2520real-world%2520scenarios.%250ASpecifically%252C%2520the%2520investigation%2520covers%2520various%2520prompt%2520engineering%2520strategies%252C%250Aincluding%2520handcrafted%2520prompts%252C%2520prompt%2520ensembles%252C%2520and%2520prompt%2520learning%250Atechniques.%2520Additionally%252C%2520we%2520introduce%2520a%2520vision-text-space%2520ensemble%2520that%250Asubstantially%2520enhances%2520average%2520performance%2520compared%2520to%2520text-space-only%250Aensembles.%2520Since%2520online%2520test-time%2520adaptation%2520has%2520shown%2520to%2520be%2520effective%2520to%250Amitigate%2520performance%2520drops%2520under%2520distribution%2520shift%252C%2520the%2520study%2520extends%2520its%250Ascope%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520existing%2520test-time%2520adaptation%2520methods%250Athat%2520were%2520originally%2520designed%2520for%2520vision-only%2520classification%2520models.%2520Through%250Aextensive%2520experimental%2520evaluations%2520conducted%2520across%2520multiple%2520datasets%2520and%250Adiverse%2520model%2520architectures%252C%2520the%2520research%2520demonstrates%2520the%2520effectiveness%2520of%250Athese%2520adaptation%2520strategies.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/mariodoebler/test-time-adaptation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lost%20Opportunity%20for%20Vision-Language%20Models%3A%20A%20Comparative%20Study%20of%0A%20%20Online%20Test-Time%20Adaptation%20for%20Vision-Language%20Models&entry.906535625=Mario%20D%C3%B6bler%20and%20Robert%20A.%20Marsden%20and%20Tobias%20Raichle%20and%20Bin%20Yang&entry.1292438233=%20%20In%20deep%20learning%2C%20maintaining%20model%20robustness%20against%20distribution%20shifts%20is%0Acritical.%20This%20work%20explores%20a%20broad%20range%20of%20possibilities%20to%20adapt%0Avision-language%20foundation%20models%20at%20test-time%2C%20with%20a%20particular%20emphasis%20on%0ACLIP%20and%20its%20variants.%20The%20study%20systematically%20examines%20prompt-based%0Atechniques%20and%20existing%20test-time%20adaptation%20methods%2C%20aiming%20to%20improve%20the%0Arobustness%20under%20distribution%20shift%20in%20diverse%20real-world%20scenarios.%0ASpecifically%2C%20the%20investigation%20covers%20various%20prompt%20engineering%20strategies%2C%0Aincluding%20handcrafted%20prompts%2C%20prompt%20ensembles%2C%20and%20prompt%20learning%0Atechniques.%20Additionally%2C%20we%20introduce%20a%20vision-text-space%20ensemble%20that%0Asubstantially%20enhances%20average%20performance%20compared%20to%20text-space-only%0Aensembles.%20Since%20online%20test-time%20adaptation%20has%20shown%20to%20be%20effective%20to%0Amitigate%20performance%20drops%20under%20distribution%20shift%2C%20the%20study%20extends%20its%0Ascope%20to%20evaluate%20the%20effectiveness%20of%20existing%20test-time%20adaptation%20methods%0Athat%20were%20originally%20designed%20for%20vision-only%20classification%20models.%20Through%0Aextensive%20experimental%20evaluations%20conducted%20across%20multiple%20datasets%20and%0Adiverse%20model%20architectures%2C%20the%20research%20demonstrates%20the%20effectiveness%20of%0Athese%20adaptation%20strategies.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/mariodoebler/test-time-adaptation%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14977v2&entry.124074799=Read"},
{"title": "Pre-processing and Compression: Understanding Hidden Representation\n  Refinement Across Imaging Domains via Intrinsic Dimension", "author": "Nicholas Konz and Maciej A. Mazurowski", "abstract": "  In recent years, there has been interest in how geometric properties such as\nintrinsic dimension (ID) of a neural network's hidden representations change\nthrough its layers, and how such properties are predictive of important model\nbehavior such as generalization ability. However, evidence has begun to emerge\nthat such behavior can change significantly depending on the domain of the\nnetwork's training data, such as natural versus medical images. Here, we\nfurther this inquiry by exploring how the ID of a network's learned\nrepresentations changes through its layers, in essence, characterizing how the\nnetwork successively refines the information content of input data to be used\nfor predictions. Analyzing eleven natural and medical image datasets across six\nnetwork architectures, we find that how ID changes through the network differs\nnoticeably between natural and medical image models. Specifically, medical\nimage models peak in representation ID earlier in the network, implying a\ndifference in the image features and their abstractness that are typically used\nfor downstream tasks in these domains. Additionally, we discover a strong\ncorrelation of this peak representation ID with the ID of the data in its input\nspace, implying that the intrinsic information content of a model's learned\nrepresentations is guided by that of the data it was trained on. Overall, our\nfindings emphasize notable discrepancies in network behavior between natural\nand non-natural imaging domains regarding hidden representation information\ncontent, and provide further insights into how a network's learned features are\nshaped by its training data.\n", "link": "http://arxiv.org/abs/2408.08381v3", "date": "2024-09-09", "relevancy": 2.7847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-processing%20and%20Compression%3A%20Understanding%20Hidden%20Representation%0A%20%20Refinement%20Across%20Imaging%20Domains%20via%20Intrinsic%20Dimension&body=Title%3A%20Pre-processing%20and%20Compression%3A%20Understanding%20Hidden%20Representation%0A%20%20Refinement%20Across%20Imaging%20Domains%20via%20Intrinsic%20Dimension%0AAuthor%3A%20Nicholas%20Konz%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20interest%20in%20how%20geometric%20properties%20such%20as%0Aintrinsic%20dimension%20%28ID%29%20of%20a%20neural%20network%27s%20hidden%20representations%20change%0Athrough%20its%20layers%2C%20and%20how%20such%20properties%20are%20predictive%20of%20important%20model%0Abehavior%20such%20as%20generalization%20ability.%20However%2C%20evidence%20has%20begun%20to%20emerge%0Athat%20such%20behavior%20can%20change%20significantly%20depending%20on%20the%20domain%20of%20the%0Anetwork%27s%20training%20data%2C%20such%20as%20natural%20versus%20medical%20images.%20Here%2C%20we%0Afurther%20this%20inquiry%20by%20exploring%20how%20the%20ID%20of%20a%20network%27s%20learned%0Arepresentations%20changes%20through%20its%20layers%2C%20in%20essence%2C%20characterizing%20how%20the%0Anetwork%20successively%20refines%20the%20information%20content%20of%20input%20data%20to%20be%20used%0Afor%20predictions.%20Analyzing%20eleven%20natural%20and%20medical%20image%20datasets%20across%20six%0Anetwork%20architectures%2C%20we%20find%20that%20how%20ID%20changes%20through%20the%20network%20differs%0Anoticeably%20between%20natural%20and%20medical%20image%20models.%20Specifically%2C%20medical%0Aimage%20models%20peak%20in%20representation%20ID%20earlier%20in%20the%20network%2C%20implying%20a%0Adifference%20in%20the%20image%20features%20and%20their%20abstractness%20that%20are%20typically%20used%0Afor%20downstream%20tasks%20in%20these%20domains.%20Additionally%2C%20we%20discover%20a%20strong%0Acorrelation%20of%20this%20peak%20representation%20ID%20with%20the%20ID%20of%20the%20data%20in%20its%20input%0Aspace%2C%20implying%20that%20the%20intrinsic%20information%20content%20of%20a%20model%27s%20learned%0Arepresentations%20is%20guided%20by%20that%20of%20the%20data%20it%20was%20trained%20on.%20Overall%2C%20our%0Afindings%20emphasize%20notable%20discrepancies%20in%20network%20behavior%20between%20natural%0Aand%20non-natural%20imaging%20domains%20regarding%20hidden%20representation%20information%0Acontent%2C%20and%20provide%20further%20insights%20into%20how%20a%20network%27s%20learned%20features%20are%0Ashaped%20by%20its%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08381v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-processing%2520and%2520Compression%253A%2520Understanding%2520Hidden%2520Representation%250A%2520%2520Refinement%2520Across%2520Imaging%2520Domains%2520via%2520Intrinsic%2520Dimension%26entry.906535625%3DNicholas%2520Konz%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520interest%2520in%2520how%2520geometric%2520properties%2520such%2520as%250Aintrinsic%2520dimension%2520%2528ID%2529%2520of%2520a%2520neural%2520network%2527s%2520hidden%2520representations%2520change%250Athrough%2520its%2520layers%252C%2520and%2520how%2520such%2520properties%2520are%2520predictive%2520of%2520important%2520model%250Abehavior%2520such%2520as%2520generalization%2520ability.%2520However%252C%2520evidence%2520has%2520begun%2520to%2520emerge%250Athat%2520such%2520behavior%2520can%2520change%2520significantly%2520depending%2520on%2520the%2520domain%2520of%2520the%250Anetwork%2527s%2520training%2520data%252C%2520such%2520as%2520natural%2520versus%2520medical%2520images.%2520Here%252C%2520we%250Afurther%2520this%2520inquiry%2520by%2520exploring%2520how%2520the%2520ID%2520of%2520a%2520network%2527s%2520learned%250Arepresentations%2520changes%2520through%2520its%2520layers%252C%2520in%2520essence%252C%2520characterizing%2520how%2520the%250Anetwork%2520successively%2520refines%2520the%2520information%2520content%2520of%2520input%2520data%2520to%2520be%2520used%250Afor%2520predictions.%2520Analyzing%2520eleven%2520natural%2520and%2520medical%2520image%2520datasets%2520across%2520six%250Anetwork%2520architectures%252C%2520we%2520find%2520that%2520how%2520ID%2520changes%2520through%2520the%2520network%2520differs%250Anoticeably%2520between%2520natural%2520and%2520medical%2520image%2520models.%2520Specifically%252C%2520medical%250Aimage%2520models%2520peak%2520in%2520representation%2520ID%2520earlier%2520in%2520the%2520network%252C%2520implying%2520a%250Adifference%2520in%2520the%2520image%2520features%2520and%2520their%2520abstractness%2520that%2520are%2520typically%2520used%250Afor%2520downstream%2520tasks%2520in%2520these%2520domains.%2520Additionally%252C%2520we%2520discover%2520a%2520strong%250Acorrelation%2520of%2520this%2520peak%2520representation%2520ID%2520with%2520the%2520ID%2520of%2520the%2520data%2520in%2520its%2520input%250Aspace%252C%2520implying%2520that%2520the%2520intrinsic%2520information%2520content%2520of%2520a%2520model%2527s%2520learned%250Arepresentations%2520is%2520guided%2520by%2520that%2520of%2520the%2520data%2520it%2520was%2520trained%2520on.%2520Overall%252C%2520our%250Afindings%2520emphasize%2520notable%2520discrepancies%2520in%2520network%2520behavior%2520between%2520natural%250Aand%2520non-natural%2520imaging%2520domains%2520regarding%2520hidden%2520representation%2520information%250Acontent%252C%2520and%2520provide%2520further%2520insights%2520into%2520how%2520a%2520network%2527s%2520learned%2520features%2520are%250Ashaped%2520by%2520its%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08381v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-processing%20and%20Compression%3A%20Understanding%20Hidden%20Representation%0A%20%20Refinement%20Across%20Imaging%20Domains%20via%20Intrinsic%20Dimension&entry.906535625=Nicholas%20Konz%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20interest%20in%20how%20geometric%20properties%20such%20as%0Aintrinsic%20dimension%20%28ID%29%20of%20a%20neural%20network%27s%20hidden%20representations%20change%0Athrough%20its%20layers%2C%20and%20how%20such%20properties%20are%20predictive%20of%20important%20model%0Abehavior%20such%20as%20generalization%20ability.%20However%2C%20evidence%20has%20begun%20to%20emerge%0Athat%20such%20behavior%20can%20change%20significantly%20depending%20on%20the%20domain%20of%20the%0Anetwork%27s%20training%20data%2C%20such%20as%20natural%20versus%20medical%20images.%20Here%2C%20we%0Afurther%20this%20inquiry%20by%20exploring%20how%20the%20ID%20of%20a%20network%27s%20learned%0Arepresentations%20changes%20through%20its%20layers%2C%20in%20essence%2C%20characterizing%20how%20the%0Anetwork%20successively%20refines%20the%20information%20content%20of%20input%20data%20to%20be%20used%0Afor%20predictions.%20Analyzing%20eleven%20natural%20and%20medical%20image%20datasets%20across%20six%0Anetwork%20architectures%2C%20we%20find%20that%20how%20ID%20changes%20through%20the%20network%20differs%0Anoticeably%20between%20natural%20and%20medical%20image%20models.%20Specifically%2C%20medical%0Aimage%20models%20peak%20in%20representation%20ID%20earlier%20in%20the%20network%2C%20implying%20a%0Adifference%20in%20the%20image%20features%20and%20their%20abstractness%20that%20are%20typically%20used%0Afor%20downstream%20tasks%20in%20these%20domains.%20Additionally%2C%20we%20discover%20a%20strong%0Acorrelation%20of%20this%20peak%20representation%20ID%20with%20the%20ID%20of%20the%20data%20in%20its%20input%0Aspace%2C%20implying%20that%20the%20intrinsic%20information%20content%20of%20a%20model%27s%20learned%0Arepresentations%20is%20guided%20by%20that%20of%20the%20data%20it%20was%20trained%20on.%20Overall%2C%20our%0Afindings%20emphasize%20notable%20discrepancies%20in%20network%20behavior%20between%20natural%0Aand%20non-natural%20imaging%20domains%20regarding%20hidden%20representation%20information%0Acontent%2C%20and%20provide%20further%20insights%20into%20how%20a%20network%27s%20learned%20features%20are%0Ashaped%20by%20its%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08381v3&entry.124074799=Read"},
{"title": "DiffCSG: Differentiable CSG via Rasterization", "author": "Haocheng Yuan and Adrien Bousseau and Hao Pan and Chengquan Zhang and Niloy J. Mitra and Changjian Li", "abstract": "  Differentiable rendering is a key ingredient for inverse rendering and\nmachine learning, as it allows to optimize scene parameters (shape, materials,\nlighting) to best fit target images. Differentiable rendering requires that\neach scene parameter relates to pixel values through differentiable operations.\nWhile 3D mesh rendering algorithms have been implemented in a differentiable\nway, these algorithms do not directly extend to Constructive-Solid-Geometry\n(CSG), a popular parametric representation of shapes, because the underlying\nboolean operations are typically performed with complex black-box\nmesh-processing libraries. We present an algorithm, DiffCSG, to render CSG\nmodels in a differentiable manner. Our algorithm builds upon CSG rasterization,\nwhich displays the result of boolean operations between primitives without\nexplicitly computing the resulting mesh and, as such, bypasses black-box mesh\nprocessing. We describe how to implement CSG rasterization within a\ndifferentiable rendering pipeline, taking special care to apply antialiasing\nalong primitive intersections to obtain gradients in such critical areas. Our\nalgorithm is simple and fast, can be easily incorporated into modern machine\nlearning setups, and enables a range of applications for computer-aided design,\nincluding direct and image-based editing of CSG primitives. Code and data:\nhttps://yyyyyhc.github.io/DiffCSG/.\n", "link": "http://arxiv.org/abs/2409.01421v2", "date": "2024-09-09", "relevancy": 2.7574, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5565}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.549}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffCSG%3A%20Differentiable%20CSG%20via%20Rasterization&body=Title%3A%20DiffCSG%3A%20Differentiable%20CSG%20via%20Rasterization%0AAuthor%3A%20Haocheng%20Yuan%20and%20Adrien%20Bousseau%20and%20Hao%20Pan%20and%20Chengquan%20Zhang%20and%20Niloy%20J.%20Mitra%20and%20Changjian%20Li%0AAbstract%3A%20%20%20Differentiable%20rendering%20is%20a%20key%20ingredient%20for%20inverse%20rendering%20and%0Amachine%20learning%2C%20as%20it%20allows%20to%20optimize%20scene%20parameters%20%28shape%2C%20materials%2C%0Alighting%29%20to%20best%20fit%20target%20images.%20Differentiable%20rendering%20requires%20that%0Aeach%20scene%20parameter%20relates%20to%20pixel%20values%20through%20differentiable%20operations.%0AWhile%203D%20mesh%20rendering%20algorithms%20have%20been%20implemented%20in%20a%20differentiable%0Away%2C%20these%20algorithms%20do%20not%20directly%20extend%20to%20Constructive-Solid-Geometry%0A%28CSG%29%2C%20a%20popular%20parametric%20representation%20of%20shapes%2C%20because%20the%20underlying%0Aboolean%20operations%20are%20typically%20performed%20with%20complex%20black-box%0Amesh-processing%20libraries.%20We%20present%20an%20algorithm%2C%20DiffCSG%2C%20to%20render%20CSG%0Amodels%20in%20a%20differentiable%20manner.%20Our%20algorithm%20builds%20upon%20CSG%20rasterization%2C%0Awhich%20displays%20the%20result%20of%20boolean%20operations%20between%20primitives%20without%0Aexplicitly%20computing%20the%20resulting%20mesh%20and%2C%20as%20such%2C%20bypasses%20black-box%20mesh%0Aprocessing.%20We%20describe%20how%20to%20implement%20CSG%20rasterization%20within%20a%0Adifferentiable%20rendering%20pipeline%2C%20taking%20special%20care%20to%20apply%20antialiasing%0Aalong%20primitive%20intersections%20to%20obtain%20gradients%20in%20such%20critical%20areas.%20Our%0Aalgorithm%20is%20simple%20and%20fast%2C%20can%20be%20easily%20incorporated%20into%20modern%20machine%0Alearning%20setups%2C%20and%20enables%20a%20range%20of%20applications%20for%20computer-aided%20design%2C%0Aincluding%20direct%20and%20image-based%20editing%20of%20CSG%20primitives.%20Code%20and%20data%3A%0Ahttps%3A//yyyyyhc.github.io/DiffCSG/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffCSG%253A%2520Differentiable%2520CSG%2520via%2520Rasterization%26entry.906535625%3DHaocheng%2520Yuan%2520and%2520Adrien%2520Bousseau%2520and%2520Hao%2520Pan%2520and%2520Chengquan%2520Zhang%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Changjian%2520Li%26entry.1292438233%3D%2520%2520Differentiable%2520rendering%2520is%2520a%2520key%2520ingredient%2520for%2520inverse%2520rendering%2520and%250Amachine%2520learning%252C%2520as%2520it%2520allows%2520to%2520optimize%2520scene%2520parameters%2520%2528shape%252C%2520materials%252C%250Alighting%2529%2520to%2520best%2520fit%2520target%2520images.%2520Differentiable%2520rendering%2520requires%2520that%250Aeach%2520scene%2520parameter%2520relates%2520to%2520pixel%2520values%2520through%2520differentiable%2520operations.%250AWhile%25203D%2520mesh%2520rendering%2520algorithms%2520have%2520been%2520implemented%2520in%2520a%2520differentiable%250Away%252C%2520these%2520algorithms%2520do%2520not%2520directly%2520extend%2520to%2520Constructive-Solid-Geometry%250A%2528CSG%2529%252C%2520a%2520popular%2520parametric%2520representation%2520of%2520shapes%252C%2520because%2520the%2520underlying%250Aboolean%2520operations%2520are%2520typically%2520performed%2520with%2520complex%2520black-box%250Amesh-processing%2520libraries.%2520We%2520present%2520an%2520algorithm%252C%2520DiffCSG%252C%2520to%2520render%2520CSG%250Amodels%2520in%2520a%2520differentiable%2520manner.%2520Our%2520algorithm%2520builds%2520upon%2520CSG%2520rasterization%252C%250Awhich%2520displays%2520the%2520result%2520of%2520boolean%2520operations%2520between%2520primitives%2520without%250Aexplicitly%2520computing%2520the%2520resulting%2520mesh%2520and%252C%2520as%2520such%252C%2520bypasses%2520black-box%2520mesh%250Aprocessing.%2520We%2520describe%2520how%2520to%2520implement%2520CSG%2520rasterization%2520within%2520a%250Adifferentiable%2520rendering%2520pipeline%252C%2520taking%2520special%2520care%2520to%2520apply%2520antialiasing%250Aalong%2520primitive%2520intersections%2520to%2520obtain%2520gradients%2520in%2520such%2520critical%2520areas.%2520Our%250Aalgorithm%2520is%2520simple%2520and%2520fast%252C%2520can%2520be%2520easily%2520incorporated%2520into%2520modern%2520machine%250Alearning%2520setups%252C%2520and%2520enables%2520a%2520range%2520of%2520applications%2520for%2520computer-aided%2520design%252C%250Aincluding%2520direct%2520and%2520image-based%2520editing%2520of%2520CSG%2520primitives.%2520Code%2520and%2520data%253A%250Ahttps%253A//yyyyyhc.github.io/DiffCSG/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffCSG%3A%20Differentiable%20CSG%20via%20Rasterization&entry.906535625=Haocheng%20Yuan%20and%20Adrien%20Bousseau%20and%20Hao%20Pan%20and%20Chengquan%20Zhang%20and%20Niloy%20J.%20Mitra%20and%20Changjian%20Li&entry.1292438233=%20%20Differentiable%20rendering%20is%20a%20key%20ingredient%20for%20inverse%20rendering%20and%0Amachine%20learning%2C%20as%20it%20allows%20to%20optimize%20scene%20parameters%20%28shape%2C%20materials%2C%0Alighting%29%20to%20best%20fit%20target%20images.%20Differentiable%20rendering%20requires%20that%0Aeach%20scene%20parameter%20relates%20to%20pixel%20values%20through%20differentiable%20operations.%0AWhile%203D%20mesh%20rendering%20algorithms%20have%20been%20implemented%20in%20a%20differentiable%0Away%2C%20these%20algorithms%20do%20not%20directly%20extend%20to%20Constructive-Solid-Geometry%0A%28CSG%29%2C%20a%20popular%20parametric%20representation%20of%20shapes%2C%20because%20the%20underlying%0Aboolean%20operations%20are%20typically%20performed%20with%20complex%20black-box%0Amesh-processing%20libraries.%20We%20present%20an%20algorithm%2C%20DiffCSG%2C%20to%20render%20CSG%0Amodels%20in%20a%20differentiable%20manner.%20Our%20algorithm%20builds%20upon%20CSG%20rasterization%2C%0Awhich%20displays%20the%20result%20of%20boolean%20operations%20between%20primitives%20without%0Aexplicitly%20computing%20the%20resulting%20mesh%20and%2C%20as%20such%2C%20bypasses%20black-box%20mesh%0Aprocessing.%20We%20describe%20how%20to%20implement%20CSG%20rasterization%20within%20a%0Adifferentiable%20rendering%20pipeline%2C%20taking%20special%20care%20to%20apply%20antialiasing%0Aalong%20primitive%20intersections%20to%20obtain%20gradients%20in%20such%20critical%20areas.%20Our%0Aalgorithm%20is%20simple%20and%20fast%2C%20can%20be%20easily%20incorporated%20into%20modern%20machine%0Alearning%20setups%2C%20and%20enables%20a%20range%20of%20applications%20for%20computer-aided%20design%2C%0Aincluding%20direct%20and%20image-based%20editing%20of%20CSG%20primitives.%20Code%20and%20data%3A%0Ahttps%3A//yyyyyhc.github.io/DiffCSG/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01421v2&entry.124074799=Read"},
{"title": "ReL-SAR: Representation Learning for Skeleton Action Recognition with\n  Convolutional Transformers and BYOL", "author": "Safwen Naimi and Wassim Bouachir and Guillaume-Alexandre Bilodeau", "abstract": "  To extract robust and generalizable skeleton action recognition features,\nlarge amounts of well-curated data are typically required, which is a\nchallenging task hindered by annotation and computation costs. Therefore,\nunsupervised representation learning is of prime importance to leverage\nunlabeled skeleton data. In this work, we investigate unsupervised\nrepresentation learning for skeleton action recognition. For this purpose, we\ndesigned a lightweight convolutional transformer framework, named ReL-SAR,\nexploiting the complementarity of convolutional and attention layers for\njointly modeling spatial and temporal cues in skeleton sequences. We also use a\nSelection-Permutation strategy for skeleton joints to ensure more informative\ndescriptions from skeletal data. Finally, we capitalize on Bootstrap Your Own\nLatent (BYOL) to learn robust representations from unlabeled skeleton sequence\ndata. We achieved very competitive results on limited-size datasets: MCAD,\nIXMAS, JHMDB, and NW-UCLA, showing the effectiveness of our proposed method\nagainst state-of-the-art methods in terms of both performance and computational\nefficiency. To ensure reproducibility and reusability, the source code\nincluding all implementation parameters is provided at:\nhttps://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL\n", "link": "http://arxiv.org/abs/2409.05749v1", "date": "2024-09-09", "relevancy": 2.689, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5394}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReL-SAR%3A%20Representation%20Learning%20for%20Skeleton%20Action%20Recognition%20with%0A%20%20Convolutional%20Transformers%20and%20BYOL&body=Title%3A%20ReL-SAR%3A%20Representation%20Learning%20for%20Skeleton%20Action%20Recognition%20with%0A%20%20Convolutional%20Transformers%20and%20BYOL%0AAuthor%3A%20Safwen%20Naimi%20and%20Wassim%20Bouachir%20and%20Guillaume-Alexandre%20Bilodeau%0AAbstract%3A%20%20%20To%20extract%20robust%20and%20generalizable%20skeleton%20action%20recognition%20features%2C%0Alarge%20amounts%20of%20well-curated%20data%20are%20typically%20required%2C%20which%20is%20a%0Achallenging%20task%20hindered%20by%20annotation%20and%20computation%20costs.%20Therefore%2C%0Aunsupervised%20representation%20learning%20is%20of%20prime%20importance%20to%20leverage%0Aunlabeled%20skeleton%20data.%20In%20this%20work%2C%20we%20investigate%20unsupervised%0Arepresentation%20learning%20for%20skeleton%20action%20recognition.%20For%20this%20purpose%2C%20we%0Adesigned%20a%20lightweight%20convolutional%20transformer%20framework%2C%20named%20ReL-SAR%2C%0Aexploiting%20the%20complementarity%20of%20convolutional%20and%20attention%20layers%20for%0Ajointly%20modeling%20spatial%20and%20temporal%20cues%20in%20skeleton%20sequences.%20We%20also%20use%20a%0ASelection-Permutation%20strategy%20for%20skeleton%20joints%20to%20ensure%20more%20informative%0Adescriptions%20from%20skeletal%20data.%20Finally%2C%20we%20capitalize%20on%20Bootstrap%20Your%20Own%0ALatent%20%28BYOL%29%20to%20learn%20robust%20representations%20from%20unlabeled%20skeleton%20sequence%0Adata.%20We%20achieved%20very%20competitive%20results%20on%20limited-size%20datasets%3A%20MCAD%2C%0AIXMAS%2C%20JHMDB%2C%20and%20NW-UCLA%2C%20showing%20the%20effectiveness%20of%20our%20proposed%20method%0Aagainst%20state-of-the-art%20methods%20in%20terms%20of%20both%20performance%20and%20computational%0Aefficiency.%20To%20ensure%20reproducibility%20and%20reusability%2C%20the%20source%20code%0Aincluding%20all%20implementation%20parameters%20is%20provided%20at%3A%0Ahttps%3A//github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReL-SAR%253A%2520Representation%2520Learning%2520for%2520Skeleton%2520Action%2520Recognition%2520with%250A%2520%2520Convolutional%2520Transformers%2520and%2520BYOL%26entry.906535625%3DSafwen%2520Naimi%2520and%2520Wassim%2520Bouachir%2520and%2520Guillaume-Alexandre%2520Bilodeau%26entry.1292438233%3D%2520%2520To%2520extract%2520robust%2520and%2520generalizable%2520skeleton%2520action%2520recognition%2520features%252C%250Alarge%2520amounts%2520of%2520well-curated%2520data%2520are%2520typically%2520required%252C%2520which%2520is%2520a%250Achallenging%2520task%2520hindered%2520by%2520annotation%2520and%2520computation%2520costs.%2520Therefore%252C%250Aunsupervised%2520representation%2520learning%2520is%2520of%2520prime%2520importance%2520to%2520leverage%250Aunlabeled%2520skeleton%2520data.%2520In%2520this%2520work%252C%2520we%2520investigate%2520unsupervised%250Arepresentation%2520learning%2520for%2520skeleton%2520action%2520recognition.%2520For%2520this%2520purpose%252C%2520we%250Adesigned%2520a%2520lightweight%2520convolutional%2520transformer%2520framework%252C%2520named%2520ReL-SAR%252C%250Aexploiting%2520the%2520complementarity%2520of%2520convolutional%2520and%2520attention%2520layers%2520for%250Ajointly%2520modeling%2520spatial%2520and%2520temporal%2520cues%2520in%2520skeleton%2520sequences.%2520We%2520also%2520use%2520a%250ASelection-Permutation%2520strategy%2520for%2520skeleton%2520joints%2520to%2520ensure%2520more%2520informative%250Adescriptions%2520from%2520skeletal%2520data.%2520Finally%252C%2520we%2520capitalize%2520on%2520Bootstrap%2520Your%2520Own%250ALatent%2520%2528BYOL%2529%2520to%2520learn%2520robust%2520representations%2520from%2520unlabeled%2520skeleton%2520sequence%250Adata.%2520We%2520achieved%2520very%2520competitive%2520results%2520on%2520limited-size%2520datasets%253A%2520MCAD%252C%250AIXMAS%252C%2520JHMDB%252C%2520and%2520NW-UCLA%252C%2520showing%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%250Aagainst%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520performance%2520and%2520computational%250Aefficiency.%2520To%2520ensure%2520reproducibility%2520and%2520reusability%252C%2520the%2520source%2520code%250Aincluding%2520all%2520implementation%2520parameters%2520is%2520provided%2520at%253A%250Ahttps%253A//github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReL-SAR%3A%20Representation%20Learning%20for%20Skeleton%20Action%20Recognition%20with%0A%20%20Convolutional%20Transformers%20and%20BYOL&entry.906535625=Safwen%20Naimi%20and%20Wassim%20Bouachir%20and%20Guillaume-Alexandre%20Bilodeau&entry.1292438233=%20%20To%20extract%20robust%20and%20generalizable%20skeleton%20action%20recognition%20features%2C%0Alarge%20amounts%20of%20well-curated%20data%20are%20typically%20required%2C%20which%20is%20a%0Achallenging%20task%20hindered%20by%20annotation%20and%20computation%20costs.%20Therefore%2C%0Aunsupervised%20representation%20learning%20is%20of%20prime%20importance%20to%20leverage%0Aunlabeled%20skeleton%20data.%20In%20this%20work%2C%20we%20investigate%20unsupervised%0Arepresentation%20learning%20for%20skeleton%20action%20recognition.%20For%20this%20purpose%2C%20we%0Adesigned%20a%20lightweight%20convolutional%20transformer%20framework%2C%20named%20ReL-SAR%2C%0Aexploiting%20the%20complementarity%20of%20convolutional%20and%20attention%20layers%20for%0Ajointly%20modeling%20spatial%20and%20temporal%20cues%20in%20skeleton%20sequences.%20We%20also%20use%20a%0ASelection-Permutation%20strategy%20for%20skeleton%20joints%20to%20ensure%20more%20informative%0Adescriptions%20from%20skeletal%20data.%20Finally%2C%20we%20capitalize%20on%20Bootstrap%20Your%20Own%0ALatent%20%28BYOL%29%20to%20learn%20robust%20representations%20from%20unlabeled%20skeleton%20sequence%0Adata.%20We%20achieved%20very%20competitive%20results%20on%20limited-size%20datasets%3A%20MCAD%2C%0AIXMAS%2C%20JHMDB%2C%20and%20NW-UCLA%2C%20showing%20the%20effectiveness%20of%20our%20proposed%20method%0Aagainst%20state-of-the-art%20methods%20in%20terms%20of%20both%20performance%20and%20computational%0Aefficiency.%20To%20ensure%20reproducibility%20and%20reusability%2C%20the%20source%20code%0Aincluding%20all%20implementation%20parameters%20is%20provided%20at%3A%0Ahttps%3A//github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05749v1&entry.124074799=Read"},
{"title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models", "author": "Emily Cheng and Richard J. Antonello", "abstract": "  Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.\n", "link": "http://arxiv.org/abs/2409.05771v1", "date": "2024-09-09", "relevancy": 2.6748, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidence%20from%20fMRI%20Supports%20a%20Two-Phase%20Abstraction%20Process%20in%20Language%0A%20%20Models&body=Title%3A%20Evidence%20from%20fMRI%20Supports%20a%20Two-Phase%20Abstraction%20Process%20in%20Language%0A%20%20Models%0AAuthor%3A%20Emily%20Cheng%20and%20Richard%20J.%20Antonello%0AAbstract%3A%20%20%20Research%20has%20repeatedly%20demonstrated%20that%20intermediate%20hidden%20states%0Aextracted%20from%20large%20language%20models%20are%20able%20to%20predict%20measured%20brain%0Aresponse%20to%20natural%20language%20stimuli.%20Yet%2C%20very%20little%20is%20known%20about%20the%0Arepresentation%20properties%20that%20enable%20this%20high%20prediction%20performance.%20Why%20is%0Ait%20the%20intermediate%20layers%2C%20and%20not%20the%20output%20layers%2C%20that%20are%20most%20capable%0Afor%20this%20unique%20and%20highly%20general%20transfer%20task%3F%20In%20this%20work%2C%20we%20show%20that%0Aevidence%20from%20language%20encoding%20models%20in%20fMRI%20supports%20the%20existence%20of%20a%0Atwo-phase%20abstraction%20process%20within%20LLMs.%20We%20use%20manifold%20learning%20methods%20to%0Ashow%20that%20this%20abstraction%20process%20naturally%20arises%20over%20the%20course%20of%20training%0Aa%20language%20model%20and%20that%20the%20first%20%22composition%22%20phase%20of%20this%20abstraction%0Aprocess%20is%20compressed%20into%20fewer%20layers%20as%20training%20continues.%20Finally%2C%20we%0Ademonstrate%20a%20strong%20correspondence%20between%20layerwise%20encoding%20performance%20and%0Athe%20intrinsic%20dimensionality%20of%20representations%20from%20LLMs.%20We%20give%20initial%0Aevidence%20that%20this%20correspondence%20primarily%20derives%20from%20the%20inherent%0Acompositionality%20of%20LLMs%20and%20not%20their%20next-word%20prediction%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidence%2520from%2520fMRI%2520Supports%2520a%2520Two-Phase%2520Abstraction%2520Process%2520in%2520Language%250A%2520%2520Models%26entry.906535625%3DEmily%2520Cheng%2520and%2520Richard%2520J.%2520Antonello%26entry.1292438233%3D%2520%2520Research%2520has%2520repeatedly%2520demonstrated%2520that%2520intermediate%2520hidden%2520states%250Aextracted%2520from%2520large%2520language%2520models%2520are%2520able%2520to%2520predict%2520measured%2520brain%250Aresponse%2520to%2520natural%2520language%2520stimuli.%2520Yet%252C%2520very%2520little%2520is%2520known%2520about%2520the%250Arepresentation%2520properties%2520that%2520enable%2520this%2520high%2520prediction%2520performance.%2520Why%2520is%250Ait%2520the%2520intermediate%2520layers%252C%2520and%2520not%2520the%2520output%2520layers%252C%2520that%2520are%2520most%2520capable%250Afor%2520this%2520unique%2520and%2520highly%2520general%2520transfer%2520task%253F%2520In%2520this%2520work%252C%2520we%2520show%2520that%250Aevidence%2520from%2520language%2520encoding%2520models%2520in%2520fMRI%2520supports%2520the%2520existence%2520of%2520a%250Atwo-phase%2520abstraction%2520process%2520within%2520LLMs.%2520We%2520use%2520manifold%2520learning%2520methods%2520to%250Ashow%2520that%2520this%2520abstraction%2520process%2520naturally%2520arises%2520over%2520the%2520course%2520of%2520training%250Aa%2520language%2520model%2520and%2520that%2520the%2520first%2520%2522composition%2522%2520phase%2520of%2520this%2520abstraction%250Aprocess%2520is%2520compressed%2520into%2520fewer%2520layers%2520as%2520training%2520continues.%2520Finally%252C%2520we%250Ademonstrate%2520a%2520strong%2520correspondence%2520between%2520layerwise%2520encoding%2520performance%2520and%250Athe%2520intrinsic%2520dimensionality%2520of%2520representations%2520from%2520LLMs.%2520We%2520give%2520initial%250Aevidence%2520that%2520this%2520correspondence%2520primarily%2520derives%2520from%2520the%2520inherent%250Acompositionality%2520of%2520LLMs%2520and%2520not%2520their%2520next-word%2520prediction%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidence%20from%20fMRI%20Supports%20a%20Two-Phase%20Abstraction%20Process%20in%20Language%0A%20%20Models&entry.906535625=Emily%20Cheng%20and%20Richard%20J.%20Antonello&entry.1292438233=%20%20Research%20has%20repeatedly%20demonstrated%20that%20intermediate%20hidden%20states%0Aextracted%20from%20large%20language%20models%20are%20able%20to%20predict%20measured%20brain%0Aresponse%20to%20natural%20language%20stimuli.%20Yet%2C%20very%20little%20is%20known%20about%20the%0Arepresentation%20properties%20that%20enable%20this%20high%20prediction%20performance.%20Why%20is%0Ait%20the%20intermediate%20layers%2C%20and%20not%20the%20output%20layers%2C%20that%20are%20most%20capable%0Afor%20this%20unique%20and%20highly%20general%20transfer%20task%3F%20In%20this%20work%2C%20we%20show%20that%0Aevidence%20from%20language%20encoding%20models%20in%20fMRI%20supports%20the%20existence%20of%20a%0Atwo-phase%20abstraction%20process%20within%20LLMs.%20We%20use%20manifold%20learning%20methods%20to%0Ashow%20that%20this%20abstraction%20process%20naturally%20arises%20over%20the%20course%20of%20training%0Aa%20language%20model%20and%20that%20the%20first%20%22composition%22%20phase%20of%20this%20abstraction%0Aprocess%20is%20compressed%20into%20fewer%20layers%20as%20training%20continues.%20Finally%2C%20we%0Ademonstrate%20a%20strong%20correspondence%20between%20layerwise%20encoding%20performance%20and%0Athe%20intrinsic%20dimensionality%20of%20representations%20from%20LLMs.%20We%20give%20initial%0Aevidence%20that%20this%20correspondence%20primarily%20derives%20from%20the%20inherent%0Acompositionality%20of%20LLMs%20and%20not%20their%20next-word%20prediction%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05771v1&entry.124074799=Read"},
{"title": "Replay Consolidation with Label Propagation for Continual Object\n  Detection", "author": "Riccardo De Monte and Davide Dalle Pezze and Marina Ceccon and Francesco Pasti and Francesco Paissan and Elisabetta Farella and Gian Antonio Susto and Nicola Bellotto", "abstract": "  Object Detection is a highly relevant computer vision problem with many\napplications such as robotics and autonomous driving. Continual Learning~(CL)\nconsiders a setting where a model incrementally learns new information while\nretaining previously acquired knowledge. This is particularly challenging since\nDeep Learning models tend to catastrophically forget old knowledge while\ntraining on new data. In particular, Continual Learning for Object\nDetection~(CLOD) poses additional difficulties compared to CL for\nClassification. In CLOD, images from previous tasks may contain unknown classes\nthat could reappear labeled in future tasks. These missing annotations cause\ntask interference issues for replay-based approaches. As a result, most works\nin the literature have focused on distillation-based approaches. However, these\napproaches are effective only when there is a strong overlap of classes across\ntasks. To address the issues of current methodologies, we propose a novel\ntechnique to solve CLOD called Replay Consolidation with Label Propagation for\nObject Detection (RCLPOD). Based on the replay method, our solution avoids task\ninterference issues by enhancing the buffer memory samples. Our method is\nevaluated against existing techniques in CLOD literature, demonstrating its\nsuperior performance on established benchmarks like VOC and COCO.\n", "link": "http://arxiv.org/abs/2409.05650v1", "date": "2024-09-09", "relevancy": 2.674, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5425}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Replay%20Consolidation%20with%20Label%20Propagation%20for%20Continual%20Object%0A%20%20Detection&body=Title%3A%20Replay%20Consolidation%20with%20Label%20Propagation%20for%20Continual%20Object%0A%20%20Detection%0AAuthor%3A%20Riccardo%20De%20Monte%20and%20Davide%20Dalle%20Pezze%20and%20Marina%20Ceccon%20and%20Francesco%20Pasti%20and%20Francesco%20Paissan%20and%20Elisabetta%20Farella%20and%20Gian%20Antonio%20Susto%20and%20Nicola%20Bellotto%0AAbstract%3A%20%20%20Object%20Detection%20is%20a%20highly%20relevant%20computer%20vision%20problem%20with%20many%0Aapplications%20such%20as%20robotics%20and%20autonomous%20driving.%20Continual%20Learning~%28CL%29%0Aconsiders%20a%20setting%20where%20a%20model%20incrementally%20learns%20new%20information%20while%0Aretaining%20previously%20acquired%20knowledge.%20This%20is%20particularly%20challenging%20since%0ADeep%20Learning%20models%20tend%20to%20catastrophically%20forget%20old%20knowledge%20while%0Atraining%20on%20new%20data.%20In%20particular%2C%20Continual%20Learning%20for%20Object%0ADetection~%28CLOD%29%20poses%20additional%20difficulties%20compared%20to%20CL%20for%0AClassification.%20In%20CLOD%2C%20images%20from%20previous%20tasks%20may%20contain%20unknown%20classes%0Athat%20could%20reappear%20labeled%20in%20future%20tasks.%20These%20missing%20annotations%20cause%0Atask%20interference%20issues%20for%20replay-based%20approaches.%20As%20a%20result%2C%20most%20works%0Ain%20the%20literature%20have%20focused%20on%20distillation-based%20approaches.%20However%2C%20these%0Aapproaches%20are%20effective%20only%20when%20there%20is%20a%20strong%20overlap%20of%20classes%20across%0Atasks.%20To%20address%20the%20issues%20of%20current%20methodologies%2C%20we%20propose%20a%20novel%0Atechnique%20to%20solve%20CLOD%20called%20Replay%20Consolidation%20with%20Label%20Propagation%20for%0AObject%20Detection%20%28RCLPOD%29.%20Based%20on%20the%20replay%20method%2C%20our%20solution%20avoids%20task%0Ainterference%20issues%20by%20enhancing%20the%20buffer%20memory%20samples.%20Our%20method%20is%0Aevaluated%20against%20existing%20techniques%20in%20CLOD%20literature%2C%20demonstrating%20its%0Asuperior%20performance%20on%20established%20benchmarks%20like%20VOC%20and%20COCO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReplay%2520Consolidation%2520with%2520Label%2520Propagation%2520for%2520Continual%2520Object%250A%2520%2520Detection%26entry.906535625%3DRiccardo%2520De%2520Monte%2520and%2520Davide%2520Dalle%2520Pezze%2520and%2520Marina%2520Ceccon%2520and%2520Francesco%2520Pasti%2520and%2520Francesco%2520Paissan%2520and%2520Elisabetta%2520Farella%2520and%2520Gian%2520Antonio%2520Susto%2520and%2520Nicola%2520Bellotto%26entry.1292438233%3D%2520%2520Object%2520Detection%2520is%2520a%2520highly%2520relevant%2520computer%2520vision%2520problem%2520with%2520many%250Aapplications%2520such%2520as%2520robotics%2520and%2520autonomous%2520driving.%2520Continual%2520Learning~%2528CL%2529%250Aconsiders%2520a%2520setting%2520where%2520a%2520model%2520incrementally%2520learns%2520new%2520information%2520while%250Aretaining%2520previously%2520acquired%2520knowledge.%2520This%2520is%2520particularly%2520challenging%2520since%250ADeep%2520Learning%2520models%2520tend%2520to%2520catastrophically%2520forget%2520old%2520knowledge%2520while%250Atraining%2520on%2520new%2520data.%2520In%2520particular%252C%2520Continual%2520Learning%2520for%2520Object%250ADetection~%2528CLOD%2529%2520poses%2520additional%2520difficulties%2520compared%2520to%2520CL%2520for%250AClassification.%2520In%2520CLOD%252C%2520images%2520from%2520previous%2520tasks%2520may%2520contain%2520unknown%2520classes%250Athat%2520could%2520reappear%2520labeled%2520in%2520future%2520tasks.%2520These%2520missing%2520annotations%2520cause%250Atask%2520interference%2520issues%2520for%2520replay-based%2520approaches.%2520As%2520a%2520result%252C%2520most%2520works%250Ain%2520the%2520literature%2520have%2520focused%2520on%2520distillation-based%2520approaches.%2520However%252C%2520these%250Aapproaches%2520are%2520effective%2520only%2520when%2520there%2520is%2520a%2520strong%2520overlap%2520of%2520classes%2520across%250Atasks.%2520To%2520address%2520the%2520issues%2520of%2520current%2520methodologies%252C%2520we%2520propose%2520a%2520novel%250Atechnique%2520to%2520solve%2520CLOD%2520called%2520Replay%2520Consolidation%2520with%2520Label%2520Propagation%2520for%250AObject%2520Detection%2520%2528RCLPOD%2529.%2520Based%2520on%2520the%2520replay%2520method%252C%2520our%2520solution%2520avoids%2520task%250Ainterference%2520issues%2520by%2520enhancing%2520the%2520buffer%2520memory%2520samples.%2520Our%2520method%2520is%250Aevaluated%2520against%2520existing%2520techniques%2520in%2520CLOD%2520literature%252C%2520demonstrating%2520its%250Asuperior%2520performance%2520on%2520established%2520benchmarks%2520like%2520VOC%2520and%2520COCO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Replay%20Consolidation%20with%20Label%20Propagation%20for%20Continual%20Object%0A%20%20Detection&entry.906535625=Riccardo%20De%20Monte%20and%20Davide%20Dalle%20Pezze%20and%20Marina%20Ceccon%20and%20Francesco%20Pasti%20and%20Francesco%20Paissan%20and%20Elisabetta%20Farella%20and%20Gian%20Antonio%20Susto%20and%20Nicola%20Bellotto&entry.1292438233=%20%20Object%20Detection%20is%20a%20highly%20relevant%20computer%20vision%20problem%20with%20many%0Aapplications%20such%20as%20robotics%20and%20autonomous%20driving.%20Continual%20Learning~%28CL%29%0Aconsiders%20a%20setting%20where%20a%20model%20incrementally%20learns%20new%20information%20while%0Aretaining%20previously%20acquired%20knowledge.%20This%20is%20particularly%20challenging%20since%0ADeep%20Learning%20models%20tend%20to%20catastrophically%20forget%20old%20knowledge%20while%0Atraining%20on%20new%20data.%20In%20particular%2C%20Continual%20Learning%20for%20Object%0ADetection~%28CLOD%29%20poses%20additional%20difficulties%20compared%20to%20CL%20for%0AClassification.%20In%20CLOD%2C%20images%20from%20previous%20tasks%20may%20contain%20unknown%20classes%0Athat%20could%20reappear%20labeled%20in%20future%20tasks.%20These%20missing%20annotations%20cause%0Atask%20interference%20issues%20for%20replay-based%20approaches.%20As%20a%20result%2C%20most%20works%0Ain%20the%20literature%20have%20focused%20on%20distillation-based%20approaches.%20However%2C%20these%0Aapproaches%20are%20effective%20only%20when%20there%20is%20a%20strong%20overlap%20of%20classes%20across%0Atasks.%20To%20address%20the%20issues%20of%20current%20methodologies%2C%20we%20propose%20a%20novel%0Atechnique%20to%20solve%20CLOD%20called%20Replay%20Consolidation%20with%20Label%20Propagation%20for%0AObject%20Detection%20%28RCLPOD%29.%20Based%20on%20the%20replay%20method%2C%20our%20solution%20avoids%20task%0Ainterference%20issues%20by%20enhancing%20the%20buffer%20memory%20samples.%20Our%20method%20is%0Aevaluated%20against%20existing%20techniques%20in%20CLOD%20literature%2C%20demonstrating%20its%0Asuperior%20performance%20on%20established%20benchmarks%20like%20VOC%20and%20COCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05650v1&entry.124074799=Read"},
{"title": "Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning", "author": "Luoyi Sun and Xuenan Xu and Mengyue Wu and Weidi Xie", "abstract": "  Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks.\n", "link": "http://arxiv.org/abs/2309.11500v4", "date": "2024-09-09", "relevancy": 2.6686, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5442}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.534}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-ACD%3A%20A%20Large-scale%20Dataset%20for%20Audio-Language%20Representation%0A%20%20Learning&body=Title%3A%20Auto-ACD%3A%20A%20Large-scale%20Dataset%20for%20Audio-Language%20Representation%0A%20%20Learning%0AAuthor%3A%20Luoyi%20Sun%20and%20Xuenan%20Xu%20and%20Mengyue%20Wu%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20Recently%2C%20the%20AI%20community%20has%20made%20significant%20strides%20in%20developing%0Apowerful%20foundation%20models%2C%20driven%20by%20large-scale%20multimodal%20datasets.%20However%2C%0Afor%20audio%20representation%20learning%2C%20existing%20datasets%20suffer%20from%20limitations%20in%0Athe%20following%20aspects%3A%20insufficient%20volume%2C%20simplistic%20content%2C%20and%20arduous%0Acollection%20procedures.%20To%20establish%20an%20audio%20dataset%20with%20high-quality%0Acaptions%2C%20we%20propose%20an%20innovative%2C%20automatic%20approach%20leveraging%20multimodal%0Ainputs%2C%20such%20as%20video%20frames%2C%20audio%20streams.%20Specifically%2C%20we%20construct%20a%0Alarge-scale%2C%20high-quality%2C%20audio-language%20dataset%2C%20named%20as%20Auto-ACD%2C%0Acomprising%20over%201.5M%20audio-text%20pairs.%20We%20exploit%20a%20series%20of%20pre-trained%0Amodels%20or%20APIs%2C%20to%20determine%20audio-visual%20synchronisation%2C%20generate%20image%0Acaptions%2C%20object%20detection%2C%20or%20audio%20tags%20for%20specific%20videos.%20Subsequently%2C%20we%0Aemploy%20LLM%20to%20paraphrase%20a%20congruent%20caption%20for%20each%20audio%2C%20guided%20by%20the%0Aextracted%20multi-modality%20clues.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20dataset%2C%20we%20train%20widely%20used%20models%20on%20our%20dataset%20and%20show%0Aperformance%20improvement%20on%20various%20downstream%20tasks%2C%20for%20example%2C%0Aaudio-language%20retrieval%2C%20audio%20captioning%2C%20zero-shot%20classification.%20In%0Aaddition%2C%20we%20establish%20a%20novel%20benchmark%20with%20environmental%20information%20and%0Aprovide%20a%20benchmark%20for%20audio-text%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11500v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-ACD%253A%2520A%2520Large-scale%2520Dataset%2520for%2520Audio-Language%2520Representation%250A%2520%2520Learning%26entry.906535625%3DLuoyi%2520Sun%2520and%2520Xuenan%2520Xu%2520and%2520Mengyue%2520Wu%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520AI%2520community%2520has%2520made%2520significant%2520strides%2520in%2520developing%250Apowerful%2520foundation%2520models%252C%2520driven%2520by%2520large-scale%2520multimodal%2520datasets.%2520However%252C%250Afor%2520audio%2520representation%2520learning%252C%2520existing%2520datasets%2520suffer%2520from%2520limitations%2520in%250Athe%2520following%2520aspects%253A%2520insufficient%2520volume%252C%2520simplistic%2520content%252C%2520and%2520arduous%250Acollection%2520procedures.%2520To%2520establish%2520an%2520audio%2520dataset%2520with%2520high-quality%250Acaptions%252C%2520we%2520propose%2520an%2520innovative%252C%2520automatic%2520approach%2520leveraging%2520multimodal%250Ainputs%252C%2520such%2520as%2520video%2520frames%252C%2520audio%2520streams.%2520Specifically%252C%2520we%2520construct%2520a%250Alarge-scale%252C%2520high-quality%252C%2520audio-language%2520dataset%252C%2520named%2520as%2520Auto-ACD%252C%250Acomprising%2520over%25201.5M%2520audio-text%2520pairs.%2520We%2520exploit%2520a%2520series%2520of%2520pre-trained%250Amodels%2520or%2520APIs%252C%2520to%2520determine%2520audio-visual%2520synchronisation%252C%2520generate%2520image%250Acaptions%252C%2520object%2520detection%252C%2520or%2520audio%2520tags%2520for%2520specific%2520videos.%2520Subsequently%252C%2520we%250Aemploy%2520LLM%2520to%2520paraphrase%2520a%2520congruent%2520caption%2520for%2520each%2520audio%252C%2520guided%2520by%2520the%250Aextracted%2520multi-modality%2520clues.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520dataset%252C%2520we%2520train%2520widely%2520used%2520models%2520on%2520our%2520dataset%2520and%2520show%250Aperformance%2520improvement%2520on%2520various%2520downstream%2520tasks%252C%2520for%2520example%252C%250Aaudio-language%2520retrieval%252C%2520audio%2520captioning%252C%2520zero-shot%2520classification.%2520In%250Aaddition%252C%2520we%2520establish%2520a%2520novel%2520benchmark%2520with%2520environmental%2520information%2520and%250Aprovide%2520a%2520benchmark%2520for%2520audio-text%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11500v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-ACD%3A%20A%20Large-scale%20Dataset%20for%20Audio-Language%20Representation%0A%20%20Learning&entry.906535625=Luoyi%20Sun%20and%20Xuenan%20Xu%20and%20Mengyue%20Wu%20and%20Weidi%20Xie&entry.1292438233=%20%20Recently%2C%20the%20AI%20community%20has%20made%20significant%20strides%20in%20developing%0Apowerful%20foundation%20models%2C%20driven%20by%20large-scale%20multimodal%20datasets.%20However%2C%0Afor%20audio%20representation%20learning%2C%20existing%20datasets%20suffer%20from%20limitations%20in%0Athe%20following%20aspects%3A%20insufficient%20volume%2C%20simplistic%20content%2C%20and%20arduous%0Acollection%20procedures.%20To%20establish%20an%20audio%20dataset%20with%20high-quality%0Acaptions%2C%20we%20propose%20an%20innovative%2C%20automatic%20approach%20leveraging%20multimodal%0Ainputs%2C%20such%20as%20video%20frames%2C%20audio%20streams.%20Specifically%2C%20we%20construct%20a%0Alarge-scale%2C%20high-quality%2C%20audio-language%20dataset%2C%20named%20as%20Auto-ACD%2C%0Acomprising%20over%201.5M%20audio-text%20pairs.%20We%20exploit%20a%20series%20of%20pre-trained%0Amodels%20or%20APIs%2C%20to%20determine%20audio-visual%20synchronisation%2C%20generate%20image%0Acaptions%2C%20object%20detection%2C%20or%20audio%20tags%20for%20specific%20videos.%20Subsequently%2C%20we%0Aemploy%20LLM%20to%20paraphrase%20a%20congruent%20caption%20for%20each%20audio%2C%20guided%20by%20the%0Aextracted%20multi-modality%20clues.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20dataset%2C%20we%20train%20widely%20used%20models%20on%20our%20dataset%20and%20show%0Aperformance%20improvement%20on%20various%20downstream%20tasks%2C%20for%20example%2C%0Aaudio-language%20retrieval%2C%20audio%20captioning%2C%20zero-shot%20classification.%20In%0Aaddition%2C%20we%20establish%20a%20novel%20benchmark%20with%20environmental%20information%20and%0Aprovide%20a%20benchmark%20for%20audio-text%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11500v4&entry.124074799=Read"},
{"title": "Learning to Model Graph Structural Information on MLPs via Graph\n  Structure Self-Contrasting", "author": "Lirong Wu and Haitao Lin and Guojiang Zhao and Cheng Tan and Stan Z. Li", "abstract": "  Recent years have witnessed great success in handling graph-related tasks\nwith Graph Neural Networks (GNNs). However, most existing GNNs are based on\nmessage passing to perform feature aggregation and transformation, where the\nstructural information is explicitly involved in the forward propagation by\ncoupling with node features through graph convolution at each layer. As a\nresult, subtle feature noise or structure perturbation may cause severe error\npropagation, resulting in extremely poor robustness. In this paper, we rethink\nthe roles played by graph structural information in graph data training and\nidentify that message passing is not the only path to modeling structural\ninformation. Inspired by this, we propose a simple but effective Graph\nStructure Self-Contrasting (GSSC) framework that learns graph structural\ninformation without message passing. The proposed framework is based purely on\nMulti-Layer Perceptrons (MLPs), where the structural information is only\nimplicitly incorporated as prior knowledge to guide the computation of\nsupervision signals, substituting the explicit message propagation as in GNNs.\nSpecifically, it first applies structural sparsification to remove potentially\nuninformative or noisy edges in the neighborhood, and then performs structural\nself-contrasting in the sparsified neighborhood to learn robust node\nrepresentations. Finally, structural sparsification and self-contrasting are\nformulated as a bi-level optimization problem and solved in a unified\nframework. Extensive experiments have qualitatively and quantitatively\ndemonstrated that the GSSC framework can produce truly encouraging performance\nwith better generalization and robustness than other leading competitors.\n", "link": "http://arxiv.org/abs/2409.05573v1", "date": "2024-09-09", "relevancy": 2.6462, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5889}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5014}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Model%20Graph%20Structural%20Information%20on%20MLPs%20via%20Graph%0A%20%20Structure%20Self-Contrasting&body=Title%3A%20Learning%20to%20Model%20Graph%20Structural%20Information%20on%20MLPs%20via%20Graph%0A%20%20Structure%20Self-Contrasting%0AAuthor%3A%20Lirong%20Wu%20and%20Haitao%20Lin%20and%20Guojiang%20Zhao%20and%20Cheng%20Tan%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20great%20success%20in%20handling%20graph-related%20tasks%0Awith%20Graph%20Neural%20Networks%20%28GNNs%29.%20However%2C%20most%20existing%20GNNs%20are%20based%20on%0Amessage%20passing%20to%20perform%20feature%20aggregation%20and%20transformation%2C%20where%20the%0Astructural%20information%20is%20explicitly%20involved%20in%20the%20forward%20propagation%20by%0Acoupling%20with%20node%20features%20through%20graph%20convolution%20at%20each%20layer.%20As%20a%0Aresult%2C%20subtle%20feature%20noise%20or%20structure%20perturbation%20may%20cause%20severe%20error%0Apropagation%2C%20resulting%20in%20extremely%20poor%20robustness.%20In%20this%20paper%2C%20we%20rethink%0Athe%20roles%20played%20by%20graph%20structural%20information%20in%20graph%20data%20training%20and%0Aidentify%20that%20message%20passing%20is%20not%20the%20only%20path%20to%20modeling%20structural%0Ainformation.%20Inspired%20by%20this%2C%20we%20propose%20a%20simple%20but%20effective%20Graph%0AStructure%20Self-Contrasting%20%28GSSC%29%20framework%20that%20learns%20graph%20structural%0Ainformation%20without%20message%20passing.%20The%20proposed%20framework%20is%20based%20purely%20on%0AMulti-Layer%20Perceptrons%20%28MLPs%29%2C%20where%20the%20structural%20information%20is%20only%0Aimplicitly%20incorporated%20as%20prior%20knowledge%20to%20guide%20the%20computation%20of%0Asupervision%20signals%2C%20substituting%20the%20explicit%20message%20propagation%20as%20in%20GNNs.%0ASpecifically%2C%20it%20first%20applies%20structural%20sparsification%20to%20remove%20potentially%0Auninformative%20or%20noisy%20edges%20in%20the%20neighborhood%2C%20and%20then%20performs%20structural%0Aself-contrasting%20in%20the%20sparsified%20neighborhood%20to%20learn%20robust%20node%0Arepresentations.%20Finally%2C%20structural%20sparsification%20and%20self-contrasting%20are%0Aformulated%20as%20a%20bi-level%20optimization%20problem%20and%20solved%20in%20a%20unified%0Aframework.%20Extensive%20experiments%20have%20qualitatively%20and%20quantitatively%0Ademonstrated%20that%20the%20GSSC%20framework%20can%20produce%20truly%20encouraging%20performance%0Awith%20better%20generalization%20and%20robustness%20than%20other%20leading%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Model%2520Graph%2520Structural%2520Information%2520on%2520MLPs%2520via%2520Graph%250A%2520%2520Structure%2520Self-Contrasting%26entry.906535625%3DLirong%2520Wu%2520and%2520Haitao%2520Lin%2520and%2520Guojiang%2520Zhao%2520and%2520Cheng%2520Tan%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520great%2520success%2520in%2520handling%2520graph-related%2520tasks%250Awith%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520However%252C%2520most%2520existing%2520GNNs%2520are%2520based%2520on%250Amessage%2520passing%2520to%2520perform%2520feature%2520aggregation%2520and%2520transformation%252C%2520where%2520the%250Astructural%2520information%2520is%2520explicitly%2520involved%2520in%2520the%2520forward%2520propagation%2520by%250Acoupling%2520with%2520node%2520features%2520through%2520graph%2520convolution%2520at%2520each%2520layer.%2520As%2520a%250Aresult%252C%2520subtle%2520feature%2520noise%2520or%2520structure%2520perturbation%2520may%2520cause%2520severe%2520error%250Apropagation%252C%2520resulting%2520in%2520extremely%2520poor%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520rethink%250Athe%2520roles%2520played%2520by%2520graph%2520structural%2520information%2520in%2520graph%2520data%2520training%2520and%250Aidentify%2520that%2520message%2520passing%2520is%2520not%2520the%2520only%2520path%2520to%2520modeling%2520structural%250Ainformation.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520simple%2520but%2520effective%2520Graph%250AStructure%2520Self-Contrasting%2520%2528GSSC%2529%2520framework%2520that%2520learns%2520graph%2520structural%250Ainformation%2520without%2520message%2520passing.%2520The%2520proposed%2520framework%2520is%2520based%2520purely%2520on%250AMulti-Layer%2520Perceptrons%2520%2528MLPs%2529%252C%2520where%2520the%2520structural%2520information%2520is%2520only%250Aimplicitly%2520incorporated%2520as%2520prior%2520knowledge%2520to%2520guide%2520the%2520computation%2520of%250Asupervision%2520signals%252C%2520substituting%2520the%2520explicit%2520message%2520propagation%2520as%2520in%2520GNNs.%250ASpecifically%252C%2520it%2520first%2520applies%2520structural%2520sparsification%2520to%2520remove%2520potentially%250Auninformative%2520or%2520noisy%2520edges%2520in%2520the%2520neighborhood%252C%2520and%2520then%2520performs%2520structural%250Aself-contrasting%2520in%2520the%2520sparsified%2520neighborhood%2520to%2520learn%2520robust%2520node%250Arepresentations.%2520Finally%252C%2520structural%2520sparsification%2520and%2520self-contrasting%2520are%250Aformulated%2520as%2520a%2520bi-level%2520optimization%2520problem%2520and%2520solved%2520in%2520a%2520unified%250Aframework.%2520Extensive%2520experiments%2520have%2520qualitatively%2520and%2520quantitatively%250Ademonstrated%2520that%2520the%2520GSSC%2520framework%2520can%2520produce%2520truly%2520encouraging%2520performance%250Awith%2520better%2520generalization%2520and%2520robustness%2520than%2520other%2520leading%2520competitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Model%20Graph%20Structural%20Information%20on%20MLPs%20via%20Graph%0A%20%20Structure%20Self-Contrasting&entry.906535625=Lirong%20Wu%20and%20Haitao%20Lin%20and%20Guojiang%20Zhao%20and%20Cheng%20Tan%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20great%20success%20in%20handling%20graph-related%20tasks%0Awith%20Graph%20Neural%20Networks%20%28GNNs%29.%20However%2C%20most%20existing%20GNNs%20are%20based%20on%0Amessage%20passing%20to%20perform%20feature%20aggregation%20and%20transformation%2C%20where%20the%0Astructural%20information%20is%20explicitly%20involved%20in%20the%20forward%20propagation%20by%0Acoupling%20with%20node%20features%20through%20graph%20convolution%20at%20each%20layer.%20As%20a%0Aresult%2C%20subtle%20feature%20noise%20or%20structure%20perturbation%20may%20cause%20severe%20error%0Apropagation%2C%20resulting%20in%20extremely%20poor%20robustness.%20In%20this%20paper%2C%20we%20rethink%0Athe%20roles%20played%20by%20graph%20structural%20information%20in%20graph%20data%20training%20and%0Aidentify%20that%20message%20passing%20is%20not%20the%20only%20path%20to%20modeling%20structural%0Ainformation.%20Inspired%20by%20this%2C%20we%20propose%20a%20simple%20but%20effective%20Graph%0AStructure%20Self-Contrasting%20%28GSSC%29%20framework%20that%20learns%20graph%20structural%0Ainformation%20without%20message%20passing.%20The%20proposed%20framework%20is%20based%20purely%20on%0AMulti-Layer%20Perceptrons%20%28MLPs%29%2C%20where%20the%20structural%20information%20is%20only%0Aimplicitly%20incorporated%20as%20prior%20knowledge%20to%20guide%20the%20computation%20of%0Asupervision%20signals%2C%20substituting%20the%20explicit%20message%20propagation%20as%20in%20GNNs.%0ASpecifically%2C%20it%20first%20applies%20structural%20sparsification%20to%20remove%20potentially%0Auninformative%20or%20noisy%20edges%20in%20the%20neighborhood%2C%20and%20then%20performs%20structural%0Aself-contrasting%20in%20the%20sparsified%20neighborhood%20to%20learn%20robust%20node%0Arepresentations.%20Finally%2C%20structural%20sparsification%20and%20self-contrasting%20are%0Aformulated%20as%20a%20bi-level%20optimization%20problem%20and%20solved%20in%20a%20unified%0Aframework.%20Extensive%20experiments%20have%20qualitatively%20and%20quantitatively%0Ademonstrated%20that%20the%20GSSC%20framework%20can%20produce%20truly%20encouraging%20performance%0Awith%20better%20generalization%20and%20robustness%20than%20other%20leading%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05573v1&entry.124074799=Read"},
{"title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering", "author": "Benjamin Attal and Dor Verbin and Ben Mildenhall and Peter Hedman and Jonathan T. Barron and Matthew O'Toole and Pratul P. Srinivasan", "abstract": "  State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.\n", "link": "http://arxiv.org/abs/2409.05867v1", "date": "2024-09-09", "relevancy": 2.6439, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5219}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flash%20Cache%3A%20Reducing%20Bias%20in%20Radiance%20Cache%20Based%20Inverse%20Rendering&body=Title%3A%20Flash%20Cache%3A%20Reducing%20Bias%20in%20Radiance%20Cache%20Based%20Inverse%20Rendering%0AAuthor%3A%20Benjamin%20Attal%20and%20Dor%20Verbin%20and%20Ben%20Mildenhall%20and%20Peter%20Hedman%20and%20Jonathan%20T.%20Barron%20and%20Matthew%20O%27Toole%20and%20Pratul%20P.%20Srinivasan%0AAbstract%3A%20%20%20State-of-the-art%20techniques%20for%203D%20reconstruction%20are%20largely%20based%20on%0Avolumetric%20scene%20representations%2C%20which%20require%20sampling%20multiple%20points%20to%0Acompute%20the%20color%20arriving%20along%20a%20ray.%20Using%20these%20representations%20for%20more%0Ageneral%20inverse%20rendering%20--%20reconstructing%20geometry%2C%20materials%2C%20and%20lighting%0Afrom%20observed%20images%20--%20is%20challenging%20because%20recursively%20path-tracing%20such%0Avolumetric%20representations%20is%20expensive.%20Recent%20works%20alleviate%20this%20issue%0Athrough%20the%20use%20of%20radiance%20caches%3A%20data%20structures%20that%20store%20the%0Asteady-state%2C%20infinite-bounce%20radiance%20arriving%20at%20any%20point%20from%20any%0Adirection.%20However%2C%20these%20solutions%20rely%20on%20approximations%20that%20introduce%20bias%0Ainto%20the%20renderings%20and%2C%20more%20importantly%2C%20into%20the%20gradients%20used%20for%0Aoptimization.%20We%20present%20a%20method%20that%20avoids%20these%20approximations%20while%0Aremaining%20computationally%20efficient.%20In%20particular%2C%20we%20leverage%20two%20techniques%0Ato%20reduce%20variance%20for%20unbiased%20estimators%20of%20the%20rendering%20equation%3A%20%281%29%20an%0Aocclusion-aware%20importance%20sampler%20for%20incoming%20illumination%20and%20%282%29%20a%20fast%0Acache%20architecture%20that%20can%20be%20used%20as%20a%20control%20variate%20for%20the%20radiance%20from%0Aa%20high-quality%2C%20but%20more%20expensive%2C%20volumetric%20cache.%20We%20show%20that%20by%20removing%0Athese%20biases%20our%20approach%20improves%20the%20generality%20of%20radiance%20cache%20based%0Ainverse%20rendering%2C%20as%20well%20as%20increasing%20quality%20in%20the%20presence%20of%20challenging%0Alight%20transport%20effects%20such%20as%20specular%20reflections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlash%2520Cache%253A%2520Reducing%2520Bias%2520in%2520Radiance%2520Cache%2520Based%2520Inverse%2520Rendering%26entry.906535625%3DBenjamin%2520Attal%2520and%2520Dor%2520Verbin%2520and%2520Ben%2520Mildenhall%2520and%2520Peter%2520Hedman%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Matthew%2520O%2527Toole%2520and%2520Pratul%2520P.%2520Srinivasan%26entry.1292438233%3D%2520%2520State-of-the-art%2520techniques%2520for%25203D%2520reconstruction%2520are%2520largely%2520based%2520on%250Avolumetric%2520scene%2520representations%252C%2520which%2520require%2520sampling%2520multiple%2520points%2520to%250Acompute%2520the%2520color%2520arriving%2520along%2520a%2520ray.%2520Using%2520these%2520representations%2520for%2520more%250Ageneral%2520inverse%2520rendering%2520--%2520reconstructing%2520geometry%252C%2520materials%252C%2520and%2520lighting%250Afrom%2520observed%2520images%2520--%2520is%2520challenging%2520because%2520recursively%2520path-tracing%2520such%250Avolumetric%2520representations%2520is%2520expensive.%2520Recent%2520works%2520alleviate%2520this%2520issue%250Athrough%2520the%2520use%2520of%2520radiance%2520caches%253A%2520data%2520structures%2520that%2520store%2520the%250Asteady-state%252C%2520infinite-bounce%2520radiance%2520arriving%2520at%2520any%2520point%2520from%2520any%250Adirection.%2520However%252C%2520these%2520solutions%2520rely%2520on%2520approximations%2520that%2520introduce%2520bias%250Ainto%2520the%2520renderings%2520and%252C%2520more%2520importantly%252C%2520into%2520the%2520gradients%2520used%2520for%250Aoptimization.%2520We%2520present%2520a%2520method%2520that%2520avoids%2520these%2520approximations%2520while%250Aremaining%2520computationally%2520efficient.%2520In%2520particular%252C%2520we%2520leverage%2520two%2520techniques%250Ato%2520reduce%2520variance%2520for%2520unbiased%2520estimators%2520of%2520the%2520rendering%2520equation%253A%2520%25281%2529%2520an%250Aocclusion-aware%2520importance%2520sampler%2520for%2520incoming%2520illumination%2520and%2520%25282%2529%2520a%2520fast%250Acache%2520architecture%2520that%2520can%2520be%2520used%2520as%2520a%2520control%2520variate%2520for%2520the%2520radiance%2520from%250Aa%2520high-quality%252C%2520but%2520more%2520expensive%252C%2520volumetric%2520cache.%2520We%2520show%2520that%2520by%2520removing%250Athese%2520biases%2520our%2520approach%2520improves%2520the%2520generality%2520of%2520radiance%2520cache%2520based%250Ainverse%2520rendering%252C%2520as%2520well%2520as%2520increasing%2520quality%2520in%2520the%2520presence%2520of%2520challenging%250Alight%2520transport%2520effects%2520such%2520as%2520specular%2520reflections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flash%20Cache%3A%20Reducing%20Bias%20in%20Radiance%20Cache%20Based%20Inverse%20Rendering&entry.906535625=Benjamin%20Attal%20and%20Dor%20Verbin%20and%20Ben%20Mildenhall%20and%20Peter%20Hedman%20and%20Jonathan%20T.%20Barron%20and%20Matthew%20O%27Toole%20and%20Pratul%20P.%20Srinivasan&entry.1292438233=%20%20State-of-the-art%20techniques%20for%203D%20reconstruction%20are%20largely%20based%20on%0Avolumetric%20scene%20representations%2C%20which%20require%20sampling%20multiple%20points%20to%0Acompute%20the%20color%20arriving%20along%20a%20ray.%20Using%20these%20representations%20for%20more%0Ageneral%20inverse%20rendering%20--%20reconstructing%20geometry%2C%20materials%2C%20and%20lighting%0Afrom%20observed%20images%20--%20is%20challenging%20because%20recursively%20path-tracing%20such%0Avolumetric%20representations%20is%20expensive.%20Recent%20works%20alleviate%20this%20issue%0Athrough%20the%20use%20of%20radiance%20caches%3A%20data%20structures%20that%20store%20the%0Asteady-state%2C%20infinite-bounce%20radiance%20arriving%20at%20any%20point%20from%20any%0Adirection.%20However%2C%20these%20solutions%20rely%20on%20approximations%20that%20introduce%20bias%0Ainto%20the%20renderings%20and%2C%20more%20importantly%2C%20into%20the%20gradients%20used%20for%0Aoptimization.%20We%20present%20a%20method%20that%20avoids%20these%20approximations%20while%0Aremaining%20computationally%20efficient.%20In%20particular%2C%20we%20leverage%20two%20techniques%0Ato%20reduce%20variance%20for%20unbiased%20estimators%20of%20the%20rendering%20equation%3A%20%281%29%20an%0Aocclusion-aware%20importance%20sampler%20for%20incoming%20illumination%20and%20%282%29%20a%20fast%0Acache%20architecture%20that%20can%20be%20used%20as%20a%20control%20variate%20for%20the%20radiance%20from%0Aa%20high-quality%2C%20but%20more%20expensive%2C%20volumetric%20cache.%20We%20show%20that%20by%20removing%0Athese%20biases%20our%20approach%20improves%20the%20generality%20of%20radiance%20cache%20based%0Ainverse%20rendering%2C%20as%20well%20as%20increasing%20quality%20in%20the%20presence%20of%20challenging%0Alight%20transport%20effects%20such%20as%20specular%20reflections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05867v1&entry.124074799=Read"},
{"title": "SOVC: Subject-Oriented Video Captioning", "author": "Chang Teng and Yunchuan Ma and Guorong Li and Yuankai Qi and Laiyu Qing and Qingming Huang", "abstract": "  Describing video content according to users' needs is a long-held goal.\nAlthough existing video captioning methods have made significant progress, the\ngenerated captions may not focus on the entity that users are particularly\ninterested in. To address this problem, we propose a new video captioning task,\nSubject-Oriented Video Captioning (SOVC), which aims to allow users to specify\nthe describing target via a bounding box. To support this task, we construct\ntwo subject-oriented video captioning datasets based on two widely used video\ncaptioning datasets: MSVD and MSRVTT, by annotating subjects in each video for\neach caption. These datasets pave the way for describing users' interested\ntargets. To tackle this task, we introduce a method tailored to this task,\nnamed SOVCNet. It consists of two key components: a subject-oriented sampling\nmodule that samples frames related to the subject to minimize irrelevant\ninformation; and a subject-oriented encoding module that utilizes the subject\nareas as hard prompts and integrates learnable soft prompts, enhancing the\nmodel's focus on the subject's activities and facilitating adaptation to the\ndownstream generation task. Extensive experimental results demonstrate the\neffectiveness of our method on this new task.\n", "link": "http://arxiv.org/abs/2312.13330v2", "date": "2024-09-09", "relevancy": 2.6209, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOVC%3A%20Subject-Oriented%20Video%20Captioning&body=Title%3A%20SOVC%3A%20Subject-Oriented%20Video%20Captioning%0AAuthor%3A%20Chang%20Teng%20and%20Yunchuan%20Ma%20and%20Guorong%20Li%20and%20Yuankai%20Qi%20and%20Laiyu%20Qing%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Describing%20video%20content%20according%20to%20users%27%20needs%20is%20a%20long-held%20goal.%0AAlthough%20existing%20video%20captioning%20methods%20have%20made%20significant%20progress%2C%20the%0Agenerated%20captions%20may%20not%20focus%20on%20the%20entity%20that%20users%20are%20particularly%0Ainterested%20in.%20To%20address%20this%20problem%2C%20we%20propose%20a%20new%20video%20captioning%20task%2C%0ASubject-Oriented%20Video%20Captioning%20%28SOVC%29%2C%20which%20aims%20to%20allow%20users%20to%20specify%0Athe%20describing%20target%20via%20a%20bounding%20box.%20To%20support%20this%20task%2C%20we%20construct%0Atwo%20subject-oriented%20video%20captioning%20datasets%20based%20on%20two%20widely%20used%20video%0Acaptioning%20datasets%3A%20MSVD%20and%20MSRVTT%2C%20by%20annotating%20subjects%20in%20each%20video%20for%0Aeach%20caption.%20These%20datasets%20pave%20the%20way%20for%20describing%20users%27%20interested%0Atargets.%20To%20tackle%20this%20task%2C%20we%20introduce%20a%20method%20tailored%20to%20this%20task%2C%0Anamed%20SOVCNet.%20It%20consists%20of%20two%20key%20components%3A%20a%20subject-oriented%20sampling%0Amodule%20that%20samples%20frames%20related%20to%20the%20subject%20to%20minimize%20irrelevant%0Ainformation%3B%20and%20a%20subject-oriented%20encoding%20module%20that%20utilizes%20the%20subject%0Aareas%20as%20hard%20prompts%20and%20integrates%20learnable%20soft%20prompts%2C%20enhancing%20the%0Amodel%27s%20focus%20on%20the%20subject%27s%20activities%20and%20facilitating%20adaptation%20to%20the%0Adownstream%20generation%20task.%20Extensive%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20on%20this%20new%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOVC%253A%2520Subject-Oriented%2520Video%2520Captioning%26entry.906535625%3DChang%2520Teng%2520and%2520Yunchuan%2520Ma%2520and%2520Guorong%2520Li%2520and%2520Yuankai%2520Qi%2520and%2520Laiyu%2520Qing%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Describing%2520video%2520content%2520according%2520to%2520users%2527%2520needs%2520is%2520a%2520long-held%2520goal.%250AAlthough%2520existing%2520video%2520captioning%2520methods%2520have%2520made%2520significant%2520progress%252C%2520the%250Agenerated%2520captions%2520may%2520not%2520focus%2520on%2520the%2520entity%2520that%2520users%2520are%2520particularly%250Ainterested%2520in.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%2520video%2520captioning%2520task%252C%250ASubject-Oriented%2520Video%2520Captioning%2520%2528SOVC%2529%252C%2520which%2520aims%2520to%2520allow%2520users%2520to%2520specify%250Athe%2520describing%2520target%2520via%2520a%2520bounding%2520box.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%250Atwo%2520subject-oriented%2520video%2520captioning%2520datasets%2520based%2520on%2520two%2520widely%2520used%2520video%250Acaptioning%2520datasets%253A%2520MSVD%2520and%2520MSRVTT%252C%2520by%2520annotating%2520subjects%2520in%2520each%2520video%2520for%250Aeach%2520caption.%2520These%2520datasets%2520pave%2520the%2520way%2520for%2520describing%2520users%2527%2520interested%250Atargets.%2520To%2520tackle%2520this%2520task%252C%2520we%2520introduce%2520a%2520method%2520tailored%2520to%2520this%2520task%252C%250Anamed%2520SOVCNet.%2520It%2520consists%2520of%2520two%2520key%2520components%253A%2520a%2520subject-oriented%2520sampling%250Amodule%2520that%2520samples%2520frames%2520related%2520to%2520the%2520subject%2520to%2520minimize%2520irrelevant%250Ainformation%253B%2520and%2520a%2520subject-oriented%2520encoding%2520module%2520that%2520utilizes%2520the%2520subject%250Aareas%2520as%2520hard%2520prompts%2520and%2520integrates%2520learnable%2520soft%2520prompts%252C%2520enhancing%2520the%250Amodel%2527s%2520focus%2520on%2520the%2520subject%2527s%2520activities%2520and%2520facilitating%2520adaptation%2520to%2520the%250Adownstream%2520generation%2520task.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520on%2520this%2520new%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOVC%3A%20Subject-Oriented%20Video%20Captioning&entry.906535625=Chang%20Teng%20and%20Yunchuan%20Ma%20and%20Guorong%20Li%20and%20Yuankai%20Qi%20and%20Laiyu%20Qing%20and%20Qingming%20Huang&entry.1292438233=%20%20Describing%20video%20content%20according%20to%20users%27%20needs%20is%20a%20long-held%20goal.%0AAlthough%20existing%20video%20captioning%20methods%20have%20made%20significant%20progress%2C%20the%0Agenerated%20captions%20may%20not%20focus%20on%20the%20entity%20that%20users%20are%20particularly%0Ainterested%20in.%20To%20address%20this%20problem%2C%20we%20propose%20a%20new%20video%20captioning%20task%2C%0ASubject-Oriented%20Video%20Captioning%20%28SOVC%29%2C%20which%20aims%20to%20allow%20users%20to%20specify%0Athe%20describing%20target%20via%20a%20bounding%20box.%20To%20support%20this%20task%2C%20we%20construct%0Atwo%20subject-oriented%20video%20captioning%20datasets%20based%20on%20two%20widely%20used%20video%0Acaptioning%20datasets%3A%20MSVD%20and%20MSRVTT%2C%20by%20annotating%20subjects%20in%20each%20video%20for%0Aeach%20caption.%20These%20datasets%20pave%20the%20way%20for%20describing%20users%27%20interested%0Atargets.%20To%20tackle%20this%20task%2C%20we%20introduce%20a%20method%20tailored%20to%20this%20task%2C%0Anamed%20SOVCNet.%20It%20consists%20of%20two%20key%20components%3A%20a%20subject-oriented%20sampling%0Amodule%20that%20samples%20frames%20related%20to%20the%20subject%20to%20minimize%20irrelevant%0Ainformation%3B%20and%20a%20subject-oriented%20encoding%20module%20that%20utilizes%20the%20subject%0Aareas%20as%20hard%20prompts%20and%20integrates%20learnable%20soft%20prompts%2C%20enhancing%20the%0Amodel%27s%20focus%20on%20the%20subject%27s%20activities%20and%20facilitating%20adaptation%20to%20the%0Adownstream%20generation%20task.%20Extensive%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20on%20this%20new%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13330v2&entry.124074799=Read"},
{"title": "A Flexible Framework for Universal Computational Aberration Correction\n  via Automatic Lens Library Generation and Domain Adaptation", "author": "Qi Jiang and Yao Gao and Shaohua Gao and Zhonghua Yi and Lei Sun and Hao Shi and Kailun Yang and Kaiwei Wang and Jian Bai", "abstract": "  Emerging universal Computational Aberration Correction (CAC) paradigms\nprovide an inspiring solution to light-weight and high-quality imaging without\nrepeated data preparation and model training to accommodate new lens designs.\nHowever, the training databases in these approaches, i.e., the lens libraries\n(LensLibs), suffer from their limited coverage of real-world aberration\nbehaviors. In this work, we set up an OmniLens framework for universal CAC,\nconsidering both the generalization ability and flexibility. OmniLens extends\nthe idea of universal CAC to a broader concept, where a base model is trained\nfor three cases, including zero-shot CAC with the pre-trained model, few-shot\nCAC with a little lens-specific data for fine-tuning, and domain adaptive CAC\nusing domain adaptation for lens-descriptions-unknown lens. In terms of\nOmniLens's data foundation, we first propose an Evolution-based Automatic\nOptical Design (EAOD) pipeline to construct LensLib automatically, coined\nAODLib, whose diversity is enriched by an evolution framework, with\ncomprehensive constraints and a hybrid optimization strategy for achieving\nrealistic aberration behaviors. For network design, we introduce the guidance\nof high-quality codebook priors to facilitate zero-shot CAC and few-shot CAC,\nwhich enhances the model's generalization ability, while also boosting its\nconvergence in a few-shot case. Furthermore, based on the statistical\nobservation of dark channel priors in optical degradation, we design an\nunsupervised regularization term to adapt the base model to the target\ndescriptions-unknown lens using its aberration images without ground truth. We\nvalidate OmniLens on 4 manually designed low-end lenses with various structures\nand aberration behaviors. Remarkably, the base model trained on AODLib exhibits\nstrong generalization capabilities, achieving 97% of the lens-specific\nperformance in a zero-shot setting.\n", "link": "http://arxiv.org/abs/2409.05809v1", "date": "2024-09-09", "relevancy": 2.5881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Flexible%20Framework%20for%20Universal%20Computational%20Aberration%20Correction%0A%20%20via%20Automatic%20Lens%20Library%20Generation%20and%20Domain%20Adaptation&body=Title%3A%20A%20Flexible%20Framework%20for%20Universal%20Computational%20Aberration%20Correction%0A%20%20via%20Automatic%20Lens%20Library%20Generation%20and%20Domain%20Adaptation%0AAuthor%3A%20Qi%20Jiang%20and%20Yao%20Gao%20and%20Shaohua%20Gao%20and%20Zhonghua%20Yi%20and%20Lei%20Sun%20and%20Hao%20Shi%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang%20and%20Jian%20Bai%0AAbstract%3A%20%20%20Emerging%20universal%20Computational%20Aberration%20Correction%20%28CAC%29%20paradigms%0Aprovide%20an%20inspiring%20solution%20to%20light-weight%20and%20high-quality%20imaging%20without%0Arepeated%20data%20preparation%20and%20model%20training%20to%20accommodate%20new%20lens%20designs.%0AHowever%2C%20the%20training%20databases%20in%20these%20approaches%2C%20i.e.%2C%20the%20lens%20libraries%0A%28LensLibs%29%2C%20suffer%20from%20their%20limited%20coverage%20of%20real-world%20aberration%0Abehaviors.%20In%20this%20work%2C%20we%20set%20up%20an%20OmniLens%20framework%20for%20universal%20CAC%2C%0Aconsidering%20both%20the%20generalization%20ability%20and%20flexibility.%20OmniLens%20extends%0Athe%20idea%20of%20universal%20CAC%20to%20a%20broader%20concept%2C%20where%20a%20base%20model%20is%20trained%0Afor%20three%20cases%2C%20including%20zero-shot%20CAC%20with%20the%20pre-trained%20model%2C%20few-shot%0ACAC%20with%20a%20little%20lens-specific%20data%20for%20fine-tuning%2C%20and%20domain%20adaptive%20CAC%0Ausing%20domain%20adaptation%20for%20lens-descriptions-unknown%20lens.%20In%20terms%20of%0AOmniLens%27s%20data%20foundation%2C%20we%20first%20propose%20an%20Evolution-based%20Automatic%0AOptical%20Design%20%28EAOD%29%20pipeline%20to%20construct%20LensLib%20automatically%2C%20coined%0AAODLib%2C%20whose%20diversity%20is%20enriched%20by%20an%20evolution%20framework%2C%20with%0Acomprehensive%20constraints%20and%20a%20hybrid%20optimization%20strategy%20for%20achieving%0Arealistic%20aberration%20behaviors.%20For%20network%20design%2C%20we%20introduce%20the%20guidance%0Aof%20high-quality%20codebook%20priors%20to%20facilitate%20zero-shot%20CAC%20and%20few-shot%20CAC%2C%0Awhich%20enhances%20the%20model%27s%20generalization%20ability%2C%20while%20also%20boosting%20its%0Aconvergence%20in%20a%20few-shot%20case.%20Furthermore%2C%20based%20on%20the%20statistical%0Aobservation%20of%20dark%20channel%20priors%20in%20optical%20degradation%2C%20we%20design%20an%0Aunsupervised%20regularization%20term%20to%20adapt%20the%20base%20model%20to%20the%20target%0Adescriptions-unknown%20lens%20using%20its%20aberration%20images%20without%20ground%20truth.%20We%0Avalidate%20OmniLens%20on%204%20manually%20designed%20low-end%20lenses%20with%20various%20structures%0Aand%20aberration%20behaviors.%20Remarkably%2C%20the%20base%20model%20trained%20on%20AODLib%20exhibits%0Astrong%20generalization%20capabilities%2C%20achieving%2097%25%20of%20the%20lens-specific%0Aperformance%20in%20a%20zero-shot%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Flexible%2520Framework%2520for%2520Universal%2520Computational%2520Aberration%2520Correction%250A%2520%2520via%2520Automatic%2520Lens%2520Library%2520Generation%2520and%2520Domain%2520Adaptation%26entry.906535625%3DQi%2520Jiang%2520and%2520Yao%2520Gao%2520and%2520Shaohua%2520Gao%2520and%2520Zhonghua%2520Yi%2520and%2520Lei%2520Sun%2520and%2520Hao%2520Shi%2520and%2520Kailun%2520Yang%2520and%2520Kaiwei%2520Wang%2520and%2520Jian%2520Bai%26entry.1292438233%3D%2520%2520Emerging%2520universal%2520Computational%2520Aberration%2520Correction%2520%2528CAC%2529%2520paradigms%250Aprovide%2520an%2520inspiring%2520solution%2520to%2520light-weight%2520and%2520high-quality%2520imaging%2520without%250Arepeated%2520data%2520preparation%2520and%2520model%2520training%2520to%2520accommodate%2520new%2520lens%2520designs.%250AHowever%252C%2520the%2520training%2520databases%2520in%2520these%2520approaches%252C%2520i.e.%252C%2520the%2520lens%2520libraries%250A%2528LensLibs%2529%252C%2520suffer%2520from%2520their%2520limited%2520coverage%2520of%2520real-world%2520aberration%250Abehaviors.%2520In%2520this%2520work%252C%2520we%2520set%2520up%2520an%2520OmniLens%2520framework%2520for%2520universal%2520CAC%252C%250Aconsidering%2520both%2520the%2520generalization%2520ability%2520and%2520flexibility.%2520OmniLens%2520extends%250Athe%2520idea%2520of%2520universal%2520CAC%2520to%2520a%2520broader%2520concept%252C%2520where%2520a%2520base%2520model%2520is%2520trained%250Afor%2520three%2520cases%252C%2520including%2520zero-shot%2520CAC%2520with%2520the%2520pre-trained%2520model%252C%2520few-shot%250ACAC%2520with%2520a%2520little%2520lens-specific%2520data%2520for%2520fine-tuning%252C%2520and%2520domain%2520adaptive%2520CAC%250Ausing%2520domain%2520adaptation%2520for%2520lens-descriptions-unknown%2520lens.%2520In%2520terms%2520of%250AOmniLens%2527s%2520data%2520foundation%252C%2520we%2520first%2520propose%2520an%2520Evolution-based%2520Automatic%250AOptical%2520Design%2520%2528EAOD%2529%2520pipeline%2520to%2520construct%2520LensLib%2520automatically%252C%2520coined%250AAODLib%252C%2520whose%2520diversity%2520is%2520enriched%2520by%2520an%2520evolution%2520framework%252C%2520with%250Acomprehensive%2520constraints%2520and%2520a%2520hybrid%2520optimization%2520strategy%2520for%2520achieving%250Arealistic%2520aberration%2520behaviors.%2520For%2520network%2520design%252C%2520we%2520introduce%2520the%2520guidance%250Aof%2520high-quality%2520codebook%2520priors%2520to%2520facilitate%2520zero-shot%2520CAC%2520and%2520few-shot%2520CAC%252C%250Awhich%2520enhances%2520the%2520model%2527s%2520generalization%2520ability%252C%2520while%2520also%2520boosting%2520its%250Aconvergence%2520in%2520a%2520few-shot%2520case.%2520Furthermore%252C%2520based%2520on%2520the%2520statistical%250Aobservation%2520of%2520dark%2520channel%2520priors%2520in%2520optical%2520degradation%252C%2520we%2520design%2520an%250Aunsupervised%2520regularization%2520term%2520to%2520adapt%2520the%2520base%2520model%2520to%2520the%2520target%250Adescriptions-unknown%2520lens%2520using%2520its%2520aberration%2520images%2520without%2520ground%2520truth.%2520We%250Avalidate%2520OmniLens%2520on%25204%2520manually%2520designed%2520low-end%2520lenses%2520with%2520various%2520structures%250Aand%2520aberration%2520behaviors.%2520Remarkably%252C%2520the%2520base%2520model%2520trained%2520on%2520AODLib%2520exhibits%250Astrong%2520generalization%2520capabilities%252C%2520achieving%252097%2525%2520of%2520the%2520lens-specific%250Aperformance%2520in%2520a%2520zero-shot%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Flexible%20Framework%20for%20Universal%20Computational%20Aberration%20Correction%0A%20%20via%20Automatic%20Lens%20Library%20Generation%20and%20Domain%20Adaptation&entry.906535625=Qi%20Jiang%20and%20Yao%20Gao%20and%20Shaohua%20Gao%20and%20Zhonghua%20Yi%20and%20Lei%20Sun%20and%20Hao%20Shi%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang%20and%20Jian%20Bai&entry.1292438233=%20%20Emerging%20universal%20Computational%20Aberration%20Correction%20%28CAC%29%20paradigms%0Aprovide%20an%20inspiring%20solution%20to%20light-weight%20and%20high-quality%20imaging%20without%0Arepeated%20data%20preparation%20and%20model%20training%20to%20accommodate%20new%20lens%20designs.%0AHowever%2C%20the%20training%20databases%20in%20these%20approaches%2C%20i.e.%2C%20the%20lens%20libraries%0A%28LensLibs%29%2C%20suffer%20from%20their%20limited%20coverage%20of%20real-world%20aberration%0Abehaviors.%20In%20this%20work%2C%20we%20set%20up%20an%20OmniLens%20framework%20for%20universal%20CAC%2C%0Aconsidering%20both%20the%20generalization%20ability%20and%20flexibility.%20OmniLens%20extends%0Athe%20idea%20of%20universal%20CAC%20to%20a%20broader%20concept%2C%20where%20a%20base%20model%20is%20trained%0Afor%20three%20cases%2C%20including%20zero-shot%20CAC%20with%20the%20pre-trained%20model%2C%20few-shot%0ACAC%20with%20a%20little%20lens-specific%20data%20for%20fine-tuning%2C%20and%20domain%20adaptive%20CAC%0Ausing%20domain%20adaptation%20for%20lens-descriptions-unknown%20lens.%20In%20terms%20of%0AOmniLens%27s%20data%20foundation%2C%20we%20first%20propose%20an%20Evolution-based%20Automatic%0AOptical%20Design%20%28EAOD%29%20pipeline%20to%20construct%20LensLib%20automatically%2C%20coined%0AAODLib%2C%20whose%20diversity%20is%20enriched%20by%20an%20evolution%20framework%2C%20with%0Acomprehensive%20constraints%20and%20a%20hybrid%20optimization%20strategy%20for%20achieving%0Arealistic%20aberration%20behaviors.%20For%20network%20design%2C%20we%20introduce%20the%20guidance%0Aof%20high-quality%20codebook%20priors%20to%20facilitate%20zero-shot%20CAC%20and%20few-shot%20CAC%2C%0Awhich%20enhances%20the%20model%27s%20generalization%20ability%2C%20while%20also%20boosting%20its%0Aconvergence%20in%20a%20few-shot%20case.%20Furthermore%2C%20based%20on%20the%20statistical%0Aobservation%20of%20dark%20channel%20priors%20in%20optical%20degradation%2C%20we%20design%20an%0Aunsupervised%20regularization%20term%20to%20adapt%20the%20base%20model%20to%20the%20target%0Adescriptions-unknown%20lens%20using%20its%20aberration%20images%20without%20ground%20truth.%20We%0Avalidate%20OmniLens%20on%204%20manually%20designed%20low-end%20lenses%20with%20various%20structures%0Aand%20aberration%20behaviors.%20Remarkably%2C%20the%20base%20model%20trained%20on%20AODLib%20exhibits%0Astrong%20generalization%20capabilities%2C%20achieving%2097%25%20of%20the%20lens-specific%0Aperformance%20in%20a%20zero-shot%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05809v1&entry.124074799=Read"},
{"title": "3D Lymphoma Segmentation on PET/CT Images via Multi-Scale Information\n  Fusion with Cross-Attention", "author": "Huan Huang and Liheng Qiu and Shenmiao Yang and Longxi Li and Jiaofen Nan and Yanting Li and Chuang Han and Fubao Zhu and Chen Zhao and Weihua Zhou", "abstract": "  Background: Accurate segmentation of diffuse large B-cell lymphoma (DLBCL)\nlesions is challenging due to their complex patterns in medical imaging.\n  Objective: This study aims to develop a precise segmentation method for DLBCL\nusing 18F-Fluorodeoxyglucose (FDG) positron emission tomography (PET) and\ncomputed tomography (CT) images.\n  Methods: We propose a 3D dual-branch encoder segmentation method using\nshifted window transformers and a Multi-Scale Information Fusion (MSIF) module.\nTo enhance feature integration, the MSIF module performs multi-scale feature\nfusion using cross-attention mechanisms with a shifted window framework. A\ngated neural network within the MSIF module dynamically balances the\ncontributions from each modality. The model was optimized using the Dice\nSimilarity Coefficient (DSC) loss function. Additionally, we computed the total\nmetabolic tumor volume (TMTV) and performed statistical analyses.\n  Results: The model was trained and validated on a dataset of 165 DLBCL\npatients using 5-fold cross-validation, achieving a DSC of 0.7512. Statistical\nanalysis showed a significant improvement over comparative methods (p < 0.05).\nAdditionally, a Pearson correlation coefficient of 0.91 and an R^2 of 0.89 were\nobserved when comparing manual annotations to segmentation results for TMTV\nmeasurement.\n  Conclusion: This study presents an effective automatic segmentation method\nfor DLBCL that leverages the complementary strengths of PET and CT imaging. Our\nmethod has the potential to improve diagnostic interpretations and assist in\ntreatment planning for DLBCL patients.\n", "link": "http://arxiv.org/abs/2402.02349v2", "date": "2024-09-09", "relevancy": 2.5835, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5067}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Lymphoma%20Segmentation%20on%20PET/CT%20Images%20via%20Multi-Scale%20Information%0A%20%20Fusion%20with%20Cross-Attention&body=Title%3A%203D%20Lymphoma%20Segmentation%20on%20PET/CT%20Images%20via%20Multi-Scale%20Information%0A%20%20Fusion%20with%20Cross-Attention%0AAuthor%3A%20Huan%20Huang%20and%20Liheng%20Qiu%20and%20Shenmiao%20Yang%20and%20Longxi%20Li%20and%20Jiaofen%20Nan%20and%20Yanting%20Li%20and%20Chuang%20Han%20and%20Fubao%20Zhu%20and%20Chen%20Zhao%20and%20Weihua%20Zhou%0AAbstract%3A%20%20%20Background%3A%20Accurate%20segmentation%20of%20diffuse%20large%20B-cell%20lymphoma%20%28DLBCL%29%0Alesions%20is%20challenging%20due%20to%20their%20complex%20patterns%20in%20medical%20imaging.%0A%20%20Objective%3A%20This%20study%20aims%20to%20develop%20a%20precise%20segmentation%20method%20for%20DLBCL%0Ausing%2018F-Fluorodeoxyglucose%20%28FDG%29%20positron%20emission%20tomography%20%28PET%29%20and%0Acomputed%20tomography%20%28CT%29%20images.%0A%20%20Methods%3A%20We%20propose%20a%203D%20dual-branch%20encoder%20segmentation%20method%20using%0Ashifted%20window%20transformers%20and%20a%20Multi-Scale%20Information%20Fusion%20%28MSIF%29%20module.%0ATo%20enhance%20feature%20integration%2C%20the%20MSIF%20module%20performs%20multi-scale%20feature%0Afusion%20using%20cross-attention%20mechanisms%20with%20a%20shifted%20window%20framework.%20A%0Agated%20neural%20network%20within%20the%20MSIF%20module%20dynamically%20balances%20the%0Acontributions%20from%20each%20modality.%20The%20model%20was%20optimized%20using%20the%20Dice%0ASimilarity%20Coefficient%20%28DSC%29%20loss%20function.%20Additionally%2C%20we%20computed%20the%20total%0Ametabolic%20tumor%20volume%20%28TMTV%29%20and%20performed%20statistical%20analyses.%0A%20%20Results%3A%20The%20model%20was%20trained%20and%20validated%20on%20a%20dataset%20of%20165%20DLBCL%0Apatients%20using%205-fold%20cross-validation%2C%20achieving%20a%20DSC%20of%200.7512.%20Statistical%0Aanalysis%20showed%20a%20significant%20improvement%20over%20comparative%20methods%20%28p%20%3C%200.05%29.%0AAdditionally%2C%20a%20Pearson%20correlation%20coefficient%20of%200.91%20and%20an%20R%5E2%20of%200.89%20were%0Aobserved%20when%20comparing%20manual%20annotations%20to%20segmentation%20results%20for%20TMTV%0Ameasurement.%0A%20%20Conclusion%3A%20This%20study%20presents%20an%20effective%20automatic%20segmentation%20method%0Afor%20DLBCL%20that%20leverages%20the%20complementary%20strengths%20of%20PET%20and%20CT%20imaging.%20Our%0Amethod%20has%20the%20potential%20to%20improve%20diagnostic%20interpretations%20and%20assist%20in%0Atreatment%20planning%20for%20DLBCL%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Lymphoma%2520Segmentation%2520on%2520PET/CT%2520Images%2520via%2520Multi-Scale%2520Information%250A%2520%2520Fusion%2520with%2520Cross-Attention%26entry.906535625%3DHuan%2520Huang%2520and%2520Liheng%2520Qiu%2520and%2520Shenmiao%2520Yang%2520and%2520Longxi%2520Li%2520and%2520Jiaofen%2520Nan%2520and%2520Yanting%2520Li%2520and%2520Chuang%2520Han%2520and%2520Fubao%2520Zhu%2520and%2520Chen%2520Zhao%2520and%2520Weihua%2520Zhou%26entry.1292438233%3D%2520%2520Background%253A%2520Accurate%2520segmentation%2520of%2520diffuse%2520large%2520B-cell%2520lymphoma%2520%2528DLBCL%2529%250Alesions%2520is%2520challenging%2520due%2520to%2520their%2520complex%2520patterns%2520in%2520medical%2520imaging.%250A%2520%2520Objective%253A%2520This%2520study%2520aims%2520to%2520develop%2520a%2520precise%2520segmentation%2520method%2520for%2520DLBCL%250Ausing%252018F-Fluorodeoxyglucose%2520%2528FDG%2529%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520and%250Acomputed%2520tomography%2520%2528CT%2529%2520images.%250A%2520%2520Methods%253A%2520We%2520propose%2520a%25203D%2520dual-branch%2520encoder%2520segmentation%2520method%2520using%250Ashifted%2520window%2520transformers%2520and%2520a%2520Multi-Scale%2520Information%2520Fusion%2520%2528MSIF%2529%2520module.%250ATo%2520enhance%2520feature%2520integration%252C%2520the%2520MSIF%2520module%2520performs%2520multi-scale%2520feature%250Afusion%2520using%2520cross-attention%2520mechanisms%2520with%2520a%2520shifted%2520window%2520framework.%2520A%250Agated%2520neural%2520network%2520within%2520the%2520MSIF%2520module%2520dynamically%2520balances%2520the%250Acontributions%2520from%2520each%2520modality.%2520The%2520model%2520was%2520optimized%2520using%2520the%2520Dice%250ASimilarity%2520Coefficient%2520%2528DSC%2529%2520loss%2520function.%2520Additionally%252C%2520we%2520computed%2520the%2520total%250Ametabolic%2520tumor%2520volume%2520%2528TMTV%2529%2520and%2520performed%2520statistical%2520analyses.%250A%2520%2520Results%253A%2520The%2520model%2520was%2520trained%2520and%2520validated%2520on%2520a%2520dataset%2520of%2520165%2520DLBCL%250Apatients%2520using%25205-fold%2520cross-validation%252C%2520achieving%2520a%2520DSC%2520of%25200.7512.%2520Statistical%250Aanalysis%2520showed%2520a%2520significant%2520improvement%2520over%2520comparative%2520methods%2520%2528p%2520%253C%25200.05%2529.%250AAdditionally%252C%2520a%2520Pearson%2520correlation%2520coefficient%2520of%25200.91%2520and%2520an%2520R%255E2%2520of%25200.89%2520were%250Aobserved%2520when%2520comparing%2520manual%2520annotations%2520to%2520segmentation%2520results%2520for%2520TMTV%250Ameasurement.%250A%2520%2520Conclusion%253A%2520This%2520study%2520presents%2520an%2520effective%2520automatic%2520segmentation%2520method%250Afor%2520DLBCL%2520that%2520leverages%2520the%2520complementary%2520strengths%2520of%2520PET%2520and%2520CT%2520imaging.%2520Our%250Amethod%2520has%2520the%2520potential%2520to%2520improve%2520diagnostic%2520interpretations%2520and%2520assist%2520in%250Atreatment%2520planning%2520for%2520DLBCL%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Lymphoma%20Segmentation%20on%20PET/CT%20Images%20via%20Multi-Scale%20Information%0A%20%20Fusion%20with%20Cross-Attention&entry.906535625=Huan%20Huang%20and%20Liheng%20Qiu%20and%20Shenmiao%20Yang%20and%20Longxi%20Li%20and%20Jiaofen%20Nan%20and%20Yanting%20Li%20and%20Chuang%20Han%20and%20Fubao%20Zhu%20and%20Chen%20Zhao%20and%20Weihua%20Zhou&entry.1292438233=%20%20Background%3A%20Accurate%20segmentation%20of%20diffuse%20large%20B-cell%20lymphoma%20%28DLBCL%29%0Alesions%20is%20challenging%20due%20to%20their%20complex%20patterns%20in%20medical%20imaging.%0A%20%20Objective%3A%20This%20study%20aims%20to%20develop%20a%20precise%20segmentation%20method%20for%20DLBCL%0Ausing%2018F-Fluorodeoxyglucose%20%28FDG%29%20positron%20emission%20tomography%20%28PET%29%20and%0Acomputed%20tomography%20%28CT%29%20images.%0A%20%20Methods%3A%20We%20propose%20a%203D%20dual-branch%20encoder%20segmentation%20method%20using%0Ashifted%20window%20transformers%20and%20a%20Multi-Scale%20Information%20Fusion%20%28MSIF%29%20module.%0ATo%20enhance%20feature%20integration%2C%20the%20MSIF%20module%20performs%20multi-scale%20feature%0Afusion%20using%20cross-attention%20mechanisms%20with%20a%20shifted%20window%20framework.%20A%0Agated%20neural%20network%20within%20the%20MSIF%20module%20dynamically%20balances%20the%0Acontributions%20from%20each%20modality.%20The%20model%20was%20optimized%20using%20the%20Dice%0ASimilarity%20Coefficient%20%28DSC%29%20loss%20function.%20Additionally%2C%20we%20computed%20the%20total%0Ametabolic%20tumor%20volume%20%28TMTV%29%20and%20performed%20statistical%20analyses.%0A%20%20Results%3A%20The%20model%20was%20trained%20and%20validated%20on%20a%20dataset%20of%20165%20DLBCL%0Apatients%20using%205-fold%20cross-validation%2C%20achieving%20a%20DSC%20of%200.7512.%20Statistical%0Aanalysis%20showed%20a%20significant%20improvement%20over%20comparative%20methods%20%28p%20%3C%200.05%29.%0AAdditionally%2C%20a%20Pearson%20correlation%20coefficient%20of%200.91%20and%20an%20R%5E2%20of%200.89%20were%0Aobserved%20when%20comparing%20manual%20annotations%20to%20segmentation%20results%20for%20TMTV%0Ameasurement.%0A%20%20Conclusion%3A%20This%20study%20presents%20an%20effective%20automatic%20segmentation%20method%0Afor%20DLBCL%20that%20leverages%20the%20complementary%20strengths%20of%20PET%20and%20CT%20imaging.%20Our%0Amethod%20has%20the%20potential%20to%20improve%20diagnostic%20interpretations%20and%20assist%20in%0Atreatment%20planning%20for%20DLBCL%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02349v2&entry.124074799=Read"},
{"title": "Referring Expression Generation in Visually Grounded Dialogue with\n  Discourse-aware Comprehension Guiding", "author": "Bram Willemsen and Gabriel Skantze", "abstract": "  We propose an approach to referring expression generation (REG) in visually\ngrounded dialogue that is meant to produce referring expressions (REs) that are\nboth discriminative and discourse-appropriate. Our method constitutes a\ntwo-stage process. First, we model REG as a text- and image-conditioned\nnext-token prediction task. REs are autoregressively generated based on their\npreceding linguistic context and a visual representation of the referent.\nSecond, we propose the use of discourse-aware comprehension guiding as part of\na generate-and-rerank strategy through which candidate REs generated with our\nREG model are reranked based on their discourse-dependent discriminatory power.\nResults from our human evaluation indicate that our proposed two-stage approach\nis effective in producing discriminative REs, with higher performance in terms\nof text-image retrieval accuracy for reranked REs compared to those generated\nusing greedy decoding.\n", "link": "http://arxiv.org/abs/2409.05721v1", "date": "2024-09-09", "relevancy": 2.5818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Referring%20Expression%20Generation%20in%20Visually%20Grounded%20Dialogue%20with%0A%20%20Discourse-aware%20Comprehension%20Guiding&body=Title%3A%20Referring%20Expression%20Generation%20in%20Visually%20Grounded%20Dialogue%20with%0A%20%20Discourse-aware%20Comprehension%20Guiding%0AAuthor%3A%20Bram%20Willemsen%20and%20Gabriel%20Skantze%0AAbstract%3A%20%20%20We%20propose%20an%20approach%20to%20referring%20expression%20generation%20%28REG%29%20in%20visually%0Agrounded%20dialogue%20that%20is%20meant%20to%20produce%20referring%20expressions%20%28REs%29%20that%20are%0Aboth%20discriminative%20and%20discourse-appropriate.%20Our%20method%20constitutes%20a%0Atwo-stage%20process.%20First%2C%20we%20model%20REG%20as%20a%20text-%20and%20image-conditioned%0Anext-token%20prediction%20task.%20REs%20are%20autoregressively%20generated%20based%20on%20their%0Apreceding%20linguistic%20context%20and%20a%20visual%20representation%20of%20the%20referent.%0ASecond%2C%20we%20propose%20the%20use%20of%20discourse-aware%20comprehension%20guiding%20as%20part%20of%0Aa%20generate-and-rerank%20strategy%20through%20which%20candidate%20REs%20generated%20with%20our%0AREG%20model%20are%20reranked%20based%20on%20their%20discourse-dependent%20discriminatory%20power.%0AResults%20from%20our%20human%20evaluation%20indicate%20that%20our%20proposed%20two-stage%20approach%0Ais%20effective%20in%20producing%20discriminative%20REs%2C%20with%20higher%20performance%20in%20terms%0Aof%20text-image%20retrieval%20accuracy%20for%20reranked%20REs%20compared%20to%20those%20generated%0Ausing%20greedy%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferring%2520Expression%2520Generation%2520in%2520Visually%2520Grounded%2520Dialogue%2520with%250A%2520%2520Discourse-aware%2520Comprehension%2520Guiding%26entry.906535625%3DBram%2520Willemsen%2520and%2520Gabriel%2520Skantze%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520approach%2520to%2520referring%2520expression%2520generation%2520%2528REG%2529%2520in%2520visually%250Agrounded%2520dialogue%2520that%2520is%2520meant%2520to%2520produce%2520referring%2520expressions%2520%2528REs%2529%2520that%2520are%250Aboth%2520discriminative%2520and%2520discourse-appropriate.%2520Our%2520method%2520constitutes%2520a%250Atwo-stage%2520process.%2520First%252C%2520we%2520model%2520REG%2520as%2520a%2520text-%2520and%2520image-conditioned%250Anext-token%2520prediction%2520task.%2520REs%2520are%2520autoregressively%2520generated%2520based%2520on%2520their%250Apreceding%2520linguistic%2520context%2520and%2520a%2520visual%2520representation%2520of%2520the%2520referent.%250ASecond%252C%2520we%2520propose%2520the%2520use%2520of%2520discourse-aware%2520comprehension%2520guiding%2520as%2520part%2520of%250Aa%2520generate-and-rerank%2520strategy%2520through%2520which%2520candidate%2520REs%2520generated%2520with%2520our%250AREG%2520model%2520are%2520reranked%2520based%2520on%2520their%2520discourse-dependent%2520discriminatory%2520power.%250AResults%2520from%2520our%2520human%2520evaluation%2520indicate%2520that%2520our%2520proposed%2520two-stage%2520approach%250Ais%2520effective%2520in%2520producing%2520discriminative%2520REs%252C%2520with%2520higher%2520performance%2520in%2520terms%250Aof%2520text-image%2520retrieval%2520accuracy%2520for%2520reranked%2520REs%2520compared%2520to%2520those%2520generated%250Ausing%2520greedy%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Referring%20Expression%20Generation%20in%20Visually%20Grounded%20Dialogue%20with%0A%20%20Discourse-aware%20Comprehension%20Guiding&entry.906535625=Bram%20Willemsen%20and%20Gabriel%20Skantze&entry.1292438233=%20%20We%20propose%20an%20approach%20to%20referring%20expression%20generation%20%28REG%29%20in%20visually%0Agrounded%20dialogue%20that%20is%20meant%20to%20produce%20referring%20expressions%20%28REs%29%20that%20are%0Aboth%20discriminative%20and%20discourse-appropriate.%20Our%20method%20constitutes%20a%0Atwo-stage%20process.%20First%2C%20we%20model%20REG%20as%20a%20text-%20and%20image-conditioned%0Anext-token%20prediction%20task.%20REs%20are%20autoregressively%20generated%20based%20on%20their%0Apreceding%20linguistic%20context%20and%20a%20visual%20representation%20of%20the%20referent.%0ASecond%2C%20we%20propose%20the%20use%20of%20discourse-aware%20comprehension%20guiding%20as%20part%20of%0Aa%20generate-and-rerank%20strategy%20through%20which%20candidate%20REs%20generated%20with%20our%0AREG%20model%20are%20reranked%20based%20on%20their%20discourse-dependent%20discriminatory%20power.%0AResults%20from%20our%20human%20evaluation%20indicate%20that%20our%20proposed%20two-stage%20approach%0Ais%20effective%20in%20producing%20discriminative%20REs%2C%20with%20higher%20performance%20in%20terms%0Aof%20text-image%20retrieval%20accuracy%20for%20reranked%20REs%20compared%20to%20those%20generated%0Ausing%20greedy%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05721v1&entry.124074799=Read"},
{"title": "Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs", "author": "Yahya Jabary and Andreas Plesner and Turlan Kuzhagaliyev and Roger Wattenhofer", "abstract": "  Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for\ncomputers but easy for humans. However, advances in image recognition models\npose a significant threat to such CAPTCHAs. These models can easily be fooled\nby generating some well-hidden \"random\" noise and adding it to the image, or\nhiding objects in the image. However, these methods are model-specific and thus\ncan not aid CAPTCHAs in fooling all models. We show in this work that by\nallowing for more significant changes to the images while preserving the\nsemantic information and keeping it solvable by humans, we can fool many\nstate-of-the-art models. Specifically, we demonstrate that by adding masks of\nvarious intensities the Accuracy @ 1 (Acc@1) drops by more than 50%-points for\nall models, and supposedly robust models such as vision transformers see an\nAcc@1 drop of 80%-points.\n  These masks can therefore effectively fool modern image classifiers, thus\nshowing that machines have not caught up with humans -- yet.\n", "link": "http://arxiv.org/abs/2409.05558v1", "date": "2024-09-09", "relevancy": 2.569, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5253}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5097}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Through%20the%20Mask%3A%20Rethinking%20Adversarial%20Examples%20for%20CAPTCHAs&body=Title%3A%20Seeing%20Through%20the%20Mask%3A%20Rethinking%20Adversarial%20Examples%20for%20CAPTCHAs%0AAuthor%3A%20Yahya%20Jabary%20and%20Andreas%20Plesner%20and%20Turlan%20Kuzhagaliyev%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Modern%20CAPTCHAs%20rely%20heavily%20on%20vision%20tasks%20that%20are%20supposedly%20hard%20for%0Acomputers%20but%20easy%20for%20humans.%20However%2C%20advances%20in%20image%20recognition%20models%0Apose%20a%20significant%20threat%20to%20such%20CAPTCHAs.%20These%20models%20can%20easily%20be%20fooled%0Aby%20generating%20some%20well-hidden%20%22random%22%20noise%20and%20adding%20it%20to%20the%20image%2C%20or%0Ahiding%20objects%20in%20the%20image.%20However%2C%20these%20methods%20are%20model-specific%20and%20thus%0Acan%20not%20aid%20CAPTCHAs%20in%20fooling%20all%20models.%20We%20show%20in%20this%20work%20that%20by%0Aallowing%20for%20more%20significant%20changes%20to%20the%20images%20while%20preserving%20the%0Asemantic%20information%20and%20keeping%20it%20solvable%20by%20humans%2C%20we%20can%20fool%20many%0Astate-of-the-art%20models.%20Specifically%2C%20we%20demonstrate%20that%20by%20adding%20masks%20of%0Avarious%20intensities%20the%20Accuracy%20%40%201%20%28Acc%401%29%20drops%20by%20more%20than%2050%25-points%20for%0Aall%20models%2C%20and%20supposedly%20robust%20models%20such%20as%20vision%20transformers%20see%20an%0AAcc%401%20drop%20of%2080%25-points.%0A%20%20These%20masks%20can%20therefore%20effectively%20fool%20modern%20image%20classifiers%2C%20thus%0Ashowing%20that%20machines%20have%20not%20caught%20up%20with%20humans%20--%20yet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Through%2520the%2520Mask%253A%2520Rethinking%2520Adversarial%2520Examples%2520for%2520CAPTCHAs%26entry.906535625%3DYahya%2520Jabary%2520and%2520Andreas%2520Plesner%2520and%2520Turlan%2520Kuzhagaliyev%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Modern%2520CAPTCHAs%2520rely%2520heavily%2520on%2520vision%2520tasks%2520that%2520are%2520supposedly%2520hard%2520for%250Acomputers%2520but%2520easy%2520for%2520humans.%2520However%252C%2520advances%2520in%2520image%2520recognition%2520models%250Apose%2520a%2520significant%2520threat%2520to%2520such%2520CAPTCHAs.%2520These%2520models%2520can%2520easily%2520be%2520fooled%250Aby%2520generating%2520some%2520well-hidden%2520%2522random%2522%2520noise%2520and%2520adding%2520it%2520to%2520the%2520image%252C%2520or%250Ahiding%2520objects%2520in%2520the%2520image.%2520However%252C%2520these%2520methods%2520are%2520model-specific%2520and%2520thus%250Acan%2520not%2520aid%2520CAPTCHAs%2520in%2520fooling%2520all%2520models.%2520We%2520show%2520in%2520this%2520work%2520that%2520by%250Aallowing%2520for%2520more%2520significant%2520changes%2520to%2520the%2520images%2520while%2520preserving%2520the%250Asemantic%2520information%2520and%2520keeping%2520it%2520solvable%2520by%2520humans%252C%2520we%2520can%2520fool%2520many%250Astate-of-the-art%2520models.%2520Specifically%252C%2520we%2520demonstrate%2520that%2520by%2520adding%2520masks%2520of%250Avarious%2520intensities%2520the%2520Accuracy%2520%2540%25201%2520%2528Acc%25401%2529%2520drops%2520by%2520more%2520than%252050%2525-points%2520for%250Aall%2520models%252C%2520and%2520supposedly%2520robust%2520models%2520such%2520as%2520vision%2520transformers%2520see%2520an%250AAcc%25401%2520drop%2520of%252080%2525-points.%250A%2520%2520These%2520masks%2520can%2520therefore%2520effectively%2520fool%2520modern%2520image%2520classifiers%252C%2520thus%250Ashowing%2520that%2520machines%2520have%2520not%2520caught%2520up%2520with%2520humans%2520--%2520yet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Through%20the%20Mask%3A%20Rethinking%20Adversarial%20Examples%20for%20CAPTCHAs&entry.906535625=Yahya%20Jabary%20and%20Andreas%20Plesner%20and%20Turlan%20Kuzhagaliyev%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Modern%20CAPTCHAs%20rely%20heavily%20on%20vision%20tasks%20that%20are%20supposedly%20hard%20for%0Acomputers%20but%20easy%20for%20humans.%20However%2C%20advances%20in%20image%20recognition%20models%0Apose%20a%20significant%20threat%20to%20such%20CAPTCHAs.%20These%20models%20can%20easily%20be%20fooled%0Aby%20generating%20some%20well-hidden%20%22random%22%20noise%20and%20adding%20it%20to%20the%20image%2C%20or%0Ahiding%20objects%20in%20the%20image.%20However%2C%20these%20methods%20are%20model-specific%20and%20thus%0Acan%20not%20aid%20CAPTCHAs%20in%20fooling%20all%20models.%20We%20show%20in%20this%20work%20that%20by%0Aallowing%20for%20more%20significant%20changes%20to%20the%20images%20while%20preserving%20the%0Asemantic%20information%20and%20keeping%20it%20solvable%20by%20humans%2C%20we%20can%20fool%20many%0Astate-of-the-art%20models.%20Specifically%2C%20we%20demonstrate%20that%20by%20adding%20masks%20of%0Avarious%20intensities%20the%20Accuracy%20%40%201%20%28Acc%401%29%20drops%20by%20more%20than%2050%25-points%20for%0Aall%20models%2C%20and%20supposedly%20robust%20models%20such%20as%20vision%20transformers%20see%20an%0AAcc%401%20drop%20of%2080%25-points.%0A%20%20These%20masks%20can%20therefore%20effectively%20fool%20modern%20image%20classifiers%2C%20thus%0Ashowing%20that%20machines%20have%20not%20caught%20up%20with%20humans%20--%20yet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05558v1&entry.124074799=Read"},
{"title": "PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal\n  Conversational Aspect-based Sentiment Analysis", "author": "Meng Luo and Hao Fei and Bobo Li and Shengqiong Wu and Qian Liu and Soujanya Poria and Erik Cambria and Mong-Li Lee and Wynne Hsu", "abstract": "  While existing Aspect-based Sentiment Analysis (ABSA) has received extensive\neffort and advancement, there are still gaps in defining a more holistic\nresearch target seamlessly integrating multimodality, conversation context,\nfine-granularity, and also covering the changing sentiment dynamics as well as\ncognitive causal rationales. This paper bridges the gaps by introducing a\nmultimodal conversational ABSA, where two novel subtasks are proposed: 1)\nPanoptic Sentiment Sextuple Extraction, panoramically recognizing holder,\ntarget, aspect, opinion, sentiment, rationale from multi-turn multi-party\nmultimodal dialogue. 2) Sentiment Flipping Analysis, detecting the dynamic\nsentiment transformation throughout the conversation with the causal reasons.\nTo benchmark the tasks, we construct PanoSent, a dataset annotated both\nmanually and automatically, featuring high quality, large scale, multimodality,\nmultilingualism, multi-scenarios, and covering both implicit and explicit\nsentiment elements. To effectively address the tasks, we devise a novel\nChain-of-Sentiment reasoning framework, together with a novel multimodal large\nlanguage model (namely Sentica) and a paraphrase-based verification mechanism.\nExtensive evaluations demonstrate the superiority of our methods over strong\nbaselines, validating the efficacy of all our proposed methods. The work is\nexpected to open up a new era for the ABSA community, and thus all our codes\nand data are open at https://PanoSent.github.io/\n", "link": "http://arxiv.org/abs/2408.09481v2", "date": "2024-09-09", "relevancy": 2.5382, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoSent%3A%20A%20Panoptic%20Sextuple%20Extraction%20Benchmark%20for%20Multimodal%0A%20%20Conversational%20Aspect-based%20Sentiment%20Analysis&body=Title%3A%20PanoSent%3A%20A%20Panoptic%20Sextuple%20Extraction%20Benchmark%20for%20Multimodal%0A%20%20Conversational%20Aspect-based%20Sentiment%20Analysis%0AAuthor%3A%20Meng%20Luo%20and%20Hao%20Fei%20and%20Bobo%20Li%20and%20Shengqiong%20Wu%20and%20Qian%20Liu%20and%20Soujanya%20Poria%20and%20Erik%20Cambria%20and%20Mong-Li%20Lee%20and%20Wynne%20Hsu%0AAbstract%3A%20%20%20While%20existing%20Aspect-based%20Sentiment%20Analysis%20%28ABSA%29%20has%20received%20extensive%0Aeffort%20and%20advancement%2C%20there%20are%20still%20gaps%20in%20defining%20a%20more%20holistic%0Aresearch%20target%20seamlessly%20integrating%20multimodality%2C%20conversation%20context%2C%0Afine-granularity%2C%20and%20also%20covering%20the%20changing%20sentiment%20dynamics%20as%20well%20as%0Acognitive%20causal%20rationales.%20This%20paper%20bridges%20the%20gaps%20by%20introducing%20a%0Amultimodal%20conversational%20ABSA%2C%20where%20two%20novel%20subtasks%20are%20proposed%3A%201%29%0APanoptic%20Sentiment%20Sextuple%20Extraction%2C%20panoramically%20recognizing%20holder%2C%0Atarget%2C%20aspect%2C%20opinion%2C%20sentiment%2C%20rationale%20from%20multi-turn%20multi-party%0Amultimodal%20dialogue.%202%29%20Sentiment%20Flipping%20Analysis%2C%20detecting%20the%20dynamic%0Asentiment%20transformation%20throughout%20the%20conversation%20with%20the%20causal%20reasons.%0ATo%20benchmark%20the%20tasks%2C%20we%20construct%20PanoSent%2C%20a%20dataset%20annotated%20both%0Amanually%20and%20automatically%2C%20featuring%20high%20quality%2C%20large%20scale%2C%20multimodality%2C%0Amultilingualism%2C%20multi-scenarios%2C%20and%20covering%20both%20implicit%20and%20explicit%0Asentiment%20elements.%20To%20effectively%20address%20the%20tasks%2C%20we%20devise%20a%20novel%0AChain-of-Sentiment%20reasoning%20framework%2C%20together%20with%20a%20novel%20multimodal%20large%0Alanguage%20model%20%28namely%20Sentica%29%20and%20a%20paraphrase-based%20verification%20mechanism.%0AExtensive%20evaluations%20demonstrate%20the%20superiority%20of%20our%20methods%20over%20strong%0Abaselines%2C%20validating%20the%20efficacy%20of%20all%20our%20proposed%20methods.%20The%20work%20is%0Aexpected%20to%20open%20up%20a%20new%20era%20for%20the%20ABSA%20community%2C%20and%20thus%20all%20our%20codes%0Aand%20data%20are%20open%20at%20https%3A//PanoSent.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoSent%253A%2520A%2520Panoptic%2520Sextuple%2520Extraction%2520Benchmark%2520for%2520Multimodal%250A%2520%2520Conversational%2520Aspect-based%2520Sentiment%2520Analysis%26entry.906535625%3DMeng%2520Luo%2520and%2520Hao%2520Fei%2520and%2520Bobo%2520Li%2520and%2520Shengqiong%2520Wu%2520and%2520Qian%2520Liu%2520and%2520Soujanya%2520Poria%2520and%2520Erik%2520Cambria%2520and%2520Mong-Li%2520Lee%2520and%2520Wynne%2520Hsu%26entry.1292438233%3D%2520%2520While%2520existing%2520Aspect-based%2520Sentiment%2520Analysis%2520%2528ABSA%2529%2520has%2520received%2520extensive%250Aeffort%2520and%2520advancement%252C%2520there%2520are%2520still%2520gaps%2520in%2520defining%2520a%2520more%2520holistic%250Aresearch%2520target%2520seamlessly%2520integrating%2520multimodality%252C%2520conversation%2520context%252C%250Afine-granularity%252C%2520and%2520also%2520covering%2520the%2520changing%2520sentiment%2520dynamics%2520as%2520well%2520as%250Acognitive%2520causal%2520rationales.%2520This%2520paper%2520bridges%2520the%2520gaps%2520by%2520introducing%2520a%250Amultimodal%2520conversational%2520ABSA%252C%2520where%2520two%2520novel%2520subtasks%2520are%2520proposed%253A%25201%2529%250APanoptic%2520Sentiment%2520Sextuple%2520Extraction%252C%2520panoramically%2520recognizing%2520holder%252C%250Atarget%252C%2520aspect%252C%2520opinion%252C%2520sentiment%252C%2520rationale%2520from%2520multi-turn%2520multi-party%250Amultimodal%2520dialogue.%25202%2529%2520Sentiment%2520Flipping%2520Analysis%252C%2520detecting%2520the%2520dynamic%250Asentiment%2520transformation%2520throughout%2520the%2520conversation%2520with%2520the%2520causal%2520reasons.%250ATo%2520benchmark%2520the%2520tasks%252C%2520we%2520construct%2520PanoSent%252C%2520a%2520dataset%2520annotated%2520both%250Amanually%2520and%2520automatically%252C%2520featuring%2520high%2520quality%252C%2520large%2520scale%252C%2520multimodality%252C%250Amultilingualism%252C%2520multi-scenarios%252C%2520and%2520covering%2520both%2520implicit%2520and%2520explicit%250Asentiment%2520elements.%2520To%2520effectively%2520address%2520the%2520tasks%252C%2520we%2520devise%2520a%2520novel%250AChain-of-Sentiment%2520reasoning%2520framework%252C%2520together%2520with%2520a%2520novel%2520multimodal%2520large%250Alanguage%2520model%2520%2528namely%2520Sentica%2529%2520and%2520a%2520paraphrase-based%2520verification%2520mechanism.%250AExtensive%2520evaluations%2520demonstrate%2520the%2520superiority%2520of%2520our%2520methods%2520over%2520strong%250Abaselines%252C%2520validating%2520the%2520efficacy%2520of%2520all%2520our%2520proposed%2520methods.%2520The%2520work%2520is%250Aexpected%2520to%2520open%2520up%2520a%2520new%2520era%2520for%2520the%2520ABSA%2520community%252C%2520and%2520thus%2520all%2520our%2520codes%250Aand%2520data%2520are%2520open%2520at%2520https%253A//PanoSent.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoSent%3A%20A%20Panoptic%20Sextuple%20Extraction%20Benchmark%20for%20Multimodal%0A%20%20Conversational%20Aspect-based%20Sentiment%20Analysis&entry.906535625=Meng%20Luo%20and%20Hao%20Fei%20and%20Bobo%20Li%20and%20Shengqiong%20Wu%20and%20Qian%20Liu%20and%20Soujanya%20Poria%20and%20Erik%20Cambria%20and%20Mong-Li%20Lee%20and%20Wynne%20Hsu&entry.1292438233=%20%20While%20existing%20Aspect-based%20Sentiment%20Analysis%20%28ABSA%29%20has%20received%20extensive%0Aeffort%20and%20advancement%2C%20there%20are%20still%20gaps%20in%20defining%20a%20more%20holistic%0Aresearch%20target%20seamlessly%20integrating%20multimodality%2C%20conversation%20context%2C%0Afine-granularity%2C%20and%20also%20covering%20the%20changing%20sentiment%20dynamics%20as%20well%20as%0Acognitive%20causal%20rationales.%20This%20paper%20bridges%20the%20gaps%20by%20introducing%20a%0Amultimodal%20conversational%20ABSA%2C%20where%20two%20novel%20subtasks%20are%20proposed%3A%201%29%0APanoptic%20Sentiment%20Sextuple%20Extraction%2C%20panoramically%20recognizing%20holder%2C%0Atarget%2C%20aspect%2C%20opinion%2C%20sentiment%2C%20rationale%20from%20multi-turn%20multi-party%0Amultimodal%20dialogue.%202%29%20Sentiment%20Flipping%20Analysis%2C%20detecting%20the%20dynamic%0Asentiment%20transformation%20throughout%20the%20conversation%20with%20the%20causal%20reasons.%0ATo%20benchmark%20the%20tasks%2C%20we%20construct%20PanoSent%2C%20a%20dataset%20annotated%20both%0Amanually%20and%20automatically%2C%20featuring%20high%20quality%2C%20large%20scale%2C%20multimodality%2C%0Amultilingualism%2C%20multi-scenarios%2C%20and%20covering%20both%20implicit%20and%20explicit%0Asentiment%20elements.%20To%20effectively%20address%20the%20tasks%2C%20we%20devise%20a%20novel%0AChain-of-Sentiment%20reasoning%20framework%2C%20together%20with%20a%20novel%20multimodal%20large%0Alanguage%20model%20%28namely%20Sentica%29%20and%20a%20paraphrase-based%20verification%20mechanism.%0AExtensive%20evaluations%20demonstrate%20the%20superiority%20of%20our%20methods%20over%20strong%0Abaselines%2C%20validating%20the%20efficacy%20of%20all%20our%20proposed%20methods.%20The%20work%20is%0Aexpected%20to%20open%20up%20a%20new%20era%20for%20the%20ABSA%20community%2C%20and%20thus%20all%20our%20codes%0Aand%20data%20are%20open%20at%20https%3A//PanoSent.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09481v2&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation of Histopathology Foundation Models for\n  Ovarian Cancer Subtype Classification", "author": "Jack Breen and Katie Allen and Kieran Zucker and Lucy Godson and Nicolas M. Orsi and Nishant Ravikumar", "abstract": "  Large pretrained transformers are increasingly being developed as generalised\nfoundation models which can underpin powerful task-specific artificial\nintelligence models. Histopathology foundation models show great promise across\nmany tasks, but analyses have typically been limited by arbitrary\nhyperparameters that were not tuned to the specific task. We report the most\nrigorous single-task validation of histopathology foundation models to date,\nspecifically in ovarian cancer morphological subtyping. Attention-based\nmultiple instance learning classifiers were compared using three\nImageNet-pretrained feature extractors and fourteen histopathology foundation\nmodels. The training set consisted of 1864 whole slide images from 434 ovarian\ncarcinoma cases at Leeds Teaching Hospitals NHS Trust. Five-class\nclassification performance was evaluated through five-fold cross-validation,\nand these cross-validation models were ensembled for hold-out testing and\nexternal validation on the Transcanadian Study and OCEAN Challenge datasets.\nThe best-performing model used the H-optimus-0 foundation model, with\nfive-class balanced accuracies of 89%, 97%, and 74% in the test sets.\nNormalisations and augmentations aided the performance of the\nImageNet-pretrained ResNets, but these were still outperformed by 13 of the 14\nfoundation models. Hyperparameter tuning the downstream classifiers improved\nperformance by a median 1.9% balanced accuracy, with many improvements being\nstatistically significant. Histopathology foundation models offer a clear\nbenefit to ovarian cancer subtyping, improving classification performance to a\ndegree where clinical utility is tangible, albeit with an increased\ncomputational burden. Such models could provide a second opinion to\nhistopathologists diagnosing challenging cases and may improve the accuracy,\nobjectivity, and efficiency of pathological diagnoses overall.\n", "link": "http://arxiv.org/abs/2405.09990v2", "date": "2024-09-09", "relevancy": 2.5253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20of%20Histopathology%20Foundation%20Models%20for%0A%20%20Ovarian%20Cancer%20Subtype%20Classification&body=Title%3A%20A%20Comprehensive%20Evaluation%20of%20Histopathology%20Foundation%20Models%20for%0A%20%20Ovarian%20Cancer%20Subtype%20Classification%0AAuthor%3A%20Jack%20Breen%20and%20Katie%20Allen%20and%20Kieran%20Zucker%20and%20Lucy%20Godson%20and%20Nicolas%20M.%20Orsi%20and%20Nishant%20Ravikumar%0AAbstract%3A%20%20%20Large%20pretrained%20transformers%20are%20increasingly%20being%20developed%20as%20generalised%0Afoundation%20models%20which%20can%20underpin%20powerful%20task-specific%20artificial%0Aintelligence%20models.%20Histopathology%20foundation%20models%20show%20great%20promise%20across%0Amany%20tasks%2C%20but%20analyses%20have%20typically%20been%20limited%20by%20arbitrary%0Ahyperparameters%20that%20were%20not%20tuned%20to%20the%20specific%20task.%20We%20report%20the%20most%0Arigorous%20single-task%20validation%20of%20histopathology%20foundation%20models%20to%20date%2C%0Aspecifically%20in%20ovarian%20cancer%20morphological%20subtyping.%20Attention-based%0Amultiple%20instance%20learning%20classifiers%20were%20compared%20using%20three%0AImageNet-pretrained%20feature%20extractors%20and%20fourteen%20histopathology%20foundation%0Amodels.%20The%20training%20set%20consisted%20of%201864%20whole%20slide%20images%20from%20434%20ovarian%0Acarcinoma%20cases%20at%20Leeds%20Teaching%20Hospitals%20NHS%20Trust.%20Five-class%0Aclassification%20performance%20was%20evaluated%20through%20five-fold%20cross-validation%2C%0Aand%20these%20cross-validation%20models%20were%20ensembled%20for%20hold-out%20testing%20and%0Aexternal%20validation%20on%20the%20Transcanadian%20Study%20and%20OCEAN%20Challenge%20datasets.%0AThe%20best-performing%20model%20used%20the%20H-optimus-0%20foundation%20model%2C%20with%0Afive-class%20balanced%20accuracies%20of%2089%25%2C%2097%25%2C%20and%2074%25%20in%20the%20test%20sets.%0ANormalisations%20and%20augmentations%20aided%20the%20performance%20of%20the%0AImageNet-pretrained%20ResNets%2C%20but%20these%20were%20still%20outperformed%20by%2013%20of%20the%2014%0Afoundation%20models.%20Hyperparameter%20tuning%20the%20downstream%20classifiers%20improved%0Aperformance%20by%20a%20median%201.9%25%20balanced%20accuracy%2C%20with%20many%20improvements%20being%0Astatistically%20significant.%20Histopathology%20foundation%20models%20offer%20a%20clear%0Abenefit%20to%20ovarian%20cancer%20subtyping%2C%20improving%20classification%20performance%20to%20a%0Adegree%20where%20clinical%20utility%20is%20tangible%2C%20albeit%20with%20an%20increased%0Acomputational%20burden.%20Such%20models%20could%20provide%20a%20second%20opinion%20to%0Ahistopathologists%20diagnosing%20challenging%20cases%20and%20may%20improve%20the%20accuracy%2C%0Aobjectivity%2C%20and%20efficiency%20of%20pathological%20diagnoses%20overall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520of%2520Histopathology%2520Foundation%2520Models%2520for%250A%2520%2520Ovarian%2520Cancer%2520Subtype%2520Classification%26entry.906535625%3DJack%2520Breen%2520and%2520Katie%2520Allen%2520and%2520Kieran%2520Zucker%2520and%2520Lucy%2520Godson%2520and%2520Nicolas%2520M.%2520Orsi%2520and%2520Nishant%2520Ravikumar%26entry.1292438233%3D%2520%2520Large%2520pretrained%2520transformers%2520are%2520increasingly%2520being%2520developed%2520as%2520generalised%250Afoundation%2520models%2520which%2520can%2520underpin%2520powerful%2520task-specific%2520artificial%250Aintelligence%2520models.%2520Histopathology%2520foundation%2520models%2520show%2520great%2520promise%2520across%250Amany%2520tasks%252C%2520but%2520analyses%2520have%2520typically%2520been%2520limited%2520by%2520arbitrary%250Ahyperparameters%2520that%2520were%2520not%2520tuned%2520to%2520the%2520specific%2520task.%2520We%2520report%2520the%2520most%250Arigorous%2520single-task%2520validation%2520of%2520histopathology%2520foundation%2520models%2520to%2520date%252C%250Aspecifically%2520in%2520ovarian%2520cancer%2520morphological%2520subtyping.%2520Attention-based%250Amultiple%2520instance%2520learning%2520classifiers%2520were%2520compared%2520using%2520three%250AImageNet-pretrained%2520feature%2520extractors%2520and%2520fourteen%2520histopathology%2520foundation%250Amodels.%2520The%2520training%2520set%2520consisted%2520of%25201864%2520whole%2520slide%2520images%2520from%2520434%2520ovarian%250Acarcinoma%2520cases%2520at%2520Leeds%2520Teaching%2520Hospitals%2520NHS%2520Trust.%2520Five-class%250Aclassification%2520performance%2520was%2520evaluated%2520through%2520five-fold%2520cross-validation%252C%250Aand%2520these%2520cross-validation%2520models%2520were%2520ensembled%2520for%2520hold-out%2520testing%2520and%250Aexternal%2520validation%2520on%2520the%2520Transcanadian%2520Study%2520and%2520OCEAN%2520Challenge%2520datasets.%250AThe%2520best-performing%2520model%2520used%2520the%2520H-optimus-0%2520foundation%2520model%252C%2520with%250Afive-class%2520balanced%2520accuracies%2520of%252089%2525%252C%252097%2525%252C%2520and%252074%2525%2520in%2520the%2520test%2520sets.%250ANormalisations%2520and%2520augmentations%2520aided%2520the%2520performance%2520of%2520the%250AImageNet-pretrained%2520ResNets%252C%2520but%2520these%2520were%2520still%2520outperformed%2520by%252013%2520of%2520the%252014%250Afoundation%2520models.%2520Hyperparameter%2520tuning%2520the%2520downstream%2520classifiers%2520improved%250Aperformance%2520by%2520a%2520median%25201.9%2525%2520balanced%2520accuracy%252C%2520with%2520many%2520improvements%2520being%250Astatistically%2520significant.%2520Histopathology%2520foundation%2520models%2520offer%2520a%2520clear%250Abenefit%2520to%2520ovarian%2520cancer%2520subtyping%252C%2520improving%2520classification%2520performance%2520to%2520a%250Adegree%2520where%2520clinical%2520utility%2520is%2520tangible%252C%2520albeit%2520with%2520an%2520increased%250Acomputational%2520burden.%2520Such%2520models%2520could%2520provide%2520a%2520second%2520opinion%2520to%250Ahistopathologists%2520diagnosing%2520challenging%2520cases%2520and%2520may%2520improve%2520the%2520accuracy%252C%250Aobjectivity%252C%2520and%2520efficiency%2520of%2520pathological%2520diagnoses%2520overall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20of%20Histopathology%20Foundation%20Models%20for%0A%20%20Ovarian%20Cancer%20Subtype%20Classification&entry.906535625=Jack%20Breen%20and%20Katie%20Allen%20and%20Kieran%20Zucker%20and%20Lucy%20Godson%20and%20Nicolas%20M.%20Orsi%20and%20Nishant%20Ravikumar&entry.1292438233=%20%20Large%20pretrained%20transformers%20are%20increasingly%20being%20developed%20as%20generalised%0Afoundation%20models%20which%20can%20underpin%20powerful%20task-specific%20artificial%0Aintelligence%20models.%20Histopathology%20foundation%20models%20show%20great%20promise%20across%0Amany%20tasks%2C%20but%20analyses%20have%20typically%20been%20limited%20by%20arbitrary%0Ahyperparameters%20that%20were%20not%20tuned%20to%20the%20specific%20task.%20We%20report%20the%20most%0Arigorous%20single-task%20validation%20of%20histopathology%20foundation%20models%20to%20date%2C%0Aspecifically%20in%20ovarian%20cancer%20morphological%20subtyping.%20Attention-based%0Amultiple%20instance%20learning%20classifiers%20were%20compared%20using%20three%0AImageNet-pretrained%20feature%20extractors%20and%20fourteen%20histopathology%20foundation%0Amodels.%20The%20training%20set%20consisted%20of%201864%20whole%20slide%20images%20from%20434%20ovarian%0Acarcinoma%20cases%20at%20Leeds%20Teaching%20Hospitals%20NHS%20Trust.%20Five-class%0Aclassification%20performance%20was%20evaluated%20through%20five-fold%20cross-validation%2C%0Aand%20these%20cross-validation%20models%20were%20ensembled%20for%20hold-out%20testing%20and%0Aexternal%20validation%20on%20the%20Transcanadian%20Study%20and%20OCEAN%20Challenge%20datasets.%0AThe%20best-performing%20model%20used%20the%20H-optimus-0%20foundation%20model%2C%20with%0Afive-class%20balanced%20accuracies%20of%2089%25%2C%2097%25%2C%20and%2074%25%20in%20the%20test%20sets.%0ANormalisations%20and%20augmentations%20aided%20the%20performance%20of%20the%0AImageNet-pretrained%20ResNets%2C%20but%20these%20were%20still%20outperformed%20by%2013%20of%20the%2014%0Afoundation%20models.%20Hyperparameter%20tuning%20the%20downstream%20classifiers%20improved%0Aperformance%20by%20a%20median%201.9%25%20balanced%20accuracy%2C%20with%20many%20improvements%20being%0Astatistically%20significant.%20Histopathology%20foundation%20models%20offer%20a%20clear%0Abenefit%20to%20ovarian%20cancer%20subtyping%2C%20improving%20classification%20performance%20to%20a%0Adegree%20where%20clinical%20utility%20is%20tangible%2C%20albeit%20with%20an%20increased%0Acomputational%20burden.%20Such%20models%20could%20provide%20a%20second%20opinion%20to%0Ahistopathologists%20diagnosing%20challenging%20cases%20and%20may%20improve%20the%20accuracy%2C%0Aobjectivity%2C%20and%20efficiency%20of%20pathological%20diagnoses%20overall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09990v2&entry.124074799=Read"},
{"title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free\n  Real Image Editing", "author": "Vadim Titov and Madina Khalmatova and Alexandra Ivanova and Dmitry Vetrov and Aibek Alanov", "abstract": "  Despite recent advances in large-scale text-to-image generative models,\nmanipulating real images with these models remains a challenging problem. The\nmain limitations of existing editing methods are that they either fail to\nperform with consistent quality on a wide range of image edits or require\ntime-consuming hyperparameter tuning or fine-tuning of the diffusion model to\npreserve the image-specific appearance of the input image. We propose a novel\napproach that is built upon a modified diffusion sampling process via the\nguidance mechanism. In this work, we explore the self-guidance technique to\npreserve the overall structure of the input image and its local regions\nappearance that should not be edited. In particular, we explicitly introduce\nlayout-preserving energy functions that are aimed to save local and global\nstructures of the source image. Additionally, we propose a noise rescaling\nmechanism that allows to preserve noise distribution by balancing the norms of\nclassifier-free guidance and our proposed guiders during generation. Such a\nguiding approach does not require fine-tuning the diffusion model and exact\ninversion process. As a result, the proposed method provides a fast and\nhigh-quality editing mechanism. In our experiments, we show through human\nevaluation and quantitative analysis that the proposed method allows to produce\ndesired editing which is more preferable by humans and also achieves a better\ntrade-off between editing quality and preservation of the original image. Our\ncode is available at https://github.com/FusionBrainLab/Guide-and-Rescale.\n", "link": "http://arxiv.org/abs/2409.01322v2", "date": "2024-09-09", "relevancy": 2.521, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6666}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6437}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guide-and-Rescale%3A%20Self-Guidance%20Mechanism%20for%20Effective%20Tuning-Free%0A%20%20Real%20Image%20Editing&body=Title%3A%20Guide-and-Rescale%3A%20Self-Guidance%20Mechanism%20for%20Effective%20Tuning-Free%0A%20%20Real%20Image%20Editing%0AAuthor%3A%20Vadim%20Titov%20and%20Madina%20Khalmatova%20and%20Alexandra%20Ivanova%20and%20Dmitry%20Vetrov%20and%20Aibek%20Alanov%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20large-scale%20text-to-image%20generative%20models%2C%0Amanipulating%20real%20images%20with%20these%20models%20remains%20a%20challenging%20problem.%20The%0Amain%20limitations%20of%20existing%20editing%20methods%20are%20that%20they%20either%20fail%20to%0Aperform%20with%20consistent%20quality%20on%20a%20wide%20range%20of%20image%20edits%20or%20require%0Atime-consuming%20hyperparameter%20tuning%20or%20fine-tuning%20of%20the%20diffusion%20model%20to%0Apreserve%20the%20image-specific%20appearance%20of%20the%20input%20image.%20We%20propose%20a%20novel%0Aapproach%20that%20is%20built%20upon%20a%20modified%20diffusion%20sampling%20process%20via%20the%0Aguidance%20mechanism.%20In%20this%20work%2C%20we%20explore%20the%20self-guidance%20technique%20to%0Apreserve%20the%20overall%20structure%20of%20the%20input%20image%20and%20its%20local%20regions%0Aappearance%20that%20should%20not%20be%20edited.%20In%20particular%2C%20we%20explicitly%20introduce%0Alayout-preserving%20energy%20functions%20that%20are%20aimed%20to%20save%20local%20and%20global%0Astructures%20of%20the%20source%20image.%20Additionally%2C%20we%20propose%20a%20noise%20rescaling%0Amechanism%20that%20allows%20to%20preserve%20noise%20distribution%20by%20balancing%20the%20norms%20of%0Aclassifier-free%20guidance%20and%20our%20proposed%20guiders%20during%20generation.%20Such%20a%0Aguiding%20approach%20does%20not%20require%20fine-tuning%20the%20diffusion%20model%20and%20exact%0Ainversion%20process.%20As%20a%20result%2C%20the%20proposed%20method%20provides%20a%20fast%20and%0Ahigh-quality%20editing%20mechanism.%20In%20our%20experiments%2C%20we%20show%20through%20human%0Aevaluation%20and%20quantitative%20analysis%20that%20the%20proposed%20method%20allows%20to%20produce%0Adesired%20editing%20which%20is%20more%20preferable%20by%20humans%20and%20also%20achieves%20a%20better%0Atrade-off%20between%20editing%20quality%20and%20preservation%20of%20the%20original%20image.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/FusionBrainLab/Guide-and-Rescale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuide-and-Rescale%253A%2520Self-Guidance%2520Mechanism%2520for%2520Effective%2520Tuning-Free%250A%2520%2520Real%2520Image%2520Editing%26entry.906535625%3DVadim%2520Titov%2520and%2520Madina%2520Khalmatova%2520and%2520Alexandra%2520Ivanova%2520and%2520Dmitry%2520Vetrov%2520and%2520Aibek%2520Alanov%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520large-scale%2520text-to-image%2520generative%2520models%252C%250Amanipulating%2520real%2520images%2520with%2520these%2520models%2520remains%2520a%2520challenging%2520problem.%2520The%250Amain%2520limitations%2520of%2520existing%2520editing%2520methods%2520are%2520that%2520they%2520either%2520fail%2520to%250Aperform%2520with%2520consistent%2520quality%2520on%2520a%2520wide%2520range%2520of%2520image%2520edits%2520or%2520require%250Atime-consuming%2520hyperparameter%2520tuning%2520or%2520fine-tuning%2520of%2520the%2520diffusion%2520model%2520to%250Apreserve%2520the%2520image-specific%2520appearance%2520of%2520the%2520input%2520image.%2520We%2520propose%2520a%2520novel%250Aapproach%2520that%2520is%2520built%2520upon%2520a%2520modified%2520diffusion%2520sampling%2520process%2520via%2520the%250Aguidance%2520mechanism.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520self-guidance%2520technique%2520to%250Apreserve%2520the%2520overall%2520structure%2520of%2520the%2520input%2520image%2520and%2520its%2520local%2520regions%250Aappearance%2520that%2520should%2520not%2520be%2520edited.%2520In%2520particular%252C%2520we%2520explicitly%2520introduce%250Alayout-preserving%2520energy%2520functions%2520that%2520are%2520aimed%2520to%2520save%2520local%2520and%2520global%250Astructures%2520of%2520the%2520source%2520image.%2520Additionally%252C%2520we%2520propose%2520a%2520noise%2520rescaling%250Amechanism%2520that%2520allows%2520to%2520preserve%2520noise%2520distribution%2520by%2520balancing%2520the%2520norms%2520of%250Aclassifier-free%2520guidance%2520and%2520our%2520proposed%2520guiders%2520during%2520generation.%2520Such%2520a%250Aguiding%2520approach%2520does%2520not%2520require%2520fine-tuning%2520the%2520diffusion%2520model%2520and%2520exact%250Ainversion%2520process.%2520As%2520a%2520result%252C%2520the%2520proposed%2520method%2520provides%2520a%2520fast%2520and%250Ahigh-quality%2520editing%2520mechanism.%2520In%2520our%2520experiments%252C%2520we%2520show%2520through%2520human%250Aevaluation%2520and%2520quantitative%2520analysis%2520that%2520the%2520proposed%2520method%2520allows%2520to%2520produce%250Adesired%2520editing%2520which%2520is%2520more%2520preferable%2520by%2520humans%2520and%2520also%2520achieves%2520a%2520better%250Atrade-off%2520between%2520editing%2520quality%2520and%2520preservation%2520of%2520the%2520original%2520image.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/FusionBrainLab/Guide-and-Rescale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guide-and-Rescale%3A%20Self-Guidance%20Mechanism%20for%20Effective%20Tuning-Free%0A%20%20Real%20Image%20Editing&entry.906535625=Vadim%20Titov%20and%20Madina%20Khalmatova%20and%20Alexandra%20Ivanova%20and%20Dmitry%20Vetrov%20and%20Aibek%20Alanov&entry.1292438233=%20%20Despite%20recent%20advances%20in%20large-scale%20text-to-image%20generative%20models%2C%0Amanipulating%20real%20images%20with%20these%20models%20remains%20a%20challenging%20problem.%20The%0Amain%20limitations%20of%20existing%20editing%20methods%20are%20that%20they%20either%20fail%20to%0Aperform%20with%20consistent%20quality%20on%20a%20wide%20range%20of%20image%20edits%20or%20require%0Atime-consuming%20hyperparameter%20tuning%20or%20fine-tuning%20of%20the%20diffusion%20model%20to%0Apreserve%20the%20image-specific%20appearance%20of%20the%20input%20image.%20We%20propose%20a%20novel%0Aapproach%20that%20is%20built%20upon%20a%20modified%20diffusion%20sampling%20process%20via%20the%0Aguidance%20mechanism.%20In%20this%20work%2C%20we%20explore%20the%20self-guidance%20technique%20to%0Apreserve%20the%20overall%20structure%20of%20the%20input%20image%20and%20its%20local%20regions%0Aappearance%20that%20should%20not%20be%20edited.%20In%20particular%2C%20we%20explicitly%20introduce%0Alayout-preserving%20energy%20functions%20that%20are%20aimed%20to%20save%20local%20and%20global%0Astructures%20of%20the%20source%20image.%20Additionally%2C%20we%20propose%20a%20noise%20rescaling%0Amechanism%20that%20allows%20to%20preserve%20noise%20distribution%20by%20balancing%20the%20norms%20of%0Aclassifier-free%20guidance%20and%20our%20proposed%20guiders%20during%20generation.%20Such%20a%0Aguiding%20approach%20does%20not%20require%20fine-tuning%20the%20diffusion%20model%20and%20exact%0Ainversion%20process.%20As%20a%20result%2C%20the%20proposed%20method%20provides%20a%20fast%20and%0Ahigh-quality%20editing%20mechanism.%20In%20our%20experiments%2C%20we%20show%20through%20human%0Aevaluation%20and%20quantitative%20analysis%20that%20the%20proposed%20method%20allows%20to%20produce%0Adesired%20editing%20which%20is%20more%20preferable%20by%20humans%20and%20also%20achieves%20a%20better%0Atrade-off%20between%20editing%20quality%20and%20preservation%20of%20the%20original%20image.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/FusionBrainLab/Guide-and-Rescale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01322v2&entry.124074799=Read"},
{"title": "Long-term Pre-training for Temporal Action Detection with Transformers", "author": "Jihwan Kim and Miso Lee and Jae-Pil Heo", "abstract": "  Temporal action detection (TAD) is challenging, yet fundamental for\nreal-world video applications. Recently, DETR-based models for TAD have been\nprevailing thanks to their unique benefits. However, transformers demand a huge\ndataset, and unfortunately data scarcity in TAD causes a severe degeneration.\nIn this paper, we identify two crucial problems from data scarcity: attention\ncollapse and imbalanced performance. To this end, we propose a new pre-training\nstrategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two\nmain components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly,\nwe synthesize long-form video features by merging video snippets of a target\nclass and non-target classes. They are analogous to untrimmed data used in TAD,\ndespite being created from trimmed data. In addition, we devise two types of\nlong-term pretext tasks to learn long-term dependency. They impose long-term\nconditions such as finding second-to-fourth or short-duration actions. Our\nextensive experiments show state-of-the-art performances in DETR-based methods\non ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate\nthat LTP significantly relieves the data scarcity issues in TAD.\n", "link": "http://arxiv.org/abs/2408.13152v2", "date": "2024-09-09", "relevancy": 2.5135, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6996}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5971}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-term%20Pre-training%20for%20Temporal%20Action%20Detection%20with%20Transformers&body=Title%3A%20Long-term%20Pre-training%20for%20Temporal%20Action%20Detection%20with%20Transformers%0AAuthor%3A%20Jihwan%20Kim%20and%20Miso%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Temporal%20action%20detection%20%28TAD%29%20is%20challenging%2C%20yet%20fundamental%20for%0Areal-world%20video%20applications.%20Recently%2C%20DETR-based%20models%20for%20TAD%20have%20been%0Aprevailing%20thanks%20to%20their%20unique%20benefits.%20However%2C%20transformers%20demand%20a%20huge%0Adataset%2C%20and%20unfortunately%20data%20scarcity%20in%20TAD%20causes%20a%20severe%20degeneration.%0AIn%20this%20paper%2C%20we%20identify%20two%20crucial%20problems%20from%20data%20scarcity%3A%20attention%0Acollapse%20and%20imbalanced%20performance.%20To%20this%20end%2C%20we%20propose%20a%20new%20pre-training%0Astrategy%2C%20Long-Term%20Pre-training%20%28LTP%29%2C%20tailored%20for%20transformers.%20LTP%20has%20two%0Amain%20components%3A%201%29%20class-wise%20synthesis%2C%202%29%20long-term%20pretext%20tasks.%20Firstly%2C%0Awe%20synthesize%20long-form%20video%20features%20by%20merging%20video%20snippets%20of%20a%20target%0Aclass%20and%20non-target%20classes.%20They%20are%20analogous%20to%20untrimmed%20data%20used%20in%20TAD%2C%0Adespite%20being%20created%20from%20trimmed%20data.%20In%20addition%2C%20we%20devise%20two%20types%20of%0Along-term%20pretext%20tasks%20to%20learn%20long-term%20dependency.%20They%20impose%20long-term%0Aconditions%20such%20as%20finding%20second-to-fourth%20or%20short-duration%20actions.%20Our%0Aextensive%20experiments%20show%20state-of-the-art%20performances%20in%20DETR-based%20methods%0Aon%20ActivityNet-v1.3%20and%20THUMOS14%20by%20a%20large%20margin.%20Moreover%2C%20we%20demonstrate%0Athat%20LTP%20significantly%20relieves%20the%20data%20scarcity%20issues%20in%20TAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-term%2520Pre-training%2520for%2520Temporal%2520Action%2520Detection%2520with%2520Transformers%26entry.906535625%3DJihwan%2520Kim%2520and%2520Miso%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Temporal%2520action%2520detection%2520%2528TAD%2529%2520is%2520challenging%252C%2520yet%2520fundamental%2520for%250Areal-world%2520video%2520applications.%2520Recently%252C%2520DETR-based%2520models%2520for%2520TAD%2520have%2520been%250Aprevailing%2520thanks%2520to%2520their%2520unique%2520benefits.%2520However%252C%2520transformers%2520demand%2520a%2520huge%250Adataset%252C%2520and%2520unfortunately%2520data%2520scarcity%2520in%2520TAD%2520causes%2520a%2520severe%2520degeneration.%250AIn%2520this%2520paper%252C%2520we%2520identify%2520two%2520crucial%2520problems%2520from%2520data%2520scarcity%253A%2520attention%250Acollapse%2520and%2520imbalanced%2520performance.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%2520pre-training%250Astrategy%252C%2520Long-Term%2520Pre-training%2520%2528LTP%2529%252C%2520tailored%2520for%2520transformers.%2520LTP%2520has%2520two%250Amain%2520components%253A%25201%2529%2520class-wise%2520synthesis%252C%25202%2529%2520long-term%2520pretext%2520tasks.%2520Firstly%252C%250Awe%2520synthesize%2520long-form%2520video%2520features%2520by%2520merging%2520video%2520snippets%2520of%2520a%2520target%250Aclass%2520and%2520non-target%2520classes.%2520They%2520are%2520analogous%2520to%2520untrimmed%2520data%2520used%2520in%2520TAD%252C%250Adespite%2520being%2520created%2520from%2520trimmed%2520data.%2520In%2520addition%252C%2520we%2520devise%2520two%2520types%2520of%250Along-term%2520pretext%2520tasks%2520to%2520learn%2520long-term%2520dependency.%2520They%2520impose%2520long-term%250Aconditions%2520such%2520as%2520finding%2520second-to-fourth%2520or%2520short-duration%2520actions.%2520Our%250Aextensive%2520experiments%2520show%2520state-of-the-art%2520performances%2520in%2520DETR-based%2520methods%250Aon%2520ActivityNet-v1.3%2520and%2520THUMOS14%2520by%2520a%2520large%2520margin.%2520Moreover%252C%2520we%2520demonstrate%250Athat%2520LTP%2520significantly%2520relieves%2520the%2520data%2520scarcity%2520issues%2520in%2520TAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-term%20Pre-training%20for%20Temporal%20Action%20Detection%20with%20Transformers&entry.906535625=Jihwan%20Kim%20and%20Miso%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Temporal%20action%20detection%20%28TAD%29%20is%20challenging%2C%20yet%20fundamental%20for%0Areal-world%20video%20applications.%20Recently%2C%20DETR-based%20models%20for%20TAD%20have%20been%0Aprevailing%20thanks%20to%20their%20unique%20benefits.%20However%2C%20transformers%20demand%20a%20huge%0Adataset%2C%20and%20unfortunately%20data%20scarcity%20in%20TAD%20causes%20a%20severe%20degeneration.%0AIn%20this%20paper%2C%20we%20identify%20two%20crucial%20problems%20from%20data%20scarcity%3A%20attention%0Acollapse%20and%20imbalanced%20performance.%20To%20this%20end%2C%20we%20propose%20a%20new%20pre-training%0Astrategy%2C%20Long-Term%20Pre-training%20%28LTP%29%2C%20tailored%20for%20transformers.%20LTP%20has%20two%0Amain%20components%3A%201%29%20class-wise%20synthesis%2C%202%29%20long-term%20pretext%20tasks.%20Firstly%2C%0Awe%20synthesize%20long-form%20video%20features%20by%20merging%20video%20snippets%20of%20a%20target%0Aclass%20and%20non-target%20classes.%20They%20are%20analogous%20to%20untrimmed%20data%20used%20in%20TAD%2C%0Adespite%20being%20created%20from%20trimmed%20data.%20In%20addition%2C%20we%20devise%20two%20types%20of%0Along-term%20pretext%20tasks%20to%20learn%20long-term%20dependency.%20They%20impose%20long-term%0Aconditions%20such%20as%20finding%20second-to-fourth%20or%20short-duration%20actions.%20Our%0Aextensive%20experiments%20show%20state-of-the-art%20performances%20in%20DETR-based%20methods%0Aon%20ActivityNet-v1.3%20and%20THUMOS14%20by%20a%20large%20margin.%20Moreover%2C%20we%20demonstrate%0Athat%20LTP%20significantly%20relieves%20the%20data%20scarcity%20issues%20in%20TAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13152v2&entry.124074799=Read"},
{"title": "Boundary Constraint-free Biomechanical Model-Based Surface Matching for\n  Intraoperative Liver Deformation Correction", "author": "Zixin Yang and Richard Simon and Kelly Merrell and Cristian. A. Linte", "abstract": "  In image-guided liver surgery, 3D-3D non-rigid registration methods play a\ncrucial role in estimating the mapping between the preoperative model and the\nintraoperative surface represented as point clouds, addressing the challenge of\ntissue deformation. Typically, these methods incorporate a biomechanical model,\nrepresented as a finite element model (FEM), used to regularize a surface\nmatching term. This paper introduces a novel 3D-3D non-rigid registration\nmethod. In contrast to the preceding techniques, our method uniquely\nincorporates the FEM within the surface matching term itself, ensuring that the\nestimated deformation maintains geometric consistency throughout the\nregistration process. Additionally, we eliminate the need to determine\nzero-boundary conditions and applied force locations in the FEM. We achieve\nthis by integrating soft springs into the stiffness matrix and allowing forces\nto be distributed across the entire liver surface. To further improve\nrobustness, we introduce a regularization technique focused on the gradient of\nthe force magnitudes. This regularization imposes spatial smoothness and helps\nprevent the overfitting of irregular noise in intraoperative data. Optimization\nis achieved through an accelerated proximal gradient algorithm, further\nenhanced by our proposed method for determining the optimal step size. Our\nmethod is evaluated and compared to both a learning-based method and a\ntraditional method that features FEM regularization using data collected on our\ncustom-developed phantom, as well as two publicly available datasets. Our\nmethod consistently outperforms or is comparable to the baseline techniques.\nBoth the code and dataset will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.09964v2", "date": "2024-09-09", "relevancy": 2.4876, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4865}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundary%20Constraint-free%20Biomechanical%20Model-Based%20Surface%20Matching%20for%0A%20%20Intraoperative%20Liver%20Deformation%20Correction&body=Title%3A%20Boundary%20Constraint-free%20Biomechanical%20Model-Based%20Surface%20Matching%20for%0A%20%20Intraoperative%20Liver%20Deformation%20Correction%0AAuthor%3A%20Zixin%20Yang%20and%20Richard%20Simon%20and%20Kelly%20Merrell%20and%20Cristian.%20A.%20Linte%0AAbstract%3A%20%20%20In%20image-guided%20liver%20surgery%2C%203D-3D%20non-rigid%20registration%20methods%20play%20a%0Acrucial%20role%20in%20estimating%20the%20mapping%20between%20the%20preoperative%20model%20and%20the%0Aintraoperative%20surface%20represented%20as%20point%20clouds%2C%20addressing%20the%20challenge%20of%0Atissue%20deformation.%20Typically%2C%20these%20methods%20incorporate%20a%20biomechanical%20model%2C%0Arepresented%20as%20a%20finite%20element%20model%20%28FEM%29%2C%20used%20to%20regularize%20a%20surface%0Amatching%20term.%20This%20paper%20introduces%20a%20novel%203D-3D%20non-rigid%20registration%0Amethod.%20In%20contrast%20to%20the%20preceding%20techniques%2C%20our%20method%20uniquely%0Aincorporates%20the%20FEM%20within%20the%20surface%20matching%20term%20itself%2C%20ensuring%20that%20the%0Aestimated%20deformation%20maintains%20geometric%20consistency%20throughout%20the%0Aregistration%20process.%20Additionally%2C%20we%20eliminate%20the%20need%20to%20determine%0Azero-boundary%20conditions%20and%20applied%20force%20locations%20in%20the%20FEM.%20We%20achieve%0Athis%20by%20integrating%20soft%20springs%20into%20the%20stiffness%20matrix%20and%20allowing%20forces%0Ato%20be%20distributed%20across%20the%20entire%20liver%20surface.%20To%20further%20improve%0Arobustness%2C%20we%20introduce%20a%20regularization%20technique%20focused%20on%20the%20gradient%20of%0Athe%20force%20magnitudes.%20This%20regularization%20imposes%20spatial%20smoothness%20and%20helps%0Aprevent%20the%20overfitting%20of%20irregular%20noise%20in%20intraoperative%20data.%20Optimization%0Ais%20achieved%20through%20an%20accelerated%20proximal%20gradient%20algorithm%2C%20further%0Aenhanced%20by%20our%20proposed%20method%20for%20determining%20the%20optimal%20step%20size.%20Our%0Amethod%20is%20evaluated%20and%20compared%20to%20both%20a%20learning-based%20method%20and%20a%0Atraditional%20method%20that%20features%20FEM%20regularization%20using%20data%20collected%20on%20our%0Acustom-developed%20phantom%2C%20as%20well%20as%20two%20publicly%20available%20datasets.%20Our%0Amethod%20consistently%20outperforms%20or%20is%20comparable%20to%20the%20baseline%20techniques.%0ABoth%20the%20code%20and%20dataset%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundary%2520Constraint-free%2520Biomechanical%2520Model-Based%2520Surface%2520Matching%2520for%250A%2520%2520Intraoperative%2520Liver%2520Deformation%2520Correction%26entry.906535625%3DZixin%2520Yang%2520and%2520Richard%2520Simon%2520and%2520Kelly%2520Merrell%2520and%2520Cristian.%2520A.%2520Linte%26entry.1292438233%3D%2520%2520In%2520image-guided%2520liver%2520surgery%252C%25203D-3D%2520non-rigid%2520registration%2520methods%2520play%2520a%250Acrucial%2520role%2520in%2520estimating%2520the%2520mapping%2520between%2520the%2520preoperative%2520model%2520and%2520the%250Aintraoperative%2520surface%2520represented%2520as%2520point%2520clouds%252C%2520addressing%2520the%2520challenge%2520of%250Atissue%2520deformation.%2520Typically%252C%2520these%2520methods%2520incorporate%2520a%2520biomechanical%2520model%252C%250Arepresented%2520as%2520a%2520finite%2520element%2520model%2520%2528FEM%2529%252C%2520used%2520to%2520regularize%2520a%2520surface%250Amatching%2520term.%2520This%2520paper%2520introduces%2520a%2520novel%25203D-3D%2520non-rigid%2520registration%250Amethod.%2520In%2520contrast%2520to%2520the%2520preceding%2520techniques%252C%2520our%2520method%2520uniquely%250Aincorporates%2520the%2520FEM%2520within%2520the%2520surface%2520matching%2520term%2520itself%252C%2520ensuring%2520that%2520the%250Aestimated%2520deformation%2520maintains%2520geometric%2520consistency%2520throughout%2520the%250Aregistration%2520process.%2520Additionally%252C%2520we%2520eliminate%2520the%2520need%2520to%2520determine%250Azero-boundary%2520conditions%2520and%2520applied%2520force%2520locations%2520in%2520the%2520FEM.%2520We%2520achieve%250Athis%2520by%2520integrating%2520soft%2520springs%2520into%2520the%2520stiffness%2520matrix%2520and%2520allowing%2520forces%250Ato%2520be%2520distributed%2520across%2520the%2520entire%2520liver%2520surface.%2520To%2520further%2520improve%250Arobustness%252C%2520we%2520introduce%2520a%2520regularization%2520technique%2520focused%2520on%2520the%2520gradient%2520of%250Athe%2520force%2520magnitudes.%2520This%2520regularization%2520imposes%2520spatial%2520smoothness%2520and%2520helps%250Aprevent%2520the%2520overfitting%2520of%2520irregular%2520noise%2520in%2520intraoperative%2520data.%2520Optimization%250Ais%2520achieved%2520through%2520an%2520accelerated%2520proximal%2520gradient%2520algorithm%252C%2520further%250Aenhanced%2520by%2520our%2520proposed%2520method%2520for%2520determining%2520the%2520optimal%2520step%2520size.%2520Our%250Amethod%2520is%2520evaluated%2520and%2520compared%2520to%2520both%2520a%2520learning-based%2520method%2520and%2520a%250Atraditional%2520method%2520that%2520features%2520FEM%2520regularization%2520using%2520data%2520collected%2520on%2520our%250Acustom-developed%2520phantom%252C%2520as%2520well%2520as%2520two%2520publicly%2520available%2520datasets.%2520Our%250Amethod%2520consistently%2520outperforms%2520or%2520is%2520comparable%2520to%2520the%2520baseline%2520techniques.%250ABoth%2520the%2520code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundary%20Constraint-free%20Biomechanical%20Model-Based%20Surface%20Matching%20for%0A%20%20Intraoperative%20Liver%20Deformation%20Correction&entry.906535625=Zixin%20Yang%20and%20Richard%20Simon%20and%20Kelly%20Merrell%20and%20Cristian.%20A.%20Linte&entry.1292438233=%20%20In%20image-guided%20liver%20surgery%2C%203D-3D%20non-rigid%20registration%20methods%20play%20a%0Acrucial%20role%20in%20estimating%20the%20mapping%20between%20the%20preoperative%20model%20and%20the%0Aintraoperative%20surface%20represented%20as%20point%20clouds%2C%20addressing%20the%20challenge%20of%0Atissue%20deformation.%20Typically%2C%20these%20methods%20incorporate%20a%20biomechanical%20model%2C%0Arepresented%20as%20a%20finite%20element%20model%20%28FEM%29%2C%20used%20to%20regularize%20a%20surface%0Amatching%20term.%20This%20paper%20introduces%20a%20novel%203D-3D%20non-rigid%20registration%0Amethod.%20In%20contrast%20to%20the%20preceding%20techniques%2C%20our%20method%20uniquely%0Aincorporates%20the%20FEM%20within%20the%20surface%20matching%20term%20itself%2C%20ensuring%20that%20the%0Aestimated%20deformation%20maintains%20geometric%20consistency%20throughout%20the%0Aregistration%20process.%20Additionally%2C%20we%20eliminate%20the%20need%20to%20determine%0Azero-boundary%20conditions%20and%20applied%20force%20locations%20in%20the%20FEM.%20We%20achieve%0Athis%20by%20integrating%20soft%20springs%20into%20the%20stiffness%20matrix%20and%20allowing%20forces%0Ato%20be%20distributed%20across%20the%20entire%20liver%20surface.%20To%20further%20improve%0Arobustness%2C%20we%20introduce%20a%20regularization%20technique%20focused%20on%20the%20gradient%20of%0Athe%20force%20magnitudes.%20This%20regularization%20imposes%20spatial%20smoothness%20and%20helps%0Aprevent%20the%20overfitting%20of%20irregular%20noise%20in%20intraoperative%20data.%20Optimization%0Ais%20achieved%20through%20an%20accelerated%20proximal%20gradient%20algorithm%2C%20further%0Aenhanced%20by%20our%20proposed%20method%20for%20determining%20the%20optimal%20step%20size.%20Our%0Amethod%20is%20evaluated%20and%20compared%20to%20both%20a%20learning-based%20method%20and%20a%0Atraditional%20method%20that%20features%20FEM%20regularization%20using%20data%20collected%20on%20our%0Acustom-developed%20phantom%2C%20as%20well%20as%20two%20publicly%20available%20datasets.%20Our%0Amethod%20consistently%20outperforms%20or%20is%20comparable%20to%20the%20baseline%20techniques.%0ABoth%20the%20code%20and%20dataset%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09964v2&entry.124074799=Read"},
{"title": "Seeing is Believing? Enhancing Vision-Language Navigation using Visual\n  Perturbations", "author": "Xuesong Zhang and Jia Li and Yunbo Xu and Zhenzhen Hu and Richang Hong", "abstract": "  Autonomous navigation for an embodied agent guided by natural language\ninstructions remains a formidable challenge in vision-and-language navigation\n(VLN). Despite remarkable recent progress in learning fine-grained and\nmultifarious visual representations, the tendency to overfit to the training\nenvironments leads to unsatisfactory generalization performance. In this work,\nwe present a versatile Multi-Branch Architecture (MBA) aimed at exploring and\nexploiting diverse visual inputs. Specifically, we introduce three distinct\nvisual variants: ground-truth depth images, visual inputs integrated with\nincongruent views, and those infused with random noise to enrich the diversity\nof visual input representation and prevent overfitting to the original RGB\nobservations. To adaptively fuse these varied inputs, the proposed MBA extend a\nbase agent model into a multi-branch variant, where each branch processes a\ndifferent visual input. Surprisingly, even random noise can further enhance\nnavigation performance in unseen environments. Extensive experiments conducted\non three VLN benchmarks (R2R, REVERIE, SOON) demonstrate that our proposed\nmethod equals or even surpasses state-of-the-art results. The source code will\nbe publicly available.\n", "link": "http://arxiv.org/abs/2409.05552v1", "date": "2024-09-09", "relevancy": 2.4791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6204}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20is%20Believing%3F%20Enhancing%20Vision-Language%20Navigation%20using%20Visual%0A%20%20Perturbations&body=Title%3A%20Seeing%20is%20Believing%3F%20Enhancing%20Vision-Language%20Navigation%20using%20Visual%0A%20%20Perturbations%0AAuthor%3A%20Xuesong%20Zhang%20and%20Jia%20Li%20and%20Yunbo%20Xu%20and%20Zhenzhen%20Hu%20and%20Richang%20Hong%0AAbstract%3A%20%20%20Autonomous%20navigation%20for%20an%20embodied%20agent%20guided%20by%20natural%20language%0Ainstructions%20remains%20a%20formidable%20challenge%20in%20vision-and-language%20navigation%0A%28VLN%29.%20Despite%20remarkable%20recent%20progress%20in%20learning%20fine-grained%20and%0Amultifarious%20visual%20representations%2C%20the%20tendency%20to%20overfit%20to%20the%20training%0Aenvironments%20leads%20to%20unsatisfactory%20generalization%20performance.%20In%20this%20work%2C%0Awe%20present%20a%20versatile%20Multi-Branch%20Architecture%20%28MBA%29%20aimed%20at%20exploring%20and%0Aexploiting%20diverse%20visual%20inputs.%20Specifically%2C%20we%20introduce%20three%20distinct%0Avisual%20variants%3A%20ground-truth%20depth%20images%2C%20visual%20inputs%20integrated%20with%0Aincongruent%20views%2C%20and%20those%20infused%20with%20random%20noise%20to%20enrich%20the%20diversity%0Aof%20visual%20input%20representation%20and%20prevent%20overfitting%20to%20the%20original%20RGB%0Aobservations.%20To%20adaptively%20fuse%20these%20varied%20inputs%2C%20the%20proposed%20MBA%20extend%20a%0Abase%20agent%20model%20into%20a%20multi-branch%20variant%2C%20where%20each%20branch%20processes%20a%0Adifferent%20visual%20input.%20Surprisingly%2C%20even%20random%20noise%20can%20further%20enhance%0Anavigation%20performance%20in%20unseen%20environments.%20Extensive%20experiments%20conducted%0Aon%20three%20VLN%20benchmarks%20%28R2R%2C%20REVERIE%2C%20SOON%29%20demonstrate%20that%20our%20proposed%0Amethod%20equals%20or%20even%20surpasses%20state-of-the-art%20results.%20The%20source%20code%20will%0Abe%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520is%2520Believing%253F%2520Enhancing%2520Vision-Language%2520Navigation%2520using%2520Visual%250A%2520%2520Perturbations%26entry.906535625%3DXuesong%2520Zhang%2520and%2520Jia%2520Li%2520and%2520Yunbo%2520Xu%2520and%2520Zhenzhen%2520Hu%2520and%2520Richang%2520Hong%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520for%2520an%2520embodied%2520agent%2520guided%2520by%2520natural%2520language%250Ainstructions%2520remains%2520a%2520formidable%2520challenge%2520in%2520vision-and-language%2520navigation%250A%2528VLN%2529.%2520Despite%2520remarkable%2520recent%2520progress%2520in%2520learning%2520fine-grained%2520and%250Amultifarious%2520visual%2520representations%252C%2520the%2520tendency%2520to%2520overfit%2520to%2520the%2520training%250Aenvironments%2520leads%2520to%2520unsatisfactory%2520generalization%2520performance.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520versatile%2520Multi-Branch%2520Architecture%2520%2528MBA%2529%2520aimed%2520at%2520exploring%2520and%250Aexploiting%2520diverse%2520visual%2520inputs.%2520Specifically%252C%2520we%2520introduce%2520three%2520distinct%250Avisual%2520variants%253A%2520ground-truth%2520depth%2520images%252C%2520visual%2520inputs%2520integrated%2520with%250Aincongruent%2520views%252C%2520and%2520those%2520infused%2520with%2520random%2520noise%2520to%2520enrich%2520the%2520diversity%250Aof%2520visual%2520input%2520representation%2520and%2520prevent%2520overfitting%2520to%2520the%2520original%2520RGB%250Aobservations.%2520To%2520adaptively%2520fuse%2520these%2520varied%2520inputs%252C%2520the%2520proposed%2520MBA%2520extend%2520a%250Abase%2520agent%2520model%2520into%2520a%2520multi-branch%2520variant%252C%2520where%2520each%2520branch%2520processes%2520a%250Adifferent%2520visual%2520input.%2520Surprisingly%252C%2520even%2520random%2520noise%2520can%2520further%2520enhance%250Anavigation%2520performance%2520in%2520unseen%2520environments.%2520Extensive%2520experiments%2520conducted%250Aon%2520three%2520VLN%2520benchmarks%2520%2528R2R%252C%2520REVERIE%252C%2520SOON%2529%2520demonstrate%2520that%2520our%2520proposed%250Amethod%2520equals%2520or%2520even%2520surpasses%2520state-of-the-art%2520results.%2520The%2520source%2520code%2520will%250Abe%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20is%20Believing%3F%20Enhancing%20Vision-Language%20Navigation%20using%20Visual%0A%20%20Perturbations&entry.906535625=Xuesong%20Zhang%20and%20Jia%20Li%20and%20Yunbo%20Xu%20and%20Zhenzhen%20Hu%20and%20Richang%20Hong&entry.1292438233=%20%20Autonomous%20navigation%20for%20an%20embodied%20agent%20guided%20by%20natural%20language%0Ainstructions%20remains%20a%20formidable%20challenge%20in%20vision-and-language%20navigation%0A%28VLN%29.%20Despite%20remarkable%20recent%20progress%20in%20learning%20fine-grained%20and%0Amultifarious%20visual%20representations%2C%20the%20tendency%20to%20overfit%20to%20the%20training%0Aenvironments%20leads%20to%20unsatisfactory%20generalization%20performance.%20In%20this%20work%2C%0Awe%20present%20a%20versatile%20Multi-Branch%20Architecture%20%28MBA%29%20aimed%20at%20exploring%20and%0Aexploiting%20diverse%20visual%20inputs.%20Specifically%2C%20we%20introduce%20three%20distinct%0Avisual%20variants%3A%20ground-truth%20depth%20images%2C%20visual%20inputs%20integrated%20with%0Aincongruent%20views%2C%20and%20those%20infused%20with%20random%20noise%20to%20enrich%20the%20diversity%0Aof%20visual%20input%20representation%20and%20prevent%20overfitting%20to%20the%20original%20RGB%0Aobservations.%20To%20adaptively%20fuse%20these%20varied%20inputs%2C%20the%20proposed%20MBA%20extend%20a%0Abase%20agent%20model%20into%20a%20multi-branch%20variant%2C%20where%20each%20branch%20processes%20a%0Adifferent%20visual%20input.%20Surprisingly%2C%20even%20random%20noise%20can%20further%20enhance%0Anavigation%20performance%20in%20unseen%20environments.%20Extensive%20experiments%20conducted%0Aon%20three%20VLN%20benchmarks%20%28R2R%2C%20REVERIE%2C%20SOON%29%20demonstrate%20that%20our%20proposed%0Amethod%20equals%20or%20even%20surpasses%20state-of-the-art%20results.%20The%20source%20code%20will%0Abe%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05552v1&entry.124074799=Read"},
{"title": "Improving Pretraining Data Using Perplexity Correlations", "author": "Tristan Thrush and Christopher Potts and Tatsunori Hashimoto", "abstract": "  Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier.\n", "link": "http://arxiv.org/abs/2409.05816v1", "date": "2024-09-09", "relevancy": 2.4736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Pretraining%20Data%20Using%20Perplexity%20Correlations&body=Title%3A%20Improving%20Pretraining%20Data%20Using%20Perplexity%20Correlations%0AAuthor%3A%20Tristan%20Thrush%20and%20Christopher%20Potts%20and%20Tatsunori%20Hashimoto%0AAbstract%3A%20%20%20Quality%20pretraining%20data%20is%20often%20seen%20as%20the%20key%20to%20high-performance%0Alanguage%20models.%20However%2C%20progress%20in%20understanding%20pretraining%20data%20has%20been%0Aslow%20due%20to%20the%20costly%20pretraining%20runs%20required%20for%20data%20selection%0Aexperiments.%20We%20present%20a%20framework%20that%20avoids%20these%20costs%20and%20selects%0Ahigh-quality%20pretraining%20data%20without%20any%20LLM%20training%20of%20our%20own.%20Our%20work%20is%0Abased%20on%20a%20simple%20observation%3A%20LLM%20losses%20on%20many%20pretraining%20texts%20are%0Acorrelated%20with%20downstream%20benchmark%20performance%2C%20and%20selecting%0Ahigh-correlation%20documents%20is%20an%20effective%20pretraining%20data%20selection%20method.%0AWe%20build%20a%20new%20statistical%20framework%20for%20data%20selection%20centered%20around%0Aestimates%20of%20perplexity-benchmark%20correlations%20and%20perform%20data%20selection%20using%0Aa%20sample%20of%2090%20LLMs%20taken%20from%20the%20Open%20LLM%20Leaderboard%20on%20texts%20from%20tens%20of%0Athousands%20of%20web%20domains.%20In%20controlled%20pretraining%20experiments%20at%20the%20160M%0Aparameter%20scale%20on%208%20benchmarks%2C%20our%20approach%20outperforms%20DSIR%20on%20every%0Abenchmark%2C%20while%20matching%20the%20best%20data%20selector%20found%20in%20DataComp-LM%2C%20a%0Ahand-engineered%20bigram%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Pretraining%2520Data%2520Using%2520Perplexity%2520Correlations%26entry.906535625%3DTristan%2520Thrush%2520and%2520Christopher%2520Potts%2520and%2520Tatsunori%2520Hashimoto%26entry.1292438233%3D%2520%2520Quality%2520pretraining%2520data%2520is%2520often%2520seen%2520as%2520the%2520key%2520to%2520high-performance%250Alanguage%2520models.%2520However%252C%2520progress%2520in%2520understanding%2520pretraining%2520data%2520has%2520been%250Aslow%2520due%2520to%2520the%2520costly%2520pretraining%2520runs%2520required%2520for%2520data%2520selection%250Aexperiments.%2520We%2520present%2520a%2520framework%2520that%2520avoids%2520these%2520costs%2520and%2520selects%250Ahigh-quality%2520pretraining%2520data%2520without%2520any%2520LLM%2520training%2520of%2520our%2520own.%2520Our%2520work%2520is%250Abased%2520on%2520a%2520simple%2520observation%253A%2520LLM%2520losses%2520on%2520many%2520pretraining%2520texts%2520are%250Acorrelated%2520with%2520downstream%2520benchmark%2520performance%252C%2520and%2520selecting%250Ahigh-correlation%2520documents%2520is%2520an%2520effective%2520pretraining%2520data%2520selection%2520method.%250AWe%2520build%2520a%2520new%2520statistical%2520framework%2520for%2520data%2520selection%2520centered%2520around%250Aestimates%2520of%2520perplexity-benchmark%2520correlations%2520and%2520perform%2520data%2520selection%2520using%250Aa%2520sample%2520of%252090%2520LLMs%2520taken%2520from%2520the%2520Open%2520LLM%2520Leaderboard%2520on%2520texts%2520from%2520tens%2520of%250Athousands%2520of%2520web%2520domains.%2520In%2520controlled%2520pretraining%2520experiments%2520at%2520the%2520160M%250Aparameter%2520scale%2520on%25208%2520benchmarks%252C%2520our%2520approach%2520outperforms%2520DSIR%2520on%2520every%250Abenchmark%252C%2520while%2520matching%2520the%2520best%2520data%2520selector%2520found%2520in%2520DataComp-LM%252C%2520a%250Ahand-engineered%2520bigram%2520classifier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Pretraining%20Data%20Using%20Perplexity%20Correlations&entry.906535625=Tristan%20Thrush%20and%20Christopher%20Potts%20and%20Tatsunori%20Hashimoto&entry.1292438233=%20%20Quality%20pretraining%20data%20is%20often%20seen%20as%20the%20key%20to%20high-performance%0Alanguage%20models.%20However%2C%20progress%20in%20understanding%20pretraining%20data%20has%20been%0Aslow%20due%20to%20the%20costly%20pretraining%20runs%20required%20for%20data%20selection%0Aexperiments.%20We%20present%20a%20framework%20that%20avoids%20these%20costs%20and%20selects%0Ahigh-quality%20pretraining%20data%20without%20any%20LLM%20training%20of%20our%20own.%20Our%20work%20is%0Abased%20on%20a%20simple%20observation%3A%20LLM%20losses%20on%20many%20pretraining%20texts%20are%0Acorrelated%20with%20downstream%20benchmark%20performance%2C%20and%20selecting%0Ahigh-correlation%20documents%20is%20an%20effective%20pretraining%20data%20selection%20method.%0AWe%20build%20a%20new%20statistical%20framework%20for%20data%20selection%20centered%20around%0Aestimates%20of%20perplexity-benchmark%20correlations%20and%20perform%20data%20selection%20using%0Aa%20sample%20of%2090%20LLMs%20taken%20from%20the%20Open%20LLM%20Leaderboard%20on%20texts%20from%20tens%20of%0Athousands%20of%20web%20domains.%20In%20controlled%20pretraining%20experiments%20at%20the%20160M%0Aparameter%20scale%20on%208%20benchmarks%2C%20our%20approach%20outperforms%20DSIR%20on%20every%0Abenchmark%2C%20while%20matching%20the%20best%20data%20selector%20found%20in%20DataComp-LM%2C%20a%0Ahand-engineered%20bigram%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05816v1&entry.124074799=Read"},
{"title": "Renormalized Connection for Scale-preferred Object Detection in\n  Satellite Imagery", "author": "Fan Zhang and Lingling Li and Licheng Jiao and Xu Liu and Fang Liu and Shuyuan Yang and Biao Hou", "abstract": "  Satellite imagery, due to its long-range imaging, brings with it a variety of\nscale-preferred tasks, such as the detection of tiny/small objects, making the\nprecise localization and detection of small objects of interest a challenging\ntask. In this article, we design a Knowledge Discovery Network (KDN) to\nimplement the renormalization group theory in terms of efficient feature\nextraction. Renormalized connection (RC) on the KDN enables ``synergistic\nfocusing'' of multi-scale features. Based on our observations of KDN, we\nabstract a class of RCs with different connection strengths, called n21C, and\ngeneralize it to FPN-based multi-branch detectors. In a series of FPN\nexperiments on the scale-preferred tasks, we found that the\n``divide-and-conquer'' idea of FPN severely hampers the detector's learning in\nthe right direction due to the large number of large-scale negative samples and\ninterference from background noise. Moreover, these negative samples cannot be\neliminated by the focal loss function. The RCs extends the multi-level\nfeature's ``divide-and-conquer'' mechanism of the FPN-based detectors to a wide\nrange of scale-preferred tasks, and enables synergistic effects of multi-level\nfeatures on the specific learning goal. In addition, interference activations\nin two aspects are greatly reduced and the detector learns in a more correct\ndirection. Extensive experiments of 17 well-designed detection architectures\nembedded with n21s on five different levels of scale-preferred tasks validate\nthe effectiveness and efficiency of the RCs. Especially the simplest linear\nform of RC, E421C performs well in all tasks and it satisfies the scaling\nproperty of RGT. We hope that our approach will transfer a large number of\nwell-designed detectors from the computer vision community to the remote\nsensing community.\n", "link": "http://arxiv.org/abs/2409.05624v1", "date": "2024-09-09", "relevancy": 2.4707, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5066}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Renormalized%20Connection%20for%20Scale-preferred%20Object%20Detection%20in%0A%20%20Satellite%20Imagery&body=Title%3A%20Renormalized%20Connection%20for%20Scale-preferred%20Object%20Detection%20in%0A%20%20Satellite%20Imagery%0AAuthor%3A%20Fan%20Zhang%20and%20Lingling%20Li%20and%20Licheng%20Jiao%20and%20Xu%20Liu%20and%20Fang%20Liu%20and%20Shuyuan%20Yang%20and%20Biao%20Hou%0AAbstract%3A%20%20%20Satellite%20imagery%2C%20due%20to%20its%20long-range%20imaging%2C%20brings%20with%20it%20a%20variety%20of%0Ascale-preferred%20tasks%2C%20such%20as%20the%20detection%20of%20tiny/small%20objects%2C%20making%20the%0Aprecise%20localization%20and%20detection%20of%20small%20objects%20of%20interest%20a%20challenging%0Atask.%20In%20this%20article%2C%20we%20design%20a%20Knowledge%20Discovery%20Network%20%28KDN%29%20to%0Aimplement%20the%20renormalization%20group%20theory%20in%20terms%20of%20efficient%20feature%0Aextraction.%20Renormalized%20connection%20%28RC%29%20on%20the%20KDN%20enables%20%60%60synergistic%0Afocusing%27%27%20of%20multi-scale%20features.%20Based%20on%20our%20observations%20of%20KDN%2C%20we%0Aabstract%20a%20class%20of%20RCs%20with%20different%20connection%20strengths%2C%20called%20n21C%2C%20and%0Ageneralize%20it%20to%20FPN-based%20multi-branch%20detectors.%20In%20a%20series%20of%20FPN%0Aexperiments%20on%20the%20scale-preferred%20tasks%2C%20we%20found%20that%20the%0A%60%60divide-and-conquer%27%27%20idea%20of%20FPN%20severely%20hampers%20the%20detector%27s%20learning%20in%0Athe%20right%20direction%20due%20to%20the%20large%20number%20of%20large-scale%20negative%20samples%20and%0Ainterference%20from%20background%20noise.%20Moreover%2C%20these%20negative%20samples%20cannot%20be%0Aeliminated%20by%20the%20focal%20loss%20function.%20The%20RCs%20extends%20the%20multi-level%0Afeature%27s%20%60%60divide-and-conquer%27%27%20mechanism%20of%20the%20FPN-based%20detectors%20to%20a%20wide%0Arange%20of%20scale-preferred%20tasks%2C%20and%20enables%20synergistic%20effects%20of%20multi-level%0Afeatures%20on%20the%20specific%20learning%20goal.%20In%20addition%2C%20interference%20activations%0Ain%20two%20aspects%20are%20greatly%20reduced%20and%20the%20detector%20learns%20in%20a%20more%20correct%0Adirection.%20Extensive%20experiments%20of%2017%20well-designed%20detection%20architectures%0Aembedded%20with%20n21s%20on%20five%20different%20levels%20of%20scale-preferred%20tasks%20validate%0Athe%20effectiveness%20and%20efficiency%20of%20the%20RCs.%20Especially%20the%20simplest%20linear%0Aform%20of%20RC%2C%20E421C%20performs%20well%20in%20all%20tasks%20and%20it%20satisfies%20the%20scaling%0Aproperty%20of%20RGT.%20We%20hope%20that%20our%20approach%20will%20transfer%20a%20large%20number%20of%0Awell-designed%20detectors%20from%20the%20computer%20vision%20community%20to%20the%20remote%0Asensing%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRenormalized%2520Connection%2520for%2520Scale-preferred%2520Object%2520Detection%2520in%250A%2520%2520Satellite%2520Imagery%26entry.906535625%3DFan%2520Zhang%2520and%2520Lingling%2520Li%2520and%2520Licheng%2520Jiao%2520and%2520Xu%2520Liu%2520and%2520Fang%2520Liu%2520and%2520Shuyuan%2520Yang%2520and%2520Biao%2520Hou%26entry.1292438233%3D%2520%2520Satellite%2520imagery%252C%2520due%2520to%2520its%2520long-range%2520imaging%252C%2520brings%2520with%2520it%2520a%2520variety%2520of%250Ascale-preferred%2520tasks%252C%2520such%2520as%2520the%2520detection%2520of%2520tiny/small%2520objects%252C%2520making%2520the%250Aprecise%2520localization%2520and%2520detection%2520of%2520small%2520objects%2520of%2520interest%2520a%2520challenging%250Atask.%2520In%2520this%2520article%252C%2520we%2520design%2520a%2520Knowledge%2520Discovery%2520Network%2520%2528KDN%2529%2520to%250Aimplement%2520the%2520renormalization%2520group%2520theory%2520in%2520terms%2520of%2520efficient%2520feature%250Aextraction.%2520Renormalized%2520connection%2520%2528RC%2529%2520on%2520the%2520KDN%2520enables%2520%2560%2560synergistic%250Afocusing%2527%2527%2520of%2520multi-scale%2520features.%2520Based%2520on%2520our%2520observations%2520of%2520KDN%252C%2520we%250Aabstract%2520a%2520class%2520of%2520RCs%2520with%2520different%2520connection%2520strengths%252C%2520called%2520n21C%252C%2520and%250Ageneralize%2520it%2520to%2520FPN-based%2520multi-branch%2520detectors.%2520In%2520a%2520series%2520of%2520FPN%250Aexperiments%2520on%2520the%2520scale-preferred%2520tasks%252C%2520we%2520found%2520that%2520the%250A%2560%2560divide-and-conquer%2527%2527%2520idea%2520of%2520FPN%2520severely%2520hampers%2520the%2520detector%2527s%2520learning%2520in%250Athe%2520right%2520direction%2520due%2520to%2520the%2520large%2520number%2520of%2520large-scale%2520negative%2520samples%2520and%250Ainterference%2520from%2520background%2520noise.%2520Moreover%252C%2520these%2520negative%2520samples%2520cannot%2520be%250Aeliminated%2520by%2520the%2520focal%2520loss%2520function.%2520The%2520RCs%2520extends%2520the%2520multi-level%250Afeature%2527s%2520%2560%2560divide-and-conquer%2527%2527%2520mechanism%2520of%2520the%2520FPN-based%2520detectors%2520to%2520a%2520wide%250Arange%2520of%2520scale-preferred%2520tasks%252C%2520and%2520enables%2520synergistic%2520effects%2520of%2520multi-level%250Afeatures%2520on%2520the%2520specific%2520learning%2520goal.%2520In%2520addition%252C%2520interference%2520activations%250Ain%2520two%2520aspects%2520are%2520greatly%2520reduced%2520and%2520the%2520detector%2520learns%2520in%2520a%2520more%2520correct%250Adirection.%2520Extensive%2520experiments%2520of%252017%2520well-designed%2520detection%2520architectures%250Aembedded%2520with%2520n21s%2520on%2520five%2520different%2520levels%2520of%2520scale-preferred%2520tasks%2520validate%250Athe%2520effectiveness%2520and%2520efficiency%2520of%2520the%2520RCs.%2520Especially%2520the%2520simplest%2520linear%250Aform%2520of%2520RC%252C%2520E421C%2520performs%2520well%2520in%2520all%2520tasks%2520and%2520it%2520satisfies%2520the%2520scaling%250Aproperty%2520of%2520RGT.%2520We%2520hope%2520that%2520our%2520approach%2520will%2520transfer%2520a%2520large%2520number%2520of%250Awell-designed%2520detectors%2520from%2520the%2520computer%2520vision%2520community%2520to%2520the%2520remote%250Asensing%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Renormalized%20Connection%20for%20Scale-preferred%20Object%20Detection%20in%0A%20%20Satellite%20Imagery&entry.906535625=Fan%20Zhang%20and%20Lingling%20Li%20and%20Licheng%20Jiao%20and%20Xu%20Liu%20and%20Fang%20Liu%20and%20Shuyuan%20Yang%20and%20Biao%20Hou&entry.1292438233=%20%20Satellite%20imagery%2C%20due%20to%20its%20long-range%20imaging%2C%20brings%20with%20it%20a%20variety%20of%0Ascale-preferred%20tasks%2C%20such%20as%20the%20detection%20of%20tiny/small%20objects%2C%20making%20the%0Aprecise%20localization%20and%20detection%20of%20small%20objects%20of%20interest%20a%20challenging%0Atask.%20In%20this%20article%2C%20we%20design%20a%20Knowledge%20Discovery%20Network%20%28KDN%29%20to%0Aimplement%20the%20renormalization%20group%20theory%20in%20terms%20of%20efficient%20feature%0Aextraction.%20Renormalized%20connection%20%28RC%29%20on%20the%20KDN%20enables%20%60%60synergistic%0Afocusing%27%27%20of%20multi-scale%20features.%20Based%20on%20our%20observations%20of%20KDN%2C%20we%0Aabstract%20a%20class%20of%20RCs%20with%20different%20connection%20strengths%2C%20called%20n21C%2C%20and%0Ageneralize%20it%20to%20FPN-based%20multi-branch%20detectors.%20In%20a%20series%20of%20FPN%0Aexperiments%20on%20the%20scale-preferred%20tasks%2C%20we%20found%20that%20the%0A%60%60divide-and-conquer%27%27%20idea%20of%20FPN%20severely%20hampers%20the%20detector%27s%20learning%20in%0Athe%20right%20direction%20due%20to%20the%20large%20number%20of%20large-scale%20negative%20samples%20and%0Ainterference%20from%20background%20noise.%20Moreover%2C%20these%20negative%20samples%20cannot%20be%0Aeliminated%20by%20the%20focal%20loss%20function.%20The%20RCs%20extends%20the%20multi-level%0Afeature%27s%20%60%60divide-and-conquer%27%27%20mechanism%20of%20the%20FPN-based%20detectors%20to%20a%20wide%0Arange%20of%20scale-preferred%20tasks%2C%20and%20enables%20synergistic%20effects%20of%20multi-level%0Afeatures%20on%20the%20specific%20learning%20goal.%20In%20addition%2C%20interference%20activations%0Ain%20two%20aspects%20are%20greatly%20reduced%20and%20the%20detector%20learns%20in%20a%20more%20correct%0Adirection.%20Extensive%20experiments%20of%2017%20well-designed%20detection%20architectures%0Aembedded%20with%20n21s%20on%20five%20different%20levels%20of%20scale-preferred%20tasks%20validate%0Athe%20effectiveness%20and%20efficiency%20of%20the%20RCs.%20Especially%20the%20simplest%20linear%0Aform%20of%20RC%2C%20E421C%20performs%20well%20in%20all%20tasks%20and%20it%20satisfies%20the%20scaling%0Aproperty%20of%20RGT.%20We%20hope%20that%20our%20approach%20will%20transfer%20a%20large%20number%20of%0Awell-designed%20detectors%20from%20the%20computer%20vision%20community%20to%20the%20remote%0Asensing%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05624v1&entry.124074799=Read"},
{"title": "DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects\n  in Unrestricted Environments", "author": "Chengzhong Ma and Houxue Yang and Hanbo Zhang and Zeyang Liu and Chao Zhao and Jian Tang and Xuguang Lan and Nanning Zheng", "abstract": "  Grasping large and flat objects (e.g. a book or a pan) is often regarded as\nan ungraspable task, which poses significant challenges due to the unreachable\ngrasping poses. Previous works leverage Extrinsic Dexterity like walls or table\nedges to grasp such objects. However, they are limited to task-specific\npolicies and lack task planning to find pre-grasp conditions. This makes it\ndifficult to adapt to various environments and extrinsic dexterity constraints.\nTherefore, we present DexDiff, a robust robotic manipulation method for\nlong-horizon planning with extrinsic dexterity. Specifically, we utilize a\nvision-language model (VLM) to perceive the environmental state and generate\nhigh-level task plans, followed by a goal-conditioned action diffusion (GCAD)\nmodel to predict the sequence of low-level actions. This model learns the\nlow-level policy from offline data with the cumulative reward guided by\nhigh-level planning as the goal condition, which allows for improved prediction\nof robot actions. Experimental results demonstrate that our method not only\neffectively performs ungraspable tasks but also generalizes to previously\nunseen objects. It outperforms baselines by a 47% higher success rate in\nsimulation and facilitates efficient deployment and manipulation in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2409.05493v1", "date": "2024-09-09", "relevancy": 2.4318, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6751}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6612}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexDiff%3A%20Towards%20Extrinsic%20Dexterity%20Manipulation%20of%20Ungraspable%20Objects%0A%20%20in%20Unrestricted%20Environments&body=Title%3A%20DexDiff%3A%20Towards%20Extrinsic%20Dexterity%20Manipulation%20of%20Ungraspable%20Objects%0A%20%20in%20Unrestricted%20Environments%0AAuthor%3A%20Chengzhong%20Ma%20and%20Houxue%20Yang%20and%20Hanbo%20Zhang%20and%20Zeyang%20Liu%20and%20Chao%20Zhao%20and%20Jian%20Tang%20and%20Xuguang%20Lan%20and%20Nanning%20Zheng%0AAbstract%3A%20%20%20Grasping%20large%20and%20flat%20objects%20%28e.g.%20a%20book%20or%20a%20pan%29%20is%20often%20regarded%20as%0Aan%20ungraspable%20task%2C%20which%20poses%20significant%20challenges%20due%20to%20the%20unreachable%0Agrasping%20poses.%20Previous%20works%20leverage%20Extrinsic%20Dexterity%20like%20walls%20or%20table%0Aedges%20to%20grasp%20such%20objects.%20However%2C%20they%20are%20limited%20to%20task-specific%0Apolicies%20and%20lack%20task%20planning%20to%20find%20pre-grasp%20conditions.%20This%20makes%20it%0Adifficult%20to%20adapt%20to%20various%20environments%20and%20extrinsic%20dexterity%20constraints.%0ATherefore%2C%20we%20present%20DexDiff%2C%20a%20robust%20robotic%20manipulation%20method%20for%0Along-horizon%20planning%20with%20extrinsic%20dexterity.%20Specifically%2C%20we%20utilize%20a%0Avision-language%20model%20%28VLM%29%20to%20perceive%20the%20environmental%20state%20and%20generate%0Ahigh-level%20task%20plans%2C%20followed%20by%20a%20goal-conditioned%20action%20diffusion%20%28GCAD%29%0Amodel%20to%20predict%20the%20sequence%20of%20low-level%20actions.%20This%20model%20learns%20the%0Alow-level%20policy%20from%20offline%20data%20with%20the%20cumulative%20reward%20guided%20by%0Ahigh-level%20planning%20as%20the%20goal%20condition%2C%20which%20allows%20for%20improved%20prediction%0Aof%20robot%20actions.%20Experimental%20results%20demonstrate%20that%20our%20method%20not%20only%0Aeffectively%20performs%20ungraspable%20tasks%20but%20also%20generalizes%20to%20previously%0Aunseen%20objects.%20It%20outperforms%20baselines%20by%20a%2047%25%20higher%20success%20rate%20in%0Asimulation%20and%20facilitates%20efficient%20deployment%20and%20manipulation%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexDiff%253A%2520Towards%2520Extrinsic%2520Dexterity%2520Manipulation%2520of%2520Ungraspable%2520Objects%250A%2520%2520in%2520Unrestricted%2520Environments%26entry.906535625%3DChengzhong%2520Ma%2520and%2520Houxue%2520Yang%2520and%2520Hanbo%2520Zhang%2520and%2520Zeyang%2520Liu%2520and%2520Chao%2520Zhao%2520and%2520Jian%2520Tang%2520and%2520Xuguang%2520Lan%2520and%2520Nanning%2520Zheng%26entry.1292438233%3D%2520%2520Grasping%2520large%2520and%2520flat%2520objects%2520%2528e.g.%2520a%2520book%2520or%2520a%2520pan%2529%2520is%2520often%2520regarded%2520as%250Aan%2520ungraspable%2520task%252C%2520which%2520poses%2520significant%2520challenges%2520due%2520to%2520the%2520unreachable%250Agrasping%2520poses.%2520Previous%2520works%2520leverage%2520Extrinsic%2520Dexterity%2520like%2520walls%2520or%2520table%250Aedges%2520to%2520grasp%2520such%2520objects.%2520However%252C%2520they%2520are%2520limited%2520to%2520task-specific%250Apolicies%2520and%2520lack%2520task%2520planning%2520to%2520find%2520pre-grasp%2520conditions.%2520This%2520makes%2520it%250Adifficult%2520to%2520adapt%2520to%2520various%2520environments%2520and%2520extrinsic%2520dexterity%2520constraints.%250ATherefore%252C%2520we%2520present%2520DexDiff%252C%2520a%2520robust%2520robotic%2520manipulation%2520method%2520for%250Along-horizon%2520planning%2520with%2520extrinsic%2520dexterity.%2520Specifically%252C%2520we%2520utilize%2520a%250Avision-language%2520model%2520%2528VLM%2529%2520to%2520perceive%2520the%2520environmental%2520state%2520and%2520generate%250Ahigh-level%2520task%2520plans%252C%2520followed%2520by%2520a%2520goal-conditioned%2520action%2520diffusion%2520%2528GCAD%2529%250Amodel%2520to%2520predict%2520the%2520sequence%2520of%2520low-level%2520actions.%2520This%2520model%2520learns%2520the%250Alow-level%2520policy%2520from%2520offline%2520data%2520with%2520the%2520cumulative%2520reward%2520guided%2520by%250Ahigh-level%2520planning%2520as%2520the%2520goal%2520condition%252C%2520which%2520allows%2520for%2520improved%2520prediction%250Aof%2520robot%2520actions.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520not%2520only%250Aeffectively%2520performs%2520ungraspable%2520tasks%2520but%2520also%2520generalizes%2520to%2520previously%250Aunseen%2520objects.%2520It%2520outperforms%2520baselines%2520by%2520a%252047%2525%2520higher%2520success%2520rate%2520in%250Asimulation%2520and%2520facilitates%2520efficient%2520deployment%2520and%2520manipulation%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexDiff%3A%20Towards%20Extrinsic%20Dexterity%20Manipulation%20of%20Ungraspable%20Objects%0A%20%20in%20Unrestricted%20Environments&entry.906535625=Chengzhong%20Ma%20and%20Houxue%20Yang%20and%20Hanbo%20Zhang%20and%20Zeyang%20Liu%20and%20Chao%20Zhao%20and%20Jian%20Tang%20and%20Xuguang%20Lan%20and%20Nanning%20Zheng&entry.1292438233=%20%20Grasping%20large%20and%20flat%20objects%20%28e.g.%20a%20book%20or%20a%20pan%29%20is%20often%20regarded%20as%0Aan%20ungraspable%20task%2C%20which%20poses%20significant%20challenges%20due%20to%20the%20unreachable%0Agrasping%20poses.%20Previous%20works%20leverage%20Extrinsic%20Dexterity%20like%20walls%20or%20table%0Aedges%20to%20grasp%20such%20objects.%20However%2C%20they%20are%20limited%20to%20task-specific%0Apolicies%20and%20lack%20task%20planning%20to%20find%20pre-grasp%20conditions.%20This%20makes%20it%0Adifficult%20to%20adapt%20to%20various%20environments%20and%20extrinsic%20dexterity%20constraints.%0ATherefore%2C%20we%20present%20DexDiff%2C%20a%20robust%20robotic%20manipulation%20method%20for%0Along-horizon%20planning%20with%20extrinsic%20dexterity.%20Specifically%2C%20we%20utilize%20a%0Avision-language%20model%20%28VLM%29%20to%20perceive%20the%20environmental%20state%20and%20generate%0Ahigh-level%20task%20plans%2C%20followed%20by%20a%20goal-conditioned%20action%20diffusion%20%28GCAD%29%0Amodel%20to%20predict%20the%20sequence%20of%20low-level%20actions.%20This%20model%20learns%20the%0Alow-level%20policy%20from%20offline%20data%20with%20the%20cumulative%20reward%20guided%20by%0Ahigh-level%20planning%20as%20the%20goal%20condition%2C%20which%20allows%20for%20improved%20prediction%0Aof%20robot%20actions.%20Experimental%20results%20demonstrate%20that%20our%20method%20not%20only%0Aeffectively%20performs%20ungraspable%20tasks%20but%20also%20generalizes%20to%20previously%0Aunseen%20objects.%20It%20outperforms%20baselines%20by%20a%2047%25%20higher%20success%20rate%20in%0Asimulation%20and%20facilitates%20efficient%20deployment%20and%20manipulation%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05493v1&entry.124074799=Read"},
{"title": "VFA: Vision Frequency Analysis of Foundation Models and Human", "author": "Mohammad-Javad Darvishi-Bayazi and Md Rifat Arefin and Jocelyn Faubert and Irina Rish", "abstract": "  Machine learning models often struggle with distribution shifts in real-world\nscenarios, whereas humans exhibit robust adaptation. Models that better align\nwith human perception may achieve higher out-of-distribution generalization. In\nthis study, we investigate how various characteristics of large-scale computer\nvision models influence their alignment with human capabilities and robustness.\nOur findings indicate that increasing model and data size and incorporating\nrich semantic information and multiple modalities enhance models' alignment\nwith human perception and their overall robustness. Our empirical analysis\ndemonstrates a strong correlation between out-of-distribution accuracy and\nhuman alignment.\n", "link": "http://arxiv.org/abs/2409.05817v1", "date": "2024-09-09", "relevancy": 2.4158, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6128}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VFA%3A%20Vision%20Frequency%20Analysis%20of%20Foundation%20Models%20and%20Human&body=Title%3A%20VFA%3A%20Vision%20Frequency%20Analysis%20of%20Foundation%20Models%20and%20Human%0AAuthor%3A%20Mohammad-Javad%20Darvishi-Bayazi%20and%20Md%20Rifat%20Arefin%20and%20Jocelyn%20Faubert%20and%20Irina%20Rish%0AAbstract%3A%20%20%20Machine%20learning%20models%20often%20struggle%20with%20distribution%20shifts%20in%20real-world%0Ascenarios%2C%20whereas%20humans%20exhibit%20robust%20adaptation.%20Models%20that%20better%20align%0Awith%20human%20perception%20may%20achieve%20higher%20out-of-distribution%20generalization.%20In%0Athis%20study%2C%20we%20investigate%20how%20various%20characteristics%20of%20large-scale%20computer%0Avision%20models%20influence%20their%20alignment%20with%20human%20capabilities%20and%20robustness.%0AOur%20findings%20indicate%20that%20increasing%20model%20and%20data%20size%20and%20incorporating%0Arich%20semantic%20information%20and%20multiple%20modalities%20enhance%20models%27%20alignment%0Awith%20human%20perception%20and%20their%20overall%20robustness.%20Our%20empirical%20analysis%0Ademonstrates%20a%20strong%20correlation%20between%20out-of-distribution%20accuracy%20and%0Ahuman%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVFA%253A%2520Vision%2520Frequency%2520Analysis%2520of%2520Foundation%2520Models%2520and%2520Human%26entry.906535625%3DMohammad-Javad%2520Darvishi-Bayazi%2520and%2520Md%2520Rifat%2520Arefin%2520and%2520Jocelyn%2520Faubert%2520and%2520Irina%2520Rish%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520often%2520struggle%2520with%2520distribution%2520shifts%2520in%2520real-world%250Ascenarios%252C%2520whereas%2520humans%2520exhibit%2520robust%2520adaptation.%2520Models%2520that%2520better%2520align%250Awith%2520human%2520perception%2520may%2520achieve%2520higher%2520out-of-distribution%2520generalization.%2520In%250Athis%2520study%252C%2520we%2520investigate%2520how%2520various%2520characteristics%2520of%2520large-scale%2520computer%250Avision%2520models%2520influence%2520their%2520alignment%2520with%2520human%2520capabilities%2520and%2520robustness.%250AOur%2520findings%2520indicate%2520that%2520increasing%2520model%2520and%2520data%2520size%2520and%2520incorporating%250Arich%2520semantic%2520information%2520and%2520multiple%2520modalities%2520enhance%2520models%2527%2520alignment%250Awith%2520human%2520perception%2520and%2520their%2520overall%2520robustness.%2520Our%2520empirical%2520analysis%250Ademonstrates%2520a%2520strong%2520correlation%2520between%2520out-of-distribution%2520accuracy%2520and%250Ahuman%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFA%3A%20Vision%20Frequency%20Analysis%20of%20Foundation%20Models%20and%20Human&entry.906535625=Mohammad-Javad%20Darvishi-Bayazi%20and%20Md%20Rifat%20Arefin%20and%20Jocelyn%20Faubert%20and%20Irina%20Rish&entry.1292438233=%20%20Machine%20learning%20models%20often%20struggle%20with%20distribution%20shifts%20in%20real-world%0Ascenarios%2C%20whereas%20humans%20exhibit%20robust%20adaptation.%20Models%20that%20better%20align%0Awith%20human%20perception%20may%20achieve%20higher%20out-of-distribution%20generalization.%20In%0Athis%20study%2C%20we%20investigate%20how%20various%20characteristics%20of%20large-scale%20computer%0Avision%20models%20influence%20their%20alignment%20with%20human%20capabilities%20and%20robustness.%0AOur%20findings%20indicate%20that%20increasing%20model%20and%20data%20size%20and%20incorporating%0Arich%20semantic%20information%20and%20multiple%20modalities%20enhance%20models%27%20alignment%0Awith%20human%20perception%20and%20their%20overall%20robustness.%20Our%20empirical%20analysis%0Ademonstrates%20a%20strong%20correlation%20between%20out-of-distribution%20accuracy%20and%0Ahuman%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05817v1&entry.124074799=Read"},
{"title": "SX-Stitch: An Efficient VMS-UNet Based Framework for Intraoperative\n  Scoliosis X-Ray Image Stitching", "author": "Yi Li and Heting Gao and Mingde He and Jinqian Liang and Jason Gu and Wei Liu", "abstract": "  In scoliosis surgery, the limited field of view of the C-arm X-ray machine\nrestricts the surgeons' holistic analysis of spinal structures .This paper\npresents an end-to-end efficient and robust intraoperative X-ray image\nstitching method for scoliosis surgery,named SX-Stitch. The method is divided\ninto two stages:segmentation and stitching. In the segmentation stage, We\npropose a medical image segmentation model named Vision Mamba of Spine-UNet\n(VMS-UNet), which utilizes the state space Mamba to capture long-distance\ncontextual information while maintaining linear computational complexity, and\nincorporates the SimAM attention mechanism, significantly improving the\nsegmentation performance.In the stitching stage, we simplify the alignment\nprocess between images to the minimization of a registration energy function.\nThe total energy function is then optimized to order unordered images, and a\nhybrid energy function is introduced to optimize the best seam, effectively\neliminating parallax artifacts. On the clinical dataset, Sx-Stitch demonstrates\nsuperiority over SOTA schemes both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2409.05681v1", "date": "2024-09-09", "relevancy": 2.412, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4934}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4874}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SX-Stitch%3A%20An%20Efficient%20VMS-UNet%20Based%20Framework%20for%20Intraoperative%0A%20%20Scoliosis%20X-Ray%20Image%20Stitching&body=Title%3A%20SX-Stitch%3A%20An%20Efficient%20VMS-UNet%20Based%20Framework%20for%20Intraoperative%0A%20%20Scoliosis%20X-Ray%20Image%20Stitching%0AAuthor%3A%20Yi%20Li%20and%20Heting%20Gao%20and%20Mingde%20He%20and%20Jinqian%20Liang%20and%20Jason%20Gu%20and%20Wei%20Liu%0AAbstract%3A%20%20%20In%20scoliosis%20surgery%2C%20the%20limited%20field%20of%20view%20of%20the%20C-arm%20X-ray%20machine%0Arestricts%20the%20surgeons%27%20holistic%20analysis%20of%20spinal%20structures%20.This%20paper%0Apresents%20an%20end-to-end%20efficient%20and%20robust%20intraoperative%20X-ray%20image%0Astitching%20method%20for%20scoliosis%20surgery%2Cnamed%20SX-Stitch.%20The%20method%20is%20divided%0Ainto%20two%20stages%3Asegmentation%20and%20stitching.%20In%20the%20segmentation%20stage%2C%20We%0Apropose%20a%20medical%20image%20segmentation%20model%20named%20Vision%20Mamba%20of%20Spine-UNet%0A%28VMS-UNet%29%2C%20which%20utilizes%20the%20state%20space%20Mamba%20to%20capture%20long-distance%0Acontextual%20information%20while%20maintaining%20linear%20computational%20complexity%2C%20and%0Aincorporates%20the%20SimAM%20attention%20mechanism%2C%20significantly%20improving%20the%0Asegmentation%20performance.In%20the%20stitching%20stage%2C%20we%20simplify%20the%20alignment%0Aprocess%20between%20images%20to%20the%20minimization%20of%20a%20registration%20energy%20function.%0AThe%20total%20energy%20function%20is%20then%20optimized%20to%20order%20unordered%20images%2C%20and%20a%0Ahybrid%20energy%20function%20is%20introduced%20to%20optimize%20the%20best%20seam%2C%20effectively%0Aeliminating%20parallax%20artifacts.%20On%20the%20clinical%20dataset%2C%20Sx-Stitch%20demonstrates%0Asuperiority%20over%20SOTA%20schemes%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSX-Stitch%253A%2520An%2520Efficient%2520VMS-UNet%2520Based%2520Framework%2520for%2520Intraoperative%250A%2520%2520Scoliosis%2520X-Ray%2520Image%2520Stitching%26entry.906535625%3DYi%2520Li%2520and%2520Heting%2520Gao%2520and%2520Mingde%2520He%2520and%2520Jinqian%2520Liang%2520and%2520Jason%2520Gu%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520In%2520scoliosis%2520surgery%252C%2520the%2520limited%2520field%2520of%2520view%2520of%2520the%2520C-arm%2520X-ray%2520machine%250Arestricts%2520the%2520surgeons%2527%2520holistic%2520analysis%2520of%2520spinal%2520structures%2520.This%2520paper%250Apresents%2520an%2520end-to-end%2520efficient%2520and%2520robust%2520intraoperative%2520X-ray%2520image%250Astitching%2520method%2520for%2520scoliosis%2520surgery%252Cnamed%2520SX-Stitch.%2520The%2520method%2520is%2520divided%250Ainto%2520two%2520stages%253Asegmentation%2520and%2520stitching.%2520In%2520the%2520segmentation%2520stage%252C%2520We%250Apropose%2520a%2520medical%2520image%2520segmentation%2520model%2520named%2520Vision%2520Mamba%2520of%2520Spine-UNet%250A%2528VMS-UNet%2529%252C%2520which%2520utilizes%2520the%2520state%2520space%2520Mamba%2520to%2520capture%2520long-distance%250Acontextual%2520information%2520while%2520maintaining%2520linear%2520computational%2520complexity%252C%2520and%250Aincorporates%2520the%2520SimAM%2520attention%2520mechanism%252C%2520significantly%2520improving%2520the%250Asegmentation%2520performance.In%2520the%2520stitching%2520stage%252C%2520we%2520simplify%2520the%2520alignment%250Aprocess%2520between%2520images%2520to%2520the%2520minimization%2520of%2520a%2520registration%2520energy%2520function.%250AThe%2520total%2520energy%2520function%2520is%2520then%2520optimized%2520to%2520order%2520unordered%2520images%252C%2520and%2520a%250Ahybrid%2520energy%2520function%2520is%2520introduced%2520to%2520optimize%2520the%2520best%2520seam%252C%2520effectively%250Aeliminating%2520parallax%2520artifacts.%2520On%2520the%2520clinical%2520dataset%252C%2520Sx-Stitch%2520demonstrates%250Asuperiority%2520over%2520SOTA%2520schemes%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SX-Stitch%3A%20An%20Efficient%20VMS-UNet%20Based%20Framework%20for%20Intraoperative%0A%20%20Scoliosis%20X-Ray%20Image%20Stitching&entry.906535625=Yi%20Li%20and%20Heting%20Gao%20and%20Mingde%20He%20and%20Jinqian%20Liang%20and%20Jason%20Gu%20and%20Wei%20Liu&entry.1292438233=%20%20In%20scoliosis%20surgery%2C%20the%20limited%20field%20of%20view%20of%20the%20C-arm%20X-ray%20machine%0Arestricts%20the%20surgeons%27%20holistic%20analysis%20of%20spinal%20structures%20.This%20paper%0Apresents%20an%20end-to-end%20efficient%20and%20robust%20intraoperative%20X-ray%20image%0Astitching%20method%20for%20scoliosis%20surgery%2Cnamed%20SX-Stitch.%20The%20method%20is%20divided%0Ainto%20two%20stages%3Asegmentation%20and%20stitching.%20In%20the%20segmentation%20stage%2C%20We%0Apropose%20a%20medical%20image%20segmentation%20model%20named%20Vision%20Mamba%20of%20Spine-UNet%0A%28VMS-UNet%29%2C%20which%20utilizes%20the%20state%20space%20Mamba%20to%20capture%20long-distance%0Acontextual%20information%20while%20maintaining%20linear%20computational%20complexity%2C%20and%0Aincorporates%20the%20SimAM%20attention%20mechanism%2C%20significantly%20improving%20the%0Asegmentation%20performance.In%20the%20stitching%20stage%2C%20we%20simplify%20the%20alignment%0Aprocess%20between%20images%20to%20the%20minimization%20of%20a%20registration%20energy%20function.%0AThe%20total%20energy%20function%20is%20then%20optimized%20to%20order%20unordered%20images%2C%20and%20a%0Ahybrid%20energy%20function%20is%20introduced%20to%20optimize%20the%20best%20seam%2C%20effectively%0Aeliminating%20parallax%20artifacts.%20On%20the%20clinical%20dataset%2C%20Sx-Stitch%20demonstrates%0Asuperiority%20over%20SOTA%20schemes%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05681v1&entry.124074799=Read"},
{"title": "PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category\n  Discovery", "author": "Fernando Julio Cendra and Bingchen Zhao and Kai Han", "abstract": "  We tackle the problem of Continual Category Discovery (CCD), which aims to\nautomatically discover novel categories in a continuous stream of unlabeled\ndata while mitigating the challenge of catastrophic forgetting -- an open\nproblem that persists even in conventional, fully supervised continual\nlearning. To address this challenge, we propose PromptCCD, a simple yet\neffective framework that utilizes a Gaussian Mixture Model (GMM) as a prompting\nmethod for CCD. At the core of PromptCCD lies the Gaussian Mixture Prompting\n(GMP) module, which acts as a dynamic pool that updates over time to facilitate\nrepresentation learning and prevent forgetting during category discovery.\nMoreover, GMP enables on-the-fly estimation of category numbers, allowing\nPromptCCD to discover categories in unlabeled data without prior knowledge of\nthe category numbers. We extend the standard evaluation metric for Generalized\nCategory Discovery (GCD) to CCD and benchmark state-of-the-art methods on\ndiverse public datasets. PromptCCD significantly outperforms existing methods,\ndemonstrating its effectiveness. Project page:\nhttps://visual-ai.github.io/promptccd .\n", "link": "http://arxiv.org/abs/2407.19001v2", "date": "2024-09-09", "relevancy": 2.3923, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4868}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.476}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromptCCD%3A%20Learning%20Gaussian%20Mixture%20Prompt%20Pool%20for%20Continual%20Category%0A%20%20Discovery&body=Title%3A%20PromptCCD%3A%20Learning%20Gaussian%20Mixture%20Prompt%20Pool%20for%20Continual%20Category%0A%20%20Discovery%0AAuthor%3A%20Fernando%20Julio%20Cendra%20and%20Bingchen%20Zhao%20and%20Kai%20Han%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20Continual%20Category%20Discovery%20%28CCD%29%2C%20which%20aims%20to%0Aautomatically%20discover%20novel%20categories%20in%20a%20continuous%20stream%20of%20unlabeled%0Adata%20while%20mitigating%20the%20challenge%20of%20catastrophic%20forgetting%20--%20an%20open%0Aproblem%20that%20persists%20even%20in%20conventional%2C%20fully%20supervised%20continual%0Alearning.%20To%20address%20this%20challenge%2C%20we%20propose%20PromptCCD%2C%20a%20simple%20yet%0Aeffective%20framework%20that%20utilizes%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20as%20a%20prompting%0Amethod%20for%20CCD.%20At%20the%20core%20of%20PromptCCD%20lies%20the%20Gaussian%20Mixture%20Prompting%0A%28GMP%29%20module%2C%20which%20acts%20as%20a%20dynamic%20pool%20that%20updates%20over%20time%20to%20facilitate%0Arepresentation%20learning%20and%20prevent%20forgetting%20during%20category%20discovery.%0AMoreover%2C%20GMP%20enables%20on-the-fly%20estimation%20of%20category%20numbers%2C%20allowing%0APromptCCD%20to%20discover%20categories%20in%20unlabeled%20data%20without%20prior%20knowledge%20of%0Athe%20category%20numbers.%20We%20extend%20the%20standard%20evaluation%20metric%20for%20Generalized%0ACategory%20Discovery%20%28GCD%29%20to%20CCD%20and%20benchmark%20state-of-the-art%20methods%20on%0Adiverse%20public%20datasets.%20PromptCCD%20significantly%20outperforms%20existing%20methods%2C%0Ademonstrating%20its%20effectiveness.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/promptccd%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptCCD%253A%2520Learning%2520Gaussian%2520Mixture%2520Prompt%2520Pool%2520for%2520Continual%2520Category%250A%2520%2520Discovery%26entry.906535625%3DFernando%2520Julio%2520Cendra%2520and%2520Bingchen%2520Zhao%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520Continual%2520Category%2520Discovery%2520%2528CCD%2529%252C%2520which%2520aims%2520to%250Aautomatically%2520discover%2520novel%2520categories%2520in%2520a%2520continuous%2520stream%2520of%2520unlabeled%250Adata%2520while%2520mitigating%2520the%2520challenge%2520of%2520catastrophic%2520forgetting%2520--%2520an%2520open%250Aproblem%2520that%2520persists%2520even%2520in%2520conventional%252C%2520fully%2520supervised%2520continual%250Alearning.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520PromptCCD%252C%2520a%2520simple%2520yet%250Aeffective%2520framework%2520that%2520utilizes%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520as%2520a%2520prompting%250Amethod%2520for%2520CCD.%2520At%2520the%2520core%2520of%2520PromptCCD%2520lies%2520the%2520Gaussian%2520Mixture%2520Prompting%250A%2528GMP%2529%2520module%252C%2520which%2520acts%2520as%2520a%2520dynamic%2520pool%2520that%2520updates%2520over%2520time%2520to%2520facilitate%250Arepresentation%2520learning%2520and%2520prevent%2520forgetting%2520during%2520category%2520discovery.%250AMoreover%252C%2520GMP%2520enables%2520on-the-fly%2520estimation%2520of%2520category%2520numbers%252C%2520allowing%250APromptCCD%2520to%2520discover%2520categories%2520in%2520unlabeled%2520data%2520without%2520prior%2520knowledge%2520of%250Athe%2520category%2520numbers.%2520We%2520extend%2520the%2520standard%2520evaluation%2520metric%2520for%2520Generalized%250ACategory%2520Discovery%2520%2528GCD%2529%2520to%2520CCD%2520and%2520benchmark%2520state-of-the-art%2520methods%2520on%250Adiverse%2520public%2520datasets.%2520PromptCCD%2520significantly%2520outperforms%2520existing%2520methods%252C%250Ademonstrating%2520its%2520effectiveness.%2520Project%2520page%253A%250Ahttps%253A//visual-ai.github.io/promptccd%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptCCD%3A%20Learning%20Gaussian%20Mixture%20Prompt%20Pool%20for%20Continual%20Category%0A%20%20Discovery&entry.906535625=Fernando%20Julio%20Cendra%20and%20Bingchen%20Zhao%20and%20Kai%20Han&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20Continual%20Category%20Discovery%20%28CCD%29%2C%20which%20aims%20to%0Aautomatically%20discover%20novel%20categories%20in%20a%20continuous%20stream%20of%20unlabeled%0Adata%20while%20mitigating%20the%20challenge%20of%20catastrophic%20forgetting%20--%20an%20open%0Aproblem%20that%20persists%20even%20in%20conventional%2C%20fully%20supervised%20continual%0Alearning.%20To%20address%20this%20challenge%2C%20we%20propose%20PromptCCD%2C%20a%20simple%20yet%0Aeffective%20framework%20that%20utilizes%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20as%20a%20prompting%0Amethod%20for%20CCD.%20At%20the%20core%20of%20PromptCCD%20lies%20the%20Gaussian%20Mixture%20Prompting%0A%28GMP%29%20module%2C%20which%20acts%20as%20a%20dynamic%20pool%20that%20updates%20over%20time%20to%20facilitate%0Arepresentation%20learning%20and%20prevent%20forgetting%20during%20category%20discovery.%0AMoreover%2C%20GMP%20enables%20on-the-fly%20estimation%20of%20category%20numbers%2C%20allowing%0APromptCCD%20to%20discover%20categories%20in%20unlabeled%20data%20without%20prior%20knowledge%20of%0Athe%20category%20numbers.%20We%20extend%20the%20standard%20evaluation%20metric%20for%20Generalized%0ACategory%20Discovery%20%28GCD%29%20to%20CCD%20and%20benchmark%20state-of-the-art%20methods%20on%0Adiverse%20public%20datasets.%20PromptCCD%20significantly%20outperforms%20existing%20methods%2C%0Ademonstrating%20its%20effectiveness.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/promptccd%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19001v2&entry.124074799=Read"},
{"title": "LLMs Will Always Hallucinate, and We Need to Live With This", "author": "Sourav Banerjee and Ayushi Agarwal and Saloni Singla", "abstract": "  As Large Language Models become more ubiquitous across domains, it becomes\nimportant to examine their inherent limitations critically. This work argues\nthat hallucinations in language models are not just occasional errors but an\ninevitable feature of these systems. We demonstrate that hallucinations stem\nfrom the fundamental mathematical and logical structure of LLMs. It is,\ntherefore, impossible to eliminate them through architectural improvements,\ndataset enhancements, or fact-checking mechanisms. Our analysis draws on\ncomputational theory and Godel's First Incompleteness Theorem, which references\nthe undecidability of problems like the Halting, Emptiness, and Acceptance\nProblems. We demonstrate that every stage of the LLM process-from training data\ncompilation to fact retrieval, intent classification, and text generation-will\nhave a non-zero probability of producing hallucinations. This work introduces\nthe concept of Structural Hallucination as an intrinsic nature of these\nsystems. By establishing the mathematical certainty of hallucinations, we\nchallenge the prevailing notion that they can be fully mitigated.\n", "link": "http://arxiv.org/abs/2409.05746v1", "date": "2024-09-09", "relevancy": 2.3622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Will%20Always%20Hallucinate%2C%20and%20We%20Need%20to%20Live%20With%20This&body=Title%3A%20LLMs%20Will%20Always%20Hallucinate%2C%20and%20We%20Need%20to%20Live%20With%20This%0AAuthor%3A%20Sourav%20Banerjee%20and%20Ayushi%20Agarwal%20and%20Saloni%20Singla%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20become%20more%20ubiquitous%20across%20domains%2C%20it%20becomes%0Aimportant%20to%20examine%20their%20inherent%20limitations%20critically.%20This%20work%20argues%0Athat%20hallucinations%20in%20language%20models%20are%20not%20just%20occasional%20errors%20but%20an%0Ainevitable%20feature%20of%20these%20systems.%20We%20demonstrate%20that%20hallucinations%20stem%0Afrom%20the%20fundamental%20mathematical%20and%20logical%20structure%20of%20LLMs.%20It%20is%2C%0Atherefore%2C%20impossible%20to%20eliminate%20them%20through%20architectural%20improvements%2C%0Adataset%20enhancements%2C%20or%20fact-checking%20mechanisms.%20Our%20analysis%20draws%20on%0Acomputational%20theory%20and%20Godel%27s%20First%20Incompleteness%20Theorem%2C%20which%20references%0Athe%20undecidability%20of%20problems%20like%20the%20Halting%2C%20Emptiness%2C%20and%20Acceptance%0AProblems.%20We%20demonstrate%20that%20every%20stage%20of%20the%20LLM%20process-from%20training%20data%0Acompilation%20to%20fact%20retrieval%2C%20intent%20classification%2C%20and%20text%20generation-will%0Ahave%20a%20non-zero%20probability%20of%20producing%20hallucinations.%20This%20work%20introduces%0Athe%20concept%20of%20Structural%20Hallucination%20as%20an%20intrinsic%20nature%20of%20these%0Asystems.%20By%20establishing%20the%20mathematical%20certainty%20of%20hallucinations%2C%20we%0Achallenge%20the%20prevailing%20notion%20that%20they%20can%20be%20fully%20mitigated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Will%2520Always%2520Hallucinate%252C%2520and%2520We%2520Need%2520to%2520Live%2520With%2520This%26entry.906535625%3DSourav%2520Banerjee%2520and%2520Ayushi%2520Agarwal%2520and%2520Saloni%2520Singla%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520become%2520more%2520ubiquitous%2520across%2520domains%252C%2520it%2520becomes%250Aimportant%2520to%2520examine%2520their%2520inherent%2520limitations%2520critically.%2520This%2520work%2520argues%250Athat%2520hallucinations%2520in%2520language%2520models%2520are%2520not%2520just%2520occasional%2520errors%2520but%2520an%250Ainevitable%2520feature%2520of%2520these%2520systems.%2520We%2520demonstrate%2520that%2520hallucinations%2520stem%250Afrom%2520the%2520fundamental%2520mathematical%2520and%2520logical%2520structure%2520of%2520LLMs.%2520It%2520is%252C%250Atherefore%252C%2520impossible%2520to%2520eliminate%2520them%2520through%2520architectural%2520improvements%252C%250Adataset%2520enhancements%252C%2520or%2520fact-checking%2520mechanisms.%2520Our%2520analysis%2520draws%2520on%250Acomputational%2520theory%2520and%2520Godel%2527s%2520First%2520Incompleteness%2520Theorem%252C%2520which%2520references%250Athe%2520undecidability%2520of%2520problems%2520like%2520the%2520Halting%252C%2520Emptiness%252C%2520and%2520Acceptance%250AProblems.%2520We%2520demonstrate%2520that%2520every%2520stage%2520of%2520the%2520LLM%2520process-from%2520training%2520data%250Acompilation%2520to%2520fact%2520retrieval%252C%2520intent%2520classification%252C%2520and%2520text%2520generation-will%250Ahave%2520a%2520non-zero%2520probability%2520of%2520producing%2520hallucinations.%2520This%2520work%2520introduces%250Athe%2520concept%2520of%2520Structural%2520Hallucination%2520as%2520an%2520intrinsic%2520nature%2520of%2520these%250Asystems.%2520By%2520establishing%2520the%2520mathematical%2520certainty%2520of%2520hallucinations%252C%2520we%250Achallenge%2520the%2520prevailing%2520notion%2520that%2520they%2520can%2520be%2520fully%2520mitigated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Will%20Always%20Hallucinate%2C%20and%20We%20Need%20to%20Live%20With%20This&entry.906535625=Sourav%20Banerjee%20and%20Ayushi%20Agarwal%20and%20Saloni%20Singla&entry.1292438233=%20%20As%20Large%20Language%20Models%20become%20more%20ubiquitous%20across%20domains%2C%20it%20becomes%0Aimportant%20to%20examine%20their%20inherent%20limitations%20critically.%20This%20work%20argues%0Athat%20hallucinations%20in%20language%20models%20are%20not%20just%20occasional%20errors%20but%20an%0Ainevitable%20feature%20of%20these%20systems.%20We%20demonstrate%20that%20hallucinations%20stem%0Afrom%20the%20fundamental%20mathematical%20and%20logical%20structure%20of%20LLMs.%20It%20is%2C%0Atherefore%2C%20impossible%20to%20eliminate%20them%20through%20architectural%20improvements%2C%0Adataset%20enhancements%2C%20or%20fact-checking%20mechanisms.%20Our%20analysis%20draws%20on%0Acomputational%20theory%20and%20Godel%27s%20First%20Incompleteness%20Theorem%2C%20which%20references%0Athe%20undecidability%20of%20problems%20like%20the%20Halting%2C%20Emptiness%2C%20and%20Acceptance%0AProblems.%20We%20demonstrate%20that%20every%20stage%20of%20the%20LLM%20process-from%20training%20data%0Acompilation%20to%20fact%20retrieval%2C%20intent%20classification%2C%20and%20text%20generation-will%0Ahave%20a%20non-zero%20probability%20of%20producing%20hallucinations.%20This%20work%20introduces%0Athe%20concept%20of%20Structural%20Hallucination%20as%20an%20intrinsic%20nature%20of%20these%0Asystems.%20By%20establishing%20the%20mathematical%20certainty%20of%20hallucinations%2C%20we%0Achallenge%20the%20prevailing%20notion%20that%20they%20can%20be%20fully%20mitigated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05746v1&entry.124074799=Read"},
{"title": "Exploiting the Vulnerability of Large Language Models via Defense-Aware\n  Architectural Backdoor", "author": "Abdullah Arafat Miah and Yu Bi", "abstract": "  Deep neural networks (DNNs) have long been recognized as vulnerable to\nbackdoor attacks. By providing poisoned training data in the fine-tuning\nprocess, the attacker can implant a backdoor into the victim model. This\nenables input samples meeting specific textual trigger patterns to be\nclassified as target labels of the attacker's choice. While such black-box\nattacks have been well explored in both computer vision and natural language\nprocessing (NLP), backdoor attacks relying on white-box attack philosophy have\nhardly been thoroughly investigated. In this paper, we take the first step to\nintroduce a new type of backdoor attack that conceals itself within the\nunderlying model architecture. Specifically, we propose to design separate\nbackdoor modules consisting of two functions: trigger detection and noise\ninjection. The add-on modules of model architecture layers can detect the\npresence of input trigger tokens and modify layer weights using Gaussian noise\nto disturb the feature distribution of the baseline model. We conduct extensive\nexperiments to evaluate our attack methods using two model architecture\nsettings on five different large language datasets. We demonstrate that the\ntraining-free architectural backdoor on a large language model poses a genuine\nthreat. Unlike the-state-of-art work, it can survive the rigorous fine-tuning\nand retraining process, as well as evade output probability-based defense\nmethods (i.e. BDDR). All the code and data is available\nhttps://github.com/SiSL-URI/Arch_Backdoor_LLM.\n", "link": "http://arxiv.org/abs/2409.01952v2", "date": "2024-09-09", "relevancy": 2.3618, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.477}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4705}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20Vulnerability%20of%20Large%20Language%20Models%20via%20Defense-Aware%0A%20%20Architectural%20Backdoor&body=Title%3A%20Exploiting%20the%20Vulnerability%20of%20Large%20Language%20Models%20via%20Defense-Aware%0A%20%20Architectural%20Backdoor%0AAuthor%3A%20Abdullah%20Arafat%20Miah%20and%20Yu%20Bi%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20long%20been%20recognized%20as%20vulnerable%20to%0Abackdoor%20attacks.%20By%20providing%20poisoned%20training%20data%20in%20the%20fine-tuning%0Aprocess%2C%20the%20attacker%20can%20implant%20a%20backdoor%20into%20the%20victim%20model.%20This%0Aenables%20input%20samples%20meeting%20specific%20textual%20trigger%20patterns%20to%20be%0Aclassified%20as%20target%20labels%20of%20the%20attacker%27s%20choice.%20While%20such%20black-box%0Aattacks%20have%20been%20well%20explored%20in%20both%20computer%20vision%20and%20natural%20language%0Aprocessing%20%28NLP%29%2C%20backdoor%20attacks%20relying%20on%20white-box%20attack%20philosophy%20have%0Ahardly%20been%20thoroughly%20investigated.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20to%0Aintroduce%20a%20new%20type%20of%20backdoor%20attack%20that%20conceals%20itself%20within%20the%0Aunderlying%20model%20architecture.%20Specifically%2C%20we%20propose%20to%20design%20separate%0Abackdoor%20modules%20consisting%20of%20two%20functions%3A%20trigger%20detection%20and%20noise%0Ainjection.%20The%20add-on%20modules%20of%20model%20architecture%20layers%20can%20detect%20the%0Apresence%20of%20input%20trigger%20tokens%20and%20modify%20layer%20weights%20using%20Gaussian%20noise%0Ato%20disturb%20the%20feature%20distribution%20of%20the%20baseline%20model.%20We%20conduct%20extensive%0Aexperiments%20to%20evaluate%20our%20attack%20methods%20using%20two%20model%20architecture%0Asettings%20on%20five%20different%20large%20language%20datasets.%20We%20demonstrate%20that%20the%0Atraining-free%20architectural%20backdoor%20on%20a%20large%20language%20model%20poses%20a%20genuine%0Athreat.%20Unlike%20the-state-of-art%20work%2C%20it%20can%20survive%20the%20rigorous%20fine-tuning%0Aand%20retraining%20process%2C%20as%20well%20as%20evade%20output%20probability-based%20defense%0Amethods%20%28i.e.%20BDDR%29.%20All%20the%20code%20and%20data%20is%20available%0Ahttps%3A//github.com/SiSL-URI/Arch_Backdoor_LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520Vulnerability%2520of%2520Large%2520Language%2520Models%2520via%2520Defense-Aware%250A%2520%2520Architectural%2520Backdoor%26entry.906535625%3DAbdullah%2520Arafat%2520Miah%2520and%2520Yu%2520Bi%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520long%2520been%2520recognized%2520as%2520vulnerable%2520to%250Abackdoor%2520attacks.%2520By%2520providing%2520poisoned%2520training%2520data%2520in%2520the%2520fine-tuning%250Aprocess%252C%2520the%2520attacker%2520can%2520implant%2520a%2520backdoor%2520into%2520the%2520victim%2520model.%2520This%250Aenables%2520input%2520samples%2520meeting%2520specific%2520textual%2520trigger%2520patterns%2520to%2520be%250Aclassified%2520as%2520target%2520labels%2520of%2520the%2520attacker%2527s%2520choice.%2520While%2520such%2520black-box%250Aattacks%2520have%2520been%2520well%2520explored%2520in%2520both%2520computer%2520vision%2520and%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%252C%2520backdoor%2520attacks%2520relying%2520on%2520white-box%2520attack%2520philosophy%2520have%250Ahardly%2520been%2520thoroughly%2520investigated.%2520In%2520this%2520paper%252C%2520we%2520take%2520the%2520first%2520step%2520to%250Aintroduce%2520a%2520new%2520type%2520of%2520backdoor%2520attack%2520that%2520conceals%2520itself%2520within%2520the%250Aunderlying%2520model%2520architecture.%2520Specifically%252C%2520we%2520propose%2520to%2520design%2520separate%250Abackdoor%2520modules%2520consisting%2520of%2520two%2520functions%253A%2520trigger%2520detection%2520and%2520noise%250Ainjection.%2520The%2520add-on%2520modules%2520of%2520model%2520architecture%2520layers%2520can%2520detect%2520the%250Apresence%2520of%2520input%2520trigger%2520tokens%2520and%2520modify%2520layer%2520weights%2520using%2520Gaussian%2520noise%250Ato%2520disturb%2520the%2520feature%2520distribution%2520of%2520the%2520baseline%2520model.%2520We%2520conduct%2520extensive%250Aexperiments%2520to%2520evaluate%2520our%2520attack%2520methods%2520using%2520two%2520model%2520architecture%250Asettings%2520on%2520five%2520different%2520large%2520language%2520datasets.%2520We%2520demonstrate%2520that%2520the%250Atraining-free%2520architectural%2520backdoor%2520on%2520a%2520large%2520language%2520model%2520poses%2520a%2520genuine%250Athreat.%2520Unlike%2520the-state-of-art%2520work%252C%2520it%2520can%2520survive%2520the%2520rigorous%2520fine-tuning%250Aand%2520retraining%2520process%252C%2520as%2520well%2520as%2520evade%2520output%2520probability-based%2520defense%250Amethods%2520%2528i.e.%2520BDDR%2529.%2520All%2520the%2520code%2520and%2520data%2520is%2520available%250Ahttps%253A//github.com/SiSL-URI/Arch_Backdoor_LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20Vulnerability%20of%20Large%20Language%20Models%20via%20Defense-Aware%0A%20%20Architectural%20Backdoor&entry.906535625=Abdullah%20Arafat%20Miah%20and%20Yu%20Bi&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20long%20been%20recognized%20as%20vulnerable%20to%0Abackdoor%20attacks.%20By%20providing%20poisoned%20training%20data%20in%20the%20fine-tuning%0Aprocess%2C%20the%20attacker%20can%20implant%20a%20backdoor%20into%20the%20victim%20model.%20This%0Aenables%20input%20samples%20meeting%20specific%20textual%20trigger%20patterns%20to%20be%0Aclassified%20as%20target%20labels%20of%20the%20attacker%27s%20choice.%20While%20such%20black-box%0Aattacks%20have%20been%20well%20explored%20in%20both%20computer%20vision%20and%20natural%20language%0Aprocessing%20%28NLP%29%2C%20backdoor%20attacks%20relying%20on%20white-box%20attack%20philosophy%20have%0Ahardly%20been%20thoroughly%20investigated.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20to%0Aintroduce%20a%20new%20type%20of%20backdoor%20attack%20that%20conceals%20itself%20within%20the%0Aunderlying%20model%20architecture.%20Specifically%2C%20we%20propose%20to%20design%20separate%0Abackdoor%20modules%20consisting%20of%20two%20functions%3A%20trigger%20detection%20and%20noise%0Ainjection.%20The%20add-on%20modules%20of%20model%20architecture%20layers%20can%20detect%20the%0Apresence%20of%20input%20trigger%20tokens%20and%20modify%20layer%20weights%20using%20Gaussian%20noise%0Ato%20disturb%20the%20feature%20distribution%20of%20the%20baseline%20model.%20We%20conduct%20extensive%0Aexperiments%20to%20evaluate%20our%20attack%20methods%20using%20two%20model%20architecture%0Asettings%20on%20five%20different%20large%20language%20datasets.%20We%20demonstrate%20that%20the%0Atraining-free%20architectural%20backdoor%20on%20a%20large%20language%20model%20poses%20a%20genuine%0Athreat.%20Unlike%20the-state-of-art%20work%2C%20it%20can%20survive%20the%20rigorous%20fine-tuning%0Aand%20retraining%20process%2C%20as%20well%20as%20evade%20output%20probability-based%20defense%0Amethods%20%28i.e.%20BDDR%29.%20All%20the%20code%20and%20data%20is%20available%0Ahttps%3A//github.com/SiSL-URI/Arch_Backdoor_LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01952v2&entry.124074799=Read"},
{"title": "SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior", "author": "Fan Yang and Tao Wang", "abstract": "  The use of deep learning methods to automatically detect students' classroom\nbehavior is a promising approach for analyzing their class performance and\nimproving teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose the Student Classroom Behavior dataset\n(SCB-dataset3), which represents real-life scenarios. Our dataset comprises\n5686 images with 45578 labels, focusing on six behaviors: hand-raising,\nreading, writing, using a phone, bowing the head, and leaning over the table.\nWe evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms,\nachieving a mean average precision (map) of up to 80.3$\\%$. We believe that our\ndataset can serve as a robust foundation for future research in student\nbehavior detection and contribute to advancements in this field. Our\nSCB-dataset3 is available for download at:\nhttps://github.com/Whiffe/SCB-dataset\n", "link": "http://arxiv.org/abs/2310.02522v2", "date": "2024-09-09", "relevancy": 2.3419, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4752}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4691}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCB-Dataset3%3A%20A%20Benchmark%20for%20Detecting%20Student%20Classroom%20Behavior&body=Title%3A%20SCB-Dataset3%3A%20A%20Benchmark%20for%20Detecting%20Student%20Classroom%20Behavior%0AAuthor%3A%20Fan%20Yang%20and%20Tao%20Wang%0AAbstract%3A%20%20%20The%20use%20of%20deep%20learning%20methods%20to%20automatically%20detect%20students%27%20classroom%0Abehavior%20is%20a%20promising%20approach%20for%20analyzing%20their%20class%20performance%20and%0Aimproving%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Adatasets%20on%20student%20behavior%20poses%20a%20challenge%20for%20researchers%20in%20this%20field.%0ATo%20address%20this%20issue%2C%20we%20propose%20the%20Student%20Classroom%20Behavior%20dataset%0A%28SCB-dataset3%29%2C%20which%20represents%20real-life%20scenarios.%20Our%20dataset%20comprises%0A5686%20images%20with%2045578%20labels%2C%20focusing%20on%20six%20behaviors%3A%20hand-raising%2C%0Areading%2C%20writing%2C%20using%20a%20phone%2C%20bowing%20the%20head%2C%20and%20leaning%20over%20the%20table.%0AWe%20evaluated%20the%20dataset%20using%20the%20YOLOv5%2C%20YOLOv7%2C%20and%20YOLOv8%20algorithms%2C%0Aachieving%20a%20mean%20average%20precision%20%28map%29%20of%20up%20to%2080.3%24%5C%25%24.%20We%20believe%20that%20our%0Adataset%20can%20serve%20as%20a%20robust%20foundation%20for%20future%20research%20in%20student%0Abehavior%20detection%20and%20contribute%20to%20advancements%20in%20this%20field.%20Our%0ASCB-dataset3%20is%20available%20for%20download%20at%3A%0Ahttps%3A//github.com/Whiffe/SCB-dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCB-Dataset3%253A%2520A%2520Benchmark%2520for%2520Detecting%2520Student%2520Classroom%2520Behavior%26entry.906535625%3DFan%2520Yang%2520and%2520Tao%2520Wang%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520deep%2520learning%2520methods%2520to%2520automatically%2520detect%2520students%2527%2520classroom%250Abehavior%2520is%2520a%2520promising%2520approach%2520for%2520analyzing%2520their%2520class%2520performance%2520and%250Aimproving%2520teaching%2520effectiveness.%2520However%252C%2520the%2520lack%2520of%2520publicly%2520available%250Adatasets%2520on%2520student%2520behavior%2520poses%2520a%2520challenge%2520for%2520researchers%2520in%2520this%2520field.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520Student%2520Classroom%2520Behavior%2520dataset%250A%2528SCB-dataset3%2529%252C%2520which%2520represents%2520real-life%2520scenarios.%2520Our%2520dataset%2520comprises%250A5686%2520images%2520with%252045578%2520labels%252C%2520focusing%2520on%2520six%2520behaviors%253A%2520hand-raising%252C%250Areading%252C%2520writing%252C%2520using%2520a%2520phone%252C%2520bowing%2520the%2520head%252C%2520and%2520leaning%2520over%2520the%2520table.%250AWe%2520evaluated%2520the%2520dataset%2520using%2520the%2520YOLOv5%252C%2520YOLOv7%252C%2520and%2520YOLOv8%2520algorithms%252C%250Aachieving%2520a%2520mean%2520average%2520precision%2520%2528map%2529%2520of%2520up%2520to%252080.3%2524%255C%2525%2524.%2520We%2520believe%2520that%2520our%250Adataset%2520can%2520serve%2520as%2520a%2520robust%2520foundation%2520for%2520future%2520research%2520in%2520student%250Abehavior%2520detection%2520and%2520contribute%2520to%2520advancements%2520in%2520this%2520field.%2520Our%250ASCB-dataset3%2520is%2520available%2520for%2520download%2520at%253A%250Ahttps%253A//github.com/Whiffe/SCB-dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCB-Dataset3%3A%20A%20Benchmark%20for%20Detecting%20Student%20Classroom%20Behavior&entry.906535625=Fan%20Yang%20and%20Tao%20Wang&entry.1292438233=%20%20The%20use%20of%20deep%20learning%20methods%20to%20automatically%20detect%20students%27%20classroom%0Abehavior%20is%20a%20promising%20approach%20for%20analyzing%20their%20class%20performance%20and%0Aimproving%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Adatasets%20on%20student%20behavior%20poses%20a%20challenge%20for%20researchers%20in%20this%20field.%0ATo%20address%20this%20issue%2C%20we%20propose%20the%20Student%20Classroom%20Behavior%20dataset%0A%28SCB-dataset3%29%2C%20which%20represents%20real-life%20scenarios.%20Our%20dataset%20comprises%0A5686%20images%20with%2045578%20labels%2C%20focusing%20on%20six%20behaviors%3A%20hand-raising%2C%0Areading%2C%20writing%2C%20using%20a%20phone%2C%20bowing%20the%20head%2C%20and%20leaning%20over%20the%20table.%0AWe%20evaluated%20the%20dataset%20using%20the%20YOLOv5%2C%20YOLOv7%2C%20and%20YOLOv8%20algorithms%2C%0Aachieving%20a%20mean%20average%20precision%20%28map%29%20of%20up%20to%2080.3%24%5C%25%24.%20We%20believe%20that%20our%0Adataset%20can%20serve%20as%20a%20robust%20foundation%20for%20future%20research%20in%20student%0Abehavior%20detection%20and%20contribute%20to%20advancements%20in%20this%20field.%20Our%0ASCB-dataset3%20is%20available%20for%20download%20at%3A%0Ahttps%3A//github.com/Whiffe/SCB-dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02522v2&entry.124074799=Read"},
{"title": "Active Collaborative Visual SLAM exploiting ORB Features", "author": "Muhammad Farhan Ahmed and Vincent Fr\u00e9mont and Isabelle Fantoni", "abstract": "  In autonomous robotics, a significant challenge involves devising robust\nsolutions for Active Collaborative SLAM (AC-SLAM). This process requires\nmultiple robots to cooperatively explore and map an unknown environment by\nintelligently coordinating their movements and sensor data acquisition. In this\narticle, we present an efficient visual AC-SLAM method using aerial and ground\nrobots for environment exploration and mapping. We propose an efficient\nfrontiers filtering method that takes into account the common IoU map frontiers\nand reduces the frontiers for each robot. Additionally, we also present an\napproach to guide robots to previously visited goal positions to promote loop\nclosure to reduce SLAM uncertainty. The proposed method is implemented in ROS\nand evaluated through simulations on publicly available datasets and similar\nmethods, achieving an accumulative average of 59% of increase in area coverage.\n", "link": "http://arxiv.org/abs/2407.05453v2", "date": "2024-09-09", "relevancy": 2.3415, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6077}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Collaborative%20Visual%20SLAM%20exploiting%20ORB%20Features&body=Title%3A%20Active%20Collaborative%20Visual%20SLAM%20exploiting%20ORB%20Features%0AAuthor%3A%20Muhammad%20Farhan%20Ahmed%20and%20Vincent%20Fr%C3%A9mont%20and%20Isabelle%20Fantoni%0AAbstract%3A%20%20%20In%20autonomous%20robotics%2C%20a%20significant%20challenge%20involves%20devising%20robust%0Asolutions%20for%20Active%20Collaborative%20SLAM%20%28AC-SLAM%29.%20This%20process%20requires%0Amultiple%20robots%20to%20cooperatively%20explore%20and%20map%20an%20unknown%20environment%20by%0Aintelligently%20coordinating%20their%20movements%20and%20sensor%20data%20acquisition.%20In%20this%0Aarticle%2C%20we%20present%20an%20efficient%20visual%20AC-SLAM%20method%20using%20aerial%20and%20ground%0Arobots%20for%20environment%20exploration%20and%20mapping.%20We%20propose%20an%20efficient%0Afrontiers%20filtering%20method%20that%20takes%20into%20account%20the%20common%20IoU%20map%20frontiers%0Aand%20reduces%20the%20frontiers%20for%20each%20robot.%20Additionally%2C%20we%20also%20present%20an%0Aapproach%20to%20guide%20robots%20to%20previously%20visited%20goal%20positions%20to%20promote%20loop%0Aclosure%20to%20reduce%20SLAM%20uncertainty.%20The%20proposed%20method%20is%20implemented%20in%20ROS%0Aand%20evaluated%20through%20simulations%20on%20publicly%20available%20datasets%20and%20similar%0Amethods%2C%20achieving%20an%20accumulative%20average%20of%2059%25%20of%20increase%20in%20area%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05453v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Collaborative%2520Visual%2520SLAM%2520exploiting%2520ORB%2520Features%26entry.906535625%3DMuhammad%2520Farhan%2520Ahmed%2520and%2520Vincent%2520Fr%25C3%25A9mont%2520and%2520Isabelle%2520Fantoni%26entry.1292438233%3D%2520%2520In%2520autonomous%2520robotics%252C%2520a%2520significant%2520challenge%2520involves%2520devising%2520robust%250Asolutions%2520for%2520Active%2520Collaborative%2520SLAM%2520%2528AC-SLAM%2529.%2520This%2520process%2520requires%250Amultiple%2520robots%2520to%2520cooperatively%2520explore%2520and%2520map%2520an%2520unknown%2520environment%2520by%250Aintelligently%2520coordinating%2520their%2520movements%2520and%2520sensor%2520data%2520acquisition.%2520In%2520this%250Aarticle%252C%2520we%2520present%2520an%2520efficient%2520visual%2520AC-SLAM%2520method%2520using%2520aerial%2520and%2520ground%250Arobots%2520for%2520environment%2520exploration%2520and%2520mapping.%2520We%2520propose%2520an%2520efficient%250Afrontiers%2520filtering%2520method%2520that%2520takes%2520into%2520account%2520the%2520common%2520IoU%2520map%2520frontiers%250Aand%2520reduces%2520the%2520frontiers%2520for%2520each%2520robot.%2520Additionally%252C%2520we%2520also%2520present%2520an%250Aapproach%2520to%2520guide%2520robots%2520to%2520previously%2520visited%2520goal%2520positions%2520to%2520promote%2520loop%250Aclosure%2520to%2520reduce%2520SLAM%2520uncertainty.%2520The%2520proposed%2520method%2520is%2520implemented%2520in%2520ROS%250Aand%2520evaluated%2520through%2520simulations%2520on%2520publicly%2520available%2520datasets%2520and%2520similar%250Amethods%252C%2520achieving%2520an%2520accumulative%2520average%2520of%252059%2525%2520of%2520increase%2520in%2520area%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05453v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Collaborative%20Visual%20SLAM%20exploiting%20ORB%20Features&entry.906535625=Muhammad%20Farhan%20Ahmed%20and%20Vincent%20Fr%C3%A9mont%20and%20Isabelle%20Fantoni&entry.1292438233=%20%20In%20autonomous%20robotics%2C%20a%20significant%20challenge%20involves%20devising%20robust%0Asolutions%20for%20Active%20Collaborative%20SLAM%20%28AC-SLAM%29.%20This%20process%20requires%0Amultiple%20robots%20to%20cooperatively%20explore%20and%20map%20an%20unknown%20environment%20by%0Aintelligently%20coordinating%20their%20movements%20and%20sensor%20data%20acquisition.%20In%20this%0Aarticle%2C%20we%20present%20an%20efficient%20visual%20AC-SLAM%20method%20using%20aerial%20and%20ground%0Arobots%20for%20environment%20exploration%20and%20mapping.%20We%20propose%20an%20efficient%0Afrontiers%20filtering%20method%20that%20takes%20into%20account%20the%20common%20IoU%20map%20frontiers%0Aand%20reduces%20the%20frontiers%20for%20each%20robot.%20Additionally%2C%20we%20also%20present%20an%0Aapproach%20to%20guide%20robots%20to%20previously%20visited%20goal%20positions%20to%20promote%20loop%0Aclosure%20to%20reduce%20SLAM%20uncertainty.%20The%20proposed%20method%20is%20implemented%20in%20ROS%0Aand%20evaluated%20through%20simulations%20on%20publicly%20available%20datasets%20and%20similar%0Amethods%2C%20achieving%20an%20accumulative%20average%20of%2059%25%20of%20increase%20in%20area%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05453v2&entry.124074799=Read"},
{"title": "StratXplore: Strategic Novelty-seeking and Instruction-aligned\n  Exploration for Vision and Language Navigation", "author": "Muraleekrishna Gopinathan and Jumana Abu-Khalaf and David Suter and Martin Masek", "abstract": "  Embodied navigation requires robots to understand and interact with the\nenvironment based on given tasks. Vision-Language Navigation (VLN) is an\nembodied navigation task, where a robot navigates within a previously seen and\nunseen environment, based on linguistic instruction and visual inputs. VLN\nagents need access to both local and global action spaces; former for immediate\ndecision making and the latter for recovering from navigational mistakes. Prior\nVLN agents rely only on instruction-viewpoint alignment for local and global\ndecision making and back-track to a previously visited viewpoint, if the\ninstruction and its current viewpoint mismatches. These methods are prone to\nmistakes, due to the complexity of the instruction and partial observability of\nthe environment. We posit that, back-tracking is sub-optimal and agent that is\naware of its mistakes can recover efficiently. For optimal recovery,\nexploration should be extended to unexplored viewpoints (or frontiers). The\noptimal frontier is a recently observed but unexplored viewpoint that aligns\nwith the instruction and is novel. We introduce a memory-based and\nmistake-aware path planning strategy for VLN agents, called\n\\textit{StratXplore}, that presents global and local action planning to select\nthe optimal frontier for path correction. The proposed method collects all past\nactions and viewpoint features during navigation and then selects the optimal\nfrontier suitable for recovery. Experimental results show this simple yet\neffective strategy improves the success rate on two VLN datasets with different\ntask complexities.\n", "link": "http://arxiv.org/abs/2409.05593v1", "date": "2024-09-09", "relevancy": 2.3343, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StratXplore%3A%20Strategic%20Novelty-seeking%20and%20Instruction-aligned%0A%20%20Exploration%20for%20Vision%20and%20Language%20Navigation&body=Title%3A%20StratXplore%3A%20Strategic%20Novelty-seeking%20and%20Instruction-aligned%0A%20%20Exploration%20for%20Vision%20and%20Language%20Navigation%0AAuthor%3A%20Muraleekrishna%20Gopinathan%20and%20Jumana%20Abu-Khalaf%20and%20David%20Suter%20and%20Martin%20Masek%0AAbstract%3A%20%20%20Embodied%20navigation%20requires%20robots%20to%20understand%20and%20interact%20with%20the%0Aenvironment%20based%20on%20given%20tasks.%20Vision-Language%20Navigation%20%28VLN%29%20is%20an%0Aembodied%20navigation%20task%2C%20where%20a%20robot%20navigates%20within%20a%20previously%20seen%20and%0Aunseen%20environment%2C%20based%20on%20linguistic%20instruction%20and%20visual%20inputs.%20VLN%0Aagents%20need%20access%20to%20both%20local%20and%20global%20action%20spaces%3B%20former%20for%20immediate%0Adecision%20making%20and%20the%20latter%20for%20recovering%20from%20navigational%20mistakes.%20Prior%0AVLN%20agents%20rely%20only%20on%20instruction-viewpoint%20alignment%20for%20local%20and%20global%0Adecision%20making%20and%20back-track%20to%20a%20previously%20visited%20viewpoint%2C%20if%20the%0Ainstruction%20and%20its%20current%20viewpoint%20mismatches.%20These%20methods%20are%20prone%20to%0Amistakes%2C%20due%20to%20the%20complexity%20of%20the%20instruction%20and%20partial%20observability%20of%0Athe%20environment.%20We%20posit%20that%2C%20back-tracking%20is%20sub-optimal%20and%20agent%20that%20is%0Aaware%20of%20its%20mistakes%20can%20recover%20efficiently.%20For%20optimal%20recovery%2C%0Aexploration%20should%20be%20extended%20to%20unexplored%20viewpoints%20%28or%20frontiers%29.%20The%0Aoptimal%20frontier%20is%20a%20recently%20observed%20but%20unexplored%20viewpoint%20that%20aligns%0Awith%20the%20instruction%20and%20is%20novel.%20We%20introduce%20a%20memory-based%20and%0Amistake-aware%20path%20planning%20strategy%20for%20VLN%20agents%2C%20called%0A%5Ctextit%7BStratXplore%7D%2C%20that%20presents%20global%20and%20local%20action%20planning%20to%20select%0Athe%20optimal%20frontier%20for%20path%20correction.%20The%20proposed%20method%20collects%20all%20past%0Aactions%20and%20viewpoint%20features%20during%20navigation%20and%20then%20selects%20the%20optimal%0Afrontier%20suitable%20for%20recovery.%20Experimental%20results%20show%20this%20simple%20yet%0Aeffective%20strategy%20improves%20the%20success%20rate%20on%20two%20VLN%20datasets%20with%20different%0Atask%20complexities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStratXplore%253A%2520Strategic%2520Novelty-seeking%2520and%2520Instruction-aligned%250A%2520%2520Exploration%2520for%2520Vision%2520and%2520Language%2520Navigation%26entry.906535625%3DMuraleekrishna%2520Gopinathan%2520and%2520Jumana%2520Abu-Khalaf%2520and%2520David%2520Suter%2520and%2520Martin%2520Masek%26entry.1292438233%3D%2520%2520Embodied%2520navigation%2520requires%2520robots%2520to%2520understand%2520and%2520interact%2520with%2520the%250Aenvironment%2520based%2520on%2520given%2520tasks.%2520Vision-Language%2520Navigation%2520%2528VLN%2529%2520is%2520an%250Aembodied%2520navigation%2520task%252C%2520where%2520a%2520robot%2520navigates%2520within%2520a%2520previously%2520seen%2520and%250Aunseen%2520environment%252C%2520based%2520on%2520linguistic%2520instruction%2520and%2520visual%2520inputs.%2520VLN%250Aagents%2520need%2520access%2520to%2520both%2520local%2520and%2520global%2520action%2520spaces%253B%2520former%2520for%2520immediate%250Adecision%2520making%2520and%2520the%2520latter%2520for%2520recovering%2520from%2520navigational%2520mistakes.%2520Prior%250AVLN%2520agents%2520rely%2520only%2520on%2520instruction-viewpoint%2520alignment%2520for%2520local%2520and%2520global%250Adecision%2520making%2520and%2520back-track%2520to%2520a%2520previously%2520visited%2520viewpoint%252C%2520if%2520the%250Ainstruction%2520and%2520its%2520current%2520viewpoint%2520mismatches.%2520These%2520methods%2520are%2520prone%2520to%250Amistakes%252C%2520due%2520to%2520the%2520complexity%2520of%2520the%2520instruction%2520and%2520partial%2520observability%2520of%250Athe%2520environment.%2520We%2520posit%2520that%252C%2520back-tracking%2520is%2520sub-optimal%2520and%2520agent%2520that%2520is%250Aaware%2520of%2520its%2520mistakes%2520can%2520recover%2520efficiently.%2520For%2520optimal%2520recovery%252C%250Aexploration%2520should%2520be%2520extended%2520to%2520unexplored%2520viewpoints%2520%2528or%2520frontiers%2529.%2520The%250Aoptimal%2520frontier%2520is%2520a%2520recently%2520observed%2520but%2520unexplored%2520viewpoint%2520that%2520aligns%250Awith%2520the%2520instruction%2520and%2520is%2520novel.%2520We%2520introduce%2520a%2520memory-based%2520and%250Amistake-aware%2520path%2520planning%2520strategy%2520for%2520VLN%2520agents%252C%2520called%250A%255Ctextit%257BStratXplore%257D%252C%2520that%2520presents%2520global%2520and%2520local%2520action%2520planning%2520to%2520select%250Athe%2520optimal%2520frontier%2520for%2520path%2520correction.%2520The%2520proposed%2520method%2520collects%2520all%2520past%250Aactions%2520and%2520viewpoint%2520features%2520during%2520navigation%2520and%2520then%2520selects%2520the%2520optimal%250Afrontier%2520suitable%2520for%2520recovery.%2520Experimental%2520results%2520show%2520this%2520simple%2520yet%250Aeffective%2520strategy%2520improves%2520the%2520success%2520rate%2520on%2520two%2520VLN%2520datasets%2520with%2520different%250Atask%2520complexities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StratXplore%3A%20Strategic%20Novelty-seeking%20and%20Instruction-aligned%0A%20%20Exploration%20for%20Vision%20and%20Language%20Navigation&entry.906535625=Muraleekrishna%20Gopinathan%20and%20Jumana%20Abu-Khalaf%20and%20David%20Suter%20and%20Martin%20Masek&entry.1292438233=%20%20Embodied%20navigation%20requires%20robots%20to%20understand%20and%20interact%20with%20the%0Aenvironment%20based%20on%20given%20tasks.%20Vision-Language%20Navigation%20%28VLN%29%20is%20an%0Aembodied%20navigation%20task%2C%20where%20a%20robot%20navigates%20within%20a%20previously%20seen%20and%0Aunseen%20environment%2C%20based%20on%20linguistic%20instruction%20and%20visual%20inputs.%20VLN%0Aagents%20need%20access%20to%20both%20local%20and%20global%20action%20spaces%3B%20former%20for%20immediate%0Adecision%20making%20and%20the%20latter%20for%20recovering%20from%20navigational%20mistakes.%20Prior%0AVLN%20agents%20rely%20only%20on%20instruction-viewpoint%20alignment%20for%20local%20and%20global%0Adecision%20making%20and%20back-track%20to%20a%20previously%20visited%20viewpoint%2C%20if%20the%0Ainstruction%20and%20its%20current%20viewpoint%20mismatches.%20These%20methods%20are%20prone%20to%0Amistakes%2C%20due%20to%20the%20complexity%20of%20the%20instruction%20and%20partial%20observability%20of%0Athe%20environment.%20We%20posit%20that%2C%20back-tracking%20is%20sub-optimal%20and%20agent%20that%20is%0Aaware%20of%20its%20mistakes%20can%20recover%20efficiently.%20For%20optimal%20recovery%2C%0Aexploration%20should%20be%20extended%20to%20unexplored%20viewpoints%20%28or%20frontiers%29.%20The%0Aoptimal%20frontier%20is%20a%20recently%20observed%20but%20unexplored%20viewpoint%20that%20aligns%0Awith%20the%20instruction%20and%20is%20novel.%20We%20introduce%20a%20memory-based%20and%0Amistake-aware%20path%20planning%20strategy%20for%20VLN%20agents%2C%20called%0A%5Ctextit%7BStratXplore%7D%2C%20that%20presents%20global%20and%20local%20action%20planning%20to%20select%0Athe%20optimal%20frontier%20for%20path%20correction.%20The%20proposed%20method%20collects%20all%20past%0Aactions%20and%20viewpoint%20features%20during%20navigation%20and%20then%20selects%20the%20optimal%0Afrontier%20suitable%20for%20recovery.%20Experimental%20results%20show%20this%20simple%20yet%0Aeffective%20strategy%20improves%20the%20success%20rate%20on%20two%20VLN%20datasets%20with%20different%0Atask%20complexities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05593v1&entry.124074799=Read"},
{"title": "HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale\n  Space Using Wearable IMUs and LiDAR", "author": "Yudi Dai and Zhiyong Wang and Xiping Lin and Chenglu Wen and Lan Xu and Siqi Shen and Yuexin Ma and Cheng Wang", "abstract": "  We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture\nmethod, aimed at accurately and efficiently creating a dynamic digital world,\ncontaining large-scale indoor-outdoor scenes, diverse human motions, rich\nhuman-human interactions, and human-environment interactions. By utilizing\nbody-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human\nmotions in unconstrained space without the need for external devices and\npre-built maps. This affords great flexibility and accessibility for\nhuman-centered interaction and 4D scene capturing in various environments.\nTaking into account that IMUs can capture human spatially unrestricted poses\nbut are prone to drifting for long-period using, and while LiDAR is stable for\nglobal localization but rough for local positions and orientations, HiSC4D\nemploys a joint optimization method, harmonizing all sensors and utilizing\nenvironment cues, yielding promising results for long-term capture in large\nscenes. To promote research of egocentric human interaction in large scenes and\nfacilitate downstream tasks, we also present a dataset, containing 8 sequences\nin 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D\nhuman motions with SMPL annotations and dynamic scenes, 31k frames of cropped\nhuman point clouds, and scene mesh of the environment. A variety of scenarios,\nsuch as the basketball gym and commercial street, alongside challenging human\nmotions, such as daily greeting, one-on-one basketball playing, and tour\nguiding, demonstrate the effectiveness and the generalization ability of\nHiSC4D. The dataset and code will be publicated on\nwww.lidarhumanmotion.net/hisc4d available for research purposes.\n", "link": "http://arxiv.org/abs/2409.04398v2", "date": "2024-09-09", "relevancy": 2.3258, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5843}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5843}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiSC4D%3A%20Human-centered%20interaction%20and%204D%20Scene%20Capture%20in%20Large-scale%0A%20%20Space%20Using%20Wearable%20IMUs%20and%20LiDAR&body=Title%3A%20HiSC4D%3A%20Human-centered%20interaction%20and%204D%20Scene%20Capture%20in%20Large-scale%0A%20%20Space%20Using%20Wearable%20IMUs%20and%20LiDAR%0AAuthor%3A%20Yudi%20Dai%20and%20Zhiyong%20Wang%20and%20Xiping%20Lin%20and%20Chenglu%20Wen%20and%20Lan%20Xu%20and%20Siqi%20Shen%20and%20Yuexin%20Ma%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20We%20introduce%20HiSC4D%2C%20a%20novel%20Human-centered%20interaction%20and%204D%20Scene%20Capture%0Amethod%2C%20aimed%20at%20accurately%20and%20efficiently%20creating%20a%20dynamic%20digital%20world%2C%0Acontaining%20large-scale%20indoor-outdoor%20scenes%2C%20diverse%20human%20motions%2C%20rich%0Ahuman-human%20interactions%2C%20and%20human-environment%20interactions.%20By%20utilizing%0Abody-mounted%20IMUs%20and%20a%20head-mounted%20LiDAR%2C%20HiSC4D%20can%20capture%20egocentric%20human%0Amotions%20in%20unconstrained%20space%20without%20the%20need%20for%20external%20devices%20and%0Apre-built%20maps.%20This%20affords%20great%20flexibility%20and%20accessibility%20for%0Ahuman-centered%20interaction%20and%204D%20scene%20capturing%20in%20various%20environments.%0ATaking%20into%20account%20that%20IMUs%20can%20capture%20human%20spatially%20unrestricted%20poses%0Abut%20are%20prone%20to%20drifting%20for%20long-period%20using%2C%20and%20while%20LiDAR%20is%20stable%20for%0Aglobal%20localization%20but%20rough%20for%20local%20positions%20and%20orientations%2C%20HiSC4D%0Aemploys%20a%20joint%20optimization%20method%2C%20harmonizing%20all%20sensors%20and%20utilizing%0Aenvironment%20cues%2C%20yielding%20promising%20results%20for%20long-term%20capture%20in%20large%0Ascenes.%20To%20promote%20research%20of%20egocentric%20human%20interaction%20in%20large%20scenes%20and%0Afacilitate%20downstream%20tasks%2C%20we%20also%20present%20a%20dataset%2C%20containing%208%20sequences%0Ain%204%20large%20scenes%20%28200%20to%205%2C000%20%24m%5E2%24%29%2C%20providing%2036k%20frames%20of%20accurate%204D%0Ahuman%20motions%20with%20SMPL%20annotations%20and%20dynamic%20scenes%2C%2031k%20frames%20of%20cropped%0Ahuman%20point%20clouds%2C%20and%20scene%20mesh%20of%20the%20environment.%20A%20variety%20of%20scenarios%2C%0Asuch%20as%20the%20basketball%20gym%20and%20commercial%20street%2C%20alongside%20challenging%20human%0Amotions%2C%20such%20as%20daily%20greeting%2C%20one-on-one%20basketball%20playing%2C%20and%20tour%0Aguiding%2C%20demonstrate%20the%20effectiveness%20and%20the%20generalization%20ability%20of%0AHiSC4D.%20The%20dataset%20and%20code%20will%20be%20publicated%20on%0Awww.lidarhumanmotion.net/hisc4d%20available%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiSC4D%253A%2520Human-centered%2520interaction%2520and%25204D%2520Scene%2520Capture%2520in%2520Large-scale%250A%2520%2520Space%2520Using%2520Wearable%2520IMUs%2520and%2520LiDAR%26entry.906535625%3DYudi%2520Dai%2520and%2520Zhiyong%2520Wang%2520and%2520Xiping%2520Lin%2520and%2520Chenglu%2520Wen%2520and%2520Lan%2520Xu%2520and%2520Siqi%2520Shen%2520and%2520Yuexin%2520Ma%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520HiSC4D%252C%2520a%2520novel%2520Human-centered%2520interaction%2520and%25204D%2520Scene%2520Capture%250Amethod%252C%2520aimed%2520at%2520accurately%2520and%2520efficiently%2520creating%2520a%2520dynamic%2520digital%2520world%252C%250Acontaining%2520large-scale%2520indoor-outdoor%2520scenes%252C%2520diverse%2520human%2520motions%252C%2520rich%250Ahuman-human%2520interactions%252C%2520and%2520human-environment%2520interactions.%2520By%2520utilizing%250Abody-mounted%2520IMUs%2520and%2520a%2520head-mounted%2520LiDAR%252C%2520HiSC4D%2520can%2520capture%2520egocentric%2520human%250Amotions%2520in%2520unconstrained%2520space%2520without%2520the%2520need%2520for%2520external%2520devices%2520and%250Apre-built%2520maps.%2520This%2520affords%2520great%2520flexibility%2520and%2520accessibility%2520for%250Ahuman-centered%2520interaction%2520and%25204D%2520scene%2520capturing%2520in%2520various%2520environments.%250ATaking%2520into%2520account%2520that%2520IMUs%2520can%2520capture%2520human%2520spatially%2520unrestricted%2520poses%250Abut%2520are%2520prone%2520to%2520drifting%2520for%2520long-period%2520using%252C%2520and%2520while%2520LiDAR%2520is%2520stable%2520for%250Aglobal%2520localization%2520but%2520rough%2520for%2520local%2520positions%2520and%2520orientations%252C%2520HiSC4D%250Aemploys%2520a%2520joint%2520optimization%2520method%252C%2520harmonizing%2520all%2520sensors%2520and%2520utilizing%250Aenvironment%2520cues%252C%2520yielding%2520promising%2520results%2520for%2520long-term%2520capture%2520in%2520large%250Ascenes.%2520To%2520promote%2520research%2520of%2520egocentric%2520human%2520interaction%2520in%2520large%2520scenes%2520and%250Afacilitate%2520downstream%2520tasks%252C%2520we%2520also%2520present%2520a%2520dataset%252C%2520containing%25208%2520sequences%250Ain%25204%2520large%2520scenes%2520%2528200%2520to%25205%252C000%2520%2524m%255E2%2524%2529%252C%2520providing%252036k%2520frames%2520of%2520accurate%25204D%250Ahuman%2520motions%2520with%2520SMPL%2520annotations%2520and%2520dynamic%2520scenes%252C%252031k%2520frames%2520of%2520cropped%250Ahuman%2520point%2520clouds%252C%2520and%2520scene%2520mesh%2520of%2520the%2520environment.%2520A%2520variety%2520of%2520scenarios%252C%250Asuch%2520as%2520the%2520basketball%2520gym%2520and%2520commercial%2520street%252C%2520alongside%2520challenging%2520human%250Amotions%252C%2520such%2520as%2520daily%2520greeting%252C%2520one-on-one%2520basketball%2520playing%252C%2520and%2520tour%250Aguiding%252C%2520demonstrate%2520the%2520effectiveness%2520and%2520the%2520generalization%2520ability%2520of%250AHiSC4D.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520publicated%2520on%250Awww.lidarhumanmotion.net/hisc4d%2520available%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiSC4D%3A%20Human-centered%20interaction%20and%204D%20Scene%20Capture%20in%20Large-scale%0A%20%20Space%20Using%20Wearable%20IMUs%20and%20LiDAR&entry.906535625=Yudi%20Dai%20and%20Zhiyong%20Wang%20and%20Xiping%20Lin%20and%20Chenglu%20Wen%20and%20Lan%20Xu%20and%20Siqi%20Shen%20and%20Yuexin%20Ma%20and%20Cheng%20Wang&entry.1292438233=%20%20We%20introduce%20HiSC4D%2C%20a%20novel%20Human-centered%20interaction%20and%204D%20Scene%20Capture%0Amethod%2C%20aimed%20at%20accurately%20and%20efficiently%20creating%20a%20dynamic%20digital%20world%2C%0Acontaining%20large-scale%20indoor-outdoor%20scenes%2C%20diverse%20human%20motions%2C%20rich%0Ahuman-human%20interactions%2C%20and%20human-environment%20interactions.%20By%20utilizing%0Abody-mounted%20IMUs%20and%20a%20head-mounted%20LiDAR%2C%20HiSC4D%20can%20capture%20egocentric%20human%0Amotions%20in%20unconstrained%20space%20without%20the%20need%20for%20external%20devices%20and%0Apre-built%20maps.%20This%20affords%20great%20flexibility%20and%20accessibility%20for%0Ahuman-centered%20interaction%20and%204D%20scene%20capturing%20in%20various%20environments.%0ATaking%20into%20account%20that%20IMUs%20can%20capture%20human%20spatially%20unrestricted%20poses%0Abut%20are%20prone%20to%20drifting%20for%20long-period%20using%2C%20and%20while%20LiDAR%20is%20stable%20for%0Aglobal%20localization%20but%20rough%20for%20local%20positions%20and%20orientations%2C%20HiSC4D%0Aemploys%20a%20joint%20optimization%20method%2C%20harmonizing%20all%20sensors%20and%20utilizing%0Aenvironment%20cues%2C%20yielding%20promising%20results%20for%20long-term%20capture%20in%20large%0Ascenes.%20To%20promote%20research%20of%20egocentric%20human%20interaction%20in%20large%20scenes%20and%0Afacilitate%20downstream%20tasks%2C%20we%20also%20present%20a%20dataset%2C%20containing%208%20sequences%0Ain%204%20large%20scenes%20%28200%20to%205%2C000%20%24m%5E2%24%29%2C%20providing%2036k%20frames%20of%20accurate%204D%0Ahuman%20motions%20with%20SMPL%20annotations%20and%20dynamic%20scenes%2C%2031k%20frames%20of%20cropped%0Ahuman%20point%20clouds%2C%20and%20scene%20mesh%20of%20the%20environment.%20A%20variety%20of%20scenarios%2C%0Asuch%20as%20the%20basketball%20gym%20and%20commercial%20street%2C%20alongside%20challenging%20human%0Amotions%2C%20such%20as%20daily%20greeting%2C%20one-on-one%20basketball%20playing%2C%20and%20tour%0Aguiding%2C%20demonstrate%20the%20effectiveness%20and%20the%20generalization%20ability%20of%0AHiSC4D.%20The%20dataset%20and%20code%20will%20be%20publicated%20on%0Awww.lidarhumanmotion.net/hisc4d%20available%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04398v2&entry.124074799=Read"},
{"title": "A Multi-Modal Approach Based on Large Vision Model for Close-Range\n  Underwater Target Localization", "author": "Mingyang Yang and Zeyu Sha and Feitian Zhang", "abstract": "  Underwater target localization uses real-time sensory measurements to\nestimate the position of underwater objects of interest, providing critical\nfeedback information for underwater robots. While acoustic sensing is the most\nacknowledged method in underwater robots and possibly the only effective\napproach for long-range underwater target localization, such a sensing modality\ngenerally suffers from low resolution, high cost and high energy consumption,\nthus leading to a mediocre performance when applied to close-range underwater\ntarget localization. On the other hand, optical sensing has attracted\nincreasing attention in the underwater robotics community for its advantages of\nhigh resolution and low cost, holding a great potential particularly in\nclose-range underwater target localization. However, most existing studies in\nunderwater optical sensing are restricted to specific types of targets due to\nthe limited training data available. In addition, these studies typically focus\non the design of estimation algorithms and ignore the influence of illumination\nconditions on the sensing performance, thus hindering wider applications in the\nreal world. To address the aforementioned issues, this paper proposes a novel\ntarget localization method that assimilates both optical and acoustic sensory\nmeasurements to estimate the 3D positions of close-range underwater targets. A\ntest platform with controllable illumination conditions is designed and\ndeveloped to experimentally investigate the proposed multi-modal sensing\napproach. A large vision model is applied to process the optical imaging\nmeasurements, eliminating the requirement for training data acquisition, thus\nsignificantly expanding the scope of potential applications. Extensive\nexperiments are conducted, the results of which validate the effectiveness of\nthe proposed underwater target localization method.\n", "link": "http://arxiv.org/abs/2401.04595v2", "date": "2024-09-09", "relevancy": 2.2828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6008}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modal%20Approach%20Based%20on%20Large%20Vision%20Model%20for%20Close-Range%0A%20%20Underwater%20Target%20Localization&body=Title%3A%20A%20Multi-Modal%20Approach%20Based%20on%20Large%20Vision%20Model%20for%20Close-Range%0A%20%20Underwater%20Target%20Localization%0AAuthor%3A%20Mingyang%20Yang%20and%20Zeyu%20Sha%20and%20Feitian%20Zhang%0AAbstract%3A%20%20%20Underwater%20target%20localization%20uses%20real-time%20sensory%20measurements%20to%0Aestimate%20the%20position%20of%20underwater%20objects%20of%20interest%2C%20providing%20critical%0Afeedback%20information%20for%20underwater%20robots.%20While%20acoustic%20sensing%20is%20the%20most%0Aacknowledged%20method%20in%20underwater%20robots%20and%20possibly%20the%20only%20effective%0Aapproach%20for%20long-range%20underwater%20target%20localization%2C%20such%20a%20sensing%20modality%0Agenerally%20suffers%20from%20low%20resolution%2C%20high%20cost%20and%20high%20energy%20consumption%2C%0Athus%20leading%20to%20a%20mediocre%20performance%20when%20applied%20to%20close-range%20underwater%0Atarget%20localization.%20On%20the%20other%20hand%2C%20optical%20sensing%20has%20attracted%0Aincreasing%20attention%20in%20the%20underwater%20robotics%20community%20for%20its%20advantages%20of%0Ahigh%20resolution%20and%20low%20cost%2C%20holding%20a%20great%20potential%20particularly%20in%0Aclose-range%20underwater%20target%20localization.%20However%2C%20most%20existing%20studies%20in%0Aunderwater%20optical%20sensing%20are%20restricted%20to%20specific%20types%20of%20targets%20due%20to%0Athe%20limited%20training%20data%20available.%20In%20addition%2C%20these%20studies%20typically%20focus%0Aon%20the%20design%20of%20estimation%20algorithms%20and%20ignore%20the%20influence%20of%20illumination%0Aconditions%20on%20the%20sensing%20performance%2C%20thus%20hindering%20wider%20applications%20in%20the%0Areal%20world.%20To%20address%20the%20aforementioned%20issues%2C%20this%20paper%20proposes%20a%20novel%0Atarget%20localization%20method%20that%20assimilates%20both%20optical%20and%20acoustic%20sensory%0Ameasurements%20to%20estimate%20the%203D%20positions%20of%20close-range%20underwater%20targets.%20A%0Atest%20platform%20with%20controllable%20illumination%20conditions%20is%20designed%20and%0Adeveloped%20to%20experimentally%20investigate%20the%20proposed%20multi-modal%20sensing%0Aapproach.%20A%20large%20vision%20model%20is%20applied%20to%20process%20the%20optical%20imaging%0Ameasurements%2C%20eliminating%20the%20requirement%20for%20training%20data%20acquisition%2C%20thus%0Asignificantly%20expanding%20the%20scope%20of%20potential%20applications.%20Extensive%0Aexperiments%20are%20conducted%2C%20the%20results%20of%20which%20validate%20the%20effectiveness%20of%0Athe%20proposed%20underwater%20target%20localization%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Modal%2520Approach%2520Based%2520on%2520Large%2520Vision%2520Model%2520for%2520Close-Range%250A%2520%2520Underwater%2520Target%2520Localization%26entry.906535625%3DMingyang%2520Yang%2520and%2520Zeyu%2520Sha%2520and%2520Feitian%2520Zhang%26entry.1292438233%3D%2520%2520Underwater%2520target%2520localization%2520uses%2520real-time%2520sensory%2520measurements%2520to%250Aestimate%2520the%2520position%2520of%2520underwater%2520objects%2520of%2520interest%252C%2520providing%2520critical%250Afeedback%2520information%2520for%2520underwater%2520robots.%2520While%2520acoustic%2520sensing%2520is%2520the%2520most%250Aacknowledged%2520method%2520in%2520underwater%2520robots%2520and%2520possibly%2520the%2520only%2520effective%250Aapproach%2520for%2520long-range%2520underwater%2520target%2520localization%252C%2520such%2520a%2520sensing%2520modality%250Agenerally%2520suffers%2520from%2520low%2520resolution%252C%2520high%2520cost%2520and%2520high%2520energy%2520consumption%252C%250Athus%2520leading%2520to%2520a%2520mediocre%2520performance%2520when%2520applied%2520to%2520close-range%2520underwater%250Atarget%2520localization.%2520On%2520the%2520other%2520hand%252C%2520optical%2520sensing%2520has%2520attracted%250Aincreasing%2520attention%2520in%2520the%2520underwater%2520robotics%2520community%2520for%2520its%2520advantages%2520of%250Ahigh%2520resolution%2520and%2520low%2520cost%252C%2520holding%2520a%2520great%2520potential%2520particularly%2520in%250Aclose-range%2520underwater%2520target%2520localization.%2520However%252C%2520most%2520existing%2520studies%2520in%250Aunderwater%2520optical%2520sensing%2520are%2520restricted%2520to%2520specific%2520types%2520of%2520targets%2520due%2520to%250Athe%2520limited%2520training%2520data%2520available.%2520In%2520addition%252C%2520these%2520studies%2520typically%2520focus%250Aon%2520the%2520design%2520of%2520estimation%2520algorithms%2520and%2520ignore%2520the%2520influence%2520of%2520illumination%250Aconditions%2520on%2520the%2520sensing%2520performance%252C%2520thus%2520hindering%2520wider%2520applications%2520in%2520the%250Areal%2520world.%2520To%2520address%2520the%2520aforementioned%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520novel%250Atarget%2520localization%2520method%2520that%2520assimilates%2520both%2520optical%2520and%2520acoustic%2520sensory%250Ameasurements%2520to%2520estimate%2520the%25203D%2520positions%2520of%2520close-range%2520underwater%2520targets.%2520A%250Atest%2520platform%2520with%2520controllable%2520illumination%2520conditions%2520is%2520designed%2520and%250Adeveloped%2520to%2520experimentally%2520investigate%2520the%2520proposed%2520multi-modal%2520sensing%250Aapproach.%2520A%2520large%2520vision%2520model%2520is%2520applied%2520to%2520process%2520the%2520optical%2520imaging%250Ameasurements%252C%2520eliminating%2520the%2520requirement%2520for%2520training%2520data%2520acquisition%252C%2520thus%250Asignificantly%2520expanding%2520the%2520scope%2520of%2520potential%2520applications.%2520Extensive%250Aexperiments%2520are%2520conducted%252C%2520the%2520results%2520of%2520which%2520validate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520underwater%2520target%2520localization%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modal%20Approach%20Based%20on%20Large%20Vision%20Model%20for%20Close-Range%0A%20%20Underwater%20Target%20Localization&entry.906535625=Mingyang%20Yang%20and%20Zeyu%20Sha%20and%20Feitian%20Zhang&entry.1292438233=%20%20Underwater%20target%20localization%20uses%20real-time%20sensory%20measurements%20to%0Aestimate%20the%20position%20of%20underwater%20objects%20of%20interest%2C%20providing%20critical%0Afeedback%20information%20for%20underwater%20robots.%20While%20acoustic%20sensing%20is%20the%20most%0Aacknowledged%20method%20in%20underwater%20robots%20and%20possibly%20the%20only%20effective%0Aapproach%20for%20long-range%20underwater%20target%20localization%2C%20such%20a%20sensing%20modality%0Agenerally%20suffers%20from%20low%20resolution%2C%20high%20cost%20and%20high%20energy%20consumption%2C%0Athus%20leading%20to%20a%20mediocre%20performance%20when%20applied%20to%20close-range%20underwater%0Atarget%20localization.%20On%20the%20other%20hand%2C%20optical%20sensing%20has%20attracted%0Aincreasing%20attention%20in%20the%20underwater%20robotics%20community%20for%20its%20advantages%20of%0Ahigh%20resolution%20and%20low%20cost%2C%20holding%20a%20great%20potential%20particularly%20in%0Aclose-range%20underwater%20target%20localization.%20However%2C%20most%20existing%20studies%20in%0Aunderwater%20optical%20sensing%20are%20restricted%20to%20specific%20types%20of%20targets%20due%20to%0Athe%20limited%20training%20data%20available.%20In%20addition%2C%20these%20studies%20typically%20focus%0Aon%20the%20design%20of%20estimation%20algorithms%20and%20ignore%20the%20influence%20of%20illumination%0Aconditions%20on%20the%20sensing%20performance%2C%20thus%20hindering%20wider%20applications%20in%20the%0Areal%20world.%20To%20address%20the%20aforementioned%20issues%2C%20this%20paper%20proposes%20a%20novel%0Atarget%20localization%20method%20that%20assimilates%20both%20optical%20and%20acoustic%20sensory%0Ameasurements%20to%20estimate%20the%203D%20positions%20of%20close-range%20underwater%20targets.%20A%0Atest%20platform%20with%20controllable%20illumination%20conditions%20is%20designed%20and%0Adeveloped%20to%20experimentally%20investigate%20the%20proposed%20multi-modal%20sensing%0Aapproach.%20A%20large%20vision%20model%20is%20applied%20to%20process%20the%20optical%20imaging%0Ameasurements%2C%20eliminating%20the%20requirement%20for%20training%20data%20acquisition%2C%20thus%0Asignificantly%20expanding%20the%20scope%20of%20potential%20applications.%20Extensive%0Aexperiments%20are%20conducted%2C%20the%20results%20of%20which%20validate%20the%20effectiveness%20of%0Athe%20proposed%20underwater%20target%20localization%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04595v2&entry.124074799=Read"},
{"title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild", "author": "Yuntian Deng and Wenting Zhao and Jack Hessel and Xiang Ren and Claire Cardie and Yejin Choi", "abstract": "  The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.\n", "link": "http://arxiv.org/abs/2409.03753v2", "date": "2024-09-09", "relevancy": 2.2827, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4628}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildVis%3A%20Open%20Source%20Visualizer%20for%20Million-Scale%20Chat%20Logs%20in%20the%20Wild&body=Title%3A%20WildVis%3A%20Open%20Source%20Visualizer%20for%20Million-Scale%20Chat%20Logs%20in%20the%20Wild%0AAuthor%3A%20Yuntian%20Deng%20and%20Wenting%20Zhao%20and%20Jack%20Hessel%20and%20Xiang%20Ren%20and%20Claire%20Cardie%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20The%20increasing%20availability%20of%20real-world%20conversation%20data%20offers%20exciting%0Aopportunities%20for%20researchers%20to%20study%20user-chatbot%20interactions.%20However%2C%20the%0Asheer%20volume%20of%20this%20data%20makes%20manually%20examining%20individual%20conversations%0Aimpractical.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20WildVis%2C%20an%20interactive%0Atool%20that%20enables%20fast%2C%20versatile%2C%20and%20large-scale%20conversation%20analysis.%0AWildVis%20provides%20search%20and%20visualization%20capabilities%20in%20the%20text%20and%0Aembedding%20spaces%20based%20on%20a%20list%20of%20criteria.%20To%20manage%20million-scale%20datasets%2C%0Awe%20implemented%20optimizations%20including%20search%20index%20construction%2C%20embedding%0Aprecomputation%20and%20compression%2C%20and%20caching%20to%20ensure%20responsive%20user%0Ainteractions%20within%20seconds.%20We%20demonstrate%20WildVis%27%20utility%20through%20three%20case%0Astudies%3A%20facilitating%20chatbot%20misuse%20research%2C%20visualizing%20and%20comparing%20topic%0Adistributions%20across%20datasets%2C%20and%20characterizing%20user-specific%20conversation%0Apatterns.%20WildVis%20is%20open-source%20and%20designed%20to%20be%20extendable%2C%20supporting%0Aadditional%20datasets%20and%20customized%20search%20and%20visualization%20functionalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildVis%253A%2520Open%2520Source%2520Visualizer%2520for%2520Million-Scale%2520Chat%2520Logs%2520in%2520the%2520Wild%26entry.906535625%3DYuntian%2520Deng%2520and%2520Wenting%2520Zhao%2520and%2520Jack%2520Hessel%2520and%2520Xiang%2520Ren%2520and%2520Claire%2520Cardie%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520The%2520increasing%2520availability%2520of%2520real-world%2520conversation%2520data%2520offers%2520exciting%250Aopportunities%2520for%2520researchers%2520to%2520study%2520user-chatbot%2520interactions.%2520However%252C%2520the%250Asheer%2520volume%2520of%2520this%2520data%2520makes%2520manually%2520examining%2520individual%2520conversations%250Aimpractical.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520WildVis%252C%2520an%2520interactive%250Atool%2520that%2520enables%2520fast%252C%2520versatile%252C%2520and%2520large-scale%2520conversation%2520analysis.%250AWildVis%2520provides%2520search%2520and%2520visualization%2520capabilities%2520in%2520the%2520text%2520and%250Aembedding%2520spaces%2520based%2520on%2520a%2520list%2520of%2520criteria.%2520To%2520manage%2520million-scale%2520datasets%252C%250Awe%2520implemented%2520optimizations%2520including%2520search%2520index%2520construction%252C%2520embedding%250Aprecomputation%2520and%2520compression%252C%2520and%2520caching%2520to%2520ensure%2520responsive%2520user%250Ainteractions%2520within%2520seconds.%2520We%2520demonstrate%2520WildVis%2527%2520utility%2520through%2520three%2520case%250Astudies%253A%2520facilitating%2520chatbot%2520misuse%2520research%252C%2520visualizing%2520and%2520comparing%2520topic%250Adistributions%2520across%2520datasets%252C%2520and%2520characterizing%2520user-specific%2520conversation%250Apatterns.%2520WildVis%2520is%2520open-source%2520and%2520designed%2520to%2520be%2520extendable%252C%2520supporting%250Aadditional%2520datasets%2520and%2520customized%2520search%2520and%2520visualization%2520functionalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildVis%3A%20Open%20Source%20Visualizer%20for%20Million-Scale%20Chat%20Logs%20in%20the%20Wild&entry.906535625=Yuntian%20Deng%20and%20Wenting%20Zhao%20and%20Jack%20Hessel%20and%20Xiang%20Ren%20and%20Claire%20Cardie%20and%20Yejin%20Choi&entry.1292438233=%20%20The%20increasing%20availability%20of%20real-world%20conversation%20data%20offers%20exciting%0Aopportunities%20for%20researchers%20to%20study%20user-chatbot%20interactions.%20However%2C%20the%0Asheer%20volume%20of%20this%20data%20makes%20manually%20examining%20individual%20conversations%0Aimpractical.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20WildVis%2C%20an%20interactive%0Atool%20that%20enables%20fast%2C%20versatile%2C%20and%20large-scale%20conversation%20analysis.%0AWildVis%20provides%20search%20and%20visualization%20capabilities%20in%20the%20text%20and%0Aembedding%20spaces%20based%20on%20a%20list%20of%20criteria.%20To%20manage%20million-scale%20datasets%2C%0Awe%20implemented%20optimizations%20including%20search%20index%20construction%2C%20embedding%0Aprecomputation%20and%20compression%2C%20and%20caching%20to%20ensure%20responsive%20user%0Ainteractions%20within%20seconds.%20We%20demonstrate%20WildVis%27%20utility%20through%20three%20case%0Astudies%3A%20facilitating%20chatbot%20misuse%20research%2C%20visualizing%20and%20comparing%20topic%0Adistributions%20across%20datasets%2C%20and%20characterizing%20user-specific%20conversation%0Apatterns.%20WildVis%20is%20open-source%20and%20designed%20to%20be%20extendable%2C%20supporting%0Aadditional%20datasets%20and%20customized%20search%20and%20visualization%20functionalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03753v2&entry.124074799=Read"},
{"title": "Prototype-Driven Multi-Feature Generation for Visible-Infrared Person\n  Re-identification", "author": "Jiarui Li and Zhen Qiu and Yilin Yang and Yuqi Li and Zeyu Dong and Chuanguang Yang", "abstract": "  The primary challenges in visible-infrared person re-identification arise\nfrom the differences between visible (vis) and infrared (ir) images, including\ninter-modal and intra-modal variations. These challenges are further\ncomplicated by varying viewpoints and irregular movements. Existing methods\noften rely on horizontal partitioning to align part-level features, which can\nintroduce inaccuracies and have limited effectiveness in reducing modality\ndiscrepancies. In this paper, we propose a novel Prototype-Driven Multi-feature\ngeneration framework (PDM) aimed at mitigating cross-modal discrepancies by\nconstructing diversified features and mining latent semantically similar\nfeatures for modal alignment. PDM comprises two key components: Multi-Feature\nGeneration Module (MFGM) and Prototype Learning Module (PLM). The MFGM\ngenerates diversity features closely distributed from modality-shared features\nto represent pedestrians. Additionally, the PLM utilizes learnable prototypes\nto excavate latent semantic similarities among local features between visible\nand infrared modalities, thereby facilitating cross-modal instance-level\nalignment. We introduce the cosine heterogeneity loss to enhance prototype\ndiversity for extracting rich local features. Extensive experiments conducted\non the SYSU-MM01 and LLCM datasets demonstrate that our approach achieves\nstate-of-the-art performance. Our codes are available at\nhttps://github.com/mmunhappy/ICASSP2025-PDM.\n", "link": "http://arxiv.org/abs/2409.05642v1", "date": "2024-09-09", "relevancy": 2.2799, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5745}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5696}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype-Driven%20Multi-Feature%20Generation%20for%20Visible-Infrared%20Person%0A%20%20Re-identification&body=Title%3A%20Prototype-Driven%20Multi-Feature%20Generation%20for%20Visible-Infrared%20Person%0A%20%20Re-identification%0AAuthor%3A%20Jiarui%20Li%20and%20Zhen%20Qiu%20and%20Yilin%20Yang%20and%20Yuqi%20Li%20and%20Zeyu%20Dong%20and%20Chuanguang%20Yang%0AAbstract%3A%20%20%20The%20primary%20challenges%20in%20visible-infrared%20person%20re-identification%20arise%0Afrom%20the%20differences%20between%20visible%20%28vis%29%20and%20infrared%20%28ir%29%20images%2C%20including%0Ainter-modal%20and%20intra-modal%20variations.%20These%20challenges%20are%20further%0Acomplicated%20by%20varying%20viewpoints%20and%20irregular%20movements.%20Existing%20methods%0Aoften%20rely%20on%20horizontal%20partitioning%20to%20align%20part-level%20features%2C%20which%20can%0Aintroduce%20inaccuracies%20and%20have%20limited%20effectiveness%20in%20reducing%20modality%0Adiscrepancies.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Prototype-Driven%20Multi-feature%0Ageneration%20framework%20%28PDM%29%20aimed%20at%20mitigating%20cross-modal%20discrepancies%20by%0Aconstructing%20diversified%20features%20and%20mining%20latent%20semantically%20similar%0Afeatures%20for%20modal%20alignment.%20PDM%20comprises%20two%20key%20components%3A%20Multi-Feature%0AGeneration%20Module%20%28MFGM%29%20and%20Prototype%20Learning%20Module%20%28PLM%29.%20The%20MFGM%0Agenerates%20diversity%20features%20closely%20distributed%20from%20modality-shared%20features%0Ato%20represent%20pedestrians.%20Additionally%2C%20the%20PLM%20utilizes%20learnable%20prototypes%0Ato%20excavate%20latent%20semantic%20similarities%20among%20local%20features%20between%20visible%0Aand%20infrared%20modalities%2C%20thereby%20facilitating%20cross-modal%20instance-level%0Aalignment.%20We%20introduce%20the%20cosine%20heterogeneity%20loss%20to%20enhance%20prototype%0Adiversity%20for%20extracting%20rich%20local%20features.%20Extensive%20experiments%20conducted%0Aon%20the%20SYSU-MM01%20and%20LLCM%20datasets%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/mmunhappy/ICASSP2025-PDM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype-Driven%2520Multi-Feature%2520Generation%2520for%2520Visible-Infrared%2520Person%250A%2520%2520Re-identification%26entry.906535625%3DJiarui%2520Li%2520and%2520Zhen%2520Qiu%2520and%2520Yilin%2520Yang%2520and%2520Yuqi%2520Li%2520and%2520Zeyu%2520Dong%2520and%2520Chuanguang%2520Yang%26entry.1292438233%3D%2520%2520The%2520primary%2520challenges%2520in%2520visible-infrared%2520person%2520re-identification%2520arise%250Afrom%2520the%2520differences%2520between%2520visible%2520%2528vis%2529%2520and%2520infrared%2520%2528ir%2529%2520images%252C%2520including%250Ainter-modal%2520and%2520intra-modal%2520variations.%2520These%2520challenges%2520are%2520further%250Acomplicated%2520by%2520varying%2520viewpoints%2520and%2520irregular%2520movements.%2520Existing%2520methods%250Aoften%2520rely%2520on%2520horizontal%2520partitioning%2520to%2520align%2520part-level%2520features%252C%2520which%2520can%250Aintroduce%2520inaccuracies%2520and%2520have%2520limited%2520effectiveness%2520in%2520reducing%2520modality%250Adiscrepancies.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Prototype-Driven%2520Multi-feature%250Ageneration%2520framework%2520%2528PDM%2529%2520aimed%2520at%2520mitigating%2520cross-modal%2520discrepancies%2520by%250Aconstructing%2520diversified%2520features%2520and%2520mining%2520latent%2520semantically%2520similar%250Afeatures%2520for%2520modal%2520alignment.%2520PDM%2520comprises%2520two%2520key%2520components%253A%2520Multi-Feature%250AGeneration%2520Module%2520%2528MFGM%2529%2520and%2520Prototype%2520Learning%2520Module%2520%2528PLM%2529.%2520The%2520MFGM%250Agenerates%2520diversity%2520features%2520closely%2520distributed%2520from%2520modality-shared%2520features%250Ato%2520represent%2520pedestrians.%2520Additionally%252C%2520the%2520PLM%2520utilizes%2520learnable%2520prototypes%250Ato%2520excavate%2520latent%2520semantic%2520similarities%2520among%2520local%2520features%2520between%2520visible%250Aand%2520infrared%2520modalities%252C%2520thereby%2520facilitating%2520cross-modal%2520instance-level%250Aalignment.%2520We%2520introduce%2520the%2520cosine%2520heterogeneity%2520loss%2520to%2520enhance%2520prototype%250Adiversity%2520for%2520extracting%2520rich%2520local%2520features.%2520Extensive%2520experiments%2520conducted%250Aon%2520the%2520SYSU-MM01%2520and%2520LLCM%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performance.%2520Our%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/mmunhappy/ICASSP2025-PDM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype-Driven%20Multi-Feature%20Generation%20for%20Visible-Infrared%20Person%0A%20%20Re-identification&entry.906535625=Jiarui%20Li%20and%20Zhen%20Qiu%20and%20Yilin%20Yang%20and%20Yuqi%20Li%20and%20Zeyu%20Dong%20and%20Chuanguang%20Yang&entry.1292438233=%20%20The%20primary%20challenges%20in%20visible-infrared%20person%20re-identification%20arise%0Afrom%20the%20differences%20between%20visible%20%28vis%29%20and%20infrared%20%28ir%29%20images%2C%20including%0Ainter-modal%20and%20intra-modal%20variations.%20These%20challenges%20are%20further%0Acomplicated%20by%20varying%20viewpoints%20and%20irregular%20movements.%20Existing%20methods%0Aoften%20rely%20on%20horizontal%20partitioning%20to%20align%20part-level%20features%2C%20which%20can%0Aintroduce%20inaccuracies%20and%20have%20limited%20effectiveness%20in%20reducing%20modality%0Adiscrepancies.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Prototype-Driven%20Multi-feature%0Ageneration%20framework%20%28PDM%29%20aimed%20at%20mitigating%20cross-modal%20discrepancies%20by%0Aconstructing%20diversified%20features%20and%20mining%20latent%20semantically%20similar%0Afeatures%20for%20modal%20alignment.%20PDM%20comprises%20two%20key%20components%3A%20Multi-Feature%0AGeneration%20Module%20%28MFGM%29%20and%20Prototype%20Learning%20Module%20%28PLM%29.%20The%20MFGM%0Agenerates%20diversity%20features%20closely%20distributed%20from%20modality-shared%20features%0Ato%20represent%20pedestrians.%20Additionally%2C%20the%20PLM%20utilizes%20learnable%20prototypes%0Ato%20excavate%20latent%20semantic%20similarities%20among%20local%20features%20between%20visible%0Aand%20infrared%20modalities%2C%20thereby%20facilitating%20cross-modal%20instance-level%0Aalignment.%20We%20introduce%20the%20cosine%20heterogeneity%20loss%20to%20enhance%20prototype%0Adiversity%20for%20extracting%20rich%20local%20features.%20Extensive%20experiments%20conducted%0Aon%20the%20SYSU-MM01%20and%20LLCM%20datasets%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/mmunhappy/ICASSP2025-PDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05642v1&entry.124074799=Read"},
{"title": "Leveraging Object Priors for Point Tracking", "author": "Bikram Boote and Anh Thai and Wenqi Jia and Ozgur Kara and Stefan Stojanov and James M. Rehg and Sangmin Lee", "abstract": "  Point tracking is a fundamental problem in computer vision with numerous\napplications in AR and robotics. A common failure mode in long-term point\ntracking occurs when the predicted point leaves the object it belongs to and\nlands on the background or another object. We identify this as the failure to\ncorrectly capture objectness properties in learning to track. To address this\nlimitation of prior work, we propose a novel objectness regularization approach\nthat guides points to be aware of object priors by forcing them to stay inside\nthe the boundaries of object instances. By capturing objectness cues at\ntraining time, we avoid the need to compute object masks during testing. In\naddition, we leverage contextual attention to enhance the feature\nrepresentation for capturing objectness at the feature level more effectively.\nAs a result, our approach achieves state-of-the-art performance on three point\ntracking benchmarks, and we further validate the effectiveness of our\ncomponents via ablation studies. The source code is available at:\nhttps://github.com/RehgLab/tracking_objectness\n", "link": "http://arxiv.org/abs/2409.05786v1", "date": "2024-09-09", "relevancy": 2.2542, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5689}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Object%20Priors%20for%20Point%20Tracking&body=Title%3A%20Leveraging%20Object%20Priors%20for%20Point%20Tracking%0AAuthor%3A%20Bikram%20Boote%20and%20Anh%20Thai%20and%20Wenqi%20Jia%20and%20Ozgur%20Kara%20and%20Stefan%20Stojanov%20and%20James%20M.%20Rehg%20and%20Sangmin%20Lee%0AAbstract%3A%20%20%20Point%20tracking%20is%20a%20fundamental%20problem%20in%20computer%20vision%20with%20numerous%0Aapplications%20in%20AR%20and%20robotics.%20A%20common%20failure%20mode%20in%20long-term%20point%0Atracking%20occurs%20when%20the%20predicted%20point%20leaves%20the%20object%20it%20belongs%20to%20and%0Alands%20on%20the%20background%20or%20another%20object.%20We%20identify%20this%20as%20the%20failure%20to%0Acorrectly%20capture%20objectness%20properties%20in%20learning%20to%20track.%20To%20address%20this%0Alimitation%20of%20prior%20work%2C%20we%20propose%20a%20novel%20objectness%20regularization%20approach%0Athat%20guides%20points%20to%20be%20aware%20of%20object%20priors%20by%20forcing%20them%20to%20stay%20inside%0Athe%20the%20boundaries%20of%20object%20instances.%20By%20capturing%20objectness%20cues%20at%0Atraining%20time%2C%20we%20avoid%20the%20need%20to%20compute%20object%20masks%20during%20testing.%20In%0Aaddition%2C%20we%20leverage%20contextual%20attention%20to%20enhance%20the%20feature%0Arepresentation%20for%20capturing%20objectness%20at%20the%20feature%20level%20more%20effectively.%0AAs%20a%20result%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20three%20point%0Atracking%20benchmarks%2C%20and%20we%20further%20validate%20the%20effectiveness%20of%20our%0Acomponents%20via%20ablation%20studies.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/RehgLab/tracking_objectness%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Object%2520Priors%2520for%2520Point%2520Tracking%26entry.906535625%3DBikram%2520Boote%2520and%2520Anh%2520Thai%2520and%2520Wenqi%2520Jia%2520and%2520Ozgur%2520Kara%2520and%2520Stefan%2520Stojanov%2520and%2520James%2520M.%2520Rehg%2520and%2520Sangmin%2520Lee%26entry.1292438233%3D%2520%2520Point%2520tracking%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%2520with%2520numerous%250Aapplications%2520in%2520AR%2520and%2520robotics.%2520A%2520common%2520failure%2520mode%2520in%2520long-term%2520point%250Atracking%2520occurs%2520when%2520the%2520predicted%2520point%2520leaves%2520the%2520object%2520it%2520belongs%2520to%2520and%250Alands%2520on%2520the%2520background%2520or%2520another%2520object.%2520We%2520identify%2520this%2520as%2520the%2520failure%2520to%250Acorrectly%2520capture%2520objectness%2520properties%2520in%2520learning%2520to%2520track.%2520To%2520address%2520this%250Alimitation%2520of%2520prior%2520work%252C%2520we%2520propose%2520a%2520novel%2520objectness%2520regularization%2520approach%250Athat%2520guides%2520points%2520to%2520be%2520aware%2520of%2520object%2520priors%2520by%2520forcing%2520them%2520to%2520stay%2520inside%250Athe%2520the%2520boundaries%2520of%2520object%2520instances.%2520By%2520capturing%2520objectness%2520cues%2520at%250Atraining%2520time%252C%2520we%2520avoid%2520the%2520need%2520to%2520compute%2520object%2520masks%2520during%2520testing.%2520In%250Aaddition%252C%2520we%2520leverage%2520contextual%2520attention%2520to%2520enhance%2520the%2520feature%250Arepresentation%2520for%2520capturing%2520objectness%2520at%2520the%2520feature%2520level%2520more%2520effectively.%250AAs%2520a%2520result%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%2520point%250Atracking%2520benchmarks%252C%2520and%2520we%2520further%2520validate%2520the%2520effectiveness%2520of%2520our%250Acomponents%2520via%2520ablation%2520studies.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/RehgLab/tracking_objectness%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Object%20Priors%20for%20Point%20Tracking&entry.906535625=Bikram%20Boote%20and%20Anh%20Thai%20and%20Wenqi%20Jia%20and%20Ozgur%20Kara%20and%20Stefan%20Stojanov%20and%20James%20M.%20Rehg%20and%20Sangmin%20Lee&entry.1292438233=%20%20Point%20tracking%20is%20a%20fundamental%20problem%20in%20computer%20vision%20with%20numerous%0Aapplications%20in%20AR%20and%20robotics.%20A%20common%20failure%20mode%20in%20long-term%20point%0Atracking%20occurs%20when%20the%20predicted%20point%20leaves%20the%20object%20it%20belongs%20to%20and%0Alands%20on%20the%20background%20or%20another%20object.%20We%20identify%20this%20as%20the%20failure%20to%0Acorrectly%20capture%20objectness%20properties%20in%20learning%20to%20track.%20To%20address%20this%0Alimitation%20of%20prior%20work%2C%20we%20propose%20a%20novel%20objectness%20regularization%20approach%0Athat%20guides%20points%20to%20be%20aware%20of%20object%20priors%20by%20forcing%20them%20to%20stay%20inside%0Athe%20the%20boundaries%20of%20object%20instances.%20By%20capturing%20objectness%20cues%20at%0Atraining%20time%2C%20we%20avoid%20the%20need%20to%20compute%20object%20masks%20during%20testing.%20In%0Aaddition%2C%20we%20leverage%20contextual%20attention%20to%20enhance%20the%20feature%0Arepresentation%20for%20capturing%20objectness%20at%20the%20feature%20level%20more%20effectively.%0AAs%20a%20result%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20three%20point%0Atracking%20benchmarks%2C%20and%20we%20further%20validate%20the%20effectiveness%20of%20our%0Acomponents%20via%20ablation%20studies.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/RehgLab/tracking_objectness%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05786v1&entry.124074799=Read"},
{"title": "LSVOS Challenge Report: Large-scale Complex and Long Video Object\n  Segmentation", "author": "Henghui Ding and Lingyi Hong and Chang Liu and Ning Xu and Linjie Yang and Yuchen Fan and Deshui Miao and Yameng Gu and Xin Li and Zhenyu He and Yaowei Wang and Ming-Hsuan Yang and Jinming Chai and Qin Ma and Junpei Zhang and Licheng Jiao and Fang Liu and Xinyu Liu and Jing Zhang and Kexin Zhang and Xu Liu and LingLing Li and Hao Fang and Feiyu Pan and Xiankai Lu and Wei Zhang and Runmin Cong and Tuyen Tran and Bin Cao and Yisi Zhang and Hanyi Wang and Xingjian He and Jing Liu", "abstract": "  Despite the promising performance of current video segmentation models on\nexisting benchmarks, these models still struggle with complex scenes. In this\npaper, we introduce the 6th Large-scale Video Object Segmentation (LSVOS)\nchallenge in conjunction with ECCV 2024 workshop. This year's challenge\nincludes two tasks: Video Object Segmentation (VOS) and Referring Video Object\nSegmentation (RVOS). In this year, we replace the classic YouTube-VOS and\nYouTube-RVOS benchmark with latest datasets MOSE, LVOS, and MeViS to assess VOS\nunder more challenging complex environments. This year's challenge attracted\n129 registered teams from more than 20 institutes across over 8 countries. This\nreport include the challenge and dataset introduction, and the methods used by\ntop 7 teams in two tracks. More details can be found in our homepage\nhttps://lsvos.github.io/.\n", "link": "http://arxiv.org/abs/2409.05847v1", "date": "2024-09-09", "relevancy": 2.2529, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSVOS%20Challenge%20Report%3A%20Large-scale%20Complex%20and%20Long%20Video%20Object%0A%20%20Segmentation&body=Title%3A%20LSVOS%20Challenge%20Report%3A%20Large-scale%20Complex%20and%20Long%20Video%20Object%0A%20%20Segmentation%0AAuthor%3A%20Henghui%20Ding%20and%20Lingyi%20Hong%20and%20Chang%20Liu%20and%20Ning%20Xu%20and%20Linjie%20Yang%20and%20Yuchen%20Fan%20and%20Deshui%20Miao%20and%20Yameng%20Gu%20and%20Xin%20Li%20and%20Zhenyu%20He%20and%20Yaowei%20Wang%20and%20Ming-Hsuan%20Yang%20and%20Jinming%20Chai%20and%20Qin%20Ma%20and%20Junpei%20Zhang%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xinyu%20Liu%20and%20Jing%20Zhang%20and%20Kexin%20Zhang%20and%20Xu%20Liu%20and%20LingLing%20Li%20and%20Hao%20Fang%20and%20Feiyu%20Pan%20and%20Xiankai%20Lu%20and%20Wei%20Zhang%20and%20Runmin%20Cong%20and%20Tuyen%20Tran%20and%20Bin%20Cao%20and%20Yisi%20Zhang%20and%20Hanyi%20Wang%20and%20Xingjian%20He%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Despite%20the%20promising%20performance%20of%20current%20video%20segmentation%20models%20on%0Aexisting%20benchmarks%2C%20these%20models%20still%20struggle%20with%20complex%20scenes.%20In%20this%0Apaper%2C%20we%20introduce%20the%206th%20Large-scale%20Video%20Object%20Segmentation%20%28LSVOS%29%0Achallenge%20in%20conjunction%20with%20ECCV%202024%20workshop.%20This%20year%27s%20challenge%0Aincludes%20two%20tasks%3A%20Video%20Object%20Segmentation%20%28VOS%29%20and%20Referring%20Video%20Object%0ASegmentation%20%28RVOS%29.%20In%20this%20year%2C%20we%20replace%20the%20classic%20YouTube-VOS%20and%0AYouTube-RVOS%20benchmark%20with%20latest%20datasets%20MOSE%2C%20LVOS%2C%20and%20MeViS%20to%20assess%20VOS%0Aunder%20more%20challenging%20complex%20environments.%20This%20year%27s%20challenge%20attracted%0A129%20registered%20teams%20from%20more%20than%2020%20institutes%20across%20over%208%20countries.%20This%0Areport%20include%20the%20challenge%20and%20dataset%20introduction%2C%20and%20the%20methods%20used%20by%0Atop%207%20teams%20in%20two%20tracks.%20More%20details%20can%20be%20found%20in%20our%20homepage%0Ahttps%3A//lsvos.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSVOS%2520Challenge%2520Report%253A%2520Large-scale%2520Complex%2520and%2520Long%2520Video%2520Object%250A%2520%2520Segmentation%26entry.906535625%3DHenghui%2520Ding%2520and%2520Lingyi%2520Hong%2520and%2520Chang%2520Liu%2520and%2520Ning%2520Xu%2520and%2520Linjie%2520Yang%2520and%2520Yuchen%2520Fan%2520and%2520Deshui%2520Miao%2520and%2520Yameng%2520Gu%2520and%2520Xin%2520Li%2520and%2520Zhenyu%2520He%2520and%2520Yaowei%2520Wang%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Jinming%2520Chai%2520and%2520Qin%2520Ma%2520and%2520Junpei%2520Zhang%2520and%2520Licheng%2520Jiao%2520and%2520Fang%2520Liu%2520and%2520Xinyu%2520Liu%2520and%2520Jing%2520Zhang%2520and%2520Kexin%2520Zhang%2520and%2520Xu%2520Liu%2520and%2520LingLing%2520Li%2520and%2520Hao%2520Fang%2520and%2520Feiyu%2520Pan%2520and%2520Xiankai%2520Lu%2520and%2520Wei%2520Zhang%2520and%2520Runmin%2520Cong%2520and%2520Tuyen%2520Tran%2520and%2520Bin%2520Cao%2520and%2520Yisi%2520Zhang%2520and%2520Hanyi%2520Wang%2520and%2520Xingjian%2520He%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Despite%2520the%2520promising%2520performance%2520of%2520current%2520video%2520segmentation%2520models%2520on%250Aexisting%2520benchmarks%252C%2520these%2520models%2520still%2520struggle%2520with%2520complex%2520scenes.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%25206th%2520Large-scale%2520Video%2520Object%2520Segmentation%2520%2528LSVOS%2529%250Achallenge%2520in%2520conjunction%2520with%2520ECCV%25202024%2520workshop.%2520This%2520year%2527s%2520challenge%250Aincludes%2520two%2520tasks%253A%2520Video%2520Object%2520Segmentation%2520%2528VOS%2529%2520and%2520Referring%2520Video%2520Object%250ASegmentation%2520%2528RVOS%2529.%2520In%2520this%2520year%252C%2520we%2520replace%2520the%2520classic%2520YouTube-VOS%2520and%250AYouTube-RVOS%2520benchmark%2520with%2520latest%2520datasets%2520MOSE%252C%2520LVOS%252C%2520and%2520MeViS%2520to%2520assess%2520VOS%250Aunder%2520more%2520challenging%2520complex%2520environments.%2520This%2520year%2527s%2520challenge%2520attracted%250A129%2520registered%2520teams%2520from%2520more%2520than%252020%2520institutes%2520across%2520over%25208%2520countries.%2520This%250Areport%2520include%2520the%2520challenge%2520and%2520dataset%2520introduction%252C%2520and%2520the%2520methods%2520used%2520by%250Atop%25207%2520teams%2520in%2520two%2520tracks.%2520More%2520details%2520can%2520be%2520found%2520in%2520our%2520homepage%250Ahttps%253A//lsvos.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSVOS%20Challenge%20Report%3A%20Large-scale%20Complex%20and%20Long%20Video%20Object%0A%20%20Segmentation&entry.906535625=Henghui%20Ding%20and%20Lingyi%20Hong%20and%20Chang%20Liu%20and%20Ning%20Xu%20and%20Linjie%20Yang%20and%20Yuchen%20Fan%20and%20Deshui%20Miao%20and%20Yameng%20Gu%20and%20Xin%20Li%20and%20Zhenyu%20He%20and%20Yaowei%20Wang%20and%20Ming-Hsuan%20Yang%20and%20Jinming%20Chai%20and%20Qin%20Ma%20and%20Junpei%20Zhang%20and%20Licheng%20Jiao%20and%20Fang%20Liu%20and%20Xinyu%20Liu%20and%20Jing%20Zhang%20and%20Kexin%20Zhang%20and%20Xu%20Liu%20and%20LingLing%20Li%20and%20Hao%20Fang%20and%20Feiyu%20Pan%20and%20Xiankai%20Lu%20and%20Wei%20Zhang%20and%20Runmin%20Cong%20and%20Tuyen%20Tran%20and%20Bin%20Cao%20and%20Yisi%20Zhang%20and%20Hanyi%20Wang%20and%20Xingjian%20He%20and%20Jing%20Liu&entry.1292438233=%20%20Despite%20the%20promising%20performance%20of%20current%20video%20segmentation%20models%20on%0Aexisting%20benchmarks%2C%20these%20models%20still%20struggle%20with%20complex%20scenes.%20In%20this%0Apaper%2C%20we%20introduce%20the%206th%20Large-scale%20Video%20Object%20Segmentation%20%28LSVOS%29%0Achallenge%20in%20conjunction%20with%20ECCV%202024%20workshop.%20This%20year%27s%20challenge%0Aincludes%20two%20tasks%3A%20Video%20Object%20Segmentation%20%28VOS%29%20and%20Referring%20Video%20Object%0ASegmentation%20%28RVOS%29.%20In%20this%20year%2C%20we%20replace%20the%20classic%20YouTube-VOS%20and%0AYouTube-RVOS%20benchmark%20with%20latest%20datasets%20MOSE%2C%20LVOS%2C%20and%20MeViS%20to%20assess%20VOS%0Aunder%20more%20challenging%20complex%20environments.%20This%20year%27s%20challenge%20attracted%0A129%20registered%20teams%20from%20more%20than%2020%20institutes%20across%20over%208%20countries.%20This%0Areport%20include%20the%20challenge%20and%20dataset%20introduction%2C%20and%20the%20methods%20used%20by%0Atop%207%20teams%20in%20two%20tracks.%20More%20details%20can%20be%20found%20in%20our%20homepage%0Ahttps%3A//lsvos.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05847v1&entry.124074799=Read"},
{"title": "Robust Loss Functions for Object Grasping under Limited Ground Truth", "author": "Yangfan Deng and Mengyao Zhang and Yong Zhao", "abstract": "  Object grasping is a crucial technology enabling robots to perceive and\ninteract with the environment sufficiently. However, in practical applications,\nresearchers are faced with missing or noisy ground truth while training the\nconvolutional neural network, which decreases the accuracy of the model.\nTherefore, different loss functions are proposed to deal with these problems to\nimprove the accuracy of the neural network. For missing ground truth, a new\npredicted category probability method is defined for unlabeled samples, which\nworks effectively in conjunction with the pseudo-labeling method. Furthermore,\nfor noisy ground truth, a symmetric loss function is introduced to resist the\ncorruption of label noises. The proposed loss functions are powerful, robust,\nand easy to use. Experimental results based on the typical grasping neural\nnetwork show that our method can improve performance by 2 to 13 percent.\n", "link": "http://arxiv.org/abs/2409.05742v1", "date": "2024-09-09", "relevancy": 2.2428, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6057}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Loss%20Functions%20for%20Object%20Grasping%20under%20Limited%20Ground%20Truth&body=Title%3A%20Robust%20Loss%20Functions%20for%20Object%20Grasping%20under%20Limited%20Ground%20Truth%0AAuthor%3A%20Yangfan%20Deng%20and%20Mengyao%20Zhang%20and%20Yong%20Zhao%0AAbstract%3A%20%20%20Object%20grasping%20is%20a%20crucial%20technology%20enabling%20robots%20to%20perceive%20and%0Ainteract%20with%20the%20environment%20sufficiently.%20However%2C%20in%20practical%20applications%2C%0Aresearchers%20are%20faced%20with%20missing%20or%20noisy%20ground%20truth%20while%20training%20the%0Aconvolutional%20neural%20network%2C%20which%20decreases%20the%20accuracy%20of%20the%20model.%0ATherefore%2C%20different%20loss%20functions%20are%20proposed%20to%20deal%20with%20these%20problems%20to%0Aimprove%20the%20accuracy%20of%20the%20neural%20network.%20For%20missing%20ground%20truth%2C%20a%20new%0Apredicted%20category%20probability%20method%20is%20defined%20for%20unlabeled%20samples%2C%20which%0Aworks%20effectively%20in%20conjunction%20with%20the%20pseudo-labeling%20method.%20Furthermore%2C%0Afor%20noisy%20ground%20truth%2C%20a%20symmetric%20loss%20function%20is%20introduced%20to%20resist%20the%0Acorruption%20of%20label%20noises.%20The%20proposed%20loss%20functions%20are%20powerful%2C%20robust%2C%0Aand%20easy%20to%20use.%20Experimental%20results%20based%20on%20the%20typical%20grasping%20neural%0Anetwork%20show%20that%20our%20method%20can%20improve%20performance%20by%202%20to%2013%20percent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Loss%2520Functions%2520for%2520Object%2520Grasping%2520under%2520Limited%2520Ground%2520Truth%26entry.906535625%3DYangfan%2520Deng%2520and%2520Mengyao%2520Zhang%2520and%2520Yong%2520Zhao%26entry.1292438233%3D%2520%2520Object%2520grasping%2520is%2520a%2520crucial%2520technology%2520enabling%2520robots%2520to%2520perceive%2520and%250Ainteract%2520with%2520the%2520environment%2520sufficiently.%2520However%252C%2520in%2520practical%2520applications%252C%250Aresearchers%2520are%2520faced%2520with%2520missing%2520or%2520noisy%2520ground%2520truth%2520while%2520training%2520the%250Aconvolutional%2520neural%2520network%252C%2520which%2520decreases%2520the%2520accuracy%2520of%2520the%2520model.%250ATherefore%252C%2520different%2520loss%2520functions%2520are%2520proposed%2520to%2520deal%2520with%2520these%2520problems%2520to%250Aimprove%2520the%2520accuracy%2520of%2520the%2520neural%2520network.%2520For%2520missing%2520ground%2520truth%252C%2520a%2520new%250Apredicted%2520category%2520probability%2520method%2520is%2520defined%2520for%2520unlabeled%2520samples%252C%2520which%250Aworks%2520effectively%2520in%2520conjunction%2520with%2520the%2520pseudo-labeling%2520method.%2520Furthermore%252C%250Afor%2520noisy%2520ground%2520truth%252C%2520a%2520symmetric%2520loss%2520function%2520is%2520introduced%2520to%2520resist%2520the%250Acorruption%2520of%2520label%2520noises.%2520The%2520proposed%2520loss%2520functions%2520are%2520powerful%252C%2520robust%252C%250Aand%2520easy%2520to%2520use.%2520Experimental%2520results%2520based%2520on%2520the%2520typical%2520grasping%2520neural%250Anetwork%2520show%2520that%2520our%2520method%2520can%2520improve%2520performance%2520by%25202%2520to%252013%2520percent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Loss%20Functions%20for%20Object%20Grasping%20under%20Limited%20Ground%20Truth&entry.906535625=Yangfan%20Deng%20and%20Mengyao%20Zhang%20and%20Yong%20Zhao&entry.1292438233=%20%20Object%20grasping%20is%20a%20crucial%20technology%20enabling%20robots%20to%20perceive%20and%0Ainteract%20with%20the%20environment%20sufficiently.%20However%2C%20in%20practical%20applications%2C%0Aresearchers%20are%20faced%20with%20missing%20or%20noisy%20ground%20truth%20while%20training%20the%0Aconvolutional%20neural%20network%2C%20which%20decreases%20the%20accuracy%20of%20the%20model.%0ATherefore%2C%20different%20loss%20functions%20are%20proposed%20to%20deal%20with%20these%20problems%20to%0Aimprove%20the%20accuracy%20of%20the%20neural%20network.%20For%20missing%20ground%20truth%2C%20a%20new%0Apredicted%20category%20probability%20method%20is%20defined%20for%20unlabeled%20samples%2C%20which%0Aworks%20effectively%20in%20conjunction%20with%20the%20pseudo-labeling%20method.%20Furthermore%2C%0Afor%20noisy%20ground%20truth%2C%20a%20symmetric%20loss%20function%20is%20introduced%20to%20resist%20the%0Acorruption%20of%20label%20noises.%20The%20proposed%20loss%20functions%20are%20powerful%2C%20robust%2C%0Aand%20easy%20to%20use.%20Experimental%20results%20based%20on%20the%20typical%20grasping%20neural%0Anetwork%20show%20that%20our%20method%20can%20improve%20performance%20by%202%20to%2013%20percent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05742v1&entry.124074799=Read"},
{"title": "Adapted-MoE: Mixture of Experts with Test-Time Adaption for Anomaly\n  Detection", "author": "Tianwu Lei and Silin Chen and Bohan Wang and Zhengkai Jiang and Ningmu Zou", "abstract": "  Most unsupervised anomaly detection methods based on representations of\nnormal samples to distinguish anomalies have recently made remarkable progress.\nHowever, existing methods only learn a single decision boundary for\ndistinguishing the samples within the training dataset, neglecting the\nvariation in feature distribution for normal samples even in the same category\nin the real world. Furthermore, it was not considered that a distribution bias\nstill exists between the test set and the train set. Therefore, we propose an\nAdapted-MoE which contains a routing network and a series of expert models to\nhandle multiple distributions of same-category samples by divide and conquer.\nSpecifically, we propose a routing network based on representation learning to\nroute same-category samples into the subclasses feature space. Then, a series\nof expert models are utilized to learn the representation of various normal\nsamples and construct several independent decision boundaries. We propose the\ntest-time adaption to eliminate the bias between the unseen test sample\nrepresentation and the feature distribution learned by the expert model. Our\nexperiments are conducted on a dataset that provides multiple subclasses from\nthree categories, namely Texture AD benchmark. The Adapted-MoE significantly\nimproves the performance of the baseline model, achieving 2.18%-7.20% and\n1.57%-16.30% increase in I-AUROC and P-AUROC, which outperforms the current\nstate-of-the-art methods. Our code is available at https://github.com/.\n", "link": "http://arxiv.org/abs/2409.05611v1", "date": "2024-09-09", "relevancy": 2.2397, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5412}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapted-MoE%3A%20Mixture%20of%20Experts%20with%20Test-Time%20Adaption%20for%20Anomaly%0A%20%20Detection&body=Title%3A%20Adapted-MoE%3A%20Mixture%20of%20Experts%20with%20Test-Time%20Adaption%20for%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Tianwu%20Lei%20and%20Silin%20Chen%20and%20Bohan%20Wang%20and%20Zhengkai%20Jiang%20and%20Ningmu%20Zou%0AAbstract%3A%20%20%20Most%20unsupervised%20anomaly%20detection%20methods%20based%20on%20representations%20of%0Anormal%20samples%20to%20distinguish%20anomalies%20have%20recently%20made%20remarkable%20progress.%0AHowever%2C%20existing%20methods%20only%20learn%20a%20single%20decision%20boundary%20for%0Adistinguishing%20the%20samples%20within%20the%20training%20dataset%2C%20neglecting%20the%0Avariation%20in%20feature%20distribution%20for%20normal%20samples%20even%20in%20the%20same%20category%0Ain%20the%20real%20world.%20Furthermore%2C%20it%20was%20not%20considered%20that%20a%20distribution%20bias%0Astill%20exists%20between%20the%20test%20set%20and%20the%20train%20set.%20Therefore%2C%20we%20propose%20an%0AAdapted-MoE%20which%20contains%20a%20routing%20network%20and%20a%20series%20of%20expert%20models%20to%0Ahandle%20multiple%20distributions%20of%20same-category%20samples%20by%20divide%20and%20conquer.%0ASpecifically%2C%20we%20propose%20a%20routing%20network%20based%20on%20representation%20learning%20to%0Aroute%20same-category%20samples%20into%20the%20subclasses%20feature%20space.%20Then%2C%20a%20series%0Aof%20expert%20models%20are%20utilized%20to%20learn%20the%20representation%20of%20various%20normal%0Asamples%20and%20construct%20several%20independent%20decision%20boundaries.%20We%20propose%20the%0Atest-time%20adaption%20to%20eliminate%20the%20bias%20between%20the%20unseen%20test%20sample%0Arepresentation%20and%20the%20feature%20distribution%20learned%20by%20the%20expert%20model.%20Our%0Aexperiments%20are%20conducted%20on%20a%20dataset%20that%20provides%20multiple%20subclasses%20from%0Athree%20categories%2C%20namely%20Texture%20AD%20benchmark.%20The%20Adapted-MoE%20significantly%0Aimproves%20the%20performance%20of%20the%20baseline%20model%2C%20achieving%202.18%25-7.20%25%20and%0A1.57%25-16.30%25%20increase%20in%20I-AUROC%20and%20P-AUROC%2C%20which%20outperforms%20the%20current%0Astate-of-the-art%20methods.%20Our%20code%20is%20available%20at%20https%3A//github.com/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapted-MoE%253A%2520Mixture%2520of%2520Experts%2520with%2520Test-Time%2520Adaption%2520for%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DTianwu%2520Lei%2520and%2520Silin%2520Chen%2520and%2520Bohan%2520Wang%2520and%2520Zhengkai%2520Jiang%2520and%2520Ningmu%2520Zou%26entry.1292438233%3D%2520%2520Most%2520unsupervised%2520anomaly%2520detection%2520methods%2520based%2520on%2520representations%2520of%250Anormal%2520samples%2520to%2520distinguish%2520anomalies%2520have%2520recently%2520made%2520remarkable%2520progress.%250AHowever%252C%2520existing%2520methods%2520only%2520learn%2520a%2520single%2520decision%2520boundary%2520for%250Adistinguishing%2520the%2520samples%2520within%2520the%2520training%2520dataset%252C%2520neglecting%2520the%250Avariation%2520in%2520feature%2520distribution%2520for%2520normal%2520samples%2520even%2520in%2520the%2520same%2520category%250Ain%2520the%2520real%2520world.%2520Furthermore%252C%2520it%2520was%2520not%2520considered%2520that%2520a%2520distribution%2520bias%250Astill%2520exists%2520between%2520the%2520test%2520set%2520and%2520the%2520train%2520set.%2520Therefore%252C%2520we%2520propose%2520an%250AAdapted-MoE%2520which%2520contains%2520a%2520routing%2520network%2520and%2520a%2520series%2520of%2520expert%2520models%2520to%250Ahandle%2520multiple%2520distributions%2520of%2520same-category%2520samples%2520by%2520divide%2520and%2520conquer.%250ASpecifically%252C%2520we%2520propose%2520a%2520routing%2520network%2520based%2520on%2520representation%2520learning%2520to%250Aroute%2520same-category%2520samples%2520into%2520the%2520subclasses%2520feature%2520space.%2520Then%252C%2520a%2520series%250Aof%2520expert%2520models%2520are%2520utilized%2520to%2520learn%2520the%2520representation%2520of%2520various%2520normal%250Asamples%2520and%2520construct%2520several%2520independent%2520decision%2520boundaries.%2520We%2520propose%2520the%250Atest-time%2520adaption%2520to%2520eliminate%2520the%2520bias%2520between%2520the%2520unseen%2520test%2520sample%250Arepresentation%2520and%2520the%2520feature%2520distribution%2520learned%2520by%2520the%2520expert%2520model.%2520Our%250Aexperiments%2520are%2520conducted%2520on%2520a%2520dataset%2520that%2520provides%2520multiple%2520subclasses%2520from%250Athree%2520categories%252C%2520namely%2520Texture%2520AD%2520benchmark.%2520The%2520Adapted-MoE%2520significantly%250Aimproves%2520the%2520performance%2520of%2520the%2520baseline%2520model%252C%2520achieving%25202.18%2525-7.20%2525%2520and%250A1.57%2525-16.30%2525%2520increase%2520in%2520I-AUROC%2520and%2520P-AUROC%252C%2520which%2520outperforms%2520the%2520current%250Astate-of-the-art%2520methods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapted-MoE%3A%20Mixture%20of%20Experts%20with%20Test-Time%20Adaption%20for%20Anomaly%0A%20%20Detection&entry.906535625=Tianwu%20Lei%20and%20Silin%20Chen%20and%20Bohan%20Wang%20and%20Zhengkai%20Jiang%20and%20Ningmu%20Zou&entry.1292438233=%20%20Most%20unsupervised%20anomaly%20detection%20methods%20based%20on%20representations%20of%0Anormal%20samples%20to%20distinguish%20anomalies%20have%20recently%20made%20remarkable%20progress.%0AHowever%2C%20existing%20methods%20only%20learn%20a%20single%20decision%20boundary%20for%0Adistinguishing%20the%20samples%20within%20the%20training%20dataset%2C%20neglecting%20the%0Avariation%20in%20feature%20distribution%20for%20normal%20samples%20even%20in%20the%20same%20category%0Ain%20the%20real%20world.%20Furthermore%2C%20it%20was%20not%20considered%20that%20a%20distribution%20bias%0Astill%20exists%20between%20the%20test%20set%20and%20the%20train%20set.%20Therefore%2C%20we%20propose%20an%0AAdapted-MoE%20which%20contains%20a%20routing%20network%20and%20a%20series%20of%20expert%20models%20to%0Ahandle%20multiple%20distributions%20of%20same-category%20samples%20by%20divide%20and%20conquer.%0ASpecifically%2C%20we%20propose%20a%20routing%20network%20based%20on%20representation%20learning%20to%0Aroute%20same-category%20samples%20into%20the%20subclasses%20feature%20space.%20Then%2C%20a%20series%0Aof%20expert%20models%20are%20utilized%20to%20learn%20the%20representation%20of%20various%20normal%0Asamples%20and%20construct%20several%20independent%20decision%20boundaries.%20We%20propose%20the%0Atest-time%20adaption%20to%20eliminate%20the%20bias%20between%20the%20unseen%20test%20sample%0Arepresentation%20and%20the%20feature%20distribution%20learned%20by%20the%20expert%20model.%20Our%0Aexperiments%20are%20conducted%20on%20a%20dataset%20that%20provides%20multiple%20subclasses%20from%0Athree%20categories%2C%20namely%20Texture%20AD%20benchmark.%20The%20Adapted-MoE%20significantly%0Aimproves%20the%20performance%20of%20the%20baseline%20model%2C%20achieving%202.18%25-7.20%25%20and%0A1.57%25-16.30%25%20increase%20in%20I-AUROC%20and%20P-AUROC%2C%20which%20outperforms%20the%20current%0Astate-of-the-art%20methods.%20Our%20code%20is%20available%20at%20https%3A//github.com/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05611v1&entry.124074799=Read"},
{"title": "FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo\n  Embeddings", "author": "Zhen Wang and Da Li and Yulin Su and Min Yang and Minghui Qiu and Walton Wang", "abstract": "  Logo embedding models convert the product logos in images into vectors,\nenabling their utilization for logo recognition and detection within e-commerce\nplatforms. This facilitates the enforcement of intellectual property rights and\nenhances product search capabilities. However, current methods treat logo\nembedding as a purely visual problem. A noteworthy issue is that visual models\ncapture features more than logos. Instead, we view this as a multimodal task,\nusing text as auxiliary information to facilitate the visual model's\nunderstanding of the logo. The emerging Multimodal Large Language Models\n(MLLMs) have demonstrated remarkable capabilities in both visual and textual\nunderstanding. Inspired by this, we propose an approach, \\textbf{FashionLOGO},\nto explore how to prompt MLLMs to generate appropriate text for product images,\nwhich can help visual models achieve better logo embeddings. We adopt a\ncross-attention transformer block that enables visual embedding to\nautomatically learn supplementary knowledge from textual embedding. Our\nextensive experiments on real-world datasets prove that FashionLOGO is capable\nof generating generic and robust logo embeddings, achieving state-of-the-art\nperformance in all benchmarks.\n", "link": "http://arxiv.org/abs/2308.09012v2", "date": "2024-09-09", "relevancy": 2.2266, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6642}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.544}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FashionLOGO%3A%20Prompting%20Multimodal%20Large%20Language%20Models%20for%20Fashion%20Logo%0A%20%20Embeddings&body=Title%3A%20FashionLOGO%3A%20Prompting%20Multimodal%20Large%20Language%20Models%20for%20Fashion%20Logo%0A%20%20Embeddings%0AAuthor%3A%20Zhen%20Wang%20and%20Da%20Li%20and%20Yulin%20Su%20and%20Min%20Yang%20and%20Minghui%20Qiu%20and%20Walton%20Wang%0AAbstract%3A%20%20%20Logo%20embedding%20models%20convert%20the%20product%20logos%20in%20images%20into%20vectors%2C%0Aenabling%20their%20utilization%20for%20logo%20recognition%20and%20detection%20within%20e-commerce%0Aplatforms.%20This%20facilitates%20the%20enforcement%20of%20intellectual%20property%20rights%20and%0Aenhances%20product%20search%20capabilities.%20However%2C%20current%20methods%20treat%20logo%0Aembedding%20as%20a%20purely%20visual%20problem.%20A%20noteworthy%20issue%20is%20that%20visual%20models%0Acapture%20features%20more%20than%20logos.%20Instead%2C%20we%20view%20this%20as%20a%20multimodal%20task%2C%0Ausing%20text%20as%20auxiliary%20information%20to%20facilitate%20the%20visual%20model%27s%0Aunderstanding%20of%20the%20logo.%20The%20emerging%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20both%20visual%20and%20textual%0Aunderstanding.%20Inspired%20by%20this%2C%20we%20propose%20an%20approach%2C%20%5Ctextbf%7BFashionLOGO%7D%2C%0Ato%20explore%20how%20to%20prompt%20MLLMs%20to%20generate%20appropriate%20text%20for%20product%20images%2C%0Awhich%20can%20help%20visual%20models%20achieve%20better%20logo%20embeddings.%20We%20adopt%20a%0Across-attention%20transformer%20block%20that%20enables%20visual%20embedding%20to%0Aautomatically%20learn%20supplementary%20knowledge%20from%20textual%20embedding.%20Our%0Aextensive%20experiments%20on%20real-world%20datasets%20prove%20that%20FashionLOGO%20is%20capable%0Aof%20generating%20generic%20and%20robust%20logo%20embeddings%2C%20achieving%20state-of-the-art%0Aperformance%20in%20all%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFashionLOGO%253A%2520Prompting%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Fashion%2520Logo%250A%2520%2520Embeddings%26entry.906535625%3DZhen%2520Wang%2520and%2520Da%2520Li%2520and%2520Yulin%2520Su%2520and%2520Min%2520Yang%2520and%2520Minghui%2520Qiu%2520and%2520Walton%2520Wang%26entry.1292438233%3D%2520%2520Logo%2520embedding%2520models%2520convert%2520the%2520product%2520logos%2520in%2520images%2520into%2520vectors%252C%250Aenabling%2520their%2520utilization%2520for%2520logo%2520recognition%2520and%2520detection%2520within%2520e-commerce%250Aplatforms.%2520This%2520facilitates%2520the%2520enforcement%2520of%2520intellectual%2520property%2520rights%2520and%250Aenhances%2520product%2520search%2520capabilities.%2520However%252C%2520current%2520methods%2520treat%2520logo%250Aembedding%2520as%2520a%2520purely%2520visual%2520problem.%2520A%2520noteworthy%2520issue%2520is%2520that%2520visual%2520models%250Acapture%2520features%2520more%2520than%2520logos.%2520Instead%252C%2520we%2520view%2520this%2520as%2520a%2520multimodal%2520task%252C%250Ausing%2520text%2520as%2520auxiliary%2520information%2520to%2520facilitate%2520the%2520visual%2520model%2527s%250Aunderstanding%2520of%2520the%2520logo.%2520The%2520emerging%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520both%2520visual%2520and%2520textual%250Aunderstanding.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520an%2520approach%252C%2520%255Ctextbf%257BFashionLOGO%257D%252C%250Ato%2520explore%2520how%2520to%2520prompt%2520MLLMs%2520to%2520generate%2520appropriate%2520text%2520for%2520product%2520images%252C%250Awhich%2520can%2520help%2520visual%2520models%2520achieve%2520better%2520logo%2520embeddings.%2520We%2520adopt%2520a%250Across-attention%2520transformer%2520block%2520that%2520enables%2520visual%2520embedding%2520to%250Aautomatically%2520learn%2520supplementary%2520knowledge%2520from%2520textual%2520embedding.%2520Our%250Aextensive%2520experiments%2520on%2520real-world%2520datasets%2520prove%2520that%2520FashionLOGO%2520is%2520capable%250Aof%2520generating%2520generic%2520and%2520robust%2520logo%2520embeddings%252C%2520achieving%2520state-of-the-art%250Aperformance%2520in%2520all%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FashionLOGO%3A%20Prompting%20Multimodal%20Large%20Language%20Models%20for%20Fashion%20Logo%0A%20%20Embeddings&entry.906535625=Zhen%20Wang%20and%20Da%20Li%20and%20Yulin%20Su%20and%20Min%20Yang%20and%20Minghui%20Qiu%20and%20Walton%20Wang&entry.1292438233=%20%20Logo%20embedding%20models%20convert%20the%20product%20logos%20in%20images%20into%20vectors%2C%0Aenabling%20their%20utilization%20for%20logo%20recognition%20and%20detection%20within%20e-commerce%0Aplatforms.%20This%20facilitates%20the%20enforcement%20of%20intellectual%20property%20rights%20and%0Aenhances%20product%20search%20capabilities.%20However%2C%20current%20methods%20treat%20logo%0Aembedding%20as%20a%20purely%20visual%20problem.%20A%20noteworthy%20issue%20is%20that%20visual%20models%0Acapture%20features%20more%20than%20logos.%20Instead%2C%20we%20view%20this%20as%20a%20multimodal%20task%2C%0Ausing%20text%20as%20auxiliary%20information%20to%20facilitate%20the%20visual%20model%27s%0Aunderstanding%20of%20the%20logo.%20The%20emerging%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20both%20visual%20and%20textual%0Aunderstanding.%20Inspired%20by%20this%2C%20we%20propose%20an%20approach%2C%20%5Ctextbf%7BFashionLOGO%7D%2C%0Ato%20explore%20how%20to%20prompt%20MLLMs%20to%20generate%20appropriate%20text%20for%20product%20images%2C%0Awhich%20can%20help%20visual%20models%20achieve%20better%20logo%20embeddings.%20We%20adopt%20a%0Across-attention%20transformer%20block%20that%20enables%20visual%20embedding%20to%0Aautomatically%20learn%20supplementary%20knowledge%20from%20textual%20embedding.%20Our%0Aextensive%20experiments%20on%20real-world%20datasets%20prove%20that%20FashionLOGO%20is%20capable%0Aof%20generating%20generic%20and%20robust%20logo%20embeddings%2C%20achieving%20state-of-the-art%0Aperformance%20in%20all%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09012v2&entry.124074799=Read"},
{"title": "LEROjD: Lidar Extended Radar-Only Object Detection", "author": "Patrick Palmer and Martin Kr\u00fcger and Stefan Sch\u00fctte and Richard Altendorfer and Ganesh Adam and Torsten Bertram", "abstract": "  Accurate 3D object detection is vital for automated driving. While lidar\nsensors are well suited for this task, they are expensive and have limitations\nin adverse weather conditions. 3+1D imaging radar sensors offer a\ncost-effective, robust alternative but face challenges due to their low\nresolution and high measurement noise. Existing 3+1D imaging radar datasets\ninclude radar and lidar data, enabling cross-modal model improvements. Although\nlidar should not be used during inference, it can aid the training of\nradar-only object detectors. We explore two strategies to transfer knowledge\nfrom the lidar to the radar domain and radar-only object detectors: 1.\nmulti-stage training with sequential lidar point cloud thin-out, and 2.\ncross-modal knowledge distillation. In the multi-stage process, three thin-out\nmethods are examined. Our results show significant performance gains of up to\n4.2 percentage points in mean Average Precision with multi-stage training and\nup to 3.9 percentage points with knowledge distillation by initializing the\nstudent with the teacher's weights. The main benefit of these approaches is\ntheir applicability to other 3D object detection networks without altering\ntheir architecture, as we show by analyzing it on two different object\ndetectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd\n", "link": "http://arxiv.org/abs/2409.05564v1", "date": "2024-09-09", "relevancy": 2.2263, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.563}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5567}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEROjD%3A%20Lidar%20Extended%20Radar-Only%20Object%20Detection&body=Title%3A%20LEROjD%3A%20Lidar%20Extended%20Radar-Only%20Object%20Detection%0AAuthor%3A%20Patrick%20Palmer%20and%20Martin%20Kr%C3%BCger%20and%20Stefan%20Sch%C3%BCtte%20and%20Richard%20Altendorfer%20and%20Ganesh%20Adam%20and%20Torsten%20Bertram%0AAbstract%3A%20%20%20Accurate%203D%20object%20detection%20is%20vital%20for%20automated%20driving.%20While%20lidar%0Asensors%20are%20well%20suited%20for%20this%20task%2C%20they%20are%20expensive%20and%20have%20limitations%0Ain%20adverse%20weather%20conditions.%203%2B1D%20imaging%20radar%20sensors%20offer%20a%0Acost-effective%2C%20robust%20alternative%20but%20face%20challenges%20due%20to%20their%20low%0Aresolution%20and%20high%20measurement%20noise.%20Existing%203%2B1D%20imaging%20radar%20datasets%0Ainclude%20radar%20and%20lidar%20data%2C%20enabling%20cross-modal%20model%20improvements.%20Although%0Alidar%20should%20not%20be%20used%20during%20inference%2C%20it%20can%20aid%20the%20training%20of%0Aradar-only%20object%20detectors.%20We%20explore%20two%20strategies%20to%20transfer%20knowledge%0Afrom%20the%20lidar%20to%20the%20radar%20domain%20and%20radar-only%20object%20detectors%3A%201.%0Amulti-stage%20training%20with%20sequential%20lidar%20point%20cloud%20thin-out%2C%20and%202.%0Across-modal%20knowledge%20distillation.%20In%20the%20multi-stage%20process%2C%20three%20thin-out%0Amethods%20are%20examined.%20Our%20results%20show%20significant%20performance%20gains%20of%20up%20to%0A4.2%20percentage%20points%20in%20mean%20Average%20Precision%20with%20multi-stage%20training%20and%0Aup%20to%203.9%20percentage%20points%20with%20knowledge%20distillation%20by%20initializing%20the%0Astudent%20with%20the%20teacher%27s%20weights.%20The%20main%20benefit%20of%20these%20approaches%20is%0Atheir%20applicability%20to%20other%203D%20object%20detection%20networks%20without%20altering%0Atheir%20architecture%2C%20as%20we%20show%20by%20analyzing%20it%20on%20two%20different%20object%0Adetectors.%20Our%20code%20is%20available%20at%20https%3A//github.com/rst-tu-dortmund/lerojd%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEROjD%253A%2520Lidar%2520Extended%2520Radar-Only%2520Object%2520Detection%26entry.906535625%3DPatrick%2520Palmer%2520and%2520Martin%2520Kr%25C3%25BCger%2520and%2520Stefan%2520Sch%25C3%25BCtte%2520and%2520Richard%2520Altendorfer%2520and%2520Ganesh%2520Adam%2520and%2520Torsten%2520Bertram%26entry.1292438233%3D%2520%2520Accurate%25203D%2520object%2520detection%2520is%2520vital%2520for%2520automated%2520driving.%2520While%2520lidar%250Asensors%2520are%2520well%2520suited%2520for%2520this%2520task%252C%2520they%2520are%2520expensive%2520and%2520have%2520limitations%250Ain%2520adverse%2520weather%2520conditions.%25203%252B1D%2520imaging%2520radar%2520sensors%2520offer%2520a%250Acost-effective%252C%2520robust%2520alternative%2520but%2520face%2520challenges%2520due%2520to%2520their%2520low%250Aresolution%2520and%2520high%2520measurement%2520noise.%2520Existing%25203%252B1D%2520imaging%2520radar%2520datasets%250Ainclude%2520radar%2520and%2520lidar%2520data%252C%2520enabling%2520cross-modal%2520model%2520improvements.%2520Although%250Alidar%2520should%2520not%2520be%2520used%2520during%2520inference%252C%2520it%2520can%2520aid%2520the%2520training%2520of%250Aradar-only%2520object%2520detectors.%2520We%2520explore%2520two%2520strategies%2520to%2520transfer%2520knowledge%250Afrom%2520the%2520lidar%2520to%2520the%2520radar%2520domain%2520and%2520radar-only%2520object%2520detectors%253A%25201.%250Amulti-stage%2520training%2520with%2520sequential%2520lidar%2520point%2520cloud%2520thin-out%252C%2520and%25202.%250Across-modal%2520knowledge%2520distillation.%2520In%2520the%2520multi-stage%2520process%252C%2520three%2520thin-out%250Amethods%2520are%2520examined.%2520Our%2520results%2520show%2520significant%2520performance%2520gains%2520of%2520up%2520to%250A4.2%2520percentage%2520points%2520in%2520mean%2520Average%2520Precision%2520with%2520multi-stage%2520training%2520and%250Aup%2520to%25203.9%2520percentage%2520points%2520with%2520knowledge%2520distillation%2520by%2520initializing%2520the%250Astudent%2520with%2520the%2520teacher%2527s%2520weights.%2520The%2520main%2520benefit%2520of%2520these%2520approaches%2520is%250Atheir%2520applicability%2520to%2520other%25203D%2520object%2520detection%2520networks%2520without%2520altering%250Atheir%2520architecture%252C%2520as%2520we%2520show%2520by%2520analyzing%2520it%2520on%2520two%2520different%2520object%250Adetectors.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/rst-tu-dortmund/lerojd%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEROjD%3A%20Lidar%20Extended%20Radar-Only%20Object%20Detection&entry.906535625=Patrick%20Palmer%20and%20Martin%20Kr%C3%BCger%20and%20Stefan%20Sch%C3%BCtte%20and%20Richard%20Altendorfer%20and%20Ganesh%20Adam%20and%20Torsten%20Bertram&entry.1292438233=%20%20Accurate%203D%20object%20detection%20is%20vital%20for%20automated%20driving.%20While%20lidar%0Asensors%20are%20well%20suited%20for%20this%20task%2C%20they%20are%20expensive%20and%20have%20limitations%0Ain%20adverse%20weather%20conditions.%203%2B1D%20imaging%20radar%20sensors%20offer%20a%0Acost-effective%2C%20robust%20alternative%20but%20face%20challenges%20due%20to%20their%20low%0Aresolution%20and%20high%20measurement%20noise.%20Existing%203%2B1D%20imaging%20radar%20datasets%0Ainclude%20radar%20and%20lidar%20data%2C%20enabling%20cross-modal%20model%20improvements.%20Although%0Alidar%20should%20not%20be%20used%20during%20inference%2C%20it%20can%20aid%20the%20training%20of%0Aradar-only%20object%20detectors.%20We%20explore%20two%20strategies%20to%20transfer%20knowledge%0Afrom%20the%20lidar%20to%20the%20radar%20domain%20and%20radar-only%20object%20detectors%3A%201.%0Amulti-stage%20training%20with%20sequential%20lidar%20point%20cloud%20thin-out%2C%20and%202.%0Across-modal%20knowledge%20distillation.%20In%20the%20multi-stage%20process%2C%20three%20thin-out%0Amethods%20are%20examined.%20Our%20results%20show%20significant%20performance%20gains%20of%20up%20to%0A4.2%20percentage%20points%20in%20mean%20Average%20Precision%20with%20multi-stage%20training%20and%0Aup%20to%203.9%20percentage%20points%20with%20knowledge%20distillation%20by%20initializing%20the%0Astudent%20with%20the%20teacher%27s%20weights.%20The%20main%20benefit%20of%20these%20approaches%20is%0Atheir%20applicability%20to%20other%203D%20object%20detection%20networks%20without%20altering%0Atheir%20architecture%2C%20as%20we%20show%20by%20analyzing%20it%20on%20two%20different%20object%0Adetectors.%20Our%20code%20is%20available%20at%20https%3A//github.com/rst-tu-dortmund/lerojd%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05564v1&entry.124074799=Read"},
{"title": "CLFT: Camera-LiDAR Fusion Transformer for Semantic Segmentation in\n  Autonomous Driving", "author": "Junyi Gu and Mauro Bellone and Tom\u00e1\u0161 Pivo\u0148ka and Raivo Sell", "abstract": "  Critical research about camera-and-LiDAR-based semantic object segmentation\nfor autonomous driving significantly benefited from the recent development of\ndeep learning. Specifically, the vision transformer is the novel ground-breaker\nthat successfully brought the multi-head-attention mechanism to computer vision\napplications. Therefore, we propose a vision-transformer-based network to carry\nout camera-LiDAR fusion for semantic segmentation applied to autonomous\ndriving. Our proposal uses the novel progressive-assemble strategy of vision\ntransformers on a double-direction network and then integrates the results in a\ncross-fusion strategy over the transformer decoder layers. Unlike other works\nin the literature, our camera-LiDAR fusion transformers have been evaluated in\nchallenging conditions like rain and low illumination, showing robust\nperformance. The paper reports the segmentation results over the vehicle and\nhuman classes in different modalities: camera-only, LiDAR-only, and\ncamera-LiDAR fusion. We perform coherent controlled benchmark experiments of\nCLFT against other networks that are also designed for semantic segmentation.\nThe experiments aim to evaluate the performance of CLFT independently from two\nperspectives: multimodal sensor fusion and backbone architectures. The\nquantitative assessments show our CLFT networks yield an improvement of up to\n10% for challenging dark-wet conditions when comparing with\nFully-Convolutional-Neural-Network-based (FCN) camera-LiDAR fusion neural\nnetwork. Contrasting to the network with transformer backbone but using single\nmodality input, the all-around improvement is 5-10%.\n", "link": "http://arxiv.org/abs/2404.17793v3", "date": "2024-09-09", "relevancy": 2.2218, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLFT%3A%20Camera-LiDAR%20Fusion%20Transformer%20for%20Semantic%20Segmentation%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20CLFT%3A%20Camera-LiDAR%20Fusion%20Transformer%20for%20Semantic%20Segmentation%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Junyi%20Gu%20and%20Mauro%20Bellone%20and%20Tom%C3%A1%C5%A1%20Pivo%C5%88ka%20and%20Raivo%20Sell%0AAbstract%3A%20%20%20Critical%20research%20about%20camera-and-LiDAR-based%20semantic%20object%20segmentation%0Afor%20autonomous%20driving%20significantly%20benefited%20from%20the%20recent%20development%20of%0Adeep%20learning.%20Specifically%2C%20the%20vision%20transformer%20is%20the%20novel%20ground-breaker%0Athat%20successfully%20brought%20the%20multi-head-attention%20mechanism%20to%20computer%20vision%0Aapplications.%20Therefore%2C%20we%20propose%20a%20vision-transformer-based%20network%20to%20carry%0Aout%20camera-LiDAR%20fusion%20for%20semantic%20segmentation%20applied%20to%20autonomous%0Adriving.%20Our%20proposal%20uses%20the%20novel%20progressive-assemble%20strategy%20of%20vision%0Atransformers%20on%20a%20double-direction%20network%20and%20then%20integrates%20the%20results%20in%20a%0Across-fusion%20strategy%20over%20the%20transformer%20decoder%20layers.%20Unlike%20other%20works%0Ain%20the%20literature%2C%20our%20camera-LiDAR%20fusion%20transformers%20have%20been%20evaluated%20in%0Achallenging%20conditions%20like%20rain%20and%20low%20illumination%2C%20showing%20robust%0Aperformance.%20The%20paper%20reports%20the%20segmentation%20results%20over%20the%20vehicle%20and%0Ahuman%20classes%20in%20different%20modalities%3A%20camera-only%2C%20LiDAR-only%2C%20and%0Acamera-LiDAR%20fusion.%20We%20perform%20coherent%20controlled%20benchmark%20experiments%20of%0ACLFT%20against%20other%20networks%20that%20are%20also%20designed%20for%20semantic%20segmentation.%0AThe%20experiments%20aim%20to%20evaluate%20the%20performance%20of%20CLFT%20independently%20from%20two%0Aperspectives%3A%20multimodal%20sensor%20fusion%20and%20backbone%20architectures.%20The%0Aquantitative%20assessments%20show%20our%20CLFT%20networks%20yield%20an%20improvement%20of%20up%20to%0A10%25%20for%20challenging%20dark-wet%20conditions%20when%20comparing%20with%0AFully-Convolutional-Neural-Network-based%20%28FCN%29%20camera-LiDAR%20fusion%20neural%0Anetwork.%20Contrasting%20to%20the%20network%20with%20transformer%20backbone%20but%20using%20single%0Amodality%20input%2C%20the%20all-around%20improvement%20is%205-10%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17793v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLFT%253A%2520Camera-LiDAR%2520Fusion%2520Transformer%2520for%2520Semantic%2520Segmentation%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DJunyi%2520Gu%2520and%2520Mauro%2520Bellone%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Pivo%25C5%2588ka%2520and%2520Raivo%2520Sell%26entry.1292438233%3D%2520%2520Critical%2520research%2520about%2520camera-and-LiDAR-based%2520semantic%2520object%2520segmentation%250Afor%2520autonomous%2520driving%2520significantly%2520benefited%2520from%2520the%2520recent%2520development%2520of%250Adeep%2520learning.%2520Specifically%252C%2520the%2520vision%2520transformer%2520is%2520the%2520novel%2520ground-breaker%250Athat%2520successfully%2520brought%2520the%2520multi-head-attention%2520mechanism%2520to%2520computer%2520vision%250Aapplications.%2520Therefore%252C%2520we%2520propose%2520a%2520vision-transformer-based%2520network%2520to%2520carry%250Aout%2520camera-LiDAR%2520fusion%2520for%2520semantic%2520segmentation%2520applied%2520to%2520autonomous%250Adriving.%2520Our%2520proposal%2520uses%2520the%2520novel%2520progressive-assemble%2520strategy%2520of%2520vision%250Atransformers%2520on%2520a%2520double-direction%2520network%2520and%2520then%2520integrates%2520the%2520results%2520in%2520a%250Across-fusion%2520strategy%2520over%2520the%2520transformer%2520decoder%2520layers.%2520Unlike%2520other%2520works%250Ain%2520the%2520literature%252C%2520our%2520camera-LiDAR%2520fusion%2520transformers%2520have%2520been%2520evaluated%2520in%250Achallenging%2520conditions%2520like%2520rain%2520and%2520low%2520illumination%252C%2520showing%2520robust%250Aperformance.%2520The%2520paper%2520reports%2520the%2520segmentation%2520results%2520over%2520the%2520vehicle%2520and%250Ahuman%2520classes%2520in%2520different%2520modalities%253A%2520camera-only%252C%2520LiDAR-only%252C%2520and%250Acamera-LiDAR%2520fusion.%2520We%2520perform%2520coherent%2520controlled%2520benchmark%2520experiments%2520of%250ACLFT%2520against%2520other%2520networks%2520that%2520are%2520also%2520designed%2520for%2520semantic%2520segmentation.%250AThe%2520experiments%2520aim%2520to%2520evaluate%2520the%2520performance%2520of%2520CLFT%2520independently%2520from%2520two%250Aperspectives%253A%2520multimodal%2520sensor%2520fusion%2520and%2520backbone%2520architectures.%2520The%250Aquantitative%2520assessments%2520show%2520our%2520CLFT%2520networks%2520yield%2520an%2520improvement%2520of%2520up%2520to%250A10%2525%2520for%2520challenging%2520dark-wet%2520conditions%2520when%2520comparing%2520with%250AFully-Convolutional-Neural-Network-based%2520%2528FCN%2529%2520camera-LiDAR%2520fusion%2520neural%250Anetwork.%2520Contrasting%2520to%2520the%2520network%2520with%2520transformer%2520backbone%2520but%2520using%2520single%250Amodality%2520input%252C%2520the%2520all-around%2520improvement%2520is%25205-10%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17793v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLFT%3A%20Camera-LiDAR%20Fusion%20Transformer%20for%20Semantic%20Segmentation%20in%0A%20%20Autonomous%20Driving&entry.906535625=Junyi%20Gu%20and%20Mauro%20Bellone%20and%20Tom%C3%A1%C5%A1%20Pivo%C5%88ka%20and%20Raivo%20Sell&entry.1292438233=%20%20Critical%20research%20about%20camera-and-LiDAR-based%20semantic%20object%20segmentation%0Afor%20autonomous%20driving%20significantly%20benefited%20from%20the%20recent%20development%20of%0Adeep%20learning.%20Specifically%2C%20the%20vision%20transformer%20is%20the%20novel%20ground-breaker%0Athat%20successfully%20brought%20the%20multi-head-attention%20mechanism%20to%20computer%20vision%0Aapplications.%20Therefore%2C%20we%20propose%20a%20vision-transformer-based%20network%20to%20carry%0Aout%20camera-LiDAR%20fusion%20for%20semantic%20segmentation%20applied%20to%20autonomous%0Adriving.%20Our%20proposal%20uses%20the%20novel%20progressive-assemble%20strategy%20of%20vision%0Atransformers%20on%20a%20double-direction%20network%20and%20then%20integrates%20the%20results%20in%20a%0Across-fusion%20strategy%20over%20the%20transformer%20decoder%20layers.%20Unlike%20other%20works%0Ain%20the%20literature%2C%20our%20camera-LiDAR%20fusion%20transformers%20have%20been%20evaluated%20in%0Achallenging%20conditions%20like%20rain%20and%20low%20illumination%2C%20showing%20robust%0Aperformance.%20The%20paper%20reports%20the%20segmentation%20results%20over%20the%20vehicle%20and%0Ahuman%20classes%20in%20different%20modalities%3A%20camera-only%2C%20LiDAR-only%2C%20and%0Acamera-LiDAR%20fusion.%20We%20perform%20coherent%20controlled%20benchmark%20experiments%20of%0ACLFT%20against%20other%20networks%20that%20are%20also%20designed%20for%20semantic%20segmentation.%0AThe%20experiments%20aim%20to%20evaluate%20the%20performance%20of%20CLFT%20independently%20from%20two%0Aperspectives%3A%20multimodal%20sensor%20fusion%20and%20backbone%20architectures.%20The%0Aquantitative%20assessments%20show%20our%20CLFT%20networks%20yield%20an%20improvement%20of%20up%20to%0A10%25%20for%20challenging%20dark-wet%20conditions%20when%20comparing%20with%0AFully-Convolutional-Neural-Network-based%20%28FCN%29%20camera-LiDAR%20fusion%20neural%0Anetwork.%20Contrasting%20to%20the%20network%20with%20transformer%20backbone%20but%20using%20single%0Amodality%20input%2C%20the%20all-around%20improvement%20is%205-10%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17793v3&entry.124074799=Read"},
{"title": "Consensus-based Distributed Quantum Kernel Learning for Speech\n  Recognition", "author": "Kuan-Cheng Chen and Wenxuan Ma and Xiaotian Xu", "abstract": "  This paper presents a Consensus-based Distributed Quantum Kernel Learning\n(CDQKL) framework aimed at improving speech recognition through distributed\nquantum computing.CDQKL addresses the challenges of scalability and data\nprivacy in centralized quantum kernel learning. It does this by distributing\ncomputational tasks across quantum terminals, which are connected through\nclassical channels. This approach enables the exchange of model parameters\nwithout sharing local training data, thereby maintaining data privacy and\nenhancing computational efficiency. Experimental evaluations on benchmark\nspeech emotion recognition datasets demonstrate that CDQKL achieves competitive\nclassification accuracy and scalability compared to centralized and local\nquantum kernel learning models. The distributed nature of CDQKL offers\nadvantages in privacy preservation and computational efficiency, making it\nsuitable for data-sensitive fields such as telecommunications, automotive, and\nfinance. The findings suggest that CDQKL can effectively leverage distributed\nquantum computing for large-scale machine-learning tasks.\n", "link": "http://arxiv.org/abs/2409.05770v1", "date": "2024-09-09", "relevancy": 2.2167, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4469}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consensus-based%20Distributed%20Quantum%20Kernel%20Learning%20for%20Speech%0A%20%20Recognition&body=Title%3A%20Consensus-based%20Distributed%20Quantum%20Kernel%20Learning%20for%20Speech%0A%20%20Recognition%0AAuthor%3A%20Kuan-Cheng%20Chen%20and%20Wenxuan%20Ma%20and%20Xiaotian%20Xu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20Consensus-based%20Distributed%20Quantum%20Kernel%20Learning%0A%28CDQKL%29%20framework%20aimed%20at%20improving%20speech%20recognition%20through%20distributed%0Aquantum%20computing.CDQKL%20addresses%20the%20challenges%20of%20scalability%20and%20data%0Aprivacy%20in%20centralized%20quantum%20kernel%20learning.%20It%20does%20this%20by%20distributing%0Acomputational%20tasks%20across%20quantum%20terminals%2C%20which%20are%20connected%20through%0Aclassical%20channels.%20This%20approach%20enables%20the%20exchange%20of%20model%20parameters%0Awithout%20sharing%20local%20training%20data%2C%20thereby%20maintaining%20data%20privacy%20and%0Aenhancing%20computational%20efficiency.%20Experimental%20evaluations%20on%20benchmark%0Aspeech%20emotion%20recognition%20datasets%20demonstrate%20that%20CDQKL%20achieves%20competitive%0Aclassification%20accuracy%20and%20scalability%20compared%20to%20centralized%20and%20local%0Aquantum%20kernel%20learning%20models.%20The%20distributed%20nature%20of%20CDQKL%20offers%0Aadvantages%20in%20privacy%20preservation%20and%20computational%20efficiency%2C%20making%20it%0Asuitable%20for%20data-sensitive%20fields%20such%20as%20telecommunications%2C%20automotive%2C%20and%0Afinance.%20The%20findings%20suggest%20that%20CDQKL%20can%20effectively%20leverage%20distributed%0Aquantum%20computing%20for%20large-scale%20machine-learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsensus-based%2520Distributed%2520Quantum%2520Kernel%2520Learning%2520for%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DKuan-Cheng%2520Chen%2520and%2520Wenxuan%2520Ma%2520and%2520Xiaotian%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520Consensus-based%2520Distributed%2520Quantum%2520Kernel%2520Learning%250A%2528CDQKL%2529%2520framework%2520aimed%2520at%2520improving%2520speech%2520recognition%2520through%2520distributed%250Aquantum%2520computing.CDQKL%2520addresses%2520the%2520challenges%2520of%2520scalability%2520and%2520data%250Aprivacy%2520in%2520centralized%2520quantum%2520kernel%2520learning.%2520It%2520does%2520this%2520by%2520distributing%250Acomputational%2520tasks%2520across%2520quantum%2520terminals%252C%2520which%2520are%2520connected%2520through%250Aclassical%2520channels.%2520This%2520approach%2520enables%2520the%2520exchange%2520of%2520model%2520parameters%250Awithout%2520sharing%2520local%2520training%2520data%252C%2520thereby%2520maintaining%2520data%2520privacy%2520and%250Aenhancing%2520computational%2520efficiency.%2520Experimental%2520evaluations%2520on%2520benchmark%250Aspeech%2520emotion%2520recognition%2520datasets%2520demonstrate%2520that%2520CDQKL%2520achieves%2520competitive%250Aclassification%2520accuracy%2520and%2520scalability%2520compared%2520to%2520centralized%2520and%2520local%250Aquantum%2520kernel%2520learning%2520models.%2520The%2520distributed%2520nature%2520of%2520CDQKL%2520offers%250Aadvantages%2520in%2520privacy%2520preservation%2520and%2520computational%2520efficiency%252C%2520making%2520it%250Asuitable%2520for%2520data-sensitive%2520fields%2520such%2520as%2520telecommunications%252C%2520automotive%252C%2520and%250Afinance.%2520The%2520findings%2520suggest%2520that%2520CDQKL%2520can%2520effectively%2520leverage%2520distributed%250Aquantum%2520computing%2520for%2520large-scale%2520machine-learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consensus-based%20Distributed%20Quantum%20Kernel%20Learning%20for%20Speech%0A%20%20Recognition&entry.906535625=Kuan-Cheng%20Chen%20and%20Wenxuan%20Ma%20and%20Xiaotian%20Xu&entry.1292438233=%20%20This%20paper%20presents%20a%20Consensus-based%20Distributed%20Quantum%20Kernel%20Learning%0A%28CDQKL%29%20framework%20aimed%20at%20improving%20speech%20recognition%20through%20distributed%0Aquantum%20computing.CDQKL%20addresses%20the%20challenges%20of%20scalability%20and%20data%0Aprivacy%20in%20centralized%20quantum%20kernel%20learning.%20It%20does%20this%20by%20distributing%0Acomputational%20tasks%20across%20quantum%20terminals%2C%20which%20are%20connected%20through%0Aclassical%20channels.%20This%20approach%20enables%20the%20exchange%20of%20model%20parameters%0Awithout%20sharing%20local%20training%20data%2C%20thereby%20maintaining%20data%20privacy%20and%0Aenhancing%20computational%20efficiency.%20Experimental%20evaluations%20on%20benchmark%0Aspeech%20emotion%20recognition%20datasets%20demonstrate%20that%20CDQKL%20achieves%20competitive%0Aclassification%20accuracy%20and%20scalability%20compared%20to%20centralized%20and%20local%0Aquantum%20kernel%20learning%20models.%20The%20distributed%20nature%20of%20CDQKL%20offers%0Aadvantages%20in%20privacy%20preservation%20and%20computational%20efficiency%2C%20making%20it%0Asuitable%20for%20data-sensitive%20fields%20such%20as%20telecommunications%2C%20automotive%2C%20and%0Afinance.%20The%20findings%20suggest%20that%20CDQKL%20can%20effectively%20leverage%20distributed%0Aquantum%20computing%20for%20large-scale%20machine-learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05770v1&entry.124074799=Read"},
{"title": "Question-Answering Dense Video Events", "author": "Hangyu Qin and Junbin Xiao and Angela Yao", "abstract": "  Multimodal Large Language Models (MLLMs) have shown excellent performance in\nquestion-answering of single-event videos. In this paper, we present\nquestion-answering dense video events, a novel task that requires answering and\ngrounding the dense-event questions in long videos, thus challenging MLLMs to\nfaithfully comprehend and reason about multiple events occurring over extended\ntime periods. To facilitate the study, we construct DeVE-QA - a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. We then\nbenchmark and show that existing MLLMs excelling at single-event QA struggle to\nperform well in DeVE-QA. For improvement, we propose DeVi, a novel\ntraining-free MLLM approach that highlights a hierarchical captioning module, a\ntemporal event memory module, and a self-consistency checking module to\nrespectively detect, contextualize and memorize, and ground dense-events in\nlong videos for question answering. Extensive experiments show that DeVi is\nsuperior at answering dense-event questions and grounding relevant video\nmoments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1\npercent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA\nrespectively.\n", "link": "http://arxiv.org/abs/2409.04388v2", "date": "2024-09-09", "relevancy": 2.211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Question-Answering%20Dense%20Video%20Events&body=Title%3A%20Question-Answering%20Dense%20Video%20Events%0AAuthor%3A%20Hangyu%20Qin%20and%20Junbin%20Xiao%20and%20Angela%20Yao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20excellent%20performance%20in%0Aquestion-answering%20of%20single-event%20videos.%20In%20this%20paper%2C%20we%20present%0Aquestion-answering%20dense%20video%20events%2C%20a%20novel%20task%20that%20requires%20answering%20and%0Agrounding%20the%20dense-event%20questions%20in%20long%20videos%2C%20thus%20challenging%20MLLMs%20to%0Afaithfully%20comprehend%20and%20reason%20about%20multiple%20events%20occurring%20over%20extended%0Atime%20periods.%20To%20facilitate%20the%20study%2C%20we%20construct%20DeVE-QA%20-%20a%20dataset%0Afeaturing%2078K%20questions%20about%2026K%20events%20on%2010.6K%20long%20videos.%20We%20then%0Abenchmark%20and%20show%20that%20existing%20MLLMs%20excelling%20at%20single-event%20QA%20struggle%20to%0Aperform%20well%20in%20DeVE-QA.%20For%20improvement%2C%20we%20propose%20DeVi%2C%20a%20novel%0Atraining-free%20MLLM%20approach%20that%20highlights%20a%20hierarchical%20captioning%20module%2C%20a%0Atemporal%20event%20memory%20module%2C%20and%20a%20self-consistency%20checking%20module%20to%0Arespectively%20detect%2C%20contextualize%20and%20memorize%2C%20and%20ground%20dense-events%20in%0Along%20videos%20for%20question%20answering.%20Extensive%20experiments%20show%20that%20DeVi%20is%0Asuperior%20at%20answering%20dense-event%20questions%20and%20grounding%20relevant%20video%0Amoments.%20Compared%20with%20existing%20MLLMs%2C%20it%20achieves%20a%20remarkable%20increase%20of%204.1%0Apercent%20and%203.7%20percent%20for%20G%28round%29QA%20accuracy%20on%20DeVE-QA%20and%20NExT-GQA%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestion-Answering%2520Dense%2520Video%2520Events%26entry.906535625%3DHangyu%2520Qin%2520and%2520Junbin%2520Xiao%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520excellent%2520performance%2520in%250Aquestion-answering%2520of%2520single-event%2520videos.%2520In%2520this%2520paper%252C%2520we%2520present%250Aquestion-answering%2520dense%2520video%2520events%252C%2520a%2520novel%2520task%2520that%2520requires%2520answering%2520and%250Agrounding%2520the%2520dense-event%2520questions%2520in%2520long%2520videos%252C%2520thus%2520challenging%2520MLLMs%2520to%250Afaithfully%2520comprehend%2520and%2520reason%2520about%2520multiple%2520events%2520occurring%2520over%2520extended%250Atime%2520periods.%2520To%2520facilitate%2520the%2520study%252C%2520we%2520construct%2520DeVE-QA%2520-%2520a%2520dataset%250Afeaturing%252078K%2520questions%2520about%252026K%2520events%2520on%252010.6K%2520long%2520videos.%2520We%2520then%250Abenchmark%2520and%2520show%2520that%2520existing%2520MLLMs%2520excelling%2520at%2520single-event%2520QA%2520struggle%2520to%250Aperform%2520well%2520in%2520DeVE-QA.%2520For%2520improvement%252C%2520we%2520propose%2520DeVi%252C%2520a%2520novel%250Atraining-free%2520MLLM%2520approach%2520that%2520highlights%2520a%2520hierarchical%2520captioning%2520module%252C%2520a%250Atemporal%2520event%2520memory%2520module%252C%2520and%2520a%2520self-consistency%2520checking%2520module%2520to%250Arespectively%2520detect%252C%2520contextualize%2520and%2520memorize%252C%2520and%2520ground%2520dense-events%2520in%250Along%2520videos%2520for%2520question%2520answering.%2520Extensive%2520experiments%2520show%2520that%2520DeVi%2520is%250Asuperior%2520at%2520answering%2520dense-event%2520questions%2520and%2520grounding%2520relevant%2520video%250Amoments.%2520Compared%2520with%2520existing%2520MLLMs%252C%2520it%2520achieves%2520a%2520remarkable%2520increase%2520of%25204.1%250Apercent%2520and%25203.7%2520percent%2520for%2520G%2528round%2529QA%2520accuracy%2520on%2520DeVE-QA%2520and%2520NExT-GQA%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Question-Answering%20Dense%20Video%20Events&entry.906535625=Hangyu%20Qin%20and%20Junbin%20Xiao%20and%20Angela%20Yao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20excellent%20performance%20in%0Aquestion-answering%20of%20single-event%20videos.%20In%20this%20paper%2C%20we%20present%0Aquestion-answering%20dense%20video%20events%2C%20a%20novel%20task%20that%20requires%20answering%20and%0Agrounding%20the%20dense-event%20questions%20in%20long%20videos%2C%20thus%20challenging%20MLLMs%20to%0Afaithfully%20comprehend%20and%20reason%20about%20multiple%20events%20occurring%20over%20extended%0Atime%20periods.%20To%20facilitate%20the%20study%2C%20we%20construct%20DeVE-QA%20-%20a%20dataset%0Afeaturing%2078K%20questions%20about%2026K%20events%20on%2010.6K%20long%20videos.%20We%20then%0Abenchmark%20and%20show%20that%20existing%20MLLMs%20excelling%20at%20single-event%20QA%20struggle%20to%0Aperform%20well%20in%20DeVE-QA.%20For%20improvement%2C%20we%20propose%20DeVi%2C%20a%20novel%0Atraining-free%20MLLM%20approach%20that%20highlights%20a%20hierarchical%20captioning%20module%2C%20a%0Atemporal%20event%20memory%20module%2C%20and%20a%20self-consistency%20checking%20module%20to%0Arespectively%20detect%2C%20contextualize%20and%20memorize%2C%20and%20ground%20dense-events%20in%0Along%20videos%20for%20question%20answering.%20Extensive%20experiments%20show%20that%20DeVi%20is%0Asuperior%20at%20answering%20dense-event%20questions%20and%20grounding%20relevant%20video%0Amoments.%20Compared%20with%20existing%20MLLMs%2C%20it%20achieves%20a%20remarkable%20increase%20of%204.1%0Apercent%20and%203.7%20percent%20for%20G%28round%29QA%20accuracy%20on%20DeVE-QA%20and%20NExT-GQA%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04388v2&entry.124074799=Read"},
{"title": "Clustering Time-Evolving Networks Using the Spatio-Temporal Graph\n  Laplacian", "author": "Maia Trower and Nata\u0161a Djurdjevac Conrad and Stefan Klus", "abstract": "  Time-evolving graphs arise frequently when modeling complex dynamical systems\nsuch as social networks, traffic flow, and biological processes. Developing\ntechniques to identify and analyze communities in these time-varying graph\nstructures is an important challenge. In this work, we generalize existing\nspectral clustering algorithms from static to dynamic graphs using canonical\ncorrelation analysis (CCA) to capture the temporal evolution of clusters. Based\non this extended canonical correlation framework, we define the spatio-temporal\ngraph Laplacian and investigate its spectral properties. We connect these\nconcepts to dynamical systems theory via transfer operators, and illustrate the\nadvantages of our method on benchmark graphs by comparison with existing\nmethods. We show that the spatio-temporal graph Laplacian allows for a clear\ninterpretation of cluster structure evolution over time for directed and\nundirected graphs.\n", "link": "http://arxiv.org/abs/2407.12864v2", "date": "2024-09-09", "relevancy": 2.2022, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4515}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4371}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20Time-Evolving%20Networks%20Using%20the%20Spatio-Temporal%20Graph%0A%20%20Laplacian&body=Title%3A%20Clustering%20Time-Evolving%20Networks%20Using%20the%20Spatio-Temporal%20Graph%0A%20%20Laplacian%0AAuthor%3A%20Maia%20Trower%20and%20Nata%C5%A1a%20Djurdjevac%20Conrad%20and%20Stefan%20Klus%0AAbstract%3A%20%20%20Time-evolving%20graphs%20arise%20frequently%20when%20modeling%20complex%20dynamical%20systems%0Asuch%20as%20social%20networks%2C%20traffic%20flow%2C%20and%20biological%20processes.%20Developing%0Atechniques%20to%20identify%20and%20analyze%20communities%20in%20these%20time-varying%20graph%0Astructures%20is%20an%20important%20challenge.%20In%20this%20work%2C%20we%20generalize%20existing%0Aspectral%20clustering%20algorithms%20from%20static%20to%20dynamic%20graphs%20using%20canonical%0Acorrelation%20analysis%20%28CCA%29%20to%20capture%20the%20temporal%20evolution%20of%20clusters.%20Based%0Aon%20this%20extended%20canonical%20correlation%20framework%2C%20we%20define%20the%20spatio-temporal%0Agraph%20Laplacian%20and%20investigate%20its%20spectral%20properties.%20We%20connect%20these%0Aconcepts%20to%20dynamical%20systems%20theory%20via%20transfer%20operators%2C%20and%20illustrate%20the%0Aadvantages%20of%20our%20method%20on%20benchmark%20graphs%20by%20comparison%20with%20existing%0Amethods.%20We%20show%20that%20the%20spatio-temporal%20graph%20Laplacian%20allows%20for%20a%20clear%0Ainterpretation%20of%20cluster%20structure%20evolution%20over%20time%20for%20directed%20and%0Aundirected%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520Time-Evolving%2520Networks%2520Using%2520the%2520Spatio-Temporal%2520Graph%250A%2520%2520Laplacian%26entry.906535625%3DMaia%2520Trower%2520and%2520Nata%25C5%25A1a%2520Djurdjevac%2520Conrad%2520and%2520Stefan%2520Klus%26entry.1292438233%3D%2520%2520Time-evolving%2520graphs%2520arise%2520frequently%2520when%2520modeling%2520complex%2520dynamical%2520systems%250Asuch%2520as%2520social%2520networks%252C%2520traffic%2520flow%252C%2520and%2520biological%2520processes.%2520Developing%250Atechniques%2520to%2520identify%2520and%2520analyze%2520communities%2520in%2520these%2520time-varying%2520graph%250Astructures%2520is%2520an%2520important%2520challenge.%2520In%2520this%2520work%252C%2520we%2520generalize%2520existing%250Aspectral%2520clustering%2520algorithms%2520from%2520static%2520to%2520dynamic%2520graphs%2520using%2520canonical%250Acorrelation%2520analysis%2520%2528CCA%2529%2520to%2520capture%2520the%2520temporal%2520evolution%2520of%2520clusters.%2520Based%250Aon%2520this%2520extended%2520canonical%2520correlation%2520framework%252C%2520we%2520define%2520the%2520spatio-temporal%250Agraph%2520Laplacian%2520and%2520investigate%2520its%2520spectral%2520properties.%2520We%2520connect%2520these%250Aconcepts%2520to%2520dynamical%2520systems%2520theory%2520via%2520transfer%2520operators%252C%2520and%2520illustrate%2520the%250Aadvantages%2520of%2520our%2520method%2520on%2520benchmark%2520graphs%2520by%2520comparison%2520with%2520existing%250Amethods.%2520We%2520show%2520that%2520the%2520spatio-temporal%2520graph%2520Laplacian%2520allows%2520for%2520a%2520clear%250Ainterpretation%2520of%2520cluster%2520structure%2520evolution%2520over%2520time%2520for%2520directed%2520and%250Aundirected%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20Time-Evolving%20Networks%20Using%20the%20Spatio-Temporal%20Graph%0A%20%20Laplacian&entry.906535625=Maia%20Trower%20and%20Nata%C5%A1a%20Djurdjevac%20Conrad%20and%20Stefan%20Klus&entry.1292438233=%20%20Time-evolving%20graphs%20arise%20frequently%20when%20modeling%20complex%20dynamical%20systems%0Asuch%20as%20social%20networks%2C%20traffic%20flow%2C%20and%20biological%20processes.%20Developing%0Atechniques%20to%20identify%20and%20analyze%20communities%20in%20these%20time-varying%20graph%0Astructures%20is%20an%20important%20challenge.%20In%20this%20work%2C%20we%20generalize%20existing%0Aspectral%20clustering%20algorithms%20from%20static%20to%20dynamic%20graphs%20using%20canonical%0Acorrelation%20analysis%20%28CCA%29%20to%20capture%20the%20temporal%20evolution%20of%20clusters.%20Based%0Aon%20this%20extended%20canonical%20correlation%20framework%2C%20we%20define%20the%20spatio-temporal%0Agraph%20Laplacian%20and%20investigate%20its%20spectral%20properties.%20We%20connect%20these%0Aconcepts%20to%20dynamical%20systems%20theory%20via%20transfer%20operators%2C%20and%20illustrate%20the%0Aadvantages%20of%20our%20method%20on%20benchmark%20graphs%20by%20comparison%20with%20existing%0Amethods.%20We%20show%20that%20the%20spatio-temporal%20graph%20Laplacian%20allows%20for%20a%20clear%0Ainterpretation%20of%20cluster%20structure%20evolution%20over%20time%20for%20directed%20and%0Aundirected%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12864v2&entry.124074799=Read"},
{"title": "DSDFormer: An Innovative Transformer-Mamba Framework for Robust\n  High-Precision Driver Distraction Identification", "author": "Junzhou Chen and Zirui Zhang and Jing Yu and Heqiang Huang and Ronghui Zhang and Xuemiao Xu and Bin Sheng and Hong Yan", "abstract": "  Driver distraction remains a leading cause of traffic accidents, posing a\ncritical threat to road safety globally. As intelligent transportation systems\nevolve, accurate and real-time identification of driver distraction has become\nessential. However, existing methods struggle to capture both global contextual\nand fine-grained local features while contending with noisy labels in training\ndatasets. To address these challenges, we propose DSDFormer, a novel framework\nthat integrates the strengths of Transformer and Mamba architectures through a\nDual State Domain Attention (DSDA) mechanism, enabling a balance between\nlong-range dependencies and detailed feature extraction for robust driver\nbehavior recognition. Additionally, we introduce Temporal Reasoning Confident\nLearning (TRCL), an unsupervised approach that refines noisy labels by\nleveraging spatiotemporal correlations in video sequences. Our model achieves\nstate-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and\ndemonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin\nplatform. Extensive experimental results confirm that DSDFormer and TRCL\nsignificantly improve both the accuracy and robustness of driver distraction\ndetection, offering a scalable solution to enhance road safety.\n", "link": "http://arxiv.org/abs/2409.05587v1", "date": "2024-09-09", "relevancy": 2.1976, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5451}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSDFormer%3A%20An%20Innovative%20Transformer-Mamba%20Framework%20for%20Robust%0A%20%20High-Precision%20Driver%20Distraction%20Identification&body=Title%3A%20DSDFormer%3A%20An%20Innovative%20Transformer-Mamba%20Framework%20for%20Robust%0A%20%20High-Precision%20Driver%20Distraction%20Identification%0AAuthor%3A%20Junzhou%20Chen%20and%20Zirui%20Zhang%20and%20Jing%20Yu%20and%20Heqiang%20Huang%20and%20Ronghui%20Zhang%20and%20Xuemiao%20Xu%20and%20Bin%20Sheng%20and%20Hong%20Yan%0AAbstract%3A%20%20%20Driver%20distraction%20remains%20a%20leading%20cause%20of%20traffic%20accidents%2C%20posing%20a%0Acritical%20threat%20to%20road%20safety%20globally.%20As%20intelligent%20transportation%20systems%0Aevolve%2C%20accurate%20and%20real-time%20identification%20of%20driver%20distraction%20has%20become%0Aessential.%20However%2C%20existing%20methods%20struggle%20to%20capture%20both%20global%20contextual%0Aand%20fine-grained%20local%20features%20while%20contending%20with%20noisy%20labels%20in%20training%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20propose%20DSDFormer%2C%20a%20novel%20framework%0Athat%20integrates%20the%20strengths%20of%20Transformer%20and%20Mamba%20architectures%20through%20a%0ADual%20State%20Domain%20Attention%20%28DSDA%29%20mechanism%2C%20enabling%20a%20balance%20between%0Along-range%20dependencies%20and%20detailed%20feature%20extraction%20for%20robust%20driver%0Abehavior%20recognition.%20Additionally%2C%20we%20introduce%20Temporal%20Reasoning%20Confident%0ALearning%20%28TRCL%29%2C%20an%20unsupervised%20approach%20that%20refines%20noisy%20labels%20by%0Aleveraging%20spatiotemporal%20correlations%20in%20video%20sequences.%20Our%20model%20achieves%0Astate-of-the-art%20performance%20on%20the%20AUC-V1%2C%20AUC-V2%2C%20and%20100-Driver%20datasets%20and%0Ademonstrates%20real-time%20processing%20efficiency%20on%20the%20NVIDIA%20Jetson%20AGX%20Orin%0Aplatform.%20Extensive%20experimental%20results%20confirm%20that%20DSDFormer%20and%20TRCL%0Asignificantly%20improve%20both%20the%20accuracy%20and%20robustness%20of%20driver%20distraction%0Adetection%2C%20offering%20a%20scalable%20solution%20to%20enhance%20road%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSDFormer%253A%2520An%2520Innovative%2520Transformer-Mamba%2520Framework%2520for%2520Robust%250A%2520%2520High-Precision%2520Driver%2520Distraction%2520Identification%26entry.906535625%3DJunzhou%2520Chen%2520and%2520Zirui%2520Zhang%2520and%2520Jing%2520Yu%2520and%2520Heqiang%2520Huang%2520and%2520Ronghui%2520Zhang%2520and%2520Xuemiao%2520Xu%2520and%2520Bin%2520Sheng%2520and%2520Hong%2520Yan%26entry.1292438233%3D%2520%2520Driver%2520distraction%2520remains%2520a%2520leading%2520cause%2520of%2520traffic%2520accidents%252C%2520posing%2520a%250Acritical%2520threat%2520to%2520road%2520safety%2520globally.%2520As%2520intelligent%2520transportation%2520systems%250Aevolve%252C%2520accurate%2520and%2520real-time%2520identification%2520of%2520driver%2520distraction%2520has%2520become%250Aessential.%2520However%252C%2520existing%2520methods%2520struggle%2520to%2520capture%2520both%2520global%2520contextual%250Aand%2520fine-grained%2520local%2520features%2520while%2520contending%2520with%2520noisy%2520labels%2520in%2520training%250Adatasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DSDFormer%252C%2520a%2520novel%2520framework%250Athat%2520integrates%2520the%2520strengths%2520of%2520Transformer%2520and%2520Mamba%2520architectures%2520through%2520a%250ADual%2520State%2520Domain%2520Attention%2520%2528DSDA%2529%2520mechanism%252C%2520enabling%2520a%2520balance%2520between%250Along-range%2520dependencies%2520and%2520detailed%2520feature%2520extraction%2520for%2520robust%2520driver%250Abehavior%2520recognition.%2520Additionally%252C%2520we%2520introduce%2520Temporal%2520Reasoning%2520Confident%250ALearning%2520%2528TRCL%2529%252C%2520an%2520unsupervised%2520approach%2520that%2520refines%2520noisy%2520labels%2520by%250Aleveraging%2520spatiotemporal%2520correlations%2520in%2520video%2520sequences.%2520Our%2520model%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520AUC-V1%252C%2520AUC-V2%252C%2520and%2520100-Driver%2520datasets%2520and%250Ademonstrates%2520real-time%2520processing%2520efficiency%2520on%2520the%2520NVIDIA%2520Jetson%2520AGX%2520Orin%250Aplatform.%2520Extensive%2520experimental%2520results%2520confirm%2520that%2520DSDFormer%2520and%2520TRCL%250Asignificantly%2520improve%2520both%2520the%2520accuracy%2520and%2520robustness%2520of%2520driver%2520distraction%250Adetection%252C%2520offering%2520a%2520scalable%2520solution%2520to%2520enhance%2520road%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSDFormer%3A%20An%20Innovative%20Transformer-Mamba%20Framework%20for%20Robust%0A%20%20High-Precision%20Driver%20Distraction%20Identification&entry.906535625=Junzhou%20Chen%20and%20Zirui%20Zhang%20and%20Jing%20Yu%20and%20Heqiang%20Huang%20and%20Ronghui%20Zhang%20and%20Xuemiao%20Xu%20and%20Bin%20Sheng%20and%20Hong%20Yan&entry.1292438233=%20%20Driver%20distraction%20remains%20a%20leading%20cause%20of%20traffic%20accidents%2C%20posing%20a%0Acritical%20threat%20to%20road%20safety%20globally.%20As%20intelligent%20transportation%20systems%0Aevolve%2C%20accurate%20and%20real-time%20identification%20of%20driver%20distraction%20has%20become%0Aessential.%20However%2C%20existing%20methods%20struggle%20to%20capture%20both%20global%20contextual%0Aand%20fine-grained%20local%20features%20while%20contending%20with%20noisy%20labels%20in%20training%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20propose%20DSDFormer%2C%20a%20novel%20framework%0Athat%20integrates%20the%20strengths%20of%20Transformer%20and%20Mamba%20architectures%20through%20a%0ADual%20State%20Domain%20Attention%20%28DSDA%29%20mechanism%2C%20enabling%20a%20balance%20between%0Along-range%20dependencies%20and%20detailed%20feature%20extraction%20for%20robust%20driver%0Abehavior%20recognition.%20Additionally%2C%20we%20introduce%20Temporal%20Reasoning%20Confident%0ALearning%20%28TRCL%29%2C%20an%20unsupervised%20approach%20that%20refines%20noisy%20labels%20by%0Aleveraging%20spatiotemporal%20correlations%20in%20video%20sequences.%20Our%20model%20achieves%0Astate-of-the-art%20performance%20on%20the%20AUC-V1%2C%20AUC-V2%2C%20and%20100-Driver%20datasets%20and%0Ademonstrates%20real-time%20processing%20efficiency%20on%20the%20NVIDIA%20Jetson%20AGX%20Orin%0Aplatform.%20Extensive%20experimental%20results%20confirm%20that%20DSDFormer%20and%20TRCL%0Asignificantly%20improve%20both%20the%20accuracy%20and%20robustness%20of%20driver%20distraction%0Adetection%2C%20offering%20a%20scalable%20solution%20to%20enhance%20road%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05587v1&entry.124074799=Read"},
{"title": "Elsevier Arena: Human Evaluation of Chemistry/Biology/Health\n  Foundational Large Language Models", "author": "Camilo Thorne and Christian Druckenbrodt and Kinga Szarkowska and Deepika Goyal and Pranita Marajan and Vijay Somanath and Corey Harper and Mao Yan and Tony Scerri", "abstract": "  The quality and capabilities of large language models cannot be currently\nfully assessed with automated, benchmark evaluations. Instead, human\nevaluations that expand on traditional qualitative techniques from natural\nlanguage generation literature are required. One recent best-practice consists\nin using A/B-testing frameworks, which capture preferences of human evaluators\nfor specific models. In this paper we describe a human evaluation experiment\nfocused on the biomedical domain (health, biology, chemistry/pharmacology)\ncarried out at Elsevier. In it a large but not massive (8.8B parameter)\ndecoder-only foundational transformer trained on a relatively small (135B\ntokens) but highly curated collection of Elsevier datasets is compared to\nOpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model\nagainst multiple criteria. Results indicate -- even if IRR scores were\ngenerally low -- a preference towards GPT-3.5-turbo, and hence towards models\nthat possess conversational abilities, are very large and were trained on very\nlarge datasets. But at the same time, indicate that for less massive models\ntraining on smaller but well-curated training sets can potentially give rise to\nviable alternatives in the biomedical domain.\n", "link": "http://arxiv.org/abs/2409.05486v1", "date": "2024-09-09", "relevancy": 2.1974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elsevier%20Arena%3A%20Human%20Evaluation%20of%20Chemistry/Biology/Health%0A%20%20Foundational%20Large%20Language%20Models&body=Title%3A%20Elsevier%20Arena%3A%20Human%20Evaluation%20of%20Chemistry/Biology/Health%0A%20%20Foundational%20Large%20Language%20Models%0AAuthor%3A%20Camilo%20Thorne%20and%20Christian%20Druckenbrodt%20and%20Kinga%20Szarkowska%20and%20Deepika%20Goyal%20and%20Pranita%20Marajan%20and%20Vijay%20Somanath%20and%20Corey%20Harper%20and%20Mao%20Yan%20and%20Tony%20Scerri%0AAbstract%3A%20%20%20The%20quality%20and%20capabilities%20of%20large%20language%20models%20cannot%20be%20currently%0Afully%20assessed%20with%20automated%2C%20benchmark%20evaluations.%20Instead%2C%20human%0Aevaluations%20that%20expand%20on%20traditional%20qualitative%20techniques%20from%20natural%0Alanguage%20generation%20literature%20are%20required.%20One%20recent%20best-practice%20consists%0Ain%20using%20A/B-testing%20frameworks%2C%20which%20capture%20preferences%20of%20human%20evaluators%0Afor%20specific%20models.%20In%20this%20paper%20we%20describe%20a%20human%20evaluation%20experiment%0Afocused%20on%20the%20biomedical%20domain%20%28health%2C%20biology%2C%20chemistry/pharmacology%29%0Acarried%20out%20at%20Elsevier.%20In%20it%20a%20large%20but%20not%20massive%20%288.8B%20parameter%29%0Adecoder-only%20foundational%20transformer%20trained%20on%20a%20relatively%20small%20%28135B%0Atokens%29%20but%20highly%20curated%20collection%20of%20Elsevier%20datasets%20is%20compared%20to%0AOpenAI%27s%20GPT-3.5-turbo%20and%20Meta%27s%20foundational%207B%20parameter%20Llama%202%20model%0Aagainst%20multiple%20criteria.%20Results%20indicate%20--%20even%20if%20IRR%20scores%20were%0Agenerally%20low%20--%20a%20preference%20towards%20GPT-3.5-turbo%2C%20and%20hence%20towards%20models%0Athat%20possess%20conversational%20abilities%2C%20are%20very%20large%20and%20were%20trained%20on%20very%0Alarge%20datasets.%20But%20at%20the%20same%20time%2C%20indicate%20that%20for%20less%20massive%20models%0Atraining%20on%20smaller%20but%20well-curated%20training%20sets%20can%20potentially%20give%20rise%20to%0Aviable%20alternatives%20in%20the%20biomedical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElsevier%2520Arena%253A%2520Human%2520Evaluation%2520of%2520Chemistry/Biology/Health%250A%2520%2520Foundational%2520Large%2520Language%2520Models%26entry.906535625%3DCamilo%2520Thorne%2520and%2520Christian%2520Druckenbrodt%2520and%2520Kinga%2520Szarkowska%2520and%2520Deepika%2520Goyal%2520and%2520Pranita%2520Marajan%2520and%2520Vijay%2520Somanath%2520and%2520Corey%2520Harper%2520and%2520Mao%2520Yan%2520and%2520Tony%2520Scerri%26entry.1292438233%3D%2520%2520The%2520quality%2520and%2520capabilities%2520of%2520large%2520language%2520models%2520cannot%2520be%2520currently%250Afully%2520assessed%2520with%2520automated%252C%2520benchmark%2520evaluations.%2520Instead%252C%2520human%250Aevaluations%2520that%2520expand%2520on%2520traditional%2520qualitative%2520techniques%2520from%2520natural%250Alanguage%2520generation%2520literature%2520are%2520required.%2520One%2520recent%2520best-practice%2520consists%250Ain%2520using%2520A/B-testing%2520frameworks%252C%2520which%2520capture%2520preferences%2520of%2520human%2520evaluators%250Afor%2520specific%2520models.%2520In%2520this%2520paper%2520we%2520describe%2520a%2520human%2520evaluation%2520experiment%250Afocused%2520on%2520the%2520biomedical%2520domain%2520%2528health%252C%2520biology%252C%2520chemistry/pharmacology%2529%250Acarried%2520out%2520at%2520Elsevier.%2520In%2520it%2520a%2520large%2520but%2520not%2520massive%2520%25288.8B%2520parameter%2529%250Adecoder-only%2520foundational%2520transformer%2520trained%2520on%2520a%2520relatively%2520small%2520%2528135B%250Atokens%2529%2520but%2520highly%2520curated%2520collection%2520of%2520Elsevier%2520datasets%2520is%2520compared%2520to%250AOpenAI%2527s%2520GPT-3.5-turbo%2520and%2520Meta%2527s%2520foundational%25207B%2520parameter%2520Llama%25202%2520model%250Aagainst%2520multiple%2520criteria.%2520Results%2520indicate%2520--%2520even%2520if%2520IRR%2520scores%2520were%250Agenerally%2520low%2520--%2520a%2520preference%2520towards%2520GPT-3.5-turbo%252C%2520and%2520hence%2520towards%2520models%250Athat%2520possess%2520conversational%2520abilities%252C%2520are%2520very%2520large%2520and%2520were%2520trained%2520on%2520very%250Alarge%2520datasets.%2520But%2520at%2520the%2520same%2520time%252C%2520indicate%2520that%2520for%2520less%2520massive%2520models%250Atraining%2520on%2520smaller%2520but%2520well-curated%2520training%2520sets%2520can%2520potentially%2520give%2520rise%2520to%250Aviable%2520alternatives%2520in%2520the%2520biomedical%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elsevier%20Arena%3A%20Human%20Evaluation%20of%20Chemistry/Biology/Health%0A%20%20Foundational%20Large%20Language%20Models&entry.906535625=Camilo%20Thorne%20and%20Christian%20Druckenbrodt%20and%20Kinga%20Szarkowska%20and%20Deepika%20Goyal%20and%20Pranita%20Marajan%20and%20Vijay%20Somanath%20and%20Corey%20Harper%20and%20Mao%20Yan%20and%20Tony%20Scerri&entry.1292438233=%20%20The%20quality%20and%20capabilities%20of%20large%20language%20models%20cannot%20be%20currently%0Afully%20assessed%20with%20automated%2C%20benchmark%20evaluations.%20Instead%2C%20human%0Aevaluations%20that%20expand%20on%20traditional%20qualitative%20techniques%20from%20natural%0Alanguage%20generation%20literature%20are%20required.%20One%20recent%20best-practice%20consists%0Ain%20using%20A/B-testing%20frameworks%2C%20which%20capture%20preferences%20of%20human%20evaluators%0Afor%20specific%20models.%20In%20this%20paper%20we%20describe%20a%20human%20evaluation%20experiment%0Afocused%20on%20the%20biomedical%20domain%20%28health%2C%20biology%2C%20chemistry/pharmacology%29%0Acarried%20out%20at%20Elsevier.%20In%20it%20a%20large%20but%20not%20massive%20%288.8B%20parameter%29%0Adecoder-only%20foundational%20transformer%20trained%20on%20a%20relatively%20small%20%28135B%0Atokens%29%20but%20highly%20curated%20collection%20of%20Elsevier%20datasets%20is%20compared%20to%0AOpenAI%27s%20GPT-3.5-turbo%20and%20Meta%27s%20foundational%207B%20parameter%20Llama%202%20model%0Aagainst%20multiple%20criteria.%20Results%20indicate%20--%20even%20if%20IRR%20scores%20were%0Agenerally%20low%20--%20a%20preference%20towards%20GPT-3.5-turbo%2C%20and%20hence%20towards%20models%0Athat%20possess%20conversational%20abilities%2C%20are%20very%20large%20and%20were%20trained%20on%20very%0Alarge%20datasets.%20But%20at%20the%20same%20time%2C%20indicate%20that%20for%20less%20massive%20models%0Atraining%20on%20smaller%20but%20well-curated%20training%20sets%20can%20potentially%20give%20rise%20to%0Aviable%20alternatives%20in%20the%20biomedical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05486v1&entry.124074799=Read"},
{"title": "G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel\n  View Synthesis", "author": "Lutao Jiang and Lin Wang", "abstract": "  Following the burgeoning interest in implicit neural representation, Neural\nLight Field (NeLF) has been introduced to predict the color of a ray directly.\nUnlike Neural Radiance Field (NeRF), NeLF does not create a point-wise\nrepresentation by predicting color and volume density for each point in space.\nHowever, the current NeLF methods face a challenge as they need to train a NeRF\nmodel first and then synthesize over 10K views to train NeLF for improved\nperformance. Additionally, the rendering quality of NeLF methods is lower\ncompared to NeRF methods. In this paper, we propose G-NeLF, a versatile\ngrid-based NeLF approach that utilizes spatial-aware features to unleash the\npotential of the neural network's inference capability, and consequently\novercome the difficulties of NeLF training. Specifically, we employ a\nspatial-aware feature sequence derived from a meticulously crafted grid as the\nray's representation. Drawing from our empirical studies on the adaptability of\nmulti-resolution hash tables, we introduce a novel grid-based ray\nrepresentation for NeLF that can represent the entire space with a very limited\nnumber of parameters. To better utilize the sequence feature, we design a\nlightweight ray color decoder that simulates the ray propagation process,\nenabling a more efficient inference of the ray's color. G-NeLF can be trained\nwithout necessitating significant storage overhead and with the model size of\nonly 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with\ngrid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its\nparameters to achieve higher performance. Our code will be released upon\nacceptance.\n", "link": "http://arxiv.org/abs/2409.05617v1", "date": "2024-09-09", "relevancy": 2.1972, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5716}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5429}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-NeLF%3A%20Memory-%20and%20Data-Efficient%20Hybrid%20Neural%20Light%20Field%20for%20Novel%0A%20%20View%20Synthesis&body=Title%3A%20G-NeLF%3A%20Memory-%20and%20Data-Efficient%20Hybrid%20Neural%20Light%20Field%20for%20Novel%0A%20%20View%20Synthesis%0AAuthor%3A%20Lutao%20Jiang%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Following%20the%20burgeoning%20interest%20in%20implicit%20neural%20representation%2C%20Neural%0ALight%20Field%20%28NeLF%29%20has%20been%20introduced%20to%20predict%20the%20color%20of%20a%20ray%20directly.%0AUnlike%20Neural%20Radiance%20Field%20%28NeRF%29%2C%20NeLF%20does%20not%20create%20a%20point-wise%0Arepresentation%20by%20predicting%20color%20and%20volume%20density%20for%20each%20point%20in%20space.%0AHowever%2C%20the%20current%20NeLF%20methods%20face%20a%20challenge%20as%20they%20need%20to%20train%20a%20NeRF%0Amodel%20first%20and%20then%20synthesize%20over%2010K%20views%20to%20train%20NeLF%20for%20improved%0Aperformance.%20Additionally%2C%20the%20rendering%20quality%20of%20NeLF%20methods%20is%20lower%0Acompared%20to%20NeRF%20methods.%20In%20this%20paper%2C%20we%20propose%20G-NeLF%2C%20a%20versatile%0Agrid-based%20NeLF%20approach%20that%20utilizes%20spatial-aware%20features%20to%20unleash%20the%0Apotential%20of%20the%20neural%20network%27s%20inference%20capability%2C%20and%20consequently%0Aovercome%20the%20difficulties%20of%20NeLF%20training.%20Specifically%2C%20we%20employ%20a%0Aspatial-aware%20feature%20sequence%20derived%20from%20a%20meticulously%20crafted%20grid%20as%20the%0Aray%27s%20representation.%20Drawing%20from%20our%20empirical%20studies%20on%20the%20adaptability%20of%0Amulti-resolution%20hash%20tables%2C%20we%20introduce%20a%20novel%20grid-based%20ray%0Arepresentation%20for%20NeLF%20that%20can%20represent%20the%20entire%20space%20with%20a%20very%20limited%0Anumber%20of%20parameters.%20To%20better%20utilize%20the%20sequence%20feature%2C%20we%20design%20a%0Alightweight%20ray%20color%20decoder%20that%20simulates%20the%20ray%20propagation%20process%2C%0Aenabling%20a%20more%20efficient%20inference%20of%20the%20ray%27s%20color.%20G-NeLF%20can%20be%20trained%0Awithout%20necessitating%20significant%20storage%20overhead%20and%20with%20the%20model%20size%20of%0Aonly%200.95%20MB%20to%20surpass%20previous%20state-of-the-art%20NeLF.%20Moreover%2C%20compared%20with%0Agrid-based%20NeRF%20methods%2C%20e.g.%2C%20Instant-NGP%2C%20we%20only%20utilize%20one-tenth%20of%20its%0Aparameters%20to%20achieve%20higher%20performance.%20Our%20code%20will%20be%20released%20upon%0Aacceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-NeLF%253A%2520Memory-%2520and%2520Data-Efficient%2520Hybrid%2520Neural%2520Light%2520Field%2520for%2520Novel%250A%2520%2520View%2520Synthesis%26entry.906535625%3DLutao%2520Jiang%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Following%2520the%2520burgeoning%2520interest%2520in%2520implicit%2520neural%2520representation%252C%2520Neural%250ALight%2520Field%2520%2528NeLF%2529%2520has%2520been%2520introduced%2520to%2520predict%2520the%2520color%2520of%2520a%2520ray%2520directly.%250AUnlike%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%252C%2520NeLF%2520does%2520not%2520create%2520a%2520point-wise%250Arepresentation%2520by%2520predicting%2520color%2520and%2520volume%2520density%2520for%2520each%2520point%2520in%2520space.%250AHowever%252C%2520the%2520current%2520NeLF%2520methods%2520face%2520a%2520challenge%2520as%2520they%2520need%2520to%2520train%2520a%2520NeRF%250Amodel%2520first%2520and%2520then%2520synthesize%2520over%252010K%2520views%2520to%2520train%2520NeLF%2520for%2520improved%250Aperformance.%2520Additionally%252C%2520the%2520rendering%2520quality%2520of%2520NeLF%2520methods%2520is%2520lower%250Acompared%2520to%2520NeRF%2520methods.%2520In%2520this%2520paper%252C%2520we%2520propose%2520G-NeLF%252C%2520a%2520versatile%250Agrid-based%2520NeLF%2520approach%2520that%2520utilizes%2520spatial-aware%2520features%2520to%2520unleash%2520the%250Apotential%2520of%2520the%2520neural%2520network%2527s%2520inference%2520capability%252C%2520and%2520consequently%250Aovercome%2520the%2520difficulties%2520of%2520NeLF%2520training.%2520Specifically%252C%2520we%2520employ%2520a%250Aspatial-aware%2520feature%2520sequence%2520derived%2520from%2520a%2520meticulously%2520crafted%2520grid%2520as%2520the%250Aray%2527s%2520representation.%2520Drawing%2520from%2520our%2520empirical%2520studies%2520on%2520the%2520adaptability%2520of%250Amulti-resolution%2520hash%2520tables%252C%2520we%2520introduce%2520a%2520novel%2520grid-based%2520ray%250Arepresentation%2520for%2520NeLF%2520that%2520can%2520represent%2520the%2520entire%2520space%2520with%2520a%2520very%2520limited%250Anumber%2520of%2520parameters.%2520To%2520better%2520utilize%2520the%2520sequence%2520feature%252C%2520we%2520design%2520a%250Alightweight%2520ray%2520color%2520decoder%2520that%2520simulates%2520the%2520ray%2520propagation%2520process%252C%250Aenabling%2520a%2520more%2520efficient%2520inference%2520of%2520the%2520ray%2527s%2520color.%2520G-NeLF%2520can%2520be%2520trained%250Awithout%2520necessitating%2520significant%2520storage%2520overhead%2520and%2520with%2520the%2520model%2520size%2520of%250Aonly%25200.95%2520MB%2520to%2520surpass%2520previous%2520state-of-the-art%2520NeLF.%2520Moreover%252C%2520compared%2520with%250Agrid-based%2520NeRF%2520methods%252C%2520e.g.%252C%2520Instant-NGP%252C%2520we%2520only%2520utilize%2520one-tenth%2520of%2520its%250Aparameters%2520to%2520achieve%2520higher%2520performance.%2520Our%2520code%2520will%2520be%2520released%2520upon%250Aacceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-NeLF%3A%20Memory-%20and%20Data-Efficient%20Hybrid%20Neural%20Light%20Field%20for%20Novel%0A%20%20View%20Synthesis&entry.906535625=Lutao%20Jiang%20and%20Lin%20Wang&entry.1292438233=%20%20Following%20the%20burgeoning%20interest%20in%20implicit%20neural%20representation%2C%20Neural%0ALight%20Field%20%28NeLF%29%20has%20been%20introduced%20to%20predict%20the%20color%20of%20a%20ray%20directly.%0AUnlike%20Neural%20Radiance%20Field%20%28NeRF%29%2C%20NeLF%20does%20not%20create%20a%20point-wise%0Arepresentation%20by%20predicting%20color%20and%20volume%20density%20for%20each%20point%20in%20space.%0AHowever%2C%20the%20current%20NeLF%20methods%20face%20a%20challenge%20as%20they%20need%20to%20train%20a%20NeRF%0Amodel%20first%20and%20then%20synthesize%20over%2010K%20views%20to%20train%20NeLF%20for%20improved%0Aperformance.%20Additionally%2C%20the%20rendering%20quality%20of%20NeLF%20methods%20is%20lower%0Acompared%20to%20NeRF%20methods.%20In%20this%20paper%2C%20we%20propose%20G-NeLF%2C%20a%20versatile%0Agrid-based%20NeLF%20approach%20that%20utilizes%20spatial-aware%20features%20to%20unleash%20the%0Apotential%20of%20the%20neural%20network%27s%20inference%20capability%2C%20and%20consequently%0Aovercome%20the%20difficulties%20of%20NeLF%20training.%20Specifically%2C%20we%20employ%20a%0Aspatial-aware%20feature%20sequence%20derived%20from%20a%20meticulously%20crafted%20grid%20as%20the%0Aray%27s%20representation.%20Drawing%20from%20our%20empirical%20studies%20on%20the%20adaptability%20of%0Amulti-resolution%20hash%20tables%2C%20we%20introduce%20a%20novel%20grid-based%20ray%0Arepresentation%20for%20NeLF%20that%20can%20represent%20the%20entire%20space%20with%20a%20very%20limited%0Anumber%20of%20parameters.%20To%20better%20utilize%20the%20sequence%20feature%2C%20we%20design%20a%0Alightweight%20ray%20color%20decoder%20that%20simulates%20the%20ray%20propagation%20process%2C%0Aenabling%20a%20more%20efficient%20inference%20of%20the%20ray%27s%20color.%20G-NeLF%20can%20be%20trained%0Awithout%20necessitating%20significant%20storage%20overhead%20and%20with%20the%20model%20size%20of%0Aonly%200.95%20MB%20to%20surpass%20previous%20state-of-the-art%20NeLF.%20Moreover%2C%20compared%20with%0Agrid-based%20NeRF%20methods%2C%20e.g.%2C%20Instant-NGP%2C%20we%20only%20utilize%20one-tenth%20of%20its%0Aparameters%20to%20achieve%20higher%20performance.%20Our%20code%20will%20be%20released%20upon%0Aacceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05617v1&entry.124074799=Read"},
{"title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark", "author": "Ge Zhang and Xinrun Du and Bei Chen and Yiming Liang and Tongxu Luo and Tianyu Zheng and Kang Zhu and Yuyang Cheng and Chunpu Xu and Shuyue Guo and Haoran Zhang and Xingwei Qu and Junjie Wang and Ruibin Yuan and Yizhi Li and Zekun Wang and Yudong Liu and Yu-Hsuan Tsai and Fengji Zhang and Chenghua Lin and Wenhao Huang and Jie Fu", "abstract": "  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n", "link": "http://arxiv.org/abs/2401.11944v3", "date": "2024-09-09", "relevancy": 2.1869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMMMU%3A%20A%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark&body=Title%3A%20CMMMU%3A%20A%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark%0AAuthor%3A%20Ge%20Zhang%20and%20Xinrun%20Du%20and%20Bei%20Chen%20and%20Yiming%20Liang%20and%20Tongxu%20Luo%20and%20Tianyu%20Zheng%20and%20Kang%20Zhu%20and%20Yuyang%20Cheng%20and%20Chunpu%20Xu%20and%20Shuyue%20Guo%20and%20Haoran%20Zhang%20and%20Xingwei%20Qu%20and%20Junjie%20Wang%20and%20Ruibin%20Yuan%20and%20Yizhi%20Li%20and%20Zekun%20Wang%20and%20Yudong%20Liu%20and%20Yu-Hsuan%20Tsai%20and%20Fengji%20Zhang%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu%0AAbstract%3A%20%20%20As%20the%20capabilities%20of%20large%20multimodal%20models%20%28LMMs%29%20continue%20to%20advance%2C%0Aevaluating%20the%20performance%20of%20LMMs%20emerges%20as%20an%20increasing%20need.%20Additionally%2C%0Athere%20is%20an%20even%20larger%20gap%20in%20evaluating%20the%20advanced%20knowledge%20and%20reasoning%0Aabilities%20of%20LMMs%20in%20non-English%20contexts%20such%20as%20Chinese.%20We%20introduce%20CMMMU%2C%0Aa%20new%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%20benchmark%0Adesigned%20to%20evaluate%20LMMs%20on%20tasks%20demanding%20college-level%20subject%20knowledge%0Aand%20deliberate%20reasoning%20in%20a%20Chinese%20context.%20CMMMU%20is%20inspired%20by%20and%0Astrictly%20follows%20the%20annotation%20and%20analysis%20pattern%20of%20MMMU.%20CMMMU%20includes%0A12k%20manually%20collected%20multimodal%20questions%20from%20college%20exams%2C%20quizzes%2C%20and%0Atextbooks%2C%20covering%20six%20core%20disciplines%3A%20Art%20%26%20Design%2C%20Business%2C%20Science%2C%0AHealth%20%26%20Medicine%2C%20Humanities%20%26%20Social%20Science%2C%20and%20Tech%20%26%20Engineering%2C%20like%0Aits%20companion%2C%20MMMU.%20These%20questions%20span%2030%20subjects%20and%20comprise%2039%20highly%0Aheterogeneous%20image%20types%2C%20such%20as%20charts%2C%20diagrams%2C%20maps%2C%20tables%2C%20music%0Asheets%2C%20and%20chemical%20structures.%20CMMMU%20focuses%20on%20complex%20perception%20and%0Areasoning%20with%20domain-specific%20knowledge%20in%20the%20Chinese%20context.%20We%20evaluate%2011%0Aopen-source%20LLMs%20and%20one%20proprietary%20GPT-4V%28ision%29.%20Even%20GPT-4V%20only%20achieves%0Aaccuracies%20of%2042%25%2C%20indicating%20a%20large%20space%20for%20improvement.%20CMMMU%20will%20boost%0Athe%20community%20to%20build%20the%20next-generation%20LMMs%20towards%20expert%20artificial%0Aintelligence%20and%20promote%20the%20democratization%20of%20LMMs%20by%20providing%20diverse%0Alanguage%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11944v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMMMU%253A%2520A%2520Chinese%2520Massive%2520Multi-discipline%2520Multimodal%2520Understanding%250A%2520%2520Benchmark%26entry.906535625%3DGe%2520Zhang%2520and%2520Xinrun%2520Du%2520and%2520Bei%2520Chen%2520and%2520Yiming%2520Liang%2520and%2520Tongxu%2520Luo%2520and%2520Tianyu%2520Zheng%2520and%2520Kang%2520Zhu%2520and%2520Yuyang%2520Cheng%2520and%2520Chunpu%2520Xu%2520and%2520Shuyue%2520Guo%2520and%2520Haoran%2520Zhang%2520and%2520Xingwei%2520Qu%2520and%2520Junjie%2520Wang%2520and%2520Ruibin%2520Yuan%2520and%2520Yizhi%2520Li%2520and%2520Zekun%2520Wang%2520and%2520Yudong%2520Liu%2520and%2520Yu-Hsuan%2520Tsai%2520and%2520Fengji%2520Zhang%2520and%2520Chenghua%2520Lin%2520and%2520Wenhao%2520Huang%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520As%2520the%2520capabilities%2520of%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520continue%2520to%2520advance%252C%250Aevaluating%2520the%2520performance%2520of%2520LMMs%2520emerges%2520as%2520an%2520increasing%2520need.%2520Additionally%252C%250Athere%2520is%2520an%2520even%2520larger%2520gap%2520in%2520evaluating%2520the%2520advanced%2520knowledge%2520and%2520reasoning%250Aabilities%2520of%2520LMMs%2520in%2520non-English%2520contexts%2520such%2520as%2520Chinese.%2520We%2520introduce%2520CMMMU%252C%250Aa%2520new%2520Chinese%2520Massive%2520Multi-discipline%2520Multimodal%2520Understanding%2520benchmark%250Adesigned%2520to%2520evaluate%2520LMMs%2520on%2520tasks%2520demanding%2520college-level%2520subject%2520knowledge%250Aand%2520deliberate%2520reasoning%2520in%2520a%2520Chinese%2520context.%2520CMMMU%2520is%2520inspired%2520by%2520and%250Astrictly%2520follows%2520the%2520annotation%2520and%2520analysis%2520pattern%2520of%2520MMMU.%2520CMMMU%2520includes%250A12k%2520manually%2520collected%2520multimodal%2520questions%2520from%2520college%2520exams%252C%2520quizzes%252C%2520and%250Atextbooks%252C%2520covering%2520six%2520core%2520disciplines%253A%2520Art%2520%2526%2520Design%252C%2520Business%252C%2520Science%252C%250AHealth%2520%2526%2520Medicine%252C%2520Humanities%2520%2526%2520Social%2520Science%252C%2520and%2520Tech%2520%2526%2520Engineering%252C%2520like%250Aits%2520companion%252C%2520MMMU.%2520These%2520questions%2520span%252030%2520subjects%2520and%2520comprise%252039%2520highly%250Aheterogeneous%2520image%2520types%252C%2520such%2520as%2520charts%252C%2520diagrams%252C%2520maps%252C%2520tables%252C%2520music%250Asheets%252C%2520and%2520chemical%2520structures.%2520CMMMU%2520focuses%2520on%2520complex%2520perception%2520and%250Areasoning%2520with%2520domain-specific%2520knowledge%2520in%2520the%2520Chinese%2520context.%2520We%2520evaluate%252011%250Aopen-source%2520LLMs%2520and%2520one%2520proprietary%2520GPT-4V%2528ision%2529.%2520Even%2520GPT-4V%2520only%2520achieves%250Aaccuracies%2520of%252042%2525%252C%2520indicating%2520a%2520large%2520space%2520for%2520improvement.%2520CMMMU%2520will%2520boost%250Athe%2520community%2520to%2520build%2520the%2520next-generation%2520LMMs%2520towards%2520expert%2520artificial%250Aintelligence%2520and%2520promote%2520the%2520democratization%2520of%2520LMMs%2520by%2520providing%2520diverse%250Alanguage%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11944v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMMMU%3A%20A%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark&entry.906535625=Ge%20Zhang%20and%20Xinrun%20Du%20and%20Bei%20Chen%20and%20Yiming%20Liang%20and%20Tongxu%20Luo%20and%20Tianyu%20Zheng%20and%20Kang%20Zhu%20and%20Yuyang%20Cheng%20and%20Chunpu%20Xu%20and%20Shuyue%20Guo%20and%20Haoran%20Zhang%20and%20Xingwei%20Qu%20and%20Junjie%20Wang%20and%20Ruibin%20Yuan%20and%20Yizhi%20Li%20and%20Zekun%20Wang%20and%20Yudong%20Liu%20and%20Yu-Hsuan%20Tsai%20and%20Fengji%20Zhang%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu&entry.1292438233=%20%20As%20the%20capabilities%20of%20large%20multimodal%20models%20%28LMMs%29%20continue%20to%20advance%2C%0Aevaluating%20the%20performance%20of%20LMMs%20emerges%20as%20an%20increasing%20need.%20Additionally%2C%0Athere%20is%20an%20even%20larger%20gap%20in%20evaluating%20the%20advanced%20knowledge%20and%20reasoning%0Aabilities%20of%20LMMs%20in%20non-English%20contexts%20such%20as%20Chinese.%20We%20introduce%20CMMMU%2C%0Aa%20new%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%20benchmark%0Adesigned%20to%20evaluate%20LMMs%20on%20tasks%20demanding%20college-level%20subject%20knowledge%0Aand%20deliberate%20reasoning%20in%20a%20Chinese%20context.%20CMMMU%20is%20inspired%20by%20and%0Astrictly%20follows%20the%20annotation%20and%20analysis%20pattern%20of%20MMMU.%20CMMMU%20includes%0A12k%20manually%20collected%20multimodal%20questions%20from%20college%20exams%2C%20quizzes%2C%20and%0Atextbooks%2C%20covering%20six%20core%20disciplines%3A%20Art%20%26%20Design%2C%20Business%2C%20Science%2C%0AHealth%20%26%20Medicine%2C%20Humanities%20%26%20Social%20Science%2C%20and%20Tech%20%26%20Engineering%2C%20like%0Aits%20companion%2C%20MMMU.%20These%20questions%20span%2030%20subjects%20and%20comprise%2039%20highly%0Aheterogeneous%20image%20types%2C%20such%20as%20charts%2C%20diagrams%2C%20maps%2C%20tables%2C%20music%0Asheets%2C%20and%20chemical%20structures.%20CMMMU%20focuses%20on%20complex%20perception%20and%0Areasoning%20with%20domain-specific%20knowledge%20in%20the%20Chinese%20context.%20We%20evaluate%2011%0Aopen-source%20LLMs%20and%20one%20proprietary%20GPT-4V%28ision%29.%20Even%20GPT-4V%20only%20achieves%0Aaccuracies%20of%2042%25%2C%20indicating%20a%20large%20space%20for%20improvement.%20CMMMU%20will%20boost%0Athe%20community%20to%20build%20the%20next-generation%20LMMs%20towards%20expert%20artificial%0Aintelligence%20and%20promote%20the%20democratization%20of%20LMMs%20by%20providing%20diverse%0Alanguage%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11944v3&entry.124074799=Read"},
{"title": "Student Classroom Behavior Detection based on Spatio-Temporal Network\n  and Multi-Model Fusion", "author": "Fan Yang and Xiaofei Wang", "abstract": "  Using deep learning methods to detect students' classroom behavior\nautomatically is a promising approach for analyzing their class performance and\nimproving teaching effectiveness. However, the lack of publicly available\nspatio-temporal datasets on student behavior, as well as the high cost of\nmanually labeling such datasets, pose significant challenges for researchers in\nthis field. To address this issue, we proposed a method for extending the\nspatio-temporal behavior dataset in Student Classroom Scenarios\n(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265\nimages with 25810 labels, focusing on 3 behaviors: hand-raising, reading,\nwriting. Our proposed method can rapidly generate spatio-temporal behavior\ndatasets without requiring extra manual labeling. Furthermore, we proposed a\nBehavior Similarity Index (BSI) to explore the similarity of behaviors. We\nevaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast\nalgorithms, achieving a mean average precision (map) of up to 82.3%. Last, we\nfused multiple models to generate student behavior-related data from various\nperspectives. The experiment further demonstrates the effectiveness of our\nmethod. And SCB-ST-Dataset4 provides a robust foundation for future research in\nstudent behavior detection, potentially contributing to advancements in this\nfield. The SCB-ST-Dataset4 is available for download at:\nhttps://github.com/Whiffe/SCB-dataset.\n", "link": "http://arxiv.org/abs/2310.16267v4", "date": "2024-09-09", "relevancy": 2.1831, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5812}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5227}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Student%20Classroom%20Behavior%20Detection%20based%20on%20Spatio-Temporal%20Network%0A%20%20and%20Multi-Model%20Fusion&body=Title%3A%20Student%20Classroom%20Behavior%20Detection%20based%20on%20Spatio-Temporal%20Network%0A%20%20and%20Multi-Model%20Fusion%0AAuthor%3A%20Fan%20Yang%20and%20Xiaofei%20Wang%0AAbstract%3A%20%20%20Using%20deep%20learning%20methods%20to%20detect%20students%27%20classroom%20behavior%0Aautomatically%20is%20a%20promising%20approach%20for%20analyzing%20their%20class%20performance%20and%0Aimproving%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Aspatio-temporal%20datasets%20on%20student%20behavior%2C%20as%20well%20as%20the%20high%20cost%20of%0Amanually%20labeling%20such%20datasets%2C%20pose%20significant%20challenges%20for%20researchers%20in%0Athis%20field.%20To%20address%20this%20issue%2C%20we%20proposed%20a%20method%20for%20extending%20the%0Aspatio-temporal%20behavior%20dataset%20in%20Student%20Classroom%20Scenarios%0A%28SCB-ST-Dataset4%29%20through%20image%20dataset.%20Our%20SCB-ST-Dataset4%20comprises%20757265%0Aimages%20with%2025810%20labels%2C%20focusing%20on%203%20behaviors%3A%20hand-raising%2C%20reading%2C%0Awriting.%20Our%20proposed%20method%20can%20rapidly%20generate%20spatio-temporal%20behavior%0Adatasets%20without%20requiring%20extra%20manual%20labeling.%20Furthermore%2C%20we%20proposed%20a%0ABehavior%20Similarity%20Index%20%28BSI%29%20to%20explore%20the%20similarity%20of%20behaviors.%20We%0Aevaluated%20the%20dataset%20using%20the%20YOLOv5%2C%20YOLOv7%2C%20YOLOv8%2C%20and%20SlowFast%0Aalgorithms%2C%20achieving%20a%20mean%20average%20precision%20%28map%29%20of%20up%20to%2082.3%25.%20Last%2C%20we%0Afused%20multiple%20models%20to%20generate%20student%20behavior-related%20data%20from%20various%0Aperspectives.%20The%20experiment%20further%20demonstrates%20the%20effectiveness%20of%20our%0Amethod.%20And%20SCB-ST-Dataset4%20provides%20a%20robust%20foundation%20for%20future%20research%20in%0Astudent%20behavior%20detection%2C%20potentially%20contributing%20to%20advancements%20in%20this%0Afield.%20The%20SCB-ST-Dataset4%20is%20available%20for%20download%20at%3A%0Ahttps%3A//github.com/Whiffe/SCB-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16267v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudent%2520Classroom%2520Behavior%2520Detection%2520based%2520on%2520Spatio-Temporal%2520Network%250A%2520%2520and%2520Multi-Model%2520Fusion%26entry.906535625%3DFan%2520Yang%2520and%2520Xiaofei%2520Wang%26entry.1292438233%3D%2520%2520Using%2520deep%2520learning%2520methods%2520to%2520detect%2520students%2527%2520classroom%2520behavior%250Aautomatically%2520is%2520a%2520promising%2520approach%2520for%2520analyzing%2520their%2520class%2520performance%2520and%250Aimproving%2520teaching%2520effectiveness.%2520However%252C%2520the%2520lack%2520of%2520publicly%2520available%250Aspatio-temporal%2520datasets%2520on%2520student%2520behavior%252C%2520as%2520well%2520as%2520the%2520high%2520cost%2520of%250Amanually%2520labeling%2520such%2520datasets%252C%2520pose%2520significant%2520challenges%2520for%2520researchers%2520in%250Athis%2520field.%2520To%2520address%2520this%2520issue%252C%2520we%2520proposed%2520a%2520method%2520for%2520extending%2520the%250Aspatio-temporal%2520behavior%2520dataset%2520in%2520Student%2520Classroom%2520Scenarios%250A%2528SCB-ST-Dataset4%2529%2520through%2520image%2520dataset.%2520Our%2520SCB-ST-Dataset4%2520comprises%2520757265%250Aimages%2520with%252025810%2520labels%252C%2520focusing%2520on%25203%2520behaviors%253A%2520hand-raising%252C%2520reading%252C%250Awriting.%2520Our%2520proposed%2520method%2520can%2520rapidly%2520generate%2520spatio-temporal%2520behavior%250Adatasets%2520without%2520requiring%2520extra%2520manual%2520labeling.%2520Furthermore%252C%2520we%2520proposed%2520a%250ABehavior%2520Similarity%2520Index%2520%2528BSI%2529%2520to%2520explore%2520the%2520similarity%2520of%2520behaviors.%2520We%250Aevaluated%2520the%2520dataset%2520using%2520the%2520YOLOv5%252C%2520YOLOv7%252C%2520YOLOv8%252C%2520and%2520SlowFast%250Aalgorithms%252C%2520achieving%2520a%2520mean%2520average%2520precision%2520%2528map%2529%2520of%2520up%2520to%252082.3%2525.%2520Last%252C%2520we%250Afused%2520multiple%2520models%2520to%2520generate%2520student%2520behavior-related%2520data%2520from%2520various%250Aperspectives.%2520The%2520experiment%2520further%2520demonstrates%2520the%2520effectiveness%2520of%2520our%250Amethod.%2520And%2520SCB-ST-Dataset4%2520provides%2520a%2520robust%2520foundation%2520for%2520future%2520research%2520in%250Astudent%2520behavior%2520detection%252C%2520potentially%2520contributing%2520to%2520advancements%2520in%2520this%250Afield.%2520The%2520SCB-ST-Dataset4%2520is%2520available%2520for%2520download%2520at%253A%250Ahttps%253A//github.com/Whiffe/SCB-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16267v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Student%20Classroom%20Behavior%20Detection%20based%20on%20Spatio-Temporal%20Network%0A%20%20and%20Multi-Model%20Fusion&entry.906535625=Fan%20Yang%20and%20Xiaofei%20Wang&entry.1292438233=%20%20Using%20deep%20learning%20methods%20to%20detect%20students%27%20classroom%20behavior%0Aautomatically%20is%20a%20promising%20approach%20for%20analyzing%20their%20class%20performance%20and%0Aimproving%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Aspatio-temporal%20datasets%20on%20student%20behavior%2C%20as%20well%20as%20the%20high%20cost%20of%0Amanually%20labeling%20such%20datasets%2C%20pose%20significant%20challenges%20for%20researchers%20in%0Athis%20field.%20To%20address%20this%20issue%2C%20we%20proposed%20a%20method%20for%20extending%20the%0Aspatio-temporal%20behavior%20dataset%20in%20Student%20Classroom%20Scenarios%0A%28SCB-ST-Dataset4%29%20through%20image%20dataset.%20Our%20SCB-ST-Dataset4%20comprises%20757265%0Aimages%20with%2025810%20labels%2C%20focusing%20on%203%20behaviors%3A%20hand-raising%2C%20reading%2C%0Awriting.%20Our%20proposed%20method%20can%20rapidly%20generate%20spatio-temporal%20behavior%0Adatasets%20without%20requiring%20extra%20manual%20labeling.%20Furthermore%2C%20we%20proposed%20a%0ABehavior%20Similarity%20Index%20%28BSI%29%20to%20explore%20the%20similarity%20of%20behaviors.%20We%0Aevaluated%20the%20dataset%20using%20the%20YOLOv5%2C%20YOLOv7%2C%20YOLOv8%2C%20and%20SlowFast%0Aalgorithms%2C%20achieving%20a%20mean%20average%20precision%20%28map%29%20of%20up%20to%2082.3%25.%20Last%2C%20we%0Afused%20multiple%20models%20to%20generate%20student%20behavior-related%20data%20from%20various%0Aperspectives.%20The%20experiment%20further%20demonstrates%20the%20effectiveness%20of%20our%0Amethod.%20And%20SCB-ST-Dataset4%20provides%20a%20robust%20foundation%20for%20future%20research%20in%0Astudent%20behavior%20detection%2C%20potentially%20contributing%20to%20advancements%20in%20this%0Afield.%20The%20SCB-ST-Dataset4%20is%20available%20for%20download%20at%3A%0Ahttps%3A//github.com/Whiffe/SCB-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16267v4&entry.124074799=Read"},
{"title": "A Spatio-Temporal Attention-Based Method for Detecting Student Classroom\n  Behaviors", "author": "Fan Yang", "abstract": "  Accurately detecting student behavior from classroom videos is beneficial for\nanalyzing their classroom status and improving teaching efficiency. However,\nlow accuracy in student classroom behavior detection is a prevalent issue. To\naddress this issue, we propose a Spatio-Temporal Attention-Based Method for\nDetecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is\nused to generate motion and environmental information feature maps from the\nvideo. Then, the spatio-temporal attention module is applied to the feature\nmaps, including information aggregation, compression and stimulation processes.\nSubsequently, attention maps in the time, channel and space dimensions are\nobtained, and multi-label behavior classification is performed based on these\nattention maps. To solve the long-tail data problem that exists in student\nclassroom behavior datasets, we use an improved focal loss function to assign\nmore weight to the tail class data during training. Experimental results are\nconducted on a self-made student classroom behavior dataset named STSCB.\nCompared with the SlowFast model, the average accuracy of student behavior\nclassification detection improves by 8.94\\% using BDSTA.\n", "link": "http://arxiv.org/abs/2310.02523v4", "date": "2024-09-09", "relevancy": 2.1806, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5874}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5185}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Spatio-Temporal%20Attention-Based%20Method%20for%20Detecting%20Student%20Classroom%0A%20%20Behaviors&body=Title%3A%20A%20Spatio-Temporal%20Attention-Based%20Method%20for%20Detecting%20Student%20Classroom%0A%20%20Behaviors%0AAuthor%3A%20Fan%20Yang%0AAbstract%3A%20%20%20Accurately%20detecting%20student%20behavior%20from%20classroom%20videos%20is%20beneficial%20for%0Aanalyzing%20their%20classroom%20status%20and%20improving%20teaching%20efficiency.%20However%2C%0Alow%20accuracy%20in%20student%20classroom%20behavior%20detection%20is%20a%20prevalent%20issue.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20Spatio-Temporal%20Attention-Based%20Method%20for%0ADetecting%20Student%20Classroom%20Behaviors%20%28BDSTA%29.%20Firstly%2C%20the%20SlowFast%20network%20is%0Aused%20to%20generate%20motion%20and%20environmental%20information%20feature%20maps%20from%20the%0Avideo.%20Then%2C%20the%20spatio-temporal%20attention%20module%20is%20applied%20to%20the%20feature%0Amaps%2C%20including%20information%20aggregation%2C%20compression%20and%20stimulation%20processes.%0ASubsequently%2C%20attention%20maps%20in%20the%20time%2C%20channel%20and%20space%20dimensions%20are%0Aobtained%2C%20and%20multi-label%20behavior%20classification%20is%20performed%20based%20on%20these%0Aattention%20maps.%20To%20solve%20the%20long-tail%20data%20problem%20that%20exists%20in%20student%0Aclassroom%20behavior%20datasets%2C%20we%20use%20an%20improved%20focal%20loss%20function%20to%20assign%0Amore%20weight%20to%20the%20tail%20class%20data%20during%20training.%20Experimental%20results%20are%0Aconducted%20on%20a%20self-made%20student%20classroom%20behavior%20dataset%20named%20STSCB.%0ACompared%20with%20the%20SlowFast%20model%2C%20the%20average%20accuracy%20of%20student%20behavior%0Aclassification%20detection%20improves%20by%208.94%5C%25%20using%20BDSTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02523v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Spatio-Temporal%2520Attention-Based%2520Method%2520for%2520Detecting%2520Student%2520Classroom%250A%2520%2520Behaviors%26entry.906535625%3DFan%2520Yang%26entry.1292438233%3D%2520%2520Accurately%2520detecting%2520student%2520behavior%2520from%2520classroom%2520videos%2520is%2520beneficial%2520for%250Aanalyzing%2520their%2520classroom%2520status%2520and%2520improving%2520teaching%2520efficiency.%2520However%252C%250Alow%2520accuracy%2520in%2520student%2520classroom%2520behavior%2520detection%2520is%2520a%2520prevalent%2520issue.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520Spatio-Temporal%2520Attention-Based%2520Method%2520for%250ADetecting%2520Student%2520Classroom%2520Behaviors%2520%2528BDSTA%2529.%2520Firstly%252C%2520the%2520SlowFast%2520network%2520is%250Aused%2520to%2520generate%2520motion%2520and%2520environmental%2520information%2520feature%2520maps%2520from%2520the%250Avideo.%2520Then%252C%2520the%2520spatio-temporal%2520attention%2520module%2520is%2520applied%2520to%2520the%2520feature%250Amaps%252C%2520including%2520information%2520aggregation%252C%2520compression%2520and%2520stimulation%2520processes.%250ASubsequently%252C%2520attention%2520maps%2520in%2520the%2520time%252C%2520channel%2520and%2520space%2520dimensions%2520are%250Aobtained%252C%2520and%2520multi-label%2520behavior%2520classification%2520is%2520performed%2520based%2520on%2520these%250Aattention%2520maps.%2520To%2520solve%2520the%2520long-tail%2520data%2520problem%2520that%2520exists%2520in%2520student%250Aclassroom%2520behavior%2520datasets%252C%2520we%2520use%2520an%2520improved%2520focal%2520loss%2520function%2520to%2520assign%250Amore%2520weight%2520to%2520the%2520tail%2520class%2520data%2520during%2520training.%2520Experimental%2520results%2520are%250Aconducted%2520on%2520a%2520self-made%2520student%2520classroom%2520behavior%2520dataset%2520named%2520STSCB.%250ACompared%2520with%2520the%2520SlowFast%2520model%252C%2520the%2520average%2520accuracy%2520of%2520student%2520behavior%250Aclassification%2520detection%2520improves%2520by%25208.94%255C%2525%2520using%2520BDSTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02523v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spatio-Temporal%20Attention-Based%20Method%20for%20Detecting%20Student%20Classroom%0A%20%20Behaviors&entry.906535625=Fan%20Yang&entry.1292438233=%20%20Accurately%20detecting%20student%20behavior%20from%20classroom%20videos%20is%20beneficial%20for%0Aanalyzing%20their%20classroom%20status%20and%20improving%20teaching%20efficiency.%20However%2C%0Alow%20accuracy%20in%20student%20classroom%20behavior%20detection%20is%20a%20prevalent%20issue.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20Spatio-Temporal%20Attention-Based%20Method%20for%0ADetecting%20Student%20Classroom%20Behaviors%20%28BDSTA%29.%20Firstly%2C%20the%20SlowFast%20network%20is%0Aused%20to%20generate%20motion%20and%20environmental%20information%20feature%20maps%20from%20the%0Avideo.%20Then%2C%20the%20spatio-temporal%20attention%20module%20is%20applied%20to%20the%20feature%0Amaps%2C%20including%20information%20aggregation%2C%20compression%20and%20stimulation%20processes.%0ASubsequently%2C%20attention%20maps%20in%20the%20time%2C%20channel%20and%20space%20dimensions%20are%0Aobtained%2C%20and%20multi-label%20behavior%20classification%20is%20performed%20based%20on%20these%0Aattention%20maps.%20To%20solve%20the%20long-tail%20data%20problem%20that%20exists%20in%20student%0Aclassroom%20behavior%20datasets%2C%20we%20use%20an%20improved%20focal%20loss%20function%20to%20assign%0Amore%20weight%20to%20the%20tail%20class%20data%20during%20training.%20Experimental%20results%20are%0Aconducted%20on%20a%20self-made%20student%20classroom%20behavior%20dataset%20named%20STSCB.%0ACompared%20with%20the%20SlowFast%20model%2C%20the%20average%20accuracy%20of%20student%20behavior%0Aclassification%20detection%20improves%20by%208.94%5C%25%20using%20BDSTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02523v4&entry.124074799=Read"},
{"title": "SUM: Saliency Unification through Mamba for Visual Attention Modeling", "author": "Alireza Hosseini and Amirhossein Kazerouni and Saeed Akhavan and Michael Brudno and Babak Taati", "abstract": "  Visual attention modeling, important for interpreting and prioritizing visual\nstimuli, plays a significant role in applications such as marketing,\nmultimedia, and robotics. Traditional saliency prediction models, especially\nthose based on Convolutional Neural Networks (CNNs) or Transformers, achieve\nnotable success by leveraging large-scale annotated datasets. However, the\ncurrent state-of-the-art (SOTA) models that use Transformers are\ncomputationally expensive. Additionally, separate models are often required for\neach image type, lacking a unified approach. In this paper, we propose Saliency\nUnification through Mamba (SUM), a novel approach that integrates the efficient\nlong-range dependency modeling of Mamba with U-Net to provide a unified model\nfor diverse image types. Using a novel Conditional Visual State Space (C-VSS)\nblock, SUM dynamically adapts to various image types, including natural scenes,\nweb pages, and commercial imagery, ensuring universal applicability across\ndifferent data types. Our comprehensive evaluations across five benchmarks\ndemonstrate that SUM seamlessly adapts to different visual characteristics and\nconsistently outperforms existing models. These results position SUM as a\nversatile and powerful tool for advancing visual attention modeling, offering a\nrobust solution universally applicable across different types of visual\ncontent.\n", "link": "http://arxiv.org/abs/2406.17815v2", "date": "2024-09-09", "relevancy": 2.1801, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUM%3A%20Saliency%20Unification%20through%20Mamba%20for%20Visual%20Attention%20Modeling&body=Title%3A%20SUM%3A%20Saliency%20Unification%20through%20Mamba%20for%20Visual%20Attention%20Modeling%0AAuthor%3A%20Alireza%20Hosseini%20and%20Amirhossein%20Kazerouni%20and%20Saeed%20Akhavan%20and%20Michael%20Brudno%20and%20Babak%20Taati%0AAbstract%3A%20%20%20Visual%20attention%20modeling%2C%20important%20for%20interpreting%20and%20prioritizing%20visual%0Astimuli%2C%20plays%20a%20significant%20role%20in%20applications%20such%20as%20marketing%2C%0Amultimedia%2C%20and%20robotics.%20Traditional%20saliency%20prediction%20models%2C%20especially%0Athose%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20or%20Transformers%2C%20achieve%0Anotable%20success%20by%20leveraging%20large-scale%20annotated%20datasets.%20However%2C%20the%0Acurrent%20state-of-the-art%20%28SOTA%29%20models%20that%20use%20Transformers%20are%0Acomputationally%20expensive.%20Additionally%2C%20separate%20models%20are%20often%20required%20for%0Aeach%20image%20type%2C%20lacking%20a%20unified%20approach.%20In%20this%20paper%2C%20we%20propose%20Saliency%0AUnification%20through%20Mamba%20%28SUM%29%2C%20a%20novel%20approach%20that%20integrates%20the%20efficient%0Along-range%20dependency%20modeling%20of%20Mamba%20with%20U-Net%20to%20provide%20a%20unified%20model%0Afor%20diverse%20image%20types.%20Using%20a%20novel%20Conditional%20Visual%20State%20Space%20%28C-VSS%29%0Ablock%2C%20SUM%20dynamically%20adapts%20to%20various%20image%20types%2C%20including%20natural%20scenes%2C%0Aweb%20pages%2C%20and%20commercial%20imagery%2C%20ensuring%20universal%20applicability%20across%0Adifferent%20data%20types.%20Our%20comprehensive%20evaluations%20across%20five%20benchmarks%0Ademonstrate%20that%20SUM%20seamlessly%20adapts%20to%20different%20visual%20characteristics%20and%0Aconsistently%20outperforms%20existing%20models.%20These%20results%20position%20SUM%20as%20a%0Aversatile%20and%20powerful%20tool%20for%20advancing%20visual%20attention%20modeling%2C%20offering%20a%0Arobust%20solution%20universally%20applicable%20across%20different%20types%20of%20visual%0Acontent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUM%253A%2520Saliency%2520Unification%2520through%2520Mamba%2520for%2520Visual%2520Attention%2520Modeling%26entry.906535625%3DAlireza%2520Hosseini%2520and%2520Amirhossein%2520Kazerouni%2520and%2520Saeed%2520Akhavan%2520and%2520Michael%2520Brudno%2520and%2520Babak%2520Taati%26entry.1292438233%3D%2520%2520Visual%2520attention%2520modeling%252C%2520important%2520for%2520interpreting%2520and%2520prioritizing%2520visual%250Astimuli%252C%2520plays%2520a%2520significant%2520role%2520in%2520applications%2520such%2520as%2520marketing%252C%250Amultimedia%252C%2520and%2520robotics.%2520Traditional%2520saliency%2520prediction%2520models%252C%2520especially%250Athose%2520based%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520or%2520Transformers%252C%2520achieve%250Anotable%2520success%2520by%2520leveraging%2520large-scale%2520annotated%2520datasets.%2520However%252C%2520the%250Acurrent%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520that%2520use%2520Transformers%2520are%250Acomputationally%2520expensive.%2520Additionally%252C%2520separate%2520models%2520are%2520often%2520required%2520for%250Aeach%2520image%2520type%252C%2520lacking%2520a%2520unified%2520approach.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Saliency%250AUnification%2520through%2520Mamba%2520%2528SUM%2529%252C%2520a%2520novel%2520approach%2520that%2520integrates%2520the%2520efficient%250Along-range%2520dependency%2520modeling%2520of%2520Mamba%2520with%2520U-Net%2520to%2520provide%2520a%2520unified%2520model%250Afor%2520diverse%2520image%2520types.%2520Using%2520a%2520novel%2520Conditional%2520Visual%2520State%2520Space%2520%2528C-VSS%2529%250Ablock%252C%2520SUM%2520dynamically%2520adapts%2520to%2520various%2520image%2520types%252C%2520including%2520natural%2520scenes%252C%250Aweb%2520pages%252C%2520and%2520commercial%2520imagery%252C%2520ensuring%2520universal%2520applicability%2520across%250Adifferent%2520data%2520types.%2520Our%2520comprehensive%2520evaluations%2520across%2520five%2520benchmarks%250Ademonstrate%2520that%2520SUM%2520seamlessly%2520adapts%2520to%2520different%2520visual%2520characteristics%2520and%250Aconsistently%2520outperforms%2520existing%2520models.%2520These%2520results%2520position%2520SUM%2520as%2520a%250Aversatile%2520and%2520powerful%2520tool%2520for%2520advancing%2520visual%2520attention%2520modeling%252C%2520offering%2520a%250Arobust%2520solution%2520universally%2520applicable%2520across%2520different%2520types%2520of%2520visual%250Acontent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUM%3A%20Saliency%20Unification%20through%20Mamba%20for%20Visual%20Attention%20Modeling&entry.906535625=Alireza%20Hosseini%20and%20Amirhossein%20Kazerouni%20and%20Saeed%20Akhavan%20and%20Michael%20Brudno%20and%20Babak%20Taati&entry.1292438233=%20%20Visual%20attention%20modeling%2C%20important%20for%20interpreting%20and%20prioritizing%20visual%0Astimuli%2C%20plays%20a%20significant%20role%20in%20applications%20such%20as%20marketing%2C%0Amultimedia%2C%20and%20robotics.%20Traditional%20saliency%20prediction%20models%2C%20especially%0Athose%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20or%20Transformers%2C%20achieve%0Anotable%20success%20by%20leveraging%20large-scale%20annotated%20datasets.%20However%2C%20the%0Acurrent%20state-of-the-art%20%28SOTA%29%20models%20that%20use%20Transformers%20are%0Acomputationally%20expensive.%20Additionally%2C%20separate%20models%20are%20often%20required%20for%0Aeach%20image%20type%2C%20lacking%20a%20unified%20approach.%20In%20this%20paper%2C%20we%20propose%20Saliency%0AUnification%20through%20Mamba%20%28SUM%29%2C%20a%20novel%20approach%20that%20integrates%20the%20efficient%0Along-range%20dependency%20modeling%20of%20Mamba%20with%20U-Net%20to%20provide%20a%20unified%20model%0Afor%20diverse%20image%20types.%20Using%20a%20novel%20Conditional%20Visual%20State%20Space%20%28C-VSS%29%0Ablock%2C%20SUM%20dynamically%20adapts%20to%20various%20image%20types%2C%20including%20natural%20scenes%2C%0Aweb%20pages%2C%20and%20commercial%20imagery%2C%20ensuring%20universal%20applicability%20across%0Adifferent%20data%20types.%20Our%20comprehensive%20evaluations%20across%20five%20benchmarks%0Ademonstrate%20that%20SUM%20seamlessly%20adapts%20to%20different%20visual%20characteristics%20and%0Aconsistently%20outperforms%20existing%20models.%20These%20results%20position%20SUM%20as%20a%0Aversatile%20and%20powerful%20tool%20for%20advancing%20visual%20attention%20modeling%2C%20offering%20a%0Arobust%20solution%20universally%20applicable%20across%20different%20types%20of%20visual%0Acontent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17815v2&entry.124074799=Read"},
{"title": "Are Heterophily-Specific GNNs and Homophily Metrics Really Effective?\n  Evaluation Pitfalls and New Benchmarks", "author": "Sitao Luan and Qincheng Lu and Chenqing Hua and Xinyu Wang and Jiaqi Zhu and Xiao-Wen Chang and Guy Wolf and Jian Tang", "abstract": "  Over the past decade, Graph Neural Networks (GNNs) have achieved great\nsuccess on machine learning tasks with relational data. However, recent studies\nhave found that heterophily can cause significant performance degradation of\nGNNs, especially on node-level tasks. Numerous heterophilic benchmark datasets\nhave been put forward to validate the efficacy of heterophily-specific GNNs and\nvarious homophily metrics have been designed to help people recognize these\nmalignant datasets. Nevertheless, there still exist multiple pitfalls that\nseverely hinder the proper evaluation of new models and metrics. In this paper,\nwe point out three most serious pitfalls: 1) a lack of hyperparameter tuning;\n2) insufficient model evaluation on the real challenging heterophilic datasets;\n3) missing quantitative evaluation benchmark for homophily metrics on synthetic\ngraphs. To overcome these challenges, we first train and fine-tune baseline\nmodels on $27$ most widely used benchmark datasets, categorize them into three\ndistinct groups: malignant, benign and ambiguous heterophilic datasets, and\nidentify the real challenging subsets of tasks. To our best knowledge, we are\nthe first to propose such taxonomy. Then, we re-evaluate $10$\nheterophily-specific state-of-the-arts (SOTA) GNNs with fine-tuned\nhyperparameters on different groups of heterophilic datasets. Based on the\nmodel performance, we reassess their effectiveness on addressing heterophily\nchallenge. At last, we evaluate $11$ popular homophily metrics on synthetic\ngraphs with three different generation approaches. To compare the metrics\nstrictly, we propose the first quantitative evaluation method based on\nFr\\'echet distance.\n", "link": "http://arxiv.org/abs/2409.05755v1", "date": "2024-09-09", "relevancy": 2.1588, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4554}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4268}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Heterophily-Specific%20GNNs%20and%20Homophily%20Metrics%20Really%20Effective%3F%0A%20%20Evaluation%20Pitfalls%20and%20New%20Benchmarks&body=Title%3A%20Are%20Heterophily-Specific%20GNNs%20and%20Homophily%20Metrics%20Really%20Effective%3F%0A%20%20Evaluation%20Pitfalls%20and%20New%20Benchmarks%0AAuthor%3A%20Sitao%20Luan%20and%20Qincheng%20Lu%20and%20Chenqing%20Hua%20and%20Xinyu%20Wang%20and%20Jiaqi%20Zhu%20and%20Xiao-Wen%20Chang%20and%20Guy%20Wolf%20and%20Jian%20Tang%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20great%0Asuccess%20on%20machine%20learning%20tasks%20with%20relational%20data.%20However%2C%20recent%20studies%0Ahave%20found%20that%20heterophily%20can%20cause%20significant%20performance%20degradation%20of%0AGNNs%2C%20especially%20on%20node-level%20tasks.%20Numerous%20heterophilic%20benchmark%20datasets%0Ahave%20been%20put%20forward%20to%20validate%20the%20efficacy%20of%20heterophily-specific%20GNNs%20and%0Avarious%20homophily%20metrics%20have%20been%20designed%20to%20help%20people%20recognize%20these%0Amalignant%20datasets.%20Nevertheless%2C%20there%20still%20exist%20multiple%20pitfalls%20that%0Aseverely%20hinder%20the%20proper%20evaluation%20of%20new%20models%20and%20metrics.%20In%20this%20paper%2C%0Awe%20point%20out%20three%20most%20serious%20pitfalls%3A%201%29%20a%20lack%20of%20hyperparameter%20tuning%3B%0A2%29%20insufficient%20model%20evaluation%20on%20the%20real%20challenging%20heterophilic%20datasets%3B%0A3%29%20missing%20quantitative%20evaluation%20benchmark%20for%20homophily%20metrics%20on%20synthetic%0Agraphs.%20To%20overcome%20these%20challenges%2C%20we%20first%20train%20and%20fine-tune%20baseline%0Amodels%20on%20%2427%24%20most%20widely%20used%20benchmark%20datasets%2C%20categorize%20them%20into%20three%0Adistinct%20groups%3A%20malignant%2C%20benign%20and%20ambiguous%20heterophilic%20datasets%2C%20and%0Aidentify%20the%20real%20challenging%20subsets%20of%20tasks.%20To%20our%20best%20knowledge%2C%20we%20are%0Athe%20first%20to%20propose%20such%20taxonomy.%20Then%2C%20we%20re-evaluate%20%2410%24%0Aheterophily-specific%20state-of-the-arts%20%28SOTA%29%20GNNs%20with%20fine-tuned%0Ahyperparameters%20on%20different%20groups%20of%20heterophilic%20datasets.%20Based%20on%20the%0Amodel%20performance%2C%20we%20reassess%20their%20effectiveness%20on%20addressing%20heterophily%0Achallenge.%20At%20last%2C%20we%20evaluate%20%2411%24%20popular%20homophily%20metrics%20on%20synthetic%0Agraphs%20with%20three%20different%20generation%20approaches.%20To%20compare%20the%20metrics%0Astrictly%2C%20we%20propose%20the%20first%20quantitative%20evaluation%20method%20based%20on%0AFr%5C%27echet%20distance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Heterophily-Specific%2520GNNs%2520and%2520Homophily%2520Metrics%2520Really%2520Effective%253F%250A%2520%2520Evaluation%2520Pitfalls%2520and%2520New%2520Benchmarks%26entry.906535625%3DSitao%2520Luan%2520and%2520Qincheng%2520Lu%2520and%2520Chenqing%2520Hua%2520and%2520Xinyu%2520Wang%2520and%2520Jiaqi%2520Zhu%2520and%2520Xiao-Wen%2520Chang%2520and%2520Guy%2520Wolf%2520and%2520Jian%2520Tang%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520great%250Asuccess%2520on%2520machine%2520learning%2520tasks%2520with%2520relational%2520data.%2520However%252C%2520recent%2520studies%250Ahave%2520found%2520that%2520heterophily%2520can%2520cause%2520significant%2520performance%2520degradation%2520of%250AGNNs%252C%2520especially%2520on%2520node-level%2520tasks.%2520Numerous%2520heterophilic%2520benchmark%2520datasets%250Ahave%2520been%2520put%2520forward%2520to%2520validate%2520the%2520efficacy%2520of%2520heterophily-specific%2520GNNs%2520and%250Avarious%2520homophily%2520metrics%2520have%2520been%2520designed%2520to%2520help%2520people%2520recognize%2520these%250Amalignant%2520datasets.%2520Nevertheless%252C%2520there%2520still%2520exist%2520multiple%2520pitfalls%2520that%250Aseverely%2520hinder%2520the%2520proper%2520evaluation%2520of%2520new%2520models%2520and%2520metrics.%2520In%2520this%2520paper%252C%250Awe%2520point%2520out%2520three%2520most%2520serious%2520pitfalls%253A%25201%2529%2520a%2520lack%2520of%2520hyperparameter%2520tuning%253B%250A2%2529%2520insufficient%2520model%2520evaluation%2520on%2520the%2520real%2520challenging%2520heterophilic%2520datasets%253B%250A3%2529%2520missing%2520quantitative%2520evaluation%2520benchmark%2520for%2520homophily%2520metrics%2520on%2520synthetic%250Agraphs.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520first%2520train%2520and%2520fine-tune%2520baseline%250Amodels%2520on%2520%252427%2524%2520most%2520widely%2520used%2520benchmark%2520datasets%252C%2520categorize%2520them%2520into%2520three%250Adistinct%2520groups%253A%2520malignant%252C%2520benign%2520and%2520ambiguous%2520heterophilic%2520datasets%252C%2520and%250Aidentify%2520the%2520real%2520challenging%2520subsets%2520of%2520tasks.%2520To%2520our%2520best%2520knowledge%252C%2520we%2520are%250Athe%2520first%2520to%2520propose%2520such%2520taxonomy.%2520Then%252C%2520we%2520re-evaluate%2520%252410%2524%250Aheterophily-specific%2520state-of-the-arts%2520%2528SOTA%2529%2520GNNs%2520with%2520fine-tuned%250Ahyperparameters%2520on%2520different%2520groups%2520of%2520heterophilic%2520datasets.%2520Based%2520on%2520the%250Amodel%2520performance%252C%2520we%2520reassess%2520their%2520effectiveness%2520on%2520addressing%2520heterophily%250Achallenge.%2520At%2520last%252C%2520we%2520evaluate%2520%252411%2524%2520popular%2520homophily%2520metrics%2520on%2520synthetic%250Agraphs%2520with%2520three%2520different%2520generation%2520approaches.%2520To%2520compare%2520the%2520metrics%250Astrictly%252C%2520we%2520propose%2520the%2520first%2520quantitative%2520evaluation%2520method%2520based%2520on%250AFr%255C%2527echet%2520distance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Heterophily-Specific%20GNNs%20and%20Homophily%20Metrics%20Really%20Effective%3F%0A%20%20Evaluation%20Pitfalls%20and%20New%20Benchmarks&entry.906535625=Sitao%20Luan%20and%20Qincheng%20Lu%20and%20Chenqing%20Hua%20and%20Xinyu%20Wang%20and%20Jiaqi%20Zhu%20and%20Xiao-Wen%20Chang%20and%20Guy%20Wolf%20and%20Jian%20Tang&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20great%0Asuccess%20on%20machine%20learning%20tasks%20with%20relational%20data.%20However%2C%20recent%20studies%0Ahave%20found%20that%20heterophily%20can%20cause%20significant%20performance%20degradation%20of%0AGNNs%2C%20especially%20on%20node-level%20tasks.%20Numerous%20heterophilic%20benchmark%20datasets%0Ahave%20been%20put%20forward%20to%20validate%20the%20efficacy%20of%20heterophily-specific%20GNNs%20and%0Avarious%20homophily%20metrics%20have%20been%20designed%20to%20help%20people%20recognize%20these%0Amalignant%20datasets.%20Nevertheless%2C%20there%20still%20exist%20multiple%20pitfalls%20that%0Aseverely%20hinder%20the%20proper%20evaluation%20of%20new%20models%20and%20metrics.%20In%20this%20paper%2C%0Awe%20point%20out%20three%20most%20serious%20pitfalls%3A%201%29%20a%20lack%20of%20hyperparameter%20tuning%3B%0A2%29%20insufficient%20model%20evaluation%20on%20the%20real%20challenging%20heterophilic%20datasets%3B%0A3%29%20missing%20quantitative%20evaluation%20benchmark%20for%20homophily%20metrics%20on%20synthetic%0Agraphs.%20To%20overcome%20these%20challenges%2C%20we%20first%20train%20and%20fine-tune%20baseline%0Amodels%20on%20%2427%24%20most%20widely%20used%20benchmark%20datasets%2C%20categorize%20them%20into%20three%0Adistinct%20groups%3A%20malignant%2C%20benign%20and%20ambiguous%20heterophilic%20datasets%2C%20and%0Aidentify%20the%20real%20challenging%20subsets%20of%20tasks.%20To%20our%20best%20knowledge%2C%20we%20are%0Athe%20first%20to%20propose%20such%20taxonomy.%20Then%2C%20we%20re-evaluate%20%2410%24%0Aheterophily-specific%20state-of-the-arts%20%28SOTA%29%20GNNs%20with%20fine-tuned%0Ahyperparameters%20on%20different%20groups%20of%20heterophilic%20datasets.%20Based%20on%20the%0Amodel%20performance%2C%20we%20reassess%20their%20effectiveness%20on%20addressing%20heterophily%0Achallenge.%20At%20last%2C%20we%20evaluate%20%2411%24%20popular%20homophily%20metrics%20on%20synthetic%0Agraphs%20with%20three%20different%20generation%20approaches.%20To%20compare%20the%20metrics%0Astrictly%2C%20we%20propose%20the%20first%20quantitative%20evaluation%20method%20based%20on%0AFr%5C%27echet%20distance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05755v1&entry.124074799=Read"},
{"title": "MeshBrush: Painting the Anatomical Mesh with Neural Stylization for\n  Endoscopy", "author": "John J. Han and Ayberk Acar and Nicholas Kavoussi and Jie Ying Wu", "abstract": "  Style transfer is a promising approach to close the sim-to-real gap in\nmedical endoscopy. Rendering synthetic endoscopic videos by traversing\npre-operative scans (such as MRI or CT) can generate structurally accurate\nsimulations as well as ground truth camera poses and depth maps. Although\nimage-to-image (I2I) translation models such as CycleGAN can imitate realistic\nendoscopic images from these simulations, they are unsuitable for\nvideo-to-video synthesis due to the lack of temporal consistency, resulting in\nartifacts between frames. We propose MeshBrush, a neural mesh stylization\nmethod to synthesize temporally consistent videos with differentiable\nrendering. MeshBrush uses the underlying geometry of patient imaging data while\nleveraging existing I2I methods. With learned per-vertex textures, the stylized\nmesh guarantees consistency while producing high-fidelity outputs. We\ndemonstrate that mesh stylization is a promising approach for creating\nrealistic simulations for downstream tasks such as training networks and\npreoperative planning. Although our method is tested and designed for\nureteroscopy, its components are transferable to general endoscopic and\nlaparoscopic procedures. The code will be made public on GitHub.\n", "link": "http://arxiv.org/abs/2404.02999v2", "date": "2024-09-09", "relevancy": 2.1523, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5928}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5011}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshBrush%3A%20Painting%20the%20Anatomical%20Mesh%20with%20Neural%20Stylization%20for%0A%20%20Endoscopy&body=Title%3A%20MeshBrush%3A%20Painting%20the%20Anatomical%20Mesh%20with%20Neural%20Stylization%20for%0A%20%20Endoscopy%0AAuthor%3A%20John%20J.%20Han%20and%20Ayberk%20Acar%20and%20Nicholas%20Kavoussi%20and%20Jie%20Ying%20Wu%0AAbstract%3A%20%20%20Style%20transfer%20is%20a%20promising%20approach%20to%20close%20the%20sim-to-real%20gap%20in%0Amedical%20endoscopy.%20Rendering%20synthetic%20endoscopic%20videos%20by%20traversing%0Apre-operative%20scans%20%28such%20as%20MRI%20or%20CT%29%20can%20generate%20structurally%20accurate%0Asimulations%20as%20well%20as%20ground%20truth%20camera%20poses%20and%20depth%20maps.%20Although%0Aimage-to-image%20%28I2I%29%20translation%20models%20such%20as%20CycleGAN%20can%20imitate%20realistic%0Aendoscopic%20images%20from%20these%20simulations%2C%20they%20are%20unsuitable%20for%0Avideo-to-video%20synthesis%20due%20to%20the%20lack%20of%20temporal%20consistency%2C%20resulting%20in%0Aartifacts%20between%20frames.%20We%20propose%20MeshBrush%2C%20a%20neural%20mesh%20stylization%0Amethod%20to%20synthesize%20temporally%20consistent%20videos%20with%20differentiable%0Arendering.%20MeshBrush%20uses%20the%20underlying%20geometry%20of%20patient%20imaging%20data%20while%0Aleveraging%20existing%20I2I%20methods.%20With%20learned%20per-vertex%20textures%2C%20the%20stylized%0Amesh%20guarantees%20consistency%20while%20producing%20high-fidelity%20outputs.%20We%0Ademonstrate%20that%20mesh%20stylization%20is%20a%20promising%20approach%20for%20creating%0Arealistic%20simulations%20for%20downstream%20tasks%20such%20as%20training%20networks%20and%0Apreoperative%20planning.%20Although%20our%20method%20is%20tested%20and%20designed%20for%0Aureteroscopy%2C%20its%20components%20are%20transferable%20to%20general%20endoscopic%20and%0Alaparoscopic%20procedures.%20The%20code%20will%20be%20made%20public%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshBrush%253A%2520Painting%2520the%2520Anatomical%2520Mesh%2520with%2520Neural%2520Stylization%2520for%250A%2520%2520Endoscopy%26entry.906535625%3DJohn%2520J.%2520Han%2520and%2520Ayberk%2520Acar%2520and%2520Nicholas%2520Kavoussi%2520and%2520Jie%2520Ying%2520Wu%26entry.1292438233%3D%2520%2520Style%2520transfer%2520is%2520a%2520promising%2520approach%2520to%2520close%2520the%2520sim-to-real%2520gap%2520in%250Amedical%2520endoscopy.%2520Rendering%2520synthetic%2520endoscopic%2520videos%2520by%2520traversing%250Apre-operative%2520scans%2520%2528such%2520as%2520MRI%2520or%2520CT%2529%2520can%2520generate%2520structurally%2520accurate%250Asimulations%2520as%2520well%2520as%2520ground%2520truth%2520camera%2520poses%2520and%2520depth%2520maps.%2520Although%250Aimage-to-image%2520%2528I2I%2529%2520translation%2520models%2520such%2520as%2520CycleGAN%2520can%2520imitate%2520realistic%250Aendoscopic%2520images%2520from%2520these%2520simulations%252C%2520they%2520are%2520unsuitable%2520for%250Avideo-to-video%2520synthesis%2520due%2520to%2520the%2520lack%2520of%2520temporal%2520consistency%252C%2520resulting%2520in%250Aartifacts%2520between%2520frames.%2520We%2520propose%2520MeshBrush%252C%2520a%2520neural%2520mesh%2520stylization%250Amethod%2520to%2520synthesize%2520temporally%2520consistent%2520videos%2520with%2520differentiable%250Arendering.%2520MeshBrush%2520uses%2520the%2520underlying%2520geometry%2520of%2520patient%2520imaging%2520data%2520while%250Aleveraging%2520existing%2520I2I%2520methods.%2520With%2520learned%2520per-vertex%2520textures%252C%2520the%2520stylized%250Amesh%2520guarantees%2520consistency%2520while%2520producing%2520high-fidelity%2520outputs.%2520We%250Ademonstrate%2520that%2520mesh%2520stylization%2520is%2520a%2520promising%2520approach%2520for%2520creating%250Arealistic%2520simulations%2520for%2520downstream%2520tasks%2520such%2520as%2520training%2520networks%2520and%250Apreoperative%2520planning.%2520Although%2520our%2520method%2520is%2520tested%2520and%2520designed%2520for%250Aureteroscopy%252C%2520its%2520components%2520are%2520transferable%2520to%2520general%2520endoscopic%2520and%250Alaparoscopic%2520procedures.%2520The%2520code%2520will%2520be%2520made%2520public%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshBrush%3A%20Painting%20the%20Anatomical%20Mesh%20with%20Neural%20Stylization%20for%0A%20%20Endoscopy&entry.906535625=John%20J.%20Han%20and%20Ayberk%20Acar%20and%20Nicholas%20Kavoussi%20and%20Jie%20Ying%20Wu&entry.1292438233=%20%20Style%20transfer%20is%20a%20promising%20approach%20to%20close%20the%20sim-to-real%20gap%20in%0Amedical%20endoscopy.%20Rendering%20synthetic%20endoscopic%20videos%20by%20traversing%0Apre-operative%20scans%20%28such%20as%20MRI%20or%20CT%29%20can%20generate%20structurally%20accurate%0Asimulations%20as%20well%20as%20ground%20truth%20camera%20poses%20and%20depth%20maps.%20Although%0Aimage-to-image%20%28I2I%29%20translation%20models%20such%20as%20CycleGAN%20can%20imitate%20realistic%0Aendoscopic%20images%20from%20these%20simulations%2C%20they%20are%20unsuitable%20for%0Avideo-to-video%20synthesis%20due%20to%20the%20lack%20of%20temporal%20consistency%2C%20resulting%20in%0Aartifacts%20between%20frames.%20We%20propose%20MeshBrush%2C%20a%20neural%20mesh%20stylization%0Amethod%20to%20synthesize%20temporally%20consistent%20videos%20with%20differentiable%0Arendering.%20MeshBrush%20uses%20the%20underlying%20geometry%20of%20patient%20imaging%20data%20while%0Aleveraging%20existing%20I2I%20methods.%20With%20learned%20per-vertex%20textures%2C%20the%20stylized%0Amesh%20guarantees%20consistency%20while%20producing%20high-fidelity%20outputs.%20We%0Ademonstrate%20that%20mesh%20stylization%20is%20a%20promising%20approach%20for%20creating%0Arealistic%20simulations%20for%20downstream%20tasks%20such%20as%20training%20networks%20and%0Apreoperative%20planning.%20Although%20our%20method%20is%20tested%20and%20designed%20for%0Aureteroscopy%2C%20its%20components%20are%20transferable%20to%20general%20endoscopic%20and%0Alaparoscopic%20procedures.%20The%20code%20will%20be%20made%20public%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02999v2&entry.124074799=Read"},
{"title": "Cherenkov Imaged Bio-morphological Features Verify Patient Positioning\n  with Deformable Tissue Translocation in Breast Radiotherapy", "author": "Yao Chen and Savannah M. Decker and Petr Bruza and David J. Gladstone and Lesley A. Jarvis and Brian W. Pogue and Kimberley S. Samkoe and Rongxiao Zhang", "abstract": "  Accurate patient positioning is critical for precise radiotherapy dose\ndelivery, as positioning errors can significantly affect treatment outcomes.\nThis study introduces a novel method for tracking loco-regional tissue\ndeformation through Cherenkov image analysis during fractionated breast cancer\nradiotherapy. The primary goal was to develop and test an algorithm for\nCherenkov-based regional position accuracy quantification, specifically for\nloco-regional deformations, which lack ideal quantification methods in\nradiotherapy. Blood vessel detection and segmentation were developed in\nCherenkov images using a tissue phantom with incremental movements, and later\napplied to images from fractionated whole breast radiotherapy in human patients\n(n=10). A combined rigid and non-rigid registration technique was used to\ndetect inter- and intra-fractional positioning variations. This approach\nquantified positioning variations in two parts: a global shift from rigid\nregistration and a two-dimensional variation map of loco-regional deformation\nfrom non-rigid registration. The methodology was validated using an\nanthropomorphic chest phantom experiment, where known treatment couch\ntranslations and respiratory motion were simulated to assess inter- and\nintra-fractional uncertainties, yielding an average accuracy of 0.83 mm for\ncouch translations up to 20 mm. Analysis of clinical Cherenkov data from ten\nbreast cancer patients showed an inter-fraction setup variation of 3.7 plus\nminus 2.4 mm relative to the first fraction and loco-regional deformations\n(95th percentile) of up to 3.3 plus minus 1.9 mm. This study presents a\nCherenkov-based approach to quantify global and local positioning variations,\ndemonstrating feasibility in addressing loco-regional deformations that\nconventional imaging techniques fail to capture.\n", "link": "http://arxiv.org/abs/2409.05680v1", "date": "2024-09-09", "relevancy": 2.1298, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4301}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4239}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cherenkov%20Imaged%20Bio-morphological%20Features%20Verify%20Patient%20Positioning%0A%20%20with%20Deformable%20Tissue%20Translocation%20in%20Breast%20Radiotherapy&body=Title%3A%20Cherenkov%20Imaged%20Bio-morphological%20Features%20Verify%20Patient%20Positioning%0A%20%20with%20Deformable%20Tissue%20Translocation%20in%20Breast%20Radiotherapy%0AAuthor%3A%20Yao%20Chen%20and%20Savannah%20M.%20Decker%20and%20Petr%20Bruza%20and%20David%20J.%20Gladstone%20and%20Lesley%20A.%20Jarvis%20and%20Brian%20W.%20Pogue%20and%20Kimberley%20S.%20Samkoe%20and%20Rongxiao%20Zhang%0AAbstract%3A%20%20%20Accurate%20patient%20positioning%20is%20critical%20for%20precise%20radiotherapy%20dose%0Adelivery%2C%20as%20positioning%20errors%20can%20significantly%20affect%20treatment%20outcomes.%0AThis%20study%20introduces%20a%20novel%20method%20for%20tracking%20loco-regional%20tissue%0Adeformation%20through%20Cherenkov%20image%20analysis%20during%20fractionated%20breast%20cancer%0Aradiotherapy.%20The%20primary%20goal%20was%20to%20develop%20and%20test%20an%20algorithm%20for%0ACherenkov-based%20regional%20position%20accuracy%20quantification%2C%20specifically%20for%0Aloco-regional%20deformations%2C%20which%20lack%20ideal%20quantification%20methods%20in%0Aradiotherapy.%20Blood%20vessel%20detection%20and%20segmentation%20were%20developed%20in%0ACherenkov%20images%20using%20a%20tissue%20phantom%20with%20incremental%20movements%2C%20and%20later%0Aapplied%20to%20images%20from%20fractionated%20whole%20breast%20radiotherapy%20in%20human%20patients%0A%28n%3D10%29.%20A%20combined%20rigid%20and%20non-rigid%20registration%20technique%20was%20used%20to%0Adetect%20inter-%20and%20intra-fractional%20positioning%20variations.%20This%20approach%0Aquantified%20positioning%20variations%20in%20two%20parts%3A%20a%20global%20shift%20from%20rigid%0Aregistration%20and%20a%20two-dimensional%20variation%20map%20of%20loco-regional%20deformation%0Afrom%20non-rigid%20registration.%20The%20methodology%20was%20validated%20using%20an%0Aanthropomorphic%20chest%20phantom%20experiment%2C%20where%20known%20treatment%20couch%0Atranslations%20and%20respiratory%20motion%20were%20simulated%20to%20assess%20inter-%20and%0Aintra-fractional%20uncertainties%2C%20yielding%20an%20average%20accuracy%20of%200.83%20mm%20for%0Acouch%20translations%20up%20to%2020%20mm.%20Analysis%20of%20clinical%20Cherenkov%20data%20from%20ten%0Abreast%20cancer%20patients%20showed%20an%20inter-fraction%20setup%20variation%20of%203.7%20plus%0Aminus%202.4%20mm%20relative%20to%20the%20first%20fraction%20and%20loco-regional%20deformations%0A%2895th%20percentile%29%20of%20up%20to%203.3%20plus%20minus%201.9%20mm.%20This%20study%20presents%20a%0ACherenkov-based%20approach%20to%20quantify%20global%20and%20local%20positioning%20variations%2C%0Ademonstrating%20feasibility%20in%20addressing%20loco-regional%20deformations%20that%0Aconventional%20imaging%20techniques%20fail%20to%20capture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCherenkov%2520Imaged%2520Bio-morphological%2520Features%2520Verify%2520Patient%2520Positioning%250A%2520%2520with%2520Deformable%2520Tissue%2520Translocation%2520in%2520Breast%2520Radiotherapy%26entry.906535625%3DYao%2520Chen%2520and%2520Savannah%2520M.%2520Decker%2520and%2520Petr%2520Bruza%2520and%2520David%2520J.%2520Gladstone%2520and%2520Lesley%2520A.%2520Jarvis%2520and%2520Brian%2520W.%2520Pogue%2520and%2520Kimberley%2520S.%2520Samkoe%2520and%2520Rongxiao%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%2520patient%2520positioning%2520is%2520critical%2520for%2520precise%2520radiotherapy%2520dose%250Adelivery%252C%2520as%2520positioning%2520errors%2520can%2520significantly%2520affect%2520treatment%2520outcomes.%250AThis%2520study%2520introduces%2520a%2520novel%2520method%2520for%2520tracking%2520loco-regional%2520tissue%250Adeformation%2520through%2520Cherenkov%2520image%2520analysis%2520during%2520fractionated%2520breast%2520cancer%250Aradiotherapy.%2520The%2520primary%2520goal%2520was%2520to%2520develop%2520and%2520test%2520an%2520algorithm%2520for%250ACherenkov-based%2520regional%2520position%2520accuracy%2520quantification%252C%2520specifically%2520for%250Aloco-regional%2520deformations%252C%2520which%2520lack%2520ideal%2520quantification%2520methods%2520in%250Aradiotherapy.%2520Blood%2520vessel%2520detection%2520and%2520segmentation%2520were%2520developed%2520in%250ACherenkov%2520images%2520using%2520a%2520tissue%2520phantom%2520with%2520incremental%2520movements%252C%2520and%2520later%250Aapplied%2520to%2520images%2520from%2520fractionated%2520whole%2520breast%2520radiotherapy%2520in%2520human%2520patients%250A%2528n%253D10%2529.%2520A%2520combined%2520rigid%2520and%2520non-rigid%2520registration%2520technique%2520was%2520used%2520to%250Adetect%2520inter-%2520and%2520intra-fractional%2520positioning%2520variations.%2520This%2520approach%250Aquantified%2520positioning%2520variations%2520in%2520two%2520parts%253A%2520a%2520global%2520shift%2520from%2520rigid%250Aregistration%2520and%2520a%2520two-dimensional%2520variation%2520map%2520of%2520loco-regional%2520deformation%250Afrom%2520non-rigid%2520registration.%2520The%2520methodology%2520was%2520validated%2520using%2520an%250Aanthropomorphic%2520chest%2520phantom%2520experiment%252C%2520where%2520known%2520treatment%2520couch%250Atranslations%2520and%2520respiratory%2520motion%2520were%2520simulated%2520to%2520assess%2520inter-%2520and%250Aintra-fractional%2520uncertainties%252C%2520yielding%2520an%2520average%2520accuracy%2520of%25200.83%2520mm%2520for%250Acouch%2520translations%2520up%2520to%252020%2520mm.%2520Analysis%2520of%2520clinical%2520Cherenkov%2520data%2520from%2520ten%250Abreast%2520cancer%2520patients%2520showed%2520an%2520inter-fraction%2520setup%2520variation%2520of%25203.7%2520plus%250Aminus%25202.4%2520mm%2520relative%2520to%2520the%2520first%2520fraction%2520and%2520loco-regional%2520deformations%250A%252895th%2520percentile%2529%2520of%2520up%2520to%25203.3%2520plus%2520minus%25201.9%2520mm.%2520This%2520study%2520presents%2520a%250ACherenkov-based%2520approach%2520to%2520quantify%2520global%2520and%2520local%2520positioning%2520variations%252C%250Ademonstrating%2520feasibility%2520in%2520addressing%2520loco-regional%2520deformations%2520that%250Aconventional%2520imaging%2520techniques%2520fail%2520to%2520capture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cherenkov%20Imaged%20Bio-morphological%20Features%20Verify%20Patient%20Positioning%0A%20%20with%20Deformable%20Tissue%20Translocation%20in%20Breast%20Radiotherapy&entry.906535625=Yao%20Chen%20and%20Savannah%20M.%20Decker%20and%20Petr%20Bruza%20and%20David%20J.%20Gladstone%20and%20Lesley%20A.%20Jarvis%20and%20Brian%20W.%20Pogue%20and%20Kimberley%20S.%20Samkoe%20and%20Rongxiao%20Zhang&entry.1292438233=%20%20Accurate%20patient%20positioning%20is%20critical%20for%20precise%20radiotherapy%20dose%0Adelivery%2C%20as%20positioning%20errors%20can%20significantly%20affect%20treatment%20outcomes.%0AThis%20study%20introduces%20a%20novel%20method%20for%20tracking%20loco-regional%20tissue%0Adeformation%20through%20Cherenkov%20image%20analysis%20during%20fractionated%20breast%20cancer%0Aradiotherapy.%20The%20primary%20goal%20was%20to%20develop%20and%20test%20an%20algorithm%20for%0ACherenkov-based%20regional%20position%20accuracy%20quantification%2C%20specifically%20for%0Aloco-regional%20deformations%2C%20which%20lack%20ideal%20quantification%20methods%20in%0Aradiotherapy.%20Blood%20vessel%20detection%20and%20segmentation%20were%20developed%20in%0ACherenkov%20images%20using%20a%20tissue%20phantom%20with%20incremental%20movements%2C%20and%20later%0Aapplied%20to%20images%20from%20fractionated%20whole%20breast%20radiotherapy%20in%20human%20patients%0A%28n%3D10%29.%20A%20combined%20rigid%20and%20non-rigid%20registration%20technique%20was%20used%20to%0Adetect%20inter-%20and%20intra-fractional%20positioning%20variations.%20This%20approach%0Aquantified%20positioning%20variations%20in%20two%20parts%3A%20a%20global%20shift%20from%20rigid%0Aregistration%20and%20a%20two-dimensional%20variation%20map%20of%20loco-regional%20deformation%0Afrom%20non-rigid%20registration.%20The%20methodology%20was%20validated%20using%20an%0Aanthropomorphic%20chest%20phantom%20experiment%2C%20where%20known%20treatment%20couch%0Atranslations%20and%20respiratory%20motion%20were%20simulated%20to%20assess%20inter-%20and%0Aintra-fractional%20uncertainties%2C%20yielding%20an%20average%20accuracy%20of%200.83%20mm%20for%0Acouch%20translations%20up%20to%2020%20mm.%20Analysis%20of%20clinical%20Cherenkov%20data%20from%20ten%0Abreast%20cancer%20patients%20showed%20an%20inter-fraction%20setup%20variation%20of%203.7%20plus%0Aminus%202.4%20mm%20relative%20to%20the%20first%20fraction%20and%20loco-regional%20deformations%0A%2895th%20percentile%29%20of%20up%20to%203.3%20plus%20minus%201.9%20mm.%20This%20study%20presents%20a%0ACherenkov-based%20approach%20to%20quantify%20global%20and%20local%20positioning%20variations%2C%0Ademonstrating%20feasibility%20in%20addressing%20loco-regional%20deformations%20that%0Aconventional%20imaging%20techniques%20fail%20to%20capture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05680v1&entry.124074799=Read"},
{"title": "AttentionX: Exploiting Consensus Discrepancy In Attention from A\n  Distributed Optimization Perspective", "author": "Guoqiang Zhang and Richard Heusdens", "abstract": "  In this paper, we extend the standard Attention in transformer by exploiting\nthe consensus discrepancy from a distributed optimization perspective, referred\nto as AttentionX. It is noted that the primal-dual method of multipliers (PDMM)\n\\cite{Zhang16PDMM} is designed to iteratively solve a broad class of\ndistributed optimization problems over a pear-to-pear (P2P) network, where\nneighbouring nodes gradually reach consensus as specified by predefined linear\nedge-constraints in the optimization process. In particular, at each iteration\nof PDMM, each node in a network first performs information-gathering from\nneighbours and then performs local information-fusion. From a high-level point\nof view, the $KQ$-softmax-based weighted summation of $V$-representations in\nAttention corresponds information-gathering from neighbours while the\nfeature-processing via the feed-forward network (FFN) in transformer\ncorresponds to local information fusion. PDMM exploits the Lagrangian\nmultipliers to capture the historical consensus discrepancy in the form of\nresidual errors of the linear edge-constraints, which plays a crucial role for\nthe algorithm to converge. Inspired by PDMM, we propose AttentionX to\nincorporate the consensus discrepancy in the output update-expression of the\nstandard Attention. The consensus discrepancy in AttentionX refers to the\ndifference between the weighted summation of $V$-representations and scaled\n$V$-representions themselves. Experiments on ViT and nanoGPT show promising\nperformance.\n", "link": "http://arxiv.org/abs/2409.04275v2", "date": "2024-09-09", "relevancy": 2.1246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5615}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5276}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionX%3A%20Exploiting%20Consensus%20Discrepancy%20In%20Attention%20from%20A%0A%20%20Distributed%20Optimization%20Perspective&body=Title%3A%20AttentionX%3A%20Exploiting%20Consensus%20Discrepancy%20In%20Attention%20from%20A%0A%20%20Distributed%20Optimization%20Perspective%0AAuthor%3A%20Guoqiang%20Zhang%20and%20Richard%20Heusdens%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20extend%20the%20standard%20Attention%20in%20transformer%20by%20exploiting%0Athe%20consensus%20discrepancy%20from%20a%20distributed%20optimization%20perspective%2C%20referred%0Ato%20as%20AttentionX.%20It%20is%20noted%20that%20the%20primal-dual%20method%20of%20multipliers%20%28PDMM%29%0A%5Ccite%7BZhang16PDMM%7D%20is%20designed%20to%20iteratively%20solve%20a%20broad%20class%20of%0Adistributed%20optimization%20problems%20over%20a%20pear-to-pear%20%28P2P%29%20network%2C%20where%0Aneighbouring%20nodes%20gradually%20reach%20consensus%20as%20specified%20by%20predefined%20linear%0Aedge-constraints%20in%20the%20optimization%20process.%20In%20particular%2C%20at%20each%20iteration%0Aof%20PDMM%2C%20each%20node%20in%20a%20network%20first%20performs%20information-gathering%20from%0Aneighbours%20and%20then%20performs%20local%20information-fusion.%20From%20a%20high-level%20point%0Aof%20view%2C%20the%20%24KQ%24-softmax-based%20weighted%20summation%20of%20%24V%24-representations%20in%0AAttention%20corresponds%20information-gathering%20from%20neighbours%20while%20the%0Afeature-processing%20via%20the%20feed-forward%20network%20%28FFN%29%20in%20transformer%0Acorresponds%20to%20local%20information%20fusion.%20PDMM%20exploits%20the%20Lagrangian%0Amultipliers%20to%20capture%20the%20historical%20consensus%20discrepancy%20in%20the%20form%20of%0Aresidual%20errors%20of%20the%20linear%20edge-constraints%2C%20which%20plays%20a%20crucial%20role%20for%0Athe%20algorithm%20to%20converge.%20Inspired%20by%20PDMM%2C%20we%20propose%20AttentionX%20to%0Aincorporate%20the%20consensus%20discrepancy%20in%20the%20output%20update-expression%20of%20the%0Astandard%20Attention.%20The%20consensus%20discrepancy%20in%20AttentionX%20refers%20to%20the%0Adifference%20between%20the%20weighted%20summation%20of%20%24V%24-representations%20and%20scaled%0A%24V%24-representions%20themselves.%20Experiments%20on%20ViT%20and%20nanoGPT%20show%20promising%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionX%253A%2520Exploiting%2520Consensus%2520Discrepancy%2520In%2520Attention%2520from%2520A%250A%2520%2520Distributed%2520Optimization%2520Perspective%26entry.906535625%3DGuoqiang%2520Zhang%2520and%2520Richard%2520Heusdens%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520standard%2520Attention%2520in%2520transformer%2520by%2520exploiting%250Athe%2520consensus%2520discrepancy%2520from%2520a%2520distributed%2520optimization%2520perspective%252C%2520referred%250Ato%2520as%2520AttentionX.%2520It%2520is%2520noted%2520that%2520the%2520primal-dual%2520method%2520of%2520multipliers%2520%2528PDMM%2529%250A%255Ccite%257BZhang16PDMM%257D%2520is%2520designed%2520to%2520iteratively%2520solve%2520a%2520broad%2520class%2520of%250Adistributed%2520optimization%2520problems%2520over%2520a%2520pear-to-pear%2520%2528P2P%2529%2520network%252C%2520where%250Aneighbouring%2520nodes%2520gradually%2520reach%2520consensus%2520as%2520specified%2520by%2520predefined%2520linear%250Aedge-constraints%2520in%2520the%2520optimization%2520process.%2520In%2520particular%252C%2520at%2520each%2520iteration%250Aof%2520PDMM%252C%2520each%2520node%2520in%2520a%2520network%2520first%2520performs%2520information-gathering%2520from%250Aneighbours%2520and%2520then%2520performs%2520local%2520information-fusion.%2520From%2520a%2520high-level%2520point%250Aof%2520view%252C%2520the%2520%2524KQ%2524-softmax-based%2520weighted%2520summation%2520of%2520%2524V%2524-representations%2520in%250AAttention%2520corresponds%2520information-gathering%2520from%2520neighbours%2520while%2520the%250Afeature-processing%2520via%2520the%2520feed-forward%2520network%2520%2528FFN%2529%2520in%2520transformer%250Acorresponds%2520to%2520local%2520information%2520fusion.%2520PDMM%2520exploits%2520the%2520Lagrangian%250Amultipliers%2520to%2520capture%2520the%2520historical%2520consensus%2520discrepancy%2520in%2520the%2520form%2520of%250Aresidual%2520errors%2520of%2520the%2520linear%2520edge-constraints%252C%2520which%2520plays%2520a%2520crucial%2520role%2520for%250Athe%2520algorithm%2520to%2520converge.%2520Inspired%2520by%2520PDMM%252C%2520we%2520propose%2520AttentionX%2520to%250Aincorporate%2520the%2520consensus%2520discrepancy%2520in%2520the%2520output%2520update-expression%2520of%2520the%250Astandard%2520Attention.%2520The%2520consensus%2520discrepancy%2520in%2520AttentionX%2520refers%2520to%2520the%250Adifference%2520between%2520the%2520weighted%2520summation%2520of%2520%2524V%2524-representations%2520and%2520scaled%250A%2524V%2524-representions%2520themselves.%2520Experiments%2520on%2520ViT%2520and%2520nanoGPT%2520show%2520promising%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionX%3A%20Exploiting%20Consensus%20Discrepancy%20In%20Attention%20from%20A%0A%20%20Distributed%20Optimization%20Perspective&entry.906535625=Guoqiang%20Zhang%20and%20Richard%20Heusdens&entry.1292438233=%20%20In%20this%20paper%2C%20we%20extend%20the%20standard%20Attention%20in%20transformer%20by%20exploiting%0Athe%20consensus%20discrepancy%20from%20a%20distributed%20optimization%20perspective%2C%20referred%0Ato%20as%20AttentionX.%20It%20is%20noted%20that%20the%20primal-dual%20method%20of%20multipliers%20%28PDMM%29%0A%5Ccite%7BZhang16PDMM%7D%20is%20designed%20to%20iteratively%20solve%20a%20broad%20class%20of%0Adistributed%20optimization%20problems%20over%20a%20pear-to-pear%20%28P2P%29%20network%2C%20where%0Aneighbouring%20nodes%20gradually%20reach%20consensus%20as%20specified%20by%20predefined%20linear%0Aedge-constraints%20in%20the%20optimization%20process.%20In%20particular%2C%20at%20each%20iteration%0Aof%20PDMM%2C%20each%20node%20in%20a%20network%20first%20performs%20information-gathering%20from%0Aneighbours%20and%20then%20performs%20local%20information-fusion.%20From%20a%20high-level%20point%0Aof%20view%2C%20the%20%24KQ%24-softmax-based%20weighted%20summation%20of%20%24V%24-representations%20in%0AAttention%20corresponds%20information-gathering%20from%20neighbours%20while%20the%0Afeature-processing%20via%20the%20feed-forward%20network%20%28FFN%29%20in%20transformer%0Acorresponds%20to%20local%20information%20fusion.%20PDMM%20exploits%20the%20Lagrangian%0Amultipliers%20to%20capture%20the%20historical%20consensus%20discrepancy%20in%20the%20form%20of%0Aresidual%20errors%20of%20the%20linear%20edge-constraints%2C%20which%20plays%20a%20crucial%20role%20for%0Athe%20algorithm%20to%20converge.%20Inspired%20by%20PDMM%2C%20we%20propose%20AttentionX%20to%0Aincorporate%20the%20consensus%20discrepancy%20in%20the%20output%20update-expression%20of%20the%0Astandard%20Attention.%20The%20consensus%20discrepancy%20in%20AttentionX%20refers%20to%20the%0Adifference%20between%20the%20weighted%20summation%20of%20%24V%24-representations%20and%20scaled%0A%24V%24-representions%20themselves.%20Experiments%20on%20ViT%20and%20nanoGPT%20show%20promising%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04275v2&entry.124074799=Read"},
{"title": "Latent 3D Brain MRI Counterfactual", "author": "Wei Peng and Tian Xia and Fabio De Sousa Ribeiro and Tomas Bosschieter and Ehsan Adeli and Qingyu Zhao and Ben Glocker and Kilian M. Pohl", "abstract": "  The number of samples in structural brain MRI studies is often too small to\nproperly train deep learning models. Generative models show promise in\naddressing this issue by effectively learning the data distribution and\ngenerating high-fidelity MRI. However, they struggle to produce diverse,\nhigh-quality data outside the distribution defined by the training data. One\nway to address the issue is using causal models developed for 3D volume\ncounterfactuals. However, accurately modeling causality in high-dimensional\nspaces is a challenge so that these models generally generate 3D brain MRIS of\nlower quality. To address these challenges, we propose a two-stage method that\nconstructs a Structural Causal Model (SCM) within the latent space. In the\nfirst stage, we employ a VQ-VAE to learn a compact embedding of the MRI volume.\nSubsequently, we integrate our causal model into this latent space and execute\na three-step counterfactual procedure using a closed-form Generalized Linear\nModel (GLM). Our experiments conducted on real-world high-resolution MRI data\n(1mm) demonstrate that our method can generate high-quality 3D MRI\ncounterfactuals.\n", "link": "http://arxiv.org/abs/2409.05585v1", "date": "2024-09-09", "relevancy": 2.1097, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5318}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5318}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%203D%20Brain%20MRI%20Counterfactual&body=Title%3A%20Latent%203D%20Brain%20MRI%20Counterfactual%0AAuthor%3A%20Wei%20Peng%20and%20Tian%20Xia%20and%20Fabio%20De%20Sousa%20Ribeiro%20and%20Tomas%20Bosschieter%20and%20Ehsan%20Adeli%20and%20Qingyu%20Zhao%20and%20Ben%20Glocker%20and%20Kilian%20M.%20Pohl%0AAbstract%3A%20%20%20The%20number%20of%20samples%20in%20structural%20brain%20MRI%20studies%20is%20often%20too%20small%20to%0Aproperly%20train%20deep%20learning%20models.%20Generative%20models%20show%20promise%20in%0Aaddressing%20this%20issue%20by%20effectively%20learning%20the%20data%20distribution%20and%0Agenerating%20high-fidelity%20MRI.%20However%2C%20they%20struggle%20to%20produce%20diverse%2C%0Ahigh-quality%20data%20outside%20the%20distribution%20defined%20by%20the%20training%20data.%20One%0Away%20to%20address%20the%20issue%20is%20using%20causal%20models%20developed%20for%203D%20volume%0Acounterfactuals.%20However%2C%20accurately%20modeling%20causality%20in%20high-dimensional%0Aspaces%20is%20a%20challenge%20so%20that%20these%20models%20generally%20generate%203D%20brain%20MRIS%20of%0Alower%20quality.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20two-stage%20method%20that%0Aconstructs%20a%20Structural%20Causal%20Model%20%28SCM%29%20within%20the%20latent%20space.%20In%20the%0Afirst%20stage%2C%20we%20employ%20a%20VQ-VAE%20to%20learn%20a%20compact%20embedding%20of%20the%20MRI%20volume.%0ASubsequently%2C%20we%20integrate%20our%20causal%20model%20into%20this%20latent%20space%20and%20execute%0Aa%20three-step%20counterfactual%20procedure%20using%20a%20closed-form%20Generalized%20Linear%0AModel%20%28GLM%29.%20Our%20experiments%20conducted%20on%20real-world%20high-resolution%20MRI%20data%0A%281mm%29%20demonstrate%20that%20our%20method%20can%20generate%20high-quality%203D%20MRI%0Acounterfactuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%25203D%2520Brain%2520MRI%2520Counterfactual%26entry.906535625%3DWei%2520Peng%2520and%2520Tian%2520Xia%2520and%2520Fabio%2520De%2520Sousa%2520Ribeiro%2520and%2520Tomas%2520Bosschieter%2520and%2520Ehsan%2520Adeli%2520and%2520Qingyu%2520Zhao%2520and%2520Ben%2520Glocker%2520and%2520Kilian%2520M.%2520Pohl%26entry.1292438233%3D%2520%2520The%2520number%2520of%2520samples%2520in%2520structural%2520brain%2520MRI%2520studies%2520is%2520often%2520too%2520small%2520to%250Aproperly%2520train%2520deep%2520learning%2520models.%2520Generative%2520models%2520show%2520promise%2520in%250Aaddressing%2520this%2520issue%2520by%2520effectively%2520learning%2520the%2520data%2520distribution%2520and%250Agenerating%2520high-fidelity%2520MRI.%2520However%252C%2520they%2520struggle%2520to%2520produce%2520diverse%252C%250Ahigh-quality%2520data%2520outside%2520the%2520distribution%2520defined%2520by%2520the%2520training%2520data.%2520One%250Away%2520to%2520address%2520the%2520issue%2520is%2520using%2520causal%2520models%2520developed%2520for%25203D%2520volume%250Acounterfactuals.%2520However%252C%2520accurately%2520modeling%2520causality%2520in%2520high-dimensional%250Aspaces%2520is%2520a%2520challenge%2520so%2520that%2520these%2520models%2520generally%2520generate%25203D%2520brain%2520MRIS%2520of%250Alower%2520quality.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520two-stage%2520method%2520that%250Aconstructs%2520a%2520Structural%2520Causal%2520Model%2520%2528SCM%2529%2520within%2520the%2520latent%2520space.%2520In%2520the%250Afirst%2520stage%252C%2520we%2520employ%2520a%2520VQ-VAE%2520to%2520learn%2520a%2520compact%2520embedding%2520of%2520the%2520MRI%2520volume.%250ASubsequently%252C%2520we%2520integrate%2520our%2520causal%2520model%2520into%2520this%2520latent%2520space%2520and%2520execute%250Aa%2520three-step%2520counterfactual%2520procedure%2520using%2520a%2520closed-form%2520Generalized%2520Linear%250AModel%2520%2528GLM%2529.%2520Our%2520experiments%2520conducted%2520on%2520real-world%2520high-resolution%2520MRI%2520data%250A%25281mm%2529%2520demonstrate%2520that%2520our%2520method%2520can%2520generate%2520high-quality%25203D%2520MRI%250Acounterfactuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%203D%20Brain%20MRI%20Counterfactual&entry.906535625=Wei%20Peng%20and%20Tian%20Xia%20and%20Fabio%20De%20Sousa%20Ribeiro%20and%20Tomas%20Bosschieter%20and%20Ehsan%20Adeli%20and%20Qingyu%20Zhao%20and%20Ben%20Glocker%20and%20Kilian%20M.%20Pohl&entry.1292438233=%20%20The%20number%20of%20samples%20in%20structural%20brain%20MRI%20studies%20is%20often%20too%20small%20to%0Aproperly%20train%20deep%20learning%20models.%20Generative%20models%20show%20promise%20in%0Aaddressing%20this%20issue%20by%20effectively%20learning%20the%20data%20distribution%20and%0Agenerating%20high-fidelity%20MRI.%20However%2C%20they%20struggle%20to%20produce%20diverse%2C%0Ahigh-quality%20data%20outside%20the%20distribution%20defined%20by%20the%20training%20data.%20One%0Away%20to%20address%20the%20issue%20is%20using%20causal%20models%20developed%20for%203D%20volume%0Acounterfactuals.%20However%2C%20accurately%20modeling%20causality%20in%20high-dimensional%0Aspaces%20is%20a%20challenge%20so%20that%20these%20models%20generally%20generate%203D%20brain%20MRIS%20of%0Alower%20quality.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20two-stage%20method%20that%0Aconstructs%20a%20Structural%20Causal%20Model%20%28SCM%29%20within%20the%20latent%20space.%20In%20the%0Afirst%20stage%2C%20we%20employ%20a%20VQ-VAE%20to%20learn%20a%20compact%20embedding%20of%20the%20MRI%20volume.%0ASubsequently%2C%20we%20integrate%20our%20causal%20model%20into%20this%20latent%20space%20and%20execute%0Aa%20three-step%20counterfactual%20procedure%20using%20a%20closed-form%20Generalized%20Linear%0AModel%20%28GLM%29.%20Our%20experiments%20conducted%20on%20real-world%20high-resolution%20MRI%20data%0A%281mm%29%20demonstrate%20that%20our%20method%20can%20generate%20high-quality%203D%20MRI%0Acounterfactuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05585v1&entry.124074799=Read"},
{"title": "Cooperative Decision-Making for CAVs at Unsignalized Intersections: A\n  MARL Approach with Attention and Hierarchical Game Priors", "author": "Jiaqi Liu and Peng Hang and Xiaoxiang Na and Chao Huang and Jian Sun", "abstract": "  The development of autonomous vehicles has shown great potential to enhance\nthe efficiency and safety of transportation systems. However, the\ndecision-making issue in complex human-machine mixed traffic scenarios, such as\nunsignalized intersections, remains a challenge for autonomous vehicles. While\nreinforcement learning (RL) has been used to solve complex decision-making\nproblems, existing RL methods still have limitations in dealing with\ncooperative decision-making of multiple connected autonomous vehicles (CAVs),\nensuring safety during exploration, and simulating realistic human driver\nbehaviors. In this paper, a novel and efficient algorithm, Multi-Agent\nGame-prior Attention Deep Deterministic Policy Gradient (MA-GA-DDPG), is\nproposed to address these limitations. Our proposed algorithm formulates the\ndecision-making problem of CAVs at unsignalized intersections as a\ndecentralized multi-agent reinforcement learning problem and incorporates an\nattention mechanism to capture interaction dependencies between ego CAV and\nother agents. The attention weights between the ego vehicle and other agents\nare then used to screen interaction objects and obtain prior hierarchical game\nrelations, based on which a safety inspector module is designed to improve the\ntraffic safety. Furthermore, both simulation and hardware-in-the-loop\nexperiments were conducted, demonstrating that our method outperforms other\nbaseline approaches in terms of driving safety, efficiency, and comfort.\n", "link": "http://arxiv.org/abs/2409.05712v1", "date": "2024-09-09", "relevancy": 2.1073, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5532}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.525}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Decision-Making%20for%20CAVs%20at%20Unsignalized%20Intersections%3A%20A%0A%20%20MARL%20Approach%20with%20Attention%20and%20Hierarchical%20Game%20Priors&body=Title%3A%20Cooperative%20Decision-Making%20for%20CAVs%20at%20Unsignalized%20Intersections%3A%20A%0A%20%20MARL%20Approach%20with%20Attention%20and%20Hierarchical%20Game%20Priors%0AAuthor%3A%20Jiaqi%20Liu%20and%20Peng%20Hang%20and%20Xiaoxiang%20Na%20and%20Chao%20Huang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20The%20development%20of%20autonomous%20vehicles%20has%20shown%20great%20potential%20to%20enhance%0Athe%20efficiency%20and%20safety%20of%20transportation%20systems.%20However%2C%20the%0Adecision-making%20issue%20in%20complex%20human-machine%20mixed%20traffic%20scenarios%2C%20such%20as%0Aunsignalized%20intersections%2C%20remains%20a%20challenge%20for%20autonomous%20vehicles.%20While%0Areinforcement%20learning%20%28RL%29%20has%20been%20used%20to%20solve%20complex%20decision-making%0Aproblems%2C%20existing%20RL%20methods%20still%20have%20limitations%20in%20dealing%20with%0Acooperative%20decision-making%20of%20multiple%20connected%20autonomous%20vehicles%20%28CAVs%29%2C%0Aensuring%20safety%20during%20exploration%2C%20and%20simulating%20realistic%20human%20driver%0Abehaviors.%20In%20this%20paper%2C%20a%20novel%20and%20efficient%20algorithm%2C%20Multi-Agent%0AGame-prior%20Attention%20Deep%20Deterministic%20Policy%20Gradient%20%28MA-GA-DDPG%29%2C%20is%0Aproposed%20to%20address%20these%20limitations.%20Our%20proposed%20algorithm%20formulates%20the%0Adecision-making%20problem%20of%20CAVs%20at%20unsignalized%20intersections%20as%20a%0Adecentralized%20multi-agent%20reinforcement%20learning%20problem%20and%20incorporates%20an%0Aattention%20mechanism%20to%20capture%20interaction%20dependencies%20between%20ego%20CAV%20and%0Aother%20agents.%20The%20attention%20weights%20between%20the%20ego%20vehicle%20and%20other%20agents%0Aare%20then%20used%20to%20screen%20interaction%20objects%20and%20obtain%20prior%20hierarchical%20game%0Arelations%2C%20based%20on%20which%20a%20safety%20inspector%20module%20is%20designed%20to%20improve%20the%0Atraffic%20safety.%20Furthermore%2C%20both%20simulation%20and%20hardware-in-the-loop%0Aexperiments%20were%20conducted%2C%20demonstrating%20that%20our%20method%20outperforms%20other%0Abaseline%20approaches%20in%20terms%20of%20driving%20safety%2C%20efficiency%2C%20and%20comfort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Decision-Making%2520for%2520CAVs%2520at%2520Unsignalized%2520Intersections%253A%2520A%250A%2520%2520MARL%2520Approach%2520with%2520Attention%2520and%2520Hierarchical%2520Game%2520Priors%26entry.906535625%3DJiaqi%2520Liu%2520and%2520Peng%2520Hang%2520and%2520Xiaoxiang%2520Na%2520and%2520Chao%2520Huang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520autonomous%2520vehicles%2520has%2520shown%2520great%2520potential%2520to%2520enhance%250Athe%2520efficiency%2520and%2520safety%2520of%2520transportation%2520systems.%2520However%252C%2520the%250Adecision-making%2520issue%2520in%2520complex%2520human-machine%2520mixed%2520traffic%2520scenarios%252C%2520such%2520as%250Aunsignalized%2520intersections%252C%2520remains%2520a%2520challenge%2520for%2520autonomous%2520vehicles.%2520While%250Areinforcement%2520learning%2520%2528RL%2529%2520has%2520been%2520used%2520to%2520solve%2520complex%2520decision-making%250Aproblems%252C%2520existing%2520RL%2520methods%2520still%2520have%2520limitations%2520in%2520dealing%2520with%250Acooperative%2520decision-making%2520of%2520multiple%2520connected%2520autonomous%2520vehicles%2520%2528CAVs%2529%252C%250Aensuring%2520safety%2520during%2520exploration%252C%2520and%2520simulating%2520realistic%2520human%2520driver%250Abehaviors.%2520In%2520this%2520paper%252C%2520a%2520novel%2520and%2520efficient%2520algorithm%252C%2520Multi-Agent%250AGame-prior%2520Attention%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528MA-GA-DDPG%2529%252C%2520is%250Aproposed%2520to%2520address%2520these%2520limitations.%2520Our%2520proposed%2520algorithm%2520formulates%2520the%250Adecision-making%2520problem%2520of%2520CAVs%2520at%2520unsignalized%2520intersections%2520as%2520a%250Adecentralized%2520multi-agent%2520reinforcement%2520learning%2520problem%2520and%2520incorporates%2520an%250Aattention%2520mechanism%2520to%2520capture%2520interaction%2520dependencies%2520between%2520ego%2520CAV%2520and%250Aother%2520agents.%2520The%2520attention%2520weights%2520between%2520the%2520ego%2520vehicle%2520and%2520other%2520agents%250Aare%2520then%2520used%2520to%2520screen%2520interaction%2520objects%2520and%2520obtain%2520prior%2520hierarchical%2520game%250Arelations%252C%2520based%2520on%2520which%2520a%2520safety%2520inspector%2520module%2520is%2520designed%2520to%2520improve%2520the%250Atraffic%2520safety.%2520Furthermore%252C%2520both%2520simulation%2520and%2520hardware-in-the-loop%250Aexperiments%2520were%2520conducted%252C%2520demonstrating%2520that%2520our%2520method%2520outperforms%2520other%250Abaseline%2520approaches%2520in%2520terms%2520of%2520driving%2520safety%252C%2520efficiency%252C%2520and%2520comfort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Decision-Making%20for%20CAVs%20at%20Unsignalized%20Intersections%3A%20A%0A%20%20MARL%20Approach%20with%20Attention%20and%20Hierarchical%20Game%20Priors&entry.906535625=Jiaqi%20Liu%20and%20Peng%20Hang%20and%20Xiaoxiang%20Na%20and%20Chao%20Huang%20and%20Jian%20Sun&entry.1292438233=%20%20The%20development%20of%20autonomous%20vehicles%20has%20shown%20great%20potential%20to%20enhance%0Athe%20efficiency%20and%20safety%20of%20transportation%20systems.%20However%2C%20the%0Adecision-making%20issue%20in%20complex%20human-machine%20mixed%20traffic%20scenarios%2C%20such%20as%0Aunsignalized%20intersections%2C%20remains%20a%20challenge%20for%20autonomous%20vehicles.%20While%0Areinforcement%20learning%20%28RL%29%20has%20been%20used%20to%20solve%20complex%20decision-making%0Aproblems%2C%20existing%20RL%20methods%20still%20have%20limitations%20in%20dealing%20with%0Acooperative%20decision-making%20of%20multiple%20connected%20autonomous%20vehicles%20%28CAVs%29%2C%0Aensuring%20safety%20during%20exploration%2C%20and%20simulating%20realistic%20human%20driver%0Abehaviors.%20In%20this%20paper%2C%20a%20novel%20and%20efficient%20algorithm%2C%20Multi-Agent%0AGame-prior%20Attention%20Deep%20Deterministic%20Policy%20Gradient%20%28MA-GA-DDPG%29%2C%20is%0Aproposed%20to%20address%20these%20limitations.%20Our%20proposed%20algorithm%20formulates%20the%0Adecision-making%20problem%20of%20CAVs%20at%20unsignalized%20intersections%20as%20a%0Adecentralized%20multi-agent%20reinforcement%20learning%20problem%20and%20incorporates%20an%0Aattention%20mechanism%20to%20capture%20interaction%20dependencies%20between%20ego%20CAV%20and%0Aother%20agents.%20The%20attention%20weights%20between%20the%20ego%20vehicle%20and%20other%20agents%0Aare%20then%20used%20to%20screen%20interaction%20objects%20and%20obtain%20prior%20hierarchical%20game%0Arelations%2C%20based%20on%20which%20a%20safety%20inspector%20module%20is%20designed%20to%20improve%20the%0Atraffic%20safety.%20Furthermore%2C%20both%20simulation%20and%20hardware-in-the-loop%0Aexperiments%20were%20conducted%2C%20demonstrating%20that%20our%20method%20outperforms%20other%0Abaseline%20approaches%20in%20terms%20of%20driving%20safety%2C%20efficiency%2C%20and%20comfort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05712v1&entry.124074799=Read"},
{"title": "A general reduced-order neural operator for spatio-temporal predictive\n  learning on complex spatial domains", "author": "Qinglu Meng and Yingguang Li and Zhiliang Deng and Xu Liu and Gengxiang Chen and Qiutong Wu and Changqing Liu and Xiaozhong Hao", "abstract": "  Predictive learning for spatio-temporal processes (PL-STP) on complex spatial\ndomains plays a critical role in various scientific and engineering fields,\nwith its essence being the construction of operators between\ninfinite-dimensional function spaces. This paper focuses on the unequal-domain\nmappings in PL-STP and categorising them into increase-domain and\ndecrease-domain mapping. Recent advances in deep learning have revealed the\ngreat potential of neural operators (NOs) to learn operators directly from\nobservational data. However, existing NOs require input space and output space\nto be the same domain, which pose challenges in ensuring predictive accuracy\nand stability for unequal-domain mappings. To this end, this study presents a\ngeneral reduced-order neural operator named Reduced-Order Neural Operator on\nRiemannian Manifolds (RO-NORM), which consists of two parts: the unequal-domain\nencoder/decoder and the same-domain approximator. Motivated by the variable\nseparation in classical modal decomposition, the unequal-domain encoder/decoder\nuses the pre-computed bases to reformulate the spatio-temporal function as a\nsum of products between spatial (or temporal) bases and corresponding\ntemporally (or spatially) distributed weight functions, thus the original\nunequal-domain mapping can be converted into a same-domain mapping.\nConsequently, the same-domain approximator NORM is applied to model the\ntransformed mapping. The performance of our proposed method has been evaluated\non six benchmark cases, including parametric PDEs, engineering and biomedical\napplications, and compared with four baseline algorithms: DeepONet,\nPOD-DeepONet, PCA-Net, and vanilla NORM. The experimental results demonstrate\nthe superiority of RO-NORM in prediction accuracy and training efficiency for\nPL-STP.\n", "link": "http://arxiv.org/abs/2409.05508v1", "date": "2024-09-09", "relevancy": 2.1015, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5304}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20general%20reduced-order%20neural%20operator%20for%20spatio-temporal%20predictive%0A%20%20learning%20on%20complex%20spatial%20domains&body=Title%3A%20A%20general%20reduced-order%20neural%20operator%20for%20spatio-temporal%20predictive%0A%20%20learning%20on%20complex%20spatial%20domains%0AAuthor%3A%20Qinglu%20Meng%20and%20Yingguang%20Li%20and%20Zhiliang%20Deng%20and%20Xu%20Liu%20and%20Gengxiang%20Chen%20and%20Qiutong%20Wu%20and%20Changqing%20Liu%20and%20Xiaozhong%20Hao%0AAbstract%3A%20%20%20Predictive%20learning%20for%20spatio-temporal%20processes%20%28PL-STP%29%20on%20complex%20spatial%0Adomains%20plays%20a%20critical%20role%20in%20various%20scientific%20and%20engineering%20fields%2C%0Awith%20its%20essence%20being%20the%20construction%20of%20operators%20between%0Ainfinite-dimensional%20function%20spaces.%20This%20paper%20focuses%20on%20the%20unequal-domain%0Amappings%20in%20PL-STP%20and%20categorising%20them%20into%20increase-domain%20and%0Adecrease-domain%20mapping.%20Recent%20advances%20in%20deep%20learning%20have%20revealed%20the%0Agreat%20potential%20of%20neural%20operators%20%28NOs%29%20to%20learn%20operators%20directly%20from%0Aobservational%20data.%20However%2C%20existing%20NOs%20require%20input%20space%20and%20output%20space%0Ato%20be%20the%20same%20domain%2C%20which%20pose%20challenges%20in%20ensuring%20predictive%20accuracy%0Aand%20stability%20for%20unequal-domain%20mappings.%20To%20this%20end%2C%20this%20study%20presents%20a%0Ageneral%20reduced-order%20neural%20operator%20named%20Reduced-Order%20Neural%20Operator%20on%0ARiemannian%20Manifolds%20%28RO-NORM%29%2C%20which%20consists%20of%20two%20parts%3A%20the%20unequal-domain%0Aencoder/decoder%20and%20the%20same-domain%20approximator.%20Motivated%20by%20the%20variable%0Aseparation%20in%20classical%20modal%20decomposition%2C%20the%20unequal-domain%20encoder/decoder%0Auses%20the%20pre-computed%20bases%20to%20reformulate%20the%20spatio-temporal%20function%20as%20a%0Asum%20of%20products%20between%20spatial%20%28or%20temporal%29%20bases%20and%20corresponding%0Atemporally%20%28or%20spatially%29%20distributed%20weight%20functions%2C%20thus%20the%20original%0Aunequal-domain%20mapping%20can%20be%20converted%20into%20a%20same-domain%20mapping.%0AConsequently%2C%20the%20same-domain%20approximator%20NORM%20is%20applied%20to%20model%20the%0Atransformed%20mapping.%20The%20performance%20of%20our%20proposed%20method%20has%20been%20evaluated%0Aon%20six%20benchmark%20cases%2C%20including%20parametric%20PDEs%2C%20engineering%20and%20biomedical%0Aapplications%2C%20and%20compared%20with%20four%20baseline%20algorithms%3A%20DeepONet%2C%0APOD-DeepONet%2C%20PCA-Net%2C%20and%20vanilla%20NORM.%20The%20experimental%20results%20demonstrate%0Athe%20superiority%20of%20RO-NORM%20in%20prediction%20accuracy%20and%20training%20efficiency%20for%0APL-STP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520general%2520reduced-order%2520neural%2520operator%2520for%2520spatio-temporal%2520predictive%250A%2520%2520learning%2520on%2520complex%2520spatial%2520domains%26entry.906535625%3DQinglu%2520Meng%2520and%2520Yingguang%2520Li%2520and%2520Zhiliang%2520Deng%2520and%2520Xu%2520Liu%2520and%2520Gengxiang%2520Chen%2520and%2520Qiutong%2520Wu%2520and%2520Changqing%2520Liu%2520and%2520Xiaozhong%2520Hao%26entry.1292438233%3D%2520%2520Predictive%2520learning%2520for%2520spatio-temporal%2520processes%2520%2528PL-STP%2529%2520on%2520complex%2520spatial%250Adomains%2520plays%2520a%2520critical%2520role%2520in%2520various%2520scientific%2520and%2520engineering%2520fields%252C%250Awith%2520its%2520essence%2520being%2520the%2520construction%2520of%2520operators%2520between%250Ainfinite-dimensional%2520function%2520spaces.%2520This%2520paper%2520focuses%2520on%2520the%2520unequal-domain%250Amappings%2520in%2520PL-STP%2520and%2520categorising%2520them%2520into%2520increase-domain%2520and%250Adecrease-domain%2520mapping.%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520revealed%2520the%250Agreat%2520potential%2520of%2520neural%2520operators%2520%2528NOs%2529%2520to%2520learn%2520operators%2520directly%2520from%250Aobservational%2520data.%2520However%252C%2520existing%2520NOs%2520require%2520input%2520space%2520and%2520output%2520space%250Ato%2520be%2520the%2520same%2520domain%252C%2520which%2520pose%2520challenges%2520in%2520ensuring%2520predictive%2520accuracy%250Aand%2520stability%2520for%2520unequal-domain%2520mappings.%2520To%2520this%2520end%252C%2520this%2520study%2520presents%2520a%250Ageneral%2520reduced-order%2520neural%2520operator%2520named%2520Reduced-Order%2520Neural%2520Operator%2520on%250ARiemannian%2520Manifolds%2520%2528RO-NORM%2529%252C%2520which%2520consists%2520of%2520two%2520parts%253A%2520the%2520unequal-domain%250Aencoder/decoder%2520and%2520the%2520same-domain%2520approximator.%2520Motivated%2520by%2520the%2520variable%250Aseparation%2520in%2520classical%2520modal%2520decomposition%252C%2520the%2520unequal-domain%2520encoder/decoder%250Auses%2520the%2520pre-computed%2520bases%2520to%2520reformulate%2520the%2520spatio-temporal%2520function%2520as%2520a%250Asum%2520of%2520products%2520between%2520spatial%2520%2528or%2520temporal%2529%2520bases%2520and%2520corresponding%250Atemporally%2520%2528or%2520spatially%2529%2520distributed%2520weight%2520functions%252C%2520thus%2520the%2520original%250Aunequal-domain%2520mapping%2520can%2520be%2520converted%2520into%2520a%2520same-domain%2520mapping.%250AConsequently%252C%2520the%2520same-domain%2520approximator%2520NORM%2520is%2520applied%2520to%2520model%2520the%250Atransformed%2520mapping.%2520The%2520performance%2520of%2520our%2520proposed%2520method%2520has%2520been%2520evaluated%250Aon%2520six%2520benchmark%2520cases%252C%2520including%2520parametric%2520PDEs%252C%2520engineering%2520and%2520biomedical%250Aapplications%252C%2520and%2520compared%2520with%2520four%2520baseline%2520algorithms%253A%2520DeepONet%252C%250APOD-DeepONet%252C%2520PCA-Net%252C%2520and%2520vanilla%2520NORM.%2520The%2520experimental%2520results%2520demonstrate%250Athe%2520superiority%2520of%2520RO-NORM%2520in%2520prediction%2520accuracy%2520and%2520training%2520efficiency%2520for%250APL-STP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20general%20reduced-order%20neural%20operator%20for%20spatio-temporal%20predictive%0A%20%20learning%20on%20complex%20spatial%20domains&entry.906535625=Qinglu%20Meng%20and%20Yingguang%20Li%20and%20Zhiliang%20Deng%20and%20Xu%20Liu%20and%20Gengxiang%20Chen%20and%20Qiutong%20Wu%20and%20Changqing%20Liu%20and%20Xiaozhong%20Hao&entry.1292438233=%20%20Predictive%20learning%20for%20spatio-temporal%20processes%20%28PL-STP%29%20on%20complex%20spatial%0Adomains%20plays%20a%20critical%20role%20in%20various%20scientific%20and%20engineering%20fields%2C%0Awith%20its%20essence%20being%20the%20construction%20of%20operators%20between%0Ainfinite-dimensional%20function%20spaces.%20This%20paper%20focuses%20on%20the%20unequal-domain%0Amappings%20in%20PL-STP%20and%20categorising%20them%20into%20increase-domain%20and%0Adecrease-domain%20mapping.%20Recent%20advances%20in%20deep%20learning%20have%20revealed%20the%0Agreat%20potential%20of%20neural%20operators%20%28NOs%29%20to%20learn%20operators%20directly%20from%0Aobservational%20data.%20However%2C%20existing%20NOs%20require%20input%20space%20and%20output%20space%0Ato%20be%20the%20same%20domain%2C%20which%20pose%20challenges%20in%20ensuring%20predictive%20accuracy%0Aand%20stability%20for%20unequal-domain%20mappings.%20To%20this%20end%2C%20this%20study%20presents%20a%0Ageneral%20reduced-order%20neural%20operator%20named%20Reduced-Order%20Neural%20Operator%20on%0ARiemannian%20Manifolds%20%28RO-NORM%29%2C%20which%20consists%20of%20two%20parts%3A%20the%20unequal-domain%0Aencoder/decoder%20and%20the%20same-domain%20approximator.%20Motivated%20by%20the%20variable%0Aseparation%20in%20classical%20modal%20decomposition%2C%20the%20unequal-domain%20encoder/decoder%0Auses%20the%20pre-computed%20bases%20to%20reformulate%20the%20spatio-temporal%20function%20as%20a%0Asum%20of%20products%20between%20spatial%20%28or%20temporal%29%20bases%20and%20corresponding%0Atemporally%20%28or%20spatially%29%20distributed%20weight%20functions%2C%20thus%20the%20original%0Aunequal-domain%20mapping%20can%20be%20converted%20into%20a%20same-domain%20mapping.%0AConsequently%2C%20the%20same-domain%20approximator%20NORM%20is%20applied%20to%20model%20the%0Atransformed%20mapping.%20The%20performance%20of%20our%20proposed%20method%20has%20been%20evaluated%0Aon%20six%20benchmark%20cases%2C%20including%20parametric%20PDEs%2C%20engineering%20and%20biomedical%0Aapplications%2C%20and%20compared%20with%20four%20baseline%20algorithms%3A%20DeepONet%2C%0APOD-DeepONet%2C%20PCA-Net%2C%20and%20vanilla%20NORM.%20The%20experimental%20results%20demonstrate%0Athe%20superiority%20of%20RO-NORM%20in%20prediction%20accuracy%20and%20training%20efficiency%20for%0APL-STP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05508v1&entry.124074799=Read"},
{"title": "A Taxonomy of Miscompressions: Preparing Image Forensics for Neural\n  Compression", "author": "Nora Hofer and Rainer B\u00f6hme", "abstract": "  Neural compression has the potential to revolutionize lossy image\ncompression. Based on generative models, recent schemes achieve unprecedented\ncompression rates at high perceptual quality but compromise semantic fidelity.\nDetails of decompressed images may appear optically flawless but semantically\ndifferent from the originals, making compression errors difficult or impossible\nto detect. We explore the problem space and propose a provisional taxonomy of\nmiscompressions. It defines three types of 'what happens' and has a binary\n'high impact' flag indicating miscompressions that alter symbols. We discuss\nhow the taxonomy can facilitate risk communication and research into\nmitigations.\n", "link": "http://arxiv.org/abs/2409.05490v1", "date": "2024-09-09", "relevancy": 2.0911, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5321}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5213}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Taxonomy%20of%20Miscompressions%3A%20Preparing%20Image%20Forensics%20for%20Neural%0A%20%20Compression&body=Title%3A%20A%20Taxonomy%20of%20Miscompressions%3A%20Preparing%20Image%20Forensics%20for%20Neural%0A%20%20Compression%0AAuthor%3A%20Nora%20Hofer%20and%20Rainer%20B%C3%B6hme%0AAbstract%3A%20%20%20Neural%20compression%20has%20the%20potential%20to%20revolutionize%20lossy%20image%0Acompression.%20Based%20on%20generative%20models%2C%20recent%20schemes%20achieve%20unprecedented%0Acompression%20rates%20at%20high%20perceptual%20quality%20but%20compromise%20semantic%20fidelity.%0ADetails%20of%20decompressed%20images%20may%20appear%20optically%20flawless%20but%20semantically%0Adifferent%20from%20the%20originals%2C%20making%20compression%20errors%20difficult%20or%20impossible%0Ato%20detect.%20We%20explore%20the%20problem%20space%20and%20propose%20a%20provisional%20taxonomy%20of%0Amiscompressions.%20It%20defines%20three%20types%20of%20%27what%20happens%27%20and%20has%20a%20binary%0A%27high%20impact%27%20flag%20indicating%20miscompressions%20that%20alter%20symbols.%20We%20discuss%0Ahow%20the%20taxonomy%20can%20facilitate%20risk%20communication%20and%20research%20into%0Amitigations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Taxonomy%2520of%2520Miscompressions%253A%2520Preparing%2520Image%2520Forensics%2520for%2520Neural%250A%2520%2520Compression%26entry.906535625%3DNora%2520Hofer%2520and%2520Rainer%2520B%25C3%25B6hme%26entry.1292438233%3D%2520%2520Neural%2520compression%2520has%2520the%2520potential%2520to%2520revolutionize%2520lossy%2520image%250Acompression.%2520Based%2520on%2520generative%2520models%252C%2520recent%2520schemes%2520achieve%2520unprecedented%250Acompression%2520rates%2520at%2520high%2520perceptual%2520quality%2520but%2520compromise%2520semantic%2520fidelity.%250ADetails%2520of%2520decompressed%2520images%2520may%2520appear%2520optically%2520flawless%2520but%2520semantically%250Adifferent%2520from%2520the%2520originals%252C%2520making%2520compression%2520errors%2520difficult%2520or%2520impossible%250Ato%2520detect.%2520We%2520explore%2520the%2520problem%2520space%2520and%2520propose%2520a%2520provisional%2520taxonomy%2520of%250Amiscompressions.%2520It%2520defines%2520three%2520types%2520of%2520%2527what%2520happens%2527%2520and%2520has%2520a%2520binary%250A%2527high%2520impact%2527%2520flag%2520indicating%2520miscompressions%2520that%2520alter%2520symbols.%2520We%2520discuss%250Ahow%2520the%2520taxonomy%2520can%2520facilitate%2520risk%2520communication%2520and%2520research%2520into%250Amitigations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Taxonomy%20of%20Miscompressions%3A%20Preparing%20Image%20Forensics%20for%20Neural%0A%20%20Compression&entry.906535625=Nora%20Hofer%20and%20Rainer%20B%C3%B6hme&entry.1292438233=%20%20Neural%20compression%20has%20the%20potential%20to%20revolutionize%20lossy%20image%0Acompression.%20Based%20on%20generative%20models%2C%20recent%20schemes%20achieve%20unprecedented%0Acompression%20rates%20at%20high%20perceptual%20quality%20but%20compromise%20semantic%20fidelity.%0ADetails%20of%20decompressed%20images%20may%20appear%20optically%20flawless%20but%20semantically%0Adifferent%20from%20the%20originals%2C%20making%20compression%20errors%20difficult%20or%20impossible%0Ato%20detect.%20We%20explore%20the%20problem%20space%20and%20propose%20a%20provisional%20taxonomy%20of%0Amiscompressions.%20It%20defines%20three%20types%20of%20%27what%20happens%27%20and%20has%20a%20binary%0A%27high%20impact%27%20flag%20indicating%20miscompressions%20that%20alter%20symbols.%20We%20discuss%0Ahow%20the%20taxonomy%20can%20facilitate%20risk%20communication%20and%20research%20into%0Amitigations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05490v1&entry.124074799=Read"},
{"title": "Retrofitting Temporal Graph Neural Networks with Transformer", "author": "Qiang Huang and Xiao Yan and Xin Wang and Susie Xi Rao and Zhichao Han and Fangcheng Fu and Wentao Zhang and Jiawei Jiang", "abstract": "  Temporal graph neural networks (TGNNs) outperform regular GNNs by\nincorporating time information into graph-based operations. However, TGNNs\nadopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored\ntraining frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN,\nwhich uses Transformer decoder as the backbone model for TGNN to enjoy\nTransformer's codebase for efficient training. In particular, Transformer\nachieves tremendous success for language modeling, and thus the community\ndeveloped high-performance kernels (e.g., flash-attention and memory-efficient\nattention) and efficient distributed training schemes (e.g., PyTorch FSDP,\nDeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling,\ni.e., the message aggregation operation between chronologically occurring nodes\nand their temporal neighbors in TGNNs can be structured as sequence modeling.\nBeside this similarity, we also incorporate a series of algorithm designs\nincluding suffix infilling, temporal graph attention with self-loop, and causal\nmasking self-attention to make TF-TGN work. During training, existing systems\nare slow in transforming the graph topology and conducting graph sampling. As\nsuch, we propose methods to parallelize the CSR format conversion and graph\nsampling. We also adapt Transformer codebase to train TF-TGN efficiently with\nmultiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art\nTGNN training frameworks. The results show that TF-TGN can accelerate training\nby over 2.20 while providing comparable or even superior accuracy to existing\nSOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.\n", "link": "http://arxiv.org/abs/2409.05477v1", "date": "2024-09-09", "relevancy": 2.0663, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5117}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrofitting%20Temporal%20Graph%20Neural%20Networks%20with%20Transformer&body=Title%3A%20Retrofitting%20Temporal%20Graph%20Neural%20Networks%20with%20Transformer%0AAuthor%3A%20Qiang%20Huang%20and%20Xiao%20Yan%20and%20Xin%20Wang%20and%20Susie%20Xi%20Rao%20and%20Zhichao%20Han%20and%20Fangcheng%20Fu%20and%20Wentao%20Zhang%20and%20Jiawei%20Jiang%0AAbstract%3A%20%20%20Temporal%20graph%20neural%20networks%20%28TGNNs%29%20outperform%20regular%20GNNs%20by%0Aincorporating%20time%20information%20into%20graph-based%20operations.%20However%2C%20TGNNs%0Aadopt%20specialized%20models%20%28e.g.%2C%20TGN%2C%20TGAT%2C%20and%20APAN%20%29%20and%20require%20tailored%0Atraining%20frameworks%20%28e.g.%2C%20TGL%20and%20ETC%29.%20In%20this%20paper%2C%20we%20propose%20TF-TGN%2C%0Awhich%20uses%20Transformer%20decoder%20as%20the%20backbone%20model%20for%20TGNN%20to%20enjoy%0ATransformer%27s%20codebase%20for%20efficient%20training.%20In%20particular%2C%20Transformer%0Aachieves%20tremendous%20success%20for%20language%20modeling%2C%20and%20thus%20the%20community%0Adeveloped%20high-performance%20kernels%20%28e.g.%2C%20flash-attention%20and%20memory-efficient%0Aattention%29%20and%20efficient%20distributed%20training%20schemes%20%28e.g.%2C%20PyTorch%20FSDP%2C%0ADeepSpeed%2C%20and%20Megatron-LM%29.%20We%20observe%20that%20TGNN%20resembles%20language%20modeling%2C%0Ai.e.%2C%20the%20message%20aggregation%20operation%20between%20chronologically%20occurring%20nodes%0Aand%20their%20temporal%20neighbors%20in%20TGNNs%20can%20be%20structured%20as%20sequence%20modeling.%0ABeside%20this%20similarity%2C%20we%20also%20incorporate%20a%20series%20of%20algorithm%20designs%0Aincluding%20suffix%20infilling%2C%20temporal%20graph%20attention%20with%20self-loop%2C%20and%20causal%0Amasking%20self-attention%20to%20make%20TF-TGN%20work.%20During%20training%2C%20existing%20systems%0Aare%20slow%20in%20transforming%20the%20graph%20topology%20and%20conducting%20graph%20sampling.%20As%0Asuch%2C%20we%20propose%20methods%20to%20parallelize%20the%20CSR%20format%20conversion%20and%20graph%0Asampling.%20We%20also%20adapt%20Transformer%20codebase%20to%20train%20TF-TGN%20efficiently%20with%0Amultiple%20GPUs.%20We%20experiment%20with%209%20graphs%20and%20compare%20with%202%20state-of-the-art%0ATGNN%20training%20frameworks.%20The%20results%20show%20that%20TF-TGN%20can%20accelerate%20training%0Aby%20over%202.20%20while%20providing%20comparable%20or%20even%20superior%20accuracy%20to%20existing%0ASOTA%20TGNNs.%20TF-TGN%20is%20available%20at%20https%3A//github.com/qianghuangwhu/TF-TGN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrofitting%2520Temporal%2520Graph%2520Neural%2520Networks%2520with%2520Transformer%26entry.906535625%3DQiang%2520Huang%2520and%2520Xiao%2520Yan%2520and%2520Xin%2520Wang%2520and%2520Susie%2520Xi%2520Rao%2520and%2520Zhichao%2520Han%2520and%2520Fangcheng%2520Fu%2520and%2520Wentao%2520Zhang%2520and%2520Jiawei%2520Jiang%26entry.1292438233%3D%2520%2520Temporal%2520graph%2520neural%2520networks%2520%2528TGNNs%2529%2520outperform%2520regular%2520GNNs%2520by%250Aincorporating%2520time%2520information%2520into%2520graph-based%2520operations.%2520However%252C%2520TGNNs%250Aadopt%2520specialized%2520models%2520%2528e.g.%252C%2520TGN%252C%2520TGAT%252C%2520and%2520APAN%2520%2529%2520and%2520require%2520tailored%250Atraining%2520frameworks%2520%2528e.g.%252C%2520TGL%2520and%2520ETC%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TF-TGN%252C%250Awhich%2520uses%2520Transformer%2520decoder%2520as%2520the%2520backbone%2520model%2520for%2520TGNN%2520to%2520enjoy%250ATransformer%2527s%2520codebase%2520for%2520efficient%2520training.%2520In%2520particular%252C%2520Transformer%250Aachieves%2520tremendous%2520success%2520for%2520language%2520modeling%252C%2520and%2520thus%2520the%2520community%250Adeveloped%2520high-performance%2520kernels%2520%2528e.g.%252C%2520flash-attention%2520and%2520memory-efficient%250Aattention%2529%2520and%2520efficient%2520distributed%2520training%2520schemes%2520%2528e.g.%252C%2520PyTorch%2520FSDP%252C%250ADeepSpeed%252C%2520and%2520Megatron-LM%2529.%2520We%2520observe%2520that%2520TGNN%2520resembles%2520language%2520modeling%252C%250Ai.e.%252C%2520the%2520message%2520aggregation%2520operation%2520between%2520chronologically%2520occurring%2520nodes%250Aand%2520their%2520temporal%2520neighbors%2520in%2520TGNNs%2520can%2520be%2520structured%2520as%2520sequence%2520modeling.%250ABeside%2520this%2520similarity%252C%2520we%2520also%2520incorporate%2520a%2520series%2520of%2520algorithm%2520designs%250Aincluding%2520suffix%2520infilling%252C%2520temporal%2520graph%2520attention%2520with%2520self-loop%252C%2520and%2520causal%250Amasking%2520self-attention%2520to%2520make%2520TF-TGN%2520work.%2520During%2520training%252C%2520existing%2520systems%250Aare%2520slow%2520in%2520transforming%2520the%2520graph%2520topology%2520and%2520conducting%2520graph%2520sampling.%2520As%250Asuch%252C%2520we%2520propose%2520methods%2520to%2520parallelize%2520the%2520CSR%2520format%2520conversion%2520and%2520graph%250Asampling.%2520We%2520also%2520adapt%2520Transformer%2520codebase%2520to%2520train%2520TF-TGN%2520efficiently%2520with%250Amultiple%2520GPUs.%2520We%2520experiment%2520with%25209%2520graphs%2520and%2520compare%2520with%25202%2520state-of-the-art%250ATGNN%2520training%2520frameworks.%2520The%2520results%2520show%2520that%2520TF-TGN%2520can%2520accelerate%2520training%250Aby%2520over%25202.20%2520while%2520providing%2520comparable%2520or%2520even%2520superior%2520accuracy%2520to%2520existing%250ASOTA%2520TGNNs.%2520TF-TGN%2520is%2520available%2520at%2520https%253A//github.com/qianghuangwhu/TF-TGN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrofitting%20Temporal%20Graph%20Neural%20Networks%20with%20Transformer&entry.906535625=Qiang%20Huang%20and%20Xiao%20Yan%20and%20Xin%20Wang%20and%20Susie%20Xi%20Rao%20and%20Zhichao%20Han%20and%20Fangcheng%20Fu%20and%20Wentao%20Zhang%20and%20Jiawei%20Jiang&entry.1292438233=%20%20Temporal%20graph%20neural%20networks%20%28TGNNs%29%20outperform%20regular%20GNNs%20by%0Aincorporating%20time%20information%20into%20graph-based%20operations.%20However%2C%20TGNNs%0Aadopt%20specialized%20models%20%28e.g.%2C%20TGN%2C%20TGAT%2C%20and%20APAN%20%29%20and%20require%20tailored%0Atraining%20frameworks%20%28e.g.%2C%20TGL%20and%20ETC%29.%20In%20this%20paper%2C%20we%20propose%20TF-TGN%2C%0Awhich%20uses%20Transformer%20decoder%20as%20the%20backbone%20model%20for%20TGNN%20to%20enjoy%0ATransformer%27s%20codebase%20for%20efficient%20training.%20In%20particular%2C%20Transformer%0Aachieves%20tremendous%20success%20for%20language%20modeling%2C%20and%20thus%20the%20community%0Adeveloped%20high-performance%20kernels%20%28e.g.%2C%20flash-attention%20and%20memory-efficient%0Aattention%29%20and%20efficient%20distributed%20training%20schemes%20%28e.g.%2C%20PyTorch%20FSDP%2C%0ADeepSpeed%2C%20and%20Megatron-LM%29.%20We%20observe%20that%20TGNN%20resembles%20language%20modeling%2C%0Ai.e.%2C%20the%20message%20aggregation%20operation%20between%20chronologically%20occurring%20nodes%0Aand%20their%20temporal%20neighbors%20in%20TGNNs%20can%20be%20structured%20as%20sequence%20modeling.%0ABeside%20this%20similarity%2C%20we%20also%20incorporate%20a%20series%20of%20algorithm%20designs%0Aincluding%20suffix%20infilling%2C%20temporal%20graph%20attention%20with%20self-loop%2C%20and%20causal%0Amasking%20self-attention%20to%20make%20TF-TGN%20work.%20During%20training%2C%20existing%20systems%0Aare%20slow%20in%20transforming%20the%20graph%20topology%20and%20conducting%20graph%20sampling.%20As%0Asuch%2C%20we%20propose%20methods%20to%20parallelize%20the%20CSR%20format%20conversion%20and%20graph%0Asampling.%20We%20also%20adapt%20Transformer%20codebase%20to%20train%20TF-TGN%20efficiently%20with%0Amultiple%20GPUs.%20We%20experiment%20with%209%20graphs%20and%20compare%20with%202%20state-of-the-art%0ATGNN%20training%20frameworks.%20The%20results%20show%20that%20TF-TGN%20can%20accelerate%20training%0Aby%20over%202.20%20while%20providing%20comparable%20or%20even%20superior%20accuracy%20to%20existing%0ASOTA%20TGNNs.%20TF-TGN%20is%20available%20at%20https%3A//github.com/qianghuangwhu/TF-TGN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05477v1&entry.124074799=Read"},
{"title": "Online Residual Learning from Offline Experts for Pedestrian Tracking", "author": "Anastasios Vlachos and Anastasios Tsiamis and Aren Karapetyan and Efe C. Balta and John Lygeros", "abstract": "  In this paper, we consider the problem of predicting unknown targets from\ndata. We propose Online Residual Learning (ORL), a method that combines online\nadaptation with offline-trained predictions. At a lower level, we employ\nmultiple offline predictions generated before or at the beginning of the\nprediction horizon. We augment every offline prediction by learning their\nrespective residual error concerning the true target state online, using the\nrecursive least squares algorithm. At a higher level, we treat the augmented\nlower-level predictors as experts, adopting the Prediction with Expert Advice\nframework. We utilize an adaptive softmax weighting scheme to form an aggregate\nprediction and provide guarantees for ORL in terms of regret. We employ ORL to\nboost performance in the setting of online pedestrian trajectory prediction.\nBased on data from the Stanford Drone Dataset, we show that ORL can demonstrate\nbest-of-both-worlds performance.\n", "link": "http://arxiv.org/abs/2409.04069v2", "date": "2024-09-09", "relevancy": 2.0585, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5292}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Residual%20Learning%20from%20Offline%20Experts%20for%20Pedestrian%20Tracking&body=Title%3A%20Online%20Residual%20Learning%20from%20Offline%20Experts%20for%20Pedestrian%20Tracking%0AAuthor%3A%20Anastasios%20Vlachos%20and%20Anastasios%20Tsiamis%20and%20Aren%20Karapetyan%20and%20Efe%20C.%20Balta%20and%20John%20Lygeros%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20predicting%20unknown%20targets%20from%0Adata.%20We%20propose%20Online%20Residual%20Learning%20%28ORL%29%2C%20a%20method%20that%20combines%20online%0Aadaptation%20with%20offline-trained%20predictions.%20At%20a%20lower%20level%2C%20we%20employ%0Amultiple%20offline%20predictions%20generated%20before%20or%20at%20the%20beginning%20of%20the%0Aprediction%20horizon.%20We%20augment%20every%20offline%20prediction%20by%20learning%20their%0Arespective%20residual%20error%20concerning%20the%20true%20target%20state%20online%2C%20using%20the%0Arecursive%20least%20squares%20algorithm.%20At%20a%20higher%20level%2C%20we%20treat%20the%20augmented%0Alower-level%20predictors%20as%20experts%2C%20adopting%20the%20Prediction%20with%20Expert%20Advice%0Aframework.%20We%20utilize%20an%20adaptive%20softmax%20weighting%20scheme%20to%20form%20an%20aggregate%0Aprediction%20and%20provide%20guarantees%20for%20ORL%20in%20terms%20of%20regret.%20We%20employ%20ORL%20to%0Aboost%20performance%20in%20the%20setting%20of%20online%20pedestrian%20trajectory%20prediction.%0ABased%20on%20data%20from%20the%20Stanford%20Drone%20Dataset%2C%20we%20show%20that%20ORL%20can%20demonstrate%0Abest-of-both-worlds%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Residual%2520Learning%2520from%2520Offline%2520Experts%2520for%2520Pedestrian%2520Tracking%26entry.906535625%3DAnastasios%2520Vlachos%2520and%2520Anastasios%2520Tsiamis%2520and%2520Aren%2520Karapetyan%2520and%2520Efe%2520C.%2520Balta%2520and%2520John%2520Lygeros%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520problem%2520of%2520predicting%2520unknown%2520targets%2520from%250Adata.%2520We%2520propose%2520Online%2520Residual%2520Learning%2520%2528ORL%2529%252C%2520a%2520method%2520that%2520combines%2520online%250Aadaptation%2520with%2520offline-trained%2520predictions.%2520At%2520a%2520lower%2520level%252C%2520we%2520employ%250Amultiple%2520offline%2520predictions%2520generated%2520before%2520or%2520at%2520the%2520beginning%2520of%2520the%250Aprediction%2520horizon.%2520We%2520augment%2520every%2520offline%2520prediction%2520by%2520learning%2520their%250Arespective%2520residual%2520error%2520concerning%2520the%2520true%2520target%2520state%2520online%252C%2520using%2520the%250Arecursive%2520least%2520squares%2520algorithm.%2520At%2520a%2520higher%2520level%252C%2520we%2520treat%2520the%2520augmented%250Alower-level%2520predictors%2520as%2520experts%252C%2520adopting%2520the%2520Prediction%2520with%2520Expert%2520Advice%250Aframework.%2520We%2520utilize%2520an%2520adaptive%2520softmax%2520weighting%2520scheme%2520to%2520form%2520an%2520aggregate%250Aprediction%2520and%2520provide%2520guarantees%2520for%2520ORL%2520in%2520terms%2520of%2520regret.%2520We%2520employ%2520ORL%2520to%250Aboost%2520performance%2520in%2520the%2520setting%2520of%2520online%2520pedestrian%2520trajectory%2520prediction.%250ABased%2520on%2520data%2520from%2520the%2520Stanford%2520Drone%2520Dataset%252C%2520we%2520show%2520that%2520ORL%2520can%2520demonstrate%250Abest-of-both-worlds%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Residual%20Learning%20from%20Offline%20Experts%20for%20Pedestrian%20Tracking&entry.906535625=Anastasios%20Vlachos%20and%20Anastasios%20Tsiamis%20and%20Aren%20Karapetyan%20and%20Efe%20C.%20Balta%20and%20John%20Lygeros&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20predicting%20unknown%20targets%20from%0Adata.%20We%20propose%20Online%20Residual%20Learning%20%28ORL%29%2C%20a%20method%20that%20combines%20online%0Aadaptation%20with%20offline-trained%20predictions.%20At%20a%20lower%20level%2C%20we%20employ%0Amultiple%20offline%20predictions%20generated%20before%20or%20at%20the%20beginning%20of%20the%0Aprediction%20horizon.%20We%20augment%20every%20offline%20prediction%20by%20learning%20their%0Arespective%20residual%20error%20concerning%20the%20true%20target%20state%20online%2C%20using%20the%0Arecursive%20least%20squares%20algorithm.%20At%20a%20higher%20level%2C%20we%20treat%20the%20augmented%0Alower-level%20predictors%20as%20experts%2C%20adopting%20the%20Prediction%20with%20Expert%20Advice%0Aframework.%20We%20utilize%20an%20adaptive%20softmax%20weighting%20scheme%20to%20form%20an%20aggregate%0Aprediction%20and%20provide%20guarantees%20for%20ORL%20in%20terms%20of%20regret.%20We%20employ%20ORL%20to%0Aboost%20performance%20in%20the%20setting%20of%20online%20pedestrian%20trajectory%20prediction.%0ABased%20on%20data%20from%20the%20Stanford%20Drone%20Dataset%2C%20we%20show%20that%20ORL%20can%20demonstrate%0Abest-of-both-worlds%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04069v2&entry.124074799=Read"},
{"title": "Real-Time Human Action Recognition on Embedded Platforms", "author": "Ruiqi Wang and Zichen Wang and Peiqi Gao and Mingzhen Li and Jaehwan Jeong and Yihang Xu and Yejin Lee and Lisa Connor and Chenyang Lu", "abstract": "  With advancements in computer vision and deep learning, video-based human\naction recognition (HAR) has become practical. However, due to the complexity\nof the computation pipeline, running HAR on live video streams incurs excessive\ndelays on embedded platforms. This work tackles the real-time performance\nchallenges of HAR with four contributions: 1) an experimental study identifying\na standard Optical Flow (OF) extraction technique as the latency bottleneck in\na state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracy\ntradeoff between the standard and deep learning approaches to OF extraction,\nwhich highlights the need for a novel, efficient motion feature extractor, 3)\nthe design of Integrated Motion Feature Extractor (IMFE), a novel single-shot\nneural network architecture for motion feature extraction with drastic\nimprovement in latency, 4) the development of RT-HARE, a real-time HAR system\ntailored for embedded platforms. Experimental results on an Nvidia Jetson\nXavier NX platform demonstrated that RT-HARE realizes real-time HAR at a video\nframe rate of 30 frames per second while delivering high levels of recognition\naccuracy.\n", "link": "http://arxiv.org/abs/2409.05662v1", "date": "2024-09-09", "relevancy": 2.0455, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5436}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5075}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Human%20Action%20Recognition%20on%20Embedded%20Platforms&body=Title%3A%20Real-Time%20Human%20Action%20Recognition%20on%20Embedded%20Platforms%0AAuthor%3A%20Ruiqi%20Wang%20and%20Zichen%20Wang%20and%20Peiqi%20Gao%20and%20Mingzhen%20Li%20and%20Jaehwan%20Jeong%20and%20Yihang%20Xu%20and%20Yejin%20Lee%20and%20Lisa%20Connor%20and%20Chenyang%20Lu%0AAbstract%3A%20%20%20With%20advancements%20in%20computer%20vision%20and%20deep%20learning%2C%20video-based%20human%0Aaction%20recognition%20%28HAR%29%20has%20become%20practical.%20However%2C%20due%20to%20the%20complexity%0Aof%20the%20computation%20pipeline%2C%20running%20HAR%20on%20live%20video%20streams%20incurs%20excessive%0Adelays%20on%20embedded%20platforms.%20This%20work%20tackles%20the%20real-time%20performance%0Achallenges%20of%20HAR%20with%20four%20contributions%3A%201%29%20an%20experimental%20study%20identifying%0Aa%20standard%20Optical%20Flow%20%28OF%29%20extraction%20technique%20as%20the%20latency%20bottleneck%20in%0Aa%20state-of-the-art%20HAR%20pipeline%2C%202%29%20an%20exploration%20of%20the%20latency-accuracy%0Atradeoff%20between%20the%20standard%20and%20deep%20learning%20approaches%20to%20OF%20extraction%2C%0Awhich%20highlights%20the%20need%20for%20a%20novel%2C%20efficient%20motion%20feature%20extractor%2C%203%29%0Athe%20design%20of%20Integrated%20Motion%20Feature%20Extractor%20%28IMFE%29%2C%20a%20novel%20single-shot%0Aneural%20network%20architecture%20for%20motion%20feature%20extraction%20with%20drastic%0Aimprovement%20in%20latency%2C%204%29%20the%20development%20of%20RT-HARE%2C%20a%20real-time%20HAR%20system%0Atailored%20for%20embedded%20platforms.%20Experimental%20results%20on%20an%20Nvidia%20Jetson%0AXavier%20NX%20platform%20demonstrated%20that%20RT-HARE%20realizes%20real-time%20HAR%20at%20a%20video%0Aframe%20rate%20of%2030%20frames%20per%20second%20while%20delivering%20high%20levels%20of%20recognition%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Human%2520Action%2520Recognition%2520on%2520Embedded%2520Platforms%26entry.906535625%3DRuiqi%2520Wang%2520and%2520Zichen%2520Wang%2520and%2520Peiqi%2520Gao%2520and%2520Mingzhen%2520Li%2520and%2520Jaehwan%2520Jeong%2520and%2520Yihang%2520Xu%2520and%2520Yejin%2520Lee%2520and%2520Lisa%2520Connor%2520and%2520Chenyang%2520Lu%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520computer%2520vision%2520and%2520deep%2520learning%252C%2520video-based%2520human%250Aaction%2520recognition%2520%2528HAR%2529%2520has%2520become%2520practical.%2520However%252C%2520due%2520to%2520the%2520complexity%250Aof%2520the%2520computation%2520pipeline%252C%2520running%2520HAR%2520on%2520live%2520video%2520streams%2520incurs%2520excessive%250Adelays%2520on%2520embedded%2520platforms.%2520This%2520work%2520tackles%2520the%2520real-time%2520performance%250Achallenges%2520of%2520HAR%2520with%2520four%2520contributions%253A%25201%2529%2520an%2520experimental%2520study%2520identifying%250Aa%2520standard%2520Optical%2520Flow%2520%2528OF%2529%2520extraction%2520technique%2520as%2520the%2520latency%2520bottleneck%2520in%250Aa%2520state-of-the-art%2520HAR%2520pipeline%252C%25202%2529%2520an%2520exploration%2520of%2520the%2520latency-accuracy%250Atradeoff%2520between%2520the%2520standard%2520and%2520deep%2520learning%2520approaches%2520to%2520OF%2520extraction%252C%250Awhich%2520highlights%2520the%2520need%2520for%2520a%2520novel%252C%2520efficient%2520motion%2520feature%2520extractor%252C%25203%2529%250Athe%2520design%2520of%2520Integrated%2520Motion%2520Feature%2520Extractor%2520%2528IMFE%2529%252C%2520a%2520novel%2520single-shot%250Aneural%2520network%2520architecture%2520for%2520motion%2520feature%2520extraction%2520with%2520drastic%250Aimprovement%2520in%2520latency%252C%25204%2529%2520the%2520development%2520of%2520RT-HARE%252C%2520a%2520real-time%2520HAR%2520system%250Atailored%2520for%2520embedded%2520platforms.%2520Experimental%2520results%2520on%2520an%2520Nvidia%2520Jetson%250AXavier%2520NX%2520platform%2520demonstrated%2520that%2520RT-HARE%2520realizes%2520real-time%2520HAR%2520at%2520a%2520video%250Aframe%2520rate%2520of%252030%2520frames%2520per%2520second%2520while%2520delivering%2520high%2520levels%2520of%2520recognition%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Human%20Action%20Recognition%20on%20Embedded%20Platforms&entry.906535625=Ruiqi%20Wang%20and%20Zichen%20Wang%20and%20Peiqi%20Gao%20and%20Mingzhen%20Li%20and%20Jaehwan%20Jeong%20and%20Yihang%20Xu%20and%20Yejin%20Lee%20and%20Lisa%20Connor%20and%20Chenyang%20Lu&entry.1292438233=%20%20With%20advancements%20in%20computer%20vision%20and%20deep%20learning%2C%20video-based%20human%0Aaction%20recognition%20%28HAR%29%20has%20become%20practical.%20However%2C%20due%20to%20the%20complexity%0Aof%20the%20computation%20pipeline%2C%20running%20HAR%20on%20live%20video%20streams%20incurs%20excessive%0Adelays%20on%20embedded%20platforms.%20This%20work%20tackles%20the%20real-time%20performance%0Achallenges%20of%20HAR%20with%20four%20contributions%3A%201%29%20an%20experimental%20study%20identifying%0Aa%20standard%20Optical%20Flow%20%28OF%29%20extraction%20technique%20as%20the%20latency%20bottleneck%20in%0Aa%20state-of-the-art%20HAR%20pipeline%2C%202%29%20an%20exploration%20of%20the%20latency-accuracy%0Atradeoff%20between%20the%20standard%20and%20deep%20learning%20approaches%20to%20OF%20extraction%2C%0Awhich%20highlights%20the%20need%20for%20a%20novel%2C%20efficient%20motion%20feature%20extractor%2C%203%29%0Athe%20design%20of%20Integrated%20Motion%20Feature%20Extractor%20%28IMFE%29%2C%20a%20novel%20single-shot%0Aneural%20network%20architecture%20for%20motion%20feature%20extraction%20with%20drastic%0Aimprovement%20in%20latency%2C%204%29%20the%20development%20of%20RT-HARE%2C%20a%20real-time%20HAR%20system%0Atailored%20for%20embedded%20platforms.%20Experimental%20results%20on%20an%20Nvidia%20Jetson%0AXavier%20NX%20platform%20demonstrated%20that%20RT-HARE%20realizes%20real-time%20HAR%20at%20a%20video%0Aframe%20rate%20of%2030%20frames%20per%20second%20while%20delivering%20high%20levels%20of%20recognition%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05662v1&entry.124074799=Read"},
{"title": "Boosting CNN-based Handwriting Recognition Systems with Learnable\n  Relaxation Labeling", "author": "Sara Ferro and Alessandro Torcinovich and Arianna Traviglia and Marcello Pelillo", "abstract": "  The primary challenge for handwriting recognition systems lies in managing\nlong-range contextual dependencies, an issue that traditional models often\nstruggle with. To mitigate it, attention mechanisms have recently been employed\nto enhance context-aware labelling, thereby achieving state-of-the-art\nperformance. In the field of pattern recognition and image analysis, however,\nthe use of contextual information in labelling problems has a long history and\ngoes back at least to the early 1970's. Among the various approaches developed\nin those years, Relaxation Labelling (RL) processes have played a prominent\nrole and have been the method of choice in the field for more than a decade.\nContrary to recent transformer-based architectures, RL processes offer a\nprincipled approach to the use of contextual constraints, having a solid\ntheoretic foundation grounded on variational inequality and game theory, as\nwell as effective algorithms with convergence guarantees. In this paper, we\npropose a novel approach to handwriting recognition that integrates the\nstrengths of two distinct methodologies. In particular, we propose integrating\n(trainable) RL processes with various well-established neural architectures and\nwe introduce a sparsification technique that accelerates the convergence of the\nalgorithm and enhances the overall system's performance. Experiments over\nseveral benchmark datasets show that RL processes can improve the\ngeneralisation ability, even surpassing in some cases transformer-based\narchitectures.\n", "link": "http://arxiv.org/abs/2409.05699v1", "date": "2024-09-09", "relevancy": 2.0402, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5317}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5171}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20CNN-based%20Handwriting%20Recognition%20Systems%20with%20Learnable%0A%20%20Relaxation%20Labeling&body=Title%3A%20Boosting%20CNN-based%20Handwriting%20Recognition%20Systems%20with%20Learnable%0A%20%20Relaxation%20Labeling%0AAuthor%3A%20Sara%20Ferro%20and%20Alessandro%20Torcinovich%20and%20Arianna%20Traviglia%20and%20Marcello%20Pelillo%0AAbstract%3A%20%20%20The%20primary%20challenge%20for%20handwriting%20recognition%20systems%20lies%20in%20managing%0Along-range%20contextual%20dependencies%2C%20an%20issue%20that%20traditional%20models%20often%0Astruggle%20with.%20To%20mitigate%20it%2C%20attention%20mechanisms%20have%20recently%20been%20employed%0Ato%20enhance%20context-aware%20labelling%2C%20thereby%20achieving%20state-of-the-art%0Aperformance.%20In%20the%20field%20of%20pattern%20recognition%20and%20image%20analysis%2C%20however%2C%0Athe%20use%20of%20contextual%20information%20in%20labelling%20problems%20has%20a%20long%20history%20and%0Agoes%20back%20at%20least%20to%20the%20early%201970%27s.%20Among%20the%20various%20approaches%20developed%0Ain%20those%20years%2C%20Relaxation%20Labelling%20%28RL%29%20processes%20have%20played%20a%20prominent%0Arole%20and%20have%20been%20the%20method%20of%20choice%20in%20the%20field%20for%20more%20than%20a%20decade.%0AContrary%20to%20recent%20transformer-based%20architectures%2C%20RL%20processes%20offer%20a%0Aprincipled%20approach%20to%20the%20use%20of%20contextual%20constraints%2C%20having%20a%20solid%0Atheoretic%20foundation%20grounded%20on%20variational%20inequality%20and%20game%20theory%2C%20as%0Awell%20as%20effective%20algorithms%20with%20convergence%20guarantees.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20to%20handwriting%20recognition%20that%20integrates%20the%0Astrengths%20of%20two%20distinct%20methodologies.%20In%20particular%2C%20we%20propose%20integrating%0A%28trainable%29%20RL%20processes%20with%20various%20well-established%20neural%20architectures%20and%0Awe%20introduce%20a%20sparsification%20technique%20that%20accelerates%20the%20convergence%20of%20the%0Aalgorithm%20and%20enhances%20the%20overall%20system%27s%20performance.%20Experiments%20over%0Aseveral%20benchmark%20datasets%20show%20that%20RL%20processes%20can%20improve%20the%0Ageneralisation%20ability%2C%20even%20surpassing%20in%20some%20cases%20transformer-based%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520CNN-based%2520Handwriting%2520Recognition%2520Systems%2520with%2520Learnable%250A%2520%2520Relaxation%2520Labeling%26entry.906535625%3DSara%2520Ferro%2520and%2520Alessandro%2520Torcinovich%2520and%2520Arianna%2520Traviglia%2520and%2520Marcello%2520Pelillo%26entry.1292438233%3D%2520%2520The%2520primary%2520challenge%2520for%2520handwriting%2520recognition%2520systems%2520lies%2520in%2520managing%250Along-range%2520contextual%2520dependencies%252C%2520an%2520issue%2520that%2520traditional%2520models%2520often%250Astruggle%2520with.%2520To%2520mitigate%2520it%252C%2520attention%2520mechanisms%2520have%2520recently%2520been%2520employed%250Ato%2520enhance%2520context-aware%2520labelling%252C%2520thereby%2520achieving%2520state-of-the-art%250Aperformance.%2520In%2520the%2520field%2520of%2520pattern%2520recognition%2520and%2520image%2520analysis%252C%2520however%252C%250Athe%2520use%2520of%2520contextual%2520information%2520in%2520labelling%2520problems%2520has%2520a%2520long%2520history%2520and%250Agoes%2520back%2520at%2520least%2520to%2520the%2520early%25201970%2527s.%2520Among%2520the%2520various%2520approaches%2520developed%250Ain%2520those%2520years%252C%2520Relaxation%2520Labelling%2520%2528RL%2529%2520processes%2520have%2520played%2520a%2520prominent%250Arole%2520and%2520have%2520been%2520the%2520method%2520of%2520choice%2520in%2520the%2520field%2520for%2520more%2520than%2520a%2520decade.%250AContrary%2520to%2520recent%2520transformer-based%2520architectures%252C%2520RL%2520processes%2520offer%2520a%250Aprincipled%2520approach%2520to%2520the%2520use%2520of%2520contextual%2520constraints%252C%2520having%2520a%2520solid%250Atheoretic%2520foundation%2520grounded%2520on%2520variational%2520inequality%2520and%2520game%2520theory%252C%2520as%250Awell%2520as%2520effective%2520algorithms%2520with%2520convergence%2520guarantees.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520to%2520handwriting%2520recognition%2520that%2520integrates%2520the%250Astrengths%2520of%2520two%2520distinct%2520methodologies.%2520In%2520particular%252C%2520we%2520propose%2520integrating%250A%2528trainable%2529%2520RL%2520processes%2520with%2520various%2520well-established%2520neural%2520architectures%2520and%250Awe%2520introduce%2520a%2520sparsification%2520technique%2520that%2520accelerates%2520the%2520convergence%2520of%2520the%250Aalgorithm%2520and%2520enhances%2520the%2520overall%2520system%2527s%2520performance.%2520Experiments%2520over%250Aseveral%2520benchmark%2520datasets%2520show%2520that%2520RL%2520processes%2520can%2520improve%2520the%250Ageneralisation%2520ability%252C%2520even%2520surpassing%2520in%2520some%2520cases%2520transformer-based%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20CNN-based%20Handwriting%20Recognition%20Systems%20with%20Learnable%0A%20%20Relaxation%20Labeling&entry.906535625=Sara%20Ferro%20and%20Alessandro%20Torcinovich%20and%20Arianna%20Traviglia%20and%20Marcello%20Pelillo&entry.1292438233=%20%20The%20primary%20challenge%20for%20handwriting%20recognition%20systems%20lies%20in%20managing%0Along-range%20contextual%20dependencies%2C%20an%20issue%20that%20traditional%20models%20often%0Astruggle%20with.%20To%20mitigate%20it%2C%20attention%20mechanisms%20have%20recently%20been%20employed%0Ato%20enhance%20context-aware%20labelling%2C%20thereby%20achieving%20state-of-the-art%0Aperformance.%20In%20the%20field%20of%20pattern%20recognition%20and%20image%20analysis%2C%20however%2C%0Athe%20use%20of%20contextual%20information%20in%20labelling%20problems%20has%20a%20long%20history%20and%0Agoes%20back%20at%20least%20to%20the%20early%201970%27s.%20Among%20the%20various%20approaches%20developed%0Ain%20those%20years%2C%20Relaxation%20Labelling%20%28RL%29%20processes%20have%20played%20a%20prominent%0Arole%20and%20have%20been%20the%20method%20of%20choice%20in%20the%20field%20for%20more%20than%20a%20decade.%0AContrary%20to%20recent%20transformer-based%20architectures%2C%20RL%20processes%20offer%20a%0Aprincipled%20approach%20to%20the%20use%20of%20contextual%20constraints%2C%20having%20a%20solid%0Atheoretic%20foundation%20grounded%20on%20variational%20inequality%20and%20game%20theory%2C%20as%0Awell%20as%20effective%20algorithms%20with%20convergence%20guarantees.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20to%20handwriting%20recognition%20that%20integrates%20the%0Astrengths%20of%20two%20distinct%20methodologies.%20In%20particular%2C%20we%20propose%20integrating%0A%28trainable%29%20RL%20processes%20with%20various%20well-established%20neural%20architectures%20and%0Awe%20introduce%20a%20sparsification%20technique%20that%20accelerates%20the%20convergence%20of%20the%0Aalgorithm%20and%20enhances%20the%20overall%20system%27s%20performance.%20Experiments%20over%0Aseveral%20benchmark%20datasets%20show%20that%20RL%20processes%20can%20improve%20the%0Ageneralisation%20ability%2C%20even%20surpassing%20in%20some%20cases%20transformer-based%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05699v1&entry.124074799=Read"},
{"title": "Student Classroom Behavior Detection based on YOLOv7-BRA and Multi-Model\n  Fusion", "author": "Fan Yang and Tao Wang and Xiaofei Wang", "abstract": "  Accurately detecting student behavior in classroom videos can aid in\nanalyzing their classroom performance and improving teaching effectiveness.\nHowever, the current accuracy rate in behavior detection is low. To address\nthis challenge, we propose the Student Classroom Behavior Detection system\nbased on based on YOLOv7-BRA (YOLOv7 with Bi-level Routing Attention ). We\nidentified eight different behavior patterns, including standing, sitting,\nspeaking, listening, walking, raising hands, reading, and writing. We\nconstructed a dataset, which contained 11,248 labels and 4,001 images, with an\nemphasis on the common behavior of raising hands in a classroom setting\n(Student Classroom Behavior dataset, SCB-Dataset). To improve detection\naccuracy, we added the biformer attention module to the YOLOv7 network.\nFinally, we fused the results from YOLOv7 CrowdHuman, SlowFast, and DeepSort\nmodels to obtain student classroom behavior data. We conducted experiments on\nthe SCB-Dataset, and YOLOv7-BRA achieved an mAP@0.5 of 87.1%, resulting in a\n2.2% improvement over previous results. Our SCB-dataset can be downloaded from:\nhttps://github.com/Whiffe/SCB-datase\n", "link": "http://arxiv.org/abs/2305.07825v2", "date": "2024-09-09", "relevancy": 2.019, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5085}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Student%20Classroom%20Behavior%20Detection%20based%20on%20YOLOv7-BRA%20and%20Multi-Model%0A%20%20Fusion&body=Title%3A%20Student%20Classroom%20Behavior%20Detection%20based%20on%20YOLOv7-BRA%20and%20Multi-Model%0A%20%20Fusion%0AAuthor%3A%20Fan%20Yang%20and%20Tao%20Wang%20and%20Xiaofei%20Wang%0AAbstract%3A%20%20%20Accurately%20detecting%20student%20behavior%20in%20classroom%20videos%20can%20aid%20in%0Aanalyzing%20their%20classroom%20performance%20and%20improving%20teaching%20effectiveness.%0AHowever%2C%20the%20current%20accuracy%20rate%20in%20behavior%20detection%20is%20low.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20Student%20Classroom%20Behavior%20Detection%20system%0Abased%20on%20based%20on%20YOLOv7-BRA%20%28YOLOv7%20with%20Bi-level%20Routing%20Attention%20%29.%20We%0Aidentified%20eight%20different%20behavior%20patterns%2C%20including%20standing%2C%20sitting%2C%0Aspeaking%2C%20listening%2C%20walking%2C%20raising%20hands%2C%20reading%2C%20and%20writing.%20We%0Aconstructed%20a%20dataset%2C%20which%20contained%2011%2C248%20labels%20and%204%2C001%20images%2C%20with%20an%0Aemphasis%20on%20the%20common%20behavior%20of%20raising%20hands%20in%20a%20classroom%20setting%0A%28Student%20Classroom%20Behavior%20dataset%2C%20SCB-Dataset%29.%20To%20improve%20detection%0Aaccuracy%2C%20we%20added%20the%20biformer%20attention%20module%20to%20the%20YOLOv7%20network.%0AFinally%2C%20we%20fused%20the%20results%20from%20YOLOv7%20CrowdHuman%2C%20SlowFast%2C%20and%20DeepSort%0Amodels%20to%20obtain%20student%20classroom%20behavior%20data.%20We%20conducted%20experiments%20on%0Athe%20SCB-Dataset%2C%20and%20YOLOv7-BRA%20achieved%20an%20mAP%400.5%20of%2087.1%25%2C%20resulting%20in%20a%0A2.2%25%20improvement%20over%20previous%20results.%20Our%20SCB-dataset%20can%20be%20downloaded%20from%3A%0Ahttps%3A//github.com/Whiffe/SCB-datase%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudent%2520Classroom%2520Behavior%2520Detection%2520based%2520on%2520YOLOv7-BRA%2520and%2520Multi-Model%250A%2520%2520Fusion%26entry.906535625%3DFan%2520Yang%2520and%2520Tao%2520Wang%2520and%2520Xiaofei%2520Wang%26entry.1292438233%3D%2520%2520Accurately%2520detecting%2520student%2520behavior%2520in%2520classroom%2520videos%2520can%2520aid%2520in%250Aanalyzing%2520their%2520classroom%2520performance%2520and%2520improving%2520teaching%2520effectiveness.%250AHowever%252C%2520the%2520current%2520accuracy%2520rate%2520in%2520behavior%2520detection%2520is%2520low.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520the%2520Student%2520Classroom%2520Behavior%2520Detection%2520system%250Abased%2520on%2520based%2520on%2520YOLOv7-BRA%2520%2528YOLOv7%2520with%2520Bi-level%2520Routing%2520Attention%2520%2529.%2520We%250Aidentified%2520eight%2520different%2520behavior%2520patterns%252C%2520including%2520standing%252C%2520sitting%252C%250Aspeaking%252C%2520listening%252C%2520walking%252C%2520raising%2520hands%252C%2520reading%252C%2520and%2520writing.%2520We%250Aconstructed%2520a%2520dataset%252C%2520which%2520contained%252011%252C248%2520labels%2520and%25204%252C001%2520images%252C%2520with%2520an%250Aemphasis%2520on%2520the%2520common%2520behavior%2520of%2520raising%2520hands%2520in%2520a%2520classroom%2520setting%250A%2528Student%2520Classroom%2520Behavior%2520dataset%252C%2520SCB-Dataset%2529.%2520To%2520improve%2520detection%250Aaccuracy%252C%2520we%2520added%2520the%2520biformer%2520attention%2520module%2520to%2520the%2520YOLOv7%2520network.%250AFinally%252C%2520we%2520fused%2520the%2520results%2520from%2520YOLOv7%2520CrowdHuman%252C%2520SlowFast%252C%2520and%2520DeepSort%250Amodels%2520to%2520obtain%2520student%2520classroom%2520behavior%2520data.%2520We%2520conducted%2520experiments%2520on%250Athe%2520SCB-Dataset%252C%2520and%2520YOLOv7-BRA%2520achieved%2520an%2520mAP%25400.5%2520of%252087.1%2525%252C%2520resulting%2520in%2520a%250A2.2%2525%2520improvement%2520over%2520previous%2520results.%2520Our%2520SCB-dataset%2520can%2520be%2520downloaded%2520from%253A%250Ahttps%253A//github.com/Whiffe/SCB-datase%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Student%20Classroom%20Behavior%20Detection%20based%20on%20YOLOv7-BRA%20and%20Multi-Model%0A%20%20Fusion&entry.906535625=Fan%20Yang%20and%20Tao%20Wang%20and%20Xiaofei%20Wang&entry.1292438233=%20%20Accurately%20detecting%20student%20behavior%20in%20classroom%20videos%20can%20aid%20in%0Aanalyzing%20their%20classroom%20performance%20and%20improving%20teaching%20effectiveness.%0AHowever%2C%20the%20current%20accuracy%20rate%20in%20behavior%20detection%20is%20low.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20Student%20Classroom%20Behavior%20Detection%20system%0Abased%20on%20based%20on%20YOLOv7-BRA%20%28YOLOv7%20with%20Bi-level%20Routing%20Attention%20%29.%20We%0Aidentified%20eight%20different%20behavior%20patterns%2C%20including%20standing%2C%20sitting%2C%0Aspeaking%2C%20listening%2C%20walking%2C%20raising%20hands%2C%20reading%2C%20and%20writing.%20We%0Aconstructed%20a%20dataset%2C%20which%20contained%2011%2C248%20labels%20and%204%2C001%20images%2C%20with%20an%0Aemphasis%20on%20the%20common%20behavior%20of%20raising%20hands%20in%20a%20classroom%20setting%0A%28Student%20Classroom%20Behavior%20dataset%2C%20SCB-Dataset%29.%20To%20improve%20detection%0Aaccuracy%2C%20we%20added%20the%20biformer%20attention%20module%20to%20the%20YOLOv7%20network.%0AFinally%2C%20we%20fused%20the%20results%20from%20YOLOv7%20CrowdHuman%2C%20SlowFast%2C%20and%20DeepSort%0Amodels%20to%20obtain%20student%20classroom%20behavior%20data.%20We%20conducted%20experiments%20on%0Athe%20SCB-Dataset%2C%20and%20YOLOv7-BRA%20achieved%20an%20mAP%400.5%20of%2087.1%25%2C%20resulting%20in%20a%0A2.2%25%20improvement%20over%20previous%20results.%20Our%20SCB-dataset%20can%20be%20downloaded%20from%3A%0Ahttps%3A//github.com/Whiffe/SCB-datase%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07825v2&entry.124074799=Read"},
{"title": "On the Computational Entanglement of Distant Features in Adversarial\n  Machine Learning", "author": "YenLung Lai and Xingbo Dong and Zhe Jin", "abstract": "  In this research, we introduce 'computational entanglement', a phenomenon in\noverparameterized neural networks where the model exploits noise patterns in\nways conceptually linked to the effects of length contraction. More specific,\nour findings demonstrate that overparameterized feedforward linear networks can\neasily achieve zero loss by fitting random noise, even with test samples that\nwere never encountered during training. This phenomenon accompanies length\ncontraction, where trained and test samples converge at the same point within a\nspacetime diagram. Unlike most models that rely on supervised learning, our\nmethod operates unsupervised, without the need for labels or gradient-based\noptimization. Additionally, we show a novel application of computational\nentanglement: transforming adversarial examples-highly non-robuts inputs\nimperceptible to human observers-into outputs that are recognizable and robust.\nThis challenges conventional views on non-robust features in adversarial\nexample generation, providing new insights into the underlying mechanisms. Our\nresults emphasize the importance of computational entanglement for enhancing\nmodel robustness and understanding neural networks in adversarial contexts.\n", "link": "http://arxiv.org/abs/2309.15669v5", "date": "2024-09-09", "relevancy": 2.0156, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4992}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Computational%20Entanglement%20of%20Distant%20Features%20in%20Adversarial%0A%20%20Machine%20Learning&body=Title%3A%20On%20the%20Computational%20Entanglement%20of%20Distant%20Features%20in%20Adversarial%0A%20%20Machine%20Learning%0AAuthor%3A%20YenLung%20Lai%20and%20Xingbo%20Dong%20and%20Zhe%20Jin%0AAbstract%3A%20%20%20In%20this%20research%2C%20we%20introduce%20%27computational%20entanglement%27%2C%20a%20phenomenon%20in%0Aoverparameterized%20neural%20networks%20where%20the%20model%20exploits%20noise%20patterns%20in%0Aways%20conceptually%20linked%20to%20the%20effects%20of%20length%20contraction.%20More%20specific%2C%0Aour%20findings%20demonstrate%20that%20overparameterized%20feedforward%20linear%20networks%20can%0Aeasily%20achieve%20zero%20loss%20by%20fitting%20random%20noise%2C%20even%20with%20test%20samples%20that%0Awere%20never%20encountered%20during%20training.%20This%20phenomenon%20accompanies%20length%0Acontraction%2C%20where%20trained%20and%20test%20samples%20converge%20at%20the%20same%20point%20within%20a%0Aspacetime%20diagram.%20Unlike%20most%20models%20that%20rely%20on%20supervised%20learning%2C%20our%0Amethod%20operates%20unsupervised%2C%20without%20the%20need%20for%20labels%20or%20gradient-based%0Aoptimization.%20Additionally%2C%20we%20show%20a%20novel%20application%20of%20computational%0Aentanglement%3A%20transforming%20adversarial%20examples-highly%20non-robuts%20inputs%0Aimperceptible%20to%20human%20observers-into%20outputs%20that%20are%20recognizable%20and%20robust.%0AThis%20challenges%20conventional%20views%20on%20non-robust%20features%20in%20adversarial%0Aexample%20generation%2C%20providing%20new%20insights%20into%20the%20underlying%20mechanisms.%20Our%0Aresults%20emphasize%20the%20importance%20of%20computational%20entanglement%20for%20enhancing%0Amodel%20robustness%20and%20understanding%20neural%20networks%20in%20adversarial%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15669v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Computational%2520Entanglement%2520of%2520Distant%2520Features%2520in%2520Adversarial%250A%2520%2520Machine%2520Learning%26entry.906535625%3DYenLung%2520Lai%2520and%2520Xingbo%2520Dong%2520and%2520Zhe%2520Jin%26entry.1292438233%3D%2520%2520In%2520this%2520research%252C%2520we%2520introduce%2520%2527computational%2520entanglement%2527%252C%2520a%2520phenomenon%2520in%250Aoverparameterized%2520neural%2520networks%2520where%2520the%2520model%2520exploits%2520noise%2520patterns%2520in%250Aways%2520conceptually%2520linked%2520to%2520the%2520effects%2520of%2520length%2520contraction.%2520More%2520specific%252C%250Aour%2520findings%2520demonstrate%2520that%2520overparameterized%2520feedforward%2520linear%2520networks%2520can%250Aeasily%2520achieve%2520zero%2520loss%2520by%2520fitting%2520random%2520noise%252C%2520even%2520with%2520test%2520samples%2520that%250Awere%2520never%2520encountered%2520during%2520training.%2520This%2520phenomenon%2520accompanies%2520length%250Acontraction%252C%2520where%2520trained%2520and%2520test%2520samples%2520converge%2520at%2520the%2520same%2520point%2520within%2520a%250Aspacetime%2520diagram.%2520Unlike%2520most%2520models%2520that%2520rely%2520on%2520supervised%2520learning%252C%2520our%250Amethod%2520operates%2520unsupervised%252C%2520without%2520the%2520need%2520for%2520labels%2520or%2520gradient-based%250Aoptimization.%2520Additionally%252C%2520we%2520show%2520a%2520novel%2520application%2520of%2520computational%250Aentanglement%253A%2520transforming%2520adversarial%2520examples-highly%2520non-robuts%2520inputs%250Aimperceptible%2520to%2520human%2520observers-into%2520outputs%2520that%2520are%2520recognizable%2520and%2520robust.%250AThis%2520challenges%2520conventional%2520views%2520on%2520non-robust%2520features%2520in%2520adversarial%250Aexample%2520generation%252C%2520providing%2520new%2520insights%2520into%2520the%2520underlying%2520mechanisms.%2520Our%250Aresults%2520emphasize%2520the%2520importance%2520of%2520computational%2520entanglement%2520for%2520enhancing%250Amodel%2520robustness%2520and%2520understanding%2520neural%2520networks%2520in%2520adversarial%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.15669v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Computational%20Entanglement%20of%20Distant%20Features%20in%20Adversarial%0A%20%20Machine%20Learning&entry.906535625=YenLung%20Lai%20and%20Xingbo%20Dong%20and%20Zhe%20Jin&entry.1292438233=%20%20In%20this%20research%2C%20we%20introduce%20%27computational%20entanglement%27%2C%20a%20phenomenon%20in%0Aoverparameterized%20neural%20networks%20where%20the%20model%20exploits%20noise%20patterns%20in%0Aways%20conceptually%20linked%20to%20the%20effects%20of%20length%20contraction.%20More%20specific%2C%0Aour%20findings%20demonstrate%20that%20overparameterized%20feedforward%20linear%20networks%20can%0Aeasily%20achieve%20zero%20loss%20by%20fitting%20random%20noise%2C%20even%20with%20test%20samples%20that%0Awere%20never%20encountered%20during%20training.%20This%20phenomenon%20accompanies%20length%0Acontraction%2C%20where%20trained%20and%20test%20samples%20converge%20at%20the%20same%20point%20within%20a%0Aspacetime%20diagram.%20Unlike%20most%20models%20that%20rely%20on%20supervised%20learning%2C%20our%0Amethod%20operates%20unsupervised%2C%20without%20the%20need%20for%20labels%20or%20gradient-based%0Aoptimization.%20Additionally%2C%20we%20show%20a%20novel%20application%20of%20computational%0Aentanglement%3A%20transforming%20adversarial%20examples-highly%20non-robuts%20inputs%0Aimperceptible%20to%20human%20observers-into%20outputs%20that%20are%20recognizable%20and%20robust.%0AThis%20challenges%20conventional%20views%20on%20non-robust%20features%20in%20adversarial%0Aexample%20generation%2C%20providing%20new%20insights%20into%20the%20underlying%20mechanisms.%20Our%0Aresults%20emphasize%20the%20importance%20of%20computational%20entanglement%20for%20enhancing%0Amodel%20robustness%20and%20understanding%20neural%20networks%20in%20adversarial%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15669v5&entry.124074799=Read"},
{"title": "Robust Real-time Segmentation of Bio-Morphological Features in Human\n  Cherenkov Imaging during Radiotherapy via Deep Learning", "author": "Shiru Wang and Yao Chen and Lesley A. Jarvis and Yucheng Tang and David J. Gladstone and Kimberley S. Samkoe and Brian W. Pogue and Petr Bruza and Rongxiao Zhang", "abstract": "  Cherenkov imaging enables real-time visualization of megavoltage X-ray or\nelectron beam delivery to the patient during Radiation Therapy (RT).\nBio-morphological features, such as vasculature, seen in these images are\npatient-specific signatures that can be used for verification of positioning\nand motion management that are essential to precise RT treatment. However until\nnow, no concerted analysis of this biological feature-based tracking was\nutilized because of the slow speed and accuracy of conventional image\nprocessing for feature segmentation. This study demonstrated the first deep\nlearning framework for such an application, achieving video frame rate\nprocessing. To address the challenge of limited annotation of these features in\nCherenkov images, a transfer learning strategy was applied. A fundus\nphotography dataset including 20,529 patch retina images with ground-truth\nvessel annotation was used to pre-train a ResNet segmentation framework.\nSubsequently, a small Cherenkov dataset (1,483 images from 212 treatment\nfractions of 19 breast cancer patients) with known annotated vasculature masks\nwas used to fine-tune the model for accurate segmentation prediction. This deep\nlearning framework achieved consistent and rapid segmentation of\nCherenkov-imaged bio-morphological features on another 19 patients, including\nsubcutaneous veins, scars, and pigmented skin. Average segmentation by the\nmodel achieved Dice score of 0.85 and required less than 0.7 milliseconds\nprocessing time per instance. The model demonstrated outstanding consistency\nagainst input image variances and speed compared to conventional manual\nsegmentation methods, laying the foundation for online segmentation in\nreal-time monitoring in a prospective setting.\n", "link": "http://arxiv.org/abs/2409.05666v1", "date": "2024-09-09", "relevancy": 2.015, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.507}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Real-time%20Segmentation%20of%20Bio-Morphological%20Features%20in%20Human%0A%20%20Cherenkov%20Imaging%20during%20Radiotherapy%20via%20Deep%20Learning&body=Title%3A%20Robust%20Real-time%20Segmentation%20of%20Bio-Morphological%20Features%20in%20Human%0A%20%20Cherenkov%20Imaging%20during%20Radiotherapy%20via%20Deep%20Learning%0AAuthor%3A%20Shiru%20Wang%20and%20Yao%20Chen%20and%20Lesley%20A.%20Jarvis%20and%20Yucheng%20Tang%20and%20David%20J.%20Gladstone%20and%20Kimberley%20S.%20Samkoe%20and%20Brian%20W.%20Pogue%20and%20Petr%20Bruza%20and%20Rongxiao%20Zhang%0AAbstract%3A%20%20%20Cherenkov%20imaging%20enables%20real-time%20visualization%20of%20megavoltage%20X-ray%20or%0Aelectron%20beam%20delivery%20to%20the%20patient%20during%20Radiation%20Therapy%20%28RT%29.%0ABio-morphological%20features%2C%20such%20as%20vasculature%2C%20seen%20in%20these%20images%20are%0Apatient-specific%20signatures%20that%20can%20be%20used%20for%20verification%20of%20positioning%0Aand%20motion%20management%20that%20are%20essential%20to%20precise%20RT%20treatment.%20However%20until%0Anow%2C%20no%20concerted%20analysis%20of%20this%20biological%20feature-based%20tracking%20was%0Autilized%20because%20of%20the%20slow%20speed%20and%20accuracy%20of%20conventional%20image%0Aprocessing%20for%20feature%20segmentation.%20This%20study%20demonstrated%20the%20first%20deep%0Alearning%20framework%20for%20such%20an%20application%2C%20achieving%20video%20frame%20rate%0Aprocessing.%20To%20address%20the%20challenge%20of%20limited%20annotation%20of%20these%20features%20in%0ACherenkov%20images%2C%20a%20transfer%20learning%20strategy%20was%20applied.%20A%20fundus%0Aphotography%20dataset%20including%2020%2C529%20patch%20retina%20images%20with%20ground-truth%0Avessel%20annotation%20was%20used%20to%20pre-train%20a%20ResNet%20segmentation%20framework.%0ASubsequently%2C%20a%20small%20Cherenkov%20dataset%20%281%2C483%20images%20from%20212%20treatment%0Afractions%20of%2019%20breast%20cancer%20patients%29%20with%20known%20annotated%20vasculature%20masks%0Awas%20used%20to%20fine-tune%20the%20model%20for%20accurate%20segmentation%20prediction.%20This%20deep%0Alearning%20framework%20achieved%20consistent%20and%20rapid%20segmentation%20of%0ACherenkov-imaged%20bio-morphological%20features%20on%20another%2019%20patients%2C%20including%0Asubcutaneous%20veins%2C%20scars%2C%20and%20pigmented%20skin.%20Average%20segmentation%20by%20the%0Amodel%20achieved%20Dice%20score%20of%200.85%20and%20required%20less%20than%200.7%20milliseconds%0Aprocessing%20time%20per%20instance.%20The%20model%20demonstrated%20outstanding%20consistency%0Aagainst%20input%20image%20variances%20and%20speed%20compared%20to%20conventional%20manual%0Asegmentation%20methods%2C%20laying%20the%20foundation%20for%20online%20segmentation%20in%0Areal-time%20monitoring%20in%20a%20prospective%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Real-time%2520Segmentation%2520of%2520Bio-Morphological%2520Features%2520in%2520Human%250A%2520%2520Cherenkov%2520Imaging%2520during%2520Radiotherapy%2520via%2520Deep%2520Learning%26entry.906535625%3DShiru%2520Wang%2520and%2520Yao%2520Chen%2520and%2520Lesley%2520A.%2520Jarvis%2520and%2520Yucheng%2520Tang%2520and%2520David%2520J.%2520Gladstone%2520and%2520Kimberley%2520S.%2520Samkoe%2520and%2520Brian%2520W.%2520Pogue%2520and%2520Petr%2520Bruza%2520and%2520Rongxiao%2520Zhang%26entry.1292438233%3D%2520%2520Cherenkov%2520imaging%2520enables%2520real-time%2520visualization%2520of%2520megavoltage%2520X-ray%2520or%250Aelectron%2520beam%2520delivery%2520to%2520the%2520patient%2520during%2520Radiation%2520Therapy%2520%2528RT%2529.%250ABio-morphological%2520features%252C%2520such%2520as%2520vasculature%252C%2520seen%2520in%2520these%2520images%2520are%250Apatient-specific%2520signatures%2520that%2520can%2520be%2520used%2520for%2520verification%2520of%2520positioning%250Aand%2520motion%2520management%2520that%2520are%2520essential%2520to%2520precise%2520RT%2520treatment.%2520However%2520until%250Anow%252C%2520no%2520concerted%2520analysis%2520of%2520this%2520biological%2520feature-based%2520tracking%2520was%250Autilized%2520because%2520of%2520the%2520slow%2520speed%2520and%2520accuracy%2520of%2520conventional%2520image%250Aprocessing%2520for%2520feature%2520segmentation.%2520This%2520study%2520demonstrated%2520the%2520first%2520deep%250Alearning%2520framework%2520for%2520such%2520an%2520application%252C%2520achieving%2520video%2520frame%2520rate%250Aprocessing.%2520To%2520address%2520the%2520challenge%2520of%2520limited%2520annotation%2520of%2520these%2520features%2520in%250ACherenkov%2520images%252C%2520a%2520transfer%2520learning%2520strategy%2520was%2520applied.%2520A%2520fundus%250Aphotography%2520dataset%2520including%252020%252C529%2520patch%2520retina%2520images%2520with%2520ground-truth%250Avessel%2520annotation%2520was%2520used%2520to%2520pre-train%2520a%2520ResNet%2520segmentation%2520framework.%250ASubsequently%252C%2520a%2520small%2520Cherenkov%2520dataset%2520%25281%252C483%2520images%2520from%2520212%2520treatment%250Afractions%2520of%252019%2520breast%2520cancer%2520patients%2529%2520with%2520known%2520annotated%2520vasculature%2520masks%250Awas%2520used%2520to%2520fine-tune%2520the%2520model%2520for%2520accurate%2520segmentation%2520prediction.%2520This%2520deep%250Alearning%2520framework%2520achieved%2520consistent%2520and%2520rapid%2520segmentation%2520of%250ACherenkov-imaged%2520bio-morphological%2520features%2520on%2520another%252019%2520patients%252C%2520including%250Asubcutaneous%2520veins%252C%2520scars%252C%2520and%2520pigmented%2520skin.%2520Average%2520segmentation%2520by%2520the%250Amodel%2520achieved%2520Dice%2520score%2520of%25200.85%2520and%2520required%2520less%2520than%25200.7%2520milliseconds%250Aprocessing%2520time%2520per%2520instance.%2520The%2520model%2520demonstrated%2520outstanding%2520consistency%250Aagainst%2520input%2520image%2520variances%2520and%2520speed%2520compared%2520to%2520conventional%2520manual%250Asegmentation%2520methods%252C%2520laying%2520the%2520foundation%2520for%2520online%2520segmentation%2520in%250Areal-time%2520monitoring%2520in%2520a%2520prospective%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Real-time%20Segmentation%20of%20Bio-Morphological%20Features%20in%20Human%0A%20%20Cherenkov%20Imaging%20during%20Radiotherapy%20via%20Deep%20Learning&entry.906535625=Shiru%20Wang%20and%20Yao%20Chen%20and%20Lesley%20A.%20Jarvis%20and%20Yucheng%20Tang%20and%20David%20J.%20Gladstone%20and%20Kimberley%20S.%20Samkoe%20and%20Brian%20W.%20Pogue%20and%20Petr%20Bruza%20and%20Rongxiao%20Zhang&entry.1292438233=%20%20Cherenkov%20imaging%20enables%20real-time%20visualization%20of%20megavoltage%20X-ray%20or%0Aelectron%20beam%20delivery%20to%20the%20patient%20during%20Radiation%20Therapy%20%28RT%29.%0ABio-morphological%20features%2C%20such%20as%20vasculature%2C%20seen%20in%20these%20images%20are%0Apatient-specific%20signatures%20that%20can%20be%20used%20for%20verification%20of%20positioning%0Aand%20motion%20management%20that%20are%20essential%20to%20precise%20RT%20treatment.%20However%20until%0Anow%2C%20no%20concerted%20analysis%20of%20this%20biological%20feature-based%20tracking%20was%0Autilized%20because%20of%20the%20slow%20speed%20and%20accuracy%20of%20conventional%20image%0Aprocessing%20for%20feature%20segmentation.%20This%20study%20demonstrated%20the%20first%20deep%0Alearning%20framework%20for%20such%20an%20application%2C%20achieving%20video%20frame%20rate%0Aprocessing.%20To%20address%20the%20challenge%20of%20limited%20annotation%20of%20these%20features%20in%0ACherenkov%20images%2C%20a%20transfer%20learning%20strategy%20was%20applied.%20A%20fundus%0Aphotography%20dataset%20including%2020%2C529%20patch%20retina%20images%20with%20ground-truth%0Avessel%20annotation%20was%20used%20to%20pre-train%20a%20ResNet%20segmentation%20framework.%0ASubsequently%2C%20a%20small%20Cherenkov%20dataset%20%281%2C483%20images%20from%20212%20treatment%0Afractions%20of%2019%20breast%20cancer%20patients%29%20with%20known%20annotated%20vasculature%20masks%0Awas%20used%20to%20fine-tune%20the%20model%20for%20accurate%20segmentation%20prediction.%20This%20deep%0Alearning%20framework%20achieved%20consistent%20and%20rapid%20segmentation%20of%0ACherenkov-imaged%20bio-morphological%20features%20on%20another%2019%20patients%2C%20including%0Asubcutaneous%20veins%2C%20scars%2C%20and%20pigmented%20skin.%20Average%20segmentation%20by%20the%0Amodel%20achieved%20Dice%20score%20of%200.85%20and%20required%20less%20than%200.7%20milliseconds%0Aprocessing%20time%20per%20instance.%20The%20model%20demonstrated%20outstanding%20consistency%0Aagainst%20input%20image%20variances%20and%20speed%20compared%20to%20conventional%20manual%0Asegmentation%20methods%2C%20laying%20the%20foundation%20for%20online%20segmentation%20in%0Areal-time%20monitoring%20in%20a%20prospective%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05666v1&entry.124074799=Read"},
{"title": "AnomalyCD: A benchmark for Earth anomaly change detection with\n  high-resolution and time-series observations", "author": "Jingtao Li and Qian Zhu and Xinyu Wang and Hengwei Zhao and Yanfei Zhong", "abstract": "  Various Earth anomalies have destroyed the stable, balanced state, resulting\nin fatalities and serious destruction of property. With the advantages of\nlarge-scale and precise observation, high-resolution remote sensing images have\nbeen widely used for anomaly monitoring and localization. Powered by the deep\nrepresentation, the existing methods have achieved remarkable advances,\nprimarily in classification and change detection techniques. However, labeled\nsamples are difficult to acquire due to the low probability of anomaly\noccurrence, and the trained models are limited to fixed anomaly categories,\nwhich hinders the application for anomalies with few samples or unknown\nanomalies. In this paper, to tackle this problem, we propose the anomaly change\ndetection (AnomalyCD) technique, which accepts time-series observations and\nlearns to identify anomalous changes by learning from the historical normal\nchange pattern. Compared to the existing techniques, AnomalyCD processes an\nunfixed number of time steps and can localize the various anomalies in a\nunified manner, without human supervision. To benchmark AnomalyCD, we\nconstructed a high-resolution dataset with time-series images dedicated to\nvarious Earth anomalies (the AnomalyCDD dataset). AnomalyCDD contains\nhigh-resolution (from 0.15 to 2.39 m/pixel), time-series (from 3 to 7 time\nsteps), and large-scale images (1927.93 km2 in total) collected globally\nFurthermore, we developed a zero-shot baseline model (AnomalyCDM), which\nimplements the AnomalyCD technique by extracting a general representation from\nthe segment anything model (SAM) and conducting temporal comparison to\ndistinguish the anomalous changes from normal changes. AnomalyCDM is designed\nas a two-stage workflow to enhance the efficiency, and has the ability to\nprocess the unseen images directly, without retraining for each scene.\n", "link": "http://arxiv.org/abs/2409.05679v1", "date": "2024-09-09", "relevancy": 2.0015, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5075}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5025}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalyCD%3A%20A%20benchmark%20for%20Earth%20anomaly%20change%20detection%20with%0A%20%20high-resolution%20and%20time-series%20observations&body=Title%3A%20AnomalyCD%3A%20A%20benchmark%20for%20Earth%20anomaly%20change%20detection%20with%0A%20%20high-resolution%20and%20time-series%20observations%0AAuthor%3A%20Jingtao%20Li%20and%20Qian%20Zhu%20and%20Xinyu%20Wang%20and%20Hengwei%20Zhao%20and%20Yanfei%20Zhong%0AAbstract%3A%20%20%20Various%20Earth%20anomalies%20have%20destroyed%20the%20stable%2C%20balanced%20state%2C%20resulting%0Ain%20fatalities%20and%20serious%20destruction%20of%20property.%20With%20the%20advantages%20of%0Alarge-scale%20and%20precise%20observation%2C%20high-resolution%20remote%20sensing%20images%20have%0Abeen%20widely%20used%20for%20anomaly%20monitoring%20and%20localization.%20Powered%20by%20the%20deep%0Arepresentation%2C%20the%20existing%20methods%20have%20achieved%20remarkable%20advances%2C%0Aprimarily%20in%20classification%20and%20change%20detection%20techniques.%20However%2C%20labeled%0Asamples%20are%20difficult%20to%20acquire%20due%20to%20the%20low%20probability%20of%20anomaly%0Aoccurrence%2C%20and%20the%20trained%20models%20are%20limited%20to%20fixed%20anomaly%20categories%2C%0Awhich%20hinders%20the%20application%20for%20anomalies%20with%20few%20samples%20or%20unknown%0Aanomalies.%20In%20this%20paper%2C%20to%20tackle%20this%20problem%2C%20we%20propose%20the%20anomaly%20change%0Adetection%20%28AnomalyCD%29%20technique%2C%20which%20accepts%20time-series%20observations%20and%0Alearns%20to%20identify%20anomalous%20changes%20by%20learning%20from%20the%20historical%20normal%0Achange%20pattern.%20Compared%20to%20the%20existing%20techniques%2C%20AnomalyCD%20processes%20an%0Aunfixed%20number%20of%20time%20steps%20and%20can%20localize%20the%20various%20anomalies%20in%20a%0Aunified%20manner%2C%20without%20human%20supervision.%20To%20benchmark%20AnomalyCD%2C%20we%0Aconstructed%20a%20high-resolution%20dataset%20with%20time-series%20images%20dedicated%20to%0Avarious%20Earth%20anomalies%20%28the%20AnomalyCDD%20dataset%29.%20AnomalyCDD%20contains%0Ahigh-resolution%20%28from%200.15%20to%202.39%20m/pixel%29%2C%20time-series%20%28from%203%20to%207%20time%0Asteps%29%2C%20and%20large-scale%20images%20%281927.93%20km2%20in%20total%29%20collected%20globally%0AFurthermore%2C%20we%20developed%20a%20zero-shot%20baseline%20model%20%28AnomalyCDM%29%2C%20which%0Aimplements%20the%20AnomalyCD%20technique%20by%20extracting%20a%20general%20representation%20from%0Athe%20segment%20anything%20model%20%28SAM%29%20and%20conducting%20temporal%20comparison%20to%0Adistinguish%20the%20anomalous%20changes%20from%20normal%20changes.%20AnomalyCDM%20is%20designed%0Aas%20a%20two-stage%20workflow%20to%20enhance%20the%20efficiency%2C%20and%20has%20the%20ability%20to%0Aprocess%20the%20unseen%20images%20directly%2C%20without%20retraining%20for%20each%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalyCD%253A%2520A%2520benchmark%2520for%2520Earth%2520anomaly%2520change%2520detection%2520with%250A%2520%2520high-resolution%2520and%2520time-series%2520observations%26entry.906535625%3DJingtao%2520Li%2520and%2520Qian%2520Zhu%2520and%2520Xinyu%2520Wang%2520and%2520Hengwei%2520Zhao%2520and%2520Yanfei%2520Zhong%26entry.1292438233%3D%2520%2520Various%2520Earth%2520anomalies%2520have%2520destroyed%2520the%2520stable%252C%2520balanced%2520state%252C%2520resulting%250Ain%2520fatalities%2520and%2520serious%2520destruction%2520of%2520property.%2520With%2520the%2520advantages%2520of%250Alarge-scale%2520and%2520precise%2520observation%252C%2520high-resolution%2520remote%2520sensing%2520images%2520have%250Abeen%2520widely%2520used%2520for%2520anomaly%2520monitoring%2520and%2520localization.%2520Powered%2520by%2520the%2520deep%250Arepresentation%252C%2520the%2520existing%2520methods%2520have%2520achieved%2520remarkable%2520advances%252C%250Aprimarily%2520in%2520classification%2520and%2520change%2520detection%2520techniques.%2520However%252C%2520labeled%250Asamples%2520are%2520difficult%2520to%2520acquire%2520due%2520to%2520the%2520low%2520probability%2520of%2520anomaly%250Aoccurrence%252C%2520and%2520the%2520trained%2520models%2520are%2520limited%2520to%2520fixed%2520anomaly%2520categories%252C%250Awhich%2520hinders%2520the%2520application%2520for%2520anomalies%2520with%2520few%2520samples%2520or%2520unknown%250Aanomalies.%2520In%2520this%2520paper%252C%2520to%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520the%2520anomaly%2520change%250Adetection%2520%2528AnomalyCD%2529%2520technique%252C%2520which%2520accepts%2520time-series%2520observations%2520and%250Alearns%2520to%2520identify%2520anomalous%2520changes%2520by%2520learning%2520from%2520the%2520historical%2520normal%250Achange%2520pattern.%2520Compared%2520to%2520the%2520existing%2520techniques%252C%2520AnomalyCD%2520processes%2520an%250Aunfixed%2520number%2520of%2520time%2520steps%2520and%2520can%2520localize%2520the%2520various%2520anomalies%2520in%2520a%250Aunified%2520manner%252C%2520without%2520human%2520supervision.%2520To%2520benchmark%2520AnomalyCD%252C%2520we%250Aconstructed%2520a%2520high-resolution%2520dataset%2520with%2520time-series%2520images%2520dedicated%2520to%250Avarious%2520Earth%2520anomalies%2520%2528the%2520AnomalyCDD%2520dataset%2529.%2520AnomalyCDD%2520contains%250Ahigh-resolution%2520%2528from%25200.15%2520to%25202.39%2520m/pixel%2529%252C%2520time-series%2520%2528from%25203%2520to%25207%2520time%250Asteps%2529%252C%2520and%2520large-scale%2520images%2520%25281927.93%2520km2%2520in%2520total%2529%2520collected%2520globally%250AFurthermore%252C%2520we%2520developed%2520a%2520zero-shot%2520baseline%2520model%2520%2528AnomalyCDM%2529%252C%2520which%250Aimplements%2520the%2520AnomalyCD%2520technique%2520by%2520extracting%2520a%2520general%2520representation%2520from%250Athe%2520segment%2520anything%2520model%2520%2528SAM%2529%2520and%2520conducting%2520temporal%2520comparison%2520to%250Adistinguish%2520the%2520anomalous%2520changes%2520from%2520normal%2520changes.%2520AnomalyCDM%2520is%2520designed%250Aas%2520a%2520two-stage%2520workflow%2520to%2520enhance%2520the%2520efficiency%252C%2520and%2520has%2520the%2520ability%2520to%250Aprocess%2520the%2520unseen%2520images%2520directly%252C%2520without%2520retraining%2520for%2520each%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyCD%3A%20A%20benchmark%20for%20Earth%20anomaly%20change%20detection%20with%0A%20%20high-resolution%20and%20time-series%20observations&entry.906535625=Jingtao%20Li%20and%20Qian%20Zhu%20and%20Xinyu%20Wang%20and%20Hengwei%20Zhao%20and%20Yanfei%20Zhong&entry.1292438233=%20%20Various%20Earth%20anomalies%20have%20destroyed%20the%20stable%2C%20balanced%20state%2C%20resulting%0Ain%20fatalities%20and%20serious%20destruction%20of%20property.%20With%20the%20advantages%20of%0Alarge-scale%20and%20precise%20observation%2C%20high-resolution%20remote%20sensing%20images%20have%0Abeen%20widely%20used%20for%20anomaly%20monitoring%20and%20localization.%20Powered%20by%20the%20deep%0Arepresentation%2C%20the%20existing%20methods%20have%20achieved%20remarkable%20advances%2C%0Aprimarily%20in%20classification%20and%20change%20detection%20techniques.%20However%2C%20labeled%0Asamples%20are%20difficult%20to%20acquire%20due%20to%20the%20low%20probability%20of%20anomaly%0Aoccurrence%2C%20and%20the%20trained%20models%20are%20limited%20to%20fixed%20anomaly%20categories%2C%0Awhich%20hinders%20the%20application%20for%20anomalies%20with%20few%20samples%20or%20unknown%0Aanomalies.%20In%20this%20paper%2C%20to%20tackle%20this%20problem%2C%20we%20propose%20the%20anomaly%20change%0Adetection%20%28AnomalyCD%29%20technique%2C%20which%20accepts%20time-series%20observations%20and%0Alearns%20to%20identify%20anomalous%20changes%20by%20learning%20from%20the%20historical%20normal%0Achange%20pattern.%20Compared%20to%20the%20existing%20techniques%2C%20AnomalyCD%20processes%20an%0Aunfixed%20number%20of%20time%20steps%20and%20can%20localize%20the%20various%20anomalies%20in%20a%0Aunified%20manner%2C%20without%20human%20supervision.%20To%20benchmark%20AnomalyCD%2C%20we%0Aconstructed%20a%20high-resolution%20dataset%20with%20time-series%20images%20dedicated%20to%0Avarious%20Earth%20anomalies%20%28the%20AnomalyCDD%20dataset%29.%20AnomalyCDD%20contains%0Ahigh-resolution%20%28from%200.15%20to%202.39%20m/pixel%29%2C%20time-series%20%28from%203%20to%207%20time%0Asteps%29%2C%20and%20large-scale%20images%20%281927.93%20km2%20in%20total%29%20collected%20globally%0AFurthermore%2C%20we%20developed%20a%20zero-shot%20baseline%20model%20%28AnomalyCDM%29%2C%20which%0Aimplements%20the%20AnomalyCD%20technique%20by%20extracting%20a%20general%20representation%20from%0Athe%20segment%20anything%20model%20%28SAM%29%20and%20conducting%20temporal%20comparison%20to%0Adistinguish%20the%20anomalous%20changes%20from%20normal%20changes.%20AnomalyCDM%20is%20designed%0Aas%20a%20two-stage%20workflow%20to%20enhance%20the%20efficiency%2C%20and%20has%20the%20ability%20to%0Aprocess%20the%20unseen%20images%20directly%2C%20without%20retraining%20for%20each%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05679v1&entry.124074799=Read"},
{"title": "Explainable AI for Engineering Design: A Unified Approach of Systems\n  Engineering and Component- Based Deep Learning Demonstrated by Energy-\n  Efficient Building Design", "author": "Philipp Geyer and Manav Mahan Singh and Xia Chen", "abstract": "  Data-driven models created by machine learning, gain in importance in all\nfields of design and engineering. They, have high potential to assist\ndecision-makers in creating novel, artefacts with better performance and\nsustainability. However,, limited generalization and the black-box nature of\nthese models, lead to limited explainability and reusability. To overcome this,\nsituation, we propose a component-based approach to create, partial component\nmodels by machine learning (ML). This, component-based approach aligns deep\nlearning with systems, engineering (SE). The key contribution of the\ncomponent-based, method is that activations at interfaces between the\ncomponents, are interpretable engineering quantities. In this way, the,\nhierarchical component system forms a deep neural network, (DNN) that a priori\nintegrates information for engineering, explainability. The, approach adapts\nthe model structure to engineering methods of, systems engineering and to\ndomain knowledge. We examine the, performance of the approach by the field of\nenergy-efficient, building design: First, we observed better generalization of\nthe, component-based method by analyzing prediction accuracy, outside the\ntraining data. Especially for representative designs, different in structure,\nwe observe a much higher accuracy, (R2 = 0.94) compared to conventional\nmonolithic methods, (R2 = 0.71). Second, we illustrate explainability by\nexemplary, demonstrating how sensitivity information from SE and rules, from\nlow-depth decision trees serve engineering. Third, we, evaluate explainability\nby qualitative and quantitative methods, demonstrating the matching of\npreliminary knowledge and data-driven, derived strategies and show correctness\nof activations at, component interfaces compared to white-box simulation\nresults, (envelope components: R2 = 0.92..0.99; zones: R2 = 0.78..0.93).\n", "link": "http://arxiv.org/abs/2108.13836v7", "date": "2024-09-09", "relevancy": 1.9994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20for%20Engineering%20Design%3A%20A%20Unified%20Approach%20of%20Systems%0A%20%20Engineering%20and%20Component-%20Based%20Deep%20Learning%20Demonstrated%20by%20Energy-%0A%20%20Efficient%20Building%20Design&body=Title%3A%20Explainable%20AI%20for%20Engineering%20Design%3A%20A%20Unified%20Approach%20of%20Systems%0A%20%20Engineering%20and%20Component-%20Based%20Deep%20Learning%20Demonstrated%20by%20Energy-%0A%20%20Efficient%20Building%20Design%0AAuthor%3A%20Philipp%20Geyer%20and%20Manav%20Mahan%20Singh%20and%20Xia%20Chen%0AAbstract%3A%20%20%20Data-driven%20models%20created%20by%20machine%20learning%2C%20gain%20in%20importance%20in%20all%0Afields%20of%20design%20and%20engineering.%20They%2C%20have%20high%20potential%20to%20assist%0Adecision-makers%20in%20creating%20novel%2C%20artefacts%20with%20better%20performance%20and%0Asustainability.%20However%2C%2C%20limited%20generalization%20and%20the%20black-box%20nature%20of%0Athese%20models%2C%20lead%20to%20limited%20explainability%20and%20reusability.%20To%20overcome%20this%2C%0Asituation%2C%20we%20propose%20a%20component-based%20approach%20to%20create%2C%20partial%20component%0Amodels%20by%20machine%20learning%20%28ML%29.%20This%2C%20component-based%20approach%20aligns%20deep%0Alearning%20with%20systems%2C%20engineering%20%28SE%29.%20The%20key%20contribution%20of%20the%0Acomponent-based%2C%20method%20is%20that%20activations%20at%20interfaces%20between%20the%0Acomponents%2C%20are%20interpretable%20engineering%20quantities.%20In%20this%20way%2C%20the%2C%0Ahierarchical%20component%20system%20forms%20a%20deep%20neural%20network%2C%20%28DNN%29%20that%20a%20priori%0Aintegrates%20information%20for%20engineering%2C%20explainability.%20The%2C%20approach%20adapts%0Athe%20model%20structure%20to%20engineering%20methods%20of%2C%20systems%20engineering%20and%20to%0Adomain%20knowledge.%20We%20examine%20the%2C%20performance%20of%20the%20approach%20by%20the%20field%20of%0Aenergy-efficient%2C%20building%20design%3A%20First%2C%20we%20observed%20better%20generalization%20of%0Athe%2C%20component-based%20method%20by%20analyzing%20prediction%20accuracy%2C%20outside%20the%0Atraining%20data.%20Especially%20for%20representative%20designs%2C%20different%20in%20structure%2C%0Awe%20observe%20a%20much%20higher%20accuracy%2C%20%28R2%20%3D%200.94%29%20compared%20to%20conventional%0Amonolithic%20methods%2C%20%28R2%20%3D%200.71%29.%20Second%2C%20we%20illustrate%20explainability%20by%0Aexemplary%2C%20demonstrating%20how%20sensitivity%20information%20from%20SE%20and%20rules%2C%20from%0Alow-depth%20decision%20trees%20serve%20engineering.%20Third%2C%20we%2C%20evaluate%20explainability%0Aby%20qualitative%20and%20quantitative%20methods%2C%20demonstrating%20the%20matching%20of%0Apreliminary%20knowledge%20and%20data-driven%2C%20derived%20strategies%20and%20show%20correctness%0Aof%20activations%20at%2C%20component%20interfaces%20compared%20to%20white-box%20simulation%0Aresults%2C%20%28envelope%20components%3A%20R2%20%3D%200.92..0.99%3B%20zones%3A%20R2%20%3D%200.78..0.93%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.13836v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520for%2520Engineering%2520Design%253A%2520A%2520Unified%2520Approach%2520of%2520Systems%250A%2520%2520Engineering%2520and%2520Component-%2520Based%2520Deep%2520Learning%2520Demonstrated%2520by%2520Energy-%250A%2520%2520Efficient%2520Building%2520Design%26entry.906535625%3DPhilipp%2520Geyer%2520and%2520Manav%2520Mahan%2520Singh%2520and%2520Xia%2520Chen%26entry.1292438233%3D%2520%2520Data-driven%2520models%2520created%2520by%2520machine%2520learning%252C%2520gain%2520in%2520importance%2520in%2520all%250Afields%2520of%2520design%2520and%2520engineering.%2520They%252C%2520have%2520high%2520potential%2520to%2520assist%250Adecision-makers%2520in%2520creating%2520novel%252C%2520artefacts%2520with%2520better%2520performance%2520and%250Asustainability.%2520However%252C%252C%2520limited%2520generalization%2520and%2520the%2520black-box%2520nature%2520of%250Athese%2520models%252C%2520lead%2520to%2520limited%2520explainability%2520and%2520reusability.%2520To%2520overcome%2520this%252C%250Asituation%252C%2520we%2520propose%2520a%2520component-based%2520approach%2520to%2520create%252C%2520partial%2520component%250Amodels%2520by%2520machine%2520learning%2520%2528ML%2529.%2520This%252C%2520component-based%2520approach%2520aligns%2520deep%250Alearning%2520with%2520systems%252C%2520engineering%2520%2528SE%2529.%2520The%2520key%2520contribution%2520of%2520the%250Acomponent-based%252C%2520method%2520is%2520that%2520activations%2520at%2520interfaces%2520between%2520the%250Acomponents%252C%2520are%2520interpretable%2520engineering%2520quantities.%2520In%2520this%2520way%252C%2520the%252C%250Ahierarchical%2520component%2520system%2520forms%2520a%2520deep%2520neural%2520network%252C%2520%2528DNN%2529%2520that%2520a%2520priori%250Aintegrates%2520information%2520for%2520engineering%252C%2520explainability.%2520The%252C%2520approach%2520adapts%250Athe%2520model%2520structure%2520to%2520engineering%2520methods%2520of%252C%2520systems%2520engineering%2520and%2520to%250Adomain%2520knowledge.%2520We%2520examine%2520the%252C%2520performance%2520of%2520the%2520approach%2520by%2520the%2520field%2520of%250Aenergy-efficient%252C%2520building%2520design%253A%2520First%252C%2520we%2520observed%2520better%2520generalization%2520of%250Athe%252C%2520component-based%2520method%2520by%2520analyzing%2520prediction%2520accuracy%252C%2520outside%2520the%250Atraining%2520data.%2520Especially%2520for%2520representative%2520designs%252C%2520different%2520in%2520structure%252C%250Awe%2520observe%2520a%2520much%2520higher%2520accuracy%252C%2520%2528R2%2520%253D%25200.94%2529%2520compared%2520to%2520conventional%250Amonolithic%2520methods%252C%2520%2528R2%2520%253D%25200.71%2529.%2520Second%252C%2520we%2520illustrate%2520explainability%2520by%250Aexemplary%252C%2520demonstrating%2520how%2520sensitivity%2520information%2520from%2520SE%2520and%2520rules%252C%2520from%250Alow-depth%2520decision%2520trees%2520serve%2520engineering.%2520Third%252C%2520we%252C%2520evaluate%2520explainability%250Aby%2520qualitative%2520and%2520quantitative%2520methods%252C%2520demonstrating%2520the%2520matching%2520of%250Apreliminary%2520knowledge%2520and%2520data-driven%252C%2520derived%2520strategies%2520and%2520show%2520correctness%250Aof%2520activations%2520at%252C%2520component%2520interfaces%2520compared%2520to%2520white-box%2520simulation%250Aresults%252C%2520%2528envelope%2520components%253A%2520R2%2520%253D%25200.92..0.99%253B%2520zones%253A%2520R2%2520%253D%25200.78..0.93%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.13836v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20for%20Engineering%20Design%3A%20A%20Unified%20Approach%20of%20Systems%0A%20%20Engineering%20and%20Component-%20Based%20Deep%20Learning%20Demonstrated%20by%20Energy-%0A%20%20Efficient%20Building%20Design&entry.906535625=Philipp%20Geyer%20and%20Manav%20Mahan%20Singh%20and%20Xia%20Chen&entry.1292438233=%20%20Data-driven%20models%20created%20by%20machine%20learning%2C%20gain%20in%20importance%20in%20all%0Afields%20of%20design%20and%20engineering.%20They%2C%20have%20high%20potential%20to%20assist%0Adecision-makers%20in%20creating%20novel%2C%20artefacts%20with%20better%20performance%20and%0Asustainability.%20However%2C%2C%20limited%20generalization%20and%20the%20black-box%20nature%20of%0Athese%20models%2C%20lead%20to%20limited%20explainability%20and%20reusability.%20To%20overcome%20this%2C%0Asituation%2C%20we%20propose%20a%20component-based%20approach%20to%20create%2C%20partial%20component%0Amodels%20by%20machine%20learning%20%28ML%29.%20This%2C%20component-based%20approach%20aligns%20deep%0Alearning%20with%20systems%2C%20engineering%20%28SE%29.%20The%20key%20contribution%20of%20the%0Acomponent-based%2C%20method%20is%20that%20activations%20at%20interfaces%20between%20the%0Acomponents%2C%20are%20interpretable%20engineering%20quantities.%20In%20this%20way%2C%20the%2C%0Ahierarchical%20component%20system%20forms%20a%20deep%20neural%20network%2C%20%28DNN%29%20that%20a%20priori%0Aintegrates%20information%20for%20engineering%2C%20explainability.%20The%2C%20approach%20adapts%0Athe%20model%20structure%20to%20engineering%20methods%20of%2C%20systems%20engineering%20and%20to%0Adomain%20knowledge.%20We%20examine%20the%2C%20performance%20of%20the%20approach%20by%20the%20field%20of%0Aenergy-efficient%2C%20building%20design%3A%20First%2C%20we%20observed%20better%20generalization%20of%0Athe%2C%20component-based%20method%20by%20analyzing%20prediction%20accuracy%2C%20outside%20the%0Atraining%20data.%20Especially%20for%20representative%20designs%2C%20different%20in%20structure%2C%0Awe%20observe%20a%20much%20higher%20accuracy%2C%20%28R2%20%3D%200.94%29%20compared%20to%20conventional%0Amonolithic%20methods%2C%20%28R2%20%3D%200.71%29.%20Second%2C%20we%20illustrate%20explainability%20by%0Aexemplary%2C%20demonstrating%20how%20sensitivity%20information%20from%20SE%20and%20rules%2C%20from%0Alow-depth%20decision%20trees%20serve%20engineering.%20Third%2C%20we%2C%20evaluate%20explainability%0Aby%20qualitative%20and%20quantitative%20methods%2C%20demonstrating%20the%20matching%20of%0Apreliminary%20knowledge%20and%20data-driven%2C%20derived%20strategies%20and%20show%20correctness%0Aof%20activations%20at%2C%20component%20interfaces%20compared%20to%20white-box%20simulation%0Aresults%2C%20%28envelope%20components%3A%20R2%20%3D%200.92..0.99%3B%20zones%3A%20R2%20%3D%200.78..0.93%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.13836v7&entry.124074799=Read"},
{"title": "CRADLE-VAE: Enhancing Single-Cell Gene Perturbation Modeling with\n  Counterfactual Reasoning-based Artifact Disentanglement", "author": "Seungheun Baek and Soyon Park and Yan Ting Chok and Junhyun Lee and Jueon Park and Mogan Gim and Jaewoo Kang", "abstract": "  Predicting cellular responses to various perturbations is a critical focus in\ndrug discovery and personalized therapeutics, with deep learning models playing\na significant role in this endeavor. Single-cell datasets contain technical\nartifacts that may hinder the predictability of such models, which poses\nquality control issues highly regarded in this area. To address this, we\npropose CRADLE-VAE, a causal generative framework tailored for single-cell gene\nperturbation modeling, enhanced with counterfactual reasoning-based artifact\ndisentanglement. Throughout training, CRADLE-VAE models the underlying latent\ndistribution of technical artifacts and perturbation effects present in\nsingle-cell datasets. It employs counterfactual reasoning to effectively\ndisentangle such artifacts by modulating the latent basal spaces and learns\nrobust features for generating cellular response data with improved quality.\nExperimental results demonstrate that this approach improves not only treatment\neffect estimation performance but also generative quality as well. The\nCRADLE-VAE codebase is publicly available at\nhttps://github.com/dmis-lab/CRADLE-VAE.\n", "link": "http://arxiv.org/abs/2409.05484v1", "date": "2024-09-09", "relevancy": 1.9988, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5023}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5002}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRADLE-VAE%3A%20Enhancing%20Single-Cell%20Gene%20Perturbation%20Modeling%20with%0A%20%20Counterfactual%20Reasoning-based%20Artifact%20Disentanglement&body=Title%3A%20CRADLE-VAE%3A%20Enhancing%20Single-Cell%20Gene%20Perturbation%20Modeling%20with%0A%20%20Counterfactual%20Reasoning-based%20Artifact%20Disentanglement%0AAuthor%3A%20Seungheun%20Baek%20and%20Soyon%20Park%20and%20Yan%20Ting%20Chok%20and%20Junhyun%20Lee%20and%20Jueon%20Park%20and%20Mogan%20Gim%20and%20Jaewoo%20Kang%0AAbstract%3A%20%20%20Predicting%20cellular%20responses%20to%20various%20perturbations%20is%20a%20critical%20focus%20in%0Adrug%20discovery%20and%20personalized%20therapeutics%2C%20with%20deep%20learning%20models%20playing%0Aa%20significant%20role%20in%20this%20endeavor.%20Single-cell%20datasets%20contain%20technical%0Aartifacts%20that%20may%20hinder%20the%20predictability%20of%20such%20models%2C%20which%20poses%0Aquality%20control%20issues%20highly%20regarded%20in%20this%20area.%20To%20address%20this%2C%20we%0Apropose%20CRADLE-VAE%2C%20a%20causal%20generative%20framework%20tailored%20for%20single-cell%20gene%0Aperturbation%20modeling%2C%20enhanced%20with%20counterfactual%20reasoning-based%20artifact%0Adisentanglement.%20Throughout%20training%2C%20CRADLE-VAE%20models%20the%20underlying%20latent%0Adistribution%20of%20technical%20artifacts%20and%20perturbation%20effects%20present%20in%0Asingle-cell%20datasets.%20It%20employs%20counterfactual%20reasoning%20to%20effectively%0Adisentangle%20such%20artifacts%20by%20modulating%20the%20latent%20basal%20spaces%20and%20learns%0Arobust%20features%20for%20generating%20cellular%20response%20data%20with%20improved%20quality.%0AExperimental%20results%20demonstrate%20that%20this%20approach%20improves%20not%20only%20treatment%0Aeffect%20estimation%20performance%20but%20also%20generative%20quality%20as%20well.%20The%0ACRADLE-VAE%20codebase%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dmis-lab/CRADLE-VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRADLE-VAE%253A%2520Enhancing%2520Single-Cell%2520Gene%2520Perturbation%2520Modeling%2520with%250A%2520%2520Counterfactual%2520Reasoning-based%2520Artifact%2520Disentanglement%26entry.906535625%3DSeungheun%2520Baek%2520and%2520Soyon%2520Park%2520and%2520Yan%2520Ting%2520Chok%2520and%2520Junhyun%2520Lee%2520and%2520Jueon%2520Park%2520and%2520Mogan%2520Gim%2520and%2520Jaewoo%2520Kang%26entry.1292438233%3D%2520%2520Predicting%2520cellular%2520responses%2520to%2520various%2520perturbations%2520is%2520a%2520critical%2520focus%2520in%250Adrug%2520discovery%2520and%2520personalized%2520therapeutics%252C%2520with%2520deep%2520learning%2520models%2520playing%250Aa%2520significant%2520role%2520in%2520this%2520endeavor.%2520Single-cell%2520datasets%2520contain%2520technical%250Aartifacts%2520that%2520may%2520hinder%2520the%2520predictability%2520of%2520such%2520models%252C%2520which%2520poses%250Aquality%2520control%2520issues%2520highly%2520regarded%2520in%2520this%2520area.%2520To%2520address%2520this%252C%2520we%250Apropose%2520CRADLE-VAE%252C%2520a%2520causal%2520generative%2520framework%2520tailored%2520for%2520single-cell%2520gene%250Aperturbation%2520modeling%252C%2520enhanced%2520with%2520counterfactual%2520reasoning-based%2520artifact%250Adisentanglement.%2520Throughout%2520training%252C%2520CRADLE-VAE%2520models%2520the%2520underlying%2520latent%250Adistribution%2520of%2520technical%2520artifacts%2520and%2520perturbation%2520effects%2520present%2520in%250Asingle-cell%2520datasets.%2520It%2520employs%2520counterfactual%2520reasoning%2520to%2520effectively%250Adisentangle%2520such%2520artifacts%2520by%2520modulating%2520the%2520latent%2520basal%2520spaces%2520and%2520learns%250Arobust%2520features%2520for%2520generating%2520cellular%2520response%2520data%2520with%2520improved%2520quality.%250AExperimental%2520results%2520demonstrate%2520that%2520this%2520approach%2520improves%2520not%2520only%2520treatment%250Aeffect%2520estimation%2520performance%2520but%2520also%2520generative%2520quality%2520as%2520well.%2520The%250ACRADLE-VAE%2520codebase%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/dmis-lab/CRADLE-VAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRADLE-VAE%3A%20Enhancing%20Single-Cell%20Gene%20Perturbation%20Modeling%20with%0A%20%20Counterfactual%20Reasoning-based%20Artifact%20Disentanglement&entry.906535625=Seungheun%20Baek%20and%20Soyon%20Park%20and%20Yan%20Ting%20Chok%20and%20Junhyun%20Lee%20and%20Jueon%20Park%20and%20Mogan%20Gim%20and%20Jaewoo%20Kang&entry.1292438233=%20%20Predicting%20cellular%20responses%20to%20various%20perturbations%20is%20a%20critical%20focus%20in%0Adrug%20discovery%20and%20personalized%20therapeutics%2C%20with%20deep%20learning%20models%20playing%0Aa%20significant%20role%20in%20this%20endeavor.%20Single-cell%20datasets%20contain%20technical%0Aartifacts%20that%20may%20hinder%20the%20predictability%20of%20such%20models%2C%20which%20poses%0Aquality%20control%20issues%20highly%20regarded%20in%20this%20area.%20To%20address%20this%2C%20we%0Apropose%20CRADLE-VAE%2C%20a%20causal%20generative%20framework%20tailored%20for%20single-cell%20gene%0Aperturbation%20modeling%2C%20enhanced%20with%20counterfactual%20reasoning-based%20artifact%0Adisentanglement.%20Throughout%20training%2C%20CRADLE-VAE%20models%20the%20underlying%20latent%0Adistribution%20of%20technical%20artifacts%20and%20perturbation%20effects%20present%20in%0Asingle-cell%20datasets.%20It%20employs%20counterfactual%20reasoning%20to%20effectively%0Adisentangle%20such%20artifacts%20by%20modulating%20the%20latent%20basal%20spaces%20and%20learns%0Arobust%20features%20for%20generating%20cellular%20response%20data%20with%20improved%20quality.%0AExperimental%20results%20demonstrate%20that%20this%20approach%20improves%20not%20only%20treatment%0Aeffect%20estimation%20performance%20but%20also%20generative%20quality%20as%20well.%20The%0ACRADLE-VAE%20codebase%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dmis-lab/CRADLE-VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05484v1&entry.124074799=Read"},
{"title": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System", "author": "B. Sankar and Dibakar Sen", "abstract": "  This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process.\n", "link": "http://arxiv.org/abs/2409.05747v1", "date": "2024-09-09", "relevancy": 1.9981, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5278}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5023}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Idea%20Generation%20Tool%20using%20a%20Structured%20Conversational%20AI%20%28CAI%29%0A%20%20System&body=Title%3A%20A%20Novel%20Idea%20Generation%20Tool%20using%20a%20Structured%20Conversational%20AI%20%28CAI%29%0A%20%20System%0AAuthor%3A%20B.%20Sankar%20and%20Dibakar%20Sen%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20conversational%20AI-enabled%20active%20ideation%0Ainterface%20as%20a%20creative%20idea-generation%20tool%20to%20assist%20novice%20designers%20in%0Amitigating%20the%20initial%20latency%20and%20ideation%20bottlenecks%20that%20are%20commonly%0Aobserved.%20It%20is%20a%20dynamic%2C%20interactive%2C%20and%20contextually%20responsive%20approach%2C%0Aactively%20involving%20a%20large%20language%20model%20%28LLM%29%20from%20the%20domain%20of%20natural%0Alanguage%20processing%20%28NLP%29%20in%20artificial%20intelligence%20%28AI%29%20to%20produce%20multiple%0Astatements%20of%20potential%20ideas%20for%20different%20design%20problems.%20Integrating%20such%0AAI%20models%20with%20ideation%20creates%20what%20we%20refer%20to%20as%20an%20Active%20Ideation%0Ascenario%2C%20which%20helps%20foster%20continuous%20dialogue-based%20interaction%2C%0Acontext-sensitive%20conversation%2C%20and%20prolific%20idea%20generation.%20A%20pilot%20study%20was%0Aconducted%20with%20thirty%20novice%20designers%20to%20generate%20ideas%20for%20given%20problems%0Ausing%20traditional%20methods%20and%20the%20new%20CAI-based%20interface.%20The%20key%20parameters%0Aof%20fluency%2C%20novelty%2C%20and%20variety%20were%20used%20to%20compare%20the%20outcomes%0Aqualitatively%20by%20a%20panel%20of%20experts.%20The%20findings%20demonstrated%20the%0Aeffectiveness%20of%20the%20proposed%20tool%20for%20generating%20prolific%2C%20diverse%20and%20novel%0Aideas.%20The%20interface%20was%20enhanced%20by%20incorporating%20a%20prompt-engineered%0Astructured%20dialogue%20style%20for%20each%20ideation%20stage%20to%20make%20it%20uniform%20and%20more%0Aconvenient%20for%20the%20designers.%20The%20resulting%20responses%20of%20such%20a%20structured%20CAI%0Ainterface%20were%20found%20to%20be%20more%20succinct%20and%20aligned%20towards%20the%20subsequent%0Adesign%20stage%2C%20namely%20conceptualization.%20The%20paper%20thus%20established%20the%20rich%0Apotential%20of%20using%20Generative%20AI%20%28Gen-AI%29%20for%20the%20early%20ill-structured%20phase%20of%0Athe%20creative%20product%20design%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Idea%2520Generation%2520Tool%2520using%2520a%2520Structured%2520Conversational%2520AI%2520%2528CAI%2529%250A%2520%2520System%26entry.906535625%3DB.%2520Sankar%2520and%2520Dibakar%2520Sen%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520conversational%2520AI-enabled%2520active%2520ideation%250Ainterface%2520as%2520a%2520creative%2520idea-generation%2520tool%2520to%2520assist%2520novice%2520designers%2520in%250Amitigating%2520the%2520initial%2520latency%2520and%2520ideation%2520bottlenecks%2520that%2520are%2520commonly%250Aobserved.%2520It%2520is%2520a%2520dynamic%252C%2520interactive%252C%2520and%2520contextually%2520responsive%2520approach%252C%250Aactively%2520involving%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520from%2520the%2520domain%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520produce%2520multiple%250Astatements%2520of%2520potential%2520ideas%2520for%2520different%2520design%2520problems.%2520Integrating%2520such%250AAI%2520models%2520with%2520ideation%2520creates%2520what%2520we%2520refer%2520to%2520as%2520an%2520Active%2520Ideation%250Ascenario%252C%2520which%2520helps%2520foster%2520continuous%2520dialogue-based%2520interaction%252C%250Acontext-sensitive%2520conversation%252C%2520and%2520prolific%2520idea%2520generation.%2520A%2520pilot%2520study%2520was%250Aconducted%2520with%2520thirty%2520novice%2520designers%2520to%2520generate%2520ideas%2520for%2520given%2520problems%250Ausing%2520traditional%2520methods%2520and%2520the%2520new%2520CAI-based%2520interface.%2520The%2520key%2520parameters%250Aof%2520fluency%252C%2520novelty%252C%2520and%2520variety%2520were%2520used%2520to%2520compare%2520the%2520outcomes%250Aqualitatively%2520by%2520a%2520panel%2520of%2520experts.%2520The%2520findings%2520demonstrated%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520tool%2520for%2520generating%2520prolific%252C%2520diverse%2520and%2520novel%250Aideas.%2520The%2520interface%2520was%2520enhanced%2520by%2520incorporating%2520a%2520prompt-engineered%250Astructured%2520dialogue%2520style%2520for%2520each%2520ideation%2520stage%2520to%2520make%2520it%2520uniform%2520and%2520more%250Aconvenient%2520for%2520the%2520designers.%2520The%2520resulting%2520responses%2520of%2520such%2520a%2520structured%2520CAI%250Ainterface%2520were%2520found%2520to%2520be%2520more%2520succinct%2520and%2520aligned%2520towards%2520the%2520subsequent%250Adesign%2520stage%252C%2520namely%2520conceptualization.%2520The%2520paper%2520thus%2520established%2520the%2520rich%250Apotential%2520of%2520using%2520Generative%2520AI%2520%2528Gen-AI%2529%2520for%2520the%2520early%2520ill-structured%2520phase%2520of%250Athe%2520creative%2520product%2520design%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Idea%20Generation%20Tool%20using%20a%20Structured%20Conversational%20AI%20%28CAI%29%0A%20%20System&entry.906535625=B.%20Sankar%20and%20Dibakar%20Sen&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20conversational%20AI-enabled%20active%20ideation%0Ainterface%20as%20a%20creative%20idea-generation%20tool%20to%20assist%20novice%20designers%20in%0Amitigating%20the%20initial%20latency%20and%20ideation%20bottlenecks%20that%20are%20commonly%0Aobserved.%20It%20is%20a%20dynamic%2C%20interactive%2C%20and%20contextually%20responsive%20approach%2C%0Aactively%20involving%20a%20large%20language%20model%20%28LLM%29%20from%20the%20domain%20of%20natural%0Alanguage%20processing%20%28NLP%29%20in%20artificial%20intelligence%20%28AI%29%20to%20produce%20multiple%0Astatements%20of%20potential%20ideas%20for%20different%20design%20problems.%20Integrating%20such%0AAI%20models%20with%20ideation%20creates%20what%20we%20refer%20to%20as%20an%20Active%20Ideation%0Ascenario%2C%20which%20helps%20foster%20continuous%20dialogue-based%20interaction%2C%0Acontext-sensitive%20conversation%2C%20and%20prolific%20idea%20generation.%20A%20pilot%20study%20was%0Aconducted%20with%20thirty%20novice%20designers%20to%20generate%20ideas%20for%20given%20problems%0Ausing%20traditional%20methods%20and%20the%20new%20CAI-based%20interface.%20The%20key%20parameters%0Aof%20fluency%2C%20novelty%2C%20and%20variety%20were%20used%20to%20compare%20the%20outcomes%0Aqualitatively%20by%20a%20panel%20of%20experts.%20The%20findings%20demonstrated%20the%0Aeffectiveness%20of%20the%20proposed%20tool%20for%20generating%20prolific%2C%20diverse%20and%20novel%0Aideas.%20The%20interface%20was%20enhanced%20by%20incorporating%20a%20prompt-engineered%0Astructured%20dialogue%20style%20for%20each%20ideation%20stage%20to%20make%20it%20uniform%20and%20more%0Aconvenient%20for%20the%20designers.%20The%20resulting%20responses%20of%20such%20a%20structured%20CAI%0Ainterface%20were%20found%20to%20be%20more%20succinct%20and%20aligned%20towards%20the%20subsequent%0Adesign%20stage%2C%20namely%20conceptualization.%20The%20paper%20thus%20established%20the%20rich%0Apotential%20of%20using%20Generative%20AI%20%28Gen-AI%29%20for%20the%20early%20ill-structured%20phase%20of%0Athe%20creative%20product%20design%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05747v1&entry.124074799=Read"},
{"title": "SynMorph: Generating Synthetic Face Morphing Dataset with Mated Samples", "author": "Haoyu Zhang and Raghavendra Ramachandra and Kiran Raja and Christoph Busch", "abstract": "  Face morphing attack detection (MAD) algorithms have become essential to\novercome the vulnerability of face recognition systems. To solve the lack of\nlarge-scale and public-available datasets due to privacy concerns and\nrestrictions, in this work we propose a new method to generate a synthetic face\nmorphing dataset with 2450 identities and more than 100k morphs. The proposed\nsynthetic face morphing dataset is unique for its high-quality samples,\ndifferent types of morphing algorithms, and the generalization for both single\nand differential morphing attack detection algorithms. For experiments, we\napply face image quality assessment and vulnerability analysis to evaluate the\nproposed synthetic face morphing dataset from the perspective of biometric\nsample quality and morphing attack potential on face recognition systems. The\nresults are benchmarked with an existing SOTA synthetic dataset and a\nrepresentative non-synthetic and indicate improvement compared with the SOTA.\nAdditionally, we design different protocols and study the applicability of\nusing the proposed synthetic dataset on training morphing attack detection\nalgorithms.\n", "link": "http://arxiv.org/abs/2409.05595v1", "date": "2024-09-09", "relevancy": 1.9913, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5139}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4997}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynMorph%3A%20Generating%20Synthetic%20Face%20Morphing%20Dataset%20with%20Mated%20Samples&body=Title%3A%20SynMorph%3A%20Generating%20Synthetic%20Face%20Morphing%20Dataset%20with%20Mated%20Samples%0AAuthor%3A%20Haoyu%20Zhang%20and%20Raghavendra%20Ramachandra%20and%20Kiran%20Raja%20and%20Christoph%20Busch%0AAbstract%3A%20%20%20Face%20morphing%20attack%20detection%20%28MAD%29%20algorithms%20have%20become%20essential%20to%0Aovercome%20the%20vulnerability%20of%20face%20recognition%20systems.%20To%20solve%20the%20lack%20of%0Alarge-scale%20and%20public-available%20datasets%20due%20to%20privacy%20concerns%20and%0Arestrictions%2C%20in%20this%20work%20we%20propose%20a%20new%20method%20to%20generate%20a%20synthetic%20face%0Amorphing%20dataset%20with%202450%20identities%20and%20more%20than%20100k%20morphs.%20The%20proposed%0Asynthetic%20face%20morphing%20dataset%20is%20unique%20for%20its%20high-quality%20samples%2C%0Adifferent%20types%20of%20morphing%20algorithms%2C%20and%20the%20generalization%20for%20both%20single%0Aand%20differential%20morphing%20attack%20detection%20algorithms.%20For%20experiments%2C%20we%0Aapply%20face%20image%20quality%20assessment%20and%20vulnerability%20analysis%20to%20evaluate%20the%0Aproposed%20synthetic%20face%20morphing%20dataset%20from%20the%20perspective%20of%20biometric%0Asample%20quality%20and%20morphing%20attack%20potential%20on%20face%20recognition%20systems.%20The%0Aresults%20are%20benchmarked%20with%20an%20existing%20SOTA%20synthetic%20dataset%20and%20a%0Arepresentative%20non-synthetic%20and%20indicate%20improvement%20compared%20with%20the%20SOTA.%0AAdditionally%2C%20we%20design%20different%20protocols%20and%20study%20the%20applicability%20of%0Ausing%20the%20proposed%20synthetic%20dataset%20on%20training%20morphing%20attack%20detection%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynMorph%253A%2520Generating%2520Synthetic%2520Face%2520Morphing%2520Dataset%2520with%2520Mated%2520Samples%26entry.906535625%3DHaoyu%2520Zhang%2520and%2520Raghavendra%2520Ramachandra%2520and%2520Kiran%2520Raja%2520and%2520Christoph%2520Busch%26entry.1292438233%3D%2520%2520Face%2520morphing%2520attack%2520detection%2520%2528MAD%2529%2520algorithms%2520have%2520become%2520essential%2520to%250Aovercome%2520the%2520vulnerability%2520of%2520face%2520recognition%2520systems.%2520To%2520solve%2520the%2520lack%2520of%250Alarge-scale%2520and%2520public-available%2520datasets%2520due%2520to%2520privacy%2520concerns%2520and%250Arestrictions%252C%2520in%2520this%2520work%2520we%2520propose%2520a%2520new%2520method%2520to%2520generate%2520a%2520synthetic%2520face%250Amorphing%2520dataset%2520with%25202450%2520identities%2520and%2520more%2520than%2520100k%2520morphs.%2520The%2520proposed%250Asynthetic%2520face%2520morphing%2520dataset%2520is%2520unique%2520for%2520its%2520high-quality%2520samples%252C%250Adifferent%2520types%2520of%2520morphing%2520algorithms%252C%2520and%2520the%2520generalization%2520for%2520both%2520single%250Aand%2520differential%2520morphing%2520attack%2520detection%2520algorithms.%2520For%2520experiments%252C%2520we%250Aapply%2520face%2520image%2520quality%2520assessment%2520and%2520vulnerability%2520analysis%2520to%2520evaluate%2520the%250Aproposed%2520synthetic%2520face%2520morphing%2520dataset%2520from%2520the%2520perspective%2520of%2520biometric%250Asample%2520quality%2520and%2520morphing%2520attack%2520potential%2520on%2520face%2520recognition%2520systems.%2520The%250Aresults%2520are%2520benchmarked%2520with%2520an%2520existing%2520SOTA%2520synthetic%2520dataset%2520and%2520a%250Arepresentative%2520non-synthetic%2520and%2520indicate%2520improvement%2520compared%2520with%2520the%2520SOTA.%250AAdditionally%252C%2520we%2520design%2520different%2520protocols%2520and%2520study%2520the%2520applicability%2520of%250Ausing%2520the%2520proposed%2520synthetic%2520dataset%2520on%2520training%2520morphing%2520attack%2520detection%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynMorph%3A%20Generating%20Synthetic%20Face%20Morphing%20Dataset%20with%20Mated%20Samples&entry.906535625=Haoyu%20Zhang%20and%20Raghavendra%20Ramachandra%20and%20Kiran%20Raja%20and%20Christoph%20Busch&entry.1292438233=%20%20Face%20morphing%20attack%20detection%20%28MAD%29%20algorithms%20have%20become%20essential%20to%0Aovercome%20the%20vulnerability%20of%20face%20recognition%20systems.%20To%20solve%20the%20lack%20of%0Alarge-scale%20and%20public-available%20datasets%20due%20to%20privacy%20concerns%20and%0Arestrictions%2C%20in%20this%20work%20we%20propose%20a%20new%20method%20to%20generate%20a%20synthetic%20face%0Amorphing%20dataset%20with%202450%20identities%20and%20more%20than%20100k%20morphs.%20The%20proposed%0Asynthetic%20face%20morphing%20dataset%20is%20unique%20for%20its%20high-quality%20samples%2C%0Adifferent%20types%20of%20morphing%20algorithms%2C%20and%20the%20generalization%20for%20both%20single%0Aand%20differential%20morphing%20attack%20detection%20algorithms.%20For%20experiments%2C%20we%0Aapply%20face%20image%20quality%20assessment%20and%20vulnerability%20analysis%20to%20evaluate%20the%0Aproposed%20synthetic%20face%20morphing%20dataset%20from%20the%20perspective%20of%20biometric%0Asample%20quality%20and%20morphing%20attack%20potential%20on%20face%20recognition%20systems.%20The%0Aresults%20are%20benchmarked%20with%20an%20existing%20SOTA%20synthetic%20dataset%20and%20a%0Arepresentative%20non-synthetic%20and%20indicate%20improvement%20compared%20with%20the%20SOTA.%0AAdditionally%2C%20we%20design%20different%20protocols%20and%20study%20the%20applicability%20of%0Ausing%20the%20proposed%20synthetic%20dataset%20on%20training%20morphing%20attack%20detection%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05595v1&entry.124074799=Read"},
{"title": "VisFly: An Efficient and Versatile Simulator for Training Vision-based\n  Flight", "author": "Fanxing Li and Fangyu Sun and Tianbao Zhang and Danping Zou", "abstract": "  We present VisFly, a quadrotor simulator designed to efficiently train\nvision-based flight policies using reinforcement learning algorithms. VisFly\noffers a user-friendly framework and interfaces, leveraging Habitat-Sim's\nrendering engines to achieve frame rates exceeding 10,000 frames per second for\nrendering motion and sensor data. The simulator incorporates differentiable\nphysics and is seamlessly wrapped with the Gym environment, facilitating the\nstraightforward implementation of various learning algorithms. It supports the\ndirectly importing open-source scene datasets compatible with Habitat-Sim,\nenabling training on diverse real-world environments simultaneously. To\nvalidate our simulator, we also make three reinforcement learning examples for\ntypical flight tasks relying on visual observations. The simulator is now\navailable at [https://github.com/SJTU-ViSYS-team/VisFly].\n", "link": "http://arxiv.org/abs/2407.14783v4", "date": "2024-09-09", "relevancy": 1.9836, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4959}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisFly%3A%20An%20Efficient%20and%20Versatile%20Simulator%20for%20Training%20Vision-based%0A%20%20Flight&body=Title%3A%20VisFly%3A%20An%20Efficient%20and%20Versatile%20Simulator%20for%20Training%20Vision-based%0A%20%20Flight%0AAuthor%3A%20Fanxing%20Li%20and%20Fangyu%20Sun%20and%20Tianbao%20Zhang%20and%20Danping%20Zou%0AAbstract%3A%20%20%20We%20present%20VisFly%2C%20a%20quadrotor%20simulator%20designed%20to%20efficiently%20train%0Avision-based%20flight%20policies%20using%20reinforcement%20learning%20algorithms.%20VisFly%0Aoffers%20a%20user-friendly%20framework%20and%20interfaces%2C%20leveraging%20Habitat-Sim%27s%0Arendering%20engines%20to%20achieve%20frame%20rates%20exceeding%2010%2C000%20frames%20per%20second%20for%0Arendering%20motion%20and%20sensor%20data.%20The%20simulator%20incorporates%20differentiable%0Aphysics%20and%20is%20seamlessly%20wrapped%20with%20the%20Gym%20environment%2C%20facilitating%20the%0Astraightforward%20implementation%20of%20various%20learning%20algorithms.%20It%20supports%20the%0Adirectly%20importing%20open-source%20scene%20datasets%20compatible%20with%20Habitat-Sim%2C%0Aenabling%20training%20on%20diverse%20real-world%20environments%20simultaneously.%20To%0Avalidate%20our%20simulator%2C%20we%20also%20make%20three%20reinforcement%20learning%20examples%20for%0Atypical%20flight%20tasks%20relying%20on%20visual%20observations.%20The%20simulator%20is%20now%0Aavailable%20at%20%5Bhttps%3A//github.com/SJTU-ViSYS-team/VisFly%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14783v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisFly%253A%2520An%2520Efficient%2520and%2520Versatile%2520Simulator%2520for%2520Training%2520Vision-based%250A%2520%2520Flight%26entry.906535625%3DFanxing%2520Li%2520and%2520Fangyu%2520Sun%2520and%2520Tianbao%2520Zhang%2520and%2520Danping%2520Zou%26entry.1292438233%3D%2520%2520We%2520present%2520VisFly%252C%2520a%2520quadrotor%2520simulator%2520designed%2520to%2520efficiently%2520train%250Avision-based%2520flight%2520policies%2520using%2520reinforcement%2520learning%2520algorithms.%2520VisFly%250Aoffers%2520a%2520user-friendly%2520framework%2520and%2520interfaces%252C%2520leveraging%2520Habitat-Sim%2527s%250Arendering%2520engines%2520to%2520achieve%2520frame%2520rates%2520exceeding%252010%252C000%2520frames%2520per%2520second%2520for%250Arendering%2520motion%2520and%2520sensor%2520data.%2520The%2520simulator%2520incorporates%2520differentiable%250Aphysics%2520and%2520is%2520seamlessly%2520wrapped%2520with%2520the%2520Gym%2520environment%252C%2520facilitating%2520the%250Astraightforward%2520implementation%2520of%2520various%2520learning%2520algorithms.%2520It%2520supports%2520the%250Adirectly%2520importing%2520open-source%2520scene%2520datasets%2520compatible%2520with%2520Habitat-Sim%252C%250Aenabling%2520training%2520on%2520diverse%2520real-world%2520environments%2520simultaneously.%2520To%250Avalidate%2520our%2520simulator%252C%2520we%2520also%2520make%2520three%2520reinforcement%2520learning%2520examples%2520for%250Atypical%2520flight%2520tasks%2520relying%2520on%2520visual%2520observations.%2520The%2520simulator%2520is%2520now%250Aavailable%2520at%2520%255Bhttps%253A//github.com/SJTU-ViSYS-team/VisFly%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14783v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisFly%3A%20An%20Efficient%20and%20Versatile%20Simulator%20for%20Training%20Vision-based%0A%20%20Flight&entry.906535625=Fanxing%20Li%20and%20Fangyu%20Sun%20and%20Tianbao%20Zhang%20and%20Danping%20Zou&entry.1292438233=%20%20We%20present%20VisFly%2C%20a%20quadrotor%20simulator%20designed%20to%20efficiently%20train%0Avision-based%20flight%20policies%20using%20reinforcement%20learning%20algorithms.%20VisFly%0Aoffers%20a%20user-friendly%20framework%20and%20interfaces%2C%20leveraging%20Habitat-Sim%27s%0Arendering%20engines%20to%20achieve%20frame%20rates%20exceeding%2010%2C000%20frames%20per%20second%20for%0Arendering%20motion%20and%20sensor%20data.%20The%20simulator%20incorporates%20differentiable%0Aphysics%20and%20is%20seamlessly%20wrapped%20with%20the%20Gym%20environment%2C%20facilitating%20the%0Astraightforward%20implementation%20of%20various%20learning%20algorithms.%20It%20supports%20the%0Adirectly%20importing%20open-source%20scene%20datasets%20compatible%20with%20Habitat-Sim%2C%0Aenabling%20training%20on%20diverse%20real-world%20environments%20simultaneously.%20To%0Avalidate%20our%20simulator%2C%20we%20also%20make%20three%20reinforcement%20learning%20examples%20for%0Atypical%20flight%20tasks%20relying%20on%20visual%20observations.%20The%20simulator%20is%20now%0Aavailable%20at%20%5Bhttps%3A//github.com/SJTU-ViSYS-team/VisFly%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14783v4&entry.124074799=Read"},
{"title": "Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in\n  Lifted Compiled Code", "author": "Gary A. McCully and John D. Hastings and Shengjie Xu and Adam Fortier", "abstract": "  Detecting vulnerabilities within compiled binaries is challenging due to lost\nhigh-level code structures and other factors such as architectural\ndependencies, compilers, and optimization options. To address these obstacles,\nthis research explores vulnerability detection using natural language\nprocessing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn\nsemantics from intermediate representation (LLVM IR) code. Long short-term\nmemory (LSTM) neural networks were trained on embeddings from encoders created\nusing approximately 48k LLVM functions from the Juliet dataset. This study is\npioneering in its comparison of word2vec models with multiple bidirectional\ntransformer (BERT, RoBERTa) embeddings built using LLVM code to train neural\nnetworks to detect vulnerabilities in compiled binaries. word2vec Skip-Gram\nmodels achieved 92% validation accuracy in detecting vulnerabilities,\noutperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This\nsuggests that complex contextual embeddings may not provide advantages over\nsimpler word2vec models for this task when a limited number (e.g. 48K) of data\nsamples are used to train the bidirectional transformer-based models. The\ncomparative results provide novel insights into selecting optimal embeddings\nfor learning compiler-independent semantic code representations to advance\nmachine learning detection of vulnerabilities in compiled binaries.\n", "link": "http://arxiv.org/abs/2405.20611v2", "date": "2024-09-09", "relevancy": 1.9758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-Directional%20Transformers%20vs.%20word2vec%3A%20Discovering%20Vulnerabilities%20in%0A%20%20Lifted%20Compiled%20Code&body=Title%3A%20Bi-Directional%20Transformers%20vs.%20word2vec%3A%20Discovering%20Vulnerabilities%20in%0A%20%20Lifted%20Compiled%20Code%0AAuthor%3A%20Gary%20A.%20McCully%20and%20John%20D.%20Hastings%20and%20Shengjie%20Xu%20and%20Adam%20Fortier%0AAbstract%3A%20%20%20Detecting%20vulnerabilities%20within%20compiled%20binaries%20is%20challenging%20due%20to%20lost%0Ahigh-level%20code%20structures%20and%20other%20factors%20such%20as%20architectural%0Adependencies%2C%20compilers%2C%20and%20optimization%20options.%20To%20address%20these%20obstacles%2C%0Athis%20research%20explores%20vulnerability%20detection%20using%20natural%20language%0Aprocessing%20%28NLP%29%20embedding%20techniques%20with%20word2vec%2C%20BERT%2C%20and%20RoBERTa%20to%20learn%0Asemantics%20from%20intermediate%20representation%20%28LLVM%20IR%29%20code.%20Long%20short-term%0Amemory%20%28LSTM%29%20neural%20networks%20were%20trained%20on%20embeddings%20from%20encoders%20created%0Ausing%20approximately%2048k%20LLVM%20functions%20from%20the%20Juliet%20dataset.%20This%20study%20is%0Apioneering%20in%20its%20comparison%20of%20word2vec%20models%20with%20multiple%20bidirectional%0Atransformer%20%28BERT%2C%20RoBERTa%29%20embeddings%20built%20using%20LLVM%20code%20to%20train%20neural%0Anetworks%20to%20detect%20vulnerabilities%20in%20compiled%20binaries.%20word2vec%20Skip-Gram%0Amodels%20achieved%2092%25%20validation%20accuracy%20in%20detecting%20vulnerabilities%2C%0Aoutperforming%20word2vec%20Continuous%20Bag%20of%20Words%20%28CBOW%29%2C%20BERT%2C%20and%20RoBERTa.%20This%0Asuggests%20that%20complex%20contextual%20embeddings%20may%20not%20provide%20advantages%20over%0Asimpler%20word2vec%20models%20for%20this%20task%20when%20a%20limited%20number%20%28e.g.%2048K%29%20of%20data%0Asamples%20are%20used%20to%20train%20the%20bidirectional%20transformer-based%20models.%20The%0Acomparative%20results%20provide%20novel%20insights%20into%20selecting%20optimal%20embeddings%0Afor%20learning%20compiler-independent%20semantic%20code%20representations%20to%20advance%0Amachine%20learning%20detection%20of%20vulnerabilities%20in%20compiled%20binaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-Directional%2520Transformers%2520vs.%2520word2vec%253A%2520Discovering%2520Vulnerabilities%2520in%250A%2520%2520Lifted%2520Compiled%2520Code%26entry.906535625%3DGary%2520A.%2520McCully%2520and%2520John%2520D.%2520Hastings%2520and%2520Shengjie%2520Xu%2520and%2520Adam%2520Fortier%26entry.1292438233%3D%2520%2520Detecting%2520vulnerabilities%2520within%2520compiled%2520binaries%2520is%2520challenging%2520due%2520to%2520lost%250Ahigh-level%2520code%2520structures%2520and%2520other%2520factors%2520such%2520as%2520architectural%250Adependencies%252C%2520compilers%252C%2520and%2520optimization%2520options.%2520To%2520address%2520these%2520obstacles%252C%250Athis%2520research%2520explores%2520vulnerability%2520detection%2520using%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520embedding%2520techniques%2520with%2520word2vec%252C%2520BERT%252C%2520and%2520RoBERTa%2520to%2520learn%250Asemantics%2520from%2520intermediate%2520representation%2520%2528LLVM%2520IR%2529%2520code.%2520Long%2520short-term%250Amemory%2520%2528LSTM%2529%2520neural%2520networks%2520were%2520trained%2520on%2520embeddings%2520from%2520encoders%2520created%250Ausing%2520approximately%252048k%2520LLVM%2520functions%2520from%2520the%2520Juliet%2520dataset.%2520This%2520study%2520is%250Apioneering%2520in%2520its%2520comparison%2520of%2520word2vec%2520models%2520with%2520multiple%2520bidirectional%250Atransformer%2520%2528BERT%252C%2520RoBERTa%2529%2520embeddings%2520built%2520using%2520LLVM%2520code%2520to%2520train%2520neural%250Anetworks%2520to%2520detect%2520vulnerabilities%2520in%2520compiled%2520binaries.%2520word2vec%2520Skip-Gram%250Amodels%2520achieved%252092%2525%2520validation%2520accuracy%2520in%2520detecting%2520vulnerabilities%252C%250Aoutperforming%2520word2vec%2520Continuous%2520Bag%2520of%2520Words%2520%2528CBOW%2529%252C%2520BERT%252C%2520and%2520RoBERTa.%2520This%250Asuggests%2520that%2520complex%2520contextual%2520embeddings%2520may%2520not%2520provide%2520advantages%2520over%250Asimpler%2520word2vec%2520models%2520for%2520this%2520task%2520when%2520a%2520limited%2520number%2520%2528e.g.%252048K%2529%2520of%2520data%250Asamples%2520are%2520used%2520to%2520train%2520the%2520bidirectional%2520transformer-based%2520models.%2520The%250Acomparative%2520results%2520provide%2520novel%2520insights%2520into%2520selecting%2520optimal%2520embeddings%250Afor%2520learning%2520compiler-independent%2520semantic%2520code%2520representations%2520to%2520advance%250Amachine%2520learning%2520detection%2520of%2520vulnerabilities%2520in%2520compiled%2520binaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-Directional%20Transformers%20vs.%20word2vec%3A%20Discovering%20Vulnerabilities%20in%0A%20%20Lifted%20Compiled%20Code&entry.906535625=Gary%20A.%20McCully%20and%20John%20D.%20Hastings%20and%20Shengjie%20Xu%20and%20Adam%20Fortier&entry.1292438233=%20%20Detecting%20vulnerabilities%20within%20compiled%20binaries%20is%20challenging%20due%20to%20lost%0Ahigh-level%20code%20structures%20and%20other%20factors%20such%20as%20architectural%0Adependencies%2C%20compilers%2C%20and%20optimization%20options.%20To%20address%20these%20obstacles%2C%0Athis%20research%20explores%20vulnerability%20detection%20using%20natural%20language%0Aprocessing%20%28NLP%29%20embedding%20techniques%20with%20word2vec%2C%20BERT%2C%20and%20RoBERTa%20to%20learn%0Asemantics%20from%20intermediate%20representation%20%28LLVM%20IR%29%20code.%20Long%20short-term%0Amemory%20%28LSTM%29%20neural%20networks%20were%20trained%20on%20embeddings%20from%20encoders%20created%0Ausing%20approximately%2048k%20LLVM%20functions%20from%20the%20Juliet%20dataset.%20This%20study%20is%0Apioneering%20in%20its%20comparison%20of%20word2vec%20models%20with%20multiple%20bidirectional%0Atransformer%20%28BERT%2C%20RoBERTa%29%20embeddings%20built%20using%20LLVM%20code%20to%20train%20neural%0Anetworks%20to%20detect%20vulnerabilities%20in%20compiled%20binaries.%20word2vec%20Skip-Gram%0Amodels%20achieved%2092%25%20validation%20accuracy%20in%20detecting%20vulnerabilities%2C%0Aoutperforming%20word2vec%20Continuous%20Bag%20of%20Words%20%28CBOW%29%2C%20BERT%2C%20and%20RoBERTa.%20This%0Asuggests%20that%20complex%20contextual%20embeddings%20may%20not%20provide%20advantages%20over%0Asimpler%20word2vec%20models%20for%20this%20task%20when%20a%20limited%20number%20%28e.g.%2048K%29%20of%20data%0Asamples%20are%20used%20to%20train%20the%20bidirectional%20transformer-based%20models.%20The%0Acomparative%20results%20provide%20novel%20insights%20into%20selecting%20optimal%20embeddings%0Afor%20learning%20compiler-independent%20semantic%20code%20representations%20to%20advance%0Amachine%20learning%20detection%20of%20vulnerabilities%20in%20compiled%20binaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20611v2&entry.124074799=Read"},
{"title": "When resampling/reweighting improves feature learning in imbalanced\n  classification?: A toy-model study", "author": "Tomoyuki Obuchi and Toshiyuki Tanaka", "abstract": "  A toy model of binary classification is studied with the aim of clarifying\nthe class-wise resampling/reweighting effect on the feature learning\nperformance under the presence of class imbalance. In the analysis, a\nhigh-dimensional limit of the feature is taken while keeping the dataset size\nratio against the feature dimension finite and the non-rigorous replica method\nfrom statistical mechanics is employed. The result shows that there exists a\ncase in which the no resampling/reweighting situation gives the best feature\nlearning performance irrespectively of the choice of losses or classifiers,\nsupporting recent findings in Cao et al. (2019); Kang et al. (2019). It is also\nrevealed that the key of the result is the symmetry of the loss and the problem\nsetting. Inspired by this, we propose a further simplified model exhibiting the\nsame property for the multiclass setting. These clarify when the class-wise\nresampling/reweighting becomes effective in imbalanced classification.\n", "link": "http://arxiv.org/abs/2409.05598v1", "date": "2024-09-09", "relevancy": 1.9758, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5418}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4617}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20resampling/reweighting%20improves%20feature%20learning%20in%20imbalanced%0A%20%20classification%3F%3A%20A%20toy-model%20study&body=Title%3A%20When%20resampling/reweighting%20improves%20feature%20learning%20in%20imbalanced%0A%20%20classification%3F%3A%20A%20toy-model%20study%0AAuthor%3A%20Tomoyuki%20Obuchi%20and%20Toshiyuki%20Tanaka%0AAbstract%3A%20%20%20A%20toy%20model%20of%20binary%20classification%20is%20studied%20with%20the%20aim%20of%20clarifying%0Athe%20class-wise%20resampling/reweighting%20effect%20on%20the%20feature%20learning%0Aperformance%20under%20the%20presence%20of%20class%20imbalance.%20In%20the%20analysis%2C%20a%0Ahigh-dimensional%20limit%20of%20the%20feature%20is%20taken%20while%20keeping%20the%20dataset%20size%0Aratio%20against%20the%20feature%20dimension%20finite%20and%20the%20non-rigorous%20replica%20method%0Afrom%20statistical%20mechanics%20is%20employed.%20The%20result%20shows%20that%20there%20exists%20a%0Acase%20in%20which%20the%20no%20resampling/reweighting%20situation%20gives%20the%20best%20feature%0Alearning%20performance%20irrespectively%20of%20the%20choice%20of%20losses%20or%20classifiers%2C%0Asupporting%20recent%20findings%20in%20Cao%20et%20al.%20%282019%29%3B%20Kang%20et%20al.%20%282019%29.%20It%20is%20also%0Arevealed%20that%20the%20key%20of%20the%20result%20is%20the%20symmetry%20of%20the%20loss%20and%20the%20problem%0Asetting.%20Inspired%20by%20this%2C%20we%20propose%20a%20further%20simplified%20model%20exhibiting%20the%0Asame%20property%20for%20the%20multiclass%20setting.%20These%20clarify%20when%20the%20class-wise%0Aresampling/reweighting%20becomes%20effective%20in%20imbalanced%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520resampling/reweighting%2520improves%2520feature%2520learning%2520in%2520imbalanced%250A%2520%2520classification%253F%253A%2520A%2520toy-model%2520study%26entry.906535625%3DTomoyuki%2520Obuchi%2520and%2520Toshiyuki%2520Tanaka%26entry.1292438233%3D%2520%2520A%2520toy%2520model%2520of%2520binary%2520classification%2520is%2520studied%2520with%2520the%2520aim%2520of%2520clarifying%250Athe%2520class-wise%2520resampling/reweighting%2520effect%2520on%2520the%2520feature%2520learning%250Aperformance%2520under%2520the%2520presence%2520of%2520class%2520imbalance.%2520In%2520the%2520analysis%252C%2520a%250Ahigh-dimensional%2520limit%2520of%2520the%2520feature%2520is%2520taken%2520while%2520keeping%2520the%2520dataset%2520size%250Aratio%2520against%2520the%2520feature%2520dimension%2520finite%2520and%2520the%2520non-rigorous%2520replica%2520method%250Afrom%2520statistical%2520mechanics%2520is%2520employed.%2520The%2520result%2520shows%2520that%2520there%2520exists%2520a%250Acase%2520in%2520which%2520the%2520no%2520resampling/reweighting%2520situation%2520gives%2520the%2520best%2520feature%250Alearning%2520performance%2520irrespectively%2520of%2520the%2520choice%2520of%2520losses%2520or%2520classifiers%252C%250Asupporting%2520recent%2520findings%2520in%2520Cao%2520et%2520al.%2520%25282019%2529%253B%2520Kang%2520et%2520al.%2520%25282019%2529.%2520It%2520is%2520also%250Arevealed%2520that%2520the%2520key%2520of%2520the%2520result%2520is%2520the%2520symmetry%2520of%2520the%2520loss%2520and%2520the%2520problem%250Asetting.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520further%2520simplified%2520model%2520exhibiting%2520the%250Asame%2520property%2520for%2520the%2520multiclass%2520setting.%2520These%2520clarify%2520when%2520the%2520class-wise%250Aresampling/reweighting%2520becomes%2520effective%2520in%2520imbalanced%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20resampling/reweighting%20improves%20feature%20learning%20in%20imbalanced%0A%20%20classification%3F%3A%20A%20toy-model%20study&entry.906535625=Tomoyuki%20Obuchi%20and%20Toshiyuki%20Tanaka&entry.1292438233=%20%20A%20toy%20model%20of%20binary%20classification%20is%20studied%20with%20the%20aim%20of%20clarifying%0Athe%20class-wise%20resampling/reweighting%20effect%20on%20the%20feature%20learning%0Aperformance%20under%20the%20presence%20of%20class%20imbalance.%20In%20the%20analysis%2C%20a%0Ahigh-dimensional%20limit%20of%20the%20feature%20is%20taken%20while%20keeping%20the%20dataset%20size%0Aratio%20against%20the%20feature%20dimension%20finite%20and%20the%20non-rigorous%20replica%20method%0Afrom%20statistical%20mechanics%20is%20employed.%20The%20result%20shows%20that%20there%20exists%20a%0Acase%20in%20which%20the%20no%20resampling/reweighting%20situation%20gives%20the%20best%20feature%0Alearning%20performance%20irrespectively%20of%20the%20choice%20of%20losses%20or%20classifiers%2C%0Asupporting%20recent%20findings%20in%20Cao%20et%20al.%20%282019%29%3B%20Kang%20et%20al.%20%282019%29.%20It%20is%20also%0Arevealed%20that%20the%20key%20of%20the%20result%20is%20the%20symmetry%20of%20the%20loss%20and%20the%20problem%0Asetting.%20Inspired%20by%20this%2C%20we%20propose%20a%20further%20simplified%20model%20exhibiting%20the%0Asame%20property%20for%20the%20multiclass%20setting.%20These%20clarify%20when%20the%20class-wise%0Aresampling/reweighting%20becomes%20effective%20in%20imbalanced%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05598v1&entry.124074799=Read"},
{"title": "3D-SAR Tomography and Machine Learning for High-Resolution Tree Height\n  Estimation", "author": "Grace Colverd and Jumpei Takami and Laura Schade and Karol Bot and Joseph A. Gallego-Mejia", "abstract": "  Accurately estimating forest biomass is crucial for global carbon cycle\nmodelling and climate change mitigation. Tree height, a key factor in biomass\ncalculations, can be measured using Synthetic Aperture Radar (SAR) technology.\nThis study applies machine learning to extract forest height data from two SAR\nproducts: Single Look Complex (SLC) images and tomographic cubes, in\npreparation for the ESA Biomass Satellite mission. We use the TomoSense\ndataset, containing SAR and LiDAR data from Germany's Eifel National Park, to\ndevelop and evaluate height estimation models. Our approach includes classical\nmethods, deep learning with a 3D U-Net, and Bayesian-optimized techniques. By\ntesting various SAR frequencies and polarimetries, we establish a baseline for\nfuture height and biomass modelling. Best-performing models predict forest\nheight to be within 2.82m mean absolute error for canopies around 30m,\nadvancing our ability to measure global carbon stocks and support climate\naction.\n", "link": "http://arxiv.org/abs/2409.05636v1", "date": "2024-09-09", "relevancy": 1.9723, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5046}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4981}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-SAR%20Tomography%20and%20Machine%20Learning%20for%20High-Resolution%20Tree%20Height%0A%20%20Estimation&body=Title%3A%203D-SAR%20Tomography%20and%20Machine%20Learning%20for%20High-Resolution%20Tree%20Height%0A%20%20Estimation%0AAuthor%3A%20Grace%20Colverd%20and%20Jumpei%20Takami%20and%20Laura%20Schade%20and%20Karol%20Bot%20and%20Joseph%20A.%20Gallego-Mejia%0AAbstract%3A%20%20%20Accurately%20estimating%20forest%20biomass%20is%20crucial%20for%20global%20carbon%20cycle%0Amodelling%20and%20climate%20change%20mitigation.%20Tree%20height%2C%20a%20key%20factor%20in%20biomass%0Acalculations%2C%20can%20be%20measured%20using%20Synthetic%20Aperture%20Radar%20%28SAR%29%20technology.%0AThis%20study%20applies%20machine%20learning%20to%20extract%20forest%20height%20data%20from%20two%20SAR%0Aproducts%3A%20Single%20Look%20Complex%20%28SLC%29%20images%20and%20tomographic%20cubes%2C%20in%0Apreparation%20for%20the%20ESA%20Biomass%20Satellite%20mission.%20We%20use%20the%20TomoSense%0Adataset%2C%20containing%20SAR%20and%20LiDAR%20data%20from%20Germany%27s%20Eifel%20National%20Park%2C%20to%0Adevelop%20and%20evaluate%20height%20estimation%20models.%20Our%20approach%20includes%20classical%0Amethods%2C%20deep%20learning%20with%20a%203D%20U-Net%2C%20and%20Bayesian-optimized%20techniques.%20By%0Atesting%20various%20SAR%20frequencies%20and%20polarimetries%2C%20we%20establish%20a%20baseline%20for%0Afuture%20height%20and%20biomass%20modelling.%20Best-performing%20models%20predict%20forest%0Aheight%20to%20be%20within%202.82m%20mean%20absolute%20error%20for%20canopies%20around%2030m%2C%0Aadvancing%20our%20ability%20to%20measure%20global%20carbon%20stocks%20and%20support%20climate%0Aaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-SAR%2520Tomography%2520and%2520Machine%2520Learning%2520for%2520High-Resolution%2520Tree%2520Height%250A%2520%2520Estimation%26entry.906535625%3DGrace%2520Colverd%2520and%2520Jumpei%2520Takami%2520and%2520Laura%2520Schade%2520and%2520Karol%2520Bot%2520and%2520Joseph%2520A.%2520Gallego-Mejia%26entry.1292438233%3D%2520%2520Accurately%2520estimating%2520forest%2520biomass%2520is%2520crucial%2520for%2520global%2520carbon%2520cycle%250Amodelling%2520and%2520climate%2520change%2520mitigation.%2520Tree%2520height%252C%2520a%2520key%2520factor%2520in%2520biomass%250Acalculations%252C%2520can%2520be%2520measured%2520using%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520technology.%250AThis%2520study%2520applies%2520machine%2520learning%2520to%2520extract%2520forest%2520height%2520data%2520from%2520two%2520SAR%250Aproducts%253A%2520Single%2520Look%2520Complex%2520%2528SLC%2529%2520images%2520and%2520tomographic%2520cubes%252C%2520in%250Apreparation%2520for%2520the%2520ESA%2520Biomass%2520Satellite%2520mission.%2520We%2520use%2520the%2520TomoSense%250Adataset%252C%2520containing%2520SAR%2520and%2520LiDAR%2520data%2520from%2520Germany%2527s%2520Eifel%2520National%2520Park%252C%2520to%250Adevelop%2520and%2520evaluate%2520height%2520estimation%2520models.%2520Our%2520approach%2520includes%2520classical%250Amethods%252C%2520deep%2520learning%2520with%2520a%25203D%2520U-Net%252C%2520and%2520Bayesian-optimized%2520techniques.%2520By%250Atesting%2520various%2520SAR%2520frequencies%2520and%2520polarimetries%252C%2520we%2520establish%2520a%2520baseline%2520for%250Afuture%2520height%2520and%2520biomass%2520modelling.%2520Best-performing%2520models%2520predict%2520forest%250Aheight%2520to%2520be%2520within%25202.82m%2520mean%2520absolute%2520error%2520for%2520canopies%2520around%252030m%252C%250Aadvancing%2520our%2520ability%2520to%2520measure%2520global%2520carbon%2520stocks%2520and%2520support%2520climate%250Aaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-SAR%20Tomography%20and%20Machine%20Learning%20for%20High-Resolution%20Tree%20Height%0A%20%20Estimation&entry.906535625=Grace%20Colverd%20and%20Jumpei%20Takami%20and%20Laura%20Schade%20and%20Karol%20Bot%20and%20Joseph%20A.%20Gallego-Mejia&entry.1292438233=%20%20Accurately%20estimating%20forest%20biomass%20is%20crucial%20for%20global%20carbon%20cycle%0Amodelling%20and%20climate%20change%20mitigation.%20Tree%20height%2C%20a%20key%20factor%20in%20biomass%0Acalculations%2C%20can%20be%20measured%20using%20Synthetic%20Aperture%20Radar%20%28SAR%29%20technology.%0AThis%20study%20applies%20machine%20learning%20to%20extract%20forest%20height%20data%20from%20two%20SAR%0Aproducts%3A%20Single%20Look%20Complex%20%28SLC%29%20images%20and%20tomographic%20cubes%2C%20in%0Apreparation%20for%20the%20ESA%20Biomass%20Satellite%20mission.%20We%20use%20the%20TomoSense%0Adataset%2C%20containing%20SAR%20and%20LiDAR%20data%20from%20Germany%27s%20Eifel%20National%20Park%2C%20to%0Adevelop%20and%20evaluate%20height%20estimation%20models.%20Our%20approach%20includes%20classical%0Amethods%2C%20deep%20learning%20with%20a%203D%20U-Net%2C%20and%20Bayesian-optimized%20techniques.%20By%0Atesting%20various%20SAR%20frequencies%20and%20polarimetries%2C%20we%20establish%20a%20baseline%20for%0Afuture%20height%20and%20biomass%20modelling.%20Best-performing%20models%20predict%20forest%0Aheight%20to%20be%20within%202.82m%20mean%20absolute%20error%20for%20canopies%20around%2030m%2C%0Aadvancing%20our%20ability%20to%20measure%20global%20carbon%20stocks%20and%20support%20climate%0Aaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05636v1&entry.124074799=Read"},
{"title": "Advanced LSTM Neural Networks for Predicting Directional Changes in\n  Sector-Specific ETFs Using Machine Learning Techniques", "author": "Rifa Gowani and Zaryab Kanjiani", "abstract": "  Trading and investing in stocks for some is their full-time career, while for\nothers, it's simply a supplementary income stream. Universal among all\ninvestors is the desire to turn a profit. The key to achieving this goal is\ndiversification. Spreading investments across sectors is critical to\nprofitability and maximizing returns. This study aims to gauge the viability of\nmachine learning methods in practicing the principle of diversification to\nmaximize portfolio returns. To test this, the study evaluates the Long-Short\nTerm Memory (LSTM) model across nine different sectors and over 2,200 stocks\nusing Vanguard's sector-based ETFs. The R-squared value across all sectors\nshowed promising results, with an average of 0.8651 and a high of 0.942 for the\nVNQ ETF. These findings suggest that the LSTM model is a capable and viable\nmodel for accurately predicting directional changes across various industry\nsectors, helping investors diversify and grow their portfolios.\n", "link": "http://arxiv.org/abs/2409.05778v1", "date": "2024-09-09", "relevancy": 1.9695, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20LSTM%20Neural%20Networks%20for%20Predicting%20Directional%20Changes%20in%0A%20%20Sector-Specific%20ETFs%20Using%20Machine%20Learning%20Techniques&body=Title%3A%20Advanced%20LSTM%20Neural%20Networks%20for%20Predicting%20Directional%20Changes%20in%0A%20%20Sector-Specific%20ETFs%20Using%20Machine%20Learning%20Techniques%0AAuthor%3A%20Rifa%20Gowani%20and%20Zaryab%20Kanjiani%0AAbstract%3A%20%20%20Trading%20and%20investing%20in%20stocks%20for%20some%20is%20their%20full-time%20career%2C%20while%20for%0Aothers%2C%20it%27s%20simply%20a%20supplementary%20income%20stream.%20Universal%20among%20all%0Ainvestors%20is%20the%20desire%20to%20turn%20a%20profit.%20The%20key%20to%20achieving%20this%20goal%20is%0Adiversification.%20Spreading%20investments%20across%20sectors%20is%20critical%20to%0Aprofitability%20and%20maximizing%20returns.%20This%20study%20aims%20to%20gauge%20the%20viability%20of%0Amachine%20learning%20methods%20in%20practicing%20the%20principle%20of%20diversification%20to%0Amaximize%20portfolio%20returns.%20To%20test%20this%2C%20the%20study%20evaluates%20the%20Long-Short%0ATerm%20Memory%20%28LSTM%29%20model%20across%20nine%20different%20sectors%20and%20over%202%2C200%20stocks%0Ausing%20Vanguard%27s%20sector-based%20ETFs.%20The%20R-squared%20value%20across%20all%20sectors%0Ashowed%20promising%20results%2C%20with%20an%20average%20of%200.8651%20and%20a%20high%20of%200.942%20for%20the%0AVNQ%20ETF.%20These%20findings%20suggest%20that%20the%20LSTM%20model%20is%20a%20capable%20and%20viable%0Amodel%20for%20accurately%20predicting%20directional%20changes%20across%20various%20industry%0Asectors%2C%20helping%20investors%20diversify%20and%20grow%20their%20portfolios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520LSTM%2520Neural%2520Networks%2520for%2520Predicting%2520Directional%2520Changes%2520in%250A%2520%2520Sector-Specific%2520ETFs%2520Using%2520Machine%2520Learning%2520Techniques%26entry.906535625%3DRifa%2520Gowani%2520and%2520Zaryab%2520Kanjiani%26entry.1292438233%3D%2520%2520Trading%2520and%2520investing%2520in%2520stocks%2520for%2520some%2520is%2520their%2520full-time%2520career%252C%2520while%2520for%250Aothers%252C%2520it%2527s%2520simply%2520a%2520supplementary%2520income%2520stream.%2520Universal%2520among%2520all%250Ainvestors%2520is%2520the%2520desire%2520to%2520turn%2520a%2520profit.%2520The%2520key%2520to%2520achieving%2520this%2520goal%2520is%250Adiversification.%2520Spreading%2520investments%2520across%2520sectors%2520is%2520critical%2520to%250Aprofitability%2520and%2520maximizing%2520returns.%2520This%2520study%2520aims%2520to%2520gauge%2520the%2520viability%2520of%250Amachine%2520learning%2520methods%2520in%2520practicing%2520the%2520principle%2520of%2520diversification%2520to%250Amaximize%2520portfolio%2520returns.%2520To%2520test%2520this%252C%2520the%2520study%2520evaluates%2520the%2520Long-Short%250ATerm%2520Memory%2520%2528LSTM%2529%2520model%2520across%2520nine%2520different%2520sectors%2520and%2520over%25202%252C200%2520stocks%250Ausing%2520Vanguard%2527s%2520sector-based%2520ETFs.%2520The%2520R-squared%2520value%2520across%2520all%2520sectors%250Ashowed%2520promising%2520results%252C%2520with%2520an%2520average%2520of%25200.8651%2520and%2520a%2520high%2520of%25200.942%2520for%2520the%250AVNQ%2520ETF.%2520These%2520findings%2520suggest%2520that%2520the%2520LSTM%2520model%2520is%2520a%2520capable%2520and%2520viable%250Amodel%2520for%2520accurately%2520predicting%2520directional%2520changes%2520across%2520various%2520industry%250Asectors%252C%2520helping%2520investors%2520diversify%2520and%2520grow%2520their%2520portfolios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20LSTM%20Neural%20Networks%20for%20Predicting%20Directional%20Changes%20in%0A%20%20Sector-Specific%20ETFs%20Using%20Machine%20Learning%20Techniques&entry.906535625=Rifa%20Gowani%20and%20Zaryab%20Kanjiani&entry.1292438233=%20%20Trading%20and%20investing%20in%20stocks%20for%20some%20is%20their%20full-time%20career%2C%20while%20for%0Aothers%2C%20it%27s%20simply%20a%20supplementary%20income%20stream.%20Universal%20among%20all%0Ainvestors%20is%20the%20desire%20to%20turn%20a%20profit.%20The%20key%20to%20achieving%20this%20goal%20is%0Adiversification.%20Spreading%20investments%20across%20sectors%20is%20critical%20to%0Aprofitability%20and%20maximizing%20returns.%20This%20study%20aims%20to%20gauge%20the%20viability%20of%0Amachine%20learning%20methods%20in%20practicing%20the%20principle%20of%20diversification%20to%0Amaximize%20portfolio%20returns.%20To%20test%20this%2C%20the%20study%20evaluates%20the%20Long-Short%0ATerm%20Memory%20%28LSTM%29%20model%20across%20nine%20different%20sectors%20and%20over%202%2C200%20stocks%0Ausing%20Vanguard%27s%20sector-based%20ETFs.%20The%20R-squared%20value%20across%20all%20sectors%0Ashowed%20promising%20results%2C%20with%20an%20average%20of%200.8651%20and%20a%20high%20of%200.942%20for%20the%0AVNQ%20ETF.%20These%20findings%20suggest%20that%20the%20LSTM%20model%20is%20a%20capable%20and%20viable%0Amodel%20for%20accurately%20predicting%20directional%20changes%20across%20various%20industry%0Asectors%2C%20helping%20investors%20diversify%20and%20grow%20their%20portfolios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05778v1&entry.124074799=Read"},
{"title": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical\n  Reasoning", "author": "Jinwei He and Feng Lu", "abstract": "  Large language models (LLMs) have been utilized in solving diverse reasoning\ntasks, encompassing common sense, arithmetic and deduction tasks. However, with\ndifficulties of reversing thinking patterns and irrelevant premises, how to\ndetermine the authenticity of the cause in abductive logical reasoning remains\nunderexplored. Inspired by hypothesis and verification method and\nidentification of irrelevant information in human thinking process, we propose\na new framework for LLMs abductive logical reasoning called CauseJudger (CJ),\nwhich identifies the authenticity of possible cause by transforming thinking\nfrom reverse to forward and removing irrelevant information. In addition, we\nconstruct an abductive logical reasoning dataset for decision task called\nCauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our\nexperiments show the efficiency of CJ with overall experiments and ablation\nexperiments as well as case studies on our dataset and reconstructed public\ndataset. Notably, CJ's implementation is efficient, requiring only two calls to\nLLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum\ncorrectness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4,\nCJ attains an accuracy exceeding 90% across all datasets.\n", "link": "http://arxiv.org/abs/2409.05559v1", "date": "2024-09-09", "relevancy": 1.9638, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5098}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CauseJudger%3A%20Identifying%20the%20Cause%20with%20LLMs%20for%20Abductive%20Logical%0A%20%20Reasoning&body=Title%3A%20CauseJudger%3A%20Identifying%20the%20Cause%20with%20LLMs%20for%20Abductive%20Logical%0A%20%20Reasoning%0AAuthor%3A%20Jinwei%20He%20and%20Feng%20Lu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20utilized%20in%20solving%20diverse%20reasoning%0Atasks%2C%20encompassing%20common%20sense%2C%20arithmetic%20and%20deduction%20tasks.%20However%2C%20with%0Adifficulties%20of%20reversing%20thinking%20patterns%20and%20irrelevant%20premises%2C%20how%20to%0Adetermine%20the%20authenticity%20of%20the%20cause%20in%20abductive%20logical%20reasoning%20remains%0Aunderexplored.%20Inspired%20by%20hypothesis%20and%20verification%20method%20and%0Aidentification%20of%20irrelevant%20information%20in%20human%20thinking%20process%2C%20we%20propose%0Aa%20new%20framework%20for%20LLMs%20abductive%20logical%20reasoning%20called%20CauseJudger%20%28CJ%29%2C%0Awhich%20identifies%20the%20authenticity%20of%20possible%20cause%20by%20transforming%20thinking%0Afrom%20reverse%20to%20forward%20and%20removing%20irrelevant%20information.%20In%20addition%2C%20we%0Aconstruct%20an%20abductive%20logical%20reasoning%20dataset%20for%20decision%20task%20called%0ACauseLogics%2C%20which%20contains%20200%2C000%20tasks%20of%20varying%20reasoning%20lengths.%20Our%0Aexperiments%20show%20the%20efficiency%20of%20CJ%20with%20overall%20experiments%20and%20ablation%0Aexperiments%20as%20well%20as%20case%20studies%20on%20our%20dataset%20and%20reconstructed%20public%0Adataset.%20Notably%2C%20CJ%27s%20implementation%20is%20efficient%2C%20requiring%20only%20two%20calls%20to%0ALLM.%20Its%20impact%20is%20profound%3A%20when%20using%20gpt-3.5%2C%20CJ%20achieves%20a%20maximum%0Acorrectness%20improvement%20of%2041%25%20compared%20to%20Zero-Shot-CoT.%20Moreover%2C%20with%20gpt-4%2C%0ACJ%20attains%20an%20accuracy%20exceeding%2090%25%20across%20all%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCauseJudger%253A%2520Identifying%2520the%2520Cause%2520with%2520LLMs%2520for%2520Abductive%2520Logical%250A%2520%2520Reasoning%26entry.906535625%3DJinwei%2520He%2520and%2520Feng%2520Lu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520utilized%2520in%2520solving%2520diverse%2520reasoning%250Atasks%252C%2520encompassing%2520common%2520sense%252C%2520arithmetic%2520and%2520deduction%2520tasks.%2520However%252C%2520with%250Adifficulties%2520of%2520reversing%2520thinking%2520patterns%2520and%2520irrelevant%2520premises%252C%2520how%2520to%250Adetermine%2520the%2520authenticity%2520of%2520the%2520cause%2520in%2520abductive%2520logical%2520reasoning%2520remains%250Aunderexplored.%2520Inspired%2520by%2520hypothesis%2520and%2520verification%2520method%2520and%250Aidentification%2520of%2520irrelevant%2520information%2520in%2520human%2520thinking%2520process%252C%2520we%2520propose%250Aa%2520new%2520framework%2520for%2520LLMs%2520abductive%2520logical%2520reasoning%2520called%2520CauseJudger%2520%2528CJ%2529%252C%250Awhich%2520identifies%2520the%2520authenticity%2520of%2520possible%2520cause%2520by%2520transforming%2520thinking%250Afrom%2520reverse%2520to%2520forward%2520and%2520removing%2520irrelevant%2520information.%2520In%2520addition%252C%2520we%250Aconstruct%2520an%2520abductive%2520logical%2520reasoning%2520dataset%2520for%2520decision%2520task%2520called%250ACauseLogics%252C%2520which%2520contains%2520200%252C000%2520tasks%2520of%2520varying%2520reasoning%2520lengths.%2520Our%250Aexperiments%2520show%2520the%2520efficiency%2520of%2520CJ%2520with%2520overall%2520experiments%2520and%2520ablation%250Aexperiments%2520as%2520well%2520as%2520case%2520studies%2520on%2520our%2520dataset%2520and%2520reconstructed%2520public%250Adataset.%2520Notably%252C%2520CJ%2527s%2520implementation%2520is%2520efficient%252C%2520requiring%2520only%2520two%2520calls%2520to%250ALLM.%2520Its%2520impact%2520is%2520profound%253A%2520when%2520using%2520gpt-3.5%252C%2520CJ%2520achieves%2520a%2520maximum%250Acorrectness%2520improvement%2520of%252041%2525%2520compared%2520to%2520Zero-Shot-CoT.%2520Moreover%252C%2520with%2520gpt-4%252C%250ACJ%2520attains%2520an%2520accuracy%2520exceeding%252090%2525%2520across%2520all%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CauseJudger%3A%20Identifying%20the%20Cause%20with%20LLMs%20for%20Abductive%20Logical%0A%20%20Reasoning&entry.906535625=Jinwei%20He%20and%20Feng%20Lu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20utilized%20in%20solving%20diverse%20reasoning%0Atasks%2C%20encompassing%20common%20sense%2C%20arithmetic%20and%20deduction%20tasks.%20However%2C%20with%0Adifficulties%20of%20reversing%20thinking%20patterns%20and%20irrelevant%20premises%2C%20how%20to%0Adetermine%20the%20authenticity%20of%20the%20cause%20in%20abductive%20logical%20reasoning%20remains%0Aunderexplored.%20Inspired%20by%20hypothesis%20and%20verification%20method%20and%0Aidentification%20of%20irrelevant%20information%20in%20human%20thinking%20process%2C%20we%20propose%0Aa%20new%20framework%20for%20LLMs%20abductive%20logical%20reasoning%20called%20CauseJudger%20%28CJ%29%2C%0Awhich%20identifies%20the%20authenticity%20of%20possible%20cause%20by%20transforming%20thinking%0Afrom%20reverse%20to%20forward%20and%20removing%20irrelevant%20information.%20In%20addition%2C%20we%0Aconstruct%20an%20abductive%20logical%20reasoning%20dataset%20for%20decision%20task%20called%0ACauseLogics%2C%20which%20contains%20200%2C000%20tasks%20of%20varying%20reasoning%20lengths.%20Our%0Aexperiments%20show%20the%20efficiency%20of%20CJ%20with%20overall%20experiments%20and%20ablation%0Aexperiments%20as%20well%20as%20case%20studies%20on%20our%20dataset%20and%20reconstructed%20public%0Adataset.%20Notably%2C%20CJ%27s%20implementation%20is%20efficient%2C%20requiring%20only%20two%20calls%20to%0ALLM.%20Its%20impact%20is%20profound%3A%20when%20using%20gpt-3.5%2C%20CJ%20achieves%20a%20maximum%0Acorrectness%20improvement%20of%2041%25%20compared%20to%20Zero-Shot-CoT.%20Moreover%2C%20with%20gpt-4%2C%0ACJ%20attains%20an%20accuracy%20exceeding%2090%25%20across%20all%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05559v1&entry.124074799=Read"},
{"title": "Student Classroom Behavior Detection based on Improved YOLOv7", "author": "Fan Yang", "abstract": "  Accurately detecting student behavior in classroom videos can aid in\nanalyzing their classroom performance and improving teaching effectiveness.\nHowever, the current accuracy rate in behavior detection is low. To address\nthis challenge, we propose the Student Classroom Behavior Detection method,\nbased on improved YOLOv7. First, we created the Student Classroom Behavior\ndataset (SCB-Dataset), which includes 18.4k labels and 4.2k images, covering\nthree behaviors: hand raising, reading, and writing. To improve detection\naccuracy in crowded scenes, we integrated the biformer attention module and\nWise-IoU into the YOLOv7 network. Finally, experiments were conducted on the\nSCB-Dataset, and the model achieved an mAP@0.5 of 79%, resulting in a 1.8%\nimprovement over previous results. The SCB-Dataset and code are available for\ndownload at: https://github.com/Whiffe/SCB-dataset.\n", "link": "http://arxiv.org/abs/2306.03318v2", "date": "2024-09-09", "relevancy": 1.9564, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Student%20Classroom%20Behavior%20Detection%20based%20on%20Improved%20YOLOv7&body=Title%3A%20Student%20Classroom%20Behavior%20Detection%20based%20on%20Improved%20YOLOv7%0AAuthor%3A%20Fan%20Yang%0AAbstract%3A%20%20%20Accurately%20detecting%20student%20behavior%20in%20classroom%20videos%20can%20aid%20in%0Aanalyzing%20their%20classroom%20performance%20and%20improving%20teaching%20effectiveness.%0AHowever%2C%20the%20current%20accuracy%20rate%20in%20behavior%20detection%20is%20low.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20Student%20Classroom%20Behavior%20Detection%20method%2C%0Abased%20on%20improved%20YOLOv7.%20First%2C%20we%20created%20the%20Student%20Classroom%20Behavior%0Adataset%20%28SCB-Dataset%29%2C%20which%20includes%2018.4k%20labels%20and%204.2k%20images%2C%20covering%0Athree%20behaviors%3A%20hand%20raising%2C%20reading%2C%20and%20writing.%20To%20improve%20detection%0Aaccuracy%20in%20crowded%20scenes%2C%20we%20integrated%20the%20biformer%20attention%20module%20and%0AWise-IoU%20into%20the%20YOLOv7%20network.%20Finally%2C%20experiments%20were%20conducted%20on%20the%0ASCB-Dataset%2C%20and%20the%20model%20achieved%20an%20mAP%400.5%20of%2079%25%2C%20resulting%20in%20a%201.8%25%0Aimprovement%20over%20previous%20results.%20The%20SCB-Dataset%20and%20code%20are%20available%20for%0Adownload%20at%3A%20https%3A//github.com/Whiffe/SCB-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudent%2520Classroom%2520Behavior%2520Detection%2520based%2520on%2520Improved%2520YOLOv7%26entry.906535625%3DFan%2520Yang%26entry.1292438233%3D%2520%2520Accurately%2520detecting%2520student%2520behavior%2520in%2520classroom%2520videos%2520can%2520aid%2520in%250Aanalyzing%2520their%2520classroom%2520performance%2520and%2520improving%2520teaching%2520effectiveness.%250AHowever%252C%2520the%2520current%2520accuracy%2520rate%2520in%2520behavior%2520detection%2520is%2520low.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520the%2520Student%2520Classroom%2520Behavior%2520Detection%2520method%252C%250Abased%2520on%2520improved%2520YOLOv7.%2520First%252C%2520we%2520created%2520the%2520Student%2520Classroom%2520Behavior%250Adataset%2520%2528SCB-Dataset%2529%252C%2520which%2520includes%252018.4k%2520labels%2520and%25204.2k%2520images%252C%2520covering%250Athree%2520behaviors%253A%2520hand%2520raising%252C%2520reading%252C%2520and%2520writing.%2520To%2520improve%2520detection%250Aaccuracy%2520in%2520crowded%2520scenes%252C%2520we%2520integrated%2520the%2520biformer%2520attention%2520module%2520and%250AWise-IoU%2520into%2520the%2520YOLOv7%2520network.%2520Finally%252C%2520experiments%2520were%2520conducted%2520on%2520the%250ASCB-Dataset%252C%2520and%2520the%2520model%2520achieved%2520an%2520mAP%25400.5%2520of%252079%2525%252C%2520resulting%2520in%2520a%25201.8%2525%250Aimprovement%2520over%2520previous%2520results.%2520The%2520SCB-Dataset%2520and%2520code%2520are%2520available%2520for%250Adownload%2520at%253A%2520https%253A//github.com/Whiffe/SCB-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Student%20Classroom%20Behavior%20Detection%20based%20on%20Improved%20YOLOv7&entry.906535625=Fan%20Yang&entry.1292438233=%20%20Accurately%20detecting%20student%20behavior%20in%20classroom%20videos%20can%20aid%20in%0Aanalyzing%20their%20classroom%20performance%20and%20improving%20teaching%20effectiveness.%0AHowever%2C%20the%20current%20accuracy%20rate%20in%20behavior%20detection%20is%20low.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20Student%20Classroom%20Behavior%20Detection%20method%2C%0Abased%20on%20improved%20YOLOv7.%20First%2C%20we%20created%20the%20Student%20Classroom%20Behavior%0Adataset%20%28SCB-Dataset%29%2C%20which%20includes%2018.4k%20labels%20and%204.2k%20images%2C%20covering%0Athree%20behaviors%3A%20hand%20raising%2C%20reading%2C%20and%20writing.%20To%20improve%20detection%0Aaccuracy%20in%20crowded%20scenes%2C%20we%20integrated%20the%20biformer%20attention%20module%20and%0AWise-IoU%20into%20the%20YOLOv7%20network.%20Finally%2C%20experiments%20were%20conducted%20on%20the%0ASCB-Dataset%2C%20and%20the%20model%20achieved%20an%20mAP%400.5%20of%2079%25%2C%20resulting%20in%20a%201.8%25%0Aimprovement%20over%20previous%20results.%20The%20SCB-Dataset%20and%20code%20are%20available%20for%0Adownload%20at%3A%20https%3A//github.com/Whiffe/SCB-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03318v2&entry.124074799=Read"},
{"title": "RGBManip: Monocular Image-based Robotic Manipulation through Active\n  Object Pose Estimation", "author": "Boshi An and Yiran Geng and Kai Chen and Xiaoqi Li and Qi Dou and Hao Dong", "abstract": "  Robotic manipulation requires accurate perception of the environment, which\nposes a significant challenge due to its inherent complexity and constantly\nchanging nature. In this context, RGB image and point-cloud observations are\ntwo commonly used modalities in visual-based robotic manipulation, but each of\nthese modalities have their own limitations. Commercial point-cloud\nobservations often suffer from issues like sparse sampling and noisy output due\nto the limits of the emission-reception imaging principle. On the other hand,\nRGB images, while rich in texture information, lack essential depth and 3D\ninformation crucial for robotic manipulation. To mitigate these challenges, we\npropose an image-only robotic manipulation framework that leverages an\neye-on-hand monocular camera installed on the robot's parallel gripper. By\nmoving with the robot gripper, this camera gains the ability to actively\nperceive object from multiple perspectives during the manipulation process.\nThis enables the estimation of 6D object poses, which can be utilized for\nmanipulation. While, obtaining images from more and diverse viewpoints\ntypically improves pose estimation, it also increases the manipulation time. To\naddress this trade-off, we employ a reinforcement learning policy to\nsynchronize the manipulation strategy with active perception, achieving a\nbalance between 6D pose accuracy and manipulation efficiency. Our experimental\nresults in both simulated and real-world environments showcase the\nstate-of-the-art effectiveness of our approach. %, which, to the best of our\nknowledge, is the first to achieve robust real-world robotic manipulation\nthrough active pose estimation. We believe that our method will inspire further\nresearch on real-world-oriented robotic manipulation.\n", "link": "http://arxiv.org/abs/2310.03478v2", "date": "2024-09-09", "relevancy": 1.8368, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6059}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGBManip%3A%20Monocular%20Image-based%20Robotic%20Manipulation%20through%20Active%0A%20%20Object%20Pose%20Estimation&body=Title%3A%20RGBManip%3A%20Monocular%20Image-based%20Robotic%20Manipulation%20through%20Active%0A%20%20Object%20Pose%20Estimation%0AAuthor%3A%20Boshi%20An%20and%20Yiran%20Geng%20and%20Kai%20Chen%20and%20Xiaoqi%20Li%20and%20Qi%20Dou%20and%20Hao%20Dong%0AAbstract%3A%20%20%20Robotic%20manipulation%20requires%20accurate%20perception%20of%20the%20environment%2C%20which%0Aposes%20a%20significant%20challenge%20due%20to%20its%20inherent%20complexity%20and%20constantly%0Achanging%20nature.%20In%20this%20context%2C%20RGB%20image%20and%20point-cloud%20observations%20are%0Atwo%20commonly%20used%20modalities%20in%20visual-based%20robotic%20manipulation%2C%20but%20each%20of%0Athese%20modalities%20have%20their%20own%20limitations.%20Commercial%20point-cloud%0Aobservations%20often%20suffer%20from%20issues%20like%20sparse%20sampling%20and%20noisy%20output%20due%0Ato%20the%20limits%20of%20the%20emission-reception%20imaging%20principle.%20On%20the%20other%20hand%2C%0ARGB%20images%2C%20while%20rich%20in%20texture%20information%2C%20lack%20essential%20depth%20and%203D%0Ainformation%20crucial%20for%20robotic%20manipulation.%20To%20mitigate%20these%20challenges%2C%20we%0Apropose%20an%20image-only%20robotic%20manipulation%20framework%20that%20leverages%20an%0Aeye-on-hand%20monocular%20camera%20installed%20on%20the%20robot%27s%20parallel%20gripper.%20By%0Amoving%20with%20the%20robot%20gripper%2C%20this%20camera%20gains%20the%20ability%20to%20actively%0Aperceive%20object%20from%20multiple%20perspectives%20during%20the%20manipulation%20process.%0AThis%20enables%20the%20estimation%20of%206D%20object%20poses%2C%20which%20can%20be%20utilized%20for%0Amanipulation.%20While%2C%20obtaining%20images%20from%20more%20and%20diverse%20viewpoints%0Atypically%20improves%20pose%20estimation%2C%20it%20also%20increases%20the%20manipulation%20time.%20To%0Aaddress%20this%20trade-off%2C%20we%20employ%20a%20reinforcement%20learning%20policy%20to%0Asynchronize%20the%20manipulation%20strategy%20with%20active%20perception%2C%20achieving%20a%0Abalance%20between%206D%20pose%20accuracy%20and%20manipulation%20efficiency.%20Our%20experimental%0Aresults%20in%20both%20simulated%20and%20real-world%20environments%20showcase%20the%0Astate-of-the-art%20effectiveness%20of%20our%20approach.%20%25%2C%20which%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20is%20the%20first%20to%20achieve%20robust%20real-world%20robotic%20manipulation%0Athrough%20active%20pose%20estimation.%20We%20believe%20that%20our%20method%20will%20inspire%20further%0Aresearch%20on%20real-world-oriented%20robotic%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGBManip%253A%2520Monocular%2520Image-based%2520Robotic%2520Manipulation%2520through%2520Active%250A%2520%2520Object%2520Pose%2520Estimation%26entry.906535625%3DBoshi%2520An%2520and%2520Yiran%2520Geng%2520and%2520Kai%2520Chen%2520and%2520Xiaoqi%2520Li%2520and%2520Qi%2520Dou%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520requires%2520accurate%2520perception%2520of%2520the%2520environment%252C%2520which%250Aposes%2520a%2520significant%2520challenge%2520due%2520to%2520its%2520inherent%2520complexity%2520and%2520constantly%250Achanging%2520nature.%2520In%2520this%2520context%252C%2520RGB%2520image%2520and%2520point-cloud%2520observations%2520are%250Atwo%2520commonly%2520used%2520modalities%2520in%2520visual-based%2520robotic%2520manipulation%252C%2520but%2520each%2520of%250Athese%2520modalities%2520have%2520their%2520own%2520limitations.%2520Commercial%2520point-cloud%250Aobservations%2520often%2520suffer%2520from%2520issues%2520like%2520sparse%2520sampling%2520and%2520noisy%2520output%2520due%250Ato%2520the%2520limits%2520of%2520the%2520emission-reception%2520imaging%2520principle.%2520On%2520the%2520other%2520hand%252C%250ARGB%2520images%252C%2520while%2520rich%2520in%2520texture%2520information%252C%2520lack%2520essential%2520depth%2520and%25203D%250Ainformation%2520crucial%2520for%2520robotic%2520manipulation.%2520To%2520mitigate%2520these%2520challenges%252C%2520we%250Apropose%2520an%2520image-only%2520robotic%2520manipulation%2520framework%2520that%2520leverages%2520an%250Aeye-on-hand%2520monocular%2520camera%2520installed%2520on%2520the%2520robot%2527s%2520parallel%2520gripper.%2520By%250Amoving%2520with%2520the%2520robot%2520gripper%252C%2520this%2520camera%2520gains%2520the%2520ability%2520to%2520actively%250Aperceive%2520object%2520from%2520multiple%2520perspectives%2520during%2520the%2520manipulation%2520process.%250AThis%2520enables%2520the%2520estimation%2520of%25206D%2520object%2520poses%252C%2520which%2520can%2520be%2520utilized%2520for%250Amanipulation.%2520While%252C%2520obtaining%2520images%2520from%2520more%2520and%2520diverse%2520viewpoints%250Atypically%2520improves%2520pose%2520estimation%252C%2520it%2520also%2520increases%2520the%2520manipulation%2520time.%2520To%250Aaddress%2520this%2520trade-off%252C%2520we%2520employ%2520a%2520reinforcement%2520learning%2520policy%2520to%250Asynchronize%2520the%2520manipulation%2520strategy%2520with%2520active%2520perception%252C%2520achieving%2520a%250Abalance%2520between%25206D%2520pose%2520accuracy%2520and%2520manipulation%2520efficiency.%2520Our%2520experimental%250Aresults%2520in%2520both%2520simulated%2520and%2520real-world%2520environments%2520showcase%2520the%250Astate-of-the-art%2520effectiveness%2520of%2520our%2520approach.%2520%2525%252C%2520which%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520is%2520the%2520first%2520to%2520achieve%2520robust%2520real-world%2520robotic%2520manipulation%250Athrough%2520active%2520pose%2520estimation.%2520We%2520believe%2520that%2520our%2520method%2520will%2520inspire%2520further%250Aresearch%2520on%2520real-world-oriented%2520robotic%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGBManip%3A%20Monocular%20Image-based%20Robotic%20Manipulation%20through%20Active%0A%20%20Object%20Pose%20Estimation&entry.906535625=Boshi%20An%20and%20Yiran%20Geng%20and%20Kai%20Chen%20and%20Xiaoqi%20Li%20and%20Qi%20Dou%20and%20Hao%20Dong&entry.1292438233=%20%20Robotic%20manipulation%20requires%20accurate%20perception%20of%20the%20environment%2C%20which%0Aposes%20a%20significant%20challenge%20due%20to%20its%20inherent%20complexity%20and%20constantly%0Achanging%20nature.%20In%20this%20context%2C%20RGB%20image%20and%20point-cloud%20observations%20are%0Atwo%20commonly%20used%20modalities%20in%20visual-based%20robotic%20manipulation%2C%20but%20each%20of%0Athese%20modalities%20have%20their%20own%20limitations.%20Commercial%20point-cloud%0Aobservations%20often%20suffer%20from%20issues%20like%20sparse%20sampling%20and%20noisy%20output%20due%0Ato%20the%20limits%20of%20the%20emission-reception%20imaging%20principle.%20On%20the%20other%20hand%2C%0ARGB%20images%2C%20while%20rich%20in%20texture%20information%2C%20lack%20essential%20depth%20and%203D%0Ainformation%20crucial%20for%20robotic%20manipulation.%20To%20mitigate%20these%20challenges%2C%20we%0Apropose%20an%20image-only%20robotic%20manipulation%20framework%20that%20leverages%20an%0Aeye-on-hand%20monocular%20camera%20installed%20on%20the%20robot%27s%20parallel%20gripper.%20By%0Amoving%20with%20the%20robot%20gripper%2C%20this%20camera%20gains%20the%20ability%20to%20actively%0Aperceive%20object%20from%20multiple%20perspectives%20during%20the%20manipulation%20process.%0AThis%20enables%20the%20estimation%20of%206D%20object%20poses%2C%20which%20can%20be%20utilized%20for%0Amanipulation.%20While%2C%20obtaining%20images%20from%20more%20and%20diverse%20viewpoints%0Atypically%20improves%20pose%20estimation%2C%20it%20also%20increases%20the%20manipulation%20time.%20To%0Aaddress%20this%20trade-off%2C%20we%20employ%20a%20reinforcement%20learning%20policy%20to%0Asynchronize%20the%20manipulation%20strategy%20with%20active%20perception%2C%20achieving%20a%0Abalance%20between%206D%20pose%20accuracy%20and%20manipulation%20efficiency.%20Our%20experimental%0Aresults%20in%20both%20simulated%20and%20real-world%20environments%20showcase%20the%0Astate-of-the-art%20effectiveness%20of%20our%20approach.%20%25%2C%20which%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20is%20the%20first%20to%20achieve%20robust%20real-world%20robotic%20manipulation%0Athrough%20active%20pose%20estimation.%20We%20believe%20that%20our%20method%20will%20inspire%20further%0Aresearch%20on%20real-world-oriented%20robotic%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03478v2&entry.124074799=Read"},
{"title": "Learning to Walk and Fly with Adversarial Motion Priors", "author": "Giuseppe L'Erario and Drew Hanover and Angel Romero and Yunlong Song and Gabriele Nava and Paolo Maria Viceconte and Daniele Pucci and Davide Scaramuzza", "abstract": "  Robot multimodal locomotion encompasses the ability to transition between\nwalking and flying, representing a significant challenge in robotics. This work\npresents an approach that enables automatic smooth transitions between legged\nand aerial locomotion. Leveraging the concept of Adversarial Motion Priors, our\nmethod allows the robot to imitate motion datasets and accomplish the desired\ntask without the need for complex reward functions. The robot learns walking\npatterns from human-like gaits and aerial locomotion patterns from motions\nobtained using trajectory optimization. Through this process, the robot adapts\nthe locomotion scheme based on environmental feedback using reinforcement\nlearning, with the spontaneous emergence of mode-switching behavior. The\nresults highlight the potential for achieving multimodal locomotion in aerial\nhumanoid robotics through automatic control of walking and flying modes, paving\nthe way for applications in diverse domains such as search and rescue,\nsurveillance, and exploration missions. This research contributes to advancing\nthe capabilities of aerial humanoid robots in terms of versatile locomotion in\nvarious environments.\n", "link": "http://arxiv.org/abs/2309.12784v3", "date": "2024-09-09", "relevancy": 1.7101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.657}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Walk%20and%20Fly%20with%20Adversarial%20Motion%20Priors&body=Title%3A%20Learning%20to%20Walk%20and%20Fly%20with%20Adversarial%20Motion%20Priors%0AAuthor%3A%20Giuseppe%20L%27Erario%20and%20Drew%20Hanover%20and%20Angel%20Romero%20and%20Yunlong%20Song%20and%20Gabriele%20Nava%20and%20Paolo%20Maria%20Viceconte%20and%20Daniele%20Pucci%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Robot%20multimodal%20locomotion%20encompasses%20the%20ability%20to%20transition%20between%0Awalking%20and%20flying%2C%20representing%20a%20significant%20challenge%20in%20robotics.%20This%20work%0Apresents%20an%20approach%20that%20enables%20automatic%20smooth%20transitions%20between%20legged%0Aand%20aerial%20locomotion.%20Leveraging%20the%20concept%20of%20Adversarial%20Motion%20Priors%2C%20our%0Amethod%20allows%20the%20robot%20to%20imitate%20motion%20datasets%20and%20accomplish%20the%20desired%0Atask%20without%20the%20need%20for%20complex%20reward%20functions.%20The%20robot%20learns%20walking%0Apatterns%20from%20human-like%20gaits%20and%20aerial%20locomotion%20patterns%20from%20motions%0Aobtained%20using%20trajectory%20optimization.%20Through%20this%20process%2C%20the%20robot%20adapts%0Athe%20locomotion%20scheme%20based%20on%20environmental%20feedback%20using%20reinforcement%0Alearning%2C%20with%20the%20spontaneous%20emergence%20of%20mode-switching%20behavior.%20The%0Aresults%20highlight%20the%20potential%20for%20achieving%20multimodal%20locomotion%20in%20aerial%0Ahumanoid%20robotics%20through%20automatic%20control%20of%20walking%20and%20flying%20modes%2C%20paving%0Athe%20way%20for%20applications%20in%20diverse%20domains%20such%20as%20search%20and%20rescue%2C%0Asurveillance%2C%20and%20exploration%20missions.%20This%20research%20contributes%20to%20advancing%0Athe%20capabilities%20of%20aerial%20humanoid%20robots%20in%20terms%20of%20versatile%20locomotion%20in%0Avarious%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12784v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Walk%2520and%2520Fly%2520with%2520Adversarial%2520Motion%2520Priors%26entry.906535625%3DGiuseppe%2520L%2527Erario%2520and%2520Drew%2520Hanover%2520and%2520Angel%2520Romero%2520and%2520Yunlong%2520Song%2520and%2520Gabriele%2520Nava%2520and%2520Paolo%2520Maria%2520Viceconte%2520and%2520Daniele%2520Pucci%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Robot%2520multimodal%2520locomotion%2520encompasses%2520the%2520ability%2520to%2520transition%2520between%250Awalking%2520and%2520flying%252C%2520representing%2520a%2520significant%2520challenge%2520in%2520robotics.%2520This%2520work%250Apresents%2520an%2520approach%2520that%2520enables%2520automatic%2520smooth%2520transitions%2520between%2520legged%250Aand%2520aerial%2520locomotion.%2520Leveraging%2520the%2520concept%2520of%2520Adversarial%2520Motion%2520Priors%252C%2520our%250Amethod%2520allows%2520the%2520robot%2520to%2520imitate%2520motion%2520datasets%2520and%2520accomplish%2520the%2520desired%250Atask%2520without%2520the%2520need%2520for%2520complex%2520reward%2520functions.%2520The%2520robot%2520learns%2520walking%250Apatterns%2520from%2520human-like%2520gaits%2520and%2520aerial%2520locomotion%2520patterns%2520from%2520motions%250Aobtained%2520using%2520trajectory%2520optimization.%2520Through%2520this%2520process%252C%2520the%2520robot%2520adapts%250Athe%2520locomotion%2520scheme%2520based%2520on%2520environmental%2520feedback%2520using%2520reinforcement%250Alearning%252C%2520with%2520the%2520spontaneous%2520emergence%2520of%2520mode-switching%2520behavior.%2520The%250Aresults%2520highlight%2520the%2520potential%2520for%2520achieving%2520multimodal%2520locomotion%2520in%2520aerial%250Ahumanoid%2520robotics%2520through%2520automatic%2520control%2520of%2520walking%2520and%2520flying%2520modes%252C%2520paving%250Athe%2520way%2520for%2520applications%2520in%2520diverse%2520domains%2520such%2520as%2520search%2520and%2520rescue%252C%250Asurveillance%252C%2520and%2520exploration%2520missions.%2520This%2520research%2520contributes%2520to%2520advancing%250Athe%2520capabilities%2520of%2520aerial%2520humanoid%2520robots%2520in%2520terms%2520of%2520versatile%2520locomotion%2520in%250Avarious%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12784v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Walk%20and%20Fly%20with%20Adversarial%20Motion%20Priors&entry.906535625=Giuseppe%20L%27Erario%20and%20Drew%20Hanover%20and%20Angel%20Romero%20and%20Yunlong%20Song%20and%20Gabriele%20Nava%20and%20Paolo%20Maria%20Viceconte%20and%20Daniele%20Pucci%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Robot%20multimodal%20locomotion%20encompasses%20the%20ability%20to%20transition%20between%0Awalking%20and%20flying%2C%20representing%20a%20significant%20challenge%20in%20robotics.%20This%20work%0Apresents%20an%20approach%20that%20enables%20automatic%20smooth%20transitions%20between%20legged%0Aand%20aerial%20locomotion.%20Leveraging%20the%20concept%20of%20Adversarial%20Motion%20Priors%2C%20our%0Amethod%20allows%20the%20robot%20to%20imitate%20motion%20datasets%20and%20accomplish%20the%20desired%0Atask%20without%20the%20need%20for%20complex%20reward%20functions.%20The%20robot%20learns%20walking%0Apatterns%20from%20human-like%20gaits%20and%20aerial%20locomotion%20patterns%20from%20motions%0Aobtained%20using%20trajectory%20optimization.%20Through%20this%20process%2C%20the%20robot%20adapts%0Athe%20locomotion%20scheme%20based%20on%20environmental%20feedback%20using%20reinforcement%0Alearning%2C%20with%20the%20spontaneous%20emergence%20of%20mode-switching%20behavior.%20The%0Aresults%20highlight%20the%20potential%20for%20achieving%20multimodal%20locomotion%20in%20aerial%0Ahumanoid%20robotics%20through%20automatic%20control%20of%20walking%20and%20flying%20modes%2C%20paving%0Athe%20way%20for%20applications%20in%20diverse%20domains%20such%20as%20search%20and%20rescue%2C%0Asurveillance%2C%20and%20exploration%20missions.%20This%20research%20contributes%20to%20advancing%0Athe%20capabilities%20of%20aerial%20humanoid%20robots%20in%20terms%20of%20versatile%20locomotion%20in%0Avarious%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12784v3&entry.124074799=Read"},
{"title": "Promptable Closed-loop Traffic Simulation", "author": "Shuhan Tan and Boris Ivanovic and Yuxiao Chen and Boyi Li and Xinshuo Weng and Yulong Cao and Philipp Kr\u00e4henb\u00fchl and Marco Pavone", "abstract": "  Simulation stands as a cornerstone for safe and efficient autonomous driving\ndevelopment. At its core a simulation system ought to produce realistic,\nreactive, and controllable traffic patterns. In this paper, we propose ProSim,\na multimodal promptable closed-loop traffic simulation framework. ProSim allows\nthe user to give a complex set of numerical, categorical or textual prompts to\ninstruct each agent's behavior and intention. ProSim then rolls out a traffic\nscenario in a closed-loop manner, modeling each agent's interaction with other\ntraffic participants. Our experiments show that ProSim achieves high prompt\ncontrollability given different user prompts, while reaching competitive\nperformance on the Waymo Sim Agents Challenge when no prompt is given. To\nsupport research on promptable traffic simulation, we create\nProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with\nover 10M text prompts for over 520k real-world driving scenarios. We will\nrelease code of ProSim as well as data and labeling tools of\nProSim-Instruct-520k at https://ariostgx.github.io/ProSim.\n", "link": "http://arxiv.org/abs/2409.05863v1", "date": "2024-09-09", "relevancy": 1.4343, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4806}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Promptable%20Closed-loop%20Traffic%20Simulation&body=Title%3A%20Promptable%20Closed-loop%20Traffic%20Simulation%0AAuthor%3A%20Shuhan%20Tan%20and%20Boris%20Ivanovic%20and%20Yuxiao%20Chen%20and%20Boyi%20Li%20and%20Xinshuo%20Weng%20and%20Yulong%20Cao%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20Simulation%20stands%20as%20a%20cornerstone%20for%20safe%20and%20efficient%20autonomous%20driving%0Adevelopment.%20At%20its%20core%20a%20simulation%20system%20ought%20to%20produce%20realistic%2C%0Areactive%2C%20and%20controllable%20traffic%20patterns.%20In%20this%20paper%2C%20we%20propose%20ProSim%2C%0Aa%20multimodal%20promptable%20closed-loop%20traffic%20simulation%20framework.%20ProSim%20allows%0Athe%20user%20to%20give%20a%20complex%20set%20of%20numerical%2C%20categorical%20or%20textual%20prompts%20to%0Ainstruct%20each%20agent%27s%20behavior%20and%20intention.%20ProSim%20then%20rolls%20out%20a%20traffic%0Ascenario%20in%20a%20closed-loop%20manner%2C%20modeling%20each%20agent%27s%20interaction%20with%20other%0Atraffic%20participants.%20Our%20experiments%20show%20that%20ProSim%20achieves%20high%20prompt%0Acontrollability%20given%20different%20user%20prompts%2C%20while%20reaching%20competitive%0Aperformance%20on%20the%20Waymo%20Sim%20Agents%20Challenge%20when%20no%20prompt%20is%20given.%20To%0Asupport%20research%20on%20promptable%20traffic%20simulation%2C%20we%20create%0AProSim-Instruct-520k%2C%20a%20multimodal%20prompt-scenario%20paired%20driving%20dataset%20with%0Aover%2010M%20text%20prompts%20for%20over%20520k%20real-world%20driving%20scenarios.%20We%20will%0Arelease%20code%20of%20ProSim%20as%20well%20as%20data%20and%20labeling%20tools%20of%0AProSim-Instruct-520k%20at%20https%3A//ariostgx.github.io/ProSim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptable%2520Closed-loop%2520Traffic%2520Simulation%26entry.906535625%3DShuhan%2520Tan%2520and%2520Boris%2520Ivanovic%2520and%2520Yuxiao%2520Chen%2520and%2520Boyi%2520Li%2520and%2520Xinshuo%2520Weng%2520and%2520Yulong%2520Cao%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520Simulation%2520stands%2520as%2520a%2520cornerstone%2520for%2520safe%2520and%2520efficient%2520autonomous%2520driving%250Adevelopment.%2520At%2520its%2520core%2520a%2520simulation%2520system%2520ought%2520to%2520produce%2520realistic%252C%250Areactive%252C%2520and%2520controllable%2520traffic%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ProSim%252C%250Aa%2520multimodal%2520promptable%2520closed-loop%2520traffic%2520simulation%2520framework.%2520ProSim%2520allows%250Athe%2520user%2520to%2520give%2520a%2520complex%2520set%2520of%2520numerical%252C%2520categorical%2520or%2520textual%2520prompts%2520to%250Ainstruct%2520each%2520agent%2527s%2520behavior%2520and%2520intention.%2520ProSim%2520then%2520rolls%2520out%2520a%2520traffic%250Ascenario%2520in%2520a%2520closed-loop%2520manner%252C%2520modeling%2520each%2520agent%2527s%2520interaction%2520with%2520other%250Atraffic%2520participants.%2520Our%2520experiments%2520show%2520that%2520ProSim%2520achieves%2520high%2520prompt%250Acontrollability%2520given%2520different%2520user%2520prompts%252C%2520while%2520reaching%2520competitive%250Aperformance%2520on%2520the%2520Waymo%2520Sim%2520Agents%2520Challenge%2520when%2520no%2520prompt%2520is%2520given.%2520To%250Asupport%2520research%2520on%2520promptable%2520traffic%2520simulation%252C%2520we%2520create%250AProSim-Instruct-520k%252C%2520a%2520multimodal%2520prompt-scenario%2520paired%2520driving%2520dataset%2520with%250Aover%252010M%2520text%2520prompts%2520for%2520over%2520520k%2520real-world%2520driving%2520scenarios.%2520We%2520will%250Arelease%2520code%2520of%2520ProSim%2520as%2520well%2520as%2520data%2520and%2520labeling%2520tools%2520of%250AProSim-Instruct-520k%2520at%2520https%253A//ariostgx.github.io/ProSim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Promptable%20Closed-loop%20Traffic%20Simulation&entry.906535625=Shuhan%20Tan%20and%20Boris%20Ivanovic%20and%20Yuxiao%20Chen%20and%20Boyi%20Li%20and%20Xinshuo%20Weng%20and%20Yulong%20Cao%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Marco%20Pavone&entry.1292438233=%20%20Simulation%20stands%20as%20a%20cornerstone%20for%20safe%20and%20efficient%20autonomous%20driving%0Adevelopment.%20At%20its%20core%20a%20simulation%20system%20ought%20to%20produce%20realistic%2C%0Areactive%2C%20and%20controllable%20traffic%20patterns.%20In%20this%20paper%2C%20we%20propose%20ProSim%2C%0Aa%20multimodal%20promptable%20closed-loop%20traffic%20simulation%20framework.%20ProSim%20allows%0Athe%20user%20to%20give%20a%20complex%20set%20of%20numerical%2C%20categorical%20or%20textual%20prompts%20to%0Ainstruct%20each%20agent%27s%20behavior%20and%20intention.%20ProSim%20then%20rolls%20out%20a%20traffic%0Ascenario%20in%20a%20closed-loop%20manner%2C%20modeling%20each%20agent%27s%20interaction%20with%20other%0Atraffic%20participants.%20Our%20experiments%20show%20that%20ProSim%20achieves%20high%20prompt%0Acontrollability%20given%20different%20user%20prompts%2C%20while%20reaching%20competitive%0Aperformance%20on%20the%20Waymo%20Sim%20Agents%20Challenge%20when%20no%20prompt%20is%20given.%20To%0Asupport%20research%20on%20promptable%20traffic%20simulation%2C%20we%20create%0AProSim-Instruct-520k%2C%20a%20multimodal%20prompt-scenario%20paired%20driving%20dataset%20with%0Aover%2010M%20text%20prompts%20for%20over%20520k%20real-world%20driving%20scenarios.%20We%20will%0Arelease%20code%20of%20ProSim%20as%20well%20as%20data%20and%20labeling%20tools%20of%0AProSim-Instruct-520k%20at%20https%3A//ariostgx.github.io/ProSim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05863v1&entry.124074799=Read"},
{"title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning", "author": "Simran Kaur and Simon Park and Anirudh Goyal and Sanjeev Arora", "abstract": "  We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.\n", "link": "http://arxiv.org/abs/2408.14774v2", "date": "2024-09-09", "relevancy": 1.8703, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4727}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4674}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruct-SkillMix%3A%20A%20Powerful%20Pipeline%20for%20LLM%20Instruction%20Tuning&body=Title%3A%20Instruct-SkillMix%3A%20A%20Powerful%20Pipeline%20for%20LLM%20Instruction%20Tuning%0AAuthor%3A%20Simran%20Kaur%20and%20Simon%20Park%20and%20Anirudh%20Goyal%20and%20Sanjeev%20Arora%0AAbstract%3A%20%20%20We%20introduce%20Instruct-SkillMix%2C%20an%20automated%20approach%20for%20creating%20diverse%2C%0Ahigh%20quality%20SFT%20data.%20The%20Instruct-SkillMix%20pipeline%20involves%20two%20stages%2C%20each%0Aleveraging%20an%20existing%20powerful%20LLM%3A%20%281%29%20Skill%20extraction%3A%20uses%20the%20LLM%20to%0Aextract%20core%20%22skills%22%20for%20instruction-following%2C%20either%20from%20existing%20datasets%2C%0Aor%20by%20directly%20prompting%20the%20model%3B%20%282%29%20Data%20generation%3A%20uses%20the%20powerful%20LLM%0Ato%20generate%20%28instruction%2C%20response%29%20data%20that%20exhibit%20a%20randomly%20chosen%20pair%20of%0Athese%20skills.%20Here%2C%20the%20use%20of%20random%20skill%20combinations%20promotes%20diversity%20and%0Adifficulty.%0A%20%20Vanilla%20SFT%20%28i.e.%2C%20no%20PPO%2C%20DPO%2C%20or%20RL%20methods%29%20on%20data%20generated%20from%0AInstruct-SkillMix%20leads%20to%20strong%20gains%20on%20instruction%20following%20benchmarks%0Asuch%20as%20AlpacaEval%202.0%2C%20MT-Bench%2C%20and%20WildBench.%20With%20just%20%244%24K%20examples%2C%0ALLaMA-3-8B-Base%20achieves%2042.76%25%20length-controlled%20win%20rate%20on%20AlpacaEval%202.0.%0ATo%20our%20knowledge%2C%20this%20achieves%20state-of-the-art%20performance%20among%20all%20models%0Athat%20have%20only%20undergone%20SFT%20%28no%20RL%20methods%29%20and%20competes%20with%20proprietary%0Amodels%20such%20as%20Claude%203%20Opus%20and%20LLaMA-3.1-405B-Instruct.%0A%20%20Ablation%20studies%20also%20suggest%20plausible%20reasons%20for%20why%20creating%20open%0Ainstruction-tuning%20datasets%20via%20naive%20crowd-sourcing%20has%20proved%20difficult.%0AIntroducing%20low%20quality%20answers%20%28%22shirkers%22%29%20in%20%2420%5C%25%24%20of%20Instruct-SkillMix%0Aexamples%20causes%20performance%20to%20plummet%2C%20sometimes%20catastrophically.%0A%20%20The%20Instruct-SkillMix%20pipeline%20is%20flexible%20and%20is%20adaptable%20to%20other%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruct-SkillMix%253A%2520A%2520Powerful%2520Pipeline%2520for%2520LLM%2520Instruction%2520Tuning%26entry.906535625%3DSimran%2520Kaur%2520and%2520Simon%2520Park%2520and%2520Anirudh%2520Goyal%2520and%2520Sanjeev%2520Arora%26entry.1292438233%3D%2520%2520We%2520introduce%2520Instruct-SkillMix%252C%2520an%2520automated%2520approach%2520for%2520creating%2520diverse%252C%250Ahigh%2520quality%2520SFT%2520data.%2520The%2520Instruct-SkillMix%2520pipeline%2520involves%2520two%2520stages%252C%2520each%250Aleveraging%2520an%2520existing%2520powerful%2520LLM%253A%2520%25281%2529%2520Skill%2520extraction%253A%2520uses%2520the%2520LLM%2520to%250Aextract%2520core%2520%2522skills%2522%2520for%2520instruction-following%252C%2520either%2520from%2520existing%2520datasets%252C%250Aor%2520by%2520directly%2520prompting%2520the%2520model%253B%2520%25282%2529%2520Data%2520generation%253A%2520uses%2520the%2520powerful%2520LLM%250Ato%2520generate%2520%2528instruction%252C%2520response%2529%2520data%2520that%2520exhibit%2520a%2520randomly%2520chosen%2520pair%2520of%250Athese%2520skills.%2520Here%252C%2520the%2520use%2520of%2520random%2520skill%2520combinations%2520promotes%2520diversity%2520and%250Adifficulty.%250A%2520%2520Vanilla%2520SFT%2520%2528i.e.%252C%2520no%2520PPO%252C%2520DPO%252C%2520or%2520RL%2520methods%2529%2520on%2520data%2520generated%2520from%250AInstruct-SkillMix%2520leads%2520to%2520strong%2520gains%2520on%2520instruction%2520following%2520benchmarks%250Asuch%2520as%2520AlpacaEval%25202.0%252C%2520MT-Bench%252C%2520and%2520WildBench.%2520With%2520just%2520%25244%2524K%2520examples%252C%250ALLaMA-3-8B-Base%2520achieves%252042.76%2525%2520length-controlled%2520win%2520rate%2520on%2520AlpacaEval%25202.0.%250ATo%2520our%2520knowledge%252C%2520this%2520achieves%2520state-of-the-art%2520performance%2520among%2520all%2520models%250Athat%2520have%2520only%2520undergone%2520SFT%2520%2528no%2520RL%2520methods%2529%2520and%2520competes%2520with%2520proprietary%250Amodels%2520such%2520as%2520Claude%25203%2520Opus%2520and%2520LLaMA-3.1-405B-Instruct.%250A%2520%2520Ablation%2520studies%2520also%2520suggest%2520plausible%2520reasons%2520for%2520why%2520creating%2520open%250Ainstruction-tuning%2520datasets%2520via%2520naive%2520crowd-sourcing%2520has%2520proved%2520difficult.%250AIntroducing%2520low%2520quality%2520answers%2520%2528%2522shirkers%2522%2529%2520in%2520%252420%255C%2525%2524%2520of%2520Instruct-SkillMix%250Aexamples%2520causes%2520performance%2520to%2520plummet%252C%2520sometimes%2520catastrophically.%250A%2520%2520The%2520Instruct-SkillMix%2520pipeline%2520is%2520flexible%2520and%2520is%2520adaptable%2520to%2520other%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruct-SkillMix%3A%20A%20Powerful%20Pipeline%20for%20LLM%20Instruction%20Tuning&entry.906535625=Simran%20Kaur%20and%20Simon%20Park%20and%20Anirudh%20Goyal%20and%20Sanjeev%20Arora&entry.1292438233=%20%20We%20introduce%20Instruct-SkillMix%2C%20an%20automated%20approach%20for%20creating%20diverse%2C%0Ahigh%20quality%20SFT%20data.%20The%20Instruct-SkillMix%20pipeline%20involves%20two%20stages%2C%20each%0Aleveraging%20an%20existing%20powerful%20LLM%3A%20%281%29%20Skill%20extraction%3A%20uses%20the%20LLM%20to%0Aextract%20core%20%22skills%22%20for%20instruction-following%2C%20either%20from%20existing%20datasets%2C%0Aor%20by%20directly%20prompting%20the%20model%3B%20%282%29%20Data%20generation%3A%20uses%20the%20powerful%20LLM%0Ato%20generate%20%28instruction%2C%20response%29%20data%20that%20exhibit%20a%20randomly%20chosen%20pair%20of%0Athese%20skills.%20Here%2C%20the%20use%20of%20random%20skill%20combinations%20promotes%20diversity%20and%0Adifficulty.%0A%20%20Vanilla%20SFT%20%28i.e.%2C%20no%20PPO%2C%20DPO%2C%20or%20RL%20methods%29%20on%20data%20generated%20from%0AInstruct-SkillMix%20leads%20to%20strong%20gains%20on%20instruction%20following%20benchmarks%0Asuch%20as%20AlpacaEval%202.0%2C%20MT-Bench%2C%20and%20WildBench.%20With%20just%20%244%24K%20examples%2C%0ALLaMA-3-8B-Base%20achieves%2042.76%25%20length-controlled%20win%20rate%20on%20AlpacaEval%202.0.%0ATo%20our%20knowledge%2C%20this%20achieves%20state-of-the-art%20performance%20among%20all%20models%0Athat%20have%20only%20undergone%20SFT%20%28no%20RL%20methods%29%20and%20competes%20with%20proprietary%0Amodels%20such%20as%20Claude%203%20Opus%20and%20LLaMA-3.1-405B-Instruct.%0A%20%20Ablation%20studies%20also%20suggest%20plausible%20reasons%20for%20why%20creating%20open%0Ainstruction-tuning%20datasets%20via%20naive%20crowd-sourcing%20has%20proved%20difficult.%0AIntroducing%20low%20quality%20answers%20%28%22shirkers%22%29%20in%20%2420%5C%25%24%20of%20Instruct-SkillMix%0Aexamples%20causes%20performance%20to%20plummet%2C%20sometimes%20catastrophically.%0A%20%20The%20Instruct-SkillMix%20pipeline%20is%20flexible%20and%20is%20adaptable%20to%20other%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14774v2&entry.124074799=Read"},
{"title": "Cross-Input Certified Training for Universal Perturbations", "author": "Changming Xu and Gagandeep Singh", "abstract": "  Existing work in trustworthy machine learning primarily focuses on\nsingle-input adversarial perturbations. In many real-world attack scenarios,\ninput-agnostic adversarial attacks, e.g. universal adversarial perturbations\n(UAPs), are much more feasible. Current certified training methods train models\nrobust to single-input perturbations but achieve suboptimal clean and UAP\naccuracy, thereby limiting their applicability in practical applications. We\npropose a novel method, CITRUS, for certified training of networks robust\nagainst UAP attackers. We show in an extensive evaluation across different\ndatasets, architectures, and perturbation magnitudes that our method\noutperforms traditional certified training methods on standard accuracy (up to\n10.3\\%) and achieves SOTA performance on the more practical certified UAP\naccuracy metric.\n", "link": "http://arxiv.org/abs/2405.09176v2", "date": "2024-09-09", "relevancy": 1.9019, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5016}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Input%20Certified%20Training%20for%20Universal%20Perturbations&body=Title%3A%20Cross-Input%20Certified%20Training%20for%20Universal%20Perturbations%0AAuthor%3A%20Changming%20Xu%20and%20Gagandeep%20Singh%0AAbstract%3A%20%20%20Existing%20work%20in%20trustworthy%20machine%20learning%20primarily%20focuses%20on%0Asingle-input%20adversarial%20perturbations.%20In%20many%20real-world%20attack%20scenarios%2C%0Ainput-agnostic%20adversarial%20attacks%2C%20e.g.%20universal%20adversarial%20perturbations%0A%28UAPs%29%2C%20are%20much%20more%20feasible.%20Current%20certified%20training%20methods%20train%20models%0Arobust%20to%20single-input%20perturbations%20but%20achieve%20suboptimal%20clean%20and%20UAP%0Aaccuracy%2C%20thereby%20limiting%20their%20applicability%20in%20practical%20applications.%20We%0Apropose%20a%20novel%20method%2C%20CITRUS%2C%20for%20certified%20training%20of%20networks%20robust%0Aagainst%20UAP%20attackers.%20We%20show%20in%20an%20extensive%20evaluation%20across%20different%0Adatasets%2C%20architectures%2C%20and%20perturbation%20magnitudes%20that%20our%20method%0Aoutperforms%20traditional%20certified%20training%20methods%20on%20standard%20accuracy%20%28up%20to%0A10.3%5C%25%29%20and%20achieves%20SOTA%20performance%20on%20the%20more%20practical%20certified%20UAP%0Aaccuracy%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Input%2520Certified%2520Training%2520for%2520Universal%2520Perturbations%26entry.906535625%3DChangming%2520Xu%2520and%2520Gagandeep%2520Singh%26entry.1292438233%3D%2520%2520Existing%2520work%2520in%2520trustworthy%2520machine%2520learning%2520primarily%2520focuses%2520on%250Asingle-input%2520adversarial%2520perturbations.%2520In%2520many%2520real-world%2520attack%2520scenarios%252C%250Ainput-agnostic%2520adversarial%2520attacks%252C%2520e.g.%2520universal%2520adversarial%2520perturbations%250A%2528UAPs%2529%252C%2520are%2520much%2520more%2520feasible.%2520Current%2520certified%2520training%2520methods%2520train%2520models%250Arobust%2520to%2520single-input%2520perturbations%2520but%2520achieve%2520suboptimal%2520clean%2520and%2520UAP%250Aaccuracy%252C%2520thereby%2520limiting%2520their%2520applicability%2520in%2520practical%2520applications.%2520We%250Apropose%2520a%2520novel%2520method%252C%2520CITRUS%252C%2520for%2520certified%2520training%2520of%2520networks%2520robust%250Aagainst%2520UAP%2520attackers.%2520We%2520show%2520in%2520an%2520extensive%2520evaluation%2520across%2520different%250Adatasets%252C%2520architectures%252C%2520and%2520perturbation%2520magnitudes%2520that%2520our%2520method%250Aoutperforms%2520traditional%2520certified%2520training%2520methods%2520on%2520standard%2520accuracy%2520%2528up%2520to%250A10.3%255C%2525%2529%2520and%2520achieves%2520SOTA%2520performance%2520on%2520the%2520more%2520practical%2520certified%2520UAP%250Aaccuracy%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Input%20Certified%20Training%20for%20Universal%20Perturbations&entry.906535625=Changming%20Xu%20and%20Gagandeep%20Singh&entry.1292438233=%20%20Existing%20work%20in%20trustworthy%20machine%20learning%20primarily%20focuses%20on%0Asingle-input%20adversarial%20perturbations.%20In%20many%20real-world%20attack%20scenarios%2C%0Ainput-agnostic%20adversarial%20attacks%2C%20e.g.%20universal%20adversarial%20perturbations%0A%28UAPs%29%2C%20are%20much%20more%20feasible.%20Current%20certified%20training%20methods%20train%20models%0Arobust%20to%20single-input%20perturbations%20but%20achieve%20suboptimal%20clean%20and%20UAP%0Aaccuracy%2C%20thereby%20limiting%20their%20applicability%20in%20practical%20applications.%20We%0Apropose%20a%20novel%20method%2C%20CITRUS%2C%20for%20certified%20training%20of%20networks%20robust%0Aagainst%20UAP%20attackers.%20We%20show%20in%20an%20extensive%20evaluation%20across%20different%0Adatasets%2C%20architectures%2C%20and%20perturbation%20magnitudes%20that%20our%20method%0Aoutperforms%20traditional%20certified%20training%20methods%20on%20standard%20accuracy%20%28up%20to%0A10.3%5C%25%29%20and%20achieves%20SOTA%20performance%20on%20the%20more%20practical%20certified%20UAP%0Aaccuracy%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09176v2&entry.124074799=Read"},
{"title": "Sim-to-Real of Soft Robots with Learned Residual Physics", "author": "Junpeng Gao and Mike Yan Michelis and Andrew Spielberg and Robert K. Katzschmann", "abstract": "  Accurately modeling soft robots in simulation is computationally expensive\nand commonly falls short of representing the real world. This well-known\ndiscrepancy, known as the sim-to-real gap, can have several causes, such as\ncoarsely approximated geometry and material models, manufacturing defects,\nviscoelasticity and plasticity, and hysteresis effects. Residual physics\nnetworks learn from real-world data to augment a discrepant model and bring it\ncloser to reality. Here, we present a residual physics method for modeling soft\nrobots with large degrees of freedom. We train neural networks to learn a\nresidual term -- the modeling error between simulated and physical systems.\nConcretely, the residual term is a force applied on the whole simulated mesh,\nwhile real position data is collected with only sparse motion markers. The\nphysical prior of the analytical simulation provides a starting point for the\nresidual network, and the combined model is more informed than if physics were\nlearned tabula rasa. We demonstrate our method on 1) a silicone elastomeric\nbeam and 2) a soft pneumatic arm with hard-to-model, anisotropic fiber\nreinforcements. Our method outperforms traditional system identification up to\n60%. We show that residual physics need not be limited to low degrees of\nfreedom but can effectively bridge the sim-to-real gap for high dimensional\nsystems.\n", "link": "http://arxiv.org/abs/2402.01086v2", "date": "2024-09-09", "relevancy": 1.0739, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6037}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5075}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim-to-Real%20of%20Soft%20Robots%20with%20Learned%20Residual%20Physics&body=Title%3A%20Sim-to-Real%20of%20Soft%20Robots%20with%20Learned%20Residual%20Physics%0AAuthor%3A%20Junpeng%20Gao%20and%20Mike%20Yan%20Michelis%20and%20Andrew%20Spielberg%20and%20Robert%20K.%20Katzschmann%0AAbstract%3A%20%20%20Accurately%20modeling%20soft%20robots%20in%20simulation%20is%20computationally%20expensive%0Aand%20commonly%20falls%20short%20of%20representing%20the%20real%20world.%20This%20well-known%0Adiscrepancy%2C%20known%20as%20the%20sim-to-real%20gap%2C%20can%20have%20several%20causes%2C%20such%20as%0Acoarsely%20approximated%20geometry%20and%20material%20models%2C%20manufacturing%20defects%2C%0Aviscoelasticity%20and%20plasticity%2C%20and%20hysteresis%20effects.%20Residual%20physics%0Anetworks%20learn%20from%20real-world%20data%20to%20augment%20a%20discrepant%20model%20and%20bring%20it%0Acloser%20to%20reality.%20Here%2C%20we%20present%20a%20residual%20physics%20method%20for%20modeling%20soft%0Arobots%20with%20large%20degrees%20of%20freedom.%20We%20train%20neural%20networks%20to%20learn%20a%0Aresidual%20term%20--%20the%20modeling%20error%20between%20simulated%20and%20physical%20systems.%0AConcretely%2C%20the%20residual%20term%20is%20a%20force%20applied%20on%20the%20whole%20simulated%20mesh%2C%0Awhile%20real%20position%20data%20is%20collected%20with%20only%20sparse%20motion%20markers.%20The%0Aphysical%20prior%20of%20the%20analytical%20simulation%20provides%20a%20starting%20point%20for%20the%0Aresidual%20network%2C%20and%20the%20combined%20model%20is%20more%20informed%20than%20if%20physics%20were%0Alearned%20tabula%20rasa.%20We%20demonstrate%20our%20method%20on%201%29%20a%20silicone%20elastomeric%0Abeam%20and%202%29%20a%20soft%20pneumatic%20arm%20with%20hard-to-model%2C%20anisotropic%20fiber%0Areinforcements.%20Our%20method%20outperforms%20traditional%20system%20identification%20up%20to%0A60%25.%20We%20show%20that%20residual%20physics%20need%20not%20be%20limited%20to%20low%20degrees%20of%0Afreedom%20but%20can%20effectively%20bridge%20the%20sim-to-real%20gap%20for%20high%20dimensional%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim-to-Real%2520of%2520Soft%2520Robots%2520with%2520Learned%2520Residual%2520Physics%26entry.906535625%3DJunpeng%2520Gao%2520and%2520Mike%2520Yan%2520Michelis%2520and%2520Andrew%2520Spielberg%2520and%2520Robert%2520K.%2520Katzschmann%26entry.1292438233%3D%2520%2520Accurately%2520modeling%2520soft%2520robots%2520in%2520simulation%2520is%2520computationally%2520expensive%250Aand%2520commonly%2520falls%2520short%2520of%2520representing%2520the%2520real%2520world.%2520This%2520well-known%250Adiscrepancy%252C%2520known%2520as%2520the%2520sim-to-real%2520gap%252C%2520can%2520have%2520several%2520causes%252C%2520such%2520as%250Acoarsely%2520approximated%2520geometry%2520and%2520material%2520models%252C%2520manufacturing%2520defects%252C%250Aviscoelasticity%2520and%2520plasticity%252C%2520and%2520hysteresis%2520effects.%2520Residual%2520physics%250Anetworks%2520learn%2520from%2520real-world%2520data%2520to%2520augment%2520a%2520discrepant%2520model%2520and%2520bring%2520it%250Acloser%2520to%2520reality.%2520Here%252C%2520we%2520present%2520a%2520residual%2520physics%2520method%2520for%2520modeling%2520soft%250Arobots%2520with%2520large%2520degrees%2520of%2520freedom.%2520We%2520train%2520neural%2520networks%2520to%2520learn%2520a%250Aresidual%2520term%2520--%2520the%2520modeling%2520error%2520between%2520simulated%2520and%2520physical%2520systems.%250AConcretely%252C%2520the%2520residual%2520term%2520is%2520a%2520force%2520applied%2520on%2520the%2520whole%2520simulated%2520mesh%252C%250Awhile%2520real%2520position%2520data%2520is%2520collected%2520with%2520only%2520sparse%2520motion%2520markers.%2520The%250Aphysical%2520prior%2520of%2520the%2520analytical%2520simulation%2520provides%2520a%2520starting%2520point%2520for%2520the%250Aresidual%2520network%252C%2520and%2520the%2520combined%2520model%2520is%2520more%2520informed%2520than%2520if%2520physics%2520were%250Alearned%2520tabula%2520rasa.%2520We%2520demonstrate%2520our%2520method%2520on%25201%2529%2520a%2520silicone%2520elastomeric%250Abeam%2520and%25202%2529%2520a%2520soft%2520pneumatic%2520arm%2520with%2520hard-to-model%252C%2520anisotropic%2520fiber%250Areinforcements.%2520Our%2520method%2520outperforms%2520traditional%2520system%2520identification%2520up%2520to%250A60%2525.%2520We%2520show%2520that%2520residual%2520physics%2520need%2520not%2520be%2520limited%2520to%2520low%2520degrees%2520of%250Afreedom%2520but%2520can%2520effectively%2520bridge%2520the%2520sim-to-real%2520gap%2520for%2520high%2520dimensional%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim-to-Real%20of%20Soft%20Robots%20with%20Learned%20Residual%20Physics&entry.906535625=Junpeng%20Gao%20and%20Mike%20Yan%20Michelis%20and%20Andrew%20Spielberg%20and%20Robert%20K.%20Katzschmann&entry.1292438233=%20%20Accurately%20modeling%20soft%20robots%20in%20simulation%20is%20computationally%20expensive%0Aand%20commonly%20falls%20short%20of%20representing%20the%20real%20world.%20This%20well-known%0Adiscrepancy%2C%20known%20as%20the%20sim-to-real%20gap%2C%20can%20have%20several%20causes%2C%20such%20as%0Acoarsely%20approximated%20geometry%20and%20material%20models%2C%20manufacturing%20defects%2C%0Aviscoelasticity%20and%20plasticity%2C%20and%20hysteresis%20effects.%20Residual%20physics%0Anetworks%20learn%20from%20real-world%20data%20to%20augment%20a%20discrepant%20model%20and%20bring%20it%0Acloser%20to%20reality.%20Here%2C%20we%20present%20a%20residual%20physics%20method%20for%20modeling%20soft%0Arobots%20with%20large%20degrees%20of%20freedom.%20We%20train%20neural%20networks%20to%20learn%20a%0Aresidual%20term%20--%20the%20modeling%20error%20between%20simulated%20and%20physical%20systems.%0AConcretely%2C%20the%20residual%20term%20is%20a%20force%20applied%20on%20the%20whole%20simulated%20mesh%2C%0Awhile%20real%20position%20data%20is%20collected%20with%20only%20sparse%20motion%20markers.%20The%0Aphysical%20prior%20of%20the%20analytical%20simulation%20provides%20a%20starting%20point%20for%20the%0Aresidual%20network%2C%20and%20the%20combined%20model%20is%20more%20informed%20than%20if%20physics%20were%0Alearned%20tabula%20rasa.%20We%20demonstrate%20our%20method%20on%201%29%20a%20silicone%20elastomeric%0Abeam%20and%202%29%20a%20soft%20pneumatic%20arm%20with%20hard-to-model%2C%20anisotropic%20fiber%0Areinforcements.%20Our%20method%20outperforms%20traditional%20system%20identification%20up%20to%0A60%25.%20We%20show%20that%20residual%20physics%20need%20not%20be%20limited%20to%20low%20degrees%20of%0Afreedom%20but%20can%20effectively%20bridge%20the%20sim-to-real%20gap%20for%20high%20dimensional%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01086v2&entry.124074799=Read"},
{"title": "Segmentation by Factorization: Unsupervised Semantic Segmentation for\n  Pathology by Factorizing Foundation Model Features", "author": "Jacob Gildenblat and Ofir Hadar", "abstract": "  We introduce Segmentation by Factorization (F-SEG), an unsupervised\nsegmentation method for pathology that generates segmentation masks from\npre-trained deep learning models. F-SEG allows the use of pre-trained deep\nneural networks, including recently developed pathology foundation models, for\nsemantic segmentation. It achieves this without requiring additional training\nor finetuning, by factorizing the spatial features extracted by the models into\nsegmentation masks and their associated concept features. We create generic\ntissue phenotypes for H&E images by training clustering models for multiple\nnumbers of clusters on features extracted from several deep learning models on\nThe Cancer Genome Atlas Program (TCGA), and then show how the clusters can be\nused for factorizing corresponding segmentation masks using off-the-shelf deep\nlearning models. Our results show that F-SEG provides robust unsupervised\nsegmentation capabilities for H&E pathology images, and that the segmentation\nquality is greatly improved by utilizing pathology foundation models. We\ndiscuss and propose methods for evaluating the performance of unsupervised\nsegmentation in pathology.\n", "link": "http://arxiv.org/abs/2409.05697v1", "date": "2024-09-09", "relevancy": 1.9464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation%20by%20Factorization%3A%20Unsupervised%20Semantic%20Segmentation%20for%0A%20%20Pathology%20by%20Factorizing%20Foundation%20Model%20Features&body=Title%3A%20Segmentation%20by%20Factorization%3A%20Unsupervised%20Semantic%20Segmentation%20for%0A%20%20Pathology%20by%20Factorizing%20Foundation%20Model%20Features%0AAuthor%3A%20Jacob%20Gildenblat%20and%20Ofir%20Hadar%0AAbstract%3A%20%20%20We%20introduce%20Segmentation%20by%20Factorization%20%28F-SEG%29%2C%20an%20unsupervised%0Asegmentation%20method%20for%20pathology%20that%20generates%20segmentation%20masks%20from%0Apre-trained%20deep%20learning%20models.%20F-SEG%20allows%20the%20use%20of%20pre-trained%20deep%0Aneural%20networks%2C%20including%20recently%20developed%20pathology%20foundation%20models%2C%20for%0Asemantic%20segmentation.%20It%20achieves%20this%20without%20requiring%20additional%20training%0Aor%20finetuning%2C%20by%20factorizing%20the%20spatial%20features%20extracted%20by%20the%20models%20into%0Asegmentation%20masks%20and%20their%20associated%20concept%20features.%20We%20create%20generic%0Atissue%20phenotypes%20for%20H%26E%20images%20by%20training%20clustering%20models%20for%20multiple%0Anumbers%20of%20clusters%20on%20features%20extracted%20from%20several%20deep%20learning%20models%20on%0AThe%20Cancer%20Genome%20Atlas%20Program%20%28TCGA%29%2C%20and%20then%20show%20how%20the%20clusters%20can%20be%0Aused%20for%20factorizing%20corresponding%20segmentation%20masks%20using%20off-the-shelf%20deep%0Alearning%20models.%20Our%20results%20show%20that%20F-SEG%20provides%20robust%20unsupervised%0Asegmentation%20capabilities%20for%20H%26E%20pathology%20images%2C%20and%20that%20the%20segmentation%0Aquality%20is%20greatly%20improved%20by%20utilizing%20pathology%20foundation%20models.%20We%0Adiscuss%20and%20propose%20methods%20for%20evaluating%20the%20performance%20of%20unsupervised%0Asegmentation%20in%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation%2520by%2520Factorization%253A%2520Unsupervised%2520Semantic%2520Segmentation%2520for%250A%2520%2520Pathology%2520by%2520Factorizing%2520Foundation%2520Model%2520Features%26entry.906535625%3DJacob%2520Gildenblat%2520and%2520Ofir%2520Hadar%26entry.1292438233%3D%2520%2520We%2520introduce%2520Segmentation%2520by%2520Factorization%2520%2528F-SEG%2529%252C%2520an%2520unsupervised%250Asegmentation%2520method%2520for%2520pathology%2520that%2520generates%2520segmentation%2520masks%2520from%250Apre-trained%2520deep%2520learning%2520models.%2520F-SEG%2520allows%2520the%2520use%2520of%2520pre-trained%2520deep%250Aneural%2520networks%252C%2520including%2520recently%2520developed%2520pathology%2520foundation%2520models%252C%2520for%250Asemantic%2520segmentation.%2520It%2520achieves%2520this%2520without%2520requiring%2520additional%2520training%250Aor%2520finetuning%252C%2520by%2520factorizing%2520the%2520spatial%2520features%2520extracted%2520by%2520the%2520models%2520into%250Asegmentation%2520masks%2520and%2520their%2520associated%2520concept%2520features.%2520We%2520create%2520generic%250Atissue%2520phenotypes%2520for%2520H%2526E%2520images%2520by%2520training%2520clustering%2520models%2520for%2520multiple%250Anumbers%2520of%2520clusters%2520on%2520features%2520extracted%2520from%2520several%2520deep%2520learning%2520models%2520on%250AThe%2520Cancer%2520Genome%2520Atlas%2520Program%2520%2528TCGA%2529%252C%2520and%2520then%2520show%2520how%2520the%2520clusters%2520can%2520be%250Aused%2520for%2520factorizing%2520corresponding%2520segmentation%2520masks%2520using%2520off-the-shelf%2520deep%250Alearning%2520models.%2520Our%2520results%2520show%2520that%2520F-SEG%2520provides%2520robust%2520unsupervised%250Asegmentation%2520capabilities%2520for%2520H%2526E%2520pathology%2520images%252C%2520and%2520that%2520the%2520segmentation%250Aquality%2520is%2520greatly%2520improved%2520by%2520utilizing%2520pathology%2520foundation%2520models.%2520We%250Adiscuss%2520and%2520propose%2520methods%2520for%2520evaluating%2520the%2520performance%2520of%2520unsupervised%250Asegmentation%2520in%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20by%20Factorization%3A%20Unsupervised%20Semantic%20Segmentation%20for%0A%20%20Pathology%20by%20Factorizing%20Foundation%20Model%20Features&entry.906535625=Jacob%20Gildenblat%20and%20Ofir%20Hadar&entry.1292438233=%20%20We%20introduce%20Segmentation%20by%20Factorization%20%28F-SEG%29%2C%20an%20unsupervised%0Asegmentation%20method%20for%20pathology%20that%20generates%20segmentation%20masks%20from%0Apre-trained%20deep%20learning%20models.%20F-SEG%20allows%20the%20use%20of%20pre-trained%20deep%0Aneural%20networks%2C%20including%20recently%20developed%20pathology%20foundation%20models%2C%20for%0Asemantic%20segmentation.%20It%20achieves%20this%20without%20requiring%20additional%20training%0Aor%20finetuning%2C%20by%20factorizing%20the%20spatial%20features%20extracted%20by%20the%20models%20into%0Asegmentation%20masks%20and%20their%20associated%20concept%20features.%20We%20create%20generic%0Atissue%20phenotypes%20for%20H%26E%20images%20by%20training%20clustering%20models%20for%20multiple%0Anumbers%20of%20clusters%20on%20features%20extracted%20from%20several%20deep%20learning%20models%20on%0AThe%20Cancer%20Genome%20Atlas%20Program%20%28TCGA%29%2C%20and%20then%20show%20how%20the%20clusters%20can%20be%0Aused%20for%20factorizing%20corresponding%20segmentation%20masks%20using%20off-the-shelf%20deep%0Alearning%20models.%20Our%20results%20show%20that%20F-SEG%20provides%20robust%20unsupervised%0Asegmentation%20capabilities%20for%20H%26E%20pathology%20images%2C%20and%20that%20the%20segmentation%0Aquality%20is%20greatly%20improved%20by%20utilizing%20pathology%20foundation%20models.%20We%0Adiscuss%20and%20propose%20methods%20for%20evaluating%20the%20performance%20of%20unsupervised%0Asegmentation%20in%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05697v1&entry.124074799=Read"},
{"title": "Enhancing Preference-based Linear Bandits via Human Response Time", "author": "Shen Li and Yuyang Zhang and Zhaolin Ren and Claire Liang and Na Li and Julie A. Shah", "abstract": "  Binary human choice feedback is widely used in interactive preference\nlearning for its simplicity, but it provides limited information about\npreference strength. To overcome this limitation, we leverage human response\ntimes, which inversely correlate with preference strength, as complementary\ninformation. Our work integrates the EZ-diffusion model, which jointly models\nhuman choices and response times, into preference-based linear bandits. We\nintroduce a computationally efficient utility estimator that reformulates the\nutility estimation problem using both choices and response times as a linear\nregression problem. Theoretical and empirical comparisons with traditional\nchoice-only estimators reveal that for queries with strong preferences (\"easy\"\nqueries), choices alone provide limited information, while response times offer\nvaluable complementary information about preference strength. As a result,\nincorporating response times makes easy queries more useful. We demonstrate\nthis advantage in the fixed-budget best-arm identification problem, with\nsimulations based on three real-world datasets, consistently showing\naccelerated learning when response times are incorporated.\n", "link": "http://arxiv.org/abs/2409.05798v1", "date": "2024-09-09", "relevancy": 1.7853, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4459}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Preference-based%20Linear%20Bandits%20via%20Human%20Response%20Time&body=Title%3A%20Enhancing%20Preference-based%20Linear%20Bandits%20via%20Human%20Response%20Time%0AAuthor%3A%20Shen%20Li%20and%20Yuyang%20Zhang%20and%20Zhaolin%20Ren%20and%20Claire%20Liang%20and%20Na%20Li%20and%20Julie%20A.%20Shah%0AAbstract%3A%20%20%20Binary%20human%20choice%20feedback%20is%20widely%20used%20in%20interactive%20preference%0Alearning%20for%20its%20simplicity%2C%20but%20it%20provides%20limited%20information%20about%0Apreference%20strength.%20To%20overcome%20this%20limitation%2C%20we%20leverage%20human%20response%0Atimes%2C%20which%20inversely%20correlate%20with%20preference%20strength%2C%20as%20complementary%0Ainformation.%20Our%20work%20integrates%20the%20EZ-diffusion%20model%2C%20which%20jointly%20models%0Ahuman%20choices%20and%20response%20times%2C%20into%20preference-based%20linear%20bandits.%20We%0Aintroduce%20a%20computationally%20efficient%20utility%20estimator%20that%20reformulates%20the%0Autility%20estimation%20problem%20using%20both%20choices%20and%20response%20times%20as%20a%20linear%0Aregression%20problem.%20Theoretical%20and%20empirical%20comparisons%20with%20traditional%0Achoice-only%20estimators%20reveal%20that%20for%20queries%20with%20strong%20preferences%20%28%22easy%22%0Aqueries%29%2C%20choices%20alone%20provide%20limited%20information%2C%20while%20response%20times%20offer%0Avaluable%20complementary%20information%20about%20preference%20strength.%20As%20a%20result%2C%0Aincorporating%20response%20times%20makes%20easy%20queries%20more%20useful.%20We%20demonstrate%0Athis%20advantage%20in%20the%20fixed-budget%20best-arm%20identification%20problem%2C%20with%0Asimulations%20based%20on%20three%20real-world%20datasets%2C%20consistently%20showing%0Aaccelerated%20learning%20when%20response%20times%20are%20incorporated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Preference-based%2520Linear%2520Bandits%2520via%2520Human%2520Response%2520Time%26entry.906535625%3DShen%2520Li%2520and%2520Yuyang%2520Zhang%2520and%2520Zhaolin%2520Ren%2520and%2520Claire%2520Liang%2520and%2520Na%2520Li%2520and%2520Julie%2520A.%2520Shah%26entry.1292438233%3D%2520%2520Binary%2520human%2520choice%2520feedback%2520is%2520widely%2520used%2520in%2520interactive%2520preference%250Alearning%2520for%2520its%2520simplicity%252C%2520but%2520it%2520provides%2520limited%2520information%2520about%250Apreference%2520strength.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520leverage%2520human%2520response%250Atimes%252C%2520which%2520inversely%2520correlate%2520with%2520preference%2520strength%252C%2520as%2520complementary%250Ainformation.%2520Our%2520work%2520integrates%2520the%2520EZ-diffusion%2520model%252C%2520which%2520jointly%2520models%250Ahuman%2520choices%2520and%2520response%2520times%252C%2520into%2520preference-based%2520linear%2520bandits.%2520We%250Aintroduce%2520a%2520computationally%2520efficient%2520utility%2520estimator%2520that%2520reformulates%2520the%250Autility%2520estimation%2520problem%2520using%2520both%2520choices%2520and%2520response%2520times%2520as%2520a%2520linear%250Aregression%2520problem.%2520Theoretical%2520and%2520empirical%2520comparisons%2520with%2520traditional%250Achoice-only%2520estimators%2520reveal%2520that%2520for%2520queries%2520with%2520strong%2520preferences%2520%2528%2522easy%2522%250Aqueries%2529%252C%2520choices%2520alone%2520provide%2520limited%2520information%252C%2520while%2520response%2520times%2520offer%250Avaluable%2520complementary%2520information%2520about%2520preference%2520strength.%2520As%2520a%2520result%252C%250Aincorporating%2520response%2520times%2520makes%2520easy%2520queries%2520more%2520useful.%2520We%2520demonstrate%250Athis%2520advantage%2520in%2520the%2520fixed-budget%2520best-arm%2520identification%2520problem%252C%2520with%250Asimulations%2520based%2520on%2520three%2520real-world%2520datasets%252C%2520consistently%2520showing%250Aaccelerated%2520learning%2520when%2520response%2520times%2520are%2520incorporated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Preference-based%20Linear%20Bandits%20via%20Human%20Response%20Time&entry.906535625=Shen%20Li%20and%20Yuyang%20Zhang%20and%20Zhaolin%20Ren%20and%20Claire%20Liang%20and%20Na%20Li%20and%20Julie%20A.%20Shah&entry.1292438233=%20%20Binary%20human%20choice%20feedback%20is%20widely%20used%20in%20interactive%20preference%0Alearning%20for%20its%20simplicity%2C%20but%20it%20provides%20limited%20information%20about%0Apreference%20strength.%20To%20overcome%20this%20limitation%2C%20we%20leverage%20human%20response%0Atimes%2C%20which%20inversely%20correlate%20with%20preference%20strength%2C%20as%20complementary%0Ainformation.%20Our%20work%20integrates%20the%20EZ-diffusion%20model%2C%20which%20jointly%20models%0Ahuman%20choices%20and%20response%20times%2C%20into%20preference-based%20linear%20bandits.%20We%0Aintroduce%20a%20computationally%20efficient%20utility%20estimator%20that%20reformulates%20the%0Autility%20estimation%20problem%20using%20both%20choices%20and%20response%20times%20as%20a%20linear%0Aregression%20problem.%20Theoretical%20and%20empirical%20comparisons%20with%20traditional%0Achoice-only%20estimators%20reveal%20that%20for%20queries%20with%20strong%20preferences%20%28%22easy%22%0Aqueries%29%2C%20choices%20alone%20provide%20limited%20information%2C%20while%20response%20times%20offer%0Avaluable%20complementary%20information%20about%20preference%20strength.%20As%20a%20result%2C%0Aincorporating%20response%20times%20makes%20easy%20queries%20more%20useful.%20We%20demonstrate%0Athis%20advantage%20in%20the%20fixed-budget%20best-arm%20identification%20problem%2C%20with%0Asimulations%20based%20on%20three%20real-world%20datasets%2C%20consistently%20showing%0Aaccelerated%20learning%20when%20response%20times%20are%20incorporated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05798v1&entry.124074799=Read"},
{"title": "EpiLearn: A Python Library for Machine Learning in Epidemic Modeling", "author": "Zewen Liu and Yunxiao Li and Mingyang Wei and Guancheng Wan and Max S. Y. Lau and Wei Jin", "abstract": "  EpiLearn is a Python toolkit developed for modeling, simulating, and\nanalyzing epidemic data. Although there exist several packages that also deal\nwith epidemic modeling, they are often restricted to mechanistic models or\ntraditional statistical tools. As machine learning continues to shape the\nworld, the gap between these packages and the latest models has become larger.\nTo bridge the gap and inspire innovative research in epidemic modeling,\nEpiLearn not only provides support for evaluating epidemic models based on\nmachine learning, but also incorporates comprehensive tools for analyzing\nepidemic data, such as simulation, visualization, transformations, etc. For the\nconvenience of both epidemiologists and data scientists, we provide a unified\nframework for training and evaluation of epidemic models on two tasks:\nForecasting and Source Detection. To facilitate the development of new models,\nEpiLearn follows a modular design, making it flexible and easy to use. In\naddition, an interactive web application is also developed to visualize the\nreal-world or simulated epidemic data. Our package is available at\nhttps://github.com/Emory-Melody/EpiLearn.\n", "link": "http://arxiv.org/abs/2406.06016v2", "date": "2024-09-09", "relevancy": 1.6592, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4371}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4187}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EpiLearn%3A%20A%20Python%20Library%20for%20Machine%20Learning%20in%20Epidemic%20Modeling&body=Title%3A%20EpiLearn%3A%20A%20Python%20Library%20for%20Machine%20Learning%20in%20Epidemic%20Modeling%0AAuthor%3A%20Zewen%20Liu%20and%20Yunxiao%20Li%20and%20Mingyang%20Wei%20and%20Guancheng%20Wan%20and%20Max%20S.%20Y.%20Lau%20and%20Wei%20Jin%0AAbstract%3A%20%20%20EpiLearn%20is%20a%20Python%20toolkit%20developed%20for%20modeling%2C%20simulating%2C%20and%0Aanalyzing%20epidemic%20data.%20Although%20there%20exist%20several%20packages%20that%20also%20deal%0Awith%20epidemic%20modeling%2C%20they%20are%20often%20restricted%20to%20mechanistic%20models%20or%0Atraditional%20statistical%20tools.%20As%20machine%20learning%20continues%20to%20shape%20the%0Aworld%2C%20the%20gap%20between%20these%20packages%20and%20the%20latest%20models%20has%20become%20larger.%0ATo%20bridge%20the%20gap%20and%20inspire%20innovative%20research%20in%20epidemic%20modeling%2C%0AEpiLearn%20not%20only%20provides%20support%20for%20evaluating%20epidemic%20models%20based%20on%0Amachine%20learning%2C%20but%20also%20incorporates%20comprehensive%20tools%20for%20analyzing%0Aepidemic%20data%2C%20such%20as%20simulation%2C%20visualization%2C%20transformations%2C%20etc.%20For%20the%0Aconvenience%20of%20both%20epidemiologists%20and%20data%20scientists%2C%20we%20provide%20a%20unified%0Aframework%20for%20training%20and%20evaluation%20of%20epidemic%20models%20on%20two%20tasks%3A%0AForecasting%20and%20Source%20Detection.%20To%20facilitate%20the%20development%20of%20new%20models%2C%0AEpiLearn%20follows%20a%20modular%20design%2C%20making%20it%20flexible%20and%20easy%20to%20use.%20In%0Aaddition%2C%20an%20interactive%20web%20application%20is%20also%20developed%20to%20visualize%20the%0Areal-world%20or%20simulated%20epidemic%20data.%20Our%20package%20is%20available%20at%0Ahttps%3A//github.com/Emory-Melody/EpiLearn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpiLearn%253A%2520A%2520Python%2520Library%2520for%2520Machine%2520Learning%2520in%2520Epidemic%2520Modeling%26entry.906535625%3DZewen%2520Liu%2520and%2520Yunxiao%2520Li%2520and%2520Mingyang%2520Wei%2520and%2520Guancheng%2520Wan%2520and%2520Max%2520S.%2520Y.%2520Lau%2520and%2520Wei%2520Jin%26entry.1292438233%3D%2520%2520EpiLearn%2520is%2520a%2520Python%2520toolkit%2520developed%2520for%2520modeling%252C%2520simulating%252C%2520and%250Aanalyzing%2520epidemic%2520data.%2520Although%2520there%2520exist%2520several%2520packages%2520that%2520also%2520deal%250Awith%2520epidemic%2520modeling%252C%2520they%2520are%2520often%2520restricted%2520to%2520mechanistic%2520models%2520or%250Atraditional%2520statistical%2520tools.%2520As%2520machine%2520learning%2520continues%2520to%2520shape%2520the%250Aworld%252C%2520the%2520gap%2520between%2520these%2520packages%2520and%2520the%2520latest%2520models%2520has%2520become%2520larger.%250ATo%2520bridge%2520the%2520gap%2520and%2520inspire%2520innovative%2520research%2520in%2520epidemic%2520modeling%252C%250AEpiLearn%2520not%2520only%2520provides%2520support%2520for%2520evaluating%2520epidemic%2520models%2520based%2520on%250Amachine%2520learning%252C%2520but%2520also%2520incorporates%2520comprehensive%2520tools%2520for%2520analyzing%250Aepidemic%2520data%252C%2520such%2520as%2520simulation%252C%2520visualization%252C%2520transformations%252C%2520etc.%2520For%2520the%250Aconvenience%2520of%2520both%2520epidemiologists%2520and%2520data%2520scientists%252C%2520we%2520provide%2520a%2520unified%250Aframework%2520for%2520training%2520and%2520evaluation%2520of%2520epidemic%2520models%2520on%2520two%2520tasks%253A%250AForecasting%2520and%2520Source%2520Detection.%2520To%2520facilitate%2520the%2520development%2520of%2520new%2520models%252C%250AEpiLearn%2520follows%2520a%2520modular%2520design%252C%2520making%2520it%2520flexible%2520and%2520easy%2520to%2520use.%2520In%250Aaddition%252C%2520an%2520interactive%2520web%2520application%2520is%2520also%2520developed%2520to%2520visualize%2520the%250Areal-world%2520or%2520simulated%2520epidemic%2520data.%2520Our%2520package%2520is%2520available%2520at%250Ahttps%253A//github.com/Emory-Melody/EpiLearn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EpiLearn%3A%20A%20Python%20Library%20for%20Machine%20Learning%20in%20Epidemic%20Modeling&entry.906535625=Zewen%20Liu%20and%20Yunxiao%20Li%20and%20Mingyang%20Wei%20and%20Guancheng%20Wan%20and%20Max%20S.%20Y.%20Lau%20and%20Wei%20Jin&entry.1292438233=%20%20EpiLearn%20is%20a%20Python%20toolkit%20developed%20for%20modeling%2C%20simulating%2C%20and%0Aanalyzing%20epidemic%20data.%20Although%20there%20exist%20several%20packages%20that%20also%20deal%0Awith%20epidemic%20modeling%2C%20they%20are%20often%20restricted%20to%20mechanistic%20models%20or%0Atraditional%20statistical%20tools.%20As%20machine%20learning%20continues%20to%20shape%20the%0Aworld%2C%20the%20gap%20between%20these%20packages%20and%20the%20latest%20models%20has%20become%20larger.%0ATo%20bridge%20the%20gap%20and%20inspire%20innovative%20research%20in%20epidemic%20modeling%2C%0AEpiLearn%20not%20only%20provides%20support%20for%20evaluating%20epidemic%20models%20based%20on%0Amachine%20learning%2C%20but%20also%20incorporates%20comprehensive%20tools%20for%20analyzing%0Aepidemic%20data%2C%20such%20as%20simulation%2C%20visualization%2C%20transformations%2C%20etc.%20For%20the%0Aconvenience%20of%20both%20epidemiologists%20and%20data%20scientists%2C%20we%20provide%20a%20unified%0Aframework%20for%20training%20and%20evaluation%20of%20epidemic%20models%20on%20two%20tasks%3A%0AForecasting%20and%20Source%20Detection.%20To%20facilitate%20the%20development%20of%20new%20models%2C%0AEpiLearn%20follows%20a%20modular%20design%2C%20making%20it%20flexible%20and%20easy%20to%20use.%20In%0Aaddition%2C%20an%20interactive%20web%20application%20is%20also%20developed%20to%20visualize%20the%0Areal-world%20or%20simulated%20epidemic%20data.%20Our%20package%20is%20available%20at%0Ahttps%3A//github.com/Emory-Melody/EpiLearn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06016v2&entry.124074799=Read"},
{"title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions", "author": "Liman Wang and Hanyang Zhong and Wenting Cao and Zeyuan Sun", "abstract": "  This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.\n", "link": "http://arxiv.org/abs/2406.10999v3", "date": "2024-09-09", "relevancy": 1.4625, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Rigor%20and%20Utility%3A%20Mitigating%20Cognitive%20Biases%20in%20Large%0A%20%20Language%20Models%20for%20Multiple-Choice%20Questions&body=Title%3A%20Balancing%20Rigor%20and%20Utility%3A%20Mitigating%20Cognitive%20Biases%20in%20Large%0A%20%20Language%20Models%20for%20Multiple-Choice%20Questions%0AAuthor%3A%20Liman%20Wang%20and%20Hanyang%20Zhong%20and%20Wenting%20Cao%20and%20Zeyuan%20Sun%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20role%20of%20cognitive%20biases%20in%20the%20decision-making%0Aprocesses%20of%20large%20language%20models%20%28LLMs%29%2C%20challenging%20the%20conventional%20goal%20of%0Aeliminating%20all%20biases.%20We%20show%20that%20certain%20cognitive%20biases%20when%20properly%0Abalanced%2C%20can%20enhance%20decision-making%20efficiency%20through%20rational%20deviations%0Aand%20heuristic%20shortcuts.%20By%20introducing%20heuristic%20moderation%20and%20an%20abstention%0Aoption%2C%20which%20allows%20LLMs%20to%20withhold%20responses%20when%20uncertain%2C%20we%20reduce%20error%0Arates%2C%20improve%20decision%20accuracy%2C%20and%20optimize%20decision%20rates.%20Using%20the%0ABalance%20Rigor%20and%20Utility%20%28BRU%29%20dataset%2C%20developed%20through%20expert%0Acollaboration%2C%20our%20findings%20demonstrate%20that%20targeted%20inspection%20of%20cognitive%0Abiases%20aligns%20LLM%20decisions%20more%20closely%20with%20human%20reasoning%2C%20enhancing%0Areliability%20and%20suggesting%20strategies%20for%20future%20improvements.%20This%20approach%0Aoffers%20a%20novel%20way%20to%20leverage%20cognitive%20biases%20to%20improve%20the%20practical%0Autility%20of%20LLMs%20across%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10999v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Rigor%2520and%2520Utility%253A%2520Mitigating%2520Cognitive%2520Biases%2520in%2520Large%250A%2520%2520Language%2520Models%2520for%2520Multiple-Choice%2520Questions%26entry.906535625%3DLiman%2520Wang%2520and%2520Hanyang%2520Zhong%2520and%2520Wenting%2520Cao%2520and%2520Zeyuan%2520Sun%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520the%2520role%2520of%2520cognitive%2520biases%2520in%2520the%2520decision-making%250Aprocesses%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520challenging%2520the%2520conventional%2520goal%2520of%250Aeliminating%2520all%2520biases.%2520We%2520show%2520that%2520certain%2520cognitive%2520biases%2520when%2520properly%250Abalanced%252C%2520can%2520enhance%2520decision-making%2520efficiency%2520through%2520rational%2520deviations%250Aand%2520heuristic%2520shortcuts.%2520By%2520introducing%2520heuristic%2520moderation%2520and%2520an%2520abstention%250Aoption%252C%2520which%2520allows%2520LLMs%2520to%2520withhold%2520responses%2520when%2520uncertain%252C%2520we%2520reduce%2520error%250Arates%252C%2520improve%2520decision%2520accuracy%252C%2520and%2520optimize%2520decision%2520rates.%2520Using%2520the%250ABalance%2520Rigor%2520and%2520Utility%2520%2528BRU%2529%2520dataset%252C%2520developed%2520through%2520expert%250Acollaboration%252C%2520our%2520findings%2520demonstrate%2520that%2520targeted%2520inspection%2520of%2520cognitive%250Abiases%2520aligns%2520LLM%2520decisions%2520more%2520closely%2520with%2520human%2520reasoning%252C%2520enhancing%250Areliability%2520and%2520suggesting%2520strategies%2520for%2520future%2520improvements.%2520This%2520approach%250Aoffers%2520a%2520novel%2520way%2520to%2520leverage%2520cognitive%2520biases%2520to%2520improve%2520the%2520practical%250Autility%2520of%2520LLMs%2520across%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10999v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Rigor%20and%20Utility%3A%20Mitigating%20Cognitive%20Biases%20in%20Large%0A%20%20Language%20Models%20for%20Multiple-Choice%20Questions&entry.906535625=Liman%20Wang%20and%20Hanyang%20Zhong%20and%20Wenting%20Cao%20and%20Zeyuan%20Sun&entry.1292438233=%20%20This%20paper%20examines%20the%20role%20of%20cognitive%20biases%20in%20the%20decision-making%0Aprocesses%20of%20large%20language%20models%20%28LLMs%29%2C%20challenging%20the%20conventional%20goal%20of%0Aeliminating%20all%20biases.%20We%20show%20that%20certain%20cognitive%20biases%20when%20properly%0Abalanced%2C%20can%20enhance%20decision-making%20efficiency%20through%20rational%20deviations%0Aand%20heuristic%20shortcuts.%20By%20introducing%20heuristic%20moderation%20and%20an%20abstention%0Aoption%2C%20which%20allows%20LLMs%20to%20withhold%20responses%20when%20uncertain%2C%20we%20reduce%20error%0Arates%2C%20improve%20decision%20accuracy%2C%20and%20optimize%20decision%20rates.%20Using%20the%0ABalance%20Rigor%20and%20Utility%20%28BRU%29%20dataset%2C%20developed%20through%20expert%0Acollaboration%2C%20our%20findings%20demonstrate%20that%20targeted%20inspection%20of%20cognitive%0Abiases%20aligns%20LLM%20decisions%20more%20closely%20with%20human%20reasoning%2C%20enhancing%0Areliability%20and%20suggesting%20strategies%20for%20future%20improvements.%20This%20approach%0Aoffers%20a%20novel%20way%20to%20leverage%20cognitive%20biases%20to%20improve%20the%20practical%0Autility%20of%20LLMs%20across%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10999v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


