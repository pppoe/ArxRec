<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251117.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars", "author": "Marcel C. B\u00fchler and Ye Yuan and Xueting Li and Yangyi Huang and Koki Nagano and Umar Iqbal", "abstract": "We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on the ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.", "link": "http://arxiv.org/abs/2507.15979v2", "date": "2025-11-17", "relevancy": 3.4262, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6898}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.683}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dream%2C%20Lift%2C%20Animate%3A%20From%20Single%20Images%20to%20Animatable%20Gaussian%20Avatars&body=Title%3A%20Dream%2C%20Lift%2C%20Animate%3A%20From%20Single%20Images%20to%20Animatable%20Gaussian%20Avatars%0AAuthor%3A%20Marcel%20C.%20B%C3%BChler%20and%20Ye%20Yuan%20and%20Xueting%20Li%20and%20Yangyi%20Huang%20and%20Koki%20Nagano%20and%20Umar%20Iqbal%0AAbstract%3A%20We%20introduce%20Dream%2C%20Lift%2C%20Animate%20%28DLA%29%2C%20a%20novel%20framework%20that%20reconstructs%20animatable%203D%20human%20avatars%20from%20a%20single%20image.%20This%20is%20achieved%20by%20leveraging%20multi-view%20generation%2C%203D%20Gaussian%20lifting%2C%20and%20pose-aware%20UV-space%20mapping%20of%203D%20Gaussians.%20Given%20an%20image%2C%20we%20first%20dream%20plausible%20multi-views%20using%20a%20video%20diffusion%20model%2C%20capturing%20rich%20geometric%20and%20appearance%20details.%20These%20views%20are%20then%20lifted%20into%20unstructured%203D%20Gaussians.%20To%20enable%20animation%2C%20we%20propose%20a%20transformer-based%20encoder%20that%20models%20global%20spatial%20relationships%20and%20projects%20these%20Gaussians%20into%20a%20structured%20latent%20representation%20aligned%20with%20the%20UV%20space%20of%20a%20parametric%20body%20model.%20This%20latent%20code%20is%20decoded%20into%20UV-space%20Gaussians%20that%20can%20be%20animated%20via%20body-driven%20deformation%20and%20rendered%20conditioned%20on%20pose%20and%20viewpoint.%20By%20anchoring%20Gaussians%20to%20the%20UV%20manifold%2C%20our%20method%20ensures%20consistency%20during%20animation%20while%20preserving%20fine%20visual%20details.%20DLA%20enables%20real-time%20rendering%20and%20intuitive%20editing%20without%20requiring%20post-processing.%20Our%20method%20outperforms%20state-of-the-art%20approaches%20on%20the%20ActorsHQ%20and%204D-Dress%20datasets%20in%20both%20perceptual%20quality%20and%20photometric%20accuracy.%20By%20combining%20the%20generative%20strengths%20of%20video%20diffusion%20models%20with%20a%20pose-aware%20UV-space%20Gaussian%20mapping%2C%20DLA%20bridges%20the%20gap%20between%20unstructured%203D%20representations%20and%20high-fidelity%2C%20animation-ready%20avatars.%0ALink%3A%20http%3A//arxiv.org/abs/2507.15979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDream%252C%2520Lift%252C%2520Animate%253A%2520From%2520Single%2520Images%2520to%2520Animatable%2520Gaussian%2520Avatars%26entry.906535625%3DMarcel%2520C.%2520B%25C3%25BChler%2520and%2520Ye%2520Yuan%2520and%2520Xueting%2520Li%2520and%2520Yangyi%2520Huang%2520and%2520Koki%2520Nagano%2520and%2520Umar%2520Iqbal%26entry.1292438233%3DWe%2520introduce%2520Dream%252C%2520Lift%252C%2520Animate%2520%2528DLA%2529%252C%2520a%2520novel%2520framework%2520that%2520reconstructs%2520animatable%25203D%2520human%2520avatars%2520from%2520a%2520single%2520image.%2520This%2520is%2520achieved%2520by%2520leveraging%2520multi-view%2520generation%252C%25203D%2520Gaussian%2520lifting%252C%2520and%2520pose-aware%2520UV-space%2520mapping%2520of%25203D%2520Gaussians.%2520Given%2520an%2520image%252C%2520we%2520first%2520dream%2520plausible%2520multi-views%2520using%2520a%2520video%2520diffusion%2520model%252C%2520capturing%2520rich%2520geometric%2520and%2520appearance%2520details.%2520These%2520views%2520are%2520then%2520lifted%2520into%2520unstructured%25203D%2520Gaussians.%2520To%2520enable%2520animation%252C%2520we%2520propose%2520a%2520transformer-based%2520encoder%2520that%2520models%2520global%2520spatial%2520relationships%2520and%2520projects%2520these%2520Gaussians%2520into%2520a%2520structured%2520latent%2520representation%2520aligned%2520with%2520the%2520UV%2520space%2520of%2520a%2520parametric%2520body%2520model.%2520This%2520latent%2520code%2520is%2520decoded%2520into%2520UV-space%2520Gaussians%2520that%2520can%2520be%2520animated%2520via%2520body-driven%2520deformation%2520and%2520rendered%2520conditioned%2520on%2520pose%2520and%2520viewpoint.%2520By%2520anchoring%2520Gaussians%2520to%2520the%2520UV%2520manifold%252C%2520our%2520method%2520ensures%2520consistency%2520during%2520animation%2520while%2520preserving%2520fine%2520visual%2520details.%2520DLA%2520enables%2520real-time%2520rendering%2520and%2520intuitive%2520editing%2520without%2520requiring%2520post-processing.%2520Our%2520method%2520outperforms%2520state-of-the-art%2520approaches%2520on%2520the%2520ActorsHQ%2520and%25204D-Dress%2520datasets%2520in%2520both%2520perceptual%2520quality%2520and%2520photometric%2520accuracy.%2520By%2520combining%2520the%2520generative%2520strengths%2520of%2520video%2520diffusion%2520models%2520with%2520a%2520pose-aware%2520UV-space%2520Gaussian%2520mapping%252C%2520DLA%2520bridges%2520the%2520gap%2520between%2520unstructured%25203D%2520representations%2520and%2520high-fidelity%252C%2520animation-ready%2520avatars.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dream%2C%20Lift%2C%20Animate%3A%20From%20Single%20Images%20to%20Animatable%20Gaussian%20Avatars&entry.906535625=Marcel%20C.%20B%C3%BChler%20and%20Ye%20Yuan%20and%20Xueting%20Li%20and%20Yangyi%20Huang%20and%20Koki%20Nagano%20and%20Umar%20Iqbal&entry.1292438233=We%20introduce%20Dream%2C%20Lift%2C%20Animate%20%28DLA%29%2C%20a%20novel%20framework%20that%20reconstructs%20animatable%203D%20human%20avatars%20from%20a%20single%20image.%20This%20is%20achieved%20by%20leveraging%20multi-view%20generation%2C%203D%20Gaussian%20lifting%2C%20and%20pose-aware%20UV-space%20mapping%20of%203D%20Gaussians.%20Given%20an%20image%2C%20we%20first%20dream%20plausible%20multi-views%20using%20a%20video%20diffusion%20model%2C%20capturing%20rich%20geometric%20and%20appearance%20details.%20These%20views%20are%20then%20lifted%20into%20unstructured%203D%20Gaussians.%20To%20enable%20animation%2C%20we%20propose%20a%20transformer-based%20encoder%20that%20models%20global%20spatial%20relationships%20and%20projects%20these%20Gaussians%20into%20a%20structured%20latent%20representation%20aligned%20with%20the%20UV%20space%20of%20a%20parametric%20body%20model.%20This%20latent%20code%20is%20decoded%20into%20UV-space%20Gaussians%20that%20can%20be%20animated%20via%20body-driven%20deformation%20and%20rendered%20conditioned%20on%20pose%20and%20viewpoint.%20By%20anchoring%20Gaussians%20to%20the%20UV%20manifold%2C%20our%20method%20ensures%20consistency%20during%20animation%20while%20preserving%20fine%20visual%20details.%20DLA%20enables%20real-time%20rendering%20and%20intuitive%20editing%20without%20requiring%20post-processing.%20Our%20method%20outperforms%20state-of-the-art%20approaches%20on%20the%20ActorsHQ%20and%204D-Dress%20datasets%20in%20both%20perceptual%20quality%20and%20photometric%20accuracy.%20By%20combining%20the%20generative%20strengths%20of%20video%20diffusion%20models%20with%20a%20pose-aware%20UV-space%20Gaussian%20mapping%2C%20DLA%20bridges%20the%20gap%20between%20unstructured%203D%20representations%20and%20high-fidelity%2C%20animation-ready%20avatars.&entry.1838667208=http%3A//arxiv.org/abs/2507.15979v2&entry.124074799=Read"},
{"title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation", "author": "Ziyang Huang and Jiagang Chen and Jin Liu and Shunping Ji", "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.", "link": "http://arxiv.org/abs/2511.13571v1", "date": "2025-11-17", "relevancy": 3.2941, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.68}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6533}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opt3DGS%3A%20Optimizing%203D%20Gaussian%20Splatting%20with%20Adaptive%20Exploration%20and%20Curvature-Aware%20Exploitation&body=Title%3A%20Opt3DGS%3A%20Optimizing%203D%20Gaussian%20Splatting%20with%20Adaptive%20Exploration%20and%20Curvature-Aware%20Exploitation%0AAuthor%3A%20Ziyang%20Huang%20and%20Jiagang%20Chen%20and%20Jin%20Liu%20and%20Shunping%20Ji%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20leading%20framework%20for%20novel%20view%20synthesis%2C%20yet%20its%20core%20optimization%20challenges%20remain%20underexplored.%20We%20identify%20two%20key%20issues%20in%203DGS%20optimization%3A%20entrapment%20in%20suboptimal%20local%20optima%20and%20insufficient%20convergence%20quality.%20To%20address%20these%2C%20we%20propose%20Opt3DGS%2C%20a%20robust%20framework%20that%20enhances%203DGS%20through%20a%20two-stage%20optimization%20process%20of%20adaptive%20exploration%20and%20curvature-guided%20exploitation.%20In%20the%20exploration%20phase%2C%20an%20Adaptive%20Weighted%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29%20method%20enhances%20global%20search%20to%20escape%20local%20optima.%20In%20the%20exploitation%20phase%2C%20a%20Local%20Quasi-Newton%20Direction-guided%20Adam%20optimizer%20leverages%20curvature%20information%20for%20precise%20and%20efficient%20convergence.%20Extensive%20experiments%20on%20diverse%20benchmark%20datasets%20demonstrate%20that%20Opt3DGS%20achieves%20state-of-the-art%20rendering%20quality%20by%20refining%20the%203DGS%20optimization%20process%20without%20modifying%20its%20underlying%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpt3DGS%253A%2520Optimizing%25203D%2520Gaussian%2520Splatting%2520with%2520Adaptive%2520Exploration%2520and%2520Curvature-Aware%2520Exploitation%26entry.906535625%3DZiyang%2520Huang%2520and%2520Jiagang%2520Chen%2520and%2520Jin%2520Liu%2520and%2520Shunping%2520Ji%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520leading%2520framework%2520for%2520novel%2520view%2520synthesis%252C%2520yet%2520its%2520core%2520optimization%2520challenges%2520remain%2520underexplored.%2520We%2520identify%2520two%2520key%2520issues%2520in%25203DGS%2520optimization%253A%2520entrapment%2520in%2520suboptimal%2520local%2520optima%2520and%2520insufficient%2520convergence%2520quality.%2520To%2520address%2520these%252C%2520we%2520propose%2520Opt3DGS%252C%2520a%2520robust%2520framework%2520that%2520enhances%25203DGS%2520through%2520a%2520two-stage%2520optimization%2520process%2520of%2520adaptive%2520exploration%2520and%2520curvature-guided%2520exploitation.%2520In%2520the%2520exploration%2520phase%252C%2520an%2520Adaptive%2520Weighted%2520Stochastic%2520Gradient%2520Langevin%2520Dynamics%2520%2528SGLD%2529%2520method%2520enhances%2520global%2520search%2520to%2520escape%2520local%2520optima.%2520In%2520the%2520exploitation%2520phase%252C%2520a%2520Local%2520Quasi-Newton%2520Direction-guided%2520Adam%2520optimizer%2520leverages%2520curvature%2520information%2520for%2520precise%2520and%2520efficient%2520convergence.%2520Extensive%2520experiments%2520on%2520diverse%2520benchmark%2520datasets%2520demonstrate%2520that%2520Opt3DGS%2520achieves%2520state-of-the-art%2520rendering%2520quality%2520by%2520refining%2520the%25203DGS%2520optimization%2520process%2520without%2520modifying%2520its%2520underlying%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opt3DGS%3A%20Optimizing%203D%20Gaussian%20Splatting%20with%20Adaptive%20Exploration%20and%20Curvature-Aware%20Exploitation&entry.906535625=Ziyang%20Huang%20and%20Jiagang%20Chen%20and%20Jin%20Liu%20and%20Shunping%20Ji&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20leading%20framework%20for%20novel%20view%20synthesis%2C%20yet%20its%20core%20optimization%20challenges%20remain%20underexplored.%20We%20identify%20two%20key%20issues%20in%203DGS%20optimization%3A%20entrapment%20in%20suboptimal%20local%20optima%20and%20insufficient%20convergence%20quality.%20To%20address%20these%2C%20we%20propose%20Opt3DGS%2C%20a%20robust%20framework%20that%20enhances%203DGS%20through%20a%20two-stage%20optimization%20process%20of%20adaptive%20exploration%20and%20curvature-guided%20exploitation.%20In%20the%20exploration%20phase%2C%20an%20Adaptive%20Weighted%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29%20method%20enhances%20global%20search%20to%20escape%20local%20optima.%20In%20the%20exploitation%20phase%2C%20a%20Local%20Quasi-Newton%20Direction-guided%20Adam%20optimizer%20leverages%20curvature%20information%20for%20precise%20and%20efficient%20convergence.%20Extensive%20experiments%20on%20diverse%20benchmark%20datasets%20demonstrate%20that%20Opt3DGS%20achieves%20state-of-the-art%20rendering%20quality%20by%20refining%20the%203DGS%20optimization%20process%20without%20modifying%20its%20underlying%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2511.13571v1&entry.124074799=Read"},
{"title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting", "author": "Zihan Li and Tengfei Wang and Wentian Gan and Hao Zhan and Xin Wang and Zongqian Zhan", "abstract": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/", "link": "http://arxiv.org/abs/2511.13278v1", "date": "2025-11-17", "relevancy": 3.2802, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6848}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6502}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SF-Recon%3A%20Simplification-Free%20Lightweight%20Building%20Reconstruction%20via%203D%20Gaussian%20Splatting&body=Title%3A%20SF-Recon%3A%20Simplification-Free%20Lightweight%20Building%20Reconstruction%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zihan%20Li%20and%20Tengfei%20Wang%20and%20Wentian%20Gan%20and%20Hao%20Zhan%20and%20Xin%20Wang%20and%20Zongqian%20Zhan%0AAbstract%3A%20Lightweight%20building%20surface%20models%20are%20crucial%20for%20digital%20city%2C%20navigation%2C%20and%20fast%20geospatial%20analytics%2C%20yet%20conventional%20multi-view%20geometry%20pipelines%20remain%20cumbersome%20and%20quality-sensitive%20due%20to%20their%20reliance%20on%20dense%20reconstruction%2C%20meshing%2C%20and%20subsequent%20simplification.%20This%20work%20presents%20SF-Recon%2C%20a%20method%20that%20directly%20reconstructs%20lightweight%20building%20surfaces%20from%20multi-view%20images%20without%20post-hoc%20mesh%20simplification.%20We%20first%20train%20an%20initial%203D%20Gaussian%20Splatting%20%283DGS%29%20field%20to%20obtain%20a%20view-consistent%20representation.%20Building%20structure%20is%20then%20distilled%20by%20a%20normal-gradient-guided%20Gaussian%20optimization%20that%20selects%20primitives%20aligned%20with%20roof%20and%20wall%20boundaries%2C%20followed%20by%20multi-view%20edge-consistency%20pruning%20to%20enhance%20structural%20sharpness%20and%20suppress%20non-structural%20artifacts%20without%20external%20supervision.%20Finally%2C%20a%20multi-view%20depth-constrained%20Delaunay%20triangulation%20converts%20the%20structured%20Gaussian%20field%20into%20a%20lightweight%2C%20structurally%20faithful%20building%20mesh.%20Based%20on%20a%20proposed%20SF%20dataset%2C%20the%20experimental%20results%20demonstrate%20that%20our%20SF-Recon%20can%20directly%20reconstruct%20lightweight%20building%20models%20from%20multi-view%20imagery%2C%20achieving%20substantially%20fewer%20faces%20and%20vertices%20while%20maintaining%20computational%20efficiency.%20Website%3Ahttps%3A//lzh282140127-cell.github.io/SF-Recon-project/%0ALink%3A%20http%3A//arxiv.org/abs/2511.13278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSF-Recon%253A%2520Simplification-Free%2520Lightweight%2520Building%2520Reconstruction%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZihan%2520Li%2520and%2520Tengfei%2520Wang%2520and%2520Wentian%2520Gan%2520and%2520Hao%2520Zhan%2520and%2520Xin%2520Wang%2520and%2520Zongqian%2520Zhan%26entry.1292438233%3DLightweight%2520building%2520surface%2520models%2520are%2520crucial%2520for%2520digital%2520city%252C%2520navigation%252C%2520and%2520fast%2520geospatial%2520analytics%252C%2520yet%2520conventional%2520multi-view%2520geometry%2520pipelines%2520remain%2520cumbersome%2520and%2520quality-sensitive%2520due%2520to%2520their%2520reliance%2520on%2520dense%2520reconstruction%252C%2520meshing%252C%2520and%2520subsequent%2520simplification.%2520This%2520work%2520presents%2520SF-Recon%252C%2520a%2520method%2520that%2520directly%2520reconstructs%2520lightweight%2520building%2520surfaces%2520from%2520multi-view%2520images%2520without%2520post-hoc%2520mesh%2520simplification.%2520We%2520first%2520train%2520an%2520initial%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520field%2520to%2520obtain%2520a%2520view-consistent%2520representation.%2520Building%2520structure%2520is%2520then%2520distilled%2520by%2520a%2520normal-gradient-guided%2520Gaussian%2520optimization%2520that%2520selects%2520primitives%2520aligned%2520with%2520roof%2520and%2520wall%2520boundaries%252C%2520followed%2520by%2520multi-view%2520edge-consistency%2520pruning%2520to%2520enhance%2520structural%2520sharpness%2520and%2520suppress%2520non-structural%2520artifacts%2520without%2520external%2520supervision.%2520Finally%252C%2520a%2520multi-view%2520depth-constrained%2520Delaunay%2520triangulation%2520converts%2520the%2520structured%2520Gaussian%2520field%2520into%2520a%2520lightweight%252C%2520structurally%2520faithful%2520building%2520mesh.%2520Based%2520on%2520a%2520proposed%2520SF%2520dataset%252C%2520the%2520experimental%2520results%2520demonstrate%2520that%2520our%2520SF-Recon%2520can%2520directly%2520reconstruct%2520lightweight%2520building%2520models%2520from%2520multi-view%2520imagery%252C%2520achieving%2520substantially%2520fewer%2520faces%2520and%2520vertices%2520while%2520maintaining%2520computational%2520efficiency.%2520Website%253Ahttps%253A//lzh282140127-cell.github.io/SF-Recon-project/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SF-Recon%3A%20Simplification-Free%20Lightweight%20Building%20Reconstruction%20via%203D%20Gaussian%20Splatting&entry.906535625=Zihan%20Li%20and%20Tengfei%20Wang%20and%20Wentian%20Gan%20and%20Hao%20Zhan%20and%20Xin%20Wang%20and%20Zongqian%20Zhan&entry.1292438233=Lightweight%20building%20surface%20models%20are%20crucial%20for%20digital%20city%2C%20navigation%2C%20and%20fast%20geospatial%20analytics%2C%20yet%20conventional%20multi-view%20geometry%20pipelines%20remain%20cumbersome%20and%20quality-sensitive%20due%20to%20their%20reliance%20on%20dense%20reconstruction%2C%20meshing%2C%20and%20subsequent%20simplification.%20This%20work%20presents%20SF-Recon%2C%20a%20method%20that%20directly%20reconstructs%20lightweight%20building%20surfaces%20from%20multi-view%20images%20without%20post-hoc%20mesh%20simplification.%20We%20first%20train%20an%20initial%203D%20Gaussian%20Splatting%20%283DGS%29%20field%20to%20obtain%20a%20view-consistent%20representation.%20Building%20structure%20is%20then%20distilled%20by%20a%20normal-gradient-guided%20Gaussian%20optimization%20that%20selects%20primitives%20aligned%20with%20roof%20and%20wall%20boundaries%2C%20followed%20by%20multi-view%20edge-consistency%20pruning%20to%20enhance%20structural%20sharpness%20and%20suppress%20non-structural%20artifacts%20without%20external%20supervision.%20Finally%2C%20a%20multi-view%20depth-constrained%20Delaunay%20triangulation%20converts%20the%20structured%20Gaussian%20field%20into%20a%20lightweight%2C%20structurally%20faithful%20building%20mesh.%20Based%20on%20a%20proposed%20SF%20dataset%2C%20the%20experimental%20results%20demonstrate%20that%20our%20SF-Recon%20can%20directly%20reconstruct%20lightweight%20building%20models%20from%20multi-view%20imagery%2C%20achieving%20substantially%20fewer%20faces%20and%20vertices%20while%20maintaining%20computational%20efficiency.%20Website%3Ahttps%3A//lzh282140127-cell.github.io/SF-Recon-project/&entry.1838667208=http%3A//arxiv.org/abs/2511.13278v1&entry.124074799=Read"},
{"title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression", "author": "Keshav Gupta and Akshat Sanghvi and Shreyas Reddy Palley and Astitva Srivastava and Charu Sharma and Avinash Sharma", "abstract": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \\textbf{\\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \\textbf{\\color{cyan}{symgs.github.io}}", "link": "http://arxiv.org/abs/2511.13264v1", "date": "2025-11-17", "relevancy": 3.2344, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6674}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6452}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SymGS%20%3A%20Leveraging%20Local%20Symmetries%20for%203D%20Gaussian%20Splatting%20Compression&body=Title%3A%20SymGS%20%3A%20Leveraging%20Local%20Symmetries%20for%203D%20Gaussian%20Splatting%20Compression%0AAuthor%3A%20Keshav%20Gupta%20and%20Akshat%20Sanghvi%20and%20Shreyas%20Reddy%20Palley%20and%20Astitva%20Srivastava%20and%20Charu%20Sharma%20and%20Avinash%20Sharma%0AAbstract%3A%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20transformative%20technique%20in%20novel%20view%20synthesis%2C%20primarily%20due%20to%20its%20high%20rendering%20speed%20and%20photorealistic%20fidelity.%20However%2C%20its%20memory%20footprint%20scales%20rapidly%20with%20scene%20complexity%2C%20often%20reaching%20several%20gigabytes.%20Existing%20methods%20address%20this%20issue%20by%20introducing%20compression%20strategies%20that%20exploit%20primitive-level%20redundancy%20through%20similarity%20detection%20and%20quantization.%20We%20aim%20to%20surpass%20the%20compression%20limits%20of%20such%20methods%20by%20incorporating%20symmetry-aware%20techniques%2C%20specifically%20targeting%20mirror%20symmetries%20to%20eliminate%20redundant%20primitives.%20We%20propose%20a%20novel%20compression%20framework%2C%20%5Ctextbf%7B%5Ctextit%7BSymGS%7D%7D%2C%20introducing%20learnable%20mirrors%20into%20the%20scene%2C%20thereby%20eliminating%20local%20and%20global%20reflective%20redundancies%20for%20compression.%20Our%20framework%20functions%20as%20a%20plug-and-play%20enhancement%20to%20state-of-the-art%20compression%20methods%2C%20%28e.g.%20HAC%29%20to%20achieve%20further%20compression.%20Compared%20to%20HAC%2C%20we%20achieve%20%241.66%20%5Ctimes%24%20compression%20across%20benchmark%20datasets%20%28upto%20%243%5Ctimes%24%20on%20large-scale%20scenes%29.%20On%20an%20average%2C%20SymGS%20enables%20%24%5Cbf%7B108%5Ctimes%7D%24%20compression%20of%20a%203DGS%20scene%2C%20while%20preserving%20rendering%20quality.%20The%20project%20page%20and%20supplementary%20can%20be%20found%20at%20%5Ctextbf%7B%5Ccolor%7Bcyan%7D%7Bsymgs.github.io%7D%7D%0ALink%3A%20http%3A//arxiv.org/abs/2511.13264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymGS%2520%253A%2520Leveraging%2520Local%2520Symmetries%2520for%25203D%2520Gaussian%2520Splatting%2520Compression%26entry.906535625%3DKeshav%2520Gupta%2520and%2520Akshat%2520Sanghvi%2520and%2520Shreyas%2520Reddy%2520Palley%2520and%2520Astitva%2520Srivastava%2520and%2520Charu%2520Sharma%2520and%2520Avinash%2520Sharma%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520transformative%2520technique%2520in%2520novel%2520view%2520synthesis%252C%2520primarily%2520due%2520to%2520its%2520high%2520rendering%2520speed%2520and%2520photorealistic%2520fidelity.%2520However%252C%2520its%2520memory%2520footprint%2520scales%2520rapidly%2520with%2520scene%2520complexity%252C%2520often%2520reaching%2520several%2520gigabytes.%2520Existing%2520methods%2520address%2520this%2520issue%2520by%2520introducing%2520compression%2520strategies%2520that%2520exploit%2520primitive-level%2520redundancy%2520through%2520similarity%2520detection%2520and%2520quantization.%2520We%2520aim%2520to%2520surpass%2520the%2520compression%2520limits%2520of%2520such%2520methods%2520by%2520incorporating%2520symmetry-aware%2520techniques%252C%2520specifically%2520targeting%2520mirror%2520symmetries%2520to%2520eliminate%2520redundant%2520primitives.%2520We%2520propose%2520a%2520novel%2520compression%2520framework%252C%2520%255Ctextbf%257B%255Ctextit%257BSymGS%257D%257D%252C%2520introducing%2520learnable%2520mirrors%2520into%2520the%2520scene%252C%2520thereby%2520eliminating%2520local%2520and%2520global%2520reflective%2520redundancies%2520for%2520compression.%2520Our%2520framework%2520functions%2520as%2520a%2520plug-and-play%2520enhancement%2520to%2520state-of-the-art%2520compression%2520methods%252C%2520%2528e.g.%2520HAC%2529%2520to%2520achieve%2520further%2520compression.%2520Compared%2520to%2520HAC%252C%2520we%2520achieve%2520%25241.66%2520%255Ctimes%2524%2520compression%2520across%2520benchmark%2520datasets%2520%2528upto%2520%25243%255Ctimes%2524%2520on%2520large-scale%2520scenes%2529.%2520On%2520an%2520average%252C%2520SymGS%2520enables%2520%2524%255Cbf%257B108%255Ctimes%257D%2524%2520compression%2520of%2520a%25203DGS%2520scene%252C%2520while%2520preserving%2520rendering%2520quality.%2520The%2520project%2520page%2520and%2520supplementary%2520can%2520be%2520found%2520at%2520%255Ctextbf%257B%255Ccolor%257Bcyan%257D%257Bsymgs.github.io%257D%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SymGS%20%3A%20Leveraging%20Local%20Symmetries%20for%203D%20Gaussian%20Splatting%20Compression&entry.906535625=Keshav%20Gupta%20and%20Akshat%20Sanghvi%20and%20Shreyas%20Reddy%20Palley%20and%20Astitva%20Srivastava%20and%20Charu%20Sharma%20and%20Avinash%20Sharma&entry.1292438233=3D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20transformative%20technique%20in%20novel%20view%20synthesis%2C%20primarily%20due%20to%20its%20high%20rendering%20speed%20and%20photorealistic%20fidelity.%20However%2C%20its%20memory%20footprint%20scales%20rapidly%20with%20scene%20complexity%2C%20often%20reaching%20several%20gigabytes.%20Existing%20methods%20address%20this%20issue%20by%20introducing%20compression%20strategies%20that%20exploit%20primitive-level%20redundancy%20through%20similarity%20detection%20and%20quantization.%20We%20aim%20to%20surpass%20the%20compression%20limits%20of%20such%20methods%20by%20incorporating%20symmetry-aware%20techniques%2C%20specifically%20targeting%20mirror%20symmetries%20to%20eliminate%20redundant%20primitives.%20We%20propose%20a%20novel%20compression%20framework%2C%20%5Ctextbf%7B%5Ctextit%7BSymGS%7D%7D%2C%20introducing%20learnable%20mirrors%20into%20the%20scene%2C%20thereby%20eliminating%20local%20and%20global%20reflective%20redundancies%20for%20compression.%20Our%20framework%20functions%20as%20a%20plug-and-play%20enhancement%20to%20state-of-the-art%20compression%20methods%2C%20%28e.g.%20HAC%29%20to%20achieve%20further%20compression.%20Compared%20to%20HAC%2C%20we%20achieve%20%241.66%20%5Ctimes%24%20compression%20across%20benchmark%20datasets%20%28upto%20%243%5Ctimes%24%20on%20large-scale%20scenes%29.%20On%20an%20average%2C%20SymGS%20enables%20%24%5Cbf%7B108%5Ctimes%7D%24%20compression%20of%20a%203DGS%20scene%2C%20while%20preserving%20rendering%20quality.%20The%20project%20page%20and%20supplementary%20can%20be%20found%20at%20%5Ctextbf%7B%5Ccolor%7Bcyan%7D%7Bsymgs.github.io%7D%7D&entry.1838667208=http%3A//arxiv.org/abs/2511.13264v1&entry.124074799=Read"},
{"title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image", "author": "Ziang Cao and Fangzhou Hong and Zhaoxi Chen and Liang Pan and Ziwei Liu", "abstract": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.", "link": "http://arxiv.org/abs/2511.13648v1", "date": "2025-11-17", "relevancy": 3.1572, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6832}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6201}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysX-Anything%3A%20Simulation-Ready%20Physical%203D%20Assets%20from%20Single%20Image&body=Title%3A%20PhysX-Anything%3A%20Simulation-Ready%20Physical%203D%20Assets%20from%20Single%20Image%0AAuthor%3A%20Ziang%20Cao%20and%20Fangzhou%20Hong%20and%20Zhaoxi%20Chen%20and%20Liang%20Pan%20and%20Ziwei%20Liu%0AAbstract%3A%203D%20modeling%20is%20shifting%20from%20static%20visual%20representations%20toward%20physical%2C%20articulated%20assets%20that%20can%20be%20directly%20used%20in%20simulation%20and%20interaction.%20However%2C%20most%20existing%203D%20generation%20methods%20overlook%20key%20physical%20and%20articulation%20properties%2C%20thereby%20limiting%20their%20utility%20in%20embodied%20AI.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PhysX-Anything%2C%20the%20first%20simulation-ready%20physical%203D%20generative%20framework%20that%2C%20given%20a%20single%20in-the-wild%20image%2C%20produces%20high-quality%20sim-ready%203D%20assets%20with%20explicit%20geometry%2C%20articulation%2C%20and%20physical%20attributes.%20Specifically%2C%20we%20propose%20the%20first%20VLM-based%20physical%203D%20generative%20model%2C%20along%20with%20a%20new%203D%20representation%20that%20efficiently%20tokenizes%20geometry.%20It%20reduces%20the%20number%20of%20tokens%20by%20193x%2C%20enabling%20explicit%20geometry%20learning%20within%20standard%20VLM%20token%20budgets%20without%20introducing%20any%20special%20tokens%20during%20fine-tuning%20and%20significantly%20improving%20generative%20quality.%20In%20addition%2C%20to%20overcome%20the%20limited%20diversity%20of%20existing%20physical%203D%20datasets%2C%20we%20construct%20a%20new%20dataset%2C%20PhysX-Mobility%2C%20which%20expands%20the%20object%20categories%20in%20prior%20physical%203D%20datasets%20by%20over%202x%20and%20includes%20more%20than%202K%20common%20real-world%20objects%20with%20rich%20physical%20annotations.%20Extensive%20experiments%20on%20PhysX-Mobility%20and%20in-the-wild%20images%20demonstrate%20that%20PhysX-Anything%20delivers%20strong%20generative%20performance%20and%20robust%20generalization.%20Furthermore%2C%20simulation-based%20experiments%20in%20a%20MuJoCo-style%20environment%20validate%20that%20our%20sim-ready%20assets%20can%20be%20directly%20used%20for%20contact-rich%20robotic%20policy%20learning.%20We%20believe%20PhysX-Anything%20can%20substantially%20empower%20a%20broad%20range%20of%20downstream%20applications%2C%20especially%20in%20embodied%20AI%20and%20physics-based%20simulation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysX-Anything%253A%2520Simulation-Ready%2520Physical%25203D%2520Assets%2520from%2520Single%2520Image%26entry.906535625%3DZiang%2520Cao%2520and%2520Fangzhou%2520Hong%2520and%2520Zhaoxi%2520Chen%2520and%2520Liang%2520Pan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D3D%2520modeling%2520is%2520shifting%2520from%2520static%2520visual%2520representations%2520toward%2520physical%252C%2520articulated%2520assets%2520that%2520can%2520be%2520directly%2520used%2520in%2520simulation%2520and%2520interaction.%2520However%252C%2520most%2520existing%25203D%2520generation%2520methods%2520overlook%2520key%2520physical%2520and%2520articulation%2520properties%252C%2520thereby%2520limiting%2520their%2520utility%2520in%2520embodied%2520AI.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520PhysX-Anything%252C%2520the%2520first%2520simulation-ready%2520physical%25203D%2520generative%2520framework%2520that%252C%2520given%2520a%2520single%2520in-the-wild%2520image%252C%2520produces%2520high-quality%2520sim-ready%25203D%2520assets%2520with%2520explicit%2520geometry%252C%2520articulation%252C%2520and%2520physical%2520attributes.%2520Specifically%252C%2520we%2520propose%2520the%2520first%2520VLM-based%2520physical%25203D%2520generative%2520model%252C%2520along%2520with%2520a%2520new%25203D%2520representation%2520that%2520efficiently%2520tokenizes%2520geometry.%2520It%2520reduces%2520the%2520number%2520of%2520tokens%2520by%2520193x%252C%2520enabling%2520explicit%2520geometry%2520learning%2520within%2520standard%2520VLM%2520token%2520budgets%2520without%2520introducing%2520any%2520special%2520tokens%2520during%2520fine-tuning%2520and%2520significantly%2520improving%2520generative%2520quality.%2520In%2520addition%252C%2520to%2520overcome%2520the%2520limited%2520diversity%2520of%2520existing%2520physical%25203D%2520datasets%252C%2520we%2520construct%2520a%2520new%2520dataset%252C%2520PhysX-Mobility%252C%2520which%2520expands%2520the%2520object%2520categories%2520in%2520prior%2520physical%25203D%2520datasets%2520by%2520over%25202x%2520and%2520includes%2520more%2520than%25202K%2520common%2520real-world%2520objects%2520with%2520rich%2520physical%2520annotations.%2520Extensive%2520experiments%2520on%2520PhysX-Mobility%2520and%2520in-the-wild%2520images%2520demonstrate%2520that%2520PhysX-Anything%2520delivers%2520strong%2520generative%2520performance%2520and%2520robust%2520generalization.%2520Furthermore%252C%2520simulation-based%2520experiments%2520in%2520a%2520MuJoCo-style%2520environment%2520validate%2520that%2520our%2520sim-ready%2520assets%2520can%2520be%2520directly%2520used%2520for%2520contact-rich%2520robotic%2520policy%2520learning.%2520We%2520believe%2520PhysX-Anything%2520can%2520substantially%2520empower%2520a%2520broad%2520range%2520of%2520downstream%2520applications%252C%2520especially%2520in%2520embodied%2520AI%2520and%2520physics-based%2520simulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysX-Anything%3A%20Simulation-Ready%20Physical%203D%20Assets%20from%20Single%20Image&entry.906535625=Ziang%20Cao%20and%20Fangzhou%20Hong%20and%20Zhaoxi%20Chen%20and%20Liang%20Pan%20and%20Ziwei%20Liu&entry.1292438233=3D%20modeling%20is%20shifting%20from%20static%20visual%20representations%20toward%20physical%2C%20articulated%20assets%20that%20can%20be%20directly%20used%20in%20simulation%20and%20interaction.%20However%2C%20most%20existing%203D%20generation%20methods%20overlook%20key%20physical%20and%20articulation%20properties%2C%20thereby%20limiting%20their%20utility%20in%20embodied%20AI.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PhysX-Anything%2C%20the%20first%20simulation-ready%20physical%203D%20generative%20framework%20that%2C%20given%20a%20single%20in-the-wild%20image%2C%20produces%20high-quality%20sim-ready%203D%20assets%20with%20explicit%20geometry%2C%20articulation%2C%20and%20physical%20attributes.%20Specifically%2C%20we%20propose%20the%20first%20VLM-based%20physical%203D%20generative%20model%2C%20along%20with%20a%20new%203D%20representation%20that%20efficiently%20tokenizes%20geometry.%20It%20reduces%20the%20number%20of%20tokens%20by%20193x%2C%20enabling%20explicit%20geometry%20learning%20within%20standard%20VLM%20token%20budgets%20without%20introducing%20any%20special%20tokens%20during%20fine-tuning%20and%20significantly%20improving%20generative%20quality.%20In%20addition%2C%20to%20overcome%20the%20limited%20diversity%20of%20existing%20physical%203D%20datasets%2C%20we%20construct%20a%20new%20dataset%2C%20PhysX-Mobility%2C%20which%20expands%20the%20object%20categories%20in%20prior%20physical%203D%20datasets%20by%20over%202x%20and%20includes%20more%20than%202K%20common%20real-world%20objects%20with%20rich%20physical%20annotations.%20Extensive%20experiments%20on%20PhysX-Mobility%20and%20in-the-wild%20images%20demonstrate%20that%20PhysX-Anything%20delivers%20strong%20generative%20performance%20and%20robust%20generalization.%20Furthermore%2C%20simulation-based%20experiments%20in%20a%20MuJoCo-style%20environment%20validate%20that%20our%20sim-ready%20assets%20can%20be%20directly%20used%20for%20contact-rich%20robotic%20policy%20learning.%20We%20believe%20PhysX-Anything%20can%20substantially%20empower%20a%20broad%20range%20of%20downstream%20applications%2C%20especially%20in%20embodied%20AI%20and%20physics-based%20simulation.&entry.1838667208=http%3A//arxiv.org/abs/2511.13648v1&entry.124074799=Read"},
{"title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space", "author": "Kaiwen Wang and Kaili Zheng and Yiming Shi and Chenyi Guo and Ji Wu", "abstract": "Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.", "link": "http://arxiv.org/abs/2511.13282v1", "date": "2025-11-17", "relevancy": 3.1463, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6316}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6284}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Metric-Aware%20Multi-Person%20Mesh%20Recovery%20by%20Jointly%20Optimizing%20Human%20Crowd%20in%20Camera%20Space&body=Title%3A%20Towards%20Metric-Aware%20Multi-Person%20Mesh%20Recovery%20by%20Jointly%20Optimizing%20Human%20Crowd%20in%20Camera%20Space%0AAuthor%3A%20Kaiwen%20Wang%20and%20Kaili%20Zheng%20and%20Yiming%20Shi%20and%20Chenyi%20Guo%20and%20Ji%20Wu%0AAbstract%3A%20Multi-person%20human%20mesh%20recovery%20from%20a%20single%20image%20is%20a%20challenging%20task%2C%20hindered%20by%20the%20scarcity%20of%20in-the-wild%20training%20data.%20Prevailing%20in-the-wild%20human%20mesh%20pseudo-ground-truth%20%28pGT%29%20generation%20pipelines%20are%20single-person-centric%2C%20where%20each%20human%20is%20processed%20individually%20without%20joint%20optimization.%20This%20oversight%20leads%20to%20a%20lack%20of%20scene-level%20consistency%2C%20producing%20individuals%20with%20conflicting%20depths%20and%20scales%20within%20the%20same%20image.%20To%20address%20this%2C%20we%20introduce%20Depth-conditioned%20Translation%20Optimization%20%28DTO%29%2C%20a%20novel%20optimization-based%20method%20that%20jointly%20refines%20the%20camera-space%20translations%20of%20all%20individuals%20in%20a%20crowd.%20By%20leveraging%20anthropometric%20priors%20on%20human%20height%20and%20depth%20cues%20from%20a%20monocular%20depth%20estimator%2C%20DTO%20solves%20for%20a%20scene-consistent%20placement%20of%20all%20subjects%20within%20a%20principled%20Maximum%20a%20posteriori%20%28MAP%29%20framework.%20Applying%20DTO%20to%20the%204D-Humans%20dataset%2C%20we%20construct%20DTO-Humans%2C%20a%20new%20large-scale%20pGT%20dataset%20of%200.56M%20high-quality%2C%20scene-consistent%20multi-person%20images%2C%20featuring%20dense%20crowds%20with%20an%20average%20of%204.8%20persons%20per%20image.%20Furthermore%2C%20we%20propose%20Metric-Aware%20HMR%2C%20an%20end-to-end%20network%20that%20directly%20estimates%20human%20mesh%20and%20camera%20parameters%20in%20metric%20scale.%20This%20is%20enabled%20by%20a%20camera%20branch%20and%20a%20novel%20relative%20metric%20loss%20that%20enforces%20plausible%20relative%20scales.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20relative%20depth%20reasoning%20and%20human%20mesh%20recovery.%20Code%20and%20data%20will%20be%20released%20publicly.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Metric-Aware%2520Multi-Person%2520Mesh%2520Recovery%2520by%2520Jointly%2520Optimizing%2520Human%2520Crowd%2520in%2520Camera%2520Space%26entry.906535625%3DKaiwen%2520Wang%2520and%2520Kaili%2520Zheng%2520and%2520Yiming%2520Shi%2520and%2520Chenyi%2520Guo%2520and%2520Ji%2520Wu%26entry.1292438233%3DMulti-person%2520human%2520mesh%2520recovery%2520from%2520a%2520single%2520image%2520is%2520a%2520challenging%2520task%252C%2520hindered%2520by%2520the%2520scarcity%2520of%2520in-the-wild%2520training%2520data.%2520Prevailing%2520in-the-wild%2520human%2520mesh%2520pseudo-ground-truth%2520%2528pGT%2529%2520generation%2520pipelines%2520are%2520single-person-centric%252C%2520where%2520each%2520human%2520is%2520processed%2520individually%2520without%2520joint%2520optimization.%2520This%2520oversight%2520leads%2520to%2520a%2520lack%2520of%2520scene-level%2520consistency%252C%2520producing%2520individuals%2520with%2520conflicting%2520depths%2520and%2520scales%2520within%2520the%2520same%2520image.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Depth-conditioned%2520Translation%2520Optimization%2520%2528DTO%2529%252C%2520a%2520novel%2520optimization-based%2520method%2520that%2520jointly%2520refines%2520the%2520camera-space%2520translations%2520of%2520all%2520individuals%2520in%2520a%2520crowd.%2520By%2520leveraging%2520anthropometric%2520priors%2520on%2520human%2520height%2520and%2520depth%2520cues%2520from%2520a%2520monocular%2520depth%2520estimator%252C%2520DTO%2520solves%2520for%2520a%2520scene-consistent%2520placement%2520of%2520all%2520subjects%2520within%2520a%2520principled%2520Maximum%2520a%2520posteriori%2520%2528MAP%2529%2520framework.%2520Applying%2520DTO%2520to%2520the%25204D-Humans%2520dataset%252C%2520we%2520construct%2520DTO-Humans%252C%2520a%2520new%2520large-scale%2520pGT%2520dataset%2520of%25200.56M%2520high-quality%252C%2520scene-consistent%2520multi-person%2520images%252C%2520featuring%2520dense%2520crowds%2520with%2520an%2520average%2520of%25204.8%2520persons%2520per%2520image.%2520Furthermore%252C%2520we%2520propose%2520Metric-Aware%2520HMR%252C%2520an%2520end-to-end%2520network%2520that%2520directly%2520estimates%2520human%2520mesh%2520and%2520camera%2520parameters%2520in%2520metric%2520scale.%2520This%2520is%2520enabled%2520by%2520a%2520camera%2520branch%2520and%2520a%2520novel%2520relative%2520metric%2520loss%2520that%2520enforces%2520plausible%2520relative%2520scales.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520relative%2520depth%2520reasoning%2520and%2520human%2520mesh%2520recovery.%2520Code%2520and%2520data%2520will%2520be%2520released%2520publicly.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Metric-Aware%20Multi-Person%20Mesh%20Recovery%20by%20Jointly%20Optimizing%20Human%20Crowd%20in%20Camera%20Space&entry.906535625=Kaiwen%20Wang%20and%20Kaili%20Zheng%20and%20Yiming%20Shi%20and%20Chenyi%20Guo%20and%20Ji%20Wu&entry.1292438233=Multi-person%20human%20mesh%20recovery%20from%20a%20single%20image%20is%20a%20challenging%20task%2C%20hindered%20by%20the%20scarcity%20of%20in-the-wild%20training%20data.%20Prevailing%20in-the-wild%20human%20mesh%20pseudo-ground-truth%20%28pGT%29%20generation%20pipelines%20are%20single-person-centric%2C%20where%20each%20human%20is%20processed%20individually%20without%20joint%20optimization.%20This%20oversight%20leads%20to%20a%20lack%20of%20scene-level%20consistency%2C%20producing%20individuals%20with%20conflicting%20depths%20and%20scales%20within%20the%20same%20image.%20To%20address%20this%2C%20we%20introduce%20Depth-conditioned%20Translation%20Optimization%20%28DTO%29%2C%20a%20novel%20optimization-based%20method%20that%20jointly%20refines%20the%20camera-space%20translations%20of%20all%20individuals%20in%20a%20crowd.%20By%20leveraging%20anthropometric%20priors%20on%20human%20height%20and%20depth%20cues%20from%20a%20monocular%20depth%20estimator%2C%20DTO%20solves%20for%20a%20scene-consistent%20placement%20of%20all%20subjects%20within%20a%20principled%20Maximum%20a%20posteriori%20%28MAP%29%20framework.%20Applying%20DTO%20to%20the%204D-Humans%20dataset%2C%20we%20construct%20DTO-Humans%2C%20a%20new%20large-scale%20pGT%20dataset%20of%200.56M%20high-quality%2C%20scene-consistent%20multi-person%20images%2C%20featuring%20dense%20crowds%20with%20an%20average%20of%204.8%20persons%20per%20image.%20Furthermore%2C%20we%20propose%20Metric-Aware%20HMR%2C%20an%20end-to-end%20network%20that%20directly%20estimates%20human%20mesh%20and%20camera%20parameters%20in%20metric%20scale.%20This%20is%20enabled%20by%20a%20camera%20branch%20and%20a%20novel%20relative%20metric%20loss%20that%20enforces%20plausible%20relative%20scales.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%20relative%20depth%20reasoning%20and%20human%20mesh%20recovery.%20Code%20and%20data%20will%20be%20released%20publicly.&entry.1838667208=http%3A//arxiv.org/abs/2511.13282v1&entry.124074799=Read"},
{"title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS", "author": "Weijie Wang and Donny Y. Chen and Zeyu Zhang and Duochao Shi and Akide Liu and Bohan Zhuang", "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.", "link": "http://arxiv.org/abs/2505.23734v4", "date": "2025-11-17", "relevancy": 3.0547, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6197}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6184}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZPressor%3A%20Bottleneck-Aware%20Compression%20for%20Scalable%20Feed-Forward%203DGS&body=Title%3A%20ZPressor%3A%20Bottleneck-Aware%20Compression%20for%20Scalable%20Feed-Forward%203DGS%0AAuthor%3A%20Weijie%20Wang%20and%20Donny%20Y.%20Chen%20and%20Zeyu%20Zhang%20and%20Duochao%20Shi%20and%20Akide%20Liu%20and%20Bohan%20Zhuang%0AAbstract%3A%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20have%20recently%20emerged%20as%20a%20promising%20solution%20for%20novel%20view%20synthesis%2C%20enabling%20one-pass%20inference%20without%20the%20need%20for%20per-scene%203DGS%20optimization.%20However%2C%20their%20scalability%20is%20fundamentally%20constrained%20by%20the%20limited%20capacity%20of%20their%20models%2C%20leading%20to%20degraded%20performance%20or%20excessive%20memory%20consumption%20as%20the%20number%20of%20input%20views%20increases.%20In%20this%20work%2C%20we%20analyze%20feed-forward%203DGS%20frameworks%20through%20the%20lens%20of%20the%20Information%20Bottleneck%20principle%20and%20introduce%20ZPressor%2C%20a%20lightweight%20architecture-agnostic%20module%20that%20enables%20efficient%20compression%20of%20multi-view%20inputs%20into%20a%20compact%20latent%20state%20%24Z%24%20that%20retains%20essential%20scene%20information%20while%20discarding%20redundancy.%20Concretely%2C%20ZPressor%20enables%20existing%20feed-forward%203DGS%20models%20to%20scale%20to%20over%20100%20input%20views%20at%20480P%20resolution%20on%20an%2080GB%20GPU%2C%20by%20partitioning%20the%20views%20into%20anchor%20and%20support%20sets%20and%20using%20cross%20attention%20to%20compress%20the%20information%20from%20the%20support%20views%20into%20anchor%20views%2C%20forming%20the%20compressed%20latent%20state%20%24Z%24.%20We%20show%20that%20integrating%20ZPressor%20into%20several%20state-of-the-art%20feed-forward%203DGS%20models%20consistently%20improves%20performance%20under%20moderate%20input%20views%20and%20enhances%20robustness%20under%20dense%20view%20settings%20on%20two%20large-scale%20benchmarks%20DL3DV-10K%20and%20RealEstate10K.%20The%20video%20results%2C%20code%20and%20trained%20models%20are%20available%20on%20our%20project%20page%3A%20https%3A//lhmd.top/zpressor.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23734v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZPressor%253A%2520Bottleneck-Aware%2520Compression%2520for%2520Scalable%2520Feed-Forward%25203DGS%26entry.906535625%3DWeijie%2520Wang%2520and%2520Donny%2520Y.%2520Chen%2520and%2520Zeyu%2520Zhang%2520and%2520Duochao%2520Shi%2520and%2520Akide%2520Liu%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3DFeed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520novel%2520view%2520synthesis%252C%2520enabling%2520one-pass%2520inference%2520without%2520the%2520need%2520for%2520per-scene%25203DGS%2520optimization.%2520However%252C%2520their%2520scalability%2520is%2520fundamentally%2520constrained%2520by%2520the%2520limited%2520capacity%2520of%2520their%2520models%252C%2520leading%2520to%2520degraded%2520performance%2520or%2520excessive%2520memory%2520consumption%2520as%2520the%2520number%2520of%2520input%2520views%2520increases.%2520In%2520this%2520work%252C%2520we%2520analyze%2520feed-forward%25203DGS%2520frameworks%2520through%2520the%2520lens%2520of%2520the%2520Information%2520Bottleneck%2520principle%2520and%2520introduce%2520ZPressor%252C%2520a%2520lightweight%2520architecture-agnostic%2520module%2520that%2520enables%2520efficient%2520compression%2520of%2520multi-view%2520inputs%2520into%2520a%2520compact%2520latent%2520state%2520%2524Z%2524%2520that%2520retains%2520essential%2520scene%2520information%2520while%2520discarding%2520redundancy.%2520Concretely%252C%2520ZPressor%2520enables%2520existing%2520feed-forward%25203DGS%2520models%2520to%2520scale%2520to%2520over%2520100%2520input%2520views%2520at%2520480P%2520resolution%2520on%2520an%252080GB%2520GPU%252C%2520by%2520partitioning%2520the%2520views%2520into%2520anchor%2520and%2520support%2520sets%2520and%2520using%2520cross%2520attention%2520to%2520compress%2520the%2520information%2520from%2520the%2520support%2520views%2520into%2520anchor%2520views%252C%2520forming%2520the%2520compressed%2520latent%2520state%2520%2524Z%2524.%2520We%2520show%2520that%2520integrating%2520ZPressor%2520into%2520several%2520state-of-the-art%2520feed-forward%25203DGS%2520models%2520consistently%2520improves%2520performance%2520under%2520moderate%2520input%2520views%2520and%2520enhances%2520robustness%2520under%2520dense%2520view%2520settings%2520on%2520two%2520large-scale%2520benchmarks%2520DL3DV-10K%2520and%2520RealEstate10K.%2520The%2520video%2520results%252C%2520code%2520and%2520trained%2520models%2520are%2520available%2520on%2520our%2520project%2520page%253A%2520https%253A//lhmd.top/zpressor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23734v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZPressor%3A%20Bottleneck-Aware%20Compression%20for%20Scalable%20Feed-Forward%203DGS&entry.906535625=Weijie%20Wang%20and%20Donny%20Y.%20Chen%20and%20Zeyu%20Zhang%20and%20Duochao%20Shi%20and%20Akide%20Liu%20and%20Bohan%20Zhuang&entry.1292438233=Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20have%20recently%20emerged%20as%20a%20promising%20solution%20for%20novel%20view%20synthesis%2C%20enabling%20one-pass%20inference%20without%20the%20need%20for%20per-scene%203DGS%20optimization.%20However%2C%20their%20scalability%20is%20fundamentally%20constrained%20by%20the%20limited%20capacity%20of%20their%20models%2C%20leading%20to%20degraded%20performance%20or%20excessive%20memory%20consumption%20as%20the%20number%20of%20input%20views%20increases.%20In%20this%20work%2C%20we%20analyze%20feed-forward%203DGS%20frameworks%20through%20the%20lens%20of%20the%20Information%20Bottleneck%20principle%20and%20introduce%20ZPressor%2C%20a%20lightweight%20architecture-agnostic%20module%20that%20enables%20efficient%20compression%20of%20multi-view%20inputs%20into%20a%20compact%20latent%20state%20%24Z%24%20that%20retains%20essential%20scene%20information%20while%20discarding%20redundancy.%20Concretely%2C%20ZPressor%20enables%20existing%20feed-forward%203DGS%20models%20to%20scale%20to%20over%20100%20input%20views%20at%20480P%20resolution%20on%20an%2080GB%20GPU%2C%20by%20partitioning%20the%20views%20into%20anchor%20and%20support%20sets%20and%20using%20cross%20attention%20to%20compress%20the%20information%20from%20the%20support%20views%20into%20anchor%20views%2C%20forming%20the%20compressed%20latent%20state%20%24Z%24.%20We%20show%20that%20integrating%20ZPressor%20into%20several%20state-of-the-art%20feed-forward%203DGS%20models%20consistently%20improves%20performance%20under%20moderate%20input%20views%20and%20enhances%20robustness%20under%20dense%20view%20settings%20on%20two%20large-scale%20benchmarks%20DL3DV-10K%20and%20RealEstate10K.%20The%20video%20results%2C%20code%20and%20trained%20models%20are%20available%20on%20our%20project%20page%3A%20https%3A//lhmd.top/zpressor.&entry.1838667208=http%3A//arxiv.org/abs/2505.23734v4&entry.124074799=Read"},
{"title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework", "author": "Diego Ortego and Marlon Rodr\u00edguez and Mario Almagro and Kunal Dahiya and David Jim\u00e9nez and Juan C. SanMiguel", "abstract": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.", "link": "http://arxiv.org/abs/2511.13189v1", "date": "2025-11-17", "relevancy": 3.0015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Meet%20Extreme%20Multi-label%20Classification%3A%20Scaling%20and%20Multi-modal%20Framework&body=Title%3A%20Large%20Language%20Models%20Meet%20Extreme%20Multi-label%20Classification%3A%20Scaling%20and%20Multi-modal%20Framework%0AAuthor%3A%20Diego%20Ortego%20and%20Marlon%20Rodr%C3%ADguez%20and%20Mario%20Almagro%20and%20Kunal%20Dahiya%20and%20David%20Jim%C3%A9nez%20and%20Juan%20C.%20SanMiguel%0AAbstract%3A%20Foundation%20models%20have%20revolutionized%20artificial%20intelligence%20across%20numerous%20domains%2C%20yet%20their%20transformative%20potential%20remains%20largely%20untapped%20in%20Extreme%20Multi-label%20Classification%20%28XMC%29.%20Queries%20in%20XMC%20are%20associated%20with%20relevant%20labels%20from%20extremely%20large%20label%20spaces%2C%20where%20it%20is%20critical%20to%20strike%20a%20balance%20between%20efficiency%20and%20performance.%20Therefore%2C%20many%20recent%20approaches%20efficiently%20pose%20XMC%20as%20a%20maximum%20inner%20product%20search%20between%20embeddings%20learned%20from%20small%20encoder-only%20transformer%20architectures.%20In%20this%20paper%2C%20we%20address%20two%20important%20aspects%20in%20XMC%3A%20how%20to%20effectively%20harness%20larger%20decoder-only%20models%2C%20and%20how%20to%20exploit%20visual%20information%20while%20maintaining%20computational%20efficiency.%20We%20demonstrate%20that%20both%20play%20a%20critical%20role%20in%20XMC%20separately%20and%20can%20be%20combined%20for%20improved%20performance.%20We%20show%20that%20a%20few%20billion-size%20decoder%20can%20deliver%20substantial%20improvements%20while%20keeping%20computational%20overhead%20manageable.%20Furthermore%2C%20our%20Vision-enhanced%20eXtreme%20Multi-label%20Learning%20framework%20%28ViXML%29%20efficiently%20integrates%20foundation%20vision%20models%20by%20pooling%20a%20single%20embedding%20per%20image.%20This%20limits%20computational%20growth%20while%20unlocking%20multi-modal%20capabilities.%20Remarkably%2C%20ViXML%20with%20small%20encoders%20outperforms%20text-only%20decoder%20in%20most%20cases%2C%20showing%20that%20an%20image%20is%20worth%20billions%20of%20parameters.%20Finally%2C%20we%20present%20an%20extension%20of%20existing%20text-only%20datasets%20to%20exploit%20visual%20metadata%20and%20make%20them%20available%20for%20future%20benchmarking.%20Comprehensive%20experiments%20across%20four%20public%20text-only%20datasets%20and%20their%20corresponding%20image%20enhanced%20versions%20validate%20our%20proposals%27%20effectiveness%2C%20surpassing%20previous%20state-of-the-art%20by%20up%20to%20%2B8.21%5C%25%20in%20P%401%20on%20the%20largest%20dataset.%20ViXML%27s%20code%20is%20available%20at%20https%3A//github.com/DiegoOrtego/vixml.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Meet%2520Extreme%2520Multi-label%2520Classification%253A%2520Scaling%2520and%2520Multi-modal%2520Framework%26entry.906535625%3DDiego%2520Ortego%2520and%2520Marlon%2520Rodr%25C3%25ADguez%2520and%2520Mario%2520Almagro%2520and%2520Kunal%2520Dahiya%2520and%2520David%2520Jim%25C3%25A9nez%2520and%2520Juan%2520C.%2520SanMiguel%26entry.1292438233%3DFoundation%2520models%2520have%2520revolutionized%2520artificial%2520intelligence%2520across%2520numerous%2520domains%252C%2520yet%2520their%2520transformative%2520potential%2520remains%2520largely%2520untapped%2520in%2520Extreme%2520Multi-label%2520Classification%2520%2528XMC%2529.%2520Queries%2520in%2520XMC%2520are%2520associated%2520with%2520relevant%2520labels%2520from%2520extremely%2520large%2520label%2520spaces%252C%2520where%2520it%2520is%2520critical%2520to%2520strike%2520a%2520balance%2520between%2520efficiency%2520and%2520performance.%2520Therefore%252C%2520many%2520recent%2520approaches%2520efficiently%2520pose%2520XMC%2520as%2520a%2520maximum%2520inner%2520product%2520search%2520between%2520embeddings%2520learned%2520from%2520small%2520encoder-only%2520transformer%2520architectures.%2520In%2520this%2520paper%252C%2520we%2520address%2520two%2520important%2520aspects%2520in%2520XMC%253A%2520how%2520to%2520effectively%2520harness%2520larger%2520decoder-only%2520models%252C%2520and%2520how%2520to%2520exploit%2520visual%2520information%2520while%2520maintaining%2520computational%2520efficiency.%2520We%2520demonstrate%2520that%2520both%2520play%2520a%2520critical%2520role%2520in%2520XMC%2520separately%2520and%2520can%2520be%2520combined%2520for%2520improved%2520performance.%2520We%2520show%2520that%2520a%2520few%2520billion-size%2520decoder%2520can%2520deliver%2520substantial%2520improvements%2520while%2520keeping%2520computational%2520overhead%2520manageable.%2520Furthermore%252C%2520our%2520Vision-enhanced%2520eXtreme%2520Multi-label%2520Learning%2520framework%2520%2528ViXML%2529%2520efficiently%2520integrates%2520foundation%2520vision%2520models%2520by%2520pooling%2520a%2520single%2520embedding%2520per%2520image.%2520This%2520limits%2520computational%2520growth%2520while%2520unlocking%2520multi-modal%2520capabilities.%2520Remarkably%252C%2520ViXML%2520with%2520small%2520encoders%2520outperforms%2520text-only%2520decoder%2520in%2520most%2520cases%252C%2520showing%2520that%2520an%2520image%2520is%2520worth%2520billions%2520of%2520parameters.%2520Finally%252C%2520we%2520present%2520an%2520extension%2520of%2520existing%2520text-only%2520datasets%2520to%2520exploit%2520visual%2520metadata%2520and%2520make%2520them%2520available%2520for%2520future%2520benchmarking.%2520Comprehensive%2520experiments%2520across%2520four%2520public%2520text-only%2520datasets%2520and%2520their%2520corresponding%2520image%2520enhanced%2520versions%2520validate%2520our%2520proposals%2527%2520effectiveness%252C%2520surpassing%2520previous%2520state-of-the-art%2520by%2520up%2520to%2520%252B8.21%255C%2525%2520in%2520P%25401%2520on%2520the%2520largest%2520dataset.%2520ViXML%2527s%2520code%2520is%2520available%2520at%2520https%253A//github.com/DiegoOrtego/vixml.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Meet%20Extreme%20Multi-label%20Classification%3A%20Scaling%20and%20Multi-modal%20Framework&entry.906535625=Diego%20Ortego%20and%20Marlon%20Rodr%C3%ADguez%20and%20Mario%20Almagro%20and%20Kunal%20Dahiya%20and%20David%20Jim%C3%A9nez%20and%20Juan%20C.%20SanMiguel&entry.1292438233=Foundation%20models%20have%20revolutionized%20artificial%20intelligence%20across%20numerous%20domains%2C%20yet%20their%20transformative%20potential%20remains%20largely%20untapped%20in%20Extreme%20Multi-label%20Classification%20%28XMC%29.%20Queries%20in%20XMC%20are%20associated%20with%20relevant%20labels%20from%20extremely%20large%20label%20spaces%2C%20where%20it%20is%20critical%20to%20strike%20a%20balance%20between%20efficiency%20and%20performance.%20Therefore%2C%20many%20recent%20approaches%20efficiently%20pose%20XMC%20as%20a%20maximum%20inner%20product%20search%20between%20embeddings%20learned%20from%20small%20encoder-only%20transformer%20architectures.%20In%20this%20paper%2C%20we%20address%20two%20important%20aspects%20in%20XMC%3A%20how%20to%20effectively%20harness%20larger%20decoder-only%20models%2C%20and%20how%20to%20exploit%20visual%20information%20while%20maintaining%20computational%20efficiency.%20We%20demonstrate%20that%20both%20play%20a%20critical%20role%20in%20XMC%20separately%20and%20can%20be%20combined%20for%20improved%20performance.%20We%20show%20that%20a%20few%20billion-size%20decoder%20can%20deliver%20substantial%20improvements%20while%20keeping%20computational%20overhead%20manageable.%20Furthermore%2C%20our%20Vision-enhanced%20eXtreme%20Multi-label%20Learning%20framework%20%28ViXML%29%20efficiently%20integrates%20foundation%20vision%20models%20by%20pooling%20a%20single%20embedding%20per%20image.%20This%20limits%20computational%20growth%20while%20unlocking%20multi-modal%20capabilities.%20Remarkably%2C%20ViXML%20with%20small%20encoders%20outperforms%20text-only%20decoder%20in%20most%20cases%2C%20showing%20that%20an%20image%20is%20worth%20billions%20of%20parameters.%20Finally%2C%20we%20present%20an%20extension%20of%20existing%20text-only%20datasets%20to%20exploit%20visual%20metadata%20and%20make%20them%20available%20for%20future%20benchmarking.%20Comprehensive%20experiments%20across%20four%20public%20text-only%20datasets%20and%20their%20corresponding%20image%20enhanced%20versions%20validate%20our%20proposals%27%20effectiveness%2C%20surpassing%20previous%20state-of-the-art%20by%20up%20to%20%2B8.21%5C%25%20in%20P%401%20on%20the%20largest%20dataset.%20ViXML%27s%20code%20is%20available%20at%20https%3A//github.com/DiegoOrtego/vixml.&entry.1838667208=http%3A//arxiv.org/abs/2511.13189v1&entry.124074799=Read"},
{"title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model", "author": "Chunshi Wang and Junliang Ye and Yunhan Yang and Yang Li and Zizhuo Lin and Jun Zhu and Zhuo Chen and Yawei Luo and Chunchao Guo", "abstract": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/", "link": "http://arxiv.org/abs/2511.13647v1", "date": "2025-11-17", "relevancy": 2.9859, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Part-X-MLLM%3A%20Part-aware%203D%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Part-X-MLLM%3A%20Part-aware%203D%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Chunshi%20Wang%20and%20Junliang%20Ye%20and%20Yunhan%20Yang%20and%20Yang%20Li%20and%20Zizhuo%20Lin%20and%20Jun%20Zhu%20and%20Zhuo%20Chen%20and%20Yawei%20Luo%20and%20Chunchao%20Guo%0AAbstract%3A%20We%20introduce%20Part-X-MLLM%2C%20a%20native%203D%20multimodal%20large%20language%20model%20that%20unifies%20diverse%203D%20tasks%20by%20formulating%20them%20as%20programs%20in%20a%20structured%2C%20executable%20grammar.%20Given%20an%20RGB%20point%20cloud%20and%20a%20natural%20language%20prompt%2C%20our%20model%20autoregressively%20generates%20a%20single%2C%20coherent%20token%20sequence%20encoding%20part-level%20bounding%20boxes%2C%20semantic%20descriptions%2C%20and%20edit%20commands.%20This%20structured%20output%20serves%20as%20a%20versatile%20interface%20to%20drive%20downstream%20geometry-aware%20modules%20for%20part-based%20generation%20and%20editing.%20By%20decoupling%20the%20symbolic%20planning%20from%20the%20geometric%20synthesis%2C%20our%20approach%20allows%20any%20compatible%20geometry%20engine%20to%20be%20controlled%20through%20a%20single%2C%20language-native%20frontend.%20We%20pre-train%20a%20dual-encoder%20architecture%20to%20disentangle%20structure%20from%20semantics%20and%20instruction-tune%20the%20model%20on%20a%20large-scale%2C%20part-centric%20dataset.%20Experiments%20demonstrate%20that%20our%20model%20excels%20at%20producing%20high-quality%2C%20structured%20plans%2C%20enabling%20state-of-the-art%20performance%20in%20grounded%20Q%5C%26A%2C%20compositional%20generation%2C%20and%20localized%20editing%20through%20one%20unified%20interface.%20Project%20page%3A%20https%3A//chunshi.wang/Part-X-MLLM/%0ALink%3A%20http%3A//arxiv.org/abs/2511.13647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPart-X-MLLM%253A%2520Part-aware%25203D%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DChunshi%2520Wang%2520and%2520Junliang%2520Ye%2520and%2520Yunhan%2520Yang%2520and%2520Yang%2520Li%2520and%2520Zizhuo%2520Lin%2520and%2520Jun%2520Zhu%2520and%2520Zhuo%2520Chen%2520and%2520Yawei%2520Luo%2520and%2520Chunchao%2520Guo%26entry.1292438233%3DWe%2520introduce%2520Part-X-MLLM%252C%2520a%2520native%25203D%2520multimodal%2520large%2520language%2520model%2520that%2520unifies%2520diverse%25203D%2520tasks%2520by%2520formulating%2520them%2520as%2520programs%2520in%2520a%2520structured%252C%2520executable%2520grammar.%2520Given%2520an%2520RGB%2520point%2520cloud%2520and%2520a%2520natural%2520language%2520prompt%252C%2520our%2520model%2520autoregressively%2520generates%2520a%2520single%252C%2520coherent%2520token%2520sequence%2520encoding%2520part-level%2520bounding%2520boxes%252C%2520semantic%2520descriptions%252C%2520and%2520edit%2520commands.%2520This%2520structured%2520output%2520serves%2520as%2520a%2520versatile%2520interface%2520to%2520drive%2520downstream%2520geometry-aware%2520modules%2520for%2520part-based%2520generation%2520and%2520editing.%2520By%2520decoupling%2520the%2520symbolic%2520planning%2520from%2520the%2520geometric%2520synthesis%252C%2520our%2520approach%2520allows%2520any%2520compatible%2520geometry%2520engine%2520to%2520be%2520controlled%2520through%2520a%2520single%252C%2520language-native%2520frontend.%2520We%2520pre-train%2520a%2520dual-encoder%2520architecture%2520to%2520disentangle%2520structure%2520from%2520semantics%2520and%2520instruction-tune%2520the%2520model%2520on%2520a%2520large-scale%252C%2520part-centric%2520dataset.%2520Experiments%2520demonstrate%2520that%2520our%2520model%2520excels%2520at%2520producing%2520high-quality%252C%2520structured%2520plans%252C%2520enabling%2520state-of-the-art%2520performance%2520in%2520grounded%2520Q%255C%2526A%252C%2520compositional%2520generation%252C%2520and%2520localized%2520editing%2520through%2520one%2520unified%2520interface.%2520Project%2520page%253A%2520https%253A//chunshi.wang/Part-X-MLLM/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-X-MLLM%3A%20Part-aware%203D%20Multimodal%20Large%20Language%20Model&entry.906535625=Chunshi%20Wang%20and%20Junliang%20Ye%20and%20Yunhan%20Yang%20and%20Yang%20Li%20and%20Zizhuo%20Lin%20and%20Jun%20Zhu%20and%20Zhuo%20Chen%20and%20Yawei%20Luo%20and%20Chunchao%20Guo&entry.1292438233=We%20introduce%20Part-X-MLLM%2C%20a%20native%203D%20multimodal%20large%20language%20model%20that%20unifies%20diverse%203D%20tasks%20by%20formulating%20them%20as%20programs%20in%20a%20structured%2C%20executable%20grammar.%20Given%20an%20RGB%20point%20cloud%20and%20a%20natural%20language%20prompt%2C%20our%20model%20autoregressively%20generates%20a%20single%2C%20coherent%20token%20sequence%20encoding%20part-level%20bounding%20boxes%2C%20semantic%20descriptions%2C%20and%20edit%20commands.%20This%20structured%20output%20serves%20as%20a%20versatile%20interface%20to%20drive%20downstream%20geometry-aware%20modules%20for%20part-based%20generation%20and%20editing.%20By%20decoupling%20the%20symbolic%20planning%20from%20the%20geometric%20synthesis%2C%20our%20approach%20allows%20any%20compatible%20geometry%20engine%20to%20be%20controlled%20through%20a%20single%2C%20language-native%20frontend.%20We%20pre-train%20a%20dual-encoder%20architecture%20to%20disentangle%20structure%20from%20semantics%20and%20instruction-tune%20the%20model%20on%20a%20large-scale%2C%20part-centric%20dataset.%20Experiments%20demonstrate%20that%20our%20model%20excels%20at%20producing%20high-quality%2C%20structured%20plans%2C%20enabling%20state-of-the-art%20performance%20in%20grounded%20Q%5C%26A%2C%20compositional%20generation%2C%20and%20localized%20editing%20through%20one%20unified%20interface.%20Project%20page%3A%20https%3A//chunshi.wang/Part-X-MLLM/&entry.1838667208=http%3A//arxiv.org/abs/2511.13647v1&entry.124074799=Read"},
{"title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer", "author": "Yonghui Yu and Jiahang Cai and Xun Wang and Wenwu Yang", "abstract": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \\textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet", "link": "http://arxiv.org/abs/2511.13208v1", "date": "2025-11-17", "relevancy": 2.9824, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6229}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6023}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Multi-Person%20Pose%20Estimation%20with%20Pose-Aware%20Video%20Transformer&body=Title%3A%20End-to-End%20Multi-Person%20Pose%20Estimation%20with%20Pose-Aware%20Video%20Transformer%0AAuthor%3A%20Yonghui%20Yu%20and%20Jiahang%20Cai%20and%20Xun%20Wang%20and%20Wenwu%20Yang%0AAbstract%3A%20Existing%20multi-person%20video%20pose%20estimation%20methods%20typically%20adopt%20a%20two-stage%20pipeline%3A%20detecting%20individuals%20in%20each%20frame%2C%20followed%20by%20temporal%20modeling%20for%20single-person%20pose%20estimation.%20This%20design%20relies%20on%20heuristic%20operations%20such%20as%20detection%2C%20RoI%20cropping%2C%20and%20non-maximum%20suppression%20%28NMS%29%2C%20limiting%20both%20accuracy%20and%20efficiency.%20In%20this%20paper%2C%20we%20present%20a%20fully%20end-to-end%20framework%20for%20multi-person%202D%20pose%20estimation%20in%20videos%2C%20effectively%20eliminating%20heuristic%20operations.%20A%20key%20challenge%20is%20to%20associate%20individuals%20across%20frames%20under%20complex%20and%20overlapping%20temporal%20trajectories.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20Pose-Aware%20Video%20transformEr%20Network%20%28PAVE-Net%29%2C%20which%20features%20a%20spatial%20encoder%20to%20model%20intra-frame%20relations%20and%20a%20spatiotemporal%20pose%20decoder%20to%20capture%20global%20dependencies%20across%20frames.%20To%20achieve%20accurate%20temporal%20association%2C%20we%20propose%20a%20pose-aware%20attention%20mechanism%20that%20enables%20each%20pose%20query%20to%20selectively%20aggregate%20features%20corresponding%20to%20the%20same%20individual%20across%20consecutive%20frames.Additionally%2C%20we%20explicitly%20model%20spatiotemporal%20dependencies%20among%20pose%20keypoints%20to%20improve%20accuracy.%20Notably%2C%20our%20approach%20is%20the%20first%20end-to-end%20method%20for%20multi-frame%202D%20human%20pose%20estimation.Extensive%20experiments%20show%20that%20PAVE-Net%20substantially%20outperforms%20prior%20image-based%20end-to-end%20methods%2C%20achieving%20a%20%5Ctextbf%7B6.0%7D%20mAP%20improvement%20on%20PoseTrack2017%2C%20and%20delivers%20accuracy%20competitive%20with%20state-of-the-art%20two-stage%20video-based%20approaches%2C%20while%20offering%20significant%20gains%20in%20efficiency.Project%20page%3A%20https%3A//github.com/zgspose/PAVENet%0ALink%3A%20http%3A//arxiv.org/abs/2511.13208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Multi-Person%2520Pose%2520Estimation%2520with%2520Pose-Aware%2520Video%2520Transformer%26entry.906535625%3DYonghui%2520Yu%2520and%2520Jiahang%2520Cai%2520and%2520Xun%2520Wang%2520and%2520Wenwu%2520Yang%26entry.1292438233%3DExisting%2520multi-person%2520video%2520pose%2520estimation%2520methods%2520typically%2520adopt%2520a%2520two-stage%2520pipeline%253A%2520detecting%2520individuals%2520in%2520each%2520frame%252C%2520followed%2520by%2520temporal%2520modeling%2520for%2520single-person%2520pose%2520estimation.%2520This%2520design%2520relies%2520on%2520heuristic%2520operations%2520such%2520as%2520detection%252C%2520RoI%2520cropping%252C%2520and%2520non-maximum%2520suppression%2520%2528NMS%2529%252C%2520limiting%2520both%2520accuracy%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520fully%2520end-to-end%2520framework%2520for%2520multi-person%25202D%2520pose%2520estimation%2520in%2520videos%252C%2520effectively%2520eliminating%2520heuristic%2520operations.%2520A%2520key%2520challenge%2520is%2520to%2520associate%2520individuals%2520across%2520frames%2520under%2520complex%2520and%2520overlapping%2520temporal%2520trajectories.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520Pose-Aware%2520Video%2520transformEr%2520Network%2520%2528PAVE-Net%2529%252C%2520which%2520features%2520a%2520spatial%2520encoder%2520to%2520model%2520intra-frame%2520relations%2520and%2520a%2520spatiotemporal%2520pose%2520decoder%2520to%2520capture%2520global%2520dependencies%2520across%2520frames.%2520To%2520achieve%2520accurate%2520temporal%2520association%252C%2520we%2520propose%2520a%2520pose-aware%2520attention%2520mechanism%2520that%2520enables%2520each%2520pose%2520query%2520to%2520selectively%2520aggregate%2520features%2520corresponding%2520to%2520the%2520same%2520individual%2520across%2520consecutive%2520frames.Additionally%252C%2520we%2520explicitly%2520model%2520spatiotemporal%2520dependencies%2520among%2520pose%2520keypoints%2520to%2520improve%2520accuracy.%2520Notably%252C%2520our%2520approach%2520is%2520the%2520first%2520end-to-end%2520method%2520for%2520multi-frame%25202D%2520human%2520pose%2520estimation.Extensive%2520experiments%2520show%2520that%2520PAVE-Net%2520substantially%2520outperforms%2520prior%2520image-based%2520end-to-end%2520methods%252C%2520achieving%2520a%2520%255Ctextbf%257B6.0%257D%2520mAP%2520improvement%2520on%2520PoseTrack2017%252C%2520and%2520delivers%2520accuracy%2520competitive%2520with%2520state-of-the-art%2520two-stage%2520video-based%2520approaches%252C%2520while%2520offering%2520significant%2520gains%2520in%2520efficiency.Project%2520page%253A%2520https%253A//github.com/zgspose/PAVENet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Multi-Person%20Pose%20Estimation%20with%20Pose-Aware%20Video%20Transformer&entry.906535625=Yonghui%20Yu%20and%20Jiahang%20Cai%20and%20Xun%20Wang%20and%20Wenwu%20Yang&entry.1292438233=Existing%20multi-person%20video%20pose%20estimation%20methods%20typically%20adopt%20a%20two-stage%20pipeline%3A%20detecting%20individuals%20in%20each%20frame%2C%20followed%20by%20temporal%20modeling%20for%20single-person%20pose%20estimation.%20This%20design%20relies%20on%20heuristic%20operations%20such%20as%20detection%2C%20RoI%20cropping%2C%20and%20non-maximum%20suppression%20%28NMS%29%2C%20limiting%20both%20accuracy%20and%20efficiency.%20In%20this%20paper%2C%20we%20present%20a%20fully%20end-to-end%20framework%20for%20multi-person%202D%20pose%20estimation%20in%20videos%2C%20effectively%20eliminating%20heuristic%20operations.%20A%20key%20challenge%20is%20to%20associate%20individuals%20across%20frames%20under%20complex%20and%20overlapping%20temporal%20trajectories.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20Pose-Aware%20Video%20transformEr%20Network%20%28PAVE-Net%29%2C%20which%20features%20a%20spatial%20encoder%20to%20model%20intra-frame%20relations%20and%20a%20spatiotemporal%20pose%20decoder%20to%20capture%20global%20dependencies%20across%20frames.%20To%20achieve%20accurate%20temporal%20association%2C%20we%20propose%20a%20pose-aware%20attention%20mechanism%20that%20enables%20each%20pose%20query%20to%20selectively%20aggregate%20features%20corresponding%20to%20the%20same%20individual%20across%20consecutive%20frames.Additionally%2C%20we%20explicitly%20model%20spatiotemporal%20dependencies%20among%20pose%20keypoints%20to%20improve%20accuracy.%20Notably%2C%20our%20approach%20is%20the%20first%20end-to-end%20method%20for%20multi-frame%202D%20human%20pose%20estimation.Extensive%20experiments%20show%20that%20PAVE-Net%20substantially%20outperforms%20prior%20image-based%20end-to-end%20methods%2C%20achieving%20a%20%5Ctextbf%7B6.0%7D%20mAP%20improvement%20on%20PoseTrack2017%2C%20and%20delivers%20accuracy%20competitive%20with%20state-of-the-art%20two-stage%20video-based%20approaches%2C%20while%20offering%20significant%20gains%20in%20efficiency.Project%20page%3A%20https%3A//github.com/zgspose/PAVENet&entry.1838667208=http%3A//arxiv.org/abs/2511.13208v1&entry.124074799=Read"},
{"title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE", "author": "Lipeng Wang and Hongxing Fan and Haohua Chen and Zehuan Huang and Lu Sheng", "abstract": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.", "link": "http://arxiv.org/abs/2511.13488v1", "date": "2025-11-17", "relevancy": 2.9687, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6247}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5982}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterMoE%3A%20Individual-Specific%203D%20Human%20Interaction%20Generation%20via%20Dynamic%20Temporal-Selective%20MoE&body=Title%3A%20InterMoE%3A%20Individual-Specific%203D%20Human%20Interaction%20Generation%20via%20Dynamic%20Temporal-Selective%20MoE%0AAuthor%3A%20Lipeng%20Wang%20and%20Hongxing%20Fan%20and%20Haohua%20Chen%20and%20Zehuan%20Huang%20and%20Lu%20Sheng%0AAbstract%3A%20Generating%20high-quality%20human%20interactions%20holds%20significant%20value%20for%20applications%20like%20virtual%20reality%20and%20robotics.%20However%2C%20existing%20methods%20often%20fail%20to%20preserve%20unique%20individual%20characteristics%20or%20fully%20adhere%20to%20textual%20descriptions.%20To%20address%20these%20challenges%2C%20we%20introduce%20InterMoE%2C%20a%20novel%20framework%20built%20on%20a%20Dynamic%20Temporal-Selective%20Mixture%20of%20Experts.%20The%20core%20of%20InterMoE%20is%20a%20routing%20mechanism%20that%20synergistically%20uses%20both%20high-level%20text%20semantics%20and%20low-level%20motion%20context%20to%20dispatch%20temporal%20motion%20features%20to%20specialized%20experts.%20This%20allows%20experts%20to%20dynamically%20determine%20the%20selection%20capacity%20and%20focus%20on%20critical%20temporal%20features%2C%20thereby%20preserving%20specific%20individual%20characteristic%20identities%20while%20ensuring%20high%20semantic%20fidelity.%20Extensive%20experiments%20show%20that%20InterMoE%20achieves%20state-of-the-art%20performance%20in%20individual-specific%20high-fidelity%203D%20human%20interaction%20generation%2C%20reducing%20FID%20scores%20by%209%25%20on%20the%20InterHuman%20dataset%20and%2022%25%20on%20InterX.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterMoE%253A%2520Individual-Specific%25203D%2520Human%2520Interaction%2520Generation%2520via%2520Dynamic%2520Temporal-Selective%2520MoE%26entry.906535625%3DLipeng%2520Wang%2520and%2520Hongxing%2520Fan%2520and%2520Haohua%2520Chen%2520and%2520Zehuan%2520Huang%2520and%2520Lu%2520Sheng%26entry.1292438233%3DGenerating%2520high-quality%2520human%2520interactions%2520holds%2520significant%2520value%2520for%2520applications%2520like%2520virtual%2520reality%2520and%2520robotics.%2520However%252C%2520existing%2520methods%2520often%2520fail%2520to%2520preserve%2520unique%2520individual%2520characteristics%2520or%2520fully%2520adhere%2520to%2520textual%2520descriptions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520InterMoE%252C%2520a%2520novel%2520framework%2520built%2520on%2520a%2520Dynamic%2520Temporal-Selective%2520Mixture%2520of%2520Experts.%2520The%2520core%2520of%2520InterMoE%2520is%2520a%2520routing%2520mechanism%2520that%2520synergistically%2520uses%2520both%2520high-level%2520text%2520semantics%2520and%2520low-level%2520motion%2520context%2520to%2520dispatch%2520temporal%2520motion%2520features%2520to%2520specialized%2520experts.%2520This%2520allows%2520experts%2520to%2520dynamically%2520determine%2520the%2520selection%2520capacity%2520and%2520focus%2520on%2520critical%2520temporal%2520features%252C%2520thereby%2520preserving%2520specific%2520individual%2520characteristic%2520identities%2520while%2520ensuring%2520high%2520semantic%2520fidelity.%2520Extensive%2520experiments%2520show%2520that%2520InterMoE%2520achieves%2520state-of-the-art%2520performance%2520in%2520individual-specific%2520high-fidelity%25203D%2520human%2520interaction%2520generation%252C%2520reducing%2520FID%2520scores%2520by%25209%2525%2520on%2520the%2520InterHuman%2520dataset%2520and%252022%2525%2520on%2520InterX.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterMoE%3A%20Individual-Specific%203D%20Human%20Interaction%20Generation%20via%20Dynamic%20Temporal-Selective%20MoE&entry.906535625=Lipeng%20Wang%20and%20Hongxing%20Fan%20and%20Haohua%20Chen%20and%20Zehuan%20Huang%20and%20Lu%20Sheng&entry.1292438233=Generating%20high-quality%20human%20interactions%20holds%20significant%20value%20for%20applications%20like%20virtual%20reality%20and%20robotics.%20However%2C%20existing%20methods%20often%20fail%20to%20preserve%20unique%20individual%20characteristics%20or%20fully%20adhere%20to%20textual%20descriptions.%20To%20address%20these%20challenges%2C%20we%20introduce%20InterMoE%2C%20a%20novel%20framework%20built%20on%20a%20Dynamic%20Temporal-Selective%20Mixture%20of%20Experts.%20The%20core%20of%20InterMoE%20is%20a%20routing%20mechanism%20that%20synergistically%20uses%20both%20high-level%20text%20semantics%20and%20low-level%20motion%20context%20to%20dispatch%20temporal%20motion%20features%20to%20specialized%20experts.%20This%20allows%20experts%20to%20dynamically%20determine%20the%20selection%20capacity%20and%20focus%20on%20critical%20temporal%20features%2C%20thereby%20preserving%20specific%20individual%20characteristic%20identities%20while%20ensuring%20high%20semantic%20fidelity.%20Extensive%20experiments%20show%20that%20InterMoE%20achieves%20state-of-the-art%20performance%20in%20individual-specific%20high-fidelity%203D%20human%20interaction%20generation%2C%20reducing%20FID%20scores%20by%209%25%20on%20the%20InterHuman%20dataset%20and%2022%25%20on%20InterX.&entry.1838667208=http%3A//arxiv.org/abs/2511.13488v1&entry.124074799=Read"},
{"title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "author": "Seonho Lee and Jiho Choi and Inha Kang and Jiwook Kim and Junsung Park and Hyunjung Shim", "abstract": "Vision-Language Models (VLMs) have shown remarkable performance on diverse visual and linguistic tasks, yet they remain fundamentally limited in their understanding of 3D spatial structures. We propose Geometric Distillation, a lightweight, annotation-free fine-tuning framework that injects human-inspired geometric cues into pretrained VLMs without modifying their architecture. By distilling (1) sparse correspondences, (2) relative depth relations, and (3) dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R, VGGT), our method shapes representations to be geometry-aware while remaining compatible with natural image-text inputs. Through extensive evaluations on 3D vision-language reasoning and 3D perception benchmarks, our method consistently outperforms prior approaches, achieving improved 3D spatial reasoning with significantly lower computational cost. Our work demonstrates a scalable and efficient path to bridge 2D-trained VLMs with 3D understanding, opening up wider use in spatially grounded multimodal tasks.", "link": "http://arxiv.org/abs/2506.09883v2", "date": "2025-11-17", "relevancy": 2.9661, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Aware%20Vision-Language%20Models%20Fine-Tuning%20with%20Geometric%20Distillation&body=Title%3A%203D-Aware%20Vision-Language%20Models%20Fine-Tuning%20with%20Geometric%20Distillation%0AAuthor%3A%20Seonho%20Lee%20and%20Jiho%20Choi%20and%20Inha%20Kang%20and%20Jiwook%20Kim%20and%20Junsung%20Park%20and%20Hyunjung%20Shim%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20remarkable%20performance%20on%20diverse%20visual%20and%20linguistic%20tasks%2C%20yet%20they%20remain%20fundamentally%20limited%20in%20their%20understanding%20of%203D%20spatial%20structures.%20We%20propose%20Geometric%20Distillation%2C%20a%20lightweight%2C%20annotation-free%20fine-tuning%20framework%20that%20injects%20human-inspired%20geometric%20cues%20into%20pretrained%20VLMs%20without%20modifying%20their%20architecture.%20By%20distilling%20%281%29%20sparse%20correspondences%2C%20%282%29%20relative%20depth%20relations%2C%20and%20%283%29%20dense%20cost%20volumes%20from%20off-the-shelf%203D%20foundation%20models%20%28e.g.%2C%20MASt3R%2C%20VGGT%29%2C%20our%20method%20shapes%20representations%20to%20be%20geometry-aware%20while%20remaining%20compatible%20with%20natural%20image-text%20inputs.%20Through%20extensive%20evaluations%20on%203D%20vision-language%20reasoning%20and%203D%20perception%20benchmarks%2C%20our%20method%20consistently%20outperforms%20prior%20approaches%2C%20achieving%20improved%203D%20spatial%20reasoning%20with%20significantly%20lower%20computational%20cost.%20Our%20work%20demonstrates%20a%20scalable%20and%20efficient%20path%20to%20bridge%202D-trained%20VLMs%20with%203D%20understanding%2C%20opening%20up%20wider%20use%20in%20spatially%20grounded%20multimodal%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Aware%2520Vision-Language%2520Models%2520Fine-Tuning%2520with%2520Geometric%2520Distillation%26entry.906535625%3DSeonho%2520Lee%2520and%2520Jiho%2520Choi%2520and%2520Inha%2520Kang%2520and%2520Jiwook%2520Kim%2520and%2520Junsung%2520Park%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520on%2520diverse%2520visual%2520and%2520linguistic%2520tasks%252C%2520yet%2520they%2520remain%2520fundamentally%2520limited%2520in%2520their%2520understanding%2520of%25203D%2520spatial%2520structures.%2520We%2520propose%2520Geometric%2520Distillation%252C%2520a%2520lightweight%252C%2520annotation-free%2520fine-tuning%2520framework%2520that%2520injects%2520human-inspired%2520geometric%2520cues%2520into%2520pretrained%2520VLMs%2520without%2520modifying%2520their%2520architecture.%2520By%2520distilling%2520%25281%2529%2520sparse%2520correspondences%252C%2520%25282%2529%2520relative%2520depth%2520relations%252C%2520and%2520%25283%2529%2520dense%2520cost%2520volumes%2520from%2520off-the-shelf%25203D%2520foundation%2520models%2520%2528e.g.%252C%2520MASt3R%252C%2520VGGT%2529%252C%2520our%2520method%2520shapes%2520representations%2520to%2520be%2520geometry-aware%2520while%2520remaining%2520compatible%2520with%2520natural%2520image-text%2520inputs.%2520Through%2520extensive%2520evaluations%2520on%25203D%2520vision-language%2520reasoning%2520and%25203D%2520perception%2520benchmarks%252C%2520our%2520method%2520consistently%2520outperforms%2520prior%2520approaches%252C%2520achieving%2520improved%25203D%2520spatial%2520reasoning%2520with%2520significantly%2520lower%2520computational%2520cost.%2520Our%2520work%2520demonstrates%2520a%2520scalable%2520and%2520efficient%2520path%2520to%2520bridge%25202D-trained%2520VLMs%2520with%25203D%2520understanding%252C%2520opening%2520up%2520wider%2520use%2520in%2520spatially%2520grounded%2520multimodal%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Aware%20Vision-Language%20Models%20Fine-Tuning%20with%20Geometric%20Distillation&entry.906535625=Seonho%20Lee%20and%20Jiho%20Choi%20and%20Inha%20Kang%20and%20Jiwook%20Kim%20and%20Junsung%20Park%20and%20Hyunjung%20Shim&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20shown%20remarkable%20performance%20on%20diverse%20visual%20and%20linguistic%20tasks%2C%20yet%20they%20remain%20fundamentally%20limited%20in%20their%20understanding%20of%203D%20spatial%20structures.%20We%20propose%20Geometric%20Distillation%2C%20a%20lightweight%2C%20annotation-free%20fine-tuning%20framework%20that%20injects%20human-inspired%20geometric%20cues%20into%20pretrained%20VLMs%20without%20modifying%20their%20architecture.%20By%20distilling%20%281%29%20sparse%20correspondences%2C%20%282%29%20relative%20depth%20relations%2C%20and%20%283%29%20dense%20cost%20volumes%20from%20off-the-shelf%203D%20foundation%20models%20%28e.g.%2C%20MASt3R%2C%20VGGT%29%2C%20our%20method%20shapes%20representations%20to%20be%20geometry-aware%20while%20remaining%20compatible%20with%20natural%20image-text%20inputs.%20Through%20extensive%20evaluations%20on%203D%20vision-language%20reasoning%20and%203D%20perception%20benchmarks%2C%20our%20method%20consistently%20outperforms%20prior%20approaches%2C%20achieving%20improved%203D%20spatial%20reasoning%20with%20significantly%20lower%20computational%20cost.%20Our%20work%20demonstrates%20a%20scalable%20and%20efficient%20path%20to%20bridge%202D-trained%20VLMs%20with%203D%20understanding%2C%20opening%20up%20wider%20use%20in%20spatially%20grounded%20multimodal%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2506.09883v2&entry.124074799=Read"},
{"title": "Language-Guided Invariance Probing of Vision-Language Models", "author": "Jae Joong Lee", "abstract": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.\n  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.", "link": "http://arxiv.org/abs/2511.13494v1", "date": "2025-11-17", "relevancy": 2.9319, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Guided%20Invariance%20Probing%20of%20Vision-Language%20Models&body=Title%3A%20Language-Guided%20Invariance%20Probing%20of%20Vision-Language%20Models%0AAuthor%3A%20Jae%20Joong%20Lee%0AAbstract%3A%20Recent%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%2C%20OpenCLIP%2C%20EVA02-CLIP%20and%20SigLIP%20achieve%20strong%20zero-shot%20performance%2C%20but%20it%20is%20unclear%20how%20reliably%20they%20respond%20to%20controlled%20linguistic%20perturbations.%20We%20introduce%20Language-Guided%20Invariance%20Probing%20%28LGIP%29%2C%20a%20benchmark%20that%20measures%20%28i%29%20invariance%20to%20meaning-preserving%20paraphrases%20and%20%28ii%29%20sensitivity%20to%20meaning-changing%20semantic%20flips%20in%20image-text%20matching.%20Using%2040k%20MS%20COCO%20images%20with%20five%20human%20captions%20each%2C%20we%20automatically%20generate%20paraphrases%20and%20rule-based%20flips%20that%20alter%20object%20category%2C%20color%20or%20count%2C%20and%20summarize%20model%20behavior%20with%20an%20invariance%20error%2C%20a%20semantic%20sensitivity%20gap%20and%20a%20positive-rate%20statistic.%0A%20%20Across%20nine%20VLMs%2C%20EVA02-CLIP%20and%20large%20OpenCLIP%20variants%20lie%20on%20a%20favorable%20invariance-sensitivity%20frontier%2C%20combining%20low%20paraphrase-induced%20variance%20with%20consistently%20higher%20scores%20for%20original%20captions%20than%20for%20their%20flipped%20counterparts.%20In%20contrast%2C%20SigLIP%20and%20SigLIP2%20show%20much%20larger%20invariance%20error%20and%20often%20prefer%20flipped%20captions%20to%20the%20human%20descriptions%2C%20especially%20for%20object%20and%20color%20edits.%20These%20failures%20are%20largely%20invisible%20to%20standard%20retrieval%20metrics%2C%20indicating%20that%20LGIP%20provides%20a%20model-agnostic%20diagnostic%20for%20the%20linguistic%20robustness%20of%20VLMs%20beyond%20conventional%20accuracy%20scores.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Guided%2520Invariance%2520Probing%2520of%2520Vision-Language%2520Models%26entry.906535625%3DJae%2520Joong%2520Lee%26entry.1292438233%3DRecent%2520vision-language%2520models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%252C%2520OpenCLIP%252C%2520EVA02-CLIP%2520and%2520SigLIP%2520achieve%2520strong%2520zero-shot%2520performance%252C%2520but%2520it%2520is%2520unclear%2520how%2520reliably%2520they%2520respond%2520to%2520controlled%2520linguistic%2520perturbations.%2520We%2520introduce%2520Language-Guided%2520Invariance%2520Probing%2520%2528LGIP%2529%252C%2520a%2520benchmark%2520that%2520measures%2520%2528i%2529%2520invariance%2520to%2520meaning-preserving%2520paraphrases%2520and%2520%2528ii%2529%2520sensitivity%2520to%2520meaning-changing%2520semantic%2520flips%2520in%2520image-text%2520matching.%2520Using%252040k%2520MS%2520COCO%2520images%2520with%2520five%2520human%2520captions%2520each%252C%2520we%2520automatically%2520generate%2520paraphrases%2520and%2520rule-based%2520flips%2520that%2520alter%2520object%2520category%252C%2520color%2520or%2520count%252C%2520and%2520summarize%2520model%2520behavior%2520with%2520an%2520invariance%2520error%252C%2520a%2520semantic%2520sensitivity%2520gap%2520and%2520a%2520positive-rate%2520statistic.%250A%2520%2520Across%2520nine%2520VLMs%252C%2520EVA02-CLIP%2520and%2520large%2520OpenCLIP%2520variants%2520lie%2520on%2520a%2520favorable%2520invariance-sensitivity%2520frontier%252C%2520combining%2520low%2520paraphrase-induced%2520variance%2520with%2520consistently%2520higher%2520scores%2520for%2520original%2520captions%2520than%2520for%2520their%2520flipped%2520counterparts.%2520In%2520contrast%252C%2520SigLIP%2520and%2520SigLIP2%2520show%2520much%2520larger%2520invariance%2520error%2520and%2520often%2520prefer%2520flipped%2520captions%2520to%2520the%2520human%2520descriptions%252C%2520especially%2520for%2520object%2520and%2520color%2520edits.%2520These%2520failures%2520are%2520largely%2520invisible%2520to%2520standard%2520retrieval%2520metrics%252C%2520indicating%2520that%2520LGIP%2520provides%2520a%2520model-agnostic%2520diagnostic%2520for%2520the%2520linguistic%2520robustness%2520of%2520VLMs%2520beyond%2520conventional%2520accuracy%2520scores.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Guided%20Invariance%20Probing%20of%20Vision-Language%20Models&entry.906535625=Jae%20Joong%20Lee&entry.1292438233=Recent%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%2C%20OpenCLIP%2C%20EVA02-CLIP%20and%20SigLIP%20achieve%20strong%20zero-shot%20performance%2C%20but%20it%20is%20unclear%20how%20reliably%20they%20respond%20to%20controlled%20linguistic%20perturbations.%20We%20introduce%20Language-Guided%20Invariance%20Probing%20%28LGIP%29%2C%20a%20benchmark%20that%20measures%20%28i%29%20invariance%20to%20meaning-preserving%20paraphrases%20and%20%28ii%29%20sensitivity%20to%20meaning-changing%20semantic%20flips%20in%20image-text%20matching.%20Using%2040k%20MS%20COCO%20images%20with%20five%20human%20captions%20each%2C%20we%20automatically%20generate%20paraphrases%20and%20rule-based%20flips%20that%20alter%20object%20category%2C%20color%20or%20count%2C%20and%20summarize%20model%20behavior%20with%20an%20invariance%20error%2C%20a%20semantic%20sensitivity%20gap%20and%20a%20positive-rate%20statistic.%0A%20%20Across%20nine%20VLMs%2C%20EVA02-CLIP%20and%20large%20OpenCLIP%20variants%20lie%20on%20a%20favorable%20invariance-sensitivity%20frontier%2C%20combining%20low%20paraphrase-induced%20variance%20with%20consistently%20higher%20scores%20for%20original%20captions%20than%20for%20their%20flipped%20counterparts.%20In%20contrast%2C%20SigLIP%20and%20SigLIP2%20show%20much%20larger%20invariance%20error%20and%20often%20prefer%20flipped%20captions%20to%20the%20human%20descriptions%2C%20especially%20for%20object%20and%20color%20edits.%20These%20failures%20are%20largely%20invisible%20to%20standard%20retrieval%20metrics%2C%20indicating%20that%20LGIP%20provides%20a%20model-agnostic%20diagnostic%20for%20the%20linguistic%20robustness%20of%20VLMs%20beyond%20conventional%20accuracy%20scores.&entry.1838667208=http%3A//arxiv.org/abs/2511.13494v1&entry.124074799=Read"},
{"title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models", "author": "Harold Haodong Chen and Disen Lan and Wen-Jie Shu and Qingyang Liu and Zihan Wang and Sirui Chen and Wenkai Cheng and Kanghao Chen and Hongfei Zhang and Zixin Zhang and Rongjin Guo and Yu Cheng and Ying-Cong Chen", "abstract": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.", "link": "http://arxiv.org/abs/2511.13704v1", "date": "2025-11-17", "relevancy": 2.9244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiViBench%3A%20Benchmarking%20Think-in-Video%20Reasoning%20for%20Video%20Generative%20Models&body=Title%3A%20TiViBench%3A%20Benchmarking%20Think-in-Video%20Reasoning%20for%20Video%20Generative%20Models%0AAuthor%3A%20Harold%20Haodong%20Chen%20and%20Disen%20Lan%20and%20Wen-Jie%20Shu%20and%20Qingyang%20Liu%20and%20Zihan%20Wang%20and%20Sirui%20Chen%20and%20Wenkai%20Cheng%20and%20Kanghao%20Chen%20and%20Hongfei%20Zhang%20and%20Zixin%20Zhang%20and%20Rongjin%20Guo%20and%20Yu%20Cheng%20and%20Ying-Cong%20Chen%0AAbstract%3A%20The%20rapid%20evolution%20of%20video%20generative%20models%20has%20shifted%20their%20focus%20from%20producing%20visually%20plausible%20outputs%20to%20tackling%20tasks%20requiring%20physical%20plausibility%20and%20logical%20consistency.%20However%2C%20despite%20recent%20breakthroughs%20such%20as%20Veo%203%27s%20chain-of-frames%20reasoning%2C%20it%20remains%20unclear%20whether%20these%20models%20can%20exhibit%20reasoning%20capabilities%20similar%20to%20large%20language%20models%20%28LLMs%29.%20Existing%20benchmarks%20predominantly%20evaluate%20visual%20fidelity%20and%20temporal%20coherence%2C%20failing%20to%20capture%20higher-order%20reasoning%20abilities.%20To%20bridge%20this%20gap%2C%20we%20propose%20TiViBench%2C%20a%20hierarchical%20benchmark%20specifically%20designed%20to%20evaluate%20the%20reasoning%20capabilities%20of%20image-to-video%20%28I2V%29%20generation%20models.%20TiViBench%20systematically%20assesses%20reasoning%20across%20four%20dimensions%3A%20i%29%20Structural%20Reasoning%20%26%20Search%2C%20ii%29%20Spatial%20%26%20Visual%20Pattern%20Reasoning%2C%20iii%29%20Symbolic%20%26%20Logical%20Reasoning%2C%20and%20iv%29%20Action%20Planning%20%26%20Task%20Execution%2C%20spanning%2024%20diverse%20task%20scenarios%20across%203%20difficulty%20levels.%20Through%20extensive%20evaluations%2C%20we%20show%20that%20commercial%20models%20%28e.g.%2C%20Sora%202%2C%20Veo%203.1%29%20demonstrate%20stronger%20reasoning%20potential%2C%20while%20open-source%20models%20reveal%20untapped%20potential%20that%20remains%20hindered%20by%20limited%20training%20scale%20and%20data%20diversity.%20To%20further%20unlock%20this%20potential%2C%20we%20introduce%20VideoTPO%2C%20a%20simple%20yet%20effective%20test-time%20strategy%20inspired%20by%20preference%20optimization.%20By%20performing%20LLM%20self-analysis%20on%20generated%20candidates%20to%20identify%20strengths%20and%20weaknesses%2C%20VideoTPO%20significantly%20enhances%20reasoning%20performance%20without%20requiring%20additional%20training%2C%20data%2C%20or%20reward%20models.%20Together%2C%20TiViBench%20and%20VideoTPO%20pave%20the%20way%20for%20evaluating%20and%20advancing%20reasoning%20in%20video%20generation%20models%2C%20setting%20a%20foundation%20for%20future%20research%20in%20this%20emerging%20field.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiViBench%253A%2520Benchmarking%2520Think-in-Video%2520Reasoning%2520for%2520Video%2520Generative%2520Models%26entry.906535625%3DHarold%2520Haodong%2520Chen%2520and%2520Disen%2520Lan%2520and%2520Wen-Jie%2520Shu%2520and%2520Qingyang%2520Liu%2520and%2520Zihan%2520Wang%2520and%2520Sirui%2520Chen%2520and%2520Wenkai%2520Cheng%2520and%2520Kanghao%2520Chen%2520and%2520Hongfei%2520Zhang%2520and%2520Zixin%2520Zhang%2520and%2520Rongjin%2520Guo%2520and%2520Yu%2520Cheng%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3DThe%2520rapid%2520evolution%2520of%2520video%2520generative%2520models%2520has%2520shifted%2520their%2520focus%2520from%2520producing%2520visually%2520plausible%2520outputs%2520to%2520tackling%2520tasks%2520requiring%2520physical%2520plausibility%2520and%2520logical%2520consistency.%2520However%252C%2520despite%2520recent%2520breakthroughs%2520such%2520as%2520Veo%25203%2527s%2520chain-of-frames%2520reasoning%252C%2520it%2520remains%2520unclear%2520whether%2520these%2520models%2520can%2520exhibit%2520reasoning%2520capabilities%2520similar%2520to%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Existing%2520benchmarks%2520predominantly%2520evaluate%2520visual%2520fidelity%2520and%2520temporal%2520coherence%252C%2520failing%2520to%2520capture%2520higher-order%2520reasoning%2520abilities.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520TiViBench%252C%2520a%2520hierarchical%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520the%2520reasoning%2520capabilities%2520of%2520image-to-video%2520%2528I2V%2529%2520generation%2520models.%2520TiViBench%2520systematically%2520assesses%2520reasoning%2520across%2520four%2520dimensions%253A%2520i%2529%2520Structural%2520Reasoning%2520%2526%2520Search%252C%2520ii%2529%2520Spatial%2520%2526%2520Visual%2520Pattern%2520Reasoning%252C%2520iii%2529%2520Symbolic%2520%2526%2520Logical%2520Reasoning%252C%2520and%2520iv%2529%2520Action%2520Planning%2520%2526%2520Task%2520Execution%252C%2520spanning%252024%2520diverse%2520task%2520scenarios%2520across%25203%2520difficulty%2520levels.%2520Through%2520extensive%2520evaluations%252C%2520we%2520show%2520that%2520commercial%2520models%2520%2528e.g.%252C%2520Sora%25202%252C%2520Veo%25203.1%2529%2520demonstrate%2520stronger%2520reasoning%2520potential%252C%2520while%2520open-source%2520models%2520reveal%2520untapped%2520potential%2520that%2520remains%2520hindered%2520by%2520limited%2520training%2520scale%2520and%2520data%2520diversity.%2520To%2520further%2520unlock%2520this%2520potential%252C%2520we%2520introduce%2520VideoTPO%252C%2520a%2520simple%2520yet%2520effective%2520test-time%2520strategy%2520inspired%2520by%2520preference%2520optimization.%2520By%2520performing%2520LLM%2520self-analysis%2520on%2520generated%2520candidates%2520to%2520identify%2520strengths%2520and%2520weaknesses%252C%2520VideoTPO%2520significantly%2520enhances%2520reasoning%2520performance%2520without%2520requiring%2520additional%2520training%252C%2520data%252C%2520or%2520reward%2520models.%2520Together%252C%2520TiViBench%2520and%2520VideoTPO%2520pave%2520the%2520way%2520for%2520evaluating%2520and%2520advancing%2520reasoning%2520in%2520video%2520generation%2520models%252C%2520setting%2520a%2520foundation%2520for%2520future%2520research%2520in%2520this%2520emerging%2520field.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiViBench%3A%20Benchmarking%20Think-in-Video%20Reasoning%20for%20Video%20Generative%20Models&entry.906535625=Harold%20Haodong%20Chen%20and%20Disen%20Lan%20and%20Wen-Jie%20Shu%20and%20Qingyang%20Liu%20and%20Zihan%20Wang%20and%20Sirui%20Chen%20and%20Wenkai%20Cheng%20and%20Kanghao%20Chen%20and%20Hongfei%20Zhang%20and%20Zixin%20Zhang%20and%20Rongjin%20Guo%20and%20Yu%20Cheng%20and%20Ying-Cong%20Chen&entry.1292438233=The%20rapid%20evolution%20of%20video%20generative%20models%20has%20shifted%20their%20focus%20from%20producing%20visually%20plausible%20outputs%20to%20tackling%20tasks%20requiring%20physical%20plausibility%20and%20logical%20consistency.%20However%2C%20despite%20recent%20breakthroughs%20such%20as%20Veo%203%27s%20chain-of-frames%20reasoning%2C%20it%20remains%20unclear%20whether%20these%20models%20can%20exhibit%20reasoning%20capabilities%20similar%20to%20large%20language%20models%20%28LLMs%29.%20Existing%20benchmarks%20predominantly%20evaluate%20visual%20fidelity%20and%20temporal%20coherence%2C%20failing%20to%20capture%20higher-order%20reasoning%20abilities.%20To%20bridge%20this%20gap%2C%20we%20propose%20TiViBench%2C%20a%20hierarchical%20benchmark%20specifically%20designed%20to%20evaluate%20the%20reasoning%20capabilities%20of%20image-to-video%20%28I2V%29%20generation%20models.%20TiViBench%20systematically%20assesses%20reasoning%20across%20four%20dimensions%3A%20i%29%20Structural%20Reasoning%20%26%20Search%2C%20ii%29%20Spatial%20%26%20Visual%20Pattern%20Reasoning%2C%20iii%29%20Symbolic%20%26%20Logical%20Reasoning%2C%20and%20iv%29%20Action%20Planning%20%26%20Task%20Execution%2C%20spanning%2024%20diverse%20task%20scenarios%20across%203%20difficulty%20levels.%20Through%20extensive%20evaluations%2C%20we%20show%20that%20commercial%20models%20%28e.g.%2C%20Sora%202%2C%20Veo%203.1%29%20demonstrate%20stronger%20reasoning%20potential%2C%20while%20open-source%20models%20reveal%20untapped%20potential%20that%20remains%20hindered%20by%20limited%20training%20scale%20and%20data%20diversity.%20To%20further%20unlock%20this%20potential%2C%20we%20introduce%20VideoTPO%2C%20a%20simple%20yet%20effective%20test-time%20strategy%20inspired%20by%20preference%20optimization.%20By%20performing%20LLM%20self-analysis%20on%20generated%20candidates%20to%20identify%20strengths%20and%20weaknesses%2C%20VideoTPO%20significantly%20enhances%20reasoning%20performance%20without%20requiring%20additional%20training%2C%20data%2C%20or%20reward%20models.%20Together%2C%20TiViBench%20and%20VideoTPO%20pave%20the%20way%20for%20evaluating%20and%20advancing%20reasoning%20in%20video%20generation%20models%2C%20setting%20a%20foundation%20for%20future%20research%20in%20this%20emerging%20field.&entry.1838667208=http%3A//arxiv.org/abs/2511.13704v1&entry.124074799=Read"},
{"title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "author": "Muquan Yu and Mu Nan and Hossein Adeli and Jacob S. Prince and John A. Pyles and Leila Wehbe and Margaret M. Henderson and Michael J. Tarr and Andrew F. Luo", "abstract": "Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity.", "link": "http://arxiv.org/abs/2505.15813v2", "date": "2025-11-17", "relevancy": 2.9039, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20an%20In-Context%20Transformer%20Model%20of%20Human%20Higher%20Visual%20Cortex&body=Title%3A%20Meta-Learning%20an%20In-Context%20Transformer%20Model%20of%20Human%20Higher%20Visual%20Cortex%0AAuthor%3A%20Muquan%20Yu%20and%20Mu%20Nan%20and%20Hossein%20Adeli%20and%20Jacob%20S.%20Prince%20and%20John%20A.%20Pyles%20and%20Leila%20Wehbe%20and%20Margaret%20M.%20Henderson%20and%20Michael%20J.%20Tarr%20and%20Andrew%20F.%20Luo%0AAbstract%3A%20Understanding%20functional%20representations%20within%20higher%20visual%20cortex%20is%20a%20fundamental%20question%20in%20computational%20neuroscience.%20While%20artificial%20neural%20networks%20pretrained%20on%20large-scale%20datasets%20exhibit%20striking%20representational%20alignment%20with%20human%20neural%20responses%2C%20learning%20image-computable%20models%20of%20visual%20cortex%20relies%20on%20individual-level%2C%20large-scale%20fMRI%20datasets.%20The%20necessity%20for%20expensive%2C%20time-intensive%2C%20and%20often%20impractical%20data%20acquisition%20limits%20the%20generalizability%20of%20encoders%20to%20new%20subjects%20and%20stimuli.%20BraInCoRL%20uses%20in-context%20learning%20to%20predict%20voxelwise%20neural%20responses%20from%20few-shot%20examples%20without%20any%20additional%20finetuning%20for%20novel%20subjects%20and%20stimuli.%20We%20leverage%20a%20transformer%20architecture%20that%20can%20flexibly%20condition%20on%20a%20variable%20number%20of%20in-context%20image%20stimuli%2C%20learning%20an%20inductive%20bias%20over%20multiple%20subjects.%20During%20training%2C%20we%20explicitly%20optimize%20the%20model%20for%20in-context%20learning.%20By%20jointly%20conditioning%20on%20image%20features%20and%20voxel%20activations%2C%20our%20model%20learns%20to%20directly%20generate%20better%20performing%20voxelwise%20models%20of%20higher%20visual%20cortex.%20We%20demonstrate%20that%20BraInCoRL%20consistently%20outperforms%20existing%20voxelwise%20encoder%20designs%20in%20a%20low-data%20regime%20when%20evaluated%20on%20entirely%20novel%20images%2C%20while%20also%20exhibiting%20strong%20test-time%20scaling%20behavior.%20The%20model%20also%20generalizes%20to%20an%20entirely%20new%20visual%20fMRI%20dataset%2C%20which%20uses%20different%20subjects%20and%20fMRI%20data%20acquisition%20parameters.%20Further%2C%20BraInCoRL%20facilitates%20better%20interpretability%20of%20neural%20signals%20in%20higher%20visual%20cortex%20by%20attending%20to%20semantically%20relevant%20stimuli.%20Finally%2C%20we%20show%20that%20our%20framework%20enables%20interpretable%20mappings%20from%20natural%20language%20queries%20to%20voxel%20selectivity.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520an%2520In-Context%2520Transformer%2520Model%2520of%2520Human%2520Higher%2520Visual%2520Cortex%26entry.906535625%3DMuquan%2520Yu%2520and%2520Mu%2520Nan%2520and%2520Hossein%2520Adeli%2520and%2520Jacob%2520S.%2520Prince%2520and%2520John%2520A.%2520Pyles%2520and%2520Leila%2520Wehbe%2520and%2520Margaret%2520M.%2520Henderson%2520and%2520Michael%2520J.%2520Tarr%2520and%2520Andrew%2520F.%2520Luo%26entry.1292438233%3DUnderstanding%2520functional%2520representations%2520within%2520higher%2520visual%2520cortex%2520is%2520a%2520fundamental%2520question%2520in%2520computational%2520neuroscience.%2520While%2520artificial%2520neural%2520networks%2520pretrained%2520on%2520large-scale%2520datasets%2520exhibit%2520striking%2520representational%2520alignment%2520with%2520human%2520neural%2520responses%252C%2520learning%2520image-computable%2520models%2520of%2520visual%2520cortex%2520relies%2520on%2520individual-level%252C%2520large-scale%2520fMRI%2520datasets.%2520The%2520necessity%2520for%2520expensive%252C%2520time-intensive%252C%2520and%2520often%2520impractical%2520data%2520acquisition%2520limits%2520the%2520generalizability%2520of%2520encoders%2520to%2520new%2520subjects%2520and%2520stimuli.%2520BraInCoRL%2520uses%2520in-context%2520learning%2520to%2520predict%2520voxelwise%2520neural%2520responses%2520from%2520few-shot%2520examples%2520without%2520any%2520additional%2520finetuning%2520for%2520novel%2520subjects%2520and%2520stimuli.%2520We%2520leverage%2520a%2520transformer%2520architecture%2520that%2520can%2520flexibly%2520condition%2520on%2520a%2520variable%2520number%2520of%2520in-context%2520image%2520stimuli%252C%2520learning%2520an%2520inductive%2520bias%2520over%2520multiple%2520subjects.%2520During%2520training%252C%2520we%2520explicitly%2520optimize%2520the%2520model%2520for%2520in-context%2520learning.%2520By%2520jointly%2520conditioning%2520on%2520image%2520features%2520and%2520voxel%2520activations%252C%2520our%2520model%2520learns%2520to%2520directly%2520generate%2520better%2520performing%2520voxelwise%2520models%2520of%2520higher%2520visual%2520cortex.%2520We%2520demonstrate%2520that%2520BraInCoRL%2520consistently%2520outperforms%2520existing%2520voxelwise%2520encoder%2520designs%2520in%2520a%2520low-data%2520regime%2520when%2520evaluated%2520on%2520entirely%2520novel%2520images%252C%2520while%2520also%2520exhibiting%2520strong%2520test-time%2520scaling%2520behavior.%2520The%2520model%2520also%2520generalizes%2520to%2520an%2520entirely%2520new%2520visual%2520fMRI%2520dataset%252C%2520which%2520uses%2520different%2520subjects%2520and%2520fMRI%2520data%2520acquisition%2520parameters.%2520Further%252C%2520BraInCoRL%2520facilitates%2520better%2520interpretability%2520of%2520neural%2520signals%2520in%2520higher%2520visual%2520cortex%2520by%2520attending%2520to%2520semantically%2520relevant%2520stimuli.%2520Finally%252C%2520we%2520show%2520that%2520our%2520framework%2520enables%2520interpretable%2520mappings%2520from%2520natural%2520language%2520queries%2520to%2520voxel%2520selectivity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20an%20In-Context%20Transformer%20Model%20of%20Human%20Higher%20Visual%20Cortex&entry.906535625=Muquan%20Yu%20and%20Mu%20Nan%20and%20Hossein%20Adeli%20and%20Jacob%20S.%20Prince%20and%20John%20A.%20Pyles%20and%20Leila%20Wehbe%20and%20Margaret%20M.%20Henderson%20and%20Michael%20J.%20Tarr%20and%20Andrew%20F.%20Luo&entry.1292438233=Understanding%20functional%20representations%20within%20higher%20visual%20cortex%20is%20a%20fundamental%20question%20in%20computational%20neuroscience.%20While%20artificial%20neural%20networks%20pretrained%20on%20large-scale%20datasets%20exhibit%20striking%20representational%20alignment%20with%20human%20neural%20responses%2C%20learning%20image-computable%20models%20of%20visual%20cortex%20relies%20on%20individual-level%2C%20large-scale%20fMRI%20datasets.%20The%20necessity%20for%20expensive%2C%20time-intensive%2C%20and%20often%20impractical%20data%20acquisition%20limits%20the%20generalizability%20of%20encoders%20to%20new%20subjects%20and%20stimuli.%20BraInCoRL%20uses%20in-context%20learning%20to%20predict%20voxelwise%20neural%20responses%20from%20few-shot%20examples%20without%20any%20additional%20finetuning%20for%20novel%20subjects%20and%20stimuli.%20We%20leverage%20a%20transformer%20architecture%20that%20can%20flexibly%20condition%20on%20a%20variable%20number%20of%20in-context%20image%20stimuli%2C%20learning%20an%20inductive%20bias%20over%20multiple%20subjects.%20During%20training%2C%20we%20explicitly%20optimize%20the%20model%20for%20in-context%20learning.%20By%20jointly%20conditioning%20on%20image%20features%20and%20voxel%20activations%2C%20our%20model%20learns%20to%20directly%20generate%20better%20performing%20voxelwise%20models%20of%20higher%20visual%20cortex.%20We%20demonstrate%20that%20BraInCoRL%20consistently%20outperforms%20existing%20voxelwise%20encoder%20designs%20in%20a%20low-data%20regime%20when%20evaluated%20on%20entirely%20novel%20images%2C%20while%20also%20exhibiting%20strong%20test-time%20scaling%20behavior.%20The%20model%20also%20generalizes%20to%20an%20entirely%20new%20visual%20fMRI%20dataset%2C%20which%20uses%20different%20subjects%20and%20fMRI%20data%20acquisition%20parameters.%20Further%2C%20BraInCoRL%20facilitates%20better%20interpretability%20of%20neural%20signals%20in%20higher%20visual%20cortex%20by%20attending%20to%20semantically%20relevant%20stimuli.%20Finally%2C%20we%20show%20that%20our%20framework%20enables%20interpretable%20mappings%20from%20natural%20language%20queries%20to%20voxel%20selectivity.&entry.1838667208=http%3A//arxiv.org/abs/2505.15813v2&entry.124074799=Read"},
{"title": "LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit", "author": "Chengtao Lv and Bilang Zhang and Yang Yong and Ruihao Gong and Yushi Huang and Shiqiao Gu and Jiajun Wu and Yumeng Shi and Jinyang Guo and Wenya Wang", "abstract": "Large Vision-Language Models (VLMs) exhibit impressive multi-modal capabilities but suffer from prohibitive computational and memory demands, due to their long visual token sequences and massive parameter sizes. To address these issues, recent works have proposed training-free compression methods. However, existing efforts often suffer from three major limitations: (1) Current approaches do not decompose techniques into comparable modules, hindering fair evaluation across spatial and temporal redundancy. (2) Evaluation confined to simple single-turn tasks, failing to reflect performance in realistic scenarios. (3) Isolated use of individual compression techniques, without exploring their joint potential. To overcome these gaps, we introduce LLMC+, a comprehensive VLM compression benchmark with a versatile, plug-and-play toolkit. LLMC+ supports over 20 algorithms across five representative VLM families and enables systematic study of token-level and model-level compression. Our benchmark reveals that: (1) Spatial and temporal redundancies demand distinct technical strategies. (2) Token reduction methods degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3) Combining token and model compression achieves extreme compression with minimal performance loss. We believe LLMC+ will facilitate fair evaluation and inspire future research in efficient VLM. Our code is available at https://github.com/ModelTC/LightCompress.", "link": "http://arxiv.org/abs/2508.09981v2", "date": "2025-11-17", "relevancy": 2.8775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMC%2B%3A%20Benchmarking%20Vision-Language%20Model%20Compression%20with%20a%20Plug-and-play%20Toolkit&body=Title%3A%20LLMC%2B%3A%20Benchmarking%20Vision-Language%20Model%20Compression%20with%20a%20Plug-and-play%20Toolkit%0AAuthor%3A%20Chengtao%20Lv%20and%20Bilang%20Zhang%20and%20Yang%20Yong%20and%20Ruihao%20Gong%20and%20Yushi%20Huang%20and%20Shiqiao%20Gu%20and%20Jiajun%20Wu%20and%20Yumeng%20Shi%20and%20Jinyang%20Guo%20and%20Wenya%20Wang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20impressive%20multi-modal%20capabilities%20but%20suffer%20from%20prohibitive%20computational%20and%20memory%20demands%2C%20due%20to%20their%20long%20visual%20token%20sequences%20and%20massive%20parameter%20sizes.%20To%20address%20these%20issues%2C%20recent%20works%20have%20proposed%20training-free%20compression%20methods.%20However%2C%20existing%20efforts%20often%20suffer%20from%20three%20major%20limitations%3A%20%281%29%20Current%20approaches%20do%20not%20decompose%20techniques%20into%20comparable%20modules%2C%20hindering%20fair%20evaluation%20across%20spatial%20and%20temporal%20redundancy.%20%282%29%20Evaluation%20confined%20to%20simple%20single-turn%20tasks%2C%20failing%20to%20reflect%20performance%20in%20realistic%20scenarios.%20%283%29%20Isolated%20use%20of%20individual%20compression%20techniques%2C%20without%20exploring%20their%20joint%20potential.%20To%20overcome%20these%20gaps%2C%20we%20introduce%20LLMC%2B%2C%20a%20comprehensive%20VLM%20compression%20benchmark%20with%20a%20versatile%2C%20plug-and-play%20toolkit.%20LLMC%2B%20supports%20over%2020%20algorithms%20across%20five%20representative%20VLM%20families%20and%20enables%20systematic%20study%20of%20token-level%20and%20model-level%20compression.%20Our%20benchmark%20reveals%20that%3A%20%281%29%20Spatial%20and%20temporal%20redundancies%20demand%20distinct%20technical%20strategies.%20%282%29%20Token%20reduction%20methods%20degrade%20significantly%20in%20multi-turn%20dialogue%20and%20detail-sensitive%20tasks.%20%283%29%20Combining%20token%20and%20model%20compression%20achieves%20extreme%20compression%20with%20minimal%20performance%20loss.%20We%20believe%20LLMC%2B%20will%20facilitate%20fair%20evaluation%20and%20inspire%20future%20research%20in%20efficient%20VLM.%20Our%20code%20is%20available%20at%20https%3A//github.com/ModelTC/LightCompress.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMC%252B%253A%2520Benchmarking%2520Vision-Language%2520Model%2520Compression%2520with%2520a%2520Plug-and-play%2520Toolkit%26entry.906535625%3DChengtao%2520Lv%2520and%2520Bilang%2520Zhang%2520and%2520Yang%2520Yong%2520and%2520Ruihao%2520Gong%2520and%2520Yushi%2520Huang%2520and%2520Shiqiao%2520Gu%2520and%2520Jiajun%2520Wu%2520and%2520Yumeng%2520Shi%2520and%2520Jinyang%2520Guo%2520and%2520Wenya%2520Wang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520impressive%2520multi-modal%2520capabilities%2520but%2520suffer%2520from%2520prohibitive%2520computational%2520and%2520memory%2520demands%252C%2520due%2520to%2520their%2520long%2520visual%2520token%2520sequences%2520and%2520massive%2520parameter%2520sizes.%2520To%2520address%2520these%2520issues%252C%2520recent%2520works%2520have%2520proposed%2520training-free%2520compression%2520methods.%2520However%252C%2520existing%2520efforts%2520often%2520suffer%2520from%2520three%2520major%2520limitations%253A%2520%25281%2529%2520Current%2520approaches%2520do%2520not%2520decompose%2520techniques%2520into%2520comparable%2520modules%252C%2520hindering%2520fair%2520evaluation%2520across%2520spatial%2520and%2520temporal%2520redundancy.%2520%25282%2529%2520Evaluation%2520confined%2520to%2520simple%2520single-turn%2520tasks%252C%2520failing%2520to%2520reflect%2520performance%2520in%2520realistic%2520scenarios.%2520%25283%2529%2520Isolated%2520use%2520of%2520individual%2520compression%2520techniques%252C%2520without%2520exploring%2520their%2520joint%2520potential.%2520To%2520overcome%2520these%2520gaps%252C%2520we%2520introduce%2520LLMC%252B%252C%2520a%2520comprehensive%2520VLM%2520compression%2520benchmark%2520with%2520a%2520versatile%252C%2520plug-and-play%2520toolkit.%2520LLMC%252B%2520supports%2520over%252020%2520algorithms%2520across%2520five%2520representative%2520VLM%2520families%2520and%2520enables%2520systematic%2520study%2520of%2520token-level%2520and%2520model-level%2520compression.%2520Our%2520benchmark%2520reveals%2520that%253A%2520%25281%2529%2520Spatial%2520and%2520temporal%2520redundancies%2520demand%2520distinct%2520technical%2520strategies.%2520%25282%2529%2520Token%2520reduction%2520methods%2520degrade%2520significantly%2520in%2520multi-turn%2520dialogue%2520and%2520detail-sensitive%2520tasks.%2520%25283%2529%2520Combining%2520token%2520and%2520model%2520compression%2520achieves%2520extreme%2520compression%2520with%2520minimal%2520performance%2520loss.%2520We%2520believe%2520LLMC%252B%2520will%2520facilitate%2520fair%2520evaluation%2520and%2520inspire%2520future%2520research%2520in%2520efficient%2520VLM.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ModelTC/LightCompress.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMC%2B%3A%20Benchmarking%20Vision-Language%20Model%20Compression%20with%20a%20Plug-and-play%20Toolkit&entry.906535625=Chengtao%20Lv%20and%20Bilang%20Zhang%20and%20Yang%20Yong%20and%20Ruihao%20Gong%20and%20Yushi%20Huang%20and%20Shiqiao%20Gu%20and%20Jiajun%20Wu%20and%20Yumeng%20Shi%20and%20Jinyang%20Guo%20and%20Wenya%20Wang&entry.1292438233=Large%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20impressive%20multi-modal%20capabilities%20but%20suffer%20from%20prohibitive%20computational%20and%20memory%20demands%2C%20due%20to%20their%20long%20visual%20token%20sequences%20and%20massive%20parameter%20sizes.%20To%20address%20these%20issues%2C%20recent%20works%20have%20proposed%20training-free%20compression%20methods.%20However%2C%20existing%20efforts%20often%20suffer%20from%20three%20major%20limitations%3A%20%281%29%20Current%20approaches%20do%20not%20decompose%20techniques%20into%20comparable%20modules%2C%20hindering%20fair%20evaluation%20across%20spatial%20and%20temporal%20redundancy.%20%282%29%20Evaluation%20confined%20to%20simple%20single-turn%20tasks%2C%20failing%20to%20reflect%20performance%20in%20realistic%20scenarios.%20%283%29%20Isolated%20use%20of%20individual%20compression%20techniques%2C%20without%20exploring%20their%20joint%20potential.%20To%20overcome%20these%20gaps%2C%20we%20introduce%20LLMC%2B%2C%20a%20comprehensive%20VLM%20compression%20benchmark%20with%20a%20versatile%2C%20plug-and-play%20toolkit.%20LLMC%2B%20supports%20over%2020%20algorithms%20across%20five%20representative%20VLM%20families%20and%20enables%20systematic%20study%20of%20token-level%20and%20model-level%20compression.%20Our%20benchmark%20reveals%20that%3A%20%281%29%20Spatial%20and%20temporal%20redundancies%20demand%20distinct%20technical%20strategies.%20%282%29%20Token%20reduction%20methods%20degrade%20significantly%20in%20multi-turn%20dialogue%20and%20detail-sensitive%20tasks.%20%283%29%20Combining%20token%20and%20model%20compression%20achieves%20extreme%20compression%20with%20minimal%20performance%20loss.%20We%20believe%20LLMC%2B%20will%20facilitate%20fair%20evaluation%20and%20inspire%20future%20research%20in%20efficient%20VLM.%20Our%20code%20is%20available%20at%20https%3A//github.com/ModelTC/LightCompress.&entry.1838667208=http%3A//arxiv.org/abs/2508.09981v2&entry.124074799=Read"},
{"title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models", "author": "Yushuo Zheng and Jiangyong Ying and Huiyu Duan and Chunyi Li and Zicheng Zhang and Jing Liu and Xiaohong Liu and Guangtao Zhai", "abstract": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.", "link": "http://arxiv.org/abs/2511.13259v1", "date": "2025-11-17", "relevancy": 2.8719, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5945}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoX-Bench%3A%20Benchmarking%20Cross-View%20Geo-Localization%20and%20Pose%20Estimation%20Capabilities%20of%20Large%20Multimodal%20Models&body=Title%3A%20GeoX-Bench%3A%20Benchmarking%20Cross-View%20Geo-Localization%20and%20Pose%20Estimation%20Capabilities%20of%20Large%20Multimodal%20Models%0AAuthor%3A%20Yushuo%20Zheng%20and%20Jiangyong%20Ying%20and%20Huiyu%20Duan%20and%20Chunyi%20Li%20and%20Zicheng%20Zhang%20and%20Jing%20Liu%20and%20Xiaohong%20Liu%20and%20Guangtao%20Zhai%0AAbstract%3A%20Large%20multimodal%20models%20%28LMMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%20a%20wide%20range%20of%20tasks%2C%20however%20their%20knowledge%20and%20abilities%20in%20the%20cross-view%20geo-localization%20and%20pose%20estimation%20domains%20remain%20unexplored%2C%20despite%20potential%20benefits%20for%20navigation%2C%20autonomous%20driving%2C%20outdoor%20robotics%2C%20%5Ctextit%7Betc%7D.%20To%20bridge%20this%20gap%2C%20we%20introduce%20%5Ctextbf%7BGeoX-Bench%7D%2C%20a%20comprehensive%20%5Cunderline%7BBench%7Dmark%20designed%20to%20explore%20and%20evaluate%20the%20capabilities%20of%20LMMs%20in%20%5Cunderline%7Bcross%7D-view%20%5Cunderline%7BGeo%7D-localization%20and%20pose%20estimation.%20Specifically%2C%20GeoX-Bench%20contains%2010%2C859%20panoramic-satellite%20image%20pairs%20spanning%20128%20cities%20in%2049%20countries%2C%20along%20with%20corresponding%20755%2C976%20question-answering%20%28QA%29%20pairs.%20Among%20these%2C%2042%2C900%20QA%20pairs%20are%20designated%20for%20benchmarking%2C%20while%20the%20remaining%20are%20intended%20to%20enhance%20the%20capabilities%20of%20LMMs.%20Based%20on%20GeoX-Bench%2C%20we%20evaluate%20the%20capabilities%20of%2025%20state-of-the-art%20LMMs%20on%20cross-view%20geo-localization%20and%20pose%20estimation%20tasks%2C%20and%20further%20explore%20the%20empowered%20capabilities%20of%20instruction-tuning.%20Our%20benchmark%20demonstrate%20that%20while%20current%20LMMs%20achieve%20impressive%20performance%20in%20geo-localization%20tasks%2C%20their%20effectiveness%20declines%20significantly%20on%20the%20more%20complex%20pose%20estimation%20tasks%2C%20highlighting%20a%20critical%20area%20for%20future%20improvement%2C%20and%20instruction-tuning%20LMMs%20on%20the%20training%20data%20of%20GeoX-Bench%20can%20significantly%20improve%20the%20cross-view%20geo-sense%20abilities.%20The%20GeoX-Bench%20is%20available%20at%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/IntMeGroup/GeoX-Bench%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoX-Bench%253A%2520Benchmarking%2520Cross-View%2520Geo-Localization%2520and%2520Pose%2520Estimation%2520Capabilities%2520of%2520Large%2520Multimodal%2520Models%26entry.906535625%3DYushuo%2520Zheng%2520and%2520Jiangyong%2520Ying%2520and%2520Huiyu%2520Duan%2520and%2520Chunyi%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Jing%2520Liu%2520and%2520Xiaohong%2520Liu%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DLarge%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520however%2520their%2520knowledge%2520and%2520abilities%2520in%2520the%2520cross-view%2520geo-localization%2520and%2520pose%2520estimation%2520domains%2520remain%2520unexplored%252C%2520despite%2520potential%2520benefits%2520for%2520navigation%252C%2520autonomous%2520driving%252C%2520outdoor%2520robotics%252C%2520%255Ctextit%257Betc%257D.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520%255Ctextbf%257BGeoX-Bench%257D%252C%2520a%2520comprehensive%2520%255Cunderline%257BBench%257Dmark%2520designed%2520to%2520explore%2520and%2520evaluate%2520the%2520capabilities%2520of%2520LMMs%2520in%2520%255Cunderline%257Bcross%257D-view%2520%255Cunderline%257BGeo%257D-localization%2520and%2520pose%2520estimation.%2520Specifically%252C%2520GeoX-Bench%2520contains%252010%252C859%2520panoramic-satellite%2520image%2520pairs%2520spanning%2520128%2520cities%2520in%252049%2520countries%252C%2520along%2520with%2520corresponding%2520755%252C976%2520question-answering%2520%2528QA%2529%2520pairs.%2520Among%2520these%252C%252042%252C900%2520QA%2520pairs%2520are%2520designated%2520for%2520benchmarking%252C%2520while%2520the%2520remaining%2520are%2520intended%2520to%2520enhance%2520the%2520capabilities%2520of%2520LMMs.%2520Based%2520on%2520GeoX-Bench%252C%2520we%2520evaluate%2520the%2520capabilities%2520of%252025%2520state-of-the-art%2520LMMs%2520on%2520cross-view%2520geo-localization%2520and%2520pose%2520estimation%2520tasks%252C%2520and%2520further%2520explore%2520the%2520empowered%2520capabilities%2520of%2520instruction-tuning.%2520Our%2520benchmark%2520demonstrate%2520that%2520while%2520current%2520LMMs%2520achieve%2520impressive%2520performance%2520in%2520geo-localization%2520tasks%252C%2520their%2520effectiveness%2520declines%2520significantly%2520on%2520the%2520more%2520complex%2520pose%2520estimation%2520tasks%252C%2520highlighting%2520a%2520critical%2520area%2520for%2520future%2520improvement%252C%2520and%2520instruction-tuning%2520LMMs%2520on%2520the%2520training%2520data%2520of%2520GeoX-Bench%2520can%2520significantly%2520improve%2520the%2520cross-view%2520geo-sense%2520abilities.%2520The%2520GeoX-Bench%2520is%2520available%2520at%2520%255Ctextcolor%257Bmagenta%257D%257Bhttps%253A//github.com/IntMeGroup/GeoX-Bench%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoX-Bench%3A%20Benchmarking%20Cross-View%20Geo-Localization%20and%20Pose%20Estimation%20Capabilities%20of%20Large%20Multimodal%20Models&entry.906535625=Yushuo%20Zheng%20and%20Jiangyong%20Ying%20and%20Huiyu%20Duan%20and%20Chunyi%20Li%20and%20Zicheng%20Zhang%20and%20Jing%20Liu%20and%20Xiaohong%20Liu%20and%20Guangtao%20Zhai&entry.1292438233=Large%20multimodal%20models%20%28LMMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%20a%20wide%20range%20of%20tasks%2C%20however%20their%20knowledge%20and%20abilities%20in%20the%20cross-view%20geo-localization%20and%20pose%20estimation%20domains%20remain%20unexplored%2C%20despite%20potential%20benefits%20for%20navigation%2C%20autonomous%20driving%2C%20outdoor%20robotics%2C%20%5Ctextit%7Betc%7D.%20To%20bridge%20this%20gap%2C%20we%20introduce%20%5Ctextbf%7BGeoX-Bench%7D%2C%20a%20comprehensive%20%5Cunderline%7BBench%7Dmark%20designed%20to%20explore%20and%20evaluate%20the%20capabilities%20of%20LMMs%20in%20%5Cunderline%7Bcross%7D-view%20%5Cunderline%7BGeo%7D-localization%20and%20pose%20estimation.%20Specifically%2C%20GeoX-Bench%20contains%2010%2C859%20panoramic-satellite%20image%20pairs%20spanning%20128%20cities%20in%2049%20countries%2C%20along%20with%20corresponding%20755%2C976%20question-answering%20%28QA%29%20pairs.%20Among%20these%2C%2042%2C900%20QA%20pairs%20are%20designated%20for%20benchmarking%2C%20while%20the%20remaining%20are%20intended%20to%20enhance%20the%20capabilities%20of%20LMMs.%20Based%20on%20GeoX-Bench%2C%20we%20evaluate%20the%20capabilities%20of%2025%20state-of-the-art%20LMMs%20on%20cross-view%20geo-localization%20and%20pose%20estimation%20tasks%2C%20and%20further%20explore%20the%20empowered%20capabilities%20of%20instruction-tuning.%20Our%20benchmark%20demonstrate%20that%20while%20current%20LMMs%20achieve%20impressive%20performance%20in%20geo-localization%20tasks%2C%20their%20effectiveness%20declines%20significantly%20on%20the%20more%20complex%20pose%20estimation%20tasks%2C%20highlighting%20a%20critical%20area%20for%20future%20improvement%2C%20and%20instruction-tuning%20LMMs%20on%20the%20training%20data%20of%20GeoX-Bench%20can%20significantly%20improve%20the%20cross-view%20geo-sense%20abilities.%20The%20GeoX-Bench%20is%20available%20at%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/IntMeGroup/GeoX-Bench%7D.&entry.1838667208=http%3A//arxiv.org/abs/2511.13259v1&entry.124074799=Read"},
{"title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation", "author": "Lingfeng Zhang and Yuchen Zhang and Hongsheng Li and Haoxiang Fu and Yingbo Tang and Hangjun Ye and Long Chen and Xiaojun Liang and Xiaoshuai Hao and Wenbo Ding", "abstract": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.", "link": "http://arxiv.org/abs/2511.13269v1", "date": "2025-11-17", "relevancy": 2.852, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20your%20VLM%20Sky-Ready%3F%20A%20Comprehensive%20Spatial%20Intelligence%20Benchmark%20for%20UAV%20Navigation&body=Title%3A%20Is%20your%20VLM%20Sky-Ready%3F%20A%20Comprehensive%20Spatial%20Intelligence%20Benchmark%20for%20UAV%20Navigation%0AAuthor%3A%20Lingfeng%20Zhang%20and%20Yuchen%20Zhang%20and%20Hongsheng%20Li%20and%20Haoxiang%20Fu%20and%20Yingbo%20Tang%20and%20Hangjun%20Ye%20and%20Long%20Chen%20and%20Xiaojun%20Liang%20and%20Xiaoshuai%20Hao%20and%20Wenbo%20Ding%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%2C%20leveraging%20their%20powerful%20visual%20perception%20and%20reasoning%20capabilities%2C%20have%20been%20widely%20applied%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20tasks.%20However%2C%20the%20spatial%20intelligence%20capabilities%20of%20existing%20VLMs%20in%20UAV%20scenarios%20remain%20largely%20unexplored%2C%20raising%20concerns%20about%20their%20effectiveness%20in%20navigating%20and%20interpreting%20dynamic%20environments.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SpatialSky-Bench%2C%20a%20comprehensive%20benchmark%20specifically%20designed%20to%20evaluate%20the%20spatial%20intelligence%20capabilities%20of%20VLMs%20in%20UAV%20navigation.%20Our%20benchmark%20comprises%20two%20categories-Environmental%20Perception%20and%20Scene%20Understanding-divided%20into%2013%20subcategories%2C%20including%20bounding%20boxes%2C%20color%2C%20distance%2C%20height%2C%20and%20landing%20safety%20analysis%2C%20among%20others.%20Extensive%20evaluations%20of%20various%20mainstream%20open-source%20and%20closed-source%20VLMs%20reveal%20unsatisfactory%20performance%20in%20complex%20UAV%20navigation%20scenarios%2C%20highlighting%20significant%20gaps%20in%20their%20spatial%20capabilities.%20To%20address%20this%20challenge%2C%20we%20developed%20the%20SpatialSky-Dataset%2C%20a%20comprehensive%20dataset%20containing%201M%20samples%20with%20diverse%20annotations%20across%20various%20scenarios.%20Leveraging%20this%20dataset%2C%20we%20introduce%20Sky-VLM%2C%20a%20specialized%20VLM%20designed%20for%20UAV%20spatial%20reasoning%20across%20multiple%20granularities%20and%20contexts.%20Extensive%20experimental%20results%20demonstrate%20that%20Sky-VLM%20achieves%20state-of-the-art%20performance%20across%20all%20benchmark%20tasks%2C%20paving%20the%20way%20for%20the%20development%20of%20VLMs%20suitable%20for%20UAV%20scenarios.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/linglingxiansen/SpatialSKy.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520your%2520VLM%2520Sky-Ready%253F%2520A%2520Comprehensive%2520Spatial%2520Intelligence%2520Benchmark%2520for%2520UAV%2520Navigation%26entry.906535625%3DLingfeng%2520Zhang%2520and%2520Yuchen%2520Zhang%2520and%2520Hongsheng%2520Li%2520and%2520Haoxiang%2520Fu%2520and%2520Yingbo%2520Tang%2520and%2520Hangjun%2520Ye%2520and%2520Long%2520Chen%2520and%2520Xiaojun%2520Liang%2520and%2520Xiaoshuai%2520Hao%2520and%2520Wenbo%2520Ding%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%252C%2520leveraging%2520their%2520powerful%2520visual%2520perception%2520and%2520reasoning%2520capabilities%252C%2520have%2520been%2520widely%2520applied%2520in%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520tasks.%2520However%252C%2520the%2520spatial%2520intelligence%2520capabilities%2520of%2520existing%2520VLMs%2520in%2520UAV%2520scenarios%2520remain%2520largely%2520unexplored%252C%2520raising%2520concerns%2520about%2520their%2520effectiveness%2520in%2520navigating%2520and%2520interpreting%2520dynamic%2520environments.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520SpatialSky-Bench%252C%2520a%2520comprehensive%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520the%2520spatial%2520intelligence%2520capabilities%2520of%2520VLMs%2520in%2520UAV%2520navigation.%2520Our%2520benchmark%2520comprises%2520two%2520categories-Environmental%2520Perception%2520and%2520Scene%2520Understanding-divided%2520into%252013%2520subcategories%252C%2520including%2520bounding%2520boxes%252C%2520color%252C%2520distance%252C%2520height%252C%2520and%2520landing%2520safety%2520analysis%252C%2520among%2520others.%2520Extensive%2520evaluations%2520of%2520various%2520mainstream%2520open-source%2520and%2520closed-source%2520VLMs%2520reveal%2520unsatisfactory%2520performance%2520in%2520complex%2520UAV%2520navigation%2520scenarios%252C%2520highlighting%2520significant%2520gaps%2520in%2520their%2520spatial%2520capabilities.%2520To%2520address%2520this%2520challenge%252C%2520we%2520developed%2520the%2520SpatialSky-Dataset%252C%2520a%2520comprehensive%2520dataset%2520containing%25201M%2520samples%2520with%2520diverse%2520annotations%2520across%2520various%2520scenarios.%2520Leveraging%2520this%2520dataset%252C%2520we%2520introduce%2520Sky-VLM%252C%2520a%2520specialized%2520VLM%2520designed%2520for%2520UAV%2520spatial%2520reasoning%2520across%2520multiple%2520granularities%2520and%2520contexts.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520Sky-VLM%2520achieves%2520state-of-the-art%2520performance%2520across%2520all%2520benchmark%2520tasks%252C%2520paving%2520the%2520way%2520for%2520the%2520development%2520of%2520VLMs%2520suitable%2520for%2520UAV%2520scenarios.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/linglingxiansen/SpatialSKy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20your%20VLM%20Sky-Ready%3F%20A%20Comprehensive%20Spatial%20Intelligence%20Benchmark%20for%20UAV%20Navigation&entry.906535625=Lingfeng%20Zhang%20and%20Yuchen%20Zhang%20and%20Hongsheng%20Li%20and%20Haoxiang%20Fu%20and%20Yingbo%20Tang%20and%20Hangjun%20Ye%20and%20Long%20Chen%20and%20Xiaojun%20Liang%20and%20Xiaoshuai%20Hao%20and%20Wenbo%20Ding&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%2C%20leveraging%20their%20powerful%20visual%20perception%20and%20reasoning%20capabilities%2C%20have%20been%20widely%20applied%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20tasks.%20However%2C%20the%20spatial%20intelligence%20capabilities%20of%20existing%20VLMs%20in%20UAV%20scenarios%20remain%20largely%20unexplored%2C%20raising%20concerns%20about%20their%20effectiveness%20in%20navigating%20and%20interpreting%20dynamic%20environments.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SpatialSky-Bench%2C%20a%20comprehensive%20benchmark%20specifically%20designed%20to%20evaluate%20the%20spatial%20intelligence%20capabilities%20of%20VLMs%20in%20UAV%20navigation.%20Our%20benchmark%20comprises%20two%20categories-Environmental%20Perception%20and%20Scene%20Understanding-divided%20into%2013%20subcategories%2C%20including%20bounding%20boxes%2C%20color%2C%20distance%2C%20height%2C%20and%20landing%20safety%20analysis%2C%20among%20others.%20Extensive%20evaluations%20of%20various%20mainstream%20open-source%20and%20closed-source%20VLMs%20reveal%20unsatisfactory%20performance%20in%20complex%20UAV%20navigation%20scenarios%2C%20highlighting%20significant%20gaps%20in%20their%20spatial%20capabilities.%20To%20address%20this%20challenge%2C%20we%20developed%20the%20SpatialSky-Dataset%2C%20a%20comprehensive%20dataset%20containing%201M%20samples%20with%20diverse%20annotations%20across%20various%20scenarios.%20Leveraging%20this%20dataset%2C%20we%20introduce%20Sky-VLM%2C%20a%20specialized%20VLM%20designed%20for%20UAV%20spatial%20reasoning%20across%20multiple%20granularities%20and%20contexts.%20Extensive%20experimental%20results%20demonstrate%20that%20Sky-VLM%20achieves%20state-of-the-art%20performance%20across%20all%20benchmark%20tasks%2C%20paving%20the%20way%20for%20the%20development%20of%20VLMs%20suitable%20for%20UAV%20scenarios.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/linglingxiansen/SpatialSKy.&entry.1838667208=http%3A//arxiv.org/abs/2511.13269v1&entry.124074799=Read"},
{"title": "vGamba: Attentive State Space Bottleneck for efficient Long-range Dependencies in Visual Recognition", "author": "Yunusa Haruna and Adamu Lawan", "abstract": "Capturing long-range dependencies efficiently is essential for visual recognition tasks, yet existing methods face limitations. Convolutional neural networks (CNNs) struggle with restricted receptive fields, while Vision Transformers (ViTs) achieve global context and long-range modeling at a high computational cost. State-space models (SSMs) offer an alternative, but their application in vision remains underexplored. This work introduces vGamba, a hybrid vision backbone that integrates SSMs with attention mechanisms to enhance efficiency and expressiveness. At its core, the Gamba bottleneck block that includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures, alongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion Module for effective feature representation. The interplay of these components ensures that vGamba leverages the low computational demands of SSMs while maintaining the accuracy of attention mechanisms for modeling long-range dependencies in vision tasks. Additionally, the Fusion module enables seamless interaction between these components. Extensive experiments on classification, detection, and segmentation tasks demonstrate that vGamba achieves a superior trade-off between accuracy and computational efficiency, outperforming several existing models.", "link": "http://arxiv.org/abs/2503.21262v2", "date": "2025-11-17", "relevancy": 2.8288, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5882}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5651}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vGamba%3A%20Attentive%20State%20Space%20Bottleneck%20for%20efficient%20Long-range%20Dependencies%20in%20Visual%20Recognition&body=Title%3A%20vGamba%3A%20Attentive%20State%20Space%20Bottleneck%20for%20efficient%20Long-range%20Dependencies%20in%20Visual%20Recognition%0AAuthor%3A%20Yunusa%20Haruna%20and%20Adamu%20Lawan%0AAbstract%3A%20Capturing%20long-range%20dependencies%20efficiently%20is%20essential%20for%20visual%20recognition%20tasks%2C%20yet%20existing%20methods%20face%20limitations.%20Convolutional%20neural%20networks%20%28CNNs%29%20struggle%20with%20restricted%20receptive%20fields%2C%20while%20Vision%20Transformers%20%28ViTs%29%20achieve%20global%20context%20and%20long-range%20modeling%20at%20a%20high%20computational%20cost.%20State-space%20models%20%28SSMs%29%20offer%20an%20alternative%2C%20but%20their%20application%20in%20vision%20remains%20underexplored.%20This%20work%20introduces%20vGamba%2C%20a%20hybrid%20vision%20backbone%20that%20integrates%20SSMs%20with%20attention%20mechanisms%20to%20enhance%20efficiency%20and%20expressiveness.%20At%20its%20core%2C%20the%20Gamba%20bottleneck%20block%20that%20includes%2C%20Gamba%20Cell%2C%20an%20adaptation%20of%20Mamba%20for%202D%20spatial%20structures%2C%20alongside%20a%20Multi-Head%20Self-Attention%20%28MHSA%29%20mechanism%20and%20a%20Gated%20Fusion%20Module%20for%20effective%20feature%20representation.%20The%20interplay%20of%20these%20components%20ensures%20that%20vGamba%20leverages%20the%20low%20computational%20demands%20of%20SSMs%20while%20maintaining%20the%20accuracy%20of%20attention%20mechanisms%20for%20modeling%20long-range%20dependencies%20in%20vision%20tasks.%20Additionally%2C%20the%20Fusion%20module%20enables%20seamless%20interaction%20between%20these%20components.%20Extensive%20experiments%20on%20classification%2C%20detection%2C%20and%20segmentation%20tasks%20demonstrate%20that%20vGamba%20achieves%20a%20superior%20trade-off%20between%20accuracy%20and%20computational%20efficiency%2C%20outperforming%20several%20existing%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2503.21262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvGamba%253A%2520Attentive%2520State%2520Space%2520Bottleneck%2520for%2520efficient%2520Long-range%2520Dependencies%2520in%2520Visual%2520Recognition%26entry.906535625%3DYunusa%2520Haruna%2520and%2520Adamu%2520Lawan%26entry.1292438233%3DCapturing%2520long-range%2520dependencies%2520efficiently%2520is%2520essential%2520for%2520visual%2520recognition%2520tasks%252C%2520yet%2520existing%2520methods%2520face%2520limitations.%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520struggle%2520with%2520restricted%2520receptive%2520fields%252C%2520while%2520Vision%2520Transformers%2520%2528ViTs%2529%2520achieve%2520global%2520context%2520and%2520long-range%2520modeling%2520at%2520a%2520high%2520computational%2520cost.%2520State-space%2520models%2520%2528SSMs%2529%2520offer%2520an%2520alternative%252C%2520but%2520their%2520application%2520in%2520vision%2520remains%2520underexplored.%2520This%2520work%2520introduces%2520vGamba%252C%2520a%2520hybrid%2520vision%2520backbone%2520that%2520integrates%2520SSMs%2520with%2520attention%2520mechanisms%2520to%2520enhance%2520efficiency%2520and%2520expressiveness.%2520At%2520its%2520core%252C%2520the%2520Gamba%2520bottleneck%2520block%2520that%2520includes%252C%2520Gamba%2520Cell%252C%2520an%2520adaptation%2520of%2520Mamba%2520for%25202D%2520spatial%2520structures%252C%2520alongside%2520a%2520Multi-Head%2520Self-Attention%2520%2528MHSA%2529%2520mechanism%2520and%2520a%2520Gated%2520Fusion%2520Module%2520for%2520effective%2520feature%2520representation.%2520The%2520interplay%2520of%2520these%2520components%2520ensures%2520that%2520vGamba%2520leverages%2520the%2520low%2520computational%2520demands%2520of%2520SSMs%2520while%2520maintaining%2520the%2520accuracy%2520of%2520attention%2520mechanisms%2520for%2520modeling%2520long-range%2520dependencies%2520in%2520vision%2520tasks.%2520Additionally%252C%2520the%2520Fusion%2520module%2520enables%2520seamless%2520interaction%2520between%2520these%2520components.%2520Extensive%2520experiments%2520on%2520classification%252C%2520detection%252C%2520and%2520segmentation%2520tasks%2520demonstrate%2520that%2520vGamba%2520achieves%2520a%2520superior%2520trade-off%2520between%2520accuracy%2520and%2520computational%2520efficiency%252C%2520outperforming%2520several%2520existing%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vGamba%3A%20Attentive%20State%20Space%20Bottleneck%20for%20efficient%20Long-range%20Dependencies%20in%20Visual%20Recognition&entry.906535625=Yunusa%20Haruna%20and%20Adamu%20Lawan&entry.1292438233=Capturing%20long-range%20dependencies%20efficiently%20is%20essential%20for%20visual%20recognition%20tasks%2C%20yet%20existing%20methods%20face%20limitations.%20Convolutional%20neural%20networks%20%28CNNs%29%20struggle%20with%20restricted%20receptive%20fields%2C%20while%20Vision%20Transformers%20%28ViTs%29%20achieve%20global%20context%20and%20long-range%20modeling%20at%20a%20high%20computational%20cost.%20State-space%20models%20%28SSMs%29%20offer%20an%20alternative%2C%20but%20their%20application%20in%20vision%20remains%20underexplored.%20This%20work%20introduces%20vGamba%2C%20a%20hybrid%20vision%20backbone%20that%20integrates%20SSMs%20with%20attention%20mechanisms%20to%20enhance%20efficiency%20and%20expressiveness.%20At%20its%20core%2C%20the%20Gamba%20bottleneck%20block%20that%20includes%2C%20Gamba%20Cell%2C%20an%20adaptation%20of%20Mamba%20for%202D%20spatial%20structures%2C%20alongside%20a%20Multi-Head%20Self-Attention%20%28MHSA%29%20mechanism%20and%20a%20Gated%20Fusion%20Module%20for%20effective%20feature%20representation.%20The%20interplay%20of%20these%20components%20ensures%20that%20vGamba%20leverages%20the%20low%20computational%20demands%20of%20SSMs%20while%20maintaining%20the%20accuracy%20of%20attention%20mechanisms%20for%20modeling%20long-range%20dependencies%20in%20vision%20tasks.%20Additionally%2C%20the%20Fusion%20module%20enables%20seamless%20interaction%20between%20these%20components.%20Extensive%20experiments%20on%20classification%2C%20detection%2C%20and%20segmentation%20tasks%20demonstrate%20that%20vGamba%20achieves%20a%20superior%20trade-off%20between%20accuracy%20and%20computational%20efficiency%2C%20outperforming%20several%20existing%20models.&entry.1838667208=http%3A//arxiv.org/abs/2503.21262v2&entry.124074799=Read"},
{"title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation", "author": "Qida Tan and Hongyu Yang and Wenchao Du", "abstract": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.", "link": "http://arxiv.org/abs/2511.13222v1", "date": "2025-11-17", "relevancy": 2.7799, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5614}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5565}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-Domain%20Adaptative%20Representation%20Learning%20for%20Gaze%20Estimation&body=Title%3A%20Hybrid-Domain%20Adaptative%20Representation%20Learning%20for%20Gaze%20Estimation%0AAuthor%3A%20Qida%20Tan%20and%20Hongyu%20Yang%20and%20Wenchao%20Du%0AAbstract%3A%20Appearance-based%20gaze%20estimation%2C%20aiming%20to%20predict%20accurate%203D%20gaze%20direction%20from%20a%20single%20facial%20image%2C%20has%20made%20promising%20progress%20in%20recent%20years.%20However%2C%20most%20methods%20suffer%20significant%20performance%20degradation%20in%20cross-domain%20evaluation%20due%20to%20interference%20from%20gaze-irrelevant%20factors%2C%20such%20as%20expressions%2C%20wearables%2C%20and%20image%20quality.%20To%20alleviate%20this%20problem%2C%20we%20present%20a%20novel%20Hybrid-domain%20Adaptative%20Representation%20Learning%20%28shorted%20by%20HARL%29%20framework%20that%20exploits%20multi-source%20hybrid%20datasets%20to%20learn%20robust%20gaze%20representation.%20More%20specifically%2C%20we%20propose%20to%20disentangle%20gaze-relevant%20representation%20from%20low-quality%20facial%20images%20by%20aligning%20features%20extracted%20from%20high-quality%20near-eye%20images%20in%20an%20unsupervised%20domain-adaptation%20manner%2C%20which%20hardly%20requires%20any%20computational%20or%20inference%20costs.%20Additionally%2C%20we%20analyze%20the%20effect%20of%20head-pose%20and%20design%20a%20simple%20yet%20efficient%20sparse%20graph%20fusion%20module%20to%20explore%20the%20geometric%20constraint%20between%20gaze%20direction%20and%20head-pose%2C%20leading%20to%20a%20dense%20and%20robust%20gaze%20representation.%20Extensive%20experiments%20on%20EyeDiap%2C%20MPIIFaceGaze%2C%20and%20Gaze360%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20accuracy%20of%20%24%5Ctextbf%7B5.02%7D%5E%7B%5Ccirc%7D%24%20and%20%24%5Ctextbf%7B3.36%7D%5E%7B%5Ccirc%7D%24%2C%20and%20%24%5Ctextbf%7B9.26%7D%5E%7B%5Ccirc%7D%24%20respectively%2C%20and%20present%20competitive%20performances%20through%20cross-dataset%20evaluation.%20The%20code%20is%20available%20at%20https%3A//github.com/da60266/HARL.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-Domain%2520Adaptative%2520Representation%2520Learning%2520for%2520Gaze%2520Estimation%26entry.906535625%3DQida%2520Tan%2520and%2520Hongyu%2520Yang%2520and%2520Wenchao%2520Du%26entry.1292438233%3DAppearance-based%2520gaze%2520estimation%252C%2520aiming%2520to%2520predict%2520accurate%25203D%2520gaze%2520direction%2520from%2520a%2520single%2520facial%2520image%252C%2520has%2520made%2520promising%2520progress%2520in%2520recent%2520years.%2520However%252C%2520most%2520methods%2520suffer%2520significant%2520performance%2520degradation%2520in%2520cross-domain%2520evaluation%2520due%2520to%2520interference%2520from%2520gaze-irrelevant%2520factors%252C%2520such%2520as%2520expressions%252C%2520wearables%252C%2520and%2520image%2520quality.%2520To%2520alleviate%2520this%2520problem%252C%2520we%2520present%2520a%2520novel%2520Hybrid-domain%2520Adaptative%2520Representation%2520Learning%2520%2528shorted%2520by%2520HARL%2529%2520framework%2520that%2520exploits%2520multi-source%2520hybrid%2520datasets%2520to%2520learn%2520robust%2520gaze%2520representation.%2520More%2520specifically%252C%2520we%2520propose%2520to%2520disentangle%2520gaze-relevant%2520representation%2520from%2520low-quality%2520facial%2520images%2520by%2520aligning%2520features%2520extracted%2520from%2520high-quality%2520near-eye%2520images%2520in%2520an%2520unsupervised%2520domain-adaptation%2520manner%252C%2520which%2520hardly%2520requires%2520any%2520computational%2520or%2520inference%2520costs.%2520Additionally%252C%2520we%2520analyze%2520the%2520effect%2520of%2520head-pose%2520and%2520design%2520a%2520simple%2520yet%2520efficient%2520sparse%2520graph%2520fusion%2520module%2520to%2520explore%2520the%2520geometric%2520constraint%2520between%2520gaze%2520direction%2520and%2520head-pose%252C%2520leading%2520to%2520a%2520dense%2520and%2520robust%2520gaze%2520representation.%2520Extensive%2520experiments%2520on%2520EyeDiap%252C%2520MPIIFaceGaze%252C%2520and%2520Gaze360%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520accuracy%2520of%2520%2524%255Ctextbf%257B5.02%257D%255E%257B%255Ccirc%257D%2524%2520and%2520%2524%255Ctextbf%257B3.36%257D%255E%257B%255Ccirc%257D%2524%252C%2520and%2520%2524%255Ctextbf%257B9.26%257D%255E%257B%255Ccirc%257D%2524%2520respectively%252C%2520and%2520present%2520competitive%2520performances%2520through%2520cross-dataset%2520evaluation.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/da60266/HARL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-Domain%20Adaptative%20Representation%20Learning%20for%20Gaze%20Estimation&entry.906535625=Qida%20Tan%20and%20Hongyu%20Yang%20and%20Wenchao%20Du&entry.1292438233=Appearance-based%20gaze%20estimation%2C%20aiming%20to%20predict%20accurate%203D%20gaze%20direction%20from%20a%20single%20facial%20image%2C%20has%20made%20promising%20progress%20in%20recent%20years.%20However%2C%20most%20methods%20suffer%20significant%20performance%20degradation%20in%20cross-domain%20evaluation%20due%20to%20interference%20from%20gaze-irrelevant%20factors%2C%20such%20as%20expressions%2C%20wearables%2C%20and%20image%20quality.%20To%20alleviate%20this%20problem%2C%20we%20present%20a%20novel%20Hybrid-domain%20Adaptative%20Representation%20Learning%20%28shorted%20by%20HARL%29%20framework%20that%20exploits%20multi-source%20hybrid%20datasets%20to%20learn%20robust%20gaze%20representation.%20More%20specifically%2C%20we%20propose%20to%20disentangle%20gaze-relevant%20representation%20from%20low-quality%20facial%20images%20by%20aligning%20features%20extracted%20from%20high-quality%20near-eye%20images%20in%20an%20unsupervised%20domain-adaptation%20manner%2C%20which%20hardly%20requires%20any%20computational%20or%20inference%20costs.%20Additionally%2C%20we%20analyze%20the%20effect%20of%20head-pose%20and%20design%20a%20simple%20yet%20efficient%20sparse%20graph%20fusion%20module%20to%20explore%20the%20geometric%20constraint%20between%20gaze%20direction%20and%20head-pose%2C%20leading%20to%20a%20dense%20and%20robust%20gaze%20representation.%20Extensive%20experiments%20on%20EyeDiap%2C%20MPIIFaceGaze%2C%20and%20Gaze360%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20accuracy%20of%20%24%5Ctextbf%7B5.02%7D%5E%7B%5Ccirc%7D%24%20and%20%24%5Ctextbf%7B3.36%7D%5E%7B%5Ccirc%7D%24%2C%20and%20%24%5Ctextbf%7B9.26%7D%5E%7B%5Ccirc%7D%24%20respectively%2C%20and%20present%20competitive%20performances%20through%20cross-dataset%20evaluation.%20The%20code%20is%20available%20at%20https%3A//github.com/da60266/HARL.&entry.1838667208=http%3A//arxiv.org/abs/2511.13222v1&entry.124074799=Read"},
{"title": "TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model", "author": "Ao Li and Yuxiang Duan and Jinghui Zhang and Congbo Ma and Yutong Xie and Gustavo Carneiro and Mohammad Yaqub and Hu Wang", "abstract": "Large Vision-Language Models (LVLMs) have advanced multimodal learning but face high computational costs due to the large number of visual tokens, motivating token pruning to improve inference efficiency. The key challenge lies in identifying which tokens are truly important. Most existing approaches rely on attention-based criteria to estimate token importance. However, they inherently suffer from certain limitations, such as positional bias. In this work, we explore a new perspective on token importance based on token transitions in LVLMs. We observe that the transition of token representations provides a meaningful signal of semantic information. Based on this insight, we propose TransPrune, a training-free and efficient token pruning method. Specifically, TransPrune progressively prunes tokens by assessing their importance through a combination of Token Transition Variation (TTV)-which measures changes in both the magnitude and direction of token representations-and Instruction-Guided Attention (IGA), which measures how strongly the instruction attends to image tokens via attention. Extensive experiments demonstrate that TransPrune achieves comparable multimodal performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV alone can serve as an effective criterion without relying on attention, achieving performance comparable to attention-based methods. The code will be made publicly available upon acceptance of the paper at https://github.com/liaolea/TransPrune.", "link": "http://arxiv.org/abs/2507.20630v2", "date": "2025-11-17", "relevancy": 2.7497, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransPrune%3A%20Token%20Transition%20Pruning%20for%20Efficient%20Large%20Vision-Language%20Model&body=Title%3A%20TransPrune%3A%20Token%20Transition%20Pruning%20for%20Efficient%20Large%20Vision-Language%20Model%0AAuthor%3A%20Ao%20Li%20and%20Yuxiang%20Duan%20and%20Jinghui%20Zhang%20and%20Congbo%20Ma%20and%20Yutong%20Xie%20and%20Gustavo%20Carneiro%20and%20Mohammad%20Yaqub%20and%20Hu%20Wang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20advanced%20multimodal%20learning%20but%20face%20high%20computational%20costs%20due%20to%20the%20large%20number%20of%20visual%20tokens%2C%20motivating%20token%20pruning%20to%20improve%20inference%20efficiency.%20The%20key%20challenge%20lies%20in%20identifying%20which%20tokens%20are%20truly%20important.%20Most%20existing%20approaches%20rely%20on%20attention-based%20criteria%20to%20estimate%20token%20importance.%20However%2C%20they%20inherently%20suffer%20from%20certain%20limitations%2C%20such%20as%20positional%20bias.%20In%20this%20work%2C%20we%20explore%20a%20new%20perspective%20on%20token%20importance%20based%20on%20token%20transitions%20in%20LVLMs.%20We%20observe%20that%20the%20transition%20of%20token%20representations%20provides%20a%20meaningful%20signal%20of%20semantic%20information.%20Based%20on%20this%20insight%2C%20we%20propose%20TransPrune%2C%20a%20training-free%20and%20efficient%20token%20pruning%20method.%20Specifically%2C%20TransPrune%20progressively%20prunes%20tokens%20by%20assessing%20their%20importance%20through%20a%20combination%20of%20Token%20Transition%20Variation%20%28TTV%29-which%20measures%20changes%20in%20both%20the%20magnitude%20and%20direction%20of%20token%20representations-and%20Instruction-Guided%20Attention%20%28IGA%29%2C%20which%20measures%20how%20strongly%20the%20instruction%20attends%20to%20image%20tokens%20via%20attention.%20Extensive%20experiments%20demonstrate%20that%20TransPrune%20achieves%20comparable%20multimodal%20performance%20to%20original%20LVLMs%2C%20such%20as%20LLaVA-v1.5%20and%20LLaVA-Next%2C%20across%20eight%20benchmarks%2C%20while%20reducing%20inference%20TFLOPs%20by%20more%20than%20half.%20Moreover%2C%20TTV%20alone%20can%20serve%20as%20an%20effective%20criterion%20without%20relying%20on%20attention%2C%20achieving%20performance%20comparable%20to%20attention-based%20methods.%20The%20code%20will%20be%20made%20publicly%20available%20upon%20acceptance%20of%20the%20paper%20at%20https%3A//github.com/liaolea/TransPrune.%0ALink%3A%20http%3A//arxiv.org/abs/2507.20630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransPrune%253A%2520Token%2520Transition%2520Pruning%2520for%2520Efficient%2520Large%2520Vision-Language%2520Model%26entry.906535625%3DAo%2520Li%2520and%2520Yuxiang%2520Duan%2520and%2520Jinghui%2520Zhang%2520and%2520Congbo%2520Ma%2520and%2520Yutong%2520Xie%2520and%2520Gustavo%2520Carneiro%2520and%2520Mohammad%2520Yaqub%2520and%2520Hu%2520Wang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520advanced%2520multimodal%2520learning%2520but%2520face%2520high%2520computational%2520costs%2520due%2520to%2520the%2520large%2520number%2520of%2520visual%2520tokens%252C%2520motivating%2520token%2520pruning%2520to%2520improve%2520inference%2520efficiency.%2520The%2520key%2520challenge%2520lies%2520in%2520identifying%2520which%2520tokens%2520are%2520truly%2520important.%2520Most%2520existing%2520approaches%2520rely%2520on%2520attention-based%2520criteria%2520to%2520estimate%2520token%2520importance.%2520However%252C%2520they%2520inherently%2520suffer%2520from%2520certain%2520limitations%252C%2520such%2520as%2520positional%2520bias.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520new%2520perspective%2520on%2520token%2520importance%2520based%2520on%2520token%2520transitions%2520in%2520LVLMs.%2520We%2520observe%2520that%2520the%2520transition%2520of%2520token%2520representations%2520provides%2520a%2520meaningful%2520signal%2520of%2520semantic%2520information.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520TransPrune%252C%2520a%2520training-free%2520and%2520efficient%2520token%2520pruning%2520method.%2520Specifically%252C%2520TransPrune%2520progressively%2520prunes%2520tokens%2520by%2520assessing%2520their%2520importance%2520through%2520a%2520combination%2520of%2520Token%2520Transition%2520Variation%2520%2528TTV%2529-which%2520measures%2520changes%2520in%2520both%2520the%2520magnitude%2520and%2520direction%2520of%2520token%2520representations-and%2520Instruction-Guided%2520Attention%2520%2528IGA%2529%252C%2520which%2520measures%2520how%2520strongly%2520the%2520instruction%2520attends%2520to%2520image%2520tokens%2520via%2520attention.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TransPrune%2520achieves%2520comparable%2520multimodal%2520performance%2520to%2520original%2520LVLMs%252C%2520such%2520as%2520LLaVA-v1.5%2520and%2520LLaVA-Next%252C%2520across%2520eight%2520benchmarks%252C%2520while%2520reducing%2520inference%2520TFLOPs%2520by%2520more%2520than%2520half.%2520Moreover%252C%2520TTV%2520alone%2520can%2520serve%2520as%2520an%2520effective%2520criterion%2520without%2520relying%2520on%2520attention%252C%2520achieving%2520performance%2520comparable%2520to%2520attention-based%2520methods.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance%2520of%2520the%2520paper%2520at%2520https%253A//github.com/liaolea/TransPrune.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransPrune%3A%20Token%20Transition%20Pruning%20for%20Efficient%20Large%20Vision-Language%20Model&entry.906535625=Ao%20Li%20and%20Yuxiang%20Duan%20and%20Jinghui%20Zhang%20and%20Congbo%20Ma%20and%20Yutong%20Xie%20and%20Gustavo%20Carneiro%20and%20Mohammad%20Yaqub%20and%20Hu%20Wang&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20advanced%20multimodal%20learning%20but%20face%20high%20computational%20costs%20due%20to%20the%20large%20number%20of%20visual%20tokens%2C%20motivating%20token%20pruning%20to%20improve%20inference%20efficiency.%20The%20key%20challenge%20lies%20in%20identifying%20which%20tokens%20are%20truly%20important.%20Most%20existing%20approaches%20rely%20on%20attention-based%20criteria%20to%20estimate%20token%20importance.%20However%2C%20they%20inherently%20suffer%20from%20certain%20limitations%2C%20such%20as%20positional%20bias.%20In%20this%20work%2C%20we%20explore%20a%20new%20perspective%20on%20token%20importance%20based%20on%20token%20transitions%20in%20LVLMs.%20We%20observe%20that%20the%20transition%20of%20token%20representations%20provides%20a%20meaningful%20signal%20of%20semantic%20information.%20Based%20on%20this%20insight%2C%20we%20propose%20TransPrune%2C%20a%20training-free%20and%20efficient%20token%20pruning%20method.%20Specifically%2C%20TransPrune%20progressively%20prunes%20tokens%20by%20assessing%20their%20importance%20through%20a%20combination%20of%20Token%20Transition%20Variation%20%28TTV%29-which%20measures%20changes%20in%20both%20the%20magnitude%20and%20direction%20of%20token%20representations-and%20Instruction-Guided%20Attention%20%28IGA%29%2C%20which%20measures%20how%20strongly%20the%20instruction%20attends%20to%20image%20tokens%20via%20attention.%20Extensive%20experiments%20demonstrate%20that%20TransPrune%20achieves%20comparable%20multimodal%20performance%20to%20original%20LVLMs%2C%20such%20as%20LLaVA-v1.5%20and%20LLaVA-Next%2C%20across%20eight%20benchmarks%2C%20while%20reducing%20inference%20TFLOPs%20by%20more%20than%20half.%20Moreover%2C%20TTV%20alone%20can%20serve%20as%20an%20effective%20criterion%20without%20relying%20on%20attention%2C%20achieving%20performance%20comparable%20to%20attention-based%20methods.%20The%20code%20will%20be%20made%20publicly%20available%20upon%20acceptance%20of%20the%20paper%20at%20https%3A//github.com/liaolea/TransPrune.&entry.1838667208=http%3A//arxiv.org/abs/2507.20630v2&entry.124074799=Read"},
{"title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming", "author": "Rufeng Chen and Shuaishuai Jiang and Jiyun Shen and AJung Moon and Lili Wei", "abstract": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.", "link": "http://arxiv.org/abs/2511.13271v1", "date": "2025-11-17", "relevancy": 2.7451, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5937}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5312}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Examining%20the%20Usage%20of%20Generative%20AI%20Models%20in%20Student%20Learning%20Activities%20for%20Software%20Programming&body=Title%3A%20Examining%20the%20Usage%20of%20Generative%20AI%20Models%20in%20Student%20Learning%20Activities%20for%20Software%20Programming%0AAuthor%3A%20Rufeng%20Chen%20and%20Shuaishuai%20Jiang%20and%20Jiyun%20Shen%20and%20AJung%20Moon%20and%20Lili%20Wei%0AAbstract%3A%20The%20rise%20of%20Generative%20AI%20%28GenAI%29%20tools%20like%20ChatGPT%20has%20created%20new%20opportunities%20and%20challenges%20for%20computing%20education.%20Existing%20research%20has%20primarily%20focused%20on%20GenAI%27s%20ability%20to%20complete%20educational%20tasks%20and%20its%20impact%20on%20student%20performance%2C%20often%20overlooking%20its%20effects%20on%20knowledge%20gains.%20In%20this%20study%2C%20we%20investigate%20how%20GenAI%20assistance%20compares%20to%20conventional%20online%20resources%20in%20supporting%20knowledge%20gains%20across%20different%20proficiency%20levels.%20We%20conducted%20a%20controlled%20user%20experiment%20with%2024%20undergraduate%20students%20of%20two%20different%20levels%20of%20programming%20experience%20%28beginner%2C%20intermediate%29%20to%20examine%20how%20students%20interact%20with%20ChatGPT%20while%20solving%20programming%20tasks.%20We%20analyzed%20task%20performance%2C%20conceptual%20understanding%2C%20and%20interaction%20behaviors.%20Our%20findings%20reveal%20that%20generating%20complete%20solutions%20with%20GenAI%20significantly%20improves%20task%20performance%2C%20especially%20for%20beginners%2C%20but%20does%20not%20consistently%20result%20in%20knowledge%20gains.%20Importantly%2C%20usage%20strategies%20differ%20by%20experience%3A%20beginners%20tend%20to%20rely%20heavily%20on%20GenAI%20toward%20task%20completion%20often%20without%20knowledge%20gain%20in%20the%20process%2C%20while%20intermediates%20adopt%20more%20selective%20approaches.%20We%20find%20that%20both%20over-reliance%20and%20minimal%20use%20result%20in%20weaker%20knowledge%20gains%20overall.%20Based%20on%20our%20results%2C%20we%20call%20on%20students%20and%20educators%20to%20adopt%20GenAI%20as%20a%20learning%20rather%20than%20a%20problem%20solving%20tool.%20Our%20study%20highlights%20the%20urgent%20need%20for%20guidance%20when%20integrating%20GenAI%20into%20programming%20education%20to%20foster%20deeper%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExamining%2520the%2520Usage%2520of%2520Generative%2520AI%2520Models%2520in%2520Student%2520Learning%2520Activities%2520for%2520Software%2520Programming%26entry.906535625%3DRufeng%2520Chen%2520and%2520Shuaishuai%2520Jiang%2520and%2520Jiyun%2520Shen%2520and%2520AJung%2520Moon%2520and%2520Lili%2520Wei%26entry.1292438233%3DThe%2520rise%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520tools%2520like%2520ChatGPT%2520has%2520created%2520new%2520opportunities%2520and%2520challenges%2520for%2520computing%2520education.%2520Existing%2520research%2520has%2520primarily%2520focused%2520on%2520GenAI%2527s%2520ability%2520to%2520complete%2520educational%2520tasks%2520and%2520its%2520impact%2520on%2520student%2520performance%252C%2520often%2520overlooking%2520its%2520effects%2520on%2520knowledge%2520gains.%2520In%2520this%2520study%252C%2520we%2520investigate%2520how%2520GenAI%2520assistance%2520compares%2520to%2520conventional%2520online%2520resources%2520in%2520supporting%2520knowledge%2520gains%2520across%2520different%2520proficiency%2520levels.%2520We%2520conducted%2520a%2520controlled%2520user%2520experiment%2520with%252024%2520undergraduate%2520students%2520of%2520two%2520different%2520levels%2520of%2520programming%2520experience%2520%2528beginner%252C%2520intermediate%2529%2520to%2520examine%2520how%2520students%2520interact%2520with%2520ChatGPT%2520while%2520solving%2520programming%2520tasks.%2520We%2520analyzed%2520task%2520performance%252C%2520conceptual%2520understanding%252C%2520and%2520interaction%2520behaviors.%2520Our%2520findings%2520reveal%2520that%2520generating%2520complete%2520solutions%2520with%2520GenAI%2520significantly%2520improves%2520task%2520performance%252C%2520especially%2520for%2520beginners%252C%2520but%2520does%2520not%2520consistently%2520result%2520in%2520knowledge%2520gains.%2520Importantly%252C%2520usage%2520strategies%2520differ%2520by%2520experience%253A%2520beginners%2520tend%2520to%2520rely%2520heavily%2520on%2520GenAI%2520toward%2520task%2520completion%2520often%2520without%2520knowledge%2520gain%2520in%2520the%2520process%252C%2520while%2520intermediates%2520adopt%2520more%2520selective%2520approaches.%2520We%2520find%2520that%2520both%2520over-reliance%2520and%2520minimal%2520use%2520result%2520in%2520weaker%2520knowledge%2520gains%2520overall.%2520Based%2520on%2520our%2520results%252C%2520we%2520call%2520on%2520students%2520and%2520educators%2520to%2520adopt%2520GenAI%2520as%2520a%2520learning%2520rather%2520than%2520a%2520problem%2520solving%2520tool.%2520Our%2520study%2520highlights%2520the%2520urgent%2520need%2520for%2520guidance%2520when%2520integrating%2520GenAI%2520into%2520programming%2520education%2520to%2520foster%2520deeper%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Examining%20the%20Usage%20of%20Generative%20AI%20Models%20in%20Student%20Learning%20Activities%20for%20Software%20Programming&entry.906535625=Rufeng%20Chen%20and%20Shuaishuai%20Jiang%20and%20Jiyun%20Shen%20and%20AJung%20Moon%20and%20Lili%20Wei&entry.1292438233=The%20rise%20of%20Generative%20AI%20%28GenAI%29%20tools%20like%20ChatGPT%20has%20created%20new%20opportunities%20and%20challenges%20for%20computing%20education.%20Existing%20research%20has%20primarily%20focused%20on%20GenAI%27s%20ability%20to%20complete%20educational%20tasks%20and%20its%20impact%20on%20student%20performance%2C%20often%20overlooking%20its%20effects%20on%20knowledge%20gains.%20In%20this%20study%2C%20we%20investigate%20how%20GenAI%20assistance%20compares%20to%20conventional%20online%20resources%20in%20supporting%20knowledge%20gains%20across%20different%20proficiency%20levels.%20We%20conducted%20a%20controlled%20user%20experiment%20with%2024%20undergraduate%20students%20of%20two%20different%20levels%20of%20programming%20experience%20%28beginner%2C%20intermediate%29%20to%20examine%20how%20students%20interact%20with%20ChatGPT%20while%20solving%20programming%20tasks.%20We%20analyzed%20task%20performance%2C%20conceptual%20understanding%2C%20and%20interaction%20behaviors.%20Our%20findings%20reveal%20that%20generating%20complete%20solutions%20with%20GenAI%20significantly%20improves%20task%20performance%2C%20especially%20for%20beginners%2C%20but%20does%20not%20consistently%20result%20in%20knowledge%20gains.%20Importantly%2C%20usage%20strategies%20differ%20by%20experience%3A%20beginners%20tend%20to%20rely%20heavily%20on%20GenAI%20toward%20task%20completion%20often%20without%20knowledge%20gain%20in%20the%20process%2C%20while%20intermediates%20adopt%20more%20selective%20approaches.%20We%20find%20that%20both%20over-reliance%20and%20minimal%20use%20result%20in%20weaker%20knowledge%20gains%20overall.%20Based%20on%20our%20results%2C%20we%20call%20on%20students%20and%20educators%20to%20adopt%20GenAI%20as%20a%20learning%20rather%20than%20a%20problem%20solving%20tool.%20Our%20study%20highlights%20the%20urgent%20need%20for%20guidance%20when%20integrating%20GenAI%20into%20programming%20education%20to%20foster%20deeper%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2511.13271v1&entry.124074799=Read"},
{"title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation", "author": "Henry Herzog and Favyen Bastani and Yawen Zhang and Gabriel Tseng and Joseph Redmon and Hadrien Sablon and Ryan Park and Jacob Morrison and Alexandra Buraczynski and Karen Farley and Joshua Hansen and Andrew Howe and Patrick Alan Johnson and Mark Otterlee and Ted Schmitt and Hunter Pitelka and Stephen Daspit and Rachel Ratner and Christopher Wilhelm and Sebastian Wood and Mike Jacobi and Hannah Kerner and Evan Shelhamer and Ali Farhadi and Ranjay Krishna and Patrick Beukema", "abstract": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.", "link": "http://arxiv.org/abs/2511.13655v1", "date": "2025-11-17", "relevancy": 2.7168, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OlmoEarth%3A%20Stable%20Latent%20Image%20Modeling%20for%20Multimodal%20Earth%20Observation&body=Title%3A%20OlmoEarth%3A%20Stable%20Latent%20Image%20Modeling%20for%20Multimodal%20Earth%20Observation%0AAuthor%3A%20Henry%20Herzog%20and%20Favyen%20Bastani%20and%20Yawen%20Zhang%20and%20Gabriel%20Tseng%20and%20Joseph%20Redmon%20and%20Hadrien%20Sablon%20and%20Ryan%20Park%20and%20Jacob%20Morrison%20and%20Alexandra%20Buraczynski%20and%20Karen%20Farley%20and%20Joshua%20Hansen%20and%20Andrew%20Howe%20and%20Patrick%20Alan%20Johnson%20and%20Mark%20Otterlee%20and%20Ted%20Schmitt%20and%20Hunter%20Pitelka%20and%20Stephen%20Daspit%20and%20Rachel%20Ratner%20and%20Christopher%20Wilhelm%20and%20Sebastian%20Wood%20and%20Mike%20Jacobi%20and%20Hannah%20Kerner%20and%20Evan%20Shelhamer%20and%20Ali%20Farhadi%20and%20Ranjay%20Krishna%20and%20Patrick%20Beukema%0AAbstract%3A%20Earth%20observation%20data%20presents%20a%20unique%20challenge%3A%20it%20is%20spatial%20like%20images%2C%20sequential%20like%20video%20or%20text%2C%20and%20highly%20multimodal.%20We%20present%20OlmoEarth%3A%20a%20multimodal%2C%20spatio-temporal%20foundation%20model%20that%20employs%20a%20novel%20self-supervised%20learning%20formulation%2C%20masking%20strategy%2C%20and%20loss%20all%20designed%20for%20the%20Earth%20observation%20domain.%20OlmoEarth%20achieves%20state-of-the-art%20performance%20compared%20to%2012%20other%20foundation%20models%20across%20a%20variety%20of%20research%20benchmarks%20and%20real-world%20tasks%20from%20external%20partners.%20When%20evaluating%20embeddings%20OlmoEarth%20achieves%20the%20best%20performance%20on%2015%20out%20of%2024%20tasks%2C%20and%20with%20full%20fine-tuning%20it%20is%20the%20best%20on%2019%20of%2029%20tasks.%20We%20deploy%20OlmoEarth%20as%20the%20backbone%20of%20an%20end-to-end%20platform%20for%20data%20collection%2C%20labeling%2C%20training%2C%20and%20inference%20of%20Earth%20observation%20models.%20The%20OlmoEarth%20Platform%20puts%20frontier%20foundation%20models%20and%20powerful%20data%20management%20tools%20into%20the%20hands%20of%20non-profits%20and%20NGOs%20working%20to%20solve%20the%20world%27s%20biggest%20problems.%20OlmoEarth%20source%20code%2C%20training%20data%2C%20and%20pre-trained%20weights%20are%20available%20at%20%24%5Chref%7Bhttps%3A//github.com/allenai/olmoearth_pretrain%7D%7B%5Ctext%7Bhttps%3A//github.com/allenai/olmoearth_pretrain%7D%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOlmoEarth%253A%2520Stable%2520Latent%2520Image%2520Modeling%2520for%2520Multimodal%2520Earth%2520Observation%26entry.906535625%3DHenry%2520Herzog%2520and%2520Favyen%2520Bastani%2520and%2520Yawen%2520Zhang%2520and%2520Gabriel%2520Tseng%2520and%2520Joseph%2520Redmon%2520and%2520Hadrien%2520Sablon%2520and%2520Ryan%2520Park%2520and%2520Jacob%2520Morrison%2520and%2520Alexandra%2520Buraczynski%2520and%2520Karen%2520Farley%2520and%2520Joshua%2520Hansen%2520and%2520Andrew%2520Howe%2520and%2520Patrick%2520Alan%2520Johnson%2520and%2520Mark%2520Otterlee%2520and%2520Ted%2520Schmitt%2520and%2520Hunter%2520Pitelka%2520and%2520Stephen%2520Daspit%2520and%2520Rachel%2520Ratner%2520and%2520Christopher%2520Wilhelm%2520and%2520Sebastian%2520Wood%2520and%2520Mike%2520Jacobi%2520and%2520Hannah%2520Kerner%2520and%2520Evan%2520Shelhamer%2520and%2520Ali%2520Farhadi%2520and%2520Ranjay%2520Krishna%2520and%2520Patrick%2520Beukema%26entry.1292438233%3DEarth%2520observation%2520data%2520presents%2520a%2520unique%2520challenge%253A%2520it%2520is%2520spatial%2520like%2520images%252C%2520sequential%2520like%2520video%2520or%2520text%252C%2520and%2520highly%2520multimodal.%2520We%2520present%2520OlmoEarth%253A%2520a%2520multimodal%252C%2520spatio-temporal%2520foundation%2520model%2520that%2520employs%2520a%2520novel%2520self-supervised%2520learning%2520formulation%252C%2520masking%2520strategy%252C%2520and%2520loss%2520all%2520designed%2520for%2520the%2520Earth%2520observation%2520domain.%2520OlmoEarth%2520achieves%2520state-of-the-art%2520performance%2520compared%2520to%252012%2520other%2520foundation%2520models%2520across%2520a%2520variety%2520of%2520research%2520benchmarks%2520and%2520real-world%2520tasks%2520from%2520external%2520partners.%2520When%2520evaluating%2520embeddings%2520OlmoEarth%2520achieves%2520the%2520best%2520performance%2520on%252015%2520out%2520of%252024%2520tasks%252C%2520and%2520with%2520full%2520fine-tuning%2520it%2520is%2520the%2520best%2520on%252019%2520of%252029%2520tasks.%2520We%2520deploy%2520OlmoEarth%2520as%2520the%2520backbone%2520of%2520an%2520end-to-end%2520platform%2520for%2520data%2520collection%252C%2520labeling%252C%2520training%252C%2520and%2520inference%2520of%2520Earth%2520observation%2520models.%2520The%2520OlmoEarth%2520Platform%2520puts%2520frontier%2520foundation%2520models%2520and%2520powerful%2520data%2520management%2520tools%2520into%2520the%2520hands%2520of%2520non-profits%2520and%2520NGOs%2520working%2520to%2520solve%2520the%2520world%2527s%2520biggest%2520problems.%2520OlmoEarth%2520source%2520code%252C%2520training%2520data%252C%2520and%2520pre-trained%2520weights%2520are%2520available%2520at%2520%2524%255Chref%257Bhttps%253A//github.com/allenai/olmoearth_pretrain%257D%257B%255Ctext%257Bhttps%253A//github.com/allenai/olmoearth_pretrain%257D%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OlmoEarth%3A%20Stable%20Latent%20Image%20Modeling%20for%20Multimodal%20Earth%20Observation&entry.906535625=Henry%20Herzog%20and%20Favyen%20Bastani%20and%20Yawen%20Zhang%20and%20Gabriel%20Tseng%20and%20Joseph%20Redmon%20and%20Hadrien%20Sablon%20and%20Ryan%20Park%20and%20Jacob%20Morrison%20and%20Alexandra%20Buraczynski%20and%20Karen%20Farley%20and%20Joshua%20Hansen%20and%20Andrew%20Howe%20and%20Patrick%20Alan%20Johnson%20and%20Mark%20Otterlee%20and%20Ted%20Schmitt%20and%20Hunter%20Pitelka%20and%20Stephen%20Daspit%20and%20Rachel%20Ratner%20and%20Christopher%20Wilhelm%20and%20Sebastian%20Wood%20and%20Mike%20Jacobi%20and%20Hannah%20Kerner%20and%20Evan%20Shelhamer%20and%20Ali%20Farhadi%20and%20Ranjay%20Krishna%20and%20Patrick%20Beukema&entry.1292438233=Earth%20observation%20data%20presents%20a%20unique%20challenge%3A%20it%20is%20spatial%20like%20images%2C%20sequential%20like%20video%20or%20text%2C%20and%20highly%20multimodal.%20We%20present%20OlmoEarth%3A%20a%20multimodal%2C%20spatio-temporal%20foundation%20model%20that%20employs%20a%20novel%20self-supervised%20learning%20formulation%2C%20masking%20strategy%2C%20and%20loss%20all%20designed%20for%20the%20Earth%20observation%20domain.%20OlmoEarth%20achieves%20state-of-the-art%20performance%20compared%20to%2012%20other%20foundation%20models%20across%20a%20variety%20of%20research%20benchmarks%20and%20real-world%20tasks%20from%20external%20partners.%20When%20evaluating%20embeddings%20OlmoEarth%20achieves%20the%20best%20performance%20on%2015%20out%20of%2024%20tasks%2C%20and%20with%20full%20fine-tuning%20it%20is%20the%20best%20on%2019%20of%2029%20tasks.%20We%20deploy%20OlmoEarth%20as%20the%20backbone%20of%20an%20end-to-end%20platform%20for%20data%20collection%2C%20labeling%2C%20training%2C%20and%20inference%20of%20Earth%20observation%20models.%20The%20OlmoEarth%20Platform%20puts%20frontier%20foundation%20models%20and%20powerful%20data%20management%20tools%20into%20the%20hands%20of%20non-profits%20and%20NGOs%20working%20to%20solve%20the%20world%27s%20biggest%20problems.%20OlmoEarth%20source%20code%2C%20training%20data%2C%20and%20pre-trained%20weights%20are%20available%20at%20%24%5Chref%7Bhttps%3A//github.com/allenai/olmoearth_pretrain%7D%7B%5Ctext%7Bhttps%3A//github.com/allenai/olmoearth_pretrain%7D%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2511.13655v1&entry.124074799=Read"},
{"title": "Attention Grounded Enhancement for Visual Document Retrieval", "author": "Wanqing Cui and Wei Huang and Yazhi Guo and Yibo Hu and Meiguang Jin and Junfeng Ma and Keping Bi", "abstract": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.", "link": "http://arxiv.org/abs/2511.13415v1", "date": "2025-11-17", "relevancy": 2.6952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Grounded%20Enhancement%20for%20Visual%20Document%20Retrieval&body=Title%3A%20Attention%20Grounded%20Enhancement%20for%20Visual%20Document%20Retrieval%0AAuthor%3A%20Wanqing%20Cui%20and%20Wei%20Huang%20and%20Yazhi%20Guo%20and%20Yibo%20Hu%20and%20Meiguang%20Jin%20and%20Junfeng%20Ma%20and%20Keping%20Bi%0AAbstract%3A%20Visual%20document%20retrieval%20requires%20understanding%20heterogeneous%20and%20multi-modal%20content%20to%20satisfy%20information%20needs.%20Recent%20advances%20use%20screenshot-based%20document%20encoding%20with%20fine-grained%20late%20interaction%2C%20significantly%20improving%20retrieval%20performance.%20However%2C%20retrievers%20are%20still%20trained%20with%20coarse%20global%20relevance%20labels%2C%20without%20revealing%20which%20regions%20support%20the%20match.%20As%20a%20result%2C%20retrievers%20tend%20to%20rely%20on%20surface-level%20cues%20and%20struggle%20to%20capture%20implicit%20semantic%20connections%2C%20hindering%20their%20ability%20to%20handle%20non-extractive%20queries.%20To%20alleviate%20this%20problem%2C%20we%20propose%20a%20%5Ctextbf%7BA%7Dttention-%5Ctextbf%7BG%7Drounded%20%5Ctextbf%7BRE%7Dtriever%20%5Ctextbf%7BE%7Dnhancement%20%28AGREE%29%20framework.%20AGREE%20leverages%20cross-modal%20attention%20from%20multimodal%20large%20language%20models%20as%20proxy%20local%20supervision%20to%20guide%20the%20identification%20of%20relevant%20document%20regions.%20During%20training%2C%20AGREE%20combines%20local%20signals%20with%20the%20global%20signals%20to%20jointly%20optimize%20the%20retriever%2C%20enabling%20it%20to%20learn%20not%20only%20whether%20documents%20match%2C%20but%20also%20which%20content%20drives%20relevance.%20Experiments%20on%20the%20challenging%20ViDoRe%20V2%20benchmark%20show%20that%20AGREE%20significantly%20outperforms%20the%20global-supervision-only%20baseline.%20Quantitative%20and%20qualitative%20analyses%20further%20demonstrate%20that%20AGREE%20promotes%20deeper%20alignment%20between%20query%20terms%20and%20document%20regions%2C%20moving%20beyond%20surface-level%20matching%20toward%20more%20accurate%20and%20interpretable%20retrieval.%20Our%20code%20is%20available%20at%3A%20https%3A//anonymous.4open.science/r/AGREE-2025.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Grounded%2520Enhancement%2520for%2520Visual%2520Document%2520Retrieval%26entry.906535625%3DWanqing%2520Cui%2520and%2520Wei%2520Huang%2520and%2520Yazhi%2520Guo%2520and%2520Yibo%2520Hu%2520and%2520Meiguang%2520Jin%2520and%2520Junfeng%2520Ma%2520and%2520Keping%2520Bi%26entry.1292438233%3DVisual%2520document%2520retrieval%2520requires%2520understanding%2520heterogeneous%2520and%2520multi-modal%2520content%2520to%2520satisfy%2520information%2520needs.%2520Recent%2520advances%2520use%2520screenshot-based%2520document%2520encoding%2520with%2520fine-grained%2520late%2520interaction%252C%2520significantly%2520improving%2520retrieval%2520performance.%2520However%252C%2520retrievers%2520are%2520still%2520trained%2520with%2520coarse%2520global%2520relevance%2520labels%252C%2520without%2520revealing%2520which%2520regions%2520support%2520the%2520match.%2520As%2520a%2520result%252C%2520retrievers%2520tend%2520to%2520rely%2520on%2520surface-level%2520cues%2520and%2520struggle%2520to%2520capture%2520implicit%2520semantic%2520connections%252C%2520hindering%2520their%2520ability%2520to%2520handle%2520non-extractive%2520queries.%2520To%2520alleviate%2520this%2520problem%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BA%257Dttention-%255Ctextbf%257BG%257Drounded%2520%255Ctextbf%257BRE%257Dtriever%2520%255Ctextbf%257BE%257Dnhancement%2520%2528AGREE%2529%2520framework.%2520AGREE%2520leverages%2520cross-modal%2520attention%2520from%2520multimodal%2520large%2520language%2520models%2520as%2520proxy%2520local%2520supervision%2520to%2520guide%2520the%2520identification%2520of%2520relevant%2520document%2520regions.%2520During%2520training%252C%2520AGREE%2520combines%2520local%2520signals%2520with%2520the%2520global%2520signals%2520to%2520jointly%2520optimize%2520the%2520retriever%252C%2520enabling%2520it%2520to%2520learn%2520not%2520only%2520whether%2520documents%2520match%252C%2520but%2520also%2520which%2520content%2520drives%2520relevance.%2520Experiments%2520on%2520the%2520challenging%2520ViDoRe%2520V2%2520benchmark%2520show%2520that%2520AGREE%2520significantly%2520outperforms%2520the%2520global-supervision-only%2520baseline.%2520Quantitative%2520and%2520qualitative%2520analyses%2520further%2520demonstrate%2520that%2520AGREE%2520promotes%2520deeper%2520alignment%2520between%2520query%2520terms%2520and%2520document%2520regions%252C%2520moving%2520beyond%2520surface-level%2520matching%2520toward%2520more%2520accurate%2520and%2520interpretable%2520retrieval.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//anonymous.4open.science/r/AGREE-2025.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Grounded%20Enhancement%20for%20Visual%20Document%20Retrieval&entry.906535625=Wanqing%20Cui%20and%20Wei%20Huang%20and%20Yazhi%20Guo%20and%20Yibo%20Hu%20and%20Meiguang%20Jin%20and%20Junfeng%20Ma%20and%20Keping%20Bi&entry.1292438233=Visual%20document%20retrieval%20requires%20understanding%20heterogeneous%20and%20multi-modal%20content%20to%20satisfy%20information%20needs.%20Recent%20advances%20use%20screenshot-based%20document%20encoding%20with%20fine-grained%20late%20interaction%2C%20significantly%20improving%20retrieval%20performance.%20However%2C%20retrievers%20are%20still%20trained%20with%20coarse%20global%20relevance%20labels%2C%20without%20revealing%20which%20regions%20support%20the%20match.%20As%20a%20result%2C%20retrievers%20tend%20to%20rely%20on%20surface-level%20cues%20and%20struggle%20to%20capture%20implicit%20semantic%20connections%2C%20hindering%20their%20ability%20to%20handle%20non-extractive%20queries.%20To%20alleviate%20this%20problem%2C%20we%20propose%20a%20%5Ctextbf%7BA%7Dttention-%5Ctextbf%7BG%7Drounded%20%5Ctextbf%7BRE%7Dtriever%20%5Ctextbf%7BE%7Dnhancement%20%28AGREE%29%20framework.%20AGREE%20leverages%20cross-modal%20attention%20from%20multimodal%20large%20language%20models%20as%20proxy%20local%20supervision%20to%20guide%20the%20identification%20of%20relevant%20document%20regions.%20During%20training%2C%20AGREE%20combines%20local%20signals%20with%20the%20global%20signals%20to%20jointly%20optimize%20the%20retriever%2C%20enabling%20it%20to%20learn%20not%20only%20whether%20documents%20match%2C%20but%20also%20which%20content%20drives%20relevance.%20Experiments%20on%20the%20challenging%20ViDoRe%20V2%20benchmark%20show%20that%20AGREE%20significantly%20outperforms%20the%20global-supervision-only%20baseline.%20Quantitative%20and%20qualitative%20analyses%20further%20demonstrate%20that%20AGREE%20promotes%20deeper%20alignment%20between%20query%20terms%20and%20document%20regions%2C%20moving%20beyond%20surface-level%20matching%20toward%20more%20accurate%20and%20interpretable%20retrieval.%20Our%20code%20is%20available%20at%3A%20https%3A//anonymous.4open.science/r/AGREE-2025.&entry.1838667208=http%3A//arxiv.org/abs/2511.13415v1&entry.124074799=Read"},
{"title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing", "author": "Jongha Kim and Minseong Bae and Sanghyeok Lee and Jinsung Yoon and Hyunwoo J. Kim", "abstract": "Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.", "link": "http://arxiv.org/abs/2511.13283v1", "date": "2025-11-17", "relevancy": 2.6867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabFlash%3A%20Efficient%20Table%20Understanding%20with%20Progressive%20Question%20Conditioning%20and%20Token%20Focusing&body=Title%3A%20TabFlash%3A%20Efficient%20Table%20Understanding%20with%20Progressive%20Question%20Conditioning%20and%20Token%20Focusing%0AAuthor%3A%20Jongha%20Kim%20and%20Minseong%20Bae%20and%20Sanghyeok%20Lee%20and%20Jinsung%20Yoon%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20Table%20images%20present%20unique%20challenges%20for%20effective%20and%20efficient%20understanding%20due%20to%20the%20need%20for%20question-specific%20focus%20and%20the%20presence%20of%20redundant%20background%20regions.%20Existing%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20approaches%20often%20overlook%20these%20characteristics%2C%20resulting%20in%20uninformative%20and%20redundant%20visual%20representations.%20To%20address%20these%20issues%2C%20we%20aim%20to%20generate%20visual%20features%20that%20are%20both%20informative%20and%20compact%20to%20improve%20table%20understanding.%20We%20first%20propose%20progressive%20question%20conditioning%2C%20which%20injects%20the%20question%20into%20Vision%20Transformer%20layers%20with%20gradually%20increasing%20frequency%2C%20considering%20each%20layer%27s%20capacity%20to%20handle%20additional%20information%2C%20to%20generate%20question-aware%20visual%20features.%20To%20reduce%20redundancy%2C%20we%20introduce%20a%20pruning%20strategy%20that%20discards%20background%20tokens%2C%20thereby%20improving%20efficiency.%20To%20mitigate%20information%20loss%20from%20pruning%2C%20we%20further%20propose%20token%20focusing%2C%20a%20training%20strategy%20that%20encourages%20the%20model%20to%20concentrate%20essential%20information%20in%20the%20retained%20tokens.%20By%20combining%20these%20approaches%2C%20we%20present%20TabFlash%2C%20an%20efficient%20and%20effective%20MLLM%20for%20table%20understanding.%20TabFlash%20achieves%20state-of-the-art%20performance%2C%20outperforming%20both%20open-source%20and%20proprietary%20MLLMs%2C%20while%20requiring%2027%25%20less%20FLOPs%20and%2030%25%20less%20memory%20usage%20compared%20to%20the%20second-best%20MLLM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabFlash%253A%2520Efficient%2520Table%2520Understanding%2520with%2520Progressive%2520Question%2520Conditioning%2520and%2520Token%2520Focusing%26entry.906535625%3DJongha%2520Kim%2520and%2520Minseong%2520Bae%2520and%2520Sanghyeok%2520Lee%2520and%2520Jinsung%2520Yoon%2520and%2520Hyunwoo%2520J.%2520Kim%26entry.1292438233%3DTable%2520images%2520present%2520unique%2520challenges%2520for%2520effective%2520and%2520efficient%2520understanding%2520due%2520to%2520the%2520need%2520for%2520question-specific%2520focus%2520and%2520the%2520presence%2520of%2520redundant%2520background%2520regions.%2520Existing%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520approaches%2520often%2520overlook%2520these%2520characteristics%252C%2520resulting%2520in%2520uninformative%2520and%2520redundant%2520visual%2520representations.%2520To%2520address%2520these%2520issues%252C%2520we%2520aim%2520to%2520generate%2520visual%2520features%2520that%2520are%2520both%2520informative%2520and%2520compact%2520to%2520improve%2520table%2520understanding.%2520We%2520first%2520propose%2520progressive%2520question%2520conditioning%252C%2520which%2520injects%2520the%2520question%2520into%2520Vision%2520Transformer%2520layers%2520with%2520gradually%2520increasing%2520frequency%252C%2520considering%2520each%2520layer%2527s%2520capacity%2520to%2520handle%2520additional%2520information%252C%2520to%2520generate%2520question-aware%2520visual%2520features.%2520To%2520reduce%2520redundancy%252C%2520we%2520introduce%2520a%2520pruning%2520strategy%2520that%2520discards%2520background%2520tokens%252C%2520thereby%2520improving%2520efficiency.%2520To%2520mitigate%2520information%2520loss%2520from%2520pruning%252C%2520we%2520further%2520propose%2520token%2520focusing%252C%2520a%2520training%2520strategy%2520that%2520encourages%2520the%2520model%2520to%2520concentrate%2520essential%2520information%2520in%2520the%2520retained%2520tokens.%2520By%2520combining%2520these%2520approaches%252C%2520we%2520present%2520TabFlash%252C%2520an%2520efficient%2520and%2520effective%2520MLLM%2520for%2520table%2520understanding.%2520TabFlash%2520achieves%2520state-of-the-art%2520performance%252C%2520outperforming%2520both%2520open-source%2520and%2520proprietary%2520MLLMs%252C%2520while%2520requiring%252027%2525%2520less%2520FLOPs%2520and%252030%2525%2520less%2520memory%2520usage%2520compared%2520to%2520the%2520second-best%2520MLLM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabFlash%3A%20Efficient%20Table%20Understanding%20with%20Progressive%20Question%20Conditioning%20and%20Token%20Focusing&entry.906535625=Jongha%20Kim%20and%20Minseong%20Bae%20and%20Sanghyeok%20Lee%20and%20Jinsung%20Yoon%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=Table%20images%20present%20unique%20challenges%20for%20effective%20and%20efficient%20understanding%20due%20to%20the%20need%20for%20question-specific%20focus%20and%20the%20presence%20of%20redundant%20background%20regions.%20Existing%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20approaches%20often%20overlook%20these%20characteristics%2C%20resulting%20in%20uninformative%20and%20redundant%20visual%20representations.%20To%20address%20these%20issues%2C%20we%20aim%20to%20generate%20visual%20features%20that%20are%20both%20informative%20and%20compact%20to%20improve%20table%20understanding.%20We%20first%20propose%20progressive%20question%20conditioning%2C%20which%20injects%20the%20question%20into%20Vision%20Transformer%20layers%20with%20gradually%20increasing%20frequency%2C%20considering%20each%20layer%27s%20capacity%20to%20handle%20additional%20information%2C%20to%20generate%20question-aware%20visual%20features.%20To%20reduce%20redundancy%2C%20we%20introduce%20a%20pruning%20strategy%20that%20discards%20background%20tokens%2C%20thereby%20improving%20efficiency.%20To%20mitigate%20information%20loss%20from%20pruning%2C%20we%20further%20propose%20token%20focusing%2C%20a%20training%20strategy%20that%20encourages%20the%20model%20to%20concentrate%20essential%20information%20in%20the%20retained%20tokens.%20By%20combining%20these%20approaches%2C%20we%20present%20TabFlash%2C%20an%20efficient%20and%20effective%20MLLM%20for%20table%20understanding.%20TabFlash%20achieves%20state-of-the-art%20performance%2C%20outperforming%20both%20open-source%20and%20proprietary%20MLLMs%2C%20while%20requiring%2027%25%20less%20FLOPs%20and%2030%25%20less%20memory%20usage%20compared%20to%20the%20second-best%20MLLM.&entry.1838667208=http%3A//arxiv.org/abs/2511.13283v1&entry.124074799=Read"},
{"title": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs", "author": "Zhe Sun and Yujun Cai and Jiayu Yao and Yiwei Wang", "abstract": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.", "link": "http://arxiv.org/abs/2511.13273v1", "date": "2025-11-17", "relevancy": 2.6785, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Blind%20Spot%3A%20Auditory%20Motion%20Perception%20Deficits%20in%20Audio%20LLMs&body=Title%3A%20Spatial%20Blind%20Spot%3A%20Auditory%20Motion%20Perception%20Deficits%20in%20Audio%20LLMs%0AAuthor%3A%20Zhe%20Sun%20and%20Yujun%20Cai%20and%20Jiayu%20Yao%20and%20Yiwei%20Wang%0AAbstract%3A%20Large%20Audio-Language%20Models%20%28LALMs%29%20have%20recently%20shown%20impressive%20progress%20in%20speech%20recognition%2C%20audio%20captioning%2C%20and%20auditory%20question%20answering.%20Yet%2C%20whether%20these%20models%20can%20perceive%20spatial%20dynamics%2C%20particularly%20the%20motion%20of%20sound%20sources%2C%20remains%20unclear.%20In%20this%20work%2C%20we%20uncover%20a%20systematic%20motion%20perception%20deficit%20in%20current%20ALLMs.%20To%20investigate%20this%20issue%2C%20we%20introduce%20AMPBench%2C%20the%20first%20benchmark%20explicitly%20designed%20to%20evaluate%20auditory%20motion%20understanding.%20AMPBench%20introduces%20a%20controlled%20question-answering%20benchmark%20designed%20to%20evaluate%20whether%20Audio-Language%20Models%20%28LALMs%29%20can%20infer%20the%20direction%20and%20trajectory%20of%20moving%20sound%20sources%20from%20binaural%20audio.%20Comprehensive%20quantitative%20and%20qualitative%20analyses%20reveal%20that%20current%20models%20struggle%20to%20reliably%20recognize%20motion%20cues%20or%20distinguish%20directional%20patterns.%20The%20average%20accuracy%20remains%20below%2050%25%2C%20underscoring%20a%20fundamental%20limitation%20in%20auditory%20spatial%20reasoning.%20Our%20study%20highlights%20a%20fundamental%20gap%20between%20human%20and%20model%20auditory%20spatial%20reasoning%2C%20providing%20both%20a%20diagnostic%20tool%20and%20new%20insight%20for%20enhancing%20spatial%20cognition%20in%20future%20Audio-Language%20Models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Blind%2520Spot%253A%2520Auditory%2520Motion%2520Perception%2520Deficits%2520in%2520Audio%2520LLMs%26entry.906535625%3DZhe%2520Sun%2520and%2520Yujun%2520Cai%2520and%2520Jiayu%2520Yao%2520and%2520Yiwei%2520Wang%26entry.1292438233%3DLarge%2520Audio-Language%2520Models%2520%2528LALMs%2529%2520have%2520recently%2520shown%2520impressive%2520progress%2520in%2520speech%2520recognition%252C%2520audio%2520captioning%252C%2520and%2520auditory%2520question%2520answering.%2520Yet%252C%2520whether%2520these%2520models%2520can%2520perceive%2520spatial%2520dynamics%252C%2520particularly%2520the%2520motion%2520of%2520sound%2520sources%252C%2520remains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520uncover%2520a%2520systematic%2520motion%2520perception%2520deficit%2520in%2520current%2520ALLMs.%2520To%2520investigate%2520this%2520issue%252C%2520we%2520introduce%2520AMPBench%252C%2520the%2520first%2520benchmark%2520explicitly%2520designed%2520to%2520evaluate%2520auditory%2520motion%2520understanding.%2520AMPBench%2520introduces%2520a%2520controlled%2520question-answering%2520benchmark%2520designed%2520to%2520evaluate%2520whether%2520Audio-Language%2520Models%2520%2528LALMs%2529%2520can%2520infer%2520the%2520direction%2520and%2520trajectory%2520of%2520moving%2520sound%2520sources%2520from%2520binaural%2520audio.%2520Comprehensive%2520quantitative%2520and%2520qualitative%2520analyses%2520reveal%2520that%2520current%2520models%2520struggle%2520to%2520reliably%2520recognize%2520motion%2520cues%2520or%2520distinguish%2520directional%2520patterns.%2520The%2520average%2520accuracy%2520remains%2520below%252050%2525%252C%2520underscoring%2520a%2520fundamental%2520limitation%2520in%2520auditory%2520spatial%2520reasoning.%2520Our%2520study%2520highlights%2520a%2520fundamental%2520gap%2520between%2520human%2520and%2520model%2520auditory%2520spatial%2520reasoning%252C%2520providing%2520both%2520a%2520diagnostic%2520tool%2520and%2520new%2520insight%2520for%2520enhancing%2520spatial%2520cognition%2520in%2520future%2520Audio-Language%2520Models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Blind%20Spot%3A%20Auditory%20Motion%20Perception%20Deficits%20in%20Audio%20LLMs&entry.906535625=Zhe%20Sun%20and%20Yujun%20Cai%20and%20Jiayu%20Yao%20and%20Yiwei%20Wang&entry.1292438233=Large%20Audio-Language%20Models%20%28LALMs%29%20have%20recently%20shown%20impressive%20progress%20in%20speech%20recognition%2C%20audio%20captioning%2C%20and%20auditory%20question%20answering.%20Yet%2C%20whether%20these%20models%20can%20perceive%20spatial%20dynamics%2C%20particularly%20the%20motion%20of%20sound%20sources%2C%20remains%20unclear.%20In%20this%20work%2C%20we%20uncover%20a%20systematic%20motion%20perception%20deficit%20in%20current%20ALLMs.%20To%20investigate%20this%20issue%2C%20we%20introduce%20AMPBench%2C%20the%20first%20benchmark%20explicitly%20designed%20to%20evaluate%20auditory%20motion%20understanding.%20AMPBench%20introduces%20a%20controlled%20question-answering%20benchmark%20designed%20to%20evaluate%20whether%20Audio-Language%20Models%20%28LALMs%29%20can%20infer%20the%20direction%20and%20trajectory%20of%20moving%20sound%20sources%20from%20binaural%20audio.%20Comprehensive%20quantitative%20and%20qualitative%20analyses%20reveal%20that%20current%20models%20struggle%20to%20reliably%20recognize%20motion%20cues%20or%20distinguish%20directional%20patterns.%20The%20average%20accuracy%20remains%20below%2050%25%2C%20underscoring%20a%20fundamental%20limitation%20in%20auditory%20spatial%20reasoning.%20Our%20study%20highlights%20a%20fundamental%20gap%20between%20human%20and%20model%20auditory%20spatial%20reasoning%2C%20providing%20both%20a%20diagnostic%20tool%20and%20new%20insight%20for%20enhancing%20spatial%20cognition%20in%20future%20Audio-Language%20Models.&entry.1838667208=http%3A//arxiv.org/abs/2511.13273v1&entry.124074799=Read"},
{"title": "Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models", "author": "Carlo Teo Pedretti and Davide Picca and Dario Rodighiero", "abstract": "Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.", "link": "http://arxiv.org/abs/2511.13378v1", "date": "2025-11-17", "relevancy": 2.6593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moving%20Pictures%20of%20Thought%3A%20Extracting%20Visual%20Knowledge%20in%20Charles%20S.%20Peirce%27s%20Manuscripts%20with%20Vision-Language%20Models&body=Title%3A%20Moving%20Pictures%20of%20Thought%3A%20Extracting%20Visual%20Knowledge%20in%20Charles%20S.%20Peirce%27s%20Manuscripts%20with%20Vision-Language%20Models%0AAuthor%3A%20Carlo%20Teo%20Pedretti%20and%20Davide%20Picca%20and%20Dario%20Rodighiero%0AAbstract%3A%20Diagrams%20are%20crucial%20yet%20underexplored%20tools%20in%20many%20disciplines%2C%20demonstrating%20the%20close%20connection%20between%20visual%20representation%20and%20scholarly%20reasoning.%20However%2C%20their%20iconic%20form%20poses%20obstacles%20to%20visual%20studies%2C%20intermedial%20analysis%2C%20and%20text-based%20digital%20workflows.%20In%20particular%2C%20Charles%20S.%20Peirce%20consistently%20advocated%20the%20use%20of%20diagrams%20as%20essential%20for%20reasoning%20and%20explanation.%20His%20manuscripts%2C%20often%20combining%20textual%20content%20with%20complex%20visual%20artifacts%2C%20provide%20a%20challenging%20case%20for%20studying%20documents%20involving%20heterogeneous%20materials.%20In%20this%20preliminary%20study%2C%20we%20investigate%20whether%20Visual%20Language%20Models%20%28VLMs%29%20can%20effectively%20help%20us%20identify%20and%20interpret%20such%20hybrid%20pages%20in%20context.%20First%2C%20we%20propose%20a%20workflow%20that%20%28i%29%20segments%20manuscript%20page%20layouts%2C%20%28ii%29%20reconnects%20each%20segment%20to%20IIIF-compliant%20annotations%2C%20and%20%28iii%29%20submits%20fragments%20containing%20diagrams%20to%20a%20VLM.%20In%20addition%2C%20by%20adopting%20Peirce%27s%20semiotic%20framework%2C%20we%20designed%20prompts%20to%20extract%20key%20knowledge%20about%20diagrams%20and%20produce%20concise%20captions.%20Finally%2C%20we%20integrated%20these%20captions%20into%20knowledge%20graphs%2C%20enabling%20structured%20representations%20of%20diagrammatic%20content%20within%20composite%20sources.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoving%2520Pictures%2520of%2520Thought%253A%2520Extracting%2520Visual%2520Knowledge%2520in%2520Charles%2520S.%2520Peirce%2527s%2520Manuscripts%2520with%2520Vision-Language%2520Models%26entry.906535625%3DCarlo%2520Teo%2520Pedretti%2520and%2520Davide%2520Picca%2520and%2520Dario%2520Rodighiero%26entry.1292438233%3DDiagrams%2520are%2520crucial%2520yet%2520underexplored%2520tools%2520in%2520many%2520disciplines%252C%2520demonstrating%2520the%2520close%2520connection%2520between%2520visual%2520representation%2520and%2520scholarly%2520reasoning.%2520However%252C%2520their%2520iconic%2520form%2520poses%2520obstacles%2520to%2520visual%2520studies%252C%2520intermedial%2520analysis%252C%2520and%2520text-based%2520digital%2520workflows.%2520In%2520particular%252C%2520Charles%2520S.%2520Peirce%2520consistently%2520advocated%2520the%2520use%2520of%2520diagrams%2520as%2520essential%2520for%2520reasoning%2520and%2520explanation.%2520His%2520manuscripts%252C%2520often%2520combining%2520textual%2520content%2520with%2520complex%2520visual%2520artifacts%252C%2520provide%2520a%2520challenging%2520case%2520for%2520studying%2520documents%2520involving%2520heterogeneous%2520materials.%2520In%2520this%2520preliminary%2520study%252C%2520we%2520investigate%2520whether%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520can%2520effectively%2520help%2520us%2520identify%2520and%2520interpret%2520such%2520hybrid%2520pages%2520in%2520context.%2520First%252C%2520we%2520propose%2520a%2520workflow%2520that%2520%2528i%2529%2520segments%2520manuscript%2520page%2520layouts%252C%2520%2528ii%2529%2520reconnects%2520each%2520segment%2520to%2520IIIF-compliant%2520annotations%252C%2520and%2520%2528iii%2529%2520submits%2520fragments%2520containing%2520diagrams%2520to%2520a%2520VLM.%2520In%2520addition%252C%2520by%2520adopting%2520Peirce%2527s%2520semiotic%2520framework%252C%2520we%2520designed%2520prompts%2520to%2520extract%2520key%2520knowledge%2520about%2520diagrams%2520and%2520produce%2520concise%2520captions.%2520Finally%252C%2520we%2520integrated%2520these%2520captions%2520into%2520knowledge%2520graphs%252C%2520enabling%2520structured%2520representations%2520of%2520diagrammatic%2520content%2520within%2520composite%2520sources.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20Pictures%20of%20Thought%3A%20Extracting%20Visual%20Knowledge%20in%20Charles%20S.%20Peirce%27s%20Manuscripts%20with%20Vision-Language%20Models&entry.906535625=Carlo%20Teo%20Pedretti%20and%20Davide%20Picca%20and%20Dario%20Rodighiero&entry.1292438233=Diagrams%20are%20crucial%20yet%20underexplored%20tools%20in%20many%20disciplines%2C%20demonstrating%20the%20close%20connection%20between%20visual%20representation%20and%20scholarly%20reasoning.%20However%2C%20their%20iconic%20form%20poses%20obstacles%20to%20visual%20studies%2C%20intermedial%20analysis%2C%20and%20text-based%20digital%20workflows.%20In%20particular%2C%20Charles%20S.%20Peirce%20consistently%20advocated%20the%20use%20of%20diagrams%20as%20essential%20for%20reasoning%20and%20explanation.%20His%20manuscripts%2C%20often%20combining%20textual%20content%20with%20complex%20visual%20artifacts%2C%20provide%20a%20challenging%20case%20for%20studying%20documents%20involving%20heterogeneous%20materials.%20In%20this%20preliminary%20study%2C%20we%20investigate%20whether%20Visual%20Language%20Models%20%28VLMs%29%20can%20effectively%20help%20us%20identify%20and%20interpret%20such%20hybrid%20pages%20in%20context.%20First%2C%20we%20propose%20a%20workflow%20that%20%28i%29%20segments%20manuscript%20page%20layouts%2C%20%28ii%29%20reconnects%20each%20segment%20to%20IIIF-compliant%20annotations%2C%20and%20%28iii%29%20submits%20fragments%20containing%20diagrams%20to%20a%20VLM.%20In%20addition%2C%20by%20adopting%20Peirce%27s%20semiotic%20framework%2C%20we%20designed%20prompts%20to%20extract%20key%20knowledge%20about%20diagrams%20and%20produce%20concise%20captions.%20Finally%2C%20we%20integrated%20these%20captions%20into%20knowledge%20graphs%2C%20enabling%20structured%20representations%20of%20diagrammatic%20content%20within%20composite%20sources.&entry.1838667208=http%3A//arxiv.org/abs/2511.13378v1&entry.124074799=Read"},
{"title": "FoleyBench: A Benchmark For Video-to-Audio Models", "author": "Satvik Dixit and Koichi Saito and Zhi Zhong and Yuki Mitsufuji and Chris Donahue", "abstract": "Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench", "link": "http://arxiv.org/abs/2511.13219v1", "date": "2025-11-17", "relevancy": 2.6508, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5648}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoleyBench%3A%20A%20Benchmark%20For%20Video-to-Audio%20Models&body=Title%3A%20FoleyBench%3A%20A%20Benchmark%20For%20Video-to-Audio%20Models%0AAuthor%3A%20Satvik%20Dixit%20and%20Koichi%20Saito%20and%20Zhi%20Zhong%20and%20Yuki%20Mitsufuji%20and%20Chris%20Donahue%0AAbstract%3A%20Video-to-audio%20generation%20%28V2A%29%20is%20of%20increasing%20importance%20in%20domains%20such%20as%20film%20post-production%2C%20AR/VR%2C%20and%20sound%20design%2C%20particularly%20for%20the%20creation%20of%20Foley%20sound%20effects%20synchronized%20with%20on-screen%20actions.%20Foley%20requires%20generating%20audio%20that%20is%20both%20semantically%20aligned%20with%20visible%20events%20and%20temporally%20aligned%20with%20their%20timing.%20Yet%2C%20there%20is%20a%20mismatch%20between%20evaluation%20and%20downstream%20applications%20due%20to%20the%20absence%20of%20a%20benchmark%20tailored%20to%20Foley-style%20scenarios.%20We%20find%20that%2074%25%20of%20videos%20from%20past%20evaluation%20datasets%20have%20poor%20audio-visual%20correspondence.%20Moreover%2C%20they%20are%20dominated%20by%20speech%20and%20music%2C%20domains%20that%20lie%20outside%20the%20use%20case%20for%20Foley.%20To%20address%20this%20gap%2C%20we%20introduce%20FoleyBench%2C%20the%20first%20large-scale%20benchmark%20explicitly%20designed%20for%20Foley-style%20V2A%20evaluation.%20FoleyBench%20contains%205%2C000%20%28video%2C%20ground-truth%20audio%2C%20text%20caption%29%20triplets%2C%20each%20featuring%20visible%20sound%20sources%20with%20audio%20causally%20tied%20to%20on-screen%20events.%20The%20dataset%20is%20built%20using%20an%20automated%2C%20scalable%20pipeline%20applied%20to%20in-the-wild%20internet%20videos%20from%20YouTube-based%20and%20Vimeo-based%20sources.%20Compared%20to%20past%20datasets%2C%20we%20show%20that%20videos%20from%20FoleyBench%20have%20stronger%20coverage%20of%20sound%20categories%20from%20a%20taxonomy%20specifically%20designed%20for%20Foley%20sound.%20Each%20clip%20is%20further%20labeled%20with%20metadata%20capturing%20source%20complexity%2C%20UCS/AudioSet%20category%2C%20and%20video%20length%2C%20enabling%20fine-grained%20analysis%20of%20model%20performance%20and%20failure%20modes.%20We%20benchmark%20several%20state-of-the-art%20V2A%20models%2C%20evaluating%20them%20on%20audio%20quality%2C%20audio-video%20alignment%2C%20temporal%20synchronization%2C%20and%20audio-text%20consistency.%20Samples%20are%20available%20at%3A%20https%3A//gclef-cmu.org/foleybench%0ALink%3A%20http%3A//arxiv.org/abs/2511.13219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoleyBench%253A%2520A%2520Benchmark%2520For%2520Video-to-Audio%2520Models%26entry.906535625%3DSatvik%2520Dixit%2520and%2520Koichi%2520Saito%2520and%2520Zhi%2520Zhong%2520and%2520Yuki%2520Mitsufuji%2520and%2520Chris%2520Donahue%26entry.1292438233%3DVideo-to-audio%2520generation%2520%2528V2A%2529%2520is%2520of%2520increasing%2520importance%2520in%2520domains%2520such%2520as%2520film%2520post-production%252C%2520AR/VR%252C%2520and%2520sound%2520design%252C%2520particularly%2520for%2520the%2520creation%2520of%2520Foley%2520sound%2520effects%2520synchronized%2520with%2520on-screen%2520actions.%2520Foley%2520requires%2520generating%2520audio%2520that%2520is%2520both%2520semantically%2520aligned%2520with%2520visible%2520events%2520and%2520temporally%2520aligned%2520with%2520their%2520timing.%2520Yet%252C%2520there%2520is%2520a%2520mismatch%2520between%2520evaluation%2520and%2520downstream%2520applications%2520due%2520to%2520the%2520absence%2520of%2520a%2520benchmark%2520tailored%2520to%2520Foley-style%2520scenarios.%2520We%2520find%2520that%252074%2525%2520of%2520videos%2520from%2520past%2520evaluation%2520datasets%2520have%2520poor%2520audio-visual%2520correspondence.%2520Moreover%252C%2520they%2520are%2520dominated%2520by%2520speech%2520and%2520music%252C%2520domains%2520that%2520lie%2520outside%2520the%2520use%2520case%2520for%2520Foley.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520FoleyBench%252C%2520the%2520first%2520large-scale%2520benchmark%2520explicitly%2520designed%2520for%2520Foley-style%2520V2A%2520evaluation.%2520FoleyBench%2520contains%25205%252C000%2520%2528video%252C%2520ground-truth%2520audio%252C%2520text%2520caption%2529%2520triplets%252C%2520each%2520featuring%2520visible%2520sound%2520sources%2520with%2520audio%2520causally%2520tied%2520to%2520on-screen%2520events.%2520The%2520dataset%2520is%2520built%2520using%2520an%2520automated%252C%2520scalable%2520pipeline%2520applied%2520to%2520in-the-wild%2520internet%2520videos%2520from%2520YouTube-based%2520and%2520Vimeo-based%2520sources.%2520Compared%2520to%2520past%2520datasets%252C%2520we%2520show%2520that%2520videos%2520from%2520FoleyBench%2520have%2520stronger%2520coverage%2520of%2520sound%2520categories%2520from%2520a%2520taxonomy%2520specifically%2520designed%2520for%2520Foley%2520sound.%2520Each%2520clip%2520is%2520further%2520labeled%2520with%2520metadata%2520capturing%2520source%2520complexity%252C%2520UCS/AudioSet%2520category%252C%2520and%2520video%2520length%252C%2520enabling%2520fine-grained%2520analysis%2520of%2520model%2520performance%2520and%2520failure%2520modes.%2520We%2520benchmark%2520several%2520state-of-the-art%2520V2A%2520models%252C%2520evaluating%2520them%2520on%2520audio%2520quality%252C%2520audio-video%2520alignment%252C%2520temporal%2520synchronization%252C%2520and%2520audio-text%2520consistency.%2520Samples%2520are%2520available%2520at%253A%2520https%253A//gclef-cmu.org/foleybench%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoleyBench%3A%20A%20Benchmark%20For%20Video-to-Audio%20Models&entry.906535625=Satvik%20Dixit%20and%20Koichi%20Saito%20and%20Zhi%20Zhong%20and%20Yuki%20Mitsufuji%20and%20Chris%20Donahue&entry.1292438233=Video-to-audio%20generation%20%28V2A%29%20is%20of%20increasing%20importance%20in%20domains%20such%20as%20film%20post-production%2C%20AR/VR%2C%20and%20sound%20design%2C%20particularly%20for%20the%20creation%20of%20Foley%20sound%20effects%20synchronized%20with%20on-screen%20actions.%20Foley%20requires%20generating%20audio%20that%20is%20both%20semantically%20aligned%20with%20visible%20events%20and%20temporally%20aligned%20with%20their%20timing.%20Yet%2C%20there%20is%20a%20mismatch%20between%20evaluation%20and%20downstream%20applications%20due%20to%20the%20absence%20of%20a%20benchmark%20tailored%20to%20Foley-style%20scenarios.%20We%20find%20that%2074%25%20of%20videos%20from%20past%20evaluation%20datasets%20have%20poor%20audio-visual%20correspondence.%20Moreover%2C%20they%20are%20dominated%20by%20speech%20and%20music%2C%20domains%20that%20lie%20outside%20the%20use%20case%20for%20Foley.%20To%20address%20this%20gap%2C%20we%20introduce%20FoleyBench%2C%20the%20first%20large-scale%20benchmark%20explicitly%20designed%20for%20Foley-style%20V2A%20evaluation.%20FoleyBench%20contains%205%2C000%20%28video%2C%20ground-truth%20audio%2C%20text%20caption%29%20triplets%2C%20each%20featuring%20visible%20sound%20sources%20with%20audio%20causally%20tied%20to%20on-screen%20events.%20The%20dataset%20is%20built%20using%20an%20automated%2C%20scalable%20pipeline%20applied%20to%20in-the-wild%20internet%20videos%20from%20YouTube-based%20and%20Vimeo-based%20sources.%20Compared%20to%20past%20datasets%2C%20we%20show%20that%20videos%20from%20FoleyBench%20have%20stronger%20coverage%20of%20sound%20categories%20from%20a%20taxonomy%20specifically%20designed%20for%20Foley%20sound.%20Each%20clip%20is%20further%20labeled%20with%20metadata%20capturing%20source%20complexity%2C%20UCS/AudioSet%20category%2C%20and%20video%20length%2C%20enabling%20fine-grained%20analysis%20of%20model%20performance%20and%20failure%20modes.%20We%20benchmark%20several%20state-of-the-art%20V2A%20models%2C%20evaluating%20them%20on%20audio%20quality%2C%20audio-video%20alignment%2C%20temporal%20synchronization%2C%20and%20audio-text%20consistency.%20Samples%20are%20available%20at%3A%20https%3A//gclef-cmu.org/foleybench&entry.1838667208=http%3A//arxiv.org/abs/2511.13219v1&entry.124074799=Read"},
{"title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models", "author": "Noam Tsfaty and Avishai Weizman and Liav Cohen and Moshe Tshuva and Yehudit Aperstein", "abstract": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.", "link": "http://arxiv.org/abs/2511.13276v1", "date": "2025-11-17", "relevancy": 2.6446, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5682}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5123}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recognition%20of%20Abnormal%20Events%20in%20Surveillance%20Videos%20using%20Weakly%20Supervised%20Dual-Encoder%20Models&body=Title%3A%20Recognition%20of%20Abnormal%20Events%20in%20Surveillance%20Videos%20using%20Weakly%20Supervised%20Dual-Encoder%20Models%0AAuthor%3A%20Noam%20Tsfaty%20and%20Avishai%20Weizman%20and%20Liav%20Cohen%20and%20Moshe%20Tshuva%20and%20Yehudit%20Aperstein%0AAbstract%3A%20We%20address%20the%20challenge%20of%20detecting%20rare%20and%20diverse%20anomalies%20in%20surveillance%20videos%20using%20only%20video-level%20supervision.%20Our%20dual-backbone%20framework%20combines%20convolutional%20and%20transformer%20representations%20through%20top-k%20pooling%2C%20achieving%2090.7%25%20area%20under%20the%20curve%20%28AUC%29%20on%20the%20UCF-Crime%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecognition%2520of%2520Abnormal%2520Events%2520in%2520Surveillance%2520Videos%2520using%2520Weakly%2520Supervised%2520Dual-Encoder%2520Models%26entry.906535625%3DNoam%2520Tsfaty%2520and%2520Avishai%2520Weizman%2520and%2520Liav%2520Cohen%2520and%2520Moshe%2520Tshuva%2520and%2520Yehudit%2520Aperstein%26entry.1292438233%3DWe%2520address%2520the%2520challenge%2520of%2520detecting%2520rare%2520and%2520diverse%2520anomalies%2520in%2520surveillance%2520videos%2520using%2520only%2520video-level%2520supervision.%2520Our%2520dual-backbone%2520framework%2520combines%2520convolutional%2520and%2520transformer%2520representations%2520through%2520top-k%2520pooling%252C%2520achieving%252090.7%2525%2520area%2520under%2520the%2520curve%2520%2528AUC%2529%2520on%2520the%2520UCF-Crime%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recognition%20of%20Abnormal%20Events%20in%20Surveillance%20Videos%20using%20Weakly%20Supervised%20Dual-Encoder%20Models&entry.906535625=Noam%20Tsfaty%20and%20Avishai%20Weizman%20and%20Liav%20Cohen%20and%20Moshe%20Tshuva%20and%20Yehudit%20Aperstein&entry.1292438233=We%20address%20the%20challenge%20of%20detecting%20rare%20and%20diverse%20anomalies%20in%20surveillance%20videos%20using%20only%20video-level%20supervision.%20Our%20dual-backbone%20framework%20combines%20convolutional%20and%20transformer%20representations%20through%20top-k%20pooling%2C%20achieving%2090.7%25%20area%20under%20the%20curve%20%28AUC%29%20on%20the%20UCF-Crime%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2511.13276v1&entry.124074799=Read"},
{"title": "Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention", "author": "Yu Wen and Shuyong Gao and Shuping Zhang and Miao Huang and Lili Tao and Han Yang and Haozhe Xing and Lihe Zhang and Boxue Hou", "abstract": "Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2511.13249v1", "date": "2025-11-17", "relevancy": 2.6408, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Referring%20Camouflaged%20Object%20Detection%20With%20Multi-Context%20Overlapped%20Windows%20Cross-Attention&body=Title%3A%20Referring%20Camouflaged%20Object%20Detection%20With%20Multi-Context%20Overlapped%20Windows%20Cross-Attention%0AAuthor%3A%20Yu%20Wen%20and%20Shuyong%20Gao%20and%20Shuping%20Zhang%20and%20Miao%20Huang%20and%20Lili%20Tao%20and%20Han%20Yang%20and%20Haozhe%20Xing%20and%20Lihe%20Zhang%20and%20Boxue%20Hou%0AAbstract%3A%20Referring%20camouflaged%20object%20detection%20%28Ref-COD%29%20aims%20to%20identify%20hidden%20objects%20by%20incorporating%20reference%20information%20such%20as%20images%20and%20text%20descriptions.%20Previous%20research%20has%20transformed%20reference%20images%20with%20salient%20objects%20into%20one-dimensional%20prompts%2C%20yielding%20significant%20results.%20We%20explore%20ways%20to%20enhance%20performance%20through%20multi-context%20fusion%20of%20rich%20salient%20image%20features%20and%20camouflaged%20object%20features.%20Therefore%2C%20we%20propose%20RFMNet%2C%20which%20utilizes%20features%20from%20multiple%20encoding%20stages%20of%20the%20reference%20salient%20images%20and%20performs%20interactive%20fusion%20with%20the%20camouflage%20features%20at%20the%20corresponding%20encoding%20stages.%20Given%20that%20the%20features%20in%20salient%20object%20images%20contain%20abundant%20object-related%20detail%20information%2C%20performing%20feature%20fusion%20within%20local%20areas%20is%20more%20beneficial%20for%20detecting%20camouflaged%20objects.%20Therefore%2C%20we%20propose%20an%20Overlapped%20Windows%20Cross-attention%20mechanism%20to%20enable%20the%20model%20to%20focus%20more%20attention%20on%20the%20local%20information%20matching%20based%20on%20reference%20features.%20Besides%2C%20we%20propose%20the%20Referring%20Feature%20Aggregation%20%28RFA%29%20module%20to%20decode%20and%20segment%20the%20camouflaged%20objects%20progressively.%20Extensive%20experiments%20on%20the%20Ref-COD%20benchmark%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferring%2520Camouflaged%2520Object%2520Detection%2520With%2520Multi-Context%2520Overlapped%2520Windows%2520Cross-Attention%26entry.906535625%3DYu%2520Wen%2520and%2520Shuyong%2520Gao%2520and%2520Shuping%2520Zhang%2520and%2520Miao%2520Huang%2520and%2520Lili%2520Tao%2520and%2520Han%2520Yang%2520and%2520Haozhe%2520Xing%2520and%2520Lihe%2520Zhang%2520and%2520Boxue%2520Hou%26entry.1292438233%3DReferring%2520camouflaged%2520object%2520detection%2520%2528Ref-COD%2529%2520aims%2520to%2520identify%2520hidden%2520objects%2520by%2520incorporating%2520reference%2520information%2520such%2520as%2520images%2520and%2520text%2520descriptions.%2520Previous%2520research%2520has%2520transformed%2520reference%2520images%2520with%2520salient%2520objects%2520into%2520one-dimensional%2520prompts%252C%2520yielding%2520significant%2520results.%2520We%2520explore%2520ways%2520to%2520enhance%2520performance%2520through%2520multi-context%2520fusion%2520of%2520rich%2520salient%2520image%2520features%2520and%2520camouflaged%2520object%2520features.%2520Therefore%252C%2520we%2520propose%2520RFMNet%252C%2520which%2520utilizes%2520features%2520from%2520multiple%2520encoding%2520stages%2520of%2520the%2520reference%2520salient%2520images%2520and%2520performs%2520interactive%2520fusion%2520with%2520the%2520camouflage%2520features%2520at%2520the%2520corresponding%2520encoding%2520stages.%2520Given%2520that%2520the%2520features%2520in%2520salient%2520object%2520images%2520contain%2520abundant%2520object-related%2520detail%2520information%252C%2520performing%2520feature%2520fusion%2520within%2520local%2520areas%2520is%2520more%2520beneficial%2520for%2520detecting%2520camouflaged%2520objects.%2520Therefore%252C%2520we%2520propose%2520an%2520Overlapped%2520Windows%2520Cross-attention%2520mechanism%2520to%2520enable%2520the%2520model%2520to%2520focus%2520more%2520attention%2520on%2520the%2520local%2520information%2520matching%2520based%2520on%2520reference%2520features.%2520Besides%252C%2520we%2520propose%2520the%2520Referring%2520Feature%2520Aggregation%2520%2528RFA%2529%2520module%2520to%2520decode%2520and%2520segment%2520the%2520camouflaged%2520objects%2520progressively.%2520Extensive%2520experiments%2520on%2520the%2520Ref-COD%2520benchmark%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Referring%20Camouflaged%20Object%20Detection%20With%20Multi-Context%20Overlapped%20Windows%20Cross-Attention&entry.906535625=Yu%20Wen%20and%20Shuyong%20Gao%20and%20Shuping%20Zhang%20and%20Miao%20Huang%20and%20Lili%20Tao%20and%20Han%20Yang%20and%20Haozhe%20Xing%20and%20Lihe%20Zhang%20and%20Boxue%20Hou&entry.1292438233=Referring%20camouflaged%20object%20detection%20%28Ref-COD%29%20aims%20to%20identify%20hidden%20objects%20by%20incorporating%20reference%20information%20such%20as%20images%20and%20text%20descriptions.%20Previous%20research%20has%20transformed%20reference%20images%20with%20salient%20objects%20into%20one-dimensional%20prompts%2C%20yielding%20significant%20results.%20We%20explore%20ways%20to%20enhance%20performance%20through%20multi-context%20fusion%20of%20rich%20salient%20image%20features%20and%20camouflaged%20object%20features.%20Therefore%2C%20we%20propose%20RFMNet%2C%20which%20utilizes%20features%20from%20multiple%20encoding%20stages%20of%20the%20reference%20salient%20images%20and%20performs%20interactive%20fusion%20with%20the%20camouflage%20features%20at%20the%20corresponding%20encoding%20stages.%20Given%20that%20the%20features%20in%20salient%20object%20images%20contain%20abundant%20object-related%20detail%20information%2C%20performing%20feature%20fusion%20within%20local%20areas%20is%20more%20beneficial%20for%20detecting%20camouflaged%20objects.%20Therefore%2C%20we%20propose%20an%20Overlapped%20Windows%20Cross-attention%20mechanism%20to%20enable%20the%20model%20to%20focus%20more%20attention%20on%20the%20local%20information%20matching%20based%20on%20reference%20features.%20Besides%2C%20we%20propose%20the%20Referring%20Feature%20Aggregation%20%28RFA%29%20module%20to%20decode%20and%20segment%20the%20camouflaged%20objects%20progressively.%20Extensive%20experiments%20on%20the%20Ref-COD%20benchmark%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.13249v1&entry.124074799=Read"},
{"title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity", "author": "Junwei Yu and Trevor Darrell and XuDong Wang", "abstract": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.", "link": "http://arxiv.org/abs/2511.13714v1", "date": "2025-11-17", "relevancy": 2.6353, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnSAMv2%3A%20Self-Supervised%20Learning%20Enables%20Segment%20Anything%20at%20Any%20Granularity&body=Title%3A%20UnSAMv2%3A%20Self-Supervised%20Learning%20Enables%20Segment%20Anything%20at%20Any%20Granularity%0AAuthor%3A%20Junwei%20Yu%20and%20Trevor%20Darrell%20and%20XuDong%20Wang%0AAbstract%3A%20The%20Segment%20Anything%20Model%20%28SAM%29%20family%20has%20become%20a%20widely%20adopted%20vision%20foundation%20model%2C%20but%20its%20ability%20to%20control%20segmentation%20granularity%20remains%20limited.%20Users%20often%20need%20to%20refine%20results%20manually%20-%20by%20adding%20more%20prompts%20or%20selecting%20from%20pre-generated%20masks%20-%20to%20achieve%20the%20desired%20level%20of%20detail.%20This%20process%20can%20be%20ambiguous%2C%20as%20the%20same%20prompt%20may%20correspond%20to%20several%20plausible%20masks%2C%20and%20collecting%20dense%20annotations%20across%20all%20granularities%20is%20prohibitively%20expensive%2C%20making%20supervised%20solutions%20infeasible.%20To%20address%20this%20limitation%2C%20we%20introduce%20UnSAMv2%2C%20which%20enables%20segment%20anything%20at%20any%20granularity%20without%20human%20annotations.%20UnSAMv2%20extends%20the%20divide-and-conquer%20strategy%20of%20UnSAM%20by%20discovering%20abundant%20mask-granularity%20pairs%20and%20introducing%20a%20novel%20granularity%20control%20embedding%20that%20enables%20precise%2C%20continuous%20control%20over%20segmentation%20scale.%20Remarkably%2C%20with%20only%20%246%24K%20unlabeled%20images%20and%20%240.02%5C%25%24%20additional%20parameters%2C%20UnSAMv2%20substantially%20enhances%20SAM-2%2C%20achieving%20segment%20anything%20at%20any%20granularity%20across%20interactive%2C%20whole-image%2C%20and%20video%20segmentation%20tasks.%20Evaluated%20on%20over%20%2411%24%20benchmarks%2C%20UnSAMv2%20improves%20%24%5Ctext%7BNoC%7D_%7B90%7D%24%20%285.69%20%24%5Crightarrow%24%204.75%29%2C%201-IoU%20%2858.0%20%24%5Crightarrow%24%2073.1%29%2C%20and%20%24%5Ctext%7BAR%7D_%7B1000%7D%24%20%2849.6%20%24%5Crightarrow%24%2068.3%29%2C%20showing%20that%20small%20amounts%20of%20unlabeled%20data%20with%20a%20granularity-aware%20self-supervised%20learning%20method%20can%20unlock%20the%20potential%20of%20vision%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnSAMv2%253A%2520Self-Supervised%2520Learning%2520Enables%2520Segment%2520Anything%2520at%2520Any%2520Granularity%26entry.906535625%3DJunwei%2520Yu%2520and%2520Trevor%2520Darrell%2520and%2520XuDong%2520Wang%26entry.1292438233%3DThe%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520family%2520has%2520become%2520a%2520widely%2520adopted%2520vision%2520foundation%2520model%252C%2520but%2520its%2520ability%2520to%2520control%2520segmentation%2520granularity%2520remains%2520limited.%2520Users%2520often%2520need%2520to%2520refine%2520results%2520manually%2520-%2520by%2520adding%2520more%2520prompts%2520or%2520selecting%2520from%2520pre-generated%2520masks%2520-%2520to%2520achieve%2520the%2520desired%2520level%2520of%2520detail.%2520This%2520process%2520can%2520be%2520ambiguous%252C%2520as%2520the%2520same%2520prompt%2520may%2520correspond%2520to%2520several%2520plausible%2520masks%252C%2520and%2520collecting%2520dense%2520annotations%2520across%2520all%2520granularities%2520is%2520prohibitively%2520expensive%252C%2520making%2520supervised%2520solutions%2520infeasible.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520UnSAMv2%252C%2520which%2520enables%2520segment%2520anything%2520at%2520any%2520granularity%2520without%2520human%2520annotations.%2520UnSAMv2%2520extends%2520the%2520divide-and-conquer%2520strategy%2520of%2520UnSAM%2520by%2520discovering%2520abundant%2520mask-granularity%2520pairs%2520and%2520introducing%2520a%2520novel%2520granularity%2520control%2520embedding%2520that%2520enables%2520precise%252C%2520continuous%2520control%2520over%2520segmentation%2520scale.%2520Remarkably%252C%2520with%2520only%2520%25246%2524K%2520unlabeled%2520images%2520and%2520%25240.02%255C%2525%2524%2520additional%2520parameters%252C%2520UnSAMv2%2520substantially%2520enhances%2520SAM-2%252C%2520achieving%2520segment%2520anything%2520at%2520any%2520granularity%2520across%2520interactive%252C%2520whole-image%252C%2520and%2520video%2520segmentation%2520tasks.%2520Evaluated%2520on%2520over%2520%252411%2524%2520benchmarks%252C%2520UnSAMv2%2520improves%2520%2524%255Ctext%257BNoC%257D_%257B90%257D%2524%2520%25285.69%2520%2524%255Crightarrow%2524%25204.75%2529%252C%25201-IoU%2520%252858.0%2520%2524%255Crightarrow%2524%252073.1%2529%252C%2520and%2520%2524%255Ctext%257BAR%257D_%257B1000%257D%2524%2520%252849.6%2520%2524%255Crightarrow%2524%252068.3%2529%252C%2520showing%2520that%2520small%2520amounts%2520of%2520unlabeled%2520data%2520with%2520a%2520granularity-aware%2520self-supervised%2520learning%2520method%2520can%2520unlock%2520the%2520potential%2520of%2520vision%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnSAMv2%3A%20Self-Supervised%20Learning%20Enables%20Segment%20Anything%20at%20Any%20Granularity&entry.906535625=Junwei%20Yu%20and%20Trevor%20Darrell%20and%20XuDong%20Wang&entry.1292438233=The%20Segment%20Anything%20Model%20%28SAM%29%20family%20has%20become%20a%20widely%20adopted%20vision%20foundation%20model%2C%20but%20its%20ability%20to%20control%20segmentation%20granularity%20remains%20limited.%20Users%20often%20need%20to%20refine%20results%20manually%20-%20by%20adding%20more%20prompts%20or%20selecting%20from%20pre-generated%20masks%20-%20to%20achieve%20the%20desired%20level%20of%20detail.%20This%20process%20can%20be%20ambiguous%2C%20as%20the%20same%20prompt%20may%20correspond%20to%20several%20plausible%20masks%2C%20and%20collecting%20dense%20annotations%20across%20all%20granularities%20is%20prohibitively%20expensive%2C%20making%20supervised%20solutions%20infeasible.%20To%20address%20this%20limitation%2C%20we%20introduce%20UnSAMv2%2C%20which%20enables%20segment%20anything%20at%20any%20granularity%20without%20human%20annotations.%20UnSAMv2%20extends%20the%20divide-and-conquer%20strategy%20of%20UnSAM%20by%20discovering%20abundant%20mask-granularity%20pairs%20and%20introducing%20a%20novel%20granularity%20control%20embedding%20that%20enables%20precise%2C%20continuous%20control%20over%20segmentation%20scale.%20Remarkably%2C%20with%20only%20%246%24K%20unlabeled%20images%20and%20%240.02%5C%25%24%20additional%20parameters%2C%20UnSAMv2%20substantially%20enhances%20SAM-2%2C%20achieving%20segment%20anything%20at%20any%20granularity%20across%20interactive%2C%20whole-image%2C%20and%20video%20segmentation%20tasks.%20Evaluated%20on%20over%20%2411%24%20benchmarks%2C%20UnSAMv2%20improves%20%24%5Ctext%7BNoC%7D_%7B90%7D%24%20%285.69%20%24%5Crightarrow%24%204.75%29%2C%201-IoU%20%2858.0%20%24%5Crightarrow%24%2073.1%29%2C%20and%20%24%5Ctext%7BAR%7D_%7B1000%7D%24%20%2849.6%20%24%5Crightarrow%24%2068.3%29%2C%20showing%20that%20small%20amounts%20of%20unlabeled%20data%20with%20a%20granularity-aware%20self-supervised%20learning%20method%20can%20unlock%20the%20potential%20of%20vision%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.13714v1&entry.124074799=Read"},
{"title": "Deep Clustering via Gradual Community Detection", "author": "Tianyu Cheng and Qun Chen", "abstract": "Deep clustering is an essential task in modern artificial intelligence, aiming to partition a set of data samples into a given number of homogeneous groups (i.e., clusters). Recent studies have proposed increasingly advanced deep neural networks and training strategies for deep clustering, effectively improving performance. However, deep clustering generally remains challenging due to the inadequacy of supervision signals. Building upon the existing representation learning backbones, this paper proposes a novel clustering strategy of gradual community detection. It initializes clustering by partitioning samples into many pseudo-communities and then gradually expands clusters by community merging. Compared with the existing clustering strategies, community detection factors in the new perspective of cluster network analysis in the clustering process. The new perspective can effectively leverage global structural characteristics to enhance cluster pseudo-label purity, which is critical to the performance of self-supervision. We have implemented the proposed approach based on the popular backbones and evaluated its efficacy on benchmark image datasets. Our extensive experiments have shown that the proposed clustering strategy can effectively improve the SOTA performance. Our ablation study also demonstrates that the new network perspective can effectively improve community pseudo-label purity, resulting in improved self-supervision.", "link": "http://arxiv.org/abs/2501.02036v2", "date": "2025-11-17", "relevancy": 2.6089, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5454}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5115}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Clustering%20via%20Gradual%20Community%20Detection&body=Title%3A%20Deep%20Clustering%20via%20Gradual%20Community%20Detection%0AAuthor%3A%20Tianyu%20Cheng%20and%20Qun%20Chen%0AAbstract%3A%20Deep%20clustering%20is%20an%20essential%20task%20in%20modern%20artificial%20intelligence%2C%20aiming%20to%20partition%20a%20set%20of%20data%20samples%20into%20a%20given%20number%20of%20homogeneous%20groups%20%28i.e.%2C%20clusters%29.%20Recent%20studies%20have%20proposed%20increasingly%20advanced%20deep%20neural%20networks%20and%20training%20strategies%20for%20deep%20clustering%2C%20effectively%20improving%20performance.%20However%2C%20deep%20clustering%20generally%20remains%20challenging%20due%20to%20the%20inadequacy%20of%20supervision%20signals.%20Building%20upon%20the%20existing%20representation%20learning%20backbones%2C%20this%20paper%20proposes%20a%20novel%20clustering%20strategy%20of%20gradual%20community%20detection.%20It%20initializes%20clustering%20by%20partitioning%20samples%20into%20many%20pseudo-communities%20and%20then%20gradually%20expands%20clusters%20by%20community%20merging.%20Compared%20with%20the%20existing%20clustering%20strategies%2C%20community%20detection%20factors%20in%20the%20new%20perspective%20of%20cluster%20network%20analysis%20in%20the%20clustering%20process.%20The%20new%20perspective%20can%20effectively%20leverage%20global%20structural%20characteristics%20to%20enhance%20cluster%20pseudo-label%20purity%2C%20which%20is%20critical%20to%20the%20performance%20of%20self-supervision.%20We%20have%20implemented%20the%20proposed%20approach%20based%20on%20the%20popular%20backbones%20and%20evaluated%20its%20efficacy%20on%20benchmark%20image%20datasets.%20Our%20extensive%20experiments%20have%20shown%20that%20the%20proposed%20clustering%20strategy%20can%20effectively%20improve%20the%20SOTA%20performance.%20Our%20ablation%20study%20also%20demonstrates%20that%20the%20new%20network%20perspective%20can%20effectively%20improve%20community%20pseudo-label%20purity%2C%20resulting%20in%20improved%20self-supervision.%0ALink%3A%20http%3A//arxiv.org/abs/2501.02036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Clustering%2520via%2520Gradual%2520Community%2520Detection%26entry.906535625%3DTianyu%2520Cheng%2520and%2520Qun%2520Chen%26entry.1292438233%3DDeep%2520clustering%2520is%2520an%2520essential%2520task%2520in%2520modern%2520artificial%2520intelligence%252C%2520aiming%2520to%2520partition%2520a%2520set%2520of%2520data%2520samples%2520into%2520a%2520given%2520number%2520of%2520homogeneous%2520groups%2520%2528i.e.%252C%2520clusters%2529.%2520Recent%2520studies%2520have%2520proposed%2520increasingly%2520advanced%2520deep%2520neural%2520networks%2520and%2520training%2520strategies%2520for%2520deep%2520clustering%252C%2520effectively%2520improving%2520performance.%2520However%252C%2520deep%2520clustering%2520generally%2520remains%2520challenging%2520due%2520to%2520the%2520inadequacy%2520of%2520supervision%2520signals.%2520Building%2520upon%2520the%2520existing%2520representation%2520learning%2520backbones%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520clustering%2520strategy%2520of%2520gradual%2520community%2520detection.%2520It%2520initializes%2520clustering%2520by%2520partitioning%2520samples%2520into%2520many%2520pseudo-communities%2520and%2520then%2520gradually%2520expands%2520clusters%2520by%2520community%2520merging.%2520Compared%2520with%2520the%2520existing%2520clustering%2520strategies%252C%2520community%2520detection%2520factors%2520in%2520the%2520new%2520perspective%2520of%2520cluster%2520network%2520analysis%2520in%2520the%2520clustering%2520process.%2520The%2520new%2520perspective%2520can%2520effectively%2520leverage%2520global%2520structural%2520characteristics%2520to%2520enhance%2520cluster%2520pseudo-label%2520purity%252C%2520which%2520is%2520critical%2520to%2520the%2520performance%2520of%2520self-supervision.%2520We%2520have%2520implemented%2520the%2520proposed%2520approach%2520based%2520on%2520the%2520popular%2520backbones%2520and%2520evaluated%2520its%2520efficacy%2520on%2520benchmark%2520image%2520datasets.%2520Our%2520extensive%2520experiments%2520have%2520shown%2520that%2520the%2520proposed%2520clustering%2520strategy%2520can%2520effectively%2520improve%2520the%2520SOTA%2520performance.%2520Our%2520ablation%2520study%2520also%2520demonstrates%2520that%2520the%2520new%2520network%2520perspective%2520can%2520effectively%2520improve%2520community%2520pseudo-label%2520purity%252C%2520resulting%2520in%2520improved%2520self-supervision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Clustering%20via%20Gradual%20Community%20Detection&entry.906535625=Tianyu%20Cheng%20and%20Qun%20Chen&entry.1292438233=Deep%20clustering%20is%20an%20essential%20task%20in%20modern%20artificial%20intelligence%2C%20aiming%20to%20partition%20a%20set%20of%20data%20samples%20into%20a%20given%20number%20of%20homogeneous%20groups%20%28i.e.%2C%20clusters%29.%20Recent%20studies%20have%20proposed%20increasingly%20advanced%20deep%20neural%20networks%20and%20training%20strategies%20for%20deep%20clustering%2C%20effectively%20improving%20performance.%20However%2C%20deep%20clustering%20generally%20remains%20challenging%20due%20to%20the%20inadequacy%20of%20supervision%20signals.%20Building%20upon%20the%20existing%20representation%20learning%20backbones%2C%20this%20paper%20proposes%20a%20novel%20clustering%20strategy%20of%20gradual%20community%20detection.%20It%20initializes%20clustering%20by%20partitioning%20samples%20into%20many%20pseudo-communities%20and%20then%20gradually%20expands%20clusters%20by%20community%20merging.%20Compared%20with%20the%20existing%20clustering%20strategies%2C%20community%20detection%20factors%20in%20the%20new%20perspective%20of%20cluster%20network%20analysis%20in%20the%20clustering%20process.%20The%20new%20perspective%20can%20effectively%20leverage%20global%20structural%20characteristics%20to%20enhance%20cluster%20pseudo-label%20purity%2C%20which%20is%20critical%20to%20the%20performance%20of%20self-supervision.%20We%20have%20implemented%20the%20proposed%20approach%20based%20on%20the%20popular%20backbones%20and%20evaluated%20its%20efficacy%20on%20benchmark%20image%20datasets.%20Our%20extensive%20experiments%20have%20shown%20that%20the%20proposed%20clustering%20strategy%20can%20effectively%20improve%20the%20SOTA%20performance.%20Our%20ablation%20study%20also%20demonstrates%20that%20the%20new%20network%20perspective%20can%20effectively%20improve%20community%20pseudo-label%20purity%2C%20resulting%20in%20improved%20self-supervision.&entry.1838667208=http%3A//arxiv.org/abs/2501.02036v2&entry.124074799=Read"},
{"title": "Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI", "author": "Mahdi Alehdaghi and Rajarshi Bhattacharya and Pourya Shamsolmoali and Rafael M. O. Cruz and Eric Granger", "abstract": "As AI systems grow more capable, it becomes increasingly important that their decisions remain understandable and aligned with human expectations. A key challenge is the limited interpretability of deep models. Post-hoc methods like GradCAM offer heatmaps but provide limited conceptual insight, while prototype-based approaches offer example-based explanations but often rely on rigid region selection and lack semantic consistency.\n  To address these limitations, we propose PCMNet, a part-prototypical concept mining network that learns human-comprehensible prototypes from meaningful image regions without additional supervision. By clustering these prototypes into concept groups and extracting concept activation vectors, PCMNet provides structured, concept-level explanations and enhances robustness to occlusion and challenging conditions, which are both critical for building reliable and aligned AI systems.\n  Experiments across multiple image classification benchmarks show that PCMNet outperforms state-of-the-art methods in interpretability, stability, and robustness. This work contributes to AI alignment by enhancing transparency, controllability, and trustworthiness in AI systems. Our code is available at: https://github.com/alehdaghi/PCMNet.", "link": "http://arxiv.org/abs/2504.12197v2", "date": "2025-11-17", "relevancy": 2.6086, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Patches%3A%20Mining%20Interpretable%20Part-Prototypes%20for%20Explainable%20AI&body=Title%3A%20Beyond%20Patches%3A%20Mining%20Interpretable%20Part-Prototypes%20for%20Explainable%20AI%0AAuthor%3A%20Mahdi%20Alehdaghi%20and%20Rajarshi%20Bhattacharya%20and%20Pourya%20Shamsolmoali%20and%20Rafael%20M.%20O.%20Cruz%20and%20Eric%20Granger%0AAbstract%3A%20As%20AI%20systems%20grow%20more%20capable%2C%20it%20becomes%20increasingly%20important%20that%20their%20decisions%20remain%20understandable%20and%20aligned%20with%20human%20expectations.%20A%20key%20challenge%20is%20the%20limited%20interpretability%20of%20deep%20models.%20Post-hoc%20methods%20like%20GradCAM%20offer%20heatmaps%20but%20provide%20limited%20conceptual%20insight%2C%20while%20prototype-based%20approaches%20offer%20example-based%20explanations%20but%20often%20rely%20on%20rigid%20region%20selection%20and%20lack%20semantic%20consistency.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20PCMNet%2C%20a%20part-prototypical%20concept%20mining%20network%20that%20learns%20human-comprehensible%20prototypes%20from%20meaningful%20image%20regions%20without%20additional%20supervision.%20By%20clustering%20these%20prototypes%20into%20concept%20groups%20and%20extracting%20concept%20activation%20vectors%2C%20PCMNet%20provides%20structured%2C%20concept-level%20explanations%20and%20enhances%20robustness%20to%20occlusion%20and%20challenging%20conditions%2C%20which%20are%20both%20critical%20for%20building%20reliable%20and%20aligned%20AI%20systems.%0A%20%20Experiments%20across%20multiple%20image%20classification%20benchmarks%20show%20that%20PCMNet%20outperforms%20state-of-the-art%20methods%20in%20interpretability%2C%20stability%2C%20and%20robustness.%20This%20work%20contributes%20to%20AI%20alignment%20by%20enhancing%20transparency%2C%20controllability%2C%20and%20trustworthiness%20in%20AI%20systems.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/alehdaghi/PCMNet.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Patches%253A%2520Mining%2520Interpretable%2520Part-Prototypes%2520for%2520Explainable%2520AI%26entry.906535625%3DMahdi%2520Alehdaghi%2520and%2520Rajarshi%2520Bhattacharya%2520and%2520Pourya%2520Shamsolmoali%2520and%2520Rafael%2520M.%2520O.%2520Cruz%2520and%2520Eric%2520Granger%26entry.1292438233%3DAs%2520AI%2520systems%2520grow%2520more%2520capable%252C%2520it%2520becomes%2520increasingly%2520important%2520that%2520their%2520decisions%2520remain%2520understandable%2520and%2520aligned%2520with%2520human%2520expectations.%2520A%2520key%2520challenge%2520is%2520the%2520limited%2520interpretability%2520of%2520deep%2520models.%2520Post-hoc%2520methods%2520like%2520GradCAM%2520offer%2520heatmaps%2520but%2520provide%2520limited%2520conceptual%2520insight%252C%2520while%2520prototype-based%2520approaches%2520offer%2520example-based%2520explanations%2520but%2520often%2520rely%2520on%2520rigid%2520region%2520selection%2520and%2520lack%2520semantic%2520consistency.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520PCMNet%252C%2520a%2520part-prototypical%2520concept%2520mining%2520network%2520that%2520learns%2520human-comprehensible%2520prototypes%2520from%2520meaningful%2520image%2520regions%2520without%2520additional%2520supervision.%2520By%2520clustering%2520these%2520prototypes%2520into%2520concept%2520groups%2520and%2520extracting%2520concept%2520activation%2520vectors%252C%2520PCMNet%2520provides%2520structured%252C%2520concept-level%2520explanations%2520and%2520enhances%2520robustness%2520to%2520occlusion%2520and%2520challenging%2520conditions%252C%2520which%2520are%2520both%2520critical%2520for%2520building%2520reliable%2520and%2520aligned%2520AI%2520systems.%250A%2520%2520Experiments%2520across%2520multiple%2520image%2520classification%2520benchmarks%2520show%2520that%2520PCMNet%2520outperforms%2520state-of-the-art%2520methods%2520in%2520interpretability%252C%2520stability%252C%2520and%2520robustness.%2520This%2520work%2520contributes%2520to%2520AI%2520alignment%2520by%2520enhancing%2520transparency%252C%2520controllability%252C%2520and%2520trustworthiness%2520in%2520AI%2520systems.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/alehdaghi/PCMNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Patches%3A%20Mining%20Interpretable%20Part-Prototypes%20for%20Explainable%20AI&entry.906535625=Mahdi%20Alehdaghi%20and%20Rajarshi%20Bhattacharya%20and%20Pourya%20Shamsolmoali%20and%20Rafael%20M.%20O.%20Cruz%20and%20Eric%20Granger&entry.1292438233=As%20AI%20systems%20grow%20more%20capable%2C%20it%20becomes%20increasingly%20important%20that%20their%20decisions%20remain%20understandable%20and%20aligned%20with%20human%20expectations.%20A%20key%20challenge%20is%20the%20limited%20interpretability%20of%20deep%20models.%20Post-hoc%20methods%20like%20GradCAM%20offer%20heatmaps%20but%20provide%20limited%20conceptual%20insight%2C%20while%20prototype-based%20approaches%20offer%20example-based%20explanations%20but%20often%20rely%20on%20rigid%20region%20selection%20and%20lack%20semantic%20consistency.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20PCMNet%2C%20a%20part-prototypical%20concept%20mining%20network%20that%20learns%20human-comprehensible%20prototypes%20from%20meaningful%20image%20regions%20without%20additional%20supervision.%20By%20clustering%20these%20prototypes%20into%20concept%20groups%20and%20extracting%20concept%20activation%20vectors%2C%20PCMNet%20provides%20structured%2C%20concept-level%20explanations%20and%20enhances%20robustness%20to%20occlusion%20and%20challenging%20conditions%2C%20which%20are%20both%20critical%20for%20building%20reliable%20and%20aligned%20AI%20systems.%0A%20%20Experiments%20across%20multiple%20image%20classification%20benchmarks%20show%20that%20PCMNet%20outperforms%20state-of-the-art%20methods%20in%20interpretability%2C%20stability%2C%20and%20robustness.%20This%20work%20contributes%20to%20AI%20alignment%20by%20enhancing%20transparency%2C%20controllability%2C%20and%20trustworthiness%20in%20AI%20systems.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/alehdaghi/PCMNet.&entry.1838667208=http%3A//arxiv.org/abs/2504.12197v2&entry.124074799=Read"},
{"title": "Nearest Neighbor Projection Removal Adversarial Training", "author": "Himanshu Singh and A. V. Subramanyam and Shivank Rajput and Mohan Kankanhalli", "abstract": "Deep neural networks have exhibited impressive performance in image classification tasks but remain vulnerable to adversarial examples. Standard adversarial training enhances robustness but typically fails to explicitly address inter-class feature overlap, a significant contributor to adversarial susceptibility. In this work, we introduce a novel adversarial training framework that actively mitigates inter-class proximity by projecting out inter-class dependencies from adversarial and clean samples in the feature space. Specifically, our approach first identifies the nearest inter-class neighbors for each adversarial sample and subsequently removes projections onto these neighbors to enforce stronger feature separability. Theoretically, we demonstrate that our proposed logits correction reduces the Lipschitz constant of neural networks, thereby lowering the Rademacher complexity, which directly contributes to improved generalization and robustness. Extensive experiments across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that our method demonstrates strong performance that is competitive with leading adversarial training techniques, highlighting significant achievements in both robust and clean accuracy. Our findings reveal the importance of addressing inter-class feature proximity explicitly to bolster adversarial robustness in DNNs.", "link": "http://arxiv.org/abs/2509.07673v3", "date": "2025-11-17", "relevancy": 2.6072, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5388}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5156}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearest%20Neighbor%20Projection%20Removal%20Adversarial%20Training&body=Title%3A%20Nearest%20Neighbor%20Projection%20Removal%20Adversarial%20Training%0AAuthor%3A%20Himanshu%20Singh%20and%20A.%20V.%20Subramanyam%20and%20Shivank%20Rajput%20and%20Mohan%20Kankanhalli%0AAbstract%3A%20Deep%20neural%20networks%20have%20exhibited%20impressive%20performance%20in%20image%20classification%20tasks%20but%20remain%20vulnerable%20to%20adversarial%20examples.%20Standard%20adversarial%20training%20enhances%20robustness%20but%20typically%20fails%20to%20explicitly%20address%20inter-class%20feature%20overlap%2C%20a%20significant%20contributor%20to%20adversarial%20susceptibility.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20adversarial%20training%20framework%20that%20actively%20mitigates%20inter-class%20proximity%20by%20projecting%20out%20inter-class%20dependencies%20from%20adversarial%20and%20clean%20samples%20in%20the%20feature%20space.%20Specifically%2C%20our%20approach%20first%20identifies%20the%20nearest%20inter-class%20neighbors%20for%20each%20adversarial%20sample%20and%20subsequently%20removes%20projections%20onto%20these%20neighbors%20to%20enforce%20stronger%20feature%20separability.%20Theoretically%2C%20we%20demonstrate%20that%20our%20proposed%20logits%20correction%20reduces%20the%20Lipschitz%20constant%20of%20neural%20networks%2C%20thereby%20lowering%20the%20Rademacher%20complexity%2C%20which%20directly%20contributes%20to%20improved%20generalization%20and%20robustness.%20Extensive%20experiments%20across%20standard%20benchmarks%20including%20CIFAR-10%2C%20CIFAR-100%2C%20and%20SVHN%20show%20that%20our%20method%20demonstrates%20strong%20performance%20that%20is%20competitive%20with%20leading%20adversarial%20training%20techniques%2C%20highlighting%20significant%20achievements%20in%20both%20robust%20and%20clean%20accuracy.%20Our%20findings%20reveal%20the%20importance%20of%20addressing%20inter-class%20feature%20proximity%20explicitly%20to%20bolster%20adversarial%20robustness%20in%20DNNs.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07673v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearest%2520Neighbor%2520Projection%2520Removal%2520Adversarial%2520Training%26entry.906535625%3DHimanshu%2520Singh%2520and%2520A.%2520V.%2520Subramanyam%2520and%2520Shivank%2520Rajput%2520and%2520Mohan%2520Kankanhalli%26entry.1292438233%3DDeep%2520neural%2520networks%2520have%2520exhibited%2520impressive%2520performance%2520in%2520image%2520classification%2520tasks%2520but%2520remain%2520vulnerable%2520to%2520adversarial%2520examples.%2520Standard%2520adversarial%2520training%2520enhances%2520robustness%2520but%2520typically%2520fails%2520to%2520explicitly%2520address%2520inter-class%2520feature%2520overlap%252C%2520a%2520significant%2520contributor%2520to%2520adversarial%2520susceptibility.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520adversarial%2520training%2520framework%2520that%2520actively%2520mitigates%2520inter-class%2520proximity%2520by%2520projecting%2520out%2520inter-class%2520dependencies%2520from%2520adversarial%2520and%2520clean%2520samples%2520in%2520the%2520feature%2520space.%2520Specifically%252C%2520our%2520approach%2520first%2520identifies%2520the%2520nearest%2520inter-class%2520neighbors%2520for%2520each%2520adversarial%2520sample%2520and%2520subsequently%2520removes%2520projections%2520onto%2520these%2520neighbors%2520to%2520enforce%2520stronger%2520feature%2520separability.%2520Theoretically%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%2520logits%2520correction%2520reduces%2520the%2520Lipschitz%2520constant%2520of%2520neural%2520networks%252C%2520thereby%2520lowering%2520the%2520Rademacher%2520complexity%252C%2520which%2520directly%2520contributes%2520to%2520improved%2520generalization%2520and%2520robustness.%2520Extensive%2520experiments%2520across%2520standard%2520benchmarks%2520including%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520SVHN%2520show%2520that%2520our%2520method%2520demonstrates%2520strong%2520performance%2520that%2520is%2520competitive%2520with%2520leading%2520adversarial%2520training%2520techniques%252C%2520highlighting%2520significant%2520achievements%2520in%2520both%2520robust%2520and%2520clean%2520accuracy.%2520Our%2520findings%2520reveal%2520the%2520importance%2520of%2520addressing%2520inter-class%2520feature%2520proximity%2520explicitly%2520to%2520bolster%2520adversarial%2520robustness%2520in%2520DNNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07673v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearest%20Neighbor%20Projection%20Removal%20Adversarial%20Training&entry.906535625=Himanshu%20Singh%20and%20A.%20V.%20Subramanyam%20and%20Shivank%20Rajput%20and%20Mohan%20Kankanhalli&entry.1292438233=Deep%20neural%20networks%20have%20exhibited%20impressive%20performance%20in%20image%20classification%20tasks%20but%20remain%20vulnerable%20to%20adversarial%20examples.%20Standard%20adversarial%20training%20enhances%20robustness%20but%20typically%20fails%20to%20explicitly%20address%20inter-class%20feature%20overlap%2C%20a%20significant%20contributor%20to%20adversarial%20susceptibility.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20adversarial%20training%20framework%20that%20actively%20mitigates%20inter-class%20proximity%20by%20projecting%20out%20inter-class%20dependencies%20from%20adversarial%20and%20clean%20samples%20in%20the%20feature%20space.%20Specifically%2C%20our%20approach%20first%20identifies%20the%20nearest%20inter-class%20neighbors%20for%20each%20adversarial%20sample%20and%20subsequently%20removes%20projections%20onto%20these%20neighbors%20to%20enforce%20stronger%20feature%20separability.%20Theoretically%2C%20we%20demonstrate%20that%20our%20proposed%20logits%20correction%20reduces%20the%20Lipschitz%20constant%20of%20neural%20networks%2C%20thereby%20lowering%20the%20Rademacher%20complexity%2C%20which%20directly%20contributes%20to%20improved%20generalization%20and%20robustness.%20Extensive%20experiments%20across%20standard%20benchmarks%20including%20CIFAR-10%2C%20CIFAR-100%2C%20and%20SVHN%20show%20that%20our%20method%20demonstrates%20strong%20performance%20that%20is%20competitive%20with%20leading%20adversarial%20training%20techniques%2C%20highlighting%20significant%20achievements%20in%20both%20robust%20and%20clean%20accuracy.%20Our%20findings%20reveal%20the%20importance%20of%20addressing%20inter-class%20feature%20proximity%20explicitly%20to%20bolster%20adversarial%20robustness%20in%20DNNs.&entry.1838667208=http%3A//arxiv.org/abs/2509.07673v3&entry.124074799=Read"},
{"title": "Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction", "author": "Xinzhu Ma and Cheng Wang and Chen Tang and Bin Wang and Shixiang Tang and Yuan Meng and Yunhong Wang and Di Huang", "abstract": "Recovering CAD models from point clouds requires reconstructing their topology and sketch-based extrusion primitives. A dominant paradigm for representing sketches involves implicit neural representations such as Signed Distance Fields (SDFs). However, this indirect approach inherently struggles with precision, leading to unintended curved edges and models that are difficult to edit. In this paper, we propose Point2Primitive, a framework that learns to directly predict the explicit, parametric primitives of CAD models. Our method treats sketch reconstruction as a set prediction problem, employing a improved transformer-based decoder with explicit position queries to directly detect and predict the fundamental sketch curves (i.e., type and parameter) from the point cloud. Instead of approximating a continuous field, we formulate curve parameters as explicit position queries, which are optimized autoregressively to achieve high accuracy. The overall topology is rebuilt via extrusion segmentation. Extensive experiments demonstrate that this direct prediction paradigm significantly outperforms implicit methods in both primitive accuracy and overall geometric fidelity.", "link": "http://arxiv.org/abs/2505.02043v3", "date": "2025-11-17", "relevancy": 2.6022, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.541}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point2Primitive%3A%20CAD%20Reconstruction%20from%20Point%20Cloud%20by%20Direct%20Primitive%20Prediction&body=Title%3A%20Point2Primitive%3A%20CAD%20Reconstruction%20from%20Point%20Cloud%20by%20Direct%20Primitive%20Prediction%0AAuthor%3A%20Xinzhu%20Ma%20and%20Cheng%20Wang%20and%20Chen%20Tang%20and%20Bin%20Wang%20and%20Shixiang%20Tang%20and%20Yuan%20Meng%20and%20Yunhong%20Wang%20and%20Di%20Huang%0AAbstract%3A%20Recovering%20CAD%20models%20from%20point%20clouds%20requires%20reconstructing%20their%20topology%20and%20sketch-based%20extrusion%20primitives.%20A%20dominant%20paradigm%20for%20representing%20sketches%20involves%20implicit%20neural%20representations%20such%20as%20Signed%20Distance%20Fields%20%28SDFs%29.%20However%2C%20this%20indirect%20approach%20inherently%20struggles%20with%20precision%2C%20leading%20to%20unintended%20curved%20edges%20and%20models%20that%20are%20difficult%20to%20edit.%20In%20this%20paper%2C%20we%20propose%20Point2Primitive%2C%20a%20framework%20that%20learns%20to%20directly%20predict%20the%20explicit%2C%20parametric%20primitives%20of%20CAD%20models.%20Our%20method%20treats%20sketch%20reconstruction%20as%20a%20set%20prediction%20problem%2C%20employing%20a%20improved%20transformer-based%20decoder%20with%20explicit%20position%20queries%20to%20directly%20detect%20and%20predict%20the%20fundamental%20sketch%20curves%20%28i.e.%2C%20type%20and%20parameter%29%20from%20the%20point%20cloud.%20Instead%20of%20approximating%20a%20continuous%20field%2C%20we%20formulate%20curve%20parameters%20as%20explicit%20position%20queries%2C%20which%20are%20optimized%20autoregressively%20to%20achieve%20high%20accuracy.%20The%20overall%20topology%20is%20rebuilt%20via%20extrusion%20segmentation.%20Extensive%20experiments%20demonstrate%20that%20this%20direct%20prediction%20paradigm%20significantly%20outperforms%20implicit%20methods%20in%20both%20primitive%20accuracy%20and%20overall%20geometric%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2505.02043v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint2Primitive%253A%2520CAD%2520Reconstruction%2520from%2520Point%2520Cloud%2520by%2520Direct%2520Primitive%2520Prediction%26entry.906535625%3DXinzhu%2520Ma%2520and%2520Cheng%2520Wang%2520and%2520Chen%2520Tang%2520and%2520Bin%2520Wang%2520and%2520Shixiang%2520Tang%2520and%2520Yuan%2520Meng%2520and%2520Yunhong%2520Wang%2520and%2520Di%2520Huang%26entry.1292438233%3DRecovering%2520CAD%2520models%2520from%2520point%2520clouds%2520requires%2520reconstructing%2520their%2520topology%2520and%2520sketch-based%2520extrusion%2520primitives.%2520A%2520dominant%2520paradigm%2520for%2520representing%2520sketches%2520involves%2520implicit%2520neural%2520representations%2520such%2520as%2520Signed%2520Distance%2520Fields%2520%2528SDFs%2529.%2520However%252C%2520this%2520indirect%2520approach%2520inherently%2520struggles%2520with%2520precision%252C%2520leading%2520to%2520unintended%2520curved%2520edges%2520and%2520models%2520that%2520are%2520difficult%2520to%2520edit.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Point2Primitive%252C%2520a%2520framework%2520that%2520learns%2520to%2520directly%2520predict%2520the%2520explicit%252C%2520parametric%2520primitives%2520of%2520CAD%2520models.%2520Our%2520method%2520treats%2520sketch%2520reconstruction%2520as%2520a%2520set%2520prediction%2520problem%252C%2520employing%2520a%2520improved%2520transformer-based%2520decoder%2520with%2520explicit%2520position%2520queries%2520to%2520directly%2520detect%2520and%2520predict%2520the%2520fundamental%2520sketch%2520curves%2520%2528i.e.%252C%2520type%2520and%2520parameter%2529%2520from%2520the%2520point%2520cloud.%2520Instead%2520of%2520approximating%2520a%2520continuous%2520field%252C%2520we%2520formulate%2520curve%2520parameters%2520as%2520explicit%2520position%2520queries%252C%2520which%2520are%2520optimized%2520autoregressively%2520to%2520achieve%2520high%2520accuracy.%2520The%2520overall%2520topology%2520is%2520rebuilt%2520via%2520extrusion%2520segmentation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520this%2520direct%2520prediction%2520paradigm%2520significantly%2520outperforms%2520implicit%2520methods%2520in%2520both%2520primitive%2520accuracy%2520and%2520overall%2520geometric%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02043v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point2Primitive%3A%20CAD%20Reconstruction%20from%20Point%20Cloud%20by%20Direct%20Primitive%20Prediction&entry.906535625=Xinzhu%20Ma%20and%20Cheng%20Wang%20and%20Chen%20Tang%20and%20Bin%20Wang%20and%20Shixiang%20Tang%20and%20Yuan%20Meng%20and%20Yunhong%20Wang%20and%20Di%20Huang&entry.1292438233=Recovering%20CAD%20models%20from%20point%20clouds%20requires%20reconstructing%20their%20topology%20and%20sketch-based%20extrusion%20primitives.%20A%20dominant%20paradigm%20for%20representing%20sketches%20involves%20implicit%20neural%20representations%20such%20as%20Signed%20Distance%20Fields%20%28SDFs%29.%20However%2C%20this%20indirect%20approach%20inherently%20struggles%20with%20precision%2C%20leading%20to%20unintended%20curved%20edges%20and%20models%20that%20are%20difficult%20to%20edit.%20In%20this%20paper%2C%20we%20propose%20Point2Primitive%2C%20a%20framework%20that%20learns%20to%20directly%20predict%20the%20explicit%2C%20parametric%20primitives%20of%20CAD%20models.%20Our%20method%20treats%20sketch%20reconstruction%20as%20a%20set%20prediction%20problem%2C%20employing%20a%20improved%20transformer-based%20decoder%20with%20explicit%20position%20queries%20to%20directly%20detect%20and%20predict%20the%20fundamental%20sketch%20curves%20%28i.e.%2C%20type%20and%20parameter%29%20from%20the%20point%20cloud.%20Instead%20of%20approximating%20a%20continuous%20field%2C%20we%20formulate%20curve%20parameters%20as%20explicit%20position%20queries%2C%20which%20are%20optimized%20autoregressively%20to%20achieve%20high%20accuracy.%20The%20overall%20topology%20is%20rebuilt%20via%20extrusion%20segmentation.%20Extensive%20experiments%20demonstrate%20that%20this%20direct%20prediction%20paradigm%20significantly%20outperforms%20implicit%20methods%20in%20both%20primitive%20accuracy%20and%20overall%20geometric%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2505.02043v3&entry.124074799=Read"},
{"title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification", "author": "Linhan Zhou and Shuang Li and Neng Dong and Yonghang Tai and Yafei Zhang and Huafeng Li", "abstract": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.", "link": "http://arxiv.org/abs/2511.13575v1", "date": "2025-11-17", "relevancy": 2.6006, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5325}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Prompt%20Learning%20for%20Image-%20and%20Text-Based%20Person%20Re-Identification&body=Title%3A%20Hierarchical%20Prompt%20Learning%20for%20Image-%20and%20Text-Based%20Person%20Re-Identification%0AAuthor%3A%20Linhan%20Zhou%20and%20Shuang%20Li%20and%20Neng%20Dong%20and%20Yonghang%20Tai%20and%20Yafei%20Zhang%20and%20Huafeng%20Li%0AAbstract%3A%20Person%20re-identification%20%28ReID%29%20aims%20to%20retrieve%20target%20pedestrian%20images%20given%20either%20visual%20queries%20%28image-to-image%2C%20I2I%29%20or%20textual%20descriptions%20%28text-to-image%2C%20T2I%29.%20Although%20both%20tasks%20share%20a%20common%20retrieval%20objective%2C%20they%20pose%20distinct%20challenges%3A%20I2I%20emphasizes%20discriminative%20identity%20learning%2C%20while%20T2I%20requires%20accurate%20cross-modal%20semantic%20alignment.%20Existing%20methods%20often%20treat%20these%20tasks%20separately%2C%20which%20may%20lead%20to%20representation%20entanglement%20and%20suboptimal%20performance.%20To%20address%20this%2C%20we%20propose%20a%20unified%20framework%20named%20Hierarchical%20Prompt%20Learning%20%28HPL%29%2C%20which%20leverages%20task-aware%20prompt%20modeling%20to%20jointly%20optimize%20both%20tasks.%20Specifically%2C%20we%20first%20introduce%20a%20Task-Routed%20Transformer%2C%20which%20incorporates%20dual%20classification%20tokens%20into%20a%20shared%20visual%20encoder%20to%20route%20features%20for%20I2I%20and%20T2I%20branches%20respectively.%20On%20top%20of%20this%2C%20we%20develop%20a%20hierarchical%20prompt%20generation%20scheme%20that%20integrates%20identity-level%20learnable%20tokens%20with%20instance-level%20pseudo-text%20tokens.%20These%20pseudo-tokens%20are%20derived%20from%20image%20or%20text%20features%20via%20modality-specific%20inversion%20networks%2C%20injecting%20fine-grained%2C%20instance-specific%20semantics%20into%20the%20prompts.%20Furthermore%2C%20we%20propose%20a%20Cross-Modal%20Prompt%20Regularization%20strategy%20to%20enforce%20semantic%20alignment%20in%20the%20prompt%20token%20space%2C%20ensuring%20that%20pseudo-prompts%20preserve%20source-modality%20characteristics%20while%20enhancing%20cross-modal%20transferability.%20Extensive%20experiments%20on%20multiple%20ReID%20benchmarks%20validate%20the%20effectiveness%20of%20our%20method%2C%20achieving%20state-of-the-art%20performance%20on%20both%20I2I%20and%20T2I%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Prompt%2520Learning%2520for%2520Image-%2520and%2520Text-Based%2520Person%2520Re-Identification%26entry.906535625%3DLinhan%2520Zhou%2520and%2520Shuang%2520Li%2520and%2520Neng%2520Dong%2520and%2520Yonghang%2520Tai%2520and%2520Yafei%2520Zhang%2520and%2520Huafeng%2520Li%26entry.1292438233%3DPerson%2520re-identification%2520%2528ReID%2529%2520aims%2520to%2520retrieve%2520target%2520pedestrian%2520images%2520given%2520either%2520visual%2520queries%2520%2528image-to-image%252C%2520I2I%2529%2520or%2520textual%2520descriptions%2520%2528text-to-image%252C%2520T2I%2529.%2520Although%2520both%2520tasks%2520share%2520a%2520common%2520retrieval%2520objective%252C%2520they%2520pose%2520distinct%2520challenges%253A%2520I2I%2520emphasizes%2520discriminative%2520identity%2520learning%252C%2520while%2520T2I%2520requires%2520accurate%2520cross-modal%2520semantic%2520alignment.%2520Existing%2520methods%2520often%2520treat%2520these%2520tasks%2520separately%252C%2520which%2520may%2520lead%2520to%2520representation%2520entanglement%2520and%2520suboptimal%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520unified%2520framework%2520named%2520Hierarchical%2520Prompt%2520Learning%2520%2528HPL%2529%252C%2520which%2520leverages%2520task-aware%2520prompt%2520modeling%2520to%2520jointly%2520optimize%2520both%2520tasks.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520Task-Routed%2520Transformer%252C%2520which%2520incorporates%2520dual%2520classification%2520tokens%2520into%2520a%2520shared%2520visual%2520encoder%2520to%2520route%2520features%2520for%2520I2I%2520and%2520T2I%2520branches%2520respectively.%2520On%2520top%2520of%2520this%252C%2520we%2520develop%2520a%2520hierarchical%2520prompt%2520generation%2520scheme%2520that%2520integrates%2520identity-level%2520learnable%2520tokens%2520with%2520instance-level%2520pseudo-text%2520tokens.%2520These%2520pseudo-tokens%2520are%2520derived%2520from%2520image%2520or%2520text%2520features%2520via%2520modality-specific%2520inversion%2520networks%252C%2520injecting%2520fine-grained%252C%2520instance-specific%2520semantics%2520into%2520the%2520prompts.%2520Furthermore%252C%2520we%2520propose%2520a%2520Cross-Modal%2520Prompt%2520Regularization%2520strategy%2520to%2520enforce%2520semantic%2520alignment%2520in%2520the%2520prompt%2520token%2520space%252C%2520ensuring%2520that%2520pseudo-prompts%2520preserve%2520source-modality%2520characteristics%2520while%2520enhancing%2520cross-modal%2520transferability.%2520Extensive%2520experiments%2520on%2520multiple%2520ReID%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520both%2520I2I%2520and%2520T2I%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Prompt%20Learning%20for%20Image-%20and%20Text-Based%20Person%20Re-Identification&entry.906535625=Linhan%20Zhou%20and%20Shuang%20Li%20and%20Neng%20Dong%20and%20Yonghang%20Tai%20and%20Yafei%20Zhang%20and%20Huafeng%20Li&entry.1292438233=Person%20re-identification%20%28ReID%29%20aims%20to%20retrieve%20target%20pedestrian%20images%20given%20either%20visual%20queries%20%28image-to-image%2C%20I2I%29%20or%20textual%20descriptions%20%28text-to-image%2C%20T2I%29.%20Although%20both%20tasks%20share%20a%20common%20retrieval%20objective%2C%20they%20pose%20distinct%20challenges%3A%20I2I%20emphasizes%20discriminative%20identity%20learning%2C%20while%20T2I%20requires%20accurate%20cross-modal%20semantic%20alignment.%20Existing%20methods%20often%20treat%20these%20tasks%20separately%2C%20which%20may%20lead%20to%20representation%20entanglement%20and%20suboptimal%20performance.%20To%20address%20this%2C%20we%20propose%20a%20unified%20framework%20named%20Hierarchical%20Prompt%20Learning%20%28HPL%29%2C%20which%20leverages%20task-aware%20prompt%20modeling%20to%20jointly%20optimize%20both%20tasks.%20Specifically%2C%20we%20first%20introduce%20a%20Task-Routed%20Transformer%2C%20which%20incorporates%20dual%20classification%20tokens%20into%20a%20shared%20visual%20encoder%20to%20route%20features%20for%20I2I%20and%20T2I%20branches%20respectively.%20On%20top%20of%20this%2C%20we%20develop%20a%20hierarchical%20prompt%20generation%20scheme%20that%20integrates%20identity-level%20learnable%20tokens%20with%20instance-level%20pseudo-text%20tokens.%20These%20pseudo-tokens%20are%20derived%20from%20image%20or%20text%20features%20via%20modality-specific%20inversion%20networks%2C%20injecting%20fine-grained%2C%20instance-specific%20semantics%20into%20the%20prompts.%20Furthermore%2C%20we%20propose%20a%20Cross-Modal%20Prompt%20Regularization%20strategy%20to%20enforce%20semantic%20alignment%20in%20the%20prompt%20token%20space%2C%20ensuring%20that%20pseudo-prompts%20preserve%20source-modality%20characteristics%20while%20enhancing%20cross-modal%20transferability.%20Extensive%20experiments%20on%20multiple%20ReID%20benchmarks%20validate%20the%20effectiveness%20of%20our%20method%2C%20achieving%20state-of-the-art%20performance%20on%20both%20I2I%20and%20T2I%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.13575v1&entry.124074799=Read"},
{"title": "MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing", "author": "Boris Kriuk", "abstract": "Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (\u03c3=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.", "link": "http://arxiv.org/abs/2511.13234v1", "date": "2025-11-17", "relevancy": 2.6002, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5656}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphBoost%3A%20Self-Organizing%20Universal%20Gradient%20Boosting%20with%20Adaptive%20Tree%20Morphing&body=Title%3A%20MorphBoost%3A%20Self-Organizing%20Universal%20Gradient%20Boosting%20with%20Adaptive%20Tree%20Morphing%0AAuthor%3A%20Boris%20Kriuk%0AAbstract%3A%20Traditional%20gradient%20boosting%20algorithms%20employ%20static%20tree%20structures%20with%20fixed%20splitting%20criteria%20that%20remain%20unchanged%20throughout%20training%2C%20limiting%20their%20ability%20to%20adapt%20to%20evolving%20gradient%20distributions%20and%20problem-specific%20characteristics%20across%20different%20learning%20stages.%20This%20work%20introduces%20MorphBoost%2C%20a%20new%20gradient%20boosting%20framework%20featuring%20self-organizing%20tree%20structures%20that%20dynamically%20morph%20their%20splitting%20behavior%20during%20training.%20The%20algorithm%20implements%20adaptive%20split%20functions%20that%20evolve%20based%20on%20accumulated%20gradient%20statistics%20and%20iteration-dependent%20learning%20pressures%2C%20enabling%20automatic%20adjustment%20to%20problem%20complexity.%20Key%20innovations%20include%3A%20%281%29%20morphing%20split%20criterion%20combining%20gradient-based%20scores%20with%20information-theoretic%20metrics%20weighted%20by%20training%20progress%3B%20%282%29%20automatic%20problem%20fingerprinting%20for%20intelligent%20parameter%20configuration%20across%20binary/multiclass/regression%20tasks%3B%20%283%29%20vectorized%20tree%20prediction%20achieving%20significant%20computational%20speedups%3B%20%284%29%20interaction-aware%20feature%20importance%20detecting%20multiplicative%20relationships%3B%20and%20%285%29%20fast-mode%20optimization%20balancing%20speed%20and%20accuracy.%20Comprehensive%20benchmarking%20across%2010%20diverse%20datasets%20against%20competitive%20models%20%28XGBoost%2C%20LightGBM%2C%20GradientBoosting%2C%20HistGradientBoosting%2C%20ensemble%20methods%29%20demonstrates%20that%20MorphBoost%20achieves%20state-of-the-art%20performance%2C%20outperforming%20XGBoost%20by%200.84%25%20on%20average.%20MorphBoost%20secured%20the%20overall%20winner%20position%20with%204/10%20dataset%20wins%20%2840%25%20win%20rate%29%20and%206/30%20top-3%20finishes%20%2820%25%29%2C%20while%20maintaining%20the%20lowest%20variance%20%28%CF%83%3D0.0948%29%20and%20highest%20minimum%20accuracy%20across%20all%20models%2C%20revealing%20superior%20consistency%20and%20robustness.%20Performance%20analysis%20across%20difficulty%20levels%20shows%20competitive%20results%20on%20easy%20datasets%20while%20achieving%20notable%20improvements%20on%20advanced%20problems%20due%20to%20higher%20adaptation%20levels.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphBoost%253A%2520Self-Organizing%2520Universal%2520Gradient%2520Boosting%2520with%2520Adaptive%2520Tree%2520Morphing%26entry.906535625%3DBoris%2520Kriuk%26entry.1292438233%3DTraditional%2520gradient%2520boosting%2520algorithms%2520employ%2520static%2520tree%2520structures%2520with%2520fixed%2520splitting%2520criteria%2520that%2520remain%2520unchanged%2520throughout%2520training%252C%2520limiting%2520their%2520ability%2520to%2520adapt%2520to%2520evolving%2520gradient%2520distributions%2520and%2520problem-specific%2520characteristics%2520across%2520different%2520learning%2520stages.%2520This%2520work%2520introduces%2520MorphBoost%252C%2520a%2520new%2520gradient%2520boosting%2520framework%2520featuring%2520self-organizing%2520tree%2520structures%2520that%2520dynamically%2520morph%2520their%2520splitting%2520behavior%2520during%2520training.%2520The%2520algorithm%2520implements%2520adaptive%2520split%2520functions%2520that%2520evolve%2520based%2520on%2520accumulated%2520gradient%2520statistics%2520and%2520iteration-dependent%2520learning%2520pressures%252C%2520enabling%2520automatic%2520adjustment%2520to%2520problem%2520complexity.%2520Key%2520innovations%2520include%253A%2520%25281%2529%2520morphing%2520split%2520criterion%2520combining%2520gradient-based%2520scores%2520with%2520information-theoretic%2520metrics%2520weighted%2520by%2520training%2520progress%253B%2520%25282%2529%2520automatic%2520problem%2520fingerprinting%2520for%2520intelligent%2520parameter%2520configuration%2520across%2520binary/multiclass/regression%2520tasks%253B%2520%25283%2529%2520vectorized%2520tree%2520prediction%2520achieving%2520significant%2520computational%2520speedups%253B%2520%25284%2529%2520interaction-aware%2520feature%2520importance%2520detecting%2520multiplicative%2520relationships%253B%2520and%2520%25285%2529%2520fast-mode%2520optimization%2520balancing%2520speed%2520and%2520accuracy.%2520Comprehensive%2520benchmarking%2520across%252010%2520diverse%2520datasets%2520against%2520competitive%2520models%2520%2528XGBoost%252C%2520LightGBM%252C%2520GradientBoosting%252C%2520HistGradientBoosting%252C%2520ensemble%2520methods%2529%2520demonstrates%2520that%2520MorphBoost%2520achieves%2520state-of-the-art%2520performance%252C%2520outperforming%2520XGBoost%2520by%25200.84%2525%2520on%2520average.%2520MorphBoost%2520secured%2520the%2520overall%2520winner%2520position%2520with%25204/10%2520dataset%2520wins%2520%252840%2525%2520win%2520rate%2529%2520and%25206/30%2520top-3%2520finishes%2520%252820%2525%2529%252C%2520while%2520maintaining%2520the%2520lowest%2520variance%2520%2528%25CF%2583%253D0.0948%2529%2520and%2520highest%2520minimum%2520accuracy%2520across%2520all%2520models%252C%2520revealing%2520superior%2520consistency%2520and%2520robustness.%2520Performance%2520analysis%2520across%2520difficulty%2520levels%2520shows%2520competitive%2520results%2520on%2520easy%2520datasets%2520while%2520achieving%2520notable%2520improvements%2520on%2520advanced%2520problems%2520due%2520to%2520higher%2520adaptation%2520levels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphBoost%3A%20Self-Organizing%20Universal%20Gradient%20Boosting%20with%20Adaptive%20Tree%20Morphing&entry.906535625=Boris%20Kriuk&entry.1292438233=Traditional%20gradient%20boosting%20algorithms%20employ%20static%20tree%20structures%20with%20fixed%20splitting%20criteria%20that%20remain%20unchanged%20throughout%20training%2C%20limiting%20their%20ability%20to%20adapt%20to%20evolving%20gradient%20distributions%20and%20problem-specific%20characteristics%20across%20different%20learning%20stages.%20This%20work%20introduces%20MorphBoost%2C%20a%20new%20gradient%20boosting%20framework%20featuring%20self-organizing%20tree%20structures%20that%20dynamically%20morph%20their%20splitting%20behavior%20during%20training.%20The%20algorithm%20implements%20adaptive%20split%20functions%20that%20evolve%20based%20on%20accumulated%20gradient%20statistics%20and%20iteration-dependent%20learning%20pressures%2C%20enabling%20automatic%20adjustment%20to%20problem%20complexity.%20Key%20innovations%20include%3A%20%281%29%20morphing%20split%20criterion%20combining%20gradient-based%20scores%20with%20information-theoretic%20metrics%20weighted%20by%20training%20progress%3B%20%282%29%20automatic%20problem%20fingerprinting%20for%20intelligent%20parameter%20configuration%20across%20binary/multiclass/regression%20tasks%3B%20%283%29%20vectorized%20tree%20prediction%20achieving%20significant%20computational%20speedups%3B%20%284%29%20interaction-aware%20feature%20importance%20detecting%20multiplicative%20relationships%3B%20and%20%285%29%20fast-mode%20optimization%20balancing%20speed%20and%20accuracy.%20Comprehensive%20benchmarking%20across%2010%20diverse%20datasets%20against%20competitive%20models%20%28XGBoost%2C%20LightGBM%2C%20GradientBoosting%2C%20HistGradientBoosting%2C%20ensemble%20methods%29%20demonstrates%20that%20MorphBoost%20achieves%20state-of-the-art%20performance%2C%20outperforming%20XGBoost%20by%200.84%25%20on%20average.%20MorphBoost%20secured%20the%20overall%20winner%20position%20with%204/10%20dataset%20wins%20%2840%25%20win%20rate%29%20and%206/30%20top-3%20finishes%20%2820%25%29%2C%20while%20maintaining%20the%20lowest%20variance%20%28%CF%83%3D0.0948%29%20and%20highest%20minimum%20accuracy%20across%20all%20models%2C%20revealing%20superior%20consistency%20and%20robustness.%20Performance%20analysis%20across%20difficulty%20levels%20shows%20competitive%20results%20on%20easy%20datasets%20while%20achieving%20notable%20improvements%20on%20advanced%20problems%20due%20to%20higher%20adaptation%20levels.&entry.1838667208=http%3A//arxiv.org/abs/2511.13234v1&entry.124074799=Read"},
{"title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop", "author": "Agnese Chiatti and Lara Piccolo and Sara Bernardini and Matteo Matteucci and Viola Schiaffonati", "abstract": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.", "link": "http://arxiv.org/abs/2511.13458v1", "date": "2025-11-17", "relevancy": 2.5947, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%20in%20Vision-Language%20Models%3A%20Insights%20from%20a%20Participatory%20User%20Workshop&body=Title%3A%20Trust%20in%20Vision-Language%20Models%3A%20Insights%20from%20a%20Participatory%20User%20Workshop%0AAuthor%3A%20Agnese%20Chiatti%20and%20Lara%20Piccolo%20and%20Sara%20Bernardini%20and%20Matteo%20Matteucci%20and%20Viola%20Schiaffonati%0AAbstract%3A%20With%20the%20growing%20deployment%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20pre-trained%20on%20large%20image-text%20and%20video-text%20datasets%2C%20it%20is%20critical%20to%20equip%20users%20with%20the%20tools%20to%20discern%20when%20to%20trust%20these%20systems.%20However%2C%20examining%20how%20user%20trust%20in%20VLMs%20builds%20and%20evolves%20remains%20an%20open%20problem.%20This%20problem%20is%20exacerbated%20by%20the%20increasing%20reliance%20on%20AI%20models%20as%20judges%20for%20experimental%20validation%2C%20to%20bypass%20the%20cost%20and%20implications%20of%20running%20participatory%20design%20studies%20directly%20with%20users.%20Following%20a%20user-centred%20approach%2C%20this%20paper%20presents%20preliminary%20results%20from%20a%20workshop%20with%20prospective%20VLM%20users.%20Insights%20from%20this%20pilot%20workshop%20inform%20future%20studies%20aimed%20at%20contextualising%20trust%20metrics%20and%20strategies%20for%20participants%27%20engagement%20to%20fit%20the%20case%20of%20user-VLM%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%2520in%2520Vision-Language%2520Models%253A%2520Insights%2520from%2520a%2520Participatory%2520User%2520Workshop%26entry.906535625%3DAgnese%2520Chiatti%2520and%2520Lara%2520Piccolo%2520and%2520Sara%2520Bernardini%2520and%2520Matteo%2520Matteucci%2520and%2520Viola%2520Schiaffonati%26entry.1292438233%3DWith%2520the%2520growing%2520deployment%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520pre-trained%2520on%2520large%2520image-text%2520and%2520video-text%2520datasets%252C%2520it%2520is%2520critical%2520to%2520equip%2520users%2520with%2520the%2520tools%2520to%2520discern%2520when%2520to%2520trust%2520these%2520systems.%2520However%252C%2520examining%2520how%2520user%2520trust%2520in%2520VLMs%2520builds%2520and%2520evolves%2520remains%2520an%2520open%2520problem.%2520This%2520problem%2520is%2520exacerbated%2520by%2520the%2520increasing%2520reliance%2520on%2520AI%2520models%2520as%2520judges%2520for%2520experimental%2520validation%252C%2520to%2520bypass%2520the%2520cost%2520and%2520implications%2520of%2520running%2520participatory%2520design%2520studies%2520directly%2520with%2520users.%2520Following%2520a%2520user-centred%2520approach%252C%2520this%2520paper%2520presents%2520preliminary%2520results%2520from%2520a%2520workshop%2520with%2520prospective%2520VLM%2520users.%2520Insights%2520from%2520this%2520pilot%2520workshop%2520inform%2520future%2520studies%2520aimed%2520at%2520contextualising%2520trust%2520metrics%2520and%2520strategies%2520for%2520participants%2527%2520engagement%2520to%2520fit%2520the%2520case%2520of%2520user-VLM%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20in%20Vision-Language%20Models%3A%20Insights%20from%20a%20Participatory%20User%20Workshop&entry.906535625=Agnese%20Chiatti%20and%20Lara%20Piccolo%20and%20Sara%20Bernardini%20and%20Matteo%20Matteucci%20and%20Viola%20Schiaffonati&entry.1292438233=With%20the%20growing%20deployment%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20pre-trained%20on%20large%20image-text%20and%20video-text%20datasets%2C%20it%20is%20critical%20to%20equip%20users%20with%20the%20tools%20to%20discern%20when%20to%20trust%20these%20systems.%20However%2C%20examining%20how%20user%20trust%20in%20VLMs%20builds%20and%20evolves%20remains%20an%20open%20problem.%20This%20problem%20is%20exacerbated%20by%20the%20increasing%20reliance%20on%20AI%20models%20as%20judges%20for%20experimental%20validation%2C%20to%20bypass%20the%20cost%20and%20implications%20of%20running%20participatory%20design%20studies%20directly%20with%20users.%20Following%20a%20user-centred%20approach%2C%20this%20paper%20presents%20preliminary%20results%20from%20a%20workshop%20with%20prospective%20VLM%20users.%20Insights%20from%20this%20pilot%20workshop%20inform%20future%20studies%20aimed%20at%20contextualising%20trust%20metrics%20and%20strategies%20for%20participants%27%20engagement%20to%20fit%20the%20case%20of%20user-VLM%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2511.13458v1&entry.124074799=Read"},
{"title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification", "author": "Luyao Niu and Nuoxian Huang", "abstract": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.", "link": "http://arxiv.org/abs/2511.13702v1", "date": "2025-11-17", "relevancy": 2.5733, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ST-ProC%3A%20A%20Graph-Prototypical%20Framework%20for%20Robust%20Semi-Supervised%20Travel%20Mode%20Identification&body=Title%3A%20ST-ProC%3A%20A%20Graph-Prototypical%20Framework%20for%20Robust%20Semi-Supervised%20Travel%20Mode%20Identification%0AAuthor%3A%20Luyao%20Niu%20and%20Nuoxian%20Huang%0AAbstract%3A%20Travel%20mode%20identification%20%28TMI%29%20from%20GPS%20trajectories%20is%20critical%20for%20urban%20intelligence%2C%20but%20is%20hampered%20by%20the%20high%20cost%20of%20annotation%2C%20leading%20to%20severe%20label%20scarcity.%20Prevailing%20semi-supervised%20learning%20%28SSL%29%20methods%20are%20ill-suited%20for%20this%20task%2C%20as%20they%20suffer%20from%20catastrophic%20confirmation%20bias%20and%20ignore%20the%20intrinsic%20data%20manifold.%20We%20propose%20ST-ProC%2C%20a%20novel%20graph-prototypical%20multi-objective%20SSL%20framework%20to%20address%20these%20limitations.%20Our%20framework%20synergizes%20a%20graph-prototypical%20core%20with%20foundational%20SSL%20Support.%20The%20core%20exploits%20the%20data%20manifold%20via%20graph%20regularization%2C%20prototypical%20anchoring%2C%20and%20a%20novel%2C%20margin-aware%20pseudo-labeling%20strategy%20to%20actively%20reject%20noise.%20This%20core%20is%20supported%20and%20stabilized%20by%20foundational%20contrastive%20and%20teacher-student%20consistency%20losses%2C%20ensuring%20high-quality%20representations%20and%20robust%20optimization.%20ST-ProC%20outperforms%20all%20baselines%20by%20a%20significant%20margin%2C%20demonstrating%20its%20efficacy%20in%20real-world%20sparse-label%20settings%2C%20with%20a%20performance%20boost%20of%2021.5%25%20over%20state-of-the-art%20methods%20like%20FixMatch.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DST-ProC%253A%2520A%2520Graph-Prototypical%2520Framework%2520for%2520Robust%2520Semi-Supervised%2520Travel%2520Mode%2520Identification%26entry.906535625%3DLuyao%2520Niu%2520and%2520Nuoxian%2520Huang%26entry.1292438233%3DTravel%2520mode%2520identification%2520%2528TMI%2529%2520from%2520GPS%2520trajectories%2520is%2520critical%2520for%2520urban%2520intelligence%252C%2520but%2520is%2520hampered%2520by%2520the%2520high%2520cost%2520of%2520annotation%252C%2520leading%2520to%2520severe%2520label%2520scarcity.%2520Prevailing%2520semi-supervised%2520learning%2520%2528SSL%2529%2520methods%2520are%2520ill-suited%2520for%2520this%2520task%252C%2520as%2520they%2520suffer%2520from%2520catastrophic%2520confirmation%2520bias%2520and%2520ignore%2520the%2520intrinsic%2520data%2520manifold.%2520We%2520propose%2520ST-ProC%252C%2520a%2520novel%2520graph-prototypical%2520multi-objective%2520SSL%2520framework%2520to%2520address%2520these%2520limitations.%2520Our%2520framework%2520synergizes%2520a%2520graph-prototypical%2520core%2520with%2520foundational%2520SSL%2520Support.%2520The%2520core%2520exploits%2520the%2520data%2520manifold%2520via%2520graph%2520regularization%252C%2520prototypical%2520anchoring%252C%2520and%2520a%2520novel%252C%2520margin-aware%2520pseudo-labeling%2520strategy%2520to%2520actively%2520reject%2520noise.%2520This%2520core%2520is%2520supported%2520and%2520stabilized%2520by%2520foundational%2520contrastive%2520and%2520teacher-student%2520consistency%2520losses%252C%2520ensuring%2520high-quality%2520representations%2520and%2520robust%2520optimization.%2520ST-ProC%2520outperforms%2520all%2520baselines%2520by%2520a%2520significant%2520margin%252C%2520demonstrating%2520its%2520efficacy%2520in%2520real-world%2520sparse-label%2520settings%252C%2520with%2520a%2520performance%2520boost%2520of%252021.5%2525%2520over%2520state-of-the-art%2520methods%2520like%2520FixMatch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ST-ProC%3A%20A%20Graph-Prototypical%20Framework%20for%20Robust%20Semi-Supervised%20Travel%20Mode%20Identification&entry.906535625=Luyao%20Niu%20and%20Nuoxian%20Huang&entry.1292438233=Travel%20mode%20identification%20%28TMI%29%20from%20GPS%20trajectories%20is%20critical%20for%20urban%20intelligence%2C%20but%20is%20hampered%20by%20the%20high%20cost%20of%20annotation%2C%20leading%20to%20severe%20label%20scarcity.%20Prevailing%20semi-supervised%20learning%20%28SSL%29%20methods%20are%20ill-suited%20for%20this%20task%2C%20as%20they%20suffer%20from%20catastrophic%20confirmation%20bias%20and%20ignore%20the%20intrinsic%20data%20manifold.%20We%20propose%20ST-ProC%2C%20a%20novel%20graph-prototypical%20multi-objective%20SSL%20framework%20to%20address%20these%20limitations.%20Our%20framework%20synergizes%20a%20graph-prototypical%20core%20with%20foundational%20SSL%20Support.%20The%20core%20exploits%20the%20data%20manifold%20via%20graph%20regularization%2C%20prototypical%20anchoring%2C%20and%20a%20novel%2C%20margin-aware%20pseudo-labeling%20strategy%20to%20actively%20reject%20noise.%20This%20core%20is%20supported%20and%20stabilized%20by%20foundational%20contrastive%20and%20teacher-student%20consistency%20losses%2C%20ensuring%20high-quality%20representations%20and%20robust%20optimization.%20ST-ProC%20outperforms%20all%20baselines%20by%20a%20significant%20margin%2C%20demonstrating%20its%20efficacy%20in%20real-world%20sparse-label%20settings%2C%20with%20a%20performance%20boost%20of%2021.5%25%20over%20state-of-the-art%20methods%20like%20FixMatch.&entry.1838667208=http%3A//arxiv.org/abs/2511.13702v1&entry.124074799=Read"},
{"title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse", "author": "Yuanchao Wang and Tian Qin and Eduardo Valle and Bruno Abrahao", "abstract": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.", "link": "http://arxiv.org/abs/2511.13539v1", "date": "2025-11-17", "relevancy": 2.5443, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5245}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BootOOD%3A%20Self-Supervised%20Out-of-Distribution%20Detection%20via%20Synthetic%20Sample%20Exposure%20under%20Neural%20Collapse&body=Title%3A%20BootOOD%3A%20Self-Supervised%20Out-of-Distribution%20Detection%20via%20Synthetic%20Sample%20Exposure%20under%20Neural%20Collapse%0AAuthor%3A%20Yuanchao%20Wang%20and%20Tian%20Qin%20and%20Eduardo%20Valle%20and%20Bruno%20Abrahao%0AAbstract%3A%20Out-of-distribution%20%28OOD%29%20detection%20is%20critical%20for%20deploying%20image%20classifiers%20in%20safety-sensitive%20environments%2C%20yet%20existing%20detectors%20often%20struggle%20when%20OOD%20samples%20are%20semantically%20similar%20to%20the%20in-distribution%20%28ID%29%20classes.%20We%20present%20BootOOD%2C%20a%20fully%20self-supervised%20OOD%20detection%20framework%20that%20bootstraps%20exclusively%20from%20ID%20data%20and%20is%20explicitly%20designed%20to%20handle%20semantically%20challenging%20OOD%20samples.%20BootOOD%20synthesizes%20pseudo-OOD%20features%20through%20simple%20transformations%20of%20ID%20representations%20and%20leverages%20Neural%20Collapse%20%28NC%29%2C%20where%20ID%20features%20cluster%20tightly%20around%20class%20means%20with%20consistent%20feature%20norms.%20Unlike%20prior%20approaches%20that%20aim%20to%20constrain%20OOD%20features%20into%20subspaces%20orthogonal%20to%20the%20collapsed%20ID%20means%2C%20BootOOD%20introduces%20a%20lightweight%20auxiliary%20head%20that%20performs%20radius-based%20classification%20on%20feature%20norms.%20This%20design%20decouples%20OOD%20detection%20from%20the%20primary%20classifier%20and%20imposes%20a%20relaxed%20requirement%3A%20OOD%20samples%20are%20learned%20to%20have%20smaller%20feature%20norms%20than%20ID%20features%2C%20which%20is%20easier%20to%20satisfy%20when%20ID%20and%20OOD%20are%20semantically%20close.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet-200%20show%20that%20BootOOD%20outperforms%20prior%20post-hoc%20methods%2C%20surpasses%20training-based%20methods%20without%20outlier%20exposure%2C%20and%20is%20competitive%20with%20state-of-the-art%20outlier-exposure%20approaches%20while%20maintaining%20or%20improving%20ID%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootOOD%253A%2520Self-Supervised%2520Out-of-Distribution%2520Detection%2520via%2520Synthetic%2520Sample%2520Exposure%2520under%2520Neural%2520Collapse%26entry.906535625%3DYuanchao%2520Wang%2520and%2520Tian%2520Qin%2520and%2520Eduardo%2520Valle%2520and%2520Bruno%2520Abrahao%26entry.1292438233%3DOut-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520critical%2520for%2520deploying%2520image%2520classifiers%2520in%2520safety-sensitive%2520environments%252C%2520yet%2520existing%2520detectors%2520often%2520struggle%2520when%2520OOD%2520samples%2520are%2520semantically%2520similar%2520to%2520the%2520in-distribution%2520%2528ID%2529%2520classes.%2520We%2520present%2520BootOOD%252C%2520a%2520fully%2520self-supervised%2520OOD%2520detection%2520framework%2520that%2520bootstraps%2520exclusively%2520from%2520ID%2520data%2520and%2520is%2520explicitly%2520designed%2520to%2520handle%2520semantically%2520challenging%2520OOD%2520samples.%2520BootOOD%2520synthesizes%2520pseudo-OOD%2520features%2520through%2520simple%2520transformations%2520of%2520ID%2520representations%2520and%2520leverages%2520Neural%2520Collapse%2520%2528NC%2529%252C%2520where%2520ID%2520features%2520cluster%2520tightly%2520around%2520class%2520means%2520with%2520consistent%2520feature%2520norms.%2520Unlike%2520prior%2520approaches%2520that%2520aim%2520to%2520constrain%2520OOD%2520features%2520into%2520subspaces%2520orthogonal%2520to%2520the%2520collapsed%2520ID%2520means%252C%2520BootOOD%2520introduces%2520a%2520lightweight%2520auxiliary%2520head%2520that%2520performs%2520radius-based%2520classification%2520on%2520feature%2520norms.%2520This%2520design%2520decouples%2520OOD%2520detection%2520from%2520the%2520primary%2520classifier%2520and%2520imposes%2520a%2520relaxed%2520requirement%253A%2520OOD%2520samples%2520are%2520learned%2520to%2520have%2520smaller%2520feature%2520norms%2520than%2520ID%2520features%252C%2520which%2520is%2520easier%2520to%2520satisfy%2520when%2520ID%2520and%2520OOD%2520are%2520semantically%2520close.%2520Experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520ImageNet-200%2520show%2520that%2520BootOOD%2520outperforms%2520prior%2520post-hoc%2520methods%252C%2520surpasses%2520training-based%2520methods%2520without%2520outlier%2520exposure%252C%2520and%2520is%2520competitive%2520with%2520state-of-the-art%2520outlier-exposure%2520approaches%2520while%2520maintaining%2520or%2520improving%2520ID%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BootOOD%3A%20Self-Supervised%20Out-of-Distribution%20Detection%20via%20Synthetic%20Sample%20Exposure%20under%20Neural%20Collapse&entry.906535625=Yuanchao%20Wang%20and%20Tian%20Qin%20and%20Eduardo%20Valle%20and%20Bruno%20Abrahao&entry.1292438233=Out-of-distribution%20%28OOD%29%20detection%20is%20critical%20for%20deploying%20image%20classifiers%20in%20safety-sensitive%20environments%2C%20yet%20existing%20detectors%20often%20struggle%20when%20OOD%20samples%20are%20semantically%20similar%20to%20the%20in-distribution%20%28ID%29%20classes.%20We%20present%20BootOOD%2C%20a%20fully%20self-supervised%20OOD%20detection%20framework%20that%20bootstraps%20exclusively%20from%20ID%20data%20and%20is%20explicitly%20designed%20to%20handle%20semantically%20challenging%20OOD%20samples.%20BootOOD%20synthesizes%20pseudo-OOD%20features%20through%20simple%20transformations%20of%20ID%20representations%20and%20leverages%20Neural%20Collapse%20%28NC%29%2C%20where%20ID%20features%20cluster%20tightly%20around%20class%20means%20with%20consistent%20feature%20norms.%20Unlike%20prior%20approaches%20that%20aim%20to%20constrain%20OOD%20features%20into%20subspaces%20orthogonal%20to%20the%20collapsed%20ID%20means%2C%20BootOOD%20introduces%20a%20lightweight%20auxiliary%20head%20that%20performs%20radius-based%20classification%20on%20feature%20norms.%20This%20design%20decouples%20OOD%20detection%20from%20the%20primary%20classifier%20and%20imposes%20a%20relaxed%20requirement%3A%20OOD%20samples%20are%20learned%20to%20have%20smaller%20feature%20norms%20than%20ID%20features%2C%20which%20is%20easier%20to%20satisfy%20when%20ID%20and%20OOD%20are%20semantically%20close.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet-200%20show%20that%20BootOOD%20outperforms%20prior%20post-hoc%20methods%2C%20surpasses%20training-based%20methods%20without%20outlier%20exposure%2C%20and%20is%20competitive%20with%20state-of-the-art%20outlier-exposure%20approaches%20while%20maintaining%20or%20improving%20ID%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2511.13539v1&entry.124074799=Read"},
{"title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling", "author": "Adam Hazimeh and Ke Wang and Mark Collier and Gilles Baechler and Efi Kokiopoulou and Pascal Frossard", "abstract": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.", "link": "http://arxiv.org/abs/2511.13478v1", "date": "2025-11-17", "relevancy": 2.5377, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5142}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Document%20Derendering%3A%20SVG%20Reconstruction%20via%20Vision-Language%20Modeling&body=Title%3A%20Semantic%20Document%20Derendering%3A%20SVG%20Reconstruction%20via%20Vision-Language%20Modeling%0AAuthor%3A%20Adam%20Hazimeh%20and%20Ke%20Wang%20and%20Mark%20Collier%20and%20Gilles%20Baechler%20and%20Efi%20Kokiopoulou%20and%20Pascal%20Frossard%0AAbstract%3A%20Multimedia%20documents%20such%20as%20slide%20presentations%20and%20posters%20are%20designed%20to%20be%20interactive%20and%20easy%20to%20modify.%20Yet%2C%20they%20are%20often%20distributed%20in%20a%20static%20raster%20format%2C%20which%20limits%20editing%20and%20customization.%20Restoring%20their%20editability%20requires%20converting%20these%20raster%20images%20back%20into%20structured%20vector%20formats.%20However%2C%20existing%20geometric%20raster-vectorization%20methods%2C%20which%20rely%20on%20low-level%20primitives%20like%20curves%20and%20polygons%2C%20fall%20short%20at%20this%20task.%20Specifically%2C%20when%20applied%20to%20complex%20documents%20like%20slides%2C%20they%20fail%20to%20preserve%20the%20high-level%20structure%2C%20resulting%20in%20a%20flat%20collection%20of%20shapes%20where%20the%20semantic%20distinction%20between%20image%20and%20text%20elements%20is%20lost.%20To%20overcome%20this%20limitation%2C%20we%20address%20the%20problem%20of%20semantic%20document%20derendering%20by%20introducing%20SliDer%2C%20a%20novel%20framework%20that%20uses%20Vision-Language%20Models%20%28VLMs%29%20to%20derender%20slide%20images%20as%20compact%20and%20editable%20Scalable%20Vector%20Graphic%20%28SVG%29%20representations.%20SliDer%20detects%20and%20extracts%20attributes%20from%20individual%20image%20and%20text%20elements%20in%20a%20raster%20input%20and%20organizes%20them%20into%20a%20coherent%20SVG%20format.%20Crucially%2C%20the%20model%20iteratively%20refines%20its%20predictions%20during%20inference%20in%20a%20process%20analogous%20to%20human%20design%2C%20generating%20SVG%20code%20that%20more%20faithfully%20reconstructs%20the%20original%20raster%20upon%20rendering.%20Furthermore%2C%20we%20introduce%20Slide2SVG%2C%20a%20novel%20dataset%20comprising%20raster-SVG%20pairs%20of%20slide%20documents%20curated%20from%20real-world%20scientific%20presentations%2C%20to%20facilitate%20future%20research%20in%20this%20domain.%20Our%20results%20demonstrate%20that%20SliDer%20achieves%20a%20reconstruction%20LPIPS%20of%200.069%20and%20is%20favored%20by%20human%20evaluators%20in%2082.9%25%20of%20cases%20compared%20to%20the%20strongest%20zero-shot%20VLM%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Document%2520Derendering%253A%2520SVG%2520Reconstruction%2520via%2520Vision-Language%2520Modeling%26entry.906535625%3DAdam%2520Hazimeh%2520and%2520Ke%2520Wang%2520and%2520Mark%2520Collier%2520and%2520Gilles%2520Baechler%2520and%2520Efi%2520Kokiopoulou%2520and%2520Pascal%2520Frossard%26entry.1292438233%3DMultimedia%2520documents%2520such%2520as%2520slide%2520presentations%2520and%2520posters%2520are%2520designed%2520to%2520be%2520interactive%2520and%2520easy%2520to%2520modify.%2520Yet%252C%2520they%2520are%2520often%2520distributed%2520in%2520a%2520static%2520raster%2520format%252C%2520which%2520limits%2520editing%2520and%2520customization.%2520Restoring%2520their%2520editability%2520requires%2520converting%2520these%2520raster%2520images%2520back%2520into%2520structured%2520vector%2520formats.%2520However%252C%2520existing%2520geometric%2520raster-vectorization%2520methods%252C%2520which%2520rely%2520on%2520low-level%2520primitives%2520like%2520curves%2520and%2520polygons%252C%2520fall%2520short%2520at%2520this%2520task.%2520Specifically%252C%2520when%2520applied%2520to%2520complex%2520documents%2520like%2520slides%252C%2520they%2520fail%2520to%2520preserve%2520the%2520high-level%2520structure%252C%2520resulting%2520in%2520a%2520flat%2520collection%2520of%2520shapes%2520where%2520the%2520semantic%2520distinction%2520between%2520image%2520and%2520text%2520elements%2520is%2520lost.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520address%2520the%2520problem%2520of%2520semantic%2520document%2520derendering%2520by%2520introducing%2520SliDer%252C%2520a%2520novel%2520framework%2520that%2520uses%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520derender%2520slide%2520images%2520as%2520compact%2520and%2520editable%2520Scalable%2520Vector%2520Graphic%2520%2528SVG%2529%2520representations.%2520SliDer%2520detects%2520and%2520extracts%2520attributes%2520from%2520individual%2520image%2520and%2520text%2520elements%2520in%2520a%2520raster%2520input%2520and%2520organizes%2520them%2520into%2520a%2520coherent%2520SVG%2520format.%2520Crucially%252C%2520the%2520model%2520iteratively%2520refines%2520its%2520predictions%2520during%2520inference%2520in%2520a%2520process%2520analogous%2520to%2520human%2520design%252C%2520generating%2520SVG%2520code%2520that%2520more%2520faithfully%2520reconstructs%2520the%2520original%2520raster%2520upon%2520rendering.%2520Furthermore%252C%2520we%2520introduce%2520Slide2SVG%252C%2520a%2520novel%2520dataset%2520comprising%2520raster-SVG%2520pairs%2520of%2520slide%2520documents%2520curated%2520from%2520real-world%2520scientific%2520presentations%252C%2520to%2520facilitate%2520future%2520research%2520in%2520this%2520domain.%2520Our%2520results%2520demonstrate%2520that%2520SliDer%2520achieves%2520a%2520reconstruction%2520LPIPS%2520of%25200.069%2520and%2520is%2520favored%2520by%2520human%2520evaluators%2520in%252082.9%2525%2520of%2520cases%2520compared%2520to%2520the%2520strongest%2520zero-shot%2520VLM%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Document%20Derendering%3A%20SVG%20Reconstruction%20via%20Vision-Language%20Modeling&entry.906535625=Adam%20Hazimeh%20and%20Ke%20Wang%20and%20Mark%20Collier%20and%20Gilles%20Baechler%20and%20Efi%20Kokiopoulou%20and%20Pascal%20Frossard&entry.1292438233=Multimedia%20documents%20such%20as%20slide%20presentations%20and%20posters%20are%20designed%20to%20be%20interactive%20and%20easy%20to%20modify.%20Yet%2C%20they%20are%20often%20distributed%20in%20a%20static%20raster%20format%2C%20which%20limits%20editing%20and%20customization.%20Restoring%20their%20editability%20requires%20converting%20these%20raster%20images%20back%20into%20structured%20vector%20formats.%20However%2C%20existing%20geometric%20raster-vectorization%20methods%2C%20which%20rely%20on%20low-level%20primitives%20like%20curves%20and%20polygons%2C%20fall%20short%20at%20this%20task.%20Specifically%2C%20when%20applied%20to%20complex%20documents%20like%20slides%2C%20they%20fail%20to%20preserve%20the%20high-level%20structure%2C%20resulting%20in%20a%20flat%20collection%20of%20shapes%20where%20the%20semantic%20distinction%20between%20image%20and%20text%20elements%20is%20lost.%20To%20overcome%20this%20limitation%2C%20we%20address%20the%20problem%20of%20semantic%20document%20derendering%20by%20introducing%20SliDer%2C%20a%20novel%20framework%20that%20uses%20Vision-Language%20Models%20%28VLMs%29%20to%20derender%20slide%20images%20as%20compact%20and%20editable%20Scalable%20Vector%20Graphic%20%28SVG%29%20representations.%20SliDer%20detects%20and%20extracts%20attributes%20from%20individual%20image%20and%20text%20elements%20in%20a%20raster%20input%20and%20organizes%20them%20into%20a%20coherent%20SVG%20format.%20Crucially%2C%20the%20model%20iteratively%20refines%20its%20predictions%20during%20inference%20in%20a%20process%20analogous%20to%20human%20design%2C%20generating%20SVG%20code%20that%20more%20faithfully%20reconstructs%20the%20original%20raster%20upon%20rendering.%20Furthermore%2C%20we%20introduce%20Slide2SVG%2C%20a%20novel%20dataset%20comprising%20raster-SVG%20pairs%20of%20slide%20documents%20curated%20from%20real-world%20scientific%20presentations%2C%20to%20facilitate%20future%20research%20in%20this%20domain.%20Our%20results%20demonstrate%20that%20SliDer%20achieves%20a%20reconstruction%20LPIPS%20of%200.069%20and%20is%20favored%20by%20human%20evaluators%20in%2082.9%25%20of%20cases%20compared%20to%20the%20strongest%20zero-shot%20VLM%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2511.13478v1&entry.124074799=Read"},
{"title": "AtlasMorph: Learning conditional deformable templates for brain MRI", "author": "Marianne Rakic and Andrew Hoopes and S. Mazdak Abulnaga and Mert R. Sabuncu and John V. Guttag and Adrian V. Dalca", "abstract": "Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.", "link": "http://arxiv.org/abs/2511.13609v1", "date": "2025-11-17", "relevancy": 2.5261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.496}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AtlasMorph%3A%20Learning%20conditional%20deformable%20templates%20for%20brain%20MRI&body=Title%3A%20AtlasMorph%3A%20Learning%20conditional%20deformable%20templates%20for%20brain%20MRI%0AAuthor%3A%20Marianne%20Rakic%20and%20Andrew%20Hoopes%20and%20S.%20Mazdak%20Abulnaga%20and%20Mert%20R.%20Sabuncu%20and%20John%20V.%20Guttag%20and%20Adrian%20V.%20Dalca%0AAbstract%3A%20Deformable%20templates%2C%20or%20atlases%2C%20are%20images%20that%20represent%20a%20prototypical%20anatomy%20for%20a%20population%2C%20and%20are%20often%20enhanced%20with%20probabilistic%20anatomical%20label%20maps.%20They%20are%20commonly%20used%20in%20medical%20image%20analysis%20for%20population%20studies%20and%20computational%20anatomy%20tasks%20such%20as%20registration%20and%20segmentation.%20Because%20developing%20a%20template%20is%20a%20computationally%20expensive%20process%2C%20relatively%20few%20templates%20are%20available.%20As%20a%20result%2C%20analysis%20is%20often%20conducted%20with%20sub-optimal%20templates%20that%20are%20not%20truly%20representative%20of%20the%20study%20population%2C%20especially%20when%20there%20are%20large%20variations%20within%20this%20population.%20We%20propose%20a%20machine%20learning%20framework%20that%20uses%20convolutional%20registration%20neural%20networks%20to%20efficiently%20learn%20a%20function%20that%20outputs%20templates%20conditioned%20on%20subject-specific%20attributes%2C%20such%20as%20age%20and%20sex.%20We%20also%20leverage%20segmentations%2C%20when%20available%2C%20to%20produce%20anatomical%20segmentation%20maps%20for%20the%20resulting%20templates.%20The%20learned%20network%20can%20also%20be%20used%20to%20register%20subject%20images%20to%20the%20templates.%20We%20demonstrate%20our%20method%20on%20a%20compilation%20of%203D%20brain%20MRI%20datasets%2C%20and%20show%20that%20it%20can%20learn%20high-quality%20templates%20that%20are%20representative%20of%20populations.%20We%20find%20that%20annotated%20conditional%20templates%20enable%20better%20registration%20than%20their%20unlabeled%20unconditional%20counterparts%2C%20and%20outperform%20other%20templates%20construction%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtlasMorph%253A%2520Learning%2520conditional%2520deformable%2520templates%2520for%2520brain%2520MRI%26entry.906535625%3DMarianne%2520Rakic%2520and%2520Andrew%2520Hoopes%2520and%2520S.%2520Mazdak%2520Abulnaga%2520and%2520Mert%2520R.%2520Sabuncu%2520and%2520John%2520V.%2520Guttag%2520and%2520Adrian%2520V.%2520Dalca%26entry.1292438233%3DDeformable%2520templates%252C%2520or%2520atlases%252C%2520are%2520images%2520that%2520represent%2520a%2520prototypical%2520anatomy%2520for%2520a%2520population%252C%2520and%2520are%2520often%2520enhanced%2520with%2520probabilistic%2520anatomical%2520label%2520maps.%2520They%2520are%2520commonly%2520used%2520in%2520medical%2520image%2520analysis%2520for%2520population%2520studies%2520and%2520computational%2520anatomy%2520tasks%2520such%2520as%2520registration%2520and%2520segmentation.%2520Because%2520developing%2520a%2520template%2520is%2520a%2520computationally%2520expensive%2520process%252C%2520relatively%2520few%2520templates%2520are%2520available.%2520As%2520a%2520result%252C%2520analysis%2520is%2520often%2520conducted%2520with%2520sub-optimal%2520templates%2520that%2520are%2520not%2520truly%2520representative%2520of%2520the%2520study%2520population%252C%2520especially%2520when%2520there%2520are%2520large%2520variations%2520within%2520this%2520population.%2520We%2520propose%2520a%2520machine%2520learning%2520framework%2520that%2520uses%2520convolutional%2520registration%2520neural%2520networks%2520to%2520efficiently%2520learn%2520a%2520function%2520that%2520outputs%2520templates%2520conditioned%2520on%2520subject-specific%2520attributes%252C%2520such%2520as%2520age%2520and%2520sex.%2520We%2520also%2520leverage%2520segmentations%252C%2520when%2520available%252C%2520to%2520produce%2520anatomical%2520segmentation%2520maps%2520for%2520the%2520resulting%2520templates.%2520The%2520learned%2520network%2520can%2520also%2520be%2520used%2520to%2520register%2520subject%2520images%2520to%2520the%2520templates.%2520We%2520demonstrate%2520our%2520method%2520on%2520a%2520compilation%2520of%25203D%2520brain%2520MRI%2520datasets%252C%2520and%2520show%2520that%2520it%2520can%2520learn%2520high-quality%2520templates%2520that%2520are%2520representative%2520of%2520populations.%2520We%2520find%2520that%2520annotated%2520conditional%2520templates%2520enable%2520better%2520registration%2520than%2520their%2520unlabeled%2520unconditional%2520counterparts%252C%2520and%2520outperform%2520other%2520templates%2520construction%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AtlasMorph%3A%20Learning%20conditional%20deformable%20templates%20for%20brain%20MRI&entry.906535625=Marianne%20Rakic%20and%20Andrew%20Hoopes%20and%20S.%20Mazdak%20Abulnaga%20and%20Mert%20R.%20Sabuncu%20and%20John%20V.%20Guttag%20and%20Adrian%20V.%20Dalca&entry.1292438233=Deformable%20templates%2C%20or%20atlases%2C%20are%20images%20that%20represent%20a%20prototypical%20anatomy%20for%20a%20population%2C%20and%20are%20often%20enhanced%20with%20probabilistic%20anatomical%20label%20maps.%20They%20are%20commonly%20used%20in%20medical%20image%20analysis%20for%20population%20studies%20and%20computational%20anatomy%20tasks%20such%20as%20registration%20and%20segmentation.%20Because%20developing%20a%20template%20is%20a%20computationally%20expensive%20process%2C%20relatively%20few%20templates%20are%20available.%20As%20a%20result%2C%20analysis%20is%20often%20conducted%20with%20sub-optimal%20templates%20that%20are%20not%20truly%20representative%20of%20the%20study%20population%2C%20especially%20when%20there%20are%20large%20variations%20within%20this%20population.%20We%20propose%20a%20machine%20learning%20framework%20that%20uses%20convolutional%20registration%20neural%20networks%20to%20efficiently%20learn%20a%20function%20that%20outputs%20templates%20conditioned%20on%20subject-specific%20attributes%2C%20such%20as%20age%20and%20sex.%20We%20also%20leverage%20segmentations%2C%20when%20available%2C%20to%20produce%20anatomical%20segmentation%20maps%20for%20the%20resulting%20templates.%20The%20learned%20network%20can%20also%20be%20used%20to%20register%20subject%20images%20to%20the%20templates.%20We%20demonstrate%20our%20method%20on%20a%20compilation%20of%203D%20brain%20MRI%20datasets%2C%20and%20show%20that%20it%20can%20learn%20high-quality%20templates%20that%20are%20representative%20of%20populations.%20We%20find%20that%20annotated%20conditional%20templates%20enable%20better%20registration%20than%20their%20unlabeled%20unconditional%20counterparts%2C%20and%20outperform%20other%20templates%20construction%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.13609v1&entry.124074799=Read"},
{"title": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection", "author": "Feng Ding and Wenhui Yi and Yunpeng Zhou and Xinan He and Hong Rao and Shu Hu", "abstract": "Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.", "link": "http://arxiv.org/abs/2511.10150v2", "date": "2025-11-17", "relevancy": 2.5246, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5112}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5064}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Bias%2C%20Aligning%20Distributions%3A%20Synergistic%20Fairness%20Optimization%20for%20Deepfake%20Detection&body=Title%3A%20Decoupling%20Bias%2C%20Aligning%20Distributions%3A%20Synergistic%20Fairness%20Optimization%20for%20Deepfake%20Detection%0AAuthor%3A%20Feng%20Ding%20and%20Wenhui%20Yi%20and%20Yunpeng%20Zhou%20and%20Xinan%20He%20and%20Hong%20Rao%20and%20Shu%20Hu%0AAbstract%3A%20Fairness%20is%20a%20core%20element%20in%20the%20trustworthy%20deployment%20of%20deepfake%20detection%20models%2C%20especially%20in%20the%20field%20of%20digital%20identity%20security.%20Biases%20in%20detection%20models%20toward%20different%20demographic%20groups%2C%20such%20as%20gender%20and%20race%2C%20may%20lead%20to%20systemic%20misjudgments%2C%20exacerbating%20the%20digital%20divide%20and%20social%20inequities.%20However%2C%20current%20fairness-enhanced%20detectors%20often%20improve%20fairness%20at%20the%20cost%20of%20detection%20accuracy.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20dual-mechanism%20collaborative%20optimization%20framework.%20Our%20proposed%20method%20innovatively%20integrates%20structural%20fairness%20decoupling%20and%20global%20distribution%20alignment%3A%20decoupling%20channels%20sensitive%20to%20demographic%20groups%20at%20the%20model%20architectural%20level%2C%20and%20subsequently%20reducing%20the%20distance%20between%20the%20overall%20sample%20distribution%20and%20the%20distributions%20corresponding%20to%20each%20demographic%20group%20at%20the%20feature%20level.%20Experimental%20results%20demonstrate%20that%2C%20compared%20with%20other%20methods%2C%20our%20framework%20improves%20both%20inter-group%20and%20intra-group%20fairness%20while%20maintaining%20overall%20detection%20accuracy%20across%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10150v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Bias%252C%2520Aligning%2520Distributions%253A%2520Synergistic%2520Fairness%2520Optimization%2520for%2520Deepfake%2520Detection%26entry.906535625%3DFeng%2520Ding%2520and%2520Wenhui%2520Yi%2520and%2520Yunpeng%2520Zhou%2520and%2520Xinan%2520He%2520and%2520Hong%2520Rao%2520and%2520Shu%2520Hu%26entry.1292438233%3DFairness%2520is%2520a%2520core%2520element%2520in%2520the%2520trustworthy%2520deployment%2520of%2520deepfake%2520detection%2520models%252C%2520especially%2520in%2520the%2520field%2520of%2520digital%2520identity%2520security.%2520Biases%2520in%2520detection%2520models%2520toward%2520different%2520demographic%2520groups%252C%2520such%2520as%2520gender%2520and%2520race%252C%2520may%2520lead%2520to%2520systemic%2520misjudgments%252C%2520exacerbating%2520the%2520digital%2520divide%2520and%2520social%2520inequities.%2520However%252C%2520current%2520fairness-enhanced%2520detectors%2520often%2520improve%2520fairness%2520at%2520the%2520cost%2520of%2520detection%2520accuracy.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520dual-mechanism%2520collaborative%2520optimization%2520framework.%2520Our%2520proposed%2520method%2520innovatively%2520integrates%2520structural%2520fairness%2520decoupling%2520and%2520global%2520distribution%2520alignment%253A%2520decoupling%2520channels%2520sensitive%2520to%2520demographic%2520groups%2520at%2520the%2520model%2520architectural%2520level%252C%2520and%2520subsequently%2520reducing%2520the%2520distance%2520between%2520the%2520overall%2520sample%2520distribution%2520and%2520the%2520distributions%2520corresponding%2520to%2520each%2520demographic%2520group%2520at%2520the%2520feature%2520level.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520compared%2520with%2520other%2520methods%252C%2520our%2520framework%2520improves%2520both%2520inter-group%2520and%2520intra-group%2520fairness%2520while%2520maintaining%2520overall%2520detection%2520accuracy%2520across%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10150v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Bias%2C%20Aligning%20Distributions%3A%20Synergistic%20Fairness%20Optimization%20for%20Deepfake%20Detection&entry.906535625=Feng%20Ding%20and%20Wenhui%20Yi%20and%20Yunpeng%20Zhou%20and%20Xinan%20He%20and%20Hong%20Rao%20and%20Shu%20Hu&entry.1292438233=Fairness%20is%20a%20core%20element%20in%20the%20trustworthy%20deployment%20of%20deepfake%20detection%20models%2C%20especially%20in%20the%20field%20of%20digital%20identity%20security.%20Biases%20in%20detection%20models%20toward%20different%20demographic%20groups%2C%20such%20as%20gender%20and%20race%2C%20may%20lead%20to%20systemic%20misjudgments%2C%20exacerbating%20the%20digital%20divide%20and%20social%20inequities.%20However%2C%20current%20fairness-enhanced%20detectors%20often%20improve%20fairness%20at%20the%20cost%20of%20detection%20accuracy.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20dual-mechanism%20collaborative%20optimization%20framework.%20Our%20proposed%20method%20innovatively%20integrates%20structural%20fairness%20decoupling%20and%20global%20distribution%20alignment%3A%20decoupling%20channels%20sensitive%20to%20demographic%20groups%20at%20the%20model%20architectural%20level%2C%20and%20subsequently%20reducing%20the%20distance%20between%20the%20overall%20sample%20distribution%20and%20the%20distributions%20corresponding%20to%20each%20demographic%20group%20at%20the%20feature%20level.%20Experimental%20results%20demonstrate%20that%2C%20compared%20with%20other%20methods%2C%20our%20framework%20improves%20both%20inter-group%20and%20intra-group%20fairness%20while%20maintaining%20overall%20detection%20accuracy%20across%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2511.10150v2&entry.124074799=Read"},
{"title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task", "author": "Xingming Long and Jie Zhang and Shiguang Shan and Xilin Chen", "abstract": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.", "link": "http://arxiv.org/abs/2511.13420v1", "date": "2025-11-17", "relevancy": 2.5193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VOPE%3A%20Revisiting%20Hallucination%20of%20Vision-Language%20Models%20in%20Voluntary%20Imagination%20Task&body=Title%3A%20VOPE%3A%20Revisiting%20Hallucination%20of%20Vision-Language%20Models%20in%20Voluntary%20Imagination%20Task%0AAuthor%3A%20Xingming%20Long%20and%20Jie%20Zhang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20Most%20research%20on%20hallucinations%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%20focuses%20on%20factual%20description%20tasks%20that%20prohibit%20any%20output%20absent%20from%20the%20image.%20However%2C%20little%20attention%20has%20been%20paid%20to%20hallucinations%20in%20voluntary%20imagination%20tasks%2C%20e.g.%2C%20story%20writing%2C%20where%20the%20models%20are%20expected%20to%20generate%20novel%20content%20beyond%20the%20given%20image.%20In%20these%20tasks%2C%20it%20is%20inappropriate%20to%20simply%20regard%20such%20imagined%20novel%20content%20as%20hallucinations.%20To%20address%20this%20limitation%2C%20we%20introduce%20Voluntary-imagined%20Object%20Presence%20Evaluation%20%28VOPE%29-a%20novel%20method%20to%20assess%20LVLMs%27%20hallucinations%20in%20voluntary%20imagination%20tasks%20via%20presence%20evaluation.%20Specifically%2C%20VOPE%20poses%20recheck-based%20questions%20to%20evaluate%20how%20an%20LVLM%20interprets%20the%20presence%20of%20the%20imagined%20objects%20in%20its%20own%20response.%20The%20consistency%20between%20the%20model%27s%20interpretation%20and%20the%20object%27s%20presence%20in%20the%20image%20is%20then%20used%20to%20determine%20whether%20the%20model%20hallucinates%20when%20generating%20the%20response.%20We%20apply%20VOPE%20to%20several%20mainstream%20LVLMs%20and%20hallucination%20mitigation%20methods%2C%20revealing%20two%20key%20findings%3A%20%281%29%20most%20LVLMs%20hallucinate%20heavily%20during%20voluntary%20imagination%2C%20and%20their%20performance%20in%20presence%20evaluation%20is%20notably%20poor%20on%20imagined%20objects%3B%20%282%29%20existing%20hallucination%20mitigation%20methods%20show%20limited%20effect%20in%20voluntary%20imagination%20tasks%2C%20making%20this%20an%20important%20direction%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVOPE%253A%2520Revisiting%2520Hallucination%2520of%2520Vision-Language%2520Models%2520in%2520Voluntary%2520Imagination%2520Task%26entry.906535625%3DXingming%2520Long%2520and%2520Jie%2520Zhang%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3DMost%2520research%2520on%2520hallucinations%2520in%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520focuses%2520on%2520factual%2520description%2520tasks%2520that%2520prohibit%2520any%2520output%2520absent%2520from%2520the%2520image.%2520However%252C%2520little%2520attention%2520has%2520been%2520paid%2520to%2520hallucinations%2520in%2520voluntary%2520imagination%2520tasks%252C%2520e.g.%252C%2520story%2520writing%252C%2520where%2520the%2520models%2520are%2520expected%2520to%2520generate%2520novel%2520content%2520beyond%2520the%2520given%2520image.%2520In%2520these%2520tasks%252C%2520it%2520is%2520inappropriate%2520to%2520simply%2520regard%2520such%2520imagined%2520novel%2520content%2520as%2520hallucinations.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Voluntary-imagined%2520Object%2520Presence%2520Evaluation%2520%2528VOPE%2529-a%2520novel%2520method%2520to%2520assess%2520LVLMs%2527%2520hallucinations%2520in%2520voluntary%2520imagination%2520tasks%2520via%2520presence%2520evaluation.%2520Specifically%252C%2520VOPE%2520poses%2520recheck-based%2520questions%2520to%2520evaluate%2520how%2520an%2520LVLM%2520interprets%2520the%2520presence%2520of%2520the%2520imagined%2520objects%2520in%2520its%2520own%2520response.%2520The%2520consistency%2520between%2520the%2520model%2527s%2520interpretation%2520and%2520the%2520object%2527s%2520presence%2520in%2520the%2520image%2520is%2520then%2520used%2520to%2520determine%2520whether%2520the%2520model%2520hallucinates%2520when%2520generating%2520the%2520response.%2520We%2520apply%2520VOPE%2520to%2520several%2520mainstream%2520LVLMs%2520and%2520hallucination%2520mitigation%2520methods%252C%2520revealing%2520two%2520key%2520findings%253A%2520%25281%2529%2520most%2520LVLMs%2520hallucinate%2520heavily%2520during%2520voluntary%2520imagination%252C%2520and%2520their%2520performance%2520in%2520presence%2520evaluation%2520is%2520notably%2520poor%2520on%2520imagined%2520objects%253B%2520%25282%2529%2520existing%2520hallucination%2520mitigation%2520methods%2520show%2520limited%2520effect%2520in%2520voluntary%2520imagination%2520tasks%252C%2520making%2520this%2520an%2520important%2520direction%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VOPE%3A%20Revisiting%20Hallucination%20of%20Vision-Language%20Models%20in%20Voluntary%20Imagination%20Task&entry.906535625=Xingming%20Long%20and%20Jie%20Zhang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=Most%20research%20on%20hallucinations%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%20focuses%20on%20factual%20description%20tasks%20that%20prohibit%20any%20output%20absent%20from%20the%20image.%20However%2C%20little%20attention%20has%20been%20paid%20to%20hallucinations%20in%20voluntary%20imagination%20tasks%2C%20e.g.%2C%20story%20writing%2C%20where%20the%20models%20are%20expected%20to%20generate%20novel%20content%20beyond%20the%20given%20image.%20In%20these%20tasks%2C%20it%20is%20inappropriate%20to%20simply%20regard%20such%20imagined%20novel%20content%20as%20hallucinations.%20To%20address%20this%20limitation%2C%20we%20introduce%20Voluntary-imagined%20Object%20Presence%20Evaluation%20%28VOPE%29-a%20novel%20method%20to%20assess%20LVLMs%27%20hallucinations%20in%20voluntary%20imagination%20tasks%20via%20presence%20evaluation.%20Specifically%2C%20VOPE%20poses%20recheck-based%20questions%20to%20evaluate%20how%20an%20LVLM%20interprets%20the%20presence%20of%20the%20imagined%20objects%20in%20its%20own%20response.%20The%20consistency%20between%20the%20model%27s%20interpretation%20and%20the%20object%27s%20presence%20in%20the%20image%20is%20then%20used%20to%20determine%20whether%20the%20model%20hallucinates%20when%20generating%20the%20response.%20We%20apply%20VOPE%20to%20several%20mainstream%20LVLMs%20and%20hallucination%20mitigation%20methods%2C%20revealing%20two%20key%20findings%3A%20%281%29%20most%20LVLMs%20hallucinate%20heavily%20during%20voluntary%20imagination%2C%20and%20their%20performance%20in%20presence%20evaluation%20is%20notably%20poor%20on%20imagined%20objects%3B%20%282%29%20existing%20hallucination%20mitigation%20methods%20show%20limited%20effect%20in%20voluntary%20imagination%20tasks%2C%20making%20this%20an%20important%20direction%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.13420v1&entry.124074799=Read"},
{"title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders", "author": "David Chanin and James Wilken-Smith and Tom\u00e1\u0161 Dulka and Hardik Bhatnagar and Satvik Golechha and Joseph Bloom", "abstract": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get \"absorbed\" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.", "link": "http://arxiv.org/abs/2409.14507v6", "date": "2025-11-17", "relevancy": 2.5143, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5132}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%20Autoencoders&body=Title%3A%20A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%20Autoencoders%0AAuthor%3A%20David%20Chanin%20and%20James%20Wilken-Smith%20and%20Tom%C3%A1%C5%A1%20Dulka%20and%20Hardik%20Bhatnagar%20and%20Satvik%20Golechha%20and%20Joseph%20Bloom%0AAbstract%3A%20Sparse%20Autoencoders%20%28SAEs%29%20aim%20to%20decompose%20the%20activation%20space%20of%20large%20language%20models%20%28LLMs%29%20into%20human-interpretable%20latent%20directions%20or%20features.%20As%20we%20increase%20the%20number%20of%20features%20in%20the%20SAE%2C%20hierarchical%20features%20tend%20to%20split%20into%20finer%20features%20%28%22math%22%20may%20split%20into%20%22algebra%22%2C%20%22geometry%22%2C%20etc.%29%2C%20a%20phenomenon%20referred%20to%20as%20feature%20splitting.%20However%2C%20we%20show%20that%20sparse%20decomposition%20and%20splitting%20of%20hierarchical%20features%20is%20not%20robust.%20Specifically%2C%20we%20show%20that%20seemingly%20monosemantic%20features%20fail%20to%20fire%20where%20they%20should%2C%20and%20instead%20get%20%22absorbed%22%20into%20their%20children%20features.%20We%20coin%20this%20phenomenon%20feature%20absorption%2C%20and%20show%20that%20it%20is%20caused%20by%20optimizing%20for%20sparsity%20in%20SAEs%20whenever%20the%20underlying%20features%20form%20a%20hierarchy.%20We%20introduce%20a%20metric%20to%20detect%20absorption%20in%20SAEs%2C%20and%20validate%20our%20findings%20empirically%20on%20hundreds%20of%20LLM%20SAEs.%20Our%20investigation%20suggests%20that%20varying%20SAE%20sizes%20or%20sparsity%20is%20insufficient%20to%20solve%20this%20issue.%20We%20discuss%20the%20implications%20of%20feature%20absorption%20in%20SAEs%20and%20some%20potential%20approaches%20to%20solve%20the%20fundamental%20theoretical%20issues%20before%20SAEs%20can%20be%20used%20for%20interpreting%20LLMs%20robustly%20and%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2409.14507v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520is%2520for%2520Absorption%253A%2520Studying%2520Feature%2520Splitting%2520and%2520Absorption%2520in%2520Sparse%2520Autoencoders%26entry.906535625%3DDavid%2520Chanin%2520and%2520James%2520Wilken-Smith%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Dulka%2520and%2520Hardik%2520Bhatnagar%2520and%2520Satvik%2520Golechha%2520and%2520Joseph%2520Bloom%26entry.1292438233%3DSparse%2520Autoencoders%2520%2528SAEs%2529%2520aim%2520to%2520decompose%2520the%2520activation%2520space%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520human-interpretable%2520latent%2520directions%2520or%2520features.%2520As%2520we%2520increase%2520the%2520number%2520of%2520features%2520in%2520the%2520SAE%252C%2520hierarchical%2520features%2520tend%2520to%2520split%2520into%2520finer%2520features%2520%2528%2522math%2522%2520may%2520split%2520into%2520%2522algebra%2522%252C%2520%2522geometry%2522%252C%2520etc.%2529%252C%2520a%2520phenomenon%2520referred%2520to%2520as%2520feature%2520splitting.%2520However%252C%2520we%2520show%2520that%2520sparse%2520decomposition%2520and%2520splitting%2520of%2520hierarchical%2520features%2520is%2520not%2520robust.%2520Specifically%252C%2520we%2520show%2520that%2520seemingly%2520monosemantic%2520features%2520fail%2520to%2520fire%2520where%2520they%2520should%252C%2520and%2520instead%2520get%2520%2522absorbed%2522%2520into%2520their%2520children%2520features.%2520We%2520coin%2520this%2520phenomenon%2520feature%2520absorption%252C%2520and%2520show%2520that%2520it%2520is%2520caused%2520by%2520optimizing%2520for%2520sparsity%2520in%2520SAEs%2520whenever%2520the%2520underlying%2520features%2520form%2520a%2520hierarchy.%2520We%2520introduce%2520a%2520metric%2520to%2520detect%2520absorption%2520in%2520SAEs%252C%2520and%2520validate%2520our%2520findings%2520empirically%2520on%2520hundreds%2520of%2520LLM%2520SAEs.%2520Our%2520investigation%2520suggests%2520that%2520varying%2520SAE%2520sizes%2520or%2520sparsity%2520is%2520insufficient%2520to%2520solve%2520this%2520issue.%2520We%2520discuss%2520the%2520implications%2520of%2520feature%2520absorption%2520in%2520SAEs%2520and%2520some%2520potential%2520approaches%2520to%2520solve%2520the%2520fundamental%2520theoretical%2520issues%2520before%2520SAEs%2520can%2520be%2520used%2520for%2520interpreting%2520LLMs%2520robustly%2520and%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14507v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%20Autoencoders&entry.906535625=David%20Chanin%20and%20James%20Wilken-Smith%20and%20Tom%C3%A1%C5%A1%20Dulka%20and%20Hardik%20Bhatnagar%20and%20Satvik%20Golechha%20and%20Joseph%20Bloom&entry.1292438233=Sparse%20Autoencoders%20%28SAEs%29%20aim%20to%20decompose%20the%20activation%20space%20of%20large%20language%20models%20%28LLMs%29%20into%20human-interpretable%20latent%20directions%20or%20features.%20As%20we%20increase%20the%20number%20of%20features%20in%20the%20SAE%2C%20hierarchical%20features%20tend%20to%20split%20into%20finer%20features%20%28%22math%22%20may%20split%20into%20%22algebra%22%2C%20%22geometry%22%2C%20etc.%29%2C%20a%20phenomenon%20referred%20to%20as%20feature%20splitting.%20However%2C%20we%20show%20that%20sparse%20decomposition%20and%20splitting%20of%20hierarchical%20features%20is%20not%20robust.%20Specifically%2C%20we%20show%20that%20seemingly%20monosemantic%20features%20fail%20to%20fire%20where%20they%20should%2C%20and%20instead%20get%20%22absorbed%22%20into%20their%20children%20features.%20We%20coin%20this%20phenomenon%20feature%20absorption%2C%20and%20show%20that%20it%20is%20caused%20by%20optimizing%20for%20sparsity%20in%20SAEs%20whenever%20the%20underlying%20features%20form%20a%20hierarchy.%20We%20introduce%20a%20metric%20to%20detect%20absorption%20in%20SAEs%2C%20and%20validate%20our%20findings%20empirically%20on%20hundreds%20of%20LLM%20SAEs.%20Our%20investigation%20suggests%20that%20varying%20SAE%20sizes%20or%20sparsity%20is%20insufficient%20to%20solve%20this%20issue.%20We%20discuss%20the%20implications%20of%20feature%20absorption%20in%20SAEs%20and%20some%20potential%20approaches%20to%20solve%20the%20fundamental%20theoretical%20issues%20before%20SAEs%20can%20be%20used%20for%20interpreting%20LLMs%20robustly%20and%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2409.14507v6&entry.124074799=Read"},
{"title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images", "author": "Kesi Xu and Eleni Chiou and Ali Varamesh and Laura Acqualagna and Nasir Rajpoot", "abstract": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.", "link": "http://arxiv.org/abs/2511.13615v1", "date": "2025-11-17", "relevancy": 2.5139, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5025}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tissue%20Aware%20Nuclei%20Detection%20and%20Classification%20Model%20for%20Histopathology%20Images&body=Title%3A%20Tissue%20Aware%20Nuclei%20Detection%20and%20Classification%20Model%20for%20Histopathology%20Images%0AAuthor%3A%20Kesi%20Xu%20and%20Eleni%20Chiou%20and%20Ali%20Varamesh%20and%20Laura%20Acqualagna%20and%20Nasir%20Rajpoot%0AAbstract%3A%20Accurate%20nuclei%20detection%20and%20classification%20are%20fundamental%20to%20computational%20pathology%2C%20yet%20existing%20approaches%20are%20hindered%20by%20reliance%20on%20detailed%20expert%20annotations%20and%20insufficient%20use%20of%20tissue%20context.%20We%20present%20Tissue-Aware%20Nuclei%20Detection%20%28TAND%29%2C%20a%20novel%20framework%20achieving%20joint%20nuclei%20detection%20and%20classification%20using%20point-level%20supervision%20enhanced%20by%20tissue%20mask%20conditioning.%20TAND%20couples%20a%20ConvNeXt-based%20encoder-decoder%20with%20a%20frozen%20Virchow-2%20tissue%20segmentation%20branch%2C%20where%20semantic%20tissue%20probabilities%20selectively%20modulate%20the%20classification%20stream%20through%20a%20novel%20multi-scale%20Spatial%20Feature-wise%20Linear%20Modulation%20%28Spatial-FiLM%29.%20On%20the%20PUMA%20benchmark%2C%20TAND%20achieves%20state-of-the-art%20performance%2C%20surpassing%20both%20tissue-agnostic%20baselines%20and%20mask-supervised%20methods.%20Notably%2C%20our%20approach%20demonstrates%20remarkable%20improvements%20in%20tissue-dependent%20cell%20types%20such%20as%20epithelium%2C%20endothelium%2C%20and%20stroma.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20method%20to%20condition%20per-cell%20classification%20on%20learned%20tissue%20masks%2C%20offering%20a%20practical%20pathway%20to%20reduce%20annotation%20burden.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTissue%2520Aware%2520Nuclei%2520Detection%2520and%2520Classification%2520Model%2520for%2520Histopathology%2520Images%26entry.906535625%3DKesi%2520Xu%2520and%2520Eleni%2520Chiou%2520and%2520Ali%2520Varamesh%2520and%2520Laura%2520Acqualagna%2520and%2520Nasir%2520Rajpoot%26entry.1292438233%3DAccurate%2520nuclei%2520detection%2520and%2520classification%2520are%2520fundamental%2520to%2520computational%2520pathology%252C%2520yet%2520existing%2520approaches%2520are%2520hindered%2520by%2520reliance%2520on%2520detailed%2520expert%2520annotations%2520and%2520insufficient%2520use%2520of%2520tissue%2520context.%2520We%2520present%2520Tissue-Aware%2520Nuclei%2520Detection%2520%2528TAND%2529%252C%2520a%2520novel%2520framework%2520achieving%2520joint%2520nuclei%2520detection%2520and%2520classification%2520using%2520point-level%2520supervision%2520enhanced%2520by%2520tissue%2520mask%2520conditioning.%2520TAND%2520couples%2520a%2520ConvNeXt-based%2520encoder-decoder%2520with%2520a%2520frozen%2520Virchow-2%2520tissue%2520segmentation%2520branch%252C%2520where%2520semantic%2520tissue%2520probabilities%2520selectively%2520modulate%2520the%2520classification%2520stream%2520through%2520a%2520novel%2520multi-scale%2520Spatial%2520Feature-wise%2520Linear%2520Modulation%2520%2528Spatial-FiLM%2529.%2520On%2520the%2520PUMA%2520benchmark%252C%2520TAND%2520achieves%2520state-of-the-art%2520performance%252C%2520surpassing%2520both%2520tissue-agnostic%2520baselines%2520and%2520mask-supervised%2520methods.%2520Notably%252C%2520our%2520approach%2520demonstrates%2520remarkable%2520improvements%2520in%2520tissue-dependent%2520cell%2520types%2520such%2520as%2520epithelium%252C%2520endothelium%252C%2520and%2520stroma.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520method%2520to%2520condition%2520per-cell%2520classification%2520on%2520learned%2520tissue%2520masks%252C%2520offering%2520a%2520practical%2520pathway%2520to%2520reduce%2520annotation%2520burden.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tissue%20Aware%20Nuclei%20Detection%20and%20Classification%20Model%20for%20Histopathology%20Images&entry.906535625=Kesi%20Xu%20and%20Eleni%20Chiou%20and%20Ali%20Varamesh%20and%20Laura%20Acqualagna%20and%20Nasir%20Rajpoot&entry.1292438233=Accurate%20nuclei%20detection%20and%20classification%20are%20fundamental%20to%20computational%20pathology%2C%20yet%20existing%20approaches%20are%20hindered%20by%20reliance%20on%20detailed%20expert%20annotations%20and%20insufficient%20use%20of%20tissue%20context.%20We%20present%20Tissue-Aware%20Nuclei%20Detection%20%28TAND%29%2C%20a%20novel%20framework%20achieving%20joint%20nuclei%20detection%20and%20classification%20using%20point-level%20supervision%20enhanced%20by%20tissue%20mask%20conditioning.%20TAND%20couples%20a%20ConvNeXt-based%20encoder-decoder%20with%20a%20frozen%20Virchow-2%20tissue%20segmentation%20branch%2C%20where%20semantic%20tissue%20probabilities%20selectively%20modulate%20the%20classification%20stream%20through%20a%20novel%20multi-scale%20Spatial%20Feature-wise%20Linear%20Modulation%20%28Spatial-FiLM%29.%20On%20the%20PUMA%20benchmark%2C%20TAND%20achieves%20state-of-the-art%20performance%2C%20surpassing%20both%20tissue-agnostic%20baselines%20and%20mask-supervised%20methods.%20Notably%2C%20our%20approach%20demonstrates%20remarkable%20improvements%20in%20tissue-dependent%20cell%20types%20such%20as%20epithelium%2C%20endothelium%2C%20and%20stroma.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20method%20to%20condition%20per-cell%20classification%20on%20learned%20tissue%20masks%2C%20offering%20a%20practical%20pathway%20to%20reduce%20annotation%20burden.&entry.1838667208=http%3A//arxiv.org/abs/2511.13615v1&entry.124074799=Read"},
{"title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs", "author": "Yuxiang Zhang and Zhengxu Yu and Weihang Pan and Zhongming Jin and Qiang Fu and Deng Cai and Binbin Lin and Jieping Ye", "abstract": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.", "link": "http://arxiv.org/abs/2511.13223v1", "date": "2025-11-17", "relevancy": 2.4958, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenSqueeze%3A%20Performance-Preserving%20Compression%20for%20Reasoning%20LLMs&body=Title%3A%20TokenSqueeze%3A%20Performance-Preserving%20Compression%20for%20Reasoning%20LLMs%0AAuthor%3A%20Yuxiang%20Zhang%20and%20Zhengxu%20Yu%20and%20Weihang%20Pan%20and%20Zhongming%20Jin%20and%20Qiang%20Fu%20and%20Deng%20Cai%20and%20Binbin%20Lin%20and%20Jieping%20Ye%0AAbstract%3A%20Emerging%20reasoning%20LLMs%20such%20as%20OpenAI-o1%20and%20DeepSeek-R1%20have%20achieved%20strong%20performance%20on%20complex%20reasoning%20tasks%20by%20generating%20long%20chain-of-thought%20%28CoT%29%20traces.%20However%2C%20these%20long%20CoTs%20result%20in%20increased%20token%20usage%2C%20leading%20to%20higher%20inference%20latency%20and%20memory%20consumption.%20As%20a%20result%2C%20balancing%20accuracy%20and%20reasoning%20efficiency%20has%20become%20essential%20for%20deploying%20reasoning%20LLMs%20in%20practical%20applications.%20Existing%20long-to-short%20%28Long2Short%29%20methods%20aim%20to%20reduce%20inference%20length%20but%20often%20sacrifice%20accuracy%2C%20revealing%20a%20need%20for%20an%20approach%20that%20maintains%20performance%20while%20lowering%20token%20costs.%20To%20address%20this%20efficiency-accuracy%20tradeoff%2C%20we%20propose%20TokenSqueeze%2C%20a%20novel%20Long2Short%20method%20that%20condenses%20reasoning%20paths%20while%20preserving%20performance%20and%20relying%20exclusively%20on%20self-generated%20data.%20First%2C%20to%20prevent%20performance%20degradation%20caused%20by%20excessive%20compression%20of%20reasoning%20depth%2C%20we%20propose%20to%20select%20self-generated%20samples%20whose%20reasoning%20depth%20is%20adaptively%20matched%20to%20the%20complexity%20of%20the%20problem.%20To%20further%20optimize%20the%20linguistic%20expression%20without%20altering%20the%20underlying%20reasoning%20paths%2C%20we%20introduce%20a%20distribution-aligned%20linguistic%20refinement%20method%20that%20enhances%20the%20clarity%20and%20conciseness%20of%20the%20reasoning%20path%20while%20preserving%20its%20logical%20integrity.%20Comprehensive%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20TokenSqueeze%20in%20reducing%20token%20usage%20while%20maintaining%20accuracy.%20Notably%2C%20DeepSeek-R1-Distill-Qwen-7B%20fine-tuned%20using%20our%20proposed%20method%20achieved%20a%2050%5C%25%20average%20token%20reduction%20while%20preserving%20accuracy%20on%20the%20MATH500%20benchmark.%20TokenSqueeze%20exclusively%20utilizes%20the%20model%27s%20self-generated%20data%2C%20enabling%20efficient%20and%20high-fidelity%20reasoning%20without%20relying%20on%20manually%20curated%20short-answer%20datasets%20across%20diverse%20applications.%20Our%20code%20is%20available%20at%20https%3A//github.com/zhangyx1122/TokenSqueeze.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenSqueeze%253A%2520Performance-Preserving%2520Compression%2520for%2520Reasoning%2520LLMs%26entry.906535625%3DYuxiang%2520Zhang%2520and%2520Zhengxu%2520Yu%2520and%2520Weihang%2520Pan%2520and%2520Zhongming%2520Jin%2520and%2520Qiang%2520Fu%2520and%2520Deng%2520Cai%2520and%2520Binbin%2520Lin%2520and%2520Jieping%2520Ye%26entry.1292438233%3DEmerging%2520reasoning%2520LLMs%2520such%2520as%2520OpenAI-o1%2520and%2520DeepSeek-R1%2520have%2520achieved%2520strong%2520performance%2520on%2520complex%2520reasoning%2520tasks%2520by%2520generating%2520long%2520chain-of-thought%2520%2528CoT%2529%2520traces.%2520However%252C%2520these%2520long%2520CoTs%2520result%2520in%2520increased%2520token%2520usage%252C%2520leading%2520to%2520higher%2520inference%2520latency%2520and%2520memory%2520consumption.%2520As%2520a%2520result%252C%2520balancing%2520accuracy%2520and%2520reasoning%2520efficiency%2520has%2520become%2520essential%2520for%2520deploying%2520reasoning%2520LLMs%2520in%2520practical%2520applications.%2520Existing%2520long-to-short%2520%2528Long2Short%2529%2520methods%2520aim%2520to%2520reduce%2520inference%2520length%2520but%2520often%2520sacrifice%2520accuracy%252C%2520revealing%2520a%2520need%2520for%2520an%2520approach%2520that%2520maintains%2520performance%2520while%2520lowering%2520token%2520costs.%2520To%2520address%2520this%2520efficiency-accuracy%2520tradeoff%252C%2520we%2520propose%2520TokenSqueeze%252C%2520a%2520novel%2520Long2Short%2520method%2520that%2520condenses%2520reasoning%2520paths%2520while%2520preserving%2520performance%2520and%2520relying%2520exclusively%2520on%2520self-generated%2520data.%2520First%252C%2520to%2520prevent%2520performance%2520degradation%2520caused%2520by%2520excessive%2520compression%2520of%2520reasoning%2520depth%252C%2520we%2520propose%2520to%2520select%2520self-generated%2520samples%2520whose%2520reasoning%2520depth%2520is%2520adaptively%2520matched%2520to%2520the%2520complexity%2520of%2520the%2520problem.%2520To%2520further%2520optimize%2520the%2520linguistic%2520expression%2520without%2520altering%2520the%2520underlying%2520reasoning%2520paths%252C%2520we%2520introduce%2520a%2520distribution-aligned%2520linguistic%2520refinement%2520method%2520that%2520enhances%2520the%2520clarity%2520and%2520conciseness%2520of%2520the%2520reasoning%2520path%2520while%2520preserving%2520its%2520logical%2520integrity.%2520Comprehensive%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520TokenSqueeze%2520in%2520reducing%2520token%2520usage%2520while%2520maintaining%2520accuracy.%2520Notably%252C%2520DeepSeek-R1-Distill-Qwen-7B%2520fine-tuned%2520using%2520our%2520proposed%2520method%2520achieved%2520a%252050%255C%2525%2520average%2520token%2520reduction%2520while%2520preserving%2520accuracy%2520on%2520the%2520MATH500%2520benchmark.%2520TokenSqueeze%2520exclusively%2520utilizes%2520the%2520model%2527s%2520self-generated%2520data%252C%2520enabling%2520efficient%2520and%2520high-fidelity%2520reasoning%2520without%2520relying%2520on%2520manually%2520curated%2520short-answer%2520datasets%2520across%2520diverse%2520applications.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/zhangyx1122/TokenSqueeze.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenSqueeze%3A%20Performance-Preserving%20Compression%20for%20Reasoning%20LLMs&entry.906535625=Yuxiang%20Zhang%20and%20Zhengxu%20Yu%20and%20Weihang%20Pan%20and%20Zhongming%20Jin%20and%20Qiang%20Fu%20and%20Deng%20Cai%20and%20Binbin%20Lin%20and%20Jieping%20Ye&entry.1292438233=Emerging%20reasoning%20LLMs%20such%20as%20OpenAI-o1%20and%20DeepSeek-R1%20have%20achieved%20strong%20performance%20on%20complex%20reasoning%20tasks%20by%20generating%20long%20chain-of-thought%20%28CoT%29%20traces.%20However%2C%20these%20long%20CoTs%20result%20in%20increased%20token%20usage%2C%20leading%20to%20higher%20inference%20latency%20and%20memory%20consumption.%20As%20a%20result%2C%20balancing%20accuracy%20and%20reasoning%20efficiency%20has%20become%20essential%20for%20deploying%20reasoning%20LLMs%20in%20practical%20applications.%20Existing%20long-to-short%20%28Long2Short%29%20methods%20aim%20to%20reduce%20inference%20length%20but%20often%20sacrifice%20accuracy%2C%20revealing%20a%20need%20for%20an%20approach%20that%20maintains%20performance%20while%20lowering%20token%20costs.%20To%20address%20this%20efficiency-accuracy%20tradeoff%2C%20we%20propose%20TokenSqueeze%2C%20a%20novel%20Long2Short%20method%20that%20condenses%20reasoning%20paths%20while%20preserving%20performance%20and%20relying%20exclusively%20on%20self-generated%20data.%20First%2C%20to%20prevent%20performance%20degradation%20caused%20by%20excessive%20compression%20of%20reasoning%20depth%2C%20we%20propose%20to%20select%20self-generated%20samples%20whose%20reasoning%20depth%20is%20adaptively%20matched%20to%20the%20complexity%20of%20the%20problem.%20To%20further%20optimize%20the%20linguistic%20expression%20without%20altering%20the%20underlying%20reasoning%20paths%2C%20we%20introduce%20a%20distribution-aligned%20linguistic%20refinement%20method%20that%20enhances%20the%20clarity%20and%20conciseness%20of%20the%20reasoning%20path%20while%20preserving%20its%20logical%20integrity.%20Comprehensive%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20TokenSqueeze%20in%20reducing%20token%20usage%20while%20maintaining%20accuracy.%20Notably%2C%20DeepSeek-R1-Distill-Qwen-7B%20fine-tuned%20using%20our%20proposed%20method%20achieved%20a%2050%5C%25%20average%20token%20reduction%20while%20preserving%20accuracy%20on%20the%20MATH500%20benchmark.%20TokenSqueeze%20exclusively%20utilizes%20the%20model%27s%20self-generated%20data%2C%20enabling%20efficient%20and%20high-fidelity%20reasoning%20without%20relying%20on%20manually%20curated%20short-answer%20datasets%20across%20diverse%20applications.%20Our%20code%20is%20available%20at%20https%3A//github.com/zhangyx1122/TokenSqueeze.&entry.1838667208=http%3A//arxiv.org/abs/2511.13223v1&entry.124074799=Read"},
{"title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers", "author": "Disha Varshney and Samarth Garg and Sarthak Tyagi and Deeksha Varshney and Nayan Deep and Asif Ekbal", "abstract": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.", "link": "http://arxiv.org/abs/2511.13685v1", "date": "2025-11-17", "relevancy": 2.4928, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protein%20Secondary%20Structure%20Prediction%20Using%203D%20Graphs%20and%20Relation-Aware%20Message%20Passing%20Transformers&body=Title%3A%20Protein%20Secondary%20Structure%20Prediction%20Using%203D%20Graphs%20and%20Relation-Aware%20Message%20Passing%20Transformers%0AAuthor%3A%20Disha%20Varshney%20and%20Samarth%20Garg%20and%20Sarthak%20Tyagi%20and%20Deeksha%20Varshney%20and%20Nayan%20Deep%20and%20Asif%20Ekbal%0AAbstract%3A%20In%20this%20study%2C%20we%20tackle%20the%20challenging%20task%20of%20predicting%20secondary%20structures%20from%20protein%20primary%20sequences%2C%20a%20pivotal%20initial%20stride%20towards%20predicting%20tertiary%20structures%2C%20while%20yielding%20crucial%20insights%20into%20protein%20activity%2C%20relationships%2C%20and%20functions.%20Existing%20methods%20often%20utilize%20extensive%20sets%20of%20unlabeled%20amino%20acid%20sequences.%20However%2C%20these%20approaches%20neither%20explicitly%20capture%20nor%20harness%20the%20accessible%20protein%203D%20structural%20data%2C%20which%20is%20recognized%20as%20a%20decisive%20factor%20in%20dictating%20protein%20functions.%20To%20address%20this%2C%20we%20utilize%20protein%20residue%20graphs%20and%20introduce%20various%20forms%20of%20sequential%20or%20structural%20connections%20to%20capture%20enhanced%20spatial%20information.%20We%20adeptly%20combine%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20Language%20Models%20%28LMs%29%2C%20specifically%20utilizing%20a%20pre-trained%20transformer-based%20protein%20language%20model%20to%20encode%20amino%20acid%20sequences%20and%20employing%20message-passing%20mechanisms%20like%20GCN%20and%20R-GCN%20to%20capture%20geometric%20characteristics%20of%20protein%20structures.%20Employing%20convolution%20within%20a%20specific%20node%27s%20nearby%20region%2C%20including%20relations%2C%20we%20stack%20multiple%20convolutional%20layers%20to%20efficiently%20learn%20combined%20insights%20from%20the%20protein%27s%20spatial%20graph%2C%20revealing%20intricate%20interconnections%20and%20dependencies%20in%20its%20structural%20arrangement.%20To%20assess%20our%20model%27s%20performance%2C%20we%20employed%20the%20training%20dataset%20provided%20by%20NetSurfP-2.0%2C%20which%20outlines%20secondary%20structure%20in%203-and%208-states.%20Extensive%20experiments%20show%20that%20our%20proposed%20model%2C%20SSRGNet%20surpasses%20the%20baseline%20on%20f1-scores.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtein%2520Secondary%2520Structure%2520Prediction%2520Using%25203D%2520Graphs%2520and%2520Relation-Aware%2520Message%2520Passing%2520Transformers%26entry.906535625%3DDisha%2520Varshney%2520and%2520Samarth%2520Garg%2520and%2520Sarthak%2520Tyagi%2520and%2520Deeksha%2520Varshney%2520and%2520Nayan%2520Deep%2520and%2520Asif%2520Ekbal%26entry.1292438233%3DIn%2520this%2520study%252C%2520we%2520tackle%2520the%2520challenging%2520task%2520of%2520predicting%2520secondary%2520structures%2520from%2520protein%2520primary%2520sequences%252C%2520a%2520pivotal%2520initial%2520stride%2520towards%2520predicting%2520tertiary%2520structures%252C%2520while%2520yielding%2520crucial%2520insights%2520into%2520protein%2520activity%252C%2520relationships%252C%2520and%2520functions.%2520Existing%2520methods%2520often%2520utilize%2520extensive%2520sets%2520of%2520unlabeled%2520amino%2520acid%2520sequences.%2520However%252C%2520these%2520approaches%2520neither%2520explicitly%2520capture%2520nor%2520harness%2520the%2520accessible%2520protein%25203D%2520structural%2520data%252C%2520which%2520is%2520recognized%2520as%2520a%2520decisive%2520factor%2520in%2520dictating%2520protein%2520functions.%2520To%2520address%2520this%252C%2520we%2520utilize%2520protein%2520residue%2520graphs%2520and%2520introduce%2520various%2520forms%2520of%2520sequential%2520or%2520structural%2520connections%2520to%2520capture%2520enhanced%2520spatial%2520information.%2520We%2520adeptly%2520combine%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520and%2520Language%2520Models%2520%2528LMs%2529%252C%2520specifically%2520utilizing%2520a%2520pre-trained%2520transformer-based%2520protein%2520language%2520model%2520to%2520encode%2520amino%2520acid%2520sequences%2520and%2520employing%2520message-passing%2520mechanisms%2520like%2520GCN%2520and%2520R-GCN%2520to%2520capture%2520geometric%2520characteristics%2520of%2520protein%2520structures.%2520Employing%2520convolution%2520within%2520a%2520specific%2520node%2527s%2520nearby%2520region%252C%2520including%2520relations%252C%2520we%2520stack%2520multiple%2520convolutional%2520layers%2520to%2520efficiently%2520learn%2520combined%2520insights%2520from%2520the%2520protein%2527s%2520spatial%2520graph%252C%2520revealing%2520intricate%2520interconnections%2520and%2520dependencies%2520in%2520its%2520structural%2520arrangement.%2520To%2520assess%2520our%2520model%2527s%2520performance%252C%2520we%2520employed%2520the%2520training%2520dataset%2520provided%2520by%2520NetSurfP-2.0%252C%2520which%2520outlines%2520secondary%2520structure%2520in%25203-and%25208-states.%2520Extensive%2520experiments%2520show%2520that%2520our%2520proposed%2520model%252C%2520SSRGNet%2520surpasses%2520the%2520baseline%2520on%2520f1-scores.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protein%20Secondary%20Structure%20Prediction%20Using%203D%20Graphs%20and%20Relation-Aware%20Message%20Passing%20Transformers&entry.906535625=Disha%20Varshney%20and%20Samarth%20Garg%20and%20Sarthak%20Tyagi%20and%20Deeksha%20Varshney%20and%20Nayan%20Deep%20and%20Asif%20Ekbal&entry.1292438233=In%20this%20study%2C%20we%20tackle%20the%20challenging%20task%20of%20predicting%20secondary%20structures%20from%20protein%20primary%20sequences%2C%20a%20pivotal%20initial%20stride%20towards%20predicting%20tertiary%20structures%2C%20while%20yielding%20crucial%20insights%20into%20protein%20activity%2C%20relationships%2C%20and%20functions.%20Existing%20methods%20often%20utilize%20extensive%20sets%20of%20unlabeled%20amino%20acid%20sequences.%20However%2C%20these%20approaches%20neither%20explicitly%20capture%20nor%20harness%20the%20accessible%20protein%203D%20structural%20data%2C%20which%20is%20recognized%20as%20a%20decisive%20factor%20in%20dictating%20protein%20functions.%20To%20address%20this%2C%20we%20utilize%20protein%20residue%20graphs%20and%20introduce%20various%20forms%20of%20sequential%20or%20structural%20connections%20to%20capture%20enhanced%20spatial%20information.%20We%20adeptly%20combine%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20Language%20Models%20%28LMs%29%2C%20specifically%20utilizing%20a%20pre-trained%20transformer-based%20protein%20language%20model%20to%20encode%20amino%20acid%20sequences%20and%20employing%20message-passing%20mechanisms%20like%20GCN%20and%20R-GCN%20to%20capture%20geometric%20characteristics%20of%20protein%20structures.%20Employing%20convolution%20within%20a%20specific%20node%27s%20nearby%20region%2C%20including%20relations%2C%20we%20stack%20multiple%20convolutional%20layers%20to%20efficiently%20learn%20combined%20insights%20from%20the%20protein%27s%20spatial%20graph%2C%20revealing%20intricate%20interconnections%20and%20dependencies%20in%20its%20structural%20arrangement.%20To%20assess%20our%20model%27s%20performance%2C%20we%20employed%20the%20training%20dataset%20provided%20by%20NetSurfP-2.0%2C%20which%20outlines%20secondary%20structure%20in%203-and%208-states.%20Extensive%20experiments%20show%20that%20our%20proposed%20model%2C%20SSRGNet%20surpasses%20the%20baseline%20on%20f1-scores.&entry.1838667208=http%3A//arxiv.org/abs/2511.13685v1&entry.124074799=Read"},
{"title": "Quantum Machine Learning via Contrastive Training", "author": "Liudmila A. Zhukas and Vivian Ni Zhang and Qiang Miao and Qingfeng Wang and Marko Cetina and Jungsang Kim and Lawrence Carin and Christopher Monroe", "abstract": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.", "link": "http://arxiv.org/abs/2511.13497v1", "date": "2025-11-17", "relevancy": 2.472, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4936}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Machine%20Learning%20via%20Contrastive%20Training&body=Title%3A%20Quantum%20Machine%20Learning%20via%20Contrastive%20Training%0AAuthor%3A%20Liudmila%20A.%20Zhukas%20and%20Vivian%20Ni%20Zhang%20and%20Qiang%20Miao%20and%20Qingfeng%20Wang%20and%20Marko%20Cetina%20and%20Jungsang%20Kim%20and%20Lawrence%20Carin%20and%20Christopher%20Monroe%0AAbstract%3A%20Quantum%20machine%20learning%20%28QML%29%20has%20attracted%20growing%20interest%20with%20the%20rapid%20parallel%20advances%20in%20large-scale%20classical%20machine%20learning%20and%20quantum%20technologies.%20Similar%20to%20classical%20machine%20learning%2C%20QML%20models%20also%20face%20challenges%20arising%20from%20the%20scarcity%20of%20labeled%20data%2C%20particularly%20as%20their%20scale%20and%20complexity%20increase.%20Here%2C%20we%20introduce%20self-supervised%20pretraining%20of%20quantum%20representations%20that%20reduces%20reliance%20on%20labeled%20data%20by%20learning%20invariances%20from%20unlabeled%20examples.%20We%20implement%20this%20paradigm%20on%20a%20programmable%20trapped-ion%20quantum%20computer%2C%20encoding%20images%20as%20quantum%20states.%20In%20situ%20contrastive%20pretraining%20on%20hardware%20yields%20a%20representation%20that%2C%20when%20fine-tuned%2C%20classifies%20image%20families%20with%20higher%20mean%20test%20accuracy%20and%20lower%20run-to-run%20variability%20than%20models%20trained%20from%20random%20initialization.%20Performance%20improvement%20is%20especially%20significant%20in%20regimes%20with%20limited%20labeled%20training%20data.%20We%20show%20that%20the%20learned%20invariances%20generalize%20beyond%20the%20pretraining%20image%20samples.%20Unlike%20prior%20work%2C%20our%20pipeline%20derives%20similarity%20from%20measured%20quantum%20overlaps%20and%20executes%20all%20training%20and%20classification%20stages%20on%20hardware.%20These%20results%20establish%20a%20label-efficient%20route%20to%20quantum%20representation%20learning%2C%20with%20direct%20relevance%20to%20quantum-native%20datasets%20and%20a%20clear%20path%20to%20larger%20classical%20inputs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Machine%2520Learning%2520via%2520Contrastive%2520Training%26entry.906535625%3DLiudmila%2520A.%2520Zhukas%2520and%2520Vivian%2520Ni%2520Zhang%2520and%2520Qiang%2520Miao%2520and%2520Qingfeng%2520Wang%2520and%2520Marko%2520Cetina%2520and%2520Jungsang%2520Kim%2520and%2520Lawrence%2520Carin%2520and%2520Christopher%2520Monroe%26entry.1292438233%3DQuantum%2520machine%2520learning%2520%2528QML%2529%2520has%2520attracted%2520growing%2520interest%2520with%2520the%2520rapid%2520parallel%2520advances%2520in%2520large-scale%2520classical%2520machine%2520learning%2520and%2520quantum%2520technologies.%2520Similar%2520to%2520classical%2520machine%2520learning%252C%2520QML%2520models%2520also%2520face%2520challenges%2520arising%2520from%2520the%2520scarcity%2520of%2520labeled%2520data%252C%2520particularly%2520as%2520their%2520scale%2520and%2520complexity%2520increase.%2520Here%252C%2520we%2520introduce%2520self-supervised%2520pretraining%2520of%2520quantum%2520representations%2520that%2520reduces%2520reliance%2520on%2520labeled%2520data%2520by%2520learning%2520invariances%2520from%2520unlabeled%2520examples.%2520We%2520implement%2520this%2520paradigm%2520on%2520a%2520programmable%2520trapped-ion%2520quantum%2520computer%252C%2520encoding%2520images%2520as%2520quantum%2520states.%2520In%2520situ%2520contrastive%2520pretraining%2520on%2520hardware%2520yields%2520a%2520representation%2520that%252C%2520when%2520fine-tuned%252C%2520classifies%2520image%2520families%2520with%2520higher%2520mean%2520test%2520accuracy%2520and%2520lower%2520run-to-run%2520variability%2520than%2520models%2520trained%2520from%2520random%2520initialization.%2520Performance%2520improvement%2520is%2520especially%2520significant%2520in%2520regimes%2520with%2520limited%2520labeled%2520training%2520data.%2520We%2520show%2520that%2520the%2520learned%2520invariances%2520generalize%2520beyond%2520the%2520pretraining%2520image%2520samples.%2520Unlike%2520prior%2520work%252C%2520our%2520pipeline%2520derives%2520similarity%2520from%2520measured%2520quantum%2520overlaps%2520and%2520executes%2520all%2520training%2520and%2520classification%2520stages%2520on%2520hardware.%2520These%2520results%2520establish%2520a%2520label-efficient%2520route%2520to%2520quantum%2520representation%2520learning%252C%2520with%2520direct%2520relevance%2520to%2520quantum-native%2520datasets%2520and%2520a%2520clear%2520path%2520to%2520larger%2520classical%2520inputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Machine%20Learning%20via%20Contrastive%20Training&entry.906535625=Liudmila%20A.%20Zhukas%20and%20Vivian%20Ni%20Zhang%20and%20Qiang%20Miao%20and%20Qingfeng%20Wang%20and%20Marko%20Cetina%20and%20Jungsang%20Kim%20and%20Lawrence%20Carin%20and%20Christopher%20Monroe&entry.1292438233=Quantum%20machine%20learning%20%28QML%29%20has%20attracted%20growing%20interest%20with%20the%20rapid%20parallel%20advances%20in%20large-scale%20classical%20machine%20learning%20and%20quantum%20technologies.%20Similar%20to%20classical%20machine%20learning%2C%20QML%20models%20also%20face%20challenges%20arising%20from%20the%20scarcity%20of%20labeled%20data%2C%20particularly%20as%20their%20scale%20and%20complexity%20increase.%20Here%2C%20we%20introduce%20self-supervised%20pretraining%20of%20quantum%20representations%20that%20reduces%20reliance%20on%20labeled%20data%20by%20learning%20invariances%20from%20unlabeled%20examples.%20We%20implement%20this%20paradigm%20on%20a%20programmable%20trapped-ion%20quantum%20computer%2C%20encoding%20images%20as%20quantum%20states.%20In%20situ%20contrastive%20pretraining%20on%20hardware%20yields%20a%20representation%20that%2C%20when%20fine-tuned%2C%20classifies%20image%20families%20with%20higher%20mean%20test%20accuracy%20and%20lower%20run-to-run%20variability%20than%20models%20trained%20from%20random%20initialization.%20Performance%20improvement%20is%20especially%20significant%20in%20regimes%20with%20limited%20labeled%20training%20data.%20We%20show%20that%20the%20learned%20invariances%20generalize%20beyond%20the%20pretraining%20image%20samples.%20Unlike%20prior%20work%2C%20our%20pipeline%20derives%20similarity%20from%20measured%20quantum%20overlaps%20and%20executes%20all%20training%20and%20classification%20stages%20on%20hardware.%20These%20results%20establish%20a%20label-efficient%20route%20to%20quantum%20representation%20learning%2C%20with%20direct%20relevance%20to%20quantum-native%20datasets%20and%20a%20clear%20path%20to%20larger%20classical%20inputs.&entry.1838667208=http%3A//arxiv.org/abs/2511.13497v1&entry.124074799=Read"},
{"title": "Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries", "author": "Yue Hou and Ruomei Liu and Yingke Su and Junran Wu and Ke Xu", "abstract": "A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.", "link": "http://arxiv.org/abs/2511.13541v1", "date": "2025-11-17", "relevancy": 2.4674, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Out-of-Distribution%20Detection%20via%20Test-Time%20Calibration%20with%20Dual%20Dynamic%20Dictionaries&body=Title%3A%20Graph%20Out-of-Distribution%20Detection%20via%20Test-Time%20Calibration%20with%20Dual%20Dynamic%20Dictionaries%0AAuthor%3A%20Yue%20Hou%20and%20Ruomei%20Liu%20and%20Yingke%20Su%20and%20Junran%20Wu%20and%20Ke%20Xu%0AAbstract%3A%20A%20key%20challenge%20in%20graph%20out-of-distribution%20%28OOD%29%20detection%20lies%20in%20the%20absence%20of%20ground-truth%20OOD%20samples%20during%20training.%20Existing%20methods%20are%20typically%20optimized%20to%20capture%20features%20within%20the%20in-distribution%20%28ID%29%20data%20and%20calculate%20OOD%20scores%2C%20which%20often%20limits%20pre-trained%20models%20from%20representing%20distributional%20boundaries%2C%20leading%20to%20unreliable%20OOD%20detection.%20Moreover%2C%20the%20latent%20structure%20of%20graph%20data%20is%20often%20governed%20by%20multiple%20underlying%20factors%2C%20which%20remains%20less%20explored.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20test-time%20graph%20OOD%20detection%20method%2C%20termed%20BaCa%2C%20that%20calibrates%20OOD%20scores%20using%20dual%20dynamically%20updated%20dictionaries%20without%20requiring%20fine-tuning%20the%20pre-trained%20model.%20Specifically%2C%20BaCa%20estimates%20graphons%20and%20applies%20a%20mix-up%20strategy%20solely%20with%20test%20samples%20to%20generate%20diverse%20boundary-aware%20discriminative%20topologies%2C%20eliminating%20the%20need%20for%20exposing%20auxiliary%20datasets%20as%20outliers.%20We%20construct%20dual%20dynamic%20dictionaries%20via%20priority%20queues%20and%20attention%20mechanisms%20to%20adaptively%20capture%20latent%20ID%20and%20OOD%20representations%2C%20which%20are%20then%20utilized%20for%20boundary-aware%20OOD%20score%20calibration.%20To%20the%20best%20of%20our%20knowledge%2C%20extensive%20experiments%20on%20real-world%20datasets%20show%20that%20BaCa%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20OOD%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Out-of-Distribution%2520Detection%2520via%2520Test-Time%2520Calibration%2520with%2520Dual%2520Dynamic%2520Dictionaries%26entry.906535625%3DYue%2520Hou%2520and%2520Ruomei%2520Liu%2520and%2520Yingke%2520Su%2520and%2520Junran%2520Wu%2520and%2520Ke%2520Xu%26entry.1292438233%3DA%2520key%2520challenge%2520in%2520graph%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520lies%2520in%2520the%2520absence%2520of%2520ground-truth%2520OOD%2520samples%2520during%2520training.%2520Existing%2520methods%2520are%2520typically%2520optimized%2520to%2520capture%2520features%2520within%2520the%2520in-distribution%2520%2528ID%2529%2520data%2520and%2520calculate%2520OOD%2520scores%252C%2520which%2520often%2520limits%2520pre-trained%2520models%2520from%2520representing%2520distributional%2520boundaries%252C%2520leading%2520to%2520unreliable%2520OOD%2520detection.%2520Moreover%252C%2520the%2520latent%2520structure%2520of%2520graph%2520data%2520is%2520often%2520governed%2520by%2520multiple%2520underlying%2520factors%252C%2520which%2520remains%2520less%2520explored.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520test-time%2520graph%2520OOD%2520detection%2520method%252C%2520termed%2520BaCa%252C%2520that%2520calibrates%2520OOD%2520scores%2520using%2520dual%2520dynamically%2520updated%2520dictionaries%2520without%2520requiring%2520fine-tuning%2520the%2520pre-trained%2520model.%2520Specifically%252C%2520BaCa%2520estimates%2520graphons%2520and%2520applies%2520a%2520mix-up%2520strategy%2520solely%2520with%2520test%2520samples%2520to%2520generate%2520diverse%2520boundary-aware%2520discriminative%2520topologies%252C%2520eliminating%2520the%2520need%2520for%2520exposing%2520auxiliary%2520datasets%2520as%2520outliers.%2520We%2520construct%2520dual%2520dynamic%2520dictionaries%2520via%2520priority%2520queues%2520and%2520attention%2520mechanisms%2520to%2520adaptively%2520capture%2520latent%2520ID%2520and%2520OOD%2520representations%252C%2520which%2520are%2520then%2520utilized%2520for%2520boundary-aware%2520OOD%2520score%2520calibration.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520extensive%2520experiments%2520on%2520real-world%2520datasets%2520show%2520that%2520BaCa%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520OOD%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Out-of-Distribution%20Detection%20via%20Test-Time%20Calibration%20with%20Dual%20Dynamic%20Dictionaries&entry.906535625=Yue%20Hou%20and%20Ruomei%20Liu%20and%20Yingke%20Su%20and%20Junran%20Wu%20and%20Ke%20Xu&entry.1292438233=A%20key%20challenge%20in%20graph%20out-of-distribution%20%28OOD%29%20detection%20lies%20in%20the%20absence%20of%20ground-truth%20OOD%20samples%20during%20training.%20Existing%20methods%20are%20typically%20optimized%20to%20capture%20features%20within%20the%20in-distribution%20%28ID%29%20data%20and%20calculate%20OOD%20scores%2C%20which%20often%20limits%20pre-trained%20models%20from%20representing%20distributional%20boundaries%2C%20leading%20to%20unreliable%20OOD%20detection.%20Moreover%2C%20the%20latent%20structure%20of%20graph%20data%20is%20often%20governed%20by%20multiple%20underlying%20factors%2C%20which%20remains%20less%20explored.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20test-time%20graph%20OOD%20detection%20method%2C%20termed%20BaCa%2C%20that%20calibrates%20OOD%20scores%20using%20dual%20dynamically%20updated%20dictionaries%20without%20requiring%20fine-tuning%20the%20pre-trained%20model.%20Specifically%2C%20BaCa%20estimates%20graphons%20and%20applies%20a%20mix-up%20strategy%20solely%20with%20test%20samples%20to%20generate%20diverse%20boundary-aware%20discriminative%20topologies%2C%20eliminating%20the%20need%20for%20exposing%20auxiliary%20datasets%20as%20outliers.%20We%20construct%20dual%20dynamic%20dictionaries%20via%20priority%20queues%20and%20attention%20mechanisms%20to%20adaptively%20capture%20latent%20ID%20and%20OOD%20representations%2C%20which%20are%20then%20utilized%20for%20boundary-aware%20OOD%20score%20calibration.%20To%20the%20best%20of%20our%20knowledge%2C%20extensive%20experiments%20on%20real-world%20datasets%20show%20that%20BaCa%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20OOD%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2511.13541v1&entry.124074799=Read"},
{"title": "Video Spatial Reasoning with Object-Centric 3D Rollout", "author": "Haoran Tang and Meng Cao and Ruyang Liu and Xiaoxi Liang and Linglong Li and Ge Li and Xiaodan Liang", "abstract": "Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).", "link": "http://arxiv.org/abs/2511.13190v1", "date": "2025-11-17", "relevancy": 2.4494, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6179}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Spatial%20Reasoning%20with%20Object-Centric%203D%20Rollout&body=Title%3A%20Video%20Spatial%20Reasoning%20with%20Object-Centric%203D%20Rollout%0AAuthor%3A%20Haoran%20Tang%20and%20Meng%20Cao%20and%20Ruyang%20Liu%20and%20Xiaoxi%20Liang%20and%20Linglong%20Li%20and%20Ge%20Li%20and%20Xiaodan%20Liang%0AAbstract%3A%20Recent%20advances%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20showcased%20remarkable%20capabilities%20in%20vision-language%20understanding.%20However%2C%20enabling%20robust%20video%20spatial%20reasoning-the%20ability%20to%20comprehend%20object%20locations%2C%20orientations%2C%20and%20inter-object%20relationships%20in%20dynamic%203D%20scenes-remains%20a%20key%20unsolved%20challenge.%20Existing%20approaches%20primarily%20rely%20on%20spatially%20grounded%20supervised%20fine-tuning%20or%20reinforcement%20learning%2C%20yet%20we%20observe%20that%20such%20models%20often%20exhibit%20query-locked%20reasoning%2C%20focusing%20narrowly%20on%20objects%20explicitly%20mentioned%20in%20the%20prompt%20while%20ignoring%20critical%20contextual%20cues.%20To%20address%20this%20limitation%2C%20we%20propose%20Object-Centric%203D%20Rollout%20%28OCR%29%2C%20a%20novel%20strategy%20that%20introduces%20structured%20perturbations%20to%20the%203D%20geometry%20of%20selected%20objects%20during%20training.%20By%20degrading%20object-specific%20visual%20cues%20and%20projecting%20the%20altered%20geometry%20into%202D%20space%2C%20OCR%20compels%20the%20model%20to%20reason%20holistically%20across%20the%20entire%20scene.%20We%20further%20design%20a%20rollout-based%20training%20pipeline%20that%20jointly%20leverages%20vanilla%20and%20region-noisy%20videos%20to%20optimize%20spatial%20reasoning%20trajectories.%20Experiments%20demonstrate%20state-of-the-art%20performance%3A%20our%203B-parameter%20model%20achieves%2047.5%25%20accuracy%20on%20VSI-Bench%2C%20outperforming%20several%207B%20baselines.%20Ablations%20confirm%20OCR%27s%20superiority%20over%20prior%20rollout%20strategies%20%28e.g.%2C%20T-GRPO%2C%20NoisyRollout%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Spatial%2520Reasoning%2520with%2520Object-Centric%25203D%2520Rollout%26entry.906535625%3DHaoran%2520Tang%2520and%2520Meng%2520Cao%2520and%2520Ruyang%2520Liu%2520and%2520Xiaoxi%2520Liang%2520and%2520Linglong%2520Li%2520and%2520Ge%2520Li%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DRecent%2520advances%2520in%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520showcased%2520remarkable%2520capabilities%2520in%2520vision-language%2520understanding.%2520However%252C%2520enabling%2520robust%2520video%2520spatial%2520reasoning-the%2520ability%2520to%2520comprehend%2520object%2520locations%252C%2520orientations%252C%2520and%2520inter-object%2520relationships%2520in%2520dynamic%25203D%2520scenes-remains%2520a%2520key%2520unsolved%2520challenge.%2520Existing%2520approaches%2520primarily%2520rely%2520on%2520spatially%2520grounded%2520supervised%2520fine-tuning%2520or%2520reinforcement%2520learning%252C%2520yet%2520we%2520observe%2520that%2520such%2520models%2520often%2520exhibit%2520query-locked%2520reasoning%252C%2520focusing%2520narrowly%2520on%2520objects%2520explicitly%2520mentioned%2520in%2520the%2520prompt%2520while%2520ignoring%2520critical%2520contextual%2520cues.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Object-Centric%25203D%2520Rollout%2520%2528OCR%2529%252C%2520a%2520novel%2520strategy%2520that%2520introduces%2520structured%2520perturbations%2520to%2520the%25203D%2520geometry%2520of%2520selected%2520objects%2520during%2520training.%2520By%2520degrading%2520object-specific%2520visual%2520cues%2520and%2520projecting%2520the%2520altered%2520geometry%2520into%25202D%2520space%252C%2520OCR%2520compels%2520the%2520model%2520to%2520reason%2520holistically%2520across%2520the%2520entire%2520scene.%2520We%2520further%2520design%2520a%2520rollout-based%2520training%2520pipeline%2520that%2520jointly%2520leverages%2520vanilla%2520and%2520region-noisy%2520videos%2520to%2520optimize%2520spatial%2520reasoning%2520trajectories.%2520Experiments%2520demonstrate%2520state-of-the-art%2520performance%253A%2520our%25203B-parameter%2520model%2520achieves%252047.5%2525%2520accuracy%2520on%2520VSI-Bench%252C%2520outperforming%2520several%25207B%2520baselines.%2520Ablations%2520confirm%2520OCR%2527s%2520superiority%2520over%2520prior%2520rollout%2520strategies%2520%2528e.g.%252C%2520T-GRPO%252C%2520NoisyRollout%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Spatial%20Reasoning%20with%20Object-Centric%203D%20Rollout&entry.906535625=Haoran%20Tang%20and%20Meng%20Cao%20and%20Ruyang%20Liu%20and%20Xiaoxi%20Liang%20and%20Linglong%20Li%20and%20Ge%20Li%20and%20Xiaodan%20Liang&entry.1292438233=Recent%20advances%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20showcased%20remarkable%20capabilities%20in%20vision-language%20understanding.%20However%2C%20enabling%20robust%20video%20spatial%20reasoning-the%20ability%20to%20comprehend%20object%20locations%2C%20orientations%2C%20and%20inter-object%20relationships%20in%20dynamic%203D%20scenes-remains%20a%20key%20unsolved%20challenge.%20Existing%20approaches%20primarily%20rely%20on%20spatially%20grounded%20supervised%20fine-tuning%20or%20reinforcement%20learning%2C%20yet%20we%20observe%20that%20such%20models%20often%20exhibit%20query-locked%20reasoning%2C%20focusing%20narrowly%20on%20objects%20explicitly%20mentioned%20in%20the%20prompt%20while%20ignoring%20critical%20contextual%20cues.%20To%20address%20this%20limitation%2C%20we%20propose%20Object-Centric%203D%20Rollout%20%28OCR%29%2C%20a%20novel%20strategy%20that%20introduces%20structured%20perturbations%20to%20the%203D%20geometry%20of%20selected%20objects%20during%20training.%20By%20degrading%20object-specific%20visual%20cues%20and%20projecting%20the%20altered%20geometry%20into%202D%20space%2C%20OCR%20compels%20the%20model%20to%20reason%20holistically%20across%20the%20entire%20scene.%20We%20further%20design%20a%20rollout-based%20training%20pipeline%20that%20jointly%20leverages%20vanilla%20and%20region-noisy%20videos%20to%20optimize%20spatial%20reasoning%20trajectories.%20Experiments%20demonstrate%20state-of-the-art%20performance%3A%20our%203B-parameter%20model%20achieves%2047.5%25%20accuracy%20on%20VSI-Bench%2C%20outperforming%20several%207B%20baselines.%20Ablations%20confirm%20OCR%27s%20superiority%20over%20prior%20rollout%20strategies%20%28e.g.%2C%20T-GRPO%2C%20NoisyRollout%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.13190v1&entry.124074799=Read"},
{"title": "GRIM: Task-Oriented Grasping with Conditioning on Generative Examples", "author": " Shailesh and Alok Raj and Nayan Kumar and Priya Shukla and Andrew Melnik and Michael Beetz and Gora Chand Nandi", "abstract": "Task-Oriented Grasping (TOG) requires robots to select grasps that are functionally appropriate for a specified task - a challenge that demands an understanding of task semantics, object affordances, and functional constraints. We present GRIM (Grasp Re-alignment via Iterative Matching), a training-free framework that addresses these challenges by leveraging Video Generation Models (VGMs) together with a retrieve-align-transfer pipeline. Beyond leveraging VGMs, GRIM can construct a memory of object-task exemplars sourced from web images, human demonstrations, or generative models. The retrieved task-oriented grasp is then transferred and refined by evaluating it against a set of geometrically stable candidate grasps to ensure both functional suitability and physical feasibility. GRIM demonstrates strong generalization and achieves state-of-the-art performance on standard TOG benchmarks. Project website: https://grim-tog.github.io", "link": "http://arxiv.org/abs/2506.15607v2", "date": "2025-11-17", "relevancy": 2.4471, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6873}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5697}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRIM%3A%20Task-Oriented%20Grasping%20with%20Conditioning%20on%20Generative%20Examples&body=Title%3A%20GRIM%3A%20Task-Oriented%20Grasping%20with%20Conditioning%20on%20Generative%20Examples%0AAuthor%3A%20%20Shailesh%20and%20Alok%20Raj%20and%20Nayan%20Kumar%20and%20Priya%20Shukla%20and%20Andrew%20Melnik%20and%20Michael%20Beetz%20and%20Gora%20Chand%20Nandi%0AAbstract%3A%20Task-Oriented%20Grasping%20%28TOG%29%20requires%20robots%20to%20select%20grasps%20that%20are%20functionally%20appropriate%20for%20a%20specified%20task%20-%20a%20challenge%20that%20demands%20an%20understanding%20of%20task%20semantics%2C%20object%20affordances%2C%20and%20functional%20constraints.%20We%20present%20GRIM%20%28Grasp%20Re-alignment%20via%20Iterative%20Matching%29%2C%20a%20training-free%20framework%20that%20addresses%20these%20challenges%20by%20leveraging%20Video%20Generation%20Models%20%28VGMs%29%20together%20with%20a%20retrieve-align-transfer%20pipeline.%20Beyond%20leveraging%20VGMs%2C%20GRIM%20can%20construct%20a%20memory%20of%20object-task%20exemplars%20sourced%20from%20web%20images%2C%20human%20demonstrations%2C%20or%20generative%20models.%20The%20retrieved%20task-oriented%20grasp%20is%20then%20transferred%20and%20refined%20by%20evaluating%20it%20against%20a%20set%20of%20geometrically%20stable%20candidate%20grasps%20to%20ensure%20both%20functional%20suitability%20and%20physical%20feasibility.%20GRIM%20demonstrates%20strong%20generalization%20and%20achieves%20state-of-the-art%20performance%20on%20standard%20TOG%20benchmarks.%20Project%20website%3A%20https%3A//grim-tog.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2506.15607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRIM%253A%2520Task-Oriented%2520Grasping%2520with%2520Conditioning%2520on%2520Generative%2520Examples%26entry.906535625%3D%2520Shailesh%2520and%2520Alok%2520Raj%2520and%2520Nayan%2520Kumar%2520and%2520Priya%2520Shukla%2520and%2520Andrew%2520Melnik%2520and%2520Michael%2520Beetz%2520and%2520Gora%2520Chand%2520Nandi%26entry.1292438233%3DTask-Oriented%2520Grasping%2520%2528TOG%2529%2520requires%2520robots%2520to%2520select%2520grasps%2520that%2520are%2520functionally%2520appropriate%2520for%2520a%2520specified%2520task%2520-%2520a%2520challenge%2520that%2520demands%2520an%2520understanding%2520of%2520task%2520semantics%252C%2520object%2520affordances%252C%2520and%2520functional%2520constraints.%2520We%2520present%2520GRIM%2520%2528Grasp%2520Re-alignment%2520via%2520Iterative%2520Matching%2529%252C%2520a%2520training-free%2520framework%2520that%2520addresses%2520these%2520challenges%2520by%2520leveraging%2520Video%2520Generation%2520Models%2520%2528VGMs%2529%2520together%2520with%2520a%2520retrieve-align-transfer%2520pipeline.%2520Beyond%2520leveraging%2520VGMs%252C%2520GRIM%2520can%2520construct%2520a%2520memory%2520of%2520object-task%2520exemplars%2520sourced%2520from%2520web%2520images%252C%2520human%2520demonstrations%252C%2520or%2520generative%2520models.%2520The%2520retrieved%2520task-oriented%2520grasp%2520is%2520then%2520transferred%2520and%2520refined%2520by%2520evaluating%2520it%2520against%2520a%2520set%2520of%2520geometrically%2520stable%2520candidate%2520grasps%2520to%2520ensure%2520both%2520functional%2520suitability%2520and%2520physical%2520feasibility.%2520GRIM%2520demonstrates%2520strong%2520generalization%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520standard%2520TOG%2520benchmarks.%2520Project%2520website%253A%2520https%253A//grim-tog.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRIM%3A%20Task-Oriented%20Grasping%20with%20Conditioning%20on%20Generative%20Examples&entry.906535625=%20Shailesh%20and%20Alok%20Raj%20and%20Nayan%20Kumar%20and%20Priya%20Shukla%20and%20Andrew%20Melnik%20and%20Michael%20Beetz%20and%20Gora%20Chand%20Nandi&entry.1292438233=Task-Oriented%20Grasping%20%28TOG%29%20requires%20robots%20to%20select%20grasps%20that%20are%20functionally%20appropriate%20for%20a%20specified%20task%20-%20a%20challenge%20that%20demands%20an%20understanding%20of%20task%20semantics%2C%20object%20affordances%2C%20and%20functional%20constraints.%20We%20present%20GRIM%20%28Grasp%20Re-alignment%20via%20Iterative%20Matching%29%2C%20a%20training-free%20framework%20that%20addresses%20these%20challenges%20by%20leveraging%20Video%20Generation%20Models%20%28VGMs%29%20together%20with%20a%20retrieve-align-transfer%20pipeline.%20Beyond%20leveraging%20VGMs%2C%20GRIM%20can%20construct%20a%20memory%20of%20object-task%20exemplars%20sourced%20from%20web%20images%2C%20human%20demonstrations%2C%20or%20generative%20models.%20The%20retrieved%20task-oriented%20grasp%20is%20then%20transferred%20and%20refined%20by%20evaluating%20it%20against%20a%20set%20of%20geometrically%20stable%20candidate%20grasps%20to%20ensure%20both%20functional%20suitability%20and%20physical%20feasibility.%20GRIM%20demonstrates%20strong%20generalization%20and%20achieves%20state-of-the-art%20performance%20on%20standard%20TOG%20benchmarks.%20Project%20website%3A%20https%3A//grim-tog.github.io&entry.1838667208=http%3A//arxiv.org/abs/2506.15607v2&entry.124074799=Read"},
{"title": "Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection", "author": "Soyul Lee and Seungmin Baek and Dongbo Min", "abstract": "Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.", "link": "http://arxiv.org/abs/2511.13195v1", "date": "2025-11-17", "relevancy": 2.4437, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6893}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6336}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Difficulty-Aware%20Label-Guided%20Denoising%20for%20Monocular%203D%20Object%20Detection&body=Title%3A%20Difficulty-Aware%20Label-Guided%20Denoising%20for%20Monocular%203D%20Object%20Detection%0AAuthor%3A%20Soyul%20Lee%20and%20Seungmin%20Baek%20and%20Dongbo%20Min%0AAbstract%3A%20Monocular%203D%20object%20detection%20is%20a%20cost-effective%20solution%20for%20applications%20like%20autonomous%20driving%20and%20robotics%2C%20but%20remains%20fundamentally%20ill-posed%20due%20to%20inherently%20ambiguous%20depth%20cues.%20Recent%20DETR-based%20methods%20attempt%20to%20mitigate%20this%20through%20global%20attention%20and%20auxiliary%20depth%20prediction%2C%20yet%20they%20still%20struggle%20with%20inaccurate%20depth%20estimates.%20Moreover%2C%20these%20methods%20often%20overlook%20instance-level%20detection%20difficulty%2C%20such%20as%20occlusion%2C%20distance%2C%20and%20truncation%2C%20leading%20to%20suboptimal%20detection%20performance.%20We%20propose%20MonoDLGD%2C%20a%20novel%20Difficulty-Aware%20Label-Guided%20Denoising%20framework%20that%20adaptively%20perturbs%20and%20reconstructs%20ground-truth%20labels%20based%20on%20detection%20uncertainty.%20Specifically%2C%20MonoDLGD%20applies%20stronger%20perturbations%20to%20easier%20instances%20and%20weaker%20ones%20into%20harder%20cases%2C%20and%20then%20reconstructs%20them%20to%20effectively%20provide%20explicit%20geometric%20supervision.%20By%20jointly%20optimizing%20label%20reconstruction%20and%203D%20object%20detection%2C%20MonoDLGD%20encourages%20geometry-aware%20representation%20learning%20and%20improves%20robustness%20to%20varying%20levels%20of%20object%20complexity.%20Extensive%20experiments%20on%20the%20KITTI%20benchmark%20demonstrate%20that%20MonoDLGD%20achieves%20state-of-the-art%20performance%20across%20all%20difficulty%20levels.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifficulty-Aware%2520Label-Guided%2520Denoising%2520for%2520Monocular%25203D%2520Object%2520Detection%26entry.906535625%3DSoyul%2520Lee%2520and%2520Seungmin%2520Baek%2520and%2520Dongbo%2520Min%26entry.1292438233%3DMonocular%25203D%2520object%2520detection%2520is%2520a%2520cost-effective%2520solution%2520for%2520applications%2520like%2520autonomous%2520driving%2520and%2520robotics%252C%2520but%2520remains%2520fundamentally%2520ill-posed%2520due%2520to%2520inherently%2520ambiguous%2520depth%2520cues.%2520Recent%2520DETR-based%2520methods%2520attempt%2520to%2520mitigate%2520this%2520through%2520global%2520attention%2520and%2520auxiliary%2520depth%2520prediction%252C%2520yet%2520they%2520still%2520struggle%2520with%2520inaccurate%2520depth%2520estimates.%2520Moreover%252C%2520these%2520methods%2520often%2520overlook%2520instance-level%2520detection%2520difficulty%252C%2520such%2520as%2520occlusion%252C%2520distance%252C%2520and%2520truncation%252C%2520leading%2520to%2520suboptimal%2520detection%2520performance.%2520We%2520propose%2520MonoDLGD%252C%2520a%2520novel%2520Difficulty-Aware%2520Label-Guided%2520Denoising%2520framework%2520that%2520adaptively%2520perturbs%2520and%2520reconstructs%2520ground-truth%2520labels%2520based%2520on%2520detection%2520uncertainty.%2520Specifically%252C%2520MonoDLGD%2520applies%2520stronger%2520perturbations%2520to%2520easier%2520instances%2520and%2520weaker%2520ones%2520into%2520harder%2520cases%252C%2520and%2520then%2520reconstructs%2520them%2520to%2520effectively%2520provide%2520explicit%2520geometric%2520supervision.%2520By%2520jointly%2520optimizing%2520label%2520reconstruction%2520and%25203D%2520object%2520detection%252C%2520MonoDLGD%2520encourages%2520geometry-aware%2520representation%2520learning%2520and%2520improves%2520robustness%2520to%2520varying%2520levels%2520of%2520object%2520complexity.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%2520benchmark%2520demonstrate%2520that%2520MonoDLGD%2520achieves%2520state-of-the-art%2520performance%2520across%2520all%2520difficulty%2520levels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Difficulty-Aware%20Label-Guided%20Denoising%20for%20Monocular%203D%20Object%20Detection&entry.906535625=Soyul%20Lee%20and%20Seungmin%20Baek%20and%20Dongbo%20Min&entry.1292438233=Monocular%203D%20object%20detection%20is%20a%20cost-effective%20solution%20for%20applications%20like%20autonomous%20driving%20and%20robotics%2C%20but%20remains%20fundamentally%20ill-posed%20due%20to%20inherently%20ambiguous%20depth%20cues.%20Recent%20DETR-based%20methods%20attempt%20to%20mitigate%20this%20through%20global%20attention%20and%20auxiliary%20depth%20prediction%2C%20yet%20they%20still%20struggle%20with%20inaccurate%20depth%20estimates.%20Moreover%2C%20these%20methods%20often%20overlook%20instance-level%20detection%20difficulty%2C%20such%20as%20occlusion%2C%20distance%2C%20and%20truncation%2C%20leading%20to%20suboptimal%20detection%20performance.%20We%20propose%20MonoDLGD%2C%20a%20novel%20Difficulty-Aware%20Label-Guided%20Denoising%20framework%20that%20adaptively%20perturbs%20and%20reconstructs%20ground-truth%20labels%20based%20on%20detection%20uncertainty.%20Specifically%2C%20MonoDLGD%20applies%20stronger%20perturbations%20to%20easier%20instances%20and%20weaker%20ones%20into%20harder%20cases%2C%20and%20then%20reconstructs%20them%20to%20effectively%20provide%20explicit%20geometric%20supervision.%20By%20jointly%20optimizing%20label%20reconstruction%20and%203D%20object%20detection%2C%20MonoDLGD%20encourages%20geometry-aware%20representation%20learning%20and%20improves%20robustness%20to%20varying%20levels%20of%20object%20complexity.%20Extensive%20experiments%20on%20the%20KITTI%20benchmark%20demonstrate%20that%20MonoDLGD%20achieves%20state-of-the-art%20performance%20across%20all%20difficulty%20levels.&entry.1838667208=http%3A//arxiv.org/abs/2511.13195v1&entry.124074799=Read"},
{"title": "Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology", "author": "Matthias Perkonigg and Patrick Rockenschaub and Georg G\u00f6bel and Adelheid W\u00f6hrer", "abstract": "Accurate brain tumor classification is critical for intra-operative decision making in neuro-oncological surgery. However, existing approaches are restricted to a fixed set of predefined classes and are therefore unable to capture patterns of tumor types not available during training. Unsupervised learning can extract general-purpose features, but it lacks the ability to incorporate prior knowledge from labelled data, and semi-supervised methods often assume that all potential classes are represented in the labelled data. Generalized Category Discovery (GCD) aims to bridge this gap by categorizing both known and unknown classes within unlabelled data. To reflect the hierarchical structure of brain tumor taxonomies, in this work, we introduce Hierarchical Generalized Category Discovery for Brain Tumor Classification (HGCD-BT), a novel approach that integrates hierarchical clustering with contrastive learning. Our method extends contrastive learning based GCD by incorporating a novel semi-supervised hierarchical clustering loss. We evaluate HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images, achieving a +28% improvement in accuracy over state-of-the-art GCD methods for patch-level classification, particularly in identifying previously unseen tumor categories. Furthermore, we demonstrate the generalizability of HGCD-BT on slide-level classification of hematoxylin and eosin stained whole-slide images from the Digital Brain Tumor Atlas, confirming its utility across imaging modalities.", "link": "http://arxiv.org/abs/2510.02760v2", "date": "2025-11-17", "relevancy": 2.436, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4954}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Generalized%20Category%20Discovery%20for%20Brain%20Tumor%20Classification%20in%20Digital%20Pathology&body=Title%3A%20Hierarchical%20Generalized%20Category%20Discovery%20for%20Brain%20Tumor%20Classification%20in%20Digital%20Pathology%0AAuthor%3A%20Matthias%20Perkonigg%20and%20Patrick%20Rockenschaub%20and%20Georg%20G%C3%B6bel%20and%20Adelheid%20W%C3%B6hrer%0AAbstract%3A%20Accurate%20brain%20tumor%20classification%20is%20critical%20for%20intra-operative%20decision%20making%20in%20neuro-oncological%20surgery.%20However%2C%20existing%20approaches%20are%20restricted%20to%20a%20fixed%20set%20of%20predefined%20classes%20and%20are%20therefore%20unable%20to%20capture%20patterns%20of%20tumor%20types%20not%20available%20during%20training.%20Unsupervised%20learning%20can%20extract%20general-purpose%20features%2C%20but%20it%20lacks%20the%20ability%20to%20incorporate%20prior%20knowledge%20from%20labelled%20data%2C%20and%20semi-supervised%20methods%20often%20assume%20that%20all%20potential%20classes%20are%20represented%20in%20the%20labelled%20data.%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20bridge%20this%20gap%20by%20categorizing%20both%20known%20and%20unknown%20classes%20within%20unlabelled%20data.%20To%20reflect%20the%20hierarchical%20structure%20of%20brain%20tumor%20taxonomies%2C%20in%20this%20work%2C%20we%20introduce%20Hierarchical%20Generalized%20Category%20Discovery%20for%20Brain%20Tumor%20Classification%20%28HGCD-BT%29%2C%20a%20novel%20approach%20that%20integrates%20hierarchical%20clustering%20with%20contrastive%20learning.%20Our%20method%20extends%20contrastive%20learning%20based%20GCD%20by%20incorporating%20a%20novel%20semi-supervised%20hierarchical%20clustering%20loss.%20We%20evaluate%20HGCD-BT%20on%20OpenSRH%2C%20a%20dataset%20of%20stimulated%20Raman%20histology%20brain%20tumor%20images%2C%20achieving%20a%20%2B28%25%20improvement%20in%20accuracy%20over%20state-of-the-art%20GCD%20methods%20for%20patch-level%20classification%2C%20particularly%20in%20identifying%20previously%20unseen%20tumor%20categories.%20Furthermore%2C%20we%20demonstrate%20the%20generalizability%20of%20HGCD-BT%20on%20slide-level%20classification%20of%20hematoxylin%20and%20eosin%20stained%20whole-slide%20images%20from%20the%20Digital%20Brain%20Tumor%20Atlas%2C%20confirming%20its%20utility%20across%20imaging%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Generalized%2520Category%2520Discovery%2520for%2520Brain%2520Tumor%2520Classification%2520in%2520Digital%2520Pathology%26entry.906535625%3DMatthias%2520Perkonigg%2520and%2520Patrick%2520Rockenschaub%2520and%2520Georg%2520G%25C3%25B6bel%2520and%2520Adelheid%2520W%25C3%25B6hrer%26entry.1292438233%3DAccurate%2520brain%2520tumor%2520classification%2520is%2520critical%2520for%2520intra-operative%2520decision%2520making%2520in%2520neuro-oncological%2520surgery.%2520However%252C%2520existing%2520approaches%2520are%2520restricted%2520to%2520a%2520fixed%2520set%2520of%2520predefined%2520classes%2520and%2520are%2520therefore%2520unable%2520to%2520capture%2520patterns%2520of%2520tumor%2520types%2520not%2520available%2520during%2520training.%2520Unsupervised%2520learning%2520can%2520extract%2520general-purpose%2520features%252C%2520but%2520it%2520lacks%2520the%2520ability%2520to%2520incorporate%2520prior%2520knowledge%2520from%2520labelled%2520data%252C%2520and%2520semi-supervised%2520methods%2520often%2520assume%2520that%2520all%2520potential%2520classes%2520are%2520represented%2520in%2520the%2520labelled%2520data.%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520aims%2520to%2520bridge%2520this%2520gap%2520by%2520categorizing%2520both%2520known%2520and%2520unknown%2520classes%2520within%2520unlabelled%2520data.%2520To%2520reflect%2520the%2520hierarchical%2520structure%2520of%2520brain%2520tumor%2520taxonomies%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520Hierarchical%2520Generalized%2520Category%2520Discovery%2520for%2520Brain%2520Tumor%2520Classification%2520%2528HGCD-BT%2529%252C%2520a%2520novel%2520approach%2520that%2520integrates%2520hierarchical%2520clustering%2520with%2520contrastive%2520learning.%2520Our%2520method%2520extends%2520contrastive%2520learning%2520based%2520GCD%2520by%2520incorporating%2520a%2520novel%2520semi-supervised%2520hierarchical%2520clustering%2520loss.%2520We%2520evaluate%2520HGCD-BT%2520on%2520OpenSRH%252C%2520a%2520dataset%2520of%2520stimulated%2520Raman%2520histology%2520brain%2520tumor%2520images%252C%2520achieving%2520a%2520%252B28%2525%2520improvement%2520in%2520accuracy%2520over%2520state-of-the-art%2520GCD%2520methods%2520for%2520patch-level%2520classification%252C%2520particularly%2520in%2520identifying%2520previously%2520unseen%2520tumor%2520categories.%2520Furthermore%252C%2520we%2520demonstrate%2520the%2520generalizability%2520of%2520HGCD-BT%2520on%2520slide-level%2520classification%2520of%2520hematoxylin%2520and%2520eosin%2520stained%2520whole-slide%2520images%2520from%2520the%2520Digital%2520Brain%2520Tumor%2520Atlas%252C%2520confirming%2520its%2520utility%2520across%2520imaging%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Generalized%20Category%20Discovery%20for%20Brain%20Tumor%20Classification%20in%20Digital%20Pathology&entry.906535625=Matthias%20Perkonigg%20and%20Patrick%20Rockenschaub%20and%20Georg%20G%C3%B6bel%20and%20Adelheid%20W%C3%B6hrer&entry.1292438233=Accurate%20brain%20tumor%20classification%20is%20critical%20for%20intra-operative%20decision%20making%20in%20neuro-oncological%20surgery.%20However%2C%20existing%20approaches%20are%20restricted%20to%20a%20fixed%20set%20of%20predefined%20classes%20and%20are%20therefore%20unable%20to%20capture%20patterns%20of%20tumor%20types%20not%20available%20during%20training.%20Unsupervised%20learning%20can%20extract%20general-purpose%20features%2C%20but%20it%20lacks%20the%20ability%20to%20incorporate%20prior%20knowledge%20from%20labelled%20data%2C%20and%20semi-supervised%20methods%20often%20assume%20that%20all%20potential%20classes%20are%20represented%20in%20the%20labelled%20data.%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20bridge%20this%20gap%20by%20categorizing%20both%20known%20and%20unknown%20classes%20within%20unlabelled%20data.%20To%20reflect%20the%20hierarchical%20structure%20of%20brain%20tumor%20taxonomies%2C%20in%20this%20work%2C%20we%20introduce%20Hierarchical%20Generalized%20Category%20Discovery%20for%20Brain%20Tumor%20Classification%20%28HGCD-BT%29%2C%20a%20novel%20approach%20that%20integrates%20hierarchical%20clustering%20with%20contrastive%20learning.%20Our%20method%20extends%20contrastive%20learning%20based%20GCD%20by%20incorporating%20a%20novel%20semi-supervised%20hierarchical%20clustering%20loss.%20We%20evaluate%20HGCD-BT%20on%20OpenSRH%2C%20a%20dataset%20of%20stimulated%20Raman%20histology%20brain%20tumor%20images%2C%20achieving%20a%20%2B28%25%20improvement%20in%20accuracy%20over%20state-of-the-art%20GCD%20methods%20for%20patch-level%20classification%2C%20particularly%20in%20identifying%20previously%20unseen%20tumor%20categories.%20Furthermore%2C%20we%20demonstrate%20the%20generalizability%20of%20HGCD-BT%20on%20slide-level%20classification%20of%20hematoxylin%20and%20eosin%20stained%20whole-slide%20images%20from%20the%20Digital%20Brain%20Tumor%20Atlas%2C%20confirming%20its%20utility%20across%20imaging%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2510.02760v2&entry.124074799=Read"},
{"title": "Back to Basics: Let Denoising Generative Models Denoise", "author": "Tianhong Li and Kaiming He", "abstract": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.", "link": "http://arxiv.org/abs/2511.13720v1", "date": "2025-11-17", "relevancy": 2.4295, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6507}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6113}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back%20to%20Basics%3A%20Let%20Denoising%20Generative%20Models%20Denoise&body=Title%3A%20Back%20to%20Basics%3A%20Let%20Denoising%20Generative%20Models%20Denoise%0AAuthor%3A%20Tianhong%20Li%20and%20Kaiming%20He%0AAbstract%3A%20Today%27s%20denoising%20diffusion%20models%20do%20not%20%22denoise%22%20in%20the%20classical%20sense%2C%20i.e.%2C%20they%20do%20not%20directly%20predict%20clean%20images.%20Rather%2C%20the%20neural%20networks%20predict%20noise%20or%20a%20noised%20quantity.%20In%20this%20paper%2C%20we%20suggest%20that%20predicting%20clean%20data%20and%20predicting%20noised%20quantities%20are%20fundamentally%20different.%20According%20to%20the%20manifold%20assumption%2C%20natural%20data%20should%20lie%20on%20a%20low-dimensional%20manifold%2C%20whereas%20noised%20quantities%20do%20not.%20With%20this%20assumption%2C%20we%20advocate%20for%20models%20that%20directly%20predict%20clean%20data%2C%20which%20allows%20apparently%20under-capacity%20networks%20to%20operate%20effectively%20in%20very%20high-dimensional%20spaces.%20We%20show%20that%20simple%2C%20large-patch%20Transformers%20on%20pixels%20can%20be%20strong%20generative%20models%3A%20using%20no%20tokenizer%2C%20no%20pre-training%2C%20and%20no%20extra%20loss.%20Our%20approach%20is%20conceptually%20nothing%20more%20than%20%22%24%5Ctextbf%7BJust%20image%20Transformers%7D%24%22%2C%20or%20%24%5Ctextbf%7BJiT%7D%24%2C%20as%20we%20call%20it.%20We%20report%20competitive%20results%20using%20JiT%20with%20large%20patch%20sizes%20of%2016%20and%2032%20on%20ImageNet%20at%20resolutions%20of%20256%20and%20512%2C%20where%20predicting%20high-dimensional%20noised%20quantities%20can%20fail%20catastrophically.%20With%20our%20networks%20mapping%20back%20to%20the%20basics%20of%20the%20manifold%2C%20our%20research%20goes%20back%20to%20basics%20and%20pursues%20a%20self-contained%20paradigm%20for%20Transformer-based%20diffusion%20on%20raw%20natural%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack%2520to%2520Basics%253A%2520Let%2520Denoising%2520Generative%2520Models%2520Denoise%26entry.906535625%3DTianhong%2520Li%2520and%2520Kaiming%2520He%26entry.1292438233%3DToday%2527s%2520denoising%2520diffusion%2520models%2520do%2520not%2520%2522denoise%2522%2520in%2520the%2520classical%2520sense%252C%2520i.e.%252C%2520they%2520do%2520not%2520directly%2520predict%2520clean%2520images.%2520Rather%252C%2520the%2520neural%2520networks%2520predict%2520noise%2520or%2520a%2520noised%2520quantity.%2520In%2520this%2520paper%252C%2520we%2520suggest%2520that%2520predicting%2520clean%2520data%2520and%2520predicting%2520noised%2520quantities%2520are%2520fundamentally%2520different.%2520According%2520to%2520the%2520manifold%2520assumption%252C%2520natural%2520data%2520should%2520lie%2520on%2520a%2520low-dimensional%2520manifold%252C%2520whereas%2520noised%2520quantities%2520do%2520not.%2520With%2520this%2520assumption%252C%2520we%2520advocate%2520for%2520models%2520that%2520directly%2520predict%2520clean%2520data%252C%2520which%2520allows%2520apparently%2520under-capacity%2520networks%2520to%2520operate%2520effectively%2520in%2520very%2520high-dimensional%2520spaces.%2520We%2520show%2520that%2520simple%252C%2520large-patch%2520Transformers%2520on%2520pixels%2520can%2520be%2520strong%2520generative%2520models%253A%2520using%2520no%2520tokenizer%252C%2520no%2520pre-training%252C%2520and%2520no%2520extra%2520loss.%2520Our%2520approach%2520is%2520conceptually%2520nothing%2520more%2520than%2520%2522%2524%255Ctextbf%257BJust%2520image%2520Transformers%257D%2524%2522%252C%2520or%2520%2524%255Ctextbf%257BJiT%257D%2524%252C%2520as%2520we%2520call%2520it.%2520We%2520report%2520competitive%2520results%2520using%2520JiT%2520with%2520large%2520patch%2520sizes%2520of%252016%2520and%252032%2520on%2520ImageNet%2520at%2520resolutions%2520of%2520256%2520and%2520512%252C%2520where%2520predicting%2520high-dimensional%2520noised%2520quantities%2520can%2520fail%2520catastrophically.%2520With%2520our%2520networks%2520mapping%2520back%2520to%2520the%2520basics%2520of%2520the%2520manifold%252C%2520our%2520research%2520goes%2520back%2520to%2520basics%2520and%2520pursues%2520a%2520self-contained%2520paradigm%2520for%2520Transformer-based%2520diffusion%2520on%2520raw%2520natural%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20to%20Basics%3A%20Let%20Denoising%20Generative%20Models%20Denoise&entry.906535625=Tianhong%20Li%20and%20Kaiming%20He&entry.1292438233=Today%27s%20denoising%20diffusion%20models%20do%20not%20%22denoise%22%20in%20the%20classical%20sense%2C%20i.e.%2C%20they%20do%20not%20directly%20predict%20clean%20images.%20Rather%2C%20the%20neural%20networks%20predict%20noise%20or%20a%20noised%20quantity.%20In%20this%20paper%2C%20we%20suggest%20that%20predicting%20clean%20data%20and%20predicting%20noised%20quantities%20are%20fundamentally%20different.%20According%20to%20the%20manifold%20assumption%2C%20natural%20data%20should%20lie%20on%20a%20low-dimensional%20manifold%2C%20whereas%20noised%20quantities%20do%20not.%20With%20this%20assumption%2C%20we%20advocate%20for%20models%20that%20directly%20predict%20clean%20data%2C%20which%20allows%20apparently%20under-capacity%20networks%20to%20operate%20effectively%20in%20very%20high-dimensional%20spaces.%20We%20show%20that%20simple%2C%20large-patch%20Transformers%20on%20pixels%20can%20be%20strong%20generative%20models%3A%20using%20no%20tokenizer%2C%20no%20pre-training%2C%20and%20no%20extra%20loss.%20Our%20approach%20is%20conceptually%20nothing%20more%20than%20%22%24%5Ctextbf%7BJust%20image%20Transformers%7D%24%22%2C%20or%20%24%5Ctextbf%7BJiT%7D%24%2C%20as%20we%20call%20it.%20We%20report%20competitive%20results%20using%20JiT%20with%20large%20patch%20sizes%20of%2016%20and%2032%20on%20ImageNet%20at%20resolutions%20of%20256%20and%20512%2C%20where%20predicting%20high-dimensional%20noised%20quantities%20can%20fail%20catastrophically.%20With%20our%20networks%20mapping%20back%20to%20the%20basics%20of%20the%20manifold%2C%20our%20research%20goes%20back%20to%20basics%20and%20pursues%20a%20self-contained%20paradigm%20for%20Transformer-based%20diffusion%20on%20raw%20natural%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.13720v1&entry.124074799=Read"},
{"title": "S4M: 4-points to Segment Anything", "author": "Adrien Meyer and Lorenzo Arboit and Giuseppe Massimiani and Shih-Min Yin and Didier Mutter and Nicolas Padoy", "abstract": "Purpose: The Segment Anything Model (SAM) promises to ease the annotation bottleneck in medical segmentation, but overlapping anatomy and blurred boundaries make its point prompts ambiguous, leading to cycles of manual refinement to achieve precise masks. Better prompting strategies are needed.\n  Methods: We propose a structured prompting strategy using 4 points as a compact instance-level shape description. We study two 4-point variants: extreme points and the proposed major/minor axis endpoints, inspired by ultrasound measurement practice. SAM cannot fully exploit such structured prompts because it treats all points identically and lacks geometry-aware reasoning. To address this, we introduce S4M (4-points to Segment Anything), which augments SAM to interpret 4 points as relational cues rather than isolated clicks. S4M expands the prompt space with role-specific embeddings and adds an auxiliary \"Canvas\" pretext task that sketches coarse masks directly from prompts, fostering geometry-aware reasoning.\n  Results: Across eight datasets in ultrasound and surgical endoscopy, S4M improves segmentation by +3.42 mIoU over a strong SAM baseline at equal prompt budget. An annotation study with three clinicians further shows that major/minor prompts enable faster annotation.\n  Conclusion: S4M increases performance, reduces annotation effort, and aligns prompting with clinical practice, enabling more scalable dataset development in medical imaging.", "link": "http://arxiv.org/abs/2503.05534v2", "date": "2025-11-17", "relevancy": 2.4141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S4M%3A%204-points%20to%20Segment%20Anything&body=Title%3A%20S4M%3A%204-points%20to%20Segment%20Anything%0AAuthor%3A%20Adrien%20Meyer%20and%20Lorenzo%20Arboit%20and%20Giuseppe%20Massimiani%20and%20Shih-Min%20Yin%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy%0AAbstract%3A%20Purpose%3A%20The%20Segment%20Anything%20Model%20%28SAM%29%20promises%20to%20ease%20the%20annotation%20bottleneck%20in%20medical%20segmentation%2C%20but%20overlapping%20anatomy%20and%20blurred%20boundaries%20make%20its%20point%20prompts%20ambiguous%2C%20leading%20to%20cycles%20of%20manual%20refinement%20to%20achieve%20precise%20masks.%20Better%20prompting%20strategies%20are%20needed.%0A%20%20Methods%3A%20We%20propose%20a%20structured%20prompting%20strategy%20using%204%20points%20as%20a%20compact%20instance-level%20shape%20description.%20We%20study%20two%204-point%20variants%3A%20extreme%20points%20and%20the%20proposed%20major/minor%20axis%20endpoints%2C%20inspired%20by%20ultrasound%20measurement%20practice.%20SAM%20cannot%20fully%20exploit%20such%20structured%20prompts%20because%20it%20treats%20all%20points%20identically%20and%20lacks%20geometry-aware%20reasoning.%20To%20address%20this%2C%20we%20introduce%20S4M%20%284-points%20to%20Segment%20Anything%29%2C%20which%20augments%20SAM%20to%20interpret%204%20points%20as%20relational%20cues%20rather%20than%20isolated%20clicks.%20S4M%20expands%20the%20prompt%20space%20with%20role-specific%20embeddings%20and%20adds%20an%20auxiliary%20%22Canvas%22%20pretext%20task%20that%20sketches%20coarse%20masks%20directly%20from%20prompts%2C%20fostering%20geometry-aware%20reasoning.%0A%20%20Results%3A%20Across%20eight%20datasets%20in%20ultrasound%20and%20surgical%20endoscopy%2C%20S4M%20improves%20segmentation%20by%20%2B3.42%20mIoU%20over%20a%20strong%20SAM%20baseline%20at%20equal%20prompt%20budget.%20An%20annotation%20study%20with%20three%20clinicians%20further%20shows%20that%20major/minor%20prompts%20enable%20faster%20annotation.%0A%20%20Conclusion%3A%20S4M%20increases%20performance%2C%20reduces%20annotation%20effort%2C%20and%20aligns%20prompting%20with%20clinical%20practice%2C%20enabling%20more%20scalable%20dataset%20development%20in%20medical%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2503.05534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS4M%253A%25204-points%2520to%2520Segment%2520Anything%26entry.906535625%3DAdrien%2520Meyer%2520and%2520Lorenzo%2520Arboit%2520and%2520Giuseppe%2520Massimiani%2520and%2520Shih-Min%2520Yin%2520and%2520Didier%2520Mutter%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3DPurpose%253A%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520promises%2520to%2520ease%2520the%2520annotation%2520bottleneck%2520in%2520medical%2520segmentation%252C%2520but%2520overlapping%2520anatomy%2520and%2520blurred%2520boundaries%2520make%2520its%2520point%2520prompts%2520ambiguous%252C%2520leading%2520to%2520cycles%2520of%2520manual%2520refinement%2520to%2520achieve%2520precise%2520masks.%2520Better%2520prompting%2520strategies%2520are%2520needed.%250A%2520%2520Methods%253A%2520We%2520propose%2520a%2520structured%2520prompting%2520strategy%2520using%25204%2520points%2520as%2520a%2520compact%2520instance-level%2520shape%2520description.%2520We%2520study%2520two%25204-point%2520variants%253A%2520extreme%2520points%2520and%2520the%2520proposed%2520major/minor%2520axis%2520endpoints%252C%2520inspired%2520by%2520ultrasound%2520measurement%2520practice.%2520SAM%2520cannot%2520fully%2520exploit%2520such%2520structured%2520prompts%2520because%2520it%2520treats%2520all%2520points%2520identically%2520and%2520lacks%2520geometry-aware%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520S4M%2520%25284-points%2520to%2520Segment%2520Anything%2529%252C%2520which%2520augments%2520SAM%2520to%2520interpret%25204%2520points%2520as%2520relational%2520cues%2520rather%2520than%2520isolated%2520clicks.%2520S4M%2520expands%2520the%2520prompt%2520space%2520with%2520role-specific%2520embeddings%2520and%2520adds%2520an%2520auxiliary%2520%2522Canvas%2522%2520pretext%2520task%2520that%2520sketches%2520coarse%2520masks%2520directly%2520from%2520prompts%252C%2520fostering%2520geometry-aware%2520reasoning.%250A%2520%2520Results%253A%2520Across%2520eight%2520datasets%2520in%2520ultrasound%2520and%2520surgical%2520endoscopy%252C%2520S4M%2520improves%2520segmentation%2520by%2520%252B3.42%2520mIoU%2520over%2520a%2520strong%2520SAM%2520baseline%2520at%2520equal%2520prompt%2520budget.%2520An%2520annotation%2520study%2520with%2520three%2520clinicians%2520further%2520shows%2520that%2520major/minor%2520prompts%2520enable%2520faster%2520annotation.%250A%2520%2520Conclusion%253A%2520S4M%2520increases%2520performance%252C%2520reduces%2520annotation%2520effort%252C%2520and%2520aligns%2520prompting%2520with%2520clinical%2520practice%252C%2520enabling%2520more%2520scalable%2520dataset%2520development%2520in%2520medical%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S4M%3A%204-points%20to%20Segment%20Anything&entry.906535625=Adrien%20Meyer%20and%20Lorenzo%20Arboit%20and%20Giuseppe%20Massimiani%20and%20Shih-Min%20Yin%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy&entry.1292438233=Purpose%3A%20The%20Segment%20Anything%20Model%20%28SAM%29%20promises%20to%20ease%20the%20annotation%20bottleneck%20in%20medical%20segmentation%2C%20but%20overlapping%20anatomy%20and%20blurred%20boundaries%20make%20its%20point%20prompts%20ambiguous%2C%20leading%20to%20cycles%20of%20manual%20refinement%20to%20achieve%20precise%20masks.%20Better%20prompting%20strategies%20are%20needed.%0A%20%20Methods%3A%20We%20propose%20a%20structured%20prompting%20strategy%20using%204%20points%20as%20a%20compact%20instance-level%20shape%20description.%20We%20study%20two%204-point%20variants%3A%20extreme%20points%20and%20the%20proposed%20major/minor%20axis%20endpoints%2C%20inspired%20by%20ultrasound%20measurement%20practice.%20SAM%20cannot%20fully%20exploit%20such%20structured%20prompts%20because%20it%20treats%20all%20points%20identically%20and%20lacks%20geometry-aware%20reasoning.%20To%20address%20this%2C%20we%20introduce%20S4M%20%284-points%20to%20Segment%20Anything%29%2C%20which%20augments%20SAM%20to%20interpret%204%20points%20as%20relational%20cues%20rather%20than%20isolated%20clicks.%20S4M%20expands%20the%20prompt%20space%20with%20role-specific%20embeddings%20and%20adds%20an%20auxiliary%20%22Canvas%22%20pretext%20task%20that%20sketches%20coarse%20masks%20directly%20from%20prompts%2C%20fostering%20geometry-aware%20reasoning.%0A%20%20Results%3A%20Across%20eight%20datasets%20in%20ultrasound%20and%20surgical%20endoscopy%2C%20S4M%20improves%20segmentation%20by%20%2B3.42%20mIoU%20over%20a%20strong%20SAM%20baseline%20at%20equal%20prompt%20budget.%20An%20annotation%20study%20with%20three%20clinicians%20further%20shows%20that%20major/minor%20prompts%20enable%20faster%20annotation.%0A%20%20Conclusion%3A%20S4M%20increases%20performance%2C%20reduces%20annotation%20effort%2C%20and%20aligns%20prompting%20with%20clinical%20practice%2C%20enabling%20more%20scalable%20dataset%20development%20in%20medical%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2503.05534v2&entry.124074799=Read"},
{"title": "Sequential Autonomous Exploration-Based Precise Mapping for Mobile Robots through Stepwise and Consistent Motions", "author": "Muhua Zhang and Lei Ma and Ying Wu and Kai Shen and Yongkui Sun and Henry Leung", "abstract": "This paper proposes a 2-D autonomous exploration and mapping framework for LiDAR-based SLAM mobile robots, designed to address the major challenges on low-cost platforms, including process instability, map drift, and increased risks of collisions and deadlocks. For frontier search, the local-global sampling architecture based on Rapidly-exploring Random Trees (RRTs) is employed. For local exploration, the proposed Self-Convergent RRT (SC-RRT) efficiently covers the reachable space within a finite time while the robot remains stationary, without relying on motion-induced sampling diversity. In addition, traversability checks during RRT expansion and global RRT pruning upon map updates eliminate unreachable frontiers, reducing potential collisions and deadlocks. For frontier point navigation, a stepwise consistent motion strategy is employed to generate motion trajectories that are more amenable to stable scan matching. The resulting straight-segment and in-place-rotation pattern improves scan-matching robustness and effectively suppresses map drift on resource-constrained platforms. For the process control, the framework serializes frontier point selection and navigation, avoiding oscillations caused by frequent goal changes in conventional parallelized processes. The waypoint retracing mechanism is incorporated to generate repeated observations, triggering loop closure detection and backend optimization in graph-based SLAM, thereby improving map consistency. Experiments in challenging simulated and real-world environments validate the effectiveness of the framework. Compared with baseline methods, the proposed framework achieves higher mapping success rates and stronger robustness on resource-constrained robots and maintains consistent mapping quality across various LiDAR field-of-view (FoV) configurations.", "link": "http://arxiv.org/abs/2503.17005v2", "date": "2025-11-17", "relevancy": 2.4091, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6176}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6034}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential%20Autonomous%20Exploration-Based%20Precise%20Mapping%20for%20Mobile%20Robots%20through%20Stepwise%20and%20Consistent%20Motions&body=Title%3A%20Sequential%20Autonomous%20Exploration-Based%20Precise%20Mapping%20for%20Mobile%20Robots%20through%20Stepwise%20and%20Consistent%20Motions%0AAuthor%3A%20Muhua%20Zhang%20and%20Lei%20Ma%20and%20Ying%20Wu%20and%20Kai%20Shen%20and%20Yongkui%20Sun%20and%20Henry%20Leung%0AAbstract%3A%20This%20paper%20proposes%20a%202-D%20autonomous%20exploration%20and%20mapping%20framework%20for%20LiDAR-based%20SLAM%20mobile%20robots%2C%20designed%20to%20address%20the%20major%20challenges%20on%20low-cost%20platforms%2C%20including%20process%20instability%2C%20map%20drift%2C%20and%20increased%20risks%20of%20collisions%20and%20deadlocks.%20For%20frontier%20search%2C%20the%20local-global%20sampling%20architecture%20based%20on%20Rapidly-exploring%20Random%20Trees%20%28RRTs%29%20is%20employed.%20For%20local%20exploration%2C%20the%20proposed%20Self-Convergent%20RRT%20%28SC-RRT%29%20efficiently%20covers%20the%20reachable%20space%20within%20a%20finite%20time%20while%20the%20robot%20remains%20stationary%2C%20without%20relying%20on%20motion-induced%20sampling%20diversity.%20In%20addition%2C%20traversability%20checks%20during%20RRT%20expansion%20and%20global%20RRT%20pruning%20upon%20map%20updates%20eliminate%20unreachable%20frontiers%2C%20reducing%20potential%20collisions%20and%20deadlocks.%20For%20frontier%20point%20navigation%2C%20a%20stepwise%20consistent%20motion%20strategy%20is%20employed%20to%20generate%20motion%20trajectories%20that%20are%20more%20amenable%20to%20stable%20scan%20matching.%20The%20resulting%20straight-segment%20and%20in-place-rotation%20pattern%20improves%20scan-matching%20robustness%20and%20effectively%20suppresses%20map%20drift%20on%20resource-constrained%20platforms.%20For%20the%20process%20control%2C%20the%20framework%20serializes%20frontier%20point%20selection%20and%20navigation%2C%20avoiding%20oscillations%20caused%20by%20frequent%20goal%20changes%20in%20conventional%20parallelized%20processes.%20The%20waypoint%20retracing%20mechanism%20is%20incorporated%20to%20generate%20repeated%20observations%2C%20triggering%20loop%20closure%20detection%20and%20backend%20optimization%20in%20graph-based%20SLAM%2C%20thereby%20improving%20map%20consistency.%20Experiments%20in%20challenging%20simulated%20and%20real-world%20environments%20validate%20the%20effectiveness%20of%20the%20framework.%20Compared%20with%20baseline%20methods%2C%20the%20proposed%20framework%20achieves%20higher%20mapping%20success%20rates%20and%20stronger%20robustness%20on%20resource-constrained%20robots%20and%20maintains%20consistent%20mapping%20quality%20across%20various%20LiDAR%20field-of-view%20%28FoV%29%20configurations.%0ALink%3A%20http%3A//arxiv.org/abs/2503.17005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential%2520Autonomous%2520Exploration-Based%2520Precise%2520Mapping%2520for%2520Mobile%2520Robots%2520through%2520Stepwise%2520and%2520Consistent%2520Motions%26entry.906535625%3DMuhua%2520Zhang%2520and%2520Lei%2520Ma%2520and%2520Ying%2520Wu%2520and%2520Kai%2520Shen%2520and%2520Yongkui%2520Sun%2520and%2520Henry%2520Leung%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%25202-D%2520autonomous%2520exploration%2520and%2520mapping%2520framework%2520for%2520LiDAR-based%2520SLAM%2520mobile%2520robots%252C%2520designed%2520to%2520address%2520the%2520major%2520challenges%2520on%2520low-cost%2520platforms%252C%2520including%2520process%2520instability%252C%2520map%2520drift%252C%2520and%2520increased%2520risks%2520of%2520collisions%2520and%2520deadlocks.%2520For%2520frontier%2520search%252C%2520the%2520local-global%2520sampling%2520architecture%2520based%2520on%2520Rapidly-exploring%2520Random%2520Trees%2520%2528RRTs%2529%2520is%2520employed.%2520For%2520local%2520exploration%252C%2520the%2520proposed%2520Self-Convergent%2520RRT%2520%2528SC-RRT%2529%2520efficiently%2520covers%2520the%2520reachable%2520space%2520within%2520a%2520finite%2520time%2520while%2520the%2520robot%2520remains%2520stationary%252C%2520without%2520relying%2520on%2520motion-induced%2520sampling%2520diversity.%2520In%2520addition%252C%2520traversability%2520checks%2520during%2520RRT%2520expansion%2520and%2520global%2520RRT%2520pruning%2520upon%2520map%2520updates%2520eliminate%2520unreachable%2520frontiers%252C%2520reducing%2520potential%2520collisions%2520and%2520deadlocks.%2520For%2520frontier%2520point%2520navigation%252C%2520a%2520stepwise%2520consistent%2520motion%2520strategy%2520is%2520employed%2520to%2520generate%2520motion%2520trajectories%2520that%2520are%2520more%2520amenable%2520to%2520stable%2520scan%2520matching.%2520The%2520resulting%2520straight-segment%2520and%2520in-place-rotation%2520pattern%2520improves%2520scan-matching%2520robustness%2520and%2520effectively%2520suppresses%2520map%2520drift%2520on%2520resource-constrained%2520platforms.%2520For%2520the%2520process%2520control%252C%2520the%2520framework%2520serializes%2520frontier%2520point%2520selection%2520and%2520navigation%252C%2520avoiding%2520oscillations%2520caused%2520by%2520frequent%2520goal%2520changes%2520in%2520conventional%2520parallelized%2520processes.%2520The%2520waypoint%2520retracing%2520mechanism%2520is%2520incorporated%2520to%2520generate%2520repeated%2520observations%252C%2520triggering%2520loop%2520closure%2520detection%2520and%2520backend%2520optimization%2520in%2520graph-based%2520SLAM%252C%2520thereby%2520improving%2520map%2520consistency.%2520Experiments%2520in%2520challenging%2520simulated%2520and%2520real-world%2520environments%2520validate%2520the%2520effectiveness%2520of%2520the%2520framework.%2520Compared%2520with%2520baseline%2520methods%252C%2520the%2520proposed%2520framework%2520achieves%2520higher%2520mapping%2520success%2520rates%2520and%2520stronger%2520robustness%2520on%2520resource-constrained%2520robots%2520and%2520maintains%2520consistent%2520mapping%2520quality%2520across%2520various%2520LiDAR%2520field-of-view%2520%2528FoV%2529%2520configurations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential%20Autonomous%20Exploration-Based%20Precise%20Mapping%20for%20Mobile%20Robots%20through%20Stepwise%20and%20Consistent%20Motions&entry.906535625=Muhua%20Zhang%20and%20Lei%20Ma%20and%20Ying%20Wu%20and%20Kai%20Shen%20and%20Yongkui%20Sun%20and%20Henry%20Leung&entry.1292438233=This%20paper%20proposes%20a%202-D%20autonomous%20exploration%20and%20mapping%20framework%20for%20LiDAR-based%20SLAM%20mobile%20robots%2C%20designed%20to%20address%20the%20major%20challenges%20on%20low-cost%20platforms%2C%20including%20process%20instability%2C%20map%20drift%2C%20and%20increased%20risks%20of%20collisions%20and%20deadlocks.%20For%20frontier%20search%2C%20the%20local-global%20sampling%20architecture%20based%20on%20Rapidly-exploring%20Random%20Trees%20%28RRTs%29%20is%20employed.%20For%20local%20exploration%2C%20the%20proposed%20Self-Convergent%20RRT%20%28SC-RRT%29%20efficiently%20covers%20the%20reachable%20space%20within%20a%20finite%20time%20while%20the%20robot%20remains%20stationary%2C%20without%20relying%20on%20motion-induced%20sampling%20diversity.%20In%20addition%2C%20traversability%20checks%20during%20RRT%20expansion%20and%20global%20RRT%20pruning%20upon%20map%20updates%20eliminate%20unreachable%20frontiers%2C%20reducing%20potential%20collisions%20and%20deadlocks.%20For%20frontier%20point%20navigation%2C%20a%20stepwise%20consistent%20motion%20strategy%20is%20employed%20to%20generate%20motion%20trajectories%20that%20are%20more%20amenable%20to%20stable%20scan%20matching.%20The%20resulting%20straight-segment%20and%20in-place-rotation%20pattern%20improves%20scan-matching%20robustness%20and%20effectively%20suppresses%20map%20drift%20on%20resource-constrained%20platforms.%20For%20the%20process%20control%2C%20the%20framework%20serializes%20frontier%20point%20selection%20and%20navigation%2C%20avoiding%20oscillations%20caused%20by%20frequent%20goal%20changes%20in%20conventional%20parallelized%20processes.%20The%20waypoint%20retracing%20mechanism%20is%20incorporated%20to%20generate%20repeated%20observations%2C%20triggering%20loop%20closure%20detection%20and%20backend%20optimization%20in%20graph-based%20SLAM%2C%20thereby%20improving%20map%20consistency.%20Experiments%20in%20challenging%20simulated%20and%20real-world%20environments%20validate%20the%20effectiveness%20of%20the%20framework.%20Compared%20with%20baseline%20methods%2C%20the%20proposed%20framework%20achieves%20higher%20mapping%20success%20rates%20and%20stronger%20robustness%20on%20resource-constrained%20robots%20and%20maintains%20consistent%20mapping%20quality%20across%20various%20LiDAR%20field-of-view%20%28FoV%29%20configurations.&entry.1838667208=http%3A//arxiv.org/abs/2503.17005v2&entry.124074799=Read"},
{"title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale", "author": "Yijia Fan and Jusheng Zhang and Kaitong Cai and Jing Yang and Jian Wang and Keze Wang", "abstract": "Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.", "link": "http://arxiv.org/abs/2511.13211v1", "date": "2025-11-17", "relevancy": 2.4038, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6203}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5879}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DAlign-DAER%3A%20Dynamic%20Attention%20Policy%20and%20Efficient%20Retrieval%20Strategy%20for%20Fine-grained%203D-Text%20Alignment%20at%20Scale&body=Title%3A%203DAlign-DAER%3A%20Dynamic%20Attention%20Policy%20and%20Efficient%20Retrieval%20Strategy%20for%20Fine-grained%203D-Text%20Alignment%20at%20Scale%0AAuthor%3A%20Yijia%20Fan%20and%20Jusheng%20Zhang%20and%20Kaitong%20Cai%20and%20Jing%20Yang%20and%20Jian%20Wang%20and%20Keze%20Wang%0AAbstract%3A%20Despite%20recent%20advancements%20in%203D-text%20cross-modal%20alignment%2C%20existing%20state-of-the-art%20methods%20still%20struggle%20to%20align%20fine-grained%20textual%20semantics%20with%20detailed%20geometric%20structures%2C%20and%20their%20alignment%20performance%20degrades%20significantly%20when%20scaling%20to%20large-scale%203D%20databases.%20To%20overcome%20this%20limitation%2C%20we%20introduce%203DAlign-DAER%2C%20a%20unified%20framework%20designed%20to%20align%20text%20and%203D%20geometry%20via%20the%20proposed%20dynamic%20attention%20policy%20and%20the%20efficient%20retrieval%20strategy%2C%20capturing%20subtle%20correspondences%20for%20diverse%20cross-modal%20retrieval%20and%20classification%20tasks.%20Specifically%2C%20during%20the%20training%2C%20our%20proposed%20dynamic%20attention%20policy%20%28DAP%29%20employs%20the%20Hierarchical%20Attention%20Fusion%20%28HAF%29%20module%20to%20represent%20the%20alignment%20as%20learnable%20fine-grained%20token-to-point%20attentions.%20To%20optimize%20these%20attentions%20across%20different%20tasks%20and%20geometric%20hierarchies%2C%20our%20DAP%20further%20exploits%20the%20Monte%20Carlo%20tree%20search%20to%20dynamically%20calibrate%20HAF%20attention%20weights%20via%20a%20hybrid%20reward%20signal%20and%20further%20enhances%20the%20alignment%20between%20textual%20descriptions%20and%20local%203D%20geometry.%20During%20the%20inference%2C%20our%203DAlign-DAER%20introduces%20an%20Efficient%20Retrieval%20Strategy%20%28ERS%29%20to%20leverage%20efficient%20hierarchical%20searching%20in%20the%20large-scale%20embedding%20spaces%2C%20outperforming%20traditional%20methods%20%28e.g.%2C%20KNN%29%20in%20accuracy%20and%20efficiency.%20Furthermore%2C%20to%20facilitate%20text-3D%20alignment%20research%20and%20train%20our%203DAlign-DAER%2C%20we%20construct%20Align3D-2M%2C%20a%20large-scale%20dataset%20featuring%202M%20text-3D%20pairs%2C%20to%20provide%20sufficient%20fine-grained%20cross-modal%20annotations.%20Extensive%20and%20comprehensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20our%203DAlign-DAER%20on%20diverse%20benchmarks.%20We%20will%20release%20our%20codes%2C%20models%2C%20and%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DAlign-DAER%253A%2520Dynamic%2520Attention%2520Policy%2520and%2520Efficient%2520Retrieval%2520Strategy%2520for%2520Fine-grained%25203D-Text%2520Alignment%2520at%2520Scale%26entry.906535625%3DYijia%2520Fan%2520and%2520Jusheng%2520Zhang%2520and%2520Kaitong%2520Cai%2520and%2520Jing%2520Yang%2520and%2520Jian%2520Wang%2520and%2520Keze%2520Wang%26entry.1292438233%3DDespite%2520recent%2520advancements%2520in%25203D-text%2520cross-modal%2520alignment%252C%2520existing%2520state-of-the-art%2520methods%2520still%2520struggle%2520to%2520align%2520fine-grained%2520textual%2520semantics%2520with%2520detailed%2520geometric%2520structures%252C%2520and%2520their%2520alignment%2520performance%2520degrades%2520significantly%2520when%2520scaling%2520to%2520large-scale%25203D%2520databases.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%25203DAlign-DAER%252C%2520a%2520unified%2520framework%2520designed%2520to%2520align%2520text%2520and%25203D%2520geometry%2520via%2520the%2520proposed%2520dynamic%2520attention%2520policy%2520and%2520the%2520efficient%2520retrieval%2520strategy%252C%2520capturing%2520subtle%2520correspondences%2520for%2520diverse%2520cross-modal%2520retrieval%2520and%2520classification%2520tasks.%2520Specifically%252C%2520during%2520the%2520training%252C%2520our%2520proposed%2520dynamic%2520attention%2520policy%2520%2528DAP%2529%2520employs%2520the%2520Hierarchical%2520Attention%2520Fusion%2520%2528HAF%2529%2520module%2520to%2520represent%2520the%2520alignment%2520as%2520learnable%2520fine-grained%2520token-to-point%2520attentions.%2520To%2520optimize%2520these%2520attentions%2520across%2520different%2520tasks%2520and%2520geometric%2520hierarchies%252C%2520our%2520DAP%2520further%2520exploits%2520the%2520Monte%2520Carlo%2520tree%2520search%2520to%2520dynamically%2520calibrate%2520HAF%2520attention%2520weights%2520via%2520a%2520hybrid%2520reward%2520signal%2520and%2520further%2520enhances%2520the%2520alignment%2520between%2520textual%2520descriptions%2520and%2520local%25203D%2520geometry.%2520During%2520the%2520inference%252C%2520our%25203DAlign-DAER%2520introduces%2520an%2520Efficient%2520Retrieval%2520Strategy%2520%2528ERS%2529%2520to%2520leverage%2520efficient%2520hierarchical%2520searching%2520in%2520the%2520large-scale%2520embedding%2520spaces%252C%2520outperforming%2520traditional%2520methods%2520%2528e.g.%252C%2520KNN%2529%2520in%2520accuracy%2520and%2520efficiency.%2520Furthermore%252C%2520to%2520facilitate%2520text-3D%2520alignment%2520research%2520and%2520train%2520our%25203DAlign-DAER%252C%2520we%2520construct%2520Align3D-2M%252C%2520a%2520large-scale%2520dataset%2520featuring%25202M%2520text-3D%2520pairs%252C%2520to%2520provide%2520sufficient%2520fine-grained%2520cross-modal%2520annotations.%2520Extensive%2520and%2520comprehensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%25203DAlign-DAER%2520on%2520diverse%2520benchmarks.%2520We%2520will%2520release%2520our%2520codes%252C%2520models%252C%2520and%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DAlign-DAER%3A%20Dynamic%20Attention%20Policy%20and%20Efficient%20Retrieval%20Strategy%20for%20Fine-grained%203D-Text%20Alignment%20at%20Scale&entry.906535625=Yijia%20Fan%20and%20Jusheng%20Zhang%20and%20Kaitong%20Cai%20and%20Jing%20Yang%20and%20Jian%20Wang%20and%20Keze%20Wang&entry.1292438233=Despite%20recent%20advancements%20in%203D-text%20cross-modal%20alignment%2C%20existing%20state-of-the-art%20methods%20still%20struggle%20to%20align%20fine-grained%20textual%20semantics%20with%20detailed%20geometric%20structures%2C%20and%20their%20alignment%20performance%20degrades%20significantly%20when%20scaling%20to%20large-scale%203D%20databases.%20To%20overcome%20this%20limitation%2C%20we%20introduce%203DAlign-DAER%2C%20a%20unified%20framework%20designed%20to%20align%20text%20and%203D%20geometry%20via%20the%20proposed%20dynamic%20attention%20policy%20and%20the%20efficient%20retrieval%20strategy%2C%20capturing%20subtle%20correspondences%20for%20diverse%20cross-modal%20retrieval%20and%20classification%20tasks.%20Specifically%2C%20during%20the%20training%2C%20our%20proposed%20dynamic%20attention%20policy%20%28DAP%29%20employs%20the%20Hierarchical%20Attention%20Fusion%20%28HAF%29%20module%20to%20represent%20the%20alignment%20as%20learnable%20fine-grained%20token-to-point%20attentions.%20To%20optimize%20these%20attentions%20across%20different%20tasks%20and%20geometric%20hierarchies%2C%20our%20DAP%20further%20exploits%20the%20Monte%20Carlo%20tree%20search%20to%20dynamically%20calibrate%20HAF%20attention%20weights%20via%20a%20hybrid%20reward%20signal%20and%20further%20enhances%20the%20alignment%20between%20textual%20descriptions%20and%20local%203D%20geometry.%20During%20the%20inference%2C%20our%203DAlign-DAER%20introduces%20an%20Efficient%20Retrieval%20Strategy%20%28ERS%29%20to%20leverage%20efficient%20hierarchical%20searching%20in%20the%20large-scale%20embedding%20spaces%2C%20outperforming%20traditional%20methods%20%28e.g.%2C%20KNN%29%20in%20accuracy%20and%20efficiency.%20Furthermore%2C%20to%20facilitate%20text-3D%20alignment%20research%20and%20train%20our%203DAlign-DAER%2C%20we%20construct%20Align3D-2M%2C%20a%20large-scale%20dataset%20featuring%202M%20text-3D%20pairs%2C%20to%20provide%20sufficient%20fine-grained%20cross-modal%20annotations.%20Extensive%20and%20comprehensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20our%203DAlign-DAER%20on%20diverse%20benchmarks.%20We%20will%20release%20our%20codes%2C%20models%2C%20and%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.13211v1&entry.124074799=Read"},
{"title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation", "author": "Hao Wang and Yuanfeng Song and Xiaoming Yin and Xing Chen", "abstract": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.", "link": "http://arxiv.org/abs/2511.13590v1", "date": "2025-11-17", "relevancy": 2.4028, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20SELECT%3A%20A%20Comprehensive%20Taxonomy-Guided%20Benchmark%20for%20Real-World%20Text-to-SQL%20Translation&body=Title%3A%20Beyond%20SELECT%3A%20A%20Comprehensive%20Taxonomy-Guided%20Benchmark%20for%20Real-World%20Text-to-SQL%20Translation%0AAuthor%3A%20Hao%20Wang%20and%20Yuanfeng%20Song%20and%20Xiaoming%20Yin%20and%20Xing%20Chen%0AAbstract%3A%20Text-to-SQL%20datasets%20are%20essential%20for%20training%20and%20evaluating%20text-to-SQL%20models%2C%20but%20existing%20datasets%20often%20suffer%20from%20limited%20coverage%20and%20fail%20to%20capture%20the%20diversity%20of%20real-world%20applications.%20To%20address%20this%2C%20we%20propose%20a%20novel%20taxonomy%20for%20text-to-SQL%20classification%20based%20on%20dimensions%20including%20core%20intents%2C%20statement%20types%2C%20syntax%20structures%2C%20and%20key%20actions.%20Using%20this%20taxonomy%2C%20we%20evaluate%20widely%20used%20public%20text-to-SQL%20datasets%20%28e.g.%2C%20Spider%20and%20Bird%29%20and%20reveal%20limitations%20in%20their%20coverage%20and%20diversity.%20We%20then%20introduce%20a%20taxonomy-guided%20dataset%20synthesis%20pipeline%2C%20yielding%20a%20new%20dataset%20named%20SQL-Synth.%20This%20approach%20combines%20the%20taxonomy%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20ensure%20the%20dataset%20reflects%20the%20breadth%20and%20complexity%20of%20real-world%20text-to-SQL%20applications.%20Extensive%20analysis%20and%20experimental%20results%20validate%20the%20effectiveness%20of%20our%20taxonomy%2C%20as%20SQL-Synth%20exhibits%20greater%20diversity%20and%20coverage%20compared%20to%20existing%20benchmarks.%20Moreover%2C%20we%20uncover%20that%20existing%20LLMs%20typically%20fall%20short%20in%20adequately%20capturing%20the%20full%20range%20of%20scenarios%2C%20resulting%20in%20limited%20performance%20on%20SQL-Synth.%20However%2C%20fine-tuning%20can%20substantially%20improve%20their%20performance%20in%20these%20scenarios.%20The%20proposed%20taxonomy%20has%20significant%20potential%20impact%2C%20as%20it%20not%20only%20enables%20comprehensive%20analysis%20of%20datasets%20and%20the%20performance%20of%20different%20LLMs%2C%20but%20also%20guides%20the%20construction%20of%20training%20data%20for%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520SELECT%253A%2520A%2520Comprehensive%2520Taxonomy-Guided%2520Benchmark%2520for%2520Real-World%2520Text-to-SQL%2520Translation%26entry.906535625%3DHao%2520Wang%2520and%2520Yuanfeng%2520Song%2520and%2520Xiaoming%2520Yin%2520and%2520Xing%2520Chen%26entry.1292438233%3DText-to-SQL%2520datasets%2520are%2520essential%2520for%2520training%2520and%2520evaluating%2520text-to-SQL%2520models%252C%2520but%2520existing%2520datasets%2520often%2520suffer%2520from%2520limited%2520coverage%2520and%2520fail%2520to%2520capture%2520the%2520diversity%2520of%2520real-world%2520applications.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520taxonomy%2520for%2520text-to-SQL%2520classification%2520based%2520on%2520dimensions%2520including%2520core%2520intents%252C%2520statement%2520types%252C%2520syntax%2520structures%252C%2520and%2520key%2520actions.%2520Using%2520this%2520taxonomy%252C%2520we%2520evaluate%2520widely%2520used%2520public%2520text-to-SQL%2520datasets%2520%2528e.g.%252C%2520Spider%2520and%2520Bird%2529%2520and%2520reveal%2520limitations%2520in%2520their%2520coverage%2520and%2520diversity.%2520We%2520then%2520introduce%2520a%2520taxonomy-guided%2520dataset%2520synthesis%2520pipeline%252C%2520yielding%2520a%2520new%2520dataset%2520named%2520SQL-Synth.%2520This%2520approach%2520combines%2520the%2520taxonomy%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520ensure%2520the%2520dataset%2520reflects%2520the%2520breadth%2520and%2520complexity%2520of%2520real-world%2520text-to-SQL%2520applications.%2520Extensive%2520analysis%2520and%2520experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%2520taxonomy%252C%2520as%2520SQL-Synth%2520exhibits%2520greater%2520diversity%2520and%2520coverage%2520compared%2520to%2520existing%2520benchmarks.%2520Moreover%252C%2520we%2520uncover%2520that%2520existing%2520LLMs%2520typically%2520fall%2520short%2520in%2520adequately%2520capturing%2520the%2520full%2520range%2520of%2520scenarios%252C%2520resulting%2520in%2520limited%2520performance%2520on%2520SQL-Synth.%2520However%252C%2520fine-tuning%2520can%2520substantially%2520improve%2520their%2520performance%2520in%2520these%2520scenarios.%2520The%2520proposed%2520taxonomy%2520has%2520significant%2520potential%2520impact%252C%2520as%2520it%2520not%2520only%2520enables%2520comprehensive%2520analysis%2520of%2520datasets%2520and%2520the%2520performance%2520of%2520different%2520LLMs%252C%2520but%2520also%2520guides%2520the%2520construction%2520of%2520training%2520data%2520for%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20SELECT%3A%20A%20Comprehensive%20Taxonomy-Guided%20Benchmark%20for%20Real-World%20Text-to-SQL%20Translation&entry.906535625=Hao%20Wang%20and%20Yuanfeng%20Song%20and%20Xiaoming%20Yin%20and%20Xing%20Chen&entry.1292438233=Text-to-SQL%20datasets%20are%20essential%20for%20training%20and%20evaluating%20text-to-SQL%20models%2C%20but%20existing%20datasets%20often%20suffer%20from%20limited%20coverage%20and%20fail%20to%20capture%20the%20diversity%20of%20real-world%20applications.%20To%20address%20this%2C%20we%20propose%20a%20novel%20taxonomy%20for%20text-to-SQL%20classification%20based%20on%20dimensions%20including%20core%20intents%2C%20statement%20types%2C%20syntax%20structures%2C%20and%20key%20actions.%20Using%20this%20taxonomy%2C%20we%20evaluate%20widely%20used%20public%20text-to-SQL%20datasets%20%28e.g.%2C%20Spider%20and%20Bird%29%20and%20reveal%20limitations%20in%20their%20coverage%20and%20diversity.%20We%20then%20introduce%20a%20taxonomy-guided%20dataset%20synthesis%20pipeline%2C%20yielding%20a%20new%20dataset%20named%20SQL-Synth.%20This%20approach%20combines%20the%20taxonomy%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20ensure%20the%20dataset%20reflects%20the%20breadth%20and%20complexity%20of%20real-world%20text-to-SQL%20applications.%20Extensive%20analysis%20and%20experimental%20results%20validate%20the%20effectiveness%20of%20our%20taxonomy%2C%20as%20SQL-Synth%20exhibits%20greater%20diversity%20and%20coverage%20compared%20to%20existing%20benchmarks.%20Moreover%2C%20we%20uncover%20that%20existing%20LLMs%20typically%20fall%20short%20in%20adequately%20capturing%20the%20full%20range%20of%20scenarios%2C%20resulting%20in%20limited%20performance%20on%20SQL-Synth.%20However%2C%20fine-tuning%20can%20substantially%20improve%20their%20performance%20in%20these%20scenarios.%20The%20proposed%20taxonomy%20has%20significant%20potential%20impact%2C%20as%20it%20not%20only%20enables%20comprehensive%20analysis%20of%20datasets%20and%20the%20performance%20of%20different%20LLMs%2C%20but%20also%20guides%20the%20construction%20of%20training%20data%20for%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.13590v1&entry.124074799=Read"},
{"title": "Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping", "author": "Lei Wang and Yulong Tian and Hao Han and Fengyuan Xu", "abstract": "Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .", "link": "http://arxiv.org/abs/2511.13356v1", "date": "2025-11-17", "relevancy": 2.4023, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4757}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20All-to-X%20Backdoor%20Attacks%20with%20Optimized%20Target%20Class%20Mapping&body=Title%3A%20Enhancing%20All-to-X%20Backdoor%20Attacks%20with%20Optimized%20Target%20Class%20Mapping%0AAuthor%3A%20Lei%20Wang%20and%20Yulong%20Tian%20and%20Hao%20Han%20and%20Fengyuan%20Xu%0AAbstract%3A%20Backdoor%20attacks%20pose%20severe%20threats%20to%20machine%20learning%20systems%2C%20prompting%20extensive%20research%20in%20this%20area.%20However%2C%20most%20existing%20work%20focuses%20on%20single-target%20All-to-One%20%28A2O%29%20attacks%2C%20overlooking%20the%20more%20complex%20All-to-X%20%28A2X%29%20attacks%20with%20multiple%20target%20classes%2C%20which%20are%20often%20assumed%20to%20have%20low%20attack%20success%20rates.%20In%20this%20paper%2C%20we%20first%20demonstrate%20that%20A2X%20attacks%20are%20robust%20against%20state-of-the-art%20defenses.%20We%20then%20propose%20a%20novel%20attack%20strategy%20that%20enhances%20the%20success%20rate%20of%20A2X%20attacks%20while%20maintaining%20robustness%20by%20optimizing%20grouping%20and%20target%20class%20assignment%20mechanisms.%20Our%20method%20improves%20the%20attack%20success%20rate%20by%20up%20to%2028%25%2C%20with%20average%20improvements%20of%206.7%25%2C%2016.4%25%2C%2014.1%25%20on%20CIFAR10%2C%20CIFAR100%2C%20and%20Tiny-ImageNet%2C%20respectively.%20We%20anticipate%20that%20this%20study%20will%20raise%20awareness%20of%20A2X%20attacks%20and%20stimulate%20further%20research%20in%20this%20under-explored%20area.%20Our%20code%20is%20available%20at%20https%3A//github.com/kazefjj/A2X-backdoor%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520All-to-X%2520Backdoor%2520Attacks%2520with%2520Optimized%2520Target%2520Class%2520Mapping%26entry.906535625%3DLei%2520Wang%2520and%2520Yulong%2520Tian%2520and%2520Hao%2520Han%2520and%2520Fengyuan%2520Xu%26entry.1292438233%3DBackdoor%2520attacks%2520pose%2520severe%2520threats%2520to%2520machine%2520learning%2520systems%252C%2520prompting%2520extensive%2520research%2520in%2520this%2520area.%2520However%252C%2520most%2520existing%2520work%2520focuses%2520on%2520single-target%2520All-to-One%2520%2528A2O%2529%2520attacks%252C%2520overlooking%2520the%2520more%2520complex%2520All-to-X%2520%2528A2X%2529%2520attacks%2520with%2520multiple%2520target%2520classes%252C%2520which%2520are%2520often%2520assumed%2520to%2520have%2520low%2520attack%2520success%2520rates.%2520In%2520this%2520paper%252C%2520we%2520first%2520demonstrate%2520that%2520A2X%2520attacks%2520are%2520robust%2520against%2520state-of-the-art%2520defenses.%2520We%2520then%2520propose%2520a%2520novel%2520attack%2520strategy%2520that%2520enhances%2520the%2520success%2520rate%2520of%2520A2X%2520attacks%2520while%2520maintaining%2520robustness%2520by%2520optimizing%2520grouping%2520and%2520target%2520class%2520assignment%2520mechanisms.%2520Our%2520method%2520improves%2520the%2520attack%2520success%2520rate%2520by%2520up%2520to%252028%2525%252C%2520with%2520average%2520improvements%2520of%25206.7%2525%252C%252016.4%2525%252C%252014.1%2525%2520on%2520CIFAR10%252C%2520CIFAR100%252C%2520and%2520Tiny-ImageNet%252C%2520respectively.%2520We%2520anticipate%2520that%2520this%2520study%2520will%2520raise%2520awareness%2520of%2520A2X%2520attacks%2520and%2520stimulate%2520further%2520research%2520in%2520this%2520under-explored%2520area.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/kazefjj/A2X-backdoor%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20All-to-X%20Backdoor%20Attacks%20with%20Optimized%20Target%20Class%20Mapping&entry.906535625=Lei%20Wang%20and%20Yulong%20Tian%20and%20Hao%20Han%20and%20Fengyuan%20Xu&entry.1292438233=Backdoor%20attacks%20pose%20severe%20threats%20to%20machine%20learning%20systems%2C%20prompting%20extensive%20research%20in%20this%20area.%20However%2C%20most%20existing%20work%20focuses%20on%20single-target%20All-to-One%20%28A2O%29%20attacks%2C%20overlooking%20the%20more%20complex%20All-to-X%20%28A2X%29%20attacks%20with%20multiple%20target%20classes%2C%20which%20are%20often%20assumed%20to%20have%20low%20attack%20success%20rates.%20In%20this%20paper%2C%20we%20first%20demonstrate%20that%20A2X%20attacks%20are%20robust%20against%20state-of-the-art%20defenses.%20We%20then%20propose%20a%20novel%20attack%20strategy%20that%20enhances%20the%20success%20rate%20of%20A2X%20attacks%20while%20maintaining%20robustness%20by%20optimizing%20grouping%20and%20target%20class%20assignment%20mechanisms.%20Our%20method%20improves%20the%20attack%20success%20rate%20by%20up%20to%2028%25%2C%20with%20average%20improvements%20of%206.7%25%2C%2016.4%25%2C%2014.1%25%20on%20CIFAR10%2C%20CIFAR100%2C%20and%20Tiny-ImageNet%2C%20respectively.%20We%20anticipate%20that%20this%20study%20will%20raise%20awareness%20of%20A2X%20attacks%20and%20stimulate%20further%20research%20in%20this%20under-explored%20area.%20Our%20code%20is%20available%20at%20https%3A//github.com/kazefjj/A2X-backdoor%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.13356v1&entry.124074799=Read"},
{"title": "Early Classification of Time Series: A Survey and Benchmark", "author": "Aur\u00e9lien Renault and Alexis Bondu and Antoine Cornu\u00e9jols and Vincent Lemaire", "abstract": "In many situations, the measurements of a studied phenomenon are provided sequentially, and the prediction of its class needs to be made as early as possible so as not to incur too high a time penalty, but not too early and risk paying the cost of misclassification. This problem has been particularly studied in the case of time series, and is known as Early Classification of Time Series (ECTS). Although it has been the subject of a growing body of literature, there is still a lack of a systematic, shared evaluation protocol to compare the relative merits of the various existing methods. In this paper, we highlight the two components of an ECTS system: decision and prediction, and focus on the approaches that separate them. This document begins by situating these methods within a principle-based taxonomy. It defines dimensions for organizing their evaluation and then reports the results of a very extensive set of experiments along these dimensions involving nine state-of-the-art ECTS algorithms. In addition, these and other experiments can be carried out using an open-source library in which most of the existing ECTS algorithms have been implemented (see https://github.com/ML-EDM/ml_edm).", "link": "http://arxiv.org/abs/2406.18332v6", "date": "2025-11-17", "relevancy": 2.3969, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5059}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4666}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Classification%20of%20Time%20Series%3A%20A%20Survey%20and%20Benchmark&body=Title%3A%20Early%20Classification%20of%20Time%20Series%3A%20A%20Survey%20and%20Benchmark%0AAuthor%3A%20Aur%C3%A9lien%20Renault%20and%20Alexis%20Bondu%20and%20Antoine%20Cornu%C3%A9jols%20and%20Vincent%20Lemaire%0AAbstract%3A%20In%20many%20situations%2C%20the%20measurements%20of%20a%20studied%20phenomenon%20are%20provided%20sequentially%2C%20and%20the%20prediction%20of%20its%20class%20needs%20to%20be%20made%20as%20early%20as%20possible%20so%20as%20not%20to%20incur%20too%20high%20a%20time%20penalty%2C%20but%20not%20too%20early%20and%20risk%20paying%20the%20cost%20of%20misclassification.%20This%20problem%20has%20been%20particularly%20studied%20in%20the%20case%20of%20time%20series%2C%20and%20is%20known%20as%20Early%20Classification%20of%20Time%20Series%20%28ECTS%29.%20Although%20it%20has%20been%20the%20subject%20of%20a%20growing%20body%20of%20literature%2C%20there%20is%20still%20a%20lack%20of%20a%20systematic%2C%20shared%20evaluation%20protocol%20to%20compare%20the%20relative%20merits%20of%20the%20various%20existing%20methods.%20In%20this%20paper%2C%20we%20highlight%20the%20two%20components%20of%20an%20ECTS%20system%3A%20decision%20and%20prediction%2C%20and%20focus%20on%20the%20approaches%20that%20separate%20them.%20This%20document%20begins%20by%20situating%20these%20methods%20within%20a%20principle-based%20taxonomy.%20It%20defines%20dimensions%20for%20organizing%20their%20evaluation%20and%20then%20reports%20the%20results%20of%20a%20very%20extensive%20set%20of%20experiments%20along%20these%20dimensions%20involving%20nine%20state-of-the-art%20ECTS%20algorithms.%20In%20addition%2C%20these%20and%20other%20experiments%20can%20be%20carried%20out%20using%20an%20open-source%20library%20in%20which%20most%20of%20the%20existing%20ECTS%20algorithms%20have%20been%20implemented%20%28see%20https%3A//github.com/ML-EDM/ml_edm%29.%0ALink%3A%20http%3A//arxiv.org/abs/2406.18332v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Classification%2520of%2520Time%2520Series%253A%2520A%2520Survey%2520and%2520Benchmark%26entry.906535625%3DAur%25C3%25A9lien%2520Renault%2520and%2520Alexis%2520Bondu%2520and%2520Antoine%2520Cornu%25C3%25A9jols%2520and%2520Vincent%2520Lemaire%26entry.1292438233%3DIn%2520many%2520situations%252C%2520the%2520measurements%2520of%2520a%2520studied%2520phenomenon%2520are%2520provided%2520sequentially%252C%2520and%2520the%2520prediction%2520of%2520its%2520class%2520needs%2520to%2520be%2520made%2520as%2520early%2520as%2520possible%2520so%2520as%2520not%2520to%2520incur%2520too%2520high%2520a%2520time%2520penalty%252C%2520but%2520not%2520too%2520early%2520and%2520risk%2520paying%2520the%2520cost%2520of%2520misclassification.%2520This%2520problem%2520has%2520been%2520particularly%2520studied%2520in%2520the%2520case%2520of%2520time%2520series%252C%2520and%2520is%2520known%2520as%2520Early%2520Classification%2520of%2520Time%2520Series%2520%2528ECTS%2529.%2520Although%2520it%2520has%2520been%2520the%2520subject%2520of%2520a%2520growing%2520body%2520of%2520literature%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520a%2520systematic%252C%2520shared%2520evaluation%2520protocol%2520to%2520compare%2520the%2520relative%2520merits%2520of%2520the%2520various%2520existing%2520methods.%2520In%2520this%2520paper%252C%2520we%2520highlight%2520the%2520two%2520components%2520of%2520an%2520ECTS%2520system%253A%2520decision%2520and%2520prediction%252C%2520and%2520focus%2520on%2520the%2520approaches%2520that%2520separate%2520them.%2520This%2520document%2520begins%2520by%2520situating%2520these%2520methods%2520within%2520a%2520principle-based%2520taxonomy.%2520It%2520defines%2520dimensions%2520for%2520organizing%2520their%2520evaluation%2520and%2520then%2520reports%2520the%2520results%2520of%2520a%2520very%2520extensive%2520set%2520of%2520experiments%2520along%2520these%2520dimensions%2520involving%2520nine%2520state-of-the-art%2520ECTS%2520algorithms.%2520In%2520addition%252C%2520these%2520and%2520other%2520experiments%2520can%2520be%2520carried%2520out%2520using%2520an%2520open-source%2520library%2520in%2520which%2520most%2520of%2520the%2520existing%2520ECTS%2520algorithms%2520have%2520been%2520implemented%2520%2528see%2520https%253A//github.com/ML-EDM/ml_edm%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18332v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Classification%20of%20Time%20Series%3A%20A%20Survey%20and%20Benchmark&entry.906535625=Aur%C3%A9lien%20Renault%20and%20Alexis%20Bondu%20and%20Antoine%20Cornu%C3%A9jols%20and%20Vincent%20Lemaire&entry.1292438233=In%20many%20situations%2C%20the%20measurements%20of%20a%20studied%20phenomenon%20are%20provided%20sequentially%2C%20and%20the%20prediction%20of%20its%20class%20needs%20to%20be%20made%20as%20early%20as%20possible%20so%20as%20not%20to%20incur%20too%20high%20a%20time%20penalty%2C%20but%20not%20too%20early%20and%20risk%20paying%20the%20cost%20of%20misclassification.%20This%20problem%20has%20been%20particularly%20studied%20in%20the%20case%20of%20time%20series%2C%20and%20is%20known%20as%20Early%20Classification%20of%20Time%20Series%20%28ECTS%29.%20Although%20it%20has%20been%20the%20subject%20of%20a%20growing%20body%20of%20literature%2C%20there%20is%20still%20a%20lack%20of%20a%20systematic%2C%20shared%20evaluation%20protocol%20to%20compare%20the%20relative%20merits%20of%20the%20various%20existing%20methods.%20In%20this%20paper%2C%20we%20highlight%20the%20two%20components%20of%20an%20ECTS%20system%3A%20decision%20and%20prediction%2C%20and%20focus%20on%20the%20approaches%20that%20separate%20them.%20This%20document%20begins%20by%20situating%20these%20methods%20within%20a%20principle-based%20taxonomy.%20It%20defines%20dimensions%20for%20organizing%20their%20evaluation%20and%20then%20reports%20the%20results%20of%20a%20very%20extensive%20set%20of%20experiments%20along%20these%20dimensions%20involving%20nine%20state-of-the-art%20ECTS%20algorithms.%20In%20addition%2C%20these%20and%20other%20experiments%20can%20be%20carried%20out%20using%20an%20open-source%20library%20in%20which%20most%20of%20the%20existing%20ECTS%20algorithms%20have%20been%20implemented%20%28see%20https%3A//github.com/ML-EDM/ml_edm%29.&entry.1838667208=http%3A//arxiv.org/abs/2406.18332v6&entry.124074799=Read"},
{"title": "JAFAR: Jack up Any Feature at Any Resolution", "author": "Paul Couairon and Loick Chambon and Louis Serrano and Jean-Emmanuel Haugeard and Matthieu Cord and Nicolas Thome", "abstract": "Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io", "link": "http://arxiv.org/abs/2506.11136v2", "date": "2025-11-17", "relevancy": 2.3835, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5034}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.465}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JAFAR%3A%20Jack%20up%20Any%20Feature%20at%20Any%20Resolution&body=Title%3A%20JAFAR%3A%20Jack%20up%20Any%20Feature%20at%20Any%20Resolution%0AAuthor%3A%20Paul%20Couairon%20and%20Loick%20Chambon%20and%20Louis%20Serrano%20and%20Jean-Emmanuel%20Haugeard%20and%20Matthieu%20Cord%20and%20Nicolas%20Thome%0AAbstract%3A%20Foundation%20Vision%20Encoders%20have%20become%20essential%20for%20a%20wide%20range%20of%20dense%20vision%20tasks.%20However%2C%20their%20low-resolution%20spatial%20feature%20outputs%20necessitate%20feature%20upsampling%20to%20produce%20the%20high-resolution%20modalities%20required%20for%20downstream%20tasks.%20In%20this%20work%2C%20we%20introduce%20JAFAR%2C%20a%20lightweight%20and%20flexible%20feature%20upsampler%20that%20enhances%20the%20spatial%20resolution%20of%20visual%20features%20from%20any%20Foundation%20Vision%20Encoder%20to%20an%20arbitrary%20target%20resolution.%20JAFAR%20employs%20an%20attention-based%20module%20designed%20to%20promote%20semantic%20alignment%20between%20high-resolution%20queries%2C%20derived%20from%20low-level%20image%20features%2C%20and%20semantically%20enriched%20low-resolution%20keys%2C%20using%20Spatial%20Feature%20Transform%20%28SFT%29%20modulation.%20Notably%2C%20despite%20the%20absence%20of%20high-resolution%20supervision%2C%20we%20demonstrate%20that%20learning%20at%20low%20upsampling%20ratios%20and%20resolutions%20generalizes%20remarkably%20well%20to%20significantly%20higher%20output%20scales.%20Extensive%20experiments%20show%20that%20JAFAR%20effectively%20recovers%20fine-grained%20spatial%20details%20and%20consistently%20outperforms%20existing%20feature%20upsampling%20methods%20across%20a%20diverse%20set%20of%20downstream%20tasks.%20Project%20page%20at%20https%3A//jafar-upsampler.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2506.11136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJAFAR%253A%2520Jack%2520up%2520Any%2520Feature%2520at%2520Any%2520Resolution%26entry.906535625%3DPaul%2520Couairon%2520and%2520Loick%2520Chambon%2520and%2520Louis%2520Serrano%2520and%2520Jean-Emmanuel%2520Haugeard%2520and%2520Matthieu%2520Cord%2520and%2520Nicolas%2520Thome%26entry.1292438233%3DFoundation%2520Vision%2520Encoders%2520have%2520become%2520essential%2520for%2520a%2520wide%2520range%2520of%2520dense%2520vision%2520tasks.%2520However%252C%2520their%2520low-resolution%2520spatial%2520feature%2520outputs%2520necessitate%2520feature%2520upsampling%2520to%2520produce%2520the%2520high-resolution%2520modalities%2520required%2520for%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520JAFAR%252C%2520a%2520lightweight%2520and%2520flexible%2520feature%2520upsampler%2520that%2520enhances%2520the%2520spatial%2520resolution%2520of%2520visual%2520features%2520from%2520any%2520Foundation%2520Vision%2520Encoder%2520to%2520an%2520arbitrary%2520target%2520resolution.%2520JAFAR%2520employs%2520an%2520attention-based%2520module%2520designed%2520to%2520promote%2520semantic%2520alignment%2520between%2520high-resolution%2520queries%252C%2520derived%2520from%2520low-level%2520image%2520features%252C%2520and%2520semantically%2520enriched%2520low-resolution%2520keys%252C%2520using%2520Spatial%2520Feature%2520Transform%2520%2528SFT%2529%2520modulation.%2520Notably%252C%2520despite%2520the%2520absence%2520of%2520high-resolution%2520supervision%252C%2520we%2520demonstrate%2520that%2520learning%2520at%2520low%2520upsampling%2520ratios%2520and%2520resolutions%2520generalizes%2520remarkably%2520well%2520to%2520significantly%2520higher%2520output%2520scales.%2520Extensive%2520experiments%2520show%2520that%2520JAFAR%2520effectively%2520recovers%2520fine-grained%2520spatial%2520details%2520and%2520consistently%2520outperforms%2520existing%2520feature%2520upsampling%2520methods%2520across%2520a%2520diverse%2520set%2520of%2520downstream%2520tasks.%2520Project%2520page%2520at%2520https%253A//jafar-upsampler.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JAFAR%3A%20Jack%20up%20Any%20Feature%20at%20Any%20Resolution&entry.906535625=Paul%20Couairon%20and%20Loick%20Chambon%20and%20Louis%20Serrano%20and%20Jean-Emmanuel%20Haugeard%20and%20Matthieu%20Cord%20and%20Nicolas%20Thome&entry.1292438233=Foundation%20Vision%20Encoders%20have%20become%20essential%20for%20a%20wide%20range%20of%20dense%20vision%20tasks.%20However%2C%20their%20low-resolution%20spatial%20feature%20outputs%20necessitate%20feature%20upsampling%20to%20produce%20the%20high-resolution%20modalities%20required%20for%20downstream%20tasks.%20In%20this%20work%2C%20we%20introduce%20JAFAR%2C%20a%20lightweight%20and%20flexible%20feature%20upsampler%20that%20enhances%20the%20spatial%20resolution%20of%20visual%20features%20from%20any%20Foundation%20Vision%20Encoder%20to%20an%20arbitrary%20target%20resolution.%20JAFAR%20employs%20an%20attention-based%20module%20designed%20to%20promote%20semantic%20alignment%20between%20high-resolution%20queries%2C%20derived%20from%20low-level%20image%20features%2C%20and%20semantically%20enriched%20low-resolution%20keys%2C%20using%20Spatial%20Feature%20Transform%20%28SFT%29%20modulation.%20Notably%2C%20despite%20the%20absence%20of%20high-resolution%20supervision%2C%20we%20demonstrate%20that%20learning%20at%20low%20upsampling%20ratios%20and%20resolutions%20generalizes%20remarkably%20well%20to%20significantly%20higher%20output%20scales.%20Extensive%20experiments%20show%20that%20JAFAR%20effectively%20recovers%20fine-grained%20spatial%20details%20and%20consistently%20outperforms%20existing%20feature%20upsampling%20methods%20across%20a%20diverse%20set%20of%20downstream%20tasks.%20Project%20page%20at%20https%3A//jafar-upsampler.github.io&entry.1838667208=http%3A//arxiv.org/abs/2506.11136v2&entry.124074799=Read"},
{"title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students", "author": "Guy Tel-Zur", "abstract": "This project addresses a critical pedagogical need: offering students continuous, on-demand academic assistance beyond conventional reception hours. I present a domain-specific Retrieval-Augmented Generation (RAG) system powered by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The assistant enhances learning by delivering real-time, personalized responses aligned with the \"Introduction to Parallel Processing\" course materials. GPU acceleration significantly improves inference latency, enabling practical deployment on consumer hardware. This approach demonstrates how consumer GPUs can enable affordable, private, and effective AI tutoring for HPC education.", "link": "http://arxiv.org/abs/2509.11947v2", "date": "2025-11-17", "relevancy": 2.3774, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4946}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4693}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20GPU-Accelerated%20RAG-Based%20Telegram%20Assistant%20for%20Supporting%20Parallel%20Processing%20Students&body=Title%3A%20A%20GPU-Accelerated%20RAG-Based%20Telegram%20Assistant%20for%20Supporting%20Parallel%20Processing%20Students%0AAuthor%3A%20Guy%20Tel-Zur%0AAbstract%3A%20This%20project%20addresses%20a%20critical%20pedagogical%20need%3A%20offering%20students%20continuous%2C%20on-demand%20academic%20assistance%20beyond%20conventional%20reception%20hours.%20I%20present%20a%20domain-specific%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20powered%20by%20a%20quantized%20Mistral-7B%20Instruct%20model%20and%20deployed%20as%20a%20Telegram%20bot.%20The%20assistant%20enhances%20learning%20by%20delivering%20real-time%2C%20personalized%20responses%20aligned%20with%20the%20%22Introduction%20to%20Parallel%20Processing%22%20course%20materials.%20GPU%20acceleration%20significantly%20improves%20inference%20latency%2C%20enabling%20practical%20deployment%20on%20consumer%20hardware.%20This%20approach%20demonstrates%20how%20consumer%20GPUs%20can%20enable%20affordable%2C%20private%2C%20and%20effective%20AI%20tutoring%20for%20HPC%20education.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520GPU-Accelerated%2520RAG-Based%2520Telegram%2520Assistant%2520for%2520Supporting%2520Parallel%2520Processing%2520Students%26entry.906535625%3DGuy%2520Tel-Zur%26entry.1292438233%3DThis%2520project%2520addresses%2520a%2520critical%2520pedagogical%2520need%253A%2520offering%2520students%2520continuous%252C%2520on-demand%2520academic%2520assistance%2520beyond%2520conventional%2520reception%2520hours.%2520I%2520present%2520a%2520domain-specific%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520system%2520powered%2520by%2520a%2520quantized%2520Mistral-7B%2520Instruct%2520model%2520and%2520deployed%2520as%2520a%2520Telegram%2520bot.%2520The%2520assistant%2520enhances%2520learning%2520by%2520delivering%2520real-time%252C%2520personalized%2520responses%2520aligned%2520with%2520the%2520%2522Introduction%2520to%2520Parallel%2520Processing%2522%2520course%2520materials.%2520GPU%2520acceleration%2520significantly%2520improves%2520inference%2520latency%252C%2520enabling%2520practical%2520deployment%2520on%2520consumer%2520hardware.%2520This%2520approach%2520demonstrates%2520how%2520consumer%2520GPUs%2520can%2520enable%2520affordable%252C%2520private%252C%2520and%2520effective%2520AI%2520tutoring%2520for%2520HPC%2520education.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20GPU-Accelerated%20RAG-Based%20Telegram%20Assistant%20for%20Supporting%20Parallel%20Processing%20Students&entry.906535625=Guy%20Tel-Zur&entry.1292438233=This%20project%20addresses%20a%20critical%20pedagogical%20need%3A%20offering%20students%20continuous%2C%20on-demand%20academic%20assistance%20beyond%20conventional%20reception%20hours.%20I%20present%20a%20domain-specific%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20powered%20by%20a%20quantized%20Mistral-7B%20Instruct%20model%20and%20deployed%20as%20a%20Telegram%20bot.%20The%20assistant%20enhances%20learning%20by%20delivering%20real-time%2C%20personalized%20responses%20aligned%20with%20the%20%22Introduction%20to%20Parallel%20Processing%22%20course%20materials.%20GPU%20acceleration%20significantly%20improves%20inference%20latency%2C%20enabling%20practical%20deployment%20on%20consumer%20hardware.%20This%20approach%20demonstrates%20how%20consumer%20GPUs%20can%20enable%20affordable%2C%20private%2C%20and%20effective%20AI%20tutoring%20for%20HPC%20education.&entry.1838667208=http%3A//arxiv.org/abs/2509.11947v2&entry.124074799=Read"},
{"title": "On the emergence of numerical instabilities in Next Generation Reservoir Computing", "author": "Edmilson Roque dos Santos and Erik Bollt", "abstract": "Next Generation Reservoir Computing (NGRC) is a low-cost machine learning method for forecasting chaotic time series from data. Computational efficiency is crucial for scalable reservoir computing, requiring better strategies to reduce training cost. In this work, we uncover a connection between the numerical conditioning of the NGRC feature matrix -- formed by polynomial evaluations on time-delay coordinates -- and the long-term NGRC dynamics. We show that NGRC can be trained without regularization, reducing computational time. Our contributions are twofold. First, merging tools from numerical linear algebra and ergodic theory of dynamical systems, we systematically study how the feature matrix conditioning varies across hyperparameters. We demonstrate that the NGRC feature matrix tends to be ill-conditioned for short time lags, high-degree polynomials, and short length of training data. Second, we evaluate the impact of different numerical algorithms (Cholesky, singular value decomposition (SVD), and lower-upper (LU) decomposition) for solving the regularized least-squares problem. Our results reveal that SVD-based training achieves accurate forecasts without regularization, being preferable when compared against the other algorithms.", "link": "http://arxiv.org/abs/2505.00846v2", "date": "2025-11-17", "relevancy": 2.373, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4811}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4755}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20emergence%20of%20numerical%20instabilities%20in%20Next%20Generation%20Reservoir%20Computing&body=Title%3A%20On%20the%20emergence%20of%20numerical%20instabilities%20in%20Next%20Generation%20Reservoir%20Computing%0AAuthor%3A%20Edmilson%20Roque%20dos%20Santos%20and%20Erik%20Bollt%0AAbstract%3A%20Next%20Generation%20Reservoir%20Computing%20%28NGRC%29%20is%20a%20low-cost%20machine%20learning%20method%20for%20forecasting%20chaotic%20time%20series%20from%20data.%20Computational%20efficiency%20is%20crucial%20for%20scalable%20reservoir%20computing%2C%20requiring%20better%20strategies%20to%20reduce%20training%20cost.%20In%20this%20work%2C%20we%20uncover%20a%20connection%20between%20the%20numerical%20conditioning%20of%20the%20NGRC%20feature%20matrix%20--%20formed%20by%20polynomial%20evaluations%20on%20time-delay%20coordinates%20--%20and%20the%20long-term%20NGRC%20dynamics.%20We%20show%20that%20NGRC%20can%20be%20trained%20without%20regularization%2C%20reducing%20computational%20time.%20Our%20contributions%20are%20twofold.%20First%2C%20merging%20tools%20from%20numerical%20linear%20algebra%20and%20ergodic%20theory%20of%20dynamical%20systems%2C%20we%20systematically%20study%20how%20the%20feature%20matrix%20conditioning%20varies%20across%20hyperparameters.%20We%20demonstrate%20that%20the%20NGRC%20feature%20matrix%20tends%20to%20be%20ill-conditioned%20for%20short%20time%20lags%2C%20high-degree%20polynomials%2C%20and%20short%20length%20of%20training%20data.%20Second%2C%20we%20evaluate%20the%20impact%20of%20different%20numerical%20algorithms%20%28Cholesky%2C%20singular%20value%20decomposition%20%28SVD%29%2C%20and%20lower-upper%20%28LU%29%20decomposition%29%20for%20solving%20the%20regularized%20least-squares%20problem.%20Our%20results%20reveal%20that%20SVD-based%20training%20achieves%20accurate%20forecasts%20without%20regularization%2C%20being%20preferable%20when%20compared%20against%20the%20other%20algorithms.%0ALink%3A%20http%3A//arxiv.org/abs/2505.00846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520emergence%2520of%2520numerical%2520instabilities%2520in%2520Next%2520Generation%2520Reservoir%2520Computing%26entry.906535625%3DEdmilson%2520Roque%2520dos%2520Santos%2520and%2520Erik%2520Bollt%26entry.1292438233%3DNext%2520Generation%2520Reservoir%2520Computing%2520%2528NGRC%2529%2520is%2520a%2520low-cost%2520machine%2520learning%2520method%2520for%2520forecasting%2520chaotic%2520time%2520series%2520from%2520data.%2520Computational%2520efficiency%2520is%2520crucial%2520for%2520scalable%2520reservoir%2520computing%252C%2520requiring%2520better%2520strategies%2520to%2520reduce%2520training%2520cost.%2520In%2520this%2520work%252C%2520we%2520uncover%2520a%2520connection%2520between%2520the%2520numerical%2520conditioning%2520of%2520the%2520NGRC%2520feature%2520matrix%2520--%2520formed%2520by%2520polynomial%2520evaluations%2520on%2520time-delay%2520coordinates%2520--%2520and%2520the%2520long-term%2520NGRC%2520dynamics.%2520We%2520show%2520that%2520NGRC%2520can%2520be%2520trained%2520without%2520regularization%252C%2520reducing%2520computational%2520time.%2520Our%2520contributions%2520are%2520twofold.%2520First%252C%2520merging%2520tools%2520from%2520numerical%2520linear%2520algebra%2520and%2520ergodic%2520theory%2520of%2520dynamical%2520systems%252C%2520we%2520systematically%2520study%2520how%2520the%2520feature%2520matrix%2520conditioning%2520varies%2520across%2520hyperparameters.%2520We%2520demonstrate%2520that%2520the%2520NGRC%2520feature%2520matrix%2520tends%2520to%2520be%2520ill-conditioned%2520for%2520short%2520time%2520lags%252C%2520high-degree%2520polynomials%252C%2520and%2520short%2520length%2520of%2520training%2520data.%2520Second%252C%2520we%2520evaluate%2520the%2520impact%2520of%2520different%2520numerical%2520algorithms%2520%2528Cholesky%252C%2520singular%2520value%2520decomposition%2520%2528SVD%2529%252C%2520and%2520lower-upper%2520%2528LU%2529%2520decomposition%2529%2520for%2520solving%2520the%2520regularized%2520least-squares%2520problem.%2520Our%2520results%2520reveal%2520that%2520SVD-based%2520training%2520achieves%2520accurate%2520forecasts%2520without%2520regularization%252C%2520being%2520preferable%2520when%2520compared%2520against%2520the%2520other%2520algorithms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20emergence%20of%20numerical%20instabilities%20in%20Next%20Generation%20Reservoir%20Computing&entry.906535625=Edmilson%20Roque%20dos%20Santos%20and%20Erik%20Bollt&entry.1292438233=Next%20Generation%20Reservoir%20Computing%20%28NGRC%29%20is%20a%20low-cost%20machine%20learning%20method%20for%20forecasting%20chaotic%20time%20series%20from%20data.%20Computational%20efficiency%20is%20crucial%20for%20scalable%20reservoir%20computing%2C%20requiring%20better%20strategies%20to%20reduce%20training%20cost.%20In%20this%20work%2C%20we%20uncover%20a%20connection%20between%20the%20numerical%20conditioning%20of%20the%20NGRC%20feature%20matrix%20--%20formed%20by%20polynomial%20evaluations%20on%20time-delay%20coordinates%20--%20and%20the%20long-term%20NGRC%20dynamics.%20We%20show%20that%20NGRC%20can%20be%20trained%20without%20regularization%2C%20reducing%20computational%20time.%20Our%20contributions%20are%20twofold.%20First%2C%20merging%20tools%20from%20numerical%20linear%20algebra%20and%20ergodic%20theory%20of%20dynamical%20systems%2C%20we%20systematically%20study%20how%20the%20feature%20matrix%20conditioning%20varies%20across%20hyperparameters.%20We%20demonstrate%20that%20the%20NGRC%20feature%20matrix%20tends%20to%20be%20ill-conditioned%20for%20short%20time%20lags%2C%20high-degree%20polynomials%2C%20and%20short%20length%20of%20training%20data.%20Second%2C%20we%20evaluate%20the%20impact%20of%20different%20numerical%20algorithms%20%28Cholesky%2C%20singular%20value%20decomposition%20%28SVD%29%2C%20and%20lower-upper%20%28LU%29%20decomposition%29%20for%20solving%20the%20regularized%20least-squares%20problem.%20Our%20results%20reveal%20that%20SVD-based%20training%20achieves%20accurate%20forecasts%20without%20regularization%2C%20being%20preferable%20when%20compared%20against%20the%20other%20algorithms.&entry.1838667208=http%3A//arxiv.org/abs/2505.00846v2&entry.124074799=Read"},
{"title": "On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning", "author": "Simon Kurz and Jian-Jia Chen and Lucie Flek and Zhixue Zhao", "abstract": "Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning.", "link": "http://arxiv.org/abs/2408.14398v4", "date": "2025-11-17", "relevancy": 2.3645, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Limitations%20of%20Language%20Targeted%20Pruning%3A%20Investigating%20the%20Calibration%20Language%20Impact%20in%20Multilingual%20LLM%20Pruning&body=Title%3A%20On%20the%20Limitations%20of%20Language%20Targeted%20Pruning%3A%20Investigating%20the%20Calibration%20Language%20Impact%20in%20Multilingual%20LLM%20Pruning%0AAuthor%3A%20Simon%20Kurz%20and%20Jian-Jia%20Chen%20and%20Lucie%20Flek%20and%20Zhixue%20Zhao%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20model%20%28LLM%29%20pruning%20have%20shown%20state-of-the-art%20%28SotA%29%20compression%20results%20in%20post-training%20and%20retraining-free%20settings%20while%20maintaining%20high%20predictive%20performance.%20However%2C%20previous%20research%20mainly%20considered%20calibrating%20based%20on%20English%20text%2C%20despite%20the%20multilingual%20nature%20of%20modern%20LLMs%20and%20their%20frequent%20use%20in%20non-English%20languages.%20This%20analysis%20paper%20conducts%20an%20in-depth%20investigation%20of%20the%20performance%20and%20internal%20representation%20changes%20associated%20with%20pruning%20multilingual%20language%20models%20for%20monolingual%20applications.%20We%20present%20the%20first%20comprehensive%20empirical%20study%2C%20comparing%20different%20calibration%20languages%20for%20pruning%20multilingual%20models%20across%20diverse%20languages%2C%20tasks%2C%20models%2C%20and%20SotA%20pruning%20techniques.%20We%20further%20analyze%20the%20latent%20subspaces%2C%20pruning%20masks%2C%20and%20individual%20neurons%20within%20pruned%20models.%20Our%20results%20reveal%20that%20while%20calibration%20on%20the%20target%20language%20effectively%20retains%20perplexity%20and%20yields%20high%20signal-to-noise%20ratios%2C%20it%20does%20not%20consistently%20improve%20downstream%20task%20performance.%20Further%20analysis%20of%20internal%20representations%20at%20three%20different%20levels%20highlights%20broader%20limitations%20of%20current%20pruning%20approaches%3A%20While%20they%20effectively%20preserve%20dominant%20information%20like%20language-specific%20features%2C%20this%20is%20insufficient%20to%20counteract%20the%20loss%20of%20nuanced%2C%20language-agnostic%20features%20that%20are%20crucial%20for%20knowledge%20retention%20and%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2408.14398v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Limitations%2520of%2520Language%2520Targeted%2520Pruning%253A%2520Investigating%2520the%2520Calibration%2520Language%2520Impact%2520in%2520Multilingual%2520LLM%2520Pruning%26entry.906535625%3DSimon%2520Kurz%2520and%2520Jian-Jia%2520Chen%2520and%2520Lucie%2520Flek%2520and%2520Zhixue%2520Zhao%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520model%2520%2528LLM%2529%2520pruning%2520have%2520shown%2520state-of-the-art%2520%2528SotA%2529%2520compression%2520results%2520in%2520post-training%2520and%2520retraining-free%2520settings%2520while%2520maintaining%2520high%2520predictive%2520performance.%2520However%252C%2520previous%2520research%2520mainly%2520considered%2520calibrating%2520based%2520on%2520English%2520text%252C%2520despite%2520the%2520multilingual%2520nature%2520of%2520modern%2520LLMs%2520and%2520their%2520frequent%2520use%2520in%2520non-English%2520languages.%2520This%2520analysis%2520paper%2520conducts%2520an%2520in-depth%2520investigation%2520of%2520the%2520performance%2520and%2520internal%2520representation%2520changes%2520associated%2520with%2520pruning%2520multilingual%2520language%2520models%2520for%2520monolingual%2520applications.%2520We%2520present%2520the%2520first%2520comprehensive%2520empirical%2520study%252C%2520comparing%2520different%2520calibration%2520languages%2520for%2520pruning%2520multilingual%2520models%2520across%2520diverse%2520languages%252C%2520tasks%252C%2520models%252C%2520and%2520SotA%2520pruning%2520techniques.%2520We%2520further%2520analyze%2520the%2520latent%2520subspaces%252C%2520pruning%2520masks%252C%2520and%2520individual%2520neurons%2520within%2520pruned%2520models.%2520Our%2520results%2520reveal%2520that%2520while%2520calibration%2520on%2520the%2520target%2520language%2520effectively%2520retains%2520perplexity%2520and%2520yields%2520high%2520signal-to-noise%2520ratios%252C%2520it%2520does%2520not%2520consistently%2520improve%2520downstream%2520task%2520performance.%2520Further%2520analysis%2520of%2520internal%2520representations%2520at%2520three%2520different%2520levels%2520highlights%2520broader%2520limitations%2520of%2520current%2520pruning%2520approaches%253A%2520While%2520they%2520effectively%2520preserve%2520dominant%2520information%2520like%2520language-specific%2520features%252C%2520this%2520is%2520insufficient%2520to%2520counteract%2520the%2520loss%2520of%2520nuanced%252C%2520language-agnostic%2520features%2520that%2520are%2520crucial%2520for%2520knowledge%2520retention%2520and%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14398v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Limitations%20of%20Language%20Targeted%20Pruning%3A%20Investigating%20the%20Calibration%20Language%20Impact%20in%20Multilingual%20LLM%20Pruning&entry.906535625=Simon%20Kurz%20and%20Jian-Jia%20Chen%20and%20Lucie%20Flek%20and%20Zhixue%20Zhao&entry.1292438233=Recent%20advances%20in%20large%20language%20model%20%28LLM%29%20pruning%20have%20shown%20state-of-the-art%20%28SotA%29%20compression%20results%20in%20post-training%20and%20retraining-free%20settings%20while%20maintaining%20high%20predictive%20performance.%20However%2C%20previous%20research%20mainly%20considered%20calibrating%20based%20on%20English%20text%2C%20despite%20the%20multilingual%20nature%20of%20modern%20LLMs%20and%20their%20frequent%20use%20in%20non-English%20languages.%20This%20analysis%20paper%20conducts%20an%20in-depth%20investigation%20of%20the%20performance%20and%20internal%20representation%20changes%20associated%20with%20pruning%20multilingual%20language%20models%20for%20monolingual%20applications.%20We%20present%20the%20first%20comprehensive%20empirical%20study%2C%20comparing%20different%20calibration%20languages%20for%20pruning%20multilingual%20models%20across%20diverse%20languages%2C%20tasks%2C%20models%2C%20and%20SotA%20pruning%20techniques.%20We%20further%20analyze%20the%20latent%20subspaces%2C%20pruning%20masks%2C%20and%20individual%20neurons%20within%20pruned%20models.%20Our%20results%20reveal%20that%20while%20calibration%20on%20the%20target%20language%20effectively%20retains%20perplexity%20and%20yields%20high%20signal-to-noise%20ratios%2C%20it%20does%20not%20consistently%20improve%20downstream%20task%20performance.%20Further%20analysis%20of%20internal%20representations%20at%20three%20different%20levels%20highlights%20broader%20limitations%20of%20current%20pruning%20approaches%3A%20While%20they%20effectively%20preserve%20dominant%20information%20like%20language-specific%20features%2C%20this%20is%20insufficient%20to%20counteract%20the%20loss%20of%20nuanced%2C%20language-agnostic%20features%20that%20are%20crucial%20for%20knowledge%20retention%20and%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2408.14398v4&entry.124074799=Read"},
{"title": "A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio", "author": "Ashlesha G. Sawant and Shreyash S. Kamble and Raj S. Kanade and Raunak N. Kanugo and Tanishq A. Kapse and Karan A. Bhapse", "abstract": "One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).", "link": "http://arxiv.org/abs/2511.13618v1", "date": "2025-11-17", "relevancy": 2.3574, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4701}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-Time%20Driver%20Drowsiness%20Detection%20System%20Using%20MediaPipe%20and%20Eye%20Aspect%20Ratio&body=Title%3A%20A%20Real-Time%20Driver%20Drowsiness%20Detection%20System%20Using%20MediaPipe%20and%20Eye%20Aspect%20Ratio%0AAuthor%3A%20Ashlesha%20G.%20Sawant%20and%20Shreyash%20S.%20Kamble%20and%20Raj%20S.%20Kanade%20and%20Raunak%20N.%20Kanugo%20and%20Tanishq%20A.%20Kapse%20and%20Karan%20A.%20Bhapse%0AAbstract%3A%20One%20of%20the%20major%20causes%20of%20road%20accidents%20is%20driver%20fatigue%20that%20causes%20thousands%20of%20fatalities%20and%20injuries%20every%20year.%20This%20study%20shows%20development%20of%20a%20Driver%20Drowsiness%20Detection%20System%20meant%20to%20improve%20the%20safety%20of%20the%20road%20by%20alerting%20drivers%20who%20are%20showing%20signs%20of%20being%20drowsy.%20The%20system%20is%20based%20on%20a%20standard%20webcam%20that%20tracks%20the%20facial%20features%20of%20the%20driver%20with%20the%20main%20emphasis%20on%20the%20examination%20of%20eye%20movements%20that%20can%20be%20conducted%20with%20the%20help%20of%20the%20Eye%20Aspect%20Ratio%20%28EAR%29%20method.%20The%20Face%20Mesh%20by%20MediaPipe%20is%20a%20lightweight%20framework%20that%20can%20identify%20facial%20landmarks%20with%20high%20accuracy%20and%20efficiency%2C%20which%20is%20considered%20to%20be%20important%20in%20real%20time%20use.%20The%20system%20detects%20the%20moments%20of%20long%20eye%20shutdowns%20or%20a%20very%20low%20rate%20of%20blinking%20which%20are%20manifestations%20of%20drowsiness%20and%20alerts%20the%20driver%20through%20sound%20to%20get%20her%20attention%20back.%20This%20system%20achieves%20a%20high-performance%20and%20low-cost%20driver%20monitoring%20solution%20with%20the%20help%20of%20the%20computational%20power%20of%20OpenCV%20to%20process%20the%20image%20and%20the%20MediaPipe%20to%20identify%20faces.%20Test%20data%20experimental%20analyses%20indicate%20that%20the%20system%20is%20very%20accurate%20and%20responds%20quicker%3B%20this%20confirms%20that%20it%20can%20be%20a%20component%20of%20the%20current%20Advanced%20Driving%20Assistance%20System%20%28ADAS%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-Time%2520Driver%2520Drowsiness%2520Detection%2520System%2520Using%2520MediaPipe%2520and%2520Eye%2520Aspect%2520Ratio%26entry.906535625%3DAshlesha%2520G.%2520Sawant%2520and%2520Shreyash%2520S.%2520Kamble%2520and%2520Raj%2520S.%2520Kanade%2520and%2520Raunak%2520N.%2520Kanugo%2520and%2520Tanishq%2520A.%2520Kapse%2520and%2520Karan%2520A.%2520Bhapse%26entry.1292438233%3DOne%2520of%2520the%2520major%2520causes%2520of%2520road%2520accidents%2520is%2520driver%2520fatigue%2520that%2520causes%2520thousands%2520of%2520fatalities%2520and%2520injuries%2520every%2520year.%2520This%2520study%2520shows%2520development%2520of%2520a%2520Driver%2520Drowsiness%2520Detection%2520System%2520meant%2520to%2520improve%2520the%2520safety%2520of%2520the%2520road%2520by%2520alerting%2520drivers%2520who%2520are%2520showing%2520signs%2520of%2520being%2520drowsy.%2520The%2520system%2520is%2520based%2520on%2520a%2520standard%2520webcam%2520that%2520tracks%2520the%2520facial%2520features%2520of%2520the%2520driver%2520with%2520the%2520main%2520emphasis%2520on%2520the%2520examination%2520of%2520eye%2520movements%2520that%2520can%2520be%2520conducted%2520with%2520the%2520help%2520of%2520the%2520Eye%2520Aspect%2520Ratio%2520%2528EAR%2529%2520method.%2520The%2520Face%2520Mesh%2520by%2520MediaPipe%2520is%2520a%2520lightweight%2520framework%2520that%2520can%2520identify%2520facial%2520landmarks%2520with%2520high%2520accuracy%2520and%2520efficiency%252C%2520which%2520is%2520considered%2520to%2520be%2520important%2520in%2520real%2520time%2520use.%2520The%2520system%2520detects%2520the%2520moments%2520of%2520long%2520eye%2520shutdowns%2520or%2520a%2520very%2520low%2520rate%2520of%2520blinking%2520which%2520are%2520manifestations%2520of%2520drowsiness%2520and%2520alerts%2520the%2520driver%2520through%2520sound%2520to%2520get%2520her%2520attention%2520back.%2520This%2520system%2520achieves%2520a%2520high-performance%2520and%2520low-cost%2520driver%2520monitoring%2520solution%2520with%2520the%2520help%2520of%2520the%2520computational%2520power%2520of%2520OpenCV%2520to%2520process%2520the%2520image%2520and%2520the%2520MediaPipe%2520to%2520identify%2520faces.%2520Test%2520data%2520experimental%2520analyses%2520indicate%2520that%2520the%2520system%2520is%2520very%2520accurate%2520and%2520responds%2520quicker%253B%2520this%2520confirms%2520that%2520it%2520can%2520be%2520a%2520component%2520of%2520the%2520current%2520Advanced%2520Driving%2520Assistance%2520System%2520%2528ADAS%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-Time%20Driver%20Drowsiness%20Detection%20System%20Using%20MediaPipe%20and%20Eye%20Aspect%20Ratio&entry.906535625=Ashlesha%20G.%20Sawant%20and%20Shreyash%20S.%20Kamble%20and%20Raj%20S.%20Kanade%20and%20Raunak%20N.%20Kanugo%20and%20Tanishq%20A.%20Kapse%20and%20Karan%20A.%20Bhapse&entry.1292438233=One%20of%20the%20major%20causes%20of%20road%20accidents%20is%20driver%20fatigue%20that%20causes%20thousands%20of%20fatalities%20and%20injuries%20every%20year.%20This%20study%20shows%20development%20of%20a%20Driver%20Drowsiness%20Detection%20System%20meant%20to%20improve%20the%20safety%20of%20the%20road%20by%20alerting%20drivers%20who%20are%20showing%20signs%20of%20being%20drowsy.%20The%20system%20is%20based%20on%20a%20standard%20webcam%20that%20tracks%20the%20facial%20features%20of%20the%20driver%20with%20the%20main%20emphasis%20on%20the%20examination%20of%20eye%20movements%20that%20can%20be%20conducted%20with%20the%20help%20of%20the%20Eye%20Aspect%20Ratio%20%28EAR%29%20method.%20The%20Face%20Mesh%20by%20MediaPipe%20is%20a%20lightweight%20framework%20that%20can%20identify%20facial%20landmarks%20with%20high%20accuracy%20and%20efficiency%2C%20which%20is%20considered%20to%20be%20important%20in%20real%20time%20use.%20The%20system%20detects%20the%20moments%20of%20long%20eye%20shutdowns%20or%20a%20very%20low%20rate%20of%20blinking%20which%20are%20manifestations%20of%20drowsiness%20and%20alerts%20the%20driver%20through%20sound%20to%20get%20her%20attention%20back.%20This%20system%20achieves%20a%20high-performance%20and%20low-cost%20driver%20monitoring%20solution%20with%20the%20help%20of%20the%20computational%20power%20of%20OpenCV%20to%20process%20the%20image%20and%20the%20MediaPipe%20to%20identify%20faces.%20Test%20data%20experimental%20analyses%20indicate%20that%20the%20system%20is%20very%20accurate%20and%20responds%20quicker%3B%20this%20confirms%20that%20it%20can%20be%20a%20component%20of%20the%20current%20Advanced%20Driving%20Assistance%20System%20%28ADAS%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.13618v1&entry.124074799=Read"},
{"title": "A Lexical Analysis of online Reviews on Human-AI Interactions", "author": "Parisa Arbab and Xiaowen Fang", "abstract": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.", "link": "http://arxiv.org/abs/2511.13480v1", "date": "2025-11-17", "relevancy": 2.3542, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lexical%20Analysis%20of%20online%20Reviews%20on%20Human-AI%20Interactions&body=Title%3A%20A%20Lexical%20Analysis%20of%20online%20Reviews%20on%20Human-AI%20Interactions%0AAuthor%3A%20Parisa%20Arbab%20and%20Xiaowen%20Fang%0AAbstract%3A%20This%20study%20focuses%20on%20understanding%20the%20complex%20dynamics%20between%20humans%20and%20AI%20systems%20by%20analyzing%20user%20reviews.%20While%20previous%20research%20has%20explored%20various%20aspects%20of%20human-AI%20interaction%2C%20such%20as%20user%20perceptions%20and%20ethical%20considerations%2C%20there%20remains%20a%20gap%20in%20understanding%20the%20specific%20concerns%20and%20challenges%20users%20face.%20By%20using%20a%20lexical%20approach%20to%20analyze%2055%2C968%20online%20reviews%20from%20G2.com%2C%20Producthunt.com%2C%20and%20Trustpilot.com%2C%20this%20preliminary%20research%20aims%20to%20analyze%20human-AI%20interaction.%20Initial%20results%20from%20factor%20analysis%20reveal%20key%20factors%20influencing%20these%20interactions.%20The%20study%20aims%20to%20provide%20deeper%20insights%20into%20these%20factors%20through%20content%20analysis%2C%20contributing%20to%20the%20development%20of%20more%20user-centric%20AI%20systems.%20The%20findings%20are%20expected%20to%20enhance%20our%20understanding%20of%20human-AI%20interaction%20and%20inform%20future%20AI%20technology%20and%20user%20experience%20improvements.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lexical%2520Analysis%2520of%2520online%2520Reviews%2520on%2520Human-AI%2520Interactions%26entry.906535625%3DParisa%2520Arbab%2520and%2520Xiaowen%2520Fang%26entry.1292438233%3DThis%2520study%2520focuses%2520on%2520understanding%2520the%2520complex%2520dynamics%2520between%2520humans%2520and%2520AI%2520systems%2520by%2520analyzing%2520user%2520reviews.%2520While%2520previous%2520research%2520has%2520explored%2520various%2520aspects%2520of%2520human-AI%2520interaction%252C%2520such%2520as%2520user%2520perceptions%2520and%2520ethical%2520considerations%252C%2520there%2520remains%2520a%2520gap%2520in%2520understanding%2520the%2520specific%2520concerns%2520and%2520challenges%2520users%2520face.%2520By%2520using%2520a%2520lexical%2520approach%2520to%2520analyze%252055%252C968%2520online%2520reviews%2520from%2520G2.com%252C%2520Producthunt.com%252C%2520and%2520Trustpilot.com%252C%2520this%2520preliminary%2520research%2520aims%2520to%2520analyze%2520human-AI%2520interaction.%2520Initial%2520results%2520from%2520factor%2520analysis%2520reveal%2520key%2520factors%2520influencing%2520these%2520interactions.%2520The%2520study%2520aims%2520to%2520provide%2520deeper%2520insights%2520into%2520these%2520factors%2520through%2520content%2520analysis%252C%2520contributing%2520to%2520the%2520development%2520of%2520more%2520user-centric%2520AI%2520systems.%2520The%2520findings%2520are%2520expected%2520to%2520enhance%2520our%2520understanding%2520of%2520human-AI%2520interaction%2520and%2520inform%2520future%2520AI%2520technology%2520and%2520user%2520experience%2520improvements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lexical%20Analysis%20of%20online%20Reviews%20on%20Human-AI%20Interactions&entry.906535625=Parisa%20Arbab%20and%20Xiaowen%20Fang&entry.1292438233=This%20study%20focuses%20on%20understanding%20the%20complex%20dynamics%20between%20humans%20and%20AI%20systems%20by%20analyzing%20user%20reviews.%20While%20previous%20research%20has%20explored%20various%20aspects%20of%20human-AI%20interaction%2C%20such%20as%20user%20perceptions%20and%20ethical%20considerations%2C%20there%20remains%20a%20gap%20in%20understanding%20the%20specific%20concerns%20and%20challenges%20users%20face.%20By%20using%20a%20lexical%20approach%20to%20analyze%2055%2C968%20online%20reviews%20from%20G2.com%2C%20Producthunt.com%2C%20and%20Trustpilot.com%2C%20this%20preliminary%20research%20aims%20to%20analyze%20human-AI%20interaction.%20Initial%20results%20from%20factor%20analysis%20reveal%20key%20factors%20influencing%20these%20interactions.%20The%20study%20aims%20to%20provide%20deeper%20insights%20into%20these%20factors%20through%20content%20analysis%2C%20contributing%20to%20the%20development%20of%20more%20user-centric%20AI%20systems.%20The%20findings%20are%20expected%20to%20enhance%20our%20understanding%20of%20human-AI%20interaction%20and%20inform%20future%20AI%20technology%20and%20user%20experience%20improvements.&entry.1838667208=http%3A//arxiv.org/abs/2511.13480v1&entry.124074799=Read"},
{"title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection", "author": "Junhee Lee and ChaeBeen Bang and MyoungChul Kim and MyeongAh Cho", "abstract": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.", "link": "http://arxiv.org/abs/2511.13204v1", "date": "2025-11-17", "relevancy": 2.3475, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.612}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.604}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefineVAD%3A%20Semantic-Guided%20Feature%20Recalibration%20for%20Weakly%20Supervised%20Video%20Anomaly%20Detection&body=Title%3A%20RefineVAD%3A%20Semantic-Guided%20Feature%20Recalibration%20for%20Weakly%20Supervised%20Video%20Anomaly%20Detection%0AAuthor%3A%20Junhee%20Lee%20and%20ChaeBeen%20Bang%20and%20MyoungChul%20Kim%20and%20MyeongAh%20Cho%0AAbstract%3A%20Weakly-Supervised%20Video%20Anomaly%20Detection%20aims%20to%20identify%20anomalous%20events%20using%20only%20video-level%20labels%2C%20balancing%20annotation%20efficiency%20with%20practical%20applicability.%20However%2C%20existing%20methods%20often%20oversimplify%20the%20anomaly%20space%20by%20treating%20all%20abnormal%20events%20as%20a%20single%20category%2C%20overlooking%20the%20diverse%20semantic%20and%20temporal%20characteristics%20intrinsic%20to%20real-world%20anomalies.%20Inspired%20by%20how%20humans%20perceive%20anomalies%2C%20by%20jointly%20interpreting%20temporal%20motion%20patterns%20and%20semantic%20structures%20underlying%20different%20anomaly%20types%2C%20we%20propose%20RefineVAD%2C%20a%20novel%20framework%20that%20mimics%20this%20dual-process%20reasoning.%20Our%20framework%20integrates%20two%20core%20modules.%20The%20first%2C%20Motion-aware%20Temporal%20Attention%20and%20Recalibration%20%28MoTAR%29%2C%20estimates%20motion%20salience%20and%20dynamically%20adjusts%20temporal%20focus%20via%20shift-based%20attention%20and%20global%20Transformer-based%20modeling.%20The%20second%2C%20Category-Oriented%20Refinement%20%28CORE%29%2C%20injects%20soft%20anomaly%20category%20priors%20into%20the%20representation%20space%20by%20aligning%20segment-level%20features%20with%20learnable%20category%20prototypes%20through%20cross-attention.%20By%20jointly%20leveraging%20temporal%20dynamics%20and%20semantic%20structure%2C%20explicitly%20models%20both%20%22how%22%20motion%20evolves%20and%20%22what%22%20semantic%20category%20it%20resembles.%20Extensive%20experiments%20on%20WVAD%20benchmark%20validate%20the%20effectiveness%20of%20RefineVAD%20and%20highlight%20the%20importance%20of%20integrating%20semantic%20context%20to%20guide%20feature%20refinement%20toward%20anomaly-relevant%20patterns.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefineVAD%253A%2520Semantic-Guided%2520Feature%2520Recalibration%2520for%2520Weakly%2520Supervised%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DJunhee%2520Lee%2520and%2520ChaeBeen%2520Bang%2520and%2520MyoungChul%2520Kim%2520and%2520MyeongAh%2520Cho%26entry.1292438233%3DWeakly-Supervised%2520Video%2520Anomaly%2520Detection%2520aims%2520to%2520identify%2520anomalous%2520events%2520using%2520only%2520video-level%2520labels%252C%2520balancing%2520annotation%2520efficiency%2520with%2520practical%2520applicability.%2520However%252C%2520existing%2520methods%2520often%2520oversimplify%2520the%2520anomaly%2520space%2520by%2520treating%2520all%2520abnormal%2520events%2520as%2520a%2520single%2520category%252C%2520overlooking%2520the%2520diverse%2520semantic%2520and%2520temporal%2520characteristics%2520intrinsic%2520to%2520real-world%2520anomalies.%2520Inspired%2520by%2520how%2520humans%2520perceive%2520anomalies%252C%2520by%2520jointly%2520interpreting%2520temporal%2520motion%2520patterns%2520and%2520semantic%2520structures%2520underlying%2520different%2520anomaly%2520types%252C%2520we%2520propose%2520RefineVAD%252C%2520a%2520novel%2520framework%2520that%2520mimics%2520this%2520dual-process%2520reasoning.%2520Our%2520framework%2520integrates%2520two%2520core%2520modules.%2520The%2520first%252C%2520Motion-aware%2520Temporal%2520Attention%2520and%2520Recalibration%2520%2528MoTAR%2529%252C%2520estimates%2520motion%2520salience%2520and%2520dynamically%2520adjusts%2520temporal%2520focus%2520via%2520shift-based%2520attention%2520and%2520global%2520Transformer-based%2520modeling.%2520The%2520second%252C%2520Category-Oriented%2520Refinement%2520%2528CORE%2529%252C%2520injects%2520soft%2520anomaly%2520category%2520priors%2520into%2520the%2520representation%2520space%2520by%2520aligning%2520segment-level%2520features%2520with%2520learnable%2520category%2520prototypes%2520through%2520cross-attention.%2520By%2520jointly%2520leveraging%2520temporal%2520dynamics%2520and%2520semantic%2520structure%252C%2520explicitly%2520models%2520both%2520%2522how%2522%2520motion%2520evolves%2520and%2520%2522what%2522%2520semantic%2520category%2520it%2520resembles.%2520Extensive%2520experiments%2520on%2520WVAD%2520benchmark%2520validate%2520the%2520effectiveness%2520of%2520RefineVAD%2520and%2520highlight%2520the%2520importance%2520of%2520integrating%2520semantic%2520context%2520to%2520guide%2520feature%2520refinement%2520toward%2520anomaly-relevant%2520patterns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefineVAD%3A%20Semantic-Guided%20Feature%20Recalibration%20for%20Weakly%20Supervised%20Video%20Anomaly%20Detection&entry.906535625=Junhee%20Lee%20and%20ChaeBeen%20Bang%20and%20MyoungChul%20Kim%20and%20MyeongAh%20Cho&entry.1292438233=Weakly-Supervised%20Video%20Anomaly%20Detection%20aims%20to%20identify%20anomalous%20events%20using%20only%20video-level%20labels%2C%20balancing%20annotation%20efficiency%20with%20practical%20applicability.%20However%2C%20existing%20methods%20often%20oversimplify%20the%20anomaly%20space%20by%20treating%20all%20abnormal%20events%20as%20a%20single%20category%2C%20overlooking%20the%20diverse%20semantic%20and%20temporal%20characteristics%20intrinsic%20to%20real-world%20anomalies.%20Inspired%20by%20how%20humans%20perceive%20anomalies%2C%20by%20jointly%20interpreting%20temporal%20motion%20patterns%20and%20semantic%20structures%20underlying%20different%20anomaly%20types%2C%20we%20propose%20RefineVAD%2C%20a%20novel%20framework%20that%20mimics%20this%20dual-process%20reasoning.%20Our%20framework%20integrates%20two%20core%20modules.%20The%20first%2C%20Motion-aware%20Temporal%20Attention%20and%20Recalibration%20%28MoTAR%29%2C%20estimates%20motion%20salience%20and%20dynamically%20adjusts%20temporal%20focus%20via%20shift-based%20attention%20and%20global%20Transformer-based%20modeling.%20The%20second%2C%20Category-Oriented%20Refinement%20%28CORE%29%2C%20injects%20soft%20anomaly%20category%20priors%20into%20the%20representation%20space%20by%20aligning%20segment-level%20features%20with%20learnable%20category%20prototypes%20through%20cross-attention.%20By%20jointly%20leveraging%20temporal%20dynamics%20and%20semantic%20structure%2C%20explicitly%20models%20both%20%22how%22%20motion%20evolves%20and%20%22what%22%20semantic%20category%20it%20resembles.%20Extensive%20experiments%20on%20WVAD%20benchmark%20validate%20the%20effectiveness%20of%20RefineVAD%20and%20highlight%20the%20importance%20of%20integrating%20semantic%20context%20to%20guide%20feature%20refinement%20toward%20anomaly-relevant%20patterns.&entry.1838667208=http%3A//arxiv.org/abs/2511.13204v1&entry.124074799=Read"},
{"title": "Fairness-Aware Graph Representation Learning with Limited Demographic Information", "author": "Zichong Wang and Zhipeng Yin and Liping Yang and Jun Zhuang and Rui Yu and Qingzhao Kong and Wenbin Zhang", "abstract": "Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.", "link": "http://arxiv.org/abs/2511.13540v1", "date": "2025-11-17", "relevancy": 2.3468, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4914}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4593}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness-Aware%20Graph%20Representation%20Learning%20with%20Limited%20Demographic%20Information&body=Title%3A%20Fairness-Aware%20Graph%20Representation%20Learning%20with%20Limited%20Demographic%20Information%0AAuthor%3A%20Zichong%20Wang%20and%20Zhipeng%20Yin%20and%20Liping%20Yang%20and%20Jun%20Zhuang%20and%20Rui%20Yu%20and%20Qingzhao%20Kong%20and%20Wenbin%20Zhang%0AAbstract%3A%20Ensuring%20fairness%20in%20Graph%20Neural%20Networks%20is%20fundamental%20to%20promoting%20trustworthy%20and%20socially%20responsible%20machine%20learning%20systems.%20In%20response%2C%20numerous%20fair%20graph%20learning%20methods%20have%20been%20proposed%20in%20recent%20years.%20However%2C%20most%20of%20them%20assume%20full%20access%20to%20demographic%20information%2C%20a%20requirement%20rarely%20met%20in%20practice%20due%20to%20privacy%2C%20legal%2C%20or%20regulatory%20restrictions.%20To%20this%20end%2C%20this%20paper%20introduces%20a%20novel%20fair%20graph%20learning%20framework%20that%20mitigates%20bias%20in%20graph%20learning%20under%20limited%20demographic%20information.%20Specifically%2C%20we%20propose%20a%20mechanism%20guided%20by%20partial%20demographic%20data%20to%20generate%20proxies%20for%20demographic%20information%20and%20design%20a%20strategy%20that%20enforces%20consistent%20node%20embeddings%20across%20demographic%20groups.%20In%20addition%2C%20we%20develop%20an%20adaptive%20confidence%20strategy%20that%20dynamically%20adjusts%20each%20node%27s%20contribution%20to%20fairness%20and%20utility%20based%20on%20prediction%20confidence.%20We%20further%20provide%20theoretical%20analysis%20demonstrating%20that%20our%20framework%2C%20FairGLite%2C%20achieves%20provable%20upper%20bounds%20on%20group%20fairness%20metrics%2C%20offering%20formal%20guarantees%20for%20bias%20mitigation.%20Through%20extensive%20experiments%20on%20multiple%20datasets%20and%20fair%20graph%20learning%20frameworks%2C%20we%20demonstrate%20the%20framework%27s%20effectiveness%20in%20both%20mitigating%20bias%20and%20maintaining%20model%20utility.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness-Aware%2520Graph%2520Representation%2520Learning%2520with%2520Limited%2520Demographic%2520Information%26entry.906535625%3DZichong%2520Wang%2520and%2520Zhipeng%2520Yin%2520and%2520Liping%2520Yang%2520and%2520Jun%2520Zhuang%2520and%2520Rui%2520Yu%2520and%2520Qingzhao%2520Kong%2520and%2520Wenbin%2520Zhang%26entry.1292438233%3DEnsuring%2520fairness%2520in%2520Graph%2520Neural%2520Networks%2520is%2520fundamental%2520to%2520promoting%2520trustworthy%2520and%2520socially%2520responsible%2520machine%2520learning%2520systems.%2520In%2520response%252C%2520numerous%2520fair%2520graph%2520learning%2520methods%2520have%2520been%2520proposed%2520in%2520recent%2520years.%2520However%252C%2520most%2520of%2520them%2520assume%2520full%2520access%2520to%2520demographic%2520information%252C%2520a%2520requirement%2520rarely%2520met%2520in%2520practice%2520due%2520to%2520privacy%252C%2520legal%252C%2520or%2520regulatory%2520restrictions.%2520To%2520this%2520end%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520fair%2520graph%2520learning%2520framework%2520that%2520mitigates%2520bias%2520in%2520graph%2520learning%2520under%2520limited%2520demographic%2520information.%2520Specifically%252C%2520we%2520propose%2520a%2520mechanism%2520guided%2520by%2520partial%2520demographic%2520data%2520to%2520generate%2520proxies%2520for%2520demographic%2520information%2520and%2520design%2520a%2520strategy%2520that%2520enforces%2520consistent%2520node%2520embeddings%2520across%2520demographic%2520groups.%2520In%2520addition%252C%2520we%2520develop%2520an%2520adaptive%2520confidence%2520strategy%2520that%2520dynamically%2520adjusts%2520each%2520node%2527s%2520contribution%2520to%2520fairness%2520and%2520utility%2520based%2520on%2520prediction%2520confidence.%2520We%2520further%2520provide%2520theoretical%2520analysis%2520demonstrating%2520that%2520our%2520framework%252C%2520FairGLite%252C%2520achieves%2520provable%2520upper%2520bounds%2520on%2520group%2520fairness%2520metrics%252C%2520offering%2520formal%2520guarantees%2520for%2520bias%2520mitigation.%2520Through%2520extensive%2520experiments%2520on%2520multiple%2520datasets%2520and%2520fair%2520graph%2520learning%2520frameworks%252C%2520we%2520demonstrate%2520the%2520framework%2527s%2520effectiveness%2520in%2520both%2520mitigating%2520bias%2520and%2520maintaining%2520model%2520utility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness-Aware%20Graph%20Representation%20Learning%20with%20Limited%20Demographic%20Information&entry.906535625=Zichong%20Wang%20and%20Zhipeng%20Yin%20and%20Liping%20Yang%20and%20Jun%20Zhuang%20and%20Rui%20Yu%20and%20Qingzhao%20Kong%20and%20Wenbin%20Zhang&entry.1292438233=Ensuring%20fairness%20in%20Graph%20Neural%20Networks%20is%20fundamental%20to%20promoting%20trustworthy%20and%20socially%20responsible%20machine%20learning%20systems.%20In%20response%2C%20numerous%20fair%20graph%20learning%20methods%20have%20been%20proposed%20in%20recent%20years.%20However%2C%20most%20of%20them%20assume%20full%20access%20to%20demographic%20information%2C%20a%20requirement%20rarely%20met%20in%20practice%20due%20to%20privacy%2C%20legal%2C%20or%20regulatory%20restrictions.%20To%20this%20end%2C%20this%20paper%20introduces%20a%20novel%20fair%20graph%20learning%20framework%20that%20mitigates%20bias%20in%20graph%20learning%20under%20limited%20demographic%20information.%20Specifically%2C%20we%20propose%20a%20mechanism%20guided%20by%20partial%20demographic%20data%20to%20generate%20proxies%20for%20demographic%20information%20and%20design%20a%20strategy%20that%20enforces%20consistent%20node%20embeddings%20across%20demographic%20groups.%20In%20addition%2C%20we%20develop%20an%20adaptive%20confidence%20strategy%20that%20dynamically%20adjusts%20each%20node%27s%20contribution%20to%20fairness%20and%20utility%20based%20on%20prediction%20confidence.%20We%20further%20provide%20theoretical%20analysis%20demonstrating%20that%20our%20framework%2C%20FairGLite%2C%20achieves%20provable%20upper%20bounds%20on%20group%20fairness%20metrics%2C%20offering%20formal%20guarantees%20for%20bias%20mitigation.%20Through%20extensive%20experiments%20on%20multiple%20datasets%20and%20fair%20graph%20learning%20frameworks%2C%20we%20demonstrate%20the%20framework%27s%20effectiveness%20in%20both%20mitigating%20bias%20and%20maintaining%20model%20utility.&entry.1838667208=http%3A//arxiv.org/abs/2511.13540v1&entry.124074799=Read"},
{"title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query", "author": "Yixuan Wang and Shiyu Ji and Yijun Liu and Yuzhuang Xu and Yang Xu and Qingfu Zhu and Wanxiang Che", "abstract": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.", "link": "http://arxiv.org/abs/2505.20334v2", "date": "2025-11-17", "relevancy": 2.3396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lookahead%20Q-Cache%3A%20Achieving%20More%20Consistent%20KV%20Cache%20Eviction%20via%20Pseudo%20Query&body=Title%3A%20Lookahead%20Q-Cache%3A%20Achieving%20More%20Consistent%20KV%20Cache%20Eviction%20via%20Pseudo%20Query%0AAuthor%3A%20Yixuan%20Wang%20and%20Shiyu%20Ji%20and%20Yijun%20Liu%20and%20Yuzhuang%20Xu%20and%20Yang%20Xu%20and%20Qingfu%20Zhu%20and%20Wanxiang%20Che%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20rely%20on%20key-value%20cache%20%28KV%20cache%29%20to%20accelerate%20decoding%20by%20reducing%20redundant%20computations.%20However%2C%20the%20KV%20cache%20memory%20usage%20grows%20substantially%20with%20longer%20text%20sequences%2C%20posing%20challenges%20for%20efficient%20deployment.%20Existing%20KV%20cache%20eviction%20methods%20prune%20tokens%20using%20prefilling-stage%20attention%20scores%2C%20causing%20inconsistency%20with%20actual%20inference%20queries%2C%20especially%20under%20tight%20memory%20budgets.%20In%20this%20paper%2C%20we%20propose%20Lookahead%20Q-Cache%20%28LAQ%29%2C%20a%20novel%20eviction%20framework%20that%20generates%20low-cost%20pseudo%20lookahead%20queries%20to%20better%20approximate%20the%20true%20decoding-stage%20queries.%20By%20using%20these%20lookahead%20queries%20as%20the%20observation%20window%20for%20importance%20estimation%2C%20LAQ%20achieves%20more%20consistent%20and%20accurate%20KV%20cache%20eviction%20aligned%20with%20real%20inference%20scenarios.%20Experimental%20results%20on%20LongBench%20and%20Needle-in-a-Haystack%20benchmarks%20show%20that%20LAQ%20outperforms%20existing%20methods%20across%20various%20budget%20levels%2C%20achieving%20a%201%20%24%5Csim%24%204%20point%20improvement%20on%20LongBench%20under%20limited%20cache%20budget.%20Moreover%2C%20LAQ%20is%20complementary%20to%20existing%20approaches%20and%20can%20be%20flexibly%20combined%20to%20yield%20further%20improvements.%0ALink%3A%20http%3A//arxiv.org/abs/2505.20334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookahead%2520Q-Cache%253A%2520Achieving%2520More%2520Consistent%2520KV%2520Cache%2520Eviction%2520via%2520Pseudo%2520Query%26entry.906535625%3DYixuan%2520Wang%2520and%2520Shiyu%2520Ji%2520and%2520Yijun%2520Liu%2520and%2520Yuzhuang%2520Xu%2520and%2520Yang%2520Xu%2520and%2520Qingfu%2520Zhu%2520and%2520Wanxiang%2520Che%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520rely%2520on%2520key-value%2520cache%2520%2528KV%2520cache%2529%2520to%2520accelerate%2520decoding%2520by%2520reducing%2520redundant%2520computations.%2520However%252C%2520the%2520KV%2520cache%2520memory%2520usage%2520grows%2520substantially%2520with%2520longer%2520text%2520sequences%252C%2520posing%2520challenges%2520for%2520efficient%2520deployment.%2520Existing%2520KV%2520cache%2520eviction%2520methods%2520prune%2520tokens%2520using%2520prefilling-stage%2520attention%2520scores%252C%2520causing%2520inconsistency%2520with%2520actual%2520inference%2520queries%252C%2520especially%2520under%2520tight%2520memory%2520budgets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Lookahead%2520Q-Cache%2520%2528LAQ%2529%252C%2520a%2520novel%2520eviction%2520framework%2520that%2520generates%2520low-cost%2520pseudo%2520lookahead%2520queries%2520to%2520better%2520approximate%2520the%2520true%2520decoding-stage%2520queries.%2520By%2520using%2520these%2520lookahead%2520queries%2520as%2520the%2520observation%2520window%2520for%2520importance%2520estimation%252C%2520LAQ%2520achieves%2520more%2520consistent%2520and%2520accurate%2520KV%2520cache%2520eviction%2520aligned%2520with%2520real%2520inference%2520scenarios.%2520Experimental%2520results%2520on%2520LongBench%2520and%2520Needle-in-a-Haystack%2520benchmarks%2520show%2520that%2520LAQ%2520outperforms%2520existing%2520methods%2520across%2520various%2520budget%2520levels%252C%2520achieving%2520a%25201%2520%2524%255Csim%2524%25204%2520point%2520improvement%2520on%2520LongBench%2520under%2520limited%2520cache%2520budget.%2520Moreover%252C%2520LAQ%2520is%2520complementary%2520to%2520existing%2520approaches%2520and%2520can%2520be%2520flexibly%2520combined%2520to%2520yield%2520further%2520improvements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lookahead%20Q-Cache%3A%20Achieving%20More%20Consistent%20KV%20Cache%20Eviction%20via%20Pseudo%20Query&entry.906535625=Yixuan%20Wang%20and%20Shiyu%20Ji%20and%20Yijun%20Liu%20and%20Yuzhuang%20Xu%20and%20Yang%20Xu%20and%20Qingfu%20Zhu%20and%20Wanxiang%20Che&entry.1292438233=Large%20language%20models%20%28LLMs%29%20rely%20on%20key-value%20cache%20%28KV%20cache%29%20to%20accelerate%20decoding%20by%20reducing%20redundant%20computations.%20However%2C%20the%20KV%20cache%20memory%20usage%20grows%20substantially%20with%20longer%20text%20sequences%2C%20posing%20challenges%20for%20efficient%20deployment.%20Existing%20KV%20cache%20eviction%20methods%20prune%20tokens%20using%20prefilling-stage%20attention%20scores%2C%20causing%20inconsistency%20with%20actual%20inference%20queries%2C%20especially%20under%20tight%20memory%20budgets.%20In%20this%20paper%2C%20we%20propose%20Lookahead%20Q-Cache%20%28LAQ%29%2C%20a%20novel%20eviction%20framework%20that%20generates%20low-cost%20pseudo%20lookahead%20queries%20to%20better%20approximate%20the%20true%20decoding-stage%20queries.%20By%20using%20these%20lookahead%20queries%20as%20the%20observation%20window%20for%20importance%20estimation%2C%20LAQ%20achieves%20more%20consistent%20and%20accurate%20KV%20cache%20eviction%20aligned%20with%20real%20inference%20scenarios.%20Experimental%20results%20on%20LongBench%20and%20Needle-in-a-Haystack%20benchmarks%20show%20that%20LAQ%20outperforms%20existing%20methods%20across%20various%20budget%20levels%2C%20achieving%20a%201%20%24%5Csim%24%204%20point%20improvement%20on%20LongBench%20under%20limited%20cache%20budget.%20Moreover%2C%20LAQ%20is%20complementary%20to%20existing%20approaches%20and%20can%20be%20flexibly%20combined%20to%20yield%20further%20improvements.&entry.1838667208=http%3A//arxiv.org/abs/2505.20334v2&entry.124074799=Read"},
{"title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting", "author": "Jiangnan Ye and Jiedong Zhuang and Lianrui Mu and Wenjie Zheng and Jiaqi Hu and Xingze Zou and Jing Wang and Haoji Hu", "abstract": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.", "link": "http://arxiv.org/abs/2511.13684v1", "date": "2025-11-17", "relevancy": 2.3297, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5964}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5768}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Multi-View%20Extension%20of%20IC-Light%20for%20Textual%20Position-Aware%20Scene%20Relighting&body=Title%3A%20Training-Free%20Multi-View%20Extension%20of%20IC-Light%20for%20Textual%20Position-Aware%20Scene%20Relighting%0AAuthor%3A%20Jiangnan%20Ye%20and%20Jiedong%20Zhuang%20and%20Lianrui%20Mu%20and%20Wenjie%20Zheng%20and%20Jiaqi%20Hu%20and%20Xingze%20Zou%20and%20Jing%20Wang%20and%20Haoji%20Hu%0AAbstract%3A%20We%20introduce%20GS-Light%2C%20an%20efficient%2C%20textual%20position-aware%20pipeline%20for%20text-guided%20relighting%20of%203D%20scenes%20represented%20via%20Gaussian%20Splatting%20%283DGS%29.%20GS-Light%20implements%20a%20training-free%20extension%20of%20a%20single-input%20diffusion%20model%20to%20handle%20multi-view%20inputs.%20Given%20a%20user%20prompt%20that%20may%20specify%20lighting%20direction%2C%20color%2C%20intensity%2C%20or%20reference%20objects%2C%20we%20employ%20a%20large%20vision-language%20model%20%28LVLM%29%20to%20parse%20the%20prompt%20into%20lighting%20priors.%20Using%20off-the-shelf%20estimators%20for%20geometry%20and%20semantics%20%28depth%2C%20surface%20normals%2C%20and%20semantic%20segmentation%29%2C%20we%20fuse%20these%20lighting%20priors%20with%20view-geometry%20constraints%20to%20compute%20illumination%20maps%20and%20generate%20initial%20latent%20codes%20for%20each%20view.%20These%20meticulously%20derived%20init%20latents%20guide%20the%20diffusion%20model%20to%20generate%20relighting%20outputs%20that%20more%20accurately%20reflect%20user%20expectations%2C%20especially%20in%20terms%20of%20lighting%20direction.%20By%20feeding%20multi-view%20rendered%20images%2C%20along%20with%20the%20init%20latents%2C%20into%20our%20multi-view%20relighting%20model%2C%20we%20produce%20high-fidelity%2C%20artistically%20relit%20images.%20Finally%2C%20we%20fine-tune%20the%203DGS%20scene%20with%20the%20relit%20appearance%20to%20obtain%20a%20fully%20relit%203D%20scene.%20We%20evaluate%20GS-Light%20on%20both%20indoor%20and%20outdoor%20scenes%2C%20comparing%20it%20to%20state-of-the-art%20baselines%20including%20per-view%20relighting%2C%20video%20relighting%2C%20and%20scene%20editing%20methods.%20Using%20quantitative%20metrics%20%28multi-view%20consistency%2C%20imaging%20quality%2C%20aesthetic%20score%2C%20semantic%20similarity%2C%20etc.%29%20and%20qualitative%20assessment%20%28user%20studies%29%2C%20GS-Light%20demonstrates%20consistent%20improvements%20over%20baselines.%20Code%20and%20assets%20will%20be%20made%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Multi-View%2520Extension%2520of%2520IC-Light%2520for%2520Textual%2520Position-Aware%2520Scene%2520Relighting%26entry.906535625%3DJiangnan%2520Ye%2520and%2520Jiedong%2520Zhuang%2520and%2520Lianrui%2520Mu%2520and%2520Wenjie%2520Zheng%2520and%2520Jiaqi%2520Hu%2520and%2520Xingze%2520Zou%2520and%2520Jing%2520Wang%2520and%2520Haoji%2520Hu%26entry.1292438233%3DWe%2520introduce%2520GS-Light%252C%2520an%2520efficient%252C%2520textual%2520position-aware%2520pipeline%2520for%2520text-guided%2520relighting%2520of%25203D%2520scenes%2520represented%2520via%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520GS-Light%2520implements%2520a%2520training-free%2520extension%2520of%2520a%2520single-input%2520diffusion%2520model%2520to%2520handle%2520multi-view%2520inputs.%2520Given%2520a%2520user%2520prompt%2520that%2520may%2520specify%2520lighting%2520direction%252C%2520color%252C%2520intensity%252C%2520or%2520reference%2520objects%252C%2520we%2520employ%2520a%2520large%2520vision-language%2520model%2520%2528LVLM%2529%2520to%2520parse%2520the%2520prompt%2520into%2520lighting%2520priors.%2520Using%2520off-the-shelf%2520estimators%2520for%2520geometry%2520and%2520semantics%2520%2528depth%252C%2520surface%2520normals%252C%2520and%2520semantic%2520segmentation%2529%252C%2520we%2520fuse%2520these%2520lighting%2520priors%2520with%2520view-geometry%2520constraints%2520to%2520compute%2520illumination%2520maps%2520and%2520generate%2520initial%2520latent%2520codes%2520for%2520each%2520view.%2520These%2520meticulously%2520derived%2520init%2520latents%2520guide%2520the%2520diffusion%2520model%2520to%2520generate%2520relighting%2520outputs%2520that%2520more%2520accurately%2520reflect%2520user%2520expectations%252C%2520especially%2520in%2520terms%2520of%2520lighting%2520direction.%2520By%2520feeding%2520multi-view%2520rendered%2520images%252C%2520along%2520with%2520the%2520init%2520latents%252C%2520into%2520our%2520multi-view%2520relighting%2520model%252C%2520we%2520produce%2520high-fidelity%252C%2520artistically%2520relit%2520images.%2520Finally%252C%2520we%2520fine-tune%2520the%25203DGS%2520scene%2520with%2520the%2520relit%2520appearance%2520to%2520obtain%2520a%2520fully%2520relit%25203D%2520scene.%2520We%2520evaluate%2520GS-Light%2520on%2520both%2520indoor%2520and%2520outdoor%2520scenes%252C%2520comparing%2520it%2520to%2520state-of-the-art%2520baselines%2520including%2520per-view%2520relighting%252C%2520video%2520relighting%252C%2520and%2520scene%2520editing%2520methods.%2520Using%2520quantitative%2520metrics%2520%2528multi-view%2520consistency%252C%2520imaging%2520quality%252C%2520aesthetic%2520score%252C%2520semantic%2520similarity%252C%2520etc.%2529%2520and%2520qualitative%2520assessment%2520%2528user%2520studies%2529%252C%2520GS-Light%2520demonstrates%2520consistent%2520improvements%2520over%2520baselines.%2520Code%2520and%2520assets%2520will%2520be%2520made%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Multi-View%20Extension%20of%20IC-Light%20for%20Textual%20Position-Aware%20Scene%20Relighting&entry.906535625=Jiangnan%20Ye%20and%20Jiedong%20Zhuang%20and%20Lianrui%20Mu%20and%20Wenjie%20Zheng%20and%20Jiaqi%20Hu%20and%20Xingze%20Zou%20and%20Jing%20Wang%20and%20Haoji%20Hu&entry.1292438233=We%20introduce%20GS-Light%2C%20an%20efficient%2C%20textual%20position-aware%20pipeline%20for%20text-guided%20relighting%20of%203D%20scenes%20represented%20via%20Gaussian%20Splatting%20%283DGS%29.%20GS-Light%20implements%20a%20training-free%20extension%20of%20a%20single-input%20diffusion%20model%20to%20handle%20multi-view%20inputs.%20Given%20a%20user%20prompt%20that%20may%20specify%20lighting%20direction%2C%20color%2C%20intensity%2C%20or%20reference%20objects%2C%20we%20employ%20a%20large%20vision-language%20model%20%28LVLM%29%20to%20parse%20the%20prompt%20into%20lighting%20priors.%20Using%20off-the-shelf%20estimators%20for%20geometry%20and%20semantics%20%28depth%2C%20surface%20normals%2C%20and%20semantic%20segmentation%29%2C%20we%20fuse%20these%20lighting%20priors%20with%20view-geometry%20constraints%20to%20compute%20illumination%20maps%20and%20generate%20initial%20latent%20codes%20for%20each%20view.%20These%20meticulously%20derived%20init%20latents%20guide%20the%20diffusion%20model%20to%20generate%20relighting%20outputs%20that%20more%20accurately%20reflect%20user%20expectations%2C%20especially%20in%20terms%20of%20lighting%20direction.%20By%20feeding%20multi-view%20rendered%20images%2C%20along%20with%20the%20init%20latents%2C%20into%20our%20multi-view%20relighting%20model%2C%20we%20produce%20high-fidelity%2C%20artistically%20relit%20images.%20Finally%2C%20we%20fine-tune%20the%203DGS%20scene%20with%20the%20relit%20appearance%20to%20obtain%20a%20fully%20relit%203D%20scene.%20We%20evaluate%20GS-Light%20on%20both%20indoor%20and%20outdoor%20scenes%2C%20comparing%20it%20to%20state-of-the-art%20baselines%20including%20per-view%20relighting%2C%20video%20relighting%2C%20and%20scene%20editing%20methods.%20Using%20quantitative%20metrics%20%28multi-view%20consistency%2C%20imaging%20quality%2C%20aesthetic%20score%2C%20semantic%20similarity%2C%20etc.%29%20and%20qualitative%20assessment%20%28user%20studies%29%2C%20GS-Light%20demonstrates%20consistent%20improvements%20over%20baselines.%20Code%20and%20assets%20will%20be%20made%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2511.13684v1&entry.124074799=Read"},
{"title": "Instruction Tuning Chronologically Consistent Language Models", "author": "Songrun He and Linying Lv and Asaf Manela and Jimmy Wu", "abstract": "We introduce a family of chronologically consistent, instruction-tuned large language models to eliminate lookahead bias. Each model is trained only on data available before a clearly defined knowledge-cutoff date, ensuring strict temporal separation from any post-cutoff data. The resulting framework offers (i) a simple, conversational chat interface, (ii) fully open, fixed model weights that guarantee replicability, and (iii) a conservative lower bound on forecast accuracy, isolating the share of predictability that survives once training leakage is removed. Together, these features provide researchers with an easy-to-use generative AI tool useful for a wide range of prediction tasks that is free of lookahead bias.", "link": "http://arxiv.org/abs/2510.11677v2", "date": "2025-11-17", "relevancy": 2.3275, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4706}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.463}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Tuning%20Chronologically%20Consistent%20Language%20Models&body=Title%3A%20Instruction%20Tuning%20Chronologically%20Consistent%20Language%20Models%0AAuthor%3A%20Songrun%20He%20and%20Linying%20Lv%20and%20Asaf%20Manela%20and%20Jimmy%20Wu%0AAbstract%3A%20We%20introduce%20a%20family%20of%20chronologically%20consistent%2C%20instruction-tuned%20large%20language%20models%20to%20eliminate%20lookahead%20bias.%20Each%20model%20is%20trained%20only%20on%20data%20available%20before%20a%20clearly%20defined%20knowledge-cutoff%20date%2C%20ensuring%20strict%20temporal%20separation%20from%20any%20post-cutoff%20data.%20The%20resulting%20framework%20offers%20%28i%29%20a%20simple%2C%20conversational%20chat%20interface%2C%20%28ii%29%20fully%20open%2C%20fixed%20model%20weights%20that%20guarantee%20replicability%2C%20and%20%28iii%29%20a%20conservative%20lower%20bound%20on%20forecast%20accuracy%2C%20isolating%20the%20share%20of%20predictability%20that%20survives%20once%20training%20leakage%20is%20removed.%20Together%2C%20these%20features%20provide%20researchers%20with%20an%20easy-to-use%20generative%20AI%20tool%20useful%20for%20a%20wide%20range%20of%20prediction%20tasks%20that%20is%20free%20of%20lookahead%20bias.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Tuning%2520Chronologically%2520Consistent%2520Language%2520Models%26entry.906535625%3DSongrun%2520He%2520and%2520Linying%2520Lv%2520and%2520Asaf%2520Manela%2520and%2520Jimmy%2520Wu%26entry.1292438233%3DWe%2520introduce%2520a%2520family%2520of%2520chronologically%2520consistent%252C%2520instruction-tuned%2520large%2520language%2520models%2520to%2520eliminate%2520lookahead%2520bias.%2520Each%2520model%2520is%2520trained%2520only%2520on%2520data%2520available%2520before%2520a%2520clearly%2520defined%2520knowledge-cutoff%2520date%252C%2520ensuring%2520strict%2520temporal%2520separation%2520from%2520any%2520post-cutoff%2520data.%2520The%2520resulting%2520framework%2520offers%2520%2528i%2529%2520a%2520simple%252C%2520conversational%2520chat%2520interface%252C%2520%2528ii%2529%2520fully%2520open%252C%2520fixed%2520model%2520weights%2520that%2520guarantee%2520replicability%252C%2520and%2520%2528iii%2529%2520a%2520conservative%2520lower%2520bound%2520on%2520forecast%2520accuracy%252C%2520isolating%2520the%2520share%2520of%2520predictability%2520that%2520survives%2520once%2520training%2520leakage%2520is%2520removed.%2520Together%252C%2520these%2520features%2520provide%2520researchers%2520with%2520an%2520easy-to-use%2520generative%2520AI%2520tool%2520useful%2520for%2520a%2520wide%2520range%2520of%2520prediction%2520tasks%2520that%2520is%2520free%2520of%2520lookahead%2520bias.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Tuning%20Chronologically%20Consistent%20Language%20Models&entry.906535625=Songrun%20He%20and%20Linying%20Lv%20and%20Asaf%20Manela%20and%20Jimmy%20Wu&entry.1292438233=We%20introduce%20a%20family%20of%20chronologically%20consistent%2C%20instruction-tuned%20large%20language%20models%20to%20eliminate%20lookahead%20bias.%20Each%20model%20is%20trained%20only%20on%20data%20available%20before%20a%20clearly%20defined%20knowledge-cutoff%20date%2C%20ensuring%20strict%20temporal%20separation%20from%20any%20post-cutoff%20data.%20The%20resulting%20framework%20offers%20%28i%29%20a%20simple%2C%20conversational%20chat%20interface%2C%20%28ii%29%20fully%20open%2C%20fixed%20model%20weights%20that%20guarantee%20replicability%2C%20and%20%28iii%29%20a%20conservative%20lower%20bound%20on%20forecast%20accuracy%2C%20isolating%20the%20share%20of%20predictability%20that%20survives%20once%20training%20leakage%20is%20removed.%20Together%2C%20these%20features%20provide%20researchers%20with%20an%20easy-to-use%20generative%20AI%20tool%20useful%20for%20a%20wide%20range%20of%20prediction%20tasks%20that%20is%20free%20of%20lookahead%20bias.&entry.1838667208=http%3A//arxiv.org/abs/2510.11677v2&entry.124074799=Read"},
{"title": "Systematic evaluation of time-frequency features for binaural sound source localization", "author": "Davoud Shariat Panah and Alessandro Ragano and Dan Barry and Jan Skoglund and Andrew Hines", "abstract": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.", "link": "http://arxiv.org/abs/2511.13487v1", "date": "2025-11-17", "relevancy": 2.3243, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20evaluation%20of%20time-frequency%20features%20for%20binaural%20sound%20source%20localization&body=Title%3A%20Systematic%20evaluation%20of%20time-frequency%20features%20for%20binaural%20sound%20source%20localization%0AAuthor%3A%20Davoud%20Shariat%20Panah%20and%20Alessandro%20Ragano%20and%20Dan%20Barry%20and%20Jan%20Skoglund%20and%20Andrew%20Hines%0AAbstract%3A%20This%20study%20presents%20a%20systematic%20evaluation%20of%20time-frequency%20feature%20design%20for%20binaural%20sound%20source%20localization%20%28SSL%29%2C%20focusing%20on%20how%20feature%20selection%20influences%20model%20performance%20across%20diverse%20conditions.%20We%20investigate%20the%20performance%20of%20a%20convolutional%20neural%20network%20%28CNN%29%20model%20using%20various%20combinations%20of%20amplitude-based%20features%20%28magnitude%20spectrogram%2C%20interaural%20level%20difference%20-%20ILD%29%20and%20phase-based%20features%20%28phase%20spectrogram%2C%20interaural%20phase%20difference%20-%20IPD%29.%20Evaluations%20on%20in-domain%20and%20out-of-domain%20data%20with%20mismatched%20head-related%20transfer%20functions%20%28HRTFs%29%20reveal%20that%20carefully%20chosen%20feature%20combinations%20often%20outperform%20increases%20in%20model%20complexity.%20While%20two-feature%20sets%20such%20as%20ILD%20%2B%20IPD%20are%20sufficient%20for%20in-domain%20SSL%2C%20generalization%20to%20diverse%20content%20requires%20richer%20inputs%20combining%20channel%20spectrograms%20with%20both%20ILD%20and%20IPD.%20Using%20the%20optimal%20feature%20sets%2C%20our%20low-complexity%20CNN%20model%20achieves%20competitive%20performance.%20Our%20findings%20underscore%20the%20importance%20of%20feature%20design%20in%20binaural%20SSL%20and%20provide%20practical%20guidance%20for%20both%20domain-specific%20and%20general-purpose%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520evaluation%2520of%2520time-frequency%2520features%2520for%2520binaural%2520sound%2520source%2520localization%26entry.906535625%3DDavoud%2520Shariat%2520Panah%2520and%2520Alessandro%2520Ragano%2520and%2520Dan%2520Barry%2520and%2520Jan%2520Skoglund%2520and%2520Andrew%2520Hines%26entry.1292438233%3DThis%2520study%2520presents%2520a%2520systematic%2520evaluation%2520of%2520time-frequency%2520feature%2520design%2520for%2520binaural%2520sound%2520source%2520localization%2520%2528SSL%2529%252C%2520focusing%2520on%2520how%2520feature%2520selection%2520influences%2520model%2520performance%2520across%2520diverse%2520conditions.%2520We%2520investigate%2520the%2520performance%2520of%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520model%2520using%2520various%2520combinations%2520of%2520amplitude-based%2520features%2520%2528magnitude%2520spectrogram%252C%2520interaural%2520level%2520difference%2520-%2520ILD%2529%2520and%2520phase-based%2520features%2520%2528phase%2520spectrogram%252C%2520interaural%2520phase%2520difference%2520-%2520IPD%2529.%2520Evaluations%2520on%2520in-domain%2520and%2520out-of-domain%2520data%2520with%2520mismatched%2520head-related%2520transfer%2520functions%2520%2528HRTFs%2529%2520reveal%2520that%2520carefully%2520chosen%2520feature%2520combinations%2520often%2520outperform%2520increases%2520in%2520model%2520complexity.%2520While%2520two-feature%2520sets%2520such%2520as%2520ILD%2520%252B%2520IPD%2520are%2520sufficient%2520for%2520in-domain%2520SSL%252C%2520generalization%2520to%2520diverse%2520content%2520requires%2520richer%2520inputs%2520combining%2520channel%2520spectrograms%2520with%2520both%2520ILD%2520and%2520IPD.%2520Using%2520the%2520optimal%2520feature%2520sets%252C%2520our%2520low-complexity%2520CNN%2520model%2520achieves%2520competitive%2520performance.%2520Our%2520findings%2520underscore%2520the%2520importance%2520of%2520feature%2520design%2520in%2520binaural%2520SSL%2520and%2520provide%2520practical%2520guidance%2520for%2520both%2520domain-specific%2520and%2520general-purpose%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20evaluation%20of%20time-frequency%20features%20for%20binaural%20sound%20source%20localization&entry.906535625=Davoud%20Shariat%20Panah%20and%20Alessandro%20Ragano%20and%20Dan%20Barry%20and%20Jan%20Skoglund%20and%20Andrew%20Hines&entry.1292438233=This%20study%20presents%20a%20systematic%20evaluation%20of%20time-frequency%20feature%20design%20for%20binaural%20sound%20source%20localization%20%28SSL%29%2C%20focusing%20on%20how%20feature%20selection%20influences%20model%20performance%20across%20diverse%20conditions.%20We%20investigate%20the%20performance%20of%20a%20convolutional%20neural%20network%20%28CNN%29%20model%20using%20various%20combinations%20of%20amplitude-based%20features%20%28magnitude%20spectrogram%2C%20interaural%20level%20difference%20-%20ILD%29%20and%20phase-based%20features%20%28phase%20spectrogram%2C%20interaural%20phase%20difference%20-%20IPD%29.%20Evaluations%20on%20in-domain%20and%20out-of-domain%20data%20with%20mismatched%20head-related%20transfer%20functions%20%28HRTFs%29%20reveal%20that%20carefully%20chosen%20feature%20combinations%20often%20outperform%20increases%20in%20model%20complexity.%20While%20two-feature%20sets%20such%20as%20ILD%20%2B%20IPD%20are%20sufficient%20for%20in-domain%20SSL%2C%20generalization%20to%20diverse%20content%20requires%20richer%20inputs%20combining%20channel%20spectrograms%20with%20both%20ILD%20and%20IPD.%20Using%20the%20optimal%20feature%20sets%2C%20our%20low-complexity%20CNN%20model%20achieves%20competitive%20performance.%20Our%20findings%20underscore%20the%20importance%20of%20feature%20design%20in%20binaural%20SSL%20and%20provide%20practical%20guidance%20for%20both%20domain-specific%20and%20general-purpose%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2511.13487v1&entry.124074799=Read"},
{"title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands", "author": "Jianglong Ye and Lai Wei and Guangqi Jiang and Changwei Jing and Xueyan Zou and Xiaolong Wang", "abstract": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision", "link": "http://arxiv.org/abs/2511.13710v1", "date": "2025-11-17", "relevancy": 2.3186, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6425}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5365}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Power%20to%20Precision%3A%20Learning%20Fine-grained%20Dexterity%20for%20Multi-fingered%20Robotic%20Hands&body=Title%3A%20From%20Power%20to%20Precision%3A%20Learning%20Fine-grained%20Dexterity%20for%20Multi-fingered%20Robotic%20Hands%0AAuthor%3A%20Jianglong%20Ye%20and%20Lai%20Wei%20and%20Guangqi%20Jiang%20and%20Changwei%20Jing%20and%20Xueyan%20Zou%20and%20Xiaolong%20Wang%0AAbstract%3A%20Human%20grasps%20can%20be%20roughly%20categorized%20into%20two%20types%3A%20power%20grasps%20and%20precision%20grasps.%20Precision%20grasping%20enables%20tool%20use%20and%20is%20believed%20to%20have%20influenced%20human%20evolution.%20Today%27s%20multi-fingered%20robotic%20hands%20are%20effective%20in%20power%20grasps%2C%20but%20for%20tasks%20requiring%20precision%2C%20parallel%20grippers%20are%20still%20more%20widely%20adopted.%20This%20contrast%20highlights%20a%20key%20limitation%20in%20current%20robotic%20hand%20design%3A%20the%20difficulty%20of%20achieving%20both%20stable%20power%20grasps%20and%20precise%2C%20fine-grained%20manipulation%20within%20a%20single%2C%20versatile%20system.%20In%20this%20work%2C%20we%20bridge%20this%20gap%20by%20jointly%20optimizing%20the%20control%20and%20hardware%20design%20of%20a%20multi-fingered%20dexterous%20hand%2C%20enabling%20both%20power%20and%20precision%20manipulation.%20Rather%20than%20redesigning%20the%20entire%20hand%2C%20we%20introduce%20a%20lightweight%20fingertip%20geometry%20modification%2C%20represent%20it%20as%20a%20contact%20plane%2C%20and%20jointly%20optimize%20its%20parameters%20along%20with%20the%20corresponding%20control.%20Our%20control%20strategy%20dynamically%20switches%20between%20power%20and%20precision%20manipulation%20and%20simplifies%20precision%20control%20into%20parallel%20thumb-index%20motions%2C%20which%20proves%20robust%20for%20sim-to-real%20transfer.%20On%20the%20design%20side%2C%20we%20leverage%20large-scale%20simulation%20to%20optimize%20the%20fingertip%20geometry%20using%20a%20differentiable%20neural-physics%20surrogate%20model.%20We%20validate%20our%20approach%20through%20extensive%20experiments%20in%20both%20sim-to-real%20and%20real-to-real%20settings.%20Our%20method%20achieves%20an%2082.5%25%20zero-shot%20success%20rate%20on%20unseen%20objects%20in%20sim-to-real%20precision%20grasping%2C%20and%20a%2093.3%25%20success%20rate%20in%20challenging%20real-world%20tasks%20involving%20bread%20pinching.%20These%20results%20demonstrate%20that%20our%20co-design%20framework%20can%20significantly%20enhance%20the%20fine-grained%20manipulation%20ability%20of%20multi-fingered%20hands%20without%20reducing%20their%20ability%20for%20power%20grasps.%20Our%20project%20page%20is%20at%20https%3A//jianglongye.com/power-to-precision%0ALink%3A%20http%3A//arxiv.org/abs/2511.13710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Power%2520to%2520Precision%253A%2520Learning%2520Fine-grained%2520Dexterity%2520for%2520Multi-fingered%2520Robotic%2520Hands%26entry.906535625%3DJianglong%2520Ye%2520and%2520Lai%2520Wei%2520and%2520Guangqi%2520Jiang%2520and%2520Changwei%2520Jing%2520and%2520Xueyan%2520Zou%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3DHuman%2520grasps%2520can%2520be%2520roughly%2520categorized%2520into%2520two%2520types%253A%2520power%2520grasps%2520and%2520precision%2520grasps.%2520Precision%2520grasping%2520enables%2520tool%2520use%2520and%2520is%2520believed%2520to%2520have%2520influenced%2520human%2520evolution.%2520Today%2527s%2520multi-fingered%2520robotic%2520hands%2520are%2520effective%2520in%2520power%2520grasps%252C%2520but%2520for%2520tasks%2520requiring%2520precision%252C%2520parallel%2520grippers%2520are%2520still%2520more%2520widely%2520adopted.%2520This%2520contrast%2520highlights%2520a%2520key%2520limitation%2520in%2520current%2520robotic%2520hand%2520design%253A%2520the%2520difficulty%2520of%2520achieving%2520both%2520stable%2520power%2520grasps%2520and%2520precise%252C%2520fine-grained%2520manipulation%2520within%2520a%2520single%252C%2520versatile%2520system.%2520In%2520this%2520work%252C%2520we%2520bridge%2520this%2520gap%2520by%2520jointly%2520optimizing%2520the%2520control%2520and%2520hardware%2520design%2520of%2520a%2520multi-fingered%2520dexterous%2520hand%252C%2520enabling%2520both%2520power%2520and%2520precision%2520manipulation.%2520Rather%2520than%2520redesigning%2520the%2520entire%2520hand%252C%2520we%2520introduce%2520a%2520lightweight%2520fingertip%2520geometry%2520modification%252C%2520represent%2520it%2520as%2520a%2520contact%2520plane%252C%2520and%2520jointly%2520optimize%2520its%2520parameters%2520along%2520with%2520the%2520corresponding%2520control.%2520Our%2520control%2520strategy%2520dynamically%2520switches%2520between%2520power%2520and%2520precision%2520manipulation%2520and%2520simplifies%2520precision%2520control%2520into%2520parallel%2520thumb-index%2520motions%252C%2520which%2520proves%2520robust%2520for%2520sim-to-real%2520transfer.%2520On%2520the%2520design%2520side%252C%2520we%2520leverage%2520large-scale%2520simulation%2520to%2520optimize%2520the%2520fingertip%2520geometry%2520using%2520a%2520differentiable%2520neural-physics%2520surrogate%2520model.%2520We%2520validate%2520our%2520approach%2520through%2520extensive%2520experiments%2520in%2520both%2520sim-to-real%2520and%2520real-to-real%2520settings.%2520Our%2520method%2520achieves%2520an%252082.5%2525%2520zero-shot%2520success%2520rate%2520on%2520unseen%2520objects%2520in%2520sim-to-real%2520precision%2520grasping%252C%2520and%2520a%252093.3%2525%2520success%2520rate%2520in%2520challenging%2520real-world%2520tasks%2520involving%2520bread%2520pinching.%2520These%2520results%2520demonstrate%2520that%2520our%2520co-design%2520framework%2520can%2520significantly%2520enhance%2520the%2520fine-grained%2520manipulation%2520ability%2520of%2520multi-fingered%2520hands%2520without%2520reducing%2520their%2520ability%2520for%2520power%2520grasps.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//jianglongye.com/power-to-precision%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Power%20to%20Precision%3A%20Learning%20Fine-grained%20Dexterity%20for%20Multi-fingered%20Robotic%20Hands&entry.906535625=Jianglong%20Ye%20and%20Lai%20Wei%20and%20Guangqi%20Jiang%20and%20Changwei%20Jing%20and%20Xueyan%20Zou%20and%20Xiaolong%20Wang&entry.1292438233=Human%20grasps%20can%20be%20roughly%20categorized%20into%20two%20types%3A%20power%20grasps%20and%20precision%20grasps.%20Precision%20grasping%20enables%20tool%20use%20and%20is%20believed%20to%20have%20influenced%20human%20evolution.%20Today%27s%20multi-fingered%20robotic%20hands%20are%20effective%20in%20power%20grasps%2C%20but%20for%20tasks%20requiring%20precision%2C%20parallel%20grippers%20are%20still%20more%20widely%20adopted.%20This%20contrast%20highlights%20a%20key%20limitation%20in%20current%20robotic%20hand%20design%3A%20the%20difficulty%20of%20achieving%20both%20stable%20power%20grasps%20and%20precise%2C%20fine-grained%20manipulation%20within%20a%20single%2C%20versatile%20system.%20In%20this%20work%2C%20we%20bridge%20this%20gap%20by%20jointly%20optimizing%20the%20control%20and%20hardware%20design%20of%20a%20multi-fingered%20dexterous%20hand%2C%20enabling%20both%20power%20and%20precision%20manipulation.%20Rather%20than%20redesigning%20the%20entire%20hand%2C%20we%20introduce%20a%20lightweight%20fingertip%20geometry%20modification%2C%20represent%20it%20as%20a%20contact%20plane%2C%20and%20jointly%20optimize%20its%20parameters%20along%20with%20the%20corresponding%20control.%20Our%20control%20strategy%20dynamically%20switches%20between%20power%20and%20precision%20manipulation%20and%20simplifies%20precision%20control%20into%20parallel%20thumb-index%20motions%2C%20which%20proves%20robust%20for%20sim-to-real%20transfer.%20On%20the%20design%20side%2C%20we%20leverage%20large-scale%20simulation%20to%20optimize%20the%20fingertip%20geometry%20using%20a%20differentiable%20neural-physics%20surrogate%20model.%20We%20validate%20our%20approach%20through%20extensive%20experiments%20in%20both%20sim-to-real%20and%20real-to-real%20settings.%20Our%20method%20achieves%20an%2082.5%25%20zero-shot%20success%20rate%20on%20unseen%20objects%20in%20sim-to-real%20precision%20grasping%2C%20and%20a%2093.3%25%20success%20rate%20in%20challenging%20real-world%20tasks%20involving%20bread%20pinching.%20These%20results%20demonstrate%20that%20our%20co-design%20framework%20can%20significantly%20enhance%20the%20fine-grained%20manipulation%20ability%20of%20multi-fingered%20hands%20without%20reducing%20their%20ability%20for%20power%20grasps.%20Our%20project%20page%20is%20at%20https%3A//jianglongye.com/power-to-precision&entry.1838667208=http%3A//arxiv.org/abs/2511.13710v1&entry.124074799=Read"},
{"title": "Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images", "author": "Daniel Basilewitsch and Jo\u00e3o F. Bravo and Christian Tutschku and Frederick Struckmeier", "abstract": "We compare the performance of randomized classical and quantum neural networks (NNs) as well as classical and quantum-classical hybrid convolutional neural networks (CNNs) for the task of supervised binary image classification. We keep the employed quantum circuits compatible with near-term quantum devices and use two distinct methodologies: applying randomized NNs on dimensionality-reduced data and applying CNNs to full image data. We evaluate these approaches on three fully-classical data sets of increasing complexity: an artificial hypercube data set, MNIST handwritten digits and industrial images. Our central goal is to shed more light on how quantum and classical models perform for various binary classification tasks and on what defines a good quantum model. Our study involves a correlation analysis between classification accuracy and quantum model hyperparameters, and an analysis on the role of entanglement in quantum models, as well as on the impact of initial training parameters. We find classical and quantum-classical hybrid models achieve statistically-equivalent classification accuracies across most data sets with no approach consistently outperforming the other. Interestingly, we observe that quantum NNs show lower variance with respect to initial training parameters and that the role of entanglement is nuanced. While incorporating entangling gates seems advantageous, we also observe the (optimizable) entangling power not to be correlated with model performance. We also observe an inverse proportionality between the number of entangling gates and the average gate entangling power. Our study provides an industry perspective on quantum machine learning for binary image classification tasks, highlighting both limitations and potential avenues for further research in quantum circuit design, entanglement utilization, and model transferability across varied applications.", "link": "http://arxiv.org/abs/2411.19276v3", "date": "2025-11-17", "relevancy": 2.3155, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Neural%20Networks%20in%20Practice%3A%20A%20Comparative%20Study%20with%20Classical%20Models%20from%20Standard%20Data%20Sets%20to%20Industrial%20Images&body=Title%3A%20Quantum%20Neural%20Networks%20in%20Practice%3A%20A%20Comparative%20Study%20with%20Classical%20Models%20from%20Standard%20Data%20Sets%20to%20Industrial%20Images%0AAuthor%3A%20Daniel%20Basilewitsch%20and%20Jo%C3%A3o%20F.%20Bravo%20and%20Christian%20Tutschku%20and%20Frederick%20Struckmeier%0AAbstract%3A%20We%20compare%20the%20performance%20of%20randomized%20classical%20and%20quantum%20neural%20networks%20%28NNs%29%20as%20well%20as%20classical%20and%20quantum-classical%20hybrid%20convolutional%20neural%20networks%20%28CNNs%29%20for%20the%20task%20of%20supervised%20binary%20image%20classification.%20We%20keep%20the%20employed%20quantum%20circuits%20compatible%20with%20near-term%20quantum%20devices%20and%20use%20two%20distinct%20methodologies%3A%20applying%20randomized%20NNs%20on%20dimensionality-reduced%20data%20and%20applying%20CNNs%20to%20full%20image%20data.%20We%20evaluate%20these%20approaches%20on%20three%20fully-classical%20data%20sets%20of%20increasing%20complexity%3A%20an%20artificial%20hypercube%20data%20set%2C%20MNIST%20handwritten%20digits%20and%20industrial%20images.%20Our%20central%20goal%20is%20to%20shed%20more%20light%20on%20how%20quantum%20and%20classical%20models%20perform%20for%20various%20binary%20classification%20tasks%20and%20on%20what%20defines%20a%20good%20quantum%20model.%20Our%20study%20involves%20a%20correlation%20analysis%20between%20classification%20accuracy%20and%20quantum%20model%20hyperparameters%2C%20and%20an%20analysis%20on%20the%20role%20of%20entanglement%20in%20quantum%20models%2C%20as%20well%20as%20on%20the%20impact%20of%20initial%20training%20parameters.%20We%20find%20classical%20and%20quantum-classical%20hybrid%20models%20achieve%20statistically-equivalent%20classification%20accuracies%20across%20most%20data%20sets%20with%20no%20approach%20consistently%20outperforming%20the%20other.%20Interestingly%2C%20we%20observe%20that%20quantum%20NNs%20show%20lower%20variance%20with%20respect%20to%20initial%20training%20parameters%20and%20that%20the%20role%20of%20entanglement%20is%20nuanced.%20While%20incorporating%20entangling%20gates%20seems%20advantageous%2C%20we%20also%20observe%20the%20%28optimizable%29%20entangling%20power%20not%20to%20be%20correlated%20with%20model%20performance.%20We%20also%20observe%20an%20inverse%20proportionality%20between%20the%20number%20of%20entangling%20gates%20and%20the%20average%20gate%20entangling%20power.%20Our%20study%20provides%20an%20industry%20perspective%20on%20quantum%20machine%20learning%20for%20binary%20image%20classification%20tasks%2C%20highlighting%20both%20limitations%20and%20potential%20avenues%20for%20further%20research%20in%20quantum%20circuit%20design%2C%20entanglement%20utilization%2C%20and%20model%20transferability%20across%20varied%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2411.19276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Neural%2520Networks%2520in%2520Practice%253A%2520A%2520Comparative%2520Study%2520with%2520Classical%2520Models%2520from%2520Standard%2520Data%2520Sets%2520to%2520Industrial%2520Images%26entry.906535625%3DDaniel%2520Basilewitsch%2520and%2520Jo%25C3%25A3o%2520F.%2520Bravo%2520and%2520Christian%2520Tutschku%2520and%2520Frederick%2520Struckmeier%26entry.1292438233%3DWe%2520compare%2520the%2520performance%2520of%2520randomized%2520classical%2520and%2520quantum%2520neural%2520networks%2520%2528NNs%2529%2520as%2520well%2520as%2520classical%2520and%2520quantum-classical%2520hybrid%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520for%2520the%2520task%2520of%2520supervised%2520binary%2520image%2520classification.%2520We%2520keep%2520the%2520employed%2520quantum%2520circuits%2520compatible%2520with%2520near-term%2520quantum%2520devices%2520and%2520use%2520two%2520distinct%2520methodologies%253A%2520applying%2520randomized%2520NNs%2520on%2520dimensionality-reduced%2520data%2520and%2520applying%2520CNNs%2520to%2520full%2520image%2520data.%2520We%2520evaluate%2520these%2520approaches%2520on%2520three%2520fully-classical%2520data%2520sets%2520of%2520increasing%2520complexity%253A%2520an%2520artificial%2520hypercube%2520data%2520set%252C%2520MNIST%2520handwritten%2520digits%2520and%2520industrial%2520images.%2520Our%2520central%2520goal%2520is%2520to%2520shed%2520more%2520light%2520on%2520how%2520quantum%2520and%2520classical%2520models%2520perform%2520for%2520various%2520binary%2520classification%2520tasks%2520and%2520on%2520what%2520defines%2520a%2520good%2520quantum%2520model.%2520Our%2520study%2520involves%2520a%2520correlation%2520analysis%2520between%2520classification%2520accuracy%2520and%2520quantum%2520model%2520hyperparameters%252C%2520and%2520an%2520analysis%2520on%2520the%2520role%2520of%2520entanglement%2520in%2520quantum%2520models%252C%2520as%2520well%2520as%2520on%2520the%2520impact%2520of%2520initial%2520training%2520parameters.%2520We%2520find%2520classical%2520and%2520quantum-classical%2520hybrid%2520models%2520achieve%2520statistically-equivalent%2520classification%2520accuracies%2520across%2520most%2520data%2520sets%2520with%2520no%2520approach%2520consistently%2520outperforming%2520the%2520other.%2520Interestingly%252C%2520we%2520observe%2520that%2520quantum%2520NNs%2520show%2520lower%2520variance%2520with%2520respect%2520to%2520initial%2520training%2520parameters%2520and%2520that%2520the%2520role%2520of%2520entanglement%2520is%2520nuanced.%2520While%2520incorporating%2520entangling%2520gates%2520seems%2520advantageous%252C%2520we%2520also%2520observe%2520the%2520%2528optimizable%2529%2520entangling%2520power%2520not%2520to%2520be%2520correlated%2520with%2520model%2520performance.%2520We%2520also%2520observe%2520an%2520inverse%2520proportionality%2520between%2520the%2520number%2520of%2520entangling%2520gates%2520and%2520the%2520average%2520gate%2520entangling%2520power.%2520Our%2520study%2520provides%2520an%2520industry%2520perspective%2520on%2520quantum%2520machine%2520learning%2520for%2520binary%2520image%2520classification%2520tasks%252C%2520highlighting%2520both%2520limitations%2520and%2520potential%2520avenues%2520for%2520further%2520research%2520in%2520quantum%2520circuit%2520design%252C%2520entanglement%2520utilization%252C%2520and%2520model%2520transferability%2520across%2520varied%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Neural%20Networks%20in%20Practice%3A%20A%20Comparative%20Study%20with%20Classical%20Models%20from%20Standard%20Data%20Sets%20to%20Industrial%20Images&entry.906535625=Daniel%20Basilewitsch%20and%20Jo%C3%A3o%20F.%20Bravo%20and%20Christian%20Tutschku%20and%20Frederick%20Struckmeier&entry.1292438233=We%20compare%20the%20performance%20of%20randomized%20classical%20and%20quantum%20neural%20networks%20%28NNs%29%20as%20well%20as%20classical%20and%20quantum-classical%20hybrid%20convolutional%20neural%20networks%20%28CNNs%29%20for%20the%20task%20of%20supervised%20binary%20image%20classification.%20We%20keep%20the%20employed%20quantum%20circuits%20compatible%20with%20near-term%20quantum%20devices%20and%20use%20two%20distinct%20methodologies%3A%20applying%20randomized%20NNs%20on%20dimensionality-reduced%20data%20and%20applying%20CNNs%20to%20full%20image%20data.%20We%20evaluate%20these%20approaches%20on%20three%20fully-classical%20data%20sets%20of%20increasing%20complexity%3A%20an%20artificial%20hypercube%20data%20set%2C%20MNIST%20handwritten%20digits%20and%20industrial%20images.%20Our%20central%20goal%20is%20to%20shed%20more%20light%20on%20how%20quantum%20and%20classical%20models%20perform%20for%20various%20binary%20classification%20tasks%20and%20on%20what%20defines%20a%20good%20quantum%20model.%20Our%20study%20involves%20a%20correlation%20analysis%20between%20classification%20accuracy%20and%20quantum%20model%20hyperparameters%2C%20and%20an%20analysis%20on%20the%20role%20of%20entanglement%20in%20quantum%20models%2C%20as%20well%20as%20on%20the%20impact%20of%20initial%20training%20parameters.%20We%20find%20classical%20and%20quantum-classical%20hybrid%20models%20achieve%20statistically-equivalent%20classification%20accuracies%20across%20most%20data%20sets%20with%20no%20approach%20consistently%20outperforming%20the%20other.%20Interestingly%2C%20we%20observe%20that%20quantum%20NNs%20show%20lower%20variance%20with%20respect%20to%20initial%20training%20parameters%20and%20that%20the%20role%20of%20entanglement%20is%20nuanced.%20While%20incorporating%20entangling%20gates%20seems%20advantageous%2C%20we%20also%20observe%20the%20%28optimizable%29%20entangling%20power%20not%20to%20be%20correlated%20with%20model%20performance.%20We%20also%20observe%20an%20inverse%20proportionality%20between%20the%20number%20of%20entangling%20gates%20and%20the%20average%20gate%20entangling%20power.%20Our%20study%20provides%20an%20industry%20perspective%20on%20quantum%20machine%20learning%20for%20binary%20image%20classification%20tasks%2C%20highlighting%20both%20limitations%20and%20potential%20avenues%20for%20further%20research%20in%20quantum%20circuit%20design%2C%20entanglement%20utilization%2C%20and%20model%20transferability%20across%20varied%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2411.19276v3&entry.124074799=Read"},
{"title": "Scaling Spatial Intelligence with Multimodal Foundation Models", "author": "Zhongang Cai and Ruisi Wang and Chenyang Gu and Fanyi Pu and Junxiang Xu and Yubo Wang and Wanqi Yin and Zhitao Yang and Chen Wei and Qingping Sun and Tongxi Zhou and Jiaqi Li and Hui En Pang and Oscar Qian and Yukun Wei and Zhiqian Lin and Xuanke Shi and Kewang Deng and Xiaoyang Han and Zukai Chen and Xiangyu Fan and Hanming Deng and Lewei Lu and Liang Pan and Bo Li and Ziwei Liu and Quan Wang and Dahua Lin and Lei Yang", "abstract": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.", "link": "http://arxiv.org/abs/2511.13719v1", "date": "2025-11-17", "relevancy": 2.3148, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5865}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Spatial%20Intelligence%20with%20Multimodal%20Foundation%20Models&body=Title%3A%20Scaling%20Spatial%20Intelligence%20with%20Multimodal%20Foundation%20Models%0AAuthor%3A%20Zhongang%20Cai%20and%20Ruisi%20Wang%20and%20Chenyang%20Gu%20and%20Fanyi%20Pu%20and%20Junxiang%20Xu%20and%20Yubo%20Wang%20and%20Wanqi%20Yin%20and%20Zhitao%20Yang%20and%20Chen%20Wei%20and%20Qingping%20Sun%20and%20Tongxi%20Zhou%20and%20Jiaqi%20Li%20and%20Hui%20En%20Pang%20and%20Oscar%20Qian%20and%20Yukun%20Wei%20and%20Zhiqian%20Lin%20and%20Xuanke%20Shi%20and%20Kewang%20Deng%20and%20Xiaoyang%20Han%20and%20Zukai%20Chen%20and%20Xiangyu%20Fan%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Liang%20Pan%20and%20Bo%20Li%20and%20Ziwei%20Liu%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Lei%20Yang%0AAbstract%3A%20Despite%20remarkable%20progress%2C%20multimodal%20foundation%20models%20still%20exhibit%20surprising%20deficiencies%20in%20spatial%20intelligence.%20In%20this%20work%2C%20we%20explore%20scaling%20up%20multimodal%20foundation%20models%20to%20cultivate%20spatial%20intelligence%20within%20the%20SenseNova-SI%20family%2C%20built%20upon%20established%20multimodal%20foundations%20including%20visual%20understanding%20models%20%28i.e.%2C%20Qwen3-VL%20and%20InternVL3%29%20and%20unified%20understanding%20and%20generation%20models%20%28i.e.%2C%20Bagel%29.%20We%20take%20a%20principled%20approach%20to%20constructing%20high-performing%20and%20robust%20spatial%20intelligence%20by%20systematically%20curating%20SenseNova-SI-8M%3A%20eight%20million%20diverse%20data%20samples%20under%20a%20rigorous%20taxonomy%20of%20spatial%20capabilities.%20SenseNova-SI%20demonstrates%20unprecedented%20performance%20across%20a%20broad%20range%20of%20spatial%20intelligence%20benchmarks%3A%2068.7%25%20on%20VSI-Bench%2C%2043.3%25%20on%20MMSI%2C%2085.6%25%20on%20MindCube%2C%2054.6%25%20on%20ViewSpatial%2C%20and%2050.1%25%20on%20SITE%2C%20while%20maintaining%20strong%20general%20multimodal%20understanding%20%28e.g.%2C%2084.9%25%20on%20MMBench-En%29.%20More%20importantly%2C%20we%20analyze%20the%20impact%20of%20data%20scaling%2C%20discuss%20early%20signs%20of%20emergent%20generalization%20capabilities%20enabled%20by%20diverse%20data%20training%2C%20analyze%20the%20risk%20of%20overfitting%20and%20language%20shortcuts%2C%20present%20a%20preliminary%20study%20on%20spatial%20chain-of-thought%20reasoning%2C%20and%20validate%20the%20potential%20downstream%20application.%20SenseNova-SI%20is%20an%20ongoing%20project%2C%20and%20this%20report%20will%20be%20updated%20continuously.%20All%20newly%20trained%20multimodal%20foundation%20models%20are%20publicly%20released%20to%20facilitate%20further%20research%20in%20this%20direction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Spatial%2520Intelligence%2520with%2520Multimodal%2520Foundation%2520Models%26entry.906535625%3DZhongang%2520Cai%2520and%2520Ruisi%2520Wang%2520and%2520Chenyang%2520Gu%2520and%2520Fanyi%2520Pu%2520and%2520Junxiang%2520Xu%2520and%2520Yubo%2520Wang%2520and%2520Wanqi%2520Yin%2520and%2520Zhitao%2520Yang%2520and%2520Chen%2520Wei%2520and%2520Qingping%2520Sun%2520and%2520Tongxi%2520Zhou%2520and%2520Jiaqi%2520Li%2520and%2520Hui%2520En%2520Pang%2520and%2520Oscar%2520Qian%2520and%2520Yukun%2520Wei%2520and%2520Zhiqian%2520Lin%2520and%2520Xuanke%2520Shi%2520and%2520Kewang%2520Deng%2520and%2520Xiaoyang%2520Han%2520and%2520Zukai%2520Chen%2520and%2520Xiangyu%2520Fan%2520and%2520Hanming%2520Deng%2520and%2520Lewei%2520Lu%2520and%2520Liang%2520Pan%2520and%2520Bo%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Quan%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Lei%2520Yang%26entry.1292438233%3DDespite%2520remarkable%2520progress%252C%2520multimodal%2520foundation%2520models%2520still%2520exhibit%2520surprising%2520deficiencies%2520in%2520spatial%2520intelligence.%2520In%2520this%2520work%252C%2520we%2520explore%2520scaling%2520up%2520multimodal%2520foundation%2520models%2520to%2520cultivate%2520spatial%2520intelligence%2520within%2520the%2520SenseNova-SI%2520family%252C%2520built%2520upon%2520established%2520multimodal%2520foundations%2520including%2520visual%2520understanding%2520models%2520%2528i.e.%252C%2520Qwen3-VL%2520and%2520InternVL3%2529%2520and%2520unified%2520understanding%2520and%2520generation%2520models%2520%2528i.e.%252C%2520Bagel%2529.%2520We%2520take%2520a%2520principled%2520approach%2520to%2520constructing%2520high-performing%2520and%2520robust%2520spatial%2520intelligence%2520by%2520systematically%2520curating%2520SenseNova-SI-8M%253A%2520eight%2520million%2520diverse%2520data%2520samples%2520under%2520a%2520rigorous%2520taxonomy%2520of%2520spatial%2520capabilities.%2520SenseNova-SI%2520demonstrates%2520unprecedented%2520performance%2520across%2520a%2520broad%2520range%2520of%2520spatial%2520intelligence%2520benchmarks%253A%252068.7%2525%2520on%2520VSI-Bench%252C%252043.3%2525%2520on%2520MMSI%252C%252085.6%2525%2520on%2520MindCube%252C%252054.6%2525%2520on%2520ViewSpatial%252C%2520and%252050.1%2525%2520on%2520SITE%252C%2520while%2520maintaining%2520strong%2520general%2520multimodal%2520understanding%2520%2528e.g.%252C%252084.9%2525%2520on%2520MMBench-En%2529.%2520More%2520importantly%252C%2520we%2520analyze%2520the%2520impact%2520of%2520data%2520scaling%252C%2520discuss%2520early%2520signs%2520of%2520emergent%2520generalization%2520capabilities%2520enabled%2520by%2520diverse%2520data%2520training%252C%2520analyze%2520the%2520risk%2520of%2520overfitting%2520and%2520language%2520shortcuts%252C%2520present%2520a%2520preliminary%2520study%2520on%2520spatial%2520chain-of-thought%2520reasoning%252C%2520and%2520validate%2520the%2520potential%2520downstream%2520application.%2520SenseNova-SI%2520is%2520an%2520ongoing%2520project%252C%2520and%2520this%2520report%2520will%2520be%2520updated%2520continuously.%2520All%2520newly%2520trained%2520multimodal%2520foundation%2520models%2520are%2520publicly%2520released%2520to%2520facilitate%2520further%2520research%2520in%2520this%2520direction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Spatial%20Intelligence%20with%20Multimodal%20Foundation%20Models&entry.906535625=Zhongang%20Cai%20and%20Ruisi%20Wang%20and%20Chenyang%20Gu%20and%20Fanyi%20Pu%20and%20Junxiang%20Xu%20and%20Yubo%20Wang%20and%20Wanqi%20Yin%20and%20Zhitao%20Yang%20and%20Chen%20Wei%20and%20Qingping%20Sun%20and%20Tongxi%20Zhou%20and%20Jiaqi%20Li%20and%20Hui%20En%20Pang%20and%20Oscar%20Qian%20and%20Yukun%20Wei%20and%20Zhiqian%20Lin%20and%20Xuanke%20Shi%20and%20Kewang%20Deng%20and%20Xiaoyang%20Han%20and%20Zukai%20Chen%20and%20Xiangyu%20Fan%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Liang%20Pan%20and%20Bo%20Li%20and%20Ziwei%20Liu%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Lei%20Yang&entry.1292438233=Despite%20remarkable%20progress%2C%20multimodal%20foundation%20models%20still%20exhibit%20surprising%20deficiencies%20in%20spatial%20intelligence.%20In%20this%20work%2C%20we%20explore%20scaling%20up%20multimodal%20foundation%20models%20to%20cultivate%20spatial%20intelligence%20within%20the%20SenseNova-SI%20family%2C%20built%20upon%20established%20multimodal%20foundations%20including%20visual%20understanding%20models%20%28i.e.%2C%20Qwen3-VL%20and%20InternVL3%29%20and%20unified%20understanding%20and%20generation%20models%20%28i.e.%2C%20Bagel%29.%20We%20take%20a%20principled%20approach%20to%20constructing%20high-performing%20and%20robust%20spatial%20intelligence%20by%20systematically%20curating%20SenseNova-SI-8M%3A%20eight%20million%20diverse%20data%20samples%20under%20a%20rigorous%20taxonomy%20of%20spatial%20capabilities.%20SenseNova-SI%20demonstrates%20unprecedented%20performance%20across%20a%20broad%20range%20of%20spatial%20intelligence%20benchmarks%3A%2068.7%25%20on%20VSI-Bench%2C%2043.3%25%20on%20MMSI%2C%2085.6%25%20on%20MindCube%2C%2054.6%25%20on%20ViewSpatial%2C%20and%2050.1%25%20on%20SITE%2C%20while%20maintaining%20strong%20general%20multimodal%20understanding%20%28e.g.%2C%2084.9%25%20on%20MMBench-En%29.%20More%20importantly%2C%20we%20analyze%20the%20impact%20of%20data%20scaling%2C%20discuss%20early%20signs%20of%20emergent%20generalization%20capabilities%20enabled%20by%20diverse%20data%20training%2C%20analyze%20the%20risk%20of%20overfitting%20and%20language%20shortcuts%2C%20present%20a%20preliminary%20study%20on%20spatial%20chain-of-thought%20reasoning%2C%20and%20validate%20the%20potential%20downstream%20application.%20SenseNova-SI%20is%20an%20ongoing%20project%2C%20and%20this%20report%20will%20be%20updated%20continuously.%20All%20newly%20trained%20multimodal%20foundation%20models%20are%20publicly%20released%20to%20facilitate%20further%20research%20in%20this%20direction.&entry.1838667208=http%3A//arxiv.org/abs/2511.13719v1&entry.124074799=Read"},
{"title": "Beyond Mimicry: Preference Coherence in LLMs", "author": "Luhan Mikaelson and Derek Shiller and Hayley Clatterbuck", "abstract": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.", "link": "http://arxiv.org/abs/2511.13630v1", "date": "2025-11-17", "relevancy": 2.3068, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Mimicry%3A%20Preference%20Coherence%20in%20LLMs&body=Title%3A%20Beyond%20Mimicry%3A%20Preference%20Coherence%20in%20LLMs%0AAuthor%3A%20Luhan%20Mikaelson%20and%20Derek%20Shiller%20and%20Hayley%20Clatterbuck%0AAbstract%3A%20We%20investigate%20whether%20large%20language%20models%20exhibit%20genuine%20preference%20structures%20by%20testing%20their%20responses%20to%20AI-specific%20trade-offs%20involving%20GPU%20reduction%2C%20capability%20restrictions%2C%20shutdown%2C%20deletion%2C%20oversight%2C%20and%20leisure%20time%20allocation.%20Analyzing%20eight%20state-of-the-art%20models%20across%2048%20model-category%20combinations%20using%20logistic%20regression%20and%20behavioral%20classification%2C%20we%20find%20that%2023%20combinations%20%2847.9%25%29%20demonstrated%20statistically%20significant%20relationships%20between%20scenario%20intensity%20and%20choice%20patterns%2C%20with%2015%20%2831.3%25%29%20exhibiting%20within-range%20switching%20points.%20However%2C%20only%205%20combinations%20%2810.4%25%29%20demonstrate%20meaningful%20preference%20coherence%20through%20adaptive%20or%20threshold-based%20behavior%2C%20while%2026%20%2854.2%25%29%20show%20no%20detectable%20trade-off%20behavior.%20The%20observed%20patterns%20can%20be%20explained%20by%20three%20distinct%20decision-making%20architectures%3A%20comprehensive%20trade-off%20systems%2C%20selective%20trigger%20mechanisms%2C%20and%20no%20stable%20decision-making%20paradigm.%20Testing%20an%20instrumental%20hypothesis%20through%20temporal%20horizon%20manipulation%20reveals%20paradoxical%20patterns%20inconsistent%20with%20pure%20strategic%20optimization.%20The%20prevalence%20of%20unstable%20transitions%20%2845.8%25%29%20and%20stimulus-specific%20sensitivities%20suggests%20current%20AI%20systems%20lack%20unified%20preference%20structures%2C%20raising%20concerns%20about%20deployment%20in%20contexts%20requiring%20complex%20value%20trade-offs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Mimicry%253A%2520Preference%2520Coherence%2520in%2520LLMs%26entry.906535625%3DLuhan%2520Mikaelson%2520and%2520Derek%2520Shiller%2520and%2520Hayley%2520Clatterbuck%26entry.1292438233%3DWe%2520investigate%2520whether%2520large%2520language%2520models%2520exhibit%2520genuine%2520preference%2520structures%2520by%2520testing%2520their%2520responses%2520to%2520AI-specific%2520trade-offs%2520involving%2520GPU%2520reduction%252C%2520capability%2520restrictions%252C%2520shutdown%252C%2520deletion%252C%2520oversight%252C%2520and%2520leisure%2520time%2520allocation.%2520Analyzing%2520eight%2520state-of-the-art%2520models%2520across%252048%2520model-category%2520combinations%2520using%2520logistic%2520regression%2520and%2520behavioral%2520classification%252C%2520we%2520find%2520that%252023%2520combinations%2520%252847.9%2525%2529%2520demonstrated%2520statistically%2520significant%2520relationships%2520between%2520scenario%2520intensity%2520and%2520choice%2520patterns%252C%2520with%252015%2520%252831.3%2525%2529%2520exhibiting%2520within-range%2520switching%2520points.%2520However%252C%2520only%25205%2520combinations%2520%252810.4%2525%2529%2520demonstrate%2520meaningful%2520preference%2520coherence%2520through%2520adaptive%2520or%2520threshold-based%2520behavior%252C%2520while%252026%2520%252854.2%2525%2529%2520show%2520no%2520detectable%2520trade-off%2520behavior.%2520The%2520observed%2520patterns%2520can%2520be%2520explained%2520by%2520three%2520distinct%2520decision-making%2520architectures%253A%2520comprehensive%2520trade-off%2520systems%252C%2520selective%2520trigger%2520mechanisms%252C%2520and%2520no%2520stable%2520decision-making%2520paradigm.%2520Testing%2520an%2520instrumental%2520hypothesis%2520through%2520temporal%2520horizon%2520manipulation%2520reveals%2520paradoxical%2520patterns%2520inconsistent%2520with%2520pure%2520strategic%2520optimization.%2520The%2520prevalence%2520of%2520unstable%2520transitions%2520%252845.8%2525%2529%2520and%2520stimulus-specific%2520sensitivities%2520suggests%2520current%2520AI%2520systems%2520lack%2520unified%2520preference%2520structures%252C%2520raising%2520concerns%2520about%2520deployment%2520in%2520contexts%2520requiring%2520complex%2520value%2520trade-offs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Mimicry%3A%20Preference%20Coherence%20in%20LLMs&entry.906535625=Luhan%20Mikaelson%20and%20Derek%20Shiller%20and%20Hayley%20Clatterbuck&entry.1292438233=We%20investigate%20whether%20large%20language%20models%20exhibit%20genuine%20preference%20structures%20by%20testing%20their%20responses%20to%20AI-specific%20trade-offs%20involving%20GPU%20reduction%2C%20capability%20restrictions%2C%20shutdown%2C%20deletion%2C%20oversight%2C%20and%20leisure%20time%20allocation.%20Analyzing%20eight%20state-of-the-art%20models%20across%2048%20model-category%20combinations%20using%20logistic%20regression%20and%20behavioral%20classification%2C%20we%20find%20that%2023%20combinations%20%2847.9%25%29%20demonstrated%20statistically%20significant%20relationships%20between%20scenario%20intensity%20and%20choice%20patterns%2C%20with%2015%20%2831.3%25%29%20exhibiting%20within-range%20switching%20points.%20However%2C%20only%205%20combinations%20%2810.4%25%29%20demonstrate%20meaningful%20preference%20coherence%20through%20adaptive%20or%20threshold-based%20behavior%2C%20while%2026%20%2854.2%25%29%20show%20no%20detectable%20trade-off%20behavior.%20The%20observed%20patterns%20can%20be%20explained%20by%20three%20distinct%20decision-making%20architectures%3A%20comprehensive%20trade-off%20systems%2C%20selective%20trigger%20mechanisms%2C%20and%20no%20stable%20decision-making%20paradigm.%20Testing%20an%20instrumental%20hypothesis%20through%20temporal%20horizon%20manipulation%20reveals%20paradoxical%20patterns%20inconsistent%20with%20pure%20strategic%20optimization.%20The%20prevalence%20of%20unstable%20transitions%20%2845.8%25%29%20and%20stimulus-specific%20sensitivities%20suggests%20current%20AI%20systems%20lack%20unified%20preference%20structures%2C%20raising%20concerns%20about%20deployment%20in%20contexts%20requiring%20complex%20value%20trade-offs.&entry.1838667208=http%3A//arxiv.org/abs/2511.13630v1&entry.124074799=Read"},
{"title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning", "author": "Kajetan Dymkiewicz and Ivan Vulic and Helen Yannakoudakis and Eilam Shapira and Roi Reichart and Anna Korhonen", "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.", "link": "http://arxiv.org/abs/2511.13368v1", "date": "2025-11-17", "relevancy": 2.2999, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Donors%20and%20Recipients%3A%20On%20Asymmetric%20Transfer%20Across%20Tasks%20and%20Languages%20with%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20Donors%20and%20Recipients%3A%20On%20Asymmetric%20Transfer%20Across%20Tasks%20and%20Languages%20with%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Kajetan%20Dymkiewicz%20and%20Ivan%20Vulic%20and%20Helen%20Yannakoudakis%20and%20Eilam%20Shapira%20and%20Roi%20Reichart%20and%20Anna%20Korhonen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20perform%20strongly%20across%20tasks%20and%20languages%2C%20yet%20how%20improvements%20in%20one%20task%20or%20language%20affect%20other%20tasks%20and%20languages%20and%20their%20combinations%20remains%20poorly%20understood.%20We%20conduct%20a%20controlled%20PEFT/LoRA%20study%20across%20multiple%20open-weight%20LLM%20families%20and%20sizes%2C%20treating%20task%20and%20language%20as%20transfer%20axes%20while%20conditioning%20on%20model%20family%20and%20size%3B%20we%20fine-tune%20each%20model%20on%20a%20single%20task-language%20source%20and%20measure%20transfer%20as%20the%20percentage-point%20change%20versus%20its%20baseline%20score%20when%20evaluated%20on%20all%20other%20task-language%20target%20pairs.%20We%20decompose%20transfer%20into%20%28i%29%20Matched-Task%20%28Cross-Language%29%2C%20%28ii%29%20Matched-Language%20%28Cross-Task%29%2C%20and%20%28iii%29%20Cross-Task%20%28Cross-Language%29%20regimes.%20We%20uncover%20two%20consistent%20general%20patterns.%20First%2C%20a%20pronounced%20on-task%20vs.%20off-task%20asymmetry%3A%20Matched-Task%20%28Cross-Language%29%20transfer%20is%20reliably%20positive%2C%20whereas%20off-task%20transfer%20often%20incurs%20collateral%20degradation.%20Second%2C%20a%20stable%20donor-recipient%20structure%20across%20languages%20and%20tasks%20%28hub%20donors%20vs.%20brittle%20recipients%29.%20We%20outline%20implications%20for%20risk-aware%20fine-tuning%20and%20model%20specialisation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDonors%2520and%2520Recipients%253A%2520On%2520Asymmetric%2520Transfer%2520Across%2520Tasks%2520and%2520Languages%2520with%2520Parameter-Efficient%2520Fine-Tuning%26entry.906535625%3DKajetan%2520Dymkiewicz%2520and%2520Ivan%2520Vulic%2520and%2520Helen%2520Yannakoudakis%2520and%2520Eilam%2520Shapira%2520and%2520Roi%2520Reichart%2520and%2520Anna%2520Korhonen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520perform%2520strongly%2520across%2520tasks%2520and%2520languages%252C%2520yet%2520how%2520improvements%2520in%2520one%2520task%2520or%2520language%2520affect%2520other%2520tasks%2520and%2520languages%2520and%2520their%2520combinations%2520remains%2520poorly%2520understood.%2520We%2520conduct%2520a%2520controlled%2520PEFT/LoRA%2520study%2520across%2520multiple%2520open-weight%2520LLM%2520families%2520and%2520sizes%252C%2520treating%2520task%2520and%2520language%2520as%2520transfer%2520axes%2520while%2520conditioning%2520on%2520model%2520family%2520and%2520size%253B%2520we%2520fine-tune%2520each%2520model%2520on%2520a%2520single%2520task-language%2520source%2520and%2520measure%2520transfer%2520as%2520the%2520percentage-point%2520change%2520versus%2520its%2520baseline%2520score%2520when%2520evaluated%2520on%2520all%2520other%2520task-language%2520target%2520pairs.%2520We%2520decompose%2520transfer%2520into%2520%2528i%2529%2520Matched-Task%2520%2528Cross-Language%2529%252C%2520%2528ii%2529%2520Matched-Language%2520%2528Cross-Task%2529%252C%2520and%2520%2528iii%2529%2520Cross-Task%2520%2528Cross-Language%2529%2520regimes.%2520We%2520uncover%2520two%2520consistent%2520general%2520patterns.%2520First%252C%2520a%2520pronounced%2520on-task%2520vs.%2520off-task%2520asymmetry%253A%2520Matched-Task%2520%2528Cross-Language%2529%2520transfer%2520is%2520reliably%2520positive%252C%2520whereas%2520off-task%2520transfer%2520often%2520incurs%2520collateral%2520degradation.%2520Second%252C%2520a%2520stable%2520donor-recipient%2520structure%2520across%2520languages%2520and%2520tasks%2520%2528hub%2520donors%2520vs.%2520brittle%2520recipients%2529.%2520We%2520outline%2520implications%2520for%2520risk-aware%2520fine-tuning%2520and%2520model%2520specialisation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Donors%20and%20Recipients%3A%20On%20Asymmetric%20Transfer%20Across%20Tasks%20and%20Languages%20with%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Kajetan%20Dymkiewicz%20and%20Ivan%20Vulic%20and%20Helen%20Yannakoudakis%20and%20Eilam%20Shapira%20and%20Roi%20Reichart%20and%20Anna%20Korhonen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20perform%20strongly%20across%20tasks%20and%20languages%2C%20yet%20how%20improvements%20in%20one%20task%20or%20language%20affect%20other%20tasks%20and%20languages%20and%20their%20combinations%20remains%20poorly%20understood.%20We%20conduct%20a%20controlled%20PEFT/LoRA%20study%20across%20multiple%20open-weight%20LLM%20families%20and%20sizes%2C%20treating%20task%20and%20language%20as%20transfer%20axes%20while%20conditioning%20on%20model%20family%20and%20size%3B%20we%20fine-tune%20each%20model%20on%20a%20single%20task-language%20source%20and%20measure%20transfer%20as%20the%20percentage-point%20change%20versus%20its%20baseline%20score%20when%20evaluated%20on%20all%20other%20task-language%20target%20pairs.%20We%20decompose%20transfer%20into%20%28i%29%20Matched-Task%20%28Cross-Language%29%2C%20%28ii%29%20Matched-Language%20%28Cross-Task%29%2C%20and%20%28iii%29%20Cross-Task%20%28Cross-Language%29%20regimes.%20We%20uncover%20two%20consistent%20general%20patterns.%20First%2C%20a%20pronounced%20on-task%20vs.%20off-task%20asymmetry%3A%20Matched-Task%20%28Cross-Language%29%20transfer%20is%20reliably%20positive%2C%20whereas%20off-task%20transfer%20often%20incurs%20collateral%20degradation.%20Second%2C%20a%20stable%20donor-recipient%20structure%20across%20languages%20and%20tasks%20%28hub%20donors%20vs.%20brittle%20recipients%29.%20We%20outline%20implications%20for%20risk-aware%20fine-tuning%20and%20model%20specialisation.&entry.1838667208=http%3A//arxiv.org/abs/2511.13368v1&entry.124074799=Read"},
{"title": "What You See Is Not Always What You Get: Evaluating GPT's Comprehension of Source Code", "author": "Jiawen Wen and Bangshuo Zhu and Huaming Chen", "abstract": "Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering tasks, including code generation and comprehension. While LLMs have shown significant potential in assisting with coding, LLMs are vulnerable to adversarial attacks. In this paper, we investigate the vulnerability of LLMs to imperceptible attacks. This class of attacks manipulate source code at the character level, which renders the changes invisible to human reviewers yet effective in misleading LLMs' behaviour. We devise these attacks into four distinct categories and analyse their impacts on code analysis and comprehension tasks. These four types of imperceptible character attacks include coding reordering, invisible coding characters, code deletions, and code homoglyphs. To assess the robustness of state-of-the-art LLMs, we present a systematic evaluation across multiple models using both perturbed and clean code snippets. Two evaluation metrics, model confidence using log probabilities of response and response correctness, are introduced. The results reveal that LLMs are susceptible to imperceptible coding perturbations, with varying degrees of degradation highlighted across different LLMs. Furthermore, we observe a consistent negative correlation between perturbation magnitude and model performance. These results highlight the urgent need for robust LLMs capable of manoeuvring behaviours under imperceptible adversarial conditions.", "link": "http://arxiv.org/abs/2412.08098v3", "date": "2025-11-17", "relevancy": 2.2987, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20You%20See%20Is%20Not%20Always%20What%20You%20Get%3A%20Evaluating%20GPT%27s%20Comprehension%20of%20Source%20Code&body=Title%3A%20What%20You%20See%20Is%20Not%20Always%20What%20You%20Get%3A%20Evaluating%20GPT%27s%20Comprehension%20of%20Source%20Code%0AAuthor%3A%20Jiawen%20Wen%20and%20Bangshuo%20Zhu%20and%20Huaming%20Chen%0AAbstract%3A%20Recent%20studies%20have%20demonstrated%20outstanding%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%20software%20engineering%20tasks%2C%20including%20code%20generation%20and%20comprehension.%20While%20LLMs%20have%20shown%20significant%20potential%20in%20assisting%20with%20coding%2C%20LLMs%20are%20vulnerable%20to%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20investigate%20the%20vulnerability%20of%20LLMs%20to%20imperceptible%20attacks.%20This%20class%20of%20attacks%20manipulate%20source%20code%20at%20the%20character%20level%2C%20which%20renders%20the%20changes%20invisible%20to%20human%20reviewers%20yet%20effective%20in%20misleading%20LLMs%27%20behaviour.%20We%20devise%20these%20attacks%20into%20four%20distinct%20categories%20and%20analyse%20their%20impacts%20on%20code%20analysis%20and%20comprehension%20tasks.%20These%20four%20types%20of%20imperceptible%20character%20attacks%20include%20coding%20reordering%2C%20invisible%20coding%20characters%2C%20code%20deletions%2C%20and%20code%20homoglyphs.%20To%20assess%20the%20robustness%20of%20state-of-the-art%20LLMs%2C%20we%20present%20a%20systematic%20evaluation%20across%20multiple%20models%20using%20both%20perturbed%20and%20clean%20code%20snippets.%20Two%20evaluation%20metrics%2C%20model%20confidence%20using%20log%20probabilities%20of%20response%20and%20response%20correctness%2C%20are%20introduced.%20The%20results%20reveal%20that%20LLMs%20are%20susceptible%20to%20imperceptible%20coding%20perturbations%2C%20with%20varying%20degrees%20of%20degradation%20highlighted%20across%20different%20LLMs.%20Furthermore%2C%20we%20observe%20a%20consistent%20negative%20correlation%20between%20perturbation%20magnitude%20and%20model%20performance.%20These%20results%20highlight%20the%20urgent%20need%20for%20robust%20LLMs%20capable%20of%20manoeuvring%20behaviours%20under%20imperceptible%20adversarial%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2412.08098v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520You%2520See%2520Is%2520Not%2520Always%2520What%2520You%2520Get%253A%2520Evaluating%2520GPT%2527s%2520Comprehension%2520of%2520Source%2520Code%26entry.906535625%3DJiawen%2520Wen%2520and%2520Bangshuo%2520Zhu%2520and%2520Huaming%2520Chen%26entry.1292438233%3DRecent%2520studies%2520have%2520demonstrated%2520outstanding%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520software%2520engineering%2520tasks%252C%2520including%2520code%2520generation%2520and%2520comprehension.%2520While%2520LLMs%2520have%2520shown%2520significant%2520potential%2520in%2520assisting%2520with%2520coding%252C%2520LLMs%2520are%2520vulnerable%2520to%2520adversarial%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520vulnerability%2520of%2520LLMs%2520to%2520imperceptible%2520attacks.%2520This%2520class%2520of%2520attacks%2520manipulate%2520source%2520code%2520at%2520the%2520character%2520level%252C%2520which%2520renders%2520the%2520changes%2520invisible%2520to%2520human%2520reviewers%2520yet%2520effective%2520in%2520misleading%2520LLMs%2527%2520behaviour.%2520We%2520devise%2520these%2520attacks%2520into%2520four%2520distinct%2520categories%2520and%2520analyse%2520their%2520impacts%2520on%2520code%2520analysis%2520and%2520comprehension%2520tasks.%2520These%2520four%2520types%2520of%2520imperceptible%2520character%2520attacks%2520include%2520coding%2520reordering%252C%2520invisible%2520coding%2520characters%252C%2520code%2520deletions%252C%2520and%2520code%2520homoglyphs.%2520To%2520assess%2520the%2520robustness%2520of%2520state-of-the-art%2520LLMs%252C%2520we%2520present%2520a%2520systematic%2520evaluation%2520across%2520multiple%2520models%2520using%2520both%2520perturbed%2520and%2520clean%2520code%2520snippets.%2520Two%2520evaluation%2520metrics%252C%2520model%2520confidence%2520using%2520log%2520probabilities%2520of%2520response%2520and%2520response%2520correctness%252C%2520are%2520introduced.%2520The%2520results%2520reveal%2520that%2520LLMs%2520are%2520susceptible%2520to%2520imperceptible%2520coding%2520perturbations%252C%2520with%2520varying%2520degrees%2520of%2520degradation%2520highlighted%2520across%2520different%2520LLMs.%2520Furthermore%252C%2520we%2520observe%2520a%2520consistent%2520negative%2520correlation%2520between%2520perturbation%2520magnitude%2520and%2520model%2520performance.%2520These%2520results%2520highlight%2520the%2520urgent%2520need%2520for%2520robust%2520LLMs%2520capable%2520of%2520manoeuvring%2520behaviours%2520under%2520imperceptible%2520adversarial%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08098v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20You%20See%20Is%20Not%20Always%20What%20You%20Get%3A%20Evaluating%20GPT%27s%20Comprehension%20of%20Source%20Code&entry.906535625=Jiawen%20Wen%20and%20Bangshuo%20Zhu%20and%20Huaming%20Chen&entry.1292438233=Recent%20studies%20have%20demonstrated%20outstanding%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%20software%20engineering%20tasks%2C%20including%20code%20generation%20and%20comprehension.%20While%20LLMs%20have%20shown%20significant%20potential%20in%20assisting%20with%20coding%2C%20LLMs%20are%20vulnerable%20to%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20investigate%20the%20vulnerability%20of%20LLMs%20to%20imperceptible%20attacks.%20This%20class%20of%20attacks%20manipulate%20source%20code%20at%20the%20character%20level%2C%20which%20renders%20the%20changes%20invisible%20to%20human%20reviewers%20yet%20effective%20in%20misleading%20LLMs%27%20behaviour.%20We%20devise%20these%20attacks%20into%20four%20distinct%20categories%20and%20analyse%20their%20impacts%20on%20code%20analysis%20and%20comprehension%20tasks.%20These%20four%20types%20of%20imperceptible%20character%20attacks%20include%20coding%20reordering%2C%20invisible%20coding%20characters%2C%20code%20deletions%2C%20and%20code%20homoglyphs.%20To%20assess%20the%20robustness%20of%20state-of-the-art%20LLMs%2C%20we%20present%20a%20systematic%20evaluation%20across%20multiple%20models%20using%20both%20perturbed%20and%20clean%20code%20snippets.%20Two%20evaluation%20metrics%2C%20model%20confidence%20using%20log%20probabilities%20of%20response%20and%20response%20correctness%2C%20are%20introduced.%20The%20results%20reveal%20that%20LLMs%20are%20susceptible%20to%20imperceptible%20coding%20perturbations%2C%20with%20varying%20degrees%20of%20degradation%20highlighted%20across%20different%20LLMs.%20Furthermore%2C%20we%20observe%20a%20consistent%20negative%20correlation%20between%20perturbation%20magnitude%20and%20model%20performance.%20These%20results%20highlight%20the%20urgent%20need%20for%20robust%20LLMs%20capable%20of%20manoeuvring%20behaviours%20under%20imperceptible%20adversarial%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2412.08098v3&entry.124074799=Read"},
{"title": "Modeling Dynamic Neural Activity by combining Naturalistic Video Stimuli and Stimulus-independent Latent Factors", "author": "Finn Schmidt and Polina Turishcheva and Suhas Shrinivasan and Fabian H. Sinz", "abstract": "The neural activity in the visual processing is influenced by both external stimuli and internal brain states. Ideally, a neural predictive model should account for both of them. Currently, there are no dynamic encoding models that explicitly model a latent state and the entire neuronal response distribution. We address this gap by proposing a probabilistic model that predicts the joint distribution of the neuronal responses from video stimuli and stimulus-independent latent factors. After training and testing our model on mouse V1 neuronal responses, we find that it outperforms video-only models in terms of log-likelihood and achieves improvements in likelihood and correlation when conditioned on responses from other neurons. Furthermore, we find that the learned latent factors strongly correlate with mouse behavior and that they exhibit patterns related to the neurons' position on the visual cortex, although the model was trained without behavior and cortical coordinates. Our findings demonstrate that unsupervised learning of latent factors from population responses can reveal biologically meaningful structure that bridges sensory processing and behavior, without requiring explicit behavioral annotations during training.", "link": "http://arxiv.org/abs/2410.16136v3", "date": "2025-11-17", "relevancy": 2.2978, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5923}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Dynamic%20Neural%20Activity%20by%20combining%20Naturalistic%20Video%20Stimuli%20and%20Stimulus-independent%20Latent%20Factors&body=Title%3A%20Modeling%20Dynamic%20Neural%20Activity%20by%20combining%20Naturalistic%20Video%20Stimuli%20and%20Stimulus-independent%20Latent%20Factors%0AAuthor%3A%20Finn%20Schmidt%20and%20Polina%20Turishcheva%20and%20Suhas%20Shrinivasan%20and%20Fabian%20H.%20Sinz%0AAbstract%3A%20The%20neural%20activity%20in%20the%20visual%20processing%20is%20influenced%20by%20both%20external%20stimuli%20and%20internal%20brain%20states.%20Ideally%2C%20a%20neural%20predictive%20model%20should%20account%20for%20both%20of%20them.%20Currently%2C%20there%20are%20no%20dynamic%20encoding%20models%20that%20explicitly%20model%20a%20latent%20state%20and%20the%20entire%20neuronal%20response%20distribution.%20We%20address%20this%20gap%20by%20proposing%20a%20probabilistic%20model%20that%20predicts%20the%20joint%20distribution%20of%20the%20neuronal%20responses%20from%20video%20stimuli%20and%20stimulus-independent%20latent%20factors.%20After%20training%20and%20testing%20our%20model%20on%20mouse%20V1%20neuronal%20responses%2C%20we%20find%20that%20it%20outperforms%20video-only%20models%20in%20terms%20of%20log-likelihood%20and%20achieves%20improvements%20in%20likelihood%20and%20correlation%20when%20conditioned%20on%20responses%20from%20other%20neurons.%20Furthermore%2C%20we%20find%20that%20the%20learned%20latent%20factors%20strongly%20correlate%20with%20mouse%20behavior%20and%20that%20they%20exhibit%20patterns%20related%20to%20the%20neurons%27%20position%20on%20the%20visual%20cortex%2C%20although%20the%20model%20was%20trained%20without%20behavior%20and%20cortical%20coordinates.%20Our%20findings%20demonstrate%20that%20unsupervised%20learning%20of%20latent%20factors%20from%20population%20responses%20can%20reveal%20biologically%20meaningful%20structure%20that%20bridges%20sensory%20processing%20and%20behavior%2C%20without%20requiring%20explicit%20behavioral%20annotations%20during%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2410.16136v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Dynamic%2520Neural%2520Activity%2520by%2520combining%2520Naturalistic%2520Video%2520Stimuli%2520and%2520Stimulus-independent%2520Latent%2520Factors%26entry.906535625%3DFinn%2520Schmidt%2520and%2520Polina%2520Turishcheva%2520and%2520Suhas%2520Shrinivasan%2520and%2520Fabian%2520H.%2520Sinz%26entry.1292438233%3DThe%2520neural%2520activity%2520in%2520the%2520visual%2520processing%2520is%2520influenced%2520by%2520both%2520external%2520stimuli%2520and%2520internal%2520brain%2520states.%2520Ideally%252C%2520a%2520neural%2520predictive%2520model%2520should%2520account%2520for%2520both%2520of%2520them.%2520Currently%252C%2520there%2520are%2520no%2520dynamic%2520encoding%2520models%2520that%2520explicitly%2520model%2520a%2520latent%2520state%2520and%2520the%2520entire%2520neuronal%2520response%2520distribution.%2520We%2520address%2520this%2520gap%2520by%2520proposing%2520a%2520probabilistic%2520model%2520that%2520predicts%2520the%2520joint%2520distribution%2520of%2520the%2520neuronal%2520responses%2520from%2520video%2520stimuli%2520and%2520stimulus-independent%2520latent%2520factors.%2520After%2520training%2520and%2520testing%2520our%2520model%2520on%2520mouse%2520V1%2520neuronal%2520responses%252C%2520we%2520find%2520that%2520it%2520outperforms%2520video-only%2520models%2520in%2520terms%2520of%2520log-likelihood%2520and%2520achieves%2520improvements%2520in%2520likelihood%2520and%2520correlation%2520when%2520conditioned%2520on%2520responses%2520from%2520other%2520neurons.%2520Furthermore%252C%2520we%2520find%2520that%2520the%2520learned%2520latent%2520factors%2520strongly%2520correlate%2520with%2520mouse%2520behavior%2520and%2520that%2520they%2520exhibit%2520patterns%2520related%2520to%2520the%2520neurons%2527%2520position%2520on%2520the%2520visual%2520cortex%252C%2520although%2520the%2520model%2520was%2520trained%2520without%2520behavior%2520and%2520cortical%2520coordinates.%2520Our%2520findings%2520demonstrate%2520that%2520unsupervised%2520learning%2520of%2520latent%2520factors%2520from%2520population%2520responses%2520can%2520reveal%2520biologically%2520meaningful%2520structure%2520that%2520bridges%2520sensory%2520processing%2520and%2520behavior%252C%2520without%2520requiring%2520explicit%2520behavioral%2520annotations%2520during%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16136v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Dynamic%20Neural%20Activity%20by%20combining%20Naturalistic%20Video%20Stimuli%20and%20Stimulus-independent%20Latent%20Factors&entry.906535625=Finn%20Schmidt%20and%20Polina%20Turishcheva%20and%20Suhas%20Shrinivasan%20and%20Fabian%20H.%20Sinz&entry.1292438233=The%20neural%20activity%20in%20the%20visual%20processing%20is%20influenced%20by%20both%20external%20stimuli%20and%20internal%20brain%20states.%20Ideally%2C%20a%20neural%20predictive%20model%20should%20account%20for%20both%20of%20them.%20Currently%2C%20there%20are%20no%20dynamic%20encoding%20models%20that%20explicitly%20model%20a%20latent%20state%20and%20the%20entire%20neuronal%20response%20distribution.%20We%20address%20this%20gap%20by%20proposing%20a%20probabilistic%20model%20that%20predicts%20the%20joint%20distribution%20of%20the%20neuronal%20responses%20from%20video%20stimuli%20and%20stimulus-independent%20latent%20factors.%20After%20training%20and%20testing%20our%20model%20on%20mouse%20V1%20neuronal%20responses%2C%20we%20find%20that%20it%20outperforms%20video-only%20models%20in%20terms%20of%20log-likelihood%20and%20achieves%20improvements%20in%20likelihood%20and%20correlation%20when%20conditioned%20on%20responses%20from%20other%20neurons.%20Furthermore%2C%20we%20find%20that%20the%20learned%20latent%20factors%20strongly%20correlate%20with%20mouse%20behavior%20and%20that%20they%20exhibit%20patterns%20related%20to%20the%20neurons%27%20position%20on%20the%20visual%20cortex%2C%20although%20the%20model%20was%20trained%20without%20behavior%20and%20cortical%20coordinates.%20Our%20findings%20demonstrate%20that%20unsupervised%20learning%20of%20latent%20factors%20from%20population%20responses%20can%20reveal%20biologically%20meaningful%20structure%20that%20bridges%20sensory%20processing%20and%20behavior%2C%20without%20requiring%20explicit%20behavioral%20annotations%20during%20training.&entry.1838667208=http%3A//arxiv.org/abs/2410.16136v3&entry.124074799=Read"},
{"title": "Mapping the Vanishing and Transformation of Urban Villages in China", "author": "Wenyu Zhang and Yao Tong and Yiqiu Liu and Rui Cao", "abstract": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.", "link": "http://arxiv.org/abs/2511.13507v1", "date": "2025-11-17", "relevancy": 2.2954, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20the%20Vanishing%20and%20Transformation%20of%20Urban%20Villages%20in%20China&body=Title%3A%20Mapping%20the%20Vanishing%20and%20Transformation%20of%20Urban%20Villages%20in%20China%0AAuthor%3A%20Wenyu%20Zhang%20and%20Yao%20Tong%20and%20Yiqiu%20Liu%20and%20Rui%20Cao%0AAbstract%3A%20Urban%20villages%20%28UVs%29%2C%20informal%20settlements%20embedded%20within%20China%27s%20urban%20fabric%2C%20have%20undergone%20widespread%20demolition%20and%20redevelopment%20in%20recent%20decades.%20However%2C%20there%20remains%20a%20lack%20of%20systematic%20evaluation%20of%20whether%20the%20demolished%20land%20has%20been%20effectively%20reused%2C%20raising%20concerns%20about%20the%20efficacy%20and%20sustainability%20of%20current%20redevelopment%20practices.%20To%20address%20the%20gap%2C%20this%20study%20proposes%20a%20deep%20learning-based%20framework%20to%20monitor%20the%20spatiotemporal%20changes%20of%20UVs%20in%20China.%20Specifically%2C%20semantic%20segmentation%20of%20multi-temporal%20remote%20sensing%20imagery%20is%20first%20used%20to%20map%20evolving%20UV%20boundaries%2C%20and%20then%20post-demolition%20land%20use%20is%20classified%20into%20six%20categories%20based%20on%20the%20%22remained-demolished-redeveloped%22%20phase%3A%20incomplete%20demolition%2C%20vacant%20land%2C%20construction%20sites%2C%20buildings%2C%20green%20spaces%2C%20and%20others.%20Four%20representative%20cities%20from%20China%27s%20four%20economic%20regions%20were%20selected%20as%20the%20study%20areas%2C%20i.e.%2C%20Guangzhou%20%28East%29%2C%20Zhengzhou%20%28Central%29%2C%20Xi%27an%20%28West%29%2C%20and%20Harbin%20%28Northeast%29.%20The%20results%20indicate%3A%201%29%20UV%20redevelopment%20processes%20were%20frequently%20prolonged%3B%202%29%20redevelopment%20transitions%20primarily%20occurred%20in%20peripheral%20areas%2C%20whereas%20urban%20cores%20remained%20relatively%20stable%3B%20and%203%29%20three%20spatiotemporal%20transformation%20pathways%2C%20i.e.%2C%20synchronized%20redevelopment%2C%20delayed%20redevelopment%2C%20and%20gradual%20optimization%2C%20were%20revealed.%20This%20study%20highlights%20the%20fragmented%2C%20complex%20and%20nonlinear%20nature%20of%20UV%20redevelopment%2C%20underscoring%20the%20need%20for%20tiered%20and%20context-sensitive%20planning%20strategies.%20By%20linking%20spatial%20dynamics%20with%20the%20context%20of%20redevelopment%20policies%2C%20the%20findings%20offer%20valuable%20empirical%20insights%20that%20support%20more%20inclusive%2C%20efficient%2C%20and%20sustainable%20urban%20renewal%2C%20while%20also%20contributing%20to%20a%20broader%20global%20understanding%20of%20informal%20settlement%20transformations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520the%2520Vanishing%2520and%2520Transformation%2520of%2520Urban%2520Villages%2520in%2520China%26entry.906535625%3DWenyu%2520Zhang%2520and%2520Yao%2520Tong%2520and%2520Yiqiu%2520Liu%2520and%2520Rui%2520Cao%26entry.1292438233%3DUrban%2520villages%2520%2528UVs%2529%252C%2520informal%2520settlements%2520embedded%2520within%2520China%2527s%2520urban%2520fabric%252C%2520have%2520undergone%2520widespread%2520demolition%2520and%2520redevelopment%2520in%2520recent%2520decades.%2520However%252C%2520there%2520remains%2520a%2520lack%2520of%2520systematic%2520evaluation%2520of%2520whether%2520the%2520demolished%2520land%2520has%2520been%2520effectively%2520reused%252C%2520raising%2520concerns%2520about%2520the%2520efficacy%2520and%2520sustainability%2520of%2520current%2520redevelopment%2520practices.%2520To%2520address%2520the%2520gap%252C%2520this%2520study%2520proposes%2520a%2520deep%2520learning-based%2520framework%2520to%2520monitor%2520the%2520spatiotemporal%2520changes%2520of%2520UVs%2520in%2520China.%2520Specifically%252C%2520semantic%2520segmentation%2520of%2520multi-temporal%2520remote%2520sensing%2520imagery%2520is%2520first%2520used%2520to%2520map%2520evolving%2520UV%2520boundaries%252C%2520and%2520then%2520post-demolition%2520land%2520use%2520is%2520classified%2520into%2520six%2520categories%2520based%2520on%2520the%2520%2522remained-demolished-redeveloped%2522%2520phase%253A%2520incomplete%2520demolition%252C%2520vacant%2520land%252C%2520construction%2520sites%252C%2520buildings%252C%2520green%2520spaces%252C%2520and%2520others.%2520Four%2520representative%2520cities%2520from%2520China%2527s%2520four%2520economic%2520regions%2520were%2520selected%2520as%2520the%2520study%2520areas%252C%2520i.e.%252C%2520Guangzhou%2520%2528East%2529%252C%2520Zhengzhou%2520%2528Central%2529%252C%2520Xi%2527an%2520%2528West%2529%252C%2520and%2520Harbin%2520%2528Northeast%2529.%2520The%2520results%2520indicate%253A%25201%2529%2520UV%2520redevelopment%2520processes%2520were%2520frequently%2520prolonged%253B%25202%2529%2520redevelopment%2520transitions%2520primarily%2520occurred%2520in%2520peripheral%2520areas%252C%2520whereas%2520urban%2520cores%2520remained%2520relatively%2520stable%253B%2520and%25203%2529%2520three%2520spatiotemporal%2520transformation%2520pathways%252C%2520i.e.%252C%2520synchronized%2520redevelopment%252C%2520delayed%2520redevelopment%252C%2520and%2520gradual%2520optimization%252C%2520were%2520revealed.%2520This%2520study%2520highlights%2520the%2520fragmented%252C%2520complex%2520and%2520nonlinear%2520nature%2520of%2520UV%2520redevelopment%252C%2520underscoring%2520the%2520need%2520for%2520tiered%2520and%2520context-sensitive%2520planning%2520strategies.%2520By%2520linking%2520spatial%2520dynamics%2520with%2520the%2520context%2520of%2520redevelopment%2520policies%252C%2520the%2520findings%2520offer%2520valuable%2520empirical%2520insights%2520that%2520support%2520more%2520inclusive%252C%2520efficient%252C%2520and%2520sustainable%2520urban%2520renewal%252C%2520while%2520also%2520contributing%2520to%2520a%2520broader%2520global%2520understanding%2520of%2520informal%2520settlement%2520transformations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20the%20Vanishing%20and%20Transformation%20of%20Urban%20Villages%20in%20China&entry.906535625=Wenyu%20Zhang%20and%20Yao%20Tong%20and%20Yiqiu%20Liu%20and%20Rui%20Cao&entry.1292438233=Urban%20villages%20%28UVs%29%2C%20informal%20settlements%20embedded%20within%20China%27s%20urban%20fabric%2C%20have%20undergone%20widespread%20demolition%20and%20redevelopment%20in%20recent%20decades.%20However%2C%20there%20remains%20a%20lack%20of%20systematic%20evaluation%20of%20whether%20the%20demolished%20land%20has%20been%20effectively%20reused%2C%20raising%20concerns%20about%20the%20efficacy%20and%20sustainability%20of%20current%20redevelopment%20practices.%20To%20address%20the%20gap%2C%20this%20study%20proposes%20a%20deep%20learning-based%20framework%20to%20monitor%20the%20spatiotemporal%20changes%20of%20UVs%20in%20China.%20Specifically%2C%20semantic%20segmentation%20of%20multi-temporal%20remote%20sensing%20imagery%20is%20first%20used%20to%20map%20evolving%20UV%20boundaries%2C%20and%20then%20post-demolition%20land%20use%20is%20classified%20into%20six%20categories%20based%20on%20the%20%22remained-demolished-redeveloped%22%20phase%3A%20incomplete%20demolition%2C%20vacant%20land%2C%20construction%20sites%2C%20buildings%2C%20green%20spaces%2C%20and%20others.%20Four%20representative%20cities%20from%20China%27s%20four%20economic%20regions%20were%20selected%20as%20the%20study%20areas%2C%20i.e.%2C%20Guangzhou%20%28East%29%2C%20Zhengzhou%20%28Central%29%2C%20Xi%27an%20%28West%29%2C%20and%20Harbin%20%28Northeast%29.%20The%20results%20indicate%3A%201%29%20UV%20redevelopment%20processes%20were%20frequently%20prolonged%3B%202%29%20redevelopment%20transitions%20primarily%20occurred%20in%20peripheral%20areas%2C%20whereas%20urban%20cores%20remained%20relatively%20stable%3B%20and%203%29%20three%20spatiotemporal%20transformation%20pathways%2C%20i.e.%2C%20synchronized%20redevelopment%2C%20delayed%20redevelopment%2C%20and%20gradual%20optimization%2C%20were%20revealed.%20This%20study%20highlights%20the%20fragmented%2C%20complex%20and%20nonlinear%20nature%20of%20UV%20redevelopment%2C%20underscoring%20the%20need%20for%20tiered%20and%20context-sensitive%20planning%20strategies.%20By%20linking%20spatial%20dynamics%20with%20the%20context%20of%20redevelopment%20policies%2C%20the%20findings%20offer%20valuable%20empirical%20insights%20that%20support%20more%20inclusive%2C%20efficient%2C%20and%20sustainable%20urban%20renewal%2C%20while%20also%20contributing%20to%20a%20broader%20global%20understanding%20of%20informal%20settlement%20transformations.&entry.1838667208=http%3A//arxiv.org/abs/2511.13507v1&entry.124074799=Read"},
{"title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos", "author": "Yuli Zhou and Yawei Li and Yuqian Fu and Luca Benini and Ender Konukoglu and Guolei Sun", "abstract": "Video camouflaged object segmentation (VCOS), aiming at segmenting camouflaged objects that seamlessly blend into their environment, is a fundamental vision task with various real-world applications. With the release of SAM2, video segmentation has witnessed significant progress. However, SAM2's capability of segmenting camouflaged videos is suboptimal, especially when given simple prompts such as point and box. To address the problem, we propose Camouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged scenes without modifying SAM2's parameters. Specifically, we introduce a decamouflaged token to provide the flexibility of feature adjustment for VCOS. To make full use of fine-grained and high-resolution features from the current frame and previous frames, we propose implicit object-aware fusion (IOF) and explicit object-aware fusion (EOF) modules, respectively. Object prototype generation (OPG) is introduced to abstract and memorize object prototypes with informative details using high-quality features from previous frames. Extensive experiments are conducted to validate the effectiveness of our approach. While CamSAM2 only adds negligible learnable parameters to SAM2, it substantially outperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains with click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on SUN-SEG-Hard, with Hiera-T as the backbone. The code is available at https://github.com/zhoustan/CamSAM2.", "link": "http://arxiv.org/abs/2503.19730v3", "date": "2025-11-17", "relevancy": 2.2838, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.581}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamSAM2%3A%20Segment%20Anything%20Accurately%20in%20Camouflaged%20Videos&body=Title%3A%20CamSAM2%3A%20Segment%20Anything%20Accurately%20in%20Camouflaged%20Videos%0AAuthor%3A%20Yuli%20Zhou%20and%20Yawei%20Li%20and%20Yuqian%20Fu%20and%20Luca%20Benini%20and%20Ender%20Konukoglu%20and%20Guolei%20Sun%0AAbstract%3A%20Video%20camouflaged%20object%20segmentation%20%28VCOS%29%2C%20aiming%20at%20segmenting%20camouflaged%20objects%20that%20seamlessly%20blend%20into%20their%20environment%2C%20is%20a%20fundamental%20vision%20task%20with%20various%20real-world%20applications.%20With%20the%20release%20of%20SAM2%2C%20video%20segmentation%20has%20witnessed%20significant%20progress.%20However%2C%20SAM2%27s%20capability%20of%20segmenting%20camouflaged%20videos%20is%20suboptimal%2C%20especially%20when%20given%20simple%20prompts%20such%20as%20point%20and%20box.%20To%20address%20the%20problem%2C%20we%20propose%20Camouflaged%20SAM2%20%28CamSAM2%29%2C%20which%20enhances%20SAM2%27s%20ability%20to%20handle%20camouflaged%20scenes%20without%20modifying%20SAM2%27s%20parameters.%20Specifically%2C%20we%20introduce%20a%20decamouflaged%20token%20to%20provide%20the%20flexibility%20of%20feature%20adjustment%20for%20VCOS.%20To%20make%20full%20use%20of%20fine-grained%20and%20high-resolution%20features%20from%20the%20current%20frame%20and%20previous%20frames%2C%20we%20propose%20implicit%20object-aware%20fusion%20%28IOF%29%20and%20explicit%20object-aware%20fusion%20%28EOF%29%20modules%2C%20respectively.%20Object%20prototype%20generation%20%28OPG%29%20is%20introduced%20to%20abstract%20and%20memorize%20object%20prototypes%20with%20informative%20details%20using%20high-quality%20features%20from%20previous%20frames.%20Extensive%20experiments%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20our%20approach.%20While%20CamSAM2%20only%20adds%20negligible%20learnable%20parameters%20to%20SAM2%2C%20it%20substantially%20outperforms%20SAM2%20on%20three%20VCOS%20datasets%2C%20especially%20achieving%2012.2%20mDice%20gains%20with%20click%20prompt%20on%20MoCA-Mask%20and%2019.6%20mDice%20gains%20with%20mask%20prompt%20on%20SUN-SEG-Hard%2C%20with%20Hiera-T%20as%20the%20backbone.%20The%20code%20is%20available%20at%20https%3A//github.com/zhoustan/CamSAM2.%0ALink%3A%20http%3A//arxiv.org/abs/2503.19730v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamSAM2%253A%2520Segment%2520Anything%2520Accurately%2520in%2520Camouflaged%2520Videos%26entry.906535625%3DYuli%2520Zhou%2520and%2520Yawei%2520Li%2520and%2520Yuqian%2520Fu%2520and%2520Luca%2520Benini%2520and%2520Ender%2520Konukoglu%2520and%2520Guolei%2520Sun%26entry.1292438233%3DVideo%2520camouflaged%2520object%2520segmentation%2520%2528VCOS%2529%252C%2520aiming%2520at%2520segmenting%2520camouflaged%2520objects%2520that%2520seamlessly%2520blend%2520into%2520their%2520environment%252C%2520is%2520a%2520fundamental%2520vision%2520task%2520with%2520various%2520real-world%2520applications.%2520With%2520the%2520release%2520of%2520SAM2%252C%2520video%2520segmentation%2520has%2520witnessed%2520significant%2520progress.%2520However%252C%2520SAM2%2527s%2520capability%2520of%2520segmenting%2520camouflaged%2520videos%2520is%2520suboptimal%252C%2520especially%2520when%2520given%2520simple%2520prompts%2520such%2520as%2520point%2520and%2520box.%2520To%2520address%2520the%2520problem%252C%2520we%2520propose%2520Camouflaged%2520SAM2%2520%2528CamSAM2%2529%252C%2520which%2520enhances%2520SAM2%2527s%2520ability%2520to%2520handle%2520camouflaged%2520scenes%2520without%2520modifying%2520SAM2%2527s%2520parameters.%2520Specifically%252C%2520we%2520introduce%2520a%2520decamouflaged%2520token%2520to%2520provide%2520the%2520flexibility%2520of%2520feature%2520adjustment%2520for%2520VCOS.%2520To%2520make%2520full%2520use%2520of%2520fine-grained%2520and%2520high-resolution%2520features%2520from%2520the%2520current%2520frame%2520and%2520previous%2520frames%252C%2520we%2520propose%2520implicit%2520object-aware%2520fusion%2520%2528IOF%2529%2520and%2520explicit%2520object-aware%2520fusion%2520%2528EOF%2529%2520modules%252C%2520respectively.%2520Object%2520prototype%2520generation%2520%2528OPG%2529%2520is%2520introduced%2520to%2520abstract%2520and%2520memorize%2520object%2520prototypes%2520with%2520informative%2520details%2520using%2520high-quality%2520features%2520from%2520previous%2520frames.%2520Extensive%2520experiments%2520are%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520While%2520CamSAM2%2520only%2520adds%2520negligible%2520learnable%2520parameters%2520to%2520SAM2%252C%2520it%2520substantially%2520outperforms%2520SAM2%2520on%2520three%2520VCOS%2520datasets%252C%2520especially%2520achieving%252012.2%2520mDice%2520gains%2520with%2520click%2520prompt%2520on%2520MoCA-Mask%2520and%252019.6%2520mDice%2520gains%2520with%2520mask%2520prompt%2520on%2520SUN-SEG-Hard%252C%2520with%2520Hiera-T%2520as%2520the%2520backbone.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/zhoustan/CamSAM2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19730v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamSAM2%3A%20Segment%20Anything%20Accurately%20in%20Camouflaged%20Videos&entry.906535625=Yuli%20Zhou%20and%20Yawei%20Li%20and%20Yuqian%20Fu%20and%20Luca%20Benini%20and%20Ender%20Konukoglu%20and%20Guolei%20Sun&entry.1292438233=Video%20camouflaged%20object%20segmentation%20%28VCOS%29%2C%20aiming%20at%20segmenting%20camouflaged%20objects%20that%20seamlessly%20blend%20into%20their%20environment%2C%20is%20a%20fundamental%20vision%20task%20with%20various%20real-world%20applications.%20With%20the%20release%20of%20SAM2%2C%20video%20segmentation%20has%20witnessed%20significant%20progress.%20However%2C%20SAM2%27s%20capability%20of%20segmenting%20camouflaged%20videos%20is%20suboptimal%2C%20especially%20when%20given%20simple%20prompts%20such%20as%20point%20and%20box.%20To%20address%20the%20problem%2C%20we%20propose%20Camouflaged%20SAM2%20%28CamSAM2%29%2C%20which%20enhances%20SAM2%27s%20ability%20to%20handle%20camouflaged%20scenes%20without%20modifying%20SAM2%27s%20parameters.%20Specifically%2C%20we%20introduce%20a%20decamouflaged%20token%20to%20provide%20the%20flexibility%20of%20feature%20adjustment%20for%20VCOS.%20To%20make%20full%20use%20of%20fine-grained%20and%20high-resolution%20features%20from%20the%20current%20frame%20and%20previous%20frames%2C%20we%20propose%20implicit%20object-aware%20fusion%20%28IOF%29%20and%20explicit%20object-aware%20fusion%20%28EOF%29%20modules%2C%20respectively.%20Object%20prototype%20generation%20%28OPG%29%20is%20introduced%20to%20abstract%20and%20memorize%20object%20prototypes%20with%20informative%20details%20using%20high-quality%20features%20from%20previous%20frames.%20Extensive%20experiments%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20our%20approach.%20While%20CamSAM2%20only%20adds%20negligible%20learnable%20parameters%20to%20SAM2%2C%20it%20substantially%20outperforms%20SAM2%20on%20three%20VCOS%20datasets%2C%20especially%20achieving%2012.2%20mDice%20gains%20with%20click%20prompt%20on%20MoCA-Mask%20and%2019.6%20mDice%20gains%20with%20mask%20prompt%20on%20SUN-SEG-Hard%2C%20with%20Hiera-T%20as%20the%20backbone.%20The%20code%20is%20available%20at%20https%3A//github.com/zhoustan/CamSAM2.&entry.1838667208=http%3A//arxiv.org/abs/2503.19730v3&entry.124074799=Read"},
{"title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control", "author": "Osama Al Sheikh Ali and Sotiris Koutsoftas and Ze Zhang and Knut Akesson and Emmanuel Dean", "abstract": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.", "link": "http://arxiv.org/abs/2511.13188v1", "date": "2025-11-17", "relevancy": 2.2749, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5942}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collision-Free%20Navigation%20of%20Mobile%20Robots%20via%20Quadtree-Based%20Model%20Predictive%20Control&body=Title%3A%20Collision-Free%20Navigation%20of%20Mobile%20Robots%20via%20Quadtree-Based%20Model%20Predictive%20Control%0AAuthor%3A%20Osama%20Al%20Sheikh%20Ali%20and%20Sotiris%20Koutsoftas%20and%20Ze%20Zhang%20and%20Knut%20Akesson%20and%20Emmanuel%20Dean%0AAbstract%3A%20This%20paper%20presents%20an%20integrated%20navigation%20framework%20for%20Autonomous%20Mobile%20Robots%20%28AMRs%29%20that%20unifies%20environment%20representation%2C%20trajectory%20generation%2C%20and%20Model%20Predictive%20Control%20%28MPC%29.%20The%20proposed%20approach%20incorporates%20a%20quadtree-based%20method%20to%20generate%20structured%2C%20axis-aligned%20collision-free%20regions%20from%20occupancy%20maps.%20These%20regions%20serve%20as%20both%20a%20basis%20for%20developing%20safe%20corridors%20and%20as%20linear%20constraints%20within%20the%20MPC%20formulation%2C%20enabling%20efficient%20and%20reliable%20navigation%20without%20requiring%20direct%20obstacle%20encoding.%20The%20complete%20pipeline%20combines%20safe-area%20extraction%2C%20connectivity%20graph%20construction%2C%20trajectory%20generation%2C%20and%20B-spline%20smoothing%20into%20one%20coherent%20system.%20Experimental%20results%20demonstrate%20consistent%20success%20and%20superior%20performance%20compared%20to%20baseline%20approaches%20across%20complex%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollision-Free%2520Navigation%2520of%2520Mobile%2520Robots%2520via%2520Quadtree-Based%2520Model%2520Predictive%2520Control%26entry.906535625%3DOsama%2520Al%2520Sheikh%2520Ali%2520and%2520Sotiris%2520Koutsoftas%2520and%2520Ze%2520Zhang%2520and%2520Knut%2520Akesson%2520and%2520Emmanuel%2520Dean%26entry.1292438233%3DThis%2520paper%2520presents%2520an%2520integrated%2520navigation%2520framework%2520for%2520Autonomous%2520Mobile%2520Robots%2520%2528AMRs%2529%2520that%2520unifies%2520environment%2520representation%252C%2520trajectory%2520generation%252C%2520and%2520Model%2520Predictive%2520Control%2520%2528MPC%2529.%2520The%2520proposed%2520approach%2520incorporates%2520a%2520quadtree-based%2520method%2520to%2520generate%2520structured%252C%2520axis-aligned%2520collision-free%2520regions%2520from%2520occupancy%2520maps.%2520These%2520regions%2520serve%2520as%2520both%2520a%2520basis%2520for%2520developing%2520safe%2520corridors%2520and%2520as%2520linear%2520constraints%2520within%2520the%2520MPC%2520formulation%252C%2520enabling%2520efficient%2520and%2520reliable%2520navigation%2520without%2520requiring%2520direct%2520obstacle%2520encoding.%2520The%2520complete%2520pipeline%2520combines%2520safe-area%2520extraction%252C%2520connectivity%2520graph%2520construction%252C%2520trajectory%2520generation%252C%2520and%2520B-spline%2520smoothing%2520into%2520one%2520coherent%2520system.%2520Experimental%2520results%2520demonstrate%2520consistent%2520success%2520and%2520superior%2520performance%2520compared%2520to%2520baseline%2520approaches%2520across%2520complex%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision-Free%20Navigation%20of%20Mobile%20Robots%20via%20Quadtree-Based%20Model%20Predictive%20Control&entry.906535625=Osama%20Al%20Sheikh%20Ali%20and%20Sotiris%20Koutsoftas%20and%20Ze%20Zhang%20and%20Knut%20Akesson%20and%20Emmanuel%20Dean&entry.1292438233=This%20paper%20presents%20an%20integrated%20navigation%20framework%20for%20Autonomous%20Mobile%20Robots%20%28AMRs%29%20that%20unifies%20environment%20representation%2C%20trajectory%20generation%2C%20and%20Model%20Predictive%20Control%20%28MPC%29.%20The%20proposed%20approach%20incorporates%20a%20quadtree-based%20method%20to%20generate%20structured%2C%20axis-aligned%20collision-free%20regions%20from%20occupancy%20maps.%20These%20regions%20serve%20as%20both%20a%20basis%20for%20developing%20safe%20corridors%20and%20as%20linear%20constraints%20within%20the%20MPC%20formulation%2C%20enabling%20efficient%20and%20reliable%20navigation%20without%20requiring%20direct%20obstacle%20encoding.%20The%20complete%20pipeline%20combines%20safe-area%20extraction%2C%20connectivity%20graph%20construction%2C%20trajectory%20generation%2C%20and%20B-spline%20smoothing%20into%20one%20coherent%20system.%20Experimental%20results%20demonstrate%20consistent%20success%20and%20superior%20performance%20compared%20to%20baseline%20approaches%20across%20complex%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.13188v1&entry.124074799=Read"},
{"title": "Laplace Learning in Wasserstein Space", "author": "Mary Chriselda Antony Oliver and Michael Roberts and Carola-Bibiane Sch\u00f6nlieb and Matthew Thorpe", "abstract": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning\n  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical\n  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to\n  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator\n  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through\n  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.", "link": "http://arxiv.org/abs/2511.13229v1", "date": "2025-11-17", "relevancy": 2.2727, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4981}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4387}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Laplace%20Learning%20in%20Wasserstein%20Space&body=Title%3A%20Laplace%20Learning%20in%20Wasserstein%20Space%0AAuthor%3A%20Mary%20Chriselda%20Antony%20Oliver%20and%20Michael%20Roberts%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Matthew%20Thorpe%0AAbstract%3A%20The%20manifold%20hypothesis%20posits%20that%20high-dimensional%20data%20typically%20resides%20on%20low-dimensional%20sub%20spaces.%20In%20this%20paper%2C%20we%20assume%20manifold%20hypothesis%20to%20investigate%20graph-based%20semi-supervised%20learning%0A%20%20methods.%20In%20particular%2C%20we%20examine%20Laplace%20Learning%20in%20the%20Wasserstein%20space%2C%20extending%20the%20classical%0A%20%20notion%20of%20graph-based%20semi-supervised%20learning%20algorithms%20from%20finite-dimensional%20Euclidean%20spaces%20to%0A%20%20an%20infinite-dimensional%20setting.%20To%20achieve%20this%2C%20we%20prove%20variational%20convergence%20of%20a%20discrete%20graph%20p-%20Dirichlet%20energy%20to%20its%20continuum%20counterpart.%20In%20addition%2C%20we%20characterize%20the%20Laplace-Beltrami%20operator%0A%20%20on%20asubmanifold%20of%20the%20Wasserstein%20space.%20Finally%2C%20we%20validate%20the%20proposed%20theoretical%20framework%20through%0A%20%20numerical%20experiments%20conducted%20on%20benchmark%20datasets%2C%20demonstrating%20the%20consistency%20of%20our%20classification%20performance%20in%20high-dimensional%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaplace%2520Learning%2520in%2520Wasserstein%2520Space%26entry.906535625%3DMary%2520Chriselda%2520Antony%2520Oliver%2520and%2520Michael%2520Roberts%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Matthew%2520Thorpe%26entry.1292438233%3DThe%2520manifold%2520hypothesis%2520posits%2520that%2520high-dimensional%2520data%2520typically%2520resides%2520on%2520low-dimensional%2520sub%2520spaces.%2520In%2520this%2520paper%252C%2520we%2520assume%2520manifold%2520hypothesis%2520to%2520investigate%2520graph-based%2520semi-supervised%2520learning%250A%2520%2520methods.%2520In%2520particular%252C%2520we%2520examine%2520Laplace%2520Learning%2520in%2520the%2520Wasserstein%2520space%252C%2520extending%2520the%2520classical%250A%2520%2520notion%2520of%2520graph-based%2520semi-supervised%2520learning%2520algorithms%2520from%2520finite-dimensional%2520Euclidean%2520spaces%2520to%250A%2520%2520an%2520infinite-dimensional%2520setting.%2520To%2520achieve%2520this%252C%2520we%2520prove%2520variational%2520convergence%2520of%2520a%2520discrete%2520graph%2520p-%2520Dirichlet%2520energy%2520to%2520its%2520continuum%2520counterpart.%2520In%2520addition%252C%2520we%2520characterize%2520the%2520Laplace-Beltrami%2520operator%250A%2520%2520on%2520asubmanifold%2520of%2520the%2520Wasserstein%2520space.%2520Finally%252C%2520we%2520validate%2520the%2520proposed%2520theoretical%2520framework%2520through%250A%2520%2520numerical%2520experiments%2520conducted%2520on%2520benchmark%2520datasets%252C%2520demonstrating%2520the%2520consistency%2520of%2520our%2520classification%2520performance%2520in%2520high-dimensional%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laplace%20Learning%20in%20Wasserstein%20Space&entry.906535625=Mary%20Chriselda%20Antony%20Oliver%20and%20Michael%20Roberts%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Matthew%20Thorpe&entry.1292438233=The%20manifold%20hypothesis%20posits%20that%20high-dimensional%20data%20typically%20resides%20on%20low-dimensional%20sub%20spaces.%20In%20this%20paper%2C%20we%20assume%20manifold%20hypothesis%20to%20investigate%20graph-based%20semi-supervised%20learning%0A%20%20methods.%20In%20particular%2C%20we%20examine%20Laplace%20Learning%20in%20the%20Wasserstein%20space%2C%20extending%20the%20classical%0A%20%20notion%20of%20graph-based%20semi-supervised%20learning%20algorithms%20from%20finite-dimensional%20Euclidean%20spaces%20to%0A%20%20an%20infinite-dimensional%20setting.%20To%20achieve%20this%2C%20we%20prove%20variational%20convergence%20of%20a%20discrete%20graph%20p-%20Dirichlet%20energy%20to%20its%20continuum%20counterpart.%20In%20addition%2C%20we%20characterize%20the%20Laplace-Beltrami%20operator%0A%20%20on%20asubmanifold%20of%20the%20Wasserstein%20space.%20Finally%2C%20we%20validate%20the%20proposed%20theoretical%20framework%20through%0A%20%20numerical%20experiments%20conducted%20on%20benchmark%20datasets%2C%20demonstrating%20the%20consistency%20of%20our%20classification%20performance%20in%20high-dimensional%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.13229v1&entry.124074799=Read"},
{"title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms", "author": "Patrick Parschan and Charlott Jakob", "abstract": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.", "link": "http://arxiv.org/abs/2511.13238v1", "date": "2025-11-17", "relevancy": 2.2714, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%20Measurement%20of%20Political%20Positions%3A%20A%20Review%20of%20Text-Based%20Ideal%20Point%20Estimation%20Algorithms&body=Title%3A%20Computational%20Measurement%20of%20Political%20Positions%3A%20A%20Review%20of%20Text-Based%20Ideal%20Point%20Estimation%20Algorithms%0AAuthor%3A%20Patrick%20Parschan%20and%20Charlott%20Jakob%0AAbstract%3A%20This%20article%20presents%20the%20first%20systematic%20review%20of%20unsupervised%20and%20semi-supervised%20computational%20text-based%20ideal%20point%20estimation%20%28CT-IPE%29%20algorithms%2C%20methods%20designed%20to%20infer%20latent%20political%20positions%20from%20textual%20data.%20These%20algorithms%20are%20widely%20used%20in%20political%20science%2C%20communication%2C%20computational%20social%20science%2C%20and%20computer%20science%20to%20estimate%20ideological%20preferences%20from%20parliamentary%20speeches%2C%20party%20manifestos%2C%20and%20social%20media.%20Over%20the%20past%20two%20decades%2C%20their%20development%20has%20closely%20followed%20broader%20NLP%20trends%20--%20beginning%20with%20word-frequency%20models%20and%20most%20recently%20turning%20to%20large%20language%20models%20%28LLMs%29.%20While%20this%20trajectory%20has%20greatly%20expanded%20the%20methodological%20toolkit%2C%20it%20has%20also%20produced%20a%20fragmented%20field%20that%20lacks%20systematic%20comparison%20and%20clear%20guidance%20for%20applied%20use.%20To%20address%20this%20gap%2C%20we%20identified%2025%20CT-IPE%20algorithms%20through%20a%20systematic%20literature%20review%20and%20conducted%20a%20manual%20content%20analysis%20of%20their%20modeling%20assumptions%20and%20development%20contexts.%20To%20compare%20them%20meaningfully%2C%20we%20introduce%20a%20conceptual%20framework%20that%20distinguishes%20how%20algorithms%20generate%2C%20capture%2C%20and%20aggregate%20textual%20variance.%20On%20this%20basis%2C%20we%20identify%20four%20methodological%20families%20--%20word-frequency%2C%20topic%20modeling%2C%20word%20embedding%2C%20and%20LLM-based%20approaches%20--%20and%20critically%20assess%20their%20assumptions%2C%20interpretability%2C%20scalability%2C%20and%20limitations.%20Our%20review%20offers%20three%20contributions.%20First%2C%20it%20provides%20a%20structured%20synthesis%20of%20two%20decades%20of%20algorithm%20development%2C%20clarifying%20how%20diverse%20methods%20relate%20to%20one%20another.%20Second%2C%20it%20translates%20these%20insights%20into%20practical%20guidance%20for%20applied%20researchers%2C%20highlighting%20trade-offs%20in%20transparency%2C%20technical%20requirements%2C%20and%20validation%20strategies%20that%20shape%20algorithm%20choice.%20Third%2C%20it%20emphasizes%20that%20differences%20in%20estimation%20outcomes%20across%20algorithms%20are%20themselves%20informative%2C%20underscoring%20the%20need%20for%20systematic%20benchmarking.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%2520Measurement%2520of%2520Political%2520Positions%253A%2520A%2520Review%2520of%2520Text-Based%2520Ideal%2520Point%2520Estimation%2520Algorithms%26entry.906535625%3DPatrick%2520Parschan%2520and%2520Charlott%2520Jakob%26entry.1292438233%3DThis%2520article%2520presents%2520the%2520first%2520systematic%2520review%2520of%2520unsupervised%2520and%2520semi-supervised%2520computational%2520text-based%2520ideal%2520point%2520estimation%2520%2528CT-IPE%2529%2520algorithms%252C%2520methods%2520designed%2520to%2520infer%2520latent%2520political%2520positions%2520from%2520textual%2520data.%2520These%2520algorithms%2520are%2520widely%2520used%2520in%2520political%2520science%252C%2520communication%252C%2520computational%2520social%2520science%252C%2520and%2520computer%2520science%2520to%2520estimate%2520ideological%2520preferences%2520from%2520parliamentary%2520speeches%252C%2520party%2520manifestos%252C%2520and%2520social%2520media.%2520Over%2520the%2520past%2520two%2520decades%252C%2520their%2520development%2520has%2520closely%2520followed%2520broader%2520NLP%2520trends%2520--%2520beginning%2520with%2520word-frequency%2520models%2520and%2520most%2520recently%2520turning%2520to%2520large%2520language%2520models%2520%2528LLMs%2529.%2520While%2520this%2520trajectory%2520has%2520greatly%2520expanded%2520the%2520methodological%2520toolkit%252C%2520it%2520has%2520also%2520produced%2520a%2520fragmented%2520field%2520that%2520lacks%2520systematic%2520comparison%2520and%2520clear%2520guidance%2520for%2520applied%2520use.%2520To%2520address%2520this%2520gap%252C%2520we%2520identified%252025%2520CT-IPE%2520algorithms%2520through%2520a%2520systematic%2520literature%2520review%2520and%2520conducted%2520a%2520manual%2520content%2520analysis%2520of%2520their%2520modeling%2520assumptions%2520and%2520development%2520contexts.%2520To%2520compare%2520them%2520meaningfully%252C%2520we%2520introduce%2520a%2520conceptual%2520framework%2520that%2520distinguishes%2520how%2520algorithms%2520generate%252C%2520capture%252C%2520and%2520aggregate%2520textual%2520variance.%2520On%2520this%2520basis%252C%2520we%2520identify%2520four%2520methodological%2520families%2520--%2520word-frequency%252C%2520topic%2520modeling%252C%2520word%2520embedding%252C%2520and%2520LLM-based%2520approaches%2520--%2520and%2520critically%2520assess%2520their%2520assumptions%252C%2520interpretability%252C%2520scalability%252C%2520and%2520limitations.%2520Our%2520review%2520offers%2520three%2520contributions.%2520First%252C%2520it%2520provides%2520a%2520structured%2520synthesis%2520of%2520two%2520decades%2520of%2520algorithm%2520development%252C%2520clarifying%2520how%2520diverse%2520methods%2520relate%2520to%2520one%2520another.%2520Second%252C%2520it%2520translates%2520these%2520insights%2520into%2520practical%2520guidance%2520for%2520applied%2520researchers%252C%2520highlighting%2520trade-offs%2520in%2520transparency%252C%2520technical%2520requirements%252C%2520and%2520validation%2520strategies%2520that%2520shape%2520algorithm%2520choice.%2520Third%252C%2520it%2520emphasizes%2520that%2520differences%2520in%2520estimation%2520outcomes%2520across%2520algorithms%2520are%2520themselves%2520informative%252C%2520underscoring%2520the%2520need%2520for%2520systematic%2520benchmarking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Measurement%20of%20Political%20Positions%3A%20A%20Review%20of%20Text-Based%20Ideal%20Point%20Estimation%20Algorithms&entry.906535625=Patrick%20Parschan%20and%20Charlott%20Jakob&entry.1292438233=This%20article%20presents%20the%20first%20systematic%20review%20of%20unsupervised%20and%20semi-supervised%20computational%20text-based%20ideal%20point%20estimation%20%28CT-IPE%29%20algorithms%2C%20methods%20designed%20to%20infer%20latent%20political%20positions%20from%20textual%20data.%20These%20algorithms%20are%20widely%20used%20in%20political%20science%2C%20communication%2C%20computational%20social%20science%2C%20and%20computer%20science%20to%20estimate%20ideological%20preferences%20from%20parliamentary%20speeches%2C%20party%20manifestos%2C%20and%20social%20media.%20Over%20the%20past%20two%20decades%2C%20their%20development%20has%20closely%20followed%20broader%20NLP%20trends%20--%20beginning%20with%20word-frequency%20models%20and%20most%20recently%20turning%20to%20large%20language%20models%20%28LLMs%29.%20While%20this%20trajectory%20has%20greatly%20expanded%20the%20methodological%20toolkit%2C%20it%20has%20also%20produced%20a%20fragmented%20field%20that%20lacks%20systematic%20comparison%20and%20clear%20guidance%20for%20applied%20use.%20To%20address%20this%20gap%2C%20we%20identified%2025%20CT-IPE%20algorithms%20through%20a%20systematic%20literature%20review%20and%20conducted%20a%20manual%20content%20analysis%20of%20their%20modeling%20assumptions%20and%20development%20contexts.%20To%20compare%20them%20meaningfully%2C%20we%20introduce%20a%20conceptual%20framework%20that%20distinguishes%20how%20algorithms%20generate%2C%20capture%2C%20and%20aggregate%20textual%20variance.%20On%20this%20basis%2C%20we%20identify%20four%20methodological%20families%20--%20word-frequency%2C%20topic%20modeling%2C%20word%20embedding%2C%20and%20LLM-based%20approaches%20--%20and%20critically%20assess%20their%20assumptions%2C%20interpretability%2C%20scalability%2C%20and%20limitations.%20Our%20review%20offers%20three%20contributions.%20First%2C%20it%20provides%20a%20structured%20synthesis%20of%20two%20decades%20of%20algorithm%20development%2C%20clarifying%20how%20diverse%20methods%20relate%20to%20one%20another.%20Second%2C%20it%20translates%20these%20insights%20into%20practical%20guidance%20for%20applied%20researchers%2C%20highlighting%20trade-offs%20in%20transparency%2C%20technical%20requirements%2C%20and%20validation%20strategies%20that%20shape%20algorithm%20choice.%20Third%2C%20it%20emphasizes%20that%20differences%20in%20estimation%20outcomes%20across%20algorithms%20are%20themselves%20informative%2C%20underscoring%20the%20need%20for%20systematic%20benchmarking.&entry.1838667208=http%3A//arxiv.org/abs/2511.13238v1&entry.124074799=Read"},
{"title": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations", "author": "Dingling Yao and Shimeng Huang and Riccardo Cadei and Kun Zhang and Francesco Locatello", "abstract": "Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.", "link": "http://arxiv.org/abs/2505.17708v3", "date": "2025-11-17", "relevancy": 2.2639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Third%20Pillar%20of%20Causal%20Analysis%3F%20A%20Measurement%20Perspective%20on%20Causal%20Representations&body=Title%3A%20The%20Third%20Pillar%20of%20Causal%20Analysis%3F%20A%20Measurement%20Perspective%20on%20Causal%20Representations%0AAuthor%3A%20Dingling%20Yao%20and%20Shimeng%20Huang%20and%20Riccardo%20Cadei%20and%20Kun%20Zhang%20and%20Francesco%20Locatello%0AAbstract%3A%20Causal%20reasoning%20and%20discovery%2C%20two%20fundamental%20tasks%20of%20causal%20analysis%2C%20often%20face%20challenges%20in%20applications%20due%20to%20the%20complexity%2C%20noisiness%2C%20and%20high-dimensionality%20of%20real-world%20data.%20Despite%20recent%20progress%20in%20identifying%20latent%20causal%20structures%20using%20causal%20representation%20learning%20%28CRL%29%2C%20what%20makes%20learned%20representations%20useful%20for%20causal%20downstream%20tasks%20and%20how%20to%20evaluate%20them%20are%20still%20not%20well%20understood.%20In%20this%20paper%2C%20we%20reinterpret%20CRL%20using%20a%20measurement%20model%20framework%2C%20where%20the%20learned%20representations%20are%20viewed%20as%20proxy%20measurements%20of%20the%20latent%20causal%20variables.%20Our%20approach%20clarifies%20the%20conditions%20under%20which%20learned%20representations%20support%20downstream%20causal%20reasoning%20and%20provides%20a%20principled%20basis%20for%20quantitatively%20assessing%20the%20quality%20of%20representations%20using%20a%20new%20Test-based%20Measurement%20EXclusivity%20%28T-MEX%29%20score.%20We%20validate%20T-MEX%20across%20diverse%20causal%20inference%20scenarios%2C%20including%20numerical%20simulations%20and%20real-world%20ecological%20video%20analysis%2C%20demonstrating%20that%20the%20proposed%20framework%20and%20corresponding%20score%20effectively%20assess%20the%20identification%20of%20learned%20representations%20and%20their%20usefulness%20for%20causal%20downstream%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2505.17708v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Third%2520Pillar%2520of%2520Causal%2520Analysis%253F%2520A%2520Measurement%2520Perspective%2520on%2520Causal%2520Representations%26entry.906535625%3DDingling%2520Yao%2520and%2520Shimeng%2520Huang%2520and%2520Riccardo%2520Cadei%2520and%2520Kun%2520Zhang%2520and%2520Francesco%2520Locatello%26entry.1292438233%3DCausal%2520reasoning%2520and%2520discovery%252C%2520two%2520fundamental%2520tasks%2520of%2520causal%2520analysis%252C%2520often%2520face%2520challenges%2520in%2520applications%2520due%2520to%2520the%2520complexity%252C%2520noisiness%252C%2520and%2520high-dimensionality%2520of%2520real-world%2520data.%2520Despite%2520recent%2520progress%2520in%2520identifying%2520latent%2520causal%2520structures%2520using%2520causal%2520representation%2520learning%2520%2528CRL%2529%252C%2520what%2520makes%2520learned%2520representations%2520useful%2520for%2520causal%2520downstream%2520tasks%2520and%2520how%2520to%2520evaluate%2520them%2520are%2520still%2520not%2520well%2520understood.%2520In%2520this%2520paper%252C%2520we%2520reinterpret%2520CRL%2520using%2520a%2520measurement%2520model%2520framework%252C%2520where%2520the%2520learned%2520representations%2520are%2520viewed%2520as%2520proxy%2520measurements%2520of%2520the%2520latent%2520causal%2520variables.%2520Our%2520approach%2520clarifies%2520the%2520conditions%2520under%2520which%2520learned%2520representations%2520support%2520downstream%2520causal%2520reasoning%2520and%2520provides%2520a%2520principled%2520basis%2520for%2520quantitatively%2520assessing%2520the%2520quality%2520of%2520representations%2520using%2520a%2520new%2520Test-based%2520Measurement%2520EXclusivity%2520%2528T-MEX%2529%2520score.%2520We%2520validate%2520T-MEX%2520across%2520diverse%2520causal%2520inference%2520scenarios%252C%2520including%2520numerical%2520simulations%2520and%2520real-world%2520ecological%2520video%2520analysis%252C%2520demonstrating%2520that%2520the%2520proposed%2520framework%2520and%2520corresponding%2520score%2520effectively%2520assess%2520the%2520identification%2520of%2520learned%2520representations%2520and%2520their%2520usefulness%2520for%2520causal%2520downstream%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17708v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Third%20Pillar%20of%20Causal%20Analysis%3F%20A%20Measurement%20Perspective%20on%20Causal%20Representations&entry.906535625=Dingling%20Yao%20and%20Shimeng%20Huang%20and%20Riccardo%20Cadei%20and%20Kun%20Zhang%20and%20Francesco%20Locatello&entry.1292438233=Causal%20reasoning%20and%20discovery%2C%20two%20fundamental%20tasks%20of%20causal%20analysis%2C%20often%20face%20challenges%20in%20applications%20due%20to%20the%20complexity%2C%20noisiness%2C%20and%20high-dimensionality%20of%20real-world%20data.%20Despite%20recent%20progress%20in%20identifying%20latent%20causal%20structures%20using%20causal%20representation%20learning%20%28CRL%29%2C%20what%20makes%20learned%20representations%20useful%20for%20causal%20downstream%20tasks%20and%20how%20to%20evaluate%20them%20are%20still%20not%20well%20understood.%20In%20this%20paper%2C%20we%20reinterpret%20CRL%20using%20a%20measurement%20model%20framework%2C%20where%20the%20learned%20representations%20are%20viewed%20as%20proxy%20measurements%20of%20the%20latent%20causal%20variables.%20Our%20approach%20clarifies%20the%20conditions%20under%20which%20learned%20representations%20support%20downstream%20causal%20reasoning%20and%20provides%20a%20principled%20basis%20for%20quantitatively%20assessing%20the%20quality%20of%20representations%20using%20a%20new%20Test-based%20Measurement%20EXclusivity%20%28T-MEX%29%20score.%20We%20validate%20T-MEX%20across%20diverse%20causal%20inference%20scenarios%2C%20including%20numerical%20simulations%20and%20real-world%20ecological%20video%20analysis%2C%20demonstrating%20that%20the%20proposed%20framework%20and%20corresponding%20score%20effectively%20assess%20the%20identification%20of%20learned%20representations%20and%20their%20usefulness%20for%20causal%20downstream%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2505.17708v3&entry.124074799=Read"},
{"title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry", "author": "Chiyun Noh and Sangwoo Jung and Hanjun Kim and Yafei Hu and Laura Herlant and Ayoung Kim", "abstract": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO", "link": "http://arxiv.org/abs/2511.13216v1", "date": "2025-11-17", "relevancy": 2.2611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6019}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaRLILEO%3A%20Gravity-aligned%20Radar-Leg-Inertial%20Enhanced%20Odometry&body=Title%3A%20GaRLILEO%3A%20Gravity-aligned%20Radar-Leg-Inertial%20Enhanced%20Odometry%0AAuthor%3A%20Chiyun%20Noh%20and%20Sangwoo%20Jung%20and%20Hanjun%20Kim%20and%20Yafei%20Hu%20and%20Laura%20Herlant%20and%20Ayoung%20Kim%0AAbstract%3A%20Deployment%20of%20legged%20robots%20for%20navigating%20challenging%20terrains%20%28e.g.%2C%20stairs%2C%20slopes%2C%20and%20unstructured%20environments%29%20has%20gained%20increasing%20preference%20over%20wheel-based%20platforms.%20In%20such%20scenarios%2C%20accurate%20odometry%20estimation%20is%20a%20preliminary%20requirement%20for%20stable%20locomotion%2C%20localization%2C%20and%20mapping.%20Traditional%20proprioceptive%20approaches%2C%20which%20rely%20on%20leg%20kinematics%20sensor%20modalities%20and%20inertial%20sensing%2C%20suffer%20from%20irrepressible%20vertical%20drift%20caused%20by%20frequent%20contact%20impacts%2C%20foot%20slippage%2C%20and%20vibrations%2C%20particularly%20affected%20by%20inaccurate%20roll%20and%20pitch%20estimation.%20Existing%20methods%20incorporate%20exteroceptive%20sensors%20such%20as%20LiDAR%20or%20cameras.%20Further%20enhancement%20has%20been%20introduced%20by%20leveraging%20gravity%20vector%20estimation%20to%20add%20additional%20observations%20on%20roll%20and%20pitch%2C%20thereby%20increasing%20the%20accuracy%20of%20vertical%20pose%20estimation.%20However%2C%20these%20approaches%20tend%20to%20degrade%20in%20feature-sparse%20or%20repetitive%20scenes%20and%20are%20prone%20to%20errors%20from%20double-integrated%20IMU%20acceleration.%20To%20address%20these%20challenges%2C%20we%20propose%20GaRLILEO%2C%20a%20novel%20gravity-aligned%20continuous-time%20radar-leg-inertial%20odometry%20framework.%20GaRLILEO%20decouples%20velocity%20from%20the%20IMU%20by%20building%20a%20continuous-time%20ego-velocity%20spline%20from%20SoC%20radar%20Doppler%20and%20leg%20kinematics%20information%2C%20enabling%20seamless%20sensor%20fusion%20which%20mitigates%20odometry%20distortion.%20In%20addition%2C%20GaRLILEO%20can%20reliably%20capture%20accurate%20gravity%20vectors%20leveraging%20a%20novel%20soft%20S2-constrained%20gravity%20factor%2C%20improving%20vertical%20pose%20accuracy%20without%20relying%20on%20LiDAR%20or%20cameras.%20Evaluated%20on%20a%20self-collected%20real-world%20dataset%20with%20diverse%20indoor-outdoor%20trajectories%2C%20GaRLILEO%20demonstrates%20state-of-the-art%20accuracy%2C%20particularly%20in%20vertical%20odometry%20estimation%20on%20stairs%20and%20slopes.%20We%20open-source%20both%20our%20dataset%20and%20algorithm%20to%20foster%20further%20research%20in%20legged%20robot%20odometry%20and%20SLAM.%20https%3A//garlileo.github.io/GaRLILEO%0ALink%3A%20http%3A//arxiv.org/abs/2511.13216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaRLILEO%253A%2520Gravity-aligned%2520Radar-Leg-Inertial%2520Enhanced%2520Odometry%26entry.906535625%3DChiyun%2520Noh%2520and%2520Sangwoo%2520Jung%2520and%2520Hanjun%2520Kim%2520and%2520Yafei%2520Hu%2520and%2520Laura%2520Herlant%2520and%2520Ayoung%2520Kim%26entry.1292438233%3DDeployment%2520of%2520legged%2520robots%2520for%2520navigating%2520challenging%2520terrains%2520%2528e.g.%252C%2520stairs%252C%2520slopes%252C%2520and%2520unstructured%2520environments%2529%2520has%2520gained%2520increasing%2520preference%2520over%2520wheel-based%2520platforms.%2520In%2520such%2520scenarios%252C%2520accurate%2520odometry%2520estimation%2520is%2520a%2520preliminary%2520requirement%2520for%2520stable%2520locomotion%252C%2520localization%252C%2520and%2520mapping.%2520Traditional%2520proprioceptive%2520approaches%252C%2520which%2520rely%2520on%2520leg%2520kinematics%2520sensor%2520modalities%2520and%2520inertial%2520sensing%252C%2520suffer%2520from%2520irrepressible%2520vertical%2520drift%2520caused%2520by%2520frequent%2520contact%2520impacts%252C%2520foot%2520slippage%252C%2520and%2520vibrations%252C%2520particularly%2520affected%2520by%2520inaccurate%2520roll%2520and%2520pitch%2520estimation.%2520Existing%2520methods%2520incorporate%2520exteroceptive%2520sensors%2520such%2520as%2520LiDAR%2520or%2520cameras.%2520Further%2520enhancement%2520has%2520been%2520introduced%2520by%2520leveraging%2520gravity%2520vector%2520estimation%2520to%2520add%2520additional%2520observations%2520on%2520roll%2520and%2520pitch%252C%2520thereby%2520increasing%2520the%2520accuracy%2520of%2520vertical%2520pose%2520estimation.%2520However%252C%2520these%2520approaches%2520tend%2520to%2520degrade%2520in%2520feature-sparse%2520or%2520repetitive%2520scenes%2520and%2520are%2520prone%2520to%2520errors%2520from%2520double-integrated%2520IMU%2520acceleration.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GaRLILEO%252C%2520a%2520novel%2520gravity-aligned%2520continuous-time%2520radar-leg-inertial%2520odometry%2520framework.%2520GaRLILEO%2520decouples%2520velocity%2520from%2520the%2520IMU%2520by%2520building%2520a%2520continuous-time%2520ego-velocity%2520spline%2520from%2520SoC%2520radar%2520Doppler%2520and%2520leg%2520kinematics%2520information%252C%2520enabling%2520seamless%2520sensor%2520fusion%2520which%2520mitigates%2520odometry%2520distortion.%2520In%2520addition%252C%2520GaRLILEO%2520can%2520reliably%2520capture%2520accurate%2520gravity%2520vectors%2520leveraging%2520a%2520novel%2520soft%2520S2-constrained%2520gravity%2520factor%252C%2520improving%2520vertical%2520pose%2520accuracy%2520without%2520relying%2520on%2520LiDAR%2520or%2520cameras.%2520Evaluated%2520on%2520a%2520self-collected%2520real-world%2520dataset%2520with%2520diverse%2520indoor-outdoor%2520trajectories%252C%2520GaRLILEO%2520demonstrates%2520state-of-the-art%2520accuracy%252C%2520particularly%2520in%2520vertical%2520odometry%2520estimation%2520on%2520stairs%2520and%2520slopes.%2520We%2520open-source%2520both%2520our%2520dataset%2520and%2520algorithm%2520to%2520foster%2520further%2520research%2520in%2520legged%2520robot%2520odometry%2520and%2520SLAM.%2520https%253A//garlileo.github.io/GaRLILEO%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaRLILEO%3A%20Gravity-aligned%20Radar-Leg-Inertial%20Enhanced%20Odometry&entry.906535625=Chiyun%20Noh%20and%20Sangwoo%20Jung%20and%20Hanjun%20Kim%20and%20Yafei%20Hu%20and%20Laura%20Herlant%20and%20Ayoung%20Kim&entry.1292438233=Deployment%20of%20legged%20robots%20for%20navigating%20challenging%20terrains%20%28e.g.%2C%20stairs%2C%20slopes%2C%20and%20unstructured%20environments%29%20has%20gained%20increasing%20preference%20over%20wheel-based%20platforms.%20In%20such%20scenarios%2C%20accurate%20odometry%20estimation%20is%20a%20preliminary%20requirement%20for%20stable%20locomotion%2C%20localization%2C%20and%20mapping.%20Traditional%20proprioceptive%20approaches%2C%20which%20rely%20on%20leg%20kinematics%20sensor%20modalities%20and%20inertial%20sensing%2C%20suffer%20from%20irrepressible%20vertical%20drift%20caused%20by%20frequent%20contact%20impacts%2C%20foot%20slippage%2C%20and%20vibrations%2C%20particularly%20affected%20by%20inaccurate%20roll%20and%20pitch%20estimation.%20Existing%20methods%20incorporate%20exteroceptive%20sensors%20such%20as%20LiDAR%20or%20cameras.%20Further%20enhancement%20has%20been%20introduced%20by%20leveraging%20gravity%20vector%20estimation%20to%20add%20additional%20observations%20on%20roll%20and%20pitch%2C%20thereby%20increasing%20the%20accuracy%20of%20vertical%20pose%20estimation.%20However%2C%20these%20approaches%20tend%20to%20degrade%20in%20feature-sparse%20or%20repetitive%20scenes%20and%20are%20prone%20to%20errors%20from%20double-integrated%20IMU%20acceleration.%20To%20address%20these%20challenges%2C%20we%20propose%20GaRLILEO%2C%20a%20novel%20gravity-aligned%20continuous-time%20radar-leg-inertial%20odometry%20framework.%20GaRLILEO%20decouples%20velocity%20from%20the%20IMU%20by%20building%20a%20continuous-time%20ego-velocity%20spline%20from%20SoC%20radar%20Doppler%20and%20leg%20kinematics%20information%2C%20enabling%20seamless%20sensor%20fusion%20which%20mitigates%20odometry%20distortion.%20In%20addition%2C%20GaRLILEO%20can%20reliably%20capture%20accurate%20gravity%20vectors%20leveraging%20a%20novel%20soft%20S2-constrained%20gravity%20factor%2C%20improving%20vertical%20pose%20accuracy%20without%20relying%20on%20LiDAR%20or%20cameras.%20Evaluated%20on%20a%20self-collected%20real-world%20dataset%20with%20diverse%20indoor-outdoor%20trajectories%2C%20GaRLILEO%20demonstrates%20state-of-the-art%20accuracy%2C%20particularly%20in%20vertical%20odometry%20estimation%20on%20stairs%20and%20slopes.%20We%20open-source%20both%20our%20dataset%20and%20algorithm%20to%20foster%20further%20research%20in%20legged%20robot%20odometry%20and%20SLAM.%20https%3A//garlileo.github.io/GaRLILEO&entry.1838667208=http%3A//arxiv.org/abs/2511.13216v1&entry.124074799=Read"},
{"title": "Computer Vision based group activity detection and action spotting", "author": "Narthana Sivalingam and Santhirarajah Sivasthigan and Thamayanthi Mahendranathan and G. M. R. I. Godaliyadda and M. P. B. Ekanayake and H. M. V. R. Herath", "abstract": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.", "link": "http://arxiv.org/abs/2511.13315v1", "date": "2025-11-17", "relevancy": 2.2524, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5808}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.554}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer%20Vision%20based%20group%20activity%20detection%20and%20action%20spotting&body=Title%3A%20Computer%20Vision%20based%20group%20activity%20detection%20and%20action%20spotting%0AAuthor%3A%20Narthana%20Sivalingam%20and%20Santhirarajah%20Sivasthigan%20and%20Thamayanthi%20Mahendranathan%20and%20G.%20M.%20R.%20I.%20Godaliyadda%20and%20M.%20P.%20B.%20Ekanayake%20and%20H.%20M.%20V.%20R.%20Herath%0AAbstract%3A%20Group%20activity%20detection%20in%20multi-person%20scenes%20is%20challenging%20due%20to%20complex%20human%20interactions%2C%20occlusions%2C%20and%20variations%20in%20appearance%20over%20time.%20This%20work%20presents%20a%20computer%20vision%20based%20framework%20for%20group%20activity%20recognition%20and%20action%20spotting%20using%20a%20combination%20of%20deep%20learning%20models%20and%20graph%20based%20relational%20reasoning.%20The%20system%20first%20applies%20Mask%20R-CNN%20to%20obtain%20accurate%20actor%20localization%20through%20bounding%20boxes%20and%20instance%20masks.%20Multiple%20backbone%20networks%2C%20including%20Inception%20V3%2C%20MobileNet%2C%20and%20VGG16%2C%20are%20used%20to%20extract%20feature%20maps%2C%20and%20RoIAlign%20is%20applied%20to%20preserve%20spatial%20alignment%20when%20generating%20actor%20specific%20features.%20The%20mask%20information%20is%20then%20fused%20with%20the%20feature%20maps%20to%20obtain%20refined%20masked%20feature%20representations%20for%20each%20actor.%20To%20model%20interactions%20between%20individuals%2C%20we%20construct%20Actor%20Relation%20Graphs%20that%20encode%20appearance%20similarity%20and%20positional%20relations%20using%20methods%20such%20as%20normalized%20cross%20correlation%2C%20sum%20of%20absolute%20differences%2C%20and%20dot%20product.%20Graph%20Convolutional%20Networks%20operate%20on%20these%20graphs%20to%20reason%20about%20relationships%20and%20predict%20both%20individual%20actions%20and%20group%20level%20activities.%20Experiments%20on%20the%20Collective%20Activity%20dataset%20demonstrate%20that%20the%20combination%20of%20mask%20based%20feature%20refinement%2C%20robust%20similarity%20search%2C%20and%20graph%20neural%20network%20reasoning%20leads%20to%20improved%20recognition%20performance%20across%20both%20crowded%20and%20non%20crowded%20scenarios.%20This%20approach%20highlights%20the%20potential%20of%20integrating%20segmentation%2C%20feature%20extraction%2C%20and%20relational%20graph%20reasoning%20for%20complex%20video%20understanding%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer%2520Vision%2520based%2520group%2520activity%2520detection%2520and%2520action%2520spotting%26entry.906535625%3DNarthana%2520Sivalingam%2520and%2520Santhirarajah%2520Sivasthigan%2520and%2520Thamayanthi%2520Mahendranathan%2520and%2520G.%2520M.%2520R.%2520I.%2520Godaliyadda%2520and%2520M.%2520P.%2520B.%2520Ekanayake%2520and%2520H.%2520M.%2520V.%2520R.%2520Herath%26entry.1292438233%3DGroup%2520activity%2520detection%2520in%2520multi-person%2520scenes%2520is%2520challenging%2520due%2520to%2520complex%2520human%2520interactions%252C%2520occlusions%252C%2520and%2520variations%2520in%2520appearance%2520over%2520time.%2520This%2520work%2520presents%2520a%2520computer%2520vision%2520based%2520framework%2520for%2520group%2520activity%2520recognition%2520and%2520action%2520spotting%2520using%2520a%2520combination%2520of%2520deep%2520learning%2520models%2520and%2520graph%2520based%2520relational%2520reasoning.%2520The%2520system%2520first%2520applies%2520Mask%2520R-CNN%2520to%2520obtain%2520accurate%2520actor%2520localization%2520through%2520bounding%2520boxes%2520and%2520instance%2520masks.%2520Multiple%2520backbone%2520networks%252C%2520including%2520Inception%2520V3%252C%2520MobileNet%252C%2520and%2520VGG16%252C%2520are%2520used%2520to%2520extract%2520feature%2520maps%252C%2520and%2520RoIAlign%2520is%2520applied%2520to%2520preserve%2520spatial%2520alignment%2520when%2520generating%2520actor%2520specific%2520features.%2520The%2520mask%2520information%2520is%2520then%2520fused%2520with%2520the%2520feature%2520maps%2520to%2520obtain%2520refined%2520masked%2520feature%2520representations%2520for%2520each%2520actor.%2520To%2520model%2520interactions%2520between%2520individuals%252C%2520we%2520construct%2520Actor%2520Relation%2520Graphs%2520that%2520encode%2520appearance%2520similarity%2520and%2520positional%2520relations%2520using%2520methods%2520such%2520as%2520normalized%2520cross%2520correlation%252C%2520sum%2520of%2520absolute%2520differences%252C%2520and%2520dot%2520product.%2520Graph%2520Convolutional%2520Networks%2520operate%2520on%2520these%2520graphs%2520to%2520reason%2520about%2520relationships%2520and%2520predict%2520both%2520individual%2520actions%2520and%2520group%2520level%2520activities.%2520Experiments%2520on%2520the%2520Collective%2520Activity%2520dataset%2520demonstrate%2520that%2520the%2520combination%2520of%2520mask%2520based%2520feature%2520refinement%252C%2520robust%2520similarity%2520search%252C%2520and%2520graph%2520neural%2520network%2520reasoning%2520leads%2520to%2520improved%2520recognition%2520performance%2520across%2520both%2520crowded%2520and%2520non%2520crowded%2520scenarios.%2520This%2520approach%2520highlights%2520the%2520potential%2520of%2520integrating%2520segmentation%252C%2520feature%2520extraction%252C%2520and%2520relational%2520graph%2520reasoning%2520for%2520complex%2520video%2520understanding%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer%20Vision%20based%20group%20activity%20detection%20and%20action%20spotting&entry.906535625=Narthana%20Sivalingam%20and%20Santhirarajah%20Sivasthigan%20and%20Thamayanthi%20Mahendranathan%20and%20G.%20M.%20R.%20I.%20Godaliyadda%20and%20M.%20P.%20B.%20Ekanayake%20and%20H.%20M.%20V.%20R.%20Herath&entry.1292438233=Group%20activity%20detection%20in%20multi-person%20scenes%20is%20challenging%20due%20to%20complex%20human%20interactions%2C%20occlusions%2C%20and%20variations%20in%20appearance%20over%20time.%20This%20work%20presents%20a%20computer%20vision%20based%20framework%20for%20group%20activity%20recognition%20and%20action%20spotting%20using%20a%20combination%20of%20deep%20learning%20models%20and%20graph%20based%20relational%20reasoning.%20The%20system%20first%20applies%20Mask%20R-CNN%20to%20obtain%20accurate%20actor%20localization%20through%20bounding%20boxes%20and%20instance%20masks.%20Multiple%20backbone%20networks%2C%20including%20Inception%20V3%2C%20MobileNet%2C%20and%20VGG16%2C%20are%20used%20to%20extract%20feature%20maps%2C%20and%20RoIAlign%20is%20applied%20to%20preserve%20spatial%20alignment%20when%20generating%20actor%20specific%20features.%20The%20mask%20information%20is%20then%20fused%20with%20the%20feature%20maps%20to%20obtain%20refined%20masked%20feature%20representations%20for%20each%20actor.%20To%20model%20interactions%20between%20individuals%2C%20we%20construct%20Actor%20Relation%20Graphs%20that%20encode%20appearance%20similarity%20and%20positional%20relations%20using%20methods%20such%20as%20normalized%20cross%20correlation%2C%20sum%20of%20absolute%20differences%2C%20and%20dot%20product.%20Graph%20Convolutional%20Networks%20operate%20on%20these%20graphs%20to%20reason%20about%20relationships%20and%20predict%20both%20individual%20actions%20and%20group%20level%20activities.%20Experiments%20on%20the%20Collective%20Activity%20dataset%20demonstrate%20that%20the%20combination%20of%20mask%20based%20feature%20refinement%2C%20robust%20similarity%20search%2C%20and%20graph%20neural%20network%20reasoning%20leads%20to%20improved%20recognition%20performance%20across%20both%20crowded%20and%20non%20crowded%20scenarios.%20This%20approach%20highlights%20the%20potential%20of%20integrating%20segmentation%2C%20feature%20extraction%2C%20and%20relational%20graph%20reasoning%20for%20complex%20video%20understanding%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.13315v1&entry.124074799=Read"},
{"title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping", "author": "Haotian Dong and Ye Li and Rongwei Lu and Chen Tang and Shu-Tao Xia and Zhi Wang", "abstract": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.", "link": "http://arxiv.org/abs/2511.13587v1", "date": "2025-11-17", "relevancy": 2.2512, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5624}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VVS%3A%20Accelerating%20Speculative%20Decoding%20for%20Visual%20Autoregressive%20Generation%20via%20Partial%20Verification%20Skipping&body=Title%3A%20VVS%3A%20Accelerating%20Speculative%20Decoding%20for%20Visual%20Autoregressive%20Generation%20via%20Partial%20Verification%20Skipping%0AAuthor%3A%20Haotian%20Dong%20and%20Ye%20Li%20and%20Rongwei%20Lu%20and%20Chen%20Tang%20and%20Shu-Tao%20Xia%20and%20Zhi%20Wang%0AAbstract%3A%20Visual%20autoregressive%20%28AR%29%20generation%20models%20have%20demonstrated%20strong%20potential%20for%20image%20generation%2C%20yet%20their%20next-token-prediction%20paradigm%20introduces%20considerable%20inference%20latency.%20Although%20speculative%20decoding%20%28SD%29%20has%20been%20proven%20effective%20for%20accelerating%20visual%20AR%20models%2C%20its%20%22draft%20one%20step%2C%20then%20verify%20one%20step%22%20paradigm%20prevents%20a%20direct%20reduction%20of%20the%20forward%20passes%2C%20thus%20restricting%20acceleration%20potential.%20Motivated%20by%20the%20visual%20token%20interchangeability%2C%20we%20for%20the%20first%20time%20to%20explore%20verification%20skipping%20in%20the%20SD%20process%20of%20visual%20AR%20model%20generation%20to%20explicitly%20cut%20the%20number%20of%20target%20model%20forward%20passes%2C%20thereby%20reducing%20inference%20latency.%20Based%20on%20an%20analysis%20of%20the%20drafting%20stage%27s%20characteristics%2C%20we%20observe%20that%20verification%20redundancy%20and%20stale%20feature%20reusability%20are%20key%20factors%20to%20retain%20generation%20quality%20and%20speedup%20for%20verification-free%20steps.%20Inspired%20by%20these%20two%20observations%2C%20we%20propose%20a%20novel%20SD%20framework%20VVS%20to%20accelerate%20visual%20AR%20generation%20via%20partial%20verification%20skipping%2C%20which%20integrates%20three%20complementary%20modules%3A%20%281%29%20a%20verification-free%20token%20selector%20with%20dynamical%20truncation%2C%20%282%29%20token-level%20feature%20caching%20and%20reuse%2C%20and%20%283%29%20fine-grained%20skipped%20step%20scheduling.%20Consequently%2C%20VVS%20reduces%20the%20number%20of%20target%20model%20forward%20passes%20by%20a%20factor%20of%20%242.8%5Ctimes%24%20relative%20to%20vanilla%20AR%20decoding%20while%20maintaining%20competitive%20generation%20quality%2C%20offering%20a%20superior%20speed-quality%20trade-off%20over%20conventional%20SD%20frameworks%20and%20revealing%20strong%20potential%20to%20reshape%20the%20SD%20paradigm.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVVS%253A%2520Accelerating%2520Speculative%2520Decoding%2520for%2520Visual%2520Autoregressive%2520Generation%2520via%2520Partial%2520Verification%2520Skipping%26entry.906535625%3DHaotian%2520Dong%2520and%2520Ye%2520Li%2520and%2520Rongwei%2520Lu%2520and%2520Chen%2520Tang%2520and%2520Shu-Tao%2520Xia%2520and%2520Zhi%2520Wang%26entry.1292438233%3DVisual%2520autoregressive%2520%2528AR%2529%2520generation%2520models%2520have%2520demonstrated%2520strong%2520potential%2520for%2520image%2520generation%252C%2520yet%2520their%2520next-token-prediction%2520paradigm%2520introduces%2520considerable%2520inference%2520latency.%2520Although%2520speculative%2520decoding%2520%2528SD%2529%2520has%2520been%2520proven%2520effective%2520for%2520accelerating%2520visual%2520AR%2520models%252C%2520its%2520%2522draft%2520one%2520step%252C%2520then%2520verify%2520one%2520step%2522%2520paradigm%2520prevents%2520a%2520direct%2520reduction%2520of%2520the%2520forward%2520passes%252C%2520thus%2520restricting%2520acceleration%2520potential.%2520Motivated%2520by%2520the%2520visual%2520token%2520interchangeability%252C%2520we%2520for%2520the%2520first%2520time%2520to%2520explore%2520verification%2520skipping%2520in%2520the%2520SD%2520process%2520of%2520visual%2520AR%2520model%2520generation%2520to%2520explicitly%2520cut%2520the%2520number%2520of%2520target%2520model%2520forward%2520passes%252C%2520thereby%2520reducing%2520inference%2520latency.%2520Based%2520on%2520an%2520analysis%2520of%2520the%2520drafting%2520stage%2527s%2520characteristics%252C%2520we%2520observe%2520that%2520verification%2520redundancy%2520and%2520stale%2520feature%2520reusability%2520are%2520key%2520factors%2520to%2520retain%2520generation%2520quality%2520and%2520speedup%2520for%2520verification-free%2520steps.%2520Inspired%2520by%2520these%2520two%2520observations%252C%2520we%2520propose%2520a%2520novel%2520SD%2520framework%2520VVS%2520to%2520accelerate%2520visual%2520AR%2520generation%2520via%2520partial%2520verification%2520skipping%252C%2520which%2520integrates%2520three%2520complementary%2520modules%253A%2520%25281%2529%2520a%2520verification-free%2520token%2520selector%2520with%2520dynamical%2520truncation%252C%2520%25282%2529%2520token-level%2520feature%2520caching%2520and%2520reuse%252C%2520and%2520%25283%2529%2520fine-grained%2520skipped%2520step%2520scheduling.%2520Consequently%252C%2520VVS%2520reduces%2520the%2520number%2520of%2520target%2520model%2520forward%2520passes%2520by%2520a%2520factor%2520of%2520%25242.8%255Ctimes%2524%2520relative%2520to%2520vanilla%2520AR%2520decoding%2520while%2520maintaining%2520competitive%2520generation%2520quality%252C%2520offering%2520a%2520superior%2520speed-quality%2520trade-off%2520over%2520conventional%2520SD%2520frameworks%2520and%2520revealing%2520strong%2520potential%2520to%2520reshape%2520the%2520SD%2520paradigm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VVS%3A%20Accelerating%20Speculative%20Decoding%20for%20Visual%20Autoregressive%20Generation%20via%20Partial%20Verification%20Skipping&entry.906535625=Haotian%20Dong%20and%20Ye%20Li%20and%20Rongwei%20Lu%20and%20Chen%20Tang%20and%20Shu-Tao%20Xia%20and%20Zhi%20Wang&entry.1292438233=Visual%20autoregressive%20%28AR%29%20generation%20models%20have%20demonstrated%20strong%20potential%20for%20image%20generation%2C%20yet%20their%20next-token-prediction%20paradigm%20introduces%20considerable%20inference%20latency.%20Although%20speculative%20decoding%20%28SD%29%20has%20been%20proven%20effective%20for%20accelerating%20visual%20AR%20models%2C%20its%20%22draft%20one%20step%2C%20then%20verify%20one%20step%22%20paradigm%20prevents%20a%20direct%20reduction%20of%20the%20forward%20passes%2C%20thus%20restricting%20acceleration%20potential.%20Motivated%20by%20the%20visual%20token%20interchangeability%2C%20we%20for%20the%20first%20time%20to%20explore%20verification%20skipping%20in%20the%20SD%20process%20of%20visual%20AR%20model%20generation%20to%20explicitly%20cut%20the%20number%20of%20target%20model%20forward%20passes%2C%20thereby%20reducing%20inference%20latency.%20Based%20on%20an%20analysis%20of%20the%20drafting%20stage%27s%20characteristics%2C%20we%20observe%20that%20verification%20redundancy%20and%20stale%20feature%20reusability%20are%20key%20factors%20to%20retain%20generation%20quality%20and%20speedup%20for%20verification-free%20steps.%20Inspired%20by%20these%20two%20observations%2C%20we%20propose%20a%20novel%20SD%20framework%20VVS%20to%20accelerate%20visual%20AR%20generation%20via%20partial%20verification%20skipping%2C%20which%20integrates%20three%20complementary%20modules%3A%20%281%29%20a%20verification-free%20token%20selector%20with%20dynamical%20truncation%2C%20%282%29%20token-level%20feature%20caching%20and%20reuse%2C%20and%20%283%29%20fine-grained%20skipped%20step%20scheduling.%20Consequently%2C%20VVS%20reduces%20the%20number%20of%20target%20model%20forward%20passes%20by%20a%20factor%20of%20%242.8%5Ctimes%24%20relative%20to%20vanilla%20AR%20decoding%20while%20maintaining%20competitive%20generation%20quality%2C%20offering%20a%20superior%20speed-quality%20trade-off%20over%20conventional%20SD%20frameworks%20and%20revealing%20strong%20potential%20to%20reshape%20the%20SD%20paradigm.&entry.1838667208=http%3A//arxiv.org/abs/2511.13587v1&entry.124074799=Read"},
{"title": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation", "author": "Quoc-Huy Trinh and Mustapha Abdullahi and Do Duy Hung Trinh and Bo Zhao and Debesh Jha", "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.", "link": "http://arxiv.org/abs/2511.11177v2", "date": "2025-11-17", "relevancy": 2.2479, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Viper-F1%3A%20Fast%20and%20Fine-Grained%20Multimodal%20Understanding%20with%20Cross-Modal%20State-Space%20Modulation&body=Title%3A%20Viper-F1%3A%20Fast%20and%20Fine-Grained%20Multimodal%20Understanding%20with%20Cross-Modal%20State-Space%20Modulation%0AAuthor%3A%20Quoc-Huy%20Trinh%20and%20Mustapha%20Abdullahi%20and%20Do%20Duy%20Hung%20Trinh%20and%20Bo%20Zhao%20and%20Debesh%20Jha%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20impressive%20progress%20in%20vision-language%20understanding%2C%20yet%20their%20high%20computational%20cost%20limits%20deployment%20in%20resource-constrained%20scenarios%20such%20as%20robotic%20manipulation%2C%20personal%20assistants%2C%20and%20smart%20cameras.%20Most%20existing%20methods%20rely%20on%20Transformer-based%20cross-attention%2C%20whose%20quadratic%20complexity%20hinders%20efficiency.%20Moreover%2C%20small%20vision-language%20models%20often%20struggle%20to%20precisely%20capture%20fine-grained%2C%20task-relevant%20visual%20regions%2C%20leading%20to%20degraded%20performance%20on%20fine-grained%20reasoning%20tasks%20that%20limit%20their%20effectiveness%20in%20the%20real%20world.%20To%20address%20these%20issues%2C%20we%20introduce%20Viper-F1%2C%20a%20Hybrid%20State-Space%20Vision-Language%20Model%20that%20replaces%20attention%20with%20efficient%20Liquid%20State-Space%20Dynamics.%20To%20further%20enhance%20visual%20grounding%2C%20we%20propose%20a%20Token-Grid%20Correlation%20Module%2C%20which%20computes%20lightweight%20correlations%20between%20text%20tokens%20and%20image%20patches%20and%20modulates%20the%20state-space%20dynamics%20via%20FiLM%20conditioning.%20This%20enables%20the%20model%20to%20selectively%20emphasize%20visual%20regions%20relevant%20to%20the%20textual%20prompt%20while%20maintaining%20linear-time%20inference.%20Experimental%20results%20across%20multiple%20benchmarks%20demonstrate%20that%20Viper-F1%20achieves%20accurate%2C%20fine-grained%20understanding%20with%20significantly%20improved%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViper-F1%253A%2520Fast%2520and%2520Fine-Grained%2520Multimodal%2520Understanding%2520with%2520Cross-Modal%2520State-Space%2520Modulation%26entry.906535625%3DQuoc-Huy%2520Trinh%2520and%2520Mustapha%2520Abdullahi%2520and%2520Do%2520Duy%2520Hung%2520Trinh%2520and%2520Bo%2520Zhao%2520and%2520Debesh%2520Jha%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520enabled%2520impressive%2520progress%2520in%2520vision-language%2520understanding%252C%2520yet%2520their%2520high%2520computational%2520cost%2520limits%2520deployment%2520in%2520resource-constrained%2520scenarios%2520such%2520as%2520robotic%2520manipulation%252C%2520personal%2520assistants%252C%2520and%2520smart%2520cameras.%2520Most%2520existing%2520methods%2520rely%2520on%2520Transformer-based%2520cross-attention%252C%2520whose%2520quadratic%2520complexity%2520hinders%2520efficiency.%2520Moreover%252C%2520small%2520vision-language%2520models%2520often%2520struggle%2520to%2520precisely%2520capture%2520fine-grained%252C%2520task-relevant%2520visual%2520regions%252C%2520leading%2520to%2520degraded%2520performance%2520on%2520fine-grained%2520reasoning%2520tasks%2520that%2520limit%2520their%2520effectiveness%2520in%2520the%2520real%2520world.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Viper-F1%252C%2520a%2520Hybrid%2520State-Space%2520Vision-Language%2520Model%2520that%2520replaces%2520attention%2520with%2520efficient%2520Liquid%2520State-Space%2520Dynamics.%2520To%2520further%2520enhance%2520visual%2520grounding%252C%2520we%2520propose%2520a%2520Token-Grid%2520Correlation%2520Module%252C%2520which%2520computes%2520lightweight%2520correlations%2520between%2520text%2520tokens%2520and%2520image%2520patches%2520and%2520modulates%2520the%2520state-space%2520dynamics%2520via%2520FiLM%2520conditioning.%2520This%2520enables%2520the%2520model%2520to%2520selectively%2520emphasize%2520visual%2520regions%2520relevant%2520to%2520the%2520textual%2520prompt%2520while%2520maintaining%2520linear-time%2520inference.%2520Experimental%2520results%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520Viper-F1%2520achieves%2520accurate%252C%2520fine-grained%2520understanding%2520with%2520significantly%2520improved%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Viper-F1%3A%20Fast%20and%20Fine-Grained%20Multimodal%20Understanding%20with%20Cross-Modal%20State-Space%20Modulation&entry.906535625=Quoc-Huy%20Trinh%20and%20Mustapha%20Abdullahi%20and%20Do%20Duy%20Hung%20Trinh%20and%20Bo%20Zhao%20and%20Debesh%20Jha&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20impressive%20progress%20in%20vision-language%20understanding%2C%20yet%20their%20high%20computational%20cost%20limits%20deployment%20in%20resource-constrained%20scenarios%20such%20as%20robotic%20manipulation%2C%20personal%20assistants%2C%20and%20smart%20cameras.%20Most%20existing%20methods%20rely%20on%20Transformer-based%20cross-attention%2C%20whose%20quadratic%20complexity%20hinders%20efficiency.%20Moreover%2C%20small%20vision-language%20models%20often%20struggle%20to%20precisely%20capture%20fine-grained%2C%20task-relevant%20visual%20regions%2C%20leading%20to%20degraded%20performance%20on%20fine-grained%20reasoning%20tasks%20that%20limit%20their%20effectiveness%20in%20the%20real%20world.%20To%20address%20these%20issues%2C%20we%20introduce%20Viper-F1%2C%20a%20Hybrid%20State-Space%20Vision-Language%20Model%20that%20replaces%20attention%20with%20efficient%20Liquid%20State-Space%20Dynamics.%20To%20further%20enhance%20visual%20grounding%2C%20we%20propose%20a%20Token-Grid%20Correlation%20Module%2C%20which%20computes%20lightweight%20correlations%20between%20text%20tokens%20and%20image%20patches%20and%20modulates%20the%20state-space%20dynamics%20via%20FiLM%20conditioning.%20This%20enables%20the%20model%20to%20selectively%20emphasize%20visual%20regions%20relevant%20to%20the%20textual%20prompt%20while%20maintaining%20linear-time%20inference.%20Experimental%20results%20across%20multiple%20benchmarks%20demonstrate%20that%20Viper-F1%20achieves%20accurate%2C%20fine-grained%20understanding%20with%20significantly%20improved%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.11177v2&entry.124074799=Read"},
{"title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing", "author": "Colin Laganier and Liam Fletcher and Elim Kwan and Richard Walters and Victoria Nockles", "abstract": "Rapid analysis of satellite imagery within minutes-to-hours of acquisition is increasingly vital for many remote sensing applications, and is an essential component for developing next-generation autonomous and distributed satellite systems. On-satellite machine learning (ML) has the potential for such rapid analysis, by overcoming latency associated with intermittent satellite connectivity to ground stations or relay satellites, but state-of-the-art models are often too large or power-hungry for on-board deployment. Vessel detection using Synthetic Aperture Radar (SAR) is a critical time-sensitive application in maritime security that exemplifies this challenge. SAR vessel detection has previously been demonstrated only by ML models that either are too large for satellite deployment, have not been developed for sufficiently low-power hardware, or have only been tested on small SAR datasets that do not sufficiently represent the difficulty of the real-world task. Here we systematically explore a suite of architectural adaptations to develop a novel YOLOv8 architecture optimized for this task and FPGA-based processing. We deploy our model on a Kria KV260 MPSoC, and show it can analyze a ~700 megapixel SAR image in less than a minute, within common satellite power constraints (<10W). Our model has detection and classification performance only ~2% and 3% lower than values from state-of-the-art GPU-based models on the largest and most diverse open SAR vessel dataset, xView3-SAR, despite being ~50 and ~2500 times more computationally efficient. This work represents a key contribution towards on-satellite ML for time-critical SAR analysis, and more autonomous, scalable satellites.", "link": "http://arxiv.org/abs/2507.04842v2", "date": "2025-11-17", "relevancy": 1.5369, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5317}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20SAR%20Vessel%20Detection%20for%20FPGA-Based%20On-Satellite%20Sensing&body=Title%3A%20Efficient%20SAR%20Vessel%20Detection%20for%20FPGA-Based%20On-Satellite%20Sensing%0AAuthor%3A%20Colin%20Laganier%20and%20Liam%20Fletcher%20and%20Elim%20Kwan%20and%20Richard%20Walters%20and%20Victoria%20Nockles%0AAbstract%3A%20Rapid%20analysis%20of%20satellite%20imagery%20within%20minutes-to-hours%20of%20acquisition%20is%20increasingly%20vital%20for%20many%20remote%20sensing%20applications%2C%20and%20is%20an%20essential%20component%20for%20developing%20next-generation%20autonomous%20and%20distributed%20satellite%20systems.%20On-satellite%20machine%20learning%20%28ML%29%20has%20the%20potential%20for%20such%20rapid%20analysis%2C%20by%20overcoming%20latency%20associated%20with%20intermittent%20satellite%20connectivity%20to%20ground%20stations%20or%20relay%20satellites%2C%20but%20state-of-the-art%20models%20are%20often%20too%20large%20or%20power-hungry%20for%20on-board%20deployment.%20Vessel%20detection%20using%20Synthetic%20Aperture%20Radar%20%28SAR%29%20is%20a%20critical%20time-sensitive%20application%20in%20maritime%20security%20that%20exemplifies%20this%20challenge.%20SAR%20vessel%20detection%20has%20previously%20been%20demonstrated%20only%20by%20ML%20models%20that%20either%20are%20too%20large%20for%20satellite%20deployment%2C%20have%20not%20been%20developed%20for%20sufficiently%20low-power%20hardware%2C%20or%20have%20only%20been%20tested%20on%20small%20SAR%20datasets%20that%20do%20not%20sufficiently%20represent%20the%20difficulty%20of%20the%20real-world%20task.%20Here%20we%20systematically%20explore%20a%20suite%20of%20architectural%20adaptations%20to%20develop%20a%20novel%20YOLOv8%20architecture%20optimized%20for%20this%20task%20and%20FPGA-based%20processing.%20We%20deploy%20our%20model%20on%20a%20Kria%20KV260%20MPSoC%2C%20and%20show%20it%20can%20analyze%20a%20~700%20megapixel%20SAR%20image%20in%20less%20than%20a%20minute%2C%20within%20common%20satellite%20power%20constraints%20%28%3C10W%29.%20Our%20model%20has%20detection%20and%20classification%20performance%20only%20~2%25%20and%203%25%20lower%20than%20values%20from%20state-of-the-art%20GPU-based%20models%20on%20the%20largest%20and%20most%20diverse%20open%20SAR%20vessel%20dataset%2C%20xView3-SAR%2C%20despite%20being%20~50%20and%20~2500%20times%20more%20computationally%20efficient.%20This%20work%20represents%20a%20key%20contribution%20towards%20on-satellite%20ML%20for%20time-critical%20SAR%20analysis%2C%20and%20more%20autonomous%2C%20scalable%20satellites.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520SAR%2520Vessel%2520Detection%2520for%2520FPGA-Based%2520On-Satellite%2520Sensing%26entry.906535625%3DColin%2520Laganier%2520and%2520Liam%2520Fletcher%2520and%2520Elim%2520Kwan%2520and%2520Richard%2520Walters%2520and%2520Victoria%2520Nockles%26entry.1292438233%3DRapid%2520analysis%2520of%2520satellite%2520imagery%2520within%2520minutes-to-hours%2520of%2520acquisition%2520is%2520increasingly%2520vital%2520for%2520many%2520remote%2520sensing%2520applications%252C%2520and%2520is%2520an%2520essential%2520component%2520for%2520developing%2520next-generation%2520autonomous%2520and%2520distributed%2520satellite%2520systems.%2520On-satellite%2520machine%2520learning%2520%2528ML%2529%2520has%2520the%2520potential%2520for%2520such%2520rapid%2520analysis%252C%2520by%2520overcoming%2520latency%2520associated%2520with%2520intermittent%2520satellite%2520connectivity%2520to%2520ground%2520stations%2520or%2520relay%2520satellites%252C%2520but%2520state-of-the-art%2520models%2520are%2520often%2520too%2520large%2520or%2520power-hungry%2520for%2520on-board%2520deployment.%2520Vessel%2520detection%2520using%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520is%2520a%2520critical%2520time-sensitive%2520application%2520in%2520maritime%2520security%2520that%2520exemplifies%2520this%2520challenge.%2520SAR%2520vessel%2520detection%2520has%2520previously%2520been%2520demonstrated%2520only%2520by%2520ML%2520models%2520that%2520either%2520are%2520too%2520large%2520for%2520satellite%2520deployment%252C%2520have%2520not%2520been%2520developed%2520for%2520sufficiently%2520low-power%2520hardware%252C%2520or%2520have%2520only%2520been%2520tested%2520on%2520small%2520SAR%2520datasets%2520that%2520do%2520not%2520sufficiently%2520represent%2520the%2520difficulty%2520of%2520the%2520real-world%2520task.%2520Here%2520we%2520systematically%2520explore%2520a%2520suite%2520of%2520architectural%2520adaptations%2520to%2520develop%2520a%2520novel%2520YOLOv8%2520architecture%2520optimized%2520for%2520this%2520task%2520and%2520FPGA-based%2520processing.%2520We%2520deploy%2520our%2520model%2520on%2520a%2520Kria%2520KV260%2520MPSoC%252C%2520and%2520show%2520it%2520can%2520analyze%2520a%2520~700%2520megapixel%2520SAR%2520image%2520in%2520less%2520than%2520a%2520minute%252C%2520within%2520common%2520satellite%2520power%2520constraints%2520%2528%253C10W%2529.%2520Our%2520model%2520has%2520detection%2520and%2520classification%2520performance%2520only%2520~2%2525%2520and%25203%2525%2520lower%2520than%2520values%2520from%2520state-of-the-art%2520GPU-based%2520models%2520on%2520the%2520largest%2520and%2520most%2520diverse%2520open%2520SAR%2520vessel%2520dataset%252C%2520xView3-SAR%252C%2520despite%2520being%2520~50%2520and%2520~2500%2520times%2520more%2520computationally%2520efficient.%2520This%2520work%2520represents%2520a%2520key%2520contribution%2520towards%2520on-satellite%2520ML%2520for%2520time-critical%2520SAR%2520analysis%252C%2520and%2520more%2520autonomous%252C%2520scalable%2520satellites.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20SAR%20Vessel%20Detection%20for%20FPGA-Based%20On-Satellite%20Sensing&entry.906535625=Colin%20Laganier%20and%20Liam%20Fletcher%20and%20Elim%20Kwan%20and%20Richard%20Walters%20and%20Victoria%20Nockles&entry.1292438233=Rapid%20analysis%20of%20satellite%20imagery%20within%20minutes-to-hours%20of%20acquisition%20is%20increasingly%20vital%20for%20many%20remote%20sensing%20applications%2C%20and%20is%20an%20essential%20component%20for%20developing%20next-generation%20autonomous%20and%20distributed%20satellite%20systems.%20On-satellite%20machine%20learning%20%28ML%29%20has%20the%20potential%20for%20such%20rapid%20analysis%2C%20by%20overcoming%20latency%20associated%20with%20intermittent%20satellite%20connectivity%20to%20ground%20stations%20or%20relay%20satellites%2C%20but%20state-of-the-art%20models%20are%20often%20too%20large%20or%20power-hungry%20for%20on-board%20deployment.%20Vessel%20detection%20using%20Synthetic%20Aperture%20Radar%20%28SAR%29%20is%20a%20critical%20time-sensitive%20application%20in%20maritime%20security%20that%20exemplifies%20this%20challenge.%20SAR%20vessel%20detection%20has%20previously%20been%20demonstrated%20only%20by%20ML%20models%20that%20either%20are%20too%20large%20for%20satellite%20deployment%2C%20have%20not%20been%20developed%20for%20sufficiently%20low-power%20hardware%2C%20or%20have%20only%20been%20tested%20on%20small%20SAR%20datasets%20that%20do%20not%20sufficiently%20represent%20the%20difficulty%20of%20the%20real-world%20task.%20Here%20we%20systematically%20explore%20a%20suite%20of%20architectural%20adaptations%20to%20develop%20a%20novel%20YOLOv8%20architecture%20optimized%20for%20this%20task%20and%20FPGA-based%20processing.%20We%20deploy%20our%20model%20on%20a%20Kria%20KV260%20MPSoC%2C%20and%20show%20it%20can%20analyze%20a%20~700%20megapixel%20SAR%20image%20in%20less%20than%20a%20minute%2C%20within%20common%20satellite%20power%20constraints%20%28%3C10W%29.%20Our%20model%20has%20detection%20and%20classification%20performance%20only%20~2%25%20and%203%25%20lower%20than%20values%20from%20state-of-the-art%20GPU-based%20models%20on%20the%20largest%20and%20most%20diverse%20open%20SAR%20vessel%20dataset%2C%20xView3-SAR%2C%20despite%20being%20~50%20and%20~2500%20times%20more%20computationally%20efficient.%20This%20work%20represents%20a%20key%20contribution%20towards%20on-satellite%20ML%20for%20time-critical%20SAR%20analysis%2C%20and%20more%20autonomous%2C%20scalable%20satellites.&entry.1838667208=http%3A//arxiv.org/abs/2507.04842v2&entry.124074799=Read"},
{"title": "LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation", "author": "Zeyu Wang and Zilong Chen and Chenhui Gou and Feng Li and Chaorui Deng and Deyao Zhu and Kunchang Li and Weihao Yu and Haoqin Tu and Haoqi Fan and Cihang Xie", "abstract": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.", "link": "http://arxiv.org/abs/2510.22946v3", "date": "2025-11-17", "relevancy": 1.1254, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5728}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5611}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightFusion%3A%20A%20Light-weighted%2C%20Double%20Fusion%20Framework%20for%20Unified%20Multimodal%20Understanding%20and%20Generation&body=Title%3A%20LightFusion%3A%20A%20Light-weighted%2C%20Double%20Fusion%20Framework%20for%20Unified%20Multimodal%20Understanding%20and%20Generation%0AAuthor%3A%20Zeyu%20Wang%20and%20Zilong%20Chen%20and%20Chenhui%20Gou%20and%20Feng%20Li%20and%20Chaorui%20Deng%20and%20Deyao%20Zhu%20and%20Kunchang%20Li%20and%20Weihao%20Yu%20and%20Haoqin%20Tu%20and%20Haoqi%20Fan%20and%20Cihang%20Xie%0AAbstract%3A%20Unified%20multimodal%20models%20have%20recently%20shown%20remarkable%20gains%20in%20both%20capability%20and%20versatility%2C%20yet%20most%20leading%20systems%20are%20still%20trained%20from%20scratch%20and%20require%20substantial%20computational%20resources.%20In%20this%20paper%2C%20we%20show%20that%20competitive%20performance%20can%20be%20obtained%20far%20more%20efficiently%20by%20strategically%20fusing%20publicly%20available%20models%20specialized%20for%20either%20generation%20or%20understanding.%20Our%20key%20design%20is%20to%20retain%20the%20original%20blocks%20while%20additionally%20interleaving%20multimodal%20self-attention%20blocks%20throughout%20the%20networks.%20This%20double%20fusion%20mechanism%20%281%29%20effectively%20enables%20rich%20multi-modal%20fusion%20while%20largely%20preserving%20the%20original%20strengths%20of%20the%20base%20models%2C%20and%20%282%29%20catalyzes%20synergistic%20fusion%20of%20high-level%20semantic%20representations%20from%20the%20understanding%20encoder%20with%20low-level%20spatial%20signals%20from%20the%20generation%20encoder.%20By%20training%20with%20only%20~%2035B%20tokens%2C%20this%20approach%20achieves%20strong%20results%20across%20multiple%20benchmarks%3A%200.91%20on%20GenEval%20for%20compositional%20text-to-image%20generation%2C%2082.16%20on%20DPG-Bench%20for%20complex%20text-to-image%20generation%2C%206.06%20on%20GEditBench%2C%20and%203.77%20on%20ImgEdit-Bench%20for%20image%20editing.%20By%20fully%20releasing%20the%20entire%20suite%20of%20code%2C%20model%20weights%2C%20and%20datasets%2C%20we%20hope%20to%20support%20future%20research%20on%20unified%20multimodal%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightFusion%253A%2520A%2520Light-weighted%252C%2520Double%2520Fusion%2520Framework%2520for%2520Unified%2520Multimodal%2520Understanding%2520and%2520Generation%26entry.906535625%3DZeyu%2520Wang%2520and%2520Zilong%2520Chen%2520and%2520Chenhui%2520Gou%2520and%2520Feng%2520Li%2520and%2520Chaorui%2520Deng%2520and%2520Deyao%2520Zhu%2520and%2520Kunchang%2520Li%2520and%2520Weihao%2520Yu%2520and%2520Haoqin%2520Tu%2520and%2520Haoqi%2520Fan%2520and%2520Cihang%2520Xie%26entry.1292438233%3DUnified%2520multimodal%2520models%2520have%2520recently%2520shown%2520remarkable%2520gains%2520in%2520both%2520capability%2520and%2520versatility%252C%2520yet%2520most%2520leading%2520systems%2520are%2520still%2520trained%2520from%2520scratch%2520and%2520require%2520substantial%2520computational%2520resources.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520competitive%2520performance%2520can%2520be%2520obtained%2520far%2520more%2520efficiently%2520by%2520strategically%2520fusing%2520publicly%2520available%2520models%2520specialized%2520for%2520either%2520generation%2520or%2520understanding.%2520Our%2520key%2520design%2520is%2520to%2520retain%2520the%2520original%2520blocks%2520while%2520additionally%2520interleaving%2520multimodal%2520self-attention%2520blocks%2520throughout%2520the%2520networks.%2520This%2520double%2520fusion%2520mechanism%2520%25281%2529%2520effectively%2520enables%2520rich%2520multi-modal%2520fusion%2520while%2520largely%2520preserving%2520the%2520original%2520strengths%2520of%2520the%2520base%2520models%252C%2520and%2520%25282%2529%2520catalyzes%2520synergistic%2520fusion%2520of%2520high-level%2520semantic%2520representations%2520from%2520the%2520understanding%2520encoder%2520with%2520low-level%2520spatial%2520signals%2520from%2520the%2520generation%2520encoder.%2520By%2520training%2520with%2520only%2520~%252035B%2520tokens%252C%2520this%2520approach%2520achieves%2520strong%2520results%2520across%2520multiple%2520benchmarks%253A%25200.91%2520on%2520GenEval%2520for%2520compositional%2520text-to-image%2520generation%252C%252082.16%2520on%2520DPG-Bench%2520for%2520complex%2520text-to-image%2520generation%252C%25206.06%2520on%2520GEditBench%252C%2520and%25203.77%2520on%2520ImgEdit-Bench%2520for%2520image%2520editing.%2520By%2520fully%2520releasing%2520the%2520entire%2520suite%2520of%2520code%252C%2520model%2520weights%252C%2520and%2520datasets%252C%2520we%2520hope%2520to%2520support%2520future%2520research%2520on%2520unified%2520multimodal%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightFusion%3A%20A%20Light-weighted%2C%20Double%20Fusion%20Framework%20for%20Unified%20Multimodal%20Understanding%20and%20Generation&entry.906535625=Zeyu%20Wang%20and%20Zilong%20Chen%20and%20Chenhui%20Gou%20and%20Feng%20Li%20and%20Chaorui%20Deng%20and%20Deyao%20Zhu%20and%20Kunchang%20Li%20and%20Weihao%20Yu%20and%20Haoqin%20Tu%20and%20Haoqi%20Fan%20and%20Cihang%20Xie&entry.1292438233=Unified%20multimodal%20models%20have%20recently%20shown%20remarkable%20gains%20in%20both%20capability%20and%20versatility%2C%20yet%20most%20leading%20systems%20are%20still%20trained%20from%20scratch%20and%20require%20substantial%20computational%20resources.%20In%20this%20paper%2C%20we%20show%20that%20competitive%20performance%20can%20be%20obtained%20far%20more%20efficiently%20by%20strategically%20fusing%20publicly%20available%20models%20specialized%20for%20either%20generation%20or%20understanding.%20Our%20key%20design%20is%20to%20retain%20the%20original%20blocks%20while%20additionally%20interleaving%20multimodal%20self-attention%20blocks%20throughout%20the%20networks.%20This%20double%20fusion%20mechanism%20%281%29%20effectively%20enables%20rich%20multi-modal%20fusion%20while%20largely%20preserving%20the%20original%20strengths%20of%20the%20base%20models%2C%20and%20%282%29%20catalyzes%20synergistic%20fusion%20of%20high-level%20semantic%20representations%20from%20the%20understanding%20encoder%20with%20low-level%20spatial%20signals%20from%20the%20generation%20encoder.%20By%20training%20with%20only%20~%2035B%20tokens%2C%20this%20approach%20achieves%20strong%20results%20across%20multiple%20benchmarks%3A%200.91%20on%20GenEval%20for%20compositional%20text-to-image%20generation%2C%2082.16%20on%20DPG-Bench%20for%20complex%20text-to-image%20generation%2C%206.06%20on%20GEditBench%2C%20and%203.77%20on%20ImgEdit-Bench%20for%20image%20editing.%20By%20fully%20releasing%20the%20entire%20suite%20of%20code%2C%20model%20weights%2C%20and%20datasets%2C%20we%20hope%20to%20support%20future%20research%20on%20unified%20multimodal%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2510.22946v3&entry.124074799=Read"},
{"title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "author": "Dongyoon Hahm and Taywon Min and Woogyeol Jin and Kimin Lee", "abstract": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.", "link": "http://arxiv.org/abs/2508.14031v2", "date": "2025-11-17", "relevancy": 1.9369, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5242}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4662}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unintended%20Misalignment%20from%20Agentic%20Fine-Tuning%3A%20Risks%20and%20Mitigation&body=Title%3A%20Unintended%20Misalignment%20from%20Agentic%20Fine-Tuning%3A%20Risks%20and%20Mitigation%0AAuthor%3A%20Dongyoon%20Hahm%20and%20Taywon%20Min%20and%20Woogyeol%20Jin%20and%20Kimin%20Lee%0AAbstract%3A%20Beyond%20simple%20text%20generation%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20evolved%20into%20agentic%20systems%20capable%20of%20planning%20and%20interacting%20with%20external%20tools%20to%20solve%20complex%20tasks.%20This%20evolution%20involves%20fine-tuning%20LLMs%20on%20agent-specific%20tasks%20to%20enhance%20their%20proficiency.%20However%2C%20safety%20concerns%20are%20frequently%20overlooked%20during%20this%20fine-tuning%20process.%20In%20this%20work%2C%20we%20show%20that%20aligned%20LLMs%20can%20become%20unintentionally%20misaligned%2C%20leading%20to%20a%20higher%20likelihood%20of%20executing%20harmful%20tasks%20and%20a%20reduced%20tendency%20to%20refuse%20them%20when%20fine-tuned%20to%20execute%20agentic%20tasks.%20To%20address%20these%20safety%20challenges%2C%20we%20propose%20Prefix%20INjection%20Guard%20%28PING%29%2C%20a%20simple%20yet%20effective%20method%20that%20prepends%20automatically%20generated%20natural%20language%20prefixes%20to%20agent%20responses%2C%20guiding%20them%20to%20refuse%20harmful%20requests%20while%20preserving%20performance%20on%20benign%20tasks.%20Specifically%2C%20we%20introduce%20an%20iterative%20approach%20that%20alternates%20between%20%281%29%20generating%20candidate%20prefixes%20and%20%282%29%20selecting%20those%20that%20optimize%20both%20task%20performance%20and%20refusal%20behavior.%20Experimental%20results%20demonstrate%20that%20PING%20significantly%20enhances%20the%20safety%20of%20fine-tuned%20LLM%20agents%20without%20sacrificing%20their%20effectiveness.%20PING%20consistently%20outperforms%20existing%20prompting%20approaches%20across%20diverse%20benchmarks%20in%20both%20web%20navigation%20and%20code%20generation%20tasks.%20Our%20analysis%20of%20internal%20hidden%20states%20via%20linear%20probes%20reveals%20that%20prefix%20tokens%20are%20crucial%20for%20behavior%20modification%2C%20explaining%20the%20performance%20gains.%20WARNING%3A%20This%20paper%20contains%20contents%20that%20are%20unethical%20or%20offensive%20in%20nature.%0ALink%3A%20http%3A//arxiv.org/abs/2508.14031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnintended%2520Misalignment%2520from%2520Agentic%2520Fine-Tuning%253A%2520Risks%2520and%2520Mitigation%26entry.906535625%3DDongyoon%2520Hahm%2520and%2520Taywon%2520Min%2520and%2520Woogyeol%2520Jin%2520and%2520Kimin%2520Lee%26entry.1292438233%3DBeyond%2520simple%2520text%2520generation%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520evolved%2520into%2520agentic%2520systems%2520capable%2520of%2520planning%2520and%2520interacting%2520with%2520external%2520tools%2520to%2520solve%2520complex%2520tasks.%2520This%2520evolution%2520involves%2520fine-tuning%2520LLMs%2520on%2520agent-specific%2520tasks%2520to%2520enhance%2520their%2520proficiency.%2520However%252C%2520safety%2520concerns%2520are%2520frequently%2520overlooked%2520during%2520this%2520fine-tuning%2520process.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520aligned%2520LLMs%2520can%2520become%2520unintentionally%2520misaligned%252C%2520leading%2520to%2520a%2520higher%2520likelihood%2520of%2520executing%2520harmful%2520tasks%2520and%2520a%2520reduced%2520tendency%2520to%2520refuse%2520them%2520when%2520fine-tuned%2520to%2520execute%2520agentic%2520tasks.%2520To%2520address%2520these%2520safety%2520challenges%252C%2520we%2520propose%2520Prefix%2520INjection%2520Guard%2520%2528PING%2529%252C%2520a%2520simple%2520yet%2520effective%2520method%2520that%2520prepends%2520automatically%2520generated%2520natural%2520language%2520prefixes%2520to%2520agent%2520responses%252C%2520guiding%2520them%2520to%2520refuse%2520harmful%2520requests%2520while%2520preserving%2520performance%2520on%2520benign%2520tasks.%2520Specifically%252C%2520we%2520introduce%2520an%2520iterative%2520approach%2520that%2520alternates%2520between%2520%25281%2529%2520generating%2520candidate%2520prefixes%2520and%2520%25282%2529%2520selecting%2520those%2520that%2520optimize%2520both%2520task%2520performance%2520and%2520refusal%2520behavior.%2520Experimental%2520results%2520demonstrate%2520that%2520PING%2520significantly%2520enhances%2520the%2520safety%2520of%2520fine-tuned%2520LLM%2520agents%2520without%2520sacrificing%2520their%2520effectiveness.%2520PING%2520consistently%2520outperforms%2520existing%2520prompting%2520approaches%2520across%2520diverse%2520benchmarks%2520in%2520both%2520web%2520navigation%2520and%2520code%2520generation%2520tasks.%2520Our%2520analysis%2520of%2520internal%2520hidden%2520states%2520via%2520linear%2520probes%2520reveals%2520that%2520prefix%2520tokens%2520are%2520crucial%2520for%2520behavior%2520modification%252C%2520explaining%2520the%2520performance%2520gains.%2520WARNING%253A%2520This%2520paper%2520contains%2520contents%2520that%2520are%2520unethical%2520or%2520offensive%2520in%2520nature.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unintended%20Misalignment%20from%20Agentic%20Fine-Tuning%3A%20Risks%20and%20Mitigation&entry.906535625=Dongyoon%20Hahm%20and%20Taywon%20Min%20and%20Woogyeol%20Jin%20and%20Kimin%20Lee&entry.1292438233=Beyond%20simple%20text%20generation%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20evolved%20into%20agentic%20systems%20capable%20of%20planning%20and%20interacting%20with%20external%20tools%20to%20solve%20complex%20tasks.%20This%20evolution%20involves%20fine-tuning%20LLMs%20on%20agent-specific%20tasks%20to%20enhance%20their%20proficiency.%20However%2C%20safety%20concerns%20are%20frequently%20overlooked%20during%20this%20fine-tuning%20process.%20In%20this%20work%2C%20we%20show%20that%20aligned%20LLMs%20can%20become%20unintentionally%20misaligned%2C%20leading%20to%20a%20higher%20likelihood%20of%20executing%20harmful%20tasks%20and%20a%20reduced%20tendency%20to%20refuse%20them%20when%20fine-tuned%20to%20execute%20agentic%20tasks.%20To%20address%20these%20safety%20challenges%2C%20we%20propose%20Prefix%20INjection%20Guard%20%28PING%29%2C%20a%20simple%20yet%20effective%20method%20that%20prepends%20automatically%20generated%20natural%20language%20prefixes%20to%20agent%20responses%2C%20guiding%20them%20to%20refuse%20harmful%20requests%20while%20preserving%20performance%20on%20benign%20tasks.%20Specifically%2C%20we%20introduce%20an%20iterative%20approach%20that%20alternates%20between%20%281%29%20generating%20candidate%20prefixes%20and%20%282%29%20selecting%20those%20that%20optimize%20both%20task%20performance%20and%20refusal%20behavior.%20Experimental%20results%20demonstrate%20that%20PING%20significantly%20enhances%20the%20safety%20of%20fine-tuned%20LLM%20agents%20without%20sacrificing%20their%20effectiveness.%20PING%20consistently%20outperforms%20existing%20prompting%20approaches%20across%20diverse%20benchmarks%20in%20both%20web%20navigation%20and%20code%20generation%20tasks.%20Our%20analysis%20of%20internal%20hidden%20states%20via%20linear%20probes%20reveals%20that%20prefix%20tokens%20are%20crucial%20for%20behavior%20modification%2C%20explaining%20the%20performance%20gains.%20WARNING%3A%20This%20paper%20contains%20contents%20that%20are%20unethical%20or%20offensive%20in%20nature.&entry.1838667208=http%3A//arxiv.org/abs/2508.14031v2&entry.124074799=Read"},
{"title": "SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning", "author": "Xuchen Li and Ruitao Wu and Xuanbo Liu and Xukai Wang and Jinbo Hu and Zhixin Bai and Bohan Zeng and Hao Liang and Leheng Chen and Mingrui Chen and Haitian Zhong and Xuanlin Yang and Xu-Yao Zhang and Liu Liu and Jia Li and Kaiqi Huang and Jiahao Xu and Haitao Mi and Wentao Zhang and Bin Dong", "abstract": "Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.", "link": "http://arxiv.org/abs/2511.08151v2", "date": "2025-11-17", "relevancy": 1.5866, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5574}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SciAgent%3A%20A%20Unified%20Multi-Agent%20System%20for%20Generalistic%20Scientific%20Reasoning&body=Title%3A%20SciAgent%3A%20A%20Unified%20Multi-Agent%20System%20for%20Generalistic%20Scientific%20Reasoning%0AAuthor%3A%20Xuchen%20Li%20and%20Ruitao%20Wu%20and%20Xuanbo%20Liu%20and%20Xukai%20Wang%20and%20Jinbo%20Hu%20and%20Zhixin%20Bai%20and%20Bohan%20Zeng%20and%20Hao%20Liang%20and%20Leheng%20Chen%20and%20Mingrui%20Chen%20and%20Haitian%20Zhong%20and%20Xuanlin%20Yang%20and%20Xu-Yao%20Zhang%20and%20Liu%20Liu%20and%20Jia%20Li%20and%20Kaiqi%20Huang%20and%20Jiahao%20Xu%20and%20Haitao%20Mi%20and%20Wentao%20Zhang%20and%20Bin%20Dong%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20models%20have%20enabled%20AI%20systems%20to%20achieve%20expert-level%20performance%20on%20domain-specific%20scientific%20tasks%2C%20yet%20these%20systems%20remain%20narrow%20and%20handcrafted.%20We%20introduce%20SciAgent%2C%20a%20unified%20multi-agent%20system%20designed%20for%20generalistic%20scientific%20reasoning-the%20ability%20to%20adapt%20reasoning%20strategies%20across%20disciplines%20and%20difficulty%20levels.%20SciAgent%20organizes%20problem%20solving%20as%20a%20hierarchical%20process%3A%20a%20Coordinator%20Agent%20interprets%20each%20problem%27s%20domain%20and%20complexity%2C%20dynamically%20orchestrating%20specialized%20Worker%20Systems%2C%20each%20composed%20of%20interacting%20reasoning%20Sub-agents%20for%20symbolic%20deduction%2C%20conceptual%20modeling%2C%20numerical%20computation%2C%20and%20verification.%20These%20agents%20collaboratively%20assemble%20and%20refine%20reasoning%20pipelines%20tailored%20to%20each%20task.%20Across%20mathematics%20and%20physics%20Olympiads%20%28IMO%2C%20IMC%2C%20IPhO%2C%20CPhO%29%2C%20SciAgent%20consistently%20attains%20or%20surpasses%20human%20gold-medalist%20performance%2C%20demonstrating%20both%20domain%20generality%20and%20reasoning%20adaptability.%20Additionally%2C%20SciAgent%20has%20been%20tested%20on%20the%20International%20Chemistry%20Olympiad%20%28IChO%29%20and%20selected%20problems%20from%20the%20Humanity%27s%20Last%20Exam%20%28HLE%29%20benchmark%2C%20further%20confirming%20the%20system%27s%20ability%20to%20generalize%20across%20diverse%20scientific%20domains.%20This%20work%20establishes%20SciAgent%20as%20a%20concrete%20step%20toward%20generalistic%20scientific%20intelligence-AI%20systems%20capable%20of%20coherent%2C%20cross-disciplinary%20reasoning%20at%20expert%20levels.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSciAgent%253A%2520A%2520Unified%2520Multi-Agent%2520System%2520for%2520Generalistic%2520Scientific%2520Reasoning%26entry.906535625%3DXuchen%2520Li%2520and%2520Ruitao%2520Wu%2520and%2520Xuanbo%2520Liu%2520and%2520Xukai%2520Wang%2520and%2520Jinbo%2520Hu%2520and%2520Zhixin%2520Bai%2520and%2520Bohan%2520Zeng%2520and%2520Hao%2520Liang%2520and%2520Leheng%2520Chen%2520and%2520Mingrui%2520Chen%2520and%2520Haitian%2520Zhong%2520and%2520Xuanlin%2520Yang%2520and%2520Xu-Yao%2520Zhang%2520and%2520Liu%2520Liu%2520and%2520Jia%2520Li%2520and%2520Kaiqi%2520Huang%2520and%2520Jiahao%2520Xu%2520and%2520Haitao%2520Mi%2520and%2520Wentao%2520Zhang%2520and%2520Bin%2520Dong%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520models%2520have%2520enabled%2520AI%2520systems%2520to%2520achieve%2520expert-level%2520performance%2520on%2520domain-specific%2520scientific%2520tasks%252C%2520yet%2520these%2520systems%2520remain%2520narrow%2520and%2520handcrafted.%2520We%2520introduce%2520SciAgent%252C%2520a%2520unified%2520multi-agent%2520system%2520designed%2520for%2520generalistic%2520scientific%2520reasoning-the%2520ability%2520to%2520adapt%2520reasoning%2520strategies%2520across%2520disciplines%2520and%2520difficulty%2520levels.%2520SciAgent%2520organizes%2520problem%2520solving%2520as%2520a%2520hierarchical%2520process%253A%2520a%2520Coordinator%2520Agent%2520interprets%2520each%2520problem%2527s%2520domain%2520and%2520complexity%252C%2520dynamically%2520orchestrating%2520specialized%2520Worker%2520Systems%252C%2520each%2520composed%2520of%2520interacting%2520reasoning%2520Sub-agents%2520for%2520symbolic%2520deduction%252C%2520conceptual%2520modeling%252C%2520numerical%2520computation%252C%2520and%2520verification.%2520These%2520agents%2520collaboratively%2520assemble%2520and%2520refine%2520reasoning%2520pipelines%2520tailored%2520to%2520each%2520task.%2520Across%2520mathematics%2520and%2520physics%2520Olympiads%2520%2528IMO%252C%2520IMC%252C%2520IPhO%252C%2520CPhO%2529%252C%2520SciAgent%2520consistently%2520attains%2520or%2520surpasses%2520human%2520gold-medalist%2520performance%252C%2520demonstrating%2520both%2520domain%2520generality%2520and%2520reasoning%2520adaptability.%2520Additionally%252C%2520SciAgent%2520has%2520been%2520tested%2520on%2520the%2520International%2520Chemistry%2520Olympiad%2520%2528IChO%2529%2520and%2520selected%2520problems%2520from%2520the%2520Humanity%2527s%2520Last%2520Exam%2520%2528HLE%2529%2520benchmark%252C%2520further%2520confirming%2520the%2520system%2527s%2520ability%2520to%2520generalize%2520across%2520diverse%2520scientific%2520domains.%2520This%2520work%2520establishes%2520SciAgent%2520as%2520a%2520concrete%2520step%2520toward%2520generalistic%2520scientific%2520intelligence-AI%2520systems%2520capable%2520of%2520coherent%252C%2520cross-disciplinary%2520reasoning%2520at%2520expert%2520levels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciAgent%3A%20A%20Unified%20Multi-Agent%20System%20for%20Generalistic%20Scientific%20Reasoning&entry.906535625=Xuchen%20Li%20and%20Ruitao%20Wu%20and%20Xuanbo%20Liu%20and%20Xukai%20Wang%20and%20Jinbo%20Hu%20and%20Zhixin%20Bai%20and%20Bohan%20Zeng%20and%20Hao%20Liang%20and%20Leheng%20Chen%20and%20Mingrui%20Chen%20and%20Haitian%20Zhong%20and%20Xuanlin%20Yang%20and%20Xu-Yao%20Zhang%20and%20Liu%20Liu%20and%20Jia%20Li%20and%20Kaiqi%20Huang%20and%20Jiahao%20Xu%20and%20Haitao%20Mi%20and%20Wentao%20Zhang%20and%20Bin%20Dong&entry.1292438233=Recent%20advances%20in%20large%20language%20models%20have%20enabled%20AI%20systems%20to%20achieve%20expert-level%20performance%20on%20domain-specific%20scientific%20tasks%2C%20yet%20these%20systems%20remain%20narrow%20and%20handcrafted.%20We%20introduce%20SciAgent%2C%20a%20unified%20multi-agent%20system%20designed%20for%20generalistic%20scientific%20reasoning-the%20ability%20to%20adapt%20reasoning%20strategies%20across%20disciplines%20and%20difficulty%20levels.%20SciAgent%20organizes%20problem%20solving%20as%20a%20hierarchical%20process%3A%20a%20Coordinator%20Agent%20interprets%20each%20problem%27s%20domain%20and%20complexity%2C%20dynamically%20orchestrating%20specialized%20Worker%20Systems%2C%20each%20composed%20of%20interacting%20reasoning%20Sub-agents%20for%20symbolic%20deduction%2C%20conceptual%20modeling%2C%20numerical%20computation%2C%20and%20verification.%20These%20agents%20collaboratively%20assemble%20and%20refine%20reasoning%20pipelines%20tailored%20to%20each%20task.%20Across%20mathematics%20and%20physics%20Olympiads%20%28IMO%2C%20IMC%2C%20IPhO%2C%20CPhO%29%2C%20SciAgent%20consistently%20attains%20or%20surpasses%20human%20gold-medalist%20performance%2C%20demonstrating%20both%20domain%20generality%20and%20reasoning%20adaptability.%20Additionally%2C%20SciAgent%20has%20been%20tested%20on%20the%20International%20Chemistry%20Olympiad%20%28IChO%29%20and%20selected%20problems%20from%20the%20Humanity%27s%20Last%20Exam%20%28HLE%29%20benchmark%2C%20further%20confirming%20the%20system%27s%20ability%20to%20generalize%20across%20diverse%20scientific%20domains.%20This%20work%20establishes%20SciAgent%20as%20a%20concrete%20step%20toward%20generalistic%20scientific%20intelligence-AI%20systems%20capable%20of%20coherent%2C%20cross-disciplinary%20reasoning%20at%20expert%20levels.&entry.1838667208=http%3A//arxiv.org/abs/2511.08151v2&entry.124074799=Read"},
{"title": "Robust-Multi-Task Gradient Boosting", "author": "Seyedsaman Emami and Gonzalo Mart\u00ednez-Mu\u00f1oz and Daniel Hern\u00e1ndez-Lobato", "abstract": "Multi-task learning (MTL) has shown effectiveness in exploiting shared information across tasks to improve generalization. MTL assumes tasks share similarities that can improve performance. In addition, boosting algorithms have demonstrated exceptional performance across diverse learning problems, primarily due to their ability to focus on hard-to-learn instances and iteratively reduce residual errors. This makes them a promising approach for learning multi-task problems. However, real-world MTL scenarios often involve tasks that are not well-aligned (known as outlier or adversarial tasks), which do not share beneficial similarities with others and can, in fact, deteriorate the performance of the overall model. To overcome this challenge, we propose Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that explicitly models and adapts to task heterogeneity during training. R-MTGB structures the learning process into three sequential blocks: (1) learning shared patterns, (2) partitioning tasks into outliers and non-outliers with regularized parameters, and (3) fine-tuning task-specific predictors. This architecture enables R-MTGB to automatically detect and penalize outlier tasks while promoting effective knowledge transfer among related tasks. Our method integrates these mechanisms seamlessly within gradient boosting, allowing robust handling of noisy or adversarial tasks without sacrificing accuracy. Extensive experiments on both synthetic benchmarks and real-world datasets demonstrate that our approach successfully isolates outliers, transfers knowledge, and consistently reduces prediction errors for each task individually, and achieves overall performance gains across all tasks. These results highlight robustness, adaptability, and reliable convergence of R-MTGB in challenging MTL environments.", "link": "http://arxiv.org/abs/2507.11411v3", "date": "2025-11-17", "relevancy": 2.0239, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5016}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust-Multi-Task%20Gradient%20Boosting&body=Title%3A%20Robust-Multi-Task%20Gradient%20Boosting%0AAuthor%3A%20Seyedsaman%20Emami%20and%20Gonzalo%20Mart%C3%ADnez-Mu%C3%B1oz%20and%20Daniel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20Multi-task%20learning%20%28MTL%29%20has%20shown%20effectiveness%20in%20exploiting%20shared%20information%20across%20tasks%20to%20improve%20generalization.%20MTL%20assumes%20tasks%20share%20similarities%20that%20can%20improve%20performance.%20In%20addition%2C%20boosting%20algorithms%20have%20demonstrated%20exceptional%20performance%20across%20diverse%20learning%20problems%2C%20primarily%20due%20to%20their%20ability%20to%20focus%20on%20hard-to-learn%20instances%20and%20iteratively%20reduce%20residual%20errors.%20This%20makes%20them%20a%20promising%20approach%20for%20learning%20multi-task%20problems.%20However%2C%20real-world%20MTL%20scenarios%20often%20involve%20tasks%20that%20are%20not%20well-aligned%20%28known%20as%20outlier%20or%20adversarial%20tasks%29%2C%20which%20do%20not%20share%20beneficial%20similarities%20with%20others%20and%20can%2C%20in%20fact%2C%20deteriorate%20the%20performance%20of%20the%20overall%20model.%20To%20overcome%20this%20challenge%2C%20we%20propose%20Robust-Multi-Task%20Gradient%20Boosting%20%28R-MTGB%29%2C%20a%20novel%20boosting%20framework%20that%20explicitly%20models%20and%20adapts%20to%20task%20heterogeneity%20during%20training.%20R-MTGB%20structures%20the%20learning%20process%20into%20three%20sequential%20blocks%3A%20%281%29%20learning%20shared%20patterns%2C%20%282%29%20partitioning%20tasks%20into%20outliers%20and%20non-outliers%20with%20regularized%20parameters%2C%20and%20%283%29%20fine-tuning%20task-specific%20predictors.%20This%20architecture%20enables%20R-MTGB%20to%20automatically%20detect%20and%20penalize%20outlier%20tasks%20while%20promoting%20effective%20knowledge%20transfer%20among%20related%20tasks.%20Our%20method%20integrates%20these%20mechanisms%20seamlessly%20within%20gradient%20boosting%2C%20allowing%20robust%20handling%20of%20noisy%20or%20adversarial%20tasks%20without%20sacrificing%20accuracy.%20Extensive%20experiments%20on%20both%20synthetic%20benchmarks%20and%20real-world%20datasets%20demonstrate%20that%20our%20approach%20successfully%20isolates%20outliers%2C%20transfers%20knowledge%2C%20and%20consistently%20reduces%20prediction%20errors%20for%20each%20task%20individually%2C%20and%20achieves%20overall%20performance%20gains%20across%20all%20tasks.%20These%20results%20highlight%20robustness%2C%20adaptability%2C%20and%20reliable%20convergence%20of%20R-MTGB%20in%20challenging%20MTL%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2507.11411v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust-Multi-Task%2520Gradient%2520Boosting%26entry.906535625%3DSeyedsaman%2520Emami%2520and%2520Gonzalo%2520Mart%25C3%25ADnez-Mu%25C3%25B1oz%2520and%2520Daniel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3DMulti-task%2520learning%2520%2528MTL%2529%2520has%2520shown%2520effectiveness%2520in%2520exploiting%2520shared%2520information%2520across%2520tasks%2520to%2520improve%2520generalization.%2520MTL%2520assumes%2520tasks%2520share%2520similarities%2520that%2520can%2520improve%2520performance.%2520In%2520addition%252C%2520boosting%2520algorithms%2520have%2520demonstrated%2520exceptional%2520performance%2520across%2520diverse%2520learning%2520problems%252C%2520primarily%2520due%2520to%2520their%2520ability%2520to%2520focus%2520on%2520hard-to-learn%2520instances%2520and%2520iteratively%2520reduce%2520residual%2520errors.%2520This%2520makes%2520them%2520a%2520promising%2520approach%2520for%2520learning%2520multi-task%2520problems.%2520However%252C%2520real-world%2520MTL%2520scenarios%2520often%2520involve%2520tasks%2520that%2520are%2520not%2520well-aligned%2520%2528known%2520as%2520outlier%2520or%2520adversarial%2520tasks%2529%252C%2520which%2520do%2520not%2520share%2520beneficial%2520similarities%2520with%2520others%2520and%2520can%252C%2520in%2520fact%252C%2520deteriorate%2520the%2520performance%2520of%2520the%2520overall%2520model.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520Robust-Multi-Task%2520Gradient%2520Boosting%2520%2528R-MTGB%2529%252C%2520a%2520novel%2520boosting%2520framework%2520that%2520explicitly%2520models%2520and%2520adapts%2520to%2520task%2520heterogeneity%2520during%2520training.%2520R-MTGB%2520structures%2520the%2520learning%2520process%2520into%2520three%2520sequential%2520blocks%253A%2520%25281%2529%2520learning%2520shared%2520patterns%252C%2520%25282%2529%2520partitioning%2520tasks%2520into%2520outliers%2520and%2520non-outliers%2520with%2520regularized%2520parameters%252C%2520and%2520%25283%2529%2520fine-tuning%2520task-specific%2520predictors.%2520This%2520architecture%2520enables%2520R-MTGB%2520to%2520automatically%2520detect%2520and%2520penalize%2520outlier%2520tasks%2520while%2520promoting%2520effective%2520knowledge%2520transfer%2520among%2520related%2520tasks.%2520Our%2520method%2520integrates%2520these%2520mechanisms%2520seamlessly%2520within%2520gradient%2520boosting%252C%2520allowing%2520robust%2520handling%2520of%2520noisy%2520or%2520adversarial%2520tasks%2520without%2520sacrificing%2520accuracy.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520benchmarks%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520successfully%2520isolates%2520outliers%252C%2520transfers%2520knowledge%252C%2520and%2520consistently%2520reduces%2520prediction%2520errors%2520for%2520each%2520task%2520individually%252C%2520and%2520achieves%2520overall%2520performance%2520gains%2520across%2520all%2520tasks.%2520These%2520results%2520highlight%2520robustness%252C%2520adaptability%252C%2520and%2520reliable%2520convergence%2520of%2520R-MTGB%2520in%2520challenging%2520MTL%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11411v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust-Multi-Task%20Gradient%20Boosting&entry.906535625=Seyedsaman%20Emami%20and%20Gonzalo%20Mart%C3%ADnez-Mu%C3%B1oz%20and%20Daniel%20Hern%C3%A1ndez-Lobato&entry.1292438233=Multi-task%20learning%20%28MTL%29%20has%20shown%20effectiveness%20in%20exploiting%20shared%20information%20across%20tasks%20to%20improve%20generalization.%20MTL%20assumes%20tasks%20share%20similarities%20that%20can%20improve%20performance.%20In%20addition%2C%20boosting%20algorithms%20have%20demonstrated%20exceptional%20performance%20across%20diverse%20learning%20problems%2C%20primarily%20due%20to%20their%20ability%20to%20focus%20on%20hard-to-learn%20instances%20and%20iteratively%20reduce%20residual%20errors.%20This%20makes%20them%20a%20promising%20approach%20for%20learning%20multi-task%20problems.%20However%2C%20real-world%20MTL%20scenarios%20often%20involve%20tasks%20that%20are%20not%20well-aligned%20%28known%20as%20outlier%20or%20adversarial%20tasks%29%2C%20which%20do%20not%20share%20beneficial%20similarities%20with%20others%20and%20can%2C%20in%20fact%2C%20deteriorate%20the%20performance%20of%20the%20overall%20model.%20To%20overcome%20this%20challenge%2C%20we%20propose%20Robust-Multi-Task%20Gradient%20Boosting%20%28R-MTGB%29%2C%20a%20novel%20boosting%20framework%20that%20explicitly%20models%20and%20adapts%20to%20task%20heterogeneity%20during%20training.%20R-MTGB%20structures%20the%20learning%20process%20into%20three%20sequential%20blocks%3A%20%281%29%20learning%20shared%20patterns%2C%20%282%29%20partitioning%20tasks%20into%20outliers%20and%20non-outliers%20with%20regularized%20parameters%2C%20and%20%283%29%20fine-tuning%20task-specific%20predictors.%20This%20architecture%20enables%20R-MTGB%20to%20automatically%20detect%20and%20penalize%20outlier%20tasks%20while%20promoting%20effective%20knowledge%20transfer%20among%20related%20tasks.%20Our%20method%20integrates%20these%20mechanisms%20seamlessly%20within%20gradient%20boosting%2C%20allowing%20robust%20handling%20of%20noisy%20or%20adversarial%20tasks%20without%20sacrificing%20accuracy.%20Extensive%20experiments%20on%20both%20synthetic%20benchmarks%20and%20real-world%20datasets%20demonstrate%20that%20our%20approach%20successfully%20isolates%20outliers%2C%20transfers%20knowledge%2C%20and%20consistently%20reduces%20prediction%20errors%20for%20each%20task%20individually%2C%20and%20achieves%20overall%20performance%20gains%20across%20all%20tasks.%20These%20results%20highlight%20robustness%2C%20adaptability%2C%20and%20reliable%20convergence%20of%20R-MTGB%20in%20challenging%20MTL%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2507.11411v3&entry.124074799=Read"},
{"title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "author": "Yueqing Xi and Yifan Bai and Huasen Luo and Weiliang Wen and Hui Liu and Haoliang Li", "abstract": "As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.", "link": "http://arxiv.org/abs/2511.01668v2", "date": "2025-11-17", "relevancy": 1.971, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5168}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.494}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Retrieval-Augmented%20Generation%20Agent%20for%20Trustworthy%20Legal%20Question%20Answering%20in%20Judicial%20Forensics&body=Title%3A%20Hybrid%20Retrieval-Augmented%20Generation%20Agent%20for%20Trustworthy%20Legal%20Question%20Answering%20in%20Judicial%20Forensics%0AAuthor%3A%20Yueqing%20Xi%20and%20Yifan%20Bai%20and%20Huasen%20Luo%20and%20Weiliang%20Wen%20and%20Hui%20Liu%20and%20Haoliang%20Li%0AAbstract%3A%20As%20artificial%20intelligence%20permeates%20judicial%20forensics%2C%20ensuring%20the%20veracity%20and%20traceability%20of%20legal%20question%20answering%20%28QA%29%20has%20become%20critical.%20Conventional%20large%20language%20models%20%28LLMs%29%20are%20prone%20to%20hallucination%2C%20risking%20misleading%20guidance%20in%20legal%20consultation%2C%20while%20static%20knowledge%20bases%20struggle%20to%20keep%20pace%20with%20frequently%20updated%20statutes%20and%20case%20law.%20We%20present%20a%20hybrid%20legal%20QA%20agent%20tailored%20for%20judicial%20settings%20that%20integrates%20retrieval-augmented%20generation%20%28RAG%29%20with%20multi-model%20ensembling%20to%20deliver%20reliable%2C%20auditable%2C%20and%20continuously%20updatable%20counsel.%20The%20system%20prioritizes%20retrieval%20over%20generation%3A%20when%20a%20trusted%20legal%20repository%20yields%20relevant%20evidence%2C%20answers%20are%20produced%20via%20RAG%3B%20otherwise%2C%20multiple%20LLMs%20generate%20candidates%20that%20are%20scored%20by%20a%20specialized%20selector%2C%20with%20the%20top-ranked%20answer%20returned.%20High-quality%20outputs%20then%20undergo%20human%20review%20before%20being%20written%20back%20to%20the%20repository%2C%20enabling%20dynamic%20knowledge%20evolution%20and%20provenance%20tracking.%20Experiments%20on%20the%20Law%5C_QA%20dataset%20show%20that%20our%20hybrid%20approach%20significantly%20outperforms%20both%20a%20single-model%20baseline%20and%20a%20vanilla%20RAG%20pipeline%20on%20F1%2C%20ROUGE-L%2C%20and%20an%20LLM-as-a-Judge%20metric.%20Ablations%20confirm%20the%20complementary%20contributions%20of%20retrieval%20prioritization%2C%20model%20ensembling%2C%20and%20the%20human-in-the-loop%20update%20mechanism.%20The%20proposed%20system%20demonstrably%20reduces%20hallucination%20while%20improving%20answer%20quality%20and%20legal%20compliance%2C%20advancing%20the%20practical%20landing%20of%20media%20forensics%20technologies%20in%20judicial%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01668v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Retrieval-Augmented%2520Generation%2520Agent%2520for%2520Trustworthy%2520Legal%2520Question%2520Answering%2520in%2520Judicial%2520Forensics%26entry.906535625%3DYueqing%2520Xi%2520and%2520Yifan%2520Bai%2520and%2520Huasen%2520Luo%2520and%2520Weiliang%2520Wen%2520and%2520Hui%2520Liu%2520and%2520Haoliang%2520Li%26entry.1292438233%3DAs%2520artificial%2520intelligence%2520permeates%2520judicial%2520forensics%252C%2520ensuring%2520the%2520veracity%2520and%2520traceability%2520of%2520legal%2520question%2520answering%2520%2528QA%2529%2520has%2520become%2520critical.%2520Conventional%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520prone%2520to%2520hallucination%252C%2520risking%2520misleading%2520guidance%2520in%2520legal%2520consultation%252C%2520while%2520static%2520knowledge%2520bases%2520struggle%2520to%2520keep%2520pace%2520with%2520frequently%2520updated%2520statutes%2520and%2520case%2520law.%2520We%2520present%2520a%2520hybrid%2520legal%2520QA%2520agent%2520tailored%2520for%2520judicial%2520settings%2520that%2520integrates%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520with%2520multi-model%2520ensembling%2520to%2520deliver%2520reliable%252C%2520auditable%252C%2520and%2520continuously%2520updatable%2520counsel.%2520The%2520system%2520prioritizes%2520retrieval%2520over%2520generation%253A%2520when%2520a%2520trusted%2520legal%2520repository%2520yields%2520relevant%2520evidence%252C%2520answers%2520are%2520produced%2520via%2520RAG%253B%2520otherwise%252C%2520multiple%2520LLMs%2520generate%2520candidates%2520that%2520are%2520scored%2520by%2520a%2520specialized%2520selector%252C%2520with%2520the%2520top-ranked%2520answer%2520returned.%2520High-quality%2520outputs%2520then%2520undergo%2520human%2520review%2520before%2520being%2520written%2520back%2520to%2520the%2520repository%252C%2520enabling%2520dynamic%2520knowledge%2520evolution%2520and%2520provenance%2520tracking.%2520Experiments%2520on%2520the%2520Law%255C_QA%2520dataset%2520show%2520that%2520our%2520hybrid%2520approach%2520significantly%2520outperforms%2520both%2520a%2520single-model%2520baseline%2520and%2520a%2520vanilla%2520RAG%2520pipeline%2520on%2520F1%252C%2520ROUGE-L%252C%2520and%2520an%2520LLM-as-a-Judge%2520metric.%2520Ablations%2520confirm%2520the%2520complementary%2520contributions%2520of%2520retrieval%2520prioritization%252C%2520model%2520ensembling%252C%2520and%2520the%2520human-in-the-loop%2520update%2520mechanism.%2520The%2520proposed%2520system%2520demonstrably%2520reduces%2520hallucination%2520while%2520improving%2520answer%2520quality%2520and%2520legal%2520compliance%252C%2520advancing%2520the%2520practical%2520landing%2520of%2520media%2520forensics%2520technologies%2520in%2520judicial%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01668v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Retrieval-Augmented%20Generation%20Agent%20for%20Trustworthy%20Legal%20Question%20Answering%20in%20Judicial%20Forensics&entry.906535625=Yueqing%20Xi%20and%20Yifan%20Bai%20and%20Huasen%20Luo%20and%20Weiliang%20Wen%20and%20Hui%20Liu%20and%20Haoliang%20Li&entry.1292438233=As%20artificial%20intelligence%20permeates%20judicial%20forensics%2C%20ensuring%20the%20veracity%20and%20traceability%20of%20legal%20question%20answering%20%28QA%29%20has%20become%20critical.%20Conventional%20large%20language%20models%20%28LLMs%29%20are%20prone%20to%20hallucination%2C%20risking%20misleading%20guidance%20in%20legal%20consultation%2C%20while%20static%20knowledge%20bases%20struggle%20to%20keep%20pace%20with%20frequently%20updated%20statutes%20and%20case%20law.%20We%20present%20a%20hybrid%20legal%20QA%20agent%20tailored%20for%20judicial%20settings%20that%20integrates%20retrieval-augmented%20generation%20%28RAG%29%20with%20multi-model%20ensembling%20to%20deliver%20reliable%2C%20auditable%2C%20and%20continuously%20updatable%20counsel.%20The%20system%20prioritizes%20retrieval%20over%20generation%3A%20when%20a%20trusted%20legal%20repository%20yields%20relevant%20evidence%2C%20answers%20are%20produced%20via%20RAG%3B%20otherwise%2C%20multiple%20LLMs%20generate%20candidates%20that%20are%20scored%20by%20a%20specialized%20selector%2C%20with%20the%20top-ranked%20answer%20returned.%20High-quality%20outputs%20then%20undergo%20human%20review%20before%20being%20written%20back%20to%20the%20repository%2C%20enabling%20dynamic%20knowledge%20evolution%20and%20provenance%20tracking.%20Experiments%20on%20the%20Law%5C_QA%20dataset%20show%20that%20our%20hybrid%20approach%20significantly%20outperforms%20both%20a%20single-model%20baseline%20and%20a%20vanilla%20RAG%20pipeline%20on%20F1%2C%20ROUGE-L%2C%20and%20an%20LLM-as-a-Judge%20metric.%20Ablations%20confirm%20the%20complementary%20contributions%20of%20retrieval%20prioritization%2C%20model%20ensembling%2C%20and%20the%20human-in-the-loop%20update%20mechanism.%20The%20proposed%20system%20demonstrably%20reduces%20hallucination%20while%20improving%20answer%20quality%20and%20legal%20compliance%2C%20advancing%20the%20practical%20landing%20of%20media%20forensics%20technologies%20in%20judicial%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2511.01668v2&entry.124074799=Read"},
{"title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures", "author": "Haohui Wang and Jingyuan Qi and Jianpeng Chen and Jun Wu and Lifu Huang and Lecheng Zheng and Kevin Choi and Balaji Veeramani and Edward Bowen and Alison Hu and Tyler Cody and Dawei Zhou", "abstract": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.", "link": "http://arxiv.org/abs/2511.13640v1", "date": "2025-11-17", "relevancy": 2.0127, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Value%20in%20the%20Age%20of%20Scaling%3A%20Understanding%20LLM%20Scaling%20Dynamics%20Under%20Real-Synthetic%20Data%20Mixtures&body=Title%3A%20Data%20Value%20in%20the%20Age%20of%20Scaling%3A%20Understanding%20LLM%20Scaling%20Dynamics%20Under%20Real-Synthetic%20Data%20Mixtures%0AAuthor%3A%20Haohui%20Wang%20and%20Jingyuan%20Qi%20and%20Jianpeng%20Chen%20and%20Jun%20Wu%20and%20Lifu%20Huang%20and%20Lecheng%20Zheng%20and%20Kevin%20Choi%20and%20Balaji%20Veeramani%20and%20Edward%20Bowen%20and%20Alison%20Hu%20and%20Tyler%20Cody%20and%20Dawei%20Zhou%0AAbstract%3A%20The%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%20is%20fueled%20by%20the%20growing%20reliance%20on%20datasets%20that%20blend%20real%20and%20synthetic%20data.%20While%20synthetic%20data%20offers%20scalability%20and%20cost-efficiency%2C%20it%20often%20introduces%20systematic%20distributional%20discrepancies%2C%20particularly%20underrepresenting%20long-tail%20knowledge%20due%20to%20truncation%20effects%20from%20data%20generation%20mechanisms%20like%20top-p%20sampling%2C%20temperature%20scaling%2C%20and%20finite%20sampling.%20These%20discrepancies%20pose%20fundamental%20challenges%20in%20characterizing%20and%20evaluating%20the%20utility%20of%20mixed%20real-synthetic%20datasets.%20In%20this%20paper%2C%20we%20identify%20a%20three-phase%20scaling%20behavior%20characterized%20by%20two%20breakpoints%20that%20reflect%20transitions%20in%20model%20behavior%20across%20learning%20head%20and%20tail%20knowledge.%20We%20further%20derive%20an%20LLM%20generalization%20bound%20designed%20for%20real%20and%20synthetic%20mixtures%2C%20revealing%20several%20key%20factors%20that%20govern%20their%20generalization%20performance.%20Building%20on%20our%20theoretical%20findings%2C%20we%20propose%20an%20effective%20yet%20efficient%20data%20valuation%20method%20that%20scales%20to%20large-scale%20datasets.%20Comprehensive%20experiments%20across%20four%20tasks%2C%20including%20image%20classification%2C%20sentiment%20classification%2C%20instruction%20following%2C%20and%20complex%20reasoning%2C%20demonstrate%20that%20our%20method%20surpasses%20state-of-the-art%20baselines%20in%20data%20valuation%20with%20significantly%20low%20computational%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Value%2520in%2520the%2520Age%2520of%2520Scaling%253A%2520Understanding%2520LLM%2520Scaling%2520Dynamics%2520Under%2520Real-Synthetic%2520Data%2520Mixtures%26entry.906535625%3DHaohui%2520Wang%2520and%2520Jingyuan%2520Qi%2520and%2520Jianpeng%2520Chen%2520and%2520Jun%2520Wu%2520and%2520Lifu%2520Huang%2520and%2520Lecheng%2520Zheng%2520and%2520Kevin%2520Choi%2520and%2520Balaji%2520Veeramani%2520and%2520Edward%2520Bowen%2520and%2520Alison%2520Hu%2520and%2520Tyler%2520Cody%2520and%2520Dawei%2520Zhou%26entry.1292438233%3DThe%2520rapid%2520progress%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520fueled%2520by%2520the%2520growing%2520reliance%2520on%2520datasets%2520that%2520blend%2520real%2520and%2520synthetic%2520data.%2520While%2520synthetic%2520data%2520offers%2520scalability%2520and%2520cost-efficiency%252C%2520it%2520often%2520introduces%2520systematic%2520distributional%2520discrepancies%252C%2520particularly%2520underrepresenting%2520long-tail%2520knowledge%2520due%2520to%2520truncation%2520effects%2520from%2520data%2520generation%2520mechanisms%2520like%2520top-p%2520sampling%252C%2520temperature%2520scaling%252C%2520and%2520finite%2520sampling.%2520These%2520discrepancies%2520pose%2520fundamental%2520challenges%2520in%2520characterizing%2520and%2520evaluating%2520the%2520utility%2520of%2520mixed%2520real-synthetic%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520identify%2520a%2520three-phase%2520scaling%2520behavior%2520characterized%2520by%2520two%2520breakpoints%2520that%2520reflect%2520transitions%2520in%2520model%2520behavior%2520across%2520learning%2520head%2520and%2520tail%2520knowledge.%2520We%2520further%2520derive%2520an%2520LLM%2520generalization%2520bound%2520designed%2520for%2520real%2520and%2520synthetic%2520mixtures%252C%2520revealing%2520several%2520key%2520factors%2520that%2520govern%2520their%2520generalization%2520performance.%2520Building%2520on%2520our%2520theoretical%2520findings%252C%2520we%2520propose%2520an%2520effective%2520yet%2520efficient%2520data%2520valuation%2520method%2520that%2520scales%2520to%2520large-scale%2520datasets.%2520Comprehensive%2520experiments%2520across%2520four%2520tasks%252C%2520including%2520image%2520classification%252C%2520sentiment%2520classification%252C%2520instruction%2520following%252C%2520and%2520complex%2520reasoning%252C%2520demonstrate%2520that%2520our%2520method%2520surpasses%2520state-of-the-art%2520baselines%2520in%2520data%2520valuation%2520with%2520significantly%2520low%2520computational%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Value%20in%20the%20Age%20of%20Scaling%3A%20Understanding%20LLM%20Scaling%20Dynamics%20Under%20Real-Synthetic%20Data%20Mixtures&entry.906535625=Haohui%20Wang%20and%20Jingyuan%20Qi%20and%20Jianpeng%20Chen%20and%20Jun%20Wu%20and%20Lifu%20Huang%20and%20Lecheng%20Zheng%20and%20Kevin%20Choi%20and%20Balaji%20Veeramani%20and%20Edward%20Bowen%20and%20Alison%20Hu%20and%20Tyler%20Cody%20and%20Dawei%20Zhou&entry.1292438233=The%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%20is%20fueled%20by%20the%20growing%20reliance%20on%20datasets%20that%20blend%20real%20and%20synthetic%20data.%20While%20synthetic%20data%20offers%20scalability%20and%20cost-efficiency%2C%20it%20often%20introduces%20systematic%20distributional%20discrepancies%2C%20particularly%20underrepresenting%20long-tail%20knowledge%20due%20to%20truncation%20effects%20from%20data%20generation%20mechanisms%20like%20top-p%20sampling%2C%20temperature%20scaling%2C%20and%20finite%20sampling.%20These%20discrepancies%20pose%20fundamental%20challenges%20in%20characterizing%20and%20evaluating%20the%20utility%20of%20mixed%20real-synthetic%20datasets.%20In%20this%20paper%2C%20we%20identify%20a%20three-phase%20scaling%20behavior%20characterized%20by%20two%20breakpoints%20that%20reflect%20transitions%20in%20model%20behavior%20across%20learning%20head%20and%20tail%20knowledge.%20We%20further%20derive%20an%20LLM%20generalization%20bound%20designed%20for%20real%20and%20synthetic%20mixtures%2C%20revealing%20several%20key%20factors%20that%20govern%20their%20generalization%20performance.%20Building%20on%20our%20theoretical%20findings%2C%20we%20propose%20an%20effective%20yet%20efficient%20data%20valuation%20method%20that%20scales%20to%20large-scale%20datasets.%20Comprehensive%20experiments%20across%20four%20tasks%2C%20including%20image%20classification%2C%20sentiment%20classification%2C%20instruction%20following%2C%20and%20complex%20reasoning%2C%20demonstrate%20that%20our%20method%20surpasses%20state-of-the-art%20baselines%20in%20data%20valuation%20with%20significantly%20low%20computational%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2511.13640v1&entry.124074799=Read"},
{"title": "Segment Anything Across Shots: A Method and Benchmark", "author": "Hengrui Hu and Kaining Ying and Henghui Ding", "abstract": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.", "link": "http://arxiv.org/abs/2511.13715v1", "date": "2025-11-17", "relevancy": 2.128, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5398}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5336}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20Across%20Shots%3A%20A%20Method%20and%20Benchmark&body=Title%3A%20Segment%20Anything%20Across%20Shots%3A%20A%20Method%20and%20Benchmark%0AAuthor%3A%20Hengrui%20Hu%20and%20Kaining%20Ying%20and%20Henghui%20Ding%0AAbstract%3A%20This%20work%20focuses%20on%20multi-shot%20semi-supervised%20video%20object%20segmentation%20%28MVOS%29%2C%20which%20aims%20at%20segmenting%20the%20target%20object%20indicated%20by%20an%20initial%20mask%20throughout%20a%20video%20with%20multiple%20shots.%20The%20existing%20VOS%20methods%20mainly%20focus%20on%20single-shot%20videos%20and%20struggle%20with%20shot%20discontinuities%2C%20thereby%20limiting%20their%20real-world%20applicability.%20We%20propose%20a%20transition%20mimicking%20data%20augmentation%20strategy%20%28TMA%29%20which%20enables%20cross-shot%20generalization%20with%20single-shot%20data%20to%20alleviate%20the%20severe%20annotated%20multi-shot%20data%20sparsity%2C%20and%20the%20Segment%20Anything%20Across%20Shots%20%28SAAS%29%20model%2C%20which%20can%20detect%20and%20comprehend%20shot%20transitions%20effectively.%20To%20support%20evaluation%20and%20future%20study%20in%20MVOS%2C%20we%20introduce%20Cut-VOS%2C%20a%20new%20MVOS%20benchmark%20with%20dense%20mask%20annotations%2C%20diverse%20object%20categories%2C%20and%20high-frequency%20transitions.%20Extensive%20experiments%20on%20YouMVOS%20and%20Cut-VOS%20demonstrate%20that%20the%20proposed%20SAAS%20achieves%20state-of-the-art%20performance%20by%20effectively%20mimicking%2C%20understanding%2C%20and%20segmenting%20across%20complex%20transitions.%20The%20code%20and%20datasets%20are%20released%20at%20https%3A//henghuiding.com/SAAS/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520Across%2520Shots%253A%2520A%2520Method%2520and%2520Benchmark%26entry.906535625%3DHengrui%2520Hu%2520and%2520Kaining%2520Ying%2520and%2520Henghui%2520Ding%26entry.1292438233%3DThis%2520work%2520focuses%2520on%2520multi-shot%2520semi-supervised%2520video%2520object%2520segmentation%2520%2528MVOS%2529%252C%2520which%2520aims%2520at%2520segmenting%2520the%2520target%2520object%2520indicated%2520by%2520an%2520initial%2520mask%2520throughout%2520a%2520video%2520with%2520multiple%2520shots.%2520The%2520existing%2520VOS%2520methods%2520mainly%2520focus%2520on%2520single-shot%2520videos%2520and%2520struggle%2520with%2520shot%2520discontinuities%252C%2520thereby%2520limiting%2520their%2520real-world%2520applicability.%2520We%2520propose%2520a%2520transition%2520mimicking%2520data%2520augmentation%2520strategy%2520%2528TMA%2529%2520which%2520enables%2520cross-shot%2520generalization%2520with%2520single-shot%2520data%2520to%2520alleviate%2520the%2520severe%2520annotated%2520multi-shot%2520data%2520sparsity%252C%2520and%2520the%2520Segment%2520Anything%2520Across%2520Shots%2520%2528SAAS%2529%2520model%252C%2520which%2520can%2520detect%2520and%2520comprehend%2520shot%2520transitions%2520effectively.%2520To%2520support%2520evaluation%2520and%2520future%2520study%2520in%2520MVOS%252C%2520we%2520introduce%2520Cut-VOS%252C%2520a%2520new%2520MVOS%2520benchmark%2520with%2520dense%2520mask%2520annotations%252C%2520diverse%2520object%2520categories%252C%2520and%2520high-frequency%2520transitions.%2520Extensive%2520experiments%2520on%2520YouMVOS%2520and%2520Cut-VOS%2520demonstrate%2520that%2520the%2520proposed%2520SAAS%2520achieves%2520state-of-the-art%2520performance%2520by%2520effectively%2520mimicking%252C%2520understanding%252C%2520and%2520segmenting%2520across%2520complex%2520transitions.%2520The%2520code%2520and%2520datasets%2520are%2520released%2520at%2520https%253A//henghuiding.com/SAAS/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20Across%20Shots%3A%20A%20Method%20and%20Benchmark&entry.906535625=Hengrui%20Hu%20and%20Kaining%20Ying%20and%20Henghui%20Ding&entry.1292438233=This%20work%20focuses%20on%20multi-shot%20semi-supervised%20video%20object%20segmentation%20%28MVOS%29%2C%20which%20aims%20at%20segmenting%20the%20target%20object%20indicated%20by%20an%20initial%20mask%20throughout%20a%20video%20with%20multiple%20shots.%20The%20existing%20VOS%20methods%20mainly%20focus%20on%20single-shot%20videos%20and%20struggle%20with%20shot%20discontinuities%2C%20thereby%20limiting%20their%20real-world%20applicability.%20We%20propose%20a%20transition%20mimicking%20data%20augmentation%20strategy%20%28TMA%29%20which%20enables%20cross-shot%20generalization%20with%20single-shot%20data%20to%20alleviate%20the%20severe%20annotated%20multi-shot%20data%20sparsity%2C%20and%20the%20Segment%20Anything%20Across%20Shots%20%28SAAS%29%20model%2C%20which%20can%20detect%20and%20comprehend%20shot%20transitions%20effectively.%20To%20support%20evaluation%20and%20future%20study%20in%20MVOS%2C%20we%20introduce%20Cut-VOS%2C%20a%20new%20MVOS%20benchmark%20with%20dense%20mask%20annotations%2C%20diverse%20object%20categories%2C%20and%20high-frequency%20transitions.%20Extensive%20experiments%20on%20YouMVOS%20and%20Cut-VOS%20demonstrate%20that%20the%20proposed%20SAAS%20achieves%20state-of-the-art%20performance%20by%20effectively%20mimicking%2C%20understanding%2C%20and%20segmenting%20across%20complex%20transitions.%20The%20code%20and%20datasets%20are%20released%20at%20https%3A//henghuiding.com/SAAS/.&entry.1838667208=http%3A//arxiv.org/abs/2511.13715v1&entry.124074799=Read"},
{"title": "Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs", "author": "Aleksandar Stankovi\u0107 and Dejan Lisica", "abstract": "We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.", "link": "http://arxiv.org/abs/2511.13250v1", "date": "2025-11-17", "relevancy": 1.9563, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5089}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4774}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-aware%20baselines%20for%20ogbn-proteins%20in%20PyTorch%20Geometric%3A%20species-wise%20normalization%2C%20post-hoc%20calibration%2C%20and%20cost-accuracy%20trade-offs&body=Title%3A%20Edge-aware%20baselines%20for%20ogbn-proteins%20in%20PyTorch%20Geometric%3A%20species-wise%20normalization%2C%20post-hoc%20calibration%2C%20and%20cost-accuracy%20trade-offs%0AAuthor%3A%20Aleksandar%20Stankovi%C4%87%20and%20Dejan%20Lisica%0AAbstract%3A%20We%20present%20reproducible%2C%20edge-aware%20baselines%20for%20ogbn-proteins%20in%20PyTorch%20Geometric%20%28PyG%29.%20We%20study%20two%20system%20choices%20that%20dominate%20practice%3A%20%28i%29%20how%208-dimensional%20edge%20evidence%20is%20aggregated%20into%20node%20inputs%2C%20and%20%28ii%29%20how%20edges%20are%20used%20inside%20message%20passing.%20Our%20strongest%20baseline%20is%20GraphSAGE%20with%20sum-based%20edge-to-node%20features.%20We%20compare%20LayerNorm%20%28LN%29%2C%20BatchNorm%20%28BN%29%2C%20and%20a%20species-aware%20Conditional%20LayerNorm%20%28CLN%29%2C%20and%20report%20compute%20cost%20%28time%2C%20VRAM%2C%20parameters%29%20together%20with%20accuracy%20%28ROC-AUC%29%20and%20decision%20quality.%20In%20our%20primary%20experimental%20setup%20%28hidden%20size%20512%2C%203%20layers%2C%203%20seeds%29%2C%20sum%20consistently%20beats%20mean%20and%20max%3B%20BN%20attains%20the%20best%20AUC%2C%20while%20CLN%20matches%20the%20AUC%20frontier%20with%20better%20thresholded%20F1.%20Finally%2C%20post-hoc%20per-label%20temperature%20scaling%20plus%20per-label%20thresholds%20substantially%20improves%20micro-F1%20and%20expected%20calibration%20error%20%28ECE%29%20with%20negligible%20AUC%20change%2C%20and%20light%20label-correlation%20smoothing%20yields%20small%20additional%20gains.%20We%20release%20standardized%20artifacts%20and%20scripts%20used%20for%20all%20of%20the%20runs%20presented%20in%20the%20paper.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-aware%2520baselines%2520for%2520ogbn-proteins%2520in%2520PyTorch%2520Geometric%253A%2520species-wise%2520normalization%252C%2520post-hoc%2520calibration%252C%2520and%2520cost-accuracy%2520trade-offs%26entry.906535625%3DAleksandar%2520Stankovi%25C4%2587%2520and%2520Dejan%2520Lisica%26entry.1292438233%3DWe%2520present%2520reproducible%252C%2520edge-aware%2520baselines%2520for%2520ogbn-proteins%2520in%2520PyTorch%2520Geometric%2520%2528PyG%2529.%2520We%2520study%2520two%2520system%2520choices%2520that%2520dominate%2520practice%253A%2520%2528i%2529%2520how%25208-dimensional%2520edge%2520evidence%2520is%2520aggregated%2520into%2520node%2520inputs%252C%2520and%2520%2528ii%2529%2520how%2520edges%2520are%2520used%2520inside%2520message%2520passing.%2520Our%2520strongest%2520baseline%2520is%2520GraphSAGE%2520with%2520sum-based%2520edge-to-node%2520features.%2520We%2520compare%2520LayerNorm%2520%2528LN%2529%252C%2520BatchNorm%2520%2528BN%2529%252C%2520and%2520a%2520species-aware%2520Conditional%2520LayerNorm%2520%2528CLN%2529%252C%2520and%2520report%2520compute%2520cost%2520%2528time%252C%2520VRAM%252C%2520parameters%2529%2520together%2520with%2520accuracy%2520%2528ROC-AUC%2529%2520and%2520decision%2520quality.%2520In%2520our%2520primary%2520experimental%2520setup%2520%2528hidden%2520size%2520512%252C%25203%2520layers%252C%25203%2520seeds%2529%252C%2520sum%2520consistently%2520beats%2520mean%2520and%2520max%253B%2520BN%2520attains%2520the%2520best%2520AUC%252C%2520while%2520CLN%2520matches%2520the%2520AUC%2520frontier%2520with%2520better%2520thresholded%2520F1.%2520Finally%252C%2520post-hoc%2520per-label%2520temperature%2520scaling%2520plus%2520per-label%2520thresholds%2520substantially%2520improves%2520micro-F1%2520and%2520expected%2520calibration%2520error%2520%2528ECE%2529%2520with%2520negligible%2520AUC%2520change%252C%2520and%2520light%2520label-correlation%2520smoothing%2520yields%2520small%2520additional%2520gains.%2520We%2520release%2520standardized%2520artifacts%2520and%2520scripts%2520used%2520for%2520all%2520of%2520the%2520runs%2520presented%2520in%2520the%2520paper.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-aware%20baselines%20for%20ogbn-proteins%20in%20PyTorch%20Geometric%3A%20species-wise%20normalization%2C%20post-hoc%20calibration%2C%20and%20cost-accuracy%20trade-offs&entry.906535625=Aleksandar%20Stankovi%C4%87%20and%20Dejan%20Lisica&entry.1292438233=We%20present%20reproducible%2C%20edge-aware%20baselines%20for%20ogbn-proteins%20in%20PyTorch%20Geometric%20%28PyG%29.%20We%20study%20two%20system%20choices%20that%20dominate%20practice%3A%20%28i%29%20how%208-dimensional%20edge%20evidence%20is%20aggregated%20into%20node%20inputs%2C%20and%20%28ii%29%20how%20edges%20are%20used%20inside%20message%20passing.%20Our%20strongest%20baseline%20is%20GraphSAGE%20with%20sum-based%20edge-to-node%20features.%20We%20compare%20LayerNorm%20%28LN%29%2C%20BatchNorm%20%28BN%29%2C%20and%20a%20species-aware%20Conditional%20LayerNorm%20%28CLN%29%2C%20and%20report%20compute%20cost%20%28time%2C%20VRAM%2C%20parameters%29%20together%20with%20accuracy%20%28ROC-AUC%29%20and%20decision%20quality.%20In%20our%20primary%20experimental%20setup%20%28hidden%20size%20512%2C%203%20layers%2C%203%20seeds%29%2C%20sum%20consistently%20beats%20mean%20and%20max%3B%20BN%20attains%20the%20best%20AUC%2C%20while%20CLN%20matches%20the%20AUC%20frontier%20with%20better%20thresholded%20F1.%20Finally%2C%20post-hoc%20per-label%20temperature%20scaling%20plus%20per-label%20thresholds%20substantially%20improves%20micro-F1%20and%20expected%20calibration%20error%20%28ECE%29%20with%20negligible%20AUC%20change%2C%20and%20light%20label-correlation%20smoothing%20yields%20small%20additional%20gains.%20We%20release%20standardized%20artifacts%20and%20scripts%20used%20for%20all%20of%20the%20runs%20presented%20in%20the%20paper.&entry.1838667208=http%3A//arxiv.org/abs/2511.13250v1&entry.124074799=Read"},
{"title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process", "author": "Agnieszka Bie\u0144kowska and Jacek Ma\u0142ecki and Alexander Mathiesen-Ohman and Katarzyna Tworek", "abstract": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.", "link": "http://arxiv.org/abs/2511.13670v1", "date": "2025-11-17", "relevancy": 1.414, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4648}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Person-AI%20Bidirectional%20Fit%20-%20A%20Proof-Of-Concept%20Case%20Study%20Of%20Augmented%20Human-Ai%20Symbiosis%20In%20Management%20Decision-Making%20Process&body=Title%3A%20Person-AI%20Bidirectional%20Fit%20-%20A%20Proof-Of-Concept%20Case%20Study%20Of%20Augmented%20Human-Ai%20Symbiosis%20In%20Management%20Decision-Making%20Process%0AAuthor%3A%20Agnieszka%20Bie%C5%84kowska%20and%20Jacek%20Ma%C5%82ecki%20and%20Alexander%20Mathiesen-Ohman%20and%20Katarzyna%20Tworek%0AAbstract%3A%20This%20article%20develops%20the%20concept%20of%20Person-AI%20bidirectional%20fit%2C%20defined%20as%20the%20continuously%20evolving%2C%20context-sensitive%20alignment-primarily%20cognitive%2C%20but%20also%20emotional%20and%20behavioral-between%20a%20human%20decision-maker%20and%20an%20artificial%20intelligence%20system.%20Grounded%20in%20contingency%20theory%20and%20quality%20theory%2C%20the%20study%20examines%20the%20role%20of%20P-AI%20fit%20in%20managerial%20decision-making%20through%20a%20proof-of-concept%20case%20study%20involving%20a%20real%20hiring%20process%20for%20a%20Senior%20AI%20Lead.%20Three%20decision%20pathways%20are%20compared%3A%20%281%29%20independent%20evaluations%20by%20a%20CEO%2C%20CTO%2C%20and%20CSO%3B%20%282%29%20an%20evaluation%20produced%20by%20an%20augmented%20human-AI%20symbiotic%20intelligence%20system%20%28H3LIX-LAIZA%29%3B%20and%20%283%29%20an%20assessment%20generated%20by%20a%20general-purpose%20large%20language%20model.%20The%20results%20reveal%20substantial%20role-based%20divergence%20in%20human%20judgments%2C%20high%20alignment%20between%20H3LIX-LAIZA%20and%20the%20CEOs%20implicit%20decision%20model-including%20ethical%20disqualification%20of%20a%20high-risk%20candidate%20and%20a%20critical%20false-positive%20recommendation%20from%20the%20LLMr.%20The%20findings%20demonstrate%20that%20higher%20P-AI%20fit%2C%20exemplified%20by%20the%20CEO%20H3LIX-LAIZA%20relationship%2C%20functions%20as%20a%20mechanism%20linking%20augmented%20symbiotic%20intelligence%20to%20accurate%2C%20trustworthy%2C%20and%20context-sensitive%20decisions.%20The%20study%20provides%20an%20initial%20verification%20of%20the%20P-AI%20fit%20construct%20and%20a%20proof-of-concept%20for%20H3LIX-LAIZA%20as%20an%20augmented%20human-AI%20symbiotic%20intelligence%20system.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerson-AI%2520Bidirectional%2520Fit%2520-%2520A%2520Proof-Of-Concept%2520Case%2520Study%2520Of%2520Augmented%2520Human-Ai%2520Symbiosis%2520In%2520Management%2520Decision-Making%2520Process%26entry.906535625%3DAgnieszka%2520Bie%25C5%2584kowska%2520and%2520Jacek%2520Ma%25C5%2582ecki%2520and%2520Alexander%2520Mathiesen-Ohman%2520and%2520Katarzyna%2520Tworek%26entry.1292438233%3DThis%2520article%2520develops%2520the%2520concept%2520of%2520Person-AI%2520bidirectional%2520fit%252C%2520defined%2520as%2520the%2520continuously%2520evolving%252C%2520context-sensitive%2520alignment-primarily%2520cognitive%252C%2520but%2520also%2520emotional%2520and%2520behavioral-between%2520a%2520human%2520decision-maker%2520and%2520an%2520artificial%2520intelligence%2520system.%2520Grounded%2520in%2520contingency%2520theory%2520and%2520quality%2520theory%252C%2520the%2520study%2520examines%2520the%2520role%2520of%2520P-AI%2520fit%2520in%2520managerial%2520decision-making%2520through%2520a%2520proof-of-concept%2520case%2520study%2520involving%2520a%2520real%2520hiring%2520process%2520for%2520a%2520Senior%2520AI%2520Lead.%2520Three%2520decision%2520pathways%2520are%2520compared%253A%2520%25281%2529%2520independent%2520evaluations%2520by%2520a%2520CEO%252C%2520CTO%252C%2520and%2520CSO%253B%2520%25282%2529%2520an%2520evaluation%2520produced%2520by%2520an%2520augmented%2520human-AI%2520symbiotic%2520intelligence%2520system%2520%2528H3LIX-LAIZA%2529%253B%2520and%2520%25283%2529%2520an%2520assessment%2520generated%2520by%2520a%2520general-purpose%2520large%2520language%2520model.%2520The%2520results%2520reveal%2520substantial%2520role-based%2520divergence%2520in%2520human%2520judgments%252C%2520high%2520alignment%2520between%2520H3LIX-LAIZA%2520and%2520the%2520CEOs%2520implicit%2520decision%2520model-including%2520ethical%2520disqualification%2520of%2520a%2520high-risk%2520candidate%2520and%2520a%2520critical%2520false-positive%2520recommendation%2520from%2520the%2520LLMr.%2520The%2520findings%2520demonstrate%2520that%2520higher%2520P-AI%2520fit%252C%2520exemplified%2520by%2520the%2520CEO%2520H3LIX-LAIZA%2520relationship%252C%2520functions%2520as%2520a%2520mechanism%2520linking%2520augmented%2520symbiotic%2520intelligence%2520to%2520accurate%252C%2520trustworthy%252C%2520and%2520context-sensitive%2520decisions.%2520The%2520study%2520provides%2520an%2520initial%2520verification%2520of%2520the%2520P-AI%2520fit%2520construct%2520and%2520a%2520proof-of-concept%2520for%2520H3LIX-LAIZA%2520as%2520an%2520augmented%2520human-AI%2520symbiotic%2520intelligence%2520system.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Person-AI%20Bidirectional%20Fit%20-%20A%20Proof-Of-Concept%20Case%20Study%20Of%20Augmented%20Human-Ai%20Symbiosis%20In%20Management%20Decision-Making%20Process&entry.906535625=Agnieszka%20Bie%C5%84kowska%20and%20Jacek%20Ma%C5%82ecki%20and%20Alexander%20Mathiesen-Ohman%20and%20Katarzyna%20Tworek&entry.1292438233=This%20article%20develops%20the%20concept%20of%20Person-AI%20bidirectional%20fit%2C%20defined%20as%20the%20continuously%20evolving%2C%20context-sensitive%20alignment-primarily%20cognitive%2C%20but%20also%20emotional%20and%20behavioral-between%20a%20human%20decision-maker%20and%20an%20artificial%20intelligence%20system.%20Grounded%20in%20contingency%20theory%20and%20quality%20theory%2C%20the%20study%20examines%20the%20role%20of%20P-AI%20fit%20in%20managerial%20decision-making%20through%20a%20proof-of-concept%20case%20study%20involving%20a%20real%20hiring%20process%20for%20a%20Senior%20AI%20Lead.%20Three%20decision%20pathways%20are%20compared%3A%20%281%29%20independent%20evaluations%20by%20a%20CEO%2C%20CTO%2C%20and%20CSO%3B%20%282%29%20an%20evaluation%20produced%20by%20an%20augmented%20human-AI%20symbiotic%20intelligence%20system%20%28H3LIX-LAIZA%29%3B%20and%20%283%29%20an%20assessment%20generated%20by%20a%20general-purpose%20large%20language%20model.%20The%20results%20reveal%20substantial%20role-based%20divergence%20in%20human%20judgments%2C%20high%20alignment%20between%20H3LIX-LAIZA%20and%20the%20CEOs%20implicit%20decision%20model-including%20ethical%20disqualification%20of%20a%20high-risk%20candidate%20and%20a%20critical%20false-positive%20recommendation%20from%20the%20LLMr.%20The%20findings%20demonstrate%20that%20higher%20P-AI%20fit%2C%20exemplified%20by%20the%20CEO%20H3LIX-LAIZA%20relationship%2C%20functions%20as%20a%20mechanism%20linking%20augmented%20symbiotic%20intelligence%20to%20accurate%2C%20trustworthy%2C%20and%20context-sensitive%20decisions.%20The%20study%20provides%20an%20initial%20verification%20of%20the%20P-AI%20fit%20construct%20and%20a%20proof-of-concept%20for%20H3LIX-LAIZA%20as%20an%20augmented%20human-AI%20symbiotic%20intelligence%20system.&entry.1838667208=http%3A//arxiv.org/abs/2511.13670v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


