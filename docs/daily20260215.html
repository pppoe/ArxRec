<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260212.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation", "author": "Chengxi Zeng and Yuxuan Jiang and Ge Gao and Shuai Wang and Duolikun Danier and Bin Zhu and Stevan Rudinac and David Bull and Fan Zhang", "abstract": "Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.", "link": "http://arxiv.org/abs/2602.12173v1", "date": "2026-02-12", "relevancy": 3.0601, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM3-LiteText%3A%20An%20Anatomical%20Study%20of%20the%20SAM3%20Text%20Encoder%20for%20Efficient%20Vision-Language%20Segmentation&body=Title%3A%20SAM3-LiteText%3A%20An%20Anatomical%20Study%20of%20the%20SAM3%20Text%20Encoder%20for%20Efficient%20Vision-Language%20Segmentation%0AAuthor%3A%20Chengxi%20Zeng%20and%20Yuxuan%20Jiang%20and%20Ge%20Gao%20and%20Shuai%20Wang%20and%20Duolikun%20Danier%20and%20Bin%20Zhu%20and%20Stevan%20Rudinac%20and%20David%20Bull%20and%20Fan%20Zhang%0AAbstract%3A%20Vision-language%20segmentation%20models%20such%20as%20SAM3%20enable%20flexible%2C%20prompt-driven%20visual%20grounding%2C%20but%20inherit%20large%2C%20general-purpose%20text%20encoders%20originally%20designed%20for%20open-ended%20language%20understanding.%20In%20practice%2C%20segmentation%20prompts%20are%20short%2C%20structured%2C%20and%20semantically%20constrained%2C%20leading%20to%20substantial%20over-provisioning%20in%20text%20encoder%20capacity%20and%20persistent%20computational%20and%20memory%20overhead.%20In%20this%20paper%2C%20we%20perform%20a%20large-scale%20anatomical%20analysis%20of%20text%20prompting%20in%20vision-language%20segmentation%2C%20covering%20404%2C796%20real%20prompts%20across%20multiple%20benchmarks.%20Our%20analysis%20reveals%20severe%20redundancy%3A%20most%20context%20windows%20are%20underutilized%2C%20vocabulary%20usage%20is%20highly%20sparse%2C%20and%20text%20embeddings%20lie%20on%20low-dimensional%20manifold%20despite%20high-dimensional%20representations.%20Motivated%20by%20these%20findings%2C%20we%20propose%20SAM3-LiteText%2C%20a%20lightweight%20text%20encoding%20framework%20that%20replaces%20the%20original%20SAM3%20text%20encoder%20with%20a%20compact%20MobileCLIP%20student%20that%20is%20optimized%20by%20knowledge%20distillation.%20Extensive%20experiments%20on%20image%20and%20video%20segmentation%20benchmarks%20show%20that%20SAM3-LiteText%20reduces%20text%20encoder%20parameters%20by%20up%20to%2088%25%2C%20substantially%20reducing%20static%20memory%20footprint%2C%20while%20maintaining%20segmentation%20performance%20comparable%20to%20the%20original%20model.%20Code%3A%20https%3A//github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM3-LiteText%253A%2520An%2520Anatomical%2520Study%2520of%2520the%2520SAM3%2520Text%2520Encoder%2520for%2520Efficient%2520Vision-Language%2520Segmentation%26entry.906535625%3DChengxi%2520Zeng%2520and%2520Yuxuan%2520Jiang%2520and%2520Ge%2520Gao%2520and%2520Shuai%2520Wang%2520and%2520Duolikun%2520Danier%2520and%2520Bin%2520Zhu%2520and%2520Stevan%2520Rudinac%2520and%2520David%2520Bull%2520and%2520Fan%2520Zhang%26entry.1292438233%3DVision-language%2520segmentation%2520models%2520such%2520as%2520SAM3%2520enable%2520flexible%252C%2520prompt-driven%2520visual%2520grounding%252C%2520but%2520inherit%2520large%252C%2520general-purpose%2520text%2520encoders%2520originally%2520designed%2520for%2520open-ended%2520language%2520understanding.%2520In%2520practice%252C%2520segmentation%2520prompts%2520are%2520short%252C%2520structured%252C%2520and%2520semantically%2520constrained%252C%2520leading%2520to%2520substantial%2520over-provisioning%2520in%2520text%2520encoder%2520capacity%2520and%2520persistent%2520computational%2520and%2520memory%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520perform%2520a%2520large-scale%2520anatomical%2520analysis%2520of%2520text%2520prompting%2520in%2520vision-language%2520segmentation%252C%2520covering%2520404%252C796%2520real%2520prompts%2520across%2520multiple%2520benchmarks.%2520Our%2520analysis%2520reveals%2520severe%2520redundancy%253A%2520most%2520context%2520windows%2520are%2520underutilized%252C%2520vocabulary%2520usage%2520is%2520highly%2520sparse%252C%2520and%2520text%2520embeddings%2520lie%2520on%2520low-dimensional%2520manifold%2520despite%2520high-dimensional%2520representations.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520SAM3-LiteText%252C%2520a%2520lightweight%2520text%2520encoding%2520framework%2520that%2520replaces%2520the%2520original%2520SAM3%2520text%2520encoder%2520with%2520a%2520compact%2520MobileCLIP%2520student%2520that%2520is%2520optimized%2520by%2520knowledge%2520distillation.%2520Extensive%2520experiments%2520on%2520image%2520and%2520video%2520segmentation%2520benchmarks%2520show%2520that%2520SAM3-LiteText%2520reduces%2520text%2520encoder%2520parameters%2520by%2520up%2520to%252088%2525%252C%2520substantially%2520reducing%2520static%2520memory%2520footprint%252C%2520while%2520maintaining%2520segmentation%2520performance%2520comparable%2520to%2520the%2520original%2520model.%2520Code%253A%2520https%253A//github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM3-LiteText%3A%20An%20Anatomical%20Study%20of%20the%20SAM3%20Text%20Encoder%20for%20Efficient%20Vision-Language%20Segmentation&entry.906535625=Chengxi%20Zeng%20and%20Yuxuan%20Jiang%20and%20Ge%20Gao%20and%20Shuai%20Wang%20and%20Duolikun%20Danier%20and%20Bin%20Zhu%20and%20Stevan%20Rudinac%20and%20David%20Bull%20and%20Fan%20Zhang&entry.1292438233=Vision-language%20segmentation%20models%20such%20as%20SAM3%20enable%20flexible%2C%20prompt-driven%20visual%20grounding%2C%20but%20inherit%20large%2C%20general-purpose%20text%20encoders%20originally%20designed%20for%20open-ended%20language%20understanding.%20In%20practice%2C%20segmentation%20prompts%20are%20short%2C%20structured%2C%20and%20semantically%20constrained%2C%20leading%20to%20substantial%20over-provisioning%20in%20text%20encoder%20capacity%20and%20persistent%20computational%20and%20memory%20overhead.%20In%20this%20paper%2C%20we%20perform%20a%20large-scale%20anatomical%20analysis%20of%20text%20prompting%20in%20vision-language%20segmentation%2C%20covering%20404%2C796%20real%20prompts%20across%20multiple%20benchmarks.%20Our%20analysis%20reveals%20severe%20redundancy%3A%20most%20context%20windows%20are%20underutilized%2C%20vocabulary%20usage%20is%20highly%20sparse%2C%20and%20text%20embeddings%20lie%20on%20low-dimensional%20manifold%20despite%20high-dimensional%20representations.%20Motivated%20by%20these%20findings%2C%20we%20propose%20SAM3-LiteText%2C%20a%20lightweight%20text%20encoding%20framework%20that%20replaces%20the%20original%20SAM3%20text%20encoder%20with%20a%20compact%20MobileCLIP%20student%20that%20is%20optimized%20by%20knowledge%20distillation.%20Extensive%20experiments%20on%20image%20and%20video%20segmentation%20benchmarks%20show%20that%20SAM3-LiteText%20reduces%20text%20encoder%20parameters%20by%20up%20to%2088%25%2C%20substantially%20reducing%20static%20memory%20footprint%2C%20while%20maintaining%20segmentation%20performance%20comparable%20to%20the%20original%20model.%20Code%3A%20https%3A//github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.&entry.1838667208=http%3A//arxiv.org/abs/2602.12173v1&entry.124074799=Read"},
{"title": "From Implicit Ambiguity to Explicit Solidity: Diagnosing Interior Geometric Degradation in Neural Radiance Fields for Dense 3D Scene Understanding", "author": "Jiangsan Zhao and Jakob Geipel and Kryzysztof Kusnierek", "abstract": "Neural Radiance Fields (NeRFs) have emerged as a powerful paradigm for multi-view reconstruction, complementing classical photogrammetric pipelines based on Structure-from-Motion (SfM) and Multi-View Stereo (MVS). However, their reliability for quantitative 3D analysis in dense, self-occluding scenes remains poorly understood. In this study, we identify a fundamental failure mode of implicit density fields under heavy occlusion, which we term Interior Geometric Degradation (IGD). We show that transmittance-based volumetric optimization satisfies photometric supervision by reconstructing hollow or fragmented structures rather than solid interiors, leading to systematic instance undercounting. Through controlled experiments on synthetic datasets with increasing occlusion, we demonstrate that state-of-the-art mask-supervised NeRFs saturate at approximately 89% instance recovery in dense scenes, despite improved surface coherence and mask quality. To overcome this limitation, we introduce an explicit geometric pipeline based on Sparse Voxel Rasterization (SVRaster), initialized from SfM feature geometry. By projecting 2D instance masks onto an explicit voxel grid and enforcing geometric separation via recursive splitting, our approach preserves physical solidity and achieves a 95.8% recovery rate in dense clusters. A sensitivity analysis using degraded segmentation masks further shows that explicit SfM-based geometry is substantially more robust to supervision failure, recovering 43% more instances than implicit baselines. These results demonstrate that explicit geometric priors are a prerequisite for reliable quantitative analysis in highly self-occluding 3D scenes.", "link": "http://arxiv.org/abs/2601.21421v2", "date": "2026-02-12", "relevancy": 3.0348, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Implicit%20Ambiguity%20to%20Explicit%20Solidity%3A%20Diagnosing%20Interior%20Geometric%20Degradation%20in%20Neural%20Radiance%20Fields%20for%20Dense%203D%20Scene%20Understanding&body=Title%3A%20From%20Implicit%20Ambiguity%20to%20Explicit%20Solidity%3A%20Diagnosing%20Interior%20Geometric%20Degradation%20in%20Neural%20Radiance%20Fields%20for%20Dense%203D%20Scene%20Understanding%0AAuthor%3A%20Jiangsan%20Zhao%20and%20Jakob%20Geipel%20and%20Kryzysztof%20Kusnierek%0AAbstract%3A%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20powerful%20paradigm%20for%20multi-view%20reconstruction%2C%20complementing%20classical%20photogrammetric%20pipelines%20based%20on%20Structure-from-Motion%20%28SfM%29%20and%20Multi-View%20Stereo%20%28MVS%29.%20However%2C%20their%20reliability%20for%20quantitative%203D%20analysis%20in%20dense%2C%20self-occluding%20scenes%20remains%20poorly%20understood.%20In%20this%20study%2C%20we%20identify%20a%20fundamental%20failure%20mode%20of%20implicit%20density%20fields%20under%20heavy%20occlusion%2C%20which%20we%20term%20Interior%20Geometric%20Degradation%20%28IGD%29.%20We%20show%20that%20transmittance-based%20volumetric%20optimization%20satisfies%20photometric%20supervision%20by%20reconstructing%20hollow%20or%20fragmented%20structures%20rather%20than%20solid%20interiors%2C%20leading%20to%20systematic%20instance%20undercounting.%20Through%20controlled%20experiments%20on%20synthetic%20datasets%20with%20increasing%20occlusion%2C%20we%20demonstrate%20that%20state-of-the-art%20mask-supervised%20NeRFs%20saturate%20at%20approximately%2089%25%20instance%20recovery%20in%20dense%20scenes%2C%20despite%20improved%20surface%20coherence%20and%20mask%20quality.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20an%20explicit%20geometric%20pipeline%20based%20on%20Sparse%20Voxel%20Rasterization%20%28SVRaster%29%2C%20initialized%20from%20SfM%20feature%20geometry.%20By%20projecting%202D%20instance%20masks%20onto%20an%20explicit%20voxel%20grid%20and%20enforcing%20geometric%20separation%20via%20recursive%20splitting%2C%20our%20approach%20preserves%20physical%20solidity%20and%20achieves%20a%2095.8%25%20recovery%20rate%20in%20dense%20clusters.%20A%20sensitivity%20analysis%20using%20degraded%20segmentation%20masks%20further%20shows%20that%20explicit%20SfM-based%20geometry%20is%20substantially%20more%20robust%20to%20supervision%20failure%2C%20recovering%2043%25%20more%20instances%20than%20implicit%20baselines.%20These%20results%20demonstrate%20that%20explicit%20geometric%20priors%20are%20a%20prerequisite%20for%20reliable%20quantitative%20analysis%20in%20highly%20self-occluding%203D%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Implicit%2520Ambiguity%2520to%2520Explicit%2520Solidity%253A%2520Diagnosing%2520Interior%2520Geometric%2520Degradation%2520in%2520Neural%2520Radiance%2520Fields%2520for%2520Dense%25203D%2520Scene%2520Understanding%26entry.906535625%3DJiangsan%2520Zhao%2520and%2520Jakob%2520Geipel%2520and%2520Kryzysztof%2520Kusnierek%26entry.1292438233%3DNeural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520multi-view%2520reconstruction%252C%2520complementing%2520classical%2520photogrammetric%2520pipelines%2520based%2520on%2520Structure-from-Motion%2520%2528SfM%2529%2520and%2520Multi-View%2520Stereo%2520%2528MVS%2529.%2520However%252C%2520their%2520reliability%2520for%2520quantitative%25203D%2520analysis%2520in%2520dense%252C%2520self-occluding%2520scenes%2520remains%2520poorly%2520understood.%2520In%2520this%2520study%252C%2520we%2520identify%2520a%2520fundamental%2520failure%2520mode%2520of%2520implicit%2520density%2520fields%2520under%2520heavy%2520occlusion%252C%2520which%2520we%2520term%2520Interior%2520Geometric%2520Degradation%2520%2528IGD%2529.%2520We%2520show%2520that%2520transmittance-based%2520volumetric%2520optimization%2520satisfies%2520photometric%2520supervision%2520by%2520reconstructing%2520hollow%2520or%2520fragmented%2520structures%2520rather%2520than%2520solid%2520interiors%252C%2520leading%2520to%2520systematic%2520instance%2520undercounting.%2520Through%2520controlled%2520experiments%2520on%2520synthetic%2520datasets%2520with%2520increasing%2520occlusion%252C%2520we%2520demonstrate%2520that%2520state-of-the-art%2520mask-supervised%2520NeRFs%2520saturate%2520at%2520approximately%252089%2525%2520instance%2520recovery%2520in%2520dense%2520scenes%252C%2520despite%2520improved%2520surface%2520coherence%2520and%2520mask%2520quality.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520an%2520explicit%2520geometric%2520pipeline%2520based%2520on%2520Sparse%2520Voxel%2520Rasterization%2520%2528SVRaster%2529%252C%2520initialized%2520from%2520SfM%2520feature%2520geometry.%2520By%2520projecting%25202D%2520instance%2520masks%2520onto%2520an%2520explicit%2520voxel%2520grid%2520and%2520enforcing%2520geometric%2520separation%2520via%2520recursive%2520splitting%252C%2520our%2520approach%2520preserves%2520physical%2520solidity%2520and%2520achieves%2520a%252095.8%2525%2520recovery%2520rate%2520in%2520dense%2520clusters.%2520A%2520sensitivity%2520analysis%2520using%2520degraded%2520segmentation%2520masks%2520further%2520shows%2520that%2520explicit%2520SfM-based%2520geometry%2520is%2520substantially%2520more%2520robust%2520to%2520supervision%2520failure%252C%2520recovering%252043%2525%2520more%2520instances%2520than%2520implicit%2520baselines.%2520These%2520results%2520demonstrate%2520that%2520explicit%2520geometric%2520priors%2520are%2520a%2520prerequisite%2520for%2520reliable%2520quantitative%2520analysis%2520in%2520highly%2520self-occluding%25203D%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Implicit%20Ambiguity%20to%20Explicit%20Solidity%3A%20Diagnosing%20Interior%20Geometric%20Degradation%20in%20Neural%20Radiance%20Fields%20for%20Dense%203D%20Scene%20Understanding&entry.906535625=Jiangsan%20Zhao%20and%20Jakob%20Geipel%20and%20Kryzysztof%20Kusnierek&entry.1292438233=Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20powerful%20paradigm%20for%20multi-view%20reconstruction%2C%20complementing%20classical%20photogrammetric%20pipelines%20based%20on%20Structure-from-Motion%20%28SfM%29%20and%20Multi-View%20Stereo%20%28MVS%29.%20However%2C%20their%20reliability%20for%20quantitative%203D%20analysis%20in%20dense%2C%20self-occluding%20scenes%20remains%20poorly%20understood.%20In%20this%20study%2C%20we%20identify%20a%20fundamental%20failure%20mode%20of%20implicit%20density%20fields%20under%20heavy%20occlusion%2C%20which%20we%20term%20Interior%20Geometric%20Degradation%20%28IGD%29.%20We%20show%20that%20transmittance-based%20volumetric%20optimization%20satisfies%20photometric%20supervision%20by%20reconstructing%20hollow%20or%20fragmented%20structures%20rather%20than%20solid%20interiors%2C%20leading%20to%20systematic%20instance%20undercounting.%20Through%20controlled%20experiments%20on%20synthetic%20datasets%20with%20increasing%20occlusion%2C%20we%20demonstrate%20that%20state-of-the-art%20mask-supervised%20NeRFs%20saturate%20at%20approximately%2089%25%20instance%20recovery%20in%20dense%20scenes%2C%20despite%20improved%20surface%20coherence%20and%20mask%20quality.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20an%20explicit%20geometric%20pipeline%20based%20on%20Sparse%20Voxel%20Rasterization%20%28SVRaster%29%2C%20initialized%20from%20SfM%20feature%20geometry.%20By%20projecting%202D%20instance%20masks%20onto%20an%20explicit%20voxel%20grid%20and%20enforcing%20geometric%20separation%20via%20recursive%20splitting%2C%20our%20approach%20preserves%20physical%20solidity%20and%20achieves%20a%2095.8%25%20recovery%20rate%20in%20dense%20clusters.%20A%20sensitivity%20analysis%20using%20degraded%20segmentation%20masks%20further%20shows%20that%20explicit%20SfM-based%20geometry%20is%20substantially%20more%20robust%20to%20supervision%20failure%2C%20recovering%2043%25%20more%20instances%20than%20implicit%20baselines.%20These%20results%20demonstrate%20that%20explicit%20geometric%20priors%20are%20a%20prerequisite%20for%20reliable%20quantitative%20analysis%20in%20highly%20self-occluding%203D%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2601.21421v2&entry.124074799=Read"},
{"title": "NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review (Updated Post-Gaussian Splatting)", "author": "Kyle Gao and Yina Gao and Hongjie He and Dening Lu and Linlin Xu and Jonathan Li", "abstract": "In March 2020, Neural Radiance Field (NeRF) revolutionized Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. In August 2023, Gaussian Splatting, a direct competitor to the NeRF-based framework, was proposed, gaining tremendous momentum and overtaking NeRF-based research in terms of interest as the dominant framework for novel view synthesis. We present a comprehensive survey of NeRF papers from the past five years (2020-2025). These include papers from the pre-Gaussian Splatting era, where NeRF dominated the field for novel view synthesis and 3D implicit and hybrid representation neural field learning. We also include works from the post-Gaussian Splatting era where NeRF and implicit/hybrid neural fields found more niche applications.\n  Our survey is organized into architecture and application-based taxonomies in the pre-Gaussian Splatting era, as well as a categorization of active research areas for NeRF, neural field, and implicit/hybrid neural representation methods. We provide an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of classical NeRF, implicit and hybrid neural representation, and neural field models, and an overview of key datasets.", "link": "http://arxiv.org/abs/2210.00379v8", "date": "2026-02-12", "relevancy": 2.9687, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6267}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6257}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF%3A%20Neural%20Radiance%20Field%20in%203D%20Vision%3A%20A%20Comprehensive%20Review%20%28Updated%20Post-Gaussian%20Splatting%29&body=Title%3A%20NeRF%3A%20Neural%20Radiance%20Field%20in%203D%20Vision%3A%20A%20Comprehensive%20Review%20%28Updated%20Post-Gaussian%20Splatting%29%0AAuthor%3A%20Kyle%20Gao%20and%20Yina%20Gao%20and%20Hongjie%20He%20and%20Dening%20Lu%20and%20Linlin%20Xu%20and%20Jonathan%20Li%0AAbstract%3A%20In%20March%202020%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20revolutionized%20Computer%20Vision%2C%20allowing%20for%20implicit%2C%20neural%20network-based%20scene%20representation%20and%20novel%20view%20synthesis.%20NeRF%20models%20have%20found%20diverse%20applications%20in%20robotics%2C%20urban%20mapping%2C%20autonomous%20navigation%2C%20virtual%20reality/augmented%20reality%2C%20and%20more.%20In%20August%202023%2C%20Gaussian%20Splatting%2C%20a%20direct%20competitor%20to%20the%20NeRF-based%20framework%2C%20was%20proposed%2C%20gaining%20tremendous%20momentum%20and%20overtaking%20NeRF-based%20research%20in%20terms%20of%20interest%20as%20the%20dominant%20framework%20for%20novel%20view%20synthesis.%20We%20present%20a%20comprehensive%20survey%20of%20NeRF%20papers%20from%20the%20past%20five%20years%20%282020-2025%29.%20These%20include%20papers%20from%20the%20pre-Gaussian%20Splatting%20era%2C%20where%20NeRF%20dominated%20the%20field%20for%20novel%20view%20synthesis%20and%203D%20implicit%20and%20hybrid%20representation%20neural%20field%20learning.%20We%20also%20include%20works%20from%20the%20post-Gaussian%20Splatting%20era%20where%20NeRF%20and%20implicit/hybrid%20neural%20fields%20found%20more%20niche%20applications.%0A%20%20Our%20survey%20is%20organized%20into%20architecture%20and%20application-based%20taxonomies%20in%20the%20pre-Gaussian%20Splatting%20era%2C%20as%20well%20as%20a%20categorization%20of%20active%20research%20areas%20for%20NeRF%2C%20neural%20field%2C%20and%20implicit/hybrid%20neural%20representation%20methods.%20We%20provide%20an%20introduction%20to%20the%20theory%20of%20NeRF%20and%20its%20training%20via%20differentiable%20volume%20rendering.%20We%20also%20present%20a%20benchmark%20comparison%20of%20the%20performance%20and%20speed%20of%20classical%20NeRF%2C%20implicit%20and%20hybrid%20neural%20representation%2C%20and%20neural%20field%20models%2C%20and%20an%20overview%20of%20key%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2210.00379v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF%253A%2520Neural%2520Radiance%2520Field%2520in%25203D%2520Vision%253A%2520A%2520Comprehensive%2520Review%2520%2528Updated%2520Post-Gaussian%2520Splatting%2529%26entry.906535625%3DKyle%2520Gao%2520and%2520Yina%2520Gao%2520and%2520Hongjie%2520He%2520and%2520Dening%2520Lu%2520and%2520Linlin%2520Xu%2520and%2520Jonathan%2520Li%26entry.1292438233%3DIn%2520March%25202020%252C%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520revolutionized%2520Computer%2520Vision%252C%2520allowing%2520for%2520implicit%252C%2520neural%2520network-based%2520scene%2520representation%2520and%2520novel%2520view%2520synthesis.%2520NeRF%2520models%2520have%2520found%2520diverse%2520applications%2520in%2520robotics%252C%2520urban%2520mapping%252C%2520autonomous%2520navigation%252C%2520virtual%2520reality/augmented%2520reality%252C%2520and%2520more.%2520In%2520August%25202023%252C%2520Gaussian%2520Splatting%252C%2520a%2520direct%2520competitor%2520to%2520the%2520NeRF-based%2520framework%252C%2520was%2520proposed%252C%2520gaining%2520tremendous%2520momentum%2520and%2520overtaking%2520NeRF-based%2520research%2520in%2520terms%2520of%2520interest%2520as%2520the%2520dominant%2520framework%2520for%2520novel%2520view%2520synthesis.%2520We%2520present%2520a%2520comprehensive%2520survey%2520of%2520NeRF%2520papers%2520from%2520the%2520past%2520five%2520years%2520%25282020-2025%2529.%2520These%2520include%2520papers%2520from%2520the%2520pre-Gaussian%2520Splatting%2520era%252C%2520where%2520NeRF%2520dominated%2520the%2520field%2520for%2520novel%2520view%2520synthesis%2520and%25203D%2520implicit%2520and%2520hybrid%2520representation%2520neural%2520field%2520learning.%2520We%2520also%2520include%2520works%2520from%2520the%2520post-Gaussian%2520Splatting%2520era%2520where%2520NeRF%2520and%2520implicit/hybrid%2520neural%2520fields%2520found%2520more%2520niche%2520applications.%250A%2520%2520Our%2520survey%2520is%2520organized%2520into%2520architecture%2520and%2520application-based%2520taxonomies%2520in%2520the%2520pre-Gaussian%2520Splatting%2520era%252C%2520as%2520well%2520as%2520a%2520categorization%2520of%2520active%2520research%2520areas%2520for%2520NeRF%252C%2520neural%2520field%252C%2520and%2520implicit/hybrid%2520neural%2520representation%2520methods.%2520We%2520provide%2520an%2520introduction%2520to%2520the%2520theory%2520of%2520NeRF%2520and%2520its%2520training%2520via%2520differentiable%2520volume%2520rendering.%2520We%2520also%2520present%2520a%2520benchmark%2520comparison%2520of%2520the%2520performance%2520and%2520speed%2520of%2520classical%2520NeRF%252C%2520implicit%2520and%2520hybrid%2520neural%2520representation%252C%2520and%2520neural%2520field%2520models%252C%2520and%2520an%2520overview%2520of%2520key%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.00379v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF%3A%20Neural%20Radiance%20Field%20in%203D%20Vision%3A%20A%20Comprehensive%20Review%20%28Updated%20Post-Gaussian%20Splatting%29&entry.906535625=Kyle%20Gao%20and%20Yina%20Gao%20and%20Hongjie%20He%20and%20Dening%20Lu%20and%20Linlin%20Xu%20and%20Jonathan%20Li&entry.1292438233=In%20March%202020%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20revolutionized%20Computer%20Vision%2C%20allowing%20for%20implicit%2C%20neural%20network-based%20scene%20representation%20and%20novel%20view%20synthesis.%20NeRF%20models%20have%20found%20diverse%20applications%20in%20robotics%2C%20urban%20mapping%2C%20autonomous%20navigation%2C%20virtual%20reality/augmented%20reality%2C%20and%20more.%20In%20August%202023%2C%20Gaussian%20Splatting%2C%20a%20direct%20competitor%20to%20the%20NeRF-based%20framework%2C%20was%20proposed%2C%20gaining%20tremendous%20momentum%20and%20overtaking%20NeRF-based%20research%20in%20terms%20of%20interest%20as%20the%20dominant%20framework%20for%20novel%20view%20synthesis.%20We%20present%20a%20comprehensive%20survey%20of%20NeRF%20papers%20from%20the%20past%20five%20years%20%282020-2025%29.%20These%20include%20papers%20from%20the%20pre-Gaussian%20Splatting%20era%2C%20where%20NeRF%20dominated%20the%20field%20for%20novel%20view%20synthesis%20and%203D%20implicit%20and%20hybrid%20representation%20neural%20field%20learning.%20We%20also%20include%20works%20from%20the%20post-Gaussian%20Splatting%20era%20where%20NeRF%20and%20implicit/hybrid%20neural%20fields%20found%20more%20niche%20applications.%0A%20%20Our%20survey%20is%20organized%20into%20architecture%20and%20application-based%20taxonomies%20in%20the%20pre-Gaussian%20Splatting%20era%2C%20as%20well%20as%20a%20categorization%20of%20active%20research%20areas%20for%20NeRF%2C%20neural%20field%2C%20and%20implicit/hybrid%20neural%20representation%20methods.%20We%20provide%20an%20introduction%20to%20the%20theory%20of%20NeRF%20and%20its%20training%20via%20differentiable%20volume%20rendering.%20We%20also%20present%20a%20benchmark%20comparison%20of%20the%20performance%20and%20speed%20of%20classical%20NeRF%2C%20implicit%20and%20hybrid%20neural%20representation%2C%20and%20neural%20field%20models%2C%20and%20an%20overview%20of%20key%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2210.00379v8&entry.124074799=Read"},
{"title": "Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark", "author": "Xinxin Liu and Zhaopan Xu and Ming Li and Kai Wang and Yong Jae Lee and Yuzhang Shang", "abstract": "While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.", "link": "http://arxiv.org/abs/2511.13853v3", "date": "2026-02-12", "relevancy": 2.9532, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20World%20Simulators%20Reason%3F%20Gen-ViRe%3A%20A%20Generative%20Visual%20Reasoning%20Benchmark&body=Title%3A%20Can%20World%20Simulators%20Reason%3F%20Gen-ViRe%3A%20A%20Generative%20Visual%20Reasoning%20Benchmark%0AAuthor%3A%20Xinxin%20Liu%20and%20Zhaopan%20Xu%20and%20Ming%20Li%20and%20Kai%20Wang%20and%20Yong%20Jae%20Lee%20and%20Yuzhang%20Shang%0AAbstract%3A%20While%20Chain-of-Thought%20%28CoT%29%20prompting%20enables%20sophisticated%20symbolic%20reasoning%20in%20LLMs%2C%20it%20remains%20confined%20to%20discrete%20text%20and%20cannot%20simulate%20the%20continuous%2C%20physics-governed%20dynamics%20of%20the%20real%20world.%20Recent%20video%20generation%20models%20have%20emerged%20as%20potential%20world%20simulators%20through%20Chain-of-Frames%20%28CoF%29%20reasoning%20--%20materializing%20thought%20as%20frame-by-frame%20visual%20sequences%2C%20with%20each%20frame%20representing%20a%20physically-grounded%20reasoning%20step.%20Despite%20compelling%20demonstrations%2C%20a%20challenge%20persists%3A%20existing%20benchmarks%2C%20focusing%20on%20fidelity%20or%20alignment%2C%20do%20not%20assess%20CoF%20reasoning%20and%20thus%20cannot%20measure%20core%20cognitive%20abilities%20in%20multi-step%20planning%2C%20algorithmic%20logic%2C%20or%20abstract%20pattern%20extrapolation.%20This%20evaluation%20void%20prevents%20systematic%20understanding%20of%20model%20capabilities%20and%20principled%20guidance%20for%20improvement.%20We%20introduce%20Gen-ViRe%20%28Generative%20Visual%20Reasoning%20Benchmark%29%2C%20a%20framework%20grounded%20in%20cognitive%20science%20and%20real-world%20AI%20applications%2C%20which%20decomposes%20CoF%20reasoning%20into%20six%20cognitive%20dimensions%20--%20from%20perceptual%20logic%20to%20abstract%20planning%20--%20and%2024%20subtasks.%20Through%20multi-source%20data%20curation%2C%20minimal%20prompting%20protocols%2C%20and%20hybrid%20VLM-assisted%20evaluation%20with%20detailed%20criteria%2C%20Gen-ViRe%20delivers%20the%20first%20quantitative%20assessment%20of%20video%20models%20as%20reasoners.%20Our%20experiments%20on%20SOTA%20systems%20reveal%20substantial%20discrepancies%20between%20impressive%20visual%20quality%20and%20actual%20reasoning%20depth%2C%20establishing%20baselines%20and%20diagnostic%20tools%20to%20advance%20genuine%20world%20simulators.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13853v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520World%2520Simulators%2520Reason%253F%2520Gen-ViRe%253A%2520A%2520Generative%2520Visual%2520Reasoning%2520Benchmark%26entry.906535625%3DXinxin%2520Liu%2520and%2520Zhaopan%2520Xu%2520and%2520Ming%2520Li%2520and%2520Kai%2520Wang%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Yuzhang%2520Shang%26entry.1292438233%3DWhile%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520enables%2520sophisticated%2520symbolic%2520reasoning%2520in%2520LLMs%252C%2520it%2520remains%2520confined%2520to%2520discrete%2520text%2520and%2520cannot%2520simulate%2520the%2520continuous%252C%2520physics-governed%2520dynamics%2520of%2520the%2520real%2520world.%2520Recent%2520video%2520generation%2520models%2520have%2520emerged%2520as%2520potential%2520world%2520simulators%2520through%2520Chain-of-Frames%2520%2528CoF%2529%2520reasoning%2520--%2520materializing%2520thought%2520as%2520frame-by-frame%2520visual%2520sequences%252C%2520with%2520each%2520frame%2520representing%2520a%2520physically-grounded%2520reasoning%2520step.%2520Despite%2520compelling%2520demonstrations%252C%2520a%2520challenge%2520persists%253A%2520existing%2520benchmarks%252C%2520focusing%2520on%2520fidelity%2520or%2520alignment%252C%2520do%2520not%2520assess%2520CoF%2520reasoning%2520and%2520thus%2520cannot%2520measure%2520core%2520cognitive%2520abilities%2520in%2520multi-step%2520planning%252C%2520algorithmic%2520logic%252C%2520or%2520abstract%2520pattern%2520extrapolation.%2520This%2520evaluation%2520void%2520prevents%2520systematic%2520understanding%2520of%2520model%2520capabilities%2520and%2520principled%2520guidance%2520for%2520improvement.%2520We%2520introduce%2520Gen-ViRe%2520%2528Generative%2520Visual%2520Reasoning%2520Benchmark%2529%252C%2520a%2520framework%2520grounded%2520in%2520cognitive%2520science%2520and%2520real-world%2520AI%2520applications%252C%2520which%2520decomposes%2520CoF%2520reasoning%2520into%2520six%2520cognitive%2520dimensions%2520--%2520from%2520perceptual%2520logic%2520to%2520abstract%2520planning%2520--%2520and%252024%2520subtasks.%2520Through%2520multi-source%2520data%2520curation%252C%2520minimal%2520prompting%2520protocols%252C%2520and%2520hybrid%2520VLM-assisted%2520evaluation%2520with%2520detailed%2520criteria%252C%2520Gen-ViRe%2520delivers%2520the%2520first%2520quantitative%2520assessment%2520of%2520video%2520models%2520as%2520reasoners.%2520Our%2520experiments%2520on%2520SOTA%2520systems%2520reveal%2520substantial%2520discrepancies%2520between%2520impressive%2520visual%2520quality%2520and%2520actual%2520reasoning%2520depth%252C%2520establishing%2520baselines%2520and%2520diagnostic%2520tools%2520to%2520advance%2520genuine%2520world%2520simulators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13853v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20World%20Simulators%20Reason%3F%20Gen-ViRe%3A%20A%20Generative%20Visual%20Reasoning%20Benchmark&entry.906535625=Xinxin%20Liu%20and%20Zhaopan%20Xu%20and%20Ming%20Li%20and%20Kai%20Wang%20and%20Yong%20Jae%20Lee%20and%20Yuzhang%20Shang&entry.1292438233=While%20Chain-of-Thought%20%28CoT%29%20prompting%20enables%20sophisticated%20symbolic%20reasoning%20in%20LLMs%2C%20it%20remains%20confined%20to%20discrete%20text%20and%20cannot%20simulate%20the%20continuous%2C%20physics-governed%20dynamics%20of%20the%20real%20world.%20Recent%20video%20generation%20models%20have%20emerged%20as%20potential%20world%20simulators%20through%20Chain-of-Frames%20%28CoF%29%20reasoning%20--%20materializing%20thought%20as%20frame-by-frame%20visual%20sequences%2C%20with%20each%20frame%20representing%20a%20physically-grounded%20reasoning%20step.%20Despite%20compelling%20demonstrations%2C%20a%20challenge%20persists%3A%20existing%20benchmarks%2C%20focusing%20on%20fidelity%20or%20alignment%2C%20do%20not%20assess%20CoF%20reasoning%20and%20thus%20cannot%20measure%20core%20cognitive%20abilities%20in%20multi-step%20planning%2C%20algorithmic%20logic%2C%20or%20abstract%20pattern%20extrapolation.%20This%20evaluation%20void%20prevents%20systematic%20understanding%20of%20model%20capabilities%20and%20principled%20guidance%20for%20improvement.%20We%20introduce%20Gen-ViRe%20%28Generative%20Visual%20Reasoning%20Benchmark%29%2C%20a%20framework%20grounded%20in%20cognitive%20science%20and%20real-world%20AI%20applications%2C%20which%20decomposes%20CoF%20reasoning%20into%20six%20cognitive%20dimensions%20--%20from%20perceptual%20logic%20to%20abstract%20planning%20--%20and%2024%20subtasks.%20Through%20multi-source%20data%20curation%2C%20minimal%20prompting%20protocols%2C%20and%20hybrid%20VLM-assisted%20evaluation%20with%20detailed%20criteria%2C%20Gen-ViRe%20delivers%20the%20first%20quantitative%20assessment%20of%20video%20models%20as%20reasoners.%20Our%20experiments%20on%20SOTA%20systems%20reveal%20substantial%20discrepancies%20between%20impressive%20visual%20quality%20and%20actual%20reasoning%20depth%2C%20establishing%20baselines%20and%20diagnostic%20tools%20to%20advance%20genuine%20world%20simulators.&entry.1838667208=http%3A//arxiv.org/abs/2511.13853v3&entry.124074799=Read"},
{"title": "Chatting with Images for Introspective Visual Thinking", "author": "Junfei Wu and Jian Guan and Qiang Liu and Shu Wu and Liang Wang and Wei Wu and Tieniu Tan", "abstract": "Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.", "link": "http://arxiv.org/abs/2602.11073v2", "date": "2026-02-12", "relevancy": 2.9334, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chatting%20with%20Images%20for%20Introspective%20Visual%20Thinking&body=Title%3A%20Chatting%20with%20Images%20for%20Introspective%20Visual%20Thinking%0AAuthor%3A%20Junfei%20Wu%20and%20Jian%20Guan%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%20and%20Wei%20Wu%20and%20Tieniu%20Tan%0AAbstract%3A%20Current%20large%20vision-language%20models%20%28LVLMs%29%20typically%20rely%20on%20text-only%20reasoning%20based%20on%20a%20single-pass%20visual%20encoding%2C%20which%20often%20leads%20to%20loss%20of%20fine-grained%20visual%20information.%20Recently%20the%20proposal%20of%20%27%27thinking%20with%20images%27%27%20attempts%20to%20alleviate%20this%20limitation%20by%20manipulating%20images%20via%20external%20tools%20or%20code%3B%20however%2C%20the%20resulting%20visual%20states%20are%20often%20insufficiently%20grounded%20in%20linguistic%20semantics%2C%20impairing%20effective%20cross-modal%20alignment%20-%20particularly%20when%20visual%20semantics%20or%20geometric%20relationships%20must%20be%20reasoned%20over%20across%20distant%20regions%20or%20multiple%20images.%20To%20address%20these%20challenges%2C%20we%20propose%20%27%27chatting%20with%20images%27%27%2C%20a%20new%20framework%20that%20reframes%20visual%20manipulation%20as%20language-guided%20feature%20modulation.%20Under%20the%20guidance%20of%20expressive%20language%20prompts%2C%20the%20model%20dynamically%20performs%20joint%20re-encoding%20over%20multiple%20image%20regions%2C%20enabling%20tighter%20coupling%20between%20linguistic%20reasoning%20and%20visual%20state%20updates.%20We%20instantiate%20this%20paradigm%20in%20ViLaVT%2C%20a%20novel%20LVLM%20equipped%20with%20a%20dynamic%20vision%20encoder%20explicitly%20designed%20for%20such%20interactive%20visual%20reasoning%2C%20and%20trained%20it%20with%20a%20two-stage%20curriculum%20combining%20supervised%20fine-tuning%20and%20reinforcement%20learning%20to%20promote%20effective%20reasoning%20behaviors.%20Extensive%20experiments%20across%20eight%20benchmarks%20demonstrate%20that%20ViLaVT%20achieves%20strong%20and%20consistent%20improvements%2C%20with%20particularly%20pronounced%20gains%20on%20complex%20multi-image%20and%20video-based%20spatial%20reasoning%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatting%2520with%2520Images%2520for%2520Introspective%2520Visual%2520Thinking%26entry.906535625%3DJunfei%2520Wu%2520and%2520Jian%2520Guan%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%2520and%2520Wei%2520Wu%2520and%2520Tieniu%2520Tan%26entry.1292438233%3DCurrent%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520typically%2520rely%2520on%2520text-only%2520reasoning%2520based%2520on%2520a%2520single-pass%2520visual%2520encoding%252C%2520which%2520often%2520leads%2520to%2520loss%2520of%2520fine-grained%2520visual%2520information.%2520Recently%2520the%2520proposal%2520of%2520%2527%2527thinking%2520with%2520images%2527%2527%2520attempts%2520to%2520alleviate%2520this%2520limitation%2520by%2520manipulating%2520images%2520via%2520external%2520tools%2520or%2520code%253B%2520however%252C%2520the%2520resulting%2520visual%2520states%2520are%2520often%2520insufficiently%2520grounded%2520in%2520linguistic%2520semantics%252C%2520impairing%2520effective%2520cross-modal%2520alignment%2520-%2520particularly%2520when%2520visual%2520semantics%2520or%2520geometric%2520relationships%2520must%2520be%2520reasoned%2520over%2520across%2520distant%2520regions%2520or%2520multiple%2520images.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%2527%2527chatting%2520with%2520images%2527%2527%252C%2520a%2520new%2520framework%2520that%2520reframes%2520visual%2520manipulation%2520as%2520language-guided%2520feature%2520modulation.%2520Under%2520the%2520guidance%2520of%2520expressive%2520language%2520prompts%252C%2520the%2520model%2520dynamically%2520performs%2520joint%2520re-encoding%2520over%2520multiple%2520image%2520regions%252C%2520enabling%2520tighter%2520coupling%2520between%2520linguistic%2520reasoning%2520and%2520visual%2520state%2520updates.%2520We%2520instantiate%2520this%2520paradigm%2520in%2520ViLaVT%252C%2520a%2520novel%2520LVLM%2520equipped%2520with%2520a%2520dynamic%2520vision%2520encoder%2520explicitly%2520designed%2520for%2520such%2520interactive%2520visual%2520reasoning%252C%2520and%2520trained%2520it%2520with%2520a%2520two-stage%2520curriculum%2520combining%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520to%2520promote%2520effective%2520reasoning%2520behaviors.%2520Extensive%2520experiments%2520across%2520eight%2520benchmarks%2520demonstrate%2520that%2520ViLaVT%2520achieves%2520strong%2520and%2520consistent%2520improvements%252C%2520with%2520particularly%2520pronounced%2520gains%2520on%2520complex%2520multi-image%2520and%2520video-based%2520spatial%2520reasoning%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chatting%20with%20Images%20for%20Introspective%20Visual%20Thinking&entry.906535625=Junfei%20Wu%20and%20Jian%20Guan%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%20and%20Wei%20Wu%20and%20Tieniu%20Tan&entry.1292438233=Current%20large%20vision-language%20models%20%28LVLMs%29%20typically%20rely%20on%20text-only%20reasoning%20based%20on%20a%20single-pass%20visual%20encoding%2C%20which%20often%20leads%20to%20loss%20of%20fine-grained%20visual%20information.%20Recently%20the%20proposal%20of%20%27%27thinking%20with%20images%27%27%20attempts%20to%20alleviate%20this%20limitation%20by%20manipulating%20images%20via%20external%20tools%20or%20code%3B%20however%2C%20the%20resulting%20visual%20states%20are%20often%20insufficiently%20grounded%20in%20linguistic%20semantics%2C%20impairing%20effective%20cross-modal%20alignment%20-%20particularly%20when%20visual%20semantics%20or%20geometric%20relationships%20must%20be%20reasoned%20over%20across%20distant%20regions%20or%20multiple%20images.%20To%20address%20these%20challenges%2C%20we%20propose%20%27%27chatting%20with%20images%27%27%2C%20a%20new%20framework%20that%20reframes%20visual%20manipulation%20as%20language-guided%20feature%20modulation.%20Under%20the%20guidance%20of%20expressive%20language%20prompts%2C%20the%20model%20dynamically%20performs%20joint%20re-encoding%20over%20multiple%20image%20regions%2C%20enabling%20tighter%20coupling%20between%20linguistic%20reasoning%20and%20visual%20state%20updates.%20We%20instantiate%20this%20paradigm%20in%20ViLaVT%2C%20a%20novel%20LVLM%20equipped%20with%20a%20dynamic%20vision%20encoder%20explicitly%20designed%20for%20such%20interactive%20visual%20reasoning%2C%20and%20trained%20it%20with%20a%20two-stage%20curriculum%20combining%20supervised%20fine-tuning%20and%20reinforcement%20learning%20to%20promote%20effective%20reasoning%20behaviors.%20Extensive%20experiments%20across%20eight%20benchmarks%20demonstrate%20that%20ViLaVT%20achieves%20strong%20and%20consistent%20improvements%2C%20with%20particularly%20pronounced%20gains%20on%20complex%20multi-image%20and%20video-based%20spatial%20reasoning%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.11073v2&entry.124074799=Read"},
{"title": "AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer", "author": "Lingting Zhu and Shengju Qian and Haidi Fan and Jiayu Dong and Zhenchao Jin and Siwei Zhou and Gen Dong and Xin Wang and Lequan Yu", "abstract": "The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling. Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets, contributing to the broader field of 3D content generation. The code is available at https://github.com/Advocate99/AssetFormer.", "link": "http://arxiv.org/abs/2602.12100v1", "date": "2026-02-12", "relevancy": 2.9275, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5801}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AssetFormer%3A%20Modular%203D%20Assets%20Generation%20with%20Autoregressive%20Transformer&body=Title%3A%20AssetFormer%3A%20Modular%203D%20Assets%20Generation%20with%20Autoregressive%20Transformer%0AAuthor%3A%20Lingting%20Zhu%20and%20Shengju%20Qian%20and%20Haidi%20Fan%20and%20Jiayu%20Dong%20and%20Zhenchao%20Jin%20and%20Siwei%20Zhou%20and%20Gen%20Dong%20and%20Xin%20Wang%20and%20Lequan%20Yu%0AAbstract%3A%20The%20digital%20industry%20demands%20high-quality%2C%20diverse%20modular%203D%20assets%2C%20especially%20for%20user-generated%20content~%28UGC%29.%20In%20this%20work%2C%20we%20introduce%20AssetFormer%2C%20an%20autoregressive%20Transformer-based%20model%20designed%20to%20generate%20modular%203D%20assets%20from%20textual%20descriptions.%20Our%20pilot%20study%20leverages%20real-world%20modular%20assets%20collected%20from%20online%20platforms.%20AssetFormer%20tackles%20the%20challenge%20of%20creating%20assets%20composed%20of%20primitives%20that%20adhere%20to%20constrained%20design%20parameters%20for%20various%20applications.%20By%20innovatively%20adapting%20module%20sequencing%20and%20decoding%20techniques%20inspired%20by%20language%20models%2C%20our%20approach%20enhances%20asset%20generation%20quality%20through%20autoregressive%20modeling.%20Initial%20results%20indicate%20the%20effectiveness%20of%20AssetFormer%20in%20streamlining%20asset%20creation%20for%20professional%20development%20and%20UGC%20scenarios.%20This%20work%20presents%20a%20flexible%20framework%20extendable%20to%20various%20types%20of%20modular%203D%20assets%2C%20contributing%20to%20the%20broader%20field%20of%203D%20content%20generation.%20The%20code%20is%20available%20at%20https%3A//github.com/Advocate99/AssetFormer.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssetFormer%253A%2520Modular%25203D%2520Assets%2520Generation%2520with%2520Autoregressive%2520Transformer%26entry.906535625%3DLingting%2520Zhu%2520and%2520Shengju%2520Qian%2520and%2520Haidi%2520Fan%2520and%2520Jiayu%2520Dong%2520and%2520Zhenchao%2520Jin%2520and%2520Siwei%2520Zhou%2520and%2520Gen%2520Dong%2520and%2520Xin%2520Wang%2520and%2520Lequan%2520Yu%26entry.1292438233%3DThe%2520digital%2520industry%2520demands%2520high-quality%252C%2520diverse%2520modular%25203D%2520assets%252C%2520especially%2520for%2520user-generated%2520content~%2528UGC%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AssetFormer%252C%2520an%2520autoregressive%2520Transformer-based%2520model%2520designed%2520to%2520generate%2520modular%25203D%2520assets%2520from%2520textual%2520descriptions.%2520Our%2520pilot%2520study%2520leverages%2520real-world%2520modular%2520assets%2520collected%2520from%2520online%2520platforms.%2520AssetFormer%2520tackles%2520the%2520challenge%2520of%2520creating%2520assets%2520composed%2520of%2520primitives%2520that%2520adhere%2520to%2520constrained%2520design%2520parameters%2520for%2520various%2520applications.%2520By%2520innovatively%2520adapting%2520module%2520sequencing%2520and%2520decoding%2520techniques%2520inspired%2520by%2520language%2520models%252C%2520our%2520approach%2520enhances%2520asset%2520generation%2520quality%2520through%2520autoregressive%2520modeling.%2520Initial%2520results%2520indicate%2520the%2520effectiveness%2520of%2520AssetFormer%2520in%2520streamlining%2520asset%2520creation%2520for%2520professional%2520development%2520and%2520UGC%2520scenarios.%2520This%2520work%2520presents%2520a%2520flexible%2520framework%2520extendable%2520to%2520various%2520types%2520of%2520modular%25203D%2520assets%252C%2520contributing%2520to%2520the%2520broader%2520field%2520of%25203D%2520content%2520generation.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Advocate99/AssetFormer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AssetFormer%3A%20Modular%203D%20Assets%20Generation%20with%20Autoregressive%20Transformer&entry.906535625=Lingting%20Zhu%20and%20Shengju%20Qian%20and%20Haidi%20Fan%20and%20Jiayu%20Dong%20and%20Zhenchao%20Jin%20and%20Siwei%20Zhou%20and%20Gen%20Dong%20and%20Xin%20Wang%20and%20Lequan%20Yu&entry.1292438233=The%20digital%20industry%20demands%20high-quality%2C%20diverse%20modular%203D%20assets%2C%20especially%20for%20user-generated%20content~%28UGC%29.%20In%20this%20work%2C%20we%20introduce%20AssetFormer%2C%20an%20autoregressive%20Transformer-based%20model%20designed%20to%20generate%20modular%203D%20assets%20from%20textual%20descriptions.%20Our%20pilot%20study%20leverages%20real-world%20modular%20assets%20collected%20from%20online%20platforms.%20AssetFormer%20tackles%20the%20challenge%20of%20creating%20assets%20composed%20of%20primitives%20that%20adhere%20to%20constrained%20design%20parameters%20for%20various%20applications.%20By%20innovatively%20adapting%20module%20sequencing%20and%20decoding%20techniques%20inspired%20by%20language%20models%2C%20our%20approach%20enhances%20asset%20generation%20quality%20through%20autoregressive%20modeling.%20Initial%20results%20indicate%20the%20effectiveness%20of%20AssetFormer%20in%20streamlining%20asset%20creation%20for%20professional%20development%20and%20UGC%20scenarios.%20This%20work%20presents%20a%20flexible%20framework%20extendable%20to%20various%20types%20of%20modular%203D%20assets%2C%20contributing%20to%20the%20broader%20field%20of%203D%20content%20generation.%20The%20code%20is%20available%20at%20https%3A//github.com/Advocate99/AssetFormer.&entry.1838667208=http%3A//arxiv.org/abs/2602.12100v1&entry.124074799=Read"},
{"title": "Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation", "author": "Enrico Guerriero and Kjersti Engan and \u00d8yvind Meinich-Bache", "abstract": "Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.", "link": "http://arxiv.org/abs/2602.12002v1", "date": "2026-02-12", "relevancy": 2.8909, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Local%20Vision-Language%20Models%20improve%20Activity%20Recognition%20over%20Vision%20Transformers%3F%20--%20Case%20Study%20on%20Newborn%20Resuscitation&body=Title%3A%20Can%20Local%20Vision-Language%20Models%20improve%20Activity%20Recognition%20over%20Vision%20Transformers%3F%20--%20Case%20Study%20on%20Newborn%20Resuscitation%0AAuthor%3A%20Enrico%20Guerriero%20and%20Kjersti%20Engan%20and%20%C3%98yvind%20Meinich-Bache%0AAbstract%3A%20Accurate%20documentation%20of%20newborn%20resuscitation%20is%20essential%20for%20quality%20improvement%20and%20adherence%20to%20clinical%20guidelines%2C%20yet%20remains%20underutilized%20in%20practice.%20Previous%20work%20using%203D-CNNs%20and%20Vision%20Transformers%20%28ViT%29%20has%20shown%20promising%20results%20in%20detecting%20key%20activities%20from%20newborn%20resuscitation%20videos%2C%20but%20also%20highlighted%20the%20challenges%20in%20recognizing%20such%20fine-grained%20activities.%20This%20work%20investigates%20the%20potential%20of%20generative%20AI%20%28GenAI%29%20methods%20to%20improve%20activity%20recognition%20from%20such%20videos.%20Specifically%2C%20we%20explore%20the%20use%20of%20local%20vision-language%20models%20%28VLMs%29%2C%20combined%20with%20large%20language%20models%20%28LLMs%29%2C%20and%20compare%20them%20to%20a%20supervised%20TimeSFormer%20baseline.%20Using%20a%20simulated%20dataset%20comprising%2013.26%20hours%20of%20newborn%20resuscitation%20videos%2C%20we%20evaluate%20several%20zero-shot%20VLM-based%20strategies%20and%20fine-tuned%20VLMs%20with%20classification%20heads%2C%20including%20Low-Rank%20Adaptation%20%28LoRA%29.%20Our%20results%20suggest%20that%20small%20%28local%29%20VLMs%20struggle%20with%20hallucinations%2C%20but%20when%20fine-tuned%20with%20LoRA%2C%20the%20results%20reach%20F1%20score%20at%200.91%2C%20surpassing%20the%20TimeSformer%20results%20of%200.70.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Local%2520Vision-Language%2520Models%2520improve%2520Activity%2520Recognition%2520over%2520Vision%2520Transformers%253F%2520--%2520Case%2520Study%2520on%2520Newborn%2520Resuscitation%26entry.906535625%3DEnrico%2520Guerriero%2520and%2520Kjersti%2520Engan%2520and%2520%25C3%2598yvind%2520Meinich-Bache%26entry.1292438233%3DAccurate%2520documentation%2520of%2520newborn%2520resuscitation%2520is%2520essential%2520for%2520quality%2520improvement%2520and%2520adherence%2520to%2520clinical%2520guidelines%252C%2520yet%2520remains%2520underutilized%2520in%2520practice.%2520Previous%2520work%2520using%25203D-CNNs%2520and%2520Vision%2520Transformers%2520%2528ViT%2529%2520has%2520shown%2520promising%2520results%2520in%2520detecting%2520key%2520activities%2520from%2520newborn%2520resuscitation%2520videos%252C%2520but%2520also%2520highlighted%2520the%2520challenges%2520in%2520recognizing%2520such%2520fine-grained%2520activities.%2520This%2520work%2520investigates%2520the%2520potential%2520of%2520generative%2520AI%2520%2528GenAI%2529%2520methods%2520to%2520improve%2520activity%2520recognition%2520from%2520such%2520videos.%2520Specifically%252C%2520we%2520explore%2520the%2520use%2520of%2520local%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520combined%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520and%2520compare%2520them%2520to%2520a%2520supervised%2520TimeSFormer%2520baseline.%2520Using%2520a%2520simulated%2520dataset%2520comprising%252013.26%2520hours%2520of%2520newborn%2520resuscitation%2520videos%252C%2520we%2520evaluate%2520several%2520zero-shot%2520VLM-based%2520strategies%2520and%2520fine-tuned%2520VLMs%2520with%2520classification%2520heads%252C%2520including%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520Our%2520results%2520suggest%2520that%2520small%2520%2528local%2529%2520VLMs%2520struggle%2520with%2520hallucinations%252C%2520but%2520when%2520fine-tuned%2520with%2520LoRA%252C%2520the%2520results%2520reach%2520F1%2520score%2520at%25200.91%252C%2520surpassing%2520the%2520TimeSformer%2520results%2520of%25200.70.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Local%20Vision-Language%20Models%20improve%20Activity%20Recognition%20over%20Vision%20Transformers%3F%20--%20Case%20Study%20on%20Newborn%20Resuscitation&entry.906535625=Enrico%20Guerriero%20and%20Kjersti%20Engan%20and%20%C3%98yvind%20Meinich-Bache&entry.1292438233=Accurate%20documentation%20of%20newborn%20resuscitation%20is%20essential%20for%20quality%20improvement%20and%20adherence%20to%20clinical%20guidelines%2C%20yet%20remains%20underutilized%20in%20practice.%20Previous%20work%20using%203D-CNNs%20and%20Vision%20Transformers%20%28ViT%29%20has%20shown%20promising%20results%20in%20detecting%20key%20activities%20from%20newborn%20resuscitation%20videos%2C%20but%20also%20highlighted%20the%20challenges%20in%20recognizing%20such%20fine-grained%20activities.%20This%20work%20investigates%20the%20potential%20of%20generative%20AI%20%28GenAI%29%20methods%20to%20improve%20activity%20recognition%20from%20such%20videos.%20Specifically%2C%20we%20explore%20the%20use%20of%20local%20vision-language%20models%20%28VLMs%29%2C%20combined%20with%20large%20language%20models%20%28LLMs%29%2C%20and%20compare%20them%20to%20a%20supervised%20TimeSFormer%20baseline.%20Using%20a%20simulated%20dataset%20comprising%2013.26%20hours%20of%20newborn%20resuscitation%20videos%2C%20we%20evaluate%20several%20zero-shot%20VLM-based%20strategies%20and%20fine-tuned%20VLMs%20with%20classification%20heads%2C%20including%20Low-Rank%20Adaptation%20%28LoRA%29.%20Our%20results%20suggest%20that%20small%20%28local%29%20VLMs%20struggle%20with%20hallucinations%2C%20but%20when%20fine-tuned%20with%20LoRA%2C%20the%20results%20reach%20F1%20score%20at%200.91%2C%20surpassing%20the%20TimeSformer%20results%20of%200.70.&entry.1838667208=http%3A//arxiv.org/abs/2602.12002v1&entry.124074799=Read"},
{"title": "TexSpot: 3D Texture Enhancement with Spatially-uniform Point Latent Representation", "author": "Ziteng Lu and Yushuang Wu and Chongjie Ye and Yuda Qiu and Jing Shao and Xiaoyang Guo and Jiaqing Zhou and Tianlei Hu and Kun Zhou and Xiaoguang Han", "abstract": "High-quality 3D texture generation remains a fundamental challenge due to the view-inconsistency inherent in current mainstream multi-view diffusion pipelines. Existing representations either rely on UV maps, which suffer from distortion during unwrapping, or point-based methods, which tightly couple texture fidelity to geometric density that limits high-resolution texture generation. To address these limitations, we introduce TexSpot, a diffusion-based texture enhancement framework. At its core is Texlet, a novel 3D texture representation that merges the geometric expressiveness of point-based 3D textures with the compactness of UV-based representation. Each Texlet latent vector encodes a local texture patch via a 2D encoder and is further aggregated using a 3D encoder to incorporate global shape context. A cascaded 3D-to-2D decoder reconstructs high-quality texture patches, enabling the Texlet space learning. Leveraging this representation, we train a diffusion transformer conditioned on Texlets to refine and enhance textures produced by multi-view diffusion methods. Extensive experiments demonstrate that TexSpot significantly improves visual fidelity, geometric consistency, and robustness over existing state-of-the-art 3D texture generation and enhancement approaches. Project page: https://anonymous.4open.science/w/TexSpot-page-2D91.", "link": "http://arxiv.org/abs/2602.12157v1", "date": "2026-02-12", "relevancy": 2.8891, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5878}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5728}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexSpot%3A%203D%20Texture%20Enhancement%20with%20Spatially-uniform%20Point%20Latent%20Representation&body=Title%3A%20TexSpot%3A%203D%20Texture%20Enhancement%20with%20Spatially-uniform%20Point%20Latent%20Representation%0AAuthor%3A%20Ziteng%20Lu%20and%20Yushuang%20Wu%20and%20Chongjie%20Ye%20and%20Yuda%20Qiu%20and%20Jing%20Shao%20and%20Xiaoyang%20Guo%20and%20Jiaqing%20Zhou%20and%20Tianlei%20Hu%20and%20Kun%20Zhou%20and%20Xiaoguang%20Han%0AAbstract%3A%20High-quality%203D%20texture%20generation%20remains%20a%20fundamental%20challenge%20due%20to%20the%20view-inconsistency%20inherent%20in%20current%20mainstream%20multi-view%20diffusion%20pipelines.%20Existing%20representations%20either%20rely%20on%20UV%20maps%2C%20which%20suffer%20from%20distortion%20during%20unwrapping%2C%20or%20point-based%20methods%2C%20which%20tightly%20couple%20texture%20fidelity%20to%20geometric%20density%20that%20limits%20high-resolution%20texture%20generation.%20To%20address%20these%20limitations%2C%20we%20introduce%20TexSpot%2C%20a%20diffusion-based%20texture%20enhancement%20framework.%20At%20its%20core%20is%20Texlet%2C%20a%20novel%203D%20texture%20representation%20that%20merges%20the%20geometric%20expressiveness%20of%20point-based%203D%20textures%20with%20the%20compactness%20of%20UV-based%20representation.%20Each%20Texlet%20latent%20vector%20encodes%20a%20local%20texture%20patch%20via%20a%202D%20encoder%20and%20is%20further%20aggregated%20using%20a%203D%20encoder%20to%20incorporate%20global%20shape%20context.%20A%20cascaded%203D-to-2D%20decoder%20reconstructs%20high-quality%20texture%20patches%2C%20enabling%20the%20Texlet%20space%20learning.%20Leveraging%20this%20representation%2C%20we%20train%20a%20diffusion%20transformer%20conditioned%20on%20Texlets%20to%20refine%20and%20enhance%20textures%20produced%20by%20multi-view%20diffusion%20methods.%20Extensive%20experiments%20demonstrate%20that%20TexSpot%20significantly%20improves%20visual%20fidelity%2C%20geometric%20consistency%2C%20and%20robustness%20over%20existing%20state-of-the-art%203D%20texture%20generation%20and%20enhancement%20approaches.%20Project%20page%3A%20https%3A//anonymous.4open.science/w/TexSpot-page-2D91.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexSpot%253A%25203D%2520Texture%2520Enhancement%2520with%2520Spatially-uniform%2520Point%2520Latent%2520Representation%26entry.906535625%3DZiteng%2520Lu%2520and%2520Yushuang%2520Wu%2520and%2520Chongjie%2520Ye%2520and%2520Yuda%2520Qiu%2520and%2520Jing%2520Shao%2520and%2520Xiaoyang%2520Guo%2520and%2520Jiaqing%2520Zhou%2520and%2520Tianlei%2520Hu%2520and%2520Kun%2520Zhou%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3DHigh-quality%25203D%2520texture%2520generation%2520remains%2520a%2520fundamental%2520challenge%2520due%2520to%2520the%2520view-inconsistency%2520inherent%2520in%2520current%2520mainstream%2520multi-view%2520diffusion%2520pipelines.%2520Existing%2520representations%2520either%2520rely%2520on%2520UV%2520maps%252C%2520which%2520suffer%2520from%2520distortion%2520during%2520unwrapping%252C%2520or%2520point-based%2520methods%252C%2520which%2520tightly%2520couple%2520texture%2520fidelity%2520to%2520geometric%2520density%2520that%2520limits%2520high-resolution%2520texture%2520generation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520TexSpot%252C%2520a%2520diffusion-based%2520texture%2520enhancement%2520framework.%2520At%2520its%2520core%2520is%2520Texlet%252C%2520a%2520novel%25203D%2520texture%2520representation%2520that%2520merges%2520the%2520geometric%2520expressiveness%2520of%2520point-based%25203D%2520textures%2520with%2520the%2520compactness%2520of%2520UV-based%2520representation.%2520Each%2520Texlet%2520latent%2520vector%2520encodes%2520a%2520local%2520texture%2520patch%2520via%2520a%25202D%2520encoder%2520and%2520is%2520further%2520aggregated%2520using%2520a%25203D%2520encoder%2520to%2520incorporate%2520global%2520shape%2520context.%2520A%2520cascaded%25203D-to-2D%2520decoder%2520reconstructs%2520high-quality%2520texture%2520patches%252C%2520enabling%2520the%2520Texlet%2520space%2520learning.%2520Leveraging%2520this%2520representation%252C%2520we%2520train%2520a%2520diffusion%2520transformer%2520conditioned%2520on%2520Texlets%2520to%2520refine%2520and%2520enhance%2520textures%2520produced%2520by%2520multi-view%2520diffusion%2520methods.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TexSpot%2520significantly%2520improves%2520visual%2520fidelity%252C%2520geometric%2520consistency%252C%2520and%2520robustness%2520over%2520existing%2520state-of-the-art%25203D%2520texture%2520generation%2520and%2520enhancement%2520approaches.%2520Project%2520page%253A%2520https%253A//anonymous.4open.science/w/TexSpot-page-2D91.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexSpot%3A%203D%20Texture%20Enhancement%20with%20Spatially-uniform%20Point%20Latent%20Representation&entry.906535625=Ziteng%20Lu%20and%20Yushuang%20Wu%20and%20Chongjie%20Ye%20and%20Yuda%20Qiu%20and%20Jing%20Shao%20and%20Xiaoyang%20Guo%20and%20Jiaqing%20Zhou%20and%20Tianlei%20Hu%20and%20Kun%20Zhou%20and%20Xiaoguang%20Han&entry.1292438233=High-quality%203D%20texture%20generation%20remains%20a%20fundamental%20challenge%20due%20to%20the%20view-inconsistency%20inherent%20in%20current%20mainstream%20multi-view%20diffusion%20pipelines.%20Existing%20representations%20either%20rely%20on%20UV%20maps%2C%20which%20suffer%20from%20distortion%20during%20unwrapping%2C%20or%20point-based%20methods%2C%20which%20tightly%20couple%20texture%20fidelity%20to%20geometric%20density%20that%20limits%20high-resolution%20texture%20generation.%20To%20address%20these%20limitations%2C%20we%20introduce%20TexSpot%2C%20a%20diffusion-based%20texture%20enhancement%20framework.%20At%20its%20core%20is%20Texlet%2C%20a%20novel%203D%20texture%20representation%20that%20merges%20the%20geometric%20expressiveness%20of%20point-based%203D%20textures%20with%20the%20compactness%20of%20UV-based%20representation.%20Each%20Texlet%20latent%20vector%20encodes%20a%20local%20texture%20patch%20via%20a%202D%20encoder%20and%20is%20further%20aggregated%20using%20a%203D%20encoder%20to%20incorporate%20global%20shape%20context.%20A%20cascaded%203D-to-2D%20decoder%20reconstructs%20high-quality%20texture%20patches%2C%20enabling%20the%20Texlet%20space%20learning.%20Leveraging%20this%20representation%2C%20we%20train%20a%20diffusion%20transformer%20conditioned%20on%20Texlets%20to%20refine%20and%20enhance%20textures%20produced%20by%20multi-view%20diffusion%20methods.%20Extensive%20experiments%20demonstrate%20that%20TexSpot%20significantly%20improves%20visual%20fidelity%2C%20geometric%20consistency%2C%20and%20robustness%20over%20existing%20state-of-the-art%203D%20texture%20generation%20and%20enhancement%20approaches.%20Project%20page%3A%20https%3A//anonymous.4open.science/w/TexSpot-page-2D91.&entry.1838667208=http%3A//arxiv.org/abs/2602.12157v1&entry.124074799=Read"},
{"title": "Kelix Technical Report", "author": "Boyang Ding and Chenglong Chu and Dunju Zang and Han Li and Jiangxia Cao and Kun Gai and Muhao Wei and Ruiming Tang and Shiyao Wang and Siyang Mao and Xinchen Luo and Yahui Liu and Zhixin Ling and Zhuoran Yang and Ziming Li and Chengru Song and Guorui Zhou and Guowang Zhang and Hao Peng and Hao Wang and Jiaxin Deng and Jin Ouyang and Jinghao Zhang and Lejian Ren and Qianqian Wang and Qigen Hu and Tao Wang and Xingmei Wang and Yiping Yang and Zixing Zhang and Ziqi Wang", "abstract": "Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.", "link": "http://arxiv.org/abs/2602.09843v3", "date": "2026-02-12", "relevancy": 2.8793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kelix%20Technical%20Report&body=Title%3A%20Kelix%20Technical%20Report%0AAuthor%3A%20Boyang%20Ding%20and%20Chenglong%20Chu%20and%20Dunju%20Zang%20and%20Han%20Li%20and%20Jiangxia%20Cao%20and%20Kun%20Gai%20and%20Muhao%20Wei%20and%20Ruiming%20Tang%20and%20Shiyao%20Wang%20and%20Siyang%20Mao%20and%20Xinchen%20Luo%20and%20Yahui%20Liu%20and%20Zhixin%20Ling%20and%20Zhuoran%20Yang%20and%20Ziming%20Li%20and%20Chengru%20Song%20and%20Guorui%20Zhou%20and%20Guowang%20Zhang%20and%20Hao%20Peng%20and%20Hao%20Wang%20and%20Jiaxin%20Deng%20and%20Jin%20Ouyang%20and%20Jinghao%20Zhang%20and%20Lejian%20Ren%20and%20Qianqian%20Wang%20and%20Qigen%20Hu%20and%20Tao%20Wang%20and%20Xingmei%20Wang%20and%20Yiping%20Yang%20and%20Zixing%20Zhang%20and%20Ziqi%20Wang%0AAbstract%3A%20Autoregressive%20large%20language%20models%20%28LLMs%29%20scale%20well%20by%20expressing%20diverse%20tasks%20as%20sequences%20of%20discrete%20natural-language%20tokens%20and%20training%20with%20next-token%20prediction%2C%20which%20unifies%20comprehension%20and%20generation%20under%20self-supervision.%20Extending%20this%20paradigm%20to%20multimodal%20data%20requires%20a%20shared%2C%20discrete%20representation%20across%20modalities.%20However%2C%20most%20vision-language%20models%20%28VLMs%29%20still%20rely%20on%20a%20hybrid%20interface%3A%20discrete%20text%20tokens%20paired%20with%20continuous%20Vision%20Transformer%20%28ViT%29%20features.%20Because%20supervision%20is%20largely%20text-driven%2C%20these%20models%20are%20often%20biased%20toward%20understanding%20and%20cannot%20fully%20leverage%20large-scale%20self-supervised%20learning%20on%20non-text%20data.%20Recent%20work%20has%20explored%20discrete%20visual%20tokenization%20to%20enable%20fully%20autoregressive%20multimodal%20modeling%2C%20showing%20promising%20progress%20toward%20unified%20understanding%20and%20generation.%20Yet%20existing%20discrete%20vision%20tokens%20frequently%20lose%20information%20due%20to%20limited%20code%20capacity%2C%20resulting%20in%20noticeably%20weaker%20understanding%20than%20continuous-feature%20VLMs.%20We%20present%20Kelix%2C%20a%20fully%20discrete%20autoregressive%20unified%20model%20that%20closes%20the%20understanding%20gap%20between%20discrete%20and%20continuous%20visual%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09843v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKelix%2520Technical%2520Report%26entry.906535625%3DBoyang%2520Ding%2520and%2520Chenglong%2520Chu%2520and%2520Dunju%2520Zang%2520and%2520Han%2520Li%2520and%2520Jiangxia%2520Cao%2520and%2520Kun%2520Gai%2520and%2520Muhao%2520Wei%2520and%2520Ruiming%2520Tang%2520and%2520Shiyao%2520Wang%2520and%2520Siyang%2520Mao%2520and%2520Xinchen%2520Luo%2520and%2520Yahui%2520Liu%2520and%2520Zhixin%2520Ling%2520and%2520Zhuoran%2520Yang%2520and%2520Ziming%2520Li%2520and%2520Chengru%2520Song%2520and%2520Guorui%2520Zhou%2520and%2520Guowang%2520Zhang%2520and%2520Hao%2520Peng%2520and%2520Hao%2520Wang%2520and%2520Jiaxin%2520Deng%2520and%2520Jin%2520Ouyang%2520and%2520Jinghao%2520Zhang%2520and%2520Lejian%2520Ren%2520and%2520Qianqian%2520Wang%2520and%2520Qigen%2520Hu%2520and%2520Tao%2520Wang%2520and%2520Xingmei%2520Wang%2520and%2520Yiping%2520Yang%2520and%2520Zixing%2520Zhang%2520and%2520Ziqi%2520Wang%26entry.1292438233%3DAutoregressive%2520large%2520language%2520models%2520%2528LLMs%2529%2520scale%2520well%2520by%2520expressing%2520diverse%2520tasks%2520as%2520sequences%2520of%2520discrete%2520natural-language%2520tokens%2520and%2520training%2520with%2520next-token%2520prediction%252C%2520which%2520unifies%2520comprehension%2520and%2520generation%2520under%2520self-supervision.%2520Extending%2520this%2520paradigm%2520to%2520multimodal%2520data%2520requires%2520a%2520shared%252C%2520discrete%2520representation%2520across%2520modalities.%2520However%252C%2520most%2520vision-language%2520models%2520%2528VLMs%2529%2520still%2520rely%2520on%2520a%2520hybrid%2520interface%253A%2520discrete%2520text%2520tokens%2520paired%2520with%2520continuous%2520Vision%2520Transformer%2520%2528ViT%2529%2520features.%2520Because%2520supervision%2520is%2520largely%2520text-driven%252C%2520these%2520models%2520are%2520often%2520biased%2520toward%2520understanding%2520and%2520cannot%2520fully%2520leverage%2520large-scale%2520self-supervised%2520learning%2520on%2520non-text%2520data.%2520Recent%2520work%2520has%2520explored%2520discrete%2520visual%2520tokenization%2520to%2520enable%2520fully%2520autoregressive%2520multimodal%2520modeling%252C%2520showing%2520promising%2520progress%2520toward%2520unified%2520understanding%2520and%2520generation.%2520Yet%2520existing%2520discrete%2520vision%2520tokens%2520frequently%2520lose%2520information%2520due%2520to%2520limited%2520code%2520capacity%252C%2520resulting%2520in%2520noticeably%2520weaker%2520understanding%2520than%2520continuous-feature%2520VLMs.%2520We%2520present%2520Kelix%252C%2520a%2520fully%2520discrete%2520autoregressive%2520unified%2520model%2520that%2520closes%2520the%2520understanding%2520gap%2520between%2520discrete%2520and%2520continuous%2520visual%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09843v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kelix%20Technical%20Report&entry.906535625=Boyang%20Ding%20and%20Chenglong%20Chu%20and%20Dunju%20Zang%20and%20Han%20Li%20and%20Jiangxia%20Cao%20and%20Kun%20Gai%20and%20Muhao%20Wei%20and%20Ruiming%20Tang%20and%20Shiyao%20Wang%20and%20Siyang%20Mao%20and%20Xinchen%20Luo%20and%20Yahui%20Liu%20and%20Zhixin%20Ling%20and%20Zhuoran%20Yang%20and%20Ziming%20Li%20and%20Chengru%20Song%20and%20Guorui%20Zhou%20and%20Guowang%20Zhang%20and%20Hao%20Peng%20and%20Hao%20Wang%20and%20Jiaxin%20Deng%20and%20Jin%20Ouyang%20and%20Jinghao%20Zhang%20and%20Lejian%20Ren%20and%20Qianqian%20Wang%20and%20Qigen%20Hu%20and%20Tao%20Wang%20and%20Xingmei%20Wang%20and%20Yiping%20Yang%20and%20Zixing%20Zhang%20and%20Ziqi%20Wang&entry.1292438233=Autoregressive%20large%20language%20models%20%28LLMs%29%20scale%20well%20by%20expressing%20diverse%20tasks%20as%20sequences%20of%20discrete%20natural-language%20tokens%20and%20training%20with%20next-token%20prediction%2C%20which%20unifies%20comprehension%20and%20generation%20under%20self-supervision.%20Extending%20this%20paradigm%20to%20multimodal%20data%20requires%20a%20shared%2C%20discrete%20representation%20across%20modalities.%20However%2C%20most%20vision-language%20models%20%28VLMs%29%20still%20rely%20on%20a%20hybrid%20interface%3A%20discrete%20text%20tokens%20paired%20with%20continuous%20Vision%20Transformer%20%28ViT%29%20features.%20Because%20supervision%20is%20largely%20text-driven%2C%20these%20models%20are%20often%20biased%20toward%20understanding%20and%20cannot%20fully%20leverage%20large-scale%20self-supervised%20learning%20on%20non-text%20data.%20Recent%20work%20has%20explored%20discrete%20visual%20tokenization%20to%20enable%20fully%20autoregressive%20multimodal%20modeling%2C%20showing%20promising%20progress%20toward%20unified%20understanding%20and%20generation.%20Yet%20existing%20discrete%20vision%20tokens%20frequently%20lose%20information%20due%20to%20limited%20code%20capacity%2C%20resulting%20in%20noticeably%20weaker%20understanding%20than%20continuous-feature%20VLMs.%20We%20present%20Kelix%2C%20a%20fully%20discrete%20autoregressive%20unified%20model%20that%20closes%20the%20understanding%20gap%20between%20discrete%20and%20continuous%20visual%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2602.09843v3&entry.124074799=Read"},
{"title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching", "author": "Minwoo Jung and Nived Chebrolu and Lucas Carvalho de Lima and Haedam Oh and Maurice Fallon and Ayoung Kim", "abstract": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.", "link": "http://arxiv.org/abs/2602.01501v3", "date": "2026-02-12", "relevancy": 2.8523, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6361}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5629}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TreeLoc%3A%206-DoF%20LiDAR%20Global%20Localization%20in%20Forests%20via%20Inter-Tree%20Geometric%20Matching&body=Title%3A%20TreeLoc%3A%206-DoF%20LiDAR%20Global%20Localization%20in%20Forests%20via%20Inter-Tree%20Geometric%20Matching%0AAuthor%3A%20Minwoo%20Jung%20and%20Nived%20Chebrolu%20and%20Lucas%20Carvalho%20de%20Lima%20and%20Haedam%20Oh%20and%20Maurice%20Fallon%20and%20Ayoung%20Kim%0AAbstract%3A%20Reliable%20localization%20is%20crucial%20for%20navigation%20in%20forests%2C%20where%20GPS%20is%20often%20degraded%20and%20LiDAR%20measurements%20are%20repetitive%2C%20occluded%2C%20and%20structurally%20complex.%20These%20conditions%20weaken%20the%20assumptions%20of%20traditional%20urban-centric%20localization%20methods%2C%20which%20assume%20that%20consistent%20features%20arise%20from%20unique%20structural%20patterns%2C%20necessitating%20forest-centric%20solutions%20to%20achieve%20robustness%20in%20these%20environments.%20To%20address%20these%20challenges%2C%20we%20propose%20TreeLoc%2C%20a%20LiDAR-based%20global%20localization%20framework%20for%20forests%20that%20handles%20place%20recognition%20and%206-DoF%20pose%20estimation.%20We%20represent%20scenes%20using%20tree%20stems%20and%20their%20Diameter%20at%20Breast%20Height%20%28DBH%29%2C%20which%20are%20aligned%20to%20a%20common%20reference%20frame%20via%20their%20axes%20and%20summarized%20using%20the%20tree%20distribution%20histogram%20%28TDH%29%20for%20coarse%20matching%2C%20followed%20by%20fine%20matching%20with%20a%202D%20triangle%20descriptor.%20Finally%2C%20pose%20estimation%20is%20achieved%20through%20a%20two-step%20geometric%20verification.%20On%20diverse%20forest%20benchmarks%2C%20TreeLoc%20outperforms%20baselines%2C%20achieving%20precise%20localization.%20Ablation%20studies%20validate%20the%20contribution%20of%20each%20component.%20We%20also%20propose%20applications%20for%20long-term%20forest%20management%20using%20descriptors%20from%20a%20compact%20global%20tree%20database.%20TreeLoc%20is%20open-sourced%20for%20the%20robotics%20community%20at%20https%3A//github.com/minwoo0611/TreeLoc.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01501v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreeLoc%253A%25206-DoF%2520LiDAR%2520Global%2520Localization%2520in%2520Forests%2520via%2520Inter-Tree%2520Geometric%2520Matching%26entry.906535625%3DMinwoo%2520Jung%2520and%2520Nived%2520Chebrolu%2520and%2520Lucas%2520Carvalho%2520de%2520Lima%2520and%2520Haedam%2520Oh%2520and%2520Maurice%2520Fallon%2520and%2520Ayoung%2520Kim%26entry.1292438233%3DReliable%2520localization%2520is%2520crucial%2520for%2520navigation%2520in%2520forests%252C%2520where%2520GPS%2520is%2520often%2520degraded%2520and%2520LiDAR%2520measurements%2520are%2520repetitive%252C%2520occluded%252C%2520and%2520structurally%2520complex.%2520These%2520conditions%2520weaken%2520the%2520assumptions%2520of%2520traditional%2520urban-centric%2520localization%2520methods%252C%2520which%2520assume%2520that%2520consistent%2520features%2520arise%2520from%2520unique%2520structural%2520patterns%252C%2520necessitating%2520forest-centric%2520solutions%2520to%2520achieve%2520robustness%2520in%2520these%2520environments.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520TreeLoc%252C%2520a%2520LiDAR-based%2520global%2520localization%2520framework%2520for%2520forests%2520that%2520handles%2520place%2520recognition%2520and%25206-DoF%2520pose%2520estimation.%2520We%2520represent%2520scenes%2520using%2520tree%2520stems%2520and%2520their%2520Diameter%2520at%2520Breast%2520Height%2520%2528DBH%2529%252C%2520which%2520are%2520aligned%2520to%2520a%2520common%2520reference%2520frame%2520via%2520their%2520axes%2520and%2520summarized%2520using%2520the%2520tree%2520distribution%2520histogram%2520%2528TDH%2529%2520for%2520coarse%2520matching%252C%2520followed%2520by%2520fine%2520matching%2520with%2520a%25202D%2520triangle%2520descriptor.%2520Finally%252C%2520pose%2520estimation%2520is%2520achieved%2520through%2520a%2520two-step%2520geometric%2520verification.%2520On%2520diverse%2520forest%2520benchmarks%252C%2520TreeLoc%2520outperforms%2520baselines%252C%2520achieving%2520precise%2520localization.%2520Ablation%2520studies%2520validate%2520the%2520contribution%2520of%2520each%2520component.%2520We%2520also%2520propose%2520applications%2520for%2520long-term%2520forest%2520management%2520using%2520descriptors%2520from%2520a%2520compact%2520global%2520tree%2520database.%2520TreeLoc%2520is%2520open-sourced%2520for%2520the%2520robotics%2520community%2520at%2520https%253A//github.com/minwoo0611/TreeLoc.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01501v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TreeLoc%3A%206-DoF%20LiDAR%20Global%20Localization%20in%20Forests%20via%20Inter-Tree%20Geometric%20Matching&entry.906535625=Minwoo%20Jung%20and%20Nived%20Chebrolu%20and%20Lucas%20Carvalho%20de%20Lima%20and%20Haedam%20Oh%20and%20Maurice%20Fallon%20and%20Ayoung%20Kim&entry.1292438233=Reliable%20localization%20is%20crucial%20for%20navigation%20in%20forests%2C%20where%20GPS%20is%20often%20degraded%20and%20LiDAR%20measurements%20are%20repetitive%2C%20occluded%2C%20and%20structurally%20complex.%20These%20conditions%20weaken%20the%20assumptions%20of%20traditional%20urban-centric%20localization%20methods%2C%20which%20assume%20that%20consistent%20features%20arise%20from%20unique%20structural%20patterns%2C%20necessitating%20forest-centric%20solutions%20to%20achieve%20robustness%20in%20these%20environments.%20To%20address%20these%20challenges%2C%20we%20propose%20TreeLoc%2C%20a%20LiDAR-based%20global%20localization%20framework%20for%20forests%20that%20handles%20place%20recognition%20and%206-DoF%20pose%20estimation.%20We%20represent%20scenes%20using%20tree%20stems%20and%20their%20Diameter%20at%20Breast%20Height%20%28DBH%29%2C%20which%20are%20aligned%20to%20a%20common%20reference%20frame%20via%20their%20axes%20and%20summarized%20using%20the%20tree%20distribution%20histogram%20%28TDH%29%20for%20coarse%20matching%2C%20followed%20by%20fine%20matching%20with%20a%202D%20triangle%20descriptor.%20Finally%2C%20pose%20estimation%20is%20achieved%20through%20a%20two-step%20geometric%20verification.%20On%20diverse%20forest%20benchmarks%2C%20TreeLoc%20outperforms%20baselines%2C%20achieving%20precise%20localization.%20Ablation%20studies%20validate%20the%20contribution%20of%20each%20component.%20We%20also%20propose%20applications%20for%20long-term%20forest%20management%20using%20descriptors%20from%20a%20compact%20global%20tree%20database.%20TreeLoc%20is%20open-sourced%20for%20the%20robotics%20community%20at%20https%3A//github.com/minwoo0611/TreeLoc.&entry.1838667208=http%3A//arxiv.org/abs/2602.01501v3&entry.124074799=Read"},
{"title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation", "author": "Junyang Chen and Xiangbo Lv and Zhiqiang Kou and Xingdong Sheng and Ning Xu and Yiguo Qiao", "abstract": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.", "link": "http://arxiv.org/abs/2602.05578v2", "date": "2026-02-12", "relevancy": 2.8409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoGoSeg%3A%20Integrating%20Local%20and%20Global%20Features%20for%20Open-Vocabulary%20Semantic%20Segmentation&body=Title%3A%20LoGoSeg%3A%20Integrating%20Local%20and%20Global%20Features%20for%20Open-Vocabulary%20Semantic%20Segmentation%0AAuthor%3A%20Junyang%20Chen%20and%20Xiangbo%20Lv%20and%20Zhiqiang%20Kou%20and%20Xingdong%20Sheng%20and%20Ning%20Xu%20and%20Yiguo%20Qiao%0AAbstract%3A%20Open-vocabulary%20semantic%20segmentation%20%28OVSS%29%20extends%20traditional%20closed-set%20segmentation%20by%20enabling%20pixel-wise%20annotation%20for%20both%20seen%20and%20unseen%20categories%20using%20arbitrary%20textual%20descriptions.%20While%20existing%20methods%20leverage%20vision-language%20models%20%28VLMs%29%20like%20CLIP%2C%20their%20reliance%20on%20image-level%20pretraining%20often%20results%20in%20imprecise%20spatial%20alignment%2C%20leading%20to%20mismatched%20segmentations%20in%20ambiguous%20or%20cluttered%20scenes.%20However%2C%20most%20existing%20approaches%20lack%20strong%20object%20priors%20and%20region-level%20constraints%2C%20which%20can%20lead%20to%20object%20hallucination%20or%20missed%20detections%2C%20further%20degrading%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20LoGoSeg%2C%20an%20efficient%20single-stage%20framework%20that%20integrates%20three%20key%20innovations%3A%20%28i%29%20an%20object%20existence%20prior%20that%20dynamically%20weights%20relevant%20categories%20through%20global%20image-text%20similarity%2C%20effectively%20reducing%20hallucinations%3B%20%28ii%29%20a%20region-aware%20alignment%20module%20that%20establishes%20precise%20region-level%20visual-textual%20correspondences%3B%20and%20%28iii%29%20a%20dual-stream%20fusion%20mechanism%20that%20optimally%20combines%20local%20structural%20information%20with%20global%20semantic%20context.%20Unlike%20prior%20works%2C%20LoGoSeg%20eliminates%20the%20need%20for%20external%20mask%20proposals%2C%20additional%20backbones%2C%20or%20extra%20datasets%2C%20ensuring%20efficiency.%20Extensive%20experiments%20on%20six%20benchmarks%20%28A-847%2C%20PC-459%2C%20A-150%2C%20PC-59%2C%20PAS-20%2C%20and%20PAS-20b%29%20demonstrate%20its%20competitive%20performance%20and%20strong%20generalization%20in%20open-vocabulary%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoGoSeg%253A%2520Integrating%2520Local%2520and%2520Global%2520Features%2520for%2520Open-Vocabulary%2520Semantic%2520Segmentation%26entry.906535625%3DJunyang%2520Chen%2520and%2520Xiangbo%2520Lv%2520and%2520Zhiqiang%2520Kou%2520and%2520Xingdong%2520Sheng%2520and%2520Ning%2520Xu%2520and%2520Yiguo%2520Qiao%26entry.1292438233%3DOpen-vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%2520extends%2520traditional%2520closed-set%2520segmentation%2520by%2520enabling%2520pixel-wise%2520annotation%2520for%2520both%2520seen%2520and%2520unseen%2520categories%2520using%2520arbitrary%2520textual%2520descriptions.%2520While%2520existing%2520methods%2520leverage%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%252C%2520their%2520reliance%2520on%2520image-level%2520pretraining%2520often%2520results%2520in%2520imprecise%2520spatial%2520alignment%252C%2520leading%2520to%2520mismatched%2520segmentations%2520in%2520ambiguous%2520or%2520cluttered%2520scenes.%2520However%252C%2520most%2520existing%2520approaches%2520lack%2520strong%2520object%2520priors%2520and%2520region-level%2520constraints%252C%2520which%2520can%2520lead%2520to%2520object%2520hallucination%2520or%2520missed%2520detections%252C%2520further%2520degrading%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520LoGoSeg%252C%2520an%2520efficient%2520single-stage%2520framework%2520that%2520integrates%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520an%2520object%2520existence%2520prior%2520that%2520dynamically%2520weights%2520relevant%2520categories%2520through%2520global%2520image-text%2520similarity%252C%2520effectively%2520reducing%2520hallucinations%253B%2520%2528ii%2529%2520a%2520region-aware%2520alignment%2520module%2520that%2520establishes%2520precise%2520region-level%2520visual-textual%2520correspondences%253B%2520and%2520%2528iii%2529%2520a%2520dual-stream%2520fusion%2520mechanism%2520that%2520optimally%2520combines%2520local%2520structural%2520information%2520with%2520global%2520semantic%2520context.%2520Unlike%2520prior%2520works%252C%2520LoGoSeg%2520eliminates%2520the%2520need%2520for%2520external%2520mask%2520proposals%252C%2520additional%2520backbones%252C%2520or%2520extra%2520datasets%252C%2520ensuring%2520efficiency.%2520Extensive%2520experiments%2520on%2520six%2520benchmarks%2520%2528A-847%252C%2520PC-459%252C%2520A-150%252C%2520PC-59%252C%2520PAS-20%252C%2520and%2520PAS-20b%2529%2520demonstrate%2520its%2520competitive%2520performance%2520and%2520strong%2520generalization%2520in%2520open-vocabulary%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoGoSeg%3A%20Integrating%20Local%20and%20Global%20Features%20for%20Open-Vocabulary%20Semantic%20Segmentation&entry.906535625=Junyang%20Chen%20and%20Xiangbo%20Lv%20and%20Zhiqiang%20Kou%20and%20Xingdong%20Sheng%20and%20Ning%20Xu%20and%20Yiguo%20Qiao&entry.1292438233=Open-vocabulary%20semantic%20segmentation%20%28OVSS%29%20extends%20traditional%20closed-set%20segmentation%20by%20enabling%20pixel-wise%20annotation%20for%20both%20seen%20and%20unseen%20categories%20using%20arbitrary%20textual%20descriptions.%20While%20existing%20methods%20leverage%20vision-language%20models%20%28VLMs%29%20like%20CLIP%2C%20their%20reliance%20on%20image-level%20pretraining%20often%20results%20in%20imprecise%20spatial%20alignment%2C%20leading%20to%20mismatched%20segmentations%20in%20ambiguous%20or%20cluttered%20scenes.%20However%2C%20most%20existing%20approaches%20lack%20strong%20object%20priors%20and%20region-level%20constraints%2C%20which%20can%20lead%20to%20object%20hallucination%20or%20missed%20detections%2C%20further%20degrading%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20LoGoSeg%2C%20an%20efficient%20single-stage%20framework%20that%20integrates%20three%20key%20innovations%3A%20%28i%29%20an%20object%20existence%20prior%20that%20dynamically%20weights%20relevant%20categories%20through%20global%20image-text%20similarity%2C%20effectively%20reducing%20hallucinations%3B%20%28ii%29%20a%20region-aware%20alignment%20module%20that%20establishes%20precise%20region-level%20visual-textual%20correspondences%3B%20and%20%28iii%29%20a%20dual-stream%20fusion%20mechanism%20that%20optimally%20combines%20local%20structural%20information%20with%20global%20semantic%20context.%20Unlike%20prior%20works%2C%20LoGoSeg%20eliminates%20the%20need%20for%20external%20mask%20proposals%2C%20additional%20backbones%2C%20or%20extra%20datasets%2C%20ensuring%20efficiency.%20Extensive%20experiments%20on%20six%20benchmarks%20%28A-847%2C%20PC-459%2C%20A-150%2C%20PC-59%2C%20PAS-20%2C%20and%20PAS-20b%29%20demonstrate%20its%20competitive%20performance%20and%20strong%20generalization%20in%20open-vocabulary%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.05578v2&entry.124074799=Read"},
{"title": "Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models", "author": "Jialin Wu and Wei Shi and Han Shen and Peigui Qi and Kunsheng Tang and Zhicong Huang and Binghao Wang and Zhou Yang", "abstract": "Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.", "link": "http://arxiv.org/abs/2602.11824v1", "date": "2026-02-12", "relevancy": 2.8339, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revis%3A%20Sparse%20Latent%20Steering%20to%20Mitigate%20Object%20Hallucination%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Revis%3A%20Sparse%20Latent%20Steering%20to%20Mitigate%20Object%20Hallucination%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Jialin%20Wu%20and%20Wei%20Shi%20and%20Han%20Shen%20and%20Peigui%20Qi%20and%20Kunsheng%20Tang%20and%20Zhicong%20Huang%20and%20Binghao%20Wang%20and%20Zhou%20Yang%0AAbstract%3A%20Despite%20the%20advanced%20capabilities%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20they%20frequently%20suffer%20from%20object%20hallucination.%20One%20reason%20is%20that%20visual%20features%20and%20pretrained%20textual%20representations%20often%20become%20intertwined%20in%20the%20deeper%20network%20layers.%20To%20address%20this%2C%20we%20propose%20REVIS%2C%20a%20training-free%20framework%20designed%20to%20explicitly%20re-activate%20this%20suppressed%20visual%20information.%20Rooted%20in%20latent%20space%20geometry%2C%20REVIS%20extracts%20the%20pure%20visual%20information%20vector%20via%20orthogonal%20projection%20and%20employs%20a%20calibrated%20strategy%20to%20perform%20sparse%20intervention%20only%20at%20the%20precise%20depth%20where%20suppression%20occurs.%20This%20surgical%20approach%20effectively%20restores%20visual%20information%20with%20minimal%20computational%20cost.%20Empirical%20evaluations%20on%20standard%20benchmarks%20demonstrate%20that%20REVIS%20reduces%20object%20hallucination%20rates%20by%20approximately%2019%25%20compared%20to%20state-of-the-art%20baselines%2C%20while%20preserving%20general%20reasoning%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevis%253A%2520Sparse%2520Latent%2520Steering%2520to%2520Mitigate%2520Object%2520Hallucination%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJialin%2520Wu%2520and%2520Wei%2520Shi%2520and%2520Han%2520Shen%2520and%2520Peigui%2520Qi%2520and%2520Kunsheng%2520Tang%2520and%2520Zhicong%2520Huang%2520and%2520Binghao%2520Wang%2520and%2520Zhou%2520Yang%26entry.1292438233%3DDespite%2520the%2520advanced%2520capabilities%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520they%2520frequently%2520suffer%2520from%2520object%2520hallucination.%2520One%2520reason%2520is%2520that%2520visual%2520features%2520and%2520pretrained%2520textual%2520representations%2520often%2520become%2520intertwined%2520in%2520the%2520deeper%2520network%2520layers.%2520To%2520address%2520this%252C%2520we%2520propose%2520REVIS%252C%2520a%2520training-free%2520framework%2520designed%2520to%2520explicitly%2520re-activate%2520this%2520suppressed%2520visual%2520information.%2520Rooted%2520in%2520latent%2520space%2520geometry%252C%2520REVIS%2520extracts%2520the%2520pure%2520visual%2520information%2520vector%2520via%2520orthogonal%2520projection%2520and%2520employs%2520a%2520calibrated%2520strategy%2520to%2520perform%2520sparse%2520intervention%2520only%2520at%2520the%2520precise%2520depth%2520where%2520suppression%2520occurs.%2520This%2520surgical%2520approach%2520effectively%2520restores%2520visual%2520information%2520with%2520minimal%2520computational%2520cost.%2520Empirical%2520evaluations%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520REVIS%2520reduces%2520object%2520hallucination%2520rates%2520by%2520approximately%252019%2525%2520compared%2520to%2520state-of-the-art%2520baselines%252C%2520while%2520preserving%2520general%2520reasoning%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revis%3A%20Sparse%20Latent%20Steering%20to%20Mitigate%20Object%20Hallucination%20in%20Large%20Vision-Language%20Models&entry.906535625=Jialin%20Wu%20and%20Wei%20Shi%20and%20Han%20Shen%20and%20Peigui%20Qi%20and%20Kunsheng%20Tang%20and%20Zhicong%20Huang%20and%20Binghao%20Wang%20and%20Zhou%20Yang&entry.1292438233=Despite%20the%20advanced%20capabilities%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20they%20frequently%20suffer%20from%20object%20hallucination.%20One%20reason%20is%20that%20visual%20features%20and%20pretrained%20textual%20representations%20often%20become%20intertwined%20in%20the%20deeper%20network%20layers.%20To%20address%20this%2C%20we%20propose%20REVIS%2C%20a%20training-free%20framework%20designed%20to%20explicitly%20re-activate%20this%20suppressed%20visual%20information.%20Rooted%20in%20latent%20space%20geometry%2C%20REVIS%20extracts%20the%20pure%20visual%20information%20vector%20via%20orthogonal%20projection%20and%20employs%20a%20calibrated%20strategy%20to%20perform%20sparse%20intervention%20only%20at%20the%20precise%20depth%20where%20suppression%20occurs.%20This%20surgical%20approach%20effectively%20restores%20visual%20information%20with%20minimal%20computational%20cost.%20Empirical%20evaluations%20on%20standard%20benchmarks%20demonstrate%20that%20REVIS%20reduces%20object%20hallucination%20rates%20by%20approximately%2019%25%20compared%20to%20state-of-the-art%20baselines%2C%20while%20preserving%20general%20reasoning%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2602.11824v1&entry.124074799=Read"},
{"title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education", "author": "Mohamed Huti and Alasdair Mackintosh and Amy Waldock and Dominic Andrews and Maxime Leli\u00e8vre and Moritz Boos and Tobias Murray and Paul Atherton and Robin A. A. Ince and Oliver G. B. Garrod", "abstract": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.", "link": "http://arxiv.org/abs/2602.12196v1", "date": "2026-02-12", "relevancy": 2.8178, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Reasoning%20Benchmark%3A%20Evaluating%20Multimodal%20LLMs%20on%20Classroom-Authentic%20Visual%20Problems%20from%20Primary%20Education&body=Title%3A%20Visual%20Reasoning%20Benchmark%3A%20Evaluating%20Multimodal%20LLMs%20on%20Classroom-Authentic%20Visual%20Problems%20from%20Primary%20Education%0AAuthor%3A%20Mohamed%20Huti%20and%20Alasdair%20Mackintosh%20and%20Amy%20Waldock%20and%20Dominic%20Andrews%20and%20Maxime%20Leli%C3%A8vre%20and%20Moritz%20Boos%20and%20Tobias%20Murray%20and%20Paul%20Atherton%20and%20Robin%20A.%20A.%20Ince%20and%20Oliver%20G.%20B.%20Garrod%0AAbstract%3A%20AI%20models%20have%20achieved%20state-of-the-art%20results%20in%20textual%20reasoning%3B%20however%2C%20their%20ability%20to%20reason%20over%20spatial%20and%20relational%20structures%20remains%20a%20critical%20bottleneck%20--%20particularly%20in%20early-grade%20maths%2C%20which%20relies%20heavily%20on%20visuals.%20This%20paper%20introduces%20the%20visual%20reasoning%20benchmark%20%28VRB%29%2C%20a%20novel%20dataset%20designed%20to%20evaluate%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20on%20their%20ability%20to%20solve%20authentic%20visual%20problems%20from%20classrooms.%20This%20benchmark%20is%20built%20on%20a%20set%20of%20701%20questions%20sourced%20from%20primary%20school%20examinations%20in%20Zambia%20and%20India%2C%20which%20cover%20a%20range%20of%20tasks%20such%20as%20reasoning%20by%20analogy%2C%20pattern%20completion%2C%20and%20spatial%20matching.%20We%20outline%20the%20methodology%20and%20development%20of%20the%20benchmark%20which%20intentionally%20uses%20unedited%2C%20minimal-text%20images%20to%20test%20if%20models%20can%20meet%20realistic%20needs%20of%20primary%20education.%20Our%20findings%20reveal%20a%20%60%60jagged%20frontier%27%27%20of%20capability%20where%20models%20demonstrate%20better%20proficiency%20in%20static%20skills%20such%20as%20counting%20and%20scaling%2C%20but%20reach%20a%20distinct%20%60%60spatial%20ceiling%27%27%20when%20faced%20with%20dynamic%20operations%20like%20folding%2C%20reflection%2C%20and%20rotation.%20These%20weaknesses%20pose%20a%20risk%20for%20classroom%20use%20on%20visual%20reasoning%20problems%2C%20with%20the%20potential%20for%20incorrect%20marking%2C%20false%20scaffolding%2C%20and%20reinforcing%20student%20misconceptions.%20Consequently%2C%20education-focused%20benchmarks%20like%20the%20VRB%20are%20essential%20for%20determining%20the%20functional%20boundaries%20of%20multimodal%20tools%20used%20in%20classrooms.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Reasoning%2520Benchmark%253A%2520Evaluating%2520Multimodal%2520LLMs%2520on%2520Classroom-Authentic%2520Visual%2520Problems%2520from%2520Primary%2520Education%26entry.906535625%3DMohamed%2520Huti%2520and%2520Alasdair%2520Mackintosh%2520and%2520Amy%2520Waldock%2520and%2520Dominic%2520Andrews%2520and%2520Maxime%2520Leli%25C3%25A8vre%2520and%2520Moritz%2520Boos%2520and%2520Tobias%2520Murray%2520and%2520Paul%2520Atherton%2520and%2520Robin%2520A.%2520A.%2520Ince%2520and%2520Oliver%2520G.%2520B.%2520Garrod%26entry.1292438233%3DAI%2520models%2520have%2520achieved%2520state-of-the-art%2520results%2520in%2520textual%2520reasoning%253B%2520however%252C%2520their%2520ability%2520to%2520reason%2520over%2520spatial%2520and%2520relational%2520structures%2520remains%2520a%2520critical%2520bottleneck%2520--%2520particularly%2520in%2520early-grade%2520maths%252C%2520which%2520relies%2520heavily%2520on%2520visuals.%2520This%2520paper%2520introduces%2520the%2520visual%2520reasoning%2520benchmark%2520%2528VRB%2529%252C%2520a%2520novel%2520dataset%2520designed%2520to%2520evaluate%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520on%2520their%2520ability%2520to%2520solve%2520authentic%2520visual%2520problems%2520from%2520classrooms.%2520This%2520benchmark%2520is%2520built%2520on%2520a%2520set%2520of%2520701%2520questions%2520sourced%2520from%2520primary%2520school%2520examinations%2520in%2520Zambia%2520and%2520India%252C%2520which%2520cover%2520a%2520range%2520of%2520tasks%2520such%2520as%2520reasoning%2520by%2520analogy%252C%2520pattern%2520completion%252C%2520and%2520spatial%2520matching.%2520We%2520outline%2520the%2520methodology%2520and%2520development%2520of%2520the%2520benchmark%2520which%2520intentionally%2520uses%2520unedited%252C%2520minimal-text%2520images%2520to%2520test%2520if%2520models%2520can%2520meet%2520realistic%2520needs%2520of%2520primary%2520education.%2520Our%2520findings%2520reveal%2520a%2520%2560%2560jagged%2520frontier%2527%2527%2520of%2520capability%2520where%2520models%2520demonstrate%2520better%2520proficiency%2520in%2520static%2520skills%2520such%2520as%2520counting%2520and%2520scaling%252C%2520but%2520reach%2520a%2520distinct%2520%2560%2560spatial%2520ceiling%2527%2527%2520when%2520faced%2520with%2520dynamic%2520operations%2520like%2520folding%252C%2520reflection%252C%2520and%2520rotation.%2520These%2520weaknesses%2520pose%2520a%2520risk%2520for%2520classroom%2520use%2520on%2520visual%2520reasoning%2520problems%252C%2520with%2520the%2520potential%2520for%2520incorrect%2520marking%252C%2520false%2520scaffolding%252C%2520and%2520reinforcing%2520student%2520misconceptions.%2520Consequently%252C%2520education-focused%2520benchmarks%2520like%2520the%2520VRB%2520are%2520essential%2520for%2520determining%2520the%2520functional%2520boundaries%2520of%2520multimodal%2520tools%2520used%2520in%2520classrooms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Reasoning%20Benchmark%3A%20Evaluating%20Multimodal%20LLMs%20on%20Classroom-Authentic%20Visual%20Problems%20from%20Primary%20Education&entry.906535625=Mohamed%20Huti%20and%20Alasdair%20Mackintosh%20and%20Amy%20Waldock%20and%20Dominic%20Andrews%20and%20Maxime%20Leli%C3%A8vre%20and%20Moritz%20Boos%20and%20Tobias%20Murray%20and%20Paul%20Atherton%20and%20Robin%20A.%20A.%20Ince%20and%20Oliver%20G.%20B.%20Garrod&entry.1292438233=AI%20models%20have%20achieved%20state-of-the-art%20results%20in%20textual%20reasoning%3B%20however%2C%20their%20ability%20to%20reason%20over%20spatial%20and%20relational%20structures%20remains%20a%20critical%20bottleneck%20--%20particularly%20in%20early-grade%20maths%2C%20which%20relies%20heavily%20on%20visuals.%20This%20paper%20introduces%20the%20visual%20reasoning%20benchmark%20%28VRB%29%2C%20a%20novel%20dataset%20designed%20to%20evaluate%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20on%20their%20ability%20to%20solve%20authentic%20visual%20problems%20from%20classrooms.%20This%20benchmark%20is%20built%20on%20a%20set%20of%20701%20questions%20sourced%20from%20primary%20school%20examinations%20in%20Zambia%20and%20India%2C%20which%20cover%20a%20range%20of%20tasks%20such%20as%20reasoning%20by%20analogy%2C%20pattern%20completion%2C%20and%20spatial%20matching.%20We%20outline%20the%20methodology%20and%20development%20of%20the%20benchmark%20which%20intentionally%20uses%20unedited%2C%20minimal-text%20images%20to%20test%20if%20models%20can%20meet%20realistic%20needs%20of%20primary%20education.%20Our%20findings%20reveal%20a%20%60%60jagged%20frontier%27%27%20of%20capability%20where%20models%20demonstrate%20better%20proficiency%20in%20static%20skills%20such%20as%20counting%20and%20scaling%2C%20but%20reach%20a%20distinct%20%60%60spatial%20ceiling%27%27%20when%20faced%20with%20dynamic%20operations%20like%20folding%2C%20reflection%2C%20and%20rotation.%20These%20weaknesses%20pose%20a%20risk%20for%20classroom%20use%20on%20visual%20reasoning%20problems%2C%20with%20the%20potential%20for%20incorrect%20marking%2C%20false%20scaffolding%2C%20and%20reinforcing%20student%20misconceptions.%20Consequently%2C%20education-focused%20benchmarks%20like%20the%20VRB%20are%20essential%20for%20determining%20the%20functional%20boundaries%20of%20multimodal%20tools%20used%20in%20classrooms.&entry.1838667208=http%3A//arxiv.org/abs/2602.12196v1&entry.124074799=Read"},
{"title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception", "author": "Lai Wei and Liangbo He and Jun Lan and Lingzhong Dong and Yutong Cai and Siyuan Li and Huijia Zhu and Weiqiang Wang and Linghe Kong and Yue Wang and Zhuosheng Zhang and Weiran Huang", "abstract": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.", "link": "http://arxiv.org/abs/2602.11858v1", "date": "2026-02-12", "relevancy": 2.8171, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zooming%20without%20Zooming%3A%20Region-to-Image%20Distillation%20for%20Fine-Grained%20Multimodal%20Perception&body=Title%3A%20Zooming%20without%20Zooming%3A%20Region-to-Image%20Distillation%20for%20Fine-Grained%20Multimodal%20Perception%0AAuthor%3A%20Lai%20Wei%20and%20Liangbo%20He%20and%20Jun%20Lan%20and%20Lingzhong%20Dong%20and%20Yutong%20Cai%20and%20Siyuan%20Li%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Linghe%20Kong%20and%20Yue%20Wang%20and%20Zhuosheng%20Zhang%20and%20Weiran%20Huang%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20broad%20visual%20understanding%20but%20still%20struggle%20with%20fine-grained%20perception%2C%20where%20decisive%20evidence%20is%20small%20and%20easily%20overwhelmed%20by%20global%20context.%20Recent%20%22Thinking-with-Images%22%20methods%20alleviate%20this%20by%20iteratively%20zooming%20in%20and%20out%20regions%20of%20interest%20during%20inference%2C%20but%20incur%20high%20latency%20due%20to%20repeated%20tool%20calls%20and%20visual%20re-encoding.%20To%20address%20this%2C%20we%20propose%20Region-to-Image%20Distillation%2C%20which%20transforms%20zooming%20from%20an%20inference-time%20tool%20into%20a%20training-time%20primitive%2C%20thereby%20internalizing%20the%20benefits%20of%20agentic%20zooming%20into%20a%20single%20forward%20pass%20of%20an%20MLLM.%20In%20particular%2C%20we%20first%20zoom%20in%20to%20micro-cropped%20regions%20to%20let%20strong%20teacher%20models%20generate%20high-quality%20VQA%20data%2C%20and%20then%20distill%20this%20region-grounded%20supervision%20back%20to%20the%20full%20image.%20After%20training%20on%20such%20data%2C%20the%20smaller%20student%20model%20improves%20%22single-glance%22%20fine-grained%20perception%20without%20tool%20use.%20To%20rigorously%20evaluate%20this%20capability%2C%20we%20further%20present%20ZoomBench%2C%20a%20hybrid-annotated%20benchmark%20of%20845%20VQA%20data%20spanning%20six%20fine-grained%20perceptual%20dimensions%2C%20together%20with%20a%20dual-view%20protocol%20that%20quantifies%20the%20global--regional%20%22zooming%20gap%22.%20Experiments%20show%20that%20our%20models%20achieve%20leading%20performance%20across%20multiple%20fine-grained%20perception%20benchmarks%2C%20and%20also%20improve%20general%20multimodal%20cognition%20on%20benchmarks%20such%20as%20visual%20reasoning%20and%20GUI%20agents.%20We%20further%20discuss%20when%20%22Thinking-with-Images%22%20is%20necessary%20versus%20when%20its%20gains%20can%20be%20distilled%20into%20a%20single%20forward%20pass.%20Our%20code%20is%20available%20at%20https%3A//github.com/inclusionAI/Zooming-without-Zooming.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZooming%2520without%2520Zooming%253A%2520Region-to-Image%2520Distillation%2520for%2520Fine-Grained%2520Multimodal%2520Perception%26entry.906535625%3DLai%2520Wei%2520and%2520Liangbo%2520He%2520and%2520Jun%2520Lan%2520and%2520Lingzhong%2520Dong%2520and%2520Yutong%2520Cai%2520and%2520Siyuan%2520Li%2520and%2520Huijia%2520Zhu%2520and%2520Weiqiang%2520Wang%2520and%2520Linghe%2520Kong%2520and%2520Yue%2520Wang%2520and%2520Zhuosheng%2520Zhang%2520and%2520Weiran%2520Huang%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520broad%2520visual%2520understanding%2520but%2520still%2520struggle%2520with%2520fine-grained%2520perception%252C%2520where%2520decisive%2520evidence%2520is%2520small%2520and%2520easily%2520overwhelmed%2520by%2520global%2520context.%2520Recent%2520%2522Thinking-with-Images%2522%2520methods%2520alleviate%2520this%2520by%2520iteratively%2520zooming%2520in%2520and%2520out%2520regions%2520of%2520interest%2520during%2520inference%252C%2520but%2520incur%2520high%2520latency%2520due%2520to%2520repeated%2520tool%2520calls%2520and%2520visual%2520re-encoding.%2520To%2520address%2520this%252C%2520we%2520propose%2520Region-to-Image%2520Distillation%252C%2520which%2520transforms%2520zooming%2520from%2520an%2520inference-time%2520tool%2520into%2520a%2520training-time%2520primitive%252C%2520thereby%2520internalizing%2520the%2520benefits%2520of%2520agentic%2520zooming%2520into%2520a%2520single%2520forward%2520pass%2520of%2520an%2520MLLM.%2520In%2520particular%252C%2520we%2520first%2520zoom%2520in%2520to%2520micro-cropped%2520regions%2520to%2520let%2520strong%2520teacher%2520models%2520generate%2520high-quality%2520VQA%2520data%252C%2520and%2520then%2520distill%2520this%2520region-grounded%2520supervision%2520back%2520to%2520the%2520full%2520image.%2520After%2520training%2520on%2520such%2520data%252C%2520the%2520smaller%2520student%2520model%2520improves%2520%2522single-glance%2522%2520fine-grained%2520perception%2520without%2520tool%2520use.%2520To%2520rigorously%2520evaluate%2520this%2520capability%252C%2520we%2520further%2520present%2520ZoomBench%252C%2520a%2520hybrid-annotated%2520benchmark%2520of%2520845%2520VQA%2520data%2520spanning%2520six%2520fine-grained%2520perceptual%2520dimensions%252C%2520together%2520with%2520a%2520dual-view%2520protocol%2520that%2520quantifies%2520the%2520global--regional%2520%2522zooming%2520gap%2522.%2520Experiments%2520show%2520that%2520our%2520models%2520achieve%2520leading%2520performance%2520across%2520multiple%2520fine-grained%2520perception%2520benchmarks%252C%2520and%2520also%2520improve%2520general%2520multimodal%2520cognition%2520on%2520benchmarks%2520such%2520as%2520visual%2520reasoning%2520and%2520GUI%2520agents.%2520We%2520further%2520discuss%2520when%2520%2522Thinking-with-Images%2522%2520is%2520necessary%2520versus%2520when%2520its%2520gains%2520can%2520be%2520distilled%2520into%2520a%2520single%2520forward%2520pass.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/inclusionAI/Zooming-without-Zooming.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zooming%20without%20Zooming%3A%20Region-to-Image%20Distillation%20for%20Fine-Grained%20Multimodal%20Perception&entry.906535625=Lai%20Wei%20and%20Liangbo%20He%20and%20Jun%20Lan%20and%20Lingzhong%20Dong%20and%20Yutong%20Cai%20and%20Siyuan%20Li%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Linghe%20Kong%20and%20Yue%20Wang%20and%20Zhuosheng%20Zhang%20and%20Weiran%20Huang&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20broad%20visual%20understanding%20but%20still%20struggle%20with%20fine-grained%20perception%2C%20where%20decisive%20evidence%20is%20small%20and%20easily%20overwhelmed%20by%20global%20context.%20Recent%20%22Thinking-with-Images%22%20methods%20alleviate%20this%20by%20iteratively%20zooming%20in%20and%20out%20regions%20of%20interest%20during%20inference%2C%20but%20incur%20high%20latency%20due%20to%20repeated%20tool%20calls%20and%20visual%20re-encoding.%20To%20address%20this%2C%20we%20propose%20Region-to-Image%20Distillation%2C%20which%20transforms%20zooming%20from%20an%20inference-time%20tool%20into%20a%20training-time%20primitive%2C%20thereby%20internalizing%20the%20benefits%20of%20agentic%20zooming%20into%20a%20single%20forward%20pass%20of%20an%20MLLM.%20In%20particular%2C%20we%20first%20zoom%20in%20to%20micro-cropped%20regions%20to%20let%20strong%20teacher%20models%20generate%20high-quality%20VQA%20data%2C%20and%20then%20distill%20this%20region-grounded%20supervision%20back%20to%20the%20full%20image.%20After%20training%20on%20such%20data%2C%20the%20smaller%20student%20model%20improves%20%22single-glance%22%20fine-grained%20perception%20without%20tool%20use.%20To%20rigorously%20evaluate%20this%20capability%2C%20we%20further%20present%20ZoomBench%2C%20a%20hybrid-annotated%20benchmark%20of%20845%20VQA%20data%20spanning%20six%20fine-grained%20perceptual%20dimensions%2C%20together%20with%20a%20dual-view%20protocol%20that%20quantifies%20the%20global--regional%20%22zooming%20gap%22.%20Experiments%20show%20that%20our%20models%20achieve%20leading%20performance%20across%20multiple%20fine-grained%20perception%20benchmarks%2C%20and%20also%20improve%20general%20multimodal%20cognition%20on%20benchmarks%20such%20as%20visual%20reasoning%20and%20GUI%20agents.%20We%20further%20discuss%20when%20%22Thinking-with-Images%22%20is%20necessary%20versus%20when%20its%20gains%20can%20be%20distilled%20into%20a%20single%20forward%20pass.%20Our%20code%20is%20available%20at%20https%3A//github.com/inclusionAI/Zooming-without-Zooming.&entry.1838667208=http%3A//arxiv.org/abs/2602.11858v1&entry.124074799=Read"},
{"title": "Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation", "author": "Lingyong Yan and Jiulong Wu and Dong Xie and Weixian Shi and Deguo Xia and Jizhou Huang", "abstract": "Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.", "link": "http://arxiv.org/abs/2602.11790v1", "date": "2026-02-12", "relevancy": 2.8134, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5836}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5574}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20End-to-End%20Video%20Models%3A%20An%20LLM-Based%20Multi-Agent%20System%20for%20Educational%20Video%20Generation&body=Title%3A%20Beyond%20End-to-End%20Video%20Models%3A%20An%20LLM-Based%20Multi-Agent%20System%20for%20Educational%20Video%20Generation%0AAuthor%3A%20Lingyong%20Yan%20and%20Jiulong%20Wu%20and%20Dong%20Xie%20and%20Weixian%20Shi%20and%20Deguo%20Xia%20and%20Jizhou%20Huang%0AAbstract%3A%20Although%20recent%20end-to-end%20video%20generation%20models%20demonstrate%20impressive%20performance%20in%20visually%20oriented%20content%20creation%2C%20they%20remain%20limited%20in%20scenarios%20that%20require%20strict%20logical%20rigor%20and%20precise%20knowledge%20representation%2C%20such%20as%20instructional%20and%20educational%20media.%20To%20address%20this%20problem%2C%20we%20propose%20LAVES%2C%20a%20hierarchical%20LLM-based%20multi-agent%20system%20for%20generating%20high-quality%20instructional%20videos%20from%20educational%20problems.%20The%20LAVES%20formulates%20educational%20video%20generation%20as%20a%20multi-objective%20task%20that%20simultaneously%20demands%20correct%20step-by-step%20reasoning%2C%20pedagogically%20coherent%20narration%2C%20semantically%20faithful%20visual%20demonstrations%2C%20and%20precise%20audio--visual%20alignment.%20To%20address%20the%20limitations%20of%20prior%20approaches--including%20low%20procedural%20fidelity%2C%20high%20production%20cost%2C%20and%20limited%20controllability--LAVES%20decomposes%20the%20generation%20workflow%20into%20specialized%20agents%20coordinated%20by%20a%20central%20Orchestrating%20Agent%20with%20explicit%20quality%20gates%20and%20iterative%20critique%20mechanisms.%20Specifically%2C%20the%20Orchestrating%20Agent%20supervises%20a%20Solution%20Agent%20for%20rigorous%20problem%20solving%2C%20an%20Illustration%20Agent%20that%20produces%20executable%20visualization%20codes%2C%20and%20a%20Narration%20Agent%20for%20learner-oriented%20instructional%20scripts.%20In%20addition%2C%20all%20outputs%20from%20the%20working%20agents%20are%20subject%20to%20semantic%20critique%2C%20rule-based%20constraints%2C%20and%20tool-based%20compilation%20checks.%20Rather%20than%20directly%20synthesizing%20pixels%2C%20the%20system%20constructs%20a%20structured%20executable%20video%20script%20that%20is%20deterministically%20compiled%20into%20synchronized%20visuals%20and%20narration%20using%20template-driven%20assembly%20rules%2C%20enabling%20fully%20automated%20end-to-end%20production%20without%20manual%20editing.%20In%20large-scale%20deployments%2C%20LAVES%20achieves%20a%20throughput%20exceeding%20one%20million%20videos%20per%20day%2C%20delivering%20over%20a%2095%25%20reduction%20in%20cost%20compared%20to%20current%20industry-standard%20approaches%20while%20maintaining%20a%20high%20acceptance%20rate.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520End-to-End%2520Video%2520Models%253A%2520An%2520LLM-Based%2520Multi-Agent%2520System%2520for%2520Educational%2520Video%2520Generation%26entry.906535625%3DLingyong%2520Yan%2520and%2520Jiulong%2520Wu%2520and%2520Dong%2520Xie%2520and%2520Weixian%2520Shi%2520and%2520Deguo%2520Xia%2520and%2520Jizhou%2520Huang%26entry.1292438233%3DAlthough%2520recent%2520end-to-end%2520video%2520generation%2520models%2520demonstrate%2520impressive%2520performance%2520in%2520visually%2520oriented%2520content%2520creation%252C%2520they%2520remain%2520limited%2520in%2520scenarios%2520that%2520require%2520strict%2520logical%2520rigor%2520and%2520precise%2520knowledge%2520representation%252C%2520such%2520as%2520instructional%2520and%2520educational%2520media.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520LAVES%252C%2520a%2520hierarchical%2520LLM-based%2520multi-agent%2520system%2520for%2520generating%2520high-quality%2520instructional%2520videos%2520from%2520educational%2520problems.%2520The%2520LAVES%2520formulates%2520educational%2520video%2520generation%2520as%2520a%2520multi-objective%2520task%2520that%2520simultaneously%2520demands%2520correct%2520step-by-step%2520reasoning%252C%2520pedagogically%2520coherent%2520narration%252C%2520semantically%2520faithful%2520visual%2520demonstrations%252C%2520and%2520precise%2520audio--visual%2520alignment.%2520To%2520address%2520the%2520limitations%2520of%2520prior%2520approaches--including%2520low%2520procedural%2520fidelity%252C%2520high%2520production%2520cost%252C%2520and%2520limited%2520controllability--LAVES%2520decomposes%2520the%2520generation%2520workflow%2520into%2520specialized%2520agents%2520coordinated%2520by%2520a%2520central%2520Orchestrating%2520Agent%2520with%2520explicit%2520quality%2520gates%2520and%2520iterative%2520critique%2520mechanisms.%2520Specifically%252C%2520the%2520Orchestrating%2520Agent%2520supervises%2520a%2520Solution%2520Agent%2520for%2520rigorous%2520problem%2520solving%252C%2520an%2520Illustration%2520Agent%2520that%2520produces%2520executable%2520visualization%2520codes%252C%2520and%2520a%2520Narration%2520Agent%2520for%2520learner-oriented%2520instructional%2520scripts.%2520In%2520addition%252C%2520all%2520outputs%2520from%2520the%2520working%2520agents%2520are%2520subject%2520to%2520semantic%2520critique%252C%2520rule-based%2520constraints%252C%2520and%2520tool-based%2520compilation%2520checks.%2520Rather%2520than%2520directly%2520synthesizing%2520pixels%252C%2520the%2520system%2520constructs%2520a%2520structured%2520executable%2520video%2520script%2520that%2520is%2520deterministically%2520compiled%2520into%2520synchronized%2520visuals%2520and%2520narration%2520using%2520template-driven%2520assembly%2520rules%252C%2520enabling%2520fully%2520automated%2520end-to-end%2520production%2520without%2520manual%2520editing.%2520In%2520large-scale%2520deployments%252C%2520LAVES%2520achieves%2520a%2520throughput%2520exceeding%2520one%2520million%2520videos%2520per%2520day%252C%2520delivering%2520over%2520a%252095%2525%2520reduction%2520in%2520cost%2520compared%2520to%2520current%2520industry-standard%2520approaches%2520while%2520maintaining%2520a%2520high%2520acceptance%2520rate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20End-to-End%20Video%20Models%3A%20An%20LLM-Based%20Multi-Agent%20System%20for%20Educational%20Video%20Generation&entry.906535625=Lingyong%20Yan%20and%20Jiulong%20Wu%20and%20Dong%20Xie%20and%20Weixian%20Shi%20and%20Deguo%20Xia%20and%20Jizhou%20Huang&entry.1292438233=Although%20recent%20end-to-end%20video%20generation%20models%20demonstrate%20impressive%20performance%20in%20visually%20oriented%20content%20creation%2C%20they%20remain%20limited%20in%20scenarios%20that%20require%20strict%20logical%20rigor%20and%20precise%20knowledge%20representation%2C%20such%20as%20instructional%20and%20educational%20media.%20To%20address%20this%20problem%2C%20we%20propose%20LAVES%2C%20a%20hierarchical%20LLM-based%20multi-agent%20system%20for%20generating%20high-quality%20instructional%20videos%20from%20educational%20problems.%20The%20LAVES%20formulates%20educational%20video%20generation%20as%20a%20multi-objective%20task%20that%20simultaneously%20demands%20correct%20step-by-step%20reasoning%2C%20pedagogically%20coherent%20narration%2C%20semantically%20faithful%20visual%20demonstrations%2C%20and%20precise%20audio--visual%20alignment.%20To%20address%20the%20limitations%20of%20prior%20approaches--including%20low%20procedural%20fidelity%2C%20high%20production%20cost%2C%20and%20limited%20controllability--LAVES%20decomposes%20the%20generation%20workflow%20into%20specialized%20agents%20coordinated%20by%20a%20central%20Orchestrating%20Agent%20with%20explicit%20quality%20gates%20and%20iterative%20critique%20mechanisms.%20Specifically%2C%20the%20Orchestrating%20Agent%20supervises%20a%20Solution%20Agent%20for%20rigorous%20problem%20solving%2C%20an%20Illustration%20Agent%20that%20produces%20executable%20visualization%20codes%2C%20and%20a%20Narration%20Agent%20for%20learner-oriented%20instructional%20scripts.%20In%20addition%2C%20all%20outputs%20from%20the%20working%20agents%20are%20subject%20to%20semantic%20critique%2C%20rule-based%20constraints%2C%20and%20tool-based%20compilation%20checks.%20Rather%20than%20directly%20synthesizing%20pixels%2C%20the%20system%20constructs%20a%20structured%20executable%20video%20script%20that%20is%20deterministically%20compiled%20into%20synchronized%20visuals%20and%20narration%20using%20template-driven%20assembly%20rules%2C%20enabling%20fully%20automated%20end-to-end%20production%20without%20manual%20editing.%20In%20large-scale%20deployments%2C%20LAVES%20achieves%20a%20throughput%20exceeding%20one%20million%20videos%20per%20day%2C%20delivering%20over%20a%2095%25%20reduction%20in%20cost%20compared%20to%20current%20industry-standard%20approaches%20while%20maintaining%20a%20high%20acceptance%20rate.&entry.1838667208=http%3A//arxiv.org/abs/2602.11790v1&entry.124074799=Read"},
{"title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation", "author": "Zhen Han and Mattias Teye and Derek Yadgaroff and Judith B\u00fctepage", "abstract": "The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters.", "link": "http://arxiv.org/abs/2507.18352v3", "date": "2026-02-12", "relevancy": 2.7573, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.588}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5368}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%20models%20through%20hybrid%20knowledge%20distillation&body=Title%3A%20Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%20models%20through%20hybrid%20knowledge%20distillation%0AAuthor%3A%20Zhen%20Han%20and%20Mattias%20Teye%20and%20Derek%20Yadgaroff%20and%20Judith%20B%C3%BCtepage%0AAbstract%3A%20The%20training%20of%20high-quality%2C%20robust%20machine%20learning%20models%20for%20speech-driven%203D%20facial%20animation%20requires%20a%20large%2C%20diverse%20dataset%20of%20high-quality%20audio-animation%20pairs.%20To%20overcome%20the%20lack%20of%20such%20a%20dataset%2C%20recent%20work%20has%20introduced%20large%20pre-trained%20speech%20encoders%20that%20are%20robust%20to%20variations%20in%20the%20input%20audio%20and%2C%20therefore%2C%20enable%20the%20facial%20animation%20model%20to%20generalize%20across%20speakers%2C%20audio%20quality%2C%20and%20languages.%20However%2C%20the%20resulting%20facial%20animation%20models%20are%20prohibitively%20large%20and%20lend%20themselves%20only%20to%20offline%20inference%20on%20a%20dedicated%20machine.%20In%20this%20work%2C%20we%20explore%20on-device%2C%20real-time%20facial%20animation%20models%20in%20the%20context%20of%20game%20development.%20We%20overcome%20the%20lack%20of%20large%20datasets%20by%20using%20hybrid%20knowledge%20distillation%20with%20pseudo-labeling.%20Given%20a%20large%20audio%20dataset%2C%20we%20employ%20a%20high-performing%20teacher%20model%20to%20train%20very%20small%20student%20models.%20In%20contrast%20to%20the%20pre-trained%20speech%20encoders%2C%20our%20student%20models%20only%20consist%20of%20convolutional%20and%20fully-connected%20layers%2C%20removing%20the%20need%20for%20attention%20context%20or%20recurrent%20updates.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20we%20can%20reduce%20the%20memory%20footprint%20to%20up%20to%203.4%20MB%20and%20required%20future%20audio%20context%20to%20up%20to%2081%20ms%20while%20maintaining%20high-quality%20animations.%20This%20paves%20the%20way%20for%20on-device%20inference%2C%20an%20important%20step%20towards%20realistic%2C%20model-driven%20digital%20characters.%0ALink%3A%20http%3A//arxiv.org/abs/2507.18352v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520is%2520not%2520small%2520enough%253A%2520High-quality%252C%2520low-resource%2520facial%2520animation%2520models%2520through%2520hybrid%2520knowledge%2520distillation%26entry.906535625%3DZhen%2520Han%2520and%2520Mattias%2520Teye%2520and%2520Derek%2520Yadgaroff%2520and%2520Judith%2520B%25C3%25BCtepage%26entry.1292438233%3DThe%2520training%2520of%2520high-quality%252C%2520robust%2520machine%2520learning%2520models%2520for%2520speech-driven%25203D%2520facial%2520animation%2520requires%2520a%2520large%252C%2520diverse%2520dataset%2520of%2520high-quality%2520audio-animation%2520pairs.%2520To%2520overcome%2520the%2520lack%2520of%2520such%2520a%2520dataset%252C%2520recent%2520work%2520has%2520introduced%2520large%2520pre-trained%2520speech%2520encoders%2520that%2520are%2520robust%2520to%2520variations%2520in%2520the%2520input%2520audio%2520and%252C%2520therefore%252C%2520enable%2520the%2520facial%2520animation%2520model%2520to%2520generalize%2520across%2520speakers%252C%2520audio%2520quality%252C%2520and%2520languages.%2520However%252C%2520the%2520resulting%2520facial%2520animation%2520models%2520are%2520prohibitively%2520large%2520and%2520lend%2520themselves%2520only%2520to%2520offline%2520inference%2520on%2520a%2520dedicated%2520machine.%2520In%2520this%2520work%252C%2520we%2520explore%2520on-device%252C%2520real-time%2520facial%2520animation%2520models%2520in%2520the%2520context%2520of%2520game%2520development.%2520We%2520overcome%2520the%2520lack%2520of%2520large%2520datasets%2520by%2520using%2520hybrid%2520knowledge%2520distillation%2520with%2520pseudo-labeling.%2520Given%2520a%2520large%2520audio%2520dataset%252C%2520we%2520employ%2520a%2520high-performing%2520teacher%2520model%2520to%2520train%2520very%2520small%2520student%2520models.%2520In%2520contrast%2520to%2520the%2520pre-trained%2520speech%2520encoders%252C%2520our%2520student%2520models%2520only%2520consist%2520of%2520convolutional%2520and%2520fully-connected%2520layers%252C%2520removing%2520the%2520need%2520for%2520attention%2520context%2520or%2520recurrent%2520updates.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520that%2520we%2520can%2520reduce%2520the%2520memory%2520footprint%2520to%2520up%2520to%25203.4%2520MB%2520and%2520required%2520future%2520audio%2520context%2520to%2520up%2520to%252081%2520ms%2520while%2520maintaining%2520high-quality%2520animations.%2520This%2520paves%2520the%2520way%2520for%2520on-device%2520inference%252C%2520an%2520important%2520step%2520towards%2520realistic%252C%2520model-driven%2520digital%2520characters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18352v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20is%20not%20small%20enough%3A%20High-quality%2C%20low-resource%20facial%20animation%20models%20through%20hybrid%20knowledge%20distillation&entry.906535625=Zhen%20Han%20and%20Mattias%20Teye%20and%20Derek%20Yadgaroff%20and%20Judith%20B%C3%BCtepage&entry.1292438233=The%20training%20of%20high-quality%2C%20robust%20machine%20learning%20models%20for%20speech-driven%203D%20facial%20animation%20requires%20a%20large%2C%20diverse%20dataset%20of%20high-quality%20audio-animation%20pairs.%20To%20overcome%20the%20lack%20of%20such%20a%20dataset%2C%20recent%20work%20has%20introduced%20large%20pre-trained%20speech%20encoders%20that%20are%20robust%20to%20variations%20in%20the%20input%20audio%20and%2C%20therefore%2C%20enable%20the%20facial%20animation%20model%20to%20generalize%20across%20speakers%2C%20audio%20quality%2C%20and%20languages.%20However%2C%20the%20resulting%20facial%20animation%20models%20are%20prohibitively%20large%20and%20lend%20themselves%20only%20to%20offline%20inference%20on%20a%20dedicated%20machine.%20In%20this%20work%2C%20we%20explore%20on-device%2C%20real-time%20facial%20animation%20models%20in%20the%20context%20of%20game%20development.%20We%20overcome%20the%20lack%20of%20large%20datasets%20by%20using%20hybrid%20knowledge%20distillation%20with%20pseudo-labeling.%20Given%20a%20large%20audio%20dataset%2C%20we%20employ%20a%20high-performing%20teacher%20model%20to%20train%20very%20small%20student%20models.%20In%20contrast%20to%20the%20pre-trained%20speech%20encoders%2C%20our%20student%20models%20only%20consist%20of%20convolutional%20and%20fully-connected%20layers%2C%20removing%20the%20need%20for%20attention%20context%20or%20recurrent%20updates.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20we%20can%20reduce%20the%20memory%20footprint%20to%20up%20to%203.4%20MB%20and%20required%20future%20audio%20context%20to%20up%20to%2081%20ms%20while%20maintaining%20high-quality%20animations.%20This%20paves%20the%20way%20for%20on-device%20inference%2C%20an%20important%20step%20towards%20realistic%2C%20model-driven%20digital%20characters.&entry.1838667208=http%3A//arxiv.org/abs/2507.18352v3&entry.124074799=Read"},
{"title": "Multiscale Vector-Quantized Variational Autoencoder for Endoscopic Image Synthesis", "author": "Dimitrios E. Diamantis and Dimitris K. Iakovidis", "abstract": "Gastrointestinal (GI) imaging via Wireless Capsule Endoscopy (WCE) generates a large number of images requiring manual screening. Deep learning-based Clinical Decision Support (CDS) systems can assist screening, yet their performance relies on the existence of large, diverse, training medical datasets. However, the scarcity of such data, due to privacy constraints and annotation costs, hinders CDS development. Generative machine learning offers a viable solution to combat this limitation. While current Synthetic Data Generation (SDG) methods, such as Generative Adversarial Networks and Variational Autoencoders have been explored, they often face challenges with training stability and capturing sufficient visual diversity, especially when synthesizing abnormal findings. This work introduces a novel VAE-based methodology for medical image synthesis and presents its application for the generation of WCE images. The novel contributions of this work include a) multiscale extension of the Vector Quantized VAE model, named as Multiscale Vector Quantized Variational Autoencoder (MSVQ-VAE); b) unlike other VAE-based SDG models for WCE image generation, MSVQ-VAE is used to seamlessly introduce abnormalities into normal WCE images; c) it enables conditional generation of synthetic images, enabling the introduction of different types of abnormalities into the normal WCE images; d) it performs experiments with a variety of abnormality types, including polyps, vascular and inflammatory conditions. The utility of the generated images for CDS is assessed via image classification. Comparative experiments demonstrate that training a CDS classifier using the abnormal images generated by the proposed methodology yield comparable results with a classifier trained with only real data. The generality of the proposed methodology promises its applicability to various domains related to medical multimedia.", "link": "http://arxiv.org/abs/2511.19578v2", "date": "2026-02-12", "relevancy": 2.7399, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5565}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5437}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiscale%20Vector-Quantized%20Variational%20Autoencoder%20for%20Endoscopic%20Image%20Synthesis&body=Title%3A%20Multiscale%20Vector-Quantized%20Variational%20Autoencoder%20for%20Endoscopic%20Image%20Synthesis%0AAuthor%3A%20Dimitrios%20E.%20Diamantis%20and%20Dimitris%20K.%20Iakovidis%0AAbstract%3A%20Gastrointestinal%20%28GI%29%20imaging%20via%20Wireless%20Capsule%20Endoscopy%20%28WCE%29%20generates%20a%20large%20number%20of%20images%20requiring%20manual%20screening.%20Deep%20learning-based%20Clinical%20Decision%20Support%20%28CDS%29%20systems%20can%20assist%20screening%2C%20yet%20their%20performance%20relies%20on%20the%20existence%20of%20large%2C%20diverse%2C%20training%20medical%20datasets.%20However%2C%20the%20scarcity%20of%20such%20data%2C%20due%20to%20privacy%20constraints%20and%20annotation%20costs%2C%20hinders%20CDS%20development.%20Generative%20machine%20learning%20offers%20a%20viable%20solution%20to%20combat%20this%20limitation.%20While%20current%20Synthetic%20Data%20Generation%20%28SDG%29%20methods%2C%20such%20as%20Generative%20Adversarial%20Networks%20and%20Variational%20Autoencoders%20have%20been%20explored%2C%20they%20often%20face%20challenges%20with%20training%20stability%20and%20capturing%20sufficient%20visual%20diversity%2C%20especially%20when%20synthesizing%20abnormal%20findings.%20This%20work%20introduces%20a%20novel%20VAE-based%20methodology%20for%20medical%20image%20synthesis%20and%20presents%20its%20application%20for%20the%20generation%20of%20WCE%20images.%20The%20novel%20contributions%20of%20this%20work%20include%20a%29%20multiscale%20extension%20of%20the%20Vector%20Quantized%20VAE%20model%2C%20named%20as%20Multiscale%20Vector%20Quantized%20Variational%20Autoencoder%20%28MSVQ-VAE%29%3B%20b%29%20unlike%20other%20VAE-based%20SDG%20models%20for%20WCE%20image%20generation%2C%20MSVQ-VAE%20is%20used%20to%20seamlessly%20introduce%20abnormalities%20into%20normal%20WCE%20images%3B%20c%29%20it%20enables%20conditional%20generation%20of%20synthetic%20images%2C%20enabling%20the%20introduction%20of%20different%20types%20of%20abnormalities%20into%20the%20normal%20WCE%20images%3B%20d%29%20it%20performs%20experiments%20with%20a%20variety%20of%20abnormality%20types%2C%20including%20polyps%2C%20vascular%20and%20inflammatory%20conditions.%20The%20utility%20of%20the%20generated%20images%20for%20CDS%20is%20assessed%20via%20image%20classification.%20Comparative%20experiments%20demonstrate%20that%20training%20a%20CDS%20classifier%20using%20the%20abnormal%20images%20generated%20by%20the%20proposed%20methodology%20yield%20comparable%20results%20with%20a%20classifier%20trained%20with%20only%20real%20data.%20The%20generality%20of%20the%20proposed%20methodology%20promises%20its%20applicability%20to%20various%20domains%20related%20to%20medical%20multimedia.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiscale%2520Vector-Quantized%2520Variational%2520Autoencoder%2520for%2520Endoscopic%2520Image%2520Synthesis%26entry.906535625%3DDimitrios%2520E.%2520Diamantis%2520and%2520Dimitris%2520K.%2520Iakovidis%26entry.1292438233%3DGastrointestinal%2520%2528GI%2529%2520imaging%2520via%2520Wireless%2520Capsule%2520Endoscopy%2520%2528WCE%2529%2520generates%2520a%2520large%2520number%2520of%2520images%2520requiring%2520manual%2520screening.%2520Deep%2520learning-based%2520Clinical%2520Decision%2520Support%2520%2528CDS%2529%2520systems%2520can%2520assist%2520screening%252C%2520yet%2520their%2520performance%2520relies%2520on%2520the%2520existence%2520of%2520large%252C%2520diverse%252C%2520training%2520medical%2520datasets.%2520However%252C%2520the%2520scarcity%2520of%2520such%2520data%252C%2520due%2520to%2520privacy%2520constraints%2520and%2520annotation%2520costs%252C%2520hinders%2520CDS%2520development.%2520Generative%2520machine%2520learning%2520offers%2520a%2520viable%2520solution%2520to%2520combat%2520this%2520limitation.%2520While%2520current%2520Synthetic%2520Data%2520Generation%2520%2528SDG%2529%2520methods%252C%2520such%2520as%2520Generative%2520Adversarial%2520Networks%2520and%2520Variational%2520Autoencoders%2520have%2520been%2520explored%252C%2520they%2520often%2520face%2520challenges%2520with%2520training%2520stability%2520and%2520capturing%2520sufficient%2520visual%2520diversity%252C%2520especially%2520when%2520synthesizing%2520abnormal%2520findings.%2520This%2520work%2520introduces%2520a%2520novel%2520VAE-based%2520methodology%2520for%2520medical%2520image%2520synthesis%2520and%2520presents%2520its%2520application%2520for%2520the%2520generation%2520of%2520WCE%2520images.%2520The%2520novel%2520contributions%2520of%2520this%2520work%2520include%2520a%2529%2520multiscale%2520extension%2520of%2520the%2520Vector%2520Quantized%2520VAE%2520model%252C%2520named%2520as%2520Multiscale%2520Vector%2520Quantized%2520Variational%2520Autoencoder%2520%2528MSVQ-VAE%2529%253B%2520b%2529%2520unlike%2520other%2520VAE-based%2520SDG%2520models%2520for%2520WCE%2520image%2520generation%252C%2520MSVQ-VAE%2520is%2520used%2520to%2520seamlessly%2520introduce%2520abnormalities%2520into%2520normal%2520WCE%2520images%253B%2520c%2529%2520it%2520enables%2520conditional%2520generation%2520of%2520synthetic%2520images%252C%2520enabling%2520the%2520introduction%2520of%2520different%2520types%2520of%2520abnormalities%2520into%2520the%2520normal%2520WCE%2520images%253B%2520d%2529%2520it%2520performs%2520experiments%2520with%2520a%2520variety%2520of%2520abnormality%2520types%252C%2520including%2520polyps%252C%2520vascular%2520and%2520inflammatory%2520conditions.%2520The%2520utility%2520of%2520the%2520generated%2520images%2520for%2520CDS%2520is%2520assessed%2520via%2520image%2520classification.%2520Comparative%2520experiments%2520demonstrate%2520that%2520training%2520a%2520CDS%2520classifier%2520using%2520the%2520abnormal%2520images%2520generated%2520by%2520the%2520proposed%2520methodology%2520yield%2520comparable%2520results%2520with%2520a%2520classifier%2520trained%2520with%2520only%2520real%2520data.%2520The%2520generality%2520of%2520the%2520proposed%2520methodology%2520promises%2520its%2520applicability%2520to%2520various%2520domains%2520related%2520to%2520medical%2520multimedia.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiscale%20Vector-Quantized%20Variational%20Autoencoder%20for%20Endoscopic%20Image%20Synthesis&entry.906535625=Dimitrios%20E.%20Diamantis%20and%20Dimitris%20K.%20Iakovidis&entry.1292438233=Gastrointestinal%20%28GI%29%20imaging%20via%20Wireless%20Capsule%20Endoscopy%20%28WCE%29%20generates%20a%20large%20number%20of%20images%20requiring%20manual%20screening.%20Deep%20learning-based%20Clinical%20Decision%20Support%20%28CDS%29%20systems%20can%20assist%20screening%2C%20yet%20their%20performance%20relies%20on%20the%20existence%20of%20large%2C%20diverse%2C%20training%20medical%20datasets.%20However%2C%20the%20scarcity%20of%20such%20data%2C%20due%20to%20privacy%20constraints%20and%20annotation%20costs%2C%20hinders%20CDS%20development.%20Generative%20machine%20learning%20offers%20a%20viable%20solution%20to%20combat%20this%20limitation.%20While%20current%20Synthetic%20Data%20Generation%20%28SDG%29%20methods%2C%20such%20as%20Generative%20Adversarial%20Networks%20and%20Variational%20Autoencoders%20have%20been%20explored%2C%20they%20often%20face%20challenges%20with%20training%20stability%20and%20capturing%20sufficient%20visual%20diversity%2C%20especially%20when%20synthesizing%20abnormal%20findings.%20This%20work%20introduces%20a%20novel%20VAE-based%20methodology%20for%20medical%20image%20synthesis%20and%20presents%20its%20application%20for%20the%20generation%20of%20WCE%20images.%20The%20novel%20contributions%20of%20this%20work%20include%20a%29%20multiscale%20extension%20of%20the%20Vector%20Quantized%20VAE%20model%2C%20named%20as%20Multiscale%20Vector%20Quantized%20Variational%20Autoencoder%20%28MSVQ-VAE%29%3B%20b%29%20unlike%20other%20VAE-based%20SDG%20models%20for%20WCE%20image%20generation%2C%20MSVQ-VAE%20is%20used%20to%20seamlessly%20introduce%20abnormalities%20into%20normal%20WCE%20images%3B%20c%29%20it%20enables%20conditional%20generation%20of%20synthetic%20images%2C%20enabling%20the%20introduction%20of%20different%20types%20of%20abnormalities%20into%20the%20normal%20WCE%20images%3B%20d%29%20it%20performs%20experiments%20with%20a%20variety%20of%20abnormality%20types%2C%20including%20polyps%2C%20vascular%20and%20inflammatory%20conditions.%20The%20utility%20of%20the%20generated%20images%20for%20CDS%20is%20assessed%20via%20image%20classification.%20Comparative%20experiments%20demonstrate%20that%20training%20a%20CDS%20classifier%20using%20the%20abnormal%20images%20generated%20by%20the%20proposed%20methodology%20yield%20comparable%20results%20with%20a%20classifier%20trained%20with%20only%20real%20data.%20The%20generality%20of%20the%20proposed%20methodology%20promises%20its%20applicability%20to%20various%20domains%20related%20to%20medical%20multimedia.&entry.1838667208=http%3A//arxiv.org/abs/2511.19578v2&entry.124074799=Read"},
{"title": "Iskra: A System for Inverse Geometry Processing", "author": "Ana Dodik and Ahmed H. Mahmoud and Justin Solomon", "abstract": "We propose a system for differentiating through solutions to geometry processing problems. Our system differentiates a broad class of geometric algorithms, exploiting existing fast problem-specific schemes common to geometry processing, including local-global and ADMM solvers. It is compatible with machine learning frameworks, opening doors to new classes of inverse geometry processing applications. We marry the scatter-gather approach to mesh processing with tensor-based workflows and rely on the adjoint method applied to user-specified imperative code to generate an efficient backward pass behind the scenes. We demonstrate our approach by differentiating through mean curvature flow, spectral conformal parameterization, geodesic distance computation, and as-rigid-as-possible deformation, examining usability and performance on these applications. Our system allows practitioners to differentiate through existing geometry processing algorithms without needing to reformulate them, resulting in low implementation effort, fast runtimes, and lower memory requirements than differentiable optimization tools not tailored to geometry processing.", "link": "http://arxiv.org/abs/2602.12105v1", "date": "2026-02-12", "relevancy": 2.7079, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5437}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5419}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iskra%3A%20A%20System%20for%20Inverse%20Geometry%20Processing&body=Title%3A%20Iskra%3A%20A%20System%20for%20Inverse%20Geometry%20Processing%0AAuthor%3A%20Ana%20Dodik%20and%20Ahmed%20H.%20Mahmoud%20and%20Justin%20Solomon%0AAbstract%3A%20We%20propose%20a%20system%20for%20differentiating%20through%20solutions%20to%20geometry%20processing%20problems.%20Our%20system%20differentiates%20a%20broad%20class%20of%20geometric%20algorithms%2C%20exploiting%20existing%20fast%20problem-specific%20schemes%20common%20to%20geometry%20processing%2C%20including%20local-global%20and%20ADMM%20solvers.%20It%20is%20compatible%20with%20machine%20learning%20frameworks%2C%20opening%20doors%20to%20new%20classes%20of%20inverse%20geometry%20processing%20applications.%20We%20marry%20the%20scatter-gather%20approach%20to%20mesh%20processing%20with%20tensor-based%20workflows%20and%20rely%20on%20the%20adjoint%20method%20applied%20to%20user-specified%20imperative%20code%20to%20generate%20an%20efficient%20backward%20pass%20behind%20the%20scenes.%20We%20demonstrate%20our%20approach%20by%20differentiating%20through%20mean%20curvature%20flow%2C%20spectral%20conformal%20parameterization%2C%20geodesic%20distance%20computation%2C%20and%20as-rigid-as-possible%20deformation%2C%20examining%20usability%20and%20performance%20on%20these%20applications.%20Our%20system%20allows%20practitioners%20to%20differentiate%20through%20existing%20geometry%20processing%20algorithms%20without%20needing%20to%20reformulate%20them%2C%20resulting%20in%20low%20implementation%20effort%2C%20fast%20runtimes%2C%20and%20lower%20memory%20requirements%20than%20differentiable%20optimization%20tools%20not%20tailored%20to%20geometry%20processing.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIskra%253A%2520A%2520System%2520for%2520Inverse%2520Geometry%2520Processing%26entry.906535625%3DAna%2520Dodik%2520and%2520Ahmed%2520H.%2520Mahmoud%2520and%2520Justin%2520Solomon%26entry.1292438233%3DWe%2520propose%2520a%2520system%2520for%2520differentiating%2520through%2520solutions%2520to%2520geometry%2520processing%2520problems.%2520Our%2520system%2520differentiates%2520a%2520broad%2520class%2520of%2520geometric%2520algorithms%252C%2520exploiting%2520existing%2520fast%2520problem-specific%2520schemes%2520common%2520to%2520geometry%2520processing%252C%2520including%2520local-global%2520and%2520ADMM%2520solvers.%2520It%2520is%2520compatible%2520with%2520machine%2520learning%2520frameworks%252C%2520opening%2520doors%2520to%2520new%2520classes%2520of%2520inverse%2520geometry%2520processing%2520applications.%2520We%2520marry%2520the%2520scatter-gather%2520approach%2520to%2520mesh%2520processing%2520with%2520tensor-based%2520workflows%2520and%2520rely%2520on%2520the%2520adjoint%2520method%2520applied%2520to%2520user-specified%2520imperative%2520code%2520to%2520generate%2520an%2520efficient%2520backward%2520pass%2520behind%2520the%2520scenes.%2520We%2520demonstrate%2520our%2520approach%2520by%2520differentiating%2520through%2520mean%2520curvature%2520flow%252C%2520spectral%2520conformal%2520parameterization%252C%2520geodesic%2520distance%2520computation%252C%2520and%2520as-rigid-as-possible%2520deformation%252C%2520examining%2520usability%2520and%2520performance%2520on%2520these%2520applications.%2520Our%2520system%2520allows%2520practitioners%2520to%2520differentiate%2520through%2520existing%2520geometry%2520processing%2520algorithms%2520without%2520needing%2520to%2520reformulate%2520them%252C%2520resulting%2520in%2520low%2520implementation%2520effort%252C%2520fast%2520runtimes%252C%2520and%2520lower%2520memory%2520requirements%2520than%2520differentiable%2520optimization%2520tools%2520not%2520tailored%2520to%2520geometry%2520processing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iskra%3A%20A%20System%20for%20Inverse%20Geometry%20Processing&entry.906535625=Ana%20Dodik%20and%20Ahmed%20H.%20Mahmoud%20and%20Justin%20Solomon&entry.1292438233=We%20propose%20a%20system%20for%20differentiating%20through%20solutions%20to%20geometry%20processing%20problems.%20Our%20system%20differentiates%20a%20broad%20class%20of%20geometric%20algorithms%2C%20exploiting%20existing%20fast%20problem-specific%20schemes%20common%20to%20geometry%20processing%2C%20including%20local-global%20and%20ADMM%20solvers.%20It%20is%20compatible%20with%20machine%20learning%20frameworks%2C%20opening%20doors%20to%20new%20classes%20of%20inverse%20geometry%20processing%20applications.%20We%20marry%20the%20scatter-gather%20approach%20to%20mesh%20processing%20with%20tensor-based%20workflows%20and%20rely%20on%20the%20adjoint%20method%20applied%20to%20user-specified%20imperative%20code%20to%20generate%20an%20efficient%20backward%20pass%20behind%20the%20scenes.%20We%20demonstrate%20our%20approach%20by%20differentiating%20through%20mean%20curvature%20flow%2C%20spectral%20conformal%20parameterization%2C%20geodesic%20distance%20computation%2C%20and%20as-rigid-as-possible%20deformation%2C%20examining%20usability%20and%20performance%20on%20these%20applications.%20Our%20system%20allows%20practitioners%20to%20differentiate%20through%20existing%20geometry%20processing%20algorithms%20without%20needing%20to%20reformulate%20them%2C%20resulting%20in%20low%20implementation%20effort%2C%20fast%20runtimes%2C%20and%20lower%20memory%20requirements%20than%20differentiable%20optimization%20tools%20not%20tailored%20to%20geometry%20processing.&entry.1838667208=http%3A//arxiv.org/abs/2602.12105v1&entry.124074799=Read"},
{"title": "EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data", "author": "Nils Lehmann and Yi Wang and Zhitong Xiong and Xiaoxiang Zhu", "abstract": "State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.", "link": "http://arxiv.org/abs/2602.12177v1", "date": "2026-02-12", "relevancy": 2.7017, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5489}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5452}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EO-VAE%3A%20Towards%20A%20Multi-sensor%20Tokenizer%20for%20Earth%20Observation%20Data&body=Title%3A%20EO-VAE%3A%20Towards%20A%20Multi-sensor%20Tokenizer%20for%20Earth%20Observation%20Data%0AAuthor%3A%20Nils%20Lehmann%20and%20Yi%20Wang%20and%20Zhitong%20Xiong%20and%20Xiaoxiang%20Zhu%0AAbstract%3A%20State-of-the-art%20generative%20image%20and%20video%20models%20rely%20heavily%20on%20tokenizers%20that%20compress%20high-dimensional%20inputs%20into%20more%20efficient%20latent%20representations.%20While%20this%20paradigm%20has%20revolutionized%20RGB%20generation%2C%20Earth%20observation%20%28EO%29%20data%20presents%20unique%20challenges%20due%20to%20diverse%20sensor%20specifications%20and%20variable%20spectral%20channels.%20We%20propose%20EO-VAE%2C%20a%20multi-sensor%20variational%20autoencoder%20designed%20to%20serve%20as%20a%20foundational%20tokenizer%20for%20the%20EO%20domain.%20Unlike%20prior%20approaches%20that%20train%20separate%20tokenizers%20for%20each%20modality%2C%20EO-VAE%20utilizes%20a%20single%20model%20to%20encode%20and%20reconstruct%20flexible%20channel%20combinations%20via%20dynamic%20hypernetworks.%20Our%20experiments%20on%20the%20TerraMesh%20dataset%20demonstrate%20that%20EO-VAE%20achieves%20superior%20reconstruction%20fidelity%20compared%20to%20the%20TerraMind%20tokenizers%2C%20establishing%20a%20robust%20baseline%20for%20latent%20generative%20modeling%20in%20remote%20sensing.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEO-VAE%253A%2520Towards%2520A%2520Multi-sensor%2520Tokenizer%2520for%2520Earth%2520Observation%2520Data%26entry.906535625%3DNils%2520Lehmann%2520and%2520Yi%2520Wang%2520and%2520Zhitong%2520Xiong%2520and%2520Xiaoxiang%2520Zhu%26entry.1292438233%3DState-of-the-art%2520generative%2520image%2520and%2520video%2520models%2520rely%2520heavily%2520on%2520tokenizers%2520that%2520compress%2520high-dimensional%2520inputs%2520into%2520more%2520efficient%2520latent%2520representations.%2520While%2520this%2520paradigm%2520has%2520revolutionized%2520RGB%2520generation%252C%2520Earth%2520observation%2520%2528EO%2529%2520data%2520presents%2520unique%2520challenges%2520due%2520to%2520diverse%2520sensor%2520specifications%2520and%2520variable%2520spectral%2520channels.%2520We%2520propose%2520EO-VAE%252C%2520a%2520multi-sensor%2520variational%2520autoencoder%2520designed%2520to%2520serve%2520as%2520a%2520foundational%2520tokenizer%2520for%2520the%2520EO%2520domain.%2520Unlike%2520prior%2520approaches%2520that%2520train%2520separate%2520tokenizers%2520for%2520each%2520modality%252C%2520EO-VAE%2520utilizes%2520a%2520single%2520model%2520to%2520encode%2520and%2520reconstruct%2520flexible%2520channel%2520combinations%2520via%2520dynamic%2520hypernetworks.%2520Our%2520experiments%2520on%2520the%2520TerraMesh%2520dataset%2520demonstrate%2520that%2520EO-VAE%2520achieves%2520superior%2520reconstruction%2520fidelity%2520compared%2520to%2520the%2520TerraMind%2520tokenizers%252C%2520establishing%2520a%2520robust%2520baseline%2520for%2520latent%2520generative%2520modeling%2520in%2520remote%2520sensing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EO-VAE%3A%20Towards%20A%20Multi-sensor%20Tokenizer%20for%20Earth%20Observation%20Data&entry.906535625=Nils%20Lehmann%20and%20Yi%20Wang%20and%20Zhitong%20Xiong%20and%20Xiaoxiang%20Zhu&entry.1292438233=State-of-the-art%20generative%20image%20and%20video%20models%20rely%20heavily%20on%20tokenizers%20that%20compress%20high-dimensional%20inputs%20into%20more%20efficient%20latent%20representations.%20While%20this%20paradigm%20has%20revolutionized%20RGB%20generation%2C%20Earth%20observation%20%28EO%29%20data%20presents%20unique%20challenges%20due%20to%20diverse%20sensor%20specifications%20and%20variable%20spectral%20channels.%20We%20propose%20EO-VAE%2C%20a%20multi-sensor%20variational%20autoencoder%20designed%20to%20serve%20as%20a%20foundational%20tokenizer%20for%20the%20EO%20domain.%20Unlike%20prior%20approaches%20that%20train%20separate%20tokenizers%20for%20each%20modality%2C%20EO-VAE%20utilizes%20a%20single%20model%20to%20encode%20and%20reconstruct%20flexible%20channel%20combinations%20via%20dynamic%20hypernetworks.%20Our%20experiments%20on%20the%20TerraMesh%20dataset%20demonstrate%20that%20EO-VAE%20achieves%20superior%20reconstruction%20fidelity%20compared%20to%20the%20TerraMind%20tokenizers%2C%20establishing%20a%20robust%20baseline%20for%20latent%20generative%20modeling%20in%20remote%20sensing.&entry.1838667208=http%3A//arxiv.org/abs/2602.12177v1&entry.124074799=Read"},
{"title": "DynaHOI: Benchmarking Hand-Object Interaction for Dynamic Target", "author": "BoCheng Hu and Zhonghan Zhao and Kaiyue Zhou and Hongwei Wang and Gaoang Wang", "abstract": "Most existing hand motion generation benchmarks for hand-object interaction (HOI) focus on static objects, leaving dynamic scenarios with moving targets and time-critical coordination largely untested. To address this gap, we introduce the DynaHOI-Gym, a unified online closed-loop platform with parameterized motion generators and rollout-based metrics for dynamic capture evaluation. Built on DynaHOI-Gym, we release DynaHOI-10M, a large-scale benchmark with 10M frames and 180K hand capture trajectories, whose target motions are organized into 8 major categories and 22 fine-grained subcategories. We also provide a simple observe-before-act baseline (ObAct) that integrates short-term observations with the current frame via spatiotemporal attention to predict actions, achieving an 8.1% improvement in location success rate.", "link": "http://arxiv.org/abs/2602.11919v1", "date": "2026-02-12", "relevancy": 2.6759, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.528}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaHOI%3A%20Benchmarking%20Hand-Object%20Interaction%20for%20Dynamic%20Target&body=Title%3A%20DynaHOI%3A%20Benchmarking%20Hand-Object%20Interaction%20for%20Dynamic%20Target%0AAuthor%3A%20BoCheng%20Hu%20and%20Zhonghan%20Zhao%20and%20Kaiyue%20Zhou%20and%20Hongwei%20Wang%20and%20Gaoang%20Wang%0AAbstract%3A%20Most%20existing%20hand%20motion%20generation%20benchmarks%20for%20hand-object%20interaction%20%28HOI%29%20focus%20on%20static%20objects%2C%20leaving%20dynamic%20scenarios%20with%20moving%20targets%20and%20time-critical%20coordination%20largely%20untested.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20DynaHOI-Gym%2C%20a%20unified%20online%20closed-loop%20platform%20with%20parameterized%20motion%20generators%20and%20rollout-based%20metrics%20for%20dynamic%20capture%20evaluation.%20Built%20on%20DynaHOI-Gym%2C%20we%20release%20DynaHOI-10M%2C%20a%20large-scale%20benchmark%20with%2010M%20frames%20and%20180K%20hand%20capture%20trajectories%2C%20whose%20target%20motions%20are%20organized%20into%208%20major%20categories%20and%2022%20fine-grained%20subcategories.%20We%20also%20provide%20a%20simple%20observe-before-act%20baseline%20%28ObAct%29%20that%20integrates%20short-term%20observations%20with%20the%20current%20frame%20via%20spatiotemporal%20attention%20to%20predict%20actions%2C%20achieving%20an%208.1%25%20improvement%20in%20location%20success%20rate.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaHOI%253A%2520Benchmarking%2520Hand-Object%2520Interaction%2520for%2520Dynamic%2520Target%26entry.906535625%3DBoCheng%2520Hu%2520and%2520Zhonghan%2520Zhao%2520and%2520Kaiyue%2520Zhou%2520and%2520Hongwei%2520Wang%2520and%2520Gaoang%2520Wang%26entry.1292438233%3DMost%2520existing%2520hand%2520motion%2520generation%2520benchmarks%2520for%2520hand-object%2520interaction%2520%2528HOI%2529%2520focus%2520on%2520static%2520objects%252C%2520leaving%2520dynamic%2520scenarios%2520with%2520moving%2520targets%2520and%2520time-critical%2520coordination%2520largely%2520untested.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520DynaHOI-Gym%252C%2520a%2520unified%2520online%2520closed-loop%2520platform%2520with%2520parameterized%2520motion%2520generators%2520and%2520rollout-based%2520metrics%2520for%2520dynamic%2520capture%2520evaluation.%2520Built%2520on%2520DynaHOI-Gym%252C%2520we%2520release%2520DynaHOI-10M%252C%2520a%2520large-scale%2520benchmark%2520with%252010M%2520frames%2520and%2520180K%2520hand%2520capture%2520trajectories%252C%2520whose%2520target%2520motions%2520are%2520organized%2520into%25208%2520major%2520categories%2520and%252022%2520fine-grained%2520subcategories.%2520We%2520also%2520provide%2520a%2520simple%2520observe-before-act%2520baseline%2520%2528ObAct%2529%2520that%2520integrates%2520short-term%2520observations%2520with%2520the%2520current%2520frame%2520via%2520spatiotemporal%2520attention%2520to%2520predict%2520actions%252C%2520achieving%2520an%25208.1%2525%2520improvement%2520in%2520location%2520success%2520rate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaHOI%3A%20Benchmarking%20Hand-Object%20Interaction%20for%20Dynamic%20Target&entry.906535625=BoCheng%20Hu%20and%20Zhonghan%20Zhao%20and%20Kaiyue%20Zhou%20and%20Hongwei%20Wang%20and%20Gaoang%20Wang&entry.1292438233=Most%20existing%20hand%20motion%20generation%20benchmarks%20for%20hand-object%20interaction%20%28HOI%29%20focus%20on%20static%20objects%2C%20leaving%20dynamic%20scenarios%20with%20moving%20targets%20and%20time-critical%20coordination%20largely%20untested.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20DynaHOI-Gym%2C%20a%20unified%20online%20closed-loop%20platform%20with%20parameterized%20motion%20generators%20and%20rollout-based%20metrics%20for%20dynamic%20capture%20evaluation.%20Built%20on%20DynaHOI-Gym%2C%20we%20release%20DynaHOI-10M%2C%20a%20large-scale%20benchmark%20with%2010M%20frames%20and%20180K%20hand%20capture%20trajectories%2C%20whose%20target%20motions%20are%20organized%20into%208%20major%20categories%20and%2022%20fine-grained%20subcategories.%20We%20also%20provide%20a%20simple%20observe-before-act%20baseline%20%28ObAct%29%20that%20integrates%20short-term%20observations%20with%20the%20current%20frame%20via%20spatiotemporal%20attention%20to%20predict%20actions%2C%20achieving%20an%208.1%25%20improvement%20in%20location%20success%20rate.&entry.1838667208=http%3A//arxiv.org/abs/2602.11919v1&entry.124074799=Read"},
{"title": "From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders", "author": "Yifan Luo and Yang Zhan and Jiedong Jiang and Tianyang Liu and Mingrui Wu and Zhennan Zhou and Bin Dong", "abstract": "Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of \"feature splitting\" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.", "link": "http://arxiv.org/abs/2602.11881v1", "date": "2026-02-12", "relevancy": 2.6331, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Atoms%20to%20Trees%3A%20Building%20a%20Structured%20Feature%20Forest%20with%20Hierarchical%20Sparse%20Autoencoders&body=Title%3A%20From%20Atoms%20to%20Trees%3A%20Building%20a%20Structured%20Feature%20Forest%20with%20Hierarchical%20Sparse%20Autoencoders%0AAuthor%3A%20Yifan%20Luo%20and%20Yang%20Zhan%20and%20Jiedong%20Jiang%20and%20Tianyang%20Liu%20and%20Mingrui%20Wu%20and%20Zhennan%20Zhou%20and%20Bin%20Dong%0AAbstract%3A%20Sparse%20autoencoders%20%28SAEs%29%20have%20proven%20effective%20for%20extracting%20monosemantic%20features%20from%20large%20language%20models%20%28LLMs%29%2C%20yet%20these%20features%20are%20typically%20identified%20in%20isolation.%20However%2C%20broad%20evidence%20suggests%20that%20LLMs%20capture%20the%20intrinsic%20structure%20of%20natural%20language%2C%20where%20the%20phenomenon%20of%20%22feature%20splitting%22%20in%20particular%20indicates%20that%20such%20structure%20is%20hierarchical.%20To%20capture%20this%2C%20we%20propose%20the%20Hierarchical%20Sparse%20Autoencoder%20%28HSAE%29%2C%20which%20jointly%20learns%20a%20series%20of%20SAEs%20and%20the%20parent-child%20relationships%20between%20their%20features.%20HSAE%20strengthens%20the%20alignment%20between%20parent%20and%20child%20features%20through%20two%20novel%20mechanisms%3A%20a%20structural%20constraint%20loss%20and%20a%20random%20feature%20perturbation%20mechanism.%20Extensive%20experiments%20across%20various%20LLMs%20and%20layers%20demonstrate%20that%20HSAE%20consistently%20recovers%20semantically%20meaningful%20hierarchies%2C%20supported%20by%20both%20qualitative%20case%20studies%20and%20rigorous%20quantitative%20metrics.%20At%20the%20same%20time%2C%20HSAE%20preserves%20the%20reconstruction%20fidelity%20and%20interpretability%20of%20standard%20SAEs%20across%20different%20dictionary%20sizes.%20Our%20work%20provides%20a%20powerful%2C%20scalable%20tool%20for%20discovering%20and%20analyzing%20the%20multi-scale%20conceptual%20structures%20embedded%20in%20LLM%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Atoms%2520to%2520Trees%253A%2520Building%2520a%2520Structured%2520Feature%2520Forest%2520with%2520Hierarchical%2520Sparse%2520Autoencoders%26entry.906535625%3DYifan%2520Luo%2520and%2520Yang%2520Zhan%2520and%2520Jiedong%2520Jiang%2520and%2520Tianyang%2520Liu%2520and%2520Mingrui%2520Wu%2520and%2520Zhennan%2520Zhou%2520and%2520Bin%2520Dong%26entry.1292438233%3DSparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520proven%2520effective%2520for%2520extracting%2520monosemantic%2520features%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%2520these%2520features%2520are%2520typically%2520identified%2520in%2520isolation.%2520However%252C%2520broad%2520evidence%2520suggests%2520that%2520LLMs%2520capture%2520the%2520intrinsic%2520structure%2520of%2520natural%2520language%252C%2520where%2520the%2520phenomenon%2520of%2520%2522feature%2520splitting%2522%2520in%2520particular%2520indicates%2520that%2520such%2520structure%2520is%2520hierarchical.%2520To%2520capture%2520this%252C%2520we%2520propose%2520the%2520Hierarchical%2520Sparse%2520Autoencoder%2520%2528HSAE%2529%252C%2520which%2520jointly%2520learns%2520a%2520series%2520of%2520SAEs%2520and%2520the%2520parent-child%2520relationships%2520between%2520their%2520features.%2520HSAE%2520strengthens%2520the%2520alignment%2520between%2520parent%2520and%2520child%2520features%2520through%2520two%2520novel%2520mechanisms%253A%2520a%2520structural%2520constraint%2520loss%2520and%2520a%2520random%2520feature%2520perturbation%2520mechanism.%2520Extensive%2520experiments%2520across%2520various%2520LLMs%2520and%2520layers%2520demonstrate%2520that%2520HSAE%2520consistently%2520recovers%2520semantically%2520meaningful%2520hierarchies%252C%2520supported%2520by%2520both%2520qualitative%2520case%2520studies%2520and%2520rigorous%2520quantitative%2520metrics.%2520At%2520the%2520same%2520time%252C%2520HSAE%2520preserves%2520the%2520reconstruction%2520fidelity%2520and%2520interpretability%2520of%2520standard%2520SAEs%2520across%2520different%2520dictionary%2520sizes.%2520Our%2520work%2520provides%2520a%2520powerful%252C%2520scalable%2520tool%2520for%2520discovering%2520and%2520analyzing%2520the%2520multi-scale%2520conceptual%2520structures%2520embedded%2520in%2520LLM%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Atoms%20to%20Trees%3A%20Building%20a%20Structured%20Feature%20Forest%20with%20Hierarchical%20Sparse%20Autoencoders&entry.906535625=Yifan%20Luo%20and%20Yang%20Zhan%20and%20Jiedong%20Jiang%20and%20Tianyang%20Liu%20and%20Mingrui%20Wu%20and%20Zhennan%20Zhou%20and%20Bin%20Dong&entry.1292438233=Sparse%20autoencoders%20%28SAEs%29%20have%20proven%20effective%20for%20extracting%20monosemantic%20features%20from%20large%20language%20models%20%28LLMs%29%2C%20yet%20these%20features%20are%20typically%20identified%20in%20isolation.%20However%2C%20broad%20evidence%20suggests%20that%20LLMs%20capture%20the%20intrinsic%20structure%20of%20natural%20language%2C%20where%20the%20phenomenon%20of%20%22feature%20splitting%22%20in%20particular%20indicates%20that%20such%20structure%20is%20hierarchical.%20To%20capture%20this%2C%20we%20propose%20the%20Hierarchical%20Sparse%20Autoencoder%20%28HSAE%29%2C%20which%20jointly%20learns%20a%20series%20of%20SAEs%20and%20the%20parent-child%20relationships%20between%20their%20features.%20HSAE%20strengthens%20the%20alignment%20between%20parent%20and%20child%20features%20through%20two%20novel%20mechanisms%3A%20a%20structural%20constraint%20loss%20and%20a%20random%20feature%20perturbation%20mechanism.%20Extensive%20experiments%20across%20various%20LLMs%20and%20layers%20demonstrate%20that%20HSAE%20consistently%20recovers%20semantically%20meaningful%20hierarchies%2C%20supported%20by%20both%20qualitative%20case%20studies%20and%20rigorous%20quantitative%20metrics.%20At%20the%20same%20time%2C%20HSAE%20preserves%20the%20reconstruction%20fidelity%20and%20interpretability%20of%20standard%20SAEs%20across%20different%20dictionary%20sizes.%20Our%20work%20provides%20a%20powerful%2C%20scalable%20tool%20for%20discovering%20and%20analyzing%20the%20multi-scale%20conceptual%20structures%20embedded%20in%20LLM%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2602.11881v1&entry.124074799=Read"},
{"title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision", "author": "Xiaohan He and Shiyang Feng and Songtao Huang and Lei Bai and Bin Wang and Bo Zhang", "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.", "link": "http://arxiv.org/abs/2602.12164v1", "date": "2026-02-12", "relevancy": 2.6047, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sci-CoE%3A%20Co-evolving%20Scientific%20Reasoning%20LLMs%20via%20Geometric%20Consensus%20with%20Sparse%20Supervision&body=Title%3A%20Sci-CoE%3A%20Co-evolving%20Scientific%20Reasoning%20LLMs%20via%20Geometric%20Consensus%20with%20Sparse%20Supervision%0AAuthor%3A%20Xiaohan%20He%20and%20Shiyang%20Feng%20and%20Songtao%20Huang%20and%20Lei%20Bai%20and%20Bin%20Wang%20and%20Bo%20Zhang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20reasoning%20capabilities%2C%20and%20co-evolving%20paradigms%20have%20shown%20promising%20results%20in%20domains%20such%20as%20code%20and%20math.%20However%2C%20in%20scientific%20reasoning%20tasks%2C%20these%20models%20remain%20fragile%20due%20to%20unreliable%20solution%20evaluation%20and%20limited%20diversity%20in%20verification%20strategies.%20In%20this%20work%2C%20we%20propose%20Sci-CoE%2C%20a%20two-stage%20scientific%20co-evolving%20framework%20that%20enables%20models%20to%20self-evolve%20as%20both%20solver%20and%20verifier%20through%20a%20transition%20from%20sparse%20supervision%20to%20unsupervised%20learning.%20In%20the%20first%20stage%2C%20the%20model%20uses%20a%20small%20set%20of%20annotated%20data%20to%20establish%20fundamental%20correctness%20judgment%20anchors%20for%20the%20Verifier.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20geometric%20reward%20mechanism%20that%20jointly%20considers%20consensus%2C%20reliability%2C%20and%20diversity%2C%20driving%20large-scale%20self-iteration%20on%20unlabeled%20data.%20Experiments%20on%20several%20general%20scientific%20benchmarks%20demonstrate%20that%20Sci-CoE%20enhances%20complex%20reasoning%20capabilities%20and%20exhibits%20strong%20scalability%2C%20facilitating%20the%20construction%20of%20more%20robust%20and%20diverse%20evaluation%20systems.%20Codes%20are%20available%20at%20https%3A//github.com/InternScience/Sci-CoE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSci-CoE%253A%2520Co-evolving%2520Scientific%2520Reasoning%2520LLMs%2520via%2520Geometric%2520Consensus%2520with%2520Sparse%2520Supervision%26entry.906535625%3DXiaohan%2520He%2520and%2520Shiyang%2520Feng%2520and%2520Songtao%2520Huang%2520and%2520Lei%2520Bai%2520and%2520Bin%2520Wang%2520and%2520Bo%2520Zhang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%2520reasoning%2520capabilities%252C%2520and%2520co-evolving%2520paradigms%2520have%2520shown%2520promising%2520results%2520in%2520domains%2520such%2520as%2520code%2520and%2520math.%2520However%252C%2520in%2520scientific%2520reasoning%2520tasks%252C%2520these%2520models%2520remain%2520fragile%2520due%2520to%2520unreliable%2520solution%2520evaluation%2520and%2520limited%2520diversity%2520in%2520verification%2520strategies.%2520In%2520this%2520work%252C%2520we%2520propose%2520Sci-CoE%252C%2520a%2520two-stage%2520scientific%2520co-evolving%2520framework%2520that%2520enables%2520models%2520to%2520self-evolve%2520as%2520both%2520solver%2520and%2520verifier%2520through%2520a%2520transition%2520from%2520sparse%2520supervision%2520to%2520unsupervised%2520learning.%2520In%2520the%2520first%2520stage%252C%2520the%2520model%2520uses%2520a%2520small%2520set%2520of%2520annotated%2520data%2520to%2520establish%2520fundamental%2520correctness%2520judgment%2520anchors%2520for%2520the%2520Verifier.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%2520geometric%2520reward%2520mechanism%2520that%2520jointly%2520considers%2520consensus%252C%2520reliability%252C%2520and%2520diversity%252C%2520driving%2520large-scale%2520self-iteration%2520on%2520unlabeled%2520data.%2520Experiments%2520on%2520several%2520general%2520scientific%2520benchmarks%2520demonstrate%2520that%2520Sci-CoE%2520enhances%2520complex%2520reasoning%2520capabilities%2520and%2520exhibits%2520strong%2520scalability%252C%2520facilitating%2520the%2520construction%2520of%2520more%2520robust%2520and%2520diverse%2520evaluation%2520systems.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/InternScience/Sci-CoE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sci-CoE%3A%20Co-evolving%20Scientific%20Reasoning%20LLMs%20via%20Geometric%20Consensus%20with%20Sparse%20Supervision&entry.906535625=Xiaohan%20He%20and%20Shiyang%20Feng%20and%20Songtao%20Huang%20and%20Lei%20Bai%20and%20Bin%20Wang%20and%20Bo%20Zhang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20reasoning%20capabilities%2C%20and%20co-evolving%20paradigms%20have%20shown%20promising%20results%20in%20domains%20such%20as%20code%20and%20math.%20However%2C%20in%20scientific%20reasoning%20tasks%2C%20these%20models%20remain%20fragile%20due%20to%20unreliable%20solution%20evaluation%20and%20limited%20diversity%20in%20verification%20strategies.%20In%20this%20work%2C%20we%20propose%20Sci-CoE%2C%20a%20two-stage%20scientific%20co-evolving%20framework%20that%20enables%20models%20to%20self-evolve%20as%20both%20solver%20and%20verifier%20through%20a%20transition%20from%20sparse%20supervision%20to%20unsupervised%20learning.%20In%20the%20first%20stage%2C%20the%20model%20uses%20a%20small%20set%20of%20annotated%20data%20to%20establish%20fundamental%20correctness%20judgment%20anchors%20for%20the%20Verifier.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20geometric%20reward%20mechanism%20that%20jointly%20considers%20consensus%2C%20reliability%2C%20and%20diversity%2C%20driving%20large-scale%20self-iteration%20on%20unlabeled%20data.%20Experiments%20on%20several%20general%20scientific%20benchmarks%20demonstrate%20that%20Sci-CoE%20enhances%20complex%20reasoning%20capabilities%20and%20exhibits%20strong%20scalability%2C%20facilitating%20the%20construction%20of%20more%20robust%20and%20diverse%20evaluation%20systems.%20Codes%20are%20available%20at%20https%3A//github.com/InternScience/Sci-CoE.&entry.1838667208=http%3A//arxiv.org/abs/2602.12164v1&entry.124074799=Read"},
{"title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding", "author": "I\u00f1igo Alonso and Imanol Miranda and Eneko Agirre and Mirella Lapata", "abstract": "While table understanding increasingly relies on pixel-only settings, current benchmarks predominantly use synthetic renderings that lack the complexity and visual diversity of real-world tables. Additionally, existing visual table understanding (VTU) datasets offer fixed examples with single visualizations and pre-defined instructions, providing no access to underlying serialized data for reformulation. We introduce TABLET, a large-scale VTU dataset with 4 million examples across 21 tasks, grounded in 2 million unique tables where 88% preserve original visualizations. To evaluate whether models are able to jointly reason over tabular and visual content, we also introduce VisualTableQA, a benchmark requiring both visual perception and table understanding. Fine-tuning vision-language models like Qwen2.5-VL-7B and Gemma 3-4B on TABLET improves performance on seen and unseen VTU tasks while increasing robustness on real-world table visualizations. By preserving original visualizations and maintaining example traceability in a unified large-scale collection, TABLET establishes a foundation for robust training and extensible evaluation of future VTU models.", "link": "http://arxiv.org/abs/2509.21205v3", "date": "2026-02-12", "relevancy": 2.5902, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding&body=Title%3A%20TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding%0AAuthor%3A%20I%C3%B1igo%20Alonso%20and%20Imanol%20Miranda%20and%20Eneko%20Agirre%20and%20Mirella%20Lapata%0AAbstract%3A%20While%20table%20understanding%20increasingly%20relies%20on%20pixel-only%20settings%2C%20current%20benchmarks%20predominantly%20use%20synthetic%20renderings%20that%20lack%20the%20complexity%20and%20visual%20diversity%20of%20real-world%20tables.%20Additionally%2C%20existing%20visual%20table%20understanding%20%28VTU%29%20datasets%20offer%20fixed%20examples%20with%20single%20visualizations%20and%20pre-defined%20instructions%2C%20providing%20no%20access%20to%20underlying%20serialized%20data%20for%20reformulation.%20We%20introduce%20TABLET%2C%20a%20large-scale%20VTU%20dataset%20with%204%20million%20examples%20across%2021%20tasks%2C%20grounded%20in%202%20million%20unique%20tables%20where%2088%25%20preserve%20original%20visualizations.%20To%20evaluate%20whether%20models%20are%20able%20to%20jointly%20reason%20over%20tabular%20and%20visual%20content%2C%20we%20also%20introduce%20VisualTableQA%2C%20a%20benchmark%20requiring%20both%20visual%20perception%20and%20table%20understanding.%20Fine-tuning%20vision-language%20models%20like%20Qwen2.5-VL-7B%20and%20Gemma%203-4B%20on%20TABLET%20improves%20performance%20on%20seen%20and%20unseen%20VTU%20tasks%20while%20increasing%20robustness%20on%20real-world%20table%20visualizations.%20By%20preserving%20original%20visualizations%20and%20maintaining%20example%20traceability%20in%20a%20unified%20large-scale%20collection%2C%20TABLET%20establishes%20a%20foundation%20for%20robust%20training%20and%20extensible%20evaluation%20of%20future%20VTU%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21205v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTABLET%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Robust%2520Visual%2520Table%2520Understanding%26entry.906535625%3DI%25C3%25B1igo%2520Alonso%2520and%2520Imanol%2520Miranda%2520and%2520Eneko%2520Agirre%2520and%2520Mirella%2520Lapata%26entry.1292438233%3DWhile%2520table%2520understanding%2520increasingly%2520relies%2520on%2520pixel-only%2520settings%252C%2520current%2520benchmarks%2520predominantly%2520use%2520synthetic%2520renderings%2520that%2520lack%2520the%2520complexity%2520and%2520visual%2520diversity%2520of%2520real-world%2520tables.%2520Additionally%252C%2520existing%2520visual%2520table%2520understanding%2520%2528VTU%2529%2520datasets%2520offer%2520fixed%2520examples%2520with%2520single%2520visualizations%2520and%2520pre-defined%2520instructions%252C%2520providing%2520no%2520access%2520to%2520underlying%2520serialized%2520data%2520for%2520reformulation.%2520We%2520introduce%2520TABLET%252C%2520a%2520large-scale%2520VTU%2520dataset%2520with%25204%2520million%2520examples%2520across%252021%2520tasks%252C%2520grounded%2520in%25202%2520million%2520unique%2520tables%2520where%252088%2525%2520preserve%2520original%2520visualizations.%2520To%2520evaluate%2520whether%2520models%2520are%2520able%2520to%2520jointly%2520reason%2520over%2520tabular%2520and%2520visual%2520content%252C%2520we%2520also%2520introduce%2520VisualTableQA%252C%2520a%2520benchmark%2520requiring%2520both%2520visual%2520perception%2520and%2520table%2520understanding.%2520Fine-tuning%2520vision-language%2520models%2520like%2520Qwen2.5-VL-7B%2520and%2520Gemma%25203-4B%2520on%2520TABLET%2520improves%2520performance%2520on%2520seen%2520and%2520unseen%2520VTU%2520tasks%2520while%2520increasing%2520robustness%2520on%2520real-world%2520table%2520visualizations.%2520By%2520preserving%2520original%2520visualizations%2520and%2520maintaining%2520example%2520traceability%2520in%2520a%2520unified%2520large-scale%2520collection%252C%2520TABLET%2520establishes%2520a%2520foundation%2520for%2520robust%2520training%2520and%2520extensible%2520evaluation%2520of%2520future%2520VTU%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21205v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TABLET%3A%20A%20Large-Scale%20Dataset%20for%20Robust%20Visual%20Table%20Understanding&entry.906535625=I%C3%B1igo%20Alonso%20and%20Imanol%20Miranda%20and%20Eneko%20Agirre%20and%20Mirella%20Lapata&entry.1292438233=While%20table%20understanding%20increasingly%20relies%20on%20pixel-only%20settings%2C%20current%20benchmarks%20predominantly%20use%20synthetic%20renderings%20that%20lack%20the%20complexity%20and%20visual%20diversity%20of%20real-world%20tables.%20Additionally%2C%20existing%20visual%20table%20understanding%20%28VTU%29%20datasets%20offer%20fixed%20examples%20with%20single%20visualizations%20and%20pre-defined%20instructions%2C%20providing%20no%20access%20to%20underlying%20serialized%20data%20for%20reformulation.%20We%20introduce%20TABLET%2C%20a%20large-scale%20VTU%20dataset%20with%204%20million%20examples%20across%2021%20tasks%2C%20grounded%20in%202%20million%20unique%20tables%20where%2088%25%20preserve%20original%20visualizations.%20To%20evaluate%20whether%20models%20are%20able%20to%20jointly%20reason%20over%20tabular%20and%20visual%20content%2C%20we%20also%20introduce%20VisualTableQA%2C%20a%20benchmark%20requiring%20both%20visual%20perception%20and%20table%20understanding.%20Fine-tuning%20vision-language%20models%20like%20Qwen2.5-VL-7B%20and%20Gemma%203-4B%20on%20TABLET%20improves%20performance%20on%20seen%20and%20unseen%20VTU%20tasks%20while%20increasing%20robustness%20on%20real-world%20table%20visualizations.%20By%20preserving%20original%20visualizations%20and%20maintaining%20example%20traceability%20in%20a%20unified%20large-scale%20collection%2C%20TABLET%20establishes%20a%20foundation%20for%20robust%20training%20and%20extensible%20evaluation%20of%20future%20VTU%20models.&entry.1838667208=http%3A//arxiv.org/abs/2509.21205v3&entry.124074799=Read"},
{"title": "CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation", "author": "Robert Cronshaw and Konstantinos Vilouras and Junyu Yan and Yuning Du and Feng Chen and Steven McDonagh and Sotirios A. Tsaftaris", "abstract": "Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while failing to capture whether the generated images reflect the intended clinical semantics, such as anatomical location and pathology. In this study, we propose the Clinical Semantics Evaluator (CSEval), a framework that leverages language models to assess clinical semantic alignment between the generated images and their conditioning prompts. Our experiments show that CSEval identifies semantic inconsistencies overlooked by other metrics and correlates with expert judgment. CSEval provides a scalable and clinically meaningful complement to existing evaluation methods, supporting the safe adoption of generative models in healthcare.", "link": "http://arxiv.org/abs/2602.12004v1", "date": "2026-02-12", "relevancy": 2.5889, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSEval%3A%20A%20Framework%20for%20Evaluating%20Clinical%20Semantics%20in%20Text-to-Image%20Generation&body=Title%3A%20CSEval%3A%20A%20Framework%20for%20Evaluating%20Clinical%20Semantics%20in%20Text-to-Image%20Generation%0AAuthor%3A%20Robert%20Cronshaw%20and%20Konstantinos%20Vilouras%20and%20Junyu%20Yan%20and%20Yuning%20Du%20and%20Feng%20Chen%20and%20Steven%20McDonagh%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20Text-to-image%20generation%20has%20been%20increasingly%20applied%20in%20medical%20domains%20for%20various%20purposes%20such%20as%20data%20augmentation%20and%20education.%20Evaluating%20the%20quality%20and%20clinical%20reliability%20of%20these%20generated%20images%20is%20essential.%20However%2C%20existing%20methods%20mainly%20assess%20image%20realism%20or%20diversity%2C%20while%20failing%20to%20capture%20whether%20the%20generated%20images%20reflect%20the%20intended%20clinical%20semantics%2C%20such%20as%20anatomical%20location%20and%20pathology.%20In%20this%20study%2C%20we%20propose%20the%20Clinical%20Semantics%20Evaluator%20%28CSEval%29%2C%20a%20framework%20that%20leverages%20language%20models%20to%20assess%20clinical%20semantic%20alignment%20between%20the%20generated%20images%20and%20their%20conditioning%20prompts.%20Our%20experiments%20show%20that%20CSEval%20identifies%20semantic%20inconsistencies%20overlooked%20by%20other%20metrics%20and%20correlates%20with%20expert%20judgment.%20CSEval%20provides%20a%20scalable%20and%20clinically%20meaningful%20complement%20to%20existing%20evaluation%20methods%2C%20supporting%20the%20safe%20adoption%20of%20generative%20models%20in%20healthcare.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSEval%253A%2520A%2520Framework%2520for%2520Evaluating%2520Clinical%2520Semantics%2520in%2520Text-to-Image%2520Generation%26entry.906535625%3DRobert%2520Cronshaw%2520and%2520Konstantinos%2520Vilouras%2520and%2520Junyu%2520Yan%2520and%2520Yuning%2520Du%2520and%2520Feng%2520Chen%2520and%2520Steven%2520McDonagh%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3DText-to-image%2520generation%2520has%2520been%2520increasingly%2520applied%2520in%2520medical%2520domains%2520for%2520various%2520purposes%2520such%2520as%2520data%2520augmentation%2520and%2520education.%2520Evaluating%2520the%2520quality%2520and%2520clinical%2520reliability%2520of%2520these%2520generated%2520images%2520is%2520essential.%2520However%252C%2520existing%2520methods%2520mainly%2520assess%2520image%2520realism%2520or%2520diversity%252C%2520while%2520failing%2520to%2520capture%2520whether%2520the%2520generated%2520images%2520reflect%2520the%2520intended%2520clinical%2520semantics%252C%2520such%2520as%2520anatomical%2520location%2520and%2520pathology.%2520In%2520this%2520study%252C%2520we%2520propose%2520the%2520Clinical%2520Semantics%2520Evaluator%2520%2528CSEval%2529%252C%2520a%2520framework%2520that%2520leverages%2520language%2520models%2520to%2520assess%2520clinical%2520semantic%2520alignment%2520between%2520the%2520generated%2520images%2520and%2520their%2520conditioning%2520prompts.%2520Our%2520experiments%2520show%2520that%2520CSEval%2520identifies%2520semantic%2520inconsistencies%2520overlooked%2520by%2520other%2520metrics%2520and%2520correlates%2520with%2520expert%2520judgment.%2520CSEval%2520provides%2520a%2520scalable%2520and%2520clinically%2520meaningful%2520complement%2520to%2520existing%2520evaluation%2520methods%252C%2520supporting%2520the%2520safe%2520adoption%2520of%2520generative%2520models%2520in%2520healthcare.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSEval%3A%20A%20Framework%20for%20Evaluating%20Clinical%20Semantics%20in%20Text-to-Image%20Generation&entry.906535625=Robert%20Cronshaw%20and%20Konstantinos%20Vilouras%20and%20Junyu%20Yan%20and%20Yuning%20Du%20and%20Feng%20Chen%20and%20Steven%20McDonagh%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=Text-to-image%20generation%20has%20been%20increasingly%20applied%20in%20medical%20domains%20for%20various%20purposes%20such%20as%20data%20augmentation%20and%20education.%20Evaluating%20the%20quality%20and%20clinical%20reliability%20of%20these%20generated%20images%20is%20essential.%20However%2C%20existing%20methods%20mainly%20assess%20image%20realism%20or%20diversity%2C%20while%20failing%20to%20capture%20whether%20the%20generated%20images%20reflect%20the%20intended%20clinical%20semantics%2C%20such%20as%20anatomical%20location%20and%20pathology.%20In%20this%20study%2C%20we%20propose%20the%20Clinical%20Semantics%20Evaluator%20%28CSEval%29%2C%20a%20framework%20that%20leverages%20language%20models%20to%20assess%20clinical%20semantic%20alignment%20between%20the%20generated%20images%20and%20their%20conditioning%20prompts.%20Our%20experiments%20show%20that%20CSEval%20identifies%20semantic%20inconsistencies%20overlooked%20by%20other%20metrics%20and%20correlates%20with%20expert%20judgment.%20CSEval%20provides%20a%20scalable%20and%20clinically%20meaningful%20complement%20to%20existing%20evaluation%20methods%2C%20supporting%20the%20safe%20adoption%20of%20generative%20models%20in%20healthcare.&entry.1838667208=http%3A//arxiv.org/abs/2602.12004v1&entry.124074799=Read"},
{"title": "An Anatomy-Aware Shared Control Approach for Assisted Teleoperation of Lung Ultrasound Examinations", "author": "Davide Nardi and Edoardo Lamon and Daniele Fontanelli and Matteo Saveriano and Luigi Palopoli", "abstract": "Although fully autonomous systems still face challenges due to patients' anatomical variability, teleoperated systems appear to be more practical in current healthcare settings. This paper presents an anatomy-aware control framework for teleoperated lung ultrasound. Leveraging biomechanically accurate 3D modelling, the system applies virtual constraints on the ultrasound probe pose and provides real-time visual feedback to assist in precise probe placement tasks. A twofold evaluation, one with 5 naive operators on a single volunteer and the second with a single experienced operator on 6 volunteers, compared our method with a standard teleoperation baseline. The results of the first one characterised the accuracy of the anatomical model and the improved perceived performance by the naive operators, while the second one focused on the efficiency of the system in improving probe placement and reducing procedure time compared to traditional teleoperation. The results demonstrate that the proposed framework enhances the physician's capabilities in executing remote lung ultrasound, reducing more than 20% of execution time on 4-point acquisitions, towards faster, more objective and repeatable exams.", "link": "http://arxiv.org/abs/2409.17395v2", "date": "2026-02-12", "relevancy": 2.5836, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5533}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5027}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Anatomy-Aware%20Shared%20Control%20Approach%20for%20Assisted%20Teleoperation%20of%20Lung%20Ultrasound%20Examinations&body=Title%3A%20An%20Anatomy-Aware%20Shared%20Control%20Approach%20for%20Assisted%20Teleoperation%20of%20Lung%20Ultrasound%20Examinations%0AAuthor%3A%20Davide%20Nardi%20and%20Edoardo%20Lamon%20and%20Daniele%20Fontanelli%20and%20Matteo%20Saveriano%20and%20Luigi%20Palopoli%0AAbstract%3A%20Although%20fully%20autonomous%20systems%20still%20face%20challenges%20due%20to%20patients%27%20anatomical%20variability%2C%20teleoperated%20systems%20appear%20to%20be%20more%20practical%20in%20current%20healthcare%20settings.%20This%20paper%20presents%20an%20anatomy-aware%20control%20framework%20for%20teleoperated%20lung%20ultrasound.%20Leveraging%20biomechanically%20accurate%203D%20modelling%2C%20the%20system%20applies%20virtual%20constraints%20on%20the%20ultrasound%20probe%20pose%20and%20provides%20real-time%20visual%20feedback%20to%20assist%20in%20precise%20probe%20placement%20tasks.%20A%20twofold%20evaluation%2C%20one%20with%205%20naive%20operators%20on%20a%20single%20volunteer%20and%20the%20second%20with%20a%20single%20experienced%20operator%20on%206%20volunteers%2C%20compared%20our%20method%20with%20a%20standard%20teleoperation%20baseline.%20The%20results%20of%20the%20first%20one%20characterised%20the%20accuracy%20of%20the%20anatomical%20model%20and%20the%20improved%20perceived%20performance%20by%20the%20naive%20operators%2C%20while%20the%20second%20one%20focused%20on%20the%20efficiency%20of%20the%20system%20in%20improving%20probe%20placement%20and%20reducing%20procedure%20time%20compared%20to%20traditional%20teleoperation.%20The%20results%20demonstrate%20that%20the%20proposed%20framework%20enhances%20the%20physician%27s%20capabilities%20in%20executing%20remote%20lung%20ultrasound%2C%20reducing%20more%20than%2020%25%20of%20execution%20time%20on%204-point%20acquisitions%2C%20towards%20faster%2C%20more%20objective%20and%20repeatable%20exams.%0ALink%3A%20http%3A//arxiv.org/abs/2409.17395v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Anatomy-Aware%2520Shared%2520Control%2520Approach%2520for%2520Assisted%2520Teleoperation%2520of%2520Lung%2520Ultrasound%2520Examinations%26entry.906535625%3DDavide%2520Nardi%2520and%2520Edoardo%2520Lamon%2520and%2520Daniele%2520Fontanelli%2520and%2520Matteo%2520Saveriano%2520and%2520Luigi%2520Palopoli%26entry.1292438233%3DAlthough%2520fully%2520autonomous%2520systems%2520still%2520face%2520challenges%2520due%2520to%2520patients%2527%2520anatomical%2520variability%252C%2520teleoperated%2520systems%2520appear%2520to%2520be%2520more%2520practical%2520in%2520current%2520healthcare%2520settings.%2520This%2520paper%2520presents%2520an%2520anatomy-aware%2520control%2520framework%2520for%2520teleoperated%2520lung%2520ultrasound.%2520Leveraging%2520biomechanically%2520accurate%25203D%2520modelling%252C%2520the%2520system%2520applies%2520virtual%2520constraints%2520on%2520the%2520ultrasound%2520probe%2520pose%2520and%2520provides%2520real-time%2520visual%2520feedback%2520to%2520assist%2520in%2520precise%2520probe%2520placement%2520tasks.%2520A%2520twofold%2520evaluation%252C%2520one%2520with%25205%2520naive%2520operators%2520on%2520a%2520single%2520volunteer%2520and%2520the%2520second%2520with%2520a%2520single%2520experienced%2520operator%2520on%25206%2520volunteers%252C%2520compared%2520our%2520method%2520with%2520a%2520standard%2520teleoperation%2520baseline.%2520The%2520results%2520of%2520the%2520first%2520one%2520characterised%2520the%2520accuracy%2520of%2520the%2520anatomical%2520model%2520and%2520the%2520improved%2520perceived%2520performance%2520by%2520the%2520naive%2520operators%252C%2520while%2520the%2520second%2520one%2520focused%2520on%2520the%2520efficiency%2520of%2520the%2520system%2520in%2520improving%2520probe%2520placement%2520and%2520reducing%2520procedure%2520time%2520compared%2520to%2520traditional%2520teleoperation.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520framework%2520enhances%2520the%2520physician%2527s%2520capabilities%2520in%2520executing%2520remote%2520lung%2520ultrasound%252C%2520reducing%2520more%2520than%252020%2525%2520of%2520execution%2520time%2520on%25204-point%2520acquisitions%252C%2520towards%2520faster%252C%2520more%2520objective%2520and%2520repeatable%2520exams.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17395v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Anatomy-Aware%20Shared%20Control%20Approach%20for%20Assisted%20Teleoperation%20of%20Lung%20Ultrasound%20Examinations&entry.906535625=Davide%20Nardi%20and%20Edoardo%20Lamon%20and%20Daniele%20Fontanelli%20and%20Matteo%20Saveriano%20and%20Luigi%20Palopoli&entry.1292438233=Although%20fully%20autonomous%20systems%20still%20face%20challenges%20due%20to%20patients%27%20anatomical%20variability%2C%20teleoperated%20systems%20appear%20to%20be%20more%20practical%20in%20current%20healthcare%20settings.%20This%20paper%20presents%20an%20anatomy-aware%20control%20framework%20for%20teleoperated%20lung%20ultrasound.%20Leveraging%20biomechanically%20accurate%203D%20modelling%2C%20the%20system%20applies%20virtual%20constraints%20on%20the%20ultrasound%20probe%20pose%20and%20provides%20real-time%20visual%20feedback%20to%20assist%20in%20precise%20probe%20placement%20tasks.%20A%20twofold%20evaluation%2C%20one%20with%205%20naive%20operators%20on%20a%20single%20volunteer%20and%20the%20second%20with%20a%20single%20experienced%20operator%20on%206%20volunteers%2C%20compared%20our%20method%20with%20a%20standard%20teleoperation%20baseline.%20The%20results%20of%20the%20first%20one%20characterised%20the%20accuracy%20of%20the%20anatomical%20model%20and%20the%20improved%20perceived%20performance%20by%20the%20naive%20operators%2C%20while%20the%20second%20one%20focused%20on%20the%20efficiency%20of%20the%20system%20in%20improving%20probe%20placement%20and%20reducing%20procedure%20time%20compared%20to%20traditional%20teleoperation.%20The%20results%20demonstrate%20that%20the%20proposed%20framework%20enhances%20the%20physician%27s%20capabilities%20in%20executing%20remote%20lung%20ultrasound%2C%20reducing%20more%20than%2020%25%20of%20execution%20time%20on%204-point%20acquisitions%2C%20towards%20faster%2C%20more%20objective%20and%20repeatable%20exams.&entry.1838667208=http%3A//arxiv.org/abs/2409.17395v2&entry.124074799=Read"},
{"title": "Temporal Difference Learning with Constrained Initial Representations", "author": "Jiafei Lyu and Jingwen Yang and Zhongjian Qiao and Runze Liu and Zeyuan Liu and Deheng Ye and Zongqing Lu and Xiu Li", "abstract": "Recently, there have been numerous attempts to enhance the sample efficiency of off-policy reinforcement learning (RL) agents when interacting with the environment, including architecture improvements and new algorithms. Despite these advances, they overlook the potential of directly constraining the initial representations of the input data, which can intuitively alleviate the distribution shift issue and stabilize training. In this paper, we introduce the Tanh function into the initial layer to fulfill such a constraint. We theoretically unpack the convergence property of the temporal difference learning with the Tanh function under linear function approximation. Motivated by theoretical insights, we present our Constrained Initial Representations framework, tagged CIR, which is made up of three components: (i) the Tanh activation along with normalization methods to stabilize representations; (ii) the skip connection module to provide a linear pathway from the shallow layer to the deep layer; (iii) the convex Q-learning that allows a more flexible value estimate and mitigates potential conservatism. Empirical results show that CIR exhibits strong performance on numerous continuous control tasks, even being competitive or surpassing existing strong baseline methods.", "link": "http://arxiv.org/abs/2602.11800v1", "date": "2026-02-12", "relevancy": 2.5632, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5028}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Difference%20Learning%20with%20Constrained%20Initial%20Representations&body=Title%3A%20Temporal%20Difference%20Learning%20with%20Constrained%20Initial%20Representations%0AAuthor%3A%20Jiafei%20Lyu%20and%20Jingwen%20Yang%20and%20Zhongjian%20Qiao%20and%20Runze%20Liu%20and%20Zeyuan%20Liu%20and%20Deheng%20Ye%20and%20Zongqing%20Lu%20and%20Xiu%20Li%0AAbstract%3A%20Recently%2C%20there%20have%20been%20numerous%20attempts%20to%20enhance%20the%20sample%20efficiency%20of%20off-policy%20reinforcement%20learning%20%28RL%29%20agents%20when%20interacting%20with%20the%20environment%2C%20including%20architecture%20improvements%20and%20new%20algorithms.%20Despite%20these%20advances%2C%20they%20overlook%20the%20potential%20of%20directly%20constraining%20the%20initial%20representations%20of%20the%20input%20data%2C%20which%20can%20intuitively%20alleviate%20the%20distribution%20shift%20issue%20and%20stabilize%20training.%20In%20this%20paper%2C%20we%20introduce%20the%20Tanh%20function%20into%20the%20initial%20layer%20to%20fulfill%20such%20a%20constraint.%20We%20theoretically%20unpack%20the%20convergence%20property%20of%20the%20temporal%20difference%20learning%20with%20the%20Tanh%20function%20under%20linear%20function%20approximation.%20Motivated%20by%20theoretical%20insights%2C%20we%20present%20our%20Constrained%20Initial%20Representations%20framework%2C%20tagged%20CIR%2C%20which%20is%20made%20up%20of%20three%20components%3A%20%28i%29%20the%20Tanh%20activation%20along%20with%20normalization%20methods%20to%20stabilize%20representations%3B%20%28ii%29%20the%20skip%20connection%20module%20to%20provide%20a%20linear%20pathway%20from%20the%20shallow%20layer%20to%20the%20deep%20layer%3B%20%28iii%29%20the%20convex%20Q-learning%20that%20allows%20a%20more%20flexible%20value%20estimate%20and%20mitigates%20potential%20conservatism.%20Empirical%20results%20show%20that%20CIR%20exhibits%20strong%20performance%20on%20numerous%20continuous%20control%20tasks%2C%20even%20being%20competitive%20or%20surpassing%20existing%20strong%20baseline%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Difference%2520Learning%2520with%2520Constrained%2520Initial%2520Representations%26entry.906535625%3DJiafei%2520Lyu%2520and%2520Jingwen%2520Yang%2520and%2520Zhongjian%2520Qiao%2520and%2520Runze%2520Liu%2520and%2520Zeyuan%2520Liu%2520and%2520Deheng%2520Ye%2520and%2520Zongqing%2520Lu%2520and%2520Xiu%2520Li%26entry.1292438233%3DRecently%252C%2520there%2520have%2520been%2520numerous%2520attempts%2520to%2520enhance%2520the%2520sample%2520efficiency%2520of%2520off-policy%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520when%2520interacting%2520with%2520the%2520environment%252C%2520including%2520architecture%2520improvements%2520and%2520new%2520algorithms.%2520Despite%2520these%2520advances%252C%2520they%2520overlook%2520the%2520potential%2520of%2520directly%2520constraining%2520the%2520initial%2520representations%2520of%2520the%2520input%2520data%252C%2520which%2520can%2520intuitively%2520alleviate%2520the%2520distribution%2520shift%2520issue%2520and%2520stabilize%2520training.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Tanh%2520function%2520into%2520the%2520initial%2520layer%2520to%2520fulfill%2520such%2520a%2520constraint.%2520We%2520theoretically%2520unpack%2520the%2520convergence%2520property%2520of%2520the%2520temporal%2520difference%2520learning%2520with%2520the%2520Tanh%2520function%2520under%2520linear%2520function%2520approximation.%2520Motivated%2520by%2520theoretical%2520insights%252C%2520we%2520present%2520our%2520Constrained%2520Initial%2520Representations%2520framework%252C%2520tagged%2520CIR%252C%2520which%2520is%2520made%2520up%2520of%2520three%2520components%253A%2520%2528i%2529%2520the%2520Tanh%2520activation%2520along%2520with%2520normalization%2520methods%2520to%2520stabilize%2520representations%253B%2520%2528ii%2529%2520the%2520skip%2520connection%2520module%2520to%2520provide%2520a%2520linear%2520pathway%2520from%2520the%2520shallow%2520layer%2520to%2520the%2520deep%2520layer%253B%2520%2528iii%2529%2520the%2520convex%2520Q-learning%2520that%2520allows%2520a%2520more%2520flexible%2520value%2520estimate%2520and%2520mitigates%2520potential%2520conservatism.%2520Empirical%2520results%2520show%2520that%2520CIR%2520exhibits%2520strong%2520performance%2520on%2520numerous%2520continuous%2520control%2520tasks%252C%2520even%2520being%2520competitive%2520or%2520surpassing%2520existing%2520strong%2520baseline%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Difference%20Learning%20with%20Constrained%20Initial%20Representations&entry.906535625=Jiafei%20Lyu%20and%20Jingwen%20Yang%20and%20Zhongjian%20Qiao%20and%20Runze%20Liu%20and%20Zeyuan%20Liu%20and%20Deheng%20Ye%20and%20Zongqing%20Lu%20and%20Xiu%20Li&entry.1292438233=Recently%2C%20there%20have%20been%20numerous%20attempts%20to%20enhance%20the%20sample%20efficiency%20of%20off-policy%20reinforcement%20learning%20%28RL%29%20agents%20when%20interacting%20with%20the%20environment%2C%20including%20architecture%20improvements%20and%20new%20algorithms.%20Despite%20these%20advances%2C%20they%20overlook%20the%20potential%20of%20directly%20constraining%20the%20initial%20representations%20of%20the%20input%20data%2C%20which%20can%20intuitively%20alleviate%20the%20distribution%20shift%20issue%20and%20stabilize%20training.%20In%20this%20paper%2C%20we%20introduce%20the%20Tanh%20function%20into%20the%20initial%20layer%20to%20fulfill%20such%20a%20constraint.%20We%20theoretically%20unpack%20the%20convergence%20property%20of%20the%20temporal%20difference%20learning%20with%20the%20Tanh%20function%20under%20linear%20function%20approximation.%20Motivated%20by%20theoretical%20insights%2C%20we%20present%20our%20Constrained%20Initial%20Representations%20framework%2C%20tagged%20CIR%2C%20which%20is%20made%20up%20of%20three%20components%3A%20%28i%29%20the%20Tanh%20activation%20along%20with%20normalization%20methods%20to%20stabilize%20representations%3B%20%28ii%29%20the%20skip%20connection%20module%20to%20provide%20a%20linear%20pathway%20from%20the%20shallow%20layer%20to%20the%20deep%20layer%3B%20%28iii%29%20the%20convex%20Q-learning%20that%20allows%20a%20more%20flexible%20value%20estimate%20and%20mitigates%20potential%20conservatism.%20Empirical%20results%20show%20that%20CIR%20exhibits%20strong%20performance%20on%20numerous%20continuous%20control%20tasks%2C%20even%20being%20competitive%20or%20surpassing%20existing%20strong%20baseline%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.11800v1&entry.124074799=Read"},
{"title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data", "author": "Patrick Langer and Thomas Kaar and Max Rosenblattl and Maxwell A. Xu and Winnie Chow and Martin Maritsch and Robert Jakob and Ning Wang and Aradhana Verma and Brian Han and Daniel Seung Kim and Henry Chubb and Scott Ceresnak and Aydin Zahedivash and Alexander Tarlochan Singh Sandhu and Fatima Rodriguez and Daniel McDuff and Elgar Fleisch and Oliver Aalami and Filipe Barata and Paul Schmiedmayer", "abstract": "LLMs have emerged as powerful tools for interpreting multimodal data. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and digital health applications. Yet, a major limitation remains their inability to handle time series. To overcome this gap, we present OpenTSLM, a family of Time Series Language Models (TSLMs) created by integrating time series as a native modality to pretrained LLMs, enabling reasoning over multiple time series of any length. We investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants against baselines that treat time series as text tokens or plots, across a suite of text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR, compared to 9.05 and 52.2 for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt grows exponentially in memory with sequence length, requiring around 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models open-source.", "link": "http://arxiv.org/abs/2510.02410v2", "date": "2026-02-12", "relevancy": 2.5612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenTSLM%3A%20Time-Series%20Language%20Models%20for%20Reasoning%20over%20Multivariate%20Medical%20Text-%20and%20Time-Series%20Data&body=Title%3A%20OpenTSLM%3A%20Time-Series%20Language%20Models%20for%20Reasoning%20over%20Multivariate%20Medical%20Text-%20and%20Time-Series%20Data%0AAuthor%3A%20Patrick%20Langer%20and%20Thomas%20Kaar%20and%20Max%20Rosenblattl%20and%20Maxwell%20A.%20Xu%20and%20Winnie%20Chow%20and%20Martin%20Maritsch%20and%20Robert%20Jakob%20and%20Ning%20Wang%20and%20Aradhana%20Verma%20and%20Brian%20Han%20and%20Daniel%20Seung%20Kim%20and%20Henry%20Chubb%20and%20Scott%20Ceresnak%20and%20Aydin%20Zahedivash%20and%20Alexander%20Tarlochan%20Singh%20Sandhu%20and%20Fatima%20Rodriguez%20and%20Daniel%20McDuff%20and%20Elgar%20Fleisch%20and%20Oliver%20Aalami%20and%20Filipe%20Barata%20and%20Paul%20Schmiedmayer%0AAbstract%3A%20LLMs%20have%20emerged%20as%20powerful%20tools%20for%20interpreting%20multimodal%20data.%20In%20medicine%2C%20they%20hold%20particular%20promise%20for%20synthesizing%20large%20volumes%20of%20clinical%20information%20into%20actionable%20insights%20and%20digital%20health%20applications.%20Yet%2C%20a%20major%20limitation%20remains%20their%20inability%20to%20handle%20time%20series.%20To%20overcome%20this%20gap%2C%20we%20present%20OpenTSLM%2C%20a%20family%20of%20Time%20Series%20Language%20Models%20%28TSLMs%29%20created%20by%20integrating%20time%20series%20as%20a%20native%20modality%20to%20pretrained%20LLMs%2C%20enabling%20reasoning%20over%20multiple%20time%20series%20of%20any%20length.%20We%20investigate%20two%20architectures%20for%20OpenTSLM.%20The%20first%2C%20OpenTSLM-SoftPrompt%2C%20models%20time%20series%20implicitly%20by%20concatenating%20learnable%20time%20series%20tokens%20with%20text%20tokens%20via%20soft%20prompting.%20Although%20parameter-efficient%2C%20we%20hypothesize%20that%20explicit%20time%20series%20modeling%20scales%20better%20and%20outperforms%20implicit%20approaches.%20We%20thus%20introduce%20OpenTSLM-Flamingo%2C%20which%20integrates%20time%20series%20with%20text%20via%20cross-attention.%20We%20benchmark%20both%20variants%20against%20baselines%20that%20treat%20time%20series%20as%20text%20tokens%20or%20plots%2C%20across%20a%20suite%20of%20text-time-series%20Chain-of-Thought%20%28CoT%29%20reasoning%20tasks.%20We%20introduce%20three%20datasets%3A%20HAR-CoT%2C%20Sleep-CoT%2C%20and%20ECG-QA-CoT.%20Across%20all%2C%20OpenTSLM%20models%20outperform%20baselines%2C%20reaching%2069.9%20F1%20in%20sleep%20staging%20and%2065.4%20in%20HAR%2C%20compared%20to%209.05%20and%2052.2%20for%20finetuned%20text-only%20models.%20Notably%2C%20even%201B-parameter%20OpenTSLM%20models%20surpass%20GPT-4o%20%2815.47%20and%202.95%29.%20OpenTSLM-Flamingo%20matches%20OpenTSLM-SoftPrompt%20in%20performance%20and%20outperforms%20on%20longer%20sequences%2C%20while%20maintaining%20stable%20memory%20requirements.%20By%20contrast%2C%20SoftPrompt%20grows%20exponentially%20in%20memory%20with%20sequence%20length%2C%20requiring%20around%20110%20GB%20compared%20to%2040%20GB%20VRAM%20when%20training%20on%20ECG-QA%20with%20LLaMA-3B.%20Expert%20reviews%20by%20clinicians%20find%20strong%20reasoning%20capabilities%20exhibited%20by%20OpenTSLMs%20on%20ECG-QA.%20To%20facilitate%20further%20research%2C%20we%20provide%20all%20code%2C%20datasets%2C%20and%20models%20open-source.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenTSLM%253A%2520Time-Series%2520Language%2520Models%2520for%2520Reasoning%2520over%2520Multivariate%2520Medical%2520Text-%2520and%2520Time-Series%2520Data%26entry.906535625%3DPatrick%2520Langer%2520and%2520Thomas%2520Kaar%2520and%2520Max%2520Rosenblattl%2520and%2520Maxwell%2520A.%2520Xu%2520and%2520Winnie%2520Chow%2520and%2520Martin%2520Maritsch%2520and%2520Robert%2520Jakob%2520and%2520Ning%2520Wang%2520and%2520Aradhana%2520Verma%2520and%2520Brian%2520Han%2520and%2520Daniel%2520Seung%2520Kim%2520and%2520Henry%2520Chubb%2520and%2520Scott%2520Ceresnak%2520and%2520Aydin%2520Zahedivash%2520and%2520Alexander%2520Tarlochan%2520Singh%2520Sandhu%2520and%2520Fatima%2520Rodriguez%2520and%2520Daniel%2520McDuff%2520and%2520Elgar%2520Fleisch%2520and%2520Oliver%2520Aalami%2520and%2520Filipe%2520Barata%2520and%2520Paul%2520Schmiedmayer%26entry.1292438233%3DLLMs%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520interpreting%2520multimodal%2520data.%2520In%2520medicine%252C%2520they%2520hold%2520particular%2520promise%2520for%2520synthesizing%2520large%2520volumes%2520of%2520clinical%2520information%2520into%2520actionable%2520insights%2520and%2520digital%2520health%2520applications.%2520Yet%252C%2520a%2520major%2520limitation%2520remains%2520their%2520inability%2520to%2520handle%2520time%2520series.%2520To%2520overcome%2520this%2520gap%252C%2520we%2520present%2520OpenTSLM%252C%2520a%2520family%2520of%2520Time%2520Series%2520Language%2520Models%2520%2528TSLMs%2529%2520created%2520by%2520integrating%2520time%2520series%2520as%2520a%2520native%2520modality%2520to%2520pretrained%2520LLMs%252C%2520enabling%2520reasoning%2520over%2520multiple%2520time%2520series%2520of%2520any%2520length.%2520We%2520investigate%2520two%2520architectures%2520for%2520OpenTSLM.%2520The%2520first%252C%2520OpenTSLM-SoftPrompt%252C%2520models%2520time%2520series%2520implicitly%2520by%2520concatenating%2520learnable%2520time%2520series%2520tokens%2520with%2520text%2520tokens%2520via%2520soft%2520prompting.%2520Although%2520parameter-efficient%252C%2520we%2520hypothesize%2520that%2520explicit%2520time%2520series%2520modeling%2520scales%2520better%2520and%2520outperforms%2520implicit%2520approaches.%2520We%2520thus%2520introduce%2520OpenTSLM-Flamingo%252C%2520which%2520integrates%2520time%2520series%2520with%2520text%2520via%2520cross-attention.%2520We%2520benchmark%2520both%2520variants%2520against%2520baselines%2520that%2520treat%2520time%2520series%2520as%2520text%2520tokens%2520or%2520plots%252C%2520across%2520a%2520suite%2520of%2520text-time-series%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520tasks.%2520We%2520introduce%2520three%2520datasets%253A%2520HAR-CoT%252C%2520Sleep-CoT%252C%2520and%2520ECG-QA-CoT.%2520Across%2520all%252C%2520OpenTSLM%2520models%2520outperform%2520baselines%252C%2520reaching%252069.9%2520F1%2520in%2520sleep%2520staging%2520and%252065.4%2520in%2520HAR%252C%2520compared%2520to%25209.05%2520and%252052.2%2520for%2520finetuned%2520text-only%2520models.%2520Notably%252C%2520even%25201B-parameter%2520OpenTSLM%2520models%2520surpass%2520GPT-4o%2520%252815.47%2520and%25202.95%2529.%2520OpenTSLM-Flamingo%2520matches%2520OpenTSLM-SoftPrompt%2520in%2520performance%2520and%2520outperforms%2520on%2520longer%2520sequences%252C%2520while%2520maintaining%2520stable%2520memory%2520requirements.%2520By%2520contrast%252C%2520SoftPrompt%2520grows%2520exponentially%2520in%2520memory%2520with%2520sequence%2520length%252C%2520requiring%2520around%2520110%2520GB%2520compared%2520to%252040%2520GB%2520VRAM%2520when%2520training%2520on%2520ECG-QA%2520with%2520LLaMA-3B.%2520Expert%2520reviews%2520by%2520clinicians%2520find%2520strong%2520reasoning%2520capabilities%2520exhibited%2520by%2520OpenTSLMs%2520on%2520ECG-QA.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520provide%2520all%2520code%252C%2520datasets%252C%2520and%2520models%2520open-source.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenTSLM%3A%20Time-Series%20Language%20Models%20for%20Reasoning%20over%20Multivariate%20Medical%20Text-%20and%20Time-Series%20Data&entry.906535625=Patrick%20Langer%20and%20Thomas%20Kaar%20and%20Max%20Rosenblattl%20and%20Maxwell%20A.%20Xu%20and%20Winnie%20Chow%20and%20Martin%20Maritsch%20and%20Robert%20Jakob%20and%20Ning%20Wang%20and%20Aradhana%20Verma%20and%20Brian%20Han%20and%20Daniel%20Seung%20Kim%20and%20Henry%20Chubb%20and%20Scott%20Ceresnak%20and%20Aydin%20Zahedivash%20and%20Alexander%20Tarlochan%20Singh%20Sandhu%20and%20Fatima%20Rodriguez%20and%20Daniel%20McDuff%20and%20Elgar%20Fleisch%20and%20Oliver%20Aalami%20and%20Filipe%20Barata%20and%20Paul%20Schmiedmayer&entry.1292438233=LLMs%20have%20emerged%20as%20powerful%20tools%20for%20interpreting%20multimodal%20data.%20In%20medicine%2C%20they%20hold%20particular%20promise%20for%20synthesizing%20large%20volumes%20of%20clinical%20information%20into%20actionable%20insights%20and%20digital%20health%20applications.%20Yet%2C%20a%20major%20limitation%20remains%20their%20inability%20to%20handle%20time%20series.%20To%20overcome%20this%20gap%2C%20we%20present%20OpenTSLM%2C%20a%20family%20of%20Time%20Series%20Language%20Models%20%28TSLMs%29%20created%20by%20integrating%20time%20series%20as%20a%20native%20modality%20to%20pretrained%20LLMs%2C%20enabling%20reasoning%20over%20multiple%20time%20series%20of%20any%20length.%20We%20investigate%20two%20architectures%20for%20OpenTSLM.%20The%20first%2C%20OpenTSLM-SoftPrompt%2C%20models%20time%20series%20implicitly%20by%20concatenating%20learnable%20time%20series%20tokens%20with%20text%20tokens%20via%20soft%20prompting.%20Although%20parameter-efficient%2C%20we%20hypothesize%20that%20explicit%20time%20series%20modeling%20scales%20better%20and%20outperforms%20implicit%20approaches.%20We%20thus%20introduce%20OpenTSLM-Flamingo%2C%20which%20integrates%20time%20series%20with%20text%20via%20cross-attention.%20We%20benchmark%20both%20variants%20against%20baselines%20that%20treat%20time%20series%20as%20text%20tokens%20or%20plots%2C%20across%20a%20suite%20of%20text-time-series%20Chain-of-Thought%20%28CoT%29%20reasoning%20tasks.%20We%20introduce%20three%20datasets%3A%20HAR-CoT%2C%20Sleep-CoT%2C%20and%20ECG-QA-CoT.%20Across%20all%2C%20OpenTSLM%20models%20outperform%20baselines%2C%20reaching%2069.9%20F1%20in%20sleep%20staging%20and%2065.4%20in%20HAR%2C%20compared%20to%209.05%20and%2052.2%20for%20finetuned%20text-only%20models.%20Notably%2C%20even%201B-parameter%20OpenTSLM%20models%20surpass%20GPT-4o%20%2815.47%20and%202.95%29.%20OpenTSLM-Flamingo%20matches%20OpenTSLM-SoftPrompt%20in%20performance%20and%20outperforms%20on%20longer%20sequences%2C%20while%20maintaining%20stable%20memory%20requirements.%20By%20contrast%2C%20SoftPrompt%20grows%20exponentially%20in%20memory%20with%20sequence%20length%2C%20requiring%20around%20110%20GB%20compared%20to%2040%20GB%20VRAM%20when%20training%20on%20ECG-QA%20with%20LLaMA-3B.%20Expert%20reviews%20by%20clinicians%20find%20strong%20reasoning%20capabilities%20exhibited%20by%20OpenTSLMs%20on%20ECG-QA.%20To%20facilitate%20further%20research%2C%20we%20provide%20all%20code%2C%20datasets%2C%20and%20models%20open-source.&entry.1838667208=http%3A//arxiv.org/abs/2510.02410v2&entry.124074799=Read"},
{"title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing", "author": "Dianyi Wang and Ruihang Li and Feng Han and Chaofan Ma and Wei Song and Siyuan Wang and Yibin Wang and Yi Xin and Hongjian Liu and Zhixiong Zhang and Shengyuan Ding and Tianhang Wang and Zhenglin Cheng and Tao Lin and Cheng Jin and Kaicheng Yu and Jingjing Chen and Wenjie Wang and Zhongyu Wei and Jiaqi Wang", "abstract": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.", "link": "http://arxiv.org/abs/2602.12205v1", "date": "2026-02-12", "relevancy": 2.5537, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6443}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6406}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepGen%201.0%3A%20A%20Lightweight%20Unified%20Multimodal%20Model%20for%20Advancing%20Image%20Generation%20and%20Editing&body=Title%3A%20DeepGen%201.0%3A%20A%20Lightweight%20Unified%20Multimodal%20Model%20for%20Advancing%20Image%20Generation%20and%20Editing%0AAuthor%3A%20Dianyi%20Wang%20and%20Ruihang%20Li%20and%20Feng%20Han%20and%20Chaofan%20Ma%20and%20Wei%20Song%20and%20Siyuan%20Wang%20and%20Yibin%20Wang%20and%20Yi%20Xin%20and%20Hongjian%20Liu%20and%20Zhixiong%20Zhang%20and%20Shengyuan%20Ding%20and%20Tianhang%20Wang%20and%20Zhenglin%20Cheng%20and%20Tao%20Lin%20and%20Cheng%20Jin%20and%20Kaicheng%20Yu%20and%20Jingjing%20Chen%20and%20Wenjie%20Wang%20and%20Zhongyu%20Wei%20and%20Jiaqi%20Wang%0AAbstract%3A%20Current%20unified%20multimodal%20models%20for%20image%20generation%20and%20editing%20typically%20rely%20on%20massive%20parameter%20scales%20%28e.g.%2C%20%3E10B%29%2C%20entailing%20prohibitive%20training%20costs%20and%20deployment%20footprints.%20In%20this%20work%2C%20we%20present%20DeepGen%201.0%2C%20a%20lightweight%205B%20unified%20model%20that%20achieves%20comprehensive%20capabilities%20competitive%20with%20or%20surpassing%20much%20larger%20counterparts.%20To%20overcome%20the%20limitations%20of%20compact%20models%20in%20semantic%20understanding%20and%20fine-grained%20control%2C%20we%20introduce%20Stacked%20Channel%20Bridging%20%28SCB%29%2C%20a%20deep%20alignment%20framework%20that%20extracts%20hierarchical%20features%20from%20multiple%20VLM%20layers%20and%20fuses%20them%20with%20learnable%20%27think%20tokens%27%20to%20provide%20the%20generative%20backbone%20with%20structured%2C%20reasoning-rich%20guidance.%20We%20further%20design%20a%20data-centric%20training%20strategy%20spanning%20three%20progressive%20stages%3A%20%281%29%20Alignment%20Pre-training%20on%20large-scale%20image-text%20pairs%20and%20editing%20triplets%20to%20synchronize%20VLM%20and%20DiT%20representations%2C%20%282%29%20Joint%20Supervised%20Fine-tuning%20on%20a%20high-quality%20mixture%20of%20generation%2C%20editing%2C%20and%20reasoning%20tasks%20to%20foster%20omni-capabilities%2C%20and%20%283%29%20Reinforcement%20Learning%20with%20MR-GRPO%2C%20which%20leverages%20a%20mixture%20of%20reward%20functions%20and%20supervision%20signals%2C%20resulting%20in%20substantial%20gains%20in%20generation%20quality%20and%20alignment%20with%20human%20preferences%2C%20while%20maintaining%20stable%20training%20progress%20and%20avoiding%20visual%20artifacts.%20Despite%20being%20trained%20on%20only%20~50M%20samples%2C%20DeepGen%201.0%20achieves%20leading%20performance%20across%20diverse%20benchmarks%2C%20surpassing%20the%2080B%20HunyuanImage%20by%2028%25%20on%20WISE%20and%20the%2027B%20Qwen-Image-Edit%20by%2037%25%20on%20UniREditBench.%20By%20open-sourcing%20our%20training%20code%2C%20weights%2C%20and%20datasets%2C%20we%20provide%20an%20efficient%2C%20high-performance%20alternative%20to%20democratize%20unified%20multimodal%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepGen%25201.0%253A%2520A%2520Lightweight%2520Unified%2520Multimodal%2520Model%2520for%2520Advancing%2520Image%2520Generation%2520and%2520Editing%26entry.906535625%3DDianyi%2520Wang%2520and%2520Ruihang%2520Li%2520and%2520Feng%2520Han%2520and%2520Chaofan%2520Ma%2520and%2520Wei%2520Song%2520and%2520Siyuan%2520Wang%2520and%2520Yibin%2520Wang%2520and%2520Yi%2520Xin%2520and%2520Hongjian%2520Liu%2520and%2520Zhixiong%2520Zhang%2520and%2520Shengyuan%2520Ding%2520and%2520Tianhang%2520Wang%2520and%2520Zhenglin%2520Cheng%2520and%2520Tao%2520Lin%2520and%2520Cheng%2520Jin%2520and%2520Kaicheng%2520Yu%2520and%2520Jingjing%2520Chen%2520and%2520Wenjie%2520Wang%2520and%2520Zhongyu%2520Wei%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DCurrent%2520unified%2520multimodal%2520models%2520for%2520image%2520generation%2520and%2520editing%2520typically%2520rely%2520on%2520massive%2520parameter%2520scales%2520%2528e.g.%252C%2520%253E10B%2529%252C%2520entailing%2520prohibitive%2520training%2520costs%2520and%2520deployment%2520footprints.%2520In%2520this%2520work%252C%2520we%2520present%2520DeepGen%25201.0%252C%2520a%2520lightweight%25205B%2520unified%2520model%2520that%2520achieves%2520comprehensive%2520capabilities%2520competitive%2520with%2520or%2520surpassing%2520much%2520larger%2520counterparts.%2520To%2520overcome%2520the%2520limitations%2520of%2520compact%2520models%2520in%2520semantic%2520understanding%2520and%2520fine-grained%2520control%252C%2520we%2520introduce%2520Stacked%2520Channel%2520Bridging%2520%2528SCB%2529%252C%2520a%2520deep%2520alignment%2520framework%2520that%2520extracts%2520hierarchical%2520features%2520from%2520multiple%2520VLM%2520layers%2520and%2520fuses%2520them%2520with%2520learnable%2520%2527think%2520tokens%2527%2520to%2520provide%2520the%2520generative%2520backbone%2520with%2520structured%252C%2520reasoning-rich%2520guidance.%2520We%2520further%2520design%2520a%2520data-centric%2520training%2520strategy%2520spanning%2520three%2520progressive%2520stages%253A%2520%25281%2529%2520Alignment%2520Pre-training%2520on%2520large-scale%2520image-text%2520pairs%2520and%2520editing%2520triplets%2520to%2520synchronize%2520VLM%2520and%2520DiT%2520representations%252C%2520%25282%2529%2520Joint%2520Supervised%2520Fine-tuning%2520on%2520a%2520high-quality%2520mixture%2520of%2520generation%252C%2520editing%252C%2520and%2520reasoning%2520tasks%2520to%2520foster%2520omni-capabilities%252C%2520and%2520%25283%2529%2520Reinforcement%2520Learning%2520with%2520MR-GRPO%252C%2520which%2520leverages%2520a%2520mixture%2520of%2520reward%2520functions%2520and%2520supervision%2520signals%252C%2520resulting%2520in%2520substantial%2520gains%2520in%2520generation%2520quality%2520and%2520alignment%2520with%2520human%2520preferences%252C%2520while%2520maintaining%2520stable%2520training%2520progress%2520and%2520avoiding%2520visual%2520artifacts.%2520Despite%2520being%2520trained%2520on%2520only%2520~50M%2520samples%252C%2520DeepGen%25201.0%2520achieves%2520leading%2520performance%2520across%2520diverse%2520benchmarks%252C%2520surpassing%2520the%252080B%2520HunyuanImage%2520by%252028%2525%2520on%2520WISE%2520and%2520the%252027B%2520Qwen-Image-Edit%2520by%252037%2525%2520on%2520UniREditBench.%2520By%2520open-sourcing%2520our%2520training%2520code%252C%2520weights%252C%2520and%2520datasets%252C%2520we%2520provide%2520an%2520efficient%252C%2520high-performance%2520alternative%2520to%2520democratize%2520unified%2520multimodal%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepGen%201.0%3A%20A%20Lightweight%20Unified%20Multimodal%20Model%20for%20Advancing%20Image%20Generation%20and%20Editing&entry.906535625=Dianyi%20Wang%20and%20Ruihang%20Li%20and%20Feng%20Han%20and%20Chaofan%20Ma%20and%20Wei%20Song%20and%20Siyuan%20Wang%20and%20Yibin%20Wang%20and%20Yi%20Xin%20and%20Hongjian%20Liu%20and%20Zhixiong%20Zhang%20and%20Shengyuan%20Ding%20and%20Tianhang%20Wang%20and%20Zhenglin%20Cheng%20and%20Tao%20Lin%20and%20Cheng%20Jin%20and%20Kaicheng%20Yu%20and%20Jingjing%20Chen%20and%20Wenjie%20Wang%20and%20Zhongyu%20Wei%20and%20Jiaqi%20Wang&entry.1292438233=Current%20unified%20multimodal%20models%20for%20image%20generation%20and%20editing%20typically%20rely%20on%20massive%20parameter%20scales%20%28e.g.%2C%20%3E10B%29%2C%20entailing%20prohibitive%20training%20costs%20and%20deployment%20footprints.%20In%20this%20work%2C%20we%20present%20DeepGen%201.0%2C%20a%20lightweight%205B%20unified%20model%20that%20achieves%20comprehensive%20capabilities%20competitive%20with%20or%20surpassing%20much%20larger%20counterparts.%20To%20overcome%20the%20limitations%20of%20compact%20models%20in%20semantic%20understanding%20and%20fine-grained%20control%2C%20we%20introduce%20Stacked%20Channel%20Bridging%20%28SCB%29%2C%20a%20deep%20alignment%20framework%20that%20extracts%20hierarchical%20features%20from%20multiple%20VLM%20layers%20and%20fuses%20them%20with%20learnable%20%27think%20tokens%27%20to%20provide%20the%20generative%20backbone%20with%20structured%2C%20reasoning-rich%20guidance.%20We%20further%20design%20a%20data-centric%20training%20strategy%20spanning%20three%20progressive%20stages%3A%20%281%29%20Alignment%20Pre-training%20on%20large-scale%20image-text%20pairs%20and%20editing%20triplets%20to%20synchronize%20VLM%20and%20DiT%20representations%2C%20%282%29%20Joint%20Supervised%20Fine-tuning%20on%20a%20high-quality%20mixture%20of%20generation%2C%20editing%2C%20and%20reasoning%20tasks%20to%20foster%20omni-capabilities%2C%20and%20%283%29%20Reinforcement%20Learning%20with%20MR-GRPO%2C%20which%20leverages%20a%20mixture%20of%20reward%20functions%20and%20supervision%20signals%2C%20resulting%20in%20substantial%20gains%20in%20generation%20quality%20and%20alignment%20with%20human%20preferences%2C%20while%20maintaining%20stable%20training%20progress%20and%20avoiding%20visual%20artifacts.%20Despite%20being%20trained%20on%20only%20~50M%20samples%2C%20DeepGen%201.0%20achieves%20leading%20performance%20across%20diverse%20benchmarks%2C%20surpassing%20the%2080B%20HunyuanImage%20by%2028%25%20on%20WISE%20and%20the%2027B%20Qwen-Image-Edit%20by%2037%25%20on%20UniREditBench.%20By%20open-sourcing%20our%20training%20code%2C%20weights%2C%20and%20datasets%2C%20we%20provide%20an%20efficient%2C%20high-performance%20alternative%20to%20democratize%20unified%20multimodal%20research.&entry.1838667208=http%3A//arxiv.org/abs/2602.12205v1&entry.124074799=Read"},
{"title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations", "author": "Minjun Zhu and Zhen Lin and Yixuan Weng and Panzhong Lu and Qiujie Xie and Yifan Wei and Sifan Liu and Qiyao Sun and Yue Zhang", "abstract": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.", "link": "http://arxiv.org/abs/2602.03828v2", "date": "2026-02-12", "relevancy": 2.5526, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5163}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5145}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoFigure%3A%20Generating%20and%20Refining%20Publication-Ready%20Scientific%20Illustrations&body=Title%3A%20AutoFigure%3A%20Generating%20and%20Refining%20Publication-Ready%20Scientific%20Illustrations%0AAuthor%3A%20Minjun%20Zhu%20and%20Zhen%20Lin%20and%20Yixuan%20Weng%20and%20Panzhong%20Lu%20and%20Qiujie%20Xie%20and%20Yifan%20Wei%20and%20Sifan%20Liu%20and%20Qiyao%20Sun%20and%20Yue%20Zhang%0AAbstract%3A%20High-quality%20scientific%20illustrations%20are%20crucial%20for%20effectively%20communicating%20complex%20scientific%20and%20technical%20concepts%2C%20yet%20their%20manual%20creation%20remains%20a%20well-recognized%20bottleneck%20in%20both%20academia%20and%20industry.%20We%20present%20FigureBench%2C%20the%20first%20large-scale%20benchmark%20for%20generating%20scientific%20illustrations%20from%20long-form%20scientific%20texts.%20It%20contains%203%2C300%20high-quality%20scientific%20text-figure%20pairs%2C%20covering%20diverse%20text-to-illustration%20tasks%20from%20scientific%20papers%2C%20surveys%2C%20blogs%2C%20and%20textbooks.%20Moreover%2C%20we%20propose%20AutoFigure%2C%20the%20first%20agentic%20framework%20that%20automatically%20generates%20high-quality%20scientific%20illustrations%20based%20on%20long-form%20scientific%20text.%20Specifically%2C%20before%20rendering%20the%20final%20result%2C%20AutoFigure%20engages%20in%20extensive%20thinking%2C%20recombination%2C%20and%20validation%20to%20produce%20a%20layout%20that%20is%20both%20structurally%20sound%20and%20aesthetically%20refined%2C%20outputting%20a%20scientific%20illustration%20that%20achieves%20both%20structural%20completeness%20and%20aesthetic%20appeal.%20Leveraging%20the%20high-quality%20data%20from%20FigureBench%2C%20we%20conduct%20extensive%20experiments%20to%20test%20the%20performance%20of%20AutoFigure%20against%20various%20baseline%20methods.%20The%20results%20demonstrate%20that%20AutoFigure%20consistently%20surpasses%20all%20baseline%20methods%2C%20producing%20publication-ready%20scientific%20illustrations.%20The%20code%2C%20dataset%20and%20huggingface%20space%20are%20released%20in%20https%3A//github.com/ResearAI/AutoFigure.%0ALink%3A%20http%3A//arxiv.org/abs/2602.03828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoFigure%253A%2520Generating%2520and%2520Refining%2520Publication-Ready%2520Scientific%2520Illustrations%26entry.906535625%3DMinjun%2520Zhu%2520and%2520Zhen%2520Lin%2520and%2520Yixuan%2520Weng%2520and%2520Panzhong%2520Lu%2520and%2520Qiujie%2520Xie%2520and%2520Yifan%2520Wei%2520and%2520Sifan%2520Liu%2520and%2520Qiyao%2520Sun%2520and%2520Yue%2520Zhang%26entry.1292438233%3DHigh-quality%2520scientific%2520illustrations%2520are%2520crucial%2520for%2520effectively%2520communicating%2520complex%2520scientific%2520and%2520technical%2520concepts%252C%2520yet%2520their%2520manual%2520creation%2520remains%2520a%2520well-recognized%2520bottleneck%2520in%2520both%2520academia%2520and%2520industry.%2520We%2520present%2520FigureBench%252C%2520the%2520first%2520large-scale%2520benchmark%2520for%2520generating%2520scientific%2520illustrations%2520from%2520long-form%2520scientific%2520texts.%2520It%2520contains%25203%252C300%2520high-quality%2520scientific%2520text-figure%2520pairs%252C%2520covering%2520diverse%2520text-to-illustration%2520tasks%2520from%2520scientific%2520papers%252C%2520surveys%252C%2520blogs%252C%2520and%2520textbooks.%2520Moreover%252C%2520we%2520propose%2520AutoFigure%252C%2520the%2520first%2520agentic%2520framework%2520that%2520automatically%2520generates%2520high-quality%2520scientific%2520illustrations%2520based%2520on%2520long-form%2520scientific%2520text.%2520Specifically%252C%2520before%2520rendering%2520the%2520final%2520result%252C%2520AutoFigure%2520engages%2520in%2520extensive%2520thinking%252C%2520recombination%252C%2520and%2520validation%2520to%2520produce%2520a%2520layout%2520that%2520is%2520both%2520structurally%2520sound%2520and%2520aesthetically%2520refined%252C%2520outputting%2520a%2520scientific%2520illustration%2520that%2520achieves%2520both%2520structural%2520completeness%2520and%2520aesthetic%2520appeal.%2520Leveraging%2520the%2520high-quality%2520data%2520from%2520FigureBench%252C%2520we%2520conduct%2520extensive%2520experiments%2520to%2520test%2520the%2520performance%2520of%2520AutoFigure%2520against%2520various%2520baseline%2520methods.%2520The%2520results%2520demonstrate%2520that%2520AutoFigure%2520consistently%2520surpasses%2520all%2520baseline%2520methods%252C%2520producing%2520publication-ready%2520scientific%2520illustrations.%2520The%2520code%252C%2520dataset%2520and%2520huggingface%2520space%2520are%2520released%2520in%2520https%253A//github.com/ResearAI/AutoFigure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.03828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoFigure%3A%20Generating%20and%20Refining%20Publication-Ready%20Scientific%20Illustrations&entry.906535625=Minjun%20Zhu%20and%20Zhen%20Lin%20and%20Yixuan%20Weng%20and%20Panzhong%20Lu%20and%20Qiujie%20Xie%20and%20Yifan%20Wei%20and%20Sifan%20Liu%20and%20Qiyao%20Sun%20and%20Yue%20Zhang&entry.1292438233=High-quality%20scientific%20illustrations%20are%20crucial%20for%20effectively%20communicating%20complex%20scientific%20and%20technical%20concepts%2C%20yet%20their%20manual%20creation%20remains%20a%20well-recognized%20bottleneck%20in%20both%20academia%20and%20industry.%20We%20present%20FigureBench%2C%20the%20first%20large-scale%20benchmark%20for%20generating%20scientific%20illustrations%20from%20long-form%20scientific%20texts.%20It%20contains%203%2C300%20high-quality%20scientific%20text-figure%20pairs%2C%20covering%20diverse%20text-to-illustration%20tasks%20from%20scientific%20papers%2C%20surveys%2C%20blogs%2C%20and%20textbooks.%20Moreover%2C%20we%20propose%20AutoFigure%2C%20the%20first%20agentic%20framework%20that%20automatically%20generates%20high-quality%20scientific%20illustrations%20based%20on%20long-form%20scientific%20text.%20Specifically%2C%20before%20rendering%20the%20final%20result%2C%20AutoFigure%20engages%20in%20extensive%20thinking%2C%20recombination%2C%20and%20validation%20to%20produce%20a%20layout%20that%20is%20both%20structurally%20sound%20and%20aesthetically%20refined%2C%20outputting%20a%20scientific%20illustration%20that%20achieves%20both%20structural%20completeness%20and%20aesthetic%20appeal.%20Leveraging%20the%20high-quality%20data%20from%20FigureBench%2C%20we%20conduct%20extensive%20experiments%20to%20test%20the%20performance%20of%20AutoFigure%20against%20various%20baseline%20methods.%20The%20results%20demonstrate%20that%20AutoFigure%20consistently%20surpasses%20all%20baseline%20methods%2C%20producing%20publication-ready%20scientific%20illustrations.%20The%20code%2C%20dataset%20and%20huggingface%20space%20are%20released%20in%20https%3A//github.com/ResearAI/AutoFigure.&entry.1838667208=http%3A//arxiv.org/abs/2602.03828v2&entry.124074799=Read"},
{"title": "Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model", "author": "Chung-Soo Ahn and Rajib Rana and Sunil Sivadas and Carlos Busso and Jagath C. Rajapakse", "abstract": "Lack of large, well-annotated emotional speech corpora continues to limit the performance and robustness of speech emotion recognition (SER), particularly as models grow more complex and the demand for multimodal systems increases. While generative data augmentation offers a promising solution, existing approaches often produce emotionally inconsistent samples due to oversimplified conditioning on categorical labels. This paper introduces a novel mutual-information-regularised generative framework that combines cross-modal alignment with feature-level synthesis. Building on an InfoGAN-style architecture, our method first learns a semantically aligned audio-text representation space using pre-trained transformers and contrastive objectives. A feature generator is then trained to produce emotion-aware audio features while employing mutual information as a quantitative regulariser to ensure strong dependency between generated features and their conditioning variables. We extend this approach to multimodal settings, enabling the generation of novel, paired (audio, text) features. Comprehensive evaluation on three benchmark datasets (IEMOCAP, MSP-IMPROV, MSP-Podcast) demonstrates that our framework consistently outperforms existing augmentation methods, achieving state-of-the-art performance with improvements of up to 2.6% in unimodal SER and 3.2% in multimodal emotion recognition. Most importantly, we demonstrate that mutual information functions as both a regulariser and a measurable metric for generative quality, offering a systematic approach to data augmentation in affective computing.", "link": "http://arxiv.org/abs/2510.10078v4", "date": "2026-02-12", "relevancy": 2.546, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5249}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5092}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Speech%20Emotion%20Recognition%20with%20Mutual%20Information%20Regularized%20Generative%20Model&body=Title%3A%20Improving%20Speech%20Emotion%20Recognition%20with%20Mutual%20Information%20Regularized%20Generative%20Model%0AAuthor%3A%20Chung-Soo%20Ahn%20and%20Rajib%20Rana%20and%20Sunil%20Sivadas%20and%20Carlos%20Busso%20and%20Jagath%20C.%20Rajapakse%0AAbstract%3A%20Lack%20of%20large%2C%20well-annotated%20emotional%20speech%20corpora%20continues%20to%20limit%20the%20performance%20and%20robustness%20of%20speech%20emotion%20recognition%20%28SER%29%2C%20particularly%20as%20models%20grow%20more%20complex%20and%20the%20demand%20for%20multimodal%20systems%20increases.%20While%20generative%20data%20augmentation%20offers%20a%20promising%20solution%2C%20existing%20approaches%20often%20produce%20emotionally%20inconsistent%20samples%20due%20to%20oversimplified%20conditioning%20on%20categorical%20labels.%20This%20paper%20introduces%20a%20novel%20mutual-information-regularised%20generative%20framework%20that%20combines%20cross-modal%20alignment%20with%20feature-level%20synthesis.%20Building%20on%20an%20InfoGAN-style%20architecture%2C%20our%20method%20first%20learns%20a%20semantically%20aligned%20audio-text%20representation%20space%20using%20pre-trained%20transformers%20and%20contrastive%20objectives.%20A%20feature%20generator%20is%20then%20trained%20to%20produce%20emotion-aware%20audio%20features%20while%20employing%20mutual%20information%20as%20a%20quantitative%20regulariser%20to%20ensure%20strong%20dependency%20between%20generated%20features%20and%20their%20conditioning%20variables.%20We%20extend%20this%20approach%20to%20multimodal%20settings%2C%20enabling%20the%20generation%20of%20novel%2C%20paired%20%28audio%2C%20text%29%20features.%20Comprehensive%20evaluation%20on%20three%20benchmark%20datasets%20%28IEMOCAP%2C%20MSP-IMPROV%2C%20MSP-Podcast%29%20demonstrates%20that%20our%20framework%20consistently%20outperforms%20existing%20augmentation%20methods%2C%20achieving%20state-of-the-art%20performance%20with%20improvements%20of%20up%20to%202.6%25%20in%20unimodal%20SER%20and%203.2%25%20in%20multimodal%20emotion%20recognition.%20Most%20importantly%2C%20we%20demonstrate%20that%20mutual%20information%20functions%20as%20both%20a%20regulariser%20and%20a%20measurable%20metric%20for%20generative%20quality%2C%20offering%20a%20systematic%20approach%20to%20data%20augmentation%20in%20affective%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10078v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Speech%2520Emotion%2520Recognition%2520with%2520Mutual%2520Information%2520Regularized%2520Generative%2520Model%26entry.906535625%3DChung-Soo%2520Ahn%2520and%2520Rajib%2520Rana%2520and%2520Sunil%2520Sivadas%2520and%2520Carlos%2520Busso%2520and%2520Jagath%2520C.%2520Rajapakse%26entry.1292438233%3DLack%2520of%2520large%252C%2520well-annotated%2520emotional%2520speech%2520corpora%2520continues%2520to%2520limit%2520the%2520performance%2520and%2520robustness%2520of%2520speech%2520emotion%2520recognition%2520%2528SER%2529%252C%2520particularly%2520as%2520models%2520grow%2520more%2520complex%2520and%2520the%2520demand%2520for%2520multimodal%2520systems%2520increases.%2520While%2520generative%2520data%2520augmentation%2520offers%2520a%2520promising%2520solution%252C%2520existing%2520approaches%2520often%2520produce%2520emotionally%2520inconsistent%2520samples%2520due%2520to%2520oversimplified%2520conditioning%2520on%2520categorical%2520labels.%2520This%2520paper%2520introduces%2520a%2520novel%2520mutual-information-regularised%2520generative%2520framework%2520that%2520combines%2520cross-modal%2520alignment%2520with%2520feature-level%2520synthesis.%2520Building%2520on%2520an%2520InfoGAN-style%2520architecture%252C%2520our%2520method%2520first%2520learns%2520a%2520semantically%2520aligned%2520audio-text%2520representation%2520space%2520using%2520pre-trained%2520transformers%2520and%2520contrastive%2520objectives.%2520A%2520feature%2520generator%2520is%2520then%2520trained%2520to%2520produce%2520emotion-aware%2520audio%2520features%2520while%2520employing%2520mutual%2520information%2520as%2520a%2520quantitative%2520regulariser%2520to%2520ensure%2520strong%2520dependency%2520between%2520generated%2520features%2520and%2520their%2520conditioning%2520variables.%2520We%2520extend%2520this%2520approach%2520to%2520multimodal%2520settings%252C%2520enabling%2520the%2520generation%2520of%2520novel%252C%2520paired%2520%2528audio%252C%2520text%2529%2520features.%2520Comprehensive%2520evaluation%2520on%2520three%2520benchmark%2520datasets%2520%2528IEMOCAP%252C%2520MSP-IMPROV%252C%2520MSP-Podcast%2529%2520demonstrates%2520that%2520our%2520framework%2520consistently%2520outperforms%2520existing%2520augmentation%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520with%2520improvements%2520of%2520up%2520to%25202.6%2525%2520in%2520unimodal%2520SER%2520and%25203.2%2525%2520in%2520multimodal%2520emotion%2520recognition.%2520Most%2520importantly%252C%2520we%2520demonstrate%2520that%2520mutual%2520information%2520functions%2520as%2520both%2520a%2520regulariser%2520and%2520a%2520measurable%2520metric%2520for%2520generative%2520quality%252C%2520offering%2520a%2520systematic%2520approach%2520to%2520data%2520augmentation%2520in%2520affective%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10078v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Speech%20Emotion%20Recognition%20with%20Mutual%20Information%20Regularized%20Generative%20Model&entry.906535625=Chung-Soo%20Ahn%20and%20Rajib%20Rana%20and%20Sunil%20Sivadas%20and%20Carlos%20Busso%20and%20Jagath%20C.%20Rajapakse&entry.1292438233=Lack%20of%20large%2C%20well-annotated%20emotional%20speech%20corpora%20continues%20to%20limit%20the%20performance%20and%20robustness%20of%20speech%20emotion%20recognition%20%28SER%29%2C%20particularly%20as%20models%20grow%20more%20complex%20and%20the%20demand%20for%20multimodal%20systems%20increases.%20While%20generative%20data%20augmentation%20offers%20a%20promising%20solution%2C%20existing%20approaches%20often%20produce%20emotionally%20inconsistent%20samples%20due%20to%20oversimplified%20conditioning%20on%20categorical%20labels.%20This%20paper%20introduces%20a%20novel%20mutual-information-regularised%20generative%20framework%20that%20combines%20cross-modal%20alignment%20with%20feature-level%20synthesis.%20Building%20on%20an%20InfoGAN-style%20architecture%2C%20our%20method%20first%20learns%20a%20semantically%20aligned%20audio-text%20representation%20space%20using%20pre-trained%20transformers%20and%20contrastive%20objectives.%20A%20feature%20generator%20is%20then%20trained%20to%20produce%20emotion-aware%20audio%20features%20while%20employing%20mutual%20information%20as%20a%20quantitative%20regulariser%20to%20ensure%20strong%20dependency%20between%20generated%20features%20and%20their%20conditioning%20variables.%20We%20extend%20this%20approach%20to%20multimodal%20settings%2C%20enabling%20the%20generation%20of%20novel%2C%20paired%20%28audio%2C%20text%29%20features.%20Comprehensive%20evaluation%20on%20three%20benchmark%20datasets%20%28IEMOCAP%2C%20MSP-IMPROV%2C%20MSP-Podcast%29%20demonstrates%20that%20our%20framework%20consistently%20outperforms%20existing%20augmentation%20methods%2C%20achieving%20state-of-the-art%20performance%20with%20improvements%20of%20up%20to%202.6%25%20in%20unimodal%20SER%20and%203.2%25%20in%20multimodal%20emotion%20recognition.%20Most%20importantly%2C%20we%20demonstrate%20that%20mutual%20information%20functions%20as%20both%20a%20regulariser%20and%20a%20measurable%20metric%20for%20generative%20quality%2C%20offering%20a%20systematic%20approach%20to%20data%20augmentation%20in%20affective%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2510.10078v4&entry.124074799=Read"},
{"title": "End-to-End Semantic ID Generation for Generative Advertisement Recommendation", "author": "Jie Jiang and Xinxun Zhang and Enming Zhang and Yuling Xiong and Jun Zhang and Jingwen Wang and Huan Yu and Yuxiang Wang and Hao Wang and Xiao Yan and Jiawei Jiang", "abstract": "Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.", "link": "http://arxiv.org/abs/2602.10445v2", "date": "2026-02-12", "relevancy": 2.5342, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5053}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Semantic%20ID%20Generation%20for%20Generative%20Advertisement%20Recommendation&body=Title%3A%20End-to-End%20Semantic%20ID%20Generation%20for%20Generative%20Advertisement%20Recommendation%0AAuthor%3A%20Jie%20Jiang%20and%20Xinxun%20Zhang%20and%20Enming%20Zhang%20and%20Yuling%20Xiong%20and%20Jun%20Zhang%20and%20Jingwen%20Wang%20and%20Huan%20Yu%20and%20Yuxiang%20Wang%20and%20Hao%20Wang%20and%20Xiao%20Yan%20and%20Jiawei%20Jiang%0AAbstract%3A%20Generative%20Recommendation%20%28GR%29%20has%20excelled%20by%20framing%20recommendation%20as%20next-token%20prediction.%20This%20paradigm%20relies%20on%20Semantic%20IDs%20%28SIDs%29%20to%20tokenize%20large-scale%20items%20into%20discrete%20sequences.%20Existing%20GR%20approaches%20predominantly%20generate%20SIDs%20via%20Residual%20Quantization%20%28RQ%29%2C%20where%20items%20are%20encoded%20into%20embeddings%20and%20then%20quantized%20to%20discrete%20SIDs.%20However%2C%20this%20paradigm%20suffers%20from%20inherent%20limitations%3A%201%29%20Objective%20misalignment%20and%20semantic%20degradation%20stemming%20from%20the%20two-stage%20compression%3B%202%29%20Error%20accumulation%20inherent%20in%20the%20structure%20of%20RQ.%20To%20address%20these%20limitations%2C%20we%20propose%20UniSID%2C%20a%20Unified%20SID%20generation%20framework%20for%20generative%20advertisement%20recommendation.%20Specifically%2C%20we%20jointly%20optimize%20embeddings%20and%20SIDs%20in%20an%20end-to-end%20manner%20from%20raw%20advertising%20data%2C%20enabling%20semantic%20information%20to%20flow%20directly%20into%20the%20SID%20space%20and%20thus%20addressing%20the%20inherent%20limitations%20of%20the%20two-stage%20cascading%20compression%20paradigm.%20To%20capture%20fine-grained%20semantics%2C%20a%20multi-granularity%20contrastive%20learning%20strategy%20is%20introduced%20to%20align%20distinct%20items%20across%20SID%20levels.%20Finally%2C%20a%20summary-based%20ad%20reconstruction%20mechanism%20is%20proposed%20to%20encourage%20SIDs%20to%20capture%20high-level%20semantic%20information%20that%20is%20not%20explicitly%20present%20in%20advertising%20contexts.%20Experiments%20demonstrate%20that%20UniSID%20consistently%20outperforms%20state-of-the-art%20SID%20generation%20methods%2C%20yielding%20up%20to%20a%204.62%25%20improvement%20in%20Hit%20Rate%20metrics%20across%20downstream%20advertising%20scenarios%20compared%20to%20the%20strongest%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Semantic%2520ID%2520Generation%2520for%2520Generative%2520Advertisement%2520Recommendation%26entry.906535625%3DJie%2520Jiang%2520and%2520Xinxun%2520Zhang%2520and%2520Enming%2520Zhang%2520and%2520Yuling%2520Xiong%2520and%2520Jun%2520Zhang%2520and%2520Jingwen%2520Wang%2520and%2520Huan%2520Yu%2520and%2520Yuxiang%2520Wang%2520and%2520Hao%2520Wang%2520and%2520Xiao%2520Yan%2520and%2520Jiawei%2520Jiang%26entry.1292438233%3DGenerative%2520Recommendation%2520%2528GR%2529%2520has%2520excelled%2520by%2520framing%2520recommendation%2520as%2520next-token%2520prediction.%2520This%2520paradigm%2520relies%2520on%2520Semantic%2520IDs%2520%2528SIDs%2529%2520to%2520tokenize%2520large-scale%2520items%2520into%2520discrete%2520sequences.%2520Existing%2520GR%2520approaches%2520predominantly%2520generate%2520SIDs%2520via%2520Residual%2520Quantization%2520%2528RQ%2529%252C%2520where%2520items%2520are%2520encoded%2520into%2520embeddings%2520and%2520then%2520quantized%2520to%2520discrete%2520SIDs.%2520However%252C%2520this%2520paradigm%2520suffers%2520from%2520inherent%2520limitations%253A%25201%2529%2520Objective%2520misalignment%2520and%2520semantic%2520degradation%2520stemming%2520from%2520the%2520two-stage%2520compression%253B%25202%2529%2520Error%2520accumulation%2520inherent%2520in%2520the%2520structure%2520of%2520RQ.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520UniSID%252C%2520a%2520Unified%2520SID%2520generation%2520framework%2520for%2520generative%2520advertisement%2520recommendation.%2520Specifically%252C%2520we%2520jointly%2520optimize%2520embeddings%2520and%2520SIDs%2520in%2520an%2520end-to-end%2520manner%2520from%2520raw%2520advertising%2520data%252C%2520enabling%2520semantic%2520information%2520to%2520flow%2520directly%2520into%2520the%2520SID%2520space%2520and%2520thus%2520addressing%2520the%2520inherent%2520limitations%2520of%2520the%2520two-stage%2520cascading%2520compression%2520paradigm.%2520To%2520capture%2520fine-grained%2520semantics%252C%2520a%2520multi-granularity%2520contrastive%2520learning%2520strategy%2520is%2520introduced%2520to%2520align%2520distinct%2520items%2520across%2520SID%2520levels.%2520Finally%252C%2520a%2520summary-based%2520ad%2520reconstruction%2520mechanism%2520is%2520proposed%2520to%2520encourage%2520SIDs%2520to%2520capture%2520high-level%2520semantic%2520information%2520that%2520is%2520not%2520explicitly%2520present%2520in%2520advertising%2520contexts.%2520Experiments%2520demonstrate%2520that%2520UniSID%2520consistently%2520outperforms%2520state-of-the-art%2520SID%2520generation%2520methods%252C%2520yielding%2520up%2520to%2520a%25204.62%2525%2520improvement%2520in%2520Hit%2520Rate%2520metrics%2520across%2520downstream%2520advertising%2520scenarios%2520compared%2520to%2520the%2520strongest%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Semantic%20ID%20Generation%20for%20Generative%20Advertisement%20Recommendation&entry.906535625=Jie%20Jiang%20and%20Xinxun%20Zhang%20and%20Enming%20Zhang%20and%20Yuling%20Xiong%20and%20Jun%20Zhang%20and%20Jingwen%20Wang%20and%20Huan%20Yu%20and%20Yuxiang%20Wang%20and%20Hao%20Wang%20and%20Xiao%20Yan%20and%20Jiawei%20Jiang&entry.1292438233=Generative%20Recommendation%20%28GR%29%20has%20excelled%20by%20framing%20recommendation%20as%20next-token%20prediction.%20This%20paradigm%20relies%20on%20Semantic%20IDs%20%28SIDs%29%20to%20tokenize%20large-scale%20items%20into%20discrete%20sequences.%20Existing%20GR%20approaches%20predominantly%20generate%20SIDs%20via%20Residual%20Quantization%20%28RQ%29%2C%20where%20items%20are%20encoded%20into%20embeddings%20and%20then%20quantized%20to%20discrete%20SIDs.%20However%2C%20this%20paradigm%20suffers%20from%20inherent%20limitations%3A%201%29%20Objective%20misalignment%20and%20semantic%20degradation%20stemming%20from%20the%20two-stage%20compression%3B%202%29%20Error%20accumulation%20inherent%20in%20the%20structure%20of%20RQ.%20To%20address%20these%20limitations%2C%20we%20propose%20UniSID%2C%20a%20Unified%20SID%20generation%20framework%20for%20generative%20advertisement%20recommendation.%20Specifically%2C%20we%20jointly%20optimize%20embeddings%20and%20SIDs%20in%20an%20end-to-end%20manner%20from%20raw%20advertising%20data%2C%20enabling%20semantic%20information%20to%20flow%20directly%20into%20the%20SID%20space%20and%20thus%20addressing%20the%20inherent%20limitations%20of%20the%20two-stage%20cascading%20compression%20paradigm.%20To%20capture%20fine-grained%20semantics%2C%20a%20multi-granularity%20contrastive%20learning%20strategy%20is%20introduced%20to%20align%20distinct%20items%20across%20SID%20levels.%20Finally%2C%20a%20summary-based%20ad%20reconstruction%20mechanism%20is%20proposed%20to%20encourage%20SIDs%20to%20capture%20high-level%20semantic%20information%20that%20is%20not%20explicitly%20present%20in%20advertising%20contexts.%20Experiments%20demonstrate%20that%20UniSID%20consistently%20outperforms%20state-of-the-art%20SID%20generation%20methods%2C%20yielding%20up%20to%20a%204.62%25%20improvement%20in%20Hit%20Rate%20metrics%20across%20downstream%20advertising%20scenarios%20compared%20to%20the%20strongest%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2602.10445v2&entry.124074799=Read"},
{"title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation", "author": "Xu Guo and Fulong Ye and Qichao Sun and Liyang Chen and Bingchuan Li and Pengze Zhang and Jiawei Liu and Songtao Zhao and Qian He and Xiangwang Hou", "abstract": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.", "link": "http://arxiv.org/abs/2602.12160v1", "date": "2026-02-12", "relevancy": 2.5258, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6834}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5949}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamID-Omni%3A%20Unified%20Framework%20for%20Controllable%20Human-Centric%20Audio-Video%20Generation&body=Title%3A%20DreamID-Omni%3A%20Unified%20Framework%20for%20Controllable%20Human-Centric%20Audio-Video%20Generation%0AAuthor%3A%20Xu%20Guo%20and%20Fulong%20Ye%20and%20Qichao%20Sun%20and%20Liyang%20Chen%20and%20Bingchuan%20Li%20and%20Pengze%20Zhang%20and%20Jiawei%20Liu%20and%20Songtao%20Zhao%20and%20Qian%20He%20and%20Xiangwang%20Hou%0AAbstract%3A%20Recent%20advancements%20in%20foundation%20models%20have%20revolutionized%20joint%20audio-video%20generation.%20However%2C%20existing%20approaches%20typically%20treat%20human-centric%20tasks%20including%20reference-based%20audio-video%20generation%20%28R2AV%29%2C%20video%20editing%20%28RV2AV%29%20and%20audio-driven%20video%20animation%20%28RA2V%29%20as%20isolated%20objectives.%20Furthermore%2C%20achieving%20precise%2C%20disentangled%20control%20over%20multiple%20character%20identities%20and%20voice%20timbres%20within%20a%20single%20framework%20remains%20an%20open%20challenge.%20In%20this%20paper%2C%20we%20propose%20DreamID-Omni%2C%20a%20unified%20framework%20for%20controllable%20human-centric%20audio-video%20generation.%20Specifically%2C%20we%20design%20a%20Symmetric%20Conditional%20Diffusion%20Transformer%20that%20integrates%20heterogeneous%20conditioning%20signals%20via%20a%20symmetric%20conditional%20injection%20scheme.%20To%20resolve%20the%20pervasive%20identity-timbre%20binding%20failures%20and%20speaker%20confusion%20in%20multi-person%20scenarios%2C%20we%20introduce%20a%20Dual-Level%20Disentanglement%20strategy%3A%20Synchronized%20RoPE%20at%20the%20signal%20level%20to%20ensure%20rigid%20attention-space%20binding%2C%20and%20Structured%20Captions%20at%20the%20semantic%20level%20to%20establish%20explicit%20attribute-subject%20mappings.%20Furthermore%2C%20we%20devise%20a%20Multi-Task%20Progressive%20Training%20scheme%20that%20leverages%20weakly-constrained%20generative%20priors%20to%20regularize%20strongly-constrained%20tasks%2C%20preventing%20overfitting%20and%20harmonizing%20disparate%20objectives.%20Extensive%20experiments%20demonstrate%20that%20DreamID-Omni%20achieves%20comprehensive%20state-of-the-art%20performance%20across%20video%2C%20audio%2C%20and%20audio-visual%20consistency%2C%20even%20outperforming%20leading%20proprietary%20commercial%20models.%20We%20will%20release%20our%20code%20to%20bridge%20the%20gap%20between%20academic%20research%20and%20commercial-grade%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamID-Omni%253A%2520Unified%2520Framework%2520for%2520Controllable%2520Human-Centric%2520Audio-Video%2520Generation%26entry.906535625%3DXu%2520Guo%2520and%2520Fulong%2520Ye%2520and%2520Qichao%2520Sun%2520and%2520Liyang%2520Chen%2520and%2520Bingchuan%2520Li%2520and%2520Pengze%2520Zhang%2520and%2520Jiawei%2520Liu%2520and%2520Songtao%2520Zhao%2520and%2520Qian%2520He%2520and%2520Xiangwang%2520Hou%26entry.1292438233%3DRecent%2520advancements%2520in%2520foundation%2520models%2520have%2520revolutionized%2520joint%2520audio-video%2520generation.%2520However%252C%2520existing%2520approaches%2520typically%2520treat%2520human-centric%2520tasks%2520including%2520reference-based%2520audio-video%2520generation%2520%2528R2AV%2529%252C%2520video%2520editing%2520%2528RV2AV%2529%2520and%2520audio-driven%2520video%2520animation%2520%2528RA2V%2529%2520as%2520isolated%2520objectives.%2520Furthermore%252C%2520achieving%2520precise%252C%2520disentangled%2520control%2520over%2520multiple%2520character%2520identities%2520and%2520voice%2520timbres%2520within%2520a%2520single%2520framework%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DreamID-Omni%252C%2520a%2520unified%2520framework%2520for%2520controllable%2520human-centric%2520audio-video%2520generation.%2520Specifically%252C%2520we%2520design%2520a%2520Symmetric%2520Conditional%2520Diffusion%2520Transformer%2520that%2520integrates%2520heterogeneous%2520conditioning%2520signals%2520via%2520a%2520symmetric%2520conditional%2520injection%2520scheme.%2520To%2520resolve%2520the%2520pervasive%2520identity-timbre%2520binding%2520failures%2520and%2520speaker%2520confusion%2520in%2520multi-person%2520scenarios%252C%2520we%2520introduce%2520a%2520Dual-Level%2520Disentanglement%2520strategy%253A%2520Synchronized%2520RoPE%2520at%2520the%2520signal%2520level%2520to%2520ensure%2520rigid%2520attention-space%2520binding%252C%2520and%2520Structured%2520Captions%2520at%2520the%2520semantic%2520level%2520to%2520establish%2520explicit%2520attribute-subject%2520mappings.%2520Furthermore%252C%2520we%2520devise%2520a%2520Multi-Task%2520Progressive%2520Training%2520scheme%2520that%2520leverages%2520weakly-constrained%2520generative%2520priors%2520to%2520regularize%2520strongly-constrained%2520tasks%252C%2520preventing%2520overfitting%2520and%2520harmonizing%2520disparate%2520objectives.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DreamID-Omni%2520achieves%2520comprehensive%2520state-of-the-art%2520performance%2520across%2520video%252C%2520audio%252C%2520and%2520audio-visual%2520consistency%252C%2520even%2520outperforming%2520leading%2520proprietary%2520commercial%2520models.%2520We%2520will%2520release%2520our%2520code%2520to%2520bridge%2520the%2520gap%2520between%2520academic%2520research%2520and%2520commercial-grade%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamID-Omni%3A%20Unified%20Framework%20for%20Controllable%20Human-Centric%20Audio-Video%20Generation&entry.906535625=Xu%20Guo%20and%20Fulong%20Ye%20and%20Qichao%20Sun%20and%20Liyang%20Chen%20and%20Bingchuan%20Li%20and%20Pengze%20Zhang%20and%20Jiawei%20Liu%20and%20Songtao%20Zhao%20and%20Qian%20He%20and%20Xiangwang%20Hou&entry.1292438233=Recent%20advancements%20in%20foundation%20models%20have%20revolutionized%20joint%20audio-video%20generation.%20However%2C%20existing%20approaches%20typically%20treat%20human-centric%20tasks%20including%20reference-based%20audio-video%20generation%20%28R2AV%29%2C%20video%20editing%20%28RV2AV%29%20and%20audio-driven%20video%20animation%20%28RA2V%29%20as%20isolated%20objectives.%20Furthermore%2C%20achieving%20precise%2C%20disentangled%20control%20over%20multiple%20character%20identities%20and%20voice%20timbres%20within%20a%20single%20framework%20remains%20an%20open%20challenge.%20In%20this%20paper%2C%20we%20propose%20DreamID-Omni%2C%20a%20unified%20framework%20for%20controllable%20human-centric%20audio-video%20generation.%20Specifically%2C%20we%20design%20a%20Symmetric%20Conditional%20Diffusion%20Transformer%20that%20integrates%20heterogeneous%20conditioning%20signals%20via%20a%20symmetric%20conditional%20injection%20scheme.%20To%20resolve%20the%20pervasive%20identity-timbre%20binding%20failures%20and%20speaker%20confusion%20in%20multi-person%20scenarios%2C%20we%20introduce%20a%20Dual-Level%20Disentanglement%20strategy%3A%20Synchronized%20RoPE%20at%20the%20signal%20level%20to%20ensure%20rigid%20attention-space%20binding%2C%20and%20Structured%20Captions%20at%20the%20semantic%20level%20to%20establish%20explicit%20attribute-subject%20mappings.%20Furthermore%2C%20we%20devise%20a%20Multi-Task%20Progressive%20Training%20scheme%20that%20leverages%20weakly-constrained%20generative%20priors%20to%20regularize%20strongly-constrained%20tasks%2C%20preventing%20overfitting%20and%20harmonizing%20disparate%20objectives.%20Extensive%20experiments%20demonstrate%20that%20DreamID-Omni%20achieves%20comprehensive%20state-of-the-art%20performance%20across%20video%2C%20audio%2C%20and%20audio-visual%20consistency%2C%20even%20outperforming%20leading%20proprietary%20commercial%20models.%20We%20will%20release%20our%20code%20to%20bridge%20the%20gap%20between%20academic%20research%20and%20commercial-grade%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.12160v1&entry.124074799=Read"},
{"title": "Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish", "author": "Chengxuan Xia and Qianye Wu and Hongbin Guan and Sixuan Tian and Yilun Hao and Xiaoyu Wu", "abstract": "Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \\textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \\textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.\n  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.", "link": "http://arxiv.org/abs/2511.10664v2", "date": "2026-02-12", "relevancy": 2.5096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Modern%20Large%20Language%20Models%20on%20Low-Resource%20and%20Morphologically%20Rich%20Languages%3AA%20Cross-Lingual%20Benchmark%20Across%20Cantonese%2C%20Japanese%2C%20and%20Turkish&body=Title%3A%20Evaluating%20Modern%20Large%20Language%20Models%20on%20Low-Resource%20and%20Morphologically%20Rich%20Languages%3AA%20Cross-Lingual%20Benchmark%20Across%20Cantonese%2C%20Japanese%2C%20and%20Turkish%0AAuthor%3A%20Chengxuan%20Xia%20and%20Qianye%20Wu%20and%20Hongbin%20Guan%20and%20Sixuan%20Tian%20and%20Yilun%20Hao%20and%20Xiaoyu%20Wu%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20results%20in%20high-resource%20languages%20like%20English%2C%20yet%20their%20effectiveness%20in%20low-resource%20and%20morphologically%20rich%20languages%20remains%20underexplored.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20evaluation%20of%20seven%20cutting-edge%20LLMs%20--%20including%20GPT-4o%2C%20GPT-4%2C%20Claude~3.5~Sonnet%2C%20LLaMA~3.1%2C%20Mistral~Large~2%2C%20LLaMA-2~Chat~13B%2C%20and%20Mistral~7B~Instruct%20--%20on%20a%20new%20cross-lingual%20benchmark%20covering%20%5Ctextbf%7BCantonese%2C%20Japanese%2C%20and%20Turkish%7D.%20Our%20benchmark%20spans%20four%20diverse%20tasks%3A%20open-domain%20question%20answering%2C%20document%20summarization%2C%20English-to-X%20translation%2C%20and%20culturally%20grounded%20dialogue.%20We%20combine%20%5Ctextbf%7Bhuman%20evaluations%7D%20%28rating%20fluency%2C%20factual%20accuracy%2C%20and%20cultural%20appropriateness%29%20with%20automated%20metrics%20%28e.g.%2C%20BLEU%2C%20ROUGE%29%20to%20assess%20model%20performance.%0A%20%20Our%20results%20reveal%20that%20while%20the%20largest%20proprietary%20models%20%28GPT-4o%2C%20GPT-4%2C%20Claude~3.5%29%20generally%20lead%20across%20languages%20and%20tasks%2C%20significant%20gaps%20persist%20in%20culturally%20nuanced%20understanding%20and%20morphological%20generalization.%20Notably%2C%20GPT-4o%20demonstrates%20robust%20multilingual%20performance%20even%20on%20cross-lingual%20tasks%2C%20and%20Claude~3.5~Sonnet%20achieves%20competitive%20accuracy%20on%20knowledge%20and%20reasoning%20benchmarks.%20However%2C%20all%20models%20struggle%20to%20some%20extent%20with%20the%20unique%20linguistic%20challenges%20of%20each%20language%2C%20such%20as%20Turkish%20agglutinative%20morphology%20and%20Cantonese%20colloquialisms.%20Smaller%20open-source%20models%20%28LLaMA-2~13B%2C%20Mistral~7B%29%20lag%20substantially%20in%20fluency%20and%20accuracy%2C%20highlighting%20the%20resource%20disparity.%20We%20provide%20detailed%20quantitative%20results%2C%20qualitative%20error%20analysis%2C%20and%20discuss%20implications%20for%20developing%20more%20culturally%20aware%20and%20linguistically%20generalizable%20LLMs.%20Our%20benchmark%20and%20evaluation%20data%20are%20released%20to%20foster%20reproducibility%20and%20further%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Modern%2520Large%2520Language%2520Models%2520on%2520Low-Resource%2520and%2520Morphologically%2520Rich%2520Languages%253AA%2520Cross-Lingual%2520Benchmark%2520Across%2520Cantonese%252C%2520Japanese%252C%2520and%2520Turkish%26entry.906535625%3DChengxuan%2520Xia%2520and%2520Qianye%2520Wu%2520and%2520Hongbin%2520Guan%2520and%2520Sixuan%2520Tian%2520and%2520Yilun%2520Hao%2520and%2520Xiaoyu%2520Wu%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520results%2520in%2520high-resource%2520languages%2520like%2520English%252C%2520yet%2520their%2520effectiveness%2520in%2520low-resource%2520and%2520morphologically%2520rich%2520languages%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520evaluation%2520of%2520seven%2520cutting-edge%2520LLMs%2520--%2520including%2520GPT-4o%252C%2520GPT-4%252C%2520Claude~3.5~Sonnet%252C%2520LLaMA~3.1%252C%2520Mistral~Large~2%252C%2520LLaMA-2~Chat~13B%252C%2520and%2520Mistral~7B~Instruct%2520--%2520on%2520a%2520new%2520cross-lingual%2520benchmark%2520covering%2520%255Ctextbf%257BCantonese%252C%2520Japanese%252C%2520and%2520Turkish%257D.%2520Our%2520benchmark%2520spans%2520four%2520diverse%2520tasks%253A%2520open-domain%2520question%2520answering%252C%2520document%2520summarization%252C%2520English-to-X%2520translation%252C%2520and%2520culturally%2520grounded%2520dialogue.%2520We%2520combine%2520%255Ctextbf%257Bhuman%2520evaluations%257D%2520%2528rating%2520fluency%252C%2520factual%2520accuracy%252C%2520and%2520cultural%2520appropriateness%2529%2520with%2520automated%2520metrics%2520%2528e.g.%252C%2520BLEU%252C%2520ROUGE%2529%2520to%2520assess%2520model%2520performance.%250A%2520%2520Our%2520results%2520reveal%2520that%2520while%2520the%2520largest%2520proprietary%2520models%2520%2528GPT-4o%252C%2520GPT-4%252C%2520Claude~3.5%2529%2520generally%2520lead%2520across%2520languages%2520and%2520tasks%252C%2520significant%2520gaps%2520persist%2520in%2520culturally%2520nuanced%2520understanding%2520and%2520morphological%2520generalization.%2520Notably%252C%2520GPT-4o%2520demonstrates%2520robust%2520multilingual%2520performance%2520even%2520on%2520cross-lingual%2520tasks%252C%2520and%2520Claude~3.5~Sonnet%2520achieves%2520competitive%2520accuracy%2520on%2520knowledge%2520and%2520reasoning%2520benchmarks.%2520However%252C%2520all%2520models%2520struggle%2520to%2520some%2520extent%2520with%2520the%2520unique%2520linguistic%2520challenges%2520of%2520each%2520language%252C%2520such%2520as%2520Turkish%2520agglutinative%2520morphology%2520and%2520Cantonese%2520colloquialisms.%2520Smaller%2520open-source%2520models%2520%2528LLaMA-2~13B%252C%2520Mistral~7B%2529%2520lag%2520substantially%2520in%2520fluency%2520and%2520accuracy%252C%2520highlighting%2520the%2520resource%2520disparity.%2520We%2520provide%2520detailed%2520quantitative%2520results%252C%2520qualitative%2520error%2520analysis%252C%2520and%2520discuss%2520implications%2520for%2520developing%2520more%2520culturally%2520aware%2520and%2520linguistically%2520generalizable%2520LLMs.%2520Our%2520benchmark%2520and%2520evaluation%2520data%2520are%2520released%2520to%2520foster%2520reproducibility%2520and%2520further%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Modern%20Large%20Language%20Models%20on%20Low-Resource%20and%20Morphologically%20Rich%20Languages%3AA%20Cross-Lingual%20Benchmark%20Across%20Cantonese%2C%20Japanese%2C%20and%20Turkish&entry.906535625=Chengxuan%20Xia%20and%20Qianye%20Wu%20and%20Hongbin%20Guan%20and%20Sixuan%20Tian%20and%20Yilun%20Hao%20and%20Xiaoyu%20Wu&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20results%20in%20high-resource%20languages%20like%20English%2C%20yet%20their%20effectiveness%20in%20low-resource%20and%20morphologically%20rich%20languages%20remains%20underexplored.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20evaluation%20of%20seven%20cutting-edge%20LLMs%20--%20including%20GPT-4o%2C%20GPT-4%2C%20Claude~3.5~Sonnet%2C%20LLaMA~3.1%2C%20Mistral~Large~2%2C%20LLaMA-2~Chat~13B%2C%20and%20Mistral~7B~Instruct%20--%20on%20a%20new%20cross-lingual%20benchmark%20covering%20%5Ctextbf%7BCantonese%2C%20Japanese%2C%20and%20Turkish%7D.%20Our%20benchmark%20spans%20four%20diverse%20tasks%3A%20open-domain%20question%20answering%2C%20document%20summarization%2C%20English-to-X%20translation%2C%20and%20culturally%20grounded%20dialogue.%20We%20combine%20%5Ctextbf%7Bhuman%20evaluations%7D%20%28rating%20fluency%2C%20factual%20accuracy%2C%20and%20cultural%20appropriateness%29%20with%20automated%20metrics%20%28e.g.%2C%20BLEU%2C%20ROUGE%29%20to%20assess%20model%20performance.%0A%20%20Our%20results%20reveal%20that%20while%20the%20largest%20proprietary%20models%20%28GPT-4o%2C%20GPT-4%2C%20Claude~3.5%29%20generally%20lead%20across%20languages%20and%20tasks%2C%20significant%20gaps%20persist%20in%20culturally%20nuanced%20understanding%20and%20morphological%20generalization.%20Notably%2C%20GPT-4o%20demonstrates%20robust%20multilingual%20performance%20even%20on%20cross-lingual%20tasks%2C%20and%20Claude~3.5~Sonnet%20achieves%20competitive%20accuracy%20on%20knowledge%20and%20reasoning%20benchmarks.%20However%2C%20all%20models%20struggle%20to%20some%20extent%20with%20the%20unique%20linguistic%20challenges%20of%20each%20language%2C%20such%20as%20Turkish%20agglutinative%20morphology%20and%20Cantonese%20colloquialisms.%20Smaller%20open-source%20models%20%28LLaMA-2~13B%2C%20Mistral~7B%29%20lag%20substantially%20in%20fluency%20and%20accuracy%2C%20highlighting%20the%20resource%20disparity.%20We%20provide%20detailed%20quantitative%20results%2C%20qualitative%20error%20analysis%2C%20and%20discuss%20implications%20for%20developing%20more%20culturally%20aware%20and%20linguistically%20generalizable%20LLMs.%20Our%20benchmark%20and%20evaluation%20data%20are%20released%20to%20foster%20reproducibility%20and%20further%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.10664v2&entry.124074799=Read"},
{"title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting", "author": "Wancai Zheng and Hao Chen and Xianlong Lu and Linlin Ou and Xinyi Yu", "abstract": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/", "link": "http://arxiv.org/abs/2602.12159v1", "date": "2026-02-12", "relevancy": 2.4968, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6269}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6226}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGSNav%3A%20Enhancing%20Vision-Language%20Model%20Reasoning%20for%20Object%20Navigation%20via%20Active%203D%20Gaussian%20Splatting&body=Title%3A%203DGSNav%3A%20Enhancing%20Vision-Language%20Model%20Reasoning%20for%20Object%20Navigation%20via%20Active%203D%20Gaussian%20Splatting%0AAuthor%3A%20Wancai%20Zheng%20and%20Hao%20Chen%20and%20Xianlong%20Lu%20and%20Linlin%20Ou%20and%20Xinyi%20Yu%0AAbstract%3A%20Object%20navigation%20is%20a%20core%20capability%20of%20embodied%20intelligence%2C%20enabling%20an%20agent%20to%20locate%20target%20objects%20in%20unknown%20environments.%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20facilitated%20zero-shot%20object%20navigation%20%28ZSON%29.%20However%2C%20existing%20methods%20often%20rely%20on%20scene%20abstractions%20that%20convert%20environments%20into%20semantic%20maps%20or%20textual%20representations%2C%20causing%20high-level%20decision%20making%20to%20be%20constrained%20by%20the%20accuracy%20of%20low-level%20perception.%20In%20this%20work%2C%20we%20present%203DGSNav%2C%20a%20novel%20ZSON%20framework%20that%20embeds%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20persistent%20memory%20for%20VLMs%20to%20enhance%20spatial%20reasoning.%20Through%20active%20perception%2C%203DGSNav%20incrementally%20constructs%20a%203DGS%20representation%20of%20the%20environment%2C%20enabling%20trajectory-guided%20free-viewpoint%20rendering%20of%20frontier-aware%20first-person%20views.%20Moreover%2C%20we%20design%20structured%20visual%20prompts%20and%20integrate%20them%20with%20Chain-of-Thought%20%28CoT%29%20prompting%20to%20further%20improve%20VLM%20reasoning.%20During%20navigation%2C%20a%20real-time%20object%20detector%20filters%20potential%20targets%2C%20while%20VLM-driven%20active%20viewpoint%20switching%20performs%20target%20re-verification%2C%20ensuring%20efficient%20and%20reliable%20recognition.%20Extensive%20evaluations%20across%20multiple%20benchmarks%20and%20real-world%20experiments%20on%20a%20quadruped%20robot%20demonstrate%20that%20our%20method%20achieves%20robust%20and%20competitive%20performance%20against%20state-of-the-art%20approaches.The%20Project%20Page%3Ahttps%3A//aczheng-cai.github.io/3dgsnav.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2602.12159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGSNav%253A%2520Enhancing%2520Vision-Language%2520Model%2520Reasoning%2520for%2520Object%2520Navigation%2520via%2520Active%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DWancai%2520Zheng%2520and%2520Hao%2520Chen%2520and%2520Xianlong%2520Lu%2520and%2520Linlin%2520Ou%2520and%2520Xinyi%2520Yu%26entry.1292438233%3DObject%2520navigation%2520is%2520a%2520core%2520capability%2520of%2520embodied%2520intelligence%252C%2520enabling%2520an%2520agent%2520to%2520locate%2520target%2520objects%2520in%2520unknown%2520environments.%2520Recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520facilitated%2520zero-shot%2520object%2520navigation%2520%2528ZSON%2529.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%2520scene%2520abstractions%2520that%2520convert%2520environments%2520into%2520semantic%2520maps%2520or%2520textual%2520representations%252C%2520causing%2520high-level%2520decision%2520making%2520to%2520be%2520constrained%2520by%2520the%2520accuracy%2520of%2520low-level%2520perception.%2520In%2520this%2520work%252C%2520we%2520present%25203DGSNav%252C%2520a%2520novel%2520ZSON%2520framework%2520that%2520embeds%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520as%2520persistent%2520memory%2520for%2520VLMs%2520to%2520enhance%2520spatial%2520reasoning.%2520Through%2520active%2520perception%252C%25203DGSNav%2520incrementally%2520constructs%2520a%25203DGS%2520representation%2520of%2520the%2520environment%252C%2520enabling%2520trajectory-guided%2520free-viewpoint%2520rendering%2520of%2520frontier-aware%2520first-person%2520views.%2520Moreover%252C%2520we%2520design%2520structured%2520visual%2520prompts%2520and%2520integrate%2520them%2520with%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520to%2520further%2520improve%2520VLM%2520reasoning.%2520During%2520navigation%252C%2520a%2520real-time%2520object%2520detector%2520filters%2520potential%2520targets%252C%2520while%2520VLM-driven%2520active%2520viewpoint%2520switching%2520performs%2520target%2520re-verification%252C%2520ensuring%2520efficient%2520and%2520reliable%2520recognition.%2520Extensive%2520evaluations%2520across%2520multiple%2520benchmarks%2520and%2520real-world%2520experiments%2520on%2520a%2520quadruped%2520robot%2520demonstrate%2520that%2520our%2520method%2520achieves%2520robust%2520and%2520competitive%2520performance%2520against%2520state-of-the-art%2520approaches.The%2520Project%2520Page%253Ahttps%253A//aczheng-cai.github.io/3dgsnav.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGSNav%3A%20Enhancing%20Vision-Language%20Model%20Reasoning%20for%20Object%20Navigation%20via%20Active%203D%20Gaussian%20Splatting&entry.906535625=Wancai%20Zheng%20and%20Hao%20Chen%20and%20Xianlong%20Lu%20and%20Linlin%20Ou%20and%20Xinyi%20Yu&entry.1292438233=Object%20navigation%20is%20a%20core%20capability%20of%20embodied%20intelligence%2C%20enabling%20an%20agent%20to%20locate%20target%20objects%20in%20unknown%20environments.%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20facilitated%20zero-shot%20object%20navigation%20%28ZSON%29.%20However%2C%20existing%20methods%20often%20rely%20on%20scene%20abstractions%20that%20convert%20environments%20into%20semantic%20maps%20or%20textual%20representations%2C%20causing%20high-level%20decision%20making%20to%20be%20constrained%20by%20the%20accuracy%20of%20low-level%20perception.%20In%20this%20work%2C%20we%20present%203DGSNav%2C%20a%20novel%20ZSON%20framework%20that%20embeds%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20persistent%20memory%20for%20VLMs%20to%20enhance%20spatial%20reasoning.%20Through%20active%20perception%2C%203DGSNav%20incrementally%20constructs%20a%203DGS%20representation%20of%20the%20environment%2C%20enabling%20trajectory-guided%20free-viewpoint%20rendering%20of%20frontier-aware%20first-person%20views.%20Moreover%2C%20we%20design%20structured%20visual%20prompts%20and%20integrate%20them%20with%20Chain-of-Thought%20%28CoT%29%20prompting%20to%20further%20improve%20VLM%20reasoning.%20During%20navigation%2C%20a%20real-time%20object%20detector%20filters%20potential%20targets%2C%20while%20VLM-driven%20active%20viewpoint%20switching%20performs%20target%20re-verification%2C%20ensuring%20efficient%20and%20reliable%20recognition.%20Extensive%20evaluations%20across%20multiple%20benchmarks%20and%20real-world%20experiments%20on%20a%20quadruped%20robot%20demonstrate%20that%20our%20method%20achieves%20robust%20and%20competitive%20performance%20against%20state-of-the-art%20approaches.The%20Project%20Page%3Ahttps%3A//aczheng-cai.github.io/3dgsnav.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2602.12159v1&entry.124074799=Read"},
{"title": "Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs", "author": "Yongcun Song and Xiaoming Yuan and Hangrui Yue and Tianyou Zeng", "abstract": "We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.", "link": "http://arxiv.org/abs/2602.12273v1", "date": "2026-02-12", "relevancy": 2.4726, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5139}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4873}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Control%3A%20The%20iUzawa-Net%20for%20Nonsmooth%20Optimal%20Control%20of%20Linear%20PDEs&body=Title%3A%20Learning%20to%20Control%3A%20The%20iUzawa-Net%20for%20Nonsmooth%20Optimal%20Control%20of%20Linear%20PDEs%0AAuthor%3A%20Yongcun%20Song%20and%20Xiaoming%20Yuan%20and%20Hangrui%20Yue%20and%20Tianyou%20Zeng%0AAbstract%3A%20We%20propose%20an%20optimization-informed%20deep%20neural%20network%20approach%2C%20named%20iUzawa-Net%2C%20aiming%20for%20the%20first%20solver%20that%20enables%20real-time%20solutions%20for%20a%20class%20of%20nonsmooth%20optimal%20control%20problems%20of%20linear%20partial%20differential%20equations%20%28PDEs%29.%20The%20iUzawa-Net%20unrolls%20an%20inexact%20Uzawa%20method%20for%20saddle%20point%20problems%2C%20replacing%20classical%20preconditioners%20and%20PDE%20solvers%20with%20specifically%20designed%20learnable%20neural%20networks.%20We%20prove%20universal%20approximation%20properties%20and%20establish%20the%20asymptotic%20%24%5Cvarepsilon%24-optimality%20for%20the%20iUzawa-Net%2C%20and%20validate%20its%20promising%20numerical%20efficiency%20through%20nonsmooth%20elliptic%20and%20parabolic%20optimal%20control%20problems.%20Our%20techniques%20offer%20a%20versatile%20framework%20for%20designing%20and%20analyzing%20various%20optimization-informed%20deep%20learning%20approaches%20to%20optimal%20control%20and%20other%20PDE-constrained%20optimization%20problems.%20The%20proposed%20learning-to-control%20approach%20synergizes%20model-based%20optimization%20algorithms%20and%20data-driven%20deep%20learning%20techniques%2C%20inheriting%20the%20merits%20of%20both%20methodologies.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Control%253A%2520The%2520iUzawa-Net%2520for%2520Nonsmooth%2520Optimal%2520Control%2520of%2520Linear%2520PDEs%26entry.906535625%3DYongcun%2520Song%2520and%2520Xiaoming%2520Yuan%2520and%2520Hangrui%2520Yue%2520and%2520Tianyou%2520Zeng%26entry.1292438233%3DWe%2520propose%2520an%2520optimization-informed%2520deep%2520neural%2520network%2520approach%252C%2520named%2520iUzawa-Net%252C%2520aiming%2520for%2520the%2520first%2520solver%2520that%2520enables%2520real-time%2520solutions%2520for%2520a%2520class%2520of%2520nonsmooth%2520optimal%2520control%2520problems%2520of%2520linear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520The%2520iUzawa-Net%2520unrolls%2520an%2520inexact%2520Uzawa%2520method%2520for%2520saddle%2520point%2520problems%252C%2520replacing%2520classical%2520preconditioners%2520and%2520PDE%2520solvers%2520with%2520specifically%2520designed%2520learnable%2520neural%2520networks.%2520We%2520prove%2520universal%2520approximation%2520properties%2520and%2520establish%2520the%2520asymptotic%2520%2524%255Cvarepsilon%2524-optimality%2520for%2520the%2520iUzawa-Net%252C%2520and%2520validate%2520its%2520promising%2520numerical%2520efficiency%2520through%2520nonsmooth%2520elliptic%2520and%2520parabolic%2520optimal%2520control%2520problems.%2520Our%2520techniques%2520offer%2520a%2520versatile%2520framework%2520for%2520designing%2520and%2520analyzing%2520various%2520optimization-informed%2520deep%2520learning%2520approaches%2520to%2520optimal%2520control%2520and%2520other%2520PDE-constrained%2520optimization%2520problems.%2520The%2520proposed%2520learning-to-control%2520approach%2520synergizes%2520model-based%2520optimization%2520algorithms%2520and%2520data-driven%2520deep%2520learning%2520techniques%252C%2520inheriting%2520the%2520merits%2520of%2520both%2520methodologies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Control%3A%20The%20iUzawa-Net%20for%20Nonsmooth%20Optimal%20Control%20of%20Linear%20PDEs&entry.906535625=Yongcun%20Song%20and%20Xiaoming%20Yuan%20and%20Hangrui%20Yue%20and%20Tianyou%20Zeng&entry.1292438233=We%20propose%20an%20optimization-informed%20deep%20neural%20network%20approach%2C%20named%20iUzawa-Net%2C%20aiming%20for%20the%20first%20solver%20that%20enables%20real-time%20solutions%20for%20a%20class%20of%20nonsmooth%20optimal%20control%20problems%20of%20linear%20partial%20differential%20equations%20%28PDEs%29.%20The%20iUzawa-Net%20unrolls%20an%20inexact%20Uzawa%20method%20for%20saddle%20point%20problems%2C%20replacing%20classical%20preconditioners%20and%20PDE%20solvers%20with%20specifically%20designed%20learnable%20neural%20networks.%20We%20prove%20universal%20approximation%20properties%20and%20establish%20the%20asymptotic%20%24%5Cvarepsilon%24-optimality%20for%20the%20iUzawa-Net%2C%20and%20validate%20its%20promising%20numerical%20efficiency%20through%20nonsmooth%20elliptic%20and%20parabolic%20optimal%20control%20problems.%20Our%20techniques%20offer%20a%20versatile%20framework%20for%20designing%20and%20analyzing%20various%20optimization-informed%20deep%20learning%20approaches%20to%20optimal%20control%20and%20other%20PDE-constrained%20optimization%20problems.%20The%20proposed%20learning-to-control%20approach%20synergizes%20model-based%20optimization%20algorithms%20and%20data-driven%20deep%20learning%20techniques%2C%20inheriting%20the%20merits%20of%20both%20methodologies.&entry.1838667208=http%3A//arxiv.org/abs/2602.12273v1&entry.124074799=Read"},
{"title": "DiffPlace: Street View Generation via Place-Controllable Diffusion Model Enhancing Place Recognition", "author": "Ji Li and Zhiwei Li and Shihao Li and Zhenjiang Yu and Boyang Wang and Haiou Liu", "abstract": "Generative models have advanced significantly in realistic image synthesis, with diffusion models excelling in quality and stability. Recent multi-view diffusion models improve 3D-aware street view generation, but they struggle to produce place-aware and background-consistent urban scenes from text, BEV maps, and object bounding boxes. This limits their effectiveness in generating realistic samples for place recognition tasks. To address these challenges, we propose DiffPlace, a novel framework that introduces a place-ID controller to enable place-controllable multi-view image generation. The place-ID controller employs linear projection, perceiver transformer, and contrastive learning to map place-ID embeddings into a fixed CLIP space, allowing the model to synthesize images with consistent background buildings while flexibly modifying foreground objects and weather conditions. Extensive experiments, including quantitative comparisons and augmented training evaluations, demonstrate that DiffPlace outperforms existing methods in both generation quality and training support for visual place recognition. Our results highlight the potential of generative models in enhancing scene-level and place-aware synthesis, providing a valuable approach for improving place recognition in autonomous driving", "link": "http://arxiv.org/abs/2602.11875v1", "date": "2026-02-12", "relevancy": 2.4724, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.621}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.618}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffPlace%3A%20Street%20View%20Generation%20via%20Place-Controllable%20Diffusion%20Model%20Enhancing%20Place%20Recognition&body=Title%3A%20DiffPlace%3A%20Street%20View%20Generation%20via%20Place-Controllable%20Diffusion%20Model%20Enhancing%20Place%20Recognition%0AAuthor%3A%20Ji%20Li%20and%20Zhiwei%20Li%20and%20Shihao%20Li%20and%20Zhenjiang%20Yu%20and%20Boyang%20Wang%20and%20Haiou%20Liu%0AAbstract%3A%20Generative%20models%20have%20advanced%20significantly%20in%20realistic%20image%20synthesis%2C%20with%20diffusion%20models%20excelling%20in%20quality%20and%20stability.%20Recent%20multi-view%20diffusion%20models%20improve%203D-aware%20street%20view%20generation%2C%20but%20they%20struggle%20to%20produce%20place-aware%20and%20background-consistent%20urban%20scenes%20from%20text%2C%20BEV%20maps%2C%20and%20object%20bounding%20boxes.%20This%20limits%20their%20effectiveness%20in%20generating%20realistic%20samples%20for%20place%20recognition%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20DiffPlace%2C%20a%20novel%20framework%20that%20introduces%20a%20place-ID%20controller%20to%20enable%20place-controllable%20multi-view%20image%20generation.%20The%20place-ID%20controller%20employs%20linear%20projection%2C%20perceiver%20transformer%2C%20and%20contrastive%20learning%20to%20map%20place-ID%20embeddings%20into%20a%20fixed%20CLIP%20space%2C%20allowing%20the%20model%20to%20synthesize%20images%20with%20consistent%20background%20buildings%20while%20flexibly%20modifying%20foreground%20objects%20and%20weather%20conditions.%20Extensive%20experiments%2C%20including%20quantitative%20comparisons%20and%20augmented%20training%20evaluations%2C%20demonstrate%20that%20DiffPlace%20outperforms%20existing%20methods%20in%20both%20generation%20quality%20and%20training%20support%20for%20visual%20place%20recognition.%20Our%20results%20highlight%20the%20potential%20of%20generative%20models%20in%20enhancing%20scene-level%20and%20place-aware%20synthesis%2C%20providing%20a%20valuable%20approach%20for%20improving%20place%20recognition%20in%20autonomous%20driving%0ALink%3A%20http%3A//arxiv.org/abs/2602.11875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffPlace%253A%2520Street%2520View%2520Generation%2520via%2520Place-Controllable%2520Diffusion%2520Model%2520Enhancing%2520Place%2520Recognition%26entry.906535625%3DJi%2520Li%2520and%2520Zhiwei%2520Li%2520and%2520Shihao%2520Li%2520and%2520Zhenjiang%2520Yu%2520and%2520Boyang%2520Wang%2520and%2520Haiou%2520Liu%26entry.1292438233%3DGenerative%2520models%2520have%2520advanced%2520significantly%2520in%2520realistic%2520image%2520synthesis%252C%2520with%2520diffusion%2520models%2520excelling%2520in%2520quality%2520and%2520stability.%2520Recent%2520multi-view%2520diffusion%2520models%2520improve%25203D-aware%2520street%2520view%2520generation%252C%2520but%2520they%2520struggle%2520to%2520produce%2520place-aware%2520and%2520background-consistent%2520urban%2520scenes%2520from%2520text%252C%2520BEV%2520maps%252C%2520and%2520object%2520bounding%2520boxes.%2520This%2520limits%2520their%2520effectiveness%2520in%2520generating%2520realistic%2520samples%2520for%2520place%2520recognition%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DiffPlace%252C%2520a%2520novel%2520framework%2520that%2520introduces%2520a%2520place-ID%2520controller%2520to%2520enable%2520place-controllable%2520multi-view%2520image%2520generation.%2520The%2520place-ID%2520controller%2520employs%2520linear%2520projection%252C%2520perceiver%2520transformer%252C%2520and%2520contrastive%2520learning%2520to%2520map%2520place-ID%2520embeddings%2520into%2520a%2520fixed%2520CLIP%2520space%252C%2520allowing%2520the%2520model%2520to%2520synthesize%2520images%2520with%2520consistent%2520background%2520buildings%2520while%2520flexibly%2520modifying%2520foreground%2520objects%2520and%2520weather%2520conditions.%2520Extensive%2520experiments%252C%2520including%2520quantitative%2520comparisons%2520and%2520augmented%2520training%2520evaluations%252C%2520demonstrate%2520that%2520DiffPlace%2520outperforms%2520existing%2520methods%2520in%2520both%2520generation%2520quality%2520and%2520training%2520support%2520for%2520visual%2520place%2520recognition.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520generative%2520models%2520in%2520enhancing%2520scene-level%2520and%2520place-aware%2520synthesis%252C%2520providing%2520a%2520valuable%2520approach%2520for%2520improving%2520place%2520recognition%2520in%2520autonomous%2520driving%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPlace%3A%20Street%20View%20Generation%20via%20Place-Controllable%20Diffusion%20Model%20Enhancing%20Place%20Recognition&entry.906535625=Ji%20Li%20and%20Zhiwei%20Li%20and%20Shihao%20Li%20and%20Zhenjiang%20Yu%20and%20Boyang%20Wang%20and%20Haiou%20Liu&entry.1292438233=Generative%20models%20have%20advanced%20significantly%20in%20realistic%20image%20synthesis%2C%20with%20diffusion%20models%20excelling%20in%20quality%20and%20stability.%20Recent%20multi-view%20diffusion%20models%20improve%203D-aware%20street%20view%20generation%2C%20but%20they%20struggle%20to%20produce%20place-aware%20and%20background-consistent%20urban%20scenes%20from%20text%2C%20BEV%20maps%2C%20and%20object%20bounding%20boxes.%20This%20limits%20their%20effectiveness%20in%20generating%20realistic%20samples%20for%20place%20recognition%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20DiffPlace%2C%20a%20novel%20framework%20that%20introduces%20a%20place-ID%20controller%20to%20enable%20place-controllable%20multi-view%20image%20generation.%20The%20place-ID%20controller%20employs%20linear%20projection%2C%20perceiver%20transformer%2C%20and%20contrastive%20learning%20to%20map%20place-ID%20embeddings%20into%20a%20fixed%20CLIP%20space%2C%20allowing%20the%20model%20to%20synthesize%20images%20with%20consistent%20background%20buildings%20while%20flexibly%20modifying%20foreground%20objects%20and%20weather%20conditions.%20Extensive%20experiments%2C%20including%20quantitative%20comparisons%20and%20augmented%20training%20evaluations%2C%20demonstrate%20that%20DiffPlace%20outperforms%20existing%20methods%20in%20both%20generation%20quality%20and%20training%20support%20for%20visual%20place%20recognition.%20Our%20results%20highlight%20the%20potential%20of%20generative%20models%20in%20enhancing%20scene-level%20and%20place-aware%20synthesis%2C%20providing%20a%20valuable%20approach%20for%20improving%20place%20recognition%20in%20autonomous%20driving&entry.1838667208=http%3A//arxiv.org/abs/2602.11875v1&entry.124074799=Read"},
{"title": "Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization", "author": "Suyash Mishra and Qiang Li and Anubhav Girdhar", "abstract": "Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses.", "link": "http://arxiv.org/abs/2602.11957v1", "date": "2026-02-12", "relevancy": 2.4541, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Two%20LLMs%20Better%20Than%20One%3F%20A%20Student-Teacher%20Dual-Head%20LLMs%20Architecture%20for%20Pharmaceutical%20Content%20Optimization&body=Title%3A%20Are%20Two%20LLMs%20Better%20Than%20One%3F%20A%20Student-Teacher%20Dual-Head%20LLMs%20Architecture%20for%20Pharmaceutical%20Content%20Optimization%0AAuthor%3A%20Suyash%20Mishra%20and%20Qiang%20Li%20and%20Anubhav%20Girdhar%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20to%20create%20content%20in%20regulated%20domains%20such%20as%20pharmaceuticals%2C%20where%20outputs%20must%20be%20scientifically%20accurate%20and%20legally%20compliant.%20Manual%20quality%20control%20%28QC%29%20is%20slow%2C%20error%20prone%2C%20and%20can%20become%20a%20publication%20bottleneck.%20We%20introduce%20LRBTC%2C%20a%20modular%20LLM%20and%20vision%20language%20model%20%28VLM%29%20driven%20QC%20architecture%20covering%20Language%2C%20Regulatory%2C%20Brand%2C%20Technical%2C%20and%20Content%20Structure%20checks.%20LRBTC%20combines%20a%20Student-Teacher%20dual%20model%20architecture%2C%20human%20in%20the%20loop%20%28HITL%29%20workflow%20with%20waterfall%20rule%20filtering%20to%20enable%20scalable%2C%20verifiable%20content%20validation%20and%20optimization.%20On%20AIReg-Bench%2C%20our%20approach%20achieves%2083.0%25%20F1%20and%2097.5%25%20recall%2C%20reducing%20missed%20violations%20by%205x%20compared%20with%20Gemini%202.5%20Pro.%20On%20CSpelling%2C%20it%20improves%20mean%20accuracy%20by%2026.7%25.%20Error%20analysis%20further%20reveals%20that%20while%20current%20models%20are%20strong%20at%20detecting%20misspellings%20%2892.5%20recall%29%2C%20they%20fail%20to%20identify%20complex%20medical%20grammatical%20%2825.0%20recall%29%20and%20punctuation%20%2841.7%20recall%29%20errors%2C%20highlighting%20a%20key%20area%20for%20future%20work.%20This%20work%20provides%20a%20practical%2C%20plug%20and%20play%20solution%20for%20reliable%2C%20transparent%20quality%20control%20of%20content%20in%20high%20stakes%2C%20compliance%20critical%20industries.%20We%20also%20provide%20access%20to%20our%20Demo%20under%20MIT%20Licenses.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Two%2520LLMs%2520Better%2520Than%2520One%253F%2520A%2520Student-Teacher%2520Dual-Head%2520LLMs%2520Architecture%2520for%2520Pharmaceutical%2520Content%2520Optimization%26entry.906535625%3DSuyash%2520Mishra%2520and%2520Qiang%2520Li%2520and%2520Anubhav%2520Girdhar%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520to%2520create%2520content%2520in%2520regulated%2520domains%2520such%2520as%2520pharmaceuticals%252C%2520where%2520outputs%2520must%2520be%2520scientifically%2520accurate%2520and%2520legally%2520compliant.%2520Manual%2520quality%2520control%2520%2528QC%2529%2520is%2520slow%252C%2520error%2520prone%252C%2520and%2520can%2520become%2520a%2520publication%2520bottleneck.%2520We%2520introduce%2520LRBTC%252C%2520a%2520modular%2520LLM%2520and%2520vision%2520language%2520model%2520%2528VLM%2529%2520driven%2520QC%2520architecture%2520covering%2520Language%252C%2520Regulatory%252C%2520Brand%252C%2520Technical%252C%2520and%2520Content%2520Structure%2520checks.%2520LRBTC%2520combines%2520a%2520Student-Teacher%2520dual%2520model%2520architecture%252C%2520human%2520in%2520the%2520loop%2520%2528HITL%2529%2520workflow%2520with%2520waterfall%2520rule%2520filtering%2520to%2520enable%2520scalable%252C%2520verifiable%2520content%2520validation%2520and%2520optimization.%2520On%2520AIReg-Bench%252C%2520our%2520approach%2520achieves%252083.0%2525%2520F1%2520and%252097.5%2525%2520recall%252C%2520reducing%2520missed%2520violations%2520by%25205x%2520compared%2520with%2520Gemini%25202.5%2520Pro.%2520On%2520CSpelling%252C%2520it%2520improves%2520mean%2520accuracy%2520by%252026.7%2525.%2520Error%2520analysis%2520further%2520reveals%2520that%2520while%2520current%2520models%2520are%2520strong%2520at%2520detecting%2520misspellings%2520%252892.5%2520recall%2529%252C%2520they%2520fail%2520to%2520identify%2520complex%2520medical%2520grammatical%2520%252825.0%2520recall%2529%2520and%2520punctuation%2520%252841.7%2520recall%2529%2520errors%252C%2520highlighting%2520a%2520key%2520area%2520for%2520future%2520work.%2520This%2520work%2520provides%2520a%2520practical%252C%2520plug%2520and%2520play%2520solution%2520for%2520reliable%252C%2520transparent%2520quality%2520control%2520of%2520content%2520in%2520high%2520stakes%252C%2520compliance%2520critical%2520industries.%2520We%2520also%2520provide%2520access%2520to%2520our%2520Demo%2520under%2520MIT%2520Licenses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Two%20LLMs%20Better%20Than%20One%3F%20A%20Student-Teacher%20Dual-Head%20LLMs%20Architecture%20for%20Pharmaceutical%20Content%20Optimization&entry.906535625=Suyash%20Mishra%20and%20Qiang%20Li%20and%20Anubhav%20Girdhar&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20to%20create%20content%20in%20regulated%20domains%20such%20as%20pharmaceuticals%2C%20where%20outputs%20must%20be%20scientifically%20accurate%20and%20legally%20compliant.%20Manual%20quality%20control%20%28QC%29%20is%20slow%2C%20error%20prone%2C%20and%20can%20become%20a%20publication%20bottleneck.%20We%20introduce%20LRBTC%2C%20a%20modular%20LLM%20and%20vision%20language%20model%20%28VLM%29%20driven%20QC%20architecture%20covering%20Language%2C%20Regulatory%2C%20Brand%2C%20Technical%2C%20and%20Content%20Structure%20checks.%20LRBTC%20combines%20a%20Student-Teacher%20dual%20model%20architecture%2C%20human%20in%20the%20loop%20%28HITL%29%20workflow%20with%20waterfall%20rule%20filtering%20to%20enable%20scalable%2C%20verifiable%20content%20validation%20and%20optimization.%20On%20AIReg-Bench%2C%20our%20approach%20achieves%2083.0%25%20F1%20and%2097.5%25%20recall%2C%20reducing%20missed%20violations%20by%205x%20compared%20with%20Gemini%202.5%20Pro.%20On%20CSpelling%2C%20it%20improves%20mean%20accuracy%20by%2026.7%25.%20Error%20analysis%20further%20reveals%20that%20while%20current%20models%20are%20strong%20at%20detecting%20misspellings%20%2892.5%20recall%29%2C%20they%20fail%20to%20identify%20complex%20medical%20grammatical%20%2825.0%20recall%29%20and%20punctuation%20%2841.7%20recall%29%20errors%2C%20highlighting%20a%20key%20area%20for%20future%20work.%20This%20work%20provides%20a%20practical%2C%20plug%20and%20play%20solution%20for%20reliable%2C%20transparent%20quality%20control%20of%20content%20in%20high%20stakes%2C%20compliance%20critical%20industries.%20We%20also%20provide%20access%20to%20our%20Demo%20under%20MIT%20Licenses.&entry.1838667208=http%3A//arxiv.org/abs/2602.11957v1&entry.124074799=Read"},
{"title": "Evaluating LLM Reasoning Beyond Correctness and CoT", "author": "Soheil Abbasloo", "abstract": "What does it truly mean for a language model to \"reason\"? Current evaluations reward models' correct standalone answers-but correctness alone reveals little about the process that produced them. We argue that reasoning should be understood not as a static chain of steps but as a dynamic trajectory in which ideas interact, clash, and evolve into integrated insights. Building on the philosophical tradition of dialectics, we introduce SIEV, a structured evaluation framework that assesses reasoning through explicit thesis-antithesis-synthesis interactions. SIEV produces interpretable trajectories that highlight key properties of reasoning-robustness to challenge, adaptability under conflict, and synthesis across competing viewpoints-dimensions that conventional correctness-based metrics cannot capture. Empirical results on GSM and MMLU demonstrate substantial gaps in the reasoning abilities of state-of-the-art models: for example, GPT-5-chat loses more than 40 points (out of 100) on GSM when evaluated through SIEV's process-oriented lens. By shifting focus from what answer a model gives to how it arrives there, SIEV enables a more transparent and principled distinction between structured reasoning and surface-level pattern generation offering a clearer foundation for assessing and understanding the reasoning capabilities of LLMs.", "link": "http://arxiv.org/abs/2510.18134v2", "date": "2026-02-12", "relevancy": 2.4299, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLM%20Reasoning%20Beyond%20Correctness%20and%20CoT&body=Title%3A%20Evaluating%20LLM%20Reasoning%20Beyond%20Correctness%20and%20CoT%0AAuthor%3A%20Soheil%20Abbasloo%0AAbstract%3A%20What%20does%20it%20truly%20mean%20for%20a%20language%20model%20to%20%22reason%22%3F%20Current%20evaluations%20reward%20models%27%20correct%20standalone%20answers-but%20correctness%20alone%20reveals%20little%20about%20the%20process%20that%20produced%20them.%20We%20argue%20that%20reasoning%20should%20be%20understood%20not%20as%20a%20static%20chain%20of%20steps%20but%20as%20a%20dynamic%20trajectory%20in%20which%20ideas%20interact%2C%20clash%2C%20and%20evolve%20into%20integrated%20insights.%20Building%20on%20the%20philosophical%20tradition%20of%20dialectics%2C%20we%20introduce%20SIEV%2C%20a%20structured%20evaluation%20framework%20that%20assesses%20reasoning%20through%20explicit%20thesis-antithesis-synthesis%20interactions.%20SIEV%20produces%20interpretable%20trajectories%20that%20highlight%20key%20properties%20of%20reasoning-robustness%20to%20challenge%2C%20adaptability%20under%20conflict%2C%20and%20synthesis%20across%20competing%20viewpoints-dimensions%20that%20conventional%20correctness-based%20metrics%20cannot%20capture.%20Empirical%20results%20on%20GSM%20and%20MMLU%20demonstrate%20substantial%20gaps%20in%20the%20reasoning%20abilities%20of%20state-of-the-art%20models%3A%20for%20example%2C%20GPT-5-chat%20loses%20more%20than%2040%20points%20%28out%20of%20100%29%20on%20GSM%20when%20evaluated%20through%20SIEV%27s%20process-oriented%20lens.%20By%20shifting%20focus%20from%20what%20answer%20a%20model%20gives%20to%20how%20it%20arrives%20there%2C%20SIEV%20enables%20a%20more%20transparent%20and%20principled%20distinction%20between%20structured%20reasoning%20and%20surface-level%20pattern%20generation%20offering%20a%20clearer%20foundation%20for%20assessing%20and%20understanding%20the%20reasoning%20capabilities%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLM%2520Reasoning%2520Beyond%2520Correctness%2520and%2520CoT%26entry.906535625%3DSoheil%2520Abbasloo%26entry.1292438233%3DWhat%2520does%2520it%2520truly%2520mean%2520for%2520a%2520language%2520model%2520to%2520%2522reason%2522%253F%2520Current%2520evaluations%2520reward%2520models%2527%2520correct%2520standalone%2520answers-but%2520correctness%2520alone%2520reveals%2520little%2520about%2520the%2520process%2520that%2520produced%2520them.%2520We%2520argue%2520that%2520reasoning%2520should%2520be%2520understood%2520not%2520as%2520a%2520static%2520chain%2520of%2520steps%2520but%2520as%2520a%2520dynamic%2520trajectory%2520in%2520which%2520ideas%2520interact%252C%2520clash%252C%2520and%2520evolve%2520into%2520integrated%2520insights.%2520Building%2520on%2520the%2520philosophical%2520tradition%2520of%2520dialectics%252C%2520we%2520introduce%2520SIEV%252C%2520a%2520structured%2520evaluation%2520framework%2520that%2520assesses%2520reasoning%2520through%2520explicit%2520thesis-antithesis-synthesis%2520interactions.%2520SIEV%2520produces%2520interpretable%2520trajectories%2520that%2520highlight%2520key%2520properties%2520of%2520reasoning-robustness%2520to%2520challenge%252C%2520adaptability%2520under%2520conflict%252C%2520and%2520synthesis%2520across%2520competing%2520viewpoints-dimensions%2520that%2520conventional%2520correctness-based%2520metrics%2520cannot%2520capture.%2520Empirical%2520results%2520on%2520GSM%2520and%2520MMLU%2520demonstrate%2520substantial%2520gaps%2520in%2520the%2520reasoning%2520abilities%2520of%2520state-of-the-art%2520models%253A%2520for%2520example%252C%2520GPT-5-chat%2520loses%2520more%2520than%252040%2520points%2520%2528out%2520of%2520100%2529%2520on%2520GSM%2520when%2520evaluated%2520through%2520SIEV%2527s%2520process-oriented%2520lens.%2520By%2520shifting%2520focus%2520from%2520what%2520answer%2520a%2520model%2520gives%2520to%2520how%2520it%2520arrives%2520there%252C%2520SIEV%2520enables%2520a%2520more%2520transparent%2520and%2520principled%2520distinction%2520between%2520structured%2520reasoning%2520and%2520surface-level%2520pattern%2520generation%2520offering%2520a%2520clearer%2520foundation%2520for%2520assessing%2520and%2520understanding%2520the%2520reasoning%2520capabilities%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLM%20Reasoning%20Beyond%20Correctness%20and%20CoT&entry.906535625=Soheil%20Abbasloo&entry.1292438233=What%20does%20it%20truly%20mean%20for%20a%20language%20model%20to%20%22reason%22%3F%20Current%20evaluations%20reward%20models%27%20correct%20standalone%20answers-but%20correctness%20alone%20reveals%20little%20about%20the%20process%20that%20produced%20them.%20We%20argue%20that%20reasoning%20should%20be%20understood%20not%20as%20a%20static%20chain%20of%20steps%20but%20as%20a%20dynamic%20trajectory%20in%20which%20ideas%20interact%2C%20clash%2C%20and%20evolve%20into%20integrated%20insights.%20Building%20on%20the%20philosophical%20tradition%20of%20dialectics%2C%20we%20introduce%20SIEV%2C%20a%20structured%20evaluation%20framework%20that%20assesses%20reasoning%20through%20explicit%20thesis-antithesis-synthesis%20interactions.%20SIEV%20produces%20interpretable%20trajectories%20that%20highlight%20key%20properties%20of%20reasoning-robustness%20to%20challenge%2C%20adaptability%20under%20conflict%2C%20and%20synthesis%20across%20competing%20viewpoints-dimensions%20that%20conventional%20correctness-based%20metrics%20cannot%20capture.%20Empirical%20results%20on%20GSM%20and%20MMLU%20demonstrate%20substantial%20gaps%20in%20the%20reasoning%20abilities%20of%20state-of-the-art%20models%3A%20for%20example%2C%20GPT-5-chat%20loses%20more%20than%2040%20points%20%28out%20of%20100%29%20on%20GSM%20when%20evaluated%20through%20SIEV%27s%20process-oriented%20lens.%20By%20shifting%20focus%20from%20what%20answer%20a%20model%20gives%20to%20how%20it%20arrives%20there%2C%20SIEV%20enables%20a%20more%20transparent%20and%20principled%20distinction%20between%20structured%20reasoning%20and%20surface-level%20pattern%20generation%20offering%20a%20clearer%20foundation%20for%20assessing%20and%20understanding%20the%20reasoning%20capabilities%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2510.18134v2&entry.124074799=Read"},
{"title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting", "author": "Zhenghuang Wu and Kang Chen and Zeyu Zhang and Hao Tang", "abstract": "Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.", "link": "http://arxiv.org/abs/2602.11769v1", "date": "2026-02-12", "relevancy": 2.4285, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6235}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6149}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light4D%3A%20Training-Free%20Extreme%20Viewpoint%204D%20Video%20Relighting&body=Title%3A%20Light4D%3A%20Training-Free%20Extreme%20Viewpoint%204D%20Video%20Relighting%0AAuthor%3A%20Zhenghuang%20Wu%20and%20Kang%20Chen%20and%20Zeyu%20Zhang%20and%20Hao%20Tang%0AAbstract%3A%20Recent%20advances%20in%20diffusion-based%20generative%20models%20have%20established%20a%20new%20paradigm%20for%20image%20and%20video%20relighting.%20However%2C%20extending%20these%20capabilities%20to%204D%20relighting%20remains%20challenging%2C%20due%20primarily%20to%20the%20scarcity%20of%20paired%204D%20relighting%20training%20data%20and%20the%20difficulty%20of%20maintaining%20temporal%20consistency%20across%20extreme%20viewpoints.%20In%20this%20work%2C%20we%20propose%20Light4D%2C%20a%20novel%20training-free%20framework%20designed%20to%20synthesize%20consistent%204D%20videos%20under%20target%20illumination%2C%20even%20under%20extreme%20viewpoint%20changes.%20First%2C%20we%20introduce%20Disentangled%20Flow%20Guidance%2C%20a%20time-aware%20strategy%20that%20effectively%20injects%20lighting%20control%20into%20the%20latent%20space%20while%20preserving%20geometric%20integrity.%20Second%2C%20to%20reinforce%20temporal%20consistency%2C%20we%20develop%20Temporal%20Consistent%20Attention%20within%20the%20IC-Light%20architecture%20and%20further%20incorporate%20deterministic%20regularization%20to%20eliminate%20appearance%20flickering.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20in%20temporal%20consistency%20and%20lighting%20fidelity%2C%20robustly%20handling%20camera%20rotations%20from%20-90%20to%2090.%20Code%3A%20https%3A//github.com/AIGeeksGroup/Light4D.%20Website%3A%20https%3A//aigeeksgroup.github.io/Light4D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight4D%253A%2520Training-Free%2520Extreme%2520Viewpoint%25204D%2520Video%2520Relighting%26entry.906535625%3DZhenghuang%2520Wu%2520and%2520Kang%2520Chen%2520and%2520Zeyu%2520Zhang%2520and%2520Hao%2520Tang%26entry.1292438233%3DRecent%2520advances%2520in%2520diffusion-based%2520generative%2520models%2520have%2520established%2520a%2520new%2520paradigm%2520for%2520image%2520and%2520video%2520relighting.%2520However%252C%2520extending%2520these%2520capabilities%2520to%25204D%2520relighting%2520remains%2520challenging%252C%2520due%2520primarily%2520to%2520the%2520scarcity%2520of%2520paired%25204D%2520relighting%2520training%2520data%2520and%2520the%2520difficulty%2520of%2520maintaining%2520temporal%2520consistency%2520across%2520extreme%2520viewpoints.%2520In%2520this%2520work%252C%2520we%2520propose%2520Light4D%252C%2520a%2520novel%2520training-free%2520framework%2520designed%2520to%2520synthesize%2520consistent%25204D%2520videos%2520under%2520target%2520illumination%252C%2520even%2520under%2520extreme%2520viewpoint%2520changes.%2520First%252C%2520we%2520introduce%2520Disentangled%2520Flow%2520Guidance%252C%2520a%2520time-aware%2520strategy%2520that%2520effectively%2520injects%2520lighting%2520control%2520into%2520the%2520latent%2520space%2520while%2520preserving%2520geometric%2520integrity.%2520Second%252C%2520to%2520reinforce%2520temporal%2520consistency%252C%2520we%2520develop%2520Temporal%2520Consistent%2520Attention%2520within%2520the%2520IC-Light%2520architecture%2520and%2520further%2520incorporate%2520deterministic%2520regularization%2520to%2520eliminate%2520appearance%2520flickering.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520performance%2520in%2520temporal%2520consistency%2520and%2520lighting%2520fidelity%252C%2520robustly%2520handling%2520camera%2520rotations%2520from%2520-90%2520to%252090.%2520Code%253A%2520https%253A//github.com/AIGeeksGroup/Light4D.%2520Website%253A%2520https%253A//aigeeksgroup.github.io/Light4D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light4D%3A%20Training-Free%20Extreme%20Viewpoint%204D%20Video%20Relighting&entry.906535625=Zhenghuang%20Wu%20and%20Kang%20Chen%20and%20Zeyu%20Zhang%20and%20Hao%20Tang&entry.1292438233=Recent%20advances%20in%20diffusion-based%20generative%20models%20have%20established%20a%20new%20paradigm%20for%20image%20and%20video%20relighting.%20However%2C%20extending%20these%20capabilities%20to%204D%20relighting%20remains%20challenging%2C%20due%20primarily%20to%20the%20scarcity%20of%20paired%204D%20relighting%20training%20data%20and%20the%20difficulty%20of%20maintaining%20temporal%20consistency%20across%20extreme%20viewpoints.%20In%20this%20work%2C%20we%20propose%20Light4D%2C%20a%20novel%20training-free%20framework%20designed%20to%20synthesize%20consistent%204D%20videos%20under%20target%20illumination%2C%20even%20under%20extreme%20viewpoint%20changes.%20First%2C%20we%20introduce%20Disentangled%20Flow%20Guidance%2C%20a%20time-aware%20strategy%20that%20effectively%20injects%20lighting%20control%20into%20the%20latent%20space%20while%20preserving%20geometric%20integrity.%20Second%2C%20to%20reinforce%20temporal%20consistency%2C%20we%20develop%20Temporal%20Consistent%20Attention%20within%20the%20IC-Light%20architecture%20and%20further%20incorporate%20deterministic%20regularization%20to%20eliminate%20appearance%20flickering.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20in%20temporal%20consistency%20and%20lighting%20fidelity%2C%20robustly%20handling%20camera%20rotations%20from%20-90%20to%2090.%20Code%3A%20https%3A//github.com/AIGeeksGroup/Light4D.%20Website%3A%20https%3A//aigeeksgroup.github.io/Light4D.&entry.1838667208=http%3A//arxiv.org/abs/2602.11769v1&entry.124074799=Read"},
{"title": "Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion", "author": "Bruno Rigal and Victor Dupriez and Alexis Mignon and Ronan Le Hy and Nicolas Mery", "abstract": "This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.\n  We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.", "link": "http://arxiv.org/abs/2602.11960v1", "date": "2026-02-12", "relevancy": 2.4254, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4945}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision-Language%20Models%20for%20French%20PDF-to-Markdown%20Conversion&body=Title%3A%20Benchmarking%20Vision-Language%20Models%20for%20French%20PDF-to-Markdown%20Conversion%0AAuthor%3A%20Bruno%20Rigal%20and%20Victor%20Dupriez%20and%20Alexis%20Mignon%20and%20Ronan%20Le%20Hy%20and%20Nicolas%20Mery%0AAbstract%3A%20This%20report%20evaluates%20PDF-to-Markdown%20conversion%20using%20recent%20Vision-Language%20Models%20%28VLMs%29%20on%20challenging%20French%20documents.%20Document%20parsing%20is%20a%20critical%20step%20for%20Retrieval-Augmented%20Generation%20%28RAG%29%20pipelines%2C%20where%20transcription%20and%20layout%20errors%20propagate%20to%20downstream%20retrieval%20and%20grounding.%20Existing%20benchmarks%20often%20emphasize%20English%20or%20Chinese%20and%20can%20over-penalize%20benign%20formatting%20and%20linearization%20choices%20%28e.g.%2C%20line%20breaks%2C%20list%20segmentation%2C%20alternative%20table%20renderings%29%20that%20are%20largely%20irrelevant%20for%20downstream%20use.%0A%20%20We%20introduce%20a%20French-focused%20benchmark%20of%20difficult%20pages%20selected%20via%20model-disagreement%20sampling%20from%20a%20corpus%20of%2060%7B%2C%7D000%20documents%2C%20covering%20handwritten%20forms%2C%20complex%20layouts%2C%20dense%20tables%2C%20and%20graphics-rich%20pages.%20Evaluation%20is%20performed%20with%20unit-test-style%20checks%20that%20target%20concrete%20failure%20modes%20%28text%20presence%2C%20reading%20order%2C%20and%20local%20table%20constraints%29%20combined%20with%20category-specific%20normalization%20designed%20to%20discount%20presentation-only%20variance.%20Across%2015%20models%2C%20we%20observe%20substantially%20higher%20robustness%20for%20the%20strongest%20proprietary%20models%20on%20handwriting%20and%20forms%2C%20while%20several%20open-weights%20systems%20remain%20competitive%20on%20standard%20printed%20layouts.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision-Language%2520Models%2520for%2520French%2520PDF-to-Markdown%2520Conversion%26entry.906535625%3DBruno%2520Rigal%2520and%2520Victor%2520Dupriez%2520and%2520Alexis%2520Mignon%2520and%2520Ronan%2520Le%2520Hy%2520and%2520Nicolas%2520Mery%26entry.1292438233%3DThis%2520report%2520evaluates%2520PDF-to-Markdown%2520conversion%2520using%2520recent%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520on%2520challenging%2520French%2520documents.%2520Document%2520parsing%2520is%2520a%2520critical%2520step%2520for%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520pipelines%252C%2520where%2520transcription%2520and%2520layout%2520errors%2520propagate%2520to%2520downstream%2520retrieval%2520and%2520grounding.%2520Existing%2520benchmarks%2520often%2520emphasize%2520English%2520or%2520Chinese%2520and%2520can%2520over-penalize%2520benign%2520formatting%2520and%2520linearization%2520choices%2520%2528e.g.%252C%2520line%2520breaks%252C%2520list%2520segmentation%252C%2520alternative%2520table%2520renderings%2529%2520that%2520are%2520largely%2520irrelevant%2520for%2520downstream%2520use.%250A%2520%2520We%2520introduce%2520a%2520French-focused%2520benchmark%2520of%2520difficult%2520pages%2520selected%2520via%2520model-disagreement%2520sampling%2520from%2520a%2520corpus%2520of%252060%257B%252C%257D000%2520documents%252C%2520covering%2520handwritten%2520forms%252C%2520complex%2520layouts%252C%2520dense%2520tables%252C%2520and%2520graphics-rich%2520pages.%2520Evaluation%2520is%2520performed%2520with%2520unit-test-style%2520checks%2520that%2520target%2520concrete%2520failure%2520modes%2520%2528text%2520presence%252C%2520reading%2520order%252C%2520and%2520local%2520table%2520constraints%2529%2520combined%2520with%2520category-specific%2520normalization%2520designed%2520to%2520discount%2520presentation-only%2520variance.%2520Across%252015%2520models%252C%2520we%2520observe%2520substantially%2520higher%2520robustness%2520for%2520the%2520strongest%2520proprietary%2520models%2520on%2520handwriting%2520and%2520forms%252C%2520while%2520several%2520open-weights%2520systems%2520remain%2520competitive%2520on%2520standard%2520printed%2520layouts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision-Language%20Models%20for%20French%20PDF-to-Markdown%20Conversion&entry.906535625=Bruno%20Rigal%20and%20Victor%20Dupriez%20and%20Alexis%20Mignon%20and%20Ronan%20Le%20Hy%20and%20Nicolas%20Mery&entry.1292438233=This%20report%20evaluates%20PDF-to-Markdown%20conversion%20using%20recent%20Vision-Language%20Models%20%28VLMs%29%20on%20challenging%20French%20documents.%20Document%20parsing%20is%20a%20critical%20step%20for%20Retrieval-Augmented%20Generation%20%28RAG%29%20pipelines%2C%20where%20transcription%20and%20layout%20errors%20propagate%20to%20downstream%20retrieval%20and%20grounding.%20Existing%20benchmarks%20often%20emphasize%20English%20or%20Chinese%20and%20can%20over-penalize%20benign%20formatting%20and%20linearization%20choices%20%28e.g.%2C%20line%20breaks%2C%20list%20segmentation%2C%20alternative%20table%20renderings%29%20that%20are%20largely%20irrelevant%20for%20downstream%20use.%0A%20%20We%20introduce%20a%20French-focused%20benchmark%20of%20difficult%20pages%20selected%20via%20model-disagreement%20sampling%20from%20a%20corpus%20of%2060%7B%2C%7D000%20documents%2C%20covering%20handwritten%20forms%2C%20complex%20layouts%2C%20dense%20tables%2C%20and%20graphics-rich%20pages.%20Evaluation%20is%20performed%20with%20unit-test-style%20checks%20that%20target%20concrete%20failure%20modes%20%28text%20presence%2C%20reading%20order%2C%20and%20local%20table%20constraints%29%20combined%20with%20category-specific%20normalization%20designed%20to%20discount%20presentation-only%20variance.%20Across%2015%20models%2C%20we%20observe%20substantially%20higher%20robustness%20for%20the%20strongest%20proprietary%20models%20on%20handwriting%20and%20forms%2C%20while%20several%20open-weights%20systems%20remain%20competitive%20on%20standard%20printed%20layouts.&entry.1838667208=http%3A//arxiv.org/abs/2602.11960v1&entry.124074799=Read"},
{"title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration", "author": "Akhiad Bercovich and Nir Ailon and Vladimir Anisimov and Tomer Asida and Nave Assaf and Mohammad Dabbah and Ido Galil and Amnon Geifman and Yonatan Geifman and Izhak Golan and Roi Koren and Itay Levy and Zach Moshe and Pavlo Molchanov and Najeeb Nabwani and Mostofa Patwari and Omri Puny and Tomer Ronen and Itamar Schen and Elad Segal and Ido Shahaf and Oren Tropp and Ran Zilberstein and Ran El-Yaniv", "abstract": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.", "link": "http://arxiv.org/abs/2602.11937v1", "date": "2026-02-12", "relevancy": 2.4173, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20Puzzle%20for%20Mixture-of-Experts%20Reasoning%20Models%20with%20Application%20to%20GPT-OSS%20Acceleration&body=Title%3A%20Extending%20Puzzle%20for%20Mixture-of-Experts%20Reasoning%20Models%20with%20Application%20to%20GPT-OSS%20Acceleration%0AAuthor%3A%20Akhiad%20Bercovich%20and%20Nir%20Ailon%20and%20Vladimir%20Anisimov%20and%20Tomer%20Asida%20and%20Nave%20Assaf%20and%20Mohammad%20Dabbah%20and%20Ido%20Galil%20and%20Amnon%20Geifman%20and%20Yonatan%20Geifman%20and%20Izhak%20Golan%20and%20Roi%20Koren%20and%20Itay%20Levy%20and%20Zach%20Moshe%20and%20Pavlo%20Molchanov%20and%20Najeeb%20Nabwani%20and%20Mostofa%20Patwari%20and%20Omri%20Puny%20and%20Tomer%20Ronen%20and%20Itamar%20Schen%20and%20Elad%20Segal%20and%20Ido%20Shahaf%20and%20Oren%20Tropp%20and%20Ran%20Zilberstein%20and%20Ran%20El-Yaniv%0AAbstract%3A%20Reasoning-focused%20LLMs%20improve%20answer%20quality%20by%20generating%20longer%20reasoning%20traces%2C%20but%20the%20additional%20tokens%20dramatically%20increase%20serving%20cost%2C%20motivating%20inference%20optimization.%20We%20extend%20and%20apply%20Puzzle%2C%20a%20post-training%20neural%20architecture%20search%20%28NAS%29%20framework%2C%20to%20gpt-oss-120B%20to%20produce%20gpt-oss-puzzle-88B%2C%20a%20deployment-optimized%20derivative.%20Our%20approach%20combines%20heterogeneous%20MoE%20expert%20pruning%2C%20selective%20replacement%20of%20full-context%20attention%20with%20window%20attention%2C%20FP8%20KV-cache%20quantization%20with%20calibrated%20scales%2C%20and%20post-training%20reinforcement%20learning%20to%20recover%20accuracy%2C%20while%20maintaining%20low%20generation%20length.%20In%20terms%20of%20per-token%20speeds%2C%20on%20an%208XH100%20node%20we%20achieve%201.63X%20and%201.22X%20throughput%20speedups%20in%20long-context%20and%20short-context%20settings%2C%20respectively.%20gpt-oss-puzzle-88B%20also%20delivers%20throughput%20speedups%20of%202.82X%20on%20a%20single%20NVIDIA%20H100%20GPU.%20However%2C%20because%20token%20counts%20can%20change%20with%20reasoning%20effort%20and%20model%20variants%2C%20per-token%20throughput%20%28tok/s%29%20and%20latency%20%28ms/token%29%20do%20not%20necessarily%20lead%20to%20end-to-end%20speedups%3A%20a%202X%20throughput%20gain%20is%20erased%20if%20traces%20grow%202X.%20Conversely%2C%20throughput%20gains%20can%20be%20spent%20on%20more%20reasoning%20tokens%20to%20improve%20accuracy%3B%20we%20therefore%20advocate%20request-level%20efficiency%20metrics%20that%20normalize%20throughput%20by%20tokens%20generated%20and%20trace%20an%20accuracy--speed%20frontier%20across%20reasoning%20efforts.%20We%20show%20that%20gpt-oss-puzzle-88B%20improves%20over%20gpt-oss-120B%20along%20the%20entire%20frontier%2C%20delivering%20up%20to%201.29X%20higher%20request-level%20efficiency.%20Across%20various%20benchmarks%2C%20gpt-oss-puzzle-88B%20matches%20or%20slightly%20exceeds%20the%20parent%20on%20suite-average%20accuracy%20across%20reasoning%20efforts%2C%20with%20retention%20ranging%20from%20100.8%25%20%28high%29%20to%20108.2%25%20%28low%29%2C%20showing%20that%20post-training%20architecture%20search%20can%20substantially%20reduce%20inference%20costs%20without%20sacrificing%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520Puzzle%2520for%2520Mixture-of-Experts%2520Reasoning%2520Models%2520with%2520Application%2520to%2520GPT-OSS%2520Acceleration%26entry.906535625%3DAkhiad%2520Bercovich%2520and%2520Nir%2520Ailon%2520and%2520Vladimir%2520Anisimov%2520and%2520Tomer%2520Asida%2520and%2520Nave%2520Assaf%2520and%2520Mohammad%2520Dabbah%2520and%2520Ido%2520Galil%2520and%2520Amnon%2520Geifman%2520and%2520Yonatan%2520Geifman%2520and%2520Izhak%2520Golan%2520and%2520Roi%2520Koren%2520and%2520Itay%2520Levy%2520and%2520Zach%2520Moshe%2520and%2520Pavlo%2520Molchanov%2520and%2520Najeeb%2520Nabwani%2520and%2520Mostofa%2520Patwari%2520and%2520Omri%2520Puny%2520and%2520Tomer%2520Ronen%2520and%2520Itamar%2520Schen%2520and%2520Elad%2520Segal%2520and%2520Ido%2520Shahaf%2520and%2520Oren%2520Tropp%2520and%2520Ran%2520Zilberstein%2520and%2520Ran%2520El-Yaniv%26entry.1292438233%3DReasoning-focused%2520LLMs%2520improve%2520answer%2520quality%2520by%2520generating%2520longer%2520reasoning%2520traces%252C%2520but%2520the%2520additional%2520tokens%2520dramatically%2520increase%2520serving%2520cost%252C%2520motivating%2520inference%2520optimization.%2520We%2520extend%2520and%2520apply%2520Puzzle%252C%2520a%2520post-training%2520neural%2520architecture%2520search%2520%2528NAS%2529%2520framework%252C%2520to%2520gpt-oss-120B%2520to%2520produce%2520gpt-oss-puzzle-88B%252C%2520a%2520deployment-optimized%2520derivative.%2520Our%2520approach%2520combines%2520heterogeneous%2520MoE%2520expert%2520pruning%252C%2520selective%2520replacement%2520of%2520full-context%2520attention%2520with%2520window%2520attention%252C%2520FP8%2520KV-cache%2520quantization%2520with%2520calibrated%2520scales%252C%2520and%2520post-training%2520reinforcement%2520learning%2520to%2520recover%2520accuracy%252C%2520while%2520maintaining%2520low%2520generation%2520length.%2520In%2520terms%2520of%2520per-token%2520speeds%252C%2520on%2520an%25208XH100%2520node%2520we%2520achieve%25201.63X%2520and%25201.22X%2520throughput%2520speedups%2520in%2520long-context%2520and%2520short-context%2520settings%252C%2520respectively.%2520gpt-oss-puzzle-88B%2520also%2520delivers%2520throughput%2520speedups%2520of%25202.82X%2520on%2520a%2520single%2520NVIDIA%2520H100%2520GPU.%2520However%252C%2520because%2520token%2520counts%2520can%2520change%2520with%2520reasoning%2520effort%2520and%2520model%2520variants%252C%2520per-token%2520throughput%2520%2528tok/s%2529%2520and%2520latency%2520%2528ms/token%2529%2520do%2520not%2520necessarily%2520lead%2520to%2520end-to-end%2520speedups%253A%2520a%25202X%2520throughput%2520gain%2520is%2520erased%2520if%2520traces%2520grow%25202X.%2520Conversely%252C%2520throughput%2520gains%2520can%2520be%2520spent%2520on%2520more%2520reasoning%2520tokens%2520to%2520improve%2520accuracy%253B%2520we%2520therefore%2520advocate%2520request-level%2520efficiency%2520metrics%2520that%2520normalize%2520throughput%2520by%2520tokens%2520generated%2520and%2520trace%2520an%2520accuracy--speed%2520frontier%2520across%2520reasoning%2520efforts.%2520We%2520show%2520that%2520gpt-oss-puzzle-88B%2520improves%2520over%2520gpt-oss-120B%2520along%2520the%2520entire%2520frontier%252C%2520delivering%2520up%2520to%25201.29X%2520higher%2520request-level%2520efficiency.%2520Across%2520various%2520benchmarks%252C%2520gpt-oss-puzzle-88B%2520matches%2520or%2520slightly%2520exceeds%2520the%2520parent%2520on%2520suite-average%2520accuracy%2520across%2520reasoning%2520efforts%252C%2520with%2520retention%2520ranging%2520from%2520100.8%2525%2520%2528high%2529%2520to%2520108.2%2525%2520%2528low%2529%252C%2520showing%2520that%2520post-training%2520architecture%2520search%2520can%2520substantially%2520reduce%2520inference%2520costs%2520without%2520sacrificing%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20Puzzle%20for%20Mixture-of-Experts%20Reasoning%20Models%20with%20Application%20to%20GPT-OSS%20Acceleration&entry.906535625=Akhiad%20Bercovich%20and%20Nir%20Ailon%20and%20Vladimir%20Anisimov%20and%20Tomer%20Asida%20and%20Nave%20Assaf%20and%20Mohammad%20Dabbah%20and%20Ido%20Galil%20and%20Amnon%20Geifman%20and%20Yonatan%20Geifman%20and%20Izhak%20Golan%20and%20Roi%20Koren%20and%20Itay%20Levy%20and%20Zach%20Moshe%20and%20Pavlo%20Molchanov%20and%20Najeeb%20Nabwani%20and%20Mostofa%20Patwari%20and%20Omri%20Puny%20and%20Tomer%20Ronen%20and%20Itamar%20Schen%20and%20Elad%20Segal%20and%20Ido%20Shahaf%20and%20Oren%20Tropp%20and%20Ran%20Zilberstein%20and%20Ran%20El-Yaniv&entry.1292438233=Reasoning-focused%20LLMs%20improve%20answer%20quality%20by%20generating%20longer%20reasoning%20traces%2C%20but%20the%20additional%20tokens%20dramatically%20increase%20serving%20cost%2C%20motivating%20inference%20optimization.%20We%20extend%20and%20apply%20Puzzle%2C%20a%20post-training%20neural%20architecture%20search%20%28NAS%29%20framework%2C%20to%20gpt-oss-120B%20to%20produce%20gpt-oss-puzzle-88B%2C%20a%20deployment-optimized%20derivative.%20Our%20approach%20combines%20heterogeneous%20MoE%20expert%20pruning%2C%20selective%20replacement%20of%20full-context%20attention%20with%20window%20attention%2C%20FP8%20KV-cache%20quantization%20with%20calibrated%20scales%2C%20and%20post-training%20reinforcement%20learning%20to%20recover%20accuracy%2C%20while%20maintaining%20low%20generation%20length.%20In%20terms%20of%20per-token%20speeds%2C%20on%20an%208XH100%20node%20we%20achieve%201.63X%20and%201.22X%20throughput%20speedups%20in%20long-context%20and%20short-context%20settings%2C%20respectively.%20gpt-oss-puzzle-88B%20also%20delivers%20throughput%20speedups%20of%202.82X%20on%20a%20single%20NVIDIA%20H100%20GPU.%20However%2C%20because%20token%20counts%20can%20change%20with%20reasoning%20effort%20and%20model%20variants%2C%20per-token%20throughput%20%28tok/s%29%20and%20latency%20%28ms/token%29%20do%20not%20necessarily%20lead%20to%20end-to-end%20speedups%3A%20a%202X%20throughput%20gain%20is%20erased%20if%20traces%20grow%202X.%20Conversely%2C%20throughput%20gains%20can%20be%20spent%20on%20more%20reasoning%20tokens%20to%20improve%20accuracy%3B%20we%20therefore%20advocate%20request-level%20efficiency%20metrics%20that%20normalize%20throughput%20by%20tokens%20generated%20and%20trace%20an%20accuracy--speed%20frontier%20across%20reasoning%20efforts.%20We%20show%20that%20gpt-oss-puzzle-88B%20improves%20over%20gpt-oss-120B%20along%20the%20entire%20frontier%2C%20delivering%20up%20to%201.29X%20higher%20request-level%20efficiency.%20Across%20various%20benchmarks%2C%20gpt-oss-puzzle-88B%20matches%20or%20slightly%20exceeds%20the%20parent%20on%20suite-average%20accuracy%20across%20reasoning%20efforts%2C%20with%20retention%20ranging%20from%20100.8%25%20%28high%29%20to%20108.2%25%20%28low%29%2C%20showing%20that%20post-training%20architecture%20search%20can%20substantially%20reduce%20inference%20costs%20without%20sacrificing%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2602.11937v1&entry.124074799=Read"},
{"title": "Empirical Gaussian Processes", "author": "Jihao Andreas Lin and Sebastian Ament and Louis C. Tiao and David Eriksson and Maximilian Balandat and Eytan Bakshy", "abstract": "Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.", "link": "http://arxiv.org/abs/2602.12082v1", "date": "2026-02-12", "relevancy": 2.4124, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4959}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4801}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Gaussian%20Processes&body=Title%3A%20Empirical%20Gaussian%20Processes%0AAuthor%3A%20Jihao%20Andreas%20Lin%20and%20Sebastian%20Ament%20and%20Louis%20C.%20Tiao%20and%20David%20Eriksson%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy%0AAbstract%3A%20Gaussian%20processes%20%28GPs%29%20are%20powerful%20and%20widely%20used%20probabilistic%20regression%20models%2C%20but%20their%20effectiveness%20in%20practice%20is%20often%20limited%20by%20the%20choice%20of%20kernel%20function.%20This%20kernel%20function%20is%20typically%20handcrafted%20from%20a%20small%20set%20of%20standard%20functions%2C%20a%20process%20that%20requires%20expert%20knowledge%2C%20results%20in%20limited%20adaptivity%20to%20data%2C%20and%20imposes%20strong%20assumptions%20on%20the%20hypothesis%20space.%20We%20study%20Empirical%20GPs%2C%20a%20principled%20framework%20for%20constructing%20flexible%2C%20data-driven%20GP%20priors%20that%20overcome%20these%20limitations.%20Rather%20than%20relying%20on%20standard%20parametric%20kernels%2C%20we%20estimate%20the%20mean%20and%20covariance%20functions%20empirically%20from%20a%20corpus%20of%20historical%20observations%2C%20enabling%20the%20prior%20to%20reflect%20rich%2C%20non-trivial%20covariance%20structures%20present%20in%20the%20data.%20Theoretically%2C%20we%20show%20that%20the%20resulting%20model%20converges%20to%20the%20GP%20that%20is%20closest%20%28in%20KL-divergence%20sense%29%20to%20the%20real%20data%20generating%20process.%20Practically%2C%20we%20formulate%20the%20problem%20of%20learning%20the%20GP%20prior%20from%20independent%20datasets%20as%20likelihood%20estimation%20and%20derive%20an%20Expectation-Maximization%20algorithm%20with%20closed-form%20updates%2C%20allowing%20the%20model%20handle%20heterogeneous%20observation%20locations%20across%20datasets.%20We%20demonstrate%20that%20Empirical%20GPs%20achieve%20competitive%20performance%20on%20learning%20curve%20extrapolation%20and%20time%20series%20forecasting%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Gaussian%2520Processes%26entry.906535625%3DJihao%2520Andreas%2520Lin%2520and%2520Sebastian%2520Ament%2520and%2520Louis%2520C.%2520Tiao%2520and%2520David%2520Eriksson%2520and%2520Maximilian%2520Balandat%2520and%2520Eytan%2520Bakshy%26entry.1292438233%3DGaussian%2520processes%2520%2528GPs%2529%2520are%2520powerful%2520and%2520widely%2520used%2520probabilistic%2520regression%2520models%252C%2520but%2520their%2520effectiveness%2520in%2520practice%2520is%2520often%2520limited%2520by%2520the%2520choice%2520of%2520kernel%2520function.%2520This%2520kernel%2520function%2520is%2520typically%2520handcrafted%2520from%2520a%2520small%2520set%2520of%2520standard%2520functions%252C%2520a%2520process%2520that%2520requires%2520expert%2520knowledge%252C%2520results%2520in%2520limited%2520adaptivity%2520to%2520data%252C%2520and%2520imposes%2520strong%2520assumptions%2520on%2520the%2520hypothesis%2520space.%2520We%2520study%2520Empirical%2520GPs%252C%2520a%2520principled%2520framework%2520for%2520constructing%2520flexible%252C%2520data-driven%2520GP%2520priors%2520that%2520overcome%2520these%2520limitations.%2520Rather%2520than%2520relying%2520on%2520standard%2520parametric%2520kernels%252C%2520we%2520estimate%2520the%2520mean%2520and%2520covariance%2520functions%2520empirically%2520from%2520a%2520corpus%2520of%2520historical%2520observations%252C%2520enabling%2520the%2520prior%2520to%2520reflect%2520rich%252C%2520non-trivial%2520covariance%2520structures%2520present%2520in%2520the%2520data.%2520Theoretically%252C%2520we%2520show%2520that%2520the%2520resulting%2520model%2520converges%2520to%2520the%2520GP%2520that%2520is%2520closest%2520%2528in%2520KL-divergence%2520sense%2529%2520to%2520the%2520real%2520data%2520generating%2520process.%2520Practically%252C%2520we%2520formulate%2520the%2520problem%2520of%2520learning%2520the%2520GP%2520prior%2520from%2520independent%2520datasets%2520as%2520likelihood%2520estimation%2520and%2520derive%2520an%2520Expectation-Maximization%2520algorithm%2520with%2520closed-form%2520updates%252C%2520allowing%2520the%2520model%2520handle%2520heterogeneous%2520observation%2520locations%2520across%2520datasets.%2520We%2520demonstrate%2520that%2520Empirical%2520GPs%2520achieve%2520competitive%2520performance%2520on%2520learning%2520curve%2520extrapolation%2520and%2520time%2520series%2520forecasting%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Gaussian%20Processes&entry.906535625=Jihao%20Andreas%20Lin%20and%20Sebastian%20Ament%20and%20Louis%20C.%20Tiao%20and%20David%20Eriksson%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy&entry.1292438233=Gaussian%20processes%20%28GPs%29%20are%20powerful%20and%20widely%20used%20probabilistic%20regression%20models%2C%20but%20their%20effectiveness%20in%20practice%20is%20often%20limited%20by%20the%20choice%20of%20kernel%20function.%20This%20kernel%20function%20is%20typically%20handcrafted%20from%20a%20small%20set%20of%20standard%20functions%2C%20a%20process%20that%20requires%20expert%20knowledge%2C%20results%20in%20limited%20adaptivity%20to%20data%2C%20and%20imposes%20strong%20assumptions%20on%20the%20hypothesis%20space.%20We%20study%20Empirical%20GPs%2C%20a%20principled%20framework%20for%20constructing%20flexible%2C%20data-driven%20GP%20priors%20that%20overcome%20these%20limitations.%20Rather%20than%20relying%20on%20standard%20parametric%20kernels%2C%20we%20estimate%20the%20mean%20and%20covariance%20functions%20empirically%20from%20a%20corpus%20of%20historical%20observations%2C%20enabling%20the%20prior%20to%20reflect%20rich%2C%20non-trivial%20covariance%20structures%20present%20in%20the%20data.%20Theoretically%2C%20we%20show%20that%20the%20resulting%20model%20converges%20to%20the%20GP%20that%20is%20closest%20%28in%20KL-divergence%20sense%29%20to%20the%20real%20data%20generating%20process.%20Practically%2C%20we%20formulate%20the%20problem%20of%20learning%20the%20GP%20prior%20from%20independent%20datasets%20as%20likelihood%20estimation%20and%20derive%20an%20Expectation-Maximization%20algorithm%20with%20closed-form%20updates%2C%20allowing%20the%20model%20handle%20heterogeneous%20observation%20locations%20across%20datasets.%20We%20demonstrate%20that%20Empirical%20GPs%20achieve%20competitive%20performance%20on%20learning%20curve%20extrapolation%20and%20time%20series%20forecasting%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.12082v1&entry.124074799=Read"},
{"title": "Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control", "author": "Yu Deng and Yufeng Jin and Xiaogang Jia and Jiahong Xue and Gerhard Neumann and Georgia Chalvatzaki", "abstract": "We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a \"blind spot\" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.", "link": "http://arxiv.org/abs/2602.11934v1", "date": "2026-02-12", "relevancy": 2.4029, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6083}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.597}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot-DIFT%3A%20Distilling%20Diffusion%20Features%20for%20Geometrically%20Consistent%20Visuomotor%20Control&body=Title%3A%20Robot-DIFT%3A%20Distilling%20Diffusion%20Features%20for%20Geometrically%20Consistent%20Visuomotor%20Control%0AAuthor%3A%20Yu%20Deng%20and%20Yufeng%20Jin%20and%20Xiaogang%20Jia%20and%20Jiahong%20Xue%20and%20Gerhard%20Neumann%20and%20Georgia%20Chalvatzaki%0AAbstract%3A%20We%20hypothesize%20that%20a%20key%20bottleneck%20in%20generalizable%20robot%20manipulation%20is%20not%20solely%20data%20scale%20or%20policy%20capacity%2C%20but%20a%20structural%20mismatch%20between%20current%20visual%20backbones%20and%20the%20physical%20requirements%20of%20closed-loop%20control.%20While%20state-of-the-art%20vision%20encoders%20%28including%20those%20used%20in%20VLAs%29%20optimize%20for%20semantic%20invariance%20to%20stabilize%20classification%2C%20manipulation%20typically%20demands%20geometric%20sensitivity%20the%20ability%20to%20map%20millimeter-level%20pose%20shifts%20to%20predictable%20feature%20changes.%20Their%20discriminative%20objective%20creates%20a%20%22blind%20spot%22%20for%20fine-grained%20control%2C%20whereas%20generative%20diffusion%20models%20inherently%20encode%20geometric%20dependencies%20within%20their%20latent%20manifolds%2C%20encouraging%20the%20preservation%20of%20dense%20multi-scale%20spatial%20structure.%20However%2C%20directly%20deploying%20stochastic%20diffusion%20features%20for%20control%20is%20hindered%20by%20stochastic%20instability%2C%20inference%20latency%2C%20and%20representation%20drift%20during%20fine-tuning.%20To%20bridge%20this%20gap%2C%20we%20propose%20Robot-DIFT%2C%20a%20framework%20that%20decouples%20the%20source%20of%20geometric%20information%20from%20the%20process%20of%20inference%20via%20Manifold%20Distillation.%20By%20distilling%20a%20frozen%20diffusion%20teacher%20into%20a%20deterministic%20Spatial-Semantic%20Feature%20Pyramid%20Network%20%28S2-FPN%29%2C%20we%20retain%20the%20rich%20geometric%20priors%20of%20the%20generative%20model%20while%20ensuring%20temporal%20stability%2C%20real-time%20execution%2C%20and%20robustness%20against%20drift.%20Pretrained%20on%20the%20large-scale%20DROID%20dataset%2C%20Robot-DIFT%20demonstrates%20superior%20geometric%20consistency%20and%20control%20performance%20compared%20to%20leading%20discriminative%20baselines%2C%20supporting%20the%20view%20that%20how%20a%20model%20learns%20to%20see%20dictates%20how%20well%20it%20can%20learn%20to%20act.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot-DIFT%253A%2520Distilling%2520Diffusion%2520Features%2520for%2520Geometrically%2520Consistent%2520Visuomotor%2520Control%26entry.906535625%3DYu%2520Deng%2520and%2520Yufeng%2520Jin%2520and%2520Xiaogang%2520Jia%2520and%2520Jiahong%2520Xue%2520and%2520Gerhard%2520Neumann%2520and%2520Georgia%2520Chalvatzaki%26entry.1292438233%3DWe%2520hypothesize%2520that%2520a%2520key%2520bottleneck%2520in%2520generalizable%2520robot%2520manipulation%2520is%2520not%2520solely%2520data%2520scale%2520or%2520policy%2520capacity%252C%2520but%2520a%2520structural%2520mismatch%2520between%2520current%2520visual%2520backbones%2520and%2520the%2520physical%2520requirements%2520of%2520closed-loop%2520control.%2520While%2520state-of-the-art%2520vision%2520encoders%2520%2528including%2520those%2520used%2520in%2520VLAs%2529%2520optimize%2520for%2520semantic%2520invariance%2520to%2520stabilize%2520classification%252C%2520manipulation%2520typically%2520demands%2520geometric%2520sensitivity%2520the%2520ability%2520to%2520map%2520millimeter-level%2520pose%2520shifts%2520to%2520predictable%2520feature%2520changes.%2520Their%2520discriminative%2520objective%2520creates%2520a%2520%2522blind%2520spot%2522%2520for%2520fine-grained%2520control%252C%2520whereas%2520generative%2520diffusion%2520models%2520inherently%2520encode%2520geometric%2520dependencies%2520within%2520their%2520latent%2520manifolds%252C%2520encouraging%2520the%2520preservation%2520of%2520dense%2520multi-scale%2520spatial%2520structure.%2520However%252C%2520directly%2520deploying%2520stochastic%2520diffusion%2520features%2520for%2520control%2520is%2520hindered%2520by%2520stochastic%2520instability%252C%2520inference%2520latency%252C%2520and%2520representation%2520drift%2520during%2520fine-tuning.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Robot-DIFT%252C%2520a%2520framework%2520that%2520decouples%2520the%2520source%2520of%2520geometric%2520information%2520from%2520the%2520process%2520of%2520inference%2520via%2520Manifold%2520Distillation.%2520By%2520distilling%2520a%2520frozen%2520diffusion%2520teacher%2520into%2520a%2520deterministic%2520Spatial-Semantic%2520Feature%2520Pyramid%2520Network%2520%2528S2-FPN%2529%252C%2520we%2520retain%2520the%2520rich%2520geometric%2520priors%2520of%2520the%2520generative%2520model%2520while%2520ensuring%2520temporal%2520stability%252C%2520real-time%2520execution%252C%2520and%2520robustness%2520against%2520drift.%2520Pretrained%2520on%2520the%2520large-scale%2520DROID%2520dataset%252C%2520Robot-DIFT%2520demonstrates%2520superior%2520geometric%2520consistency%2520and%2520control%2520performance%2520compared%2520to%2520leading%2520discriminative%2520baselines%252C%2520supporting%2520the%2520view%2520that%2520how%2520a%2520model%2520learns%2520to%2520see%2520dictates%2520how%2520well%2520it%2520can%2520learn%2520to%2520act.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot-DIFT%3A%20Distilling%20Diffusion%20Features%20for%20Geometrically%20Consistent%20Visuomotor%20Control&entry.906535625=Yu%20Deng%20and%20Yufeng%20Jin%20and%20Xiaogang%20Jia%20and%20Jiahong%20Xue%20and%20Gerhard%20Neumann%20and%20Georgia%20Chalvatzaki&entry.1292438233=We%20hypothesize%20that%20a%20key%20bottleneck%20in%20generalizable%20robot%20manipulation%20is%20not%20solely%20data%20scale%20or%20policy%20capacity%2C%20but%20a%20structural%20mismatch%20between%20current%20visual%20backbones%20and%20the%20physical%20requirements%20of%20closed-loop%20control.%20While%20state-of-the-art%20vision%20encoders%20%28including%20those%20used%20in%20VLAs%29%20optimize%20for%20semantic%20invariance%20to%20stabilize%20classification%2C%20manipulation%20typically%20demands%20geometric%20sensitivity%20the%20ability%20to%20map%20millimeter-level%20pose%20shifts%20to%20predictable%20feature%20changes.%20Their%20discriminative%20objective%20creates%20a%20%22blind%20spot%22%20for%20fine-grained%20control%2C%20whereas%20generative%20diffusion%20models%20inherently%20encode%20geometric%20dependencies%20within%20their%20latent%20manifolds%2C%20encouraging%20the%20preservation%20of%20dense%20multi-scale%20spatial%20structure.%20However%2C%20directly%20deploying%20stochastic%20diffusion%20features%20for%20control%20is%20hindered%20by%20stochastic%20instability%2C%20inference%20latency%2C%20and%20representation%20drift%20during%20fine-tuning.%20To%20bridge%20this%20gap%2C%20we%20propose%20Robot-DIFT%2C%20a%20framework%20that%20decouples%20the%20source%20of%20geometric%20information%20from%20the%20process%20of%20inference%20via%20Manifold%20Distillation.%20By%20distilling%20a%20frozen%20diffusion%20teacher%20into%20a%20deterministic%20Spatial-Semantic%20Feature%20Pyramid%20Network%20%28S2-FPN%29%2C%20we%20retain%20the%20rich%20geometric%20priors%20of%20the%20generative%20model%20while%20ensuring%20temporal%20stability%2C%20real-time%20execution%2C%20and%20robustness%20against%20drift.%20Pretrained%20on%20the%20large-scale%20DROID%20dataset%2C%20Robot-DIFT%20demonstrates%20superior%20geometric%20consistency%20and%20control%20performance%20compared%20to%20leading%20discriminative%20baselines%2C%20supporting%20the%20view%20that%20how%20a%20model%20learns%20to%20see%20dictates%20how%20well%20it%20can%20learn%20to%20act.&entry.1838667208=http%3A//arxiv.org/abs/2602.11934v1&entry.124074799=Read"},
{"title": "AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution", "author": "Taian Guo and Haiyang Shen and Junyu Luo and Binqi Chen and Hongjun Ding and Jinsheng Huang and Luchen Liu and Yun Ma and Ming Zhang", "abstract": "Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.", "link": "http://arxiv.org/abs/2602.11917v1", "date": "2026-02-12", "relevancy": 2.3901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaPROBE%3A%20Alpha%20Mining%20via%20Principled%20Retrieval%20and%20On-graph%20biased%20evolution&body=Title%3A%20AlphaPROBE%3A%20Alpha%20Mining%20via%20Principled%20Retrieval%20and%20On-graph%20biased%20evolution%0AAuthor%3A%20Taian%20Guo%20and%20Haiyang%20Shen%20and%20Junyu%20Luo%20and%20Binqi%20Chen%20and%20Hongjun%20Ding%20and%20Jinsheng%20Huang%20and%20Luchen%20Liu%20and%20Yun%20Ma%20and%20Ming%20Zhang%0AAbstract%3A%20Extracting%20signals%20through%20alpha%20factor%20mining%20is%20a%20fundamental%20challenge%20in%20quantitative%20finance.%20Existing%20automated%20methods%20primarily%20follow%20two%20paradigms%3A%20Decoupled%20Factor%20Generation%2C%20which%20treats%20factor%20discovery%20as%20isolated%20events%2C%20and%20Iterative%20Factor%20Evolution%2C%20which%20focuses%20on%20local%20parent-child%20refinements.%20However%2C%20both%20paradigms%20lack%20a%20global%20structural%20view%2C%20often%20treating%20factor%20pools%20as%20unstructured%20collections%20or%20fragmented%20chains%2C%20which%20leads%20to%20redundant%20search%20and%20limited%20diversity.%20To%20address%20these%20limitations%2C%20we%20introduce%20AlphaPROBE%20%28Alpha%20Mining%20via%20Principled%20Retrieval%20and%20On-graph%20Biased%20Evolution%29%2C%20a%20framework%20that%20reframes%20alpha%20mining%20as%20the%20strategic%20navigation%20of%20a%20Directed%20Acyclic%20Graph%20%28DAG%29.%20By%20modeling%20factors%20as%20nodes%20and%20evolutionary%20links%20as%20edges%2C%20AlphaPROBE%20treats%20the%20factor%20pool%20as%20a%20dynamic%2C%20interconnected%20ecosystem.%20The%20framework%20consists%20of%20two%20core%20components%3A%20a%20Bayesian%20Factor%20Retriever%20that%20identifies%20high-potential%20seeds%20by%20balancing%20exploitation%20and%20exploration%20through%20a%20posterior%20probability%20model%2C%20and%20a%20DAG-aware%20Factor%20Generator%20that%20leverages%20the%20full%20ancestral%20trace%20of%20factors%20to%20produce%20context-aware%2C%20nonredundant%20optimizations.%20Extensive%20experiments%20on%20three%20major%20Chinese%20stock%20market%20datasets%20against%208%20competitive%20baselines%20demonstrate%20that%20AlphaPROBE%20significantly%20gains%20enhanced%20performance%20in%20predictive%20accuracy%2C%20return%20stability%20and%20training%20efficiency.%20Our%20results%20confirm%20that%20leveraging%20global%20evolutionary%20topology%20is%20essential%20for%20efficient%20and%20robust%20automated%20alpha%20discovery.%20We%20have%20open-sourced%20our%20implementation%20at%20https%3A//github.com/gta0804/AlphaPROBE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaPROBE%253A%2520Alpha%2520Mining%2520via%2520Principled%2520Retrieval%2520and%2520On-graph%2520biased%2520evolution%26entry.906535625%3DTaian%2520Guo%2520and%2520Haiyang%2520Shen%2520and%2520Junyu%2520Luo%2520and%2520Binqi%2520Chen%2520and%2520Hongjun%2520Ding%2520and%2520Jinsheng%2520Huang%2520and%2520Luchen%2520Liu%2520and%2520Yun%2520Ma%2520and%2520Ming%2520Zhang%26entry.1292438233%3DExtracting%2520signals%2520through%2520alpha%2520factor%2520mining%2520is%2520a%2520fundamental%2520challenge%2520in%2520quantitative%2520finance.%2520Existing%2520automated%2520methods%2520primarily%2520follow%2520two%2520paradigms%253A%2520Decoupled%2520Factor%2520Generation%252C%2520which%2520treats%2520factor%2520discovery%2520as%2520isolated%2520events%252C%2520and%2520Iterative%2520Factor%2520Evolution%252C%2520which%2520focuses%2520on%2520local%2520parent-child%2520refinements.%2520However%252C%2520both%2520paradigms%2520lack%2520a%2520global%2520structural%2520view%252C%2520often%2520treating%2520factor%2520pools%2520as%2520unstructured%2520collections%2520or%2520fragmented%2520chains%252C%2520which%2520leads%2520to%2520redundant%2520search%2520and%2520limited%2520diversity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520AlphaPROBE%2520%2528Alpha%2520Mining%2520via%2520Principled%2520Retrieval%2520and%2520On-graph%2520Biased%2520Evolution%2529%252C%2520a%2520framework%2520that%2520reframes%2520alpha%2520mining%2520as%2520the%2520strategic%2520navigation%2520of%2520a%2520Directed%2520Acyclic%2520Graph%2520%2528DAG%2529.%2520By%2520modeling%2520factors%2520as%2520nodes%2520and%2520evolutionary%2520links%2520as%2520edges%252C%2520AlphaPROBE%2520treats%2520the%2520factor%2520pool%2520as%2520a%2520dynamic%252C%2520interconnected%2520ecosystem.%2520The%2520framework%2520consists%2520of%2520two%2520core%2520components%253A%2520a%2520Bayesian%2520Factor%2520Retriever%2520that%2520identifies%2520high-potential%2520seeds%2520by%2520balancing%2520exploitation%2520and%2520exploration%2520through%2520a%2520posterior%2520probability%2520model%252C%2520and%2520a%2520DAG-aware%2520Factor%2520Generator%2520that%2520leverages%2520the%2520full%2520ancestral%2520trace%2520of%2520factors%2520to%2520produce%2520context-aware%252C%2520nonredundant%2520optimizations.%2520Extensive%2520experiments%2520on%2520three%2520major%2520Chinese%2520stock%2520market%2520datasets%2520against%25208%2520competitive%2520baselines%2520demonstrate%2520that%2520AlphaPROBE%2520significantly%2520gains%2520enhanced%2520performance%2520in%2520predictive%2520accuracy%252C%2520return%2520stability%2520and%2520training%2520efficiency.%2520Our%2520results%2520confirm%2520that%2520leveraging%2520global%2520evolutionary%2520topology%2520is%2520essential%2520for%2520efficient%2520and%2520robust%2520automated%2520alpha%2520discovery.%2520We%2520have%2520open-sourced%2520our%2520implementation%2520at%2520https%253A//github.com/gta0804/AlphaPROBE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaPROBE%3A%20Alpha%20Mining%20via%20Principled%20Retrieval%20and%20On-graph%20biased%20evolution&entry.906535625=Taian%20Guo%20and%20Haiyang%20Shen%20and%20Junyu%20Luo%20and%20Binqi%20Chen%20and%20Hongjun%20Ding%20and%20Jinsheng%20Huang%20and%20Luchen%20Liu%20and%20Yun%20Ma%20and%20Ming%20Zhang&entry.1292438233=Extracting%20signals%20through%20alpha%20factor%20mining%20is%20a%20fundamental%20challenge%20in%20quantitative%20finance.%20Existing%20automated%20methods%20primarily%20follow%20two%20paradigms%3A%20Decoupled%20Factor%20Generation%2C%20which%20treats%20factor%20discovery%20as%20isolated%20events%2C%20and%20Iterative%20Factor%20Evolution%2C%20which%20focuses%20on%20local%20parent-child%20refinements.%20However%2C%20both%20paradigms%20lack%20a%20global%20structural%20view%2C%20often%20treating%20factor%20pools%20as%20unstructured%20collections%20or%20fragmented%20chains%2C%20which%20leads%20to%20redundant%20search%20and%20limited%20diversity.%20To%20address%20these%20limitations%2C%20we%20introduce%20AlphaPROBE%20%28Alpha%20Mining%20via%20Principled%20Retrieval%20and%20On-graph%20Biased%20Evolution%29%2C%20a%20framework%20that%20reframes%20alpha%20mining%20as%20the%20strategic%20navigation%20of%20a%20Directed%20Acyclic%20Graph%20%28DAG%29.%20By%20modeling%20factors%20as%20nodes%20and%20evolutionary%20links%20as%20edges%2C%20AlphaPROBE%20treats%20the%20factor%20pool%20as%20a%20dynamic%2C%20interconnected%20ecosystem.%20The%20framework%20consists%20of%20two%20core%20components%3A%20a%20Bayesian%20Factor%20Retriever%20that%20identifies%20high-potential%20seeds%20by%20balancing%20exploitation%20and%20exploration%20through%20a%20posterior%20probability%20model%2C%20and%20a%20DAG-aware%20Factor%20Generator%20that%20leverages%20the%20full%20ancestral%20trace%20of%20factors%20to%20produce%20context-aware%2C%20nonredundant%20optimizations.%20Extensive%20experiments%20on%20three%20major%20Chinese%20stock%20market%20datasets%20against%208%20competitive%20baselines%20demonstrate%20that%20AlphaPROBE%20significantly%20gains%20enhanced%20performance%20in%20predictive%20accuracy%2C%20return%20stability%20and%20training%20efficiency.%20Our%20results%20confirm%20that%20leveraging%20global%20evolutionary%20topology%20is%20essential%20for%20efficient%20and%20robust%20automated%20alpha%20discovery.%20We%20have%20open-sourced%20our%20implementation%20at%20https%3A//github.com/gta0804/AlphaPROBE.&entry.1838667208=http%3A//arxiv.org/abs/2602.11917v1&entry.124074799=Read"},
{"title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations", "author": "Ruiqian Nai and Boyuan Zheng and Junming Zhao and Haodong Zhu and Sicong Dai and Zunhao Chen and Yihang Hu and Yingdong Hu and Tong Zhang and Chuan Wen and Yang Gao", "abstract": "Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.", "link": "http://arxiv.org/abs/2602.06643v2", "date": "2026-02-12", "relevancy": 2.3896, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6158}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5904}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humanoid%20Manipulation%20Interface%3A%20Humanoid%20Whole-Body%20Manipulation%20from%20Robot-Free%20Demonstrations&body=Title%3A%20Humanoid%20Manipulation%20Interface%3A%20Humanoid%20Whole-Body%20Manipulation%20from%20Robot-Free%20Demonstrations%0AAuthor%3A%20Ruiqian%20Nai%20and%20Boyuan%20Zheng%20and%20Junming%20Zhao%20and%20Haodong%20Zhu%20and%20Sicong%20Dai%20and%20Zunhao%20Chen%20and%20Yihang%20Hu%20and%20Yingdong%20Hu%20and%20Tong%20Zhang%20and%20Chuan%20Wen%20and%20Yang%20Gao%0AAbstract%3A%20Current%20approaches%20for%20humanoid%20whole-body%20manipulation%2C%20primarily%20relying%20on%20teleoperation%20or%20visual%20sim-to-real%20reinforcement%20learning%2C%20are%20hindered%20by%20hardware%20logistics%20and%20complex%20reward%20engineering.%20Consequently%2C%20demonstrated%20autonomous%20skills%20remain%20limited%20and%20are%20typically%20restricted%20to%20controlled%20environments.%20In%20this%20paper%2C%20we%20present%20the%20Humanoid%20Manipulation%20Interface%20%28HuMI%29%2C%20a%20portable%20and%20efficient%20framework%20for%20learning%20diverse%20whole-body%20manipulation%20tasks%20across%20various%20environments.%20HuMI%20enables%20robot-free%20data%20collection%20by%20capturing%20rich%20whole-body%20motion%20using%20portable%20hardware.%20This%20data%20drives%20a%20hierarchical%20learning%20pipeline%20that%20translates%20human%20motions%20into%20dexterous%20and%20feasible%20humanoid%20skills.%20Extensive%20experiments%20across%20five%20whole-body%20tasks--including%20kneeling%2C%20squatting%2C%20tossing%2C%20walking%2C%20and%20bimanual%20manipulation--demonstrate%20that%20HuMI%20achieves%20a%203x%20increase%20in%20data%20collection%20efficiency%20compared%20to%20teleoperation%20and%20attains%20a%2070%25%20success%20rate%20in%20unseen%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanoid%2520Manipulation%2520Interface%253A%2520Humanoid%2520Whole-Body%2520Manipulation%2520from%2520Robot-Free%2520Demonstrations%26entry.906535625%3DRuiqian%2520Nai%2520and%2520Boyuan%2520Zheng%2520and%2520Junming%2520Zhao%2520and%2520Haodong%2520Zhu%2520and%2520Sicong%2520Dai%2520and%2520Zunhao%2520Chen%2520and%2520Yihang%2520Hu%2520and%2520Yingdong%2520Hu%2520and%2520Tong%2520Zhang%2520and%2520Chuan%2520Wen%2520and%2520Yang%2520Gao%26entry.1292438233%3DCurrent%2520approaches%2520for%2520humanoid%2520whole-body%2520manipulation%252C%2520primarily%2520relying%2520on%2520teleoperation%2520or%2520visual%2520sim-to-real%2520reinforcement%2520learning%252C%2520are%2520hindered%2520by%2520hardware%2520logistics%2520and%2520complex%2520reward%2520engineering.%2520Consequently%252C%2520demonstrated%2520autonomous%2520skills%2520remain%2520limited%2520and%2520are%2520typically%2520restricted%2520to%2520controlled%2520environments.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Humanoid%2520Manipulation%2520Interface%2520%2528HuMI%2529%252C%2520a%2520portable%2520and%2520efficient%2520framework%2520for%2520learning%2520diverse%2520whole-body%2520manipulation%2520tasks%2520across%2520various%2520environments.%2520HuMI%2520enables%2520robot-free%2520data%2520collection%2520by%2520capturing%2520rich%2520whole-body%2520motion%2520using%2520portable%2520hardware.%2520This%2520data%2520drives%2520a%2520hierarchical%2520learning%2520pipeline%2520that%2520translates%2520human%2520motions%2520into%2520dexterous%2520and%2520feasible%2520humanoid%2520skills.%2520Extensive%2520experiments%2520across%2520five%2520whole-body%2520tasks--including%2520kneeling%252C%2520squatting%252C%2520tossing%252C%2520walking%252C%2520and%2520bimanual%2520manipulation--demonstrate%2520that%2520HuMI%2520achieves%2520a%25203x%2520increase%2520in%2520data%2520collection%2520efficiency%2520compared%2520to%2520teleoperation%2520and%2520attains%2520a%252070%2525%2520success%2520rate%2520in%2520unseen%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humanoid%20Manipulation%20Interface%3A%20Humanoid%20Whole-Body%20Manipulation%20from%20Robot-Free%20Demonstrations&entry.906535625=Ruiqian%20Nai%20and%20Boyuan%20Zheng%20and%20Junming%20Zhao%20and%20Haodong%20Zhu%20and%20Sicong%20Dai%20and%20Zunhao%20Chen%20and%20Yihang%20Hu%20and%20Yingdong%20Hu%20and%20Tong%20Zhang%20and%20Chuan%20Wen%20and%20Yang%20Gao&entry.1292438233=Current%20approaches%20for%20humanoid%20whole-body%20manipulation%2C%20primarily%20relying%20on%20teleoperation%20or%20visual%20sim-to-real%20reinforcement%20learning%2C%20are%20hindered%20by%20hardware%20logistics%20and%20complex%20reward%20engineering.%20Consequently%2C%20demonstrated%20autonomous%20skills%20remain%20limited%20and%20are%20typically%20restricted%20to%20controlled%20environments.%20In%20this%20paper%2C%20we%20present%20the%20Humanoid%20Manipulation%20Interface%20%28HuMI%29%2C%20a%20portable%20and%20efficient%20framework%20for%20learning%20diverse%20whole-body%20manipulation%20tasks%20across%20various%20environments.%20HuMI%20enables%20robot-free%20data%20collection%20by%20capturing%20rich%20whole-body%20motion%20using%20portable%20hardware.%20This%20data%20drives%20a%20hierarchical%20learning%20pipeline%20that%20translates%20human%20motions%20into%20dexterous%20and%20feasible%20humanoid%20skills.%20Extensive%20experiments%20across%20five%20whole-body%20tasks--including%20kneeling%2C%20squatting%2C%20tossing%2C%20walking%2C%20and%20bimanual%20manipulation--demonstrate%20that%20HuMI%20achieves%20a%203x%20increase%20in%20data%20collection%20efficiency%20compared%20to%20teleoperation%20and%20attains%20a%2070%25%20success%20rate%20in%20unseen%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.06643v2&entry.124074799=Read"},
{"title": "Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting", "author": "Hanzhou Liu and Jia Huang and Mi Lu and Srikanth Saripalli and Peng Jiang", "abstract": "We present Stylos, a single-forward 3D Gaussian framework for 3D style transfer that operates on unposed content, from a single image to a multi-view collection, conditioned on a separate reference style image. Stylos synthesizes a stylized 3D Gaussian scene without per-scene optimization or precomputed poses, achieving geometry-aware, view-consistent stylization that generalizes to unseen categories, scenes, and styles. At its core, Stylos adopts a Transformer backbone with two pathways: geometry predictions retain self-attention to preserve geometric fidelity, while style is injected via global cross-attention to enforce visual consistency across views. With the addition of a voxel-based 3D style loss that aligns aggregated scene features to style statistics, Stylos enforces view-consistent stylization while preserving geometry. Experiments across multiple datasets demonstrate that Stylos delivers high-quality zero-shot stylization, highlighting the effectiveness of global style-content coupling, the proposed 3D style loss, and the scalability of our framework from single view to large-scale multi-view settings. Our codes are available at https://github.com/HanzhouLiu/Stylos.", "link": "http://arxiv.org/abs/2509.26455v2", "date": "2026-02-12", "relevancy": 2.3828, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6186}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5869}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stylos%3A%20Multi-View%203D%20Stylization%20with%20Single-Forward%20Gaussian%20Splatting&body=Title%3A%20Stylos%3A%20Multi-View%203D%20Stylization%20with%20Single-Forward%20Gaussian%20Splatting%0AAuthor%3A%20Hanzhou%20Liu%20and%20Jia%20Huang%20and%20Mi%20Lu%20and%20Srikanth%20Saripalli%20and%20Peng%20Jiang%0AAbstract%3A%20We%20present%20Stylos%2C%20a%20single-forward%203D%20Gaussian%20framework%20for%203D%20style%20transfer%20that%20operates%20on%20unposed%20content%2C%20from%20a%20single%20image%20to%20a%20multi-view%20collection%2C%20conditioned%20on%20a%20separate%20reference%20style%20image.%20Stylos%20synthesizes%20a%20stylized%203D%20Gaussian%20scene%20without%20per-scene%20optimization%20or%20precomputed%20poses%2C%20achieving%20geometry-aware%2C%20view-consistent%20stylization%20that%20generalizes%20to%20unseen%20categories%2C%20scenes%2C%20and%20styles.%20At%20its%20core%2C%20Stylos%20adopts%20a%20Transformer%20backbone%20with%20two%20pathways%3A%20geometry%20predictions%20retain%20self-attention%20to%20preserve%20geometric%20fidelity%2C%20while%20style%20is%20injected%20via%20global%20cross-attention%20to%20enforce%20visual%20consistency%20across%20views.%20With%20the%20addition%20of%20a%20voxel-based%203D%20style%20loss%20that%20aligns%20aggregated%20scene%20features%20to%20style%20statistics%2C%20Stylos%20enforces%20view-consistent%20stylization%20while%20preserving%20geometry.%20Experiments%20across%20multiple%20datasets%20demonstrate%20that%20Stylos%20delivers%20high-quality%20zero-shot%20stylization%2C%20highlighting%20the%20effectiveness%20of%20global%20style-content%20coupling%2C%20the%20proposed%203D%20style%20loss%2C%20and%20the%20scalability%20of%20our%20framework%20from%20single%20view%20to%20large-scale%20multi-view%20settings.%20Our%20codes%20are%20available%20at%20https%3A//github.com/HanzhouLiu/Stylos.%0ALink%3A%20http%3A//arxiv.org/abs/2509.26455v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStylos%253A%2520Multi-View%25203D%2520Stylization%2520with%2520Single-Forward%2520Gaussian%2520Splatting%26entry.906535625%3DHanzhou%2520Liu%2520and%2520Jia%2520Huang%2520and%2520Mi%2520Lu%2520and%2520Srikanth%2520Saripalli%2520and%2520Peng%2520Jiang%26entry.1292438233%3DWe%2520present%2520Stylos%252C%2520a%2520single-forward%25203D%2520Gaussian%2520framework%2520for%25203D%2520style%2520transfer%2520that%2520operates%2520on%2520unposed%2520content%252C%2520from%2520a%2520single%2520image%2520to%2520a%2520multi-view%2520collection%252C%2520conditioned%2520on%2520a%2520separate%2520reference%2520style%2520image.%2520Stylos%2520synthesizes%2520a%2520stylized%25203D%2520Gaussian%2520scene%2520without%2520per-scene%2520optimization%2520or%2520precomputed%2520poses%252C%2520achieving%2520geometry-aware%252C%2520view-consistent%2520stylization%2520that%2520generalizes%2520to%2520unseen%2520categories%252C%2520scenes%252C%2520and%2520styles.%2520At%2520its%2520core%252C%2520Stylos%2520adopts%2520a%2520Transformer%2520backbone%2520with%2520two%2520pathways%253A%2520geometry%2520predictions%2520retain%2520self-attention%2520to%2520preserve%2520geometric%2520fidelity%252C%2520while%2520style%2520is%2520injected%2520via%2520global%2520cross-attention%2520to%2520enforce%2520visual%2520consistency%2520across%2520views.%2520With%2520the%2520addition%2520of%2520a%2520voxel-based%25203D%2520style%2520loss%2520that%2520aligns%2520aggregated%2520scene%2520features%2520to%2520style%2520statistics%252C%2520Stylos%2520enforces%2520view-consistent%2520stylization%2520while%2520preserving%2520geometry.%2520Experiments%2520across%2520multiple%2520datasets%2520demonstrate%2520that%2520Stylos%2520delivers%2520high-quality%2520zero-shot%2520stylization%252C%2520highlighting%2520the%2520effectiveness%2520of%2520global%2520style-content%2520coupling%252C%2520the%2520proposed%25203D%2520style%2520loss%252C%2520and%2520the%2520scalability%2520of%2520our%2520framework%2520from%2520single%2520view%2520to%2520large-scale%2520multi-view%2520settings.%2520Our%2520codes%2520are%2520available%2520at%2520https%253A//github.com/HanzhouLiu/Stylos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26455v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stylos%3A%20Multi-View%203D%20Stylization%20with%20Single-Forward%20Gaussian%20Splatting&entry.906535625=Hanzhou%20Liu%20and%20Jia%20Huang%20and%20Mi%20Lu%20and%20Srikanth%20Saripalli%20and%20Peng%20Jiang&entry.1292438233=We%20present%20Stylos%2C%20a%20single-forward%203D%20Gaussian%20framework%20for%203D%20style%20transfer%20that%20operates%20on%20unposed%20content%2C%20from%20a%20single%20image%20to%20a%20multi-view%20collection%2C%20conditioned%20on%20a%20separate%20reference%20style%20image.%20Stylos%20synthesizes%20a%20stylized%203D%20Gaussian%20scene%20without%20per-scene%20optimization%20or%20precomputed%20poses%2C%20achieving%20geometry-aware%2C%20view-consistent%20stylization%20that%20generalizes%20to%20unseen%20categories%2C%20scenes%2C%20and%20styles.%20At%20its%20core%2C%20Stylos%20adopts%20a%20Transformer%20backbone%20with%20two%20pathways%3A%20geometry%20predictions%20retain%20self-attention%20to%20preserve%20geometric%20fidelity%2C%20while%20style%20is%20injected%20via%20global%20cross-attention%20to%20enforce%20visual%20consistency%20across%20views.%20With%20the%20addition%20of%20a%20voxel-based%203D%20style%20loss%20that%20aligns%20aggregated%20scene%20features%20to%20style%20statistics%2C%20Stylos%20enforces%20view-consistent%20stylization%20while%20preserving%20geometry.%20Experiments%20across%20multiple%20datasets%20demonstrate%20that%20Stylos%20delivers%20high-quality%20zero-shot%20stylization%2C%20highlighting%20the%20effectiveness%20of%20global%20style-content%20coupling%2C%20the%20proposed%203D%20style%20loss%2C%20and%20the%20scalability%20of%20our%20framework%20from%20single%20view%20to%20large-scale%20multi-view%20settings.%20Our%20codes%20are%20available%20at%20https%3A//github.com/HanzhouLiu/Stylos.&entry.1838667208=http%3A//arxiv.org/abs/2509.26455v2&entry.124074799=Read"},
{"title": "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty", "author": "Zewei Yu and Lirong Gao and Yuke Zhu and Bo Zheng and Sheng Guo and Haobo Wang and Junbo Zhao", "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .", "link": "http://arxiv.org/abs/2602.12113v1", "date": "2026-02-12", "relevancy": 2.3775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stop%20Unnecessary%20Reflection%3A%20Training%20LRMs%20for%20Efficient%20Reasoning%20with%20Adaptive%20Reflection%20and%20Length%20Coordinated%20Penalty&body=Title%3A%20Stop%20Unnecessary%20Reflection%3A%20Training%20LRMs%20for%20Efficient%20Reasoning%20with%20Adaptive%20Reflection%20and%20Length%20Coordinated%20Penalty%0AAuthor%3A%20Zewei%20Yu%20and%20Lirong%20Gao%20and%20Yuke%20Zhu%20and%20Bo%20Zheng%20and%20Sheng%20Guo%20and%20Haobo%20Wang%20and%20Junbo%20Zhao%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20remarkable%20performance%20on%20complex%20reasoning%20tasks%20by%20employing%20test-time%20scaling.%20However%2C%20they%20often%20generate%20over-long%20chains-of-thought%20that%2C%20driven%20by%20substantial%20reflections%20such%20as%20repetitive%20self-questioning%20and%20circular%20reasoning%2C%20lead%20to%20high%20token%20consumption%2C%20substantial%20computational%20overhead%2C%20and%20increased%20latency%20without%20improving%20accuracy%2C%20particularly%20in%20smaller%20models.%20Our%20observation%20reveals%20that%20increasing%20problem%20complexity%20induces%20more%20excessive%20and%20unnecessary%20reflection%2C%20which%20in%20turn%20reduces%20accuracy%20and%20increases%20token%20overhead.%20To%20address%20this%20challenge%2C%20we%20propose%20Adaptive%20Reflection%20and%20Length%20Coordinated%20Penalty%20%28ARLCP%29%2C%20a%20novel%20reinforcement%20learning%20framework%20designed%20to%20dynamically%20balance%20reasoning%20efficiency%20and%20solution%20accuracy.%20ARLCP%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20reflection%20penalty%20that%20adaptively%20curtails%20unnecessary%20reflective%20steps%20while%20preserving%20essential%20reasoning%2C%20and%20%282%29%20a%20length%20penalty%20calibrated%20to%20the%20estimated%20complexity%20of%20the%20problem.%20By%20coordinating%20these%20penalties%2C%20ARLCP%20encourages%20the%20model%20to%20generate%20more%20concise%20and%20effective%20reasoning%20paths.%20We%20evaluate%20our%20method%20on%20five%20mathematical%20reasoning%20benchmarks%20using%20DeepSeek-R1-Distill-Qwen-1.5B%20and%20DeepSeek-R1-Distill-Qwen-7B%20models.%20Experimental%20results%20show%20that%20ARLCP%20achieves%20a%20superior%20efficiency-accuracy%20trade-off%20compared%20to%20existing%20approaches.%20For%20the%201.5B%20model%2C%20it%20reduces%20the%20average%20response%20length%20by%2053.1%25%20while%20simultaneously%20improving%20accuracy%20by%205.8%25.%20For%20the%207B%20model%2C%20it%20achieves%20a%2035.0%25%20reduction%20in%20length%20with%20a%202.7%25%20accuracy%20gain.%20The%20code%20is%20released%20at%20https%3A//github.com/ZeweiYu1/ARLCP%20.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStop%2520Unnecessary%2520Reflection%253A%2520Training%2520LRMs%2520for%2520Efficient%2520Reasoning%2520with%2520Adaptive%2520Reflection%2520and%2520Length%2520Coordinated%2520Penalty%26entry.906535625%3DZewei%2520Yu%2520and%2520Lirong%2520Gao%2520and%2520Yuke%2520Zhu%2520and%2520Bo%2520Zheng%2520and%2520Sheng%2520Guo%2520and%2520Haobo%2520Wang%2520and%2520Junbo%2520Zhao%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520on%2520complex%2520reasoning%2520tasks%2520by%2520employing%2520test-time%2520scaling.%2520However%252C%2520they%2520often%2520generate%2520over-long%2520chains-of-thought%2520that%252C%2520driven%2520by%2520substantial%2520reflections%2520such%2520as%2520repetitive%2520self-questioning%2520and%2520circular%2520reasoning%252C%2520lead%2520to%2520high%2520token%2520consumption%252C%2520substantial%2520computational%2520overhead%252C%2520and%2520increased%2520latency%2520without%2520improving%2520accuracy%252C%2520particularly%2520in%2520smaller%2520models.%2520Our%2520observation%2520reveals%2520that%2520increasing%2520problem%2520complexity%2520induces%2520more%2520excessive%2520and%2520unnecessary%2520reflection%252C%2520which%2520in%2520turn%2520reduces%2520accuracy%2520and%2520increases%2520token%2520overhead.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Adaptive%2520Reflection%2520and%2520Length%2520Coordinated%2520Penalty%2520%2528ARLCP%2529%252C%2520a%2520novel%2520reinforcement%2520learning%2520framework%2520designed%2520to%2520dynamically%2520balance%2520reasoning%2520efficiency%2520and%2520solution%2520accuracy.%2520ARLCP%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520reflection%2520penalty%2520that%2520adaptively%2520curtails%2520unnecessary%2520reflective%2520steps%2520while%2520preserving%2520essential%2520reasoning%252C%2520and%2520%25282%2529%2520a%2520length%2520penalty%2520calibrated%2520to%2520the%2520estimated%2520complexity%2520of%2520the%2520problem.%2520By%2520coordinating%2520these%2520penalties%252C%2520ARLCP%2520encourages%2520the%2520model%2520to%2520generate%2520more%2520concise%2520and%2520effective%2520reasoning%2520paths.%2520We%2520evaluate%2520our%2520method%2520on%2520five%2520mathematical%2520reasoning%2520benchmarks%2520using%2520DeepSeek-R1-Distill-Qwen-1.5B%2520and%2520DeepSeek-R1-Distill-Qwen-7B%2520models.%2520Experimental%2520results%2520show%2520that%2520ARLCP%2520achieves%2520a%2520superior%2520efficiency-accuracy%2520trade-off%2520compared%2520to%2520existing%2520approaches.%2520For%2520the%25201.5B%2520model%252C%2520it%2520reduces%2520the%2520average%2520response%2520length%2520by%252053.1%2525%2520while%2520simultaneously%2520improving%2520accuracy%2520by%25205.8%2525.%2520For%2520the%25207B%2520model%252C%2520it%2520achieves%2520a%252035.0%2525%2520reduction%2520in%2520length%2520with%2520a%25202.7%2525%2520accuracy%2520gain.%2520The%2520code%2520is%2520released%2520at%2520https%253A//github.com/ZeweiYu1/ARLCP%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stop%20Unnecessary%20Reflection%3A%20Training%20LRMs%20for%20Efficient%20Reasoning%20with%20Adaptive%20Reflection%20and%20Length%20Coordinated%20Penalty&entry.906535625=Zewei%20Yu%20and%20Lirong%20Gao%20and%20Yuke%20Zhu%20and%20Bo%20Zheng%20and%20Sheng%20Guo%20and%20Haobo%20Wang%20and%20Junbo%20Zhao&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20have%20demonstrated%20remarkable%20performance%20on%20complex%20reasoning%20tasks%20by%20employing%20test-time%20scaling.%20However%2C%20they%20often%20generate%20over-long%20chains-of-thought%20that%2C%20driven%20by%20substantial%20reflections%20such%20as%20repetitive%20self-questioning%20and%20circular%20reasoning%2C%20lead%20to%20high%20token%20consumption%2C%20substantial%20computational%20overhead%2C%20and%20increased%20latency%20without%20improving%20accuracy%2C%20particularly%20in%20smaller%20models.%20Our%20observation%20reveals%20that%20increasing%20problem%20complexity%20induces%20more%20excessive%20and%20unnecessary%20reflection%2C%20which%20in%20turn%20reduces%20accuracy%20and%20increases%20token%20overhead.%20To%20address%20this%20challenge%2C%20we%20propose%20Adaptive%20Reflection%20and%20Length%20Coordinated%20Penalty%20%28ARLCP%29%2C%20a%20novel%20reinforcement%20learning%20framework%20designed%20to%20dynamically%20balance%20reasoning%20efficiency%20and%20solution%20accuracy.%20ARLCP%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20reflection%20penalty%20that%20adaptively%20curtails%20unnecessary%20reflective%20steps%20while%20preserving%20essential%20reasoning%2C%20and%20%282%29%20a%20length%20penalty%20calibrated%20to%20the%20estimated%20complexity%20of%20the%20problem.%20By%20coordinating%20these%20penalties%2C%20ARLCP%20encourages%20the%20model%20to%20generate%20more%20concise%20and%20effective%20reasoning%20paths.%20We%20evaluate%20our%20method%20on%20five%20mathematical%20reasoning%20benchmarks%20using%20DeepSeek-R1-Distill-Qwen-1.5B%20and%20DeepSeek-R1-Distill-Qwen-7B%20models.%20Experimental%20results%20show%20that%20ARLCP%20achieves%20a%20superior%20efficiency-accuracy%20trade-off%20compared%20to%20existing%20approaches.%20For%20the%201.5B%20model%2C%20it%20reduces%20the%20average%20response%20length%20by%2053.1%25%20while%20simultaneously%20improving%20accuracy%20by%205.8%25.%20For%20the%207B%20model%2C%20it%20achieves%20a%2035.0%25%20reduction%20in%20length%20with%20a%202.7%25%20accuracy%20gain.%20The%20code%20is%20released%20at%20https%3A//github.com/ZeweiYu1/ARLCP%20.&entry.1838667208=http%3A//arxiv.org/abs/2602.12113v1&entry.124074799=Read"},
{"title": "EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation", "author": "Lior Broide and Roni Stern and Argaman Mordoch", "abstract": "Search-Based Software Testing (SBST) is a well-established approach for automated unit test generation, yet it often suffers from premature convergence and limited diversity in the generated test suites. Recently, Large Language Models (LLMs) have emerged as an alternative technique for unit test generation. We present EvoGPT, a hybrid test generation system that integrates LLM-based test generation with SBST-based test suite optimization. EvoGPT uses LLMs to generate an initial population of test suites, and uses an Evolutionary Algorithm (EA) to further optimize this test suite population. A distinguishing feature of EvoGPT is its explicit enforcement of diversity, achieved through the use of multiple temperatures and prompt instructions during test generation. In addition, each LLM-generated test is refined using a generation-repair loop and coverage-guided assertion generation. To address evolutionary plateaus, EvoGPT also detects stagnation during search and injects additional LLM-generated tests aimed at previously uncovered branches. Here too diversity is enforced using multiple temperatures and prompt instructions. We evaluate EvoGPT on Defects4J, a standard benchmark for test generation. The results show that EvoGPT achieves, on average, a 10% improvement in both code coverage and mutation score metrics compared to TestART, an LLM-only baseline; and EvoSuite, a standard SBST baseline. An ablation study indicates that explicitly enforcing diversity both at initialization and during the search is key to effectively leveraging LLMs for automated unit test generation.", "link": "http://arxiv.org/abs/2505.12424v3", "date": "2026-02-12", "relevancy": 2.3635, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4907}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.474}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoGPT%3A%20Leveraging%20LLM-Driven%20Seed%20Diversity%20to%20Improve%20Search-Based%20Test%20Suite%20Generation&body=Title%3A%20EvoGPT%3A%20Leveraging%20LLM-Driven%20Seed%20Diversity%20to%20Improve%20Search-Based%20Test%20Suite%20Generation%0AAuthor%3A%20Lior%20Broide%20and%20Roni%20Stern%20and%20Argaman%20Mordoch%0AAbstract%3A%20Search-Based%20Software%20Testing%20%28SBST%29%20is%20a%20well-established%20approach%20for%20automated%20unit%20test%20generation%2C%20yet%20it%20often%20suffers%20from%20premature%20convergence%20and%20limited%20diversity%20in%20the%20generated%20test%20suites.%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20an%20alternative%20technique%20for%20unit%20test%20generation.%20We%20present%20EvoGPT%2C%20a%20hybrid%20test%20generation%20system%20that%20integrates%20LLM-based%20test%20generation%20with%20SBST-based%20test%20suite%20optimization.%20EvoGPT%20uses%20LLMs%20to%20generate%20an%20initial%20population%20of%20test%20suites%2C%20and%20uses%20an%20Evolutionary%20Algorithm%20%28EA%29%20to%20further%20optimize%20this%20test%20suite%20population.%20A%20distinguishing%20feature%20of%20EvoGPT%20is%20its%20explicit%20enforcement%20of%20diversity%2C%20achieved%20through%20the%20use%20of%20multiple%20temperatures%20and%20prompt%20instructions%20during%20test%20generation.%20In%20addition%2C%20each%20LLM-generated%20test%20is%20refined%20using%20a%20generation-repair%20loop%20and%20coverage-guided%20assertion%20generation.%20To%20address%20evolutionary%20plateaus%2C%20EvoGPT%20also%20detects%20stagnation%20during%20search%20and%20injects%20additional%20LLM-generated%20tests%20aimed%20at%20previously%20uncovered%20branches.%20Here%20too%20diversity%20is%20enforced%20using%20multiple%20temperatures%20and%20prompt%20instructions.%20We%20evaluate%20EvoGPT%20on%20Defects4J%2C%20a%20standard%20benchmark%20for%20test%20generation.%20The%20results%20show%20that%20EvoGPT%20achieves%2C%20on%20average%2C%20a%2010%25%20improvement%20in%20both%20code%20coverage%20and%20mutation%20score%20metrics%20compared%20to%20TestART%2C%20an%20LLM-only%20baseline%3B%20and%20EvoSuite%2C%20a%20standard%20SBST%20baseline.%20An%20ablation%20study%20indicates%20that%20explicitly%20enforcing%20diversity%20both%20at%20initialization%20and%20during%20the%20search%20is%20key%20to%20effectively%20leveraging%20LLMs%20for%20automated%20unit%20test%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2505.12424v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoGPT%253A%2520Leveraging%2520LLM-Driven%2520Seed%2520Diversity%2520to%2520Improve%2520Search-Based%2520Test%2520Suite%2520Generation%26entry.906535625%3DLior%2520Broide%2520and%2520Roni%2520Stern%2520and%2520Argaman%2520Mordoch%26entry.1292438233%3DSearch-Based%2520Software%2520Testing%2520%2528SBST%2529%2520is%2520a%2520well-established%2520approach%2520for%2520automated%2520unit%2520test%2520generation%252C%2520yet%2520it%2520often%2520suffers%2520from%2520premature%2520convergence%2520and%2520limited%2520diversity%2520in%2520the%2520generated%2520test%2520suites.%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520an%2520alternative%2520technique%2520for%2520unit%2520test%2520generation.%2520We%2520present%2520EvoGPT%252C%2520a%2520hybrid%2520test%2520generation%2520system%2520that%2520integrates%2520LLM-based%2520test%2520generation%2520with%2520SBST-based%2520test%2520suite%2520optimization.%2520EvoGPT%2520uses%2520LLMs%2520to%2520generate%2520an%2520initial%2520population%2520of%2520test%2520suites%252C%2520and%2520uses%2520an%2520Evolutionary%2520Algorithm%2520%2528EA%2529%2520to%2520further%2520optimize%2520this%2520test%2520suite%2520population.%2520A%2520distinguishing%2520feature%2520of%2520EvoGPT%2520is%2520its%2520explicit%2520enforcement%2520of%2520diversity%252C%2520achieved%2520through%2520the%2520use%2520of%2520multiple%2520temperatures%2520and%2520prompt%2520instructions%2520during%2520test%2520generation.%2520In%2520addition%252C%2520each%2520LLM-generated%2520test%2520is%2520refined%2520using%2520a%2520generation-repair%2520loop%2520and%2520coverage-guided%2520assertion%2520generation.%2520To%2520address%2520evolutionary%2520plateaus%252C%2520EvoGPT%2520also%2520detects%2520stagnation%2520during%2520search%2520and%2520injects%2520additional%2520LLM-generated%2520tests%2520aimed%2520at%2520previously%2520uncovered%2520branches.%2520Here%2520too%2520diversity%2520is%2520enforced%2520using%2520multiple%2520temperatures%2520and%2520prompt%2520instructions.%2520We%2520evaluate%2520EvoGPT%2520on%2520Defects4J%252C%2520a%2520standard%2520benchmark%2520for%2520test%2520generation.%2520The%2520results%2520show%2520that%2520EvoGPT%2520achieves%252C%2520on%2520average%252C%2520a%252010%2525%2520improvement%2520in%2520both%2520code%2520coverage%2520and%2520mutation%2520score%2520metrics%2520compared%2520to%2520TestART%252C%2520an%2520LLM-only%2520baseline%253B%2520and%2520EvoSuite%252C%2520a%2520standard%2520SBST%2520baseline.%2520An%2520ablation%2520study%2520indicates%2520that%2520explicitly%2520enforcing%2520diversity%2520both%2520at%2520initialization%2520and%2520during%2520the%2520search%2520is%2520key%2520to%2520effectively%2520leveraging%2520LLMs%2520for%2520automated%2520unit%2520test%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12424v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoGPT%3A%20Leveraging%20LLM-Driven%20Seed%20Diversity%20to%20Improve%20Search-Based%20Test%20Suite%20Generation&entry.906535625=Lior%20Broide%20and%20Roni%20Stern%20and%20Argaman%20Mordoch&entry.1292438233=Search-Based%20Software%20Testing%20%28SBST%29%20is%20a%20well-established%20approach%20for%20automated%20unit%20test%20generation%2C%20yet%20it%20often%20suffers%20from%20premature%20convergence%20and%20limited%20diversity%20in%20the%20generated%20test%20suites.%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20an%20alternative%20technique%20for%20unit%20test%20generation.%20We%20present%20EvoGPT%2C%20a%20hybrid%20test%20generation%20system%20that%20integrates%20LLM-based%20test%20generation%20with%20SBST-based%20test%20suite%20optimization.%20EvoGPT%20uses%20LLMs%20to%20generate%20an%20initial%20population%20of%20test%20suites%2C%20and%20uses%20an%20Evolutionary%20Algorithm%20%28EA%29%20to%20further%20optimize%20this%20test%20suite%20population.%20A%20distinguishing%20feature%20of%20EvoGPT%20is%20its%20explicit%20enforcement%20of%20diversity%2C%20achieved%20through%20the%20use%20of%20multiple%20temperatures%20and%20prompt%20instructions%20during%20test%20generation.%20In%20addition%2C%20each%20LLM-generated%20test%20is%20refined%20using%20a%20generation-repair%20loop%20and%20coverage-guided%20assertion%20generation.%20To%20address%20evolutionary%20plateaus%2C%20EvoGPT%20also%20detects%20stagnation%20during%20search%20and%20injects%20additional%20LLM-generated%20tests%20aimed%20at%20previously%20uncovered%20branches.%20Here%20too%20diversity%20is%20enforced%20using%20multiple%20temperatures%20and%20prompt%20instructions.%20We%20evaluate%20EvoGPT%20on%20Defects4J%2C%20a%20standard%20benchmark%20for%20test%20generation.%20The%20results%20show%20that%20EvoGPT%20achieves%2C%20on%20average%2C%20a%2010%25%20improvement%20in%20both%20code%20coverage%20and%20mutation%20score%20metrics%20compared%20to%20TestART%2C%20an%20LLM-only%20baseline%3B%20and%20EvoSuite%2C%20a%20standard%20SBST%20baseline.%20An%20ablation%20study%20indicates%20that%20explicitly%20enforcing%20diversity%20both%20at%20initialization%20and%20during%20the%20search%20is%20key%20to%20effectively%20leveraging%20LLMs%20for%20automated%20unit%20test%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2505.12424v3&entry.124074799=Read"},
{"title": "Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms", "author": "Nobuyuki Ota", "abstract": "Current biological AI models lack interpretability -- their internal representations do not correspond to\n  biological relationships that researchers can examine. Here we present CDT-II, an \"AI microscope\" whose\n  attention maps are directly interpretable as regulatory structure. By mirroring the central dogma in its\n  architecture, CDT-II ensures that each attention mechanism corresponds to a specific biological relationship:\n  DNA self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA\n  cross-attention for transcriptional control. Using only genomic embeddings and raw per-cell expression, CDT-II\n  enables experimental biologists to observe regulatory networks in their own data. Applied to K562 CRISPRi\n  data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B regulatory\n  network without supervision (6.6-fold enrichment, $P = 3.5 \\times 10^{-17}$). Systematic comparison against\n  ENCODE K562 regulatory annotations reveals that cross-attention autonomously focuses on known regulatory\n  elements -- DNase hypersensitive sites ($201\\times$ enrichment), CTCF binding sites ($28\\times$), and histone\n  marks -- across all five held-out genes. Two distinct attention mechanisms independently identify an\n  overlapping RNA processing module (80% gene overlap; RNA binding enrichment $P = 1 \\times 10^{-16}$). CDT-II\n  establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing regulatory\n  structure rather than merely optimizing predictions.", "link": "http://arxiv.org/abs/2602.08751v2", "date": "2026-02-12", "relevancy": 2.3548, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Central%20Dogma%20Transformer%20II%3A%20An%20AI%20Microscope%20for%20Understanding%20Cellular%20Regulatory%20Mechanisms&body=Title%3A%20Central%20Dogma%20Transformer%20II%3A%20An%20AI%20Microscope%20for%20Understanding%20Cellular%20Regulatory%20Mechanisms%0AAuthor%3A%20Nobuyuki%20Ota%0AAbstract%3A%20Current%20biological%20AI%20models%20lack%20interpretability%20--%20their%20internal%20representations%20do%20not%20correspond%20to%0A%20%20biological%20relationships%20that%20researchers%20can%20examine.%20Here%20we%20present%20CDT-II%2C%20an%20%22AI%20microscope%22%20whose%0A%20%20attention%20maps%20are%20directly%20interpretable%20as%20regulatory%20structure.%20By%20mirroring%20the%20central%20dogma%20in%20its%0A%20%20architecture%2C%20CDT-II%20ensures%20that%20each%20attention%20mechanism%20corresponds%20to%20a%20specific%20biological%20relationship%3A%0A%20%20DNA%20self-attention%20for%20genomic%20relationships%2C%20RNA%20self-attention%20for%20gene%20co-regulation%2C%20and%20DNA-to-RNA%0A%20%20cross-attention%20for%20transcriptional%20control.%20Using%20only%20genomic%20embeddings%20and%20raw%20per-cell%20expression%2C%20CDT-II%0A%20%20enables%20experimental%20biologists%20to%20observe%20regulatory%20networks%20in%20their%20own%20data.%20Applied%20to%20K562%20CRISPRi%0A%20%20data%2C%20CDT-II%20predicts%20perturbation%20effects%20%28per-gene%20mean%20%24r%20%3D%200.84%24%29%20and%20recovers%20the%20GFI1B%20regulatory%0A%20%20network%20without%20supervision%20%286.6-fold%20enrichment%2C%20%24P%20%3D%203.5%20%5Ctimes%2010%5E%7B-17%7D%24%29.%20Systematic%20comparison%20against%0A%20%20ENCODE%20K562%20regulatory%20annotations%20reveals%20that%20cross-attention%20autonomously%20focuses%20on%20known%20regulatory%0A%20%20elements%20--%20DNase%20hypersensitive%20sites%20%28%24201%5Ctimes%24%20enrichment%29%2C%20CTCF%20binding%20sites%20%28%2428%5Ctimes%24%29%2C%20and%20histone%0A%20%20marks%20--%20across%20all%20five%20held-out%20genes.%20Two%20distinct%20attention%20mechanisms%20independently%20identify%20an%0A%20%20overlapping%20RNA%20processing%20module%20%2880%25%20gene%20overlap%3B%20RNA%20binding%20enrichment%20%24P%20%3D%201%20%5Ctimes%2010%5E%7B-16%7D%24%29.%20CDT-II%0A%20%20establishes%20mechanism-oriented%20AI%20as%20an%20alternative%20to%20task-oriented%20approaches%2C%20revealing%20regulatory%0A%20%20structure%20rather%20than%20merely%20optimizing%20predictions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentral%2520Dogma%2520Transformer%2520II%253A%2520An%2520AI%2520Microscope%2520for%2520Understanding%2520Cellular%2520Regulatory%2520Mechanisms%26entry.906535625%3DNobuyuki%2520Ota%26entry.1292438233%3DCurrent%2520biological%2520AI%2520models%2520lack%2520interpretability%2520--%2520their%2520internal%2520representations%2520do%2520not%2520correspond%2520to%250A%2520%2520biological%2520relationships%2520that%2520researchers%2520can%2520examine.%2520Here%2520we%2520present%2520CDT-II%252C%2520an%2520%2522AI%2520microscope%2522%2520whose%250A%2520%2520attention%2520maps%2520are%2520directly%2520interpretable%2520as%2520regulatory%2520structure.%2520By%2520mirroring%2520the%2520central%2520dogma%2520in%2520its%250A%2520%2520architecture%252C%2520CDT-II%2520ensures%2520that%2520each%2520attention%2520mechanism%2520corresponds%2520to%2520a%2520specific%2520biological%2520relationship%253A%250A%2520%2520DNA%2520self-attention%2520for%2520genomic%2520relationships%252C%2520RNA%2520self-attention%2520for%2520gene%2520co-regulation%252C%2520and%2520DNA-to-RNA%250A%2520%2520cross-attention%2520for%2520transcriptional%2520control.%2520Using%2520only%2520genomic%2520embeddings%2520and%2520raw%2520per-cell%2520expression%252C%2520CDT-II%250A%2520%2520enables%2520experimental%2520biologists%2520to%2520observe%2520regulatory%2520networks%2520in%2520their%2520own%2520data.%2520Applied%2520to%2520K562%2520CRISPRi%250A%2520%2520data%252C%2520CDT-II%2520predicts%2520perturbation%2520effects%2520%2528per-gene%2520mean%2520%2524r%2520%253D%25200.84%2524%2529%2520and%2520recovers%2520the%2520GFI1B%2520regulatory%250A%2520%2520network%2520without%2520supervision%2520%25286.6-fold%2520enrichment%252C%2520%2524P%2520%253D%25203.5%2520%255Ctimes%252010%255E%257B-17%257D%2524%2529.%2520Systematic%2520comparison%2520against%250A%2520%2520ENCODE%2520K562%2520regulatory%2520annotations%2520reveals%2520that%2520cross-attention%2520autonomously%2520focuses%2520on%2520known%2520regulatory%250A%2520%2520elements%2520--%2520DNase%2520hypersensitive%2520sites%2520%2528%2524201%255Ctimes%2524%2520enrichment%2529%252C%2520CTCF%2520binding%2520sites%2520%2528%252428%255Ctimes%2524%2529%252C%2520and%2520histone%250A%2520%2520marks%2520--%2520across%2520all%2520five%2520held-out%2520genes.%2520Two%2520distinct%2520attention%2520mechanisms%2520independently%2520identify%2520an%250A%2520%2520overlapping%2520RNA%2520processing%2520module%2520%252880%2525%2520gene%2520overlap%253B%2520RNA%2520binding%2520enrichment%2520%2524P%2520%253D%25201%2520%255Ctimes%252010%255E%257B-16%257D%2524%2529.%2520CDT-II%250A%2520%2520establishes%2520mechanism-oriented%2520AI%2520as%2520an%2520alternative%2520to%2520task-oriented%2520approaches%252C%2520revealing%2520regulatory%250A%2520%2520structure%2520rather%2520than%2520merely%2520optimizing%2520predictions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Central%20Dogma%20Transformer%20II%3A%20An%20AI%20Microscope%20for%20Understanding%20Cellular%20Regulatory%20Mechanisms&entry.906535625=Nobuyuki%20Ota&entry.1292438233=Current%20biological%20AI%20models%20lack%20interpretability%20--%20their%20internal%20representations%20do%20not%20correspond%20to%0A%20%20biological%20relationships%20that%20researchers%20can%20examine.%20Here%20we%20present%20CDT-II%2C%20an%20%22AI%20microscope%22%20whose%0A%20%20attention%20maps%20are%20directly%20interpretable%20as%20regulatory%20structure.%20By%20mirroring%20the%20central%20dogma%20in%20its%0A%20%20architecture%2C%20CDT-II%20ensures%20that%20each%20attention%20mechanism%20corresponds%20to%20a%20specific%20biological%20relationship%3A%0A%20%20DNA%20self-attention%20for%20genomic%20relationships%2C%20RNA%20self-attention%20for%20gene%20co-regulation%2C%20and%20DNA-to-RNA%0A%20%20cross-attention%20for%20transcriptional%20control.%20Using%20only%20genomic%20embeddings%20and%20raw%20per-cell%20expression%2C%20CDT-II%0A%20%20enables%20experimental%20biologists%20to%20observe%20regulatory%20networks%20in%20their%20own%20data.%20Applied%20to%20K562%20CRISPRi%0A%20%20data%2C%20CDT-II%20predicts%20perturbation%20effects%20%28per-gene%20mean%20%24r%20%3D%200.84%24%29%20and%20recovers%20the%20GFI1B%20regulatory%0A%20%20network%20without%20supervision%20%286.6-fold%20enrichment%2C%20%24P%20%3D%203.5%20%5Ctimes%2010%5E%7B-17%7D%24%29.%20Systematic%20comparison%20against%0A%20%20ENCODE%20K562%20regulatory%20annotations%20reveals%20that%20cross-attention%20autonomously%20focuses%20on%20known%20regulatory%0A%20%20elements%20--%20DNase%20hypersensitive%20sites%20%28%24201%5Ctimes%24%20enrichment%29%2C%20CTCF%20binding%20sites%20%28%2428%5Ctimes%24%29%2C%20and%20histone%0A%20%20marks%20--%20across%20all%20five%20held-out%20genes.%20Two%20distinct%20attention%20mechanisms%20independently%20identify%20an%0A%20%20overlapping%20RNA%20processing%20module%20%2880%25%20gene%20overlap%3B%20RNA%20binding%20enrichment%20%24P%20%3D%201%20%5Ctimes%2010%5E%7B-16%7D%24%29.%20CDT-II%0A%20%20establishes%20mechanism-oriented%20AI%20as%20an%20alternative%20to%20task-oriented%20approaches%2C%20revealing%20regulatory%0A%20%20structure%20rather%20than%20merely%20optimizing%20predictions.&entry.1838667208=http%3A//arxiv.org/abs/2602.08751v2&entry.124074799=Read"},
{"title": "Deep learning Based Correction Algorithms for 3D Medical Reconstruction in Computed Tomography and Macroscopic Imaging", "author": "Tomasz Les and Tomasz Markiewicz and Malgorzata Lorent and Miroslaw Dziekiewicz and Krzysztof Siwek", "abstract": "This paper introduces a hybrid two-stage registration framework for reconstructing three-dimensional (3D) kidney anatomy from macroscopic slices, using CT-derived models as the geometric reference standard. The approach addresses the data-scarcity and high-distortion challenges typical of macroscopic imaging, where fully learning-based registration (e.g., VoxelMorph) often fails to generalize due to limited training diversity and large nonrigid deformations that exceed the capture range of unconstrained convolutional filters. In the proposed pipeline, the Optimal Cross-section Matching (OCM) algorithm first performs constrained global alignment: translation, rotation, and uniform scaling to establish anatomically consistent slice initialization. Next, a lightweight deep-learning refinement network, inspired by VoxelMorph, predicts residual local deformations between consecutive slices. The core novelty of this architecture lies in its hierarchical decomposition of the registration manifold. This hybrid OCM+DL design integrates explicit geometric priors with the flexible learning capacity of neural networks, ensuring stable optimization and plausible deformation fields even with few training examples. Experiments on an original dataset of 40 kidneys demonstrated better results compared to single-stage baselines. The pipeline maintains physical calibration via Hough-based grid detection and employs Bezier-based contour smoothing for robust meshing and volume estimation. Although validated on kidney data, the proposed framework generalizes to other soft-tissue organs reconstructed from optical or photographic cross-sections. By decoupling interpretable global optimization from data-efficient deep refinement, the method advances the precision, reproducibility, and anatomical realism of multimodal 3D reconstructions for surgical planning, morphological assessment, and medical education.", "link": "http://arxiv.org/abs/2602.00220v2", "date": "2026-02-12", "relevancy": 2.3487, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6039}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20Based%20Correction%20Algorithms%20for%203D%20Medical%20Reconstruction%20in%20Computed%20Tomography%20and%20Macroscopic%20Imaging&body=Title%3A%20Deep%20learning%20Based%20Correction%20Algorithms%20for%203D%20Medical%20Reconstruction%20in%20Computed%20Tomography%20and%20Macroscopic%20Imaging%0AAuthor%3A%20Tomasz%20Les%20and%20Tomasz%20Markiewicz%20and%20Malgorzata%20Lorent%20and%20Miroslaw%20Dziekiewicz%20and%20Krzysztof%20Siwek%0AAbstract%3A%20This%20paper%20introduces%20a%20hybrid%20two-stage%20registration%20framework%20for%20reconstructing%20three-dimensional%20%283D%29%20kidney%20anatomy%20from%20macroscopic%20slices%2C%20using%20CT-derived%20models%20as%20the%20geometric%20reference%20standard.%20The%20approach%20addresses%20the%20data-scarcity%20and%20high-distortion%20challenges%20typical%20of%20macroscopic%20imaging%2C%20where%20fully%20learning-based%20registration%20%28e.g.%2C%20VoxelMorph%29%20often%20fails%20to%20generalize%20due%20to%20limited%20training%20diversity%20and%20large%20nonrigid%20deformations%20that%20exceed%20the%20capture%20range%20of%20unconstrained%20convolutional%20filters.%20In%20the%20proposed%20pipeline%2C%20the%20Optimal%20Cross-section%20Matching%20%28OCM%29%20algorithm%20first%20performs%20constrained%20global%20alignment%3A%20translation%2C%20rotation%2C%20and%20uniform%20scaling%20to%20establish%20anatomically%20consistent%20slice%20initialization.%20Next%2C%20a%20lightweight%20deep-learning%20refinement%20network%2C%20inspired%20by%20VoxelMorph%2C%20predicts%20residual%20local%20deformations%20between%20consecutive%20slices.%20The%20core%20novelty%20of%20this%20architecture%20lies%20in%20its%20hierarchical%20decomposition%20of%20the%20registration%20manifold.%20This%20hybrid%20OCM%2BDL%20design%20integrates%20explicit%20geometric%20priors%20with%20the%20flexible%20learning%20capacity%20of%20neural%20networks%2C%20ensuring%20stable%20optimization%20and%20plausible%20deformation%20fields%20even%20with%20few%20training%20examples.%20Experiments%20on%20an%20original%20dataset%20of%2040%20kidneys%20demonstrated%20better%20results%20compared%20to%20single-stage%20baselines.%20The%20pipeline%20maintains%20physical%20calibration%20via%20Hough-based%20grid%20detection%20and%20employs%20Bezier-based%20contour%20smoothing%20for%20robust%20meshing%20and%20volume%20estimation.%20Although%20validated%20on%20kidney%20data%2C%20the%20proposed%20framework%20generalizes%20to%20other%20soft-tissue%20organs%20reconstructed%20from%20optical%20or%20photographic%20cross-sections.%20By%20decoupling%20interpretable%20global%20optimization%20from%20data-efficient%20deep%20refinement%2C%20the%20method%20advances%20the%20precision%2C%20reproducibility%2C%20and%20anatomical%20realism%20of%20multimodal%203D%20reconstructions%20for%20surgical%20planning%2C%20morphological%20assessment%2C%20and%20medical%20education.%0ALink%3A%20http%3A//arxiv.org/abs/2602.00220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520Based%2520Correction%2520Algorithms%2520for%25203D%2520Medical%2520Reconstruction%2520in%2520Computed%2520Tomography%2520and%2520Macroscopic%2520Imaging%26entry.906535625%3DTomasz%2520Les%2520and%2520Tomasz%2520Markiewicz%2520and%2520Malgorzata%2520Lorent%2520and%2520Miroslaw%2520Dziekiewicz%2520and%2520Krzysztof%2520Siwek%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520hybrid%2520two-stage%2520registration%2520framework%2520for%2520reconstructing%2520three-dimensional%2520%25283D%2529%2520kidney%2520anatomy%2520from%2520macroscopic%2520slices%252C%2520using%2520CT-derived%2520models%2520as%2520the%2520geometric%2520reference%2520standard.%2520The%2520approach%2520addresses%2520the%2520data-scarcity%2520and%2520high-distortion%2520challenges%2520typical%2520of%2520macroscopic%2520imaging%252C%2520where%2520fully%2520learning-based%2520registration%2520%2528e.g.%252C%2520VoxelMorph%2529%2520often%2520fails%2520to%2520generalize%2520due%2520to%2520limited%2520training%2520diversity%2520and%2520large%2520nonrigid%2520deformations%2520that%2520exceed%2520the%2520capture%2520range%2520of%2520unconstrained%2520convolutional%2520filters.%2520In%2520the%2520proposed%2520pipeline%252C%2520the%2520Optimal%2520Cross-section%2520Matching%2520%2528OCM%2529%2520algorithm%2520first%2520performs%2520constrained%2520global%2520alignment%253A%2520translation%252C%2520rotation%252C%2520and%2520uniform%2520scaling%2520to%2520establish%2520anatomically%2520consistent%2520slice%2520initialization.%2520Next%252C%2520a%2520lightweight%2520deep-learning%2520refinement%2520network%252C%2520inspired%2520by%2520VoxelMorph%252C%2520predicts%2520residual%2520local%2520deformations%2520between%2520consecutive%2520slices.%2520The%2520core%2520novelty%2520of%2520this%2520architecture%2520lies%2520in%2520its%2520hierarchical%2520decomposition%2520of%2520the%2520registration%2520manifold.%2520This%2520hybrid%2520OCM%252BDL%2520design%2520integrates%2520explicit%2520geometric%2520priors%2520with%2520the%2520flexible%2520learning%2520capacity%2520of%2520neural%2520networks%252C%2520ensuring%2520stable%2520optimization%2520and%2520plausible%2520deformation%2520fields%2520even%2520with%2520few%2520training%2520examples.%2520Experiments%2520on%2520an%2520original%2520dataset%2520of%252040%2520kidneys%2520demonstrated%2520better%2520results%2520compared%2520to%2520single-stage%2520baselines.%2520The%2520pipeline%2520maintains%2520physical%2520calibration%2520via%2520Hough-based%2520grid%2520detection%2520and%2520employs%2520Bezier-based%2520contour%2520smoothing%2520for%2520robust%2520meshing%2520and%2520volume%2520estimation.%2520Although%2520validated%2520on%2520kidney%2520data%252C%2520the%2520proposed%2520framework%2520generalizes%2520to%2520other%2520soft-tissue%2520organs%2520reconstructed%2520from%2520optical%2520or%2520photographic%2520cross-sections.%2520By%2520decoupling%2520interpretable%2520global%2520optimization%2520from%2520data-efficient%2520deep%2520refinement%252C%2520the%2520method%2520advances%2520the%2520precision%252C%2520reproducibility%252C%2520and%2520anatomical%2520realism%2520of%2520multimodal%25203D%2520reconstructions%2520for%2520surgical%2520planning%252C%2520morphological%2520assessment%252C%2520and%2520medical%2520education.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.00220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20Based%20Correction%20Algorithms%20for%203D%20Medical%20Reconstruction%20in%20Computed%20Tomography%20and%20Macroscopic%20Imaging&entry.906535625=Tomasz%20Les%20and%20Tomasz%20Markiewicz%20and%20Malgorzata%20Lorent%20and%20Miroslaw%20Dziekiewicz%20and%20Krzysztof%20Siwek&entry.1292438233=This%20paper%20introduces%20a%20hybrid%20two-stage%20registration%20framework%20for%20reconstructing%20three-dimensional%20%283D%29%20kidney%20anatomy%20from%20macroscopic%20slices%2C%20using%20CT-derived%20models%20as%20the%20geometric%20reference%20standard.%20The%20approach%20addresses%20the%20data-scarcity%20and%20high-distortion%20challenges%20typical%20of%20macroscopic%20imaging%2C%20where%20fully%20learning-based%20registration%20%28e.g.%2C%20VoxelMorph%29%20often%20fails%20to%20generalize%20due%20to%20limited%20training%20diversity%20and%20large%20nonrigid%20deformations%20that%20exceed%20the%20capture%20range%20of%20unconstrained%20convolutional%20filters.%20In%20the%20proposed%20pipeline%2C%20the%20Optimal%20Cross-section%20Matching%20%28OCM%29%20algorithm%20first%20performs%20constrained%20global%20alignment%3A%20translation%2C%20rotation%2C%20and%20uniform%20scaling%20to%20establish%20anatomically%20consistent%20slice%20initialization.%20Next%2C%20a%20lightweight%20deep-learning%20refinement%20network%2C%20inspired%20by%20VoxelMorph%2C%20predicts%20residual%20local%20deformations%20between%20consecutive%20slices.%20The%20core%20novelty%20of%20this%20architecture%20lies%20in%20its%20hierarchical%20decomposition%20of%20the%20registration%20manifold.%20This%20hybrid%20OCM%2BDL%20design%20integrates%20explicit%20geometric%20priors%20with%20the%20flexible%20learning%20capacity%20of%20neural%20networks%2C%20ensuring%20stable%20optimization%20and%20plausible%20deformation%20fields%20even%20with%20few%20training%20examples.%20Experiments%20on%20an%20original%20dataset%20of%2040%20kidneys%20demonstrated%20better%20results%20compared%20to%20single-stage%20baselines.%20The%20pipeline%20maintains%20physical%20calibration%20via%20Hough-based%20grid%20detection%20and%20employs%20Bezier-based%20contour%20smoothing%20for%20robust%20meshing%20and%20volume%20estimation.%20Although%20validated%20on%20kidney%20data%2C%20the%20proposed%20framework%20generalizes%20to%20other%20soft-tissue%20organs%20reconstructed%20from%20optical%20or%20photographic%20cross-sections.%20By%20decoupling%20interpretable%20global%20optimization%20from%20data-efficient%20deep%20refinement%2C%20the%20method%20advances%20the%20precision%2C%20reproducibility%2C%20and%20anatomical%20realism%20of%20multimodal%203D%20reconstructions%20for%20surgical%20planning%2C%20morphological%20assessment%2C%20and%20medical%20education.&entry.1838667208=http%3A//arxiv.org/abs/2602.00220v2&entry.124074799=Read"},
{"title": "Projected Representation Conditioning for High-fidelity Novel View Synthesis", "author": "Min-Seop Kwak and Minkyung Kwon and Jinhyeok Choi and Jiho Park and Seungryong Kim", "abstract": "We propose a novel framework for diffusion-based novel view synthesis in which we leverage external representations as conditions, harnessing their geometric and semantic correspondence properties for enhanced geometric consistency in generated novel viewpoints. First, we provide a detailed analysis exploring the correspondence capabilities emergent in the spatial attention of external visual representations. Building from these insights, we propose a representation-guided novel view synthesis through dedicated representation projection modules that inject external representations into the diffusion process, a methodology named ReNoV, short for representation-guided novel view synthesis. Our experiments show that this design yields marked improvements in both reconstruction fidelity and inpainting quality, outperforming prior diffusion-based novel-view methods on standard benchmarks and enabling robust synthesis from sparse, unposed image collections.", "link": "http://arxiv.org/abs/2602.12003v1", "date": "2026-02-12", "relevancy": 2.3481, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.588}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.588}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projected%20Representation%20Conditioning%20for%20High-fidelity%20Novel%20View%20Synthesis&body=Title%3A%20Projected%20Representation%20Conditioning%20for%20High-fidelity%20Novel%20View%20Synthesis%0AAuthor%3A%20Min-Seop%20Kwak%20and%20Minkyung%20Kwon%20and%20Jinhyeok%20Choi%20and%20Jiho%20Park%20and%20Seungryong%20Kim%0AAbstract%3A%20We%20propose%20a%20novel%20framework%20for%20diffusion-based%20novel%20view%20synthesis%20in%20which%20we%20leverage%20external%20representations%20as%20conditions%2C%20harnessing%20their%20geometric%20and%20semantic%20correspondence%20properties%20for%20enhanced%20geometric%20consistency%20in%20generated%20novel%20viewpoints.%20First%2C%20we%20provide%20a%20detailed%20analysis%20exploring%20the%20correspondence%20capabilities%20emergent%20in%20the%20spatial%20attention%20of%20external%20visual%20representations.%20Building%20from%20these%20insights%2C%20we%20propose%20a%20representation-guided%20novel%20view%20synthesis%20through%20dedicated%20representation%20projection%20modules%20that%20inject%20external%20representations%20into%20the%20diffusion%20process%2C%20a%20methodology%20named%20ReNoV%2C%20short%20for%20representation-guided%20novel%20view%20synthesis.%20Our%20experiments%20show%20that%20this%20design%20yields%20marked%20improvements%20in%20both%20reconstruction%20fidelity%20and%20inpainting%20quality%2C%20outperforming%20prior%20diffusion-based%20novel-view%20methods%20on%20standard%20benchmarks%20and%20enabling%20robust%20synthesis%20from%20sparse%2C%20unposed%20image%20collections.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjected%2520Representation%2520Conditioning%2520for%2520High-fidelity%2520Novel%2520View%2520Synthesis%26entry.906535625%3DMin-Seop%2520Kwak%2520and%2520Minkyung%2520Kwon%2520and%2520Jinhyeok%2520Choi%2520and%2520Jiho%2520Park%2520and%2520Seungryong%2520Kim%26entry.1292438233%3DWe%2520propose%2520a%2520novel%2520framework%2520for%2520diffusion-based%2520novel%2520view%2520synthesis%2520in%2520which%2520we%2520leverage%2520external%2520representations%2520as%2520conditions%252C%2520harnessing%2520their%2520geometric%2520and%2520semantic%2520correspondence%2520properties%2520for%2520enhanced%2520geometric%2520consistency%2520in%2520generated%2520novel%2520viewpoints.%2520First%252C%2520we%2520provide%2520a%2520detailed%2520analysis%2520exploring%2520the%2520correspondence%2520capabilities%2520emergent%2520in%2520the%2520spatial%2520attention%2520of%2520external%2520visual%2520representations.%2520Building%2520from%2520these%2520insights%252C%2520we%2520propose%2520a%2520representation-guided%2520novel%2520view%2520synthesis%2520through%2520dedicated%2520representation%2520projection%2520modules%2520that%2520inject%2520external%2520representations%2520into%2520the%2520diffusion%2520process%252C%2520a%2520methodology%2520named%2520ReNoV%252C%2520short%2520for%2520representation-guided%2520novel%2520view%2520synthesis.%2520Our%2520experiments%2520show%2520that%2520this%2520design%2520yields%2520marked%2520improvements%2520in%2520both%2520reconstruction%2520fidelity%2520and%2520inpainting%2520quality%252C%2520outperforming%2520prior%2520diffusion-based%2520novel-view%2520methods%2520on%2520standard%2520benchmarks%2520and%2520enabling%2520robust%2520synthesis%2520from%2520sparse%252C%2520unposed%2520image%2520collections.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projected%20Representation%20Conditioning%20for%20High-fidelity%20Novel%20View%20Synthesis&entry.906535625=Min-Seop%20Kwak%20and%20Minkyung%20Kwon%20and%20Jinhyeok%20Choi%20and%20Jiho%20Park%20and%20Seungryong%20Kim&entry.1292438233=We%20propose%20a%20novel%20framework%20for%20diffusion-based%20novel%20view%20synthesis%20in%20which%20we%20leverage%20external%20representations%20as%20conditions%2C%20harnessing%20their%20geometric%20and%20semantic%20correspondence%20properties%20for%20enhanced%20geometric%20consistency%20in%20generated%20novel%20viewpoints.%20First%2C%20we%20provide%20a%20detailed%20analysis%20exploring%20the%20correspondence%20capabilities%20emergent%20in%20the%20spatial%20attention%20of%20external%20visual%20representations.%20Building%20from%20these%20insights%2C%20we%20propose%20a%20representation-guided%20novel%20view%20synthesis%20through%20dedicated%20representation%20projection%20modules%20that%20inject%20external%20representations%20into%20the%20diffusion%20process%2C%20a%20methodology%20named%20ReNoV%2C%20short%20for%20representation-guided%20novel%20view%20synthesis.%20Our%20experiments%20show%20that%20this%20design%20yields%20marked%20improvements%20in%20both%20reconstruction%20fidelity%20and%20inpainting%20quality%2C%20outperforming%20prior%20diffusion-based%20novel-view%20methods%20on%20standard%20benchmarks%20and%20enabling%20robust%20synthesis%20from%20sparse%2C%20unposed%20image%20collections.&entry.1838667208=http%3A//arxiv.org/abs/2602.12003v1&entry.124074799=Read"},
{"title": "Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards", "author": "Ryo Mikasa and Shun-ichiro Hayashi and Daichi Mukunoki and Tetsuya Hoshino and Takahiro Katagiri", "abstract": "Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.", "link": "http://arxiv.org/abs/2602.12049v1", "date": "2026-02-12", "relevancy": 2.3415, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4782}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4742}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20HPC%20Code%20Generation%20Capability%20of%20LLMs%20via%20Online%20Reinforcement%20Learning%20with%20Real-Machine%20Benchmark%20Rewards&body=Title%3A%20Improving%20HPC%20Code%20Generation%20Capability%20of%20LLMs%20via%20Online%20Reinforcement%20Learning%20with%20Real-Machine%20Benchmark%20Rewards%0AAuthor%3A%20Ryo%20Mikasa%20and%20Shun-ichiro%20Hayashi%20and%20Daichi%20Mukunoki%20and%20Tetsuya%20Hoshino%20and%20Takahiro%20Katagiri%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20code%20generation%20capabilities%2C%20yet%20the%20runtime%20performance%20of%20generated%20code%20is%20not%20guaranteed%2C%20and%20there%20have%20been%20few%20attempts%20to%20train%20LLMs%20using%20runtime%20performance%20as%20a%20reward%20in%20the%20HPC%20domain.%20We%20propose%20an%20online%20reinforcement%20learning%20approach%20that%20executes%20LLM-generated%20code%20on%20a%20supercomputer%20and%20directly%20feeds%20back%20the%20measured%20runtime%20performance%20%28GFLOPS%29%20as%20a%20reward.%20We%20further%20introduce%20a%20Staged%20Quality-Diversity%20%28SQD%29%20algorithm%20that%20progressively%20varies%20the%20permitted%20optimization%20techniques%20on%20a%20per-problem%20basis%2C%20enabling%20the%20model%20to%20learn%20code%20optimization%20from%20diverse%20perspectives.%20We%20build%20a%20distributed%20system%20connecting%20a%20GPU%20training%20cluster%20with%20a%20CPU%20benchmarking%20cluster%2C%20and%20train%20Qwen2.5%20Coder%2014B%20on%20a%20double-precision%20matrix%20multiplication%20task%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Through%20two%20experiments%2C%20we%20show%20that%20reinforcement%20learning%20combining%20runtime%20performance%20feedback%20with%20staged%20optimization%20can%20improve%20the%20HPC%20code%20generation%20capability%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520HPC%2520Code%2520Generation%2520Capability%2520of%2520LLMs%2520via%2520Online%2520Reinforcement%2520Learning%2520with%2520Real-Machine%2520Benchmark%2520Rewards%26entry.906535625%3DRyo%2520Mikasa%2520and%2520Shun-ichiro%2520Hayashi%2520and%2520Daichi%2520Mukunoki%2520and%2520Tetsuya%2520Hoshino%2520and%2520Takahiro%2520Katagiri%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520code%2520generation%2520capabilities%252C%2520yet%2520the%2520runtime%2520performance%2520of%2520generated%2520code%2520is%2520not%2520guaranteed%252C%2520and%2520there%2520have%2520been%2520few%2520attempts%2520to%2520train%2520LLMs%2520using%2520runtime%2520performance%2520as%2520a%2520reward%2520in%2520the%2520HPC%2520domain.%2520We%2520propose%2520an%2520online%2520reinforcement%2520learning%2520approach%2520that%2520executes%2520LLM-generated%2520code%2520on%2520a%2520supercomputer%2520and%2520directly%2520feeds%2520back%2520the%2520measured%2520runtime%2520performance%2520%2528GFLOPS%2529%2520as%2520a%2520reward.%2520We%2520further%2520introduce%2520a%2520Staged%2520Quality-Diversity%2520%2528SQD%2529%2520algorithm%2520that%2520progressively%2520varies%2520the%2520permitted%2520optimization%2520techniques%2520on%2520a%2520per-problem%2520basis%252C%2520enabling%2520the%2520model%2520to%2520learn%2520code%2520optimization%2520from%2520diverse%2520perspectives.%2520We%2520build%2520a%2520distributed%2520system%2520connecting%2520a%2520GPU%2520training%2520cluster%2520with%2520a%2520CPU%2520benchmarking%2520cluster%252C%2520and%2520train%2520Qwen2.5%2520Coder%252014B%2520on%2520a%2520double-precision%2520matrix%2520multiplication%2520task%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529.%2520Through%2520two%2520experiments%252C%2520we%2520show%2520that%2520reinforcement%2520learning%2520combining%2520runtime%2520performance%2520feedback%2520with%2520staged%2520optimization%2520can%2520improve%2520the%2520HPC%2520code%2520generation%2520capability%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20HPC%20Code%20Generation%20Capability%20of%20LLMs%20via%20Online%20Reinforcement%20Learning%20with%20Real-Machine%20Benchmark%20Rewards&entry.906535625=Ryo%20Mikasa%20and%20Shun-ichiro%20Hayashi%20and%20Daichi%20Mukunoki%20and%20Tetsuya%20Hoshino%20and%20Takahiro%20Katagiri&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%20code%20generation%20capabilities%2C%20yet%20the%20runtime%20performance%20of%20generated%20code%20is%20not%20guaranteed%2C%20and%20there%20have%20been%20few%20attempts%20to%20train%20LLMs%20using%20runtime%20performance%20as%20a%20reward%20in%20the%20HPC%20domain.%20We%20propose%20an%20online%20reinforcement%20learning%20approach%20that%20executes%20LLM-generated%20code%20on%20a%20supercomputer%20and%20directly%20feeds%20back%20the%20measured%20runtime%20performance%20%28GFLOPS%29%20as%20a%20reward.%20We%20further%20introduce%20a%20Staged%20Quality-Diversity%20%28SQD%29%20algorithm%20that%20progressively%20varies%20the%20permitted%20optimization%20techniques%20on%20a%20per-problem%20basis%2C%20enabling%20the%20model%20to%20learn%20code%20optimization%20from%20diverse%20perspectives.%20We%20build%20a%20distributed%20system%20connecting%20a%20GPU%20training%20cluster%20with%20a%20CPU%20benchmarking%20cluster%2C%20and%20train%20Qwen2.5%20Coder%2014B%20on%20a%20double-precision%20matrix%20multiplication%20task%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Through%20two%20experiments%2C%20we%20show%20that%20reinforcement%20learning%20combining%20runtime%20performance%20feedback%20with%20staged%20optimization%20can%20improve%20the%20HPC%20code%20generation%20capability%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2602.12049v1&entry.124074799=Read"},
{"title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration", "author": "Kfir Goldberg and Elad Richardson and Yael Vinker", "abstract": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.", "link": "http://arxiv.org/abs/2602.08615v2", "date": "2026-02-12", "relevancy": 2.339, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6217}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inspiration%20Seeds%3A%20Learning%20Non-Literal%20Visual%20Combinations%20for%20Generative%20Exploration&body=Title%3A%20Inspiration%20Seeds%3A%20Learning%20Non-Literal%20Visual%20Combinations%20for%20Generative%20Exploration%0AAuthor%3A%20Kfir%20Goldberg%20and%20Elad%20Richardson%20and%20Yael%20Vinker%0AAbstract%3A%20While%20generative%20models%20have%20become%20powerful%20tools%20for%20image%20synthesis%2C%20they%20are%20typically%20optimized%20for%20executing%20carefully%20crafted%20textual%20prompts%2C%20offering%20limited%20support%20for%20the%20open-ended%20visual%20exploration%20that%20often%20precedes%20idea%20formation.%20In%20contrast%2C%20designers%20frequently%20draw%20inspiration%20from%20loosely%20connected%20visual%20references%2C%20seeking%20emergent%20connections%20that%20spark%20new%20ideas.%20We%20propose%20Inspiration%20Seeds%2C%20a%20generative%20framework%20that%20shifts%20image%20generation%20from%20final%20execution%20to%20exploratory%20ideation.%20Given%20two%20input%20images%2C%20our%20model%20produces%20diverse%2C%20visually%20coherent%20compositions%20that%20reveal%20latent%20relationships%20between%20inputs%2C%20without%20relying%20on%20user-specified%20text%20prompts.%20Our%20approach%20is%20feed-forward%2C%20trained%20on%20synthetic%20triplets%20of%20decomposed%20visual%20aspects%20derived%20entirely%20through%20visual%20means%3A%20we%20use%20CLIP%20Sparse%20Autoencoders%20to%20extract%20editing%20directions%20in%20CLIP%20latent%20space%20and%20isolate%20concept%20pairs.%20By%20removing%20the%20reliance%20on%20language%20and%20enabling%20fast%2C%20intuitive%20recombination%2C%20our%20method%20supports%20visual%20ideation%20at%20the%20early%20and%20ambiguous%20stages%20of%20creative%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInspiration%2520Seeds%253A%2520Learning%2520Non-Literal%2520Visual%2520Combinations%2520for%2520Generative%2520Exploration%26entry.906535625%3DKfir%2520Goldberg%2520and%2520Elad%2520Richardson%2520and%2520Yael%2520Vinker%26entry.1292438233%3DWhile%2520generative%2520models%2520have%2520become%2520powerful%2520tools%2520for%2520image%2520synthesis%252C%2520they%2520are%2520typically%2520optimized%2520for%2520executing%2520carefully%2520crafted%2520textual%2520prompts%252C%2520offering%2520limited%2520support%2520for%2520the%2520open-ended%2520visual%2520exploration%2520that%2520often%2520precedes%2520idea%2520formation.%2520In%2520contrast%252C%2520designers%2520frequently%2520draw%2520inspiration%2520from%2520loosely%2520connected%2520visual%2520references%252C%2520seeking%2520emergent%2520connections%2520that%2520spark%2520new%2520ideas.%2520We%2520propose%2520Inspiration%2520Seeds%252C%2520a%2520generative%2520framework%2520that%2520shifts%2520image%2520generation%2520from%2520final%2520execution%2520to%2520exploratory%2520ideation.%2520Given%2520two%2520input%2520images%252C%2520our%2520model%2520produces%2520diverse%252C%2520visually%2520coherent%2520compositions%2520that%2520reveal%2520latent%2520relationships%2520between%2520inputs%252C%2520without%2520relying%2520on%2520user-specified%2520text%2520prompts.%2520Our%2520approach%2520is%2520feed-forward%252C%2520trained%2520on%2520synthetic%2520triplets%2520of%2520decomposed%2520visual%2520aspects%2520derived%2520entirely%2520through%2520visual%2520means%253A%2520we%2520use%2520CLIP%2520Sparse%2520Autoencoders%2520to%2520extract%2520editing%2520directions%2520in%2520CLIP%2520latent%2520space%2520and%2520isolate%2520concept%2520pairs.%2520By%2520removing%2520the%2520reliance%2520on%2520language%2520and%2520enabling%2520fast%252C%2520intuitive%2520recombination%252C%2520our%2520method%2520supports%2520visual%2520ideation%2520at%2520the%2520early%2520and%2520ambiguous%2520stages%2520of%2520creative%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inspiration%20Seeds%3A%20Learning%20Non-Literal%20Visual%20Combinations%20for%20Generative%20Exploration&entry.906535625=Kfir%20Goldberg%20and%20Elad%20Richardson%20and%20Yael%20Vinker&entry.1292438233=While%20generative%20models%20have%20become%20powerful%20tools%20for%20image%20synthesis%2C%20they%20are%20typically%20optimized%20for%20executing%20carefully%20crafted%20textual%20prompts%2C%20offering%20limited%20support%20for%20the%20open-ended%20visual%20exploration%20that%20often%20precedes%20idea%20formation.%20In%20contrast%2C%20designers%20frequently%20draw%20inspiration%20from%20loosely%20connected%20visual%20references%2C%20seeking%20emergent%20connections%20that%20spark%20new%20ideas.%20We%20propose%20Inspiration%20Seeds%2C%20a%20generative%20framework%20that%20shifts%20image%20generation%20from%20final%20execution%20to%20exploratory%20ideation.%20Given%20two%20input%20images%2C%20our%20model%20produces%20diverse%2C%20visually%20coherent%20compositions%20that%20reveal%20latent%20relationships%20between%20inputs%2C%20without%20relying%20on%20user-specified%20text%20prompts.%20Our%20approach%20is%20feed-forward%2C%20trained%20on%20synthetic%20triplets%20of%20decomposed%20visual%20aspects%20derived%20entirely%20through%20visual%20means%3A%20we%20use%20CLIP%20Sparse%20Autoencoders%20to%20extract%20editing%20directions%20in%20CLIP%20latent%20space%20and%20isolate%20concept%20pairs.%20By%20removing%20the%20reliance%20on%20language%20and%20enabling%20fast%2C%20intuitive%20recombination%2C%20our%20method%20supports%20visual%20ideation%20at%20the%20early%20and%20ambiguous%20stages%20of%20creative%20work.&entry.1838667208=http%3A//arxiv.org/abs/2602.08615v2&entry.124074799=Read"},
{"title": "Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning", "author": "Gabriel Y. Arteaga and Marius Aasan and Rwiddhi Chakraborty and Martine Hjelkrem-Tan and Thalles Silva and Michael Kampffmeyer and Ad\u00edn Ram\u00edrez Rivera", "abstract": "Prototypical self-supervised learning methods consistently suffer from partial prototype collapse, where multiple prototypes converge to nearly identical representations. This undermines their central purpose -- providing diverse and informative targets to guide encoders toward rich representations -- and has led practitioners to over-parameterize prototype sets or add ad-hoc regularizers, which mitigate symptoms rather than address the root cause. We empirically trace the collapse to the joint optimization of encoders and prototypes, which encourages a type of shortcut learning: early in training prototypes drift toward redundant representations that minimize loss without necessarily enhancing representation diversity. To break the joint optimization, we introduce a fully decoupled training strategy that learns prototypes and encoders under separate objectives. Concretely, we model prototypes as a Gaussian mixture updated with an online EM-style procedure, independent of the encoder's loss. This simple yet principled decoupling eliminates prototype collapse without explicit regularization and yields consistently diverse prototypes and stronger downstream performance.", "link": "http://arxiv.org/abs/2510.20108v2", "date": "2026-02-12", "relevancy": 2.3352, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4972}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Prototypes%20Collapse%3A%20Diagnosing%20and%20Preventing%20Partial%20Collapse%20in%20Prototypical%20Self-Supervised%20Learning&body=Title%3A%20Why%20Prototypes%20Collapse%3A%20Diagnosing%20and%20Preventing%20Partial%20Collapse%20in%20Prototypical%20Self-Supervised%20Learning%0AAuthor%3A%20Gabriel%20Y.%20Arteaga%20and%20Marius%20Aasan%20and%20Rwiddhi%20Chakraborty%20and%20Martine%20Hjelkrem-Tan%20and%20Thalles%20Silva%20and%20Michael%20Kampffmeyer%20and%20Ad%C3%ADn%20Ram%C3%ADrez%20Rivera%0AAbstract%3A%20Prototypical%20self-supervised%20learning%20methods%20consistently%20suffer%20from%20partial%20prototype%20collapse%2C%20where%20multiple%20prototypes%20converge%20to%20nearly%20identical%20representations.%20This%20undermines%20their%20central%20purpose%20--%20providing%20diverse%20and%20informative%20targets%20to%20guide%20encoders%20toward%20rich%20representations%20--%20and%20has%20led%20practitioners%20to%20over-parameterize%20prototype%20sets%20or%20add%20ad-hoc%20regularizers%2C%20which%20mitigate%20symptoms%20rather%20than%20address%20the%20root%20cause.%20We%20empirically%20trace%20the%20collapse%20to%20the%20joint%20optimization%20of%20encoders%20and%20prototypes%2C%20which%20encourages%20a%20type%20of%20shortcut%20learning%3A%20early%20in%20training%20prototypes%20drift%20toward%20redundant%20representations%20that%20minimize%20loss%20without%20necessarily%20enhancing%20representation%20diversity.%20To%20break%20the%20joint%20optimization%2C%20we%20introduce%20a%20fully%20decoupled%20training%20strategy%20that%20learns%20prototypes%20and%20encoders%20under%20separate%20objectives.%20Concretely%2C%20we%20model%20prototypes%20as%20a%20Gaussian%20mixture%20updated%20with%20an%20online%20EM-style%20procedure%2C%20independent%20of%20the%20encoder%27s%20loss.%20This%20simple%20yet%20principled%20decoupling%20eliminates%20prototype%20collapse%20without%20explicit%20regularization%20and%20yields%20consistently%20diverse%20prototypes%20and%20stronger%20downstream%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Prototypes%2520Collapse%253A%2520Diagnosing%2520and%2520Preventing%2520Partial%2520Collapse%2520in%2520Prototypical%2520Self-Supervised%2520Learning%26entry.906535625%3DGabriel%2520Y.%2520Arteaga%2520and%2520Marius%2520Aasan%2520and%2520Rwiddhi%2520Chakraborty%2520and%2520Martine%2520Hjelkrem-Tan%2520and%2520Thalles%2520Silva%2520and%2520Michael%2520Kampffmeyer%2520and%2520Ad%25C3%25ADn%2520Ram%25C3%25ADrez%2520Rivera%26entry.1292438233%3DPrototypical%2520self-supervised%2520learning%2520methods%2520consistently%2520suffer%2520from%2520partial%2520prototype%2520collapse%252C%2520where%2520multiple%2520prototypes%2520converge%2520to%2520nearly%2520identical%2520representations.%2520This%2520undermines%2520their%2520central%2520purpose%2520--%2520providing%2520diverse%2520and%2520informative%2520targets%2520to%2520guide%2520encoders%2520toward%2520rich%2520representations%2520--%2520and%2520has%2520led%2520practitioners%2520to%2520over-parameterize%2520prototype%2520sets%2520or%2520add%2520ad-hoc%2520regularizers%252C%2520which%2520mitigate%2520symptoms%2520rather%2520than%2520address%2520the%2520root%2520cause.%2520We%2520empirically%2520trace%2520the%2520collapse%2520to%2520the%2520joint%2520optimization%2520of%2520encoders%2520and%2520prototypes%252C%2520which%2520encourages%2520a%2520type%2520of%2520shortcut%2520learning%253A%2520early%2520in%2520training%2520prototypes%2520drift%2520toward%2520redundant%2520representations%2520that%2520minimize%2520loss%2520without%2520necessarily%2520enhancing%2520representation%2520diversity.%2520To%2520break%2520the%2520joint%2520optimization%252C%2520we%2520introduce%2520a%2520fully%2520decoupled%2520training%2520strategy%2520that%2520learns%2520prototypes%2520and%2520encoders%2520under%2520separate%2520objectives.%2520Concretely%252C%2520we%2520model%2520prototypes%2520as%2520a%2520Gaussian%2520mixture%2520updated%2520with%2520an%2520online%2520EM-style%2520procedure%252C%2520independent%2520of%2520the%2520encoder%2527s%2520loss.%2520This%2520simple%2520yet%2520principled%2520decoupling%2520eliminates%2520prototype%2520collapse%2520without%2520explicit%2520regularization%2520and%2520yields%2520consistently%2520diverse%2520prototypes%2520and%2520stronger%2520downstream%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Prototypes%20Collapse%3A%20Diagnosing%20and%20Preventing%20Partial%20Collapse%20in%20Prototypical%20Self-Supervised%20Learning&entry.906535625=Gabriel%20Y.%20Arteaga%20and%20Marius%20Aasan%20and%20Rwiddhi%20Chakraborty%20and%20Martine%20Hjelkrem-Tan%20and%20Thalles%20Silva%20and%20Michael%20Kampffmeyer%20and%20Ad%C3%ADn%20Ram%C3%ADrez%20Rivera&entry.1292438233=Prototypical%20self-supervised%20learning%20methods%20consistently%20suffer%20from%20partial%20prototype%20collapse%2C%20where%20multiple%20prototypes%20converge%20to%20nearly%20identical%20representations.%20This%20undermines%20their%20central%20purpose%20--%20providing%20diverse%20and%20informative%20targets%20to%20guide%20encoders%20toward%20rich%20representations%20--%20and%20has%20led%20practitioners%20to%20over-parameterize%20prototype%20sets%20or%20add%20ad-hoc%20regularizers%2C%20which%20mitigate%20symptoms%20rather%20than%20address%20the%20root%20cause.%20We%20empirically%20trace%20the%20collapse%20to%20the%20joint%20optimization%20of%20encoders%20and%20prototypes%2C%20which%20encourages%20a%20type%20of%20shortcut%20learning%3A%20early%20in%20training%20prototypes%20drift%20toward%20redundant%20representations%20that%20minimize%20loss%20without%20necessarily%20enhancing%20representation%20diversity.%20To%20break%20the%20joint%20optimization%2C%20we%20introduce%20a%20fully%20decoupled%20training%20strategy%20that%20learns%20prototypes%20and%20encoders%20under%20separate%20objectives.%20Concretely%2C%20we%20model%20prototypes%20as%20a%20Gaussian%20mixture%20updated%20with%20an%20online%20EM-style%20procedure%2C%20independent%20of%20the%20encoder%27s%20loss.%20This%20simple%20yet%20principled%20decoupling%20eliminates%20prototype%20collapse%20without%20explicit%20regularization%20and%20yields%20consistently%20diverse%20prototypes%20and%20stronger%20downstream%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2510.20108v2&entry.124074799=Read"},
{"title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair", "author": "Zhiyong Chen and Jialun Cao and Chang Xu and Shing-Chi Cheung", "abstract": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.", "link": "http://arxiv.org/abs/2602.12058v1", "date": "2026-02-12", "relevancy": 2.3249, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ModelWisdom%3A%20An%20Integrated%20Toolkit%20for%20TLA%2B%20Model%20Visualization%2C%20Digest%20and%20Repair&body=Title%3A%20ModelWisdom%3A%20An%20Integrated%20Toolkit%20for%20TLA%2B%20Model%20Visualization%2C%20Digest%20and%20Repair%0AAuthor%3A%20Zhiyong%20Chen%20and%20Jialun%20Cao%20and%20Chang%20Xu%20and%20Shing-Chi%20Cheung%0AAbstract%3A%20Model%20checking%20in%20TLA%2B%20provides%20strong%20correctness%20guarantees%2C%20yet%20practitioners%20continue%20to%20face%20significant%20challenges%20in%20interpreting%20counterexamples%2C%20understanding%20large%20state-transition%20graphs%2C%20and%20repairing%20faulty%20models.%20These%20difficulties%20stem%20from%20the%20limited%20explainability%20of%20raw%20model-checker%20output%20and%20the%20substantial%20manual%20effort%20required%20to%20trace%20violations%20back%20to%20source%20specifications.%20Although%20the%20TLA%2B%20Toolbox%20includes%20a%20state%20diagram%20viewer%2C%20it%20offers%20only%20a%20static%2C%20fully%20expanded%20graph%20without%20folding%2C%20color%20highlighting%2C%20or%20semantic%20explanations%2C%20which%20limits%20its%20scalability%20and%20interpretability.%20We%20present%20ModelWisdom%2C%20an%20interactive%20environment%20that%20uses%20visualization%20and%20large%20language%20models%20to%20make%20TLA%2B%20model%20checking%20more%20interpretable%20and%20actionable.%20ModelWisdom%20offers%3A%20%28i%29%20Model%20Visualization%2C%20with%20colorized%20violation%20highlighting%2C%20click-through%20links%20from%20transitions%20to%20TLA%2B%20code%2C%20and%20mapping%20between%20violating%20states%20and%20broken%20properties%3B%20%28ii%29%20Graph%20Optimization%2C%20including%20tree-based%20structuring%20and%20node/edge%20folding%20to%20manage%20large%20models%3B%20%28iii%29%20Model%20Digest%2C%20which%20summarizes%20and%20explains%20subgraphs%20via%20large%20language%20models%20%28LLMs%29%20and%20performs%20preprocessing%20and%20partial%20explanations%3B%20and%20%28iv%29%20Model%20Repair%2C%20which%20extracts%20error%20information%20and%20supports%20iterative%20debugging.%20Together%2C%20these%20capabilities%20turn%20raw%20model-checker%20output%20into%20an%20interactive%2C%20explainable%20workflow%2C%20improving%20understanding%20and%20reducing%20debugging%20effort%20for%20nontrivial%20TLA%2B%20specifications.%20The%20website%20to%20ModelWisdom%20is%20available%3A%20https%3A//model-wisdom.pages.dev.%20A%20demonstrative%20video%20can%20be%20found%20at%20https%3A//www.youtube.com/watch%3Fv%3DplyZo30VShA.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModelWisdom%253A%2520An%2520Integrated%2520Toolkit%2520for%2520TLA%252B%2520Model%2520Visualization%252C%2520Digest%2520and%2520Repair%26entry.906535625%3DZhiyong%2520Chen%2520and%2520Jialun%2520Cao%2520and%2520Chang%2520Xu%2520and%2520Shing-Chi%2520Cheung%26entry.1292438233%3DModel%2520checking%2520in%2520TLA%252B%2520provides%2520strong%2520correctness%2520guarantees%252C%2520yet%2520practitioners%2520continue%2520to%2520face%2520significant%2520challenges%2520in%2520interpreting%2520counterexamples%252C%2520understanding%2520large%2520state-transition%2520graphs%252C%2520and%2520repairing%2520faulty%2520models.%2520These%2520difficulties%2520stem%2520from%2520the%2520limited%2520explainability%2520of%2520raw%2520model-checker%2520output%2520and%2520the%2520substantial%2520manual%2520effort%2520required%2520to%2520trace%2520violations%2520back%2520to%2520source%2520specifications.%2520Although%2520the%2520TLA%252B%2520Toolbox%2520includes%2520a%2520state%2520diagram%2520viewer%252C%2520it%2520offers%2520only%2520a%2520static%252C%2520fully%2520expanded%2520graph%2520without%2520folding%252C%2520color%2520highlighting%252C%2520or%2520semantic%2520explanations%252C%2520which%2520limits%2520its%2520scalability%2520and%2520interpretability.%2520We%2520present%2520ModelWisdom%252C%2520an%2520interactive%2520environment%2520that%2520uses%2520visualization%2520and%2520large%2520language%2520models%2520to%2520make%2520TLA%252B%2520model%2520checking%2520more%2520interpretable%2520and%2520actionable.%2520ModelWisdom%2520offers%253A%2520%2528i%2529%2520Model%2520Visualization%252C%2520with%2520colorized%2520violation%2520highlighting%252C%2520click-through%2520links%2520from%2520transitions%2520to%2520TLA%252B%2520code%252C%2520and%2520mapping%2520between%2520violating%2520states%2520and%2520broken%2520properties%253B%2520%2528ii%2529%2520Graph%2520Optimization%252C%2520including%2520tree-based%2520structuring%2520and%2520node/edge%2520folding%2520to%2520manage%2520large%2520models%253B%2520%2528iii%2529%2520Model%2520Digest%252C%2520which%2520summarizes%2520and%2520explains%2520subgraphs%2520via%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520performs%2520preprocessing%2520and%2520partial%2520explanations%253B%2520and%2520%2528iv%2529%2520Model%2520Repair%252C%2520which%2520extracts%2520error%2520information%2520and%2520supports%2520iterative%2520debugging.%2520Together%252C%2520these%2520capabilities%2520turn%2520raw%2520model-checker%2520output%2520into%2520an%2520interactive%252C%2520explainable%2520workflow%252C%2520improving%2520understanding%2520and%2520reducing%2520debugging%2520effort%2520for%2520nontrivial%2520TLA%252B%2520specifications.%2520The%2520website%2520to%2520ModelWisdom%2520is%2520available%253A%2520https%253A//model-wisdom.pages.dev.%2520A%2520demonstrative%2520video%2520can%2520be%2520found%2520at%2520https%253A//www.youtube.com/watch%253Fv%253DplyZo30VShA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ModelWisdom%3A%20An%20Integrated%20Toolkit%20for%20TLA%2B%20Model%20Visualization%2C%20Digest%20and%20Repair&entry.906535625=Zhiyong%20Chen%20and%20Jialun%20Cao%20and%20Chang%20Xu%20and%20Shing-Chi%20Cheung&entry.1292438233=Model%20checking%20in%20TLA%2B%20provides%20strong%20correctness%20guarantees%2C%20yet%20practitioners%20continue%20to%20face%20significant%20challenges%20in%20interpreting%20counterexamples%2C%20understanding%20large%20state-transition%20graphs%2C%20and%20repairing%20faulty%20models.%20These%20difficulties%20stem%20from%20the%20limited%20explainability%20of%20raw%20model-checker%20output%20and%20the%20substantial%20manual%20effort%20required%20to%20trace%20violations%20back%20to%20source%20specifications.%20Although%20the%20TLA%2B%20Toolbox%20includes%20a%20state%20diagram%20viewer%2C%20it%20offers%20only%20a%20static%2C%20fully%20expanded%20graph%20without%20folding%2C%20color%20highlighting%2C%20or%20semantic%20explanations%2C%20which%20limits%20its%20scalability%20and%20interpretability.%20We%20present%20ModelWisdom%2C%20an%20interactive%20environment%20that%20uses%20visualization%20and%20large%20language%20models%20to%20make%20TLA%2B%20model%20checking%20more%20interpretable%20and%20actionable.%20ModelWisdom%20offers%3A%20%28i%29%20Model%20Visualization%2C%20with%20colorized%20violation%20highlighting%2C%20click-through%20links%20from%20transitions%20to%20TLA%2B%20code%2C%20and%20mapping%20between%20violating%20states%20and%20broken%20properties%3B%20%28ii%29%20Graph%20Optimization%2C%20including%20tree-based%20structuring%20and%20node/edge%20folding%20to%20manage%20large%20models%3B%20%28iii%29%20Model%20Digest%2C%20which%20summarizes%20and%20explains%20subgraphs%20via%20large%20language%20models%20%28LLMs%29%20and%20performs%20preprocessing%20and%20partial%20explanations%3B%20and%20%28iv%29%20Model%20Repair%2C%20which%20extracts%20error%20information%20and%20supports%20iterative%20debugging.%20Together%2C%20these%20capabilities%20turn%20raw%20model-checker%20output%20into%20an%20interactive%2C%20explainable%20workflow%2C%20improving%20understanding%20and%20reducing%20debugging%20effort%20for%20nontrivial%20TLA%2B%20specifications.%20The%20website%20to%20ModelWisdom%20is%20available%3A%20https%3A//model-wisdom.pages.dev.%20A%20demonstrative%20video%20can%20be%20found%20at%20https%3A//www.youtube.com/watch%3Fv%3DplyZo30VShA.&entry.1838667208=http%3A//arxiv.org/abs/2602.12058v1&entry.124074799=Read"},
{"title": "In-Context Function Learning in Large Language Models", "author": "Elif Akata and Konstantinos Voudouris and Vincent Fortuin and Eric Schulz", "abstract": "Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.", "link": "http://arxiv.org/abs/2602.11863v1", "date": "2026-02-12", "relevancy": 2.3249, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Function%20Learning%20in%20Large%20Language%20Models&body=Title%3A%20In-Context%20Function%20Learning%20in%20Large%20Language%20Models%0AAuthor%3A%20Elif%20Akata%20and%20Konstantinos%20Voudouris%20and%20Vincent%20Fortuin%20and%20Eric%20Schulz%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20can%20learn%20from%20a%20few%20demonstrations%20provided%20at%20inference%20time.%20We%20study%20this%20in-context%20learning%20phenomenon%20through%20the%20lens%20of%20Gaussian%20Processes%20%28GPs%29.%20We%20build%20controlled%20experiments%20where%20models%20observe%20sequences%20of%20multivariate%20scalar-valued%20function%20samples%20drawn%20from%20known%20GP%20priors.%20We%20evaluate%20prediction%20error%20in%20relation%20to%20the%20number%20of%20demonstrations%20and%20compare%20against%20two%20principled%20references%3A%20%28i%29%20an%20empirical%20GP-regression%20learner%20that%20gives%20a%20lower%20bound%20on%20achievable%20error%2C%20and%20%28ii%29%20the%20expected%20error%20of%20a%201-nearest-neighbor%20%281-NN%29%20rule%2C%20which%20gives%20a%20data-driven%20upper%20bound.%20Across%20model%20sizes%2C%20we%20find%20that%20LLM%20learning%20curves%20are%20strongly%20influenced%20by%20the%20function-generating%20kernels%20and%20approach%20the%20GP%20lower%20bound%20as%20the%20number%20of%20demonstrations%20increases.%20We%20then%20study%20the%20inductive%20biases%20of%20these%20models%20using%20a%20likelihood-based%20analysis.%20We%20find%20that%20LLM%20predictions%20are%20most%20likely%20under%20less%20smooth%20GP%20kernels.%20Finally%2C%20we%20explore%20whether%20post-training%20can%20shift%20these%20inductive%20biases%20and%20improve%20sample-efficiency%20on%20functions%20sampled%20from%20GPs%20with%20smoother%20kernels.%20We%20find%20that%20both%20reinforcement%20learning%20and%20supervised%20fine-tuning%20can%20effectively%20shift%20inductive%20biases%20in%20the%20direction%20of%20the%20training%20data.%20Together%2C%20our%20framework%20quantifies%20the%20extent%20to%20which%20LLMs%20behave%20like%20GP%20learners%20and%20provides%20tools%20for%20steering%20their%20inductive%20biases%20for%20continuous%20function%20learning%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Function%2520Learning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DElif%2520Akata%2520and%2520Konstantinos%2520Voudouris%2520and%2520Vincent%2520Fortuin%2520and%2520Eric%2520Schulz%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520can%2520learn%2520from%2520a%2520few%2520demonstrations%2520provided%2520at%2520inference%2520time.%2520We%2520study%2520this%2520in-context%2520learning%2520phenomenon%2520through%2520the%2520lens%2520of%2520Gaussian%2520Processes%2520%2528GPs%2529.%2520We%2520build%2520controlled%2520experiments%2520where%2520models%2520observe%2520sequences%2520of%2520multivariate%2520scalar-valued%2520function%2520samples%2520drawn%2520from%2520known%2520GP%2520priors.%2520We%2520evaluate%2520prediction%2520error%2520in%2520relation%2520to%2520the%2520number%2520of%2520demonstrations%2520and%2520compare%2520against%2520two%2520principled%2520references%253A%2520%2528i%2529%2520an%2520empirical%2520GP-regression%2520learner%2520that%2520gives%2520a%2520lower%2520bound%2520on%2520achievable%2520error%252C%2520and%2520%2528ii%2529%2520the%2520expected%2520error%2520of%2520a%25201-nearest-neighbor%2520%25281-NN%2529%2520rule%252C%2520which%2520gives%2520a%2520data-driven%2520upper%2520bound.%2520Across%2520model%2520sizes%252C%2520we%2520find%2520that%2520LLM%2520learning%2520curves%2520are%2520strongly%2520influenced%2520by%2520the%2520function-generating%2520kernels%2520and%2520approach%2520the%2520GP%2520lower%2520bound%2520as%2520the%2520number%2520of%2520demonstrations%2520increases.%2520We%2520then%2520study%2520the%2520inductive%2520biases%2520of%2520these%2520models%2520using%2520a%2520likelihood-based%2520analysis.%2520We%2520find%2520that%2520LLM%2520predictions%2520are%2520most%2520likely%2520under%2520less%2520smooth%2520GP%2520kernels.%2520Finally%252C%2520we%2520explore%2520whether%2520post-training%2520can%2520shift%2520these%2520inductive%2520biases%2520and%2520improve%2520sample-efficiency%2520on%2520functions%2520sampled%2520from%2520GPs%2520with%2520smoother%2520kernels.%2520We%2520find%2520that%2520both%2520reinforcement%2520learning%2520and%2520supervised%2520fine-tuning%2520can%2520effectively%2520shift%2520inductive%2520biases%2520in%2520the%2520direction%2520of%2520the%2520training%2520data.%2520Together%252C%2520our%2520framework%2520quantifies%2520the%2520extent%2520to%2520which%2520LLMs%2520behave%2520like%2520GP%2520learners%2520and%2520provides%2520tools%2520for%2520steering%2520their%2520inductive%2520biases%2520for%2520continuous%2520function%2520learning%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Function%20Learning%20in%20Large%20Language%20Models&entry.906535625=Elif%20Akata%20and%20Konstantinos%20Voudouris%20and%20Vincent%20Fortuin%20and%20Eric%20Schulz&entry.1292438233=Large%20language%20models%20%28LLMs%29%20can%20learn%20from%20a%20few%20demonstrations%20provided%20at%20inference%20time.%20We%20study%20this%20in-context%20learning%20phenomenon%20through%20the%20lens%20of%20Gaussian%20Processes%20%28GPs%29.%20We%20build%20controlled%20experiments%20where%20models%20observe%20sequences%20of%20multivariate%20scalar-valued%20function%20samples%20drawn%20from%20known%20GP%20priors.%20We%20evaluate%20prediction%20error%20in%20relation%20to%20the%20number%20of%20demonstrations%20and%20compare%20against%20two%20principled%20references%3A%20%28i%29%20an%20empirical%20GP-regression%20learner%20that%20gives%20a%20lower%20bound%20on%20achievable%20error%2C%20and%20%28ii%29%20the%20expected%20error%20of%20a%201-nearest-neighbor%20%281-NN%29%20rule%2C%20which%20gives%20a%20data-driven%20upper%20bound.%20Across%20model%20sizes%2C%20we%20find%20that%20LLM%20learning%20curves%20are%20strongly%20influenced%20by%20the%20function-generating%20kernels%20and%20approach%20the%20GP%20lower%20bound%20as%20the%20number%20of%20demonstrations%20increases.%20We%20then%20study%20the%20inductive%20biases%20of%20these%20models%20using%20a%20likelihood-based%20analysis.%20We%20find%20that%20LLM%20predictions%20are%20most%20likely%20under%20less%20smooth%20GP%20kernels.%20Finally%2C%20we%20explore%20whether%20post-training%20can%20shift%20these%20inductive%20biases%20and%20improve%20sample-efficiency%20on%20functions%20sampled%20from%20GPs%20with%20smoother%20kernels.%20We%20find%20that%20both%20reinforcement%20learning%20and%20supervised%20fine-tuning%20can%20effectively%20shift%20inductive%20biases%20in%20the%20direction%20of%20the%20training%20data.%20Together%2C%20our%20framework%20quantifies%20the%20extent%20to%20which%20LLMs%20behave%20like%20GP%20learners%20and%20provides%20tools%20for%20steering%20their%20inductive%20biases%20for%20continuous%20function%20learning%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.11863v1&entry.124074799=Read"},
{"title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation", "author": "Ruihang Xu and Dewei Zhou and Fan Ma and Yi Yang", "abstract": "Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. To address the absence of a large-scale, high-quality dataset for this task, we introduce IMIG-100K, the first dataset to provide detailed layout and identity annotations specifically designed for Multi-Instance Generation. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods especially in layout control and identity fidelity.", "link": "http://arxiv.org/abs/2510.11000v3", "date": "2026-02-12", "relevancy": 2.323, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6037}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5815}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextGen%3A%20Contextual%20Layout%20Anchoring%20for%20Identity-Consistent%20Multi-Instance%20Generation&body=Title%3A%20ContextGen%3A%20Contextual%20Layout%20Anchoring%20for%20Identity-Consistent%20Multi-Instance%20Generation%0AAuthor%3A%20Ruihang%20Xu%20and%20Dewei%20Zhou%20and%20Fan%20Ma%20and%20Yi%20Yang%0AAbstract%3A%20Multi-instance%20image%20generation%20%28MIG%29%20remains%20a%20significant%20challenge%20for%20modern%20diffusion%20models%20due%20to%20key%20limitations%20in%20achieving%20precise%20control%20over%20object%20layout%20and%20preserving%20the%20identity%20of%20multiple%20distinct%20subjects.%20To%20address%20these%20limitations%2C%20we%20introduce%20ContextGen%2C%20a%20novel%20Diffusion%20Transformer%20framework%20for%20multi-instance%20generation%20that%20is%20guided%20by%20both%20layout%20and%20reference%20images.%20Our%20approach%20integrates%20two%20key%20technical%20contributions%3A%20a%20Contextual%20Layout%20Anchoring%20%28CLA%29%20mechanism%20that%20incorporates%20the%20composite%20layout%20image%20into%20the%20generation%20context%20to%20robustly%20anchor%20the%20objects%20in%20their%20desired%20positions%2C%20and%20Identity%20Consistency%20Attention%20%28ICA%29%2C%20an%20innovative%20attention%20mechanism%20that%20leverages%20contextual%20reference%20images%20to%20ensure%20the%20identity%20consistency%20of%20multiple%20instances.%20To%20address%20the%20absence%20of%20a%20large-scale%2C%20high-quality%20dataset%20for%20this%20task%2C%20we%20introduce%20IMIG-100K%2C%20the%20first%20dataset%20to%20provide%20detailed%20layout%20and%20identity%20annotations%20specifically%20designed%20for%20Multi-Instance%20Generation.%20Extensive%20experiments%20demonstrate%20that%20ContextGen%20sets%20a%20new%20state-of-the-art%2C%20outperforming%20existing%20methods%20especially%20in%20layout%20control%20and%20identity%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11000v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextGen%253A%2520Contextual%2520Layout%2520Anchoring%2520for%2520Identity-Consistent%2520Multi-Instance%2520Generation%26entry.906535625%3DRuihang%2520Xu%2520and%2520Dewei%2520Zhou%2520and%2520Fan%2520Ma%2520and%2520Yi%2520Yang%26entry.1292438233%3DMulti-instance%2520image%2520generation%2520%2528MIG%2529%2520remains%2520a%2520significant%2520challenge%2520for%2520modern%2520diffusion%2520models%2520due%2520to%2520key%2520limitations%2520in%2520achieving%2520precise%2520control%2520over%2520object%2520layout%2520and%2520preserving%2520the%2520identity%2520of%2520multiple%2520distinct%2520subjects.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520ContextGen%252C%2520a%2520novel%2520Diffusion%2520Transformer%2520framework%2520for%2520multi-instance%2520generation%2520that%2520is%2520guided%2520by%2520both%2520layout%2520and%2520reference%2520images.%2520Our%2520approach%2520integrates%2520two%2520key%2520technical%2520contributions%253A%2520a%2520Contextual%2520Layout%2520Anchoring%2520%2528CLA%2529%2520mechanism%2520that%2520incorporates%2520the%2520composite%2520layout%2520image%2520into%2520the%2520generation%2520context%2520to%2520robustly%2520anchor%2520the%2520objects%2520in%2520their%2520desired%2520positions%252C%2520and%2520Identity%2520Consistency%2520Attention%2520%2528ICA%2529%252C%2520an%2520innovative%2520attention%2520mechanism%2520that%2520leverages%2520contextual%2520reference%2520images%2520to%2520ensure%2520the%2520identity%2520consistency%2520of%2520multiple%2520instances.%2520To%2520address%2520the%2520absence%2520of%2520a%2520large-scale%252C%2520high-quality%2520dataset%2520for%2520this%2520task%252C%2520we%2520introduce%2520IMIG-100K%252C%2520the%2520first%2520dataset%2520to%2520provide%2520detailed%2520layout%2520and%2520identity%2520annotations%2520specifically%2520designed%2520for%2520Multi-Instance%2520Generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ContextGen%2520sets%2520a%2520new%2520state-of-the-art%252C%2520outperforming%2520existing%2520methods%2520especially%2520in%2520layout%2520control%2520and%2520identity%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11000v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextGen%3A%20Contextual%20Layout%20Anchoring%20for%20Identity-Consistent%20Multi-Instance%20Generation&entry.906535625=Ruihang%20Xu%20and%20Dewei%20Zhou%20and%20Fan%20Ma%20and%20Yi%20Yang&entry.1292438233=Multi-instance%20image%20generation%20%28MIG%29%20remains%20a%20significant%20challenge%20for%20modern%20diffusion%20models%20due%20to%20key%20limitations%20in%20achieving%20precise%20control%20over%20object%20layout%20and%20preserving%20the%20identity%20of%20multiple%20distinct%20subjects.%20To%20address%20these%20limitations%2C%20we%20introduce%20ContextGen%2C%20a%20novel%20Diffusion%20Transformer%20framework%20for%20multi-instance%20generation%20that%20is%20guided%20by%20both%20layout%20and%20reference%20images.%20Our%20approach%20integrates%20two%20key%20technical%20contributions%3A%20a%20Contextual%20Layout%20Anchoring%20%28CLA%29%20mechanism%20that%20incorporates%20the%20composite%20layout%20image%20into%20the%20generation%20context%20to%20robustly%20anchor%20the%20objects%20in%20their%20desired%20positions%2C%20and%20Identity%20Consistency%20Attention%20%28ICA%29%2C%20an%20innovative%20attention%20mechanism%20that%20leverages%20contextual%20reference%20images%20to%20ensure%20the%20identity%20consistency%20of%20multiple%20instances.%20To%20address%20the%20absence%20of%20a%20large-scale%2C%20high-quality%20dataset%20for%20this%20task%2C%20we%20introduce%20IMIG-100K%2C%20the%20first%20dataset%20to%20provide%20detailed%20layout%20and%20identity%20annotations%20specifically%20designed%20for%20Multi-Instance%20Generation.%20Extensive%20experiments%20demonstrate%20that%20ContextGen%20sets%20a%20new%20state-of-the-art%2C%20outperforming%20existing%20methods%20especially%20in%20layout%20control%20and%20identity%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2510.11000v3&entry.124074799=Read"},
{"title": "General Humanoid Whole-Body Control via Pretraining and Fast Adaptation", "author": "Zepeng Wang and Jiangxing Wang and Shiqing Yao and Yu Zhang and Ziluo Ding and Ming Yang and Yuxuan Wang and Haobin Jiang and Chao Ma and Xiaochuan Shi and Zongqing Lu", "abstract": "Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.", "link": "http://arxiv.org/abs/2602.11929v1", "date": "2026-02-12", "relevancy": 2.3218, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5989}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5771}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Humanoid%20Whole-Body%20Control%20via%20Pretraining%20and%20Fast%20Adaptation&body=Title%3A%20General%20Humanoid%20Whole-Body%20Control%20via%20Pretraining%20and%20Fast%20Adaptation%0AAuthor%3A%20Zepeng%20Wang%20and%20Jiangxing%20Wang%20and%20Shiqing%20Yao%20and%20Yu%20Zhang%20and%20Ziluo%20Ding%20and%20Ming%20Yang%20and%20Yuxuan%20Wang%20and%20Haobin%20Jiang%20and%20Chao%20Ma%20and%20Xiaochuan%20Shi%20and%20Zongqing%20Lu%0AAbstract%3A%20Learning%20a%20general%20whole-body%20controller%20for%20humanoid%20robots%20remains%20challenging%20due%20to%20the%20diversity%20of%20motion%20distributions%2C%20the%20difficulty%20of%20fast%20adaptation%2C%20and%20the%20need%20for%20robust%20balance%20in%20high-dynamic%20scenarios.%20Existing%20approaches%20often%20require%20task-specific%20training%20or%20suffer%20from%20performance%20degradation%20when%20adapting%20to%20new%20motions.%20In%20this%20paper%2C%20we%20present%20FAST%2C%20a%20general%20humanoid%20whole-body%20control%20framework%20that%20enables%20Fast%20Adaptation%20and%20Stable%20Motion%20Tracking.%20FAST%20introduces%20Parseval-Guided%20Residual%20Policy%20Adaptation%2C%20which%20learns%20a%20lightweight%20delta%20action%20policy%20under%20orthogonality%20and%20KL%20constraints%2C%20enabling%20efficient%20adaptation%20to%20out-of-distribution%20motions%20while%20mitigating%20catastrophic%20forgetting.%20To%20further%20improve%20physical%20robustness%2C%20we%20propose%20Center-of-Mass-Aware%20Control%2C%20which%20incorporates%20CoM-related%20observations%20and%20objectives%20to%20enhance%20balance%20when%20tracking%20challenging%20reference%20motions.%20Extensive%20experiments%20in%20simulation%20and%20real-world%20deployment%20demonstrate%20that%20FAST%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20robustness%2C%20adaptation%20efficiency%2C%20and%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Humanoid%2520Whole-Body%2520Control%2520via%2520Pretraining%2520and%2520Fast%2520Adaptation%26entry.906535625%3DZepeng%2520Wang%2520and%2520Jiangxing%2520Wang%2520and%2520Shiqing%2520Yao%2520and%2520Yu%2520Zhang%2520and%2520Ziluo%2520Ding%2520and%2520Ming%2520Yang%2520and%2520Yuxuan%2520Wang%2520and%2520Haobin%2520Jiang%2520and%2520Chao%2520Ma%2520and%2520Xiaochuan%2520Shi%2520and%2520Zongqing%2520Lu%26entry.1292438233%3DLearning%2520a%2520general%2520whole-body%2520controller%2520for%2520humanoid%2520robots%2520remains%2520challenging%2520due%2520to%2520the%2520diversity%2520of%2520motion%2520distributions%252C%2520the%2520difficulty%2520of%2520fast%2520adaptation%252C%2520and%2520the%2520need%2520for%2520robust%2520balance%2520in%2520high-dynamic%2520scenarios.%2520Existing%2520approaches%2520often%2520require%2520task-specific%2520training%2520or%2520suffer%2520from%2520performance%2520degradation%2520when%2520adapting%2520to%2520new%2520motions.%2520In%2520this%2520paper%252C%2520we%2520present%2520FAST%252C%2520a%2520general%2520humanoid%2520whole-body%2520control%2520framework%2520that%2520enables%2520Fast%2520Adaptation%2520and%2520Stable%2520Motion%2520Tracking.%2520FAST%2520introduces%2520Parseval-Guided%2520Residual%2520Policy%2520Adaptation%252C%2520which%2520learns%2520a%2520lightweight%2520delta%2520action%2520policy%2520under%2520orthogonality%2520and%2520KL%2520constraints%252C%2520enabling%2520efficient%2520adaptation%2520to%2520out-of-distribution%2520motions%2520while%2520mitigating%2520catastrophic%2520forgetting.%2520To%2520further%2520improve%2520physical%2520robustness%252C%2520we%2520propose%2520Center-of-Mass-Aware%2520Control%252C%2520which%2520incorporates%2520CoM-related%2520observations%2520and%2520objectives%2520to%2520enhance%2520balance%2520when%2520tracking%2520challenging%2520reference%2520motions.%2520Extensive%2520experiments%2520in%2520simulation%2520and%2520real-world%2520deployment%2520demonstrate%2520that%2520FAST%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520robustness%252C%2520adaptation%2520efficiency%252C%2520and%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Humanoid%20Whole-Body%20Control%20via%20Pretraining%20and%20Fast%20Adaptation&entry.906535625=Zepeng%20Wang%20and%20Jiangxing%20Wang%20and%20Shiqing%20Yao%20and%20Yu%20Zhang%20and%20Ziluo%20Ding%20and%20Ming%20Yang%20and%20Yuxuan%20Wang%20and%20Haobin%20Jiang%20and%20Chao%20Ma%20and%20Xiaochuan%20Shi%20and%20Zongqing%20Lu&entry.1292438233=Learning%20a%20general%20whole-body%20controller%20for%20humanoid%20robots%20remains%20challenging%20due%20to%20the%20diversity%20of%20motion%20distributions%2C%20the%20difficulty%20of%20fast%20adaptation%2C%20and%20the%20need%20for%20robust%20balance%20in%20high-dynamic%20scenarios.%20Existing%20approaches%20often%20require%20task-specific%20training%20or%20suffer%20from%20performance%20degradation%20when%20adapting%20to%20new%20motions.%20In%20this%20paper%2C%20we%20present%20FAST%2C%20a%20general%20humanoid%20whole-body%20control%20framework%20that%20enables%20Fast%20Adaptation%20and%20Stable%20Motion%20Tracking.%20FAST%20introduces%20Parseval-Guided%20Residual%20Policy%20Adaptation%2C%20which%20learns%20a%20lightweight%20delta%20action%20policy%20under%20orthogonality%20and%20KL%20constraints%2C%20enabling%20efficient%20adaptation%20to%20out-of-distribution%20motions%20while%20mitigating%20catastrophic%20forgetting.%20To%20further%20improve%20physical%20robustness%2C%20we%20propose%20Center-of-Mass-Aware%20Control%2C%20which%20incorporates%20CoM-related%20observations%20and%20objectives%20to%20enhance%20balance%20when%20tracking%20challenging%20reference%20motions.%20Extensive%20experiments%20in%20simulation%20and%20real-world%20deployment%20demonstrate%20that%20FAST%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20robustness%2C%20adaptation%20efficiency%2C%20and%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2602.11929v1&entry.124074799=Read"},
{"title": "SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG", "author": "Elham Rostami and Aref Einizade and Taous-Meriem Laleg-Kirati", "abstract": "Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.", "link": "http://arxiv.org/abs/2602.11801v1", "date": "2026-02-12", "relevancy": 2.3217, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4551}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaTeoGL%3A%20Spatiotemporal%20Graph%20Learning%20for%20Interpretable%20Seizure%20Onset%20Zone%20Analysis%20from%20Intracranial%20EEG&body=Title%3A%20SpaTeoGL%3A%20Spatiotemporal%20Graph%20Learning%20for%20Interpretable%20Seizure%20Onset%20Zone%20Analysis%20from%20Intracranial%20EEG%0AAuthor%3A%20Elham%20Rostami%20and%20Aref%20Einizade%20and%20Taous-Meriem%20Laleg-Kirati%0AAbstract%3A%20Accurate%20localization%20of%20the%20seizure%20onset%20zone%20%28SOZ%29%20from%20intracranial%20EEG%20%28iEEG%29%20is%20essential%20for%20epilepsy%20surgery%20but%20is%20challenged%20by%20complex%20spatiotemporal%20seizure%20dynamics.%20We%20propose%20SpaTeoGL%2C%20a%20spatiotemporal%20graph%20learning%20framework%20for%20interpretable%20seizure%20network%20analysis.%20SpaTeoGL%20jointly%20learns%20window-level%20spatial%20graphs%20capturing%20interactions%20among%20iEEG%20electrodes%20and%20a%20temporal%20graph%20linking%20time%20windows%20based%20on%20similarity%20of%20their%20spatial%20structure.%20The%20method%20is%20formulated%20within%20a%20smooth%20graph%20signal%20processing%20framework%20and%20solved%20via%20an%20alternating%20block%20coordinate%20descent%20algorithm%20with%20convergence%20guarantees.%20Experiments%20on%20a%20multicenter%20iEEG%20dataset%20with%20successful%20surgical%20outcomes%20show%20that%20SpaTeoGL%20is%20competitive%20with%20a%20baseline%20based%20on%20horizontal%20visibility%20graphs%20and%20logistic%20regression%2C%20while%20improving%20non-SOZ%20identification%20and%20providing%20interpretable%20insights%20into%20seizure%20onset%20and%20propagation%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaTeoGL%253A%2520Spatiotemporal%2520Graph%2520Learning%2520for%2520Interpretable%2520Seizure%2520Onset%2520Zone%2520Analysis%2520from%2520Intracranial%2520EEG%26entry.906535625%3DElham%2520Rostami%2520and%2520Aref%2520Einizade%2520and%2520Taous-Meriem%2520Laleg-Kirati%26entry.1292438233%3DAccurate%2520localization%2520of%2520the%2520seizure%2520onset%2520zone%2520%2528SOZ%2529%2520from%2520intracranial%2520EEG%2520%2528iEEG%2529%2520is%2520essential%2520for%2520epilepsy%2520surgery%2520but%2520is%2520challenged%2520by%2520complex%2520spatiotemporal%2520seizure%2520dynamics.%2520We%2520propose%2520SpaTeoGL%252C%2520a%2520spatiotemporal%2520graph%2520learning%2520framework%2520for%2520interpretable%2520seizure%2520network%2520analysis.%2520SpaTeoGL%2520jointly%2520learns%2520window-level%2520spatial%2520graphs%2520capturing%2520interactions%2520among%2520iEEG%2520electrodes%2520and%2520a%2520temporal%2520graph%2520linking%2520time%2520windows%2520based%2520on%2520similarity%2520of%2520their%2520spatial%2520structure.%2520The%2520method%2520is%2520formulated%2520within%2520a%2520smooth%2520graph%2520signal%2520processing%2520framework%2520and%2520solved%2520via%2520an%2520alternating%2520block%2520coordinate%2520descent%2520algorithm%2520with%2520convergence%2520guarantees.%2520Experiments%2520on%2520a%2520multicenter%2520iEEG%2520dataset%2520with%2520successful%2520surgical%2520outcomes%2520show%2520that%2520SpaTeoGL%2520is%2520competitive%2520with%2520a%2520baseline%2520based%2520on%2520horizontal%2520visibility%2520graphs%2520and%2520logistic%2520regression%252C%2520while%2520improving%2520non-SOZ%2520identification%2520and%2520providing%2520interpretable%2520insights%2520into%2520seizure%2520onset%2520and%2520propagation%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaTeoGL%3A%20Spatiotemporal%20Graph%20Learning%20for%20Interpretable%20Seizure%20Onset%20Zone%20Analysis%20from%20Intracranial%20EEG&entry.906535625=Elham%20Rostami%20and%20Aref%20Einizade%20and%20Taous-Meriem%20Laleg-Kirati&entry.1292438233=Accurate%20localization%20of%20the%20seizure%20onset%20zone%20%28SOZ%29%20from%20intracranial%20EEG%20%28iEEG%29%20is%20essential%20for%20epilepsy%20surgery%20but%20is%20challenged%20by%20complex%20spatiotemporal%20seizure%20dynamics.%20We%20propose%20SpaTeoGL%2C%20a%20spatiotemporal%20graph%20learning%20framework%20for%20interpretable%20seizure%20network%20analysis.%20SpaTeoGL%20jointly%20learns%20window-level%20spatial%20graphs%20capturing%20interactions%20among%20iEEG%20electrodes%20and%20a%20temporal%20graph%20linking%20time%20windows%20based%20on%20similarity%20of%20their%20spatial%20structure.%20The%20method%20is%20formulated%20within%20a%20smooth%20graph%20signal%20processing%20framework%20and%20solved%20via%20an%20alternating%20block%20coordinate%20descent%20algorithm%20with%20convergence%20guarantees.%20Experiments%20on%20a%20multicenter%20iEEG%20dataset%20with%20successful%20surgical%20outcomes%20show%20that%20SpaTeoGL%20is%20competitive%20with%20a%20baseline%20based%20on%20horizontal%20visibility%20graphs%20and%20logistic%20regression%2C%20while%20improving%20non-SOZ%20identification%20and%20providing%20interpretable%20insights%20into%20seizure%20onset%20and%20propagation%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2602.11801v1&entry.124074799=Read"},
{"title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation", "author": "Weixing Zhang and Bowen Jiang and Yuhong Fu and Anne Koziolek and Regina Hebig and Daniel Str\u00fcber", "abstract": "Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.", "link": "http://arxiv.org/abs/2602.11904v1", "date": "2026-02-12", "relevancy": 2.3027, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4691}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20LLMs%20to%20support%20co-evolution%20between%20definitions%20and%20instances%20of%20textual%20DSLs%3A%20A%20Systematic%20Evaluation&body=Title%3A%20Leveraging%20LLMs%20to%20support%20co-evolution%20between%20definitions%20and%20instances%20of%20textual%20DSLs%3A%20A%20Systematic%20Evaluation%0AAuthor%3A%20Weixing%20Zhang%20and%20Bowen%20Jiang%20and%20Yuhong%20Fu%20and%20Anne%20Koziolek%20and%20Regina%20Hebig%20and%20Daniel%20Str%C3%BCber%0AAbstract%3A%20Software%20languages%20evolve%20over%20time%20for%20reasons%20such%20as%20feature%20additions.%20When%20grammars%20evolve%2C%20textual%20instances%20that%20originally%20conformed%20to%20them%20may%20become%20outdated.%20While%20model-driven%20engineering%20provides%20many%20techniques%20for%20co-evolving%20models%20with%20metamodel%20changes%2C%20these%20approaches%20are%20not%20designed%20for%20textual%20DSLs%20and%20may%20lose%20human-relevant%20information%20such%20as%20layout%20and%20comments.%20This%20study%20systematically%20evaluates%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%20for%20co-evolving%20grammars%20and%20instances%20of%20textual%20DSLs.%20Using%20Claude%20Sonnet%204.5%20and%20GPT-5.2%20across%20ten%20case%20languages%20with%20ten%20runs%20each%2C%20we%20assess%20both%20correctness%20and%20preservation%20of%20human-oriented%20information.%20Results%20show%20strong%20performance%20on%20small-scale%20cases%20%28%24%5Cgeq%2494%25%20precision%20and%20recall%20for%20instances%20requiring%20fewer%20than%2020%20modified%20lines%29%2C%20but%20performance%20degraded%20with%20scale%3A%20Claude%20maintains%2085%25%20recall%20at%2040%20lines%2C%20while%20GPT%20fails%20on%20the%20largest%20instances.%20Response%20time%20increases%20substantially%20with%20instance%20size%2C%20and%20grammar%20evolution%20complexity%20and%20deletion%20granularity%20affect%20performance%20more%20than%20change%20type.%20These%20findings%20clarify%20when%20LLM-based%20co-evolution%20is%20effective%20and%20where%20current%20limitations%20remain.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520LLMs%2520to%2520support%2520co-evolution%2520between%2520definitions%2520and%2520instances%2520of%2520textual%2520DSLs%253A%2520A%2520Systematic%2520Evaluation%26entry.906535625%3DWeixing%2520Zhang%2520and%2520Bowen%2520Jiang%2520and%2520Yuhong%2520Fu%2520and%2520Anne%2520Koziolek%2520and%2520Regina%2520Hebig%2520and%2520Daniel%2520Str%25C3%25BCber%26entry.1292438233%3DSoftware%2520languages%2520evolve%2520over%2520time%2520for%2520reasons%2520such%2520as%2520feature%2520additions.%2520When%2520grammars%2520evolve%252C%2520textual%2520instances%2520that%2520originally%2520conformed%2520to%2520them%2520may%2520become%2520outdated.%2520While%2520model-driven%2520engineering%2520provides%2520many%2520techniques%2520for%2520co-evolving%2520models%2520with%2520metamodel%2520changes%252C%2520these%2520approaches%2520are%2520not%2520designed%2520for%2520textual%2520DSLs%2520and%2520may%2520lose%2520human-relevant%2520information%2520such%2520as%2520layout%2520and%2520comments.%2520This%2520study%2520systematically%2520evaluates%2520the%2520potential%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520co-evolving%2520grammars%2520and%2520instances%2520of%2520textual%2520DSLs.%2520Using%2520Claude%2520Sonnet%25204.5%2520and%2520GPT-5.2%2520across%2520ten%2520case%2520languages%2520with%2520ten%2520runs%2520each%252C%2520we%2520assess%2520both%2520correctness%2520and%2520preservation%2520of%2520human-oriented%2520information.%2520Results%2520show%2520strong%2520performance%2520on%2520small-scale%2520cases%2520%2528%2524%255Cgeq%252494%2525%2520precision%2520and%2520recall%2520for%2520instances%2520requiring%2520fewer%2520than%252020%2520modified%2520lines%2529%252C%2520but%2520performance%2520degraded%2520with%2520scale%253A%2520Claude%2520maintains%252085%2525%2520recall%2520at%252040%2520lines%252C%2520while%2520GPT%2520fails%2520on%2520the%2520largest%2520instances.%2520Response%2520time%2520increases%2520substantially%2520with%2520instance%2520size%252C%2520and%2520grammar%2520evolution%2520complexity%2520and%2520deletion%2520granularity%2520affect%2520performance%2520more%2520than%2520change%2520type.%2520These%2520findings%2520clarify%2520when%2520LLM-based%2520co-evolution%2520is%2520effective%2520and%2520where%2520current%2520limitations%2520remain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20LLMs%20to%20support%20co-evolution%20between%20definitions%20and%20instances%20of%20textual%20DSLs%3A%20A%20Systematic%20Evaluation&entry.906535625=Weixing%20Zhang%20and%20Bowen%20Jiang%20and%20Yuhong%20Fu%20and%20Anne%20Koziolek%20and%20Regina%20Hebig%20and%20Daniel%20Str%C3%BCber&entry.1292438233=Software%20languages%20evolve%20over%20time%20for%20reasons%20such%20as%20feature%20additions.%20When%20grammars%20evolve%2C%20textual%20instances%20that%20originally%20conformed%20to%20them%20may%20become%20outdated.%20While%20model-driven%20engineering%20provides%20many%20techniques%20for%20co-evolving%20models%20with%20metamodel%20changes%2C%20these%20approaches%20are%20not%20designed%20for%20textual%20DSLs%20and%20may%20lose%20human-relevant%20information%20such%20as%20layout%20and%20comments.%20This%20study%20systematically%20evaluates%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%20for%20co-evolving%20grammars%20and%20instances%20of%20textual%20DSLs.%20Using%20Claude%20Sonnet%204.5%20and%20GPT-5.2%20across%20ten%20case%20languages%20with%20ten%20runs%20each%2C%20we%20assess%20both%20correctness%20and%20preservation%20of%20human-oriented%20information.%20Results%20show%20strong%20performance%20on%20small-scale%20cases%20%28%24%5Cgeq%2494%25%20precision%20and%20recall%20for%20instances%20requiring%20fewer%20than%2020%20modified%20lines%29%2C%20but%20performance%20degraded%20with%20scale%3A%20Claude%20maintains%2085%25%20recall%20at%2040%20lines%2C%20while%20GPT%20fails%20on%20the%20largest%20instances.%20Response%20time%20increases%20substantially%20with%20instance%20size%2C%20and%20grammar%20evolution%20complexity%20and%20deletion%20granularity%20affect%20performance%20more%20than%20change%20type.%20These%20findings%20clarify%20when%20LLM-based%20co-evolution%20is%20effective%20and%20where%20current%20limitations%20remain.&entry.1838667208=http%3A//arxiv.org/abs/2602.11904v1&entry.124074799=Read"},
{"title": "GPT-4o Lacks Core Features of Theory of Mind", "author": "John Muchovej and Amanda Royka and Shane Lee and Julian Jara-Ettinger", "abstract": "Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.", "link": "http://arxiv.org/abs/2602.12150v1", "date": "2026-02-12", "relevancy": 2.2935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT-4o%20Lacks%20Core%20Features%20of%20Theory%20of%20Mind&body=Title%3A%20GPT-4o%20Lacks%20Core%20Features%20of%20Theory%20of%20Mind%0AAuthor%3A%20John%20Muchovej%20and%20Amanda%20Royka%20and%20Shane%20Lee%20and%20Julian%20Jara-Ettinger%0AAbstract%3A%20Do%20Large%20Language%20Models%20%28LLMs%29%20possess%20a%20Theory%20of%20Mind%20%28ToM%29%3F%20Research%20into%20this%20question%20has%20focused%20on%20evaluating%20LLMs%20against%20benchmarks%20and%20found%20success%20across%20a%20range%20of%20social%20tasks.%20However%2C%20these%20evaluations%20do%20not%20test%20for%20the%20actual%20representations%20posited%20by%20ToM%3A%20namely%2C%20a%20causal%20model%20of%20mental%20states%20and%20behavior.%20Here%2C%20we%20use%20a%20cognitively-grounded%20definition%20of%20ToM%20to%20develop%20and%20test%20a%20new%20evaluation%20framework.%20Specifically%2C%20our%20approach%20probes%20whether%20LLMs%20have%20a%20coherent%2C%20domain-general%2C%20and%20consistent%20model%20of%20how%20mental%20states%20cause%20behavior%20--%20regardless%20of%20whether%20that%20model%20matches%20a%20human-like%20ToM.%20We%20find%20that%20even%20though%20LLMs%20succeed%20in%20approximating%20human%20judgments%20in%20a%20simple%20ToM%20paradigm%2C%20they%20fail%20at%20a%20logically%20equivalent%20task%20and%20exhibit%20low%20consistency%20between%20their%20action%20predictions%20and%20corresponding%20mental%20state%20inferences.%20As%20such%2C%20these%20findings%20suggest%20that%20the%20social%20proficiency%20exhibited%20by%20LLMs%20is%20not%20the%20result%20of%20an%20domain-general%20or%20consistent%20ToM.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT-4o%2520Lacks%2520Core%2520Features%2520of%2520Theory%2520of%2520Mind%26entry.906535625%3DJohn%2520Muchovej%2520and%2520Amanda%2520Royka%2520and%2520Shane%2520Lee%2520and%2520Julian%2520Jara-Ettinger%26entry.1292438233%3DDo%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520possess%2520a%2520Theory%2520of%2520Mind%2520%2528ToM%2529%253F%2520Research%2520into%2520this%2520question%2520has%2520focused%2520on%2520evaluating%2520LLMs%2520against%2520benchmarks%2520and%2520found%2520success%2520across%2520a%2520range%2520of%2520social%2520tasks.%2520However%252C%2520these%2520evaluations%2520do%2520not%2520test%2520for%2520the%2520actual%2520representations%2520posited%2520by%2520ToM%253A%2520namely%252C%2520a%2520causal%2520model%2520of%2520mental%2520states%2520and%2520behavior.%2520Here%252C%2520we%2520use%2520a%2520cognitively-grounded%2520definition%2520of%2520ToM%2520to%2520develop%2520and%2520test%2520a%2520new%2520evaluation%2520framework.%2520Specifically%252C%2520our%2520approach%2520probes%2520whether%2520LLMs%2520have%2520a%2520coherent%252C%2520domain-general%252C%2520and%2520consistent%2520model%2520of%2520how%2520mental%2520states%2520cause%2520behavior%2520--%2520regardless%2520of%2520whether%2520that%2520model%2520matches%2520a%2520human-like%2520ToM.%2520We%2520find%2520that%2520even%2520though%2520LLMs%2520succeed%2520in%2520approximating%2520human%2520judgments%2520in%2520a%2520simple%2520ToM%2520paradigm%252C%2520they%2520fail%2520at%2520a%2520logically%2520equivalent%2520task%2520and%2520exhibit%2520low%2520consistency%2520between%2520their%2520action%2520predictions%2520and%2520corresponding%2520mental%2520state%2520inferences.%2520As%2520such%252C%2520these%2520findings%2520suggest%2520that%2520the%2520social%2520proficiency%2520exhibited%2520by%2520LLMs%2520is%2520not%2520the%2520result%2520of%2520an%2520domain-general%2520or%2520consistent%2520ToM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT-4o%20Lacks%20Core%20Features%20of%20Theory%20of%20Mind&entry.906535625=John%20Muchovej%20and%20Amanda%20Royka%20and%20Shane%20Lee%20and%20Julian%20Jara-Ettinger&entry.1292438233=Do%20Large%20Language%20Models%20%28LLMs%29%20possess%20a%20Theory%20of%20Mind%20%28ToM%29%3F%20Research%20into%20this%20question%20has%20focused%20on%20evaluating%20LLMs%20against%20benchmarks%20and%20found%20success%20across%20a%20range%20of%20social%20tasks.%20However%2C%20these%20evaluations%20do%20not%20test%20for%20the%20actual%20representations%20posited%20by%20ToM%3A%20namely%2C%20a%20causal%20model%20of%20mental%20states%20and%20behavior.%20Here%2C%20we%20use%20a%20cognitively-grounded%20definition%20of%20ToM%20to%20develop%20and%20test%20a%20new%20evaluation%20framework.%20Specifically%2C%20our%20approach%20probes%20whether%20LLMs%20have%20a%20coherent%2C%20domain-general%2C%20and%20consistent%20model%20of%20how%20mental%20states%20cause%20behavior%20--%20regardless%20of%20whether%20that%20model%20matches%20a%20human-like%20ToM.%20We%20find%20that%20even%20though%20LLMs%20succeed%20in%20approximating%20human%20judgments%20in%20a%20simple%20ToM%20paradigm%2C%20they%20fail%20at%20a%20logically%20equivalent%20task%20and%20exhibit%20low%20consistency%20between%20their%20action%20predictions%20and%20corresponding%20mental%20state%20inferences.%20As%20such%2C%20these%20findings%20suggest%20that%20the%20social%20proficiency%20exhibited%20by%20LLMs%20is%20not%20the%20result%20of%20an%20domain-general%20or%20consistent%20ToM.&entry.1838667208=http%3A//arxiv.org/abs/2602.12150v1&entry.124074799=Read"},
{"title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication", "author": "Ralph Kr\u00fcger", "abstract": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.", "link": "http://arxiv.org/abs/2602.12251v1", "date": "2026-02-12", "relevancy": 2.2819, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4695}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4556}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20technical%20curriculum%20on%20language-oriented%20artificial%20intelligence%20in%20translation%20and%20specialised%20communication&body=Title%3A%20A%20technical%20curriculum%20on%20language-oriented%20artificial%20intelligence%20in%20translation%20and%20specialised%20communication%0AAuthor%3A%20Ralph%20Kr%C3%BCger%0AAbstract%3A%20This%20paper%20presents%20a%20technical%20curriculum%20on%20language-oriented%20artificial%20intelligence%20%28AI%29%20in%20the%20language%20and%20translation%20%28L%26T%29%20industry.%20The%20curriculum%20aims%20to%20foster%20domain-specific%20technical%20AI%20literacy%20among%20stakeholders%20in%20the%20fields%20of%20translation%20and%20specialised%20communication%20by%20exposing%20them%20to%20the%20conceptual%20and%20technical/algorithmic%20foundations%20of%20modern%20language-oriented%20AI%20in%20an%20accessible%20way.%20The%20core%20curriculum%20focuses%20on%201%29%20vector%20embeddings%2C%202%29%20the%20technical%20foundations%20of%20neural%20networks%2C%203%29%20tokenization%20and%204%29%20transformer%20neural%20networks.%20It%20is%20intended%20to%20help%20users%20develop%20computational%20thinking%20as%20well%20as%20algorithmic%20awareness%20and%20algorithmic%20agency%2C%20ultimately%20contributing%20to%20their%20digital%20resilience%20in%20AI-driven%20work%20environments.%20The%20didactic%20suitability%20of%20the%20curriculum%20was%20tested%20in%20an%20AI-focused%20MA%20course%20at%20the%20Institute%20of%20Translation%20and%20Multilingual%20Communication%20at%20TH%20Koeln.%20Results%20suggest%20the%20didactic%20effectiveness%20of%20the%20curriculum%2C%20but%20participant%20feedback%20indicates%20that%20it%20should%20be%20embedded%20into%20higher-level%20didactic%20scaffolding%20-%20e.g.%2C%20in%20the%20form%20of%20lecturer%20support%20-%20in%20order%20to%20enable%20optimal%20learning%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520technical%2520curriculum%2520on%2520language-oriented%2520artificial%2520intelligence%2520in%2520translation%2520and%2520specialised%2520communication%26entry.906535625%3DRalph%2520Kr%25C3%25BCger%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520technical%2520curriculum%2520on%2520language-oriented%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520the%2520language%2520and%2520translation%2520%2528L%2526T%2529%2520industry.%2520The%2520curriculum%2520aims%2520to%2520foster%2520domain-specific%2520technical%2520AI%2520literacy%2520among%2520stakeholders%2520in%2520the%2520fields%2520of%2520translation%2520and%2520specialised%2520communication%2520by%2520exposing%2520them%2520to%2520the%2520conceptual%2520and%2520technical/algorithmic%2520foundations%2520of%2520modern%2520language-oriented%2520AI%2520in%2520an%2520accessible%2520way.%2520The%2520core%2520curriculum%2520focuses%2520on%25201%2529%2520vector%2520embeddings%252C%25202%2529%2520the%2520technical%2520foundations%2520of%2520neural%2520networks%252C%25203%2529%2520tokenization%2520and%25204%2529%2520transformer%2520neural%2520networks.%2520It%2520is%2520intended%2520to%2520help%2520users%2520develop%2520computational%2520thinking%2520as%2520well%2520as%2520algorithmic%2520awareness%2520and%2520algorithmic%2520agency%252C%2520ultimately%2520contributing%2520to%2520their%2520digital%2520resilience%2520in%2520AI-driven%2520work%2520environments.%2520The%2520didactic%2520suitability%2520of%2520the%2520curriculum%2520was%2520tested%2520in%2520an%2520AI-focused%2520MA%2520course%2520at%2520the%2520Institute%2520of%2520Translation%2520and%2520Multilingual%2520Communication%2520at%2520TH%2520Koeln.%2520Results%2520suggest%2520the%2520didactic%2520effectiveness%2520of%2520the%2520curriculum%252C%2520but%2520participant%2520feedback%2520indicates%2520that%2520it%2520should%2520be%2520embedded%2520into%2520higher-level%2520didactic%2520scaffolding%2520-%2520e.g.%252C%2520in%2520the%2520form%2520of%2520lecturer%2520support%2520-%2520in%2520order%2520to%2520enable%2520optimal%2520learning%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20technical%20curriculum%20on%20language-oriented%20artificial%20intelligence%20in%20translation%20and%20specialised%20communication&entry.906535625=Ralph%20Kr%C3%BCger&entry.1292438233=This%20paper%20presents%20a%20technical%20curriculum%20on%20language-oriented%20artificial%20intelligence%20%28AI%29%20in%20the%20language%20and%20translation%20%28L%26T%29%20industry.%20The%20curriculum%20aims%20to%20foster%20domain-specific%20technical%20AI%20literacy%20among%20stakeholders%20in%20the%20fields%20of%20translation%20and%20specialised%20communication%20by%20exposing%20them%20to%20the%20conceptual%20and%20technical/algorithmic%20foundations%20of%20modern%20language-oriented%20AI%20in%20an%20accessible%20way.%20The%20core%20curriculum%20focuses%20on%201%29%20vector%20embeddings%2C%202%29%20the%20technical%20foundations%20of%20neural%20networks%2C%203%29%20tokenization%20and%204%29%20transformer%20neural%20networks.%20It%20is%20intended%20to%20help%20users%20develop%20computational%20thinking%20as%20well%20as%20algorithmic%20awareness%20and%20algorithmic%20agency%2C%20ultimately%20contributing%20to%20their%20digital%20resilience%20in%20AI-driven%20work%20environments.%20The%20didactic%20suitability%20of%20the%20curriculum%20was%20tested%20in%20an%20AI-focused%20MA%20course%20at%20the%20Institute%20of%20Translation%20and%20Multilingual%20Communication%20at%20TH%20Koeln.%20Results%20suggest%20the%20didactic%20effectiveness%20of%20the%20curriculum%2C%20but%20participant%20feedback%20indicates%20that%20it%20should%20be%20embedded%20into%20higher-level%20didactic%20scaffolding%20-%20e.g.%2C%20in%20the%20form%20of%20lecturer%20support%20-%20in%20order%20to%20enable%20optimal%20learning%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2602.12251v1&entry.124074799=Read"},
{"title": "Dual Frequency Branch Framework with Reconstructed Sliding Windows Attention for AI-Generated Image Detection", "author": "Jiazhen Yan and Ziqiang Li and Fan Wang and Ziwen He and Zhangjie Fu", "abstract": "The rapid advancement of Generative Adversarial Networks (GANs) and diffusion models has enabled the creation of highly realistic synthetic images, presenting significant societal risks, such as misinformation and deception. As a result, detecting AI-generated images has emerged as a critical challenge. Existing researches emphasize extracting fine-grained features to enhance detector generalization, yet they often lack consideration for the importance and interdependencies of internal elements within local regions and are limited to a single frequency domain, hindering the capture of general forgery traces. To overcome the aforementioned limitations, we first utilize a sliding window to restrict the attention mechanism to a local window, and reconstruct the features within the window to model the relationships between neighboring internal elements within the local region. Then, we design a dual frequency domain branch framework consisting of four frequency domain subbands of DWT and the phase part of FFT to enrich the extraction of local forgery features from different perspectives. Through feature enrichment of dual frequency domain branches and fine-grained feature extraction of reconstruction sliding window attention, our method achieves superior generalization detection capabilities on both GAN and diffusion model-based generative images. Evaluated on diverse datasets comprising images from 65 distinct generative models, our approach achieves a 2.13\\% improvement in detection accuracy over state-of-the-art methods.", "link": "http://arxiv.org/abs/2501.15253v3", "date": "2026-02-12", "relevancy": 2.2739, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5709}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5702}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Frequency%20Branch%20Framework%20with%20Reconstructed%20Sliding%20Windows%20Attention%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20Dual%20Frequency%20Branch%20Framework%20with%20Reconstructed%20Sliding%20Windows%20Attention%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Jiazhen%20Yan%20and%20Ziqiang%20Li%20and%20Fan%20Wang%20and%20Ziwen%20He%20and%20Zhangjie%20Fu%0AAbstract%3A%20The%20rapid%20advancement%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20diffusion%20models%20has%20enabled%20the%20creation%20of%20highly%20realistic%20synthetic%20images%2C%20presenting%20significant%20societal%20risks%2C%20such%20as%20misinformation%20and%20deception.%20As%20a%20result%2C%20detecting%20AI-generated%20images%20has%20emerged%20as%20a%20critical%20challenge.%20Existing%20researches%20emphasize%20extracting%20fine-grained%20features%20to%20enhance%20detector%20generalization%2C%20yet%20they%20often%20lack%20consideration%20for%20the%20importance%20and%20interdependencies%20of%20internal%20elements%20within%20local%20regions%20and%20are%20limited%20to%20a%20single%20frequency%20domain%2C%20hindering%20the%20capture%20of%20general%20forgery%20traces.%20To%20overcome%20the%20aforementioned%20limitations%2C%20we%20first%20utilize%20a%20sliding%20window%20to%20restrict%20the%20attention%20mechanism%20to%20a%20local%20window%2C%20and%20reconstruct%20the%20features%20within%20the%20window%20to%20model%20the%20relationships%20between%20neighboring%20internal%20elements%20within%20the%20local%20region.%20Then%2C%20we%20design%20a%20dual%20frequency%20domain%20branch%20framework%20consisting%20of%20four%20frequency%20domain%20subbands%20of%20DWT%20and%20the%20phase%20part%20of%20FFT%20to%20enrich%20the%20extraction%20of%20local%20forgery%20features%20from%20different%20perspectives.%20Through%20feature%20enrichment%20of%20dual%20frequency%20domain%20branches%20and%20fine-grained%20feature%20extraction%20of%20reconstruction%20sliding%20window%20attention%2C%20our%20method%20achieves%20superior%20generalization%20detection%20capabilities%20on%20both%20GAN%20and%20diffusion%20model-based%20generative%20images.%20Evaluated%20on%20diverse%20datasets%20comprising%20images%20from%2065%20distinct%20generative%20models%2C%20our%20approach%20achieves%20a%202.13%5C%25%20improvement%20in%20detection%20accuracy%20over%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2501.15253v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Frequency%2520Branch%2520Framework%2520with%2520Reconstructed%2520Sliding%2520Windows%2520Attention%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DJiazhen%2520Yan%2520and%2520Ziqiang%2520Li%2520and%2520Fan%2520Wang%2520and%2520Ziwen%2520He%2520and%2520Zhangjie%2520Fu%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520and%2520diffusion%2520models%2520has%2520enabled%2520the%2520creation%2520of%2520highly%2520realistic%2520synthetic%2520images%252C%2520presenting%2520significant%2520societal%2520risks%252C%2520such%2520as%2520misinformation%2520and%2520deception.%2520As%2520a%2520result%252C%2520detecting%2520AI-generated%2520images%2520has%2520emerged%2520as%2520a%2520critical%2520challenge.%2520Existing%2520researches%2520emphasize%2520extracting%2520fine-grained%2520features%2520to%2520enhance%2520detector%2520generalization%252C%2520yet%2520they%2520often%2520lack%2520consideration%2520for%2520the%2520importance%2520and%2520interdependencies%2520of%2520internal%2520elements%2520within%2520local%2520regions%2520and%2520are%2520limited%2520to%2520a%2520single%2520frequency%2520domain%252C%2520hindering%2520the%2520capture%2520of%2520general%2520forgery%2520traces.%2520To%2520overcome%2520the%2520aforementioned%2520limitations%252C%2520we%2520first%2520utilize%2520a%2520sliding%2520window%2520to%2520restrict%2520the%2520attention%2520mechanism%2520to%2520a%2520local%2520window%252C%2520and%2520reconstruct%2520the%2520features%2520within%2520the%2520window%2520to%2520model%2520the%2520relationships%2520between%2520neighboring%2520internal%2520elements%2520within%2520the%2520local%2520region.%2520Then%252C%2520we%2520design%2520a%2520dual%2520frequency%2520domain%2520branch%2520framework%2520consisting%2520of%2520four%2520frequency%2520domain%2520subbands%2520of%2520DWT%2520and%2520the%2520phase%2520part%2520of%2520FFT%2520to%2520enrich%2520the%2520extraction%2520of%2520local%2520forgery%2520features%2520from%2520different%2520perspectives.%2520Through%2520feature%2520enrichment%2520of%2520dual%2520frequency%2520domain%2520branches%2520and%2520fine-grained%2520feature%2520extraction%2520of%2520reconstruction%2520sliding%2520window%2520attention%252C%2520our%2520method%2520achieves%2520superior%2520generalization%2520detection%2520capabilities%2520on%2520both%2520GAN%2520and%2520diffusion%2520model-based%2520generative%2520images.%2520Evaluated%2520on%2520diverse%2520datasets%2520comprising%2520images%2520from%252065%2520distinct%2520generative%2520models%252C%2520our%2520approach%2520achieves%2520a%25202.13%255C%2525%2520improvement%2520in%2520detection%2520accuracy%2520over%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15253v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Frequency%20Branch%20Framework%20with%20Reconstructed%20Sliding%20Windows%20Attention%20for%20AI-Generated%20Image%20Detection&entry.906535625=Jiazhen%20Yan%20and%20Ziqiang%20Li%20and%20Fan%20Wang%20and%20Ziwen%20He%20and%20Zhangjie%20Fu&entry.1292438233=The%20rapid%20advancement%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20diffusion%20models%20has%20enabled%20the%20creation%20of%20highly%20realistic%20synthetic%20images%2C%20presenting%20significant%20societal%20risks%2C%20such%20as%20misinformation%20and%20deception.%20As%20a%20result%2C%20detecting%20AI-generated%20images%20has%20emerged%20as%20a%20critical%20challenge.%20Existing%20researches%20emphasize%20extracting%20fine-grained%20features%20to%20enhance%20detector%20generalization%2C%20yet%20they%20often%20lack%20consideration%20for%20the%20importance%20and%20interdependencies%20of%20internal%20elements%20within%20local%20regions%20and%20are%20limited%20to%20a%20single%20frequency%20domain%2C%20hindering%20the%20capture%20of%20general%20forgery%20traces.%20To%20overcome%20the%20aforementioned%20limitations%2C%20we%20first%20utilize%20a%20sliding%20window%20to%20restrict%20the%20attention%20mechanism%20to%20a%20local%20window%2C%20and%20reconstruct%20the%20features%20within%20the%20window%20to%20model%20the%20relationships%20between%20neighboring%20internal%20elements%20within%20the%20local%20region.%20Then%2C%20we%20design%20a%20dual%20frequency%20domain%20branch%20framework%20consisting%20of%20four%20frequency%20domain%20subbands%20of%20DWT%20and%20the%20phase%20part%20of%20FFT%20to%20enrich%20the%20extraction%20of%20local%20forgery%20features%20from%20different%20perspectives.%20Through%20feature%20enrichment%20of%20dual%20frequency%20domain%20branches%20and%20fine-grained%20feature%20extraction%20of%20reconstruction%20sliding%20window%20attention%2C%20our%20method%20achieves%20superior%20generalization%20detection%20capabilities%20on%20both%20GAN%20and%20diffusion%20model-based%20generative%20images.%20Evaluated%20on%20diverse%20datasets%20comprising%20images%20from%2065%20distinct%20generative%20models%2C%20our%20approach%20achieves%20a%202.13%5C%25%20improvement%20in%20detection%20accuracy%20over%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2501.15253v3&entry.124074799=Read"},
{"title": "Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design", "author": "Mohammad Khosravi and Setareh Maghsudi", "abstract": "We study the design of resilient and reliable communication networks in which a signal can be transferred only up to a limited distance before its quality falls below an acceptable threshold. When excessive signal degradation occurs, regeneration is required through regenerators installed at selected network nodes. In this work, both network links and nodes are subject to uncertainty. The installation costs of regenerators are modeled using a budgeted uncertainty set. In addition, link lengths follow a dynamic budgeted uncertainty set introduced in this paper, where deviations may vary over time. Robust optimization seeks solutions whose performance is guaranteed under all scenarios represented by the underlying uncertainty set. Accordingly, the objective is to identify a minimum-cost subset of nodes for regenerator deployment that ensures full network connectivity, even under the worst possible realizations of uncertainty. To solve the problem, we first formulate it within a robust optimization framework, and then develop scalable solution methods based on column-and-constraint generation, Benders decomposition, and iterative robust optimization. In addition, we formulate a learning-based hide-and-seek game to further analyze the problem structure. The proposed approaches are evaluated against classical static budgeted robust models and deterministic worst-case formulations. Both theoretical analysis and computational results demonstrate the effectiveness and advantages of our methodology.", "link": "http://arxiv.org/abs/2602.11854v1", "date": "2026-02-12", "relevancy": 2.2683, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4613}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Optimization%20Approach%20and%20Learning%20Based%20Hide-and-Seek%20Game%20for%20Resilient%20Network%20Design&body=Title%3A%20Robust%20Optimization%20Approach%20and%20Learning%20Based%20Hide-and-Seek%20Game%20for%20Resilient%20Network%20Design%0AAuthor%3A%20Mohammad%20Khosravi%20and%20Setareh%20Maghsudi%0AAbstract%3A%20We%20study%20the%20design%20of%20resilient%20and%20reliable%20communication%20networks%20in%20which%20a%20signal%20can%20be%20transferred%20only%20up%20to%20a%20limited%20distance%20before%20its%20quality%20falls%20below%20an%20acceptable%20threshold.%20When%20excessive%20signal%20degradation%20occurs%2C%20regeneration%20is%20required%20through%20regenerators%20installed%20at%20selected%20network%20nodes.%20In%20this%20work%2C%20both%20network%20links%20and%20nodes%20are%20subject%20to%20uncertainty.%20The%20installation%20costs%20of%20regenerators%20are%20modeled%20using%20a%20budgeted%20uncertainty%20set.%20In%20addition%2C%20link%20lengths%20follow%20a%20dynamic%20budgeted%20uncertainty%20set%20introduced%20in%20this%20paper%2C%20where%20deviations%20may%20vary%20over%20time.%20Robust%20optimization%20seeks%20solutions%20whose%20performance%20is%20guaranteed%20under%20all%20scenarios%20represented%20by%20the%20underlying%20uncertainty%20set.%20Accordingly%2C%20the%20objective%20is%20to%20identify%20a%20minimum-cost%20subset%20of%20nodes%20for%20regenerator%20deployment%20that%20ensures%20full%20network%20connectivity%2C%20even%20under%20the%20worst%20possible%20realizations%20of%20uncertainty.%20To%20solve%20the%20problem%2C%20we%20first%20formulate%20it%20within%20a%20robust%20optimization%20framework%2C%20and%20then%20develop%20scalable%20solution%20methods%20based%20on%20column-and-constraint%20generation%2C%20Benders%20decomposition%2C%20and%20iterative%20robust%20optimization.%20In%20addition%2C%20we%20formulate%20a%20learning-based%20hide-and-seek%20game%20to%20further%20analyze%20the%20problem%20structure.%20The%20proposed%20approaches%20are%20evaluated%20against%20classical%20static%20budgeted%20robust%20models%20and%20deterministic%20worst-case%20formulations.%20Both%20theoretical%20analysis%20and%20computational%20results%20demonstrate%20the%20effectiveness%20and%20advantages%20of%20our%20methodology.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Optimization%2520Approach%2520and%2520Learning%2520Based%2520Hide-and-Seek%2520Game%2520for%2520Resilient%2520Network%2520Design%26entry.906535625%3DMohammad%2520Khosravi%2520and%2520Setareh%2520Maghsudi%26entry.1292438233%3DWe%2520study%2520the%2520design%2520of%2520resilient%2520and%2520reliable%2520communication%2520networks%2520in%2520which%2520a%2520signal%2520can%2520be%2520transferred%2520only%2520up%2520to%2520a%2520limited%2520distance%2520before%2520its%2520quality%2520falls%2520below%2520an%2520acceptable%2520threshold.%2520When%2520excessive%2520signal%2520degradation%2520occurs%252C%2520regeneration%2520is%2520required%2520through%2520regenerators%2520installed%2520at%2520selected%2520network%2520nodes.%2520In%2520this%2520work%252C%2520both%2520network%2520links%2520and%2520nodes%2520are%2520subject%2520to%2520uncertainty.%2520The%2520installation%2520costs%2520of%2520regenerators%2520are%2520modeled%2520using%2520a%2520budgeted%2520uncertainty%2520set.%2520In%2520addition%252C%2520link%2520lengths%2520follow%2520a%2520dynamic%2520budgeted%2520uncertainty%2520set%2520introduced%2520in%2520this%2520paper%252C%2520where%2520deviations%2520may%2520vary%2520over%2520time.%2520Robust%2520optimization%2520seeks%2520solutions%2520whose%2520performance%2520is%2520guaranteed%2520under%2520all%2520scenarios%2520represented%2520by%2520the%2520underlying%2520uncertainty%2520set.%2520Accordingly%252C%2520the%2520objective%2520is%2520to%2520identify%2520a%2520minimum-cost%2520subset%2520of%2520nodes%2520for%2520regenerator%2520deployment%2520that%2520ensures%2520full%2520network%2520connectivity%252C%2520even%2520under%2520the%2520worst%2520possible%2520realizations%2520of%2520uncertainty.%2520To%2520solve%2520the%2520problem%252C%2520we%2520first%2520formulate%2520it%2520within%2520a%2520robust%2520optimization%2520framework%252C%2520and%2520then%2520develop%2520scalable%2520solution%2520methods%2520based%2520on%2520column-and-constraint%2520generation%252C%2520Benders%2520decomposition%252C%2520and%2520iterative%2520robust%2520optimization.%2520In%2520addition%252C%2520we%2520formulate%2520a%2520learning-based%2520hide-and-seek%2520game%2520to%2520further%2520analyze%2520the%2520problem%2520structure.%2520The%2520proposed%2520approaches%2520are%2520evaluated%2520against%2520classical%2520static%2520budgeted%2520robust%2520models%2520and%2520deterministic%2520worst-case%2520formulations.%2520Both%2520theoretical%2520analysis%2520and%2520computational%2520results%2520demonstrate%2520the%2520effectiveness%2520and%2520advantages%2520of%2520our%2520methodology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Optimization%20Approach%20and%20Learning%20Based%20Hide-and-Seek%20Game%20for%20Resilient%20Network%20Design&entry.906535625=Mohammad%20Khosravi%20and%20Setareh%20Maghsudi&entry.1292438233=We%20study%20the%20design%20of%20resilient%20and%20reliable%20communication%20networks%20in%20which%20a%20signal%20can%20be%20transferred%20only%20up%20to%20a%20limited%20distance%20before%20its%20quality%20falls%20below%20an%20acceptable%20threshold.%20When%20excessive%20signal%20degradation%20occurs%2C%20regeneration%20is%20required%20through%20regenerators%20installed%20at%20selected%20network%20nodes.%20In%20this%20work%2C%20both%20network%20links%20and%20nodes%20are%20subject%20to%20uncertainty.%20The%20installation%20costs%20of%20regenerators%20are%20modeled%20using%20a%20budgeted%20uncertainty%20set.%20In%20addition%2C%20link%20lengths%20follow%20a%20dynamic%20budgeted%20uncertainty%20set%20introduced%20in%20this%20paper%2C%20where%20deviations%20may%20vary%20over%20time.%20Robust%20optimization%20seeks%20solutions%20whose%20performance%20is%20guaranteed%20under%20all%20scenarios%20represented%20by%20the%20underlying%20uncertainty%20set.%20Accordingly%2C%20the%20objective%20is%20to%20identify%20a%20minimum-cost%20subset%20of%20nodes%20for%20regenerator%20deployment%20that%20ensures%20full%20network%20connectivity%2C%20even%20under%20the%20worst%20possible%20realizations%20of%20uncertainty.%20To%20solve%20the%20problem%2C%20we%20first%20formulate%20it%20within%20a%20robust%20optimization%20framework%2C%20and%20then%20develop%20scalable%20solution%20methods%20based%20on%20column-and-constraint%20generation%2C%20Benders%20decomposition%2C%20and%20iterative%20robust%20optimization.%20In%20addition%2C%20we%20formulate%20a%20learning-based%20hide-and-seek%20game%20to%20further%20analyze%20the%20problem%20structure.%20The%20proposed%20approaches%20are%20evaluated%20against%20classical%20static%20budgeted%20robust%20models%20and%20deterministic%20worst-case%20formulations.%20Both%20theoretical%20analysis%20and%20computational%20results%20demonstrate%20the%20effectiveness%20and%20advantages%20of%20our%20methodology.&entry.1838667208=http%3A//arxiv.org/abs/2602.11854v1&entry.124074799=Read"},
{"title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models", "author": "Shangchen Miao and Ningya Feng and Jialong Wu and Ye Lin and Xu He and Dong Li and Mingsheng Long", "abstract": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.", "link": "http://arxiv.org/abs/2602.11832v1", "date": "2026-02-12", "relevancy": 2.2669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JEPA-VLA%3A%20Video%20Predictive%20Embedding%20is%20Needed%20for%20VLA%20Models&body=Title%3A%20JEPA-VLA%3A%20Video%20Predictive%20Embedding%20is%20Needed%20for%20VLA%20Models%0AAuthor%3A%20Shangchen%20Miao%20and%20Ningya%20Feng%20and%20Jialong%20Wu%20and%20Ye%20Lin%20and%20Xu%20He%20and%20Dong%20Li%20and%20Mingsheng%20Long%0AAbstract%3A%20Recent%20vision-language-action%20%28VLA%29%20models%20built%20upon%20pretrained%20vision-language%20models%20%28VLMs%29%20have%20achieved%20significant%20improvements%20in%20robotic%20manipulation.%20However%2C%20current%20VLAs%20still%20suffer%20from%20low%20sample%20efficiency%20and%20limited%20generalization.%20This%20paper%20argues%20that%20these%20limitations%20are%20closely%20tied%20to%20an%20overlooked%20component%2C%20pretrained%20visual%20representation%2C%20which%20offers%20insufficient%20knowledge%20on%20both%20aspects%20of%20environment%20understanding%20and%20policy%20prior.%20Through%20an%20in-depth%20analysis%2C%20we%20find%20that%20commonly%20used%20visual%20representations%20in%20VLAs%2C%20whether%20pretrained%20via%20language-image%20contrastive%20learning%20or%20image-based%20self-supervised%20learning%2C%20remain%20inadequate%20at%20capturing%20crucial%2C%20task-relevant%20environment%20information%20and%20at%20inducing%20effective%20policy%20priors%2C%20i.e.%2C%20anticipatory%20knowledge%20of%20how%20the%20environment%20evolves%20under%20successful%20task%20execution.%20In%20contrast%2C%20we%20discover%20that%20predictive%20embeddings%20pretrained%20on%20videos%2C%20in%20particular%20V-JEPA%202%2C%20are%20adept%20at%20flexibly%20discarding%20unpredictable%20environment%20factors%20and%20encoding%20task-relevant%20temporal%20dynamics%2C%20thereby%20effectively%20compensating%20for%20key%20shortcomings%20of%20existing%20visual%20representations%20in%20VLAs.%20Building%20on%20these%20observations%2C%20we%20introduce%20JEPA-VLA%2C%20a%20simple%20yet%20effective%20approach%20that%20adaptively%20integrates%20predictive%20embeddings%20into%20existing%20VLAs.%20Our%20experiments%20demonstrate%20that%20JEPA-VLA%20yields%20substantial%20performance%20gains%20across%20a%20range%20of%20benchmarks%2C%20including%20LIBERO%2C%20LIBERO-plus%2C%20RoboTwin2.0%2C%20and%20real-robot%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJEPA-VLA%253A%2520Video%2520Predictive%2520Embedding%2520is%2520Needed%2520for%2520VLA%2520Models%26entry.906535625%3DShangchen%2520Miao%2520and%2520Ningya%2520Feng%2520and%2520Jialong%2520Wu%2520and%2520Ye%2520Lin%2520and%2520Xu%2520He%2520and%2520Dong%2520Li%2520and%2520Mingsheng%2520Long%26entry.1292438233%3DRecent%2520vision-language-action%2520%2528VLA%2529%2520models%2520built%2520upon%2520pretrained%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520significant%2520improvements%2520in%2520robotic%2520manipulation.%2520However%252C%2520current%2520VLAs%2520still%2520suffer%2520from%2520low%2520sample%2520efficiency%2520and%2520limited%2520generalization.%2520This%2520paper%2520argues%2520that%2520these%2520limitations%2520are%2520closely%2520tied%2520to%2520an%2520overlooked%2520component%252C%2520pretrained%2520visual%2520representation%252C%2520which%2520offers%2520insufficient%2520knowledge%2520on%2520both%2520aspects%2520of%2520environment%2520understanding%2520and%2520policy%2520prior.%2520Through%2520an%2520in-depth%2520analysis%252C%2520we%2520find%2520that%2520commonly%2520used%2520visual%2520representations%2520in%2520VLAs%252C%2520whether%2520pretrained%2520via%2520language-image%2520contrastive%2520learning%2520or%2520image-based%2520self-supervised%2520learning%252C%2520remain%2520inadequate%2520at%2520capturing%2520crucial%252C%2520task-relevant%2520environment%2520information%2520and%2520at%2520inducing%2520effective%2520policy%2520priors%252C%2520i.e.%252C%2520anticipatory%2520knowledge%2520of%2520how%2520the%2520environment%2520evolves%2520under%2520successful%2520task%2520execution.%2520In%2520contrast%252C%2520we%2520discover%2520that%2520predictive%2520embeddings%2520pretrained%2520on%2520videos%252C%2520in%2520particular%2520V-JEPA%25202%252C%2520are%2520adept%2520at%2520flexibly%2520discarding%2520unpredictable%2520environment%2520factors%2520and%2520encoding%2520task-relevant%2520temporal%2520dynamics%252C%2520thereby%2520effectively%2520compensating%2520for%2520key%2520shortcomings%2520of%2520existing%2520visual%2520representations%2520in%2520VLAs.%2520Building%2520on%2520these%2520observations%252C%2520we%2520introduce%2520JEPA-VLA%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520that%2520adaptively%2520integrates%2520predictive%2520embeddings%2520into%2520existing%2520VLAs.%2520Our%2520experiments%2520demonstrate%2520that%2520JEPA-VLA%2520yields%2520substantial%2520performance%2520gains%2520across%2520a%2520range%2520of%2520benchmarks%252C%2520including%2520LIBERO%252C%2520LIBERO-plus%252C%2520RoboTwin2.0%252C%2520and%2520real-robot%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JEPA-VLA%3A%20Video%20Predictive%20Embedding%20is%20Needed%20for%20VLA%20Models&entry.906535625=Shangchen%20Miao%20and%20Ningya%20Feng%20and%20Jialong%20Wu%20and%20Ye%20Lin%20and%20Xu%20He%20and%20Dong%20Li%20and%20Mingsheng%20Long&entry.1292438233=Recent%20vision-language-action%20%28VLA%29%20models%20built%20upon%20pretrained%20vision-language%20models%20%28VLMs%29%20have%20achieved%20significant%20improvements%20in%20robotic%20manipulation.%20However%2C%20current%20VLAs%20still%20suffer%20from%20low%20sample%20efficiency%20and%20limited%20generalization.%20This%20paper%20argues%20that%20these%20limitations%20are%20closely%20tied%20to%20an%20overlooked%20component%2C%20pretrained%20visual%20representation%2C%20which%20offers%20insufficient%20knowledge%20on%20both%20aspects%20of%20environment%20understanding%20and%20policy%20prior.%20Through%20an%20in-depth%20analysis%2C%20we%20find%20that%20commonly%20used%20visual%20representations%20in%20VLAs%2C%20whether%20pretrained%20via%20language-image%20contrastive%20learning%20or%20image-based%20self-supervised%20learning%2C%20remain%20inadequate%20at%20capturing%20crucial%2C%20task-relevant%20environment%20information%20and%20at%20inducing%20effective%20policy%20priors%2C%20i.e.%2C%20anticipatory%20knowledge%20of%20how%20the%20environment%20evolves%20under%20successful%20task%20execution.%20In%20contrast%2C%20we%20discover%20that%20predictive%20embeddings%20pretrained%20on%20videos%2C%20in%20particular%20V-JEPA%202%2C%20are%20adept%20at%20flexibly%20discarding%20unpredictable%20environment%20factors%20and%20encoding%20task-relevant%20temporal%20dynamics%2C%20thereby%20effectively%20compensating%20for%20key%20shortcomings%20of%20existing%20visual%20representations%20in%20VLAs.%20Building%20on%20these%20observations%2C%20we%20introduce%20JEPA-VLA%2C%20a%20simple%20yet%20effective%20approach%20that%20adaptively%20integrates%20predictive%20embeddings%20into%20existing%20VLAs.%20Our%20experiments%20demonstrate%20that%20JEPA-VLA%20yields%20substantial%20performance%20gains%20across%20a%20range%20of%20benchmarks%2C%20including%20LIBERO%2C%20LIBERO-plus%2C%20RoboTwin2.0%2C%20and%20real-robot%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.11832v1&entry.124074799=Read"},
{"title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation", "author": "Emma Hoes and K. Jonathan Klueser and Fabrizio Gilardi", "abstract": "Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.", "link": "http://arxiv.org/abs/2602.12207v1", "date": "2026-02-12", "relevancy": 2.254, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.458}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIRENA%3A%20Virtual%20Arena%20for%20Research%2C%20Education%2C%20and%20Democratic%20Innovation&body=Title%3A%20VIRENA%3A%20Virtual%20Arena%20for%20Research%2C%20Education%2C%20and%20Democratic%20Innovation%0AAuthor%3A%20Emma%20Hoes%20and%20K.%20Jonathan%20Klueser%20and%20Fabrizio%20Gilardi%0AAbstract%3A%20Digital%20platforms%20shape%20how%20people%20communicate%2C%20deliberate%2C%20and%20form%20opinions.%20Studying%20these%20dynamics%20has%20become%20increasingly%20difficult%20due%20to%20restricted%20data%20access%2C%20ethical%20constraints%20on%20real-world%20experiments%2C%20and%20limitations%20of%20existing%20research%20tools.%20VIRENA%20%28Virtual%20Arena%29%20is%20a%20platform%20that%20enables%20controlled%20experimentation%20in%20realistic%20social%20media%20environments.%20Multiple%20participants%20interact%20simultaneously%20in%20realistic%20replicas%20of%20feed-based%20platforms%20%28Instagram%2C%20Facebook%2C%20Reddit%29%20and%20messaging%20apps%20%28WhatsApp%2C%20Messenger%29.%20Large%20language%20model-powered%20AI%20agents%20participate%20alongside%20humans%20with%20configurable%20personas%20and%20realistic%20behavior.%20Researchers%20can%20manipulate%20content%20moderation%20approaches%2C%20pre-schedule%20stimulus%20content%2C%20and%20run%20experiments%20across%20conditions%20through%20a%20visual%20interface%20requiring%20no%20programming%20skills.%20VIRENA%20makes%20possible%20research%20designs%20that%20were%20previously%20impractical%3A%20studying%20human--AI%20interaction%20in%20realistic%20social%20contexts%2C%20experimentally%20comparing%20moderation%20interventions%2C%20and%20observing%20group%20deliberation%20as%20it%20unfolds.%20Built%20on%20open-source%20technologies%20that%20ensure%20data%20remain%20under%20institutional%20control%20and%20comply%20with%20data%20protection%20requirements%2C%20VIRENA%20is%20currently%20in%20use%20at%20the%20University%20of%20Zurich%20and%20available%20for%20pilot%20collaborations.%20Designed%20for%20researchers%2C%20educators%2C%20and%20public%20organizations%20alike%2C%20VIRENA%27s%20no-code%20interface%20makes%20controlled%20social%20media%20simulation%20accessible%20across%20disciplines%20and%20sectors.%20This%20paper%20documents%20its%20design%2C%20architecture%2C%20and%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIRENA%253A%2520Virtual%2520Arena%2520for%2520Research%252C%2520Education%252C%2520and%2520Democratic%2520Innovation%26entry.906535625%3DEmma%2520Hoes%2520and%2520K.%2520Jonathan%2520Klueser%2520and%2520Fabrizio%2520Gilardi%26entry.1292438233%3DDigital%2520platforms%2520shape%2520how%2520people%2520communicate%252C%2520deliberate%252C%2520and%2520form%2520opinions.%2520Studying%2520these%2520dynamics%2520has%2520become%2520increasingly%2520difficult%2520due%2520to%2520restricted%2520data%2520access%252C%2520ethical%2520constraints%2520on%2520real-world%2520experiments%252C%2520and%2520limitations%2520of%2520existing%2520research%2520tools.%2520VIRENA%2520%2528Virtual%2520Arena%2529%2520is%2520a%2520platform%2520that%2520enables%2520controlled%2520experimentation%2520in%2520realistic%2520social%2520media%2520environments.%2520Multiple%2520participants%2520interact%2520simultaneously%2520in%2520realistic%2520replicas%2520of%2520feed-based%2520platforms%2520%2528Instagram%252C%2520Facebook%252C%2520Reddit%2529%2520and%2520messaging%2520apps%2520%2528WhatsApp%252C%2520Messenger%2529.%2520Large%2520language%2520model-powered%2520AI%2520agents%2520participate%2520alongside%2520humans%2520with%2520configurable%2520personas%2520and%2520realistic%2520behavior.%2520Researchers%2520can%2520manipulate%2520content%2520moderation%2520approaches%252C%2520pre-schedule%2520stimulus%2520content%252C%2520and%2520run%2520experiments%2520across%2520conditions%2520through%2520a%2520visual%2520interface%2520requiring%2520no%2520programming%2520skills.%2520VIRENA%2520makes%2520possible%2520research%2520designs%2520that%2520were%2520previously%2520impractical%253A%2520studying%2520human--AI%2520interaction%2520in%2520realistic%2520social%2520contexts%252C%2520experimentally%2520comparing%2520moderation%2520interventions%252C%2520and%2520observing%2520group%2520deliberation%2520as%2520it%2520unfolds.%2520Built%2520on%2520open-source%2520technologies%2520that%2520ensure%2520data%2520remain%2520under%2520institutional%2520control%2520and%2520comply%2520with%2520data%2520protection%2520requirements%252C%2520VIRENA%2520is%2520currently%2520in%2520use%2520at%2520the%2520University%2520of%2520Zurich%2520and%2520available%2520for%2520pilot%2520collaborations.%2520Designed%2520for%2520researchers%252C%2520educators%252C%2520and%2520public%2520organizations%2520alike%252C%2520VIRENA%2527s%2520no-code%2520interface%2520makes%2520controlled%2520social%2520media%2520simulation%2520accessible%2520across%2520disciplines%2520and%2520sectors.%2520This%2520paper%2520documents%2520its%2520design%252C%2520architecture%252C%2520and%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIRENA%3A%20Virtual%20Arena%20for%20Research%2C%20Education%2C%20and%20Democratic%20Innovation&entry.906535625=Emma%20Hoes%20and%20K.%20Jonathan%20Klueser%20and%20Fabrizio%20Gilardi&entry.1292438233=Digital%20platforms%20shape%20how%20people%20communicate%2C%20deliberate%2C%20and%20form%20opinions.%20Studying%20these%20dynamics%20has%20become%20increasingly%20difficult%20due%20to%20restricted%20data%20access%2C%20ethical%20constraints%20on%20real-world%20experiments%2C%20and%20limitations%20of%20existing%20research%20tools.%20VIRENA%20%28Virtual%20Arena%29%20is%20a%20platform%20that%20enables%20controlled%20experimentation%20in%20realistic%20social%20media%20environments.%20Multiple%20participants%20interact%20simultaneously%20in%20realistic%20replicas%20of%20feed-based%20platforms%20%28Instagram%2C%20Facebook%2C%20Reddit%29%20and%20messaging%20apps%20%28WhatsApp%2C%20Messenger%29.%20Large%20language%20model-powered%20AI%20agents%20participate%20alongside%20humans%20with%20configurable%20personas%20and%20realistic%20behavior.%20Researchers%20can%20manipulate%20content%20moderation%20approaches%2C%20pre-schedule%20stimulus%20content%2C%20and%20run%20experiments%20across%20conditions%20through%20a%20visual%20interface%20requiring%20no%20programming%20skills.%20VIRENA%20makes%20possible%20research%20designs%20that%20were%20previously%20impractical%3A%20studying%20human--AI%20interaction%20in%20realistic%20social%20contexts%2C%20experimentally%20comparing%20moderation%20interventions%2C%20and%20observing%20group%20deliberation%20as%20it%20unfolds.%20Built%20on%20open-source%20technologies%20that%20ensure%20data%20remain%20under%20institutional%20control%20and%20comply%20with%20data%20protection%20requirements%2C%20VIRENA%20is%20currently%20in%20use%20at%20the%20University%20of%20Zurich%20and%20available%20for%20pilot%20collaborations.%20Designed%20for%20researchers%2C%20educators%2C%20and%20public%20organizations%20alike%2C%20VIRENA%27s%20no-code%20interface%20makes%20controlled%20social%20media%20simulation%20accessible%20across%20disciplines%20and%20sectors.%20This%20paper%20documents%20its%20design%2C%20architecture%2C%20and%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2602.12207v1&entry.124074799=Read"},
{"title": "HoloBrain-0 Technical Report", "author": "Xuewu Lin and Tianwei Lin and Yun Du and Hongyu Xie and Yiwei Jin and Jiawei Li and Shijie Wu and Qingze Wang and Mengdi Li and Mengao Zhao and Ziang Li and Chaodong Huang and Hongzhe Bi and Lichao Huang and Zhizhong Su", "abstract": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.", "link": "http://arxiv.org/abs/2602.12062v1", "date": "2026-02-12", "relevancy": 2.2536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloBrain-0%20Technical%20Report&body=Title%3A%20HoloBrain-0%20Technical%20Report%0AAuthor%3A%20Xuewu%20Lin%20and%20Tianwei%20Lin%20and%20Yun%20Du%20and%20Hongyu%20Xie%20and%20Yiwei%20Jin%20and%20Jiawei%20Li%20and%20Shijie%20Wu%20and%20Qingze%20Wang%20and%20Mengdi%20Li%20and%20Mengao%20Zhao%20and%20Ziang%20Li%20and%20Chaodong%20Huang%20and%20Hongzhe%20Bi%20and%20Lichao%20Huang%20and%20Zhizhong%20Su%0AAbstract%3A%20In%20this%20work%2C%20we%20introduce%20HoloBrain-0%2C%20a%20comprehensive%20Vision-Language-Action%20%28VLA%29%20framework%20that%20bridges%20the%20gap%20between%20foundation%20model%20research%20and%20reliable%20real-world%20robot%20deployment.%20The%20core%20of%20our%20system%20is%20a%20novel%20VLA%20architecture%20that%20explicitly%20incorporates%20robot%20embodiment%20priors%2C%20including%20multi-view%20camera%20parameters%20and%20kinematic%20descriptions%20%28URDF%29%2C%20to%20enhance%203D%20spatial%20reasoning%20and%20support%20diverse%20embodiments.%20We%20validate%20this%20design%20through%20a%20scalable%20%60%60pre-train%20then%20post-train%22%20paradigm%2C%20achieving%20state-of-the-art%20results%20on%20simulation%20benchmarks%20such%20as%20RoboTwin%202.0%2C%20LIBERO%2C%20and%20GenieSim%2C%20as%20well%20as%20strong%20results%20on%20challenging%20long-horizon%20real-world%20manipulation%20tasks.%20Notably%2C%20our%20efficient%200.2B-parameter%20variant%20rivals%20significantly%20larger%20baselines%2C%20enabling%20low-latency%20on-device%20deployment.%20To%20further%20accelerate%20research%20and%20practical%20adoption%2C%20we%20fully%20open-source%20the%20entire%20HoloBrain%20ecosystem%2C%20which%20includes%3A%20%281%29%20powerful%20pre-trained%20VLA%20foundations%3B%20%282%29%20post-trained%20checkpoints%20for%20multiple%20simulation%20suites%20and%20real-world%20tasks%3B%20and%20%283%29%20RoboOrchard%2C%20a%20full-stack%20VLA%20infrastructure%20for%20data%20curation%2C%20model%20training%20and%20deployment.%20Together%20with%20standardized%20data%20collection%20protocols%2C%20this%20release%20provides%20the%20community%20with%20a%20complete%2C%20reproducible%20path%20toward%20high-performance%20robotic%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloBrain-0%2520Technical%2520Report%26entry.906535625%3DXuewu%2520Lin%2520and%2520Tianwei%2520Lin%2520and%2520Yun%2520Du%2520and%2520Hongyu%2520Xie%2520and%2520Yiwei%2520Jin%2520and%2520Jiawei%2520Li%2520and%2520Shijie%2520Wu%2520and%2520Qingze%2520Wang%2520and%2520Mengdi%2520Li%2520and%2520Mengao%2520Zhao%2520and%2520Ziang%2520Li%2520and%2520Chaodong%2520Huang%2520and%2520Hongzhe%2520Bi%2520and%2520Lichao%2520Huang%2520and%2520Zhizhong%2520Su%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520introduce%2520HoloBrain-0%252C%2520a%2520comprehensive%2520Vision-Language-Action%2520%2528VLA%2529%2520framework%2520that%2520bridges%2520the%2520gap%2520between%2520foundation%2520model%2520research%2520and%2520reliable%2520real-world%2520robot%2520deployment.%2520The%2520core%2520of%2520our%2520system%2520is%2520a%2520novel%2520VLA%2520architecture%2520that%2520explicitly%2520incorporates%2520robot%2520embodiment%2520priors%252C%2520including%2520multi-view%2520camera%2520parameters%2520and%2520kinematic%2520descriptions%2520%2528URDF%2529%252C%2520to%2520enhance%25203D%2520spatial%2520reasoning%2520and%2520support%2520diverse%2520embodiments.%2520We%2520validate%2520this%2520design%2520through%2520a%2520scalable%2520%2560%2560pre-train%2520then%2520post-train%2522%2520paradigm%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520simulation%2520benchmarks%2520such%2520as%2520RoboTwin%25202.0%252C%2520LIBERO%252C%2520and%2520GenieSim%252C%2520as%2520well%2520as%2520strong%2520results%2520on%2520challenging%2520long-horizon%2520real-world%2520manipulation%2520tasks.%2520Notably%252C%2520our%2520efficient%25200.2B-parameter%2520variant%2520rivals%2520significantly%2520larger%2520baselines%252C%2520enabling%2520low-latency%2520on-device%2520deployment.%2520To%2520further%2520accelerate%2520research%2520and%2520practical%2520adoption%252C%2520we%2520fully%2520open-source%2520the%2520entire%2520HoloBrain%2520ecosystem%252C%2520which%2520includes%253A%2520%25281%2529%2520powerful%2520pre-trained%2520VLA%2520foundations%253B%2520%25282%2529%2520post-trained%2520checkpoints%2520for%2520multiple%2520simulation%2520suites%2520and%2520real-world%2520tasks%253B%2520and%2520%25283%2529%2520RoboOrchard%252C%2520a%2520full-stack%2520VLA%2520infrastructure%2520for%2520data%2520curation%252C%2520model%2520training%2520and%2520deployment.%2520Together%2520with%2520standardized%2520data%2520collection%2520protocols%252C%2520this%2520release%2520provides%2520the%2520community%2520with%2520a%2520complete%252C%2520reproducible%2520path%2520toward%2520high-performance%2520robotic%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloBrain-0%20Technical%20Report&entry.906535625=Xuewu%20Lin%20and%20Tianwei%20Lin%20and%20Yun%20Du%20and%20Hongyu%20Xie%20and%20Yiwei%20Jin%20and%20Jiawei%20Li%20and%20Shijie%20Wu%20and%20Qingze%20Wang%20and%20Mengdi%20Li%20and%20Mengao%20Zhao%20and%20Ziang%20Li%20and%20Chaodong%20Huang%20and%20Hongzhe%20Bi%20and%20Lichao%20Huang%20and%20Zhizhong%20Su&entry.1292438233=In%20this%20work%2C%20we%20introduce%20HoloBrain-0%2C%20a%20comprehensive%20Vision-Language-Action%20%28VLA%29%20framework%20that%20bridges%20the%20gap%20between%20foundation%20model%20research%20and%20reliable%20real-world%20robot%20deployment.%20The%20core%20of%20our%20system%20is%20a%20novel%20VLA%20architecture%20that%20explicitly%20incorporates%20robot%20embodiment%20priors%2C%20including%20multi-view%20camera%20parameters%20and%20kinematic%20descriptions%20%28URDF%29%2C%20to%20enhance%203D%20spatial%20reasoning%20and%20support%20diverse%20embodiments.%20We%20validate%20this%20design%20through%20a%20scalable%20%60%60pre-train%20then%20post-train%22%20paradigm%2C%20achieving%20state-of-the-art%20results%20on%20simulation%20benchmarks%20such%20as%20RoboTwin%202.0%2C%20LIBERO%2C%20and%20GenieSim%2C%20as%20well%20as%20strong%20results%20on%20challenging%20long-horizon%20real-world%20manipulation%20tasks.%20Notably%2C%20our%20efficient%200.2B-parameter%20variant%20rivals%20significantly%20larger%20baselines%2C%20enabling%20low-latency%20on-device%20deployment.%20To%20further%20accelerate%20research%20and%20practical%20adoption%2C%20we%20fully%20open-source%20the%20entire%20HoloBrain%20ecosystem%2C%20which%20includes%3A%20%281%29%20powerful%20pre-trained%20VLA%20foundations%3B%20%282%29%20post-trained%20checkpoints%20for%20multiple%20simulation%20suites%20and%20real-world%20tasks%3B%20and%20%283%29%20RoboOrchard%2C%20a%20full-stack%20VLA%20infrastructure%20for%20data%20curation%2C%20model%20training%20and%20deployment.%20Together%20with%20standardized%20data%20collection%20protocols%2C%20this%20release%20provides%20the%20community%20with%20a%20complete%2C%20reproducible%20path%20toward%20high-performance%20robotic%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2602.12062v1&entry.124074799=Read"},
{"title": "Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors", "author": "Arian Khorasani and Nathaniel Chen and Yug D Oswal and Akshat Santhana Gopalan and Egemen Kolemen and Ravid Shwartz-Ziv", "abstract": "How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.", "link": "http://arxiv.org/abs/2602.00315v2", "date": "2026-02-12", "relevancy": 2.2529, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5857}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5808}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Loss%20Curve%3A%20Scaling%20Laws%2C%20Active%20Learning%2C%20and%20the%20Limits%20of%20Learning%20from%20Exact%20Posteriors&body=Title%3A%20Beyond%20the%20Loss%20Curve%3A%20Scaling%20Laws%2C%20Active%20Learning%2C%20and%20the%20Limits%20of%20Learning%20from%20Exact%20Posteriors%0AAuthor%3A%20Arian%20Khorasani%20and%20Nathaniel%20Chen%20and%20Yug%20D%20Oswal%20and%20Akshat%20Santhana%20Gopalan%20and%20Egemen%20Kolemen%20and%20Ravid%20Shwartz-Ziv%0AAbstract%3A%20How%20close%20are%20neural%20networks%20to%20the%20best%20they%20could%20possibly%20do%3F%20Standard%20benchmarks%20cannot%20answer%20this%20because%20they%20lack%20access%20to%20the%20true%20posterior%20p%28y%7Cx%29.%20We%20use%20class-conditional%20normalizing%20flows%20as%20oracles%20that%20make%20exact%20posteriors%20tractable%20on%20realistic%20images%20%28AFHQ%2C%20ImageNet%29.%20This%20enables%20five%20lines%20of%20investigation.%20Scaling%20laws%3A%20Prediction%20error%20decomposes%20into%20irreducible%20aleatoric%20uncertainty%20and%20reducible%20epistemic%20error%3B%20the%20epistemic%20component%20follows%20a%20power%20law%20in%20dataset%20size%2C%20continuing%20to%20shrink%20even%20when%20total%20loss%20plateaus.%20Limits%20of%20learning%3A%20The%20aleatoric%20floor%20is%20exactly%20measurable%2C%20and%20architectures%20differ%20markedly%20in%20how%20they%20approach%20it%3A%20ResNets%20exhibit%20clean%20power-law%20scaling%20while%20Vision%20Transformers%20stall%20in%20low-data%20regimes.%20Soft%20labels%3A%20Oracle%20posteriors%20contain%20learnable%20structure%20beyond%20class%20labels%3A%20training%20with%20exact%20posteriors%20outperforms%20hard%20labels%20and%20yields%20near-perfect%20calibration.%20Distribution%20shift%3A%20The%20oracle%20computes%20exact%20KL%20divergence%20of%20controlled%20perturbations%2C%20revealing%20that%20shift%20type%20matters%20more%20than%20shift%20magnitude%3A%20class%20imbalance%20barely%20affects%20accuracy%20at%20divergence%20values%20where%20input%20noise%20causes%20catastrophic%20degradation.%20Active%20learning%3A%20Exact%20epistemic%20uncertainty%20distinguishes%20genuinely%20informative%20samples%20from%20inherently%20ambiguous%20ones%2C%20improving%20sample%20efficiency.%20Our%20framework%20reveals%20that%20standard%20metrics%20hide%20ongoing%20learning%2C%20mask%20architectural%20differences%2C%20and%20cannot%20diagnose%20the%20nature%20of%20distribution%20shift.%0ALink%3A%20http%3A//arxiv.org/abs/2602.00315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Loss%2520Curve%253A%2520Scaling%2520Laws%252C%2520Active%2520Learning%252C%2520and%2520the%2520Limits%2520of%2520Learning%2520from%2520Exact%2520Posteriors%26entry.906535625%3DArian%2520Khorasani%2520and%2520Nathaniel%2520Chen%2520and%2520Yug%2520D%2520Oswal%2520and%2520Akshat%2520Santhana%2520Gopalan%2520and%2520Egemen%2520Kolemen%2520and%2520Ravid%2520Shwartz-Ziv%26entry.1292438233%3DHow%2520close%2520are%2520neural%2520networks%2520to%2520the%2520best%2520they%2520could%2520possibly%2520do%253F%2520Standard%2520benchmarks%2520cannot%2520answer%2520this%2520because%2520they%2520lack%2520access%2520to%2520the%2520true%2520posterior%2520p%2528y%257Cx%2529.%2520We%2520use%2520class-conditional%2520normalizing%2520flows%2520as%2520oracles%2520that%2520make%2520exact%2520posteriors%2520tractable%2520on%2520realistic%2520images%2520%2528AFHQ%252C%2520ImageNet%2529.%2520This%2520enables%2520five%2520lines%2520of%2520investigation.%2520Scaling%2520laws%253A%2520Prediction%2520error%2520decomposes%2520into%2520irreducible%2520aleatoric%2520uncertainty%2520and%2520reducible%2520epistemic%2520error%253B%2520the%2520epistemic%2520component%2520follows%2520a%2520power%2520law%2520in%2520dataset%2520size%252C%2520continuing%2520to%2520shrink%2520even%2520when%2520total%2520loss%2520plateaus.%2520Limits%2520of%2520learning%253A%2520The%2520aleatoric%2520floor%2520is%2520exactly%2520measurable%252C%2520and%2520architectures%2520differ%2520markedly%2520in%2520how%2520they%2520approach%2520it%253A%2520ResNets%2520exhibit%2520clean%2520power-law%2520scaling%2520while%2520Vision%2520Transformers%2520stall%2520in%2520low-data%2520regimes.%2520Soft%2520labels%253A%2520Oracle%2520posteriors%2520contain%2520learnable%2520structure%2520beyond%2520class%2520labels%253A%2520training%2520with%2520exact%2520posteriors%2520outperforms%2520hard%2520labels%2520and%2520yields%2520near-perfect%2520calibration.%2520Distribution%2520shift%253A%2520The%2520oracle%2520computes%2520exact%2520KL%2520divergence%2520of%2520controlled%2520perturbations%252C%2520revealing%2520that%2520shift%2520type%2520matters%2520more%2520than%2520shift%2520magnitude%253A%2520class%2520imbalance%2520barely%2520affects%2520accuracy%2520at%2520divergence%2520values%2520where%2520input%2520noise%2520causes%2520catastrophic%2520degradation.%2520Active%2520learning%253A%2520Exact%2520epistemic%2520uncertainty%2520distinguishes%2520genuinely%2520informative%2520samples%2520from%2520inherently%2520ambiguous%2520ones%252C%2520improving%2520sample%2520efficiency.%2520Our%2520framework%2520reveals%2520that%2520standard%2520metrics%2520hide%2520ongoing%2520learning%252C%2520mask%2520architectural%2520differences%252C%2520and%2520cannot%2520diagnose%2520the%2520nature%2520of%2520distribution%2520shift.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.00315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Loss%20Curve%3A%20Scaling%20Laws%2C%20Active%20Learning%2C%20and%20the%20Limits%20of%20Learning%20from%20Exact%20Posteriors&entry.906535625=Arian%20Khorasani%20and%20Nathaniel%20Chen%20and%20Yug%20D%20Oswal%20and%20Akshat%20Santhana%20Gopalan%20and%20Egemen%20Kolemen%20and%20Ravid%20Shwartz-Ziv&entry.1292438233=How%20close%20are%20neural%20networks%20to%20the%20best%20they%20could%20possibly%20do%3F%20Standard%20benchmarks%20cannot%20answer%20this%20because%20they%20lack%20access%20to%20the%20true%20posterior%20p%28y%7Cx%29.%20We%20use%20class-conditional%20normalizing%20flows%20as%20oracles%20that%20make%20exact%20posteriors%20tractable%20on%20realistic%20images%20%28AFHQ%2C%20ImageNet%29.%20This%20enables%20five%20lines%20of%20investigation.%20Scaling%20laws%3A%20Prediction%20error%20decomposes%20into%20irreducible%20aleatoric%20uncertainty%20and%20reducible%20epistemic%20error%3B%20the%20epistemic%20component%20follows%20a%20power%20law%20in%20dataset%20size%2C%20continuing%20to%20shrink%20even%20when%20total%20loss%20plateaus.%20Limits%20of%20learning%3A%20The%20aleatoric%20floor%20is%20exactly%20measurable%2C%20and%20architectures%20differ%20markedly%20in%20how%20they%20approach%20it%3A%20ResNets%20exhibit%20clean%20power-law%20scaling%20while%20Vision%20Transformers%20stall%20in%20low-data%20regimes.%20Soft%20labels%3A%20Oracle%20posteriors%20contain%20learnable%20structure%20beyond%20class%20labels%3A%20training%20with%20exact%20posteriors%20outperforms%20hard%20labels%20and%20yields%20near-perfect%20calibration.%20Distribution%20shift%3A%20The%20oracle%20computes%20exact%20KL%20divergence%20of%20controlled%20perturbations%2C%20revealing%20that%20shift%20type%20matters%20more%20than%20shift%20magnitude%3A%20class%20imbalance%20barely%20affects%20accuracy%20at%20divergence%20values%20where%20input%20noise%20causes%20catastrophic%20degradation.%20Active%20learning%3A%20Exact%20epistemic%20uncertainty%20distinguishes%20genuinely%20informative%20samples%20from%20inherently%20ambiguous%20ones%2C%20improving%20sample%20efficiency.%20Our%20framework%20reveals%20that%20standard%20metrics%20hide%20ongoing%20learning%2C%20mask%20architectural%20differences%2C%20and%20cannot%20diagnose%20the%20nature%20of%20distribution%20shift.&entry.1838667208=http%3A//arxiv.org/abs/2602.00315v2&entry.124074799=Read"},
{"title": "UPDA: Unsupervised Progressive Domain Adaptation for No-Reference Point Cloud Quality Assessment", "author": "Bingxu Xie and Fang Zhou and Jincan Wu and Yonghui Liu and Weiqing Li and Zhiyong Su", "abstract": "While no-reference point cloud quality assessment (NR-PCQA) approaches have achieved significant progress over the past decade, their performance often degrades substantially when a distribution gap exists between the training (source domain) and testing (target domain) data. However, to date, limited attention has been paid to transferring NR-PCQA models across domains. To address this challenge, we propose the first unsupervised progressive domain adaptation (UPDA) framework for NR-PCQA, which introduces a two-stage coarse-to-fine alignment paradigm to address domain shifts. At the coarse-grained stage, a discrepancy-aware coarse-grained alignment method is designed to capture relative quality relationships between cross-domain samples through a novel quality-discrepancy-aware hybrid loss, circumventing the challenges of direct absolute feature alignment. At the fine-grained stage, a perception fusion fine-grained alignment approach with symmetric feature fusion is developed to identify domain-invariant features, while a conditional discriminator selectively enhances the transfer of quality-relevant features. Extensive experiments demonstrate that the proposed UPDA effectively enhances the performance of NR-PCQA methods in cross-domain scenarios, validating its practical applicability. The code is available at https://github.com/yokeno1/UPDA-main.", "link": "http://arxiv.org/abs/2602.11969v1", "date": "2026-02-12", "relevancy": 2.2466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5752}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5632}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UPDA%3A%20Unsupervised%20Progressive%20Domain%20Adaptation%20for%20No-Reference%20Point%20Cloud%20Quality%20Assessment&body=Title%3A%20UPDA%3A%20Unsupervised%20Progressive%20Domain%20Adaptation%20for%20No-Reference%20Point%20Cloud%20Quality%20Assessment%0AAuthor%3A%20Bingxu%20Xie%20and%20Fang%20Zhou%20and%20Jincan%20Wu%20and%20Yonghui%20Liu%20and%20Weiqing%20Li%20and%20Zhiyong%20Su%0AAbstract%3A%20While%20no-reference%20point%20cloud%20quality%20assessment%20%28NR-PCQA%29%20approaches%20have%20achieved%20significant%20progress%20over%20the%20past%20decade%2C%20their%20performance%20often%20degrades%20substantially%20when%20a%20distribution%20gap%20exists%20between%20the%20training%20%28source%20domain%29%20and%20testing%20%28target%20domain%29%20data.%20However%2C%20to%20date%2C%20limited%20attention%20has%20been%20paid%20to%20transferring%20NR-PCQA%20models%20across%20domains.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20first%20unsupervised%20progressive%20domain%20adaptation%20%28UPDA%29%20framework%20for%20NR-PCQA%2C%20which%20introduces%20a%20two-stage%20coarse-to-fine%20alignment%20paradigm%20to%20address%20domain%20shifts.%20At%20the%20coarse-grained%20stage%2C%20a%20discrepancy-aware%20coarse-grained%20alignment%20method%20is%20designed%20to%20capture%20relative%20quality%20relationships%20between%20cross-domain%20samples%20through%20a%20novel%20quality-discrepancy-aware%20hybrid%20loss%2C%20circumventing%20the%20challenges%20of%20direct%20absolute%20feature%20alignment.%20At%20the%20fine-grained%20stage%2C%20a%20perception%20fusion%20fine-grained%20alignment%20approach%20with%20symmetric%20feature%20fusion%20is%20developed%20to%20identify%20domain-invariant%20features%2C%20while%20a%20conditional%20discriminator%20selectively%20enhances%20the%20transfer%20of%20quality-relevant%20features.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20UPDA%20effectively%20enhances%20the%20performance%20of%20NR-PCQA%20methods%20in%20cross-domain%20scenarios%2C%20validating%20its%20practical%20applicability.%20The%20code%20is%20available%20at%20https%3A//github.com/yokeno1/UPDA-main.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUPDA%253A%2520Unsupervised%2520Progressive%2520Domain%2520Adaptation%2520for%2520No-Reference%2520Point%2520Cloud%2520Quality%2520Assessment%26entry.906535625%3DBingxu%2520Xie%2520and%2520Fang%2520Zhou%2520and%2520Jincan%2520Wu%2520and%2520Yonghui%2520Liu%2520and%2520Weiqing%2520Li%2520and%2520Zhiyong%2520Su%26entry.1292438233%3DWhile%2520no-reference%2520point%2520cloud%2520quality%2520assessment%2520%2528NR-PCQA%2529%2520approaches%2520have%2520achieved%2520significant%2520progress%2520over%2520the%2520past%2520decade%252C%2520their%2520performance%2520often%2520degrades%2520substantially%2520when%2520a%2520distribution%2520gap%2520exists%2520between%2520the%2520training%2520%2528source%2520domain%2529%2520and%2520testing%2520%2528target%2520domain%2529%2520data.%2520However%252C%2520to%2520date%252C%2520limited%2520attention%2520has%2520been%2520paid%2520to%2520transferring%2520NR-PCQA%2520models%2520across%2520domains.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520first%2520unsupervised%2520progressive%2520domain%2520adaptation%2520%2528UPDA%2529%2520framework%2520for%2520NR-PCQA%252C%2520which%2520introduces%2520a%2520two-stage%2520coarse-to-fine%2520alignment%2520paradigm%2520to%2520address%2520domain%2520shifts.%2520At%2520the%2520coarse-grained%2520stage%252C%2520a%2520discrepancy-aware%2520coarse-grained%2520alignment%2520method%2520is%2520designed%2520to%2520capture%2520relative%2520quality%2520relationships%2520between%2520cross-domain%2520samples%2520through%2520a%2520novel%2520quality-discrepancy-aware%2520hybrid%2520loss%252C%2520circumventing%2520the%2520challenges%2520of%2520direct%2520absolute%2520feature%2520alignment.%2520At%2520the%2520fine-grained%2520stage%252C%2520a%2520perception%2520fusion%2520fine-grained%2520alignment%2520approach%2520with%2520symmetric%2520feature%2520fusion%2520is%2520developed%2520to%2520identify%2520domain-invariant%2520features%252C%2520while%2520a%2520conditional%2520discriminator%2520selectively%2520enhances%2520the%2520transfer%2520of%2520quality-relevant%2520features.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520UPDA%2520effectively%2520enhances%2520the%2520performance%2520of%2520NR-PCQA%2520methods%2520in%2520cross-domain%2520scenarios%252C%2520validating%2520its%2520practical%2520applicability.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/yokeno1/UPDA-main.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UPDA%3A%20Unsupervised%20Progressive%20Domain%20Adaptation%20for%20No-Reference%20Point%20Cloud%20Quality%20Assessment&entry.906535625=Bingxu%20Xie%20and%20Fang%20Zhou%20and%20Jincan%20Wu%20and%20Yonghui%20Liu%20and%20Weiqing%20Li%20and%20Zhiyong%20Su&entry.1292438233=While%20no-reference%20point%20cloud%20quality%20assessment%20%28NR-PCQA%29%20approaches%20have%20achieved%20significant%20progress%20over%20the%20past%20decade%2C%20their%20performance%20often%20degrades%20substantially%20when%20a%20distribution%20gap%20exists%20between%20the%20training%20%28source%20domain%29%20and%20testing%20%28target%20domain%29%20data.%20However%2C%20to%20date%2C%20limited%20attention%20has%20been%20paid%20to%20transferring%20NR-PCQA%20models%20across%20domains.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20first%20unsupervised%20progressive%20domain%20adaptation%20%28UPDA%29%20framework%20for%20NR-PCQA%2C%20which%20introduces%20a%20two-stage%20coarse-to-fine%20alignment%20paradigm%20to%20address%20domain%20shifts.%20At%20the%20coarse-grained%20stage%2C%20a%20discrepancy-aware%20coarse-grained%20alignment%20method%20is%20designed%20to%20capture%20relative%20quality%20relationships%20between%20cross-domain%20samples%20through%20a%20novel%20quality-discrepancy-aware%20hybrid%20loss%2C%20circumventing%20the%20challenges%20of%20direct%20absolute%20feature%20alignment.%20At%20the%20fine-grained%20stage%2C%20a%20perception%20fusion%20fine-grained%20alignment%20approach%20with%20symmetric%20feature%20fusion%20is%20developed%20to%20identify%20domain-invariant%20features%2C%20while%20a%20conditional%20discriminator%20selectively%20enhances%20the%20transfer%20of%20quality-relevant%20features.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20UPDA%20effectively%20enhances%20the%20performance%20of%20NR-PCQA%20methods%20in%20cross-domain%20scenarios%2C%20validating%20its%20practical%20applicability.%20The%20code%20is%20available%20at%20https%3A//github.com/yokeno1/UPDA-main.&entry.1838667208=http%3A//arxiv.org/abs/2602.11969v1&entry.124074799=Read"},
{"title": "Decentralized Multi-Robot Obstacle Detection and Tracking in a Maritime Scenario", "author": "Muhammad Farhan Ahmed and Vincent Fr\u00e9mont", "abstract": "Autonomous aerial-surface robot teams are promising for maritime monitoring. Robust deployment requires reliable perception over reflective water and scalable coordination under limited communication. We present a decentralized multi-robot framework for detecting and tracking floating containers using multiple UAVs cooperating with an autonomous surface vessel. Each UAV performs YOLOv8 and stereo-disparity-based visual detection, then tracks targets with per-object EKFs using uncertainty-aware data association. Compact track summaries are exchanged and fused conservatively via covariance intersection, ensuring consistency under unknown correlations. An information-driven assignment module allocates targets and selects UAV hover viewpoints by trading expected uncertainty reduction against travel effort and safety separation. Simulation results in a maritime scenario demonstrate improved coverage, localization accuracy, and tracking consistency while maintaining modest communication requirements.", "link": "http://arxiv.org/abs/2602.12012v1", "date": "2026-02-12", "relevancy": 2.2331, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5717}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5675}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Multi-Robot%20Obstacle%20Detection%20and%20Tracking%20in%20a%20Maritime%20Scenario&body=Title%3A%20Decentralized%20Multi-Robot%20Obstacle%20Detection%20and%20Tracking%20in%20a%20Maritime%20Scenario%0AAuthor%3A%20Muhammad%20Farhan%20Ahmed%20and%20Vincent%20Fr%C3%A9mont%0AAbstract%3A%20Autonomous%20aerial-surface%20robot%20teams%20are%20promising%20for%20maritime%20monitoring.%20Robust%20deployment%20requires%20reliable%20perception%20over%20reflective%20water%20and%20scalable%20coordination%20under%20limited%20communication.%20We%20present%20a%20decentralized%20multi-robot%20framework%20for%20detecting%20and%20tracking%20floating%20containers%20using%20multiple%20UAVs%20cooperating%20with%20an%20autonomous%20surface%20vessel.%20Each%20UAV%20performs%20YOLOv8%20and%20stereo-disparity-based%20visual%20detection%2C%20then%20tracks%20targets%20with%20per-object%20EKFs%20using%20uncertainty-aware%20data%20association.%20Compact%20track%20summaries%20are%20exchanged%20and%20fused%20conservatively%20via%20covariance%20intersection%2C%20ensuring%20consistency%20under%20unknown%20correlations.%20An%20information-driven%20assignment%20module%20allocates%20targets%20and%20selects%20UAV%20hover%20viewpoints%20by%20trading%20expected%20uncertainty%20reduction%20against%20travel%20effort%20and%20safety%20separation.%20Simulation%20results%20in%20a%20maritime%20scenario%20demonstrate%20improved%20coverage%2C%20localization%20accuracy%2C%20and%20tracking%20consistency%20while%20maintaining%20modest%20communication%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Multi-Robot%2520Obstacle%2520Detection%2520and%2520Tracking%2520in%2520a%2520Maritime%2520Scenario%26entry.906535625%3DMuhammad%2520Farhan%2520Ahmed%2520and%2520Vincent%2520Fr%25C3%25A9mont%26entry.1292438233%3DAutonomous%2520aerial-surface%2520robot%2520teams%2520are%2520promising%2520for%2520maritime%2520monitoring.%2520Robust%2520deployment%2520requires%2520reliable%2520perception%2520over%2520reflective%2520water%2520and%2520scalable%2520coordination%2520under%2520limited%2520communication.%2520We%2520present%2520a%2520decentralized%2520multi-robot%2520framework%2520for%2520detecting%2520and%2520tracking%2520floating%2520containers%2520using%2520multiple%2520UAVs%2520cooperating%2520with%2520an%2520autonomous%2520surface%2520vessel.%2520Each%2520UAV%2520performs%2520YOLOv8%2520and%2520stereo-disparity-based%2520visual%2520detection%252C%2520then%2520tracks%2520targets%2520with%2520per-object%2520EKFs%2520using%2520uncertainty-aware%2520data%2520association.%2520Compact%2520track%2520summaries%2520are%2520exchanged%2520and%2520fused%2520conservatively%2520via%2520covariance%2520intersection%252C%2520ensuring%2520consistency%2520under%2520unknown%2520correlations.%2520An%2520information-driven%2520assignment%2520module%2520allocates%2520targets%2520and%2520selects%2520UAV%2520hover%2520viewpoints%2520by%2520trading%2520expected%2520uncertainty%2520reduction%2520against%2520travel%2520effort%2520and%2520safety%2520separation.%2520Simulation%2520results%2520in%2520a%2520maritime%2520scenario%2520demonstrate%2520improved%2520coverage%252C%2520localization%2520accuracy%252C%2520and%2520tracking%2520consistency%2520while%2520maintaining%2520modest%2520communication%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Multi-Robot%20Obstacle%20Detection%20and%20Tracking%20in%20a%20Maritime%20Scenario&entry.906535625=Muhammad%20Farhan%20Ahmed%20and%20Vincent%20Fr%C3%A9mont&entry.1292438233=Autonomous%20aerial-surface%20robot%20teams%20are%20promising%20for%20maritime%20monitoring.%20Robust%20deployment%20requires%20reliable%20perception%20over%20reflective%20water%20and%20scalable%20coordination%20under%20limited%20communication.%20We%20present%20a%20decentralized%20multi-robot%20framework%20for%20detecting%20and%20tracking%20floating%20containers%20using%20multiple%20UAVs%20cooperating%20with%20an%20autonomous%20surface%20vessel.%20Each%20UAV%20performs%20YOLOv8%20and%20stereo-disparity-based%20visual%20detection%2C%20then%20tracks%20targets%20with%20per-object%20EKFs%20using%20uncertainty-aware%20data%20association.%20Compact%20track%20summaries%20are%20exchanged%20and%20fused%20conservatively%20via%20covariance%20intersection%2C%20ensuring%20consistency%20under%20unknown%20correlations.%20An%20information-driven%20assignment%20module%20allocates%20targets%20and%20selects%20UAV%20hover%20viewpoints%20by%20trading%20expected%20uncertainty%20reduction%20against%20travel%20effort%20and%20safety%20separation.%20Simulation%20results%20in%20a%20maritime%20scenario%20demonstrate%20improved%20coverage%2C%20localization%20accuracy%2C%20and%20tracking%20consistency%20while%20maintaining%20modest%20communication%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2602.12012v1&entry.124074799=Read"},
{"title": "Accelerating nuclear-norm regularized low-rank matrix optimization through Burer-Monteiro decomposition", "author": "Ching-pei Lee and Ling Liang and Tianyun Tang and Kim-Chuan Toh", "abstract": "This work proposes a rapid algorithm, BM-Global, for nuclear-norm-regularized convex and low-rank matrix optimization problems. BM-Global efficiently decreases the objective value via low-cost steps leveraging the nonconvex but smooth Burer-Monteiro (BM) decomposition, while effectively escapes saddle points and spurious local minima ubiquitous in the BM form to obtain guarantees of fast convergence rates to the global optima of the original nuclear-norm-regularized problem through aperiodic inexact proximal gradient steps on it. The proposed approach adaptively adjusts the rank for the BM decomposition and can provably identify an optimal rank for the BM decomposition problem automatically in the course of optimization through tools of manifold identification. BM-Global hence also spends significantly less time on parameter tuning than existing matrix-factorization methods, which require an exhaustive search for finding this optimal rank. Extensive experiments on real-world large-scale problems of recommendation systems, regularized kernel estimation, and molecular conformation confirm that BM-Global can indeed effectively escapes spurious local minima at which existing BM approaches are stuck, and is a magnitude faster than state-of-the-art algorithms for low-rank matrix optimization problems involving a nuclear-norm regularizer. Based on this research, we have released an open-source package of the proposed BM-Global at https://www.github.com/leepei/BM-Global/.", "link": "http://arxiv.org/abs/2204.14067v4", "date": "2026-02-12", "relevancy": 2.2298, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4516}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.449}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20nuclear-norm%20regularized%20low-rank%20matrix%20optimization%20through%20Burer-Monteiro%20decomposition&body=Title%3A%20Accelerating%20nuclear-norm%20regularized%20low-rank%20matrix%20optimization%20through%20Burer-Monteiro%20decomposition%0AAuthor%3A%20Ching-pei%20Lee%20and%20Ling%20Liang%20and%20Tianyun%20Tang%20and%20Kim-Chuan%20Toh%0AAbstract%3A%20This%20work%20proposes%20a%20rapid%20algorithm%2C%20BM-Global%2C%20for%20nuclear-norm-regularized%20convex%20and%20low-rank%20matrix%20optimization%20problems.%20BM-Global%20efficiently%20decreases%20the%20objective%20value%20via%20low-cost%20steps%20leveraging%20the%20nonconvex%20but%20smooth%20Burer-Monteiro%20%28BM%29%20decomposition%2C%20while%20effectively%20escapes%20saddle%20points%20and%20spurious%20local%20minima%20ubiquitous%20in%20the%20BM%20form%20to%20obtain%20guarantees%20of%20fast%20convergence%20rates%20to%20the%20global%20optima%20of%20the%20original%20nuclear-norm-regularized%20problem%20through%20aperiodic%20inexact%20proximal%20gradient%20steps%20on%20it.%20The%20proposed%20approach%20adaptively%20adjusts%20the%20rank%20for%20the%20BM%20decomposition%20and%20can%20provably%20identify%20an%20optimal%20rank%20for%20the%20BM%20decomposition%20problem%20automatically%20in%20the%20course%20of%20optimization%20through%20tools%20of%20manifold%20identification.%20BM-Global%20hence%20also%20spends%20significantly%20less%20time%20on%20parameter%20tuning%20than%20existing%20matrix-factorization%20methods%2C%20which%20require%20an%20exhaustive%20search%20for%20finding%20this%20optimal%20rank.%20Extensive%20experiments%20on%20real-world%20large-scale%20problems%20of%20recommendation%20systems%2C%20regularized%20kernel%20estimation%2C%20and%20molecular%20conformation%20confirm%20that%20BM-Global%20can%20indeed%20effectively%20escapes%20spurious%20local%20minima%20at%20which%20existing%20BM%20approaches%20are%20stuck%2C%20and%20is%20a%20magnitude%20faster%20than%20state-of-the-art%20algorithms%20for%20low-rank%20matrix%20optimization%20problems%20involving%20a%20nuclear-norm%20regularizer.%20Based%20on%20this%20research%2C%20we%20have%20released%20an%20open-source%20package%20of%20the%20proposed%20BM-Global%20at%20https%3A//www.github.com/leepei/BM-Global/.%0ALink%3A%20http%3A//arxiv.org/abs/2204.14067v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520nuclear-norm%2520regularized%2520low-rank%2520matrix%2520optimization%2520through%2520Burer-Monteiro%2520decomposition%26entry.906535625%3DChing-pei%2520Lee%2520and%2520Ling%2520Liang%2520and%2520Tianyun%2520Tang%2520and%2520Kim-Chuan%2520Toh%26entry.1292438233%3DThis%2520work%2520proposes%2520a%2520rapid%2520algorithm%252C%2520BM-Global%252C%2520for%2520nuclear-norm-regularized%2520convex%2520and%2520low-rank%2520matrix%2520optimization%2520problems.%2520BM-Global%2520efficiently%2520decreases%2520the%2520objective%2520value%2520via%2520low-cost%2520steps%2520leveraging%2520the%2520nonconvex%2520but%2520smooth%2520Burer-Monteiro%2520%2528BM%2529%2520decomposition%252C%2520while%2520effectively%2520escapes%2520saddle%2520points%2520and%2520spurious%2520local%2520minima%2520ubiquitous%2520in%2520the%2520BM%2520form%2520to%2520obtain%2520guarantees%2520of%2520fast%2520convergence%2520rates%2520to%2520the%2520global%2520optima%2520of%2520the%2520original%2520nuclear-norm-regularized%2520problem%2520through%2520aperiodic%2520inexact%2520proximal%2520gradient%2520steps%2520on%2520it.%2520The%2520proposed%2520approach%2520adaptively%2520adjusts%2520the%2520rank%2520for%2520the%2520BM%2520decomposition%2520and%2520can%2520provably%2520identify%2520an%2520optimal%2520rank%2520for%2520the%2520BM%2520decomposition%2520problem%2520automatically%2520in%2520the%2520course%2520of%2520optimization%2520through%2520tools%2520of%2520manifold%2520identification.%2520BM-Global%2520hence%2520also%2520spends%2520significantly%2520less%2520time%2520on%2520parameter%2520tuning%2520than%2520existing%2520matrix-factorization%2520methods%252C%2520which%2520require%2520an%2520exhaustive%2520search%2520for%2520finding%2520this%2520optimal%2520rank.%2520Extensive%2520experiments%2520on%2520real-world%2520large-scale%2520problems%2520of%2520recommendation%2520systems%252C%2520regularized%2520kernel%2520estimation%252C%2520and%2520molecular%2520conformation%2520confirm%2520that%2520BM-Global%2520can%2520indeed%2520effectively%2520escapes%2520spurious%2520local%2520minima%2520at%2520which%2520existing%2520BM%2520approaches%2520are%2520stuck%252C%2520and%2520is%2520a%2520magnitude%2520faster%2520than%2520state-of-the-art%2520algorithms%2520for%2520low-rank%2520matrix%2520optimization%2520problems%2520involving%2520a%2520nuclear-norm%2520regularizer.%2520Based%2520on%2520this%2520research%252C%2520we%2520have%2520released%2520an%2520open-source%2520package%2520of%2520the%2520proposed%2520BM-Global%2520at%2520https%253A//www.github.com/leepei/BM-Global/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.14067v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20nuclear-norm%20regularized%20low-rank%20matrix%20optimization%20through%20Burer-Monteiro%20decomposition&entry.906535625=Ching-pei%20Lee%20and%20Ling%20Liang%20and%20Tianyun%20Tang%20and%20Kim-Chuan%20Toh&entry.1292438233=This%20work%20proposes%20a%20rapid%20algorithm%2C%20BM-Global%2C%20for%20nuclear-norm-regularized%20convex%20and%20low-rank%20matrix%20optimization%20problems.%20BM-Global%20efficiently%20decreases%20the%20objective%20value%20via%20low-cost%20steps%20leveraging%20the%20nonconvex%20but%20smooth%20Burer-Monteiro%20%28BM%29%20decomposition%2C%20while%20effectively%20escapes%20saddle%20points%20and%20spurious%20local%20minima%20ubiquitous%20in%20the%20BM%20form%20to%20obtain%20guarantees%20of%20fast%20convergence%20rates%20to%20the%20global%20optima%20of%20the%20original%20nuclear-norm-regularized%20problem%20through%20aperiodic%20inexact%20proximal%20gradient%20steps%20on%20it.%20The%20proposed%20approach%20adaptively%20adjusts%20the%20rank%20for%20the%20BM%20decomposition%20and%20can%20provably%20identify%20an%20optimal%20rank%20for%20the%20BM%20decomposition%20problem%20automatically%20in%20the%20course%20of%20optimization%20through%20tools%20of%20manifold%20identification.%20BM-Global%20hence%20also%20spends%20significantly%20less%20time%20on%20parameter%20tuning%20than%20existing%20matrix-factorization%20methods%2C%20which%20require%20an%20exhaustive%20search%20for%20finding%20this%20optimal%20rank.%20Extensive%20experiments%20on%20real-world%20large-scale%20problems%20of%20recommendation%20systems%2C%20regularized%20kernel%20estimation%2C%20and%20molecular%20conformation%20confirm%20that%20BM-Global%20can%20indeed%20effectively%20escapes%20spurious%20local%20minima%20at%20which%20existing%20BM%20approaches%20are%20stuck%2C%20and%20is%20a%20magnitude%20faster%20than%20state-of-the-art%20algorithms%20for%20low-rank%20matrix%20optimization%20problems%20involving%20a%20nuclear-norm%20regularizer.%20Based%20on%20this%20research%2C%20we%20have%20released%20an%20open-source%20package%20of%20the%20proposed%20BM-Global%20at%20https%3A//www.github.com/leepei/BM-Global/.&entry.1838667208=http%3A//arxiv.org/abs/2204.14067v4&entry.124074799=Read"},
{"title": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion", "author": "Fabio Montello and Ronja G\u00fcldenring and Simone Scardapane and Lazaros Nalpantidis", "abstract": "Model compression is essential in the deployment of large Computer Vision models on embedded devices. However, static optimization techniques (e.g. pruning, quantization, etc.) neglect the fact that different inputs have different complexities, thus requiring different amount of computations. Dynamic Neural Networks allow to condition the number of computations to the specific input. The current literature on the topic is very extensive and fragmented. We present a comprehensive survey that synthesizes and unifies existing Dynamic Neural Networks research in the context of Computer Vision. Additionally, we provide a logical taxonomy based on which component of the network is adaptive: the output, the computation graph or the input. Furthermore, we argue that Dynamic Neural Networks are particularly beneficial in the context of Sensor Fusion for better adaptivity, noise reduction and information prioritization. We present preliminary works in this direction. We complement this survey with a curated repository listing all the surveyed papers, each with a brief summary of the solution and the code base when available: https://github.com/DTU-PAS/awesome-dynn-for-cv .", "link": "http://arxiv.org/abs/2501.07451v3", "date": "2026-02-12", "relevancy": 2.2298, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5389}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Dynamic%20Neural%20Networks%3A%20from%20Computer%20Vision%20to%20Multi-modal%20Sensor%20Fusion&body=Title%3A%20A%20Survey%20on%20Dynamic%20Neural%20Networks%3A%20from%20Computer%20Vision%20to%20Multi-modal%20Sensor%20Fusion%0AAuthor%3A%20Fabio%20Montello%20and%20Ronja%20G%C3%BCldenring%20and%20Simone%20Scardapane%20and%20Lazaros%20Nalpantidis%0AAbstract%3A%20Model%20compression%20is%20essential%20in%20the%20deployment%20of%20large%20Computer%20Vision%20models%20on%20embedded%20devices.%20However%2C%20static%20optimization%20techniques%20%28e.g.%20pruning%2C%20quantization%2C%20etc.%29%20neglect%20the%20fact%20that%20different%20inputs%20have%20different%20complexities%2C%20thus%20requiring%20different%20amount%20of%20computations.%20Dynamic%20Neural%20Networks%20allow%20to%20condition%20the%20number%20of%20computations%20to%20the%20specific%20input.%20The%20current%20literature%20on%20the%20topic%20is%20very%20extensive%20and%20fragmented.%20We%20present%20a%20comprehensive%20survey%20that%20synthesizes%20and%20unifies%20existing%20Dynamic%20Neural%20Networks%20research%20in%20the%20context%20of%20Computer%20Vision.%20Additionally%2C%20we%20provide%20a%20logical%20taxonomy%20based%20on%20which%20component%20of%20the%20network%20is%20adaptive%3A%20the%20output%2C%20the%20computation%20graph%20or%20the%20input.%20Furthermore%2C%20we%20argue%20that%20Dynamic%20Neural%20Networks%20are%20particularly%20beneficial%20in%20the%20context%20of%20Sensor%20Fusion%20for%20better%20adaptivity%2C%20noise%20reduction%20and%20information%20prioritization.%20We%20present%20preliminary%20works%20in%20this%20direction.%20We%20complement%20this%20survey%20with%20a%20curated%20repository%20listing%20all%20the%20surveyed%20papers%2C%20each%20with%20a%20brief%20summary%20of%20the%20solution%20and%20the%20code%20base%20when%20available%3A%20https%3A//github.com/DTU-PAS/awesome-dynn-for-cv%20.%0ALink%3A%20http%3A//arxiv.org/abs/2501.07451v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Dynamic%2520Neural%2520Networks%253A%2520from%2520Computer%2520Vision%2520to%2520Multi-modal%2520Sensor%2520Fusion%26entry.906535625%3DFabio%2520Montello%2520and%2520Ronja%2520G%25C3%25BCldenring%2520and%2520Simone%2520Scardapane%2520and%2520Lazaros%2520Nalpantidis%26entry.1292438233%3DModel%2520compression%2520is%2520essential%2520in%2520the%2520deployment%2520of%2520large%2520Computer%2520Vision%2520models%2520on%2520embedded%2520devices.%2520However%252C%2520static%2520optimization%2520techniques%2520%2528e.g.%2520pruning%252C%2520quantization%252C%2520etc.%2529%2520neglect%2520the%2520fact%2520that%2520different%2520inputs%2520have%2520different%2520complexities%252C%2520thus%2520requiring%2520different%2520amount%2520of%2520computations.%2520Dynamic%2520Neural%2520Networks%2520allow%2520to%2520condition%2520the%2520number%2520of%2520computations%2520to%2520the%2520specific%2520input.%2520The%2520current%2520literature%2520on%2520the%2520topic%2520is%2520very%2520extensive%2520and%2520fragmented.%2520We%2520present%2520a%2520comprehensive%2520survey%2520that%2520synthesizes%2520and%2520unifies%2520existing%2520Dynamic%2520Neural%2520Networks%2520research%2520in%2520the%2520context%2520of%2520Computer%2520Vision.%2520Additionally%252C%2520we%2520provide%2520a%2520logical%2520taxonomy%2520based%2520on%2520which%2520component%2520of%2520the%2520network%2520is%2520adaptive%253A%2520the%2520output%252C%2520the%2520computation%2520graph%2520or%2520the%2520input.%2520Furthermore%252C%2520we%2520argue%2520that%2520Dynamic%2520Neural%2520Networks%2520are%2520particularly%2520beneficial%2520in%2520the%2520context%2520of%2520Sensor%2520Fusion%2520for%2520better%2520adaptivity%252C%2520noise%2520reduction%2520and%2520information%2520prioritization.%2520We%2520present%2520preliminary%2520works%2520in%2520this%2520direction.%2520We%2520complement%2520this%2520survey%2520with%2520a%2520curated%2520repository%2520listing%2520all%2520the%2520surveyed%2520papers%252C%2520each%2520with%2520a%2520brief%2520summary%2520of%2520the%2520solution%2520and%2520the%2520code%2520base%2520when%2520available%253A%2520https%253A//github.com/DTU-PAS/awesome-dynn-for-cv%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07451v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Dynamic%20Neural%20Networks%3A%20from%20Computer%20Vision%20to%20Multi-modal%20Sensor%20Fusion&entry.906535625=Fabio%20Montello%20and%20Ronja%20G%C3%BCldenring%20and%20Simone%20Scardapane%20and%20Lazaros%20Nalpantidis&entry.1292438233=Model%20compression%20is%20essential%20in%20the%20deployment%20of%20large%20Computer%20Vision%20models%20on%20embedded%20devices.%20However%2C%20static%20optimization%20techniques%20%28e.g.%20pruning%2C%20quantization%2C%20etc.%29%20neglect%20the%20fact%20that%20different%20inputs%20have%20different%20complexities%2C%20thus%20requiring%20different%20amount%20of%20computations.%20Dynamic%20Neural%20Networks%20allow%20to%20condition%20the%20number%20of%20computations%20to%20the%20specific%20input.%20The%20current%20literature%20on%20the%20topic%20is%20very%20extensive%20and%20fragmented.%20We%20present%20a%20comprehensive%20survey%20that%20synthesizes%20and%20unifies%20existing%20Dynamic%20Neural%20Networks%20research%20in%20the%20context%20of%20Computer%20Vision.%20Additionally%2C%20we%20provide%20a%20logical%20taxonomy%20based%20on%20which%20component%20of%20the%20network%20is%20adaptive%3A%20the%20output%2C%20the%20computation%20graph%20or%20the%20input.%20Furthermore%2C%20we%20argue%20that%20Dynamic%20Neural%20Networks%20are%20particularly%20beneficial%20in%20the%20context%20of%20Sensor%20Fusion%20for%20better%20adaptivity%2C%20noise%20reduction%20and%20information%20prioritization.%20We%20present%20preliminary%20works%20in%20this%20direction.%20We%20complement%20this%20survey%20with%20a%20curated%20repository%20listing%20all%20the%20surveyed%20papers%2C%20each%20with%20a%20brief%20summary%20of%20the%20solution%20and%20the%20code%20base%20when%20available%3A%20https%3A//github.com/DTU-PAS/awesome-dynn-for-cv%20.&entry.1838667208=http%3A//arxiv.org/abs/2501.07451v3&entry.124074799=Read"},
{"title": "Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models", "author": "Lu Tao and Jinxuan Luo and Yousuke Watanabe and Zhengshu Zhou and Yuhuan Lu and Shen Ying and Pan Zhang and Fei Zhao and Hiroaki Takada", "abstract": "Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical occlusions. Although DM-enhanced ADS have been successfully deployed in real-world applications in Japan, existing DM systems still lack a natural-language-supported (NLS) human interface, which could substantially enhance human-DM interaction. To address this gap, this paper introduces VRCsim, a VRC cooperative perception (CP) simulation framework designed to generate streaming VRC-CP data. Based on VRCsim, we construct a question-answering data set, VRC-QA, focused on spatial querying and reasoning in mixed-traffic scenes. Building upon VRCsim and VRC-QA, we further propose Talk2DM, a plug-and-play module that extends VRC-DM systems with NLS querying and commonsense reasoning capabilities. Talk2DM is built upon a novel chain-of-prompt (CoP) mechanism that progressively integrates human-defined rules with the commonsense knowledge of large language models (LLMs). Experiments on VRC-QA show that Talk2DM can seamlessly switch across different LLMs while maintaining high NLS query accuracy, demonstrating strong generalization capability. Although larger models tend to achieve higher accuracy, they incur significant efficiency degradation. Our results reveal that Talk2DM, powered by Qwen3:8B, Gemma3:27B, and GPT-oss models, achieves over 93\\% NLS query accuracy with an average response time of only 2-5 seconds, indicating strong practical potential.", "link": "http://arxiv.org/abs/2602.11860v1", "date": "2026-02-12", "relevancy": 2.2206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talk2DM%3A%20Enabling%20Natural%20Language%20Querying%20and%20Commonsense%20Reasoning%20for%20Vehicle-Road-Cloud%20Integrated%20Dynamic%20Maps%20with%20Large%20Language%20Models&body=Title%3A%20Talk2DM%3A%20Enabling%20Natural%20Language%20Querying%20and%20Commonsense%20Reasoning%20for%20Vehicle-Road-Cloud%20Integrated%20Dynamic%20Maps%20with%20Large%20Language%20Models%0AAuthor%3A%20Lu%20Tao%20and%20Jinxuan%20Luo%20and%20Yousuke%20Watanabe%20and%20Zhengshu%20Zhou%20and%20Yuhuan%20Lu%20and%20Shen%20Ying%20and%20Pan%20Zhang%20and%20Fei%20Zhao%20and%20Hiroaki%20Takada%0AAbstract%3A%20Dynamic%20maps%20%28DM%29%20serve%20as%20the%20fundamental%20information%20infrastructure%20for%20vehicle-road-cloud%20%28VRC%29%20cooperative%20autonomous%20driving%20in%20China%20and%20Japan.%20By%20providing%20comprehensive%20traffic%20scene%20representations%2C%20DM%20overcome%20the%20limitations%20of%20standalone%20autonomous%20driving%20systems%20%28ADS%29%2C%20such%20as%20physical%20occlusions.%20Although%20DM-enhanced%20ADS%20have%20been%20successfully%20deployed%20in%20real-world%20applications%20in%20Japan%2C%20existing%20DM%20systems%20still%20lack%20a%20natural-language-supported%20%28NLS%29%20human%20interface%2C%20which%20could%20substantially%20enhance%20human-DM%20interaction.%20To%20address%20this%20gap%2C%20this%20paper%20introduces%20VRCsim%2C%20a%20VRC%20cooperative%20perception%20%28CP%29%20simulation%20framework%20designed%20to%20generate%20streaming%20VRC-CP%20data.%20Based%20on%20VRCsim%2C%20we%20construct%20a%20question-answering%20data%20set%2C%20VRC-QA%2C%20focused%20on%20spatial%20querying%20and%20reasoning%20in%20mixed-traffic%20scenes.%20Building%20upon%20VRCsim%20and%20VRC-QA%2C%20we%20further%20propose%20Talk2DM%2C%20a%20plug-and-play%20module%20that%20extends%20VRC-DM%20systems%20with%20NLS%20querying%20and%20commonsense%20reasoning%20capabilities.%20Talk2DM%20is%20built%20upon%20a%20novel%20chain-of-prompt%20%28CoP%29%20mechanism%20that%20progressively%20integrates%20human-defined%20rules%20with%20the%20commonsense%20knowledge%20of%20large%20language%20models%20%28LLMs%29.%20Experiments%20on%20VRC-QA%20show%20that%20Talk2DM%20can%20seamlessly%20switch%20across%20different%20LLMs%20while%20maintaining%20high%20NLS%20query%20accuracy%2C%20demonstrating%20strong%20generalization%20capability.%20Although%20larger%20models%20tend%20to%20achieve%20higher%20accuracy%2C%20they%20incur%20significant%20efficiency%20degradation.%20Our%20results%20reveal%20that%20Talk2DM%2C%20powered%20by%20Qwen3%3A8B%2C%20Gemma3%3A27B%2C%20and%20GPT-oss%20models%2C%20achieves%20over%2093%5C%25%20NLS%20query%20accuracy%20with%20an%20average%20response%20time%20of%20only%202-5%20seconds%2C%20indicating%20strong%20practical%20potential.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalk2DM%253A%2520Enabling%2520Natural%2520Language%2520Querying%2520and%2520Commonsense%2520Reasoning%2520for%2520Vehicle-Road-Cloud%2520Integrated%2520Dynamic%2520Maps%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DLu%2520Tao%2520and%2520Jinxuan%2520Luo%2520and%2520Yousuke%2520Watanabe%2520and%2520Zhengshu%2520Zhou%2520and%2520Yuhuan%2520Lu%2520and%2520Shen%2520Ying%2520and%2520Pan%2520Zhang%2520and%2520Fei%2520Zhao%2520and%2520Hiroaki%2520Takada%26entry.1292438233%3DDynamic%2520maps%2520%2528DM%2529%2520serve%2520as%2520the%2520fundamental%2520information%2520infrastructure%2520for%2520vehicle-road-cloud%2520%2528VRC%2529%2520cooperative%2520autonomous%2520driving%2520in%2520China%2520and%2520Japan.%2520By%2520providing%2520comprehensive%2520traffic%2520scene%2520representations%252C%2520DM%2520overcome%2520the%2520limitations%2520of%2520standalone%2520autonomous%2520driving%2520systems%2520%2528ADS%2529%252C%2520such%2520as%2520physical%2520occlusions.%2520Although%2520DM-enhanced%2520ADS%2520have%2520been%2520successfully%2520deployed%2520in%2520real-world%2520applications%2520in%2520Japan%252C%2520existing%2520DM%2520systems%2520still%2520lack%2520a%2520natural-language-supported%2520%2528NLS%2529%2520human%2520interface%252C%2520which%2520could%2520substantially%2520enhance%2520human-DM%2520interaction.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%2520introduces%2520VRCsim%252C%2520a%2520VRC%2520cooperative%2520perception%2520%2528CP%2529%2520simulation%2520framework%2520designed%2520to%2520generate%2520streaming%2520VRC-CP%2520data.%2520Based%2520on%2520VRCsim%252C%2520we%2520construct%2520a%2520question-answering%2520data%2520set%252C%2520VRC-QA%252C%2520focused%2520on%2520spatial%2520querying%2520and%2520reasoning%2520in%2520mixed-traffic%2520scenes.%2520Building%2520upon%2520VRCsim%2520and%2520VRC-QA%252C%2520we%2520further%2520propose%2520Talk2DM%252C%2520a%2520plug-and-play%2520module%2520that%2520extends%2520VRC-DM%2520systems%2520with%2520NLS%2520querying%2520and%2520commonsense%2520reasoning%2520capabilities.%2520Talk2DM%2520is%2520built%2520upon%2520a%2520novel%2520chain-of-prompt%2520%2528CoP%2529%2520mechanism%2520that%2520progressively%2520integrates%2520human-defined%2520rules%2520with%2520the%2520commonsense%2520knowledge%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Experiments%2520on%2520VRC-QA%2520show%2520that%2520Talk2DM%2520can%2520seamlessly%2520switch%2520across%2520different%2520LLMs%2520while%2520maintaining%2520high%2520NLS%2520query%2520accuracy%252C%2520demonstrating%2520strong%2520generalization%2520capability.%2520Although%2520larger%2520models%2520tend%2520to%2520achieve%2520higher%2520accuracy%252C%2520they%2520incur%2520significant%2520efficiency%2520degradation.%2520Our%2520results%2520reveal%2520that%2520Talk2DM%252C%2520powered%2520by%2520Qwen3%253A8B%252C%2520Gemma3%253A27B%252C%2520and%2520GPT-oss%2520models%252C%2520achieves%2520over%252093%255C%2525%2520NLS%2520query%2520accuracy%2520with%2520an%2520average%2520response%2520time%2520of%2520only%25202-5%2520seconds%252C%2520indicating%2520strong%2520practical%2520potential.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talk2DM%3A%20Enabling%20Natural%20Language%20Querying%20and%20Commonsense%20Reasoning%20for%20Vehicle-Road-Cloud%20Integrated%20Dynamic%20Maps%20with%20Large%20Language%20Models&entry.906535625=Lu%20Tao%20and%20Jinxuan%20Luo%20and%20Yousuke%20Watanabe%20and%20Zhengshu%20Zhou%20and%20Yuhuan%20Lu%20and%20Shen%20Ying%20and%20Pan%20Zhang%20and%20Fei%20Zhao%20and%20Hiroaki%20Takada&entry.1292438233=Dynamic%20maps%20%28DM%29%20serve%20as%20the%20fundamental%20information%20infrastructure%20for%20vehicle-road-cloud%20%28VRC%29%20cooperative%20autonomous%20driving%20in%20China%20and%20Japan.%20By%20providing%20comprehensive%20traffic%20scene%20representations%2C%20DM%20overcome%20the%20limitations%20of%20standalone%20autonomous%20driving%20systems%20%28ADS%29%2C%20such%20as%20physical%20occlusions.%20Although%20DM-enhanced%20ADS%20have%20been%20successfully%20deployed%20in%20real-world%20applications%20in%20Japan%2C%20existing%20DM%20systems%20still%20lack%20a%20natural-language-supported%20%28NLS%29%20human%20interface%2C%20which%20could%20substantially%20enhance%20human-DM%20interaction.%20To%20address%20this%20gap%2C%20this%20paper%20introduces%20VRCsim%2C%20a%20VRC%20cooperative%20perception%20%28CP%29%20simulation%20framework%20designed%20to%20generate%20streaming%20VRC-CP%20data.%20Based%20on%20VRCsim%2C%20we%20construct%20a%20question-answering%20data%20set%2C%20VRC-QA%2C%20focused%20on%20spatial%20querying%20and%20reasoning%20in%20mixed-traffic%20scenes.%20Building%20upon%20VRCsim%20and%20VRC-QA%2C%20we%20further%20propose%20Talk2DM%2C%20a%20plug-and-play%20module%20that%20extends%20VRC-DM%20systems%20with%20NLS%20querying%20and%20commonsense%20reasoning%20capabilities.%20Talk2DM%20is%20built%20upon%20a%20novel%20chain-of-prompt%20%28CoP%29%20mechanism%20that%20progressively%20integrates%20human-defined%20rules%20with%20the%20commonsense%20knowledge%20of%20large%20language%20models%20%28LLMs%29.%20Experiments%20on%20VRC-QA%20show%20that%20Talk2DM%20can%20seamlessly%20switch%20across%20different%20LLMs%20while%20maintaining%20high%20NLS%20query%20accuracy%2C%20demonstrating%20strong%20generalization%20capability.%20Although%20larger%20models%20tend%20to%20achieve%20higher%20accuracy%2C%20they%20incur%20significant%20efficiency%20degradation.%20Our%20results%20reveal%20that%20Talk2DM%2C%20powered%20by%20Qwen3%3A8B%2C%20Gemma3%3A27B%2C%20and%20GPT-oss%20models%2C%20achieves%20over%2093%5C%25%20NLS%20query%20accuracy%20with%20an%20average%20response%20time%20of%20only%202-5%20seconds%2C%20indicating%20strong%20practical%20potential.&entry.1838667208=http%3A//arxiv.org/abs/2602.11860v1&entry.124074799=Read"},
{"title": "Uncertainty-driven Embedding Convolution", "author": "Sungjun Lim and Kangjun Noh and Youngjun Choi and Heeyoung Lee and Kyungwoo Song", "abstract": "Text embeddings are essential components in modern NLP pipelines. Although numerous embedding models have been proposed, no single model consistently dominates across domains and tasks. This variability motivates the use of ensemble techniques to combine complementary strengths. However, most existing ensemble methods operate on deterministic embeddings and fail to account for model-specific uncertainty, limiting their robustness and reliability in downstream applications. To address these limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC first transforms deterministic embeddings into probabilistic ones in a post-hoc manner. It then computes adaptive ensemble coefficients based on embedding uncertainty, derived from a principled surrogate-loss formulation. Additionally, UEC employs an uncertainty-aware similarity function that directly incorporates uncertainty into the similarity scoring, providing a theoretically grounded and efficient surrogate to distributional distances. Extensive experiments on diverse benchmarks demonstrate that UEC consistently improves both performance and robustness by leveraging principled uncertainty modeling.", "link": "http://arxiv.org/abs/2507.20718v4", "date": "2026-02-12", "relevancy": 2.2108, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6125}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5662}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-driven%20Embedding%20Convolution&body=Title%3A%20Uncertainty-driven%20Embedding%20Convolution%0AAuthor%3A%20Sungjun%20Lim%20and%20Kangjun%20Noh%20and%20Youngjun%20Choi%20and%20Heeyoung%20Lee%20and%20Kyungwoo%20Song%0AAbstract%3A%20Text%20embeddings%20are%20essential%20components%20in%20modern%20NLP%20pipelines.%20Although%20numerous%20embedding%20models%20have%20been%20proposed%2C%20no%20single%20model%20consistently%20dominates%20across%20domains%20and%20tasks.%20This%20variability%20motivates%20the%20use%20of%20ensemble%20techniques%20to%20combine%20complementary%20strengths.%20However%2C%20most%20existing%20ensemble%20methods%20operate%20on%20deterministic%20embeddings%20and%20fail%20to%20account%20for%20model-specific%20uncertainty%2C%20limiting%20their%20robustness%20and%20reliability%20in%20downstream%20applications.%20To%20address%20these%20limitations%2C%20we%20propose%20Uncertainty-driven%20Embedding%20Convolution%20%28UEC%29.%20UEC%20first%20transforms%20deterministic%20embeddings%20into%20probabilistic%20ones%20in%20a%20post-hoc%20manner.%20It%20then%20computes%20adaptive%20ensemble%20coefficients%20based%20on%20embedding%20uncertainty%2C%20derived%20from%20a%20principled%20surrogate-loss%20formulation.%20Additionally%2C%20UEC%20employs%20an%20uncertainty-aware%20similarity%20function%20that%20directly%20incorporates%20uncertainty%20into%20the%20similarity%20scoring%2C%20providing%20a%20theoretically%20grounded%20and%20efficient%20surrogate%20to%20distributional%20distances.%20Extensive%20experiments%20on%20diverse%20benchmarks%20demonstrate%20that%20UEC%20consistently%20improves%20both%20performance%20and%20robustness%20by%20leveraging%20principled%20uncertainty%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2507.20718v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-driven%2520Embedding%2520Convolution%26entry.906535625%3DSungjun%2520Lim%2520and%2520Kangjun%2520Noh%2520and%2520Youngjun%2520Choi%2520and%2520Heeyoung%2520Lee%2520and%2520Kyungwoo%2520Song%26entry.1292438233%3DText%2520embeddings%2520are%2520essential%2520components%2520in%2520modern%2520NLP%2520pipelines.%2520Although%2520numerous%2520embedding%2520models%2520have%2520been%2520proposed%252C%2520no%2520single%2520model%2520consistently%2520dominates%2520across%2520domains%2520and%2520tasks.%2520This%2520variability%2520motivates%2520the%2520use%2520of%2520ensemble%2520techniques%2520to%2520combine%2520complementary%2520strengths.%2520However%252C%2520most%2520existing%2520ensemble%2520methods%2520operate%2520on%2520deterministic%2520embeddings%2520and%2520fail%2520to%2520account%2520for%2520model-specific%2520uncertainty%252C%2520limiting%2520their%2520robustness%2520and%2520reliability%2520in%2520downstream%2520applications.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Uncertainty-driven%2520Embedding%2520Convolution%2520%2528UEC%2529.%2520UEC%2520first%2520transforms%2520deterministic%2520embeddings%2520into%2520probabilistic%2520ones%2520in%2520a%2520post-hoc%2520manner.%2520It%2520then%2520computes%2520adaptive%2520ensemble%2520coefficients%2520based%2520on%2520embedding%2520uncertainty%252C%2520derived%2520from%2520a%2520principled%2520surrogate-loss%2520formulation.%2520Additionally%252C%2520UEC%2520employs%2520an%2520uncertainty-aware%2520similarity%2520function%2520that%2520directly%2520incorporates%2520uncertainty%2520into%2520the%2520similarity%2520scoring%252C%2520providing%2520a%2520theoretically%2520grounded%2520and%2520efficient%2520surrogate%2520to%2520distributional%2520distances.%2520Extensive%2520experiments%2520on%2520diverse%2520benchmarks%2520demonstrate%2520that%2520UEC%2520consistently%2520improves%2520both%2520performance%2520and%2520robustness%2520by%2520leveraging%2520principled%2520uncertainty%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20718v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-driven%20Embedding%20Convolution&entry.906535625=Sungjun%20Lim%20and%20Kangjun%20Noh%20and%20Youngjun%20Choi%20and%20Heeyoung%20Lee%20and%20Kyungwoo%20Song&entry.1292438233=Text%20embeddings%20are%20essential%20components%20in%20modern%20NLP%20pipelines.%20Although%20numerous%20embedding%20models%20have%20been%20proposed%2C%20no%20single%20model%20consistently%20dominates%20across%20domains%20and%20tasks.%20This%20variability%20motivates%20the%20use%20of%20ensemble%20techniques%20to%20combine%20complementary%20strengths.%20However%2C%20most%20existing%20ensemble%20methods%20operate%20on%20deterministic%20embeddings%20and%20fail%20to%20account%20for%20model-specific%20uncertainty%2C%20limiting%20their%20robustness%20and%20reliability%20in%20downstream%20applications.%20To%20address%20these%20limitations%2C%20we%20propose%20Uncertainty-driven%20Embedding%20Convolution%20%28UEC%29.%20UEC%20first%20transforms%20deterministic%20embeddings%20into%20probabilistic%20ones%20in%20a%20post-hoc%20manner.%20It%20then%20computes%20adaptive%20ensemble%20coefficients%20based%20on%20embedding%20uncertainty%2C%20derived%20from%20a%20principled%20surrogate-loss%20formulation.%20Additionally%2C%20UEC%20employs%20an%20uncertainty-aware%20similarity%20function%20that%20directly%20incorporates%20uncertainty%20into%20the%20similarity%20scoring%2C%20providing%20a%20theoretically%20grounded%20and%20efficient%20surrogate%20to%20distributional%20distances.%20Extensive%20experiments%20on%20diverse%20benchmarks%20demonstrate%20that%20UEC%20consistently%20improves%20both%20performance%20and%20robustness%20by%20leveraging%20principled%20uncertainty%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2507.20718v4&entry.124074799=Read"},
{"title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation", "author": "Wenjing Lu and Yi Hong and Yang Yang", "abstract": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.", "link": "http://arxiv.org/abs/2512.13101v2", "date": "2026-02-12", "relevancy": 2.2084, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6079}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5432}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonizing%20Generalization%20and%20Specialization%3A%20Uncertainty-Informed%20Collaborative%20Learning%20for%20Semi-supervised%20Medical%20Image%20Segmentation&body=Title%3A%20Harmonizing%20Generalization%20and%20Specialization%3A%20Uncertainty-Informed%20Collaborative%20Learning%20for%20Semi-supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Wenjing%20Lu%20and%20Yi%20Hong%20and%20Yang%20Yang%0AAbstract%3A%20Vision%20foundation%20models%20have%20demonstrated%20strong%20generalization%20in%20medical%20image%20segmentation%20by%20leveraging%20large-scale%2C%20heterogeneous%20pretraining.%20However%2C%20they%20often%20struggle%20to%20generalize%20to%20specialized%20clinical%20tasks%20under%20limited%20annotations%20or%20rare%20pathological%20variations%2C%20due%20to%20a%20mismatch%20between%20general%20priors%20and%20task-specific%20requirements.%20To%20address%20this%2C%20we%20propose%20Uncertainty-informed%20Collaborative%20Learning%20%28UnCoL%29%2C%20a%20dual-teacher%20framework%20that%20harmonizes%20generalization%20and%20specialization%20in%20semi-supervised%20medical%20image%20segmentation.%20Specifically%2C%20UnCoL%20distills%20both%20visual%20and%20semantic%20representations%20from%20a%20frozen%20foundation%20model%20to%20transfer%20general%20knowledge%2C%20while%20concurrently%20maintaining%20a%20progressively%20adapting%20teacher%20to%20capture%20fine-grained%20and%20task-specific%20representations.%20To%20balance%20guidance%20from%20both%20teachers%2C%20pseudo-label%20learning%20in%20UnCoL%20is%20adaptively%20regulated%20by%20predictive%20uncertainty%2C%20which%20selectively%20suppresses%20unreliable%20supervision%20and%20stabilizes%20learning%20in%20ambiguous%20regions.%20Experiments%20on%20diverse%202D%20and%203D%20segmentation%20benchmarks%20show%20that%20UnCoL%20consistently%20outperforms%20state-of-the-art%20semi-supervised%20methods%20and%20foundation%20model%20baselines.%20Moreover%2C%20our%20model%20delivers%20near%20fully%20supervised%20performance%20with%20markedly%20reduced%20annotation%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonizing%2520Generalization%2520and%2520Specialization%253A%2520Uncertainty-Informed%2520Collaborative%2520Learning%2520for%2520Semi-supervised%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DWenjing%2520Lu%2520and%2520Yi%2520Hong%2520and%2520Yang%2520Yang%26entry.1292438233%3DVision%2520foundation%2520models%2520have%2520demonstrated%2520strong%2520generalization%2520in%2520medical%2520image%2520segmentation%2520by%2520leveraging%2520large-scale%252C%2520heterogeneous%2520pretraining.%2520However%252C%2520they%2520often%2520struggle%2520to%2520generalize%2520to%2520specialized%2520clinical%2520tasks%2520under%2520limited%2520annotations%2520or%2520rare%2520pathological%2520variations%252C%2520due%2520to%2520a%2520mismatch%2520between%2520general%2520priors%2520and%2520task-specific%2520requirements.%2520To%2520address%2520this%252C%2520we%2520propose%2520Uncertainty-informed%2520Collaborative%2520Learning%2520%2528UnCoL%2529%252C%2520a%2520dual-teacher%2520framework%2520that%2520harmonizes%2520generalization%2520and%2520specialization%2520in%2520semi-supervised%2520medical%2520image%2520segmentation.%2520Specifically%252C%2520UnCoL%2520distills%2520both%2520visual%2520and%2520semantic%2520representations%2520from%2520a%2520frozen%2520foundation%2520model%2520to%2520transfer%2520general%2520knowledge%252C%2520while%2520concurrently%2520maintaining%2520a%2520progressively%2520adapting%2520teacher%2520to%2520capture%2520fine-grained%2520and%2520task-specific%2520representations.%2520To%2520balance%2520guidance%2520from%2520both%2520teachers%252C%2520pseudo-label%2520learning%2520in%2520UnCoL%2520is%2520adaptively%2520regulated%2520by%2520predictive%2520uncertainty%252C%2520which%2520selectively%2520suppresses%2520unreliable%2520supervision%2520and%2520stabilizes%2520learning%2520in%2520ambiguous%2520regions.%2520Experiments%2520on%2520diverse%25202D%2520and%25203D%2520segmentation%2520benchmarks%2520show%2520that%2520UnCoL%2520consistently%2520outperforms%2520state-of-the-art%2520semi-supervised%2520methods%2520and%2520foundation%2520model%2520baselines.%2520Moreover%252C%2520our%2520model%2520delivers%2520near%2520fully%2520supervised%2520performance%2520with%2520markedly%2520reduced%2520annotation%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonizing%20Generalization%20and%20Specialization%3A%20Uncertainty-Informed%20Collaborative%20Learning%20for%20Semi-supervised%20Medical%20Image%20Segmentation&entry.906535625=Wenjing%20Lu%20and%20Yi%20Hong%20and%20Yang%20Yang&entry.1292438233=Vision%20foundation%20models%20have%20demonstrated%20strong%20generalization%20in%20medical%20image%20segmentation%20by%20leveraging%20large-scale%2C%20heterogeneous%20pretraining.%20However%2C%20they%20often%20struggle%20to%20generalize%20to%20specialized%20clinical%20tasks%20under%20limited%20annotations%20or%20rare%20pathological%20variations%2C%20due%20to%20a%20mismatch%20between%20general%20priors%20and%20task-specific%20requirements.%20To%20address%20this%2C%20we%20propose%20Uncertainty-informed%20Collaborative%20Learning%20%28UnCoL%29%2C%20a%20dual-teacher%20framework%20that%20harmonizes%20generalization%20and%20specialization%20in%20semi-supervised%20medical%20image%20segmentation.%20Specifically%2C%20UnCoL%20distills%20both%20visual%20and%20semantic%20representations%20from%20a%20frozen%20foundation%20model%20to%20transfer%20general%20knowledge%2C%20while%20concurrently%20maintaining%20a%20progressively%20adapting%20teacher%20to%20capture%20fine-grained%20and%20task-specific%20representations.%20To%20balance%20guidance%20from%20both%20teachers%2C%20pseudo-label%20learning%20in%20UnCoL%20is%20adaptively%20regulated%20by%20predictive%20uncertainty%2C%20which%20selectively%20suppresses%20unreliable%20supervision%20and%20stabilizes%20learning%20in%20ambiguous%20regions.%20Experiments%20on%20diverse%202D%20and%203D%20segmentation%20benchmarks%20show%20that%20UnCoL%20consistently%20outperforms%20state-of-the-art%20semi-supervised%20methods%20and%20foundation%20model%20baselines.%20Moreover%2C%20our%20model%20delivers%20near%20fully%20supervised%20performance%20with%20markedly%20reduced%20annotation%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2512.13101v2&entry.124074799=Read"},
{"title": "Efficient Segment Anything with Depth-Aware Fusion and Limited Training Data", "author": "Yiming Zhou and Xuenjie Xie and Panfeng Li and Albrecht Kunz and Ahmad Osman and Xavier Maldague", "abstract": "Segment Anything Models (SAM) achieve impressive universal segmentation performance but require massive datasets (e.g., 11M images) and rely solely on RGB inputs. Recent efficient variants reduce computation but still depend on large-scale training. We propose a lightweight RGB-D fusion framework that augments EfficientViT-SAM with monocular depth priors. Depth maps are generated with a pretrained estimator and fused mid-level with RGB features through a dedicated depth encoder. Trained on only 11.2k samples (less than 0.1\\% of SA-1B), our method achieves higher accuracy than EfficientViT-SAM, showing that depth cues provide strong geometric priors for segmentation.", "link": "http://arxiv.org/abs/2602.11804v1", "date": "2026-02-12", "relevancy": 2.1983, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5558}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5453}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Segment%20Anything%20with%20Depth-Aware%20Fusion%20and%20Limited%20Training%20Data&body=Title%3A%20Efficient%20Segment%20Anything%20with%20Depth-Aware%20Fusion%20and%20Limited%20Training%20Data%0AAuthor%3A%20Yiming%20Zhou%20and%20Xuenjie%20Xie%20and%20Panfeng%20Li%20and%20Albrecht%20Kunz%20and%20Ahmad%20Osman%20and%20Xavier%20Maldague%0AAbstract%3A%20Segment%20Anything%20Models%20%28SAM%29%20achieve%20impressive%20universal%20segmentation%20performance%20but%20require%20massive%20datasets%20%28e.g.%2C%2011M%20images%29%20and%20rely%20solely%20on%20RGB%20inputs.%20Recent%20efficient%20variants%20reduce%20computation%20but%20still%20depend%20on%20large-scale%20training.%20We%20propose%20a%20lightweight%20RGB-D%20fusion%20framework%20that%20augments%20EfficientViT-SAM%20with%20monocular%20depth%20priors.%20Depth%20maps%20are%20generated%20with%20a%20pretrained%20estimator%20and%20fused%20mid-level%20with%20RGB%20features%20through%20a%20dedicated%20depth%20encoder.%20Trained%20on%20only%2011.2k%20samples%20%28less%20than%200.1%5C%25%20of%20SA-1B%29%2C%20our%20method%20achieves%20higher%20accuracy%20than%20EfficientViT-SAM%2C%20showing%20that%20depth%20cues%20provide%20strong%20geometric%20priors%20for%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Segment%2520Anything%2520with%2520Depth-Aware%2520Fusion%2520and%2520Limited%2520Training%2520Data%26entry.906535625%3DYiming%2520Zhou%2520and%2520Xuenjie%2520Xie%2520and%2520Panfeng%2520Li%2520and%2520Albrecht%2520Kunz%2520and%2520Ahmad%2520Osman%2520and%2520Xavier%2520Maldague%26entry.1292438233%3DSegment%2520Anything%2520Models%2520%2528SAM%2529%2520achieve%2520impressive%2520universal%2520segmentation%2520performance%2520but%2520require%2520massive%2520datasets%2520%2528e.g.%252C%252011M%2520images%2529%2520and%2520rely%2520solely%2520on%2520RGB%2520inputs.%2520Recent%2520efficient%2520variants%2520reduce%2520computation%2520but%2520still%2520depend%2520on%2520large-scale%2520training.%2520We%2520propose%2520a%2520lightweight%2520RGB-D%2520fusion%2520framework%2520that%2520augments%2520EfficientViT-SAM%2520with%2520monocular%2520depth%2520priors.%2520Depth%2520maps%2520are%2520generated%2520with%2520a%2520pretrained%2520estimator%2520and%2520fused%2520mid-level%2520with%2520RGB%2520features%2520through%2520a%2520dedicated%2520depth%2520encoder.%2520Trained%2520on%2520only%252011.2k%2520samples%2520%2528less%2520than%25200.1%255C%2525%2520of%2520SA-1B%2529%252C%2520our%2520method%2520achieves%2520higher%2520accuracy%2520than%2520EfficientViT-SAM%252C%2520showing%2520that%2520depth%2520cues%2520provide%2520strong%2520geometric%2520priors%2520for%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Segment%20Anything%20with%20Depth-Aware%20Fusion%20and%20Limited%20Training%20Data&entry.906535625=Yiming%20Zhou%20and%20Xuenjie%20Xie%20and%20Panfeng%20Li%20and%20Albrecht%20Kunz%20and%20Ahmad%20Osman%20and%20Xavier%20Maldague&entry.1292438233=Segment%20Anything%20Models%20%28SAM%29%20achieve%20impressive%20universal%20segmentation%20performance%20but%20require%20massive%20datasets%20%28e.g.%2C%2011M%20images%29%20and%20rely%20solely%20on%20RGB%20inputs.%20Recent%20efficient%20variants%20reduce%20computation%20but%20still%20depend%20on%20large-scale%20training.%20We%20propose%20a%20lightweight%20RGB-D%20fusion%20framework%20that%20augments%20EfficientViT-SAM%20with%20monocular%20depth%20priors.%20Depth%20maps%20are%20generated%20with%20a%20pretrained%20estimator%20and%20fused%20mid-level%20with%20RGB%20features%20through%20a%20dedicated%20depth%20encoder.%20Trained%20on%20only%2011.2k%20samples%20%28less%20than%200.1%5C%25%20of%20SA-1B%29%2C%20our%20method%20achieves%20higher%20accuracy%20than%20EfficientViT-SAM%2C%20showing%20that%20depth%20cues%20provide%20strong%20geometric%20priors%20for%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2602.11804v1&entry.124074799=Read"},
{"title": "FAIL: Flow Matching Adversarial Imitation Learning for Image Generation", "author": "Yeyao Ma and Chen Li and Xiaosong Zhang and Han Hu and Weidi Xie", "abstract": "Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.", "link": "http://arxiv.org/abs/2602.12155v1", "date": "2026-02-12", "relevancy": 2.1726, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.601}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5352}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAIL%3A%20Flow%20Matching%20Adversarial%20Imitation%20Learning%20for%20Image%20Generation&body=Title%3A%20FAIL%3A%20Flow%20Matching%20Adversarial%20Imitation%20Learning%20for%20Image%20Generation%0AAuthor%3A%20Yeyao%20Ma%20and%20Chen%20Li%20and%20Xiaosong%20Zhang%20and%20Han%20Hu%20and%20Weidi%20Xie%0AAbstract%3A%20Post-training%20of%20flow%20matching%20models-aligning%20the%20output%20distribution%20with%20a%20high-quality%20target-is%20mathematically%20equivalent%20to%20imitation%20learning.%20While%20Supervised%20Fine-Tuning%20mimics%20expert%20demonstrations%20effectively%2C%20it%20cannot%20correct%20policy%20drift%20in%20unseen%20states.%20Preference%20optimization%20methods%20address%20this%20but%20require%20costly%20preference%20pairs%20or%20reward%20modeling.%20We%20propose%20Flow%20Matching%20Adversarial%20Imitation%20Learning%20%28FAIL%29%2C%20which%20minimizes%20policy-expert%20divergence%20through%20adversarial%20training%20without%20explicit%20rewards%20or%20pairwise%20comparisons.%20We%20derive%20two%20algorithms%3A%20FAIL-PD%20exploits%20differentiable%20ODE%20solvers%20for%20low-variance%20pathwise%20gradients%2C%20while%20FAIL-PG%20provides%20a%20black-box%20alternative%20for%20discrete%20or%20computationally%20constrained%20settings.%20Fine-tuning%20FLUX%20with%20only%2013%2C000%20demonstrations%20from%20Nano%20Banana%20pro%2C%20FAIL%20achieves%20competitive%20performance%20on%20prompt%20following%20and%20aesthetic%20benchmarks.%20Furthermore%2C%20the%20framework%20generalizes%20effectively%20to%20discrete%20image%20and%20video%20generation%2C%20and%20functions%20as%20a%20robust%20regularizer%20to%20mitigate%20reward%20hacking%20in%20reward-based%20optimization.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/HansPolo113/FAIL.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAIL%253A%2520Flow%2520Matching%2520Adversarial%2520Imitation%2520Learning%2520for%2520Image%2520Generation%26entry.906535625%3DYeyao%2520Ma%2520and%2520Chen%2520Li%2520and%2520Xiaosong%2520Zhang%2520and%2520Han%2520Hu%2520and%2520Weidi%2520Xie%26entry.1292438233%3DPost-training%2520of%2520flow%2520matching%2520models-aligning%2520the%2520output%2520distribution%2520with%2520a%2520high-quality%2520target-is%2520mathematically%2520equivalent%2520to%2520imitation%2520learning.%2520While%2520Supervised%2520Fine-Tuning%2520mimics%2520expert%2520demonstrations%2520effectively%252C%2520it%2520cannot%2520correct%2520policy%2520drift%2520in%2520unseen%2520states.%2520Preference%2520optimization%2520methods%2520address%2520this%2520but%2520require%2520costly%2520preference%2520pairs%2520or%2520reward%2520modeling.%2520We%2520propose%2520Flow%2520Matching%2520Adversarial%2520Imitation%2520Learning%2520%2528FAIL%2529%252C%2520which%2520minimizes%2520policy-expert%2520divergence%2520through%2520adversarial%2520training%2520without%2520explicit%2520rewards%2520or%2520pairwise%2520comparisons.%2520We%2520derive%2520two%2520algorithms%253A%2520FAIL-PD%2520exploits%2520differentiable%2520ODE%2520solvers%2520for%2520low-variance%2520pathwise%2520gradients%252C%2520while%2520FAIL-PG%2520provides%2520a%2520black-box%2520alternative%2520for%2520discrete%2520or%2520computationally%2520constrained%2520settings.%2520Fine-tuning%2520FLUX%2520with%2520only%252013%252C000%2520demonstrations%2520from%2520Nano%2520Banana%2520pro%252C%2520FAIL%2520achieves%2520competitive%2520performance%2520on%2520prompt%2520following%2520and%2520aesthetic%2520benchmarks.%2520Furthermore%252C%2520the%2520framework%2520generalizes%2520effectively%2520to%2520discrete%2520image%2520and%2520video%2520generation%252C%2520and%2520functions%2520as%2520a%2520robust%2520regularizer%2520to%2520mitigate%2520reward%2520hacking%2520in%2520reward-based%2520optimization.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/HansPolo113/FAIL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAIL%3A%20Flow%20Matching%20Adversarial%20Imitation%20Learning%20for%20Image%20Generation&entry.906535625=Yeyao%20Ma%20and%20Chen%20Li%20and%20Xiaosong%20Zhang%20and%20Han%20Hu%20and%20Weidi%20Xie&entry.1292438233=Post-training%20of%20flow%20matching%20models-aligning%20the%20output%20distribution%20with%20a%20high-quality%20target-is%20mathematically%20equivalent%20to%20imitation%20learning.%20While%20Supervised%20Fine-Tuning%20mimics%20expert%20demonstrations%20effectively%2C%20it%20cannot%20correct%20policy%20drift%20in%20unseen%20states.%20Preference%20optimization%20methods%20address%20this%20but%20require%20costly%20preference%20pairs%20or%20reward%20modeling.%20We%20propose%20Flow%20Matching%20Adversarial%20Imitation%20Learning%20%28FAIL%29%2C%20which%20minimizes%20policy-expert%20divergence%20through%20adversarial%20training%20without%20explicit%20rewards%20or%20pairwise%20comparisons.%20We%20derive%20two%20algorithms%3A%20FAIL-PD%20exploits%20differentiable%20ODE%20solvers%20for%20low-variance%20pathwise%20gradients%2C%20while%20FAIL-PG%20provides%20a%20black-box%20alternative%20for%20discrete%20or%20computationally%20constrained%20settings.%20Fine-tuning%20FLUX%20with%20only%2013%2C000%20demonstrations%20from%20Nano%20Banana%20pro%2C%20FAIL%20achieves%20competitive%20performance%20on%20prompt%20following%20and%20aesthetic%20benchmarks.%20Furthermore%2C%20the%20framework%20generalizes%20effectively%20to%20discrete%20image%20and%20video%20generation%2C%20and%20functions%20as%20a%20robust%20regularizer%20to%20mitigate%20reward%20hacking%20in%20reward-based%20optimization.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/HansPolo113/FAIL.&entry.1838667208=http%3A//arxiv.org/abs/2602.12155v1&entry.124074799=Read"},
{"title": "On the implicit regularization of Langevin dynamics with projected noise", "author": "Govind Menon and Austin J. Stromme and Adrien Vacher", "abstract": "We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.", "link": "http://arxiv.org/abs/2602.12257v1", "date": "2026-02-12", "relevancy": 2.1694, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4397}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4327}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20implicit%20regularization%20of%20Langevin%20dynamics%20with%20projected%20noise&body=Title%3A%20On%20the%20implicit%20regularization%20of%20Langevin%20dynamics%20with%20projected%20noise%0AAuthor%3A%20Govind%20Menon%20and%20Austin%20J.%20Stromme%20and%20Adrien%20Vacher%0AAbstract%3A%20We%20study%20Langevin%20dynamics%20with%20noise%20projected%20onto%20the%20directions%20orthogonal%20to%20an%20isometric%20group%20action.%20This%20mathematical%20model%20is%20introduced%20to%20shed%20new%20light%20on%20the%20effects%20of%20symmetry%20on%20stochastic%20gradient%20descent%20for%20over-parametrized%20models.%20Our%20main%20result%20identifies%20a%20novel%20form%20of%20implicit%20regularization%3A%20when%20the%20initial%20and%20target%20density%20are%20both%20invariant%20under%20the%20group%20action%2C%20Langevin%20dynamics%20with%20projected%20noise%20is%20equivalent%20in%20law%20to%20Langevin%20dynamics%20with%20isotropic%20diffusion%20but%20with%20an%20additional%20drift%20term%20proportional%20to%20the%20negative%20log%20volume%20of%20the%20group%20orbit.%20We%20prove%20this%20result%20by%20constructing%20a%20coupling%20of%20the%20two%20processes%20via%20a%20third%20process%20on%20the%20group%20itself%2C%20and%20identify%20the%20additional%20drift%20as%20the%20mean%20curvature%20of%20the%20orbits.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520implicit%2520regularization%2520of%2520Langevin%2520dynamics%2520with%2520projected%2520noise%26entry.906535625%3DGovind%2520Menon%2520and%2520Austin%2520J.%2520Stromme%2520and%2520Adrien%2520Vacher%26entry.1292438233%3DWe%2520study%2520Langevin%2520dynamics%2520with%2520noise%2520projected%2520onto%2520the%2520directions%2520orthogonal%2520to%2520an%2520isometric%2520group%2520action.%2520This%2520mathematical%2520model%2520is%2520introduced%2520to%2520shed%2520new%2520light%2520on%2520the%2520effects%2520of%2520symmetry%2520on%2520stochastic%2520gradient%2520descent%2520for%2520over-parametrized%2520models.%2520Our%2520main%2520result%2520identifies%2520a%2520novel%2520form%2520of%2520implicit%2520regularization%253A%2520when%2520the%2520initial%2520and%2520target%2520density%2520are%2520both%2520invariant%2520under%2520the%2520group%2520action%252C%2520Langevin%2520dynamics%2520with%2520projected%2520noise%2520is%2520equivalent%2520in%2520law%2520to%2520Langevin%2520dynamics%2520with%2520isotropic%2520diffusion%2520but%2520with%2520an%2520additional%2520drift%2520term%2520proportional%2520to%2520the%2520negative%2520log%2520volume%2520of%2520the%2520group%2520orbit.%2520We%2520prove%2520this%2520result%2520by%2520constructing%2520a%2520coupling%2520of%2520the%2520two%2520processes%2520via%2520a%2520third%2520process%2520on%2520the%2520group%2520itself%252C%2520and%2520identify%2520the%2520additional%2520drift%2520as%2520the%2520mean%2520curvature%2520of%2520the%2520orbits.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20implicit%20regularization%20of%20Langevin%20dynamics%20with%20projected%20noise&entry.906535625=Govind%20Menon%20and%20Austin%20J.%20Stromme%20and%20Adrien%20Vacher&entry.1292438233=We%20study%20Langevin%20dynamics%20with%20noise%20projected%20onto%20the%20directions%20orthogonal%20to%20an%20isometric%20group%20action.%20This%20mathematical%20model%20is%20introduced%20to%20shed%20new%20light%20on%20the%20effects%20of%20symmetry%20on%20stochastic%20gradient%20descent%20for%20over-parametrized%20models.%20Our%20main%20result%20identifies%20a%20novel%20form%20of%20implicit%20regularization%3A%20when%20the%20initial%20and%20target%20density%20are%20both%20invariant%20under%20the%20group%20action%2C%20Langevin%20dynamics%20with%20projected%20noise%20is%20equivalent%20in%20law%20to%20Langevin%20dynamics%20with%20isotropic%20diffusion%20but%20with%20an%20additional%20drift%20term%20proportional%20to%20the%20negative%20log%20volume%20of%20the%20group%20orbit.%20We%20prove%20this%20result%20by%20constructing%20a%20coupling%20of%20the%20two%20processes%20via%20a%20third%20process%20on%20the%20group%20itself%2C%20and%20identify%20the%20additional%20drift%20as%20the%20mean%20curvature%20of%20the%20orbits.&entry.1838667208=http%3A//arxiv.org/abs/2602.12257v1&entry.124074799=Read"},
{"title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes", "author": "Jian Xu and Qibin Zhao and John Paisley and Delu Zeng", "abstract": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian modeling but pose substantial challenges for posterior inference, especially over inducing variables. Denoising diffusion variational inference (DDVI) addresses this by modeling the posterior as a time-reversed diffusion from a simple Gaussian prior. However, DDVI's fixed unconditional starting distribution remains far from the complex true posterior, resulting in inefficient inference trajectories and slow convergence. In this work, we propose Diffusion Bridge Variational Inference (DBVI), a principled extension of DDVI that initiates the reverse diffusion from a learnable, data-dependent initial distribution. This initialization is parameterized via an amortized neural network and progressively adapted using gradients from the ELBO objective, reducing the posterior gap and improving sample efficiency. To enable scalable amortization, we design the network to operate on the inducing inputs, which serve as structured, low-dimensional summaries of the dataset and naturally align with the inducing variables' shape. DBVI retains the mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We derive a tractable training objective under this formulation and implement DBVI for scalable inference in large-scale DGPs. Across regression, classification, and image reconstruction tasks, DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality.", "link": "http://arxiv.org/abs/2509.19078v3", "date": "2026-02-12", "relevancy": 2.1685, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5837}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.546}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Bridge%20Variational%20Inference%20for%20Deep%20Gaussian%20Processes&body=Title%3A%20Diffusion%20Bridge%20Variational%20Inference%20for%20Deep%20Gaussian%20Processes%0AAuthor%3A%20Jian%20Xu%20and%20Qibin%20Zhao%20and%20John%20Paisley%20and%20Delu%20Zeng%0AAbstract%3A%20Deep%20Gaussian%20processes%20%28DGPs%29%20enable%20expressive%20hierarchical%20Bayesian%20modeling%20but%20pose%20substantial%20challenges%20for%20posterior%20inference%2C%20especially%20over%20inducing%20variables.%20Denoising%20diffusion%20variational%20inference%20%28DDVI%29%20addresses%20this%20by%20modeling%20the%20posterior%20as%20a%20time-reversed%20diffusion%20from%20a%20simple%20Gaussian%20prior.%20However%2C%20DDVI%27s%20fixed%20unconditional%20starting%20distribution%20remains%20far%20from%20the%20complex%20true%20posterior%2C%20resulting%20in%20inefficient%20inference%20trajectories%20and%20slow%20convergence.%20In%20this%20work%2C%20we%20propose%20Diffusion%20Bridge%20Variational%20Inference%20%28DBVI%29%2C%20a%20principled%20extension%20of%20DDVI%20that%20initiates%20the%20reverse%20diffusion%20from%20a%20learnable%2C%20data-dependent%20initial%20distribution.%20This%20initialization%20is%20parameterized%20via%20an%20amortized%20neural%20network%20and%20progressively%20adapted%20using%20gradients%20from%20the%20ELBO%20objective%2C%20reducing%20the%20posterior%20gap%20and%20improving%20sample%20efficiency.%20To%20enable%20scalable%20amortization%2C%20we%20design%20the%20network%20to%20operate%20on%20the%20inducing%20inputs%2C%20which%20serve%20as%20structured%2C%20low-dimensional%20summaries%20of%20the%20dataset%20and%20naturally%20align%20with%20the%20inducing%20variables%27%20shape.%20DBVI%20retains%20the%20mathematical%20elegance%20of%20DDVI%2C%20including%20Girsanov-based%20ELBOs%20and%20reverse-time%20SDEs%2Cwhile%20reinterpreting%20the%20prior%20via%20a%20Doob-bridged%20diffusion%20process.%20We%20derive%20a%20tractable%20training%20objective%20under%20this%20formulation%20and%20implement%20DBVI%20for%20scalable%20inference%20in%20large-scale%20DGPs.%20Across%20regression%2C%20classification%2C%20and%20image%20reconstruction%20tasks%2C%20DBVI%20consistently%20outperforms%20DDVI%20and%20other%20variational%20baselines%20in%20predictive%20accuracy%2C%20convergence%20speed%2C%20and%20posterior%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19078v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Bridge%2520Variational%2520Inference%2520for%2520Deep%2520Gaussian%2520Processes%26entry.906535625%3DJian%2520Xu%2520and%2520Qibin%2520Zhao%2520and%2520John%2520Paisley%2520and%2520Delu%2520Zeng%26entry.1292438233%3DDeep%2520Gaussian%2520processes%2520%2528DGPs%2529%2520enable%2520expressive%2520hierarchical%2520Bayesian%2520modeling%2520but%2520pose%2520substantial%2520challenges%2520for%2520posterior%2520inference%252C%2520especially%2520over%2520inducing%2520variables.%2520Denoising%2520diffusion%2520variational%2520inference%2520%2528DDVI%2529%2520addresses%2520this%2520by%2520modeling%2520the%2520posterior%2520as%2520a%2520time-reversed%2520diffusion%2520from%2520a%2520simple%2520Gaussian%2520prior.%2520However%252C%2520DDVI%2527s%2520fixed%2520unconditional%2520starting%2520distribution%2520remains%2520far%2520from%2520the%2520complex%2520true%2520posterior%252C%2520resulting%2520in%2520inefficient%2520inference%2520trajectories%2520and%2520slow%2520convergence.%2520In%2520this%2520work%252C%2520we%2520propose%2520Diffusion%2520Bridge%2520Variational%2520Inference%2520%2528DBVI%2529%252C%2520a%2520principled%2520extension%2520of%2520DDVI%2520that%2520initiates%2520the%2520reverse%2520diffusion%2520from%2520a%2520learnable%252C%2520data-dependent%2520initial%2520distribution.%2520This%2520initialization%2520is%2520parameterized%2520via%2520an%2520amortized%2520neural%2520network%2520and%2520progressively%2520adapted%2520using%2520gradients%2520from%2520the%2520ELBO%2520objective%252C%2520reducing%2520the%2520posterior%2520gap%2520and%2520improving%2520sample%2520efficiency.%2520To%2520enable%2520scalable%2520amortization%252C%2520we%2520design%2520the%2520network%2520to%2520operate%2520on%2520the%2520inducing%2520inputs%252C%2520which%2520serve%2520as%2520structured%252C%2520low-dimensional%2520summaries%2520of%2520the%2520dataset%2520and%2520naturally%2520align%2520with%2520the%2520inducing%2520variables%2527%2520shape.%2520DBVI%2520retains%2520the%2520mathematical%2520elegance%2520of%2520DDVI%252C%2520including%2520Girsanov-based%2520ELBOs%2520and%2520reverse-time%2520SDEs%252Cwhile%2520reinterpreting%2520the%2520prior%2520via%2520a%2520Doob-bridged%2520diffusion%2520process.%2520We%2520derive%2520a%2520tractable%2520training%2520objective%2520under%2520this%2520formulation%2520and%2520implement%2520DBVI%2520for%2520scalable%2520inference%2520in%2520large-scale%2520DGPs.%2520Across%2520regression%252C%2520classification%252C%2520and%2520image%2520reconstruction%2520tasks%252C%2520DBVI%2520consistently%2520outperforms%2520DDVI%2520and%2520other%2520variational%2520baselines%2520in%2520predictive%2520accuracy%252C%2520convergence%2520speed%252C%2520and%2520posterior%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19078v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Bridge%20Variational%20Inference%20for%20Deep%20Gaussian%20Processes&entry.906535625=Jian%20Xu%20and%20Qibin%20Zhao%20and%20John%20Paisley%20and%20Delu%20Zeng&entry.1292438233=Deep%20Gaussian%20processes%20%28DGPs%29%20enable%20expressive%20hierarchical%20Bayesian%20modeling%20but%20pose%20substantial%20challenges%20for%20posterior%20inference%2C%20especially%20over%20inducing%20variables.%20Denoising%20diffusion%20variational%20inference%20%28DDVI%29%20addresses%20this%20by%20modeling%20the%20posterior%20as%20a%20time-reversed%20diffusion%20from%20a%20simple%20Gaussian%20prior.%20However%2C%20DDVI%27s%20fixed%20unconditional%20starting%20distribution%20remains%20far%20from%20the%20complex%20true%20posterior%2C%20resulting%20in%20inefficient%20inference%20trajectories%20and%20slow%20convergence.%20In%20this%20work%2C%20we%20propose%20Diffusion%20Bridge%20Variational%20Inference%20%28DBVI%29%2C%20a%20principled%20extension%20of%20DDVI%20that%20initiates%20the%20reverse%20diffusion%20from%20a%20learnable%2C%20data-dependent%20initial%20distribution.%20This%20initialization%20is%20parameterized%20via%20an%20amortized%20neural%20network%20and%20progressively%20adapted%20using%20gradients%20from%20the%20ELBO%20objective%2C%20reducing%20the%20posterior%20gap%20and%20improving%20sample%20efficiency.%20To%20enable%20scalable%20amortization%2C%20we%20design%20the%20network%20to%20operate%20on%20the%20inducing%20inputs%2C%20which%20serve%20as%20structured%2C%20low-dimensional%20summaries%20of%20the%20dataset%20and%20naturally%20align%20with%20the%20inducing%20variables%27%20shape.%20DBVI%20retains%20the%20mathematical%20elegance%20of%20DDVI%2C%20including%20Girsanov-based%20ELBOs%20and%20reverse-time%20SDEs%2Cwhile%20reinterpreting%20the%20prior%20via%20a%20Doob-bridged%20diffusion%20process.%20We%20derive%20a%20tractable%20training%20objective%20under%20this%20formulation%20and%20implement%20DBVI%20for%20scalable%20inference%20in%20large-scale%20DGPs.%20Across%20regression%2C%20classification%2C%20and%20image%20reconstruction%20tasks%2C%20DBVI%20consistently%20outperforms%20DDVI%20and%20other%20variational%20baselines%20in%20predictive%20accuracy%2C%20convergence%20speed%2C%20and%20posterior%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2509.19078v3&entry.124074799=Read"},
{"title": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL", "author": "Jinjie Shen and Jing Wu and Yaxiong Wang and Lechao Cheng and Shengeng Tang and Tianrui Hui and Nan Pu and Zhun Zhong", "abstract": "Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \\textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.", "link": "http://arxiv.org/abs/2602.10687v2", "date": "2026-02-12", "relevancy": 2.1498, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5485}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniVL-Guard%3A%20Towards%20Unified%20Vision-Language%20Forgery%20Detection%20and%20Grounding%20via%20Balanced%20RL&body=Title%3A%20OmniVL-Guard%3A%20Towards%20Unified%20Vision-Language%20Forgery%20Detection%20and%20Grounding%20via%20Balanced%20RL%0AAuthor%3A%20Jinjie%20Shen%20and%20Jing%20Wu%20and%20Yaxiong%20Wang%20and%20Lechao%20Cheng%20and%20Shengeng%20Tang%20and%20Tianrui%20Hui%20and%20Nan%20Pu%20and%20Zhun%20Zhong%0AAbstract%3A%20Existing%20forgery%20detection%20methods%20are%20often%20limited%20to%20uni-modal%20or%20bi-modal%20settings%2C%20failing%20to%20handle%20the%20interleaved%20text%2C%20images%2C%20and%20videos%20prevalent%20in%20real-world%20misinformation.%20To%20bridge%20this%20gap%2C%20this%20paper%20targets%20to%20develop%20a%20unified%20framework%20for%20omnibus%20vision-language%20forgery%20detection%20and%20grounding.%20In%20this%20unified%20setting%2C%20the%20%7Binterplay%7D%20between%20diverse%20modalities%20and%20the%20dual%20requirements%20of%20simultaneous%20detection%20and%20localization%20pose%20a%20critical%20%60%60difficulty%20bias%60%60%20problem%3A%20the%20simpler%20veracity%20classification%20task%20tends%20to%20dominate%20the%20gradients%2C%20leading%20to%20suboptimal%20performance%20in%20fine-grained%20grounding%20during%20multi-task%20optimization.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BOmniVL-Guard%7D%2C%20a%20balanced%20reinforcement%20learning%20framework%20for%20omnibus%20vision-language%20forgery%20detection%20and%20grounding.%20Particularly%2C%20OmniVL-Guard%20comprises%20two%20core%20designs%3A%20Self-Evolving%20CoT%20Generatio%20and%20Adaptive%20Reward%20Scaling%20Policy%20Optimization%20%28ARSPO%29.%20%7BSelf-Evolving%20CoT%20Generation%7D%20synthesizes%20high-quality%20reasoning%20paths%2C%20effectively%20overcoming%20the%20cold-start%20challenge.%20Building%20upon%20this%2C%20%7BAdaptive%20Reward%20Scaling%20Policy%20Optimization%20%28ARSPO%29%7D%20dynamically%20modulates%20reward%20scales%20and%20task%20weights%2C%20ensuring%20a%20balanced%20joint%20optimization.%20Extensive%20experiments%20demonstrate%20that%20OmniVL-Guard%20significantly%20outperforms%20state-of-the-art%20methods%20and%20exhibits%20zero-shot%20robust%20generalization%20across%20out-of-domain%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniVL-Guard%253A%2520Towards%2520Unified%2520Vision-Language%2520Forgery%2520Detection%2520and%2520Grounding%2520via%2520Balanced%2520RL%26entry.906535625%3DJinjie%2520Shen%2520and%2520Jing%2520Wu%2520and%2520Yaxiong%2520Wang%2520and%2520Lechao%2520Cheng%2520and%2520Shengeng%2520Tang%2520and%2520Tianrui%2520Hui%2520and%2520Nan%2520Pu%2520and%2520Zhun%2520Zhong%26entry.1292438233%3DExisting%2520forgery%2520detection%2520methods%2520are%2520often%2520limited%2520to%2520uni-modal%2520or%2520bi-modal%2520settings%252C%2520failing%2520to%2520handle%2520the%2520interleaved%2520text%252C%2520images%252C%2520and%2520videos%2520prevalent%2520in%2520real-world%2520misinformation.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520targets%2520to%2520develop%2520a%2520unified%2520framework%2520for%2520omnibus%2520vision-language%2520forgery%2520detection%2520and%2520grounding.%2520In%2520this%2520unified%2520setting%252C%2520the%2520%257Binterplay%257D%2520between%2520diverse%2520modalities%2520and%2520the%2520dual%2520requirements%2520of%2520simultaneous%2520detection%2520and%2520localization%2520pose%2520a%2520critical%2520%2560%2560difficulty%2520bias%2560%2560%2520problem%253A%2520the%2520simpler%2520veracity%2520classification%2520task%2520tends%2520to%2520dominate%2520the%2520gradients%252C%2520leading%2520to%2520suboptimal%2520performance%2520in%2520fine-grained%2520grounding%2520during%2520multi-task%2520optimization.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%255Ctextbf%257BOmniVL-Guard%257D%252C%2520a%2520balanced%2520reinforcement%2520learning%2520framework%2520for%2520omnibus%2520vision-language%2520forgery%2520detection%2520and%2520grounding.%2520Particularly%252C%2520OmniVL-Guard%2520comprises%2520two%2520core%2520designs%253A%2520Self-Evolving%2520CoT%2520Generatio%2520and%2520Adaptive%2520Reward%2520Scaling%2520Policy%2520Optimization%2520%2528ARSPO%2529.%2520%257BSelf-Evolving%2520CoT%2520Generation%257D%2520synthesizes%2520high-quality%2520reasoning%2520paths%252C%2520effectively%2520overcoming%2520the%2520cold-start%2520challenge.%2520Building%2520upon%2520this%252C%2520%257BAdaptive%2520Reward%2520Scaling%2520Policy%2520Optimization%2520%2528ARSPO%2529%257D%2520dynamically%2520modulates%2520reward%2520scales%2520and%2520task%2520weights%252C%2520ensuring%2520a%2520balanced%2520joint%2520optimization.%2520Extensive%2520experiments%2520demonstrate%2520that%2520OmniVL-Guard%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520and%2520exhibits%2520zero-shot%2520robust%2520generalization%2520across%2520out-of-domain%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniVL-Guard%3A%20Towards%20Unified%20Vision-Language%20Forgery%20Detection%20and%20Grounding%20via%20Balanced%20RL&entry.906535625=Jinjie%20Shen%20and%20Jing%20Wu%20and%20Yaxiong%20Wang%20and%20Lechao%20Cheng%20and%20Shengeng%20Tang%20and%20Tianrui%20Hui%20and%20Nan%20Pu%20and%20Zhun%20Zhong&entry.1292438233=Existing%20forgery%20detection%20methods%20are%20often%20limited%20to%20uni-modal%20or%20bi-modal%20settings%2C%20failing%20to%20handle%20the%20interleaved%20text%2C%20images%2C%20and%20videos%20prevalent%20in%20real-world%20misinformation.%20To%20bridge%20this%20gap%2C%20this%20paper%20targets%20to%20develop%20a%20unified%20framework%20for%20omnibus%20vision-language%20forgery%20detection%20and%20grounding.%20In%20this%20unified%20setting%2C%20the%20%7Binterplay%7D%20between%20diverse%20modalities%20and%20the%20dual%20requirements%20of%20simultaneous%20detection%20and%20localization%20pose%20a%20critical%20%60%60difficulty%20bias%60%60%20problem%3A%20the%20simpler%20veracity%20classification%20task%20tends%20to%20dominate%20the%20gradients%2C%20leading%20to%20suboptimal%20performance%20in%20fine-grained%20grounding%20during%20multi-task%20optimization.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BOmniVL-Guard%7D%2C%20a%20balanced%20reinforcement%20learning%20framework%20for%20omnibus%20vision-language%20forgery%20detection%20and%20grounding.%20Particularly%2C%20OmniVL-Guard%20comprises%20two%20core%20designs%3A%20Self-Evolving%20CoT%20Generatio%20and%20Adaptive%20Reward%20Scaling%20Policy%20Optimization%20%28ARSPO%29.%20%7BSelf-Evolving%20CoT%20Generation%7D%20synthesizes%20high-quality%20reasoning%20paths%2C%20effectively%20overcoming%20the%20cold-start%20challenge.%20Building%20upon%20this%2C%20%7BAdaptive%20Reward%20Scaling%20Policy%20Optimization%20%28ARSPO%29%7D%20dynamically%20modulates%20reward%20scales%20and%20task%20weights%2C%20ensuring%20a%20balanced%20joint%20optimization.%20Extensive%20experiments%20demonstrate%20that%20OmniVL-Guard%20significantly%20outperforms%20state-of-the-art%20methods%20and%20exhibits%20zero-shot%20robust%20generalization%20across%20out-of-domain%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2602.10687v2&entry.124074799=Read"},
{"title": "Synthesis of Late Gadolinium Enhancement Images via Implicit Neural Representations for Cardiac Scar Segmentation", "author": "Soufiane Ben Haddou and Laura Alvarez-Florez and Erik J. Bekkers and Fleur V. Y. Tjong and Ahmad S. Amin and Connie R. Bezzina and Ivana I\u0161gum", "abstract": "Late gadolinium enhancement (LGE) imaging is the clinical standard for myocardial scar assessment, but limited annotated datasets hinder the development of automated segmentation methods. We propose a novel framework that synthesises both LGE images and their corresponding segmentation masks using implicit neural representations (INRs) combined with denoising diffusion models. Our approach first trains INRs to capture continuous spatial representations of LGE data and associated myocardium and fibrosis masks. These INRs are then compressed into compact latent embeddings, preserving essential anatomical information. A diffusion model operates on this latent space to generate new representations, which are decoded into synthetic LGE images with anatomically consistent segmentation masks. Experiments on 133 cardiac MRI scans suggest that augmenting training data with 200 synthetic volumes contributes to improved fibrosis segmentation performance, with the Dice score showing an increase from 0.509 to 0.524. Our approach provides an annotation-free method to help mitigate data scarcity.The code for this research is publicly available.", "link": "http://arxiv.org/abs/2602.11942v1", "date": "2026-02-12", "relevancy": 2.1464, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5505}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5355}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesis%20of%20Late%20Gadolinium%20Enhancement%20Images%20via%20Implicit%20Neural%20Representations%20for%20Cardiac%20Scar%20Segmentation&body=Title%3A%20Synthesis%20of%20Late%20Gadolinium%20Enhancement%20Images%20via%20Implicit%20Neural%20Representations%20for%20Cardiac%20Scar%20Segmentation%0AAuthor%3A%20Soufiane%20Ben%20Haddou%20and%20Laura%20Alvarez-Florez%20and%20Erik%20J.%20Bekkers%20and%20Fleur%20V.%20Y.%20Tjong%20and%20Ahmad%20S.%20Amin%20and%20Connie%20R.%20Bezzina%20and%20Ivana%20I%C5%A1gum%0AAbstract%3A%20Late%20gadolinium%20enhancement%20%28LGE%29%20imaging%20is%20the%20clinical%20standard%20for%20myocardial%20scar%20assessment%2C%20but%20limited%20annotated%20datasets%20hinder%20the%20development%20of%20automated%20segmentation%20methods.%20We%20propose%20a%20novel%20framework%20that%20synthesises%20both%20LGE%20images%20and%20their%20corresponding%20segmentation%20masks%20using%20implicit%20neural%20representations%20%28INRs%29%20combined%20with%20denoising%20diffusion%20models.%20Our%20approach%20first%20trains%20INRs%20to%20capture%20continuous%20spatial%20representations%20of%20LGE%20data%20and%20associated%20myocardium%20and%20fibrosis%20masks.%20These%20INRs%20are%20then%20compressed%20into%20compact%20latent%20embeddings%2C%20preserving%20essential%20anatomical%20information.%20A%20diffusion%20model%20operates%20on%20this%20latent%20space%20to%20generate%20new%20representations%2C%20which%20are%20decoded%20into%20synthetic%20LGE%20images%20with%20anatomically%20consistent%20segmentation%20masks.%20Experiments%20on%20133%20cardiac%20MRI%20scans%20suggest%20that%20augmenting%20training%20data%20with%20200%20synthetic%20volumes%20contributes%20to%20improved%20fibrosis%20segmentation%20performance%2C%20with%20the%20Dice%20score%20showing%20an%20increase%20from%200.509%20to%200.524.%20Our%20approach%20provides%20an%20annotation-free%20method%20to%20help%20mitigate%20data%20scarcity.The%20code%20for%20this%20research%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesis%2520of%2520Late%2520Gadolinium%2520Enhancement%2520Images%2520via%2520Implicit%2520Neural%2520Representations%2520for%2520Cardiac%2520Scar%2520Segmentation%26entry.906535625%3DSoufiane%2520Ben%2520Haddou%2520and%2520Laura%2520Alvarez-Florez%2520and%2520Erik%2520J.%2520Bekkers%2520and%2520Fleur%2520V.%2520Y.%2520Tjong%2520and%2520Ahmad%2520S.%2520Amin%2520and%2520Connie%2520R.%2520Bezzina%2520and%2520Ivana%2520I%25C5%25A1gum%26entry.1292438233%3DLate%2520gadolinium%2520enhancement%2520%2528LGE%2529%2520imaging%2520is%2520the%2520clinical%2520standard%2520for%2520myocardial%2520scar%2520assessment%252C%2520but%2520limited%2520annotated%2520datasets%2520hinder%2520the%2520development%2520of%2520automated%2520segmentation%2520methods.%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520synthesises%2520both%2520LGE%2520images%2520and%2520their%2520corresponding%2520segmentation%2520masks%2520using%2520implicit%2520neural%2520representations%2520%2528INRs%2529%2520combined%2520with%2520denoising%2520diffusion%2520models.%2520Our%2520approach%2520first%2520trains%2520INRs%2520to%2520capture%2520continuous%2520spatial%2520representations%2520of%2520LGE%2520data%2520and%2520associated%2520myocardium%2520and%2520fibrosis%2520masks.%2520These%2520INRs%2520are%2520then%2520compressed%2520into%2520compact%2520latent%2520embeddings%252C%2520preserving%2520essential%2520anatomical%2520information.%2520A%2520diffusion%2520model%2520operates%2520on%2520this%2520latent%2520space%2520to%2520generate%2520new%2520representations%252C%2520which%2520are%2520decoded%2520into%2520synthetic%2520LGE%2520images%2520with%2520anatomically%2520consistent%2520segmentation%2520masks.%2520Experiments%2520on%2520133%2520cardiac%2520MRI%2520scans%2520suggest%2520that%2520augmenting%2520training%2520data%2520with%2520200%2520synthetic%2520volumes%2520contributes%2520to%2520improved%2520fibrosis%2520segmentation%2520performance%252C%2520with%2520the%2520Dice%2520score%2520showing%2520an%2520increase%2520from%25200.509%2520to%25200.524.%2520Our%2520approach%2520provides%2520an%2520annotation-free%2520method%2520to%2520help%2520mitigate%2520data%2520scarcity.The%2520code%2520for%2520this%2520research%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesis%20of%20Late%20Gadolinium%20Enhancement%20Images%20via%20Implicit%20Neural%20Representations%20for%20Cardiac%20Scar%20Segmentation&entry.906535625=Soufiane%20Ben%20Haddou%20and%20Laura%20Alvarez-Florez%20and%20Erik%20J.%20Bekkers%20and%20Fleur%20V.%20Y.%20Tjong%20and%20Ahmad%20S.%20Amin%20and%20Connie%20R.%20Bezzina%20and%20Ivana%20I%C5%A1gum&entry.1292438233=Late%20gadolinium%20enhancement%20%28LGE%29%20imaging%20is%20the%20clinical%20standard%20for%20myocardial%20scar%20assessment%2C%20but%20limited%20annotated%20datasets%20hinder%20the%20development%20of%20automated%20segmentation%20methods.%20We%20propose%20a%20novel%20framework%20that%20synthesises%20both%20LGE%20images%20and%20their%20corresponding%20segmentation%20masks%20using%20implicit%20neural%20representations%20%28INRs%29%20combined%20with%20denoising%20diffusion%20models.%20Our%20approach%20first%20trains%20INRs%20to%20capture%20continuous%20spatial%20representations%20of%20LGE%20data%20and%20associated%20myocardium%20and%20fibrosis%20masks.%20These%20INRs%20are%20then%20compressed%20into%20compact%20latent%20embeddings%2C%20preserving%20essential%20anatomical%20information.%20A%20diffusion%20model%20operates%20on%20this%20latent%20space%20to%20generate%20new%20representations%2C%20which%20are%20decoded%20into%20synthetic%20LGE%20images%20with%20anatomically%20consistent%20segmentation%20masks.%20Experiments%20on%20133%20cardiac%20MRI%20scans%20suggest%20that%20augmenting%20training%20data%20with%20200%20synthetic%20volumes%20contributes%20to%20improved%20fibrosis%20segmentation%20performance%2C%20with%20the%20Dice%20score%20showing%20an%20increase%20from%200.509%20to%200.524.%20Our%20approach%20provides%20an%20annotation-free%20method%20to%20help%20mitigate%20data%20scarcity.The%20code%20for%20this%20research%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2602.11942v1&entry.124074799=Read"},
{"title": "dVoting: Fast Voting for dLLMs", "author": "Sicheng Feng and Zigeng Chen and Xinyin Ma and Gongfan Fang and Xinchao Wang", "abstract": "Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting", "link": "http://arxiv.org/abs/2602.12153v1", "date": "2026-02-12", "relevancy": 2.1328, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5297}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20dVoting%3A%20Fast%20Voting%20for%20dLLMs&body=Title%3A%20dVoting%3A%20Fast%20Voting%20for%20dLLMs%0AAuthor%3A%20Sicheng%20Feng%20and%20Zigeng%20Chen%20and%20Xinyin%20Ma%20and%20Gongfan%20Fang%20and%20Xinchao%20Wang%0AAbstract%3A%20Diffusion%20Large%20Language%20Models%20%28dLLMs%29%20represent%20a%20new%20paradigm%20beyond%20autoregressive%20modeling%2C%20offering%20competitive%20performance%20while%20naturally%20enabling%20a%20flexible%20decoding%20process.%20Specifically%2C%20dLLMs%20can%20generate%20tokens%20at%20arbitrary%20positions%20in%20parallel%2C%20endowing%20them%20with%20significant%20potential%20for%20parallel%20test-time%20scaling%2C%20which%20was%20previously%20constrained%20by%20severe%20inefficiency%20in%20autoregressive%20modeling.%20In%20this%20work%2C%20we%20introduce%20dVoting%2C%20a%20fast%20voting%20technique%20that%20boosts%20reasoning%20capability%20without%20training%2C%20with%20only%20an%20acceptable%20extra%20computational%20overhead.%20dVoting%20is%20motivated%20by%20the%20observation%20that%2C%20across%20multiple%20samples%20for%20the%20same%20prompt%2C%20token%20predictions%20remain%20largely%20consistent%2C%20whereas%20performance%20is%20determined%20by%20a%20small%20subset%20of%20tokens%20exhibiting%20cross-sample%20variability.%20Leveraging%20the%20arbitrary-position%20generation%20capability%20of%20dLLMs%2C%20dVoting%20performs%20iterative%20refinement%20by%20sampling%2C%20identifying%20uncertain%20tokens%20via%20consistency%20analysis%2C%20regenerating%20them%20through%20voting%2C%20and%20repeating%20this%20process%20until%20convergence.%20Extensive%20evaluations%20demonstrate%20that%20dVoting%20consistently%20improves%20performance%20across%20various%20benchmarks.%20It%20achieves%20gains%20of%206.22%25-7.66%25%20on%20GSM8K%2C%204.40%25-7.20%25%20on%20MATH500%2C%203.16%25-14.84%25%20on%20ARC-C%2C%20and%204.83%25-5.74%25%20on%20MMLU.%20Our%20code%20is%20available%20at%20https%3A//github.com/fscdc/dVoting%0ALink%3A%20http%3A//arxiv.org/abs/2602.12153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DdVoting%253A%2520Fast%2520Voting%2520for%2520dLLMs%26entry.906535625%3DSicheng%2520Feng%2520and%2520Zigeng%2520Chen%2520and%2520Xinyin%2520Ma%2520and%2520Gongfan%2520Fang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3DDiffusion%2520Large%2520Language%2520Models%2520%2528dLLMs%2529%2520represent%2520a%2520new%2520paradigm%2520beyond%2520autoregressive%2520modeling%252C%2520offering%2520competitive%2520performance%2520while%2520naturally%2520enabling%2520a%2520flexible%2520decoding%2520process.%2520Specifically%252C%2520dLLMs%2520can%2520generate%2520tokens%2520at%2520arbitrary%2520positions%2520in%2520parallel%252C%2520endowing%2520them%2520with%2520significant%2520potential%2520for%2520parallel%2520test-time%2520scaling%252C%2520which%2520was%2520previously%2520constrained%2520by%2520severe%2520inefficiency%2520in%2520autoregressive%2520modeling.%2520In%2520this%2520work%252C%2520we%2520introduce%2520dVoting%252C%2520a%2520fast%2520voting%2520technique%2520that%2520boosts%2520reasoning%2520capability%2520without%2520training%252C%2520with%2520only%2520an%2520acceptable%2520extra%2520computational%2520overhead.%2520dVoting%2520is%2520motivated%2520by%2520the%2520observation%2520that%252C%2520across%2520multiple%2520samples%2520for%2520the%2520same%2520prompt%252C%2520token%2520predictions%2520remain%2520largely%2520consistent%252C%2520whereas%2520performance%2520is%2520determined%2520by%2520a%2520small%2520subset%2520of%2520tokens%2520exhibiting%2520cross-sample%2520variability.%2520Leveraging%2520the%2520arbitrary-position%2520generation%2520capability%2520of%2520dLLMs%252C%2520dVoting%2520performs%2520iterative%2520refinement%2520by%2520sampling%252C%2520identifying%2520uncertain%2520tokens%2520via%2520consistency%2520analysis%252C%2520regenerating%2520them%2520through%2520voting%252C%2520and%2520repeating%2520this%2520process%2520until%2520convergence.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520dVoting%2520consistently%2520improves%2520performance%2520across%2520various%2520benchmarks.%2520It%2520achieves%2520gains%2520of%25206.22%2525-7.66%2525%2520on%2520GSM8K%252C%25204.40%2525-7.20%2525%2520on%2520MATH500%252C%25203.16%2525-14.84%2525%2520on%2520ARC-C%252C%2520and%25204.83%2525-5.74%2525%2520on%2520MMLU.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/fscdc/dVoting%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=dVoting%3A%20Fast%20Voting%20for%20dLLMs&entry.906535625=Sicheng%20Feng%20and%20Zigeng%20Chen%20and%20Xinyin%20Ma%20and%20Gongfan%20Fang%20and%20Xinchao%20Wang&entry.1292438233=Diffusion%20Large%20Language%20Models%20%28dLLMs%29%20represent%20a%20new%20paradigm%20beyond%20autoregressive%20modeling%2C%20offering%20competitive%20performance%20while%20naturally%20enabling%20a%20flexible%20decoding%20process.%20Specifically%2C%20dLLMs%20can%20generate%20tokens%20at%20arbitrary%20positions%20in%20parallel%2C%20endowing%20them%20with%20significant%20potential%20for%20parallel%20test-time%20scaling%2C%20which%20was%20previously%20constrained%20by%20severe%20inefficiency%20in%20autoregressive%20modeling.%20In%20this%20work%2C%20we%20introduce%20dVoting%2C%20a%20fast%20voting%20technique%20that%20boosts%20reasoning%20capability%20without%20training%2C%20with%20only%20an%20acceptable%20extra%20computational%20overhead.%20dVoting%20is%20motivated%20by%20the%20observation%20that%2C%20across%20multiple%20samples%20for%20the%20same%20prompt%2C%20token%20predictions%20remain%20largely%20consistent%2C%20whereas%20performance%20is%20determined%20by%20a%20small%20subset%20of%20tokens%20exhibiting%20cross-sample%20variability.%20Leveraging%20the%20arbitrary-position%20generation%20capability%20of%20dLLMs%2C%20dVoting%20performs%20iterative%20refinement%20by%20sampling%2C%20identifying%20uncertain%20tokens%20via%20consistency%20analysis%2C%20regenerating%20them%20through%20voting%2C%20and%20repeating%20this%20process%20until%20convergence.%20Extensive%20evaluations%20demonstrate%20that%20dVoting%20consistently%20improves%20performance%20across%20various%20benchmarks.%20It%20achieves%20gains%20of%206.22%25-7.66%25%20on%20GSM8K%2C%204.40%25-7.20%25%20on%20MATH500%2C%203.16%25-14.84%25%20on%20ARC-C%2C%20and%204.83%25-5.74%25%20on%20MMLU.%20Our%20code%20is%20available%20at%20https%3A//github.com/fscdc/dVoting&entry.1838667208=http%3A//arxiv.org/abs/2602.12153v1&entry.124074799=Read"},
{"title": "HLA: Hadamard Linear Attention", "author": "Hanno Ackermann and Hong Cai and Mohsen Ghafoorian and Amirhossein Habibian", "abstract": "The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.\n  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.", "link": "http://arxiv.org/abs/2602.12128v1", "date": "2026-02-12", "relevancy": 2.1294, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5781}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HLA%3A%20Hadamard%20Linear%20Attention&body=Title%3A%20HLA%3A%20Hadamard%20Linear%20Attention%0AAuthor%3A%20Hanno%20Ackermann%20and%20Hong%20Cai%20and%20Mohsen%20Ghafoorian%20and%20Amirhossein%20Habibian%0AAbstract%3A%20The%20attention%20mechanism%20is%20an%20important%20reason%20for%20the%20success%20of%20transformers.%20It%20relies%20on%20computing%20pairwise%20relations%20between%20tokens.%20To%20reduce%20the%20high%20computational%20cost%20of%20standard%20quadratic%20attention%2C%20linear%20attention%20has%20been%20proposed%20as%20an%20efficient%20approximation.%20It%20employs%20kernel%20functions%20that%20are%20applied%20independently%20to%20the%20inputs%20before%20the%20pairwise%20similarities%20are%20calculated.%20That%20allows%20for%20an%20efficient%20computational%20procedure%20which%2C%20however%2C%20amounts%20to%20a%20low-degree%20rational%20function%20approximating%20softmax.%0A%20%20We%20propose%20Hadamard%20Linear%20Attention%20%28HLA%29.%20Unlike%20previous%20works%20on%20linear%20attention%2C%20the%20nonlinearity%20in%20HLA%20is%20not%20applied%20separately%20to%20queries%20and%20keys%2C%20but%2C%20analogously%20to%20standard%20softmax%20attention%2C%20after%20the%20pairwise%20similarities%20have%20been%20computed.%20It%20will%20be%20shown%20that%20the%20proposed%20nonlinearity%20amounts%20to%20a%20higher-degree%20rational%20function%20to%20approximate%20softmax.%20An%20efficient%20computational%20scheme%20for%20the%20proposed%20method%20is%20derived%20that%20is%20similar%20to%20that%20of%20standard%20linear%20attention.%20In%20contrast%20to%20other%20approaches%2C%20no%20time-consuming%20tensor%20reshaping%20is%20necessary%20to%20apply%20the%20proposed%20algorithm.%20The%20effectiveness%20of%20the%20approach%20is%20demonstrated%20by%20applying%20it%20to%20a%20large%20diffusion%20transformer%20model%20for%20video%20generation%2C%20an%20application%20that%20involves%20very%20large%20amounts%20of%20tokens.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHLA%253A%2520Hadamard%2520Linear%2520Attention%26entry.906535625%3DHanno%2520Ackermann%2520and%2520Hong%2520Cai%2520and%2520Mohsen%2520Ghafoorian%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3DThe%2520attention%2520mechanism%2520is%2520an%2520important%2520reason%2520for%2520the%2520success%2520of%2520transformers.%2520It%2520relies%2520on%2520computing%2520pairwise%2520relations%2520between%2520tokens.%2520To%2520reduce%2520the%2520high%2520computational%2520cost%2520of%2520standard%2520quadratic%2520attention%252C%2520linear%2520attention%2520has%2520been%2520proposed%2520as%2520an%2520efficient%2520approximation.%2520It%2520employs%2520kernel%2520functions%2520that%2520are%2520applied%2520independently%2520to%2520the%2520inputs%2520before%2520the%2520pairwise%2520similarities%2520are%2520calculated.%2520That%2520allows%2520for%2520an%2520efficient%2520computational%2520procedure%2520which%252C%2520however%252C%2520amounts%2520to%2520a%2520low-degree%2520rational%2520function%2520approximating%2520softmax.%250A%2520%2520We%2520propose%2520Hadamard%2520Linear%2520Attention%2520%2528HLA%2529.%2520Unlike%2520previous%2520works%2520on%2520linear%2520attention%252C%2520the%2520nonlinearity%2520in%2520HLA%2520is%2520not%2520applied%2520separately%2520to%2520queries%2520and%2520keys%252C%2520but%252C%2520analogously%2520to%2520standard%2520softmax%2520attention%252C%2520after%2520the%2520pairwise%2520similarities%2520have%2520been%2520computed.%2520It%2520will%2520be%2520shown%2520that%2520the%2520proposed%2520nonlinearity%2520amounts%2520to%2520a%2520higher-degree%2520rational%2520function%2520to%2520approximate%2520softmax.%2520An%2520efficient%2520computational%2520scheme%2520for%2520the%2520proposed%2520method%2520is%2520derived%2520that%2520is%2520similar%2520to%2520that%2520of%2520standard%2520linear%2520attention.%2520In%2520contrast%2520to%2520other%2520approaches%252C%2520no%2520time-consuming%2520tensor%2520reshaping%2520is%2520necessary%2520to%2520apply%2520the%2520proposed%2520algorithm.%2520The%2520effectiveness%2520of%2520the%2520approach%2520is%2520demonstrated%2520by%2520applying%2520it%2520to%2520a%2520large%2520diffusion%2520transformer%2520model%2520for%2520video%2520generation%252C%2520an%2520application%2520that%2520involves%2520very%2520large%2520amounts%2520of%2520tokens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HLA%3A%20Hadamard%20Linear%20Attention&entry.906535625=Hanno%20Ackermann%20and%20Hong%20Cai%20and%20Mohsen%20Ghafoorian%20and%20Amirhossein%20Habibian&entry.1292438233=The%20attention%20mechanism%20is%20an%20important%20reason%20for%20the%20success%20of%20transformers.%20It%20relies%20on%20computing%20pairwise%20relations%20between%20tokens.%20To%20reduce%20the%20high%20computational%20cost%20of%20standard%20quadratic%20attention%2C%20linear%20attention%20has%20been%20proposed%20as%20an%20efficient%20approximation.%20It%20employs%20kernel%20functions%20that%20are%20applied%20independently%20to%20the%20inputs%20before%20the%20pairwise%20similarities%20are%20calculated.%20That%20allows%20for%20an%20efficient%20computational%20procedure%20which%2C%20however%2C%20amounts%20to%20a%20low-degree%20rational%20function%20approximating%20softmax.%0A%20%20We%20propose%20Hadamard%20Linear%20Attention%20%28HLA%29.%20Unlike%20previous%20works%20on%20linear%20attention%2C%20the%20nonlinearity%20in%20HLA%20is%20not%20applied%20separately%20to%20queries%20and%20keys%2C%20but%2C%20analogously%20to%20standard%20softmax%20attention%2C%20after%20the%20pairwise%20similarities%20have%20been%20computed.%20It%20will%20be%20shown%20that%20the%20proposed%20nonlinearity%20amounts%20to%20a%20higher-degree%20rational%20function%20to%20approximate%20softmax.%20An%20efficient%20computational%20scheme%20for%20the%20proposed%20method%20is%20derived%20that%20is%20similar%20to%20that%20of%20standard%20linear%20attention.%20In%20contrast%20to%20other%20approaches%2C%20no%20time-consuming%20tensor%20reshaping%20is%20necessary%20to%20apply%20the%20proposed%20algorithm.%20The%20effectiveness%20of%20the%20approach%20is%20demonstrated%20by%20applying%20it%20to%20a%20large%20diffusion%20transformer%20model%20for%20video%20generation%2C%20an%20application%20that%20involves%20very%20large%20amounts%20of%20tokens.&entry.1838667208=http%3A//arxiv.org/abs/2602.12128v1&entry.124074799=Read"},
{"title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale", "author": "Damon McMillan", "abstract": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables. Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact or novel formats can incur a token overhead driven by grep output density and pattern unfamiliarity, with the magnitude depending on model capability. These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.", "link": "http://arxiv.org/abs/2602.05447v2", "date": "2026-02-12", "relevancy": 2.1243, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Context%20Engineering%20for%20File-Native%20Agentic%20Systems%3A%20Evaluating%20Schema%20Accuracy%2C%20Format%20Effectiveness%2C%20and%20Multi-File%20Navigation%20at%20Scale&body=Title%3A%20Structured%20Context%20Engineering%20for%20File-Native%20Agentic%20Systems%3A%20Evaluating%20Schema%20Accuracy%2C%20Format%20Effectiveness%2C%20and%20Multi-File%20Navigation%20at%20Scale%0AAuthor%3A%20Damon%20McMillan%0AAbstract%3A%20Large%20Language%20Model%20agents%20increasingly%20operate%20external%20systems%20through%20programmatic%20interfaces%2C%20yet%20practitioners%20lack%20empirical%20guidance%20on%20how%20to%20structure%20the%20context%20these%20agents%20consume.%20Using%20SQL%20generation%20as%20a%20proxy%20for%20programmatic%20agent%20operations%2C%20we%20present%20a%20systematic%20study%20of%20context%20engineering%20for%20structured%20data%2C%20comprising%209%2C649%20experiments%20across%2011%20models%2C%204%20formats%20%28YAML%2C%20Markdown%2C%20JSON%2C%20Token-Oriented%20Object%20Notation%20%5BTOON%5D%29%2C%20and%20schemas%20ranging%20from%2010%20to%2010%2C000%20tables.%20Our%20findings%20challenge%20common%20assumptions.%20First%2C%20architecture%20choice%20is%20model-dependent%3A%20file-based%20context%20retrieval%20improves%20accuracy%20for%20frontier-tier%20models%20%28Claude%2C%20GPT%2C%20Gemini%3B%20%2B2.7%25%2C%20p%3D0.029%29%20but%20shows%20mixed%20results%20for%20open%20source%20models%20%28aggregate%20-7.7%25%2C%20p%3C0.001%29%2C%20with%20deficits%20varying%20substantially%20by%20model.%20Second%2C%20format%20does%20not%20significantly%20affect%20aggregate%20accuracy%20%28chi-squared%3D2.45%2C%20p%3D0.484%29%2C%20though%20individual%20models%2C%20particularly%20open%20source%2C%20exhibit%20format-specific%20sensitivities.%20Third%2C%20model%20capability%20is%20the%20dominant%20factor%2C%20with%20a%2021%20percentage%20point%20accuracy%20gap%20between%20frontier%20and%20open%20source%20tiers%20that%20dwarfs%20any%20format%20or%20architecture%20effect.%20Fourth%2C%20file-native%20agents%20scale%20to%2010%2C000%20tables%20through%20domain-partitioned%20schemas%20while%20maintaining%20high%20navigation%20accuracy.%20Fifth%2C%20file%20size%20does%20not%20predict%20runtime%20efficiency%3A%20compact%20or%20novel%20formats%20can%20incur%20a%20token%20overhead%20driven%20by%20grep%20output%20density%20and%20pattern%20unfamiliarity%2C%20with%20the%20magnitude%20depending%20on%20model%20capability.%20These%20findings%20provide%20practitioners%20with%20evidence-based%20guidance%20for%20deploying%20LLM%20agents%20on%20structured%20systems%2C%20demonstrating%20that%20architectural%20decisions%20should%20be%20tailored%20to%20model%20capability%20rather%20than%20assuming%20universal%20best%20practices.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Context%2520Engineering%2520for%2520File-Native%2520Agentic%2520Systems%253A%2520Evaluating%2520Schema%2520Accuracy%252C%2520Format%2520Effectiveness%252C%2520and%2520Multi-File%2520Navigation%2520at%2520Scale%26entry.906535625%3DDamon%2520McMillan%26entry.1292438233%3DLarge%2520Language%2520Model%2520agents%2520increasingly%2520operate%2520external%2520systems%2520through%2520programmatic%2520interfaces%252C%2520yet%2520practitioners%2520lack%2520empirical%2520guidance%2520on%2520how%2520to%2520structure%2520the%2520context%2520these%2520agents%2520consume.%2520Using%2520SQL%2520generation%2520as%2520a%2520proxy%2520for%2520programmatic%2520agent%2520operations%252C%2520we%2520present%2520a%2520systematic%2520study%2520of%2520context%2520engineering%2520for%2520structured%2520data%252C%2520comprising%25209%252C649%2520experiments%2520across%252011%2520models%252C%25204%2520formats%2520%2528YAML%252C%2520Markdown%252C%2520JSON%252C%2520Token-Oriented%2520Object%2520Notation%2520%255BTOON%255D%2529%252C%2520and%2520schemas%2520ranging%2520from%252010%2520to%252010%252C000%2520tables.%2520Our%2520findings%2520challenge%2520common%2520assumptions.%2520First%252C%2520architecture%2520choice%2520is%2520model-dependent%253A%2520file-based%2520context%2520retrieval%2520improves%2520accuracy%2520for%2520frontier-tier%2520models%2520%2528Claude%252C%2520GPT%252C%2520Gemini%253B%2520%252B2.7%2525%252C%2520p%253D0.029%2529%2520but%2520shows%2520mixed%2520results%2520for%2520open%2520source%2520models%2520%2528aggregate%2520-7.7%2525%252C%2520p%253C0.001%2529%252C%2520with%2520deficits%2520varying%2520substantially%2520by%2520model.%2520Second%252C%2520format%2520does%2520not%2520significantly%2520affect%2520aggregate%2520accuracy%2520%2528chi-squared%253D2.45%252C%2520p%253D0.484%2529%252C%2520though%2520individual%2520models%252C%2520particularly%2520open%2520source%252C%2520exhibit%2520format-specific%2520sensitivities.%2520Third%252C%2520model%2520capability%2520is%2520the%2520dominant%2520factor%252C%2520with%2520a%252021%2520percentage%2520point%2520accuracy%2520gap%2520between%2520frontier%2520and%2520open%2520source%2520tiers%2520that%2520dwarfs%2520any%2520format%2520or%2520architecture%2520effect.%2520Fourth%252C%2520file-native%2520agents%2520scale%2520to%252010%252C000%2520tables%2520through%2520domain-partitioned%2520schemas%2520while%2520maintaining%2520high%2520navigation%2520accuracy.%2520Fifth%252C%2520file%2520size%2520does%2520not%2520predict%2520runtime%2520efficiency%253A%2520compact%2520or%2520novel%2520formats%2520can%2520incur%2520a%2520token%2520overhead%2520driven%2520by%2520grep%2520output%2520density%2520and%2520pattern%2520unfamiliarity%252C%2520with%2520the%2520magnitude%2520depending%2520on%2520model%2520capability.%2520These%2520findings%2520provide%2520practitioners%2520with%2520evidence-based%2520guidance%2520for%2520deploying%2520LLM%2520agents%2520on%2520structured%2520systems%252C%2520demonstrating%2520that%2520architectural%2520decisions%2520should%2520be%2520tailored%2520to%2520model%2520capability%2520rather%2520than%2520assuming%2520universal%2520best%2520practices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Context%20Engineering%20for%20File-Native%20Agentic%20Systems%3A%20Evaluating%20Schema%20Accuracy%2C%20Format%20Effectiveness%2C%20and%20Multi-File%20Navigation%20at%20Scale&entry.906535625=Damon%20McMillan&entry.1292438233=Large%20Language%20Model%20agents%20increasingly%20operate%20external%20systems%20through%20programmatic%20interfaces%2C%20yet%20practitioners%20lack%20empirical%20guidance%20on%20how%20to%20structure%20the%20context%20these%20agents%20consume.%20Using%20SQL%20generation%20as%20a%20proxy%20for%20programmatic%20agent%20operations%2C%20we%20present%20a%20systematic%20study%20of%20context%20engineering%20for%20structured%20data%2C%20comprising%209%2C649%20experiments%20across%2011%20models%2C%204%20formats%20%28YAML%2C%20Markdown%2C%20JSON%2C%20Token-Oriented%20Object%20Notation%20%5BTOON%5D%29%2C%20and%20schemas%20ranging%20from%2010%20to%2010%2C000%20tables.%20Our%20findings%20challenge%20common%20assumptions.%20First%2C%20architecture%20choice%20is%20model-dependent%3A%20file-based%20context%20retrieval%20improves%20accuracy%20for%20frontier-tier%20models%20%28Claude%2C%20GPT%2C%20Gemini%3B%20%2B2.7%25%2C%20p%3D0.029%29%20but%20shows%20mixed%20results%20for%20open%20source%20models%20%28aggregate%20-7.7%25%2C%20p%3C0.001%29%2C%20with%20deficits%20varying%20substantially%20by%20model.%20Second%2C%20format%20does%20not%20significantly%20affect%20aggregate%20accuracy%20%28chi-squared%3D2.45%2C%20p%3D0.484%29%2C%20though%20individual%20models%2C%20particularly%20open%20source%2C%20exhibit%20format-specific%20sensitivities.%20Third%2C%20model%20capability%20is%20the%20dominant%20factor%2C%20with%20a%2021%20percentage%20point%20accuracy%20gap%20between%20frontier%20and%20open%20source%20tiers%20that%20dwarfs%20any%20format%20or%20architecture%20effect.%20Fourth%2C%20file-native%20agents%20scale%20to%2010%2C000%20tables%20through%20domain-partitioned%20schemas%20while%20maintaining%20high%20navigation%20accuracy.%20Fifth%2C%20file%20size%20does%20not%20predict%20runtime%20efficiency%3A%20compact%20or%20novel%20formats%20can%20incur%20a%20token%20overhead%20driven%20by%20grep%20output%20density%20and%20pattern%20unfamiliarity%2C%20with%20the%20magnitude%20depending%20on%20model%20capability.%20These%20findings%20provide%20practitioners%20with%20evidence-based%20guidance%20for%20deploying%20LLM%20agents%20on%20structured%20systems%2C%20demonstrating%20that%20architectural%20decisions%20should%20be%20tailored%20to%20model%20capability%20rather%20than%20assuming%20universal%20best%20practices.&entry.1838667208=http%3A//arxiv.org/abs/2602.05447v2&entry.124074799=Read"},
{"title": "Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation", "author": "Pingjun Pan and Tingting Zhou and Peiyao Lu and Tingting Fei and Hongxiang Chen and Chuanjiang Luo", "abstract": "Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.", "link": "http://arxiv.org/abs/2602.11799v1", "date": "2026-02-12", "relevancy": 2.1189, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5766}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5217}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi-SAM%3A%20A%20Hierarchical%20Structure-Aware%20Multi-modal%20Framework%20for%20Large-Scale%20Recommendation&body=Title%3A%20Hi-SAM%3A%20A%20Hierarchical%20Structure-Aware%20Multi-modal%20Framework%20for%20Large-Scale%20Recommendation%0AAuthor%3A%20Pingjun%20Pan%20and%20Tingting%20Zhou%20and%20Peiyao%20Lu%20and%20Tingting%20Fei%20and%20Hongxiang%20Chen%20and%20Chuanjiang%20Luo%0AAbstract%3A%20Multi-modal%20recommendation%20has%20gained%20traction%20as%20items%20possess%20rich%20attributes%20like%20text%20and%20images.%20Semantic%20ID-based%20approaches%20effectively%20discretize%20this%20information%20into%20compact%20tokens.%20However%2C%20two%20challenges%20persist%3A%20%281%29%20Suboptimal%20Tokenization%3A%20existing%20methods%20%28e.g.%2C%20RQ-VAE%29%20lack%20disentanglement%20between%20shared%20cross-modal%20semantics%20and%20modality-specific%20details%2C%20causing%20redundancy%20or%20collapse%3B%20%282%29%20Architecture-Data%20Mismatch%3A%20vanilla%20Transformers%20treat%20semantic%20IDs%20as%20flat%20streams%2C%20ignoring%20the%20hierarchy%20of%20user%20interactions%2C%20items%2C%20and%20tokens.%20Expanding%20items%20into%20multiple%20tokens%20amplifies%20length%20and%20noise%2C%20biasing%20attention%20toward%20local%20details%20over%20holistic%20semantics.%20We%20propose%20Hi-SAM%2C%20a%20Hierarchical%20Structure-Aware%20Multi-modal%20framework%20with%20two%20designs%3A%20%281%29%20Disentangled%20Semantic%20Tokenizer%20%28DST%29%3A%20unifies%20modalities%20via%20geometry-aware%20alignment%20and%20quantizes%20them%20via%20a%20coarse-to-fine%20strategy.%20Shared%20codebooks%20distill%20consensus%20while%20modality-specific%20ones%20recover%20nuances%20from%20residuals%2C%20enforced%20by%20mutual%20information%20minimization%3B%20%282%29%20Hierarchical%20Memory-Anchor%20Transformer%20%28HMAT%29%3A%20splits%20positional%20encoding%20into%20inter-%20and%20intra-item%20subspaces%20via%20Hierarchical%20RoPE%20to%20restore%20hierarchy.%20It%20inserts%20Anchor%20Tokens%20to%20condense%20items%20into%20compact%20memory%2C%20retaining%20details%20for%20the%20current%20item%20while%20accessing%20history%20only%20through%20compressed%20summaries.%20Experiments%20on%20real-world%20datasets%20show%20consistent%20improvements%20over%20SOTA%20baselines%2C%20especially%20in%20cold-start%20scenarios.%20Deployed%20on%20a%20large-scale%20social%20platform%20serving%20millions%20of%20users%2C%20Hi-SAM%20achieved%20a%206.55%25%20gain%20in%20the%20core%20online%20metric.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi-SAM%253A%2520A%2520Hierarchical%2520Structure-Aware%2520Multi-modal%2520Framework%2520for%2520Large-Scale%2520Recommendation%26entry.906535625%3DPingjun%2520Pan%2520and%2520Tingting%2520Zhou%2520and%2520Peiyao%2520Lu%2520and%2520Tingting%2520Fei%2520and%2520Hongxiang%2520Chen%2520and%2520Chuanjiang%2520Luo%26entry.1292438233%3DMulti-modal%2520recommendation%2520has%2520gained%2520traction%2520as%2520items%2520possess%2520rich%2520attributes%2520like%2520text%2520and%2520images.%2520Semantic%2520ID-based%2520approaches%2520effectively%2520discretize%2520this%2520information%2520into%2520compact%2520tokens.%2520However%252C%2520two%2520challenges%2520persist%253A%2520%25281%2529%2520Suboptimal%2520Tokenization%253A%2520existing%2520methods%2520%2528e.g.%252C%2520RQ-VAE%2529%2520lack%2520disentanglement%2520between%2520shared%2520cross-modal%2520semantics%2520and%2520modality-specific%2520details%252C%2520causing%2520redundancy%2520or%2520collapse%253B%2520%25282%2529%2520Architecture-Data%2520Mismatch%253A%2520vanilla%2520Transformers%2520treat%2520semantic%2520IDs%2520as%2520flat%2520streams%252C%2520ignoring%2520the%2520hierarchy%2520of%2520user%2520interactions%252C%2520items%252C%2520and%2520tokens.%2520Expanding%2520items%2520into%2520multiple%2520tokens%2520amplifies%2520length%2520and%2520noise%252C%2520biasing%2520attention%2520toward%2520local%2520details%2520over%2520holistic%2520semantics.%2520We%2520propose%2520Hi-SAM%252C%2520a%2520Hierarchical%2520Structure-Aware%2520Multi-modal%2520framework%2520with%2520two%2520designs%253A%2520%25281%2529%2520Disentangled%2520Semantic%2520Tokenizer%2520%2528DST%2529%253A%2520unifies%2520modalities%2520via%2520geometry-aware%2520alignment%2520and%2520quantizes%2520them%2520via%2520a%2520coarse-to-fine%2520strategy.%2520Shared%2520codebooks%2520distill%2520consensus%2520while%2520modality-specific%2520ones%2520recover%2520nuances%2520from%2520residuals%252C%2520enforced%2520by%2520mutual%2520information%2520minimization%253B%2520%25282%2529%2520Hierarchical%2520Memory-Anchor%2520Transformer%2520%2528HMAT%2529%253A%2520splits%2520positional%2520encoding%2520into%2520inter-%2520and%2520intra-item%2520subspaces%2520via%2520Hierarchical%2520RoPE%2520to%2520restore%2520hierarchy.%2520It%2520inserts%2520Anchor%2520Tokens%2520to%2520condense%2520items%2520into%2520compact%2520memory%252C%2520retaining%2520details%2520for%2520the%2520current%2520item%2520while%2520accessing%2520history%2520only%2520through%2520compressed%2520summaries.%2520Experiments%2520on%2520real-world%2520datasets%2520show%2520consistent%2520improvements%2520over%2520SOTA%2520baselines%252C%2520especially%2520in%2520cold-start%2520scenarios.%2520Deployed%2520on%2520a%2520large-scale%2520social%2520platform%2520serving%2520millions%2520of%2520users%252C%2520Hi-SAM%2520achieved%2520a%25206.55%2525%2520gain%2520in%2520the%2520core%2520online%2520metric.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi-SAM%3A%20A%20Hierarchical%20Structure-Aware%20Multi-modal%20Framework%20for%20Large-Scale%20Recommendation&entry.906535625=Pingjun%20Pan%20and%20Tingting%20Zhou%20and%20Peiyao%20Lu%20and%20Tingting%20Fei%20and%20Hongxiang%20Chen%20and%20Chuanjiang%20Luo&entry.1292438233=Multi-modal%20recommendation%20has%20gained%20traction%20as%20items%20possess%20rich%20attributes%20like%20text%20and%20images.%20Semantic%20ID-based%20approaches%20effectively%20discretize%20this%20information%20into%20compact%20tokens.%20However%2C%20two%20challenges%20persist%3A%20%281%29%20Suboptimal%20Tokenization%3A%20existing%20methods%20%28e.g.%2C%20RQ-VAE%29%20lack%20disentanglement%20between%20shared%20cross-modal%20semantics%20and%20modality-specific%20details%2C%20causing%20redundancy%20or%20collapse%3B%20%282%29%20Architecture-Data%20Mismatch%3A%20vanilla%20Transformers%20treat%20semantic%20IDs%20as%20flat%20streams%2C%20ignoring%20the%20hierarchy%20of%20user%20interactions%2C%20items%2C%20and%20tokens.%20Expanding%20items%20into%20multiple%20tokens%20amplifies%20length%20and%20noise%2C%20biasing%20attention%20toward%20local%20details%20over%20holistic%20semantics.%20We%20propose%20Hi-SAM%2C%20a%20Hierarchical%20Structure-Aware%20Multi-modal%20framework%20with%20two%20designs%3A%20%281%29%20Disentangled%20Semantic%20Tokenizer%20%28DST%29%3A%20unifies%20modalities%20via%20geometry-aware%20alignment%20and%20quantizes%20them%20via%20a%20coarse-to-fine%20strategy.%20Shared%20codebooks%20distill%20consensus%20while%20modality-specific%20ones%20recover%20nuances%20from%20residuals%2C%20enforced%20by%20mutual%20information%20minimization%3B%20%282%29%20Hierarchical%20Memory-Anchor%20Transformer%20%28HMAT%29%3A%20splits%20positional%20encoding%20into%20inter-%20and%20intra-item%20subspaces%20via%20Hierarchical%20RoPE%20to%20restore%20hierarchy.%20It%20inserts%20Anchor%20Tokens%20to%20condense%20items%20into%20compact%20memory%2C%20retaining%20details%20for%20the%20current%20item%20while%20accessing%20history%20only%20through%20compressed%20summaries.%20Experiments%20on%20real-world%20datasets%20show%20consistent%20improvements%20over%20SOTA%20baselines%2C%20especially%20in%20cold-start%20scenarios.%20Deployed%20on%20a%20large-scale%20social%20platform%20serving%20millions%20of%20users%2C%20Hi-SAM%20achieved%20a%206.55%25%20gain%20in%20the%20core%20online%20metric.&entry.1838667208=http%3A//arxiv.org/abs/2602.11799v1&entry.124074799=Read"},
{"title": "Safety Beyond the Training Data: Robust Out-of-Distribution MPC via Conformalized System Level Synthesis", "author": "Anutam Srinivasan and Antoine Leeman and Glen Chou", "abstract": "We present a novel framework for robust out-of-distribution planning and control using conformal prediction (CP) and system level synthesis (SLS), addressing the challenge of ensuring safety and robustness when using learned dynamics models beyond the training data distribution. We first derive high-confidence model error bounds using weighted CP with a learned, state-control-dependent covariance model. These bounds are integrated into an SLS-based robust nonlinear model predictive control (MPC) formulation, which performs constraint tightening over the prediction horizon via volume-optimized forward reachable sets. We provide theoretical guarantees on coverage and robustness under distributional drift, and analyze the impact of data density and trajectory tube size on prediction coverage. Empirically, we demonstrate our method on nonlinear systems of increasing complexity, including a 4D car and a {12D} quadcopter, improving safety and robustness compared to fixed-bound and non-robust baselines, especially outside of the data distribution.", "link": "http://arxiv.org/abs/2602.12047v1", "date": "2026-02-12", "relevancy": 2.1105, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Beyond%20the%20Training%20Data%3A%20Robust%20Out-of-Distribution%20MPC%20via%20Conformalized%20System%20Level%20Synthesis&body=Title%3A%20Safety%20Beyond%20the%20Training%20Data%3A%20Robust%20Out-of-Distribution%20MPC%20via%20Conformalized%20System%20Level%20Synthesis%0AAuthor%3A%20Anutam%20Srinivasan%20and%20Antoine%20Leeman%20and%20Glen%20Chou%0AAbstract%3A%20We%20present%20a%20novel%20framework%20for%20robust%20out-of-distribution%20planning%20and%20control%20using%20conformal%20prediction%20%28CP%29%20and%20system%20level%20synthesis%20%28SLS%29%2C%20addressing%20the%20challenge%20of%20ensuring%20safety%20and%20robustness%20when%20using%20learned%20dynamics%20models%20beyond%20the%20training%20data%20distribution.%20We%20first%20derive%20high-confidence%20model%20error%20bounds%20using%20weighted%20CP%20with%20a%20learned%2C%20state-control-dependent%20covariance%20model.%20These%20bounds%20are%20integrated%20into%20an%20SLS-based%20robust%20nonlinear%20model%20predictive%20control%20%28MPC%29%20formulation%2C%20which%20performs%20constraint%20tightening%20over%20the%20prediction%20horizon%20via%20volume-optimized%20forward%20reachable%20sets.%20We%20provide%20theoretical%20guarantees%20on%20coverage%20and%20robustness%20under%20distributional%20drift%2C%20and%20analyze%20the%20impact%20of%20data%20density%20and%20trajectory%20tube%20size%20on%20prediction%20coverage.%20Empirically%2C%20we%20demonstrate%20our%20method%20on%20nonlinear%20systems%20of%20increasing%20complexity%2C%20including%20a%204D%20car%20and%20a%20%7B12D%7D%20quadcopter%2C%20improving%20safety%20and%20robustness%20compared%20to%20fixed-bound%20and%20non-robust%20baselines%2C%20especially%20outside%20of%20the%20data%20distribution.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Beyond%2520the%2520Training%2520Data%253A%2520Robust%2520Out-of-Distribution%2520MPC%2520via%2520Conformalized%2520System%2520Level%2520Synthesis%26entry.906535625%3DAnutam%2520Srinivasan%2520and%2520Antoine%2520Leeman%2520and%2520Glen%2520Chou%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520framework%2520for%2520robust%2520out-of-distribution%2520planning%2520and%2520control%2520using%2520conformal%2520prediction%2520%2528CP%2529%2520and%2520system%2520level%2520synthesis%2520%2528SLS%2529%252C%2520addressing%2520the%2520challenge%2520of%2520ensuring%2520safety%2520and%2520robustness%2520when%2520using%2520learned%2520dynamics%2520models%2520beyond%2520the%2520training%2520data%2520distribution.%2520We%2520first%2520derive%2520high-confidence%2520model%2520error%2520bounds%2520using%2520weighted%2520CP%2520with%2520a%2520learned%252C%2520state-control-dependent%2520covariance%2520model.%2520These%2520bounds%2520are%2520integrated%2520into%2520an%2520SLS-based%2520robust%2520nonlinear%2520model%2520predictive%2520control%2520%2528MPC%2529%2520formulation%252C%2520which%2520performs%2520constraint%2520tightening%2520over%2520the%2520prediction%2520horizon%2520via%2520volume-optimized%2520forward%2520reachable%2520sets.%2520We%2520provide%2520theoretical%2520guarantees%2520on%2520coverage%2520and%2520robustness%2520under%2520distributional%2520drift%252C%2520and%2520analyze%2520the%2520impact%2520of%2520data%2520density%2520and%2520trajectory%2520tube%2520size%2520on%2520prediction%2520coverage.%2520Empirically%252C%2520we%2520demonstrate%2520our%2520method%2520on%2520nonlinear%2520systems%2520of%2520increasing%2520complexity%252C%2520including%2520a%25204D%2520car%2520and%2520a%2520%257B12D%257D%2520quadcopter%252C%2520improving%2520safety%2520and%2520robustness%2520compared%2520to%2520fixed-bound%2520and%2520non-robust%2520baselines%252C%2520especially%2520outside%2520of%2520the%2520data%2520distribution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Beyond%20the%20Training%20Data%3A%20Robust%20Out-of-Distribution%20MPC%20via%20Conformalized%20System%20Level%20Synthesis&entry.906535625=Anutam%20Srinivasan%20and%20Antoine%20Leeman%20and%20Glen%20Chou&entry.1292438233=We%20present%20a%20novel%20framework%20for%20robust%20out-of-distribution%20planning%20and%20control%20using%20conformal%20prediction%20%28CP%29%20and%20system%20level%20synthesis%20%28SLS%29%2C%20addressing%20the%20challenge%20of%20ensuring%20safety%20and%20robustness%20when%20using%20learned%20dynamics%20models%20beyond%20the%20training%20data%20distribution.%20We%20first%20derive%20high-confidence%20model%20error%20bounds%20using%20weighted%20CP%20with%20a%20learned%2C%20state-control-dependent%20covariance%20model.%20These%20bounds%20are%20integrated%20into%20an%20SLS-based%20robust%20nonlinear%20model%20predictive%20control%20%28MPC%29%20formulation%2C%20which%20performs%20constraint%20tightening%20over%20the%20prediction%20horizon%20via%20volume-optimized%20forward%20reachable%20sets.%20We%20provide%20theoretical%20guarantees%20on%20coverage%20and%20robustness%20under%20distributional%20drift%2C%20and%20analyze%20the%20impact%20of%20data%20density%20and%20trajectory%20tube%20size%20on%20prediction%20coverage.%20Empirically%2C%20we%20demonstrate%20our%20method%20on%20nonlinear%20systems%20of%20increasing%20complexity%2C%20including%20a%204D%20car%20and%20a%20%7B12D%7D%20quadcopter%2C%20improving%20safety%20and%20robustness%20compared%20to%20fixed-bound%20and%20non-robust%20baselines%2C%20especially%20outside%20of%20the%20data%20distribution.&entry.1838667208=http%3A//arxiv.org/abs/2602.12047v1&entry.124074799=Read"},
{"title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design", "author": "Yordan Yordanov and Matteo Forasassi and Bayar Menzat and Ruizhi Wang and Chang Qi and Markus Kaltenberger and Amine M'Charrak and Tommaso Salvatori and Thomas Lukasiewicz", "abstract": "While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. \"woman\") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.", "link": "http://arxiv.org/abs/2602.11852v1", "date": "2026-02-12", "relevancy": 2.109, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5871}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Transformer%3A%20Towards%20Language%20Model%20Architectures%20Interpretable%20by%20Design&body=Title%3A%20Prototype%20Transformer%3A%20Towards%20Language%20Model%20Architectures%20Interpretable%20by%20Design%0AAuthor%3A%20Yordan%20Yordanov%20and%20Matteo%20Forasassi%20and%20Bayar%20Menzat%20and%20Ruizhi%20Wang%20and%20Chang%20Qi%20and%20Markus%20Kaltenberger%20and%20Amine%20M%27Charrak%20and%20Tommaso%20Salvatori%20and%20Thomas%20Lukasiewicz%0AAbstract%3A%20While%20state-of-the-art%20language%20models%20%28LMs%29%20surpass%20the%20vast%20majority%20of%20humans%20in%20certain%20domains%2C%20their%20reasoning%20remains%20largely%20opaque%2C%20undermining%20trust%20in%20their%20output.%20Furthermore%2C%20while%20autoregressive%20LMs%20can%20output%20explicit%20reasoning%2C%20their%20true%20reasoning%20process%20is%20opaque%2C%20which%20introduces%20risks%20like%20deception%20and%20hallucination.%20In%20this%20work%2C%20we%20introduce%20the%20Prototype%20Transformer%20%28ProtoT%29%20--%20an%20autoregressive%20LM%20architecture%20based%20on%20prototypes%20%28parameter%20vectors%29%2C%20posed%20as%20an%20alternative%20to%20the%20standard%20self-attention-based%20transformers.%20ProtoT%20works%20by%20means%20of%20two-way%20communication%20between%20the%20input%20sequence%20and%20the%20prototypes%2C%20and%20we%20show%20that%20this%20leads%20to%20the%20prototypes%20automatically%20capturing%20nameable%20concepts%20%28e.g.%20%22woman%22%29%20during%20training.%20They%20provide%20the%20potential%20to%20interpret%20the%20model%27s%20reasoning%20and%20allow%20for%20targeted%20edits%20of%20its%20behavior.%20Furthermore%2C%20by%20design%2C%20the%20prototypes%20create%20communication%20channels%20that%20aggregate%20contextual%20information%20at%20different%20time%20scales%2C%20aiding%20interpretability.%20In%20terms%20of%20computation%20scalability%2C%20ProtoT%20scales%20linearly%20with%20sequence%20length%20vs%20the%20quadratic%20scalability%20of%20SOTA%20self-attention%20transformers.%20Compared%20to%20baselines%2C%20ProtoT%20scales%20well%20with%20model%20and%20data%20size%2C%20and%20performs%20well%20on%20text%20generation%20and%20downstream%20tasks%20%28GLUE%29.%20ProtoT%20exhibits%20robustness%20to%20input%20perturbations%20on%20par%20or%20better%20than%20some%20baselines%2C%20but%20differs%20from%20them%20by%20providing%20interpretable%20pathways%20showing%20how%20robustness%20and%20sensitivity%20arises.%20Reaching%20close%20to%20the%20performance%20of%20state-of-the-art%20architectures%2C%20ProtoT%20paves%20the%20way%20to%20creating%20well-performing%20autoregressive%20LMs%20interpretable%20by%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Transformer%253A%2520Towards%2520Language%2520Model%2520Architectures%2520Interpretable%2520by%2520Design%26entry.906535625%3DYordan%2520Yordanov%2520and%2520Matteo%2520Forasassi%2520and%2520Bayar%2520Menzat%2520and%2520Ruizhi%2520Wang%2520and%2520Chang%2520Qi%2520and%2520Markus%2520Kaltenberger%2520and%2520Amine%2520M%2527Charrak%2520and%2520Tommaso%2520Salvatori%2520and%2520Thomas%2520Lukasiewicz%26entry.1292438233%3DWhile%2520state-of-the-art%2520language%2520models%2520%2528LMs%2529%2520surpass%2520the%2520vast%2520majority%2520of%2520humans%2520in%2520certain%2520domains%252C%2520their%2520reasoning%2520remains%2520largely%2520opaque%252C%2520undermining%2520trust%2520in%2520their%2520output.%2520Furthermore%252C%2520while%2520autoregressive%2520LMs%2520can%2520output%2520explicit%2520reasoning%252C%2520their%2520true%2520reasoning%2520process%2520is%2520opaque%252C%2520which%2520introduces%2520risks%2520like%2520deception%2520and%2520hallucination.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Prototype%2520Transformer%2520%2528ProtoT%2529%2520--%2520an%2520autoregressive%2520LM%2520architecture%2520based%2520on%2520prototypes%2520%2528parameter%2520vectors%2529%252C%2520posed%2520as%2520an%2520alternative%2520to%2520the%2520standard%2520self-attention-based%2520transformers.%2520ProtoT%2520works%2520by%2520means%2520of%2520two-way%2520communication%2520between%2520the%2520input%2520sequence%2520and%2520the%2520prototypes%252C%2520and%2520we%2520show%2520that%2520this%2520leads%2520to%2520the%2520prototypes%2520automatically%2520capturing%2520nameable%2520concepts%2520%2528e.g.%2520%2522woman%2522%2529%2520during%2520training.%2520They%2520provide%2520the%2520potential%2520to%2520interpret%2520the%2520model%2527s%2520reasoning%2520and%2520allow%2520for%2520targeted%2520edits%2520of%2520its%2520behavior.%2520Furthermore%252C%2520by%2520design%252C%2520the%2520prototypes%2520create%2520communication%2520channels%2520that%2520aggregate%2520contextual%2520information%2520at%2520different%2520time%2520scales%252C%2520aiding%2520interpretability.%2520In%2520terms%2520of%2520computation%2520scalability%252C%2520ProtoT%2520scales%2520linearly%2520with%2520sequence%2520length%2520vs%2520the%2520quadratic%2520scalability%2520of%2520SOTA%2520self-attention%2520transformers.%2520Compared%2520to%2520baselines%252C%2520ProtoT%2520scales%2520well%2520with%2520model%2520and%2520data%2520size%252C%2520and%2520performs%2520well%2520on%2520text%2520generation%2520and%2520downstream%2520tasks%2520%2528GLUE%2529.%2520ProtoT%2520exhibits%2520robustness%2520to%2520input%2520perturbations%2520on%2520par%2520or%2520better%2520than%2520some%2520baselines%252C%2520but%2520differs%2520from%2520them%2520by%2520providing%2520interpretable%2520pathways%2520showing%2520how%2520robustness%2520and%2520sensitivity%2520arises.%2520Reaching%2520close%2520to%2520the%2520performance%2520of%2520state-of-the-art%2520architectures%252C%2520ProtoT%2520paves%2520the%2520way%2520to%2520creating%2520well-performing%2520autoregressive%2520LMs%2520interpretable%2520by%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Transformer%3A%20Towards%20Language%20Model%20Architectures%20Interpretable%20by%20Design&entry.906535625=Yordan%20Yordanov%20and%20Matteo%20Forasassi%20and%20Bayar%20Menzat%20and%20Ruizhi%20Wang%20and%20Chang%20Qi%20and%20Markus%20Kaltenberger%20and%20Amine%20M%27Charrak%20and%20Tommaso%20Salvatori%20and%20Thomas%20Lukasiewicz&entry.1292438233=While%20state-of-the-art%20language%20models%20%28LMs%29%20surpass%20the%20vast%20majority%20of%20humans%20in%20certain%20domains%2C%20their%20reasoning%20remains%20largely%20opaque%2C%20undermining%20trust%20in%20their%20output.%20Furthermore%2C%20while%20autoregressive%20LMs%20can%20output%20explicit%20reasoning%2C%20their%20true%20reasoning%20process%20is%20opaque%2C%20which%20introduces%20risks%20like%20deception%20and%20hallucination.%20In%20this%20work%2C%20we%20introduce%20the%20Prototype%20Transformer%20%28ProtoT%29%20--%20an%20autoregressive%20LM%20architecture%20based%20on%20prototypes%20%28parameter%20vectors%29%2C%20posed%20as%20an%20alternative%20to%20the%20standard%20self-attention-based%20transformers.%20ProtoT%20works%20by%20means%20of%20two-way%20communication%20between%20the%20input%20sequence%20and%20the%20prototypes%2C%20and%20we%20show%20that%20this%20leads%20to%20the%20prototypes%20automatically%20capturing%20nameable%20concepts%20%28e.g.%20%22woman%22%29%20during%20training.%20They%20provide%20the%20potential%20to%20interpret%20the%20model%27s%20reasoning%20and%20allow%20for%20targeted%20edits%20of%20its%20behavior.%20Furthermore%2C%20by%20design%2C%20the%20prototypes%20create%20communication%20channels%20that%20aggregate%20contextual%20information%20at%20different%20time%20scales%2C%20aiding%20interpretability.%20In%20terms%20of%20computation%20scalability%2C%20ProtoT%20scales%20linearly%20with%20sequence%20length%20vs%20the%20quadratic%20scalability%20of%20SOTA%20self-attention%20transformers.%20Compared%20to%20baselines%2C%20ProtoT%20scales%20well%20with%20model%20and%20data%20size%2C%20and%20performs%20well%20on%20text%20generation%20and%20downstream%20tasks%20%28GLUE%29.%20ProtoT%20exhibits%20robustness%20to%20input%20perturbations%20on%20par%20or%20better%20than%20some%20baselines%2C%20but%20differs%20from%20them%20by%20providing%20interpretable%20pathways%20showing%20how%20robustness%20and%20sensitivity%20arises.%20Reaching%20close%20to%20the%20performance%20of%20state-of-the-art%20architectures%2C%20ProtoT%20paves%20the%20way%20to%20creating%20well-performing%20autoregressive%20LMs%20interpretable%20by%20design.&entry.1838667208=http%3A//arxiv.org/abs/2602.11852v1&entry.124074799=Read"},
{"title": "Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?", "author": "Weixian Waylon Li and Hyeonjun Kim and Mihai Cucuringu and Tiejun Ma", "abstract": "Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.", "link": "http://arxiv.org/abs/2505.07078v5", "date": "2026-02-12", "relevancy": 2.0981, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLM-based%20Financial%20Investing%20Strategies%20Outperform%20the%20Market%20in%20Long%20Run%3F&body=Title%3A%20Can%20LLM-based%20Financial%20Investing%20Strategies%20Outperform%20the%20Market%20in%20Long%20Run%3F%0AAuthor%3A%20Weixian%20Waylon%20Li%20and%20Hyeonjun%20Kim%20and%20Mihai%20Cucuringu%20and%20Tiejun%20Ma%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20been%20leveraged%20for%20asset%20pricing%20tasks%20and%20stock%20trading%20applications%2C%20enabling%20AI%20agents%20to%20generate%20investment%20decisions%20from%20unstructured%20financial%20data.%20However%2C%20most%20evaluations%20of%20LLM%20timing-based%20investing%20strategies%20are%20conducted%20on%20narrow%20timeframes%20and%20limited%20stock%20universes%2C%20overstating%20effectiveness%20due%20to%20survivorship%20and%20data-snooping%20biases.%20We%20critically%20assess%20their%20generalizability%20and%20robustness%20by%20proposing%20FINSABER%2C%20a%20backtesting%20framework%20evaluating%20timing-based%20strategies%20across%20longer%20periods%20and%20a%20larger%20universe%20of%20symbols.%20Systematic%20backtests%20over%20two%20decades%20and%20100%2B%20symbols%20reveal%20that%20previously%20reported%20LLM%20advantages%20deteriorate%20significantly%20under%20broader%20cross-section%20and%20over%20a%20longer-term%20evaluation.%20Our%20market%20regime%20analysis%20further%20demonstrates%20that%20LLM%20strategies%20are%20overly%20conservative%20in%20bull%20markets%2C%20underperforming%20passive%20benchmarks%2C%20and%20overly%20aggressive%20in%20bear%20markets%2C%20incurring%20heavy%20losses.%20These%20findings%20highlight%20the%20need%20to%20develop%20LLM%20strategies%20that%20are%20able%20to%20prioritise%20trend%20detection%20and%20regime-aware%20risk%20controls%20over%20mere%20scaling%20of%20framework%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2505.07078v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLM-based%2520Financial%2520Investing%2520Strategies%2520Outperform%2520the%2520Market%2520in%2520Long%2520Run%253F%26entry.906535625%3DWeixian%2520Waylon%2520Li%2520and%2520Hyeonjun%2520Kim%2520and%2520Mihai%2520Cucuringu%2520and%2520Tiejun%2520Ma%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520been%2520leveraged%2520for%2520asset%2520pricing%2520tasks%2520and%2520stock%2520trading%2520applications%252C%2520enabling%2520AI%2520agents%2520to%2520generate%2520investment%2520decisions%2520from%2520unstructured%2520financial%2520data.%2520However%252C%2520most%2520evaluations%2520of%2520LLM%2520timing-based%2520investing%2520strategies%2520are%2520conducted%2520on%2520narrow%2520timeframes%2520and%2520limited%2520stock%2520universes%252C%2520overstating%2520effectiveness%2520due%2520to%2520survivorship%2520and%2520data-snooping%2520biases.%2520We%2520critically%2520assess%2520their%2520generalizability%2520and%2520robustness%2520by%2520proposing%2520FINSABER%252C%2520a%2520backtesting%2520framework%2520evaluating%2520timing-based%2520strategies%2520across%2520longer%2520periods%2520and%2520a%2520larger%2520universe%2520of%2520symbols.%2520Systematic%2520backtests%2520over%2520two%2520decades%2520and%2520100%252B%2520symbols%2520reveal%2520that%2520previously%2520reported%2520LLM%2520advantages%2520deteriorate%2520significantly%2520under%2520broader%2520cross-section%2520and%2520over%2520a%2520longer-term%2520evaluation.%2520Our%2520market%2520regime%2520analysis%2520further%2520demonstrates%2520that%2520LLM%2520strategies%2520are%2520overly%2520conservative%2520in%2520bull%2520markets%252C%2520underperforming%2520passive%2520benchmarks%252C%2520and%2520overly%2520aggressive%2520in%2520bear%2520markets%252C%2520incurring%2520heavy%2520losses.%2520These%2520findings%2520highlight%2520the%2520need%2520to%2520develop%2520LLM%2520strategies%2520that%2520are%2520able%2520to%2520prioritise%2520trend%2520detection%2520and%2520regime-aware%2520risk%2520controls%2520over%2520mere%2520scaling%2520of%2520framework%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07078v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLM-based%20Financial%20Investing%20Strategies%20Outperform%20the%20Market%20in%20Long%20Run%3F&entry.906535625=Weixian%20Waylon%20Li%20and%20Hyeonjun%20Kim%20and%20Mihai%20Cucuringu%20and%20Tiejun%20Ma&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20recently%20been%20leveraged%20for%20asset%20pricing%20tasks%20and%20stock%20trading%20applications%2C%20enabling%20AI%20agents%20to%20generate%20investment%20decisions%20from%20unstructured%20financial%20data.%20However%2C%20most%20evaluations%20of%20LLM%20timing-based%20investing%20strategies%20are%20conducted%20on%20narrow%20timeframes%20and%20limited%20stock%20universes%2C%20overstating%20effectiveness%20due%20to%20survivorship%20and%20data-snooping%20biases.%20We%20critically%20assess%20their%20generalizability%20and%20robustness%20by%20proposing%20FINSABER%2C%20a%20backtesting%20framework%20evaluating%20timing-based%20strategies%20across%20longer%20periods%20and%20a%20larger%20universe%20of%20symbols.%20Systematic%20backtests%20over%20two%20decades%20and%20100%2B%20symbols%20reveal%20that%20previously%20reported%20LLM%20advantages%20deteriorate%20significantly%20under%20broader%20cross-section%20and%20over%20a%20longer-term%20evaluation.%20Our%20market%20regime%20analysis%20further%20demonstrates%20that%20LLM%20strategies%20are%20overly%20conservative%20in%20bull%20markets%2C%20underperforming%20passive%20benchmarks%2C%20and%20overly%20aggressive%20in%20bear%20markets%2C%20incurring%20heavy%20losses.%20These%20findings%20highlight%20the%20need%20to%20develop%20LLM%20strategies%20that%20are%20able%20to%20prioritise%20trend%20detection%20and%20regime-aware%20risk%20controls%20over%20mere%20scaling%20of%20framework%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2505.07078v5&entry.124074799=Read"},
{"title": "Model-based controller assisted domain randomization for transient vibration suppression of nonlinear powertrain system with parametric uncertainty", "author": "Heisei Yonezawa and Ansei Yonezawa and Itsuro Kajiwara", "abstract": "Complex mechanical systems such as vehicle powertrains are inherently subject to multiple nonlinearities and uncertainties arising from parametric variations. Modeling errors are therefore unavoidable, making the transfer of control systems from simulation to real-world systems a critical challenge. Traditional robust controls have limitations in handling certain types of nonlinearities and uncertainties, requiring a more practical approach capable of comprehensively compensating for these various constraints. This study proposes a new robust control approach using the framework of deep reinforcement learning (DRL). The key strategy lies in the synergy among domain randomization-based DRL, long short-term memory (LSTM)-based actor and critic networks, and model-based control (MBC). The problem setup is modeled via the latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an environment simulator is randomized during training to improve the robustness of the control system to real testing environments. The randomization increases training difficulties as well as conservativeness of the resultant control system; therefore, progress is assisted by concurrent use of a model-based controller based on a physics-based system model. Compared to traditional DRL-based controls, the proposed approach is smarter in that we can achieve a high level of generalization ability with a more compact neural network architecture and a smaller amount of training data. The controller is verified via practical application to active damping for a complex powertrain system with nonlinearities and parametric variations. Comparative tests demonstrate the high robustness of the proposed approach.", "link": "http://arxiv.org/abs/2504.19715v2", "date": "2026-02-12", "relevancy": 2.0904, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5674}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5148}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-based%20controller%20assisted%20domain%20randomization%20for%20transient%20vibration%20suppression%20of%20nonlinear%20powertrain%20system%20with%20parametric%20uncertainty&body=Title%3A%20Model-based%20controller%20assisted%20domain%20randomization%20for%20transient%20vibration%20suppression%20of%20nonlinear%20powertrain%20system%20with%20parametric%20uncertainty%0AAuthor%3A%20Heisei%20Yonezawa%20and%20Ansei%20Yonezawa%20and%20Itsuro%20Kajiwara%0AAbstract%3A%20Complex%20mechanical%20systems%20such%20as%20vehicle%20powertrains%20are%20inherently%20subject%20to%20multiple%20nonlinearities%20and%20uncertainties%20arising%20from%20parametric%20variations.%20Modeling%20errors%20are%20therefore%20unavoidable%2C%20making%20the%20transfer%20of%20control%20systems%20from%20simulation%20to%20real-world%20systems%20a%20critical%20challenge.%20Traditional%20robust%20controls%20have%20limitations%20in%20handling%20certain%20types%20of%20nonlinearities%20and%20uncertainties%2C%20requiring%20a%20more%20practical%20approach%20capable%20of%20comprehensively%20compensating%20for%20these%20various%20constraints.%20This%20study%20proposes%20a%20new%20robust%20control%20approach%20using%20the%20framework%20of%20deep%20reinforcement%20learning%20%28DRL%29.%20The%20key%20strategy%20lies%20in%20the%20synergy%20among%20domain%20randomization-based%20DRL%2C%20long%20short-term%20memory%20%28LSTM%29-based%20actor%20and%20critic%20networks%2C%20and%20model-based%20control%20%28MBC%29.%20The%20problem%20setup%20is%20modeled%20via%20the%20latent%20Markov%20decision%20process%20%28LMDP%29%2C%20a%20set%20of%20vanilla%20MDPs%2C%20for%20a%20controlled%20system%20subject%20to%20uncertainties%20and%20nonlinearities.%20In%20LMDP%2C%20the%20dynamics%20of%20an%20environment%20simulator%20is%20randomized%20during%20training%20to%20improve%20the%20robustness%20of%20the%20control%20system%20to%20real%20testing%20environments.%20The%20randomization%20increases%20training%20difficulties%20as%20well%20as%20conservativeness%20of%20the%20resultant%20control%20system%3B%20therefore%2C%20progress%20is%20assisted%20by%20concurrent%20use%20of%20a%20model-based%20controller%20based%20on%20a%20physics-based%20system%20model.%20Compared%20to%20traditional%20DRL-based%20controls%2C%20the%20proposed%20approach%20is%20smarter%20in%20that%20we%20can%20achieve%20a%20high%20level%20of%20generalization%20ability%20with%20a%20more%20compact%20neural%20network%20architecture%20and%20a%20smaller%20amount%20of%20training%20data.%20The%20controller%20is%20verified%20via%20practical%20application%20to%20active%20damping%20for%20a%20complex%20powertrain%20system%20with%20nonlinearities%20and%20parametric%20variations.%20Comparative%20tests%20demonstrate%20the%20high%20robustness%20of%20the%20proposed%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2504.19715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-based%2520controller%2520assisted%2520domain%2520randomization%2520for%2520transient%2520vibration%2520suppression%2520of%2520nonlinear%2520powertrain%2520system%2520with%2520parametric%2520uncertainty%26entry.906535625%3DHeisei%2520Yonezawa%2520and%2520Ansei%2520Yonezawa%2520and%2520Itsuro%2520Kajiwara%26entry.1292438233%3DComplex%2520mechanical%2520systems%2520such%2520as%2520vehicle%2520powertrains%2520are%2520inherently%2520subject%2520to%2520multiple%2520nonlinearities%2520and%2520uncertainties%2520arising%2520from%2520parametric%2520variations.%2520Modeling%2520errors%2520are%2520therefore%2520unavoidable%252C%2520making%2520the%2520transfer%2520of%2520control%2520systems%2520from%2520simulation%2520to%2520real-world%2520systems%2520a%2520critical%2520challenge.%2520Traditional%2520robust%2520controls%2520have%2520limitations%2520in%2520handling%2520certain%2520types%2520of%2520nonlinearities%2520and%2520uncertainties%252C%2520requiring%2520a%2520more%2520practical%2520approach%2520capable%2520of%2520comprehensively%2520compensating%2520for%2520these%2520various%2520constraints.%2520This%2520study%2520proposes%2520a%2520new%2520robust%2520control%2520approach%2520using%2520the%2520framework%2520of%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529.%2520The%2520key%2520strategy%2520lies%2520in%2520the%2520synergy%2520among%2520domain%2520randomization-based%2520DRL%252C%2520long%2520short-term%2520memory%2520%2528LSTM%2529-based%2520actor%2520and%2520critic%2520networks%252C%2520and%2520model-based%2520control%2520%2528MBC%2529.%2520The%2520problem%2520setup%2520is%2520modeled%2520via%2520the%2520latent%2520Markov%2520decision%2520process%2520%2528LMDP%2529%252C%2520a%2520set%2520of%2520vanilla%2520MDPs%252C%2520for%2520a%2520controlled%2520system%2520subject%2520to%2520uncertainties%2520and%2520nonlinearities.%2520In%2520LMDP%252C%2520the%2520dynamics%2520of%2520an%2520environment%2520simulator%2520is%2520randomized%2520during%2520training%2520to%2520improve%2520the%2520robustness%2520of%2520the%2520control%2520system%2520to%2520real%2520testing%2520environments.%2520The%2520randomization%2520increases%2520training%2520difficulties%2520as%2520well%2520as%2520conservativeness%2520of%2520the%2520resultant%2520control%2520system%253B%2520therefore%252C%2520progress%2520is%2520assisted%2520by%2520concurrent%2520use%2520of%2520a%2520model-based%2520controller%2520based%2520on%2520a%2520physics-based%2520system%2520model.%2520Compared%2520to%2520traditional%2520DRL-based%2520controls%252C%2520the%2520proposed%2520approach%2520is%2520smarter%2520in%2520that%2520we%2520can%2520achieve%2520a%2520high%2520level%2520of%2520generalization%2520ability%2520with%2520a%2520more%2520compact%2520neural%2520network%2520architecture%2520and%2520a%2520smaller%2520amount%2520of%2520training%2520data.%2520The%2520controller%2520is%2520verified%2520via%2520practical%2520application%2520to%2520active%2520damping%2520for%2520a%2520complex%2520powertrain%2520system%2520with%2520nonlinearities%2520and%2520parametric%2520variations.%2520Comparative%2520tests%2520demonstrate%2520the%2520high%2520robustness%2520of%2520the%2520proposed%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-based%20controller%20assisted%20domain%20randomization%20for%20transient%20vibration%20suppression%20of%20nonlinear%20powertrain%20system%20with%20parametric%20uncertainty&entry.906535625=Heisei%20Yonezawa%20and%20Ansei%20Yonezawa%20and%20Itsuro%20Kajiwara&entry.1292438233=Complex%20mechanical%20systems%20such%20as%20vehicle%20powertrains%20are%20inherently%20subject%20to%20multiple%20nonlinearities%20and%20uncertainties%20arising%20from%20parametric%20variations.%20Modeling%20errors%20are%20therefore%20unavoidable%2C%20making%20the%20transfer%20of%20control%20systems%20from%20simulation%20to%20real-world%20systems%20a%20critical%20challenge.%20Traditional%20robust%20controls%20have%20limitations%20in%20handling%20certain%20types%20of%20nonlinearities%20and%20uncertainties%2C%20requiring%20a%20more%20practical%20approach%20capable%20of%20comprehensively%20compensating%20for%20these%20various%20constraints.%20This%20study%20proposes%20a%20new%20robust%20control%20approach%20using%20the%20framework%20of%20deep%20reinforcement%20learning%20%28DRL%29.%20The%20key%20strategy%20lies%20in%20the%20synergy%20among%20domain%20randomization-based%20DRL%2C%20long%20short-term%20memory%20%28LSTM%29-based%20actor%20and%20critic%20networks%2C%20and%20model-based%20control%20%28MBC%29.%20The%20problem%20setup%20is%20modeled%20via%20the%20latent%20Markov%20decision%20process%20%28LMDP%29%2C%20a%20set%20of%20vanilla%20MDPs%2C%20for%20a%20controlled%20system%20subject%20to%20uncertainties%20and%20nonlinearities.%20In%20LMDP%2C%20the%20dynamics%20of%20an%20environment%20simulator%20is%20randomized%20during%20training%20to%20improve%20the%20robustness%20of%20the%20control%20system%20to%20real%20testing%20environments.%20The%20randomization%20increases%20training%20difficulties%20as%20well%20as%20conservativeness%20of%20the%20resultant%20control%20system%3B%20therefore%2C%20progress%20is%20assisted%20by%20concurrent%20use%20of%20a%20model-based%20controller%20based%20on%20a%20physics-based%20system%20model.%20Compared%20to%20traditional%20DRL-based%20controls%2C%20the%20proposed%20approach%20is%20smarter%20in%20that%20we%20can%20achieve%20a%20high%20level%20of%20generalization%20ability%20with%20a%20more%20compact%20neural%20network%20architecture%20and%20a%20smaller%20amount%20of%20training%20data.%20The%20controller%20is%20verified%20via%20practical%20application%20to%20active%20damping%20for%20a%20complex%20powertrain%20system%20with%20nonlinearities%20and%20parametric%20variations.%20Comparative%20tests%20demonstrate%20the%20high%20robustness%20of%20the%20proposed%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2504.19715v2&entry.124074799=Read"},
{"title": "Sub--Riemannian boundary value problems for Optimal Geometric Locomotion", "author": "Oliver Gross and Florine Hartwig and Martin Rumpf and Peter Schr\u00f6der", "abstract": "We propose a geometric model for optimal shape-change-induced motions of slender locomotors, e.g., snakes slithering on sand. In these scenarios, the motion of a body in world coordinates is completely determined by the sequence of shapes it assumes. Specifically, we formulate Lagrangian least-dissipation principles as boundary value problems whose solutions are given by sub-Riemannian geodesics. Notably, our geometric model accounts not only for the energy dissipated by the body's displacement through the environment, but also for the energy dissipated by the animal's metabolism or a robot's actuators to induce shape changes such as bending and stretching, thus capturing overall locomotion efficiency. Our continuous model, together with a consistent time and space discretization, enables numerical computation of sub-Riemannian geodesics for three different types of boundary conditions, i.e., fixing initial and target body, restricting to cyclic motion, or solely prescribing body displacement and orientation. The resulting optimal deformation gaits qualitatively match observed motion trajectories of organisms such as snakes and spermatozoa, as well as known optimality results for low-dimensional systems such as Purcell's swimmers. Moreover, being geometrically less rigid than previous frameworks, our model enables new insights into locomotion mechanisms of, e.g., generalized Purcell's swimmers. The code is publicly available.", "link": "http://arxiv.org/abs/2602.12199v1", "date": "2026-02-12", "relevancy": 2.0879, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sub--Riemannian%20boundary%20value%20problems%20for%20Optimal%20Geometric%20Locomotion&body=Title%3A%20Sub--Riemannian%20boundary%20value%20problems%20for%20Optimal%20Geometric%20Locomotion%0AAuthor%3A%20Oliver%20Gross%20and%20Florine%20Hartwig%20and%20Martin%20Rumpf%20and%20Peter%20Schr%C3%B6der%0AAbstract%3A%20We%20propose%20a%20geometric%20model%20for%20optimal%20shape-change-induced%20motions%20of%20slender%20locomotors%2C%20e.g.%2C%20snakes%20slithering%20on%20sand.%20In%20these%20scenarios%2C%20the%20motion%20of%20a%20body%20in%20world%20coordinates%20is%20completely%20determined%20by%20the%20sequence%20of%20shapes%20it%20assumes.%20Specifically%2C%20we%20formulate%20Lagrangian%20least-dissipation%20principles%20as%20boundary%20value%20problems%20whose%20solutions%20are%20given%20by%20sub-Riemannian%20geodesics.%20Notably%2C%20our%20geometric%20model%20accounts%20not%20only%20for%20the%20energy%20dissipated%20by%20the%20body%27s%20displacement%20through%20the%20environment%2C%20but%20also%20for%20the%20energy%20dissipated%20by%20the%20animal%27s%20metabolism%20or%20a%20robot%27s%20actuators%20to%20induce%20shape%20changes%20such%20as%20bending%20and%20stretching%2C%20thus%20capturing%20overall%20locomotion%20efficiency.%20Our%20continuous%20model%2C%20together%20with%20a%20consistent%20time%20and%20space%20discretization%2C%20enables%20numerical%20computation%20of%20sub-Riemannian%20geodesics%20for%20three%20different%20types%20of%20boundary%20conditions%2C%20i.e.%2C%20fixing%20initial%20and%20target%20body%2C%20restricting%20to%20cyclic%20motion%2C%20or%20solely%20prescribing%20body%20displacement%20and%20orientation.%20The%20resulting%20optimal%20deformation%20gaits%20qualitatively%20match%20observed%20motion%20trajectories%20of%20organisms%20such%20as%20snakes%20and%20spermatozoa%2C%20as%20well%20as%20known%20optimality%20results%20for%20low-dimensional%20systems%20such%20as%20Purcell%27s%20swimmers.%20Moreover%2C%20being%20geometrically%20less%20rigid%20than%20previous%20frameworks%2C%20our%20model%20enables%20new%20insights%20into%20locomotion%20mechanisms%20of%2C%20e.g.%2C%20generalized%20Purcell%27s%20swimmers.%20The%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSub--Riemannian%2520boundary%2520value%2520problems%2520for%2520Optimal%2520Geometric%2520Locomotion%26entry.906535625%3DOliver%2520Gross%2520and%2520Florine%2520Hartwig%2520and%2520Martin%2520Rumpf%2520and%2520Peter%2520Schr%25C3%25B6der%26entry.1292438233%3DWe%2520propose%2520a%2520geometric%2520model%2520for%2520optimal%2520shape-change-induced%2520motions%2520of%2520slender%2520locomotors%252C%2520e.g.%252C%2520snakes%2520slithering%2520on%2520sand.%2520In%2520these%2520scenarios%252C%2520the%2520motion%2520of%2520a%2520body%2520in%2520world%2520coordinates%2520is%2520completely%2520determined%2520by%2520the%2520sequence%2520of%2520shapes%2520it%2520assumes.%2520Specifically%252C%2520we%2520formulate%2520Lagrangian%2520least-dissipation%2520principles%2520as%2520boundary%2520value%2520problems%2520whose%2520solutions%2520are%2520given%2520by%2520sub-Riemannian%2520geodesics.%2520Notably%252C%2520our%2520geometric%2520model%2520accounts%2520not%2520only%2520for%2520the%2520energy%2520dissipated%2520by%2520the%2520body%2527s%2520displacement%2520through%2520the%2520environment%252C%2520but%2520also%2520for%2520the%2520energy%2520dissipated%2520by%2520the%2520animal%2527s%2520metabolism%2520or%2520a%2520robot%2527s%2520actuators%2520to%2520induce%2520shape%2520changes%2520such%2520as%2520bending%2520and%2520stretching%252C%2520thus%2520capturing%2520overall%2520locomotion%2520efficiency.%2520Our%2520continuous%2520model%252C%2520together%2520with%2520a%2520consistent%2520time%2520and%2520space%2520discretization%252C%2520enables%2520numerical%2520computation%2520of%2520sub-Riemannian%2520geodesics%2520for%2520three%2520different%2520types%2520of%2520boundary%2520conditions%252C%2520i.e.%252C%2520fixing%2520initial%2520and%2520target%2520body%252C%2520restricting%2520to%2520cyclic%2520motion%252C%2520or%2520solely%2520prescribing%2520body%2520displacement%2520and%2520orientation.%2520The%2520resulting%2520optimal%2520deformation%2520gaits%2520qualitatively%2520match%2520observed%2520motion%2520trajectories%2520of%2520organisms%2520such%2520as%2520snakes%2520and%2520spermatozoa%252C%2520as%2520well%2520as%2520known%2520optimality%2520results%2520for%2520low-dimensional%2520systems%2520such%2520as%2520Purcell%2527s%2520swimmers.%2520Moreover%252C%2520being%2520geometrically%2520less%2520rigid%2520than%2520previous%2520frameworks%252C%2520our%2520model%2520enables%2520new%2520insights%2520into%2520locomotion%2520mechanisms%2520of%252C%2520e.g.%252C%2520generalized%2520Purcell%2527s%2520swimmers.%2520The%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sub--Riemannian%20boundary%20value%20problems%20for%20Optimal%20Geometric%20Locomotion&entry.906535625=Oliver%20Gross%20and%20Florine%20Hartwig%20and%20Martin%20Rumpf%20and%20Peter%20Schr%C3%B6der&entry.1292438233=We%20propose%20a%20geometric%20model%20for%20optimal%20shape-change-induced%20motions%20of%20slender%20locomotors%2C%20e.g.%2C%20snakes%20slithering%20on%20sand.%20In%20these%20scenarios%2C%20the%20motion%20of%20a%20body%20in%20world%20coordinates%20is%20completely%20determined%20by%20the%20sequence%20of%20shapes%20it%20assumes.%20Specifically%2C%20we%20formulate%20Lagrangian%20least-dissipation%20principles%20as%20boundary%20value%20problems%20whose%20solutions%20are%20given%20by%20sub-Riemannian%20geodesics.%20Notably%2C%20our%20geometric%20model%20accounts%20not%20only%20for%20the%20energy%20dissipated%20by%20the%20body%27s%20displacement%20through%20the%20environment%2C%20but%20also%20for%20the%20energy%20dissipated%20by%20the%20animal%27s%20metabolism%20or%20a%20robot%27s%20actuators%20to%20induce%20shape%20changes%20such%20as%20bending%20and%20stretching%2C%20thus%20capturing%20overall%20locomotion%20efficiency.%20Our%20continuous%20model%2C%20together%20with%20a%20consistent%20time%20and%20space%20discretization%2C%20enables%20numerical%20computation%20of%20sub-Riemannian%20geodesics%20for%20three%20different%20types%20of%20boundary%20conditions%2C%20i.e.%2C%20fixing%20initial%20and%20target%20body%2C%20restricting%20to%20cyclic%20motion%2C%20or%20solely%20prescribing%20body%20displacement%20and%20orientation.%20The%20resulting%20optimal%20deformation%20gaits%20qualitatively%20match%20observed%20motion%20trajectories%20of%20organisms%20such%20as%20snakes%20and%20spermatozoa%2C%20as%20well%20as%20known%20optimality%20results%20for%20low-dimensional%20systems%20such%20as%20Purcell%27s%20swimmers.%20Moreover%2C%20being%20geometrically%20less%20rigid%20than%20previous%20frameworks%2C%20our%20model%20enables%20new%20insights%20into%20locomotion%20mechanisms%20of%2C%20e.g.%2C%20generalized%20Purcell%27s%20swimmers.%20The%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2602.12199v1&entry.124074799=Read"},
{"title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols", "author": "Yixuan Yang and Cuifeng Gao and Daoyuan Wu and Yufan Chen and Yingjiu Li and Shuai Wang", "abstract": "Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and significantly expands their attack surface. In this paper, we present the first formalization of a secure MCP and its required specifications. Based on this foundation, we establish a comprehensive MCP security taxonomy that extends existing models by incorporating protocol-level and host-side threats, identifying 17 distinct attack types across four primary attack surfaces. Building on these specifications, we introduce MCPSecBench, a systematic security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, attack scripts, a GUI test harness, and protection mechanisms to evaluate these threats across three major MCP platforms. MCPSecBench is designed to be modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for rigorous assessment. Our evaluation across three major MCP platforms reveals that all attack surfaces yield successful compromises. Core vulnerabilities universally affect Claude, OpenAI, and Cursor, while server-side and specific client-side attacks exhibit considerable variability across different hosts and models. Furthermore, current protection mechanisms proved largely ineffective, achieving an average success rate of less than 30%. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all protocol layers.", "link": "http://arxiv.org/abs/2508.13220v3", "date": "2026-02-12", "relevancy": 1.7691, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4434}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCPSecBench%3A%20A%20Systematic%20Security%20Benchmark%20and%20Playground%20for%20Testing%20Model%20Context%20Protocols&body=Title%3A%20MCPSecBench%3A%20A%20Systematic%20Security%20Benchmark%20and%20Playground%20for%20Testing%20Model%20Context%20Protocols%0AAuthor%3A%20Yixuan%20Yang%20and%20Cuifeng%20Gao%20and%20Daoyuan%20Wu%20and%20Yufan%20Chen%20and%20Yingjiu%20Li%20and%20Shuai%20Wang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20integrated%20into%20real-world%20applications%20via%20the%20Model%20Context%20Protocol%20%28MCP%29%2C%20a%20universal%20open%20standard%20for%20connecting%20AI%20agents%20with%20data%20sources%20and%20external%20tools.%20While%20MCP%20enhances%20the%20capabilities%20of%20LLM-based%20agents%2C%20it%20also%20introduces%20new%20security%20risks%20and%20significantly%20expands%20their%20attack%20surface.%20In%20this%20paper%2C%20we%20present%20the%20first%20formalization%20of%20a%20secure%20MCP%20and%20its%20required%20specifications.%20Based%20on%20this%20foundation%2C%20we%20establish%20a%20comprehensive%20MCP%20security%20taxonomy%20that%20extends%20existing%20models%20by%20incorporating%20protocol-level%20and%20host-side%20threats%2C%20identifying%2017%20distinct%20attack%20types%20across%20four%20primary%20attack%20surfaces.%20Building%20on%20these%20specifications%2C%20we%20introduce%20MCPSecBench%2C%20a%20systematic%20security%20benchmark%20and%20playground%20that%20integrates%20prompt%20datasets%2C%20MCP%20servers%2C%20MCP%20clients%2C%20attack%20scripts%2C%20a%20GUI%20test%20harness%2C%20and%20protection%20mechanisms%20to%20evaluate%20these%20threats%20across%20three%20major%20MCP%20platforms.%20MCPSecBench%20is%20designed%20to%20be%20modular%20and%20extensible%2C%20allowing%20researchers%20to%20incorporate%20custom%20implementations%20of%20clients%2C%20servers%2C%20and%20transport%20protocols%20for%20rigorous%20assessment.%20Our%20evaluation%20across%20three%20major%20MCP%20platforms%20reveals%20that%20all%20attack%20surfaces%20yield%20successful%20compromises.%20Core%20vulnerabilities%20universally%20affect%20Claude%2C%20OpenAI%2C%20and%20Cursor%2C%20while%20server-side%20and%20specific%20client-side%20attacks%20exhibit%20considerable%20variability%20across%20different%20hosts%20and%20models.%20Furthermore%2C%20current%20protection%20mechanisms%20proved%20largely%20ineffective%2C%20achieving%20an%20average%20success%20rate%20of%20less%20than%2030%25.%20Overall%2C%20MCPSecBench%20standardizes%20the%20evaluation%20of%20MCP%20security%20and%20enables%20rigorous%20testing%20across%20all%20protocol%20layers.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCPSecBench%253A%2520A%2520Systematic%2520Security%2520Benchmark%2520and%2520Playground%2520for%2520Testing%2520Model%2520Context%2520Protocols%26entry.906535625%3DYixuan%2520Yang%2520and%2520Cuifeng%2520Gao%2520and%2520Daoyuan%2520Wu%2520and%2520Yufan%2520Chen%2520and%2520Yingjiu%2520Li%2520and%2520Shuai%2520Wang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520integrated%2520into%2520real-world%2520applications%2520via%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%252C%2520a%2520universal%2520open%2520standard%2520for%2520connecting%2520AI%2520agents%2520with%2520data%2520sources%2520and%2520external%2520tools.%2520While%2520MCP%2520enhances%2520the%2520capabilities%2520of%2520LLM-based%2520agents%252C%2520it%2520also%2520introduces%2520new%2520security%2520risks%2520and%2520significantly%2520expands%2520their%2520attack%2520surface.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520formalization%2520of%2520a%2520secure%2520MCP%2520and%2520its%2520required%2520specifications.%2520Based%2520on%2520this%2520foundation%252C%2520we%2520establish%2520a%2520comprehensive%2520MCP%2520security%2520taxonomy%2520that%2520extends%2520existing%2520models%2520by%2520incorporating%2520protocol-level%2520and%2520host-side%2520threats%252C%2520identifying%252017%2520distinct%2520attack%2520types%2520across%2520four%2520primary%2520attack%2520surfaces.%2520Building%2520on%2520these%2520specifications%252C%2520we%2520introduce%2520MCPSecBench%252C%2520a%2520systematic%2520security%2520benchmark%2520and%2520playground%2520that%2520integrates%2520prompt%2520datasets%252C%2520MCP%2520servers%252C%2520MCP%2520clients%252C%2520attack%2520scripts%252C%2520a%2520GUI%2520test%2520harness%252C%2520and%2520protection%2520mechanisms%2520to%2520evaluate%2520these%2520threats%2520across%2520three%2520major%2520MCP%2520platforms.%2520MCPSecBench%2520is%2520designed%2520to%2520be%2520modular%2520and%2520extensible%252C%2520allowing%2520researchers%2520to%2520incorporate%2520custom%2520implementations%2520of%2520clients%252C%2520servers%252C%2520and%2520transport%2520protocols%2520for%2520rigorous%2520assessment.%2520Our%2520evaluation%2520across%2520three%2520major%2520MCP%2520platforms%2520reveals%2520that%2520all%2520attack%2520surfaces%2520yield%2520successful%2520compromises.%2520Core%2520vulnerabilities%2520universally%2520affect%2520Claude%252C%2520OpenAI%252C%2520and%2520Cursor%252C%2520while%2520server-side%2520and%2520specific%2520client-side%2520attacks%2520exhibit%2520considerable%2520variability%2520across%2520different%2520hosts%2520and%2520models.%2520Furthermore%252C%2520current%2520protection%2520mechanisms%2520proved%2520largely%2520ineffective%252C%2520achieving%2520an%2520average%2520success%2520rate%2520of%2520less%2520than%252030%2525.%2520Overall%252C%2520MCPSecBench%2520standardizes%2520the%2520evaluation%2520of%2520MCP%2520security%2520and%2520enables%2520rigorous%2520testing%2520across%2520all%2520protocol%2520layers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCPSecBench%3A%20A%20Systematic%20Security%20Benchmark%20and%20Playground%20for%20Testing%20Model%20Context%20Protocols&entry.906535625=Yixuan%20Yang%20and%20Cuifeng%20Gao%20and%20Daoyuan%20Wu%20and%20Yufan%20Chen%20and%20Yingjiu%20Li%20and%20Shuai%20Wang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20integrated%20into%20real-world%20applications%20via%20the%20Model%20Context%20Protocol%20%28MCP%29%2C%20a%20universal%20open%20standard%20for%20connecting%20AI%20agents%20with%20data%20sources%20and%20external%20tools.%20While%20MCP%20enhances%20the%20capabilities%20of%20LLM-based%20agents%2C%20it%20also%20introduces%20new%20security%20risks%20and%20significantly%20expands%20their%20attack%20surface.%20In%20this%20paper%2C%20we%20present%20the%20first%20formalization%20of%20a%20secure%20MCP%20and%20its%20required%20specifications.%20Based%20on%20this%20foundation%2C%20we%20establish%20a%20comprehensive%20MCP%20security%20taxonomy%20that%20extends%20existing%20models%20by%20incorporating%20protocol-level%20and%20host-side%20threats%2C%20identifying%2017%20distinct%20attack%20types%20across%20four%20primary%20attack%20surfaces.%20Building%20on%20these%20specifications%2C%20we%20introduce%20MCPSecBench%2C%20a%20systematic%20security%20benchmark%20and%20playground%20that%20integrates%20prompt%20datasets%2C%20MCP%20servers%2C%20MCP%20clients%2C%20attack%20scripts%2C%20a%20GUI%20test%20harness%2C%20and%20protection%20mechanisms%20to%20evaluate%20these%20threats%20across%20three%20major%20MCP%20platforms.%20MCPSecBench%20is%20designed%20to%20be%20modular%20and%20extensible%2C%20allowing%20researchers%20to%20incorporate%20custom%20implementations%20of%20clients%2C%20servers%2C%20and%20transport%20protocols%20for%20rigorous%20assessment.%20Our%20evaluation%20across%20three%20major%20MCP%20platforms%20reveals%20that%20all%20attack%20surfaces%20yield%20successful%20compromises.%20Core%20vulnerabilities%20universally%20affect%20Claude%2C%20OpenAI%2C%20and%20Cursor%2C%20while%20server-side%20and%20specific%20client-side%20attacks%20exhibit%20considerable%20variability%20across%20different%20hosts%20and%20models.%20Furthermore%2C%20current%20protection%20mechanisms%20proved%20largely%20ineffective%2C%20achieving%20an%20average%20success%20rate%20of%20less%20than%2030%25.%20Overall%2C%20MCPSecBench%20standardizes%20the%20evaluation%20of%20MCP%20security%20and%20enables%20rigorous%20testing%20across%20all%20protocol%20layers.&entry.1838667208=http%3A//arxiv.org/abs/2508.13220v3&entry.124074799=Read"},
{"title": "Three factor delay learning rules for spiking neural networks", "author": "Luke Vassallo and Nima Taherinejad", "abstract": "Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.", "link": "http://arxiv.org/abs/2601.00668v2", "date": "2026-02-12", "relevancy": 1.8752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.493}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4885}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three%20factor%20delay%20learning%20rules%20for%20spiking%20neural%20networks&body=Title%3A%20Three%20factor%20delay%20learning%20rules%20for%20spiking%20neural%20networks%0AAuthor%3A%20Luke%20Vassallo%20and%20Nima%20Taherinejad%0AAbstract%3A%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20dynamical%20systems%20that%20operate%20on%20spatiotemporal%20data%2C%20yet%20their%20learnable%20parameters%20are%20often%20limited%20to%20synaptic%20weights%2C%20contributing%20little%20to%20temporal%20pattern%20recognition.%20Learnable%20parameters%20that%20delay%20spike%20times%20can%20improve%20classification%20performance%20in%20temporal%20tasks%2C%20but%20existing%20methods%20rely%20on%20large%20networks%20and%20offline%20learning%2C%20making%20them%20unsuitable%20for%20real-time%20operation%20in%20resource-constrained%20environments.%20In%20this%20paper%2C%20we%20introduce%20synaptic%20and%20axonal%20delays%20to%20leaky%20integrate%20and%20fire%20%28LIF%29-based%20feedforward%20and%20recurrent%20SNNs%2C%20and%20propose%20three-factor%20learning%20rules%20to%20simultaneously%20learn%20delay%20parameters%20online.%20We%20employ%20a%20smooth%20Gaussian%20surrogate%20to%20approximate%20spike%20derivatives%20exclusively%20for%20the%20eligibility%20trace%20calculation%2C%20and%20together%20with%20a%20top-down%20error%20signal%20determine%20parameter%20updates.%20Our%20experiments%20show%20that%20incorporating%20delays%20improves%20accuracy%20by%20up%20to%2020%25%20over%20a%20weights-only%20baseline%2C%20and%20for%20networks%20with%20similar%20parameter%20counts%2C%20jointly%20learning%20weights%20and%20delays%20yields%20up%20to%2014%25%20higher%20accuracy.%20On%20the%20SHD%20speech%20recognition%20dataset%2C%20our%20method%20achieves%20similar%20accuracy%20to%20offline%20backpropagation-based%20approaches.%20Compared%20to%20state-of-the-art%20methods%2C%20it%20reduces%20model%20size%20by%206.6x%20and%20inference%20latency%20by%2067%25%2C%20with%20only%20a%202.4%25%20drop%20in%20classification%20accuracy.%20Our%20findings%20benefit%20the%20design%20of%20power%20and%20area-constrained%20neuromorphic%20processors%20by%20enabling%20on-device%20learning%20and%20lowering%20memory%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00668v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree%2520factor%2520delay%2520learning%2520rules%2520for%2520spiking%2520neural%2520networks%26entry.906535625%3DLuke%2520Vassallo%2520and%2520Nima%2520Taherinejad%26entry.1292438233%3DSpiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520dynamical%2520systems%2520that%2520operate%2520on%2520spatiotemporal%2520data%252C%2520yet%2520their%2520learnable%2520parameters%2520are%2520often%2520limited%2520to%2520synaptic%2520weights%252C%2520contributing%2520little%2520to%2520temporal%2520pattern%2520recognition.%2520Learnable%2520parameters%2520that%2520delay%2520spike%2520times%2520can%2520improve%2520classification%2520performance%2520in%2520temporal%2520tasks%252C%2520but%2520existing%2520methods%2520rely%2520on%2520large%2520networks%2520and%2520offline%2520learning%252C%2520making%2520them%2520unsuitable%2520for%2520real-time%2520operation%2520in%2520resource-constrained%2520environments.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520synaptic%2520and%2520axonal%2520delays%2520to%2520leaky%2520integrate%2520and%2520fire%2520%2528LIF%2529-based%2520feedforward%2520and%2520recurrent%2520SNNs%252C%2520and%2520propose%2520three-factor%2520learning%2520rules%2520to%2520simultaneously%2520learn%2520delay%2520parameters%2520online.%2520We%2520employ%2520a%2520smooth%2520Gaussian%2520surrogate%2520to%2520approximate%2520spike%2520derivatives%2520exclusively%2520for%2520the%2520eligibility%2520trace%2520calculation%252C%2520and%2520together%2520with%2520a%2520top-down%2520error%2520signal%2520determine%2520parameter%2520updates.%2520Our%2520experiments%2520show%2520that%2520incorporating%2520delays%2520improves%2520accuracy%2520by%2520up%2520to%252020%2525%2520over%2520a%2520weights-only%2520baseline%252C%2520and%2520for%2520networks%2520with%2520similar%2520parameter%2520counts%252C%2520jointly%2520learning%2520weights%2520and%2520delays%2520yields%2520up%2520to%252014%2525%2520higher%2520accuracy.%2520On%2520the%2520SHD%2520speech%2520recognition%2520dataset%252C%2520our%2520method%2520achieves%2520similar%2520accuracy%2520to%2520offline%2520backpropagation-based%2520approaches.%2520Compared%2520to%2520state-of-the-art%2520methods%252C%2520it%2520reduces%2520model%2520size%2520by%25206.6x%2520and%2520inference%2520latency%2520by%252067%2525%252C%2520with%2520only%2520a%25202.4%2525%2520drop%2520in%2520classification%2520accuracy.%2520Our%2520findings%2520benefit%2520the%2520design%2520of%2520power%2520and%2520area-constrained%2520neuromorphic%2520processors%2520by%2520enabling%2520on-device%2520learning%2520and%2520lowering%2520memory%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00668v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three%20factor%20delay%20learning%20rules%20for%20spiking%20neural%20networks&entry.906535625=Luke%20Vassallo%20and%20Nima%20Taherinejad&entry.1292438233=Spiking%20Neural%20Networks%20%28SNNs%29%20are%20dynamical%20systems%20that%20operate%20on%20spatiotemporal%20data%2C%20yet%20their%20learnable%20parameters%20are%20often%20limited%20to%20synaptic%20weights%2C%20contributing%20little%20to%20temporal%20pattern%20recognition.%20Learnable%20parameters%20that%20delay%20spike%20times%20can%20improve%20classification%20performance%20in%20temporal%20tasks%2C%20but%20existing%20methods%20rely%20on%20large%20networks%20and%20offline%20learning%2C%20making%20them%20unsuitable%20for%20real-time%20operation%20in%20resource-constrained%20environments.%20In%20this%20paper%2C%20we%20introduce%20synaptic%20and%20axonal%20delays%20to%20leaky%20integrate%20and%20fire%20%28LIF%29-based%20feedforward%20and%20recurrent%20SNNs%2C%20and%20propose%20three-factor%20learning%20rules%20to%20simultaneously%20learn%20delay%20parameters%20online.%20We%20employ%20a%20smooth%20Gaussian%20surrogate%20to%20approximate%20spike%20derivatives%20exclusively%20for%20the%20eligibility%20trace%20calculation%2C%20and%20together%20with%20a%20top-down%20error%20signal%20determine%20parameter%20updates.%20Our%20experiments%20show%20that%20incorporating%20delays%20improves%20accuracy%20by%20up%20to%2020%25%20over%20a%20weights-only%20baseline%2C%20and%20for%20networks%20with%20similar%20parameter%20counts%2C%20jointly%20learning%20weights%20and%20delays%20yields%20up%20to%2014%25%20higher%20accuracy.%20On%20the%20SHD%20speech%20recognition%20dataset%2C%20our%20method%20achieves%20similar%20accuracy%20to%20offline%20backpropagation-based%20approaches.%20Compared%20to%20state-of-the-art%20methods%2C%20it%20reduces%20model%20size%20by%206.6x%20and%20inference%20latency%20by%2067%25%2C%20with%20only%20a%202.4%25%20drop%20in%20classification%20accuracy.%20Our%20findings%20benefit%20the%20design%20of%20power%20and%20area-constrained%20neuromorphic%20processors%20by%20enabling%20on-device%20learning%20and%20lowering%20memory%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2601.00668v2&entry.124074799=Read"},
{"title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix", "author": "Ahmet Caner Y\u00fcz\u00fcg\u00fcler and Ahmet \u00c7elik and Jiawei Zhuang and Lukas Cavigelli", "abstract": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel formulation, MLA allows two functionally equivalent but computationally distinct kernel implementations: naive and absorb. While the naive kernels (e.g., FlashAttention) are typically preferred in training and prefill for their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely on the absorb method to minimize HBM bandwidth usage. However, the compute-bound nature of the absorb implementations prohibits performance benefits from data reuse opportunities in attention calculations, such as shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that combines naive and absorb formulations to harness the strengths of both. TyphoonMLA effectively leverages the shared prefix by applying the naive formulation to the compute-bound parts of attention calculations, while reducing the bandwidth requirements for non-shared parts by using the absorb formulation. As a result, TyphoonMLA improves the throughput of attention calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with only a 3% overhead in HBM size.", "link": "http://arxiv.org/abs/2509.21081v2", "date": "2026-02-12", "relevancy": 2.0162, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TyphoonMLA%3A%20A%20Mixed%20Naive-Absorb%20MLA%20Kernel%20For%20Shared%20Prefix&body=Title%3A%20TyphoonMLA%3A%20A%20Mixed%20Naive-Absorb%20MLA%20Kernel%20For%20Shared%20Prefix%0AAuthor%3A%20Ahmet%20Caner%20Y%C3%BCz%C3%BCg%C3%BCler%20and%20Ahmet%20%C3%87elik%20and%20Jiawei%20Zhuang%20and%20Lukas%20Cavigelli%0AAbstract%3A%20Multi-Head%20Latent%20Attention%20%28MLA%29%20is%20a%20recent%20attention%20mechanism%20adopted%20in%20state-of-the-art%20LLMs%20such%20as%20DeepSeek-v3%20and%20Kimi%20K2.%20Thanks%20to%20its%20novel%20formulation%2C%20MLA%20allows%20two%20functionally%20equivalent%20but%20computationally%20distinct%20kernel%20implementations%3A%20naive%20and%20absorb.%20While%20the%20naive%20kernels%20%28e.g.%2C%20FlashAttention%29%20are%20typically%20preferred%20in%20training%20and%20prefill%20for%20their%20computational%20efficiency%2C%20existing%20decoding%20kernels%20%28e.g.%2C%20FlashMLA%29%20rely%20on%20the%20absorb%20method%20to%20minimize%20HBM%20bandwidth%20usage.%20However%2C%20the%20compute-bound%20nature%20of%20the%20absorb%20implementations%20prohibits%20performance%20benefits%20from%20data%20reuse%20opportunities%20in%20attention%20calculations%2C%20such%20as%20shared%20prefixes.%20In%20this%20work%2C%20we%20introduce%20TyphoonMLA%2C%20a%20hybrid%20approach%20that%20combines%20naive%20and%20absorb%20formulations%20to%20harness%20the%20strengths%20of%20both.%20TyphoonMLA%20effectively%20leverages%20the%20shared%20prefix%20by%20applying%20the%20naive%20formulation%20to%20the%20compute-bound%20parts%20of%20attention%20calculations%2C%20while%20reducing%20the%20bandwidth%20requirements%20for%20non-shared%20parts%20by%20using%20the%20absorb%20formulation.%20As%20a%20result%2C%20TyphoonMLA%20improves%20the%20throughput%20of%20attention%20calculations%20in%20MLA%20architectures%20by%20up%20to%203x%20and%203.24x%20on%20NPU%20and%20GPUs%2C%20with%20only%20a%203%25%20overhead%20in%20HBM%20size.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTyphoonMLA%253A%2520A%2520Mixed%2520Naive-Absorb%2520MLA%2520Kernel%2520For%2520Shared%2520Prefix%26entry.906535625%3DAhmet%2520Caner%2520Y%25C3%25BCz%25C3%25BCg%25C3%25BCler%2520and%2520Ahmet%2520%25C3%2587elik%2520and%2520Jiawei%2520Zhuang%2520and%2520Lukas%2520Cavigelli%26entry.1292438233%3DMulti-Head%2520Latent%2520Attention%2520%2528MLA%2529%2520is%2520a%2520recent%2520attention%2520mechanism%2520adopted%2520in%2520state-of-the-art%2520LLMs%2520such%2520as%2520DeepSeek-v3%2520and%2520Kimi%2520K2.%2520Thanks%2520to%2520its%2520novel%2520formulation%252C%2520MLA%2520allows%2520two%2520functionally%2520equivalent%2520but%2520computationally%2520distinct%2520kernel%2520implementations%253A%2520naive%2520and%2520absorb.%2520While%2520the%2520naive%2520kernels%2520%2528e.g.%252C%2520FlashAttention%2529%2520are%2520typically%2520preferred%2520in%2520training%2520and%2520prefill%2520for%2520their%2520computational%2520efficiency%252C%2520existing%2520decoding%2520kernels%2520%2528e.g.%252C%2520FlashMLA%2529%2520rely%2520on%2520the%2520absorb%2520method%2520to%2520minimize%2520HBM%2520bandwidth%2520usage.%2520However%252C%2520the%2520compute-bound%2520nature%2520of%2520the%2520absorb%2520implementations%2520prohibits%2520performance%2520benefits%2520from%2520data%2520reuse%2520opportunities%2520in%2520attention%2520calculations%252C%2520such%2520as%2520shared%2520prefixes.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TyphoonMLA%252C%2520a%2520hybrid%2520approach%2520that%2520combines%2520naive%2520and%2520absorb%2520formulations%2520to%2520harness%2520the%2520strengths%2520of%2520both.%2520TyphoonMLA%2520effectively%2520leverages%2520the%2520shared%2520prefix%2520by%2520applying%2520the%2520naive%2520formulation%2520to%2520the%2520compute-bound%2520parts%2520of%2520attention%2520calculations%252C%2520while%2520reducing%2520the%2520bandwidth%2520requirements%2520for%2520non-shared%2520parts%2520by%2520using%2520the%2520absorb%2520formulation.%2520As%2520a%2520result%252C%2520TyphoonMLA%2520improves%2520the%2520throughput%2520of%2520attention%2520calculations%2520in%2520MLA%2520architectures%2520by%2520up%2520to%25203x%2520and%25203.24x%2520on%2520NPU%2520and%2520GPUs%252C%2520with%2520only%2520a%25203%2525%2520overhead%2520in%2520HBM%2520size.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TyphoonMLA%3A%20A%20Mixed%20Naive-Absorb%20MLA%20Kernel%20For%20Shared%20Prefix&entry.906535625=Ahmet%20Caner%20Y%C3%BCz%C3%BCg%C3%BCler%20and%20Ahmet%20%C3%87elik%20and%20Jiawei%20Zhuang%20and%20Lukas%20Cavigelli&entry.1292438233=Multi-Head%20Latent%20Attention%20%28MLA%29%20is%20a%20recent%20attention%20mechanism%20adopted%20in%20state-of-the-art%20LLMs%20such%20as%20DeepSeek-v3%20and%20Kimi%20K2.%20Thanks%20to%20its%20novel%20formulation%2C%20MLA%20allows%20two%20functionally%20equivalent%20but%20computationally%20distinct%20kernel%20implementations%3A%20naive%20and%20absorb.%20While%20the%20naive%20kernels%20%28e.g.%2C%20FlashAttention%29%20are%20typically%20preferred%20in%20training%20and%20prefill%20for%20their%20computational%20efficiency%2C%20existing%20decoding%20kernels%20%28e.g.%2C%20FlashMLA%29%20rely%20on%20the%20absorb%20method%20to%20minimize%20HBM%20bandwidth%20usage.%20However%2C%20the%20compute-bound%20nature%20of%20the%20absorb%20implementations%20prohibits%20performance%20benefits%20from%20data%20reuse%20opportunities%20in%20attention%20calculations%2C%20such%20as%20shared%20prefixes.%20In%20this%20work%2C%20we%20introduce%20TyphoonMLA%2C%20a%20hybrid%20approach%20that%20combines%20naive%20and%20absorb%20formulations%20to%20harness%20the%20strengths%20of%20both.%20TyphoonMLA%20effectively%20leverages%20the%20shared%20prefix%20by%20applying%20the%20naive%20formulation%20to%20the%20compute-bound%20parts%20of%20attention%20calculations%2C%20while%20reducing%20the%20bandwidth%20requirements%20for%20non-shared%20parts%20by%20using%20the%20absorb%20formulation.%20As%20a%20result%2C%20TyphoonMLA%20improves%20the%20throughput%20of%20attention%20calculations%20in%20MLA%20architectures%20by%20up%20to%203x%20and%203.24x%20on%20NPU%20and%20GPUs%2C%20with%20only%20a%203%25%20overhead%20in%20HBM%20size.&entry.1838667208=http%3A//arxiv.org/abs/2509.21081v2&entry.124074799=Read"},
{"title": "Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL", "author": "Xun Shao and Aoba Otani and Yuto Hirasuka and Runji Cai and Seng W. Loke", "abstract": "This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.", "link": "http://arxiv.org/abs/2511.11696v2", "date": "2026-02-12", "relevancy": 2.0281, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5148}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Dignity-Aware%20AI%3A%20Next-Generation%20Elderly%20Monitoring%20from%20Fall%20Detection%20to%20ADL&body=Title%3A%20Toward%20Dignity-Aware%20AI%3A%20Next-Generation%20Elderly%20Monitoring%20from%20Fall%20Detection%20to%20ADL%0AAuthor%3A%20Xun%20Shao%20and%20Aoba%20Otani%20and%20Yuto%20Hirasuka%20and%20Runji%20Cai%20and%20Seng%20W.%20Loke%0AAbstract%3A%20This%20position%20paper%20envisions%20a%20next-generation%20elderly%20monitoring%20system%20that%20moves%20beyond%20fall%20detection%20toward%20the%20broader%20goal%20of%20Activities%20of%20Daily%20Living%20%28ADL%29%20recognition.%20Our%20ultimate%20aim%20is%20to%20design%20privacy-preserving%2C%20edge-deployed%2C%20and%20federated%20AI%20systems%20that%20can%20robustly%20detect%20and%20understand%20daily%20routines%2C%20supporting%20independence%20and%20dignity%20in%20aging%20societies.%20At%20present%2C%20ADL-specific%20datasets%20are%20still%20under%20collection.%20As%20a%20preliminary%20step%2C%20we%20demonstrate%20feasibility%20through%20experiments%20using%20the%20SISFall%20dataset%20and%20its%20GAN-augmented%20variants%2C%20treating%20fall%20detection%20as%20a%20proxy%20task.%20We%20report%20initial%20results%20on%20federated%20learning%20with%20non-IID%20conditions%2C%20and%20embedded%20deployment%20on%20Jetson%20Orin%20Nano%20devices.%20We%20then%20outline%20open%20challenges%20such%20as%20domain%20shift%2C%20data%20scarcity%2C%20and%20privacy%20risks%2C%20and%20propose%20directions%20toward%20full%20ADL%20monitoring%20in%20smart-room%20environments.%20This%20work%20highlights%20the%20transition%20from%20single-task%20detection%20to%20comprehensive%20daily%20activity%20recognition%2C%20providing%20both%20early%20evidence%20and%20a%20roadmap%20for%20sustainable%20and%20human-centered%20elderly%20care%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Dignity-Aware%2520AI%253A%2520Next-Generation%2520Elderly%2520Monitoring%2520from%2520Fall%2520Detection%2520to%2520ADL%26entry.906535625%3DXun%2520Shao%2520and%2520Aoba%2520Otani%2520and%2520Yuto%2520Hirasuka%2520and%2520Runji%2520Cai%2520and%2520Seng%2520W.%2520Loke%26entry.1292438233%3DThis%2520position%2520paper%2520envisions%2520a%2520next-generation%2520elderly%2520monitoring%2520system%2520that%2520moves%2520beyond%2520fall%2520detection%2520toward%2520the%2520broader%2520goal%2520of%2520Activities%2520of%2520Daily%2520Living%2520%2528ADL%2529%2520recognition.%2520Our%2520ultimate%2520aim%2520is%2520to%2520design%2520privacy-preserving%252C%2520edge-deployed%252C%2520and%2520federated%2520AI%2520systems%2520that%2520can%2520robustly%2520detect%2520and%2520understand%2520daily%2520routines%252C%2520supporting%2520independence%2520and%2520dignity%2520in%2520aging%2520societies.%2520At%2520present%252C%2520ADL-specific%2520datasets%2520are%2520still%2520under%2520collection.%2520As%2520a%2520preliminary%2520step%252C%2520we%2520demonstrate%2520feasibility%2520through%2520experiments%2520using%2520the%2520SISFall%2520dataset%2520and%2520its%2520GAN-augmented%2520variants%252C%2520treating%2520fall%2520detection%2520as%2520a%2520proxy%2520task.%2520We%2520report%2520initial%2520results%2520on%2520federated%2520learning%2520with%2520non-IID%2520conditions%252C%2520and%2520embedded%2520deployment%2520on%2520Jetson%2520Orin%2520Nano%2520devices.%2520We%2520then%2520outline%2520open%2520challenges%2520such%2520as%2520domain%2520shift%252C%2520data%2520scarcity%252C%2520and%2520privacy%2520risks%252C%2520and%2520propose%2520directions%2520toward%2520full%2520ADL%2520monitoring%2520in%2520smart-room%2520environments.%2520This%2520work%2520highlights%2520the%2520transition%2520from%2520single-task%2520detection%2520to%2520comprehensive%2520daily%2520activity%2520recognition%252C%2520providing%2520both%2520early%2520evidence%2520and%2520a%2520roadmap%2520for%2520sustainable%2520and%2520human-centered%2520elderly%2520care%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Dignity-Aware%20AI%3A%20Next-Generation%20Elderly%20Monitoring%20from%20Fall%20Detection%20to%20ADL&entry.906535625=Xun%20Shao%20and%20Aoba%20Otani%20and%20Yuto%20Hirasuka%20and%20Runji%20Cai%20and%20Seng%20W.%20Loke&entry.1292438233=This%20position%20paper%20envisions%20a%20next-generation%20elderly%20monitoring%20system%20that%20moves%20beyond%20fall%20detection%20toward%20the%20broader%20goal%20of%20Activities%20of%20Daily%20Living%20%28ADL%29%20recognition.%20Our%20ultimate%20aim%20is%20to%20design%20privacy-preserving%2C%20edge-deployed%2C%20and%20federated%20AI%20systems%20that%20can%20robustly%20detect%20and%20understand%20daily%20routines%2C%20supporting%20independence%20and%20dignity%20in%20aging%20societies.%20At%20present%2C%20ADL-specific%20datasets%20are%20still%20under%20collection.%20As%20a%20preliminary%20step%2C%20we%20demonstrate%20feasibility%20through%20experiments%20using%20the%20SISFall%20dataset%20and%20its%20GAN-augmented%20variants%2C%20treating%20fall%20detection%20as%20a%20proxy%20task.%20We%20report%20initial%20results%20on%20federated%20learning%20with%20non-IID%20conditions%2C%20and%20embedded%20deployment%20on%20Jetson%20Orin%20Nano%20devices.%20We%20then%20outline%20open%20challenges%20such%20as%20domain%20shift%2C%20data%20scarcity%2C%20and%20privacy%20risks%2C%20and%20propose%20directions%20toward%20full%20ADL%20monitoring%20in%20smart-room%20environments.%20This%20work%20highlights%20the%20transition%20from%20single-task%20detection%20to%20comprehensive%20daily%20activity%20recognition%2C%20providing%20both%20early%20evidence%20and%20a%20roadmap%20for%20sustainable%20and%20human-centered%20elderly%20care%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2511.11696v2&entry.124074799=Read"},
{"title": "Is Online Linear Optimization Sufficient for Strategic Robustness?", "author": "Yang Cai and Haipeng Luo and Chen-Yu Wei and Weiqiang Zheng", "abstract": "We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller's manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.\n  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\\sqrt{T \\log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\\sqrt{T (\\log K+\\log(T/\u03b4)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].", "link": "http://arxiv.org/abs/2602.12253v1", "date": "2026-02-12", "relevancy": 1.5384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3968}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3882}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Online%20Linear%20Optimization%20Sufficient%20for%20Strategic%20Robustness%3F&body=Title%3A%20Is%20Online%20Linear%20Optimization%20Sufficient%20for%20Strategic%20Robustness%3F%0AAuthor%3A%20Yang%20Cai%20and%20Haipeng%20Luo%20and%20Chen-Yu%20Wei%20and%20Weiqiang%20Zheng%0AAbstract%3A%20We%20consider%20bidding%20in%20repeated%20Bayesian%20first-price%20auctions.%20Bidding%20algorithms%20that%20achieve%20optimal%20regret%20have%20been%20extensively%20studied%2C%20but%20their%20strategic%20robustness%20to%20the%20seller%27s%20manipulation%20remains%20relatively%20underexplored.%20Bidding%20algorithms%20based%20on%20no-swap-regret%20algorithms%20achieve%20both%20desirable%20properties%2C%20but%20are%20suboptimal%20in%20terms%20of%20statistical%20and%20computational%20efficiency.%20In%20contrast%2C%20online%20gradient%20ascent%20is%20the%20only%20algorithm%20that%20achieves%20%24O%28%5Csqrt%7BTK%7D%29%24%20regret%20and%20strategic%20robustness%20%5BKSS24%5D%2C%20where%20%24T%24%20denotes%20the%20number%20of%20auctions%20and%20%24K%24%20the%20number%20of%20bids.%0A%20%20In%20this%20paper%2C%20we%20explore%20whether%20simple%20online%20linear%20optimization%20%28OLO%29%20algorithms%20suffice%20for%20bidding%20algorithms%20with%20both%20desirable%20properties.%20Our%20main%20result%20shows%20that%20sublinear%20linearized%20regret%20is%20sufficient%20for%20strategic%20robustness.%20Specifically%2C%20we%20construct%20simple%20black-box%20reductions%20that%20convert%20any%20OLO%20algorithm%20into%20a%20strategically%20robust%20no-regret%20bidding%20algorithm%2C%20in%20both%20known%20and%20unknown%20value%20distribution%20settings.%20For%20the%20known%20value%20distribution%20case%2C%20our%20reduction%20yields%20a%20bidding%20algorithm%20that%20achieves%20%24O%28%5Csqrt%7BT%20%5Clog%20K%7D%29%24%20regret%20and%20strategic%20robustness%20%28with%20exponential%20improvement%20on%20the%20%24K%24-dependence%20compared%20to%20%5BKSS24%5D%29.%20For%20the%20unknown%20value%20distribution%20case%2C%20our%20reduction%20gives%20a%20bidding%20algorithm%20with%20high-probability%20%24O%28%5Csqrt%7BT%20%28%5Clog%20K%2B%5Clog%28T/%CE%B4%29%7D%29%24%20regret%20and%20strategic%20robustness%2C%20while%20removing%20the%20bounded%20density%20assumption%20made%20in%20%5BKSS24%5D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Online%2520Linear%2520Optimization%2520Sufficient%2520for%2520Strategic%2520Robustness%253F%26entry.906535625%3DYang%2520Cai%2520and%2520Haipeng%2520Luo%2520and%2520Chen-Yu%2520Wei%2520and%2520Weiqiang%2520Zheng%26entry.1292438233%3DWe%2520consider%2520bidding%2520in%2520repeated%2520Bayesian%2520first-price%2520auctions.%2520Bidding%2520algorithms%2520that%2520achieve%2520optimal%2520regret%2520have%2520been%2520extensively%2520studied%252C%2520but%2520their%2520strategic%2520robustness%2520to%2520the%2520seller%2527s%2520manipulation%2520remains%2520relatively%2520underexplored.%2520Bidding%2520algorithms%2520based%2520on%2520no-swap-regret%2520algorithms%2520achieve%2520both%2520desirable%2520properties%252C%2520but%2520are%2520suboptimal%2520in%2520terms%2520of%2520statistical%2520and%2520computational%2520efficiency.%2520In%2520contrast%252C%2520online%2520gradient%2520ascent%2520is%2520the%2520only%2520algorithm%2520that%2520achieves%2520%2524O%2528%255Csqrt%257BTK%257D%2529%2524%2520regret%2520and%2520strategic%2520robustness%2520%255BKSS24%255D%252C%2520where%2520%2524T%2524%2520denotes%2520the%2520number%2520of%2520auctions%2520and%2520%2524K%2524%2520the%2520number%2520of%2520bids.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520whether%2520simple%2520online%2520linear%2520optimization%2520%2528OLO%2529%2520algorithms%2520suffice%2520for%2520bidding%2520algorithms%2520with%2520both%2520desirable%2520properties.%2520Our%2520main%2520result%2520shows%2520that%2520sublinear%2520linearized%2520regret%2520is%2520sufficient%2520for%2520strategic%2520robustness.%2520Specifically%252C%2520we%2520construct%2520simple%2520black-box%2520reductions%2520that%2520convert%2520any%2520OLO%2520algorithm%2520into%2520a%2520strategically%2520robust%2520no-regret%2520bidding%2520algorithm%252C%2520in%2520both%2520known%2520and%2520unknown%2520value%2520distribution%2520settings.%2520For%2520the%2520known%2520value%2520distribution%2520case%252C%2520our%2520reduction%2520yields%2520a%2520bidding%2520algorithm%2520that%2520achieves%2520%2524O%2528%255Csqrt%257BT%2520%255Clog%2520K%257D%2529%2524%2520regret%2520and%2520strategic%2520robustness%2520%2528with%2520exponential%2520improvement%2520on%2520the%2520%2524K%2524-dependence%2520compared%2520to%2520%255BKSS24%255D%2529.%2520For%2520the%2520unknown%2520value%2520distribution%2520case%252C%2520our%2520reduction%2520gives%2520a%2520bidding%2520algorithm%2520with%2520high-probability%2520%2524O%2528%255Csqrt%257BT%2520%2528%255Clog%2520K%252B%255Clog%2528T/%25CE%25B4%2529%257D%2529%2524%2520regret%2520and%2520strategic%2520robustness%252C%2520while%2520removing%2520the%2520bounded%2520density%2520assumption%2520made%2520in%2520%255BKSS24%255D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Online%20Linear%20Optimization%20Sufficient%20for%20Strategic%20Robustness%3F&entry.906535625=Yang%20Cai%20and%20Haipeng%20Luo%20and%20Chen-Yu%20Wei%20and%20Weiqiang%20Zheng&entry.1292438233=We%20consider%20bidding%20in%20repeated%20Bayesian%20first-price%20auctions.%20Bidding%20algorithms%20that%20achieve%20optimal%20regret%20have%20been%20extensively%20studied%2C%20but%20their%20strategic%20robustness%20to%20the%20seller%27s%20manipulation%20remains%20relatively%20underexplored.%20Bidding%20algorithms%20based%20on%20no-swap-regret%20algorithms%20achieve%20both%20desirable%20properties%2C%20but%20are%20suboptimal%20in%20terms%20of%20statistical%20and%20computational%20efficiency.%20In%20contrast%2C%20online%20gradient%20ascent%20is%20the%20only%20algorithm%20that%20achieves%20%24O%28%5Csqrt%7BTK%7D%29%24%20regret%20and%20strategic%20robustness%20%5BKSS24%5D%2C%20where%20%24T%24%20denotes%20the%20number%20of%20auctions%20and%20%24K%24%20the%20number%20of%20bids.%0A%20%20In%20this%20paper%2C%20we%20explore%20whether%20simple%20online%20linear%20optimization%20%28OLO%29%20algorithms%20suffice%20for%20bidding%20algorithms%20with%20both%20desirable%20properties.%20Our%20main%20result%20shows%20that%20sublinear%20linearized%20regret%20is%20sufficient%20for%20strategic%20robustness.%20Specifically%2C%20we%20construct%20simple%20black-box%20reductions%20that%20convert%20any%20OLO%20algorithm%20into%20a%20strategically%20robust%20no-regret%20bidding%20algorithm%2C%20in%20both%20known%20and%20unknown%20value%20distribution%20settings.%20For%20the%20known%20value%20distribution%20case%2C%20our%20reduction%20yields%20a%20bidding%20algorithm%20that%20achieves%20%24O%28%5Csqrt%7BT%20%5Clog%20K%7D%29%24%20regret%20and%20strategic%20robustness%20%28with%20exponential%20improvement%20on%20the%20%24K%24-dependence%20compared%20to%20%5BKSS24%5D%29.%20For%20the%20unknown%20value%20distribution%20case%2C%20our%20reduction%20gives%20a%20bidding%20algorithm%20with%20high-probability%20%24O%28%5Csqrt%7BT%20%28%5Clog%20K%2B%5Clog%28T/%CE%B4%29%7D%29%24%20regret%20and%20strategic%20robustness%2C%20while%20removing%20the%20bounded%20density%20assumption%20made%20in%20%5BKSS24%5D.&entry.1838667208=http%3A//arxiv.org/abs/2602.12253v1&entry.124074799=Read"},
{"title": "From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL", "author": "Ziyi Zhao and Qingchuan Li and Yuxuan Xu", "abstract": "Path signatures embed trajectories into tensor algebra and constitute a universal, non-parametric representation of paths; however, in the standard form, they collapse temporal structure into a single global object, which limits their suitability for decision-making problems that require step-wise reactivity. We propose the Incremental Signature Contribution (ISC) method, which decomposes truncated path signatures into a temporally ordered sequence of elements in the tensor-algebra space, corresponding to incremental contributions induced by last path increments. This reconstruction preserves the algebraic structure and expressivity of signatures, while making their internal temporal evolution explicit, enabling processing signature-based representations via sequential modeling approaches. In contrast to full signatures, ISC is inherently sensitive to instantaneous trajectory updates, which is critical for sensitive and stability-requiring control dynamics. Building on this representation, we introduce ISC-Transformer (ISCT), an offline reinforcement learning model that integrates ISC into a standard Transformer architecture without further architectural modification. We evaluate ISCT on HalfCheetah, Walker2d, Hopper, and Maze2d, including settings with delayed rewards and downgraded datasets. The results demonstrate that ISC method provides a theoretically grounded and practically effective alternative to path processing for temporally sensitive control tasks.", "link": "http://arxiv.org/abs/2602.11805v1", "date": "2026-02-12", "relevancy": 1.0097, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.503}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Path%20Signatures%20to%20Sequential%20Modeling%3A%20Incremental%20Signature%20Contributions%20for%20Offline%20RL&body=Title%3A%20From%20Path%20Signatures%20to%20Sequential%20Modeling%3A%20Incremental%20Signature%20Contributions%20for%20Offline%20RL%0AAuthor%3A%20Ziyi%20Zhao%20and%20Qingchuan%20Li%20and%20Yuxuan%20Xu%0AAbstract%3A%20Path%20signatures%20embed%20trajectories%20into%20tensor%20algebra%20and%20constitute%20a%20universal%2C%20non-parametric%20representation%20of%20paths%3B%20however%2C%20in%20the%20standard%20form%2C%20they%20collapse%20temporal%20structure%20into%20a%20single%20global%20object%2C%20which%20limits%20their%20suitability%20for%20decision-making%20problems%20that%20require%20step-wise%20reactivity.%20We%20propose%20the%20Incremental%20Signature%20Contribution%20%28ISC%29%20method%2C%20which%20decomposes%20truncated%20path%20signatures%20into%20a%20temporally%20ordered%20sequence%20of%20elements%20in%20the%20tensor-algebra%20space%2C%20corresponding%20to%20incremental%20contributions%20induced%20by%20last%20path%20increments.%20This%20reconstruction%20preserves%20the%20algebraic%20structure%20and%20expressivity%20of%20signatures%2C%20while%20making%20their%20internal%20temporal%20evolution%20explicit%2C%20enabling%20processing%20signature-based%20representations%20via%20sequential%20modeling%20approaches.%20In%20contrast%20to%20full%20signatures%2C%20ISC%20is%20inherently%20sensitive%20to%20instantaneous%20trajectory%20updates%2C%20which%20is%20critical%20for%20sensitive%20and%20stability-requiring%20control%20dynamics.%20Building%20on%20this%20representation%2C%20we%20introduce%20ISC-Transformer%20%28ISCT%29%2C%20an%20offline%20reinforcement%20learning%20model%20that%20integrates%20ISC%20into%20a%20standard%20Transformer%20architecture%20without%20further%20architectural%20modification.%20We%20evaluate%20ISCT%20on%20HalfCheetah%2C%20Walker2d%2C%20Hopper%2C%20and%20Maze2d%2C%20including%20settings%20with%20delayed%20rewards%20and%20downgraded%20datasets.%20The%20results%20demonstrate%20that%20ISC%20method%20provides%20a%20theoretically%20grounded%20and%20practically%20effective%20alternative%20to%20path%20processing%20for%20temporally%20sensitive%20control%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Path%2520Signatures%2520to%2520Sequential%2520Modeling%253A%2520Incremental%2520Signature%2520Contributions%2520for%2520Offline%2520RL%26entry.906535625%3DZiyi%2520Zhao%2520and%2520Qingchuan%2520Li%2520and%2520Yuxuan%2520Xu%26entry.1292438233%3DPath%2520signatures%2520embed%2520trajectories%2520into%2520tensor%2520algebra%2520and%2520constitute%2520a%2520universal%252C%2520non-parametric%2520representation%2520of%2520paths%253B%2520however%252C%2520in%2520the%2520standard%2520form%252C%2520they%2520collapse%2520temporal%2520structure%2520into%2520a%2520single%2520global%2520object%252C%2520which%2520limits%2520their%2520suitability%2520for%2520decision-making%2520problems%2520that%2520require%2520step-wise%2520reactivity.%2520We%2520propose%2520the%2520Incremental%2520Signature%2520Contribution%2520%2528ISC%2529%2520method%252C%2520which%2520decomposes%2520truncated%2520path%2520signatures%2520into%2520a%2520temporally%2520ordered%2520sequence%2520of%2520elements%2520in%2520the%2520tensor-algebra%2520space%252C%2520corresponding%2520to%2520incremental%2520contributions%2520induced%2520by%2520last%2520path%2520increments.%2520This%2520reconstruction%2520preserves%2520the%2520algebraic%2520structure%2520and%2520expressivity%2520of%2520signatures%252C%2520while%2520making%2520their%2520internal%2520temporal%2520evolution%2520explicit%252C%2520enabling%2520processing%2520signature-based%2520representations%2520via%2520sequential%2520modeling%2520approaches.%2520In%2520contrast%2520to%2520full%2520signatures%252C%2520ISC%2520is%2520inherently%2520sensitive%2520to%2520instantaneous%2520trajectory%2520updates%252C%2520which%2520is%2520critical%2520for%2520sensitive%2520and%2520stability-requiring%2520control%2520dynamics.%2520Building%2520on%2520this%2520representation%252C%2520we%2520introduce%2520ISC-Transformer%2520%2528ISCT%2529%252C%2520an%2520offline%2520reinforcement%2520learning%2520model%2520that%2520integrates%2520ISC%2520into%2520a%2520standard%2520Transformer%2520architecture%2520without%2520further%2520architectural%2520modification.%2520We%2520evaluate%2520ISCT%2520on%2520HalfCheetah%252C%2520Walker2d%252C%2520Hopper%252C%2520and%2520Maze2d%252C%2520including%2520settings%2520with%2520delayed%2520rewards%2520and%2520downgraded%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520ISC%2520method%2520provides%2520a%2520theoretically%2520grounded%2520and%2520practically%2520effective%2520alternative%2520to%2520path%2520processing%2520for%2520temporally%2520sensitive%2520control%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Path%20Signatures%20to%20Sequential%20Modeling%3A%20Incremental%20Signature%20Contributions%20for%20Offline%20RL&entry.906535625=Ziyi%20Zhao%20and%20Qingchuan%20Li%20and%20Yuxuan%20Xu&entry.1292438233=Path%20signatures%20embed%20trajectories%20into%20tensor%20algebra%20and%20constitute%20a%20universal%2C%20non-parametric%20representation%20of%20paths%3B%20however%2C%20in%20the%20standard%20form%2C%20they%20collapse%20temporal%20structure%20into%20a%20single%20global%20object%2C%20which%20limits%20their%20suitability%20for%20decision-making%20problems%20that%20require%20step-wise%20reactivity.%20We%20propose%20the%20Incremental%20Signature%20Contribution%20%28ISC%29%20method%2C%20which%20decomposes%20truncated%20path%20signatures%20into%20a%20temporally%20ordered%20sequence%20of%20elements%20in%20the%20tensor-algebra%20space%2C%20corresponding%20to%20incremental%20contributions%20induced%20by%20last%20path%20increments.%20This%20reconstruction%20preserves%20the%20algebraic%20structure%20and%20expressivity%20of%20signatures%2C%20while%20making%20their%20internal%20temporal%20evolution%20explicit%2C%20enabling%20processing%20signature-based%20representations%20via%20sequential%20modeling%20approaches.%20In%20contrast%20to%20full%20signatures%2C%20ISC%20is%20inherently%20sensitive%20to%20instantaneous%20trajectory%20updates%2C%20which%20is%20critical%20for%20sensitive%20and%20stability-requiring%20control%20dynamics.%20Building%20on%20this%20representation%2C%20we%20introduce%20ISC-Transformer%20%28ISCT%29%2C%20an%20offline%20reinforcement%20learning%20model%20that%20integrates%20ISC%20into%20a%20standard%20Transformer%20architecture%20without%20further%20architectural%20modification.%20We%20evaluate%20ISCT%20on%20HalfCheetah%2C%20Walker2d%2C%20Hopper%2C%20and%20Maze2d%2C%20including%20settings%20with%20delayed%20rewards%20and%20downgraded%20datasets.%20The%20results%20demonstrate%20that%20ISC%20method%20provides%20a%20theoretically%20grounded%20and%20practically%20effective%20alternative%20to%20path%20processing%20for%20temporally%20sensitive%20control%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.11805v1&entry.124074799=Read"},
{"title": "Efficient and Sharp Off-Policy Learning under Unobserved Confounding", "author": "Konstantin Hess and Dennis Frauen and Valentyn Melnychuk and Stefan Feuerriegel", "abstract": "We develop a novel method for personalized off-policy learning in scenarios with unobserved confounding. Thereby, we address a key limitation of standard policy learning: standard policy learning assumes unconfoundedness, meaning that no unobserved factors influence both treatment assignment and outcomes. However, this assumption is often violated, because of which standard policy learning produces biased estimates and thus leads to policies that can be harmful. To address this limitation, we employ causal sensitivity analysis and derive a semi-parametrically efficient estimator for a sharp bound on the value function under unobserved confounding. Our estimator has three advantages: (1) Unlike existing works, our estimator avoids unstable minimax optimization based on inverse propensity weighted outcomes. (2) Our estimator is semi-parametrically efficient. (3) We prove that our estimator leads to the optimal confounding-robust policy. Finally, we extend our theory to the related task of policy improvement under unobserved confounding, i.e., when a baseline policy such as the standard of care is available. We show in experiments with synthetic and real-world data that our method outperforms simple plug-in approaches and existing baselines. Our method is highly relevant for decision-making where unobserved confounding can be problematic, such as in healthcare and public policy.", "link": "http://arxiv.org/abs/2502.13022v2", "date": "2026-02-12", "relevancy": 1.8707, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4871}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Sharp%20Off-Policy%20Learning%20under%20Unobserved%20Confounding&body=Title%3A%20Efficient%20and%20Sharp%20Off-Policy%20Learning%20under%20Unobserved%20Confounding%0AAuthor%3A%20Konstantin%20Hess%20and%20Dennis%20Frauen%20and%20Valentyn%20Melnychuk%20and%20Stefan%20Feuerriegel%0AAbstract%3A%20We%20develop%20a%20novel%20method%20for%20personalized%20off-policy%20learning%20in%20scenarios%20with%20unobserved%20confounding.%20Thereby%2C%20we%20address%20a%20key%20limitation%20of%20standard%20policy%20learning%3A%20standard%20policy%20learning%20assumes%20unconfoundedness%2C%20meaning%20that%20no%20unobserved%20factors%20influence%20both%20treatment%20assignment%20and%20outcomes.%20However%2C%20this%20assumption%20is%20often%20violated%2C%20because%20of%20which%20standard%20policy%20learning%20produces%20biased%20estimates%20and%20thus%20leads%20to%20policies%20that%20can%20be%20harmful.%20To%20address%20this%20limitation%2C%20we%20employ%20causal%20sensitivity%20analysis%20and%20derive%20a%20semi-parametrically%20efficient%20estimator%20for%20a%20sharp%20bound%20on%20the%20value%20function%20under%20unobserved%20confounding.%20Our%20estimator%20has%20three%20advantages%3A%20%281%29%20Unlike%20existing%20works%2C%20our%20estimator%20avoids%20unstable%20minimax%20optimization%20based%20on%20inverse%20propensity%20weighted%20outcomes.%20%282%29%20Our%20estimator%20is%20semi-parametrically%20efficient.%20%283%29%20We%20prove%20that%20our%20estimator%20leads%20to%20the%20optimal%20confounding-robust%20policy.%20Finally%2C%20we%20extend%20our%20theory%20to%20the%20related%20task%20of%20policy%20improvement%20under%20unobserved%20confounding%2C%20i.e.%2C%20when%20a%20baseline%20policy%20such%20as%20the%20standard%20of%20care%20is%20available.%20We%20show%20in%20experiments%20with%20synthetic%20and%20real-world%20data%20that%20our%20method%20outperforms%20simple%20plug-in%20approaches%20and%20existing%20baselines.%20Our%20method%20is%20highly%20relevant%20for%20decision-making%20where%20unobserved%20confounding%20can%20be%20problematic%2C%20such%20as%20in%20healthcare%20and%20public%20policy.%0ALink%3A%20http%3A//arxiv.org/abs/2502.13022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Sharp%2520Off-Policy%2520Learning%2520under%2520Unobserved%2520Confounding%26entry.906535625%3DKonstantin%2520Hess%2520and%2520Dennis%2520Frauen%2520and%2520Valentyn%2520Melnychuk%2520and%2520Stefan%2520Feuerriegel%26entry.1292438233%3DWe%2520develop%2520a%2520novel%2520method%2520for%2520personalized%2520off-policy%2520learning%2520in%2520scenarios%2520with%2520unobserved%2520confounding.%2520Thereby%252C%2520we%2520address%2520a%2520key%2520limitation%2520of%2520standard%2520policy%2520learning%253A%2520standard%2520policy%2520learning%2520assumes%2520unconfoundedness%252C%2520meaning%2520that%2520no%2520unobserved%2520factors%2520influence%2520both%2520treatment%2520assignment%2520and%2520outcomes.%2520However%252C%2520this%2520assumption%2520is%2520often%2520violated%252C%2520because%2520of%2520which%2520standard%2520policy%2520learning%2520produces%2520biased%2520estimates%2520and%2520thus%2520leads%2520to%2520policies%2520that%2520can%2520be%2520harmful.%2520To%2520address%2520this%2520limitation%252C%2520we%2520employ%2520causal%2520sensitivity%2520analysis%2520and%2520derive%2520a%2520semi-parametrically%2520efficient%2520estimator%2520for%2520a%2520sharp%2520bound%2520on%2520the%2520value%2520function%2520under%2520unobserved%2520confounding.%2520Our%2520estimator%2520has%2520three%2520advantages%253A%2520%25281%2529%2520Unlike%2520existing%2520works%252C%2520our%2520estimator%2520avoids%2520unstable%2520minimax%2520optimization%2520based%2520on%2520inverse%2520propensity%2520weighted%2520outcomes.%2520%25282%2529%2520Our%2520estimator%2520is%2520semi-parametrically%2520efficient.%2520%25283%2529%2520We%2520prove%2520that%2520our%2520estimator%2520leads%2520to%2520the%2520optimal%2520confounding-robust%2520policy.%2520Finally%252C%2520we%2520extend%2520our%2520theory%2520to%2520the%2520related%2520task%2520of%2520policy%2520improvement%2520under%2520unobserved%2520confounding%252C%2520i.e.%252C%2520when%2520a%2520baseline%2520policy%2520such%2520as%2520the%2520standard%2520of%2520care%2520is%2520available.%2520We%2520show%2520in%2520experiments%2520with%2520synthetic%2520and%2520real-world%2520data%2520that%2520our%2520method%2520outperforms%2520simple%2520plug-in%2520approaches%2520and%2520existing%2520baselines.%2520Our%2520method%2520is%2520highly%2520relevant%2520for%2520decision-making%2520where%2520unobserved%2520confounding%2520can%2520be%2520problematic%252C%2520such%2520as%2520in%2520healthcare%2520and%2520public%2520policy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Sharp%20Off-Policy%20Learning%20under%20Unobserved%20Confounding&entry.906535625=Konstantin%20Hess%20and%20Dennis%20Frauen%20and%20Valentyn%20Melnychuk%20and%20Stefan%20Feuerriegel&entry.1292438233=We%20develop%20a%20novel%20method%20for%20personalized%20off-policy%20learning%20in%20scenarios%20with%20unobserved%20confounding.%20Thereby%2C%20we%20address%20a%20key%20limitation%20of%20standard%20policy%20learning%3A%20standard%20policy%20learning%20assumes%20unconfoundedness%2C%20meaning%20that%20no%20unobserved%20factors%20influence%20both%20treatment%20assignment%20and%20outcomes.%20However%2C%20this%20assumption%20is%20often%20violated%2C%20because%20of%20which%20standard%20policy%20learning%20produces%20biased%20estimates%20and%20thus%20leads%20to%20policies%20that%20can%20be%20harmful.%20To%20address%20this%20limitation%2C%20we%20employ%20causal%20sensitivity%20analysis%20and%20derive%20a%20semi-parametrically%20efficient%20estimator%20for%20a%20sharp%20bound%20on%20the%20value%20function%20under%20unobserved%20confounding.%20Our%20estimator%20has%20three%20advantages%3A%20%281%29%20Unlike%20existing%20works%2C%20our%20estimator%20avoids%20unstable%20minimax%20optimization%20based%20on%20inverse%20propensity%20weighted%20outcomes.%20%282%29%20Our%20estimator%20is%20semi-parametrically%20efficient.%20%283%29%20We%20prove%20that%20our%20estimator%20leads%20to%20the%20optimal%20confounding-robust%20policy.%20Finally%2C%20we%20extend%20our%20theory%20to%20the%20related%20task%20of%20policy%20improvement%20under%20unobserved%20confounding%2C%20i.e.%2C%20when%20a%20baseline%20policy%20such%20as%20the%20standard%20of%20care%20is%20available.%20We%20show%20in%20experiments%20with%20synthetic%20and%20real-world%20data%20that%20our%20method%20outperforms%20simple%20plug-in%20approaches%20and%20existing%20baselines.%20Our%20method%20is%20highly%20relevant%20for%20decision-making%20where%20unobserved%20confounding%20can%20be%20problematic%2C%20such%20as%20in%20healthcare%20and%20public%20policy.&entry.1838667208=http%3A//arxiv.org/abs/2502.13022v2&entry.124074799=Read"},
{"title": "Deep Kernel Fusion for Transformers", "author": "Zixi Zhang and Zhiwen Mo and Yiren Zhao and Robert Mullins", "abstract": "Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.", "link": "http://arxiv.org/abs/2602.11808v1", "date": "2026-02-12", "relevancy": 1.4762, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5422}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5021}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Kernel%20Fusion%20for%20Transformers&body=Title%3A%20Deep%20Kernel%20Fusion%20for%20Transformers%0AAuthor%3A%20Zixi%20Zhang%20and%20Zhiwen%20Mo%20and%20Yiren%20Zhao%20and%20Robert%20Mullins%0AAbstract%3A%20Agentic%20LLM%20inference%20with%20long%20contexts%20is%20increasingly%20limited%20by%20memory%20bandwidth%20rather%20than%20compute.%20In%20this%20setting%2C%20SwiGLU%20MLP%20blocks%2C%20whose%20large%20weights%20exceed%20cache%20capacity%2C%20become%20a%20major%20yet%20under-optimized%20bottleneck.%20We%20propose%20DeepFusionKernel%2C%20a%20deeply%20fused%20kernel%20that%20cuts%20HBM%20traffic%20and%20boosts%20cache%20reuse%2C%20delivering%20up%20to%2013.2%25%20speedup%20on%20H100%20and%209.7%25%20on%20A100%20over%20SGLang.%20Integrated%20with%20SGLang%20and%20paired%20with%20a%20kernel%20scheduler%2C%20DeepFusionKernel%20ensures%20consistent%20accelerations%20over%20generation%20lengths%2C%20while%20remaining%20adaptable%20to%20diverse%20models%2C%20inference%20configurations%2C%20and%20hardware%20platforms.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Kernel%2520Fusion%2520for%2520Transformers%26entry.906535625%3DZixi%2520Zhang%2520and%2520Zhiwen%2520Mo%2520and%2520Yiren%2520Zhao%2520and%2520Robert%2520Mullins%26entry.1292438233%3DAgentic%2520LLM%2520inference%2520with%2520long%2520contexts%2520is%2520increasingly%2520limited%2520by%2520memory%2520bandwidth%2520rather%2520than%2520compute.%2520In%2520this%2520setting%252C%2520SwiGLU%2520MLP%2520blocks%252C%2520whose%2520large%2520weights%2520exceed%2520cache%2520capacity%252C%2520become%2520a%2520major%2520yet%2520under-optimized%2520bottleneck.%2520We%2520propose%2520DeepFusionKernel%252C%2520a%2520deeply%2520fused%2520kernel%2520that%2520cuts%2520HBM%2520traffic%2520and%2520boosts%2520cache%2520reuse%252C%2520delivering%2520up%2520to%252013.2%2525%2520speedup%2520on%2520H100%2520and%25209.7%2525%2520on%2520A100%2520over%2520SGLang.%2520Integrated%2520with%2520SGLang%2520and%2520paired%2520with%2520a%2520kernel%2520scheduler%252C%2520DeepFusionKernel%2520ensures%2520consistent%2520accelerations%2520over%2520generation%2520lengths%252C%2520while%2520remaining%2520adaptable%2520to%2520diverse%2520models%252C%2520inference%2520configurations%252C%2520and%2520hardware%2520platforms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Kernel%20Fusion%20for%20Transformers&entry.906535625=Zixi%20Zhang%20and%20Zhiwen%20Mo%20and%20Yiren%20Zhao%20and%20Robert%20Mullins&entry.1292438233=Agentic%20LLM%20inference%20with%20long%20contexts%20is%20increasingly%20limited%20by%20memory%20bandwidth%20rather%20than%20compute.%20In%20this%20setting%2C%20SwiGLU%20MLP%20blocks%2C%20whose%20large%20weights%20exceed%20cache%20capacity%2C%20become%20a%20major%20yet%20under-optimized%20bottleneck.%20We%20propose%20DeepFusionKernel%2C%20a%20deeply%20fused%20kernel%20that%20cuts%20HBM%20traffic%20and%20boosts%20cache%20reuse%2C%20delivering%20up%20to%2013.2%25%20speedup%20on%20H100%20and%209.7%25%20on%20A100%20over%20SGLang.%20Integrated%20with%20SGLang%20and%20paired%20with%20a%20kernel%20scheduler%2C%20DeepFusionKernel%20ensures%20consistent%20accelerations%20over%20generation%20lengths%2C%20while%20remaining%20adaptable%20to%20diverse%20models%2C%20inference%20configurations%2C%20and%20hardware%20platforms.&entry.1838667208=http%3A//arxiv.org/abs/2602.11808v1&entry.124074799=Read"},
{"title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model", "author": "Yanjiang Guo and Tony Lee and Lucy Xiaoyang Shi and Jianyu Chen and Percy Liang and Chelsea Finn", "abstract": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w", "link": "http://arxiv.org/abs/2602.12063v1", "date": "2026-02-12", "relevancy": 1.0517, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5295}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5262}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLAW%3A%20Iterative%20Co-Improvement%20of%20Vision-Language-Action%20Policy%20and%20World%20Model&body=Title%3A%20VLAW%3A%20Iterative%20Co-Improvement%20of%20Vision-Language-Action%20Policy%20and%20World%20Model%0AAuthor%3A%20Yanjiang%20Guo%20and%20Tony%20Lee%20and%20Lucy%20Xiaoyang%20Shi%20and%20Jianyu%20Chen%20and%20Percy%20Liang%20and%20Chelsea%20Finn%0AAbstract%3A%20The%20goal%20of%20this%20paper%20is%20to%20improve%20the%20performance%20and%20reliability%20of%20vision-language-action%20%28VLA%29%20models%20through%20iterative%20online%20interaction.%20Since%20collecting%20policy%20rollouts%20in%20the%20real%20world%20is%20expensive%2C%20we%20investigate%20whether%20a%20learned%20simulator-specifically%2C%20an%20action-conditioned%20video%20generation%20model-can%20be%20used%20to%20generate%20additional%20rollout%20data.%20Unfortunately%2C%20existing%20world%20models%20lack%20the%20physical%20fidelity%20necessary%20for%20policy%20improvement%3A%20they%20are%20predominantly%20trained%20on%20demonstration%20datasets%20that%20lack%20coverage%20of%20many%20different%20physical%20interactions%20%28particularly%20failure%20cases%29%20and%20struggle%20to%20accurately%20model%20small%20yet%20critical%20physical%20details%20in%20contact-rich%20object%20manipulation.%20We%20propose%20a%20simple%20iterative%20improvement%20algorithm%20that%20uses%20real-world%20roll-out%20data%20to%20improve%20the%20fidelity%20of%20the%20world%20model%2C%20which%20can%20then%2C%20in%20turn%2C%20be%20used%20to%20generate%20supplemental%20synthetic%20data%20for%20improving%20the%20VLA%20model.%20In%20our%20experiments%20on%20a%20real%20robot%2C%20we%20use%20this%20approach%20to%20improve%20the%20performance%20of%20a%20state-of-the-art%20VLA%20model%20on%20multiple%20downstream%20tasks.%20We%20achieve%20a%2039.2%25%20absolute%20success%20rate%20improvement%20over%20the%20base%20policy%20and%2011.6%25%20improvement%20from%20training%20with%20the%20generated%20synthetic%20rollouts.%20Videos%20can%20be%20found%20at%20this%20anonymous%20website%3A%20https%3A//sites.google.com/view/vla-w%0ALink%3A%20http%3A//arxiv.org/abs/2602.12063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLAW%253A%2520Iterative%2520Co-Improvement%2520of%2520Vision-Language-Action%2520Policy%2520and%2520World%2520Model%26entry.906535625%3DYanjiang%2520Guo%2520and%2520Tony%2520Lee%2520and%2520Lucy%2520Xiaoyang%2520Shi%2520and%2520Jianyu%2520Chen%2520and%2520Percy%2520Liang%2520and%2520Chelsea%2520Finn%26entry.1292438233%3DThe%2520goal%2520of%2520this%2520paper%2520is%2520to%2520improve%2520the%2520performance%2520and%2520reliability%2520of%2520vision-language-action%2520%2528VLA%2529%2520models%2520through%2520iterative%2520online%2520interaction.%2520Since%2520collecting%2520policy%2520rollouts%2520in%2520the%2520real%2520world%2520is%2520expensive%252C%2520we%2520investigate%2520whether%2520a%2520learned%2520simulator-specifically%252C%2520an%2520action-conditioned%2520video%2520generation%2520model-can%2520be%2520used%2520to%2520generate%2520additional%2520rollout%2520data.%2520Unfortunately%252C%2520existing%2520world%2520models%2520lack%2520the%2520physical%2520fidelity%2520necessary%2520for%2520policy%2520improvement%253A%2520they%2520are%2520predominantly%2520trained%2520on%2520demonstration%2520datasets%2520that%2520lack%2520coverage%2520of%2520many%2520different%2520physical%2520interactions%2520%2528particularly%2520failure%2520cases%2529%2520and%2520struggle%2520to%2520accurately%2520model%2520small%2520yet%2520critical%2520physical%2520details%2520in%2520contact-rich%2520object%2520manipulation.%2520We%2520propose%2520a%2520simple%2520iterative%2520improvement%2520algorithm%2520that%2520uses%2520real-world%2520roll-out%2520data%2520to%2520improve%2520the%2520fidelity%2520of%2520the%2520world%2520model%252C%2520which%2520can%2520then%252C%2520in%2520turn%252C%2520be%2520used%2520to%2520generate%2520supplemental%2520synthetic%2520data%2520for%2520improving%2520the%2520VLA%2520model.%2520In%2520our%2520experiments%2520on%2520a%2520real%2520robot%252C%2520we%2520use%2520this%2520approach%2520to%2520improve%2520the%2520performance%2520of%2520a%2520state-of-the-art%2520VLA%2520model%2520on%2520multiple%2520downstream%2520tasks.%2520We%2520achieve%2520a%252039.2%2525%2520absolute%2520success%2520rate%2520improvement%2520over%2520the%2520base%2520policy%2520and%252011.6%2525%2520improvement%2520from%2520training%2520with%2520the%2520generated%2520synthetic%2520rollouts.%2520Videos%2520can%2520be%2520found%2520at%2520this%2520anonymous%2520website%253A%2520https%253A//sites.google.com/view/vla-w%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLAW%3A%20Iterative%20Co-Improvement%20of%20Vision-Language-Action%20Policy%20and%20World%20Model&entry.906535625=Yanjiang%20Guo%20and%20Tony%20Lee%20and%20Lucy%20Xiaoyang%20Shi%20and%20Jianyu%20Chen%20and%20Percy%20Liang%20and%20Chelsea%20Finn&entry.1292438233=The%20goal%20of%20this%20paper%20is%20to%20improve%20the%20performance%20and%20reliability%20of%20vision-language-action%20%28VLA%29%20models%20through%20iterative%20online%20interaction.%20Since%20collecting%20policy%20rollouts%20in%20the%20real%20world%20is%20expensive%2C%20we%20investigate%20whether%20a%20learned%20simulator-specifically%2C%20an%20action-conditioned%20video%20generation%20model-can%20be%20used%20to%20generate%20additional%20rollout%20data.%20Unfortunately%2C%20existing%20world%20models%20lack%20the%20physical%20fidelity%20necessary%20for%20policy%20improvement%3A%20they%20are%20predominantly%20trained%20on%20demonstration%20datasets%20that%20lack%20coverage%20of%20many%20different%20physical%20interactions%20%28particularly%20failure%20cases%29%20and%20struggle%20to%20accurately%20model%20small%20yet%20critical%20physical%20details%20in%20contact-rich%20object%20manipulation.%20We%20propose%20a%20simple%20iterative%20improvement%20algorithm%20that%20uses%20real-world%20roll-out%20data%20to%20improve%20the%20fidelity%20of%20the%20world%20model%2C%20which%20can%20then%2C%20in%20turn%2C%20be%20used%20to%20generate%20supplemental%20synthetic%20data%20for%20improving%20the%20VLA%20model.%20In%20our%20experiments%20on%20a%20real%20robot%2C%20we%20use%20this%20approach%20to%20improve%20the%20performance%20of%20a%20state-of-the-art%20VLA%20model%20on%20multiple%20downstream%20tasks.%20We%20achieve%20a%2039.2%25%20absolute%20success%20rate%20improvement%20over%20the%20base%20policy%20and%2011.6%25%20improvement%20from%20training%20with%20the%20generated%20synthetic%20rollouts.%20Videos%20can%20be%20found%20at%20this%20anonymous%20website%3A%20https%3A//sites.google.com/view/vla-w&entry.1838667208=http%3A//arxiv.org/abs/2602.12063v1&entry.124074799=Read"},
{"title": "Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks", "author": "Zhihong Liu and Yang Li and Rengming Huang and Cewu Lu and Panpan Cai", "abstract": "Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-horizon planning in large environments given ambiguous human instructions. At its core, AHAT utilizes an LLM trained to map task instructions and textual scene graphs into grounded subgoals defined in the Planning Domain Definition Language (PDDL). These subgoals are subsequently solved to generate feasible and optimal long-horizon plans through explicit symbolic reasoning. To enhance the model's ability to decompose complex and ambiguous intentions, we introduce TGPO, a novel reinforcement learning algorithm that integrates external correction of intermediate reasoning traces into Group Relative Policy Optimization (GRPO). Experiments demonstrate that AHAT achieves significant performance gains over state-of-the-art prompting, planning, and learning methods, particularly in human-style household tasks characterized by brief instructions but requiring complex execution plans.", "link": "http://arxiv.org/abs/2602.12244v1", "date": "2026-02-12", "relevancy": 1.5945, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any%20House%20Any%20Task%3A%20Scalable%20Long-Horizon%20Planning%20for%20Abstract%20Human%20Tasks&body=Title%3A%20Any%20House%20Any%20Task%3A%20Scalable%20Long-Horizon%20Planning%20for%20Abstract%20Human%20Tasks%0AAuthor%3A%20Zhihong%20Liu%20and%20Yang%20Li%20and%20Rengming%20Huang%20and%20Cewu%20Lu%20and%20Panpan%20Cai%0AAbstract%3A%20Open%20world%20language%20conditioned%20task%20planning%20is%20crucial%20for%20robots%20operating%20in%20large-scale%20household%20environments.%20While%20many%20recent%20works%20attempt%20to%20address%20this%20problem%20using%20Large%20Language%20Models%20%28LLMs%29%20via%20prompting%20or%20training%2C%20a%20key%20challenge%20remains%20scalability.%20Performance%20often%20degrades%20rapidly%20with%20increasing%20environment%20size%2C%20plan%20length%2C%20instruction%20ambiguity%2C%20and%20constraint%20complexity.%20In%20this%20work%2C%20we%20propose%20Any%20House%20Any%20Task%20%28AHAT%29%2C%20a%20household%20task%20planner%20optimized%20for%20long-horizon%20planning%20in%20large%20environments%20given%20ambiguous%20human%20instructions.%20At%20its%20core%2C%20AHAT%20utilizes%20an%20LLM%20trained%20to%20map%20task%20instructions%20and%20textual%20scene%20graphs%20into%20grounded%20subgoals%20defined%20in%20the%20Planning%20Domain%20Definition%20Language%20%28PDDL%29.%20These%20subgoals%20are%20subsequently%20solved%20to%20generate%20feasible%20and%20optimal%20long-horizon%20plans%20through%20explicit%20symbolic%20reasoning.%20To%20enhance%20the%20model%27s%20ability%20to%20decompose%20complex%20and%20ambiguous%20intentions%2C%20we%20introduce%20TGPO%2C%20a%20novel%20reinforcement%20learning%20algorithm%20that%20integrates%20external%20correction%20of%20intermediate%20reasoning%20traces%20into%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Experiments%20demonstrate%20that%20AHAT%20achieves%20significant%20performance%20gains%20over%20state-of-the-art%20prompting%2C%20planning%2C%20and%20learning%20methods%2C%20particularly%20in%20human-style%20household%20tasks%20characterized%20by%20brief%20instructions%20but%20requiring%20complex%20execution%20plans.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny%2520House%2520Any%2520Task%253A%2520Scalable%2520Long-Horizon%2520Planning%2520for%2520Abstract%2520Human%2520Tasks%26entry.906535625%3DZhihong%2520Liu%2520and%2520Yang%2520Li%2520and%2520Rengming%2520Huang%2520and%2520Cewu%2520Lu%2520and%2520Panpan%2520Cai%26entry.1292438233%3DOpen%2520world%2520language%2520conditioned%2520task%2520planning%2520is%2520crucial%2520for%2520robots%2520operating%2520in%2520large-scale%2520household%2520environments.%2520While%2520many%2520recent%2520works%2520attempt%2520to%2520address%2520this%2520problem%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520via%2520prompting%2520or%2520training%252C%2520a%2520key%2520challenge%2520remains%2520scalability.%2520Performance%2520often%2520degrades%2520rapidly%2520with%2520increasing%2520environment%2520size%252C%2520plan%2520length%252C%2520instruction%2520ambiguity%252C%2520and%2520constraint%2520complexity.%2520In%2520this%2520work%252C%2520we%2520propose%2520Any%2520House%2520Any%2520Task%2520%2528AHAT%2529%252C%2520a%2520household%2520task%2520planner%2520optimized%2520for%2520long-horizon%2520planning%2520in%2520large%2520environments%2520given%2520ambiguous%2520human%2520instructions.%2520At%2520its%2520core%252C%2520AHAT%2520utilizes%2520an%2520LLM%2520trained%2520to%2520map%2520task%2520instructions%2520and%2520textual%2520scene%2520graphs%2520into%2520grounded%2520subgoals%2520defined%2520in%2520the%2520Planning%2520Domain%2520Definition%2520Language%2520%2528PDDL%2529.%2520These%2520subgoals%2520are%2520subsequently%2520solved%2520to%2520generate%2520feasible%2520and%2520optimal%2520long-horizon%2520plans%2520through%2520explicit%2520symbolic%2520reasoning.%2520To%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520decompose%2520complex%2520and%2520ambiguous%2520intentions%252C%2520we%2520introduce%2520TGPO%252C%2520a%2520novel%2520reinforcement%2520learning%2520algorithm%2520that%2520integrates%2520external%2520correction%2520of%2520intermediate%2520reasoning%2520traces%2520into%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529.%2520Experiments%2520demonstrate%2520that%2520AHAT%2520achieves%2520significant%2520performance%2520gains%2520over%2520state-of-the-art%2520prompting%252C%2520planning%252C%2520and%2520learning%2520methods%252C%2520particularly%2520in%2520human-style%2520household%2520tasks%2520characterized%2520by%2520brief%2520instructions%2520but%2520requiring%2520complex%2520execution%2520plans.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any%20House%20Any%20Task%3A%20Scalable%20Long-Horizon%20Planning%20for%20Abstract%20Human%20Tasks&entry.906535625=Zhihong%20Liu%20and%20Yang%20Li%20and%20Rengming%20Huang%20and%20Cewu%20Lu%20and%20Panpan%20Cai&entry.1292438233=Open%20world%20language%20conditioned%20task%20planning%20is%20crucial%20for%20robots%20operating%20in%20large-scale%20household%20environments.%20While%20many%20recent%20works%20attempt%20to%20address%20this%20problem%20using%20Large%20Language%20Models%20%28LLMs%29%20via%20prompting%20or%20training%2C%20a%20key%20challenge%20remains%20scalability.%20Performance%20often%20degrades%20rapidly%20with%20increasing%20environment%20size%2C%20plan%20length%2C%20instruction%20ambiguity%2C%20and%20constraint%20complexity.%20In%20this%20work%2C%20we%20propose%20Any%20House%20Any%20Task%20%28AHAT%29%2C%20a%20household%20task%20planner%20optimized%20for%20long-horizon%20planning%20in%20large%20environments%20given%20ambiguous%20human%20instructions.%20At%20its%20core%2C%20AHAT%20utilizes%20an%20LLM%20trained%20to%20map%20task%20instructions%20and%20textual%20scene%20graphs%20into%20grounded%20subgoals%20defined%20in%20the%20Planning%20Domain%20Definition%20Language%20%28PDDL%29.%20These%20subgoals%20are%20subsequently%20solved%20to%20generate%20feasible%20and%20optimal%20long-horizon%20plans%20through%20explicit%20symbolic%20reasoning.%20To%20enhance%20the%20model%27s%20ability%20to%20decompose%20complex%20and%20ambiguous%20intentions%2C%20we%20introduce%20TGPO%2C%20a%20novel%20reinforcement%20learning%20algorithm%20that%20integrates%20external%20correction%20of%20intermediate%20reasoning%20traces%20into%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Experiments%20demonstrate%20that%20AHAT%20achieves%20significant%20performance%20gains%20over%20state-of-the-art%20prompting%2C%20planning%2C%20and%20learning%20methods%2C%20particularly%20in%20human-style%20household%20tasks%20characterized%20by%20brief%20instructions%20but%20requiring%20complex%20execution%20plans.&entry.1838667208=http%3A//arxiv.org/abs/2602.12244v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


