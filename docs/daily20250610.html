<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250609.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression\n  of Dynamic Scenes", "author": "Allen Tu and Haiyang Ying and Alex Hanson and Yonghan Lee and Tom Goldstein and Matthias Zwicker", "abstract": "  Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve\nhigh-quality novel view synthesis by using neural networks to predict the\ntime-varying deformation of each Gaussian. However, performing per-Gaussian\nneural inference at every frame poses a significant bottleneck, limiting\nrendering speed and increasing memory and compute requirements. In this paper,\nwe present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general\npipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS\nrepresentations by reducing neural inference through two complementary\ntechniques. First, we propose a temporal sensitivity pruning score that\nidentifies and removes Gaussians with low contribution to the dynamic scene\nreconstruction. We also introduce an annealing smooth pruning mechanism that\nimproves pruning robustness in real-world scenes with imprecise camera poses.\nSecond, we propose GroupFlow, a motion analysis technique that clusters\nGaussians by trajectory similarity and predicts a single rigid transformation\nper group instead of separate deformations for each Gaussian. Together, our\ntechniques accelerate rendering by $10.37\\times$, reduce model size by\n$7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset.\nSpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on\nthe D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be\nintegrated into any deformable 3DGS or 4DGS framework.\n", "link": "http://arxiv.org/abs/2506.07917v1", "date": "2025-06-09", "relevancy": 3.4028, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7029}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6706}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speedy%20Deformable%203D%20Gaussian%20Splatting%3A%20Fast%20Rendering%20and%20Compression%0A%20%20of%20Dynamic%20Scenes&body=Title%3A%20Speedy%20Deformable%203D%20Gaussian%20Splatting%3A%20Fast%20Rendering%20and%20Compression%0A%20%20of%20Dynamic%20Scenes%0AAuthor%3A%20Allen%20Tu%20and%20Haiyang%20Ying%20and%20Alex%20Hanson%20and%20Yonghan%20Lee%20and%20Tom%20Goldstein%20and%20Matthias%20Zwicker%0AAbstract%3A%20%20%20Recent%20extensions%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20dynamic%20scenes%20achieve%0Ahigh-quality%20novel%20view%20synthesis%20by%20using%20neural%20networks%20to%20predict%20the%0Atime-varying%20deformation%20of%20each%20Gaussian.%20However%2C%20performing%20per-Gaussian%0Aneural%20inference%20at%20every%20frame%20poses%20a%20significant%20bottleneck%2C%20limiting%0Arendering%20speed%20and%20increasing%20memory%20and%20compute%20requirements.%20In%20this%20paper%2C%0Awe%20present%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20%28SpeeDe3DGS%29%2C%20a%20general%0Apipeline%20for%20accelerating%20the%20rendering%20speed%20of%20dynamic%203DGS%20and%204DGS%0Arepresentations%20by%20reducing%20neural%20inference%20through%20two%20complementary%0Atechniques.%20First%2C%20we%20propose%20a%20temporal%20sensitivity%20pruning%20score%20that%0Aidentifies%20and%20removes%20Gaussians%20with%20low%20contribution%20to%20the%20dynamic%20scene%0Areconstruction.%20We%20also%20introduce%20an%20annealing%20smooth%20pruning%20mechanism%20that%0Aimproves%20pruning%20robustness%20in%20real-world%20scenes%20with%20imprecise%20camera%20poses.%0ASecond%2C%20we%20propose%20GroupFlow%2C%20a%20motion%20analysis%20technique%20that%20clusters%0AGaussians%20by%20trajectory%20similarity%20and%20predicts%20a%20single%20rigid%20transformation%0Aper%20group%20instead%20of%20separate%20deformations%20for%20each%20Gaussian.%20Together%2C%20our%0Atechniques%20accelerate%20rendering%20by%20%2410.37%5Ctimes%24%2C%20reduce%20model%20size%20by%0A%247.71%5Ctimes%24%2C%20and%20shorten%20training%20time%20by%20%242.71%5Ctimes%24%20on%20the%20NeRF-DS%20dataset.%0ASpeeDe3DGS%20also%20improves%20rendering%20speed%20by%20%244.20%5Ctimes%24%20and%20%2458.23%5Ctimes%24%20on%0Athe%20D-NeRF%20and%20HyperNeRF%20vrig%20datasets.%20Our%20methods%20are%20modular%20and%20can%20be%0Aintegrated%20into%20any%20deformable%203DGS%20or%204DGS%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeedy%2520Deformable%25203D%2520Gaussian%2520Splatting%253A%2520Fast%2520Rendering%2520and%2520Compression%250A%2520%2520of%2520Dynamic%2520Scenes%26entry.906535625%3DAllen%2520Tu%2520and%2520Haiyang%2520Ying%2520and%2520Alex%2520Hanson%2520and%2520Yonghan%2520Lee%2520and%2520Tom%2520Goldstein%2520and%2520Matthias%2520Zwicker%26entry.1292438233%3D%2520%2520Recent%2520extensions%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%2520dynamic%2520scenes%2520achieve%250Ahigh-quality%2520novel%2520view%2520synthesis%2520by%2520using%2520neural%2520networks%2520to%2520predict%2520the%250Atime-varying%2520deformation%2520of%2520each%2520Gaussian.%2520However%252C%2520performing%2520per-Gaussian%250Aneural%2520inference%2520at%2520every%2520frame%2520poses%2520a%2520significant%2520bottleneck%252C%2520limiting%250Arendering%2520speed%2520and%2520increasing%2520memory%2520and%2520compute%2520requirements.%2520In%2520this%2520paper%252C%250Awe%2520present%2520Speedy%2520Deformable%25203D%2520Gaussian%2520Splatting%2520%2528SpeeDe3DGS%2529%252C%2520a%2520general%250Apipeline%2520for%2520accelerating%2520the%2520rendering%2520speed%2520of%2520dynamic%25203DGS%2520and%25204DGS%250Arepresentations%2520by%2520reducing%2520neural%2520inference%2520through%2520two%2520complementary%250Atechniques.%2520First%252C%2520we%2520propose%2520a%2520temporal%2520sensitivity%2520pruning%2520score%2520that%250Aidentifies%2520and%2520removes%2520Gaussians%2520with%2520low%2520contribution%2520to%2520the%2520dynamic%2520scene%250Areconstruction.%2520We%2520also%2520introduce%2520an%2520annealing%2520smooth%2520pruning%2520mechanism%2520that%250Aimproves%2520pruning%2520robustness%2520in%2520real-world%2520scenes%2520with%2520imprecise%2520camera%2520poses.%250ASecond%252C%2520we%2520propose%2520GroupFlow%252C%2520a%2520motion%2520analysis%2520technique%2520that%2520clusters%250AGaussians%2520by%2520trajectory%2520similarity%2520and%2520predicts%2520a%2520single%2520rigid%2520transformation%250Aper%2520group%2520instead%2520of%2520separate%2520deformations%2520for%2520each%2520Gaussian.%2520Together%252C%2520our%250Atechniques%2520accelerate%2520rendering%2520by%2520%252410.37%255Ctimes%2524%252C%2520reduce%2520model%2520size%2520by%250A%25247.71%255Ctimes%2524%252C%2520and%2520shorten%2520training%2520time%2520by%2520%25242.71%255Ctimes%2524%2520on%2520the%2520NeRF-DS%2520dataset.%250ASpeeDe3DGS%2520also%2520improves%2520rendering%2520speed%2520by%2520%25244.20%255Ctimes%2524%2520and%2520%252458.23%255Ctimes%2524%2520on%250Athe%2520D-NeRF%2520and%2520HyperNeRF%2520vrig%2520datasets.%2520Our%2520methods%2520are%2520modular%2520and%2520can%2520be%250Aintegrated%2520into%2520any%2520deformable%25203DGS%2520or%25204DGS%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speedy%20Deformable%203D%20Gaussian%20Splatting%3A%20Fast%20Rendering%20and%20Compression%0A%20%20of%20Dynamic%20Scenes&entry.906535625=Allen%20Tu%20and%20Haiyang%20Ying%20and%20Alex%20Hanson%20and%20Yonghan%20Lee%20and%20Tom%20Goldstein%20and%20Matthias%20Zwicker&entry.1292438233=%20%20Recent%20extensions%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20dynamic%20scenes%20achieve%0Ahigh-quality%20novel%20view%20synthesis%20by%20using%20neural%20networks%20to%20predict%20the%0Atime-varying%20deformation%20of%20each%20Gaussian.%20However%2C%20performing%20per-Gaussian%0Aneural%20inference%20at%20every%20frame%20poses%20a%20significant%20bottleneck%2C%20limiting%0Arendering%20speed%20and%20increasing%20memory%20and%20compute%20requirements.%20In%20this%20paper%2C%0Awe%20present%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20%28SpeeDe3DGS%29%2C%20a%20general%0Apipeline%20for%20accelerating%20the%20rendering%20speed%20of%20dynamic%203DGS%20and%204DGS%0Arepresentations%20by%20reducing%20neural%20inference%20through%20two%20complementary%0Atechniques.%20First%2C%20we%20propose%20a%20temporal%20sensitivity%20pruning%20score%20that%0Aidentifies%20and%20removes%20Gaussians%20with%20low%20contribution%20to%20the%20dynamic%20scene%0Areconstruction.%20We%20also%20introduce%20an%20annealing%20smooth%20pruning%20mechanism%20that%0Aimproves%20pruning%20robustness%20in%20real-world%20scenes%20with%20imprecise%20camera%20poses.%0ASecond%2C%20we%20propose%20GroupFlow%2C%20a%20motion%20analysis%20technique%20that%20clusters%0AGaussians%20by%20trajectory%20similarity%20and%20predicts%20a%20single%20rigid%20transformation%0Aper%20group%20instead%20of%20separate%20deformations%20for%20each%20Gaussian.%20Together%2C%20our%0Atechniques%20accelerate%20rendering%20by%20%2410.37%5Ctimes%24%2C%20reduce%20model%20size%20by%0A%247.71%5Ctimes%24%2C%20and%20shorten%20training%20time%20by%20%242.71%5Ctimes%24%20on%20the%20NeRF-DS%20dataset.%0ASpeeDe3DGS%20also%20improves%20rendering%20speed%20by%20%244.20%5Ctimes%24%20and%20%2458.23%5Ctimes%24%20on%0Athe%20D-NeRF%20and%20HyperNeRF%20vrig%20datasets.%20Our%20methods%20are%20modular%20and%20can%20be%0Aintegrated%20into%20any%20deformable%203DGS%20or%204DGS%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07917v1&entry.124074799=Read"},
{"title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians", "author": "Zeyu Xiao and Zhenyi Wu and Mingyang Sun and Qipeng Yan and Yufan Guo and Zhuoer Liang and Lihua Zhang", "abstract": "  3D Gaussian Splatting has achieved remarkable success in reconstructing both\nstatic and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\nprimitives, interactions between objects suffer from inaccurate 3D\nsegmentation, imprecise deformation among different materials, and severe\nrendering artifacts. To address these challenges, we introduce PIG:\nPhysically-Based Multi-Material Interaction with 3D Gaussians, a novel approach\nthat combines 3D object segmentation with the simulation of interacting objects\nin high precision. Firstly, our method facilitates fast and accurate mapping\nfrom 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.\nSecondly, we assign unique physical properties to correspondingly segmented\nobjects within the scene for multi-material coupled interactions. Finally, we\nhave successfully embedded constraint scales into deformation gradients,\nspecifically clamping the scaling and rotation properties of the Gaussian\nprimitives to eliminate artifacts and achieve geometric fidelity and visual\nconsistency. Experimental results demonstrate that our method not only\noutperforms the state-of-the-art (SOTA) in terms of visual quality, but also\nopens up new directions and pipelines for the field of physically realistic\nscene generation.\n", "link": "http://arxiv.org/abs/2506.07657v1", "date": "2025-06-09", "relevancy": 3.3698, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6781}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6734}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIG%3A%20Physically-based%20Multi-Material%20Interaction%20with%203D%20Gaussians&body=Title%3A%20PIG%3A%20Physically-based%20Multi-Material%20Interaction%20with%203D%20Gaussians%0AAuthor%3A%20Zeyu%20Xiao%20and%20Zhenyi%20Wu%20and%20Mingyang%20Sun%20and%20Qipeng%20Yan%20and%20Yufan%20Guo%20and%20Zhuoer%20Liang%20and%20Lihua%20Zhang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20achieved%20remarkable%20success%20in%20reconstructing%20both%0Astatic%20and%20dynamic%203D%20scenes.%20However%2C%20in%20a%20scene%20represented%20by%203D%20Gaussian%0Aprimitives%2C%20interactions%20between%20objects%20suffer%20from%20inaccurate%203D%0Asegmentation%2C%20imprecise%20deformation%20among%20different%20materials%2C%20and%20severe%0Arendering%20artifacts.%20To%20address%20these%20challenges%2C%20we%20introduce%20PIG%3A%0APhysically-Based%20Multi-Material%20Interaction%20with%203D%20Gaussians%2C%20a%20novel%20approach%0Athat%20combines%203D%20object%20segmentation%20with%20the%20simulation%20of%20interacting%20objects%0Ain%20high%20precision.%20Firstly%2C%20our%20method%20facilitates%20fast%20and%20accurate%20mapping%0Afrom%202D%20pixels%20to%203D%20Gaussians%2C%20enabling%20precise%203D%20object-level%20segmentation.%0ASecondly%2C%20we%20assign%20unique%20physical%20properties%20to%20correspondingly%20segmented%0Aobjects%20within%20the%20scene%20for%20multi-material%20coupled%20interactions.%20Finally%2C%20we%0Ahave%20successfully%20embedded%20constraint%20scales%20into%20deformation%20gradients%2C%0Aspecifically%20clamping%20the%20scaling%20and%20rotation%20properties%20of%20the%20Gaussian%0Aprimitives%20to%20eliminate%20artifacts%20and%20achieve%20geometric%20fidelity%20and%20visual%0Aconsistency.%20Experimental%20results%20demonstrate%20that%20our%20method%20not%20only%0Aoutperforms%20the%20state-of-the-art%20%28SOTA%29%20in%20terms%20of%20visual%20quality%2C%20but%20also%0Aopens%20up%20new%20directions%20and%20pipelines%20for%20the%20field%20of%20physically%20realistic%0Ascene%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIG%253A%2520Physically-based%2520Multi-Material%2520Interaction%2520with%25203D%2520Gaussians%26entry.906535625%3DZeyu%2520Xiao%2520and%2520Zhenyi%2520Wu%2520and%2520Mingyang%2520Sun%2520and%2520Qipeng%2520Yan%2520and%2520Yufan%2520Guo%2520and%2520Zhuoer%2520Liang%2520and%2520Lihua%2520Zhang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520achieved%2520remarkable%2520success%2520in%2520reconstructing%2520both%250Astatic%2520and%2520dynamic%25203D%2520scenes.%2520However%252C%2520in%2520a%2520scene%2520represented%2520by%25203D%2520Gaussian%250Aprimitives%252C%2520interactions%2520between%2520objects%2520suffer%2520from%2520inaccurate%25203D%250Asegmentation%252C%2520imprecise%2520deformation%2520among%2520different%2520materials%252C%2520and%2520severe%250Arendering%2520artifacts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520PIG%253A%250APhysically-Based%2520Multi-Material%2520Interaction%2520with%25203D%2520Gaussians%252C%2520a%2520novel%2520approach%250Athat%2520combines%25203D%2520object%2520segmentation%2520with%2520the%2520simulation%2520of%2520interacting%2520objects%250Ain%2520high%2520precision.%2520Firstly%252C%2520our%2520method%2520facilitates%2520fast%2520and%2520accurate%2520mapping%250Afrom%25202D%2520pixels%2520to%25203D%2520Gaussians%252C%2520enabling%2520precise%25203D%2520object-level%2520segmentation.%250ASecondly%252C%2520we%2520assign%2520unique%2520physical%2520properties%2520to%2520correspondingly%2520segmented%250Aobjects%2520within%2520the%2520scene%2520for%2520multi-material%2520coupled%2520interactions.%2520Finally%252C%2520we%250Ahave%2520successfully%2520embedded%2520constraint%2520scales%2520into%2520deformation%2520gradients%252C%250Aspecifically%2520clamping%2520the%2520scaling%2520and%2520rotation%2520properties%2520of%2520the%2520Gaussian%250Aprimitives%2520to%2520eliminate%2520artifacts%2520and%2520achieve%2520geometric%2520fidelity%2520and%2520visual%250Aconsistency.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520not%2520only%250Aoutperforms%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520in%2520terms%2520of%2520visual%2520quality%252C%2520but%2520also%250Aopens%2520up%2520new%2520directions%2520and%2520pipelines%2520for%2520the%2520field%2520of%2520physically%2520realistic%250Ascene%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIG%3A%20Physically-based%20Multi-Material%20Interaction%20with%203D%20Gaussians&entry.906535625=Zeyu%20Xiao%20and%20Zhenyi%20Wu%20and%20Mingyang%20Sun%20and%20Qipeng%20Yan%20and%20Yufan%20Guo%20and%20Zhuoer%20Liang%20and%20Lihua%20Zhang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20achieved%20remarkable%20success%20in%20reconstructing%20both%0Astatic%20and%20dynamic%203D%20scenes.%20However%2C%20in%20a%20scene%20represented%20by%203D%20Gaussian%0Aprimitives%2C%20interactions%20between%20objects%20suffer%20from%20inaccurate%203D%0Asegmentation%2C%20imprecise%20deformation%20among%20different%20materials%2C%20and%20severe%0Arendering%20artifacts.%20To%20address%20these%20challenges%2C%20we%20introduce%20PIG%3A%0APhysically-Based%20Multi-Material%20Interaction%20with%203D%20Gaussians%2C%20a%20novel%20approach%0Athat%20combines%203D%20object%20segmentation%20with%20the%20simulation%20of%20interacting%20objects%0Ain%20high%20precision.%20Firstly%2C%20our%20method%20facilitates%20fast%20and%20accurate%20mapping%0Afrom%202D%20pixels%20to%203D%20Gaussians%2C%20enabling%20precise%203D%20object-level%20segmentation.%0ASecondly%2C%20we%20assign%20unique%20physical%20properties%20to%20correspondingly%20segmented%0Aobjects%20within%20the%20scene%20for%20multi-material%20coupled%20interactions.%20Finally%2C%20we%0Ahave%20successfully%20embedded%20constraint%20scales%20into%20deformation%20gradients%2C%0Aspecifically%20clamping%20the%20scaling%20and%20rotation%20properties%20of%20the%20Gaussian%0Aprimitives%20to%20eliminate%20artifacts%20and%20achieve%20geometric%20fidelity%20and%20visual%0Aconsistency.%20Experimental%20results%20demonstrate%20that%20our%20method%20not%20only%0Aoutperforms%20the%20state-of-the-art%20%28SOTA%29%20in%20terms%20of%20visual%20quality%2C%20but%20also%0Aopens%20up%20new%20directions%20and%20pipelines%20for%20the%20field%20of%20physically%20realistic%0Ascene%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07657v1&entry.124074799=Read"},
{"title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian\n  Splatting", "author": "Jens Piekenbrinck and Christian Schmidt and Alexander Hermans and Narunas Vaskevicius and Timm Linder and Bastian Leibe", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nneural scene reconstruction, offering high-quality novel view synthesis while\nmaintaining computational efficiency. In this paper, we extend the capabilities\nof 3DGS beyond pure scene representation by introducing an approach for\nopen-vocabulary 3D instance segmentation without requiring manual labeling,\ntermed OpenSplat3D. Our method leverages feature-splatting techniques to\nassociate semantic information with individual Gaussians, enabling fine-grained\nscene understanding. We incorporate Segment Anything Model instance masks with\na contrastive loss formulation as guidance for the instance features to achieve\naccurate instance-level segmentation. Furthermore, we utilize language\nembeddings of a vision-language model, allowing for flexible, text-driven\ninstance identification. This combination enables our system to identify and\nsegment arbitrary objects in 3D scenes based on natural language descriptions.\nWe show results on LERF-mask and LERF-OVS as well as the full ScanNet++\nvalidation set, demonstrating the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2506.07697v1", "date": "2025-06-09", "relevancy": 3.3684, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7162}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6984}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSplat3D%3A%20Open-Vocabulary%203D%20Instance%20Segmentation%20using%20Gaussian%0A%20%20Splatting&body=Title%3A%20OpenSplat3D%3A%20Open-Vocabulary%203D%20Instance%20Segmentation%20using%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jens%20Piekenbrinck%20and%20Christian%20Schmidt%20and%20Alexander%20Hermans%20and%20Narunas%20Vaskevicius%20and%20Timm%20Linder%20and%20Bastian%20Leibe%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20representation%20for%0Aneural%20scene%20reconstruction%2C%20offering%20high-quality%20novel%20view%20synthesis%20while%0Amaintaining%20computational%20efficiency.%20In%20this%20paper%2C%20we%20extend%20the%20capabilities%0Aof%203DGS%20beyond%20pure%20scene%20representation%20by%20introducing%20an%20approach%20for%0Aopen-vocabulary%203D%20instance%20segmentation%20without%20requiring%20manual%20labeling%2C%0Atermed%20OpenSplat3D.%20Our%20method%20leverages%20feature-splatting%20techniques%20to%0Aassociate%20semantic%20information%20with%20individual%20Gaussians%2C%20enabling%20fine-grained%0Ascene%20understanding.%20We%20incorporate%20Segment%20Anything%20Model%20instance%20masks%20with%0Aa%20contrastive%20loss%20formulation%20as%20guidance%20for%20the%20instance%20features%20to%20achieve%0Aaccurate%20instance-level%20segmentation.%20Furthermore%2C%20we%20utilize%20language%0Aembeddings%20of%20a%20vision-language%20model%2C%20allowing%20for%20flexible%2C%20text-driven%0Ainstance%20identification.%20This%20combination%20enables%20our%20system%20to%20identify%20and%0Asegment%20arbitrary%20objects%20in%203D%20scenes%20based%20on%20natural%20language%20descriptions.%0AWe%20show%20results%20on%20LERF-mask%20and%20LERF-OVS%20as%20well%20as%20the%20full%20ScanNet%2B%2B%0Avalidation%20set%2C%20demonstrating%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSplat3D%253A%2520Open-Vocabulary%25203D%2520Instance%2520Segmentation%2520using%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJens%2520Piekenbrinck%2520and%2520Christian%2520Schmidt%2520and%2520Alexander%2520Hermans%2520and%2520Narunas%2520Vaskevicius%2520and%2520Timm%2520Linder%2520and%2520Bastian%2520Leibe%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520representation%2520for%250Aneural%2520scene%2520reconstruction%252C%2520offering%2520high-quality%2520novel%2520view%2520synthesis%2520while%250Amaintaining%2520computational%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520capabilities%250Aof%25203DGS%2520beyond%2520pure%2520scene%2520representation%2520by%2520introducing%2520an%2520approach%2520for%250Aopen-vocabulary%25203D%2520instance%2520segmentation%2520without%2520requiring%2520manual%2520labeling%252C%250Atermed%2520OpenSplat3D.%2520Our%2520method%2520leverages%2520feature-splatting%2520techniques%2520to%250Aassociate%2520semantic%2520information%2520with%2520individual%2520Gaussians%252C%2520enabling%2520fine-grained%250Ascene%2520understanding.%2520We%2520incorporate%2520Segment%2520Anything%2520Model%2520instance%2520masks%2520with%250Aa%2520contrastive%2520loss%2520formulation%2520as%2520guidance%2520for%2520the%2520instance%2520features%2520to%2520achieve%250Aaccurate%2520instance-level%2520segmentation.%2520Furthermore%252C%2520we%2520utilize%2520language%250Aembeddings%2520of%2520a%2520vision-language%2520model%252C%2520allowing%2520for%2520flexible%252C%2520text-driven%250Ainstance%2520identification.%2520This%2520combination%2520enables%2520our%2520system%2520to%2520identify%2520and%250Asegment%2520arbitrary%2520objects%2520in%25203D%2520scenes%2520based%2520on%2520natural%2520language%2520descriptions.%250AWe%2520show%2520results%2520on%2520LERF-mask%2520and%2520LERF-OVS%2520as%2520well%2520as%2520the%2520full%2520ScanNet%252B%252B%250Avalidation%2520set%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSplat3D%3A%20Open-Vocabulary%203D%20Instance%20Segmentation%20using%20Gaussian%0A%20%20Splatting&entry.906535625=Jens%20Piekenbrinck%20and%20Christian%20Schmidt%20and%20Alexander%20Hermans%20and%20Narunas%20Vaskevicius%20and%20Timm%20Linder%20and%20Bastian%20Leibe&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20representation%20for%0Aneural%20scene%20reconstruction%2C%20offering%20high-quality%20novel%20view%20synthesis%20while%0Amaintaining%20computational%20efficiency.%20In%20this%20paper%2C%20we%20extend%20the%20capabilities%0Aof%203DGS%20beyond%20pure%20scene%20representation%20by%20introducing%20an%20approach%20for%0Aopen-vocabulary%203D%20instance%20segmentation%20without%20requiring%20manual%20labeling%2C%0Atermed%20OpenSplat3D.%20Our%20method%20leverages%20feature-splatting%20techniques%20to%0Aassociate%20semantic%20information%20with%20individual%20Gaussians%2C%20enabling%20fine-grained%0Ascene%20understanding.%20We%20incorporate%20Segment%20Anything%20Model%20instance%20masks%20with%0Aa%20contrastive%20loss%20formulation%20as%20guidance%20for%20the%20instance%20features%20to%20achieve%0Aaccurate%20instance-level%20segmentation.%20Furthermore%2C%20we%20utilize%20language%0Aembeddings%20of%20a%20vision-language%20model%2C%20allowing%20for%20flexible%2C%20text-driven%0Ainstance%20identification.%20This%20combination%20enables%20our%20system%20to%20identify%20and%0Asegment%20arbitrary%20objects%20in%203D%20scenes%20based%20on%20natural%20language%20descriptions.%0AWe%20show%20results%20on%20LERF-mask%20and%20LERF-OVS%20as%20well%20as%20the%20full%20ScanNet%2B%2B%0Avalidation%20set%2C%20demonstrating%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07697v1&entry.124074799=Read"},
{"title": "Aligning Text, Images, and 3D Structure Token-by-Token", "author": "Aadarsh Sahoo and Vansh Tibrewal and Georgia Gkioxari", "abstract": "  Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/\n", "link": "http://arxiv.org/abs/2506.08002v1", "date": "2025-06-09", "relevancy": 3.346, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Text%2C%20Images%2C%20and%203D%20Structure%20Token-by-Token&body=Title%3A%20Aligning%20Text%2C%20Images%2C%20and%203D%20Structure%20Token-by-Token%0AAuthor%3A%20Aadarsh%20Sahoo%20and%20Vansh%20Tibrewal%20and%20Georgia%20Gkioxari%0AAbstract%3A%20%20%20Creating%20machines%20capable%20of%20understanding%20the%20world%20in%203D%20is%20essential%20in%0Aassisting%20designers%20that%20build%20and%20edit%203D%20environments%20and%20robots%20navigating%0Aand%20interacting%20within%20a%20three-dimensional%20space.%20Inspired%20by%20advances%20in%0Alanguage%20and%20image%20modeling%2C%20we%20investigate%20the%20potential%20of%20autoregressive%0Amodels%20for%20a%20new%20modality%3A%20structured%203D%20scenes.%20To%20this%20end%2C%20we%20propose%20a%0Aunified%20LLM%20framework%20that%20aligns%20language%2C%20images%2C%20and%203D%20scenes%20and%20provide%20a%0Adetailed%20%27%27cookbook%27%27%20outlining%20critical%20design%20choices%20for%20achieving%20optimal%0Atraining%20and%20performance%20addressing%20key%20questions%20related%20to%20data%0Arepresentation%2C%20modality-specific%20objectives%2C%20and%20more.%20We%20evaluate%20performance%0Aacross%20four%20core%203D%20tasks%20--%20rendering%2C%20recognition%2C%20instruction-following%2C%20and%0Aquestion-answering%20--%20and%20four%203D%20datasets%2C%20synthetic%20and%20real-world.%20We%20extend%0Aour%20approach%20to%20reconstruct%20complex%203D%20object%20shapes%20by%20enriching%20our%203D%0Amodality%20with%20quantized%20shape%20encodings%2C%20and%20show%20our%20model%27s%20effectiveness%20on%0Areal-world%203D%20object%20recognition%20tasks.%20Project%20webpage%3A%0Ahttps%3A//glab-caltech.github.io/kyvo/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Text%252C%2520Images%252C%2520and%25203D%2520Structure%2520Token-by-Token%26entry.906535625%3DAadarsh%2520Sahoo%2520and%2520Vansh%2520Tibrewal%2520and%2520Georgia%2520Gkioxari%26entry.1292438233%3D%2520%2520Creating%2520machines%2520capable%2520of%2520understanding%2520the%2520world%2520in%25203D%2520is%2520essential%2520in%250Aassisting%2520designers%2520that%2520build%2520and%2520edit%25203D%2520environments%2520and%2520robots%2520navigating%250Aand%2520interacting%2520within%2520a%2520three-dimensional%2520space.%2520Inspired%2520by%2520advances%2520in%250Alanguage%2520and%2520image%2520modeling%252C%2520we%2520investigate%2520the%2520potential%2520of%2520autoregressive%250Amodels%2520for%2520a%2520new%2520modality%253A%2520structured%25203D%2520scenes.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Aunified%2520LLM%2520framework%2520that%2520aligns%2520language%252C%2520images%252C%2520and%25203D%2520scenes%2520and%2520provide%2520a%250Adetailed%2520%2527%2527cookbook%2527%2527%2520outlining%2520critical%2520design%2520choices%2520for%2520achieving%2520optimal%250Atraining%2520and%2520performance%2520addressing%2520key%2520questions%2520related%2520to%2520data%250Arepresentation%252C%2520modality-specific%2520objectives%252C%2520and%2520more.%2520We%2520evaluate%2520performance%250Aacross%2520four%2520core%25203D%2520tasks%2520--%2520rendering%252C%2520recognition%252C%2520instruction-following%252C%2520and%250Aquestion-answering%2520--%2520and%2520four%25203D%2520datasets%252C%2520synthetic%2520and%2520real-world.%2520We%2520extend%250Aour%2520approach%2520to%2520reconstruct%2520complex%25203D%2520object%2520shapes%2520by%2520enriching%2520our%25203D%250Amodality%2520with%2520quantized%2520shape%2520encodings%252C%2520and%2520show%2520our%2520model%2527s%2520effectiveness%2520on%250Areal-world%25203D%2520object%2520recognition%2520tasks.%2520Project%2520webpage%253A%250Ahttps%253A//glab-caltech.github.io/kyvo/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Text%2C%20Images%2C%20and%203D%20Structure%20Token-by-Token&entry.906535625=Aadarsh%20Sahoo%20and%20Vansh%20Tibrewal%20and%20Georgia%20Gkioxari&entry.1292438233=%20%20Creating%20machines%20capable%20of%20understanding%20the%20world%20in%203D%20is%20essential%20in%0Aassisting%20designers%20that%20build%20and%20edit%203D%20environments%20and%20robots%20navigating%0Aand%20interacting%20within%20a%20three-dimensional%20space.%20Inspired%20by%20advances%20in%0Alanguage%20and%20image%20modeling%2C%20we%20investigate%20the%20potential%20of%20autoregressive%0Amodels%20for%20a%20new%20modality%3A%20structured%203D%20scenes.%20To%20this%20end%2C%20we%20propose%20a%0Aunified%20LLM%20framework%20that%20aligns%20language%2C%20images%2C%20and%203D%20scenes%20and%20provide%20a%0Adetailed%20%27%27cookbook%27%27%20outlining%20critical%20design%20choices%20for%20achieving%20optimal%0Atraining%20and%20performance%20addressing%20key%20questions%20related%20to%20data%0Arepresentation%2C%20modality-specific%20objectives%2C%20and%20more.%20We%20evaluate%20performance%0Aacross%20four%20core%203D%20tasks%20--%20rendering%2C%20recognition%2C%20instruction-following%2C%20and%0Aquestion-answering%20--%20and%20four%203D%20datasets%2C%20synthetic%20and%20real-world.%20We%20extend%0Aour%20approach%20to%20reconstruct%20complex%203D%20object%20shapes%20by%20enriching%20our%203D%0Amodality%20with%20quantized%20shape%20encodings%2C%20and%20show%20our%20model%27s%20effectiveness%20on%0Areal-world%203D%20object%20recognition%20tasks.%20Project%20webpage%3A%0Ahttps%3A//glab-caltech.github.io/kyvo/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08002v1&entry.124074799=Read"},
{"title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for\n  High-Fidelity Super-Resolution", "author": "Shuja Khalid and Mohamed Ibrahim and Yang Liu", "abstract": "  We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.\n", "link": "http://arxiv.org/abs/2506.07897v1", "date": "2025-06-09", "relevancy": 3.3389, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6844}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6787}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianVAE%3A%20Adaptive%20Learning%20Dynamics%20of%203D%20Gaussians%20for%0A%20%20High-Fidelity%20Super-Resolution&body=Title%3A%20GaussianVAE%3A%20Adaptive%20Learning%20Dynamics%20of%203D%20Gaussians%20for%0A%20%20High-Fidelity%20Super-Resolution%0AAuthor%3A%20Shuja%20Khalid%20and%20Mohamed%20Ibrahim%20and%20Yang%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20enhancing%20the%20resolution%20and%20geometric%0Afidelity%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20beyond%20native%20training%20resolution.%0ACurrent%203DGS%20methods%20are%20fundamentally%20limited%20by%20their%20input%20resolution%2C%0Aproducing%20reconstructions%20that%20cannot%20extrapolate%20finer%20details%20than%20are%0Apresent%20in%20the%20training%20views.%20Our%20work%20breaks%20this%20limitation%20through%20a%0Alightweight%20generative%20model%20that%20predicts%20and%20refines%20additional%203D%20Gaussians%0Awhere%20needed%20most.%20The%20key%20innovation%20is%20our%20Hessian-assisted%20sampling%0Astrategy%2C%20which%20intelligently%20identifies%20regions%20that%20are%20likely%20to%20benefit%0Afrom%20densification%2C%20ensuring%20computational%20efficiency.%20Unlike%20computationally%0Aintensive%20GANs%20or%20diffusion%20approaches%2C%20our%20method%20operates%20in%20real-time%0A%280.015s%20per%20inference%20on%20a%20single%20consumer-grade%20GPU%29%2C%20making%20it%20practical%20for%0Ainteractive%20applications.%20Comprehensive%20experiments%20demonstrate%20significant%0Aimprovements%20in%20both%20geometric%20accuracy%20and%20rendering%20quality%20compared%20to%0Astate-of-the-art%20methods%2C%20establishing%20a%20new%20paradigm%20for%20resolution-free%203D%0Ascene%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianVAE%253A%2520Adaptive%2520Learning%2520Dynamics%2520of%25203D%2520Gaussians%2520for%250A%2520%2520High-Fidelity%2520Super-Resolution%26entry.906535625%3DShuja%2520Khalid%2520and%2520Mohamed%2520Ibrahim%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520enhancing%2520the%2520resolution%2520and%2520geometric%250Afidelity%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520beyond%2520native%2520training%2520resolution.%250ACurrent%25203DGS%2520methods%2520are%2520fundamentally%2520limited%2520by%2520their%2520input%2520resolution%252C%250Aproducing%2520reconstructions%2520that%2520cannot%2520extrapolate%2520finer%2520details%2520than%2520are%250Apresent%2520in%2520the%2520training%2520views.%2520Our%2520work%2520breaks%2520this%2520limitation%2520through%2520a%250Alightweight%2520generative%2520model%2520that%2520predicts%2520and%2520refines%2520additional%25203D%2520Gaussians%250Awhere%2520needed%2520most.%2520The%2520key%2520innovation%2520is%2520our%2520Hessian-assisted%2520sampling%250Astrategy%252C%2520which%2520intelligently%2520identifies%2520regions%2520that%2520are%2520likely%2520to%2520benefit%250Afrom%2520densification%252C%2520ensuring%2520computational%2520efficiency.%2520Unlike%2520computationally%250Aintensive%2520GANs%2520or%2520diffusion%2520approaches%252C%2520our%2520method%2520operates%2520in%2520real-time%250A%25280.015s%2520per%2520inference%2520on%2520a%2520single%2520consumer-grade%2520GPU%2529%252C%2520making%2520it%2520practical%2520for%250Ainteractive%2520applications.%2520Comprehensive%2520experiments%2520demonstrate%2520significant%250Aimprovements%2520in%2520both%2520geometric%2520accuracy%2520and%2520rendering%2520quality%2520compared%2520to%250Astate-of-the-art%2520methods%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520resolution-free%25203D%250Ascene%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianVAE%3A%20Adaptive%20Learning%20Dynamics%20of%203D%20Gaussians%20for%0A%20%20High-Fidelity%20Super-Resolution&entry.906535625=Shuja%20Khalid%20and%20Mohamed%20Ibrahim%20and%20Yang%20Liu&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20enhancing%20the%20resolution%20and%20geometric%0Afidelity%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20beyond%20native%20training%20resolution.%0ACurrent%203DGS%20methods%20are%20fundamentally%20limited%20by%20their%20input%20resolution%2C%0Aproducing%20reconstructions%20that%20cannot%20extrapolate%20finer%20details%20than%20are%0Apresent%20in%20the%20training%20views.%20Our%20work%20breaks%20this%20limitation%20through%20a%0Alightweight%20generative%20model%20that%20predicts%20and%20refines%20additional%203D%20Gaussians%0Awhere%20needed%20most.%20The%20key%20innovation%20is%20our%20Hessian-assisted%20sampling%0Astrategy%2C%20which%20intelligently%20identifies%20regions%20that%20are%20likely%20to%20benefit%0Afrom%20densification%2C%20ensuring%20computational%20efficiency.%20Unlike%20computationally%0Aintensive%20GANs%20or%20diffusion%20approaches%2C%20our%20method%20operates%20in%20real-time%0A%280.015s%20per%20inference%20on%20a%20single%20consumer-grade%20GPU%29%2C%20making%20it%20practical%20for%0Ainteractive%20applications.%20Comprehensive%20experiments%20demonstrate%20significant%0Aimprovements%20in%20both%20geometric%20accuracy%20and%20rendering%20quality%20compared%20to%0Astate-of-the-art%20methods%2C%20establishing%20a%20new%20paradigm%20for%20resolution-free%203D%0Ascene%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07897v1&entry.124074799=Read"},
{"title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline\n  Sparse Views", "author": "Xiaohan Lu and Jiaye Fu and Jiaqi Zhang and Zetian Song and Chuanmin Jia and Siwei Ma", "abstract": "  Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising\nresults for novel view synthesis (NVS) from sparse input views, particularly\nunder narrow-baseline conditions. However, its performance significantly\ndegrades in wide-baseline scenarios due to limited texture details and\ngeometric inconsistencies across views. To address these challenges, in this\npaper, we propose ProSplat, a two-stage feed-forward framework designed for\nhigh-fidelity rendering under wide-baseline conditions. The first stage\ninvolves generating 3D Gaussian primitives via a 3DGS generator. In the second\nstage, rendered views from these primitives are enhanced through an improvement\nmodel. Specifically, this improvement model is based on a one-step diffusion\nmodel, further optimized by our proposed Maximum Overlap Reference view\nInjection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI\nsupplements missing texture and color by strategically selecting a reference\nview with maximum viewpoint overlap, while DWEA enforces geometric consistency\nusing epipolar constraints. Additionally, we introduce a divide-and-conquer\ntraining strategy that aligns data distributions between the two stages through\njoint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K\ndatasets under wide-baseline settings. Experimental results demonstrate that\nProSplat achieves an average improvement of 1 dB in PSNR compared to recent\nSOTA methods.\n", "link": "http://arxiv.org/abs/2506.07670v1", "date": "2025-06-09", "relevancy": 3.3359, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7192}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.665}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProSplat%3A%20Improved%20Feed-Forward%203D%20Gaussian%20Splatting%20for%20Wide-Baseline%0A%20%20Sparse%20Views&body=Title%3A%20ProSplat%3A%20Improved%20Feed-Forward%203D%20Gaussian%20Splatting%20for%20Wide-Baseline%0A%20%20Sparse%20Views%0AAuthor%3A%20Xiaohan%20Lu%20and%20Jiaye%20Fu%20and%20Jiaqi%20Zhang%20and%20Zetian%20Song%20and%20Chuanmin%20Jia%20and%20Siwei%20Ma%0AAbstract%3A%20%20%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20demonstrated%20promising%0Aresults%20for%20novel%20view%20synthesis%20%28NVS%29%20from%20sparse%20input%20views%2C%20particularly%0Aunder%20narrow-baseline%20conditions.%20However%2C%20its%20performance%20significantly%0Adegrades%20in%20wide-baseline%20scenarios%20due%20to%20limited%20texture%20details%20and%0Ageometric%20inconsistencies%20across%20views.%20To%20address%20these%20challenges%2C%20in%20this%0Apaper%2C%20we%20propose%20ProSplat%2C%20a%20two-stage%20feed-forward%20framework%20designed%20for%0Ahigh-fidelity%20rendering%20under%20wide-baseline%20conditions.%20The%20first%20stage%0Ainvolves%20generating%203D%20Gaussian%20primitives%20via%20a%203DGS%20generator.%20In%20the%20second%0Astage%2C%20rendered%20views%20from%20these%20primitives%20are%20enhanced%20through%20an%20improvement%0Amodel.%20Specifically%2C%20this%20improvement%20model%20is%20based%20on%20a%20one-step%20diffusion%0Amodel%2C%20further%20optimized%20by%20our%20proposed%20Maximum%20Overlap%20Reference%20view%0AInjection%20%28MORI%29%20and%20Distance-Weighted%20Epipolar%20Attention%20%28DWEA%29.%20MORI%0Asupplements%20missing%20texture%20and%20color%20by%20strategically%20selecting%20a%20reference%0Aview%20with%20maximum%20viewpoint%20overlap%2C%20while%20DWEA%20enforces%20geometric%20consistency%0Ausing%20epipolar%20constraints.%20Additionally%2C%20we%20introduce%20a%20divide-and-conquer%0Atraining%20strategy%20that%20aligns%20data%20distributions%20between%20the%20two%20stages%20through%0Ajoint%20optimization.%20We%20evaluate%20ProSplat%20on%20the%20RealEstate10K%20and%20DL3DV-10K%0Adatasets%20under%20wide-baseline%20settings.%20Experimental%20results%20demonstrate%20that%0AProSplat%20achieves%20an%20average%20improvement%20of%201%20dB%20in%20PSNR%20compared%20to%20recent%0ASOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProSplat%253A%2520Improved%2520Feed-Forward%25203D%2520Gaussian%2520Splatting%2520for%2520Wide-Baseline%250A%2520%2520Sparse%2520Views%26entry.906535625%3DXiaohan%2520Lu%2520and%2520Jiaye%2520Fu%2520and%2520Jiaqi%2520Zhang%2520and%2520Zetian%2520Song%2520and%2520Chuanmin%2520Jia%2520and%2520Siwei%2520Ma%26entry.1292438233%3D%2520%2520Feed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520demonstrated%2520promising%250Aresults%2520for%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520from%2520sparse%2520input%2520views%252C%2520particularly%250Aunder%2520narrow-baseline%2520conditions.%2520However%252C%2520its%2520performance%2520significantly%250Adegrades%2520in%2520wide-baseline%2520scenarios%2520due%2520to%2520limited%2520texture%2520details%2520and%250Ageometric%2520inconsistencies%2520across%2520views.%2520To%2520address%2520these%2520challenges%252C%2520in%2520this%250Apaper%252C%2520we%2520propose%2520ProSplat%252C%2520a%2520two-stage%2520feed-forward%2520framework%2520designed%2520for%250Ahigh-fidelity%2520rendering%2520under%2520wide-baseline%2520conditions.%2520The%2520first%2520stage%250Ainvolves%2520generating%25203D%2520Gaussian%2520primitives%2520via%2520a%25203DGS%2520generator.%2520In%2520the%2520second%250Astage%252C%2520rendered%2520views%2520from%2520these%2520primitives%2520are%2520enhanced%2520through%2520an%2520improvement%250Amodel.%2520Specifically%252C%2520this%2520improvement%2520model%2520is%2520based%2520on%2520a%2520one-step%2520diffusion%250Amodel%252C%2520further%2520optimized%2520by%2520our%2520proposed%2520Maximum%2520Overlap%2520Reference%2520view%250AInjection%2520%2528MORI%2529%2520and%2520Distance-Weighted%2520Epipolar%2520Attention%2520%2528DWEA%2529.%2520MORI%250Asupplements%2520missing%2520texture%2520and%2520color%2520by%2520strategically%2520selecting%2520a%2520reference%250Aview%2520with%2520maximum%2520viewpoint%2520overlap%252C%2520while%2520DWEA%2520enforces%2520geometric%2520consistency%250Ausing%2520epipolar%2520constraints.%2520Additionally%252C%2520we%2520introduce%2520a%2520divide-and-conquer%250Atraining%2520strategy%2520that%2520aligns%2520data%2520distributions%2520between%2520the%2520two%2520stages%2520through%250Ajoint%2520optimization.%2520We%2520evaluate%2520ProSplat%2520on%2520the%2520RealEstate10K%2520and%2520DL3DV-10K%250Adatasets%2520under%2520wide-baseline%2520settings.%2520Experimental%2520results%2520demonstrate%2520that%250AProSplat%2520achieves%2520an%2520average%2520improvement%2520of%25201%2520dB%2520in%2520PSNR%2520compared%2520to%2520recent%250ASOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProSplat%3A%20Improved%20Feed-Forward%203D%20Gaussian%20Splatting%20for%20Wide-Baseline%0A%20%20Sparse%20Views&entry.906535625=Xiaohan%20Lu%20and%20Jiaye%20Fu%20and%20Jiaqi%20Zhang%20and%20Zetian%20Song%20and%20Chuanmin%20Jia%20and%20Siwei%20Ma&entry.1292438233=%20%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20demonstrated%20promising%0Aresults%20for%20novel%20view%20synthesis%20%28NVS%29%20from%20sparse%20input%20views%2C%20particularly%0Aunder%20narrow-baseline%20conditions.%20However%2C%20its%20performance%20significantly%0Adegrades%20in%20wide-baseline%20scenarios%20due%20to%20limited%20texture%20details%20and%0Ageometric%20inconsistencies%20across%20views.%20To%20address%20these%20challenges%2C%20in%20this%0Apaper%2C%20we%20propose%20ProSplat%2C%20a%20two-stage%20feed-forward%20framework%20designed%20for%0Ahigh-fidelity%20rendering%20under%20wide-baseline%20conditions.%20The%20first%20stage%0Ainvolves%20generating%203D%20Gaussian%20primitives%20via%20a%203DGS%20generator.%20In%20the%20second%0Astage%2C%20rendered%20views%20from%20these%20primitives%20are%20enhanced%20through%20an%20improvement%0Amodel.%20Specifically%2C%20this%20improvement%20model%20is%20based%20on%20a%20one-step%20diffusion%0Amodel%2C%20further%20optimized%20by%20our%20proposed%20Maximum%20Overlap%20Reference%20view%0AInjection%20%28MORI%29%20and%20Distance-Weighted%20Epipolar%20Attention%20%28DWEA%29.%20MORI%0Asupplements%20missing%20texture%20and%20color%20by%20strategically%20selecting%20a%20reference%0Aview%20with%20maximum%20viewpoint%20overlap%2C%20while%20DWEA%20enforces%20geometric%20consistency%0Ausing%20epipolar%20constraints.%20Additionally%2C%20we%20introduce%20a%20divide-and-conquer%0Atraining%20strategy%20that%20aligns%20data%20distributions%20between%20the%20two%20stages%20through%0Ajoint%20optimization.%20We%20evaluate%20ProSplat%20on%20the%20RealEstate10K%20and%20DL3DV-10K%0Adatasets%20under%20wide-baseline%20settings.%20Experimental%20results%20demonstrate%20that%0AProSplat%20achieves%20an%20average%20improvement%20of%201%20dB%20in%20PSNR%20compared%20to%20recent%0ASOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07670v1&entry.124074799=Read"},
{"title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving\n  Simulation", "author": "William Ljungbergh and Bernardo Taveira and Wenzhao Zheng and Adam Tonderski and Chensheng Peng and Fredrik Kahl and Christoffer Petersson and Michael Felsberg and Kurt Keutzer and Masayoshi Tomizuka and Wei Zhan", "abstract": "  Validating autonomous driving (AD) systems requires diverse and\nsafety-critical testing, making photorealistic virtual environments essential.\nTraditional simulation platforms, while controllable, are resource-intensive to\nscale and often suffer from a domain gap with real-world data. In contrast,\nneural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a\nscalable solution for creating photorealistic digital twins of real-world\ndriving scenes. However, they struggle with dynamic object manipulation and\nreusability as their per-scene optimization-based methodology tends to result\nin incomplete object models with integrated illumination effects. This paper\nintroduces R3D2, a lightweight, one-step diffusion model designed to overcome\nthese limitations and enable realistic insertion of complete 3D assets into\nexisting scenes by generating plausible rendering effects-such as shadows and\nconsistent lighting-in real time. This is achieved by training R3D2 on a novel\ndataset: 3DGS object assets are generated from in-the-wild AD data using an\nimage-conditioned 3D generative model, and then synthetically placed into\nneural rendering-based virtual environments, allowing R3D2 to learn realistic\nintegration. Quantitative and qualitative evaluations demonstrate that R3D2\nsignificantly enhances the realism of inserted assets, enabling use-cases like\ntext-to-3D asset insertion and cross-scene/dataset object transfer, allowing\nfor true scalability in AD validation. To promote further research in scalable\nand realistic AD simulation, we will release our dataset and code, see\nhttps://research.zenseact.com/publications/R3D2/.\n", "link": "http://arxiv.org/abs/2506.07826v1", "date": "2025-06-09", "relevancy": 3.2759, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6668}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6668}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R3D2%3A%20Realistic%203D%20Asset%20Insertion%20via%20Diffusion%20for%20Autonomous%20Driving%0A%20%20Simulation&body=Title%3A%20R3D2%3A%20Realistic%203D%20Asset%20Insertion%20via%20Diffusion%20for%20Autonomous%20Driving%0A%20%20Simulation%0AAuthor%3A%20William%20Ljungbergh%20and%20Bernardo%20Taveira%20and%20Wenzhao%20Zheng%20and%20Adam%20Tonderski%20and%20Chensheng%20Peng%20and%20Fredrik%20Kahl%20and%20Christoffer%20Petersson%20and%20Michael%20Felsberg%20and%20Kurt%20Keutzer%20and%20Masayoshi%20Tomizuka%20and%20Wei%20Zhan%0AAbstract%3A%20%20%20Validating%20autonomous%20driving%20%28AD%29%20systems%20requires%20diverse%20and%0Asafety-critical%20testing%2C%20making%20photorealistic%20virtual%20environments%20essential.%0ATraditional%20simulation%20platforms%2C%20while%20controllable%2C%20are%20resource-intensive%20to%0Ascale%20and%20often%20suffer%20from%20a%20domain%20gap%20with%20real-world%20data.%20In%20contrast%2C%0Aneural%20reconstruction%20methods%20like%203D%20Gaussian%20Splatting%20%283DGS%29%20offer%20a%0Ascalable%20solution%20for%20creating%20photorealistic%20digital%20twins%20of%20real-world%0Adriving%20scenes.%20However%2C%20they%20struggle%20with%20dynamic%20object%20manipulation%20and%0Areusability%20as%20their%20per-scene%20optimization-based%20methodology%20tends%20to%20result%0Ain%20incomplete%20object%20models%20with%20integrated%20illumination%20effects.%20This%20paper%0Aintroduces%20R3D2%2C%20a%20lightweight%2C%20one-step%20diffusion%20model%20designed%20to%20overcome%0Athese%20limitations%20and%20enable%20realistic%20insertion%20of%20complete%203D%20assets%20into%0Aexisting%20scenes%20by%20generating%20plausible%20rendering%20effects-such%20as%20shadows%20and%0Aconsistent%20lighting-in%20real%20time.%20This%20is%20achieved%20by%20training%20R3D2%20on%20a%20novel%0Adataset%3A%203DGS%20object%20assets%20are%20generated%20from%20in-the-wild%20AD%20data%20using%20an%0Aimage-conditioned%203D%20generative%20model%2C%20and%20then%20synthetically%20placed%20into%0Aneural%20rendering-based%20virtual%20environments%2C%20allowing%20R3D2%20to%20learn%20realistic%0Aintegration.%20Quantitative%20and%20qualitative%20evaluations%20demonstrate%20that%20R3D2%0Asignificantly%20enhances%20the%20realism%20of%20inserted%20assets%2C%20enabling%20use-cases%20like%0Atext-to-3D%20asset%20insertion%20and%20cross-scene/dataset%20object%20transfer%2C%20allowing%0Afor%20true%20scalability%20in%20AD%20validation.%20To%20promote%20further%20research%20in%20scalable%0Aand%20realistic%20AD%20simulation%2C%20we%20will%20release%20our%20dataset%20and%20code%2C%20see%0Ahttps%3A//research.zenseact.com/publications/R3D2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR3D2%253A%2520Realistic%25203D%2520Asset%2520Insertion%2520via%2520Diffusion%2520for%2520Autonomous%2520Driving%250A%2520%2520Simulation%26entry.906535625%3DWilliam%2520Ljungbergh%2520and%2520Bernardo%2520Taveira%2520and%2520Wenzhao%2520Zheng%2520and%2520Adam%2520Tonderski%2520and%2520Chensheng%2520Peng%2520and%2520Fredrik%2520Kahl%2520and%2520Christoffer%2520Petersson%2520and%2520Michael%2520Felsberg%2520and%2520Kurt%2520Keutzer%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Wei%2520Zhan%26entry.1292438233%3D%2520%2520Validating%2520autonomous%2520driving%2520%2528AD%2529%2520systems%2520requires%2520diverse%2520and%250Asafety-critical%2520testing%252C%2520making%2520photorealistic%2520virtual%2520environments%2520essential.%250ATraditional%2520simulation%2520platforms%252C%2520while%2520controllable%252C%2520are%2520resource-intensive%2520to%250Ascale%2520and%2520often%2520suffer%2520from%2520a%2520domain%2520gap%2520with%2520real-world%2520data.%2520In%2520contrast%252C%250Aneural%2520reconstruction%2520methods%2520like%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520offer%2520a%250Ascalable%2520solution%2520for%2520creating%2520photorealistic%2520digital%2520twins%2520of%2520real-world%250Adriving%2520scenes.%2520However%252C%2520they%2520struggle%2520with%2520dynamic%2520object%2520manipulation%2520and%250Areusability%2520as%2520their%2520per-scene%2520optimization-based%2520methodology%2520tends%2520to%2520result%250Ain%2520incomplete%2520object%2520models%2520with%2520integrated%2520illumination%2520effects.%2520This%2520paper%250Aintroduces%2520R3D2%252C%2520a%2520lightweight%252C%2520one-step%2520diffusion%2520model%2520designed%2520to%2520overcome%250Athese%2520limitations%2520and%2520enable%2520realistic%2520insertion%2520of%2520complete%25203D%2520assets%2520into%250Aexisting%2520scenes%2520by%2520generating%2520plausible%2520rendering%2520effects-such%2520as%2520shadows%2520and%250Aconsistent%2520lighting-in%2520real%2520time.%2520This%2520is%2520achieved%2520by%2520training%2520R3D2%2520on%2520a%2520novel%250Adataset%253A%25203DGS%2520object%2520assets%2520are%2520generated%2520from%2520in-the-wild%2520AD%2520data%2520using%2520an%250Aimage-conditioned%25203D%2520generative%2520model%252C%2520and%2520then%2520synthetically%2520placed%2520into%250Aneural%2520rendering-based%2520virtual%2520environments%252C%2520allowing%2520R3D2%2520to%2520learn%2520realistic%250Aintegration.%2520Quantitative%2520and%2520qualitative%2520evaluations%2520demonstrate%2520that%2520R3D2%250Asignificantly%2520enhances%2520the%2520realism%2520of%2520inserted%2520assets%252C%2520enabling%2520use-cases%2520like%250Atext-to-3D%2520asset%2520insertion%2520and%2520cross-scene/dataset%2520object%2520transfer%252C%2520allowing%250Afor%2520true%2520scalability%2520in%2520AD%2520validation.%2520To%2520promote%2520further%2520research%2520in%2520scalable%250Aand%2520realistic%2520AD%2520simulation%252C%2520we%2520will%2520release%2520our%2520dataset%2520and%2520code%252C%2520see%250Ahttps%253A//research.zenseact.com/publications/R3D2/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R3D2%3A%20Realistic%203D%20Asset%20Insertion%20via%20Diffusion%20for%20Autonomous%20Driving%0A%20%20Simulation&entry.906535625=William%20Ljungbergh%20and%20Bernardo%20Taveira%20and%20Wenzhao%20Zheng%20and%20Adam%20Tonderski%20and%20Chensheng%20Peng%20and%20Fredrik%20Kahl%20and%20Christoffer%20Petersson%20and%20Michael%20Felsberg%20and%20Kurt%20Keutzer%20and%20Masayoshi%20Tomizuka%20and%20Wei%20Zhan&entry.1292438233=%20%20Validating%20autonomous%20driving%20%28AD%29%20systems%20requires%20diverse%20and%0Asafety-critical%20testing%2C%20making%20photorealistic%20virtual%20environments%20essential.%0ATraditional%20simulation%20platforms%2C%20while%20controllable%2C%20are%20resource-intensive%20to%0Ascale%20and%20often%20suffer%20from%20a%20domain%20gap%20with%20real-world%20data.%20In%20contrast%2C%0Aneural%20reconstruction%20methods%20like%203D%20Gaussian%20Splatting%20%283DGS%29%20offer%20a%0Ascalable%20solution%20for%20creating%20photorealistic%20digital%20twins%20of%20real-world%0Adriving%20scenes.%20However%2C%20they%20struggle%20with%20dynamic%20object%20manipulation%20and%0Areusability%20as%20their%20per-scene%20optimization-based%20methodology%20tends%20to%20result%0Ain%20incomplete%20object%20models%20with%20integrated%20illumination%20effects.%20This%20paper%0Aintroduces%20R3D2%2C%20a%20lightweight%2C%20one-step%20diffusion%20model%20designed%20to%20overcome%0Athese%20limitations%20and%20enable%20realistic%20insertion%20of%20complete%203D%20assets%20into%0Aexisting%20scenes%20by%20generating%20plausible%20rendering%20effects-such%20as%20shadows%20and%0Aconsistent%20lighting-in%20real%20time.%20This%20is%20achieved%20by%20training%20R3D2%20on%20a%20novel%0Adataset%3A%203DGS%20object%20assets%20are%20generated%20from%20in-the-wild%20AD%20data%20using%20an%0Aimage-conditioned%203D%20generative%20model%2C%20and%20then%20synthetically%20placed%20into%0Aneural%20rendering-based%20virtual%20environments%2C%20allowing%20R3D2%20to%20learn%20realistic%0Aintegration.%20Quantitative%20and%20qualitative%20evaluations%20demonstrate%20that%20R3D2%0Asignificantly%20enhances%20the%20realism%20of%20inserted%20assets%2C%20enabling%20use-cases%20like%0Atext-to-3D%20asset%20insertion%20and%20cross-scene/dataset%20object%20transfer%2C%20allowing%0Afor%20true%20scalability%20in%20AD%20validation.%20To%20promote%20further%20research%20in%20scalable%0Aand%20realistic%20AD%20simulation%2C%20we%20will%20release%20our%20dataset%20and%20code%2C%20see%0Ahttps%3A//research.zenseact.com/publications/R3D2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07826v1&entry.124074799=Read"},
{"title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models", "author": "Wenyan Cong and Yiqing Liang and Yancheng Zhang and Ziyi Yang and Yan Wang and Boris Ivanovic and Marco Pavone and Chen Chen and Zhangyang Wang and Zhiwen Fan", "abstract": "  Spatial intelligence, encompassing 3D reconstruction, perception, and\nreasoning, is fundamental to applications such as robotics, aerial imaging, and\nextended reality. A key enabler is the real-time, accurate estimation of core\n3D attributes (camera parameters, point clouds, depth maps, and 3D point\ntracks) from unstructured or streaming imagery. Inspired by the success of\nlarge foundation models in language and 2D vision, a new class of end-to-end 3D\ngeometric foundation models (GFMs) has emerged, directly predicting dense 3D\nrepresentations in a single feed-forward pass, eliminating the need for slow or\nunavailable precomputed camera parameters. Since late 2023, the field has\nexploded with diverse variants, but systematic evaluation is lacking. In this\nwork, we present the first comprehensive benchmark for 3D GFMs, covering five\ncore tasks: sparse-view depth estimation, video depth estimation, 3D\nreconstruction, multi-view pose estimation, novel view synthesis, and spanning\nboth standard and challenging out-of-distribution datasets. Our standardized\ntoolkit automates dataset handling, evaluation protocols, and metric\ncomputation to ensure fair, reproducible comparisons. We evaluate 16\nstate-of-the-art GFMs, revealing their strengths and limitations across tasks\nand domains, and derive key insights to guide future model scaling and\noptimization. All code, evaluation scripts, and processed data will be publicly\nreleased to accelerate research in 3D spatial intelligence.\n", "link": "http://arxiv.org/abs/2506.01933v2", "date": "2025-06-09", "relevancy": 3.1546, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6382}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E3D-Bench%3A%20A%20Benchmark%20for%20End-to-End%203D%20Geometric%20Foundation%20Models&body=Title%3A%20E3D-Bench%3A%20A%20Benchmark%20for%20End-to-End%203D%20Geometric%20Foundation%20Models%0AAuthor%3A%20Wenyan%20Cong%20and%20Yiqing%20Liang%20and%20Yancheng%20Zhang%20and%20Ziyi%20Yang%20and%20Yan%20Wang%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Chen%20Chen%20and%20Zhangyang%20Wang%20and%20Zhiwen%20Fan%0AAbstract%3A%20%20%20Spatial%20intelligence%2C%20encompassing%203D%20reconstruction%2C%20perception%2C%20and%0Areasoning%2C%20is%20fundamental%20to%20applications%20such%20as%20robotics%2C%20aerial%20imaging%2C%20and%0Aextended%20reality.%20A%20key%20enabler%20is%20the%20real-time%2C%20accurate%20estimation%20of%20core%0A3D%20attributes%20%28camera%20parameters%2C%20point%20clouds%2C%20depth%20maps%2C%20and%203D%20point%0Atracks%29%20from%20unstructured%20or%20streaming%20imagery.%20Inspired%20by%20the%20success%20of%0Alarge%20foundation%20models%20in%20language%20and%202D%20vision%2C%20a%20new%20class%20of%20end-to-end%203D%0Ageometric%20foundation%20models%20%28GFMs%29%20has%20emerged%2C%20directly%20predicting%20dense%203D%0Arepresentations%20in%20a%20single%20feed-forward%20pass%2C%20eliminating%20the%20need%20for%20slow%20or%0Aunavailable%20precomputed%20camera%20parameters.%20Since%20late%202023%2C%20the%20field%20has%0Aexploded%20with%20diverse%20variants%2C%20but%20systematic%20evaluation%20is%20lacking.%20In%20this%0Awork%2C%20we%20present%20the%20first%20comprehensive%20benchmark%20for%203D%20GFMs%2C%20covering%20five%0Acore%20tasks%3A%20sparse-view%20depth%20estimation%2C%20video%20depth%20estimation%2C%203D%0Areconstruction%2C%20multi-view%20pose%20estimation%2C%20novel%20view%20synthesis%2C%20and%20spanning%0Aboth%20standard%20and%20challenging%20out-of-distribution%20datasets.%20Our%20standardized%0Atoolkit%20automates%20dataset%20handling%2C%20evaluation%20protocols%2C%20and%20metric%0Acomputation%20to%20ensure%20fair%2C%20reproducible%20comparisons.%20We%20evaluate%2016%0Astate-of-the-art%20GFMs%2C%20revealing%20their%20strengths%20and%20limitations%20across%20tasks%0Aand%20domains%2C%20and%20derive%20key%20insights%20to%20guide%20future%20model%20scaling%20and%0Aoptimization.%20All%20code%2C%20evaluation%20scripts%2C%20and%20processed%20data%20will%20be%20publicly%0Areleased%20to%20accelerate%20research%20in%203D%20spatial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01933v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE3D-Bench%253A%2520A%2520Benchmark%2520for%2520End-to-End%25203D%2520Geometric%2520Foundation%2520Models%26entry.906535625%3DWenyan%2520Cong%2520and%2520Yiqing%2520Liang%2520and%2520Yancheng%2520Zhang%2520and%2520Ziyi%2520Yang%2520and%2520Yan%2520Wang%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Chen%2520Chen%2520and%2520Zhangyang%2520Wang%2520and%2520Zhiwen%2520Fan%26entry.1292438233%3D%2520%2520Spatial%2520intelligence%252C%2520encompassing%25203D%2520reconstruction%252C%2520perception%252C%2520and%250Areasoning%252C%2520is%2520fundamental%2520to%2520applications%2520such%2520as%2520robotics%252C%2520aerial%2520imaging%252C%2520and%250Aextended%2520reality.%2520A%2520key%2520enabler%2520is%2520the%2520real-time%252C%2520accurate%2520estimation%2520of%2520core%250A3D%2520attributes%2520%2528camera%2520parameters%252C%2520point%2520clouds%252C%2520depth%2520maps%252C%2520and%25203D%2520point%250Atracks%2529%2520from%2520unstructured%2520or%2520streaming%2520imagery.%2520Inspired%2520by%2520the%2520success%2520of%250Alarge%2520foundation%2520models%2520in%2520language%2520and%25202D%2520vision%252C%2520a%2520new%2520class%2520of%2520end-to-end%25203D%250Ageometric%2520foundation%2520models%2520%2528GFMs%2529%2520has%2520emerged%252C%2520directly%2520predicting%2520dense%25203D%250Arepresentations%2520in%2520a%2520single%2520feed-forward%2520pass%252C%2520eliminating%2520the%2520need%2520for%2520slow%2520or%250Aunavailable%2520precomputed%2520camera%2520parameters.%2520Since%2520late%25202023%252C%2520the%2520field%2520has%250Aexploded%2520with%2520diverse%2520variants%252C%2520but%2520systematic%2520evaluation%2520is%2520lacking.%2520In%2520this%250Awork%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520benchmark%2520for%25203D%2520GFMs%252C%2520covering%2520five%250Acore%2520tasks%253A%2520sparse-view%2520depth%2520estimation%252C%2520video%2520depth%2520estimation%252C%25203D%250Areconstruction%252C%2520multi-view%2520pose%2520estimation%252C%2520novel%2520view%2520synthesis%252C%2520and%2520spanning%250Aboth%2520standard%2520and%2520challenging%2520out-of-distribution%2520datasets.%2520Our%2520standardized%250Atoolkit%2520automates%2520dataset%2520handling%252C%2520evaluation%2520protocols%252C%2520and%2520metric%250Acomputation%2520to%2520ensure%2520fair%252C%2520reproducible%2520comparisons.%2520We%2520evaluate%252016%250Astate-of-the-art%2520GFMs%252C%2520revealing%2520their%2520strengths%2520and%2520limitations%2520across%2520tasks%250Aand%2520domains%252C%2520and%2520derive%2520key%2520insights%2520to%2520guide%2520future%2520model%2520scaling%2520and%250Aoptimization.%2520All%2520code%252C%2520evaluation%2520scripts%252C%2520and%2520processed%2520data%2520will%2520be%2520publicly%250Areleased%2520to%2520accelerate%2520research%2520in%25203D%2520spatial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01933v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3D-Bench%3A%20A%20Benchmark%20for%20End-to-End%203D%20Geometric%20Foundation%20Models&entry.906535625=Wenyan%20Cong%20and%20Yiqing%20Liang%20and%20Yancheng%20Zhang%20and%20Ziyi%20Yang%20and%20Yan%20Wang%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Chen%20Chen%20and%20Zhangyang%20Wang%20and%20Zhiwen%20Fan&entry.1292438233=%20%20Spatial%20intelligence%2C%20encompassing%203D%20reconstruction%2C%20perception%2C%20and%0Areasoning%2C%20is%20fundamental%20to%20applications%20such%20as%20robotics%2C%20aerial%20imaging%2C%20and%0Aextended%20reality.%20A%20key%20enabler%20is%20the%20real-time%2C%20accurate%20estimation%20of%20core%0A3D%20attributes%20%28camera%20parameters%2C%20point%20clouds%2C%20depth%20maps%2C%20and%203D%20point%0Atracks%29%20from%20unstructured%20or%20streaming%20imagery.%20Inspired%20by%20the%20success%20of%0Alarge%20foundation%20models%20in%20language%20and%202D%20vision%2C%20a%20new%20class%20of%20end-to-end%203D%0Ageometric%20foundation%20models%20%28GFMs%29%20has%20emerged%2C%20directly%20predicting%20dense%203D%0Arepresentations%20in%20a%20single%20feed-forward%20pass%2C%20eliminating%20the%20need%20for%20slow%20or%0Aunavailable%20precomputed%20camera%20parameters.%20Since%20late%202023%2C%20the%20field%20has%0Aexploded%20with%20diverse%20variants%2C%20but%20systematic%20evaluation%20is%20lacking.%20In%20this%0Awork%2C%20we%20present%20the%20first%20comprehensive%20benchmark%20for%203D%20GFMs%2C%20covering%20five%0Acore%20tasks%3A%20sparse-view%20depth%20estimation%2C%20video%20depth%20estimation%2C%203D%0Areconstruction%2C%20multi-view%20pose%20estimation%2C%20novel%20view%20synthesis%2C%20and%20spanning%0Aboth%20standard%20and%20challenging%20out-of-distribution%20datasets.%20Our%20standardized%0Atoolkit%20automates%20dataset%20handling%2C%20evaluation%20protocols%2C%20and%20metric%0Acomputation%20to%20ensure%20fair%2C%20reproducible%20comparisons.%20We%20evaluate%2016%0Astate-of-the-art%20GFMs%2C%20revealing%20their%20strengths%20and%20limitations%20across%20tasks%0Aand%20domains%2C%20and%20derive%20key%20insights%20to%20guide%20future%20model%20scaling%20and%0Aoptimization.%20All%20code%2C%20evaluation%20scripts%2C%20and%20processed%20data%20will%20be%20publicly%0Areleased%20to%20accelerate%20research%20in%203D%20spatial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01933v2&entry.124074799=Read"},
{"title": "EmoVOCA: Speech-Driven Emotional 3D Talking Heads", "author": "Federico Nocentini and Claudio Ferrari and Stefano Berretti", "abstract": "  The domain of 3D talking head generation has witnessed significant progress\nin recent years. A notable challenge in this field consists in blending\nspeech-related motions with expression dynamics, which is primarily caused by\nthe lack of comprehensive 3D datasets that combine diversity in spoken\nsentences with a variety of facial expressions. Whereas literature works\nattempted to exploit 2D video data and parametric 3D models as a workaround,\nthese still show limitations when jointly modeling the two motions. In this\nwork, we address this problem from a different perspective, and propose an\ninnovative data-driven technique that we used for creating a synthetic dataset,\ncalled EmoVOCA, obtained by combining a collection of inexpressive 3D talking\nheads and a set of 3D expressive sequences. To demonstrate the advantages of\nthis approach, and the quality of the dataset, we then designed and trained an\nemotional 3D talking head generator that accepts a 3D face, an audio file, an\nemotion label, and an intensity value as inputs, and learns to animate the\naudio-synchronized lip movements with expressive traits of the face.\nComprehensive experiments, both quantitative and qualitative, using our data\nand generator evidence superior ability in synthesizing convincing animations,\nwhen compared with the best performing methods in the literature. Our code and\npre-trained model will be made available.\n", "link": "http://arxiv.org/abs/2403.12886v3", "date": "2025-06-09", "relevancy": 3.1312, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6314}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6314}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads&body=Title%3A%20EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads%0AAuthor%3A%20Federico%20Nocentini%20and%20Claudio%20Ferrari%20and%20Stefano%20Berretti%0AAbstract%3A%20%20%20The%20domain%20of%203D%20talking%20head%20generation%20has%20witnessed%20significant%20progress%0Ain%20recent%20years.%20A%20notable%20challenge%20in%20this%20field%20consists%20in%20blending%0Aspeech-related%20motions%20with%20expression%20dynamics%2C%20which%20is%20primarily%20caused%20by%0Athe%20lack%20of%20comprehensive%203D%20datasets%20that%20combine%20diversity%20in%20spoken%0Asentences%20with%20a%20variety%20of%20facial%20expressions.%20Whereas%20literature%20works%0Aattempted%20to%20exploit%202D%20video%20data%20and%20parametric%203D%20models%20as%20a%20workaround%2C%0Athese%20still%20show%20limitations%20when%20jointly%20modeling%20the%20two%20motions.%20In%20this%0Awork%2C%20we%20address%20this%20problem%20from%20a%20different%20perspective%2C%20and%20propose%20an%0Ainnovative%20data-driven%20technique%20that%20we%20used%20for%20creating%20a%20synthetic%20dataset%2C%0Acalled%20EmoVOCA%2C%20obtained%20by%20combining%20a%20collection%20of%20inexpressive%203D%20talking%0Aheads%20and%20a%20set%20of%203D%20expressive%20sequences.%20To%20demonstrate%20the%20advantages%20of%0Athis%20approach%2C%20and%20the%20quality%20of%20the%20dataset%2C%20we%20then%20designed%20and%20trained%20an%0Aemotional%203D%20talking%20head%20generator%20that%20accepts%20a%203D%20face%2C%20an%20audio%20file%2C%20an%0Aemotion%20label%2C%20and%20an%20intensity%20value%20as%20inputs%2C%20and%20learns%20to%20animate%20the%0Aaudio-synchronized%20lip%20movements%20with%20expressive%20traits%20of%20the%20face.%0AComprehensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20using%20our%20data%0Aand%20generator%20evidence%20superior%20ability%20in%20synthesizing%20convincing%20animations%2C%0Awhen%20compared%20with%20the%20best%20performing%20methods%20in%20the%20literature.%20Our%20code%20and%0Apre-trained%20model%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12886v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoVOCA%253A%2520Speech-Driven%2520Emotional%25203D%2520Talking%2520Heads%26entry.906535625%3DFederico%2520Nocentini%2520and%2520Claudio%2520Ferrari%2520and%2520Stefano%2520Berretti%26entry.1292438233%3D%2520%2520The%2520domain%2520of%25203D%2520talking%2520head%2520generation%2520has%2520witnessed%2520significant%2520progress%250Ain%2520recent%2520years.%2520A%2520notable%2520challenge%2520in%2520this%2520field%2520consists%2520in%2520blending%250Aspeech-related%2520motions%2520with%2520expression%2520dynamics%252C%2520which%2520is%2520primarily%2520caused%2520by%250Athe%2520lack%2520of%2520comprehensive%25203D%2520datasets%2520that%2520combine%2520diversity%2520in%2520spoken%250Asentences%2520with%2520a%2520variety%2520of%2520facial%2520expressions.%2520Whereas%2520literature%2520works%250Aattempted%2520to%2520exploit%25202D%2520video%2520data%2520and%2520parametric%25203D%2520models%2520as%2520a%2520workaround%252C%250Athese%2520still%2520show%2520limitations%2520when%2520jointly%2520modeling%2520the%2520two%2520motions.%2520In%2520this%250Awork%252C%2520we%2520address%2520this%2520problem%2520from%2520a%2520different%2520perspective%252C%2520and%2520propose%2520an%250Ainnovative%2520data-driven%2520technique%2520that%2520we%2520used%2520for%2520creating%2520a%2520synthetic%2520dataset%252C%250Acalled%2520EmoVOCA%252C%2520obtained%2520by%2520combining%2520a%2520collection%2520of%2520inexpressive%25203D%2520talking%250Aheads%2520and%2520a%2520set%2520of%25203D%2520expressive%2520sequences.%2520To%2520demonstrate%2520the%2520advantages%2520of%250Athis%2520approach%252C%2520and%2520the%2520quality%2520of%2520the%2520dataset%252C%2520we%2520then%2520designed%2520and%2520trained%2520an%250Aemotional%25203D%2520talking%2520head%2520generator%2520that%2520accepts%2520a%25203D%2520face%252C%2520an%2520audio%2520file%252C%2520an%250Aemotion%2520label%252C%2520and%2520an%2520intensity%2520value%2520as%2520inputs%252C%2520and%2520learns%2520to%2520animate%2520the%250Aaudio-synchronized%2520lip%2520movements%2520with%2520expressive%2520traits%2520of%2520the%2520face.%250AComprehensive%2520experiments%252C%2520both%2520quantitative%2520and%2520qualitative%252C%2520using%2520our%2520data%250Aand%2520generator%2520evidence%2520superior%2520ability%2520in%2520synthesizing%2520convincing%2520animations%252C%250Awhen%2520compared%2520with%2520the%2520best%2520performing%2520methods%2520in%2520the%2520literature.%2520Our%2520code%2520and%250Apre-trained%2520model%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12886v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads&entry.906535625=Federico%20Nocentini%20and%20Claudio%20Ferrari%20and%20Stefano%20Berretti&entry.1292438233=%20%20The%20domain%20of%203D%20talking%20head%20generation%20has%20witnessed%20significant%20progress%0Ain%20recent%20years.%20A%20notable%20challenge%20in%20this%20field%20consists%20in%20blending%0Aspeech-related%20motions%20with%20expression%20dynamics%2C%20which%20is%20primarily%20caused%20by%0Athe%20lack%20of%20comprehensive%203D%20datasets%20that%20combine%20diversity%20in%20spoken%0Asentences%20with%20a%20variety%20of%20facial%20expressions.%20Whereas%20literature%20works%0Aattempted%20to%20exploit%202D%20video%20data%20and%20parametric%203D%20models%20as%20a%20workaround%2C%0Athese%20still%20show%20limitations%20when%20jointly%20modeling%20the%20two%20motions.%20In%20this%0Awork%2C%20we%20address%20this%20problem%20from%20a%20different%20perspective%2C%20and%20propose%20an%0Ainnovative%20data-driven%20technique%20that%20we%20used%20for%20creating%20a%20synthetic%20dataset%2C%0Acalled%20EmoVOCA%2C%20obtained%20by%20combining%20a%20collection%20of%20inexpressive%203D%20talking%0Aheads%20and%20a%20set%20of%203D%20expressive%20sequences.%20To%20demonstrate%20the%20advantages%20of%0Athis%20approach%2C%20and%20the%20quality%20of%20the%20dataset%2C%20we%20then%20designed%20and%20trained%20an%0Aemotional%203D%20talking%20head%20generator%20that%20accepts%20a%203D%20face%2C%20an%20audio%20file%2C%20an%0Aemotion%20label%2C%20and%20an%20intensity%20value%20as%20inputs%2C%20and%20learns%20to%20animate%20the%0Aaudio-synchronized%20lip%20movements%20with%20expressive%20traits%20of%20the%20face.%0AComprehensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20using%20our%20data%0Aand%20generator%20evidence%20superior%20ability%20in%20synthesizing%20convincing%20animations%2C%0Awhen%20compared%20with%20the%20best%20performing%20methods%20in%20the%20literature.%20Our%20code%20and%0Apre-trained%20model%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12886v3&entry.124074799=Read"},
{"title": "From Thousands to Billions: 3D Visual Language Grounding via\n  Render-Supervised Distillation from 2D VLMs", "author": "Ang Cao and Sergio Arnaud and Oleksandr Maksymets and Jianing Yang and Ayush Jain and Sriram Yenamandra and Ada Martin and Vincent-Pierre Berges and Paul McVay and Ruslan Partsey and Aravind Rajeswaran and Franziska Meier and Justin Johnson and Jeong Joon Park and Alexander Sax", "abstract": "  3D vision-language grounding faces a fundamental data bottleneck: while 2D\nmodels train on billions of images, 3D models have access to only thousands of\nlabeled scenes--a six-order-of-magnitude gap that severely limits performance.\nWe introduce $\\textbf{LIFT-GS}$, a practical distillation technique that\novercomes this limitation by using differentiable rendering to bridge 3D and 2D\nsupervision. LIFT-GS predicts 3D Gaussian representations from point clouds and\nuses them to render predicted language-conditioned 3D masks into 2D views,\nenabling supervision from 2D foundation models (SAM, CLIP, LLaMA) without\nrequiring any 3D annotations. This render-supervised formulation enables\nend-to-end training of complete encoder-decoder architectures and is inherently\nmodel-agnostic. LIFT-GS achieves state-of-the-art results with $25.7\\%$ mAP on\nopen-vocabulary instance segmentation (vs. $20.2\\%$ prior SOTA) and consistent\n$10-30\\%$ improvements on referential grounding tasks. Remarkably, pretraining\neffectively multiplies fine-tuning datasets by 2X, demonstrating strong scaling\nproperties that suggest 3D VLG currently operates in a severely data-scarce\nregime. Project page: https://liftgs.github.io\n", "link": "http://arxiv.org/abs/2502.20389v2", "date": "2025-06-09", "relevancy": 3.1176, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Thousands%20to%20Billions%3A%203D%20Visual%20Language%20Grounding%20via%0A%20%20Render-Supervised%20Distillation%20from%202D%20VLMs&body=Title%3A%20From%20Thousands%20to%20Billions%3A%203D%20Visual%20Language%20Grounding%20via%0A%20%20Render-Supervised%20Distillation%20from%202D%20VLMs%0AAuthor%3A%20Ang%20Cao%20and%20Sergio%20Arnaud%20and%20Oleksandr%20Maksymets%20and%20Jianing%20Yang%20and%20Ayush%20Jain%20and%20Sriram%20Yenamandra%20and%20Ada%20Martin%20and%20Vincent-Pierre%20Berges%20and%20Paul%20McVay%20and%20Ruslan%20Partsey%20and%20Aravind%20Rajeswaran%20and%20Franziska%20Meier%20and%20Justin%20Johnson%20and%20Jeong%20Joon%20Park%20and%20Alexander%20Sax%0AAbstract%3A%20%20%203D%20vision-language%20grounding%20faces%20a%20fundamental%20data%20bottleneck%3A%20while%202D%0Amodels%20train%20on%20billions%20of%20images%2C%203D%20models%20have%20access%20to%20only%20thousands%20of%0Alabeled%20scenes--a%20six-order-of-magnitude%20gap%20that%20severely%20limits%20performance.%0AWe%20introduce%20%24%5Ctextbf%7BLIFT-GS%7D%24%2C%20a%20practical%20distillation%20technique%20that%0Aovercomes%20this%20limitation%20by%20using%20differentiable%20rendering%20to%20bridge%203D%20and%202D%0Asupervision.%20LIFT-GS%20predicts%203D%20Gaussian%20representations%20from%20point%20clouds%20and%0Auses%20them%20to%20render%20predicted%20language-conditioned%203D%20masks%20into%202D%20views%2C%0Aenabling%20supervision%20from%202D%20foundation%20models%20%28SAM%2C%20CLIP%2C%20LLaMA%29%20without%0Arequiring%20any%203D%20annotations.%20This%20render-supervised%20formulation%20enables%0Aend-to-end%20training%20of%20complete%20encoder-decoder%20architectures%20and%20is%20inherently%0Amodel-agnostic.%20LIFT-GS%20achieves%20state-of-the-art%20results%20with%20%2425.7%5C%25%24%20mAP%20on%0Aopen-vocabulary%20instance%20segmentation%20%28vs.%20%2420.2%5C%25%24%20prior%20SOTA%29%20and%20consistent%0A%2410-30%5C%25%24%20improvements%20on%20referential%20grounding%20tasks.%20Remarkably%2C%20pretraining%0Aeffectively%20multiplies%20fine-tuning%20datasets%20by%202X%2C%20demonstrating%20strong%20scaling%0Aproperties%20that%20suggest%203D%20VLG%20currently%20operates%20in%20a%20severely%20data-scarce%0Aregime.%20Project%20page%3A%20https%3A//liftgs.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Thousands%2520to%2520Billions%253A%25203D%2520Visual%2520Language%2520Grounding%2520via%250A%2520%2520Render-Supervised%2520Distillation%2520from%25202D%2520VLMs%26entry.906535625%3DAng%2520Cao%2520and%2520Sergio%2520Arnaud%2520and%2520Oleksandr%2520Maksymets%2520and%2520Jianing%2520Yang%2520and%2520Ayush%2520Jain%2520and%2520Sriram%2520Yenamandra%2520and%2520Ada%2520Martin%2520and%2520Vincent-Pierre%2520Berges%2520and%2520Paul%2520McVay%2520and%2520Ruslan%2520Partsey%2520and%2520Aravind%2520Rajeswaran%2520and%2520Franziska%2520Meier%2520and%2520Justin%2520Johnson%2520and%2520Jeong%2520Joon%2520Park%2520and%2520Alexander%2520Sax%26entry.1292438233%3D%2520%25203D%2520vision-language%2520grounding%2520faces%2520a%2520fundamental%2520data%2520bottleneck%253A%2520while%25202D%250Amodels%2520train%2520on%2520billions%2520of%2520images%252C%25203D%2520models%2520have%2520access%2520to%2520only%2520thousands%2520of%250Alabeled%2520scenes--a%2520six-order-of-magnitude%2520gap%2520that%2520severely%2520limits%2520performance.%250AWe%2520introduce%2520%2524%255Ctextbf%257BLIFT-GS%257D%2524%252C%2520a%2520practical%2520distillation%2520technique%2520that%250Aovercomes%2520this%2520limitation%2520by%2520using%2520differentiable%2520rendering%2520to%2520bridge%25203D%2520and%25202D%250Asupervision.%2520LIFT-GS%2520predicts%25203D%2520Gaussian%2520representations%2520from%2520point%2520clouds%2520and%250Auses%2520them%2520to%2520render%2520predicted%2520language-conditioned%25203D%2520masks%2520into%25202D%2520views%252C%250Aenabling%2520supervision%2520from%25202D%2520foundation%2520models%2520%2528SAM%252C%2520CLIP%252C%2520LLaMA%2529%2520without%250Arequiring%2520any%25203D%2520annotations.%2520This%2520render-supervised%2520formulation%2520enables%250Aend-to-end%2520training%2520of%2520complete%2520encoder-decoder%2520architectures%2520and%2520is%2520inherently%250Amodel-agnostic.%2520LIFT-GS%2520achieves%2520state-of-the-art%2520results%2520with%2520%252425.7%255C%2525%2524%2520mAP%2520on%250Aopen-vocabulary%2520instance%2520segmentation%2520%2528vs.%2520%252420.2%255C%2525%2524%2520prior%2520SOTA%2529%2520and%2520consistent%250A%252410-30%255C%2525%2524%2520improvements%2520on%2520referential%2520grounding%2520tasks.%2520Remarkably%252C%2520pretraining%250Aeffectively%2520multiplies%2520fine-tuning%2520datasets%2520by%25202X%252C%2520demonstrating%2520strong%2520scaling%250Aproperties%2520that%2520suggest%25203D%2520VLG%2520currently%2520operates%2520in%2520a%2520severely%2520data-scarce%250Aregime.%2520Project%2520page%253A%2520https%253A//liftgs.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Thousands%20to%20Billions%3A%203D%20Visual%20Language%20Grounding%20via%0A%20%20Render-Supervised%20Distillation%20from%202D%20VLMs&entry.906535625=Ang%20Cao%20and%20Sergio%20Arnaud%20and%20Oleksandr%20Maksymets%20and%20Jianing%20Yang%20and%20Ayush%20Jain%20and%20Sriram%20Yenamandra%20and%20Ada%20Martin%20and%20Vincent-Pierre%20Berges%20and%20Paul%20McVay%20and%20Ruslan%20Partsey%20and%20Aravind%20Rajeswaran%20and%20Franziska%20Meier%20and%20Justin%20Johnson%20and%20Jeong%20Joon%20Park%20and%20Alexander%20Sax&entry.1292438233=%20%203D%20vision-language%20grounding%20faces%20a%20fundamental%20data%20bottleneck%3A%20while%202D%0Amodels%20train%20on%20billions%20of%20images%2C%203D%20models%20have%20access%20to%20only%20thousands%20of%0Alabeled%20scenes--a%20six-order-of-magnitude%20gap%20that%20severely%20limits%20performance.%0AWe%20introduce%20%24%5Ctextbf%7BLIFT-GS%7D%24%2C%20a%20practical%20distillation%20technique%20that%0Aovercomes%20this%20limitation%20by%20using%20differentiable%20rendering%20to%20bridge%203D%20and%202D%0Asupervision.%20LIFT-GS%20predicts%203D%20Gaussian%20representations%20from%20point%20clouds%20and%0Auses%20them%20to%20render%20predicted%20language-conditioned%203D%20masks%20into%202D%20views%2C%0Aenabling%20supervision%20from%202D%20foundation%20models%20%28SAM%2C%20CLIP%2C%20LLaMA%29%20without%0Arequiring%20any%203D%20annotations.%20This%20render-supervised%20formulation%20enables%0Aend-to-end%20training%20of%20complete%20encoder-decoder%20architectures%20and%20is%20inherently%0Amodel-agnostic.%20LIFT-GS%20achieves%20state-of-the-art%20results%20with%20%2425.7%5C%25%24%20mAP%20on%0Aopen-vocabulary%20instance%20segmentation%20%28vs.%20%2420.2%5C%25%24%20prior%20SOTA%29%20and%20consistent%0A%2410-30%5C%25%24%20improvements%20on%20referential%20grounding%20tasks.%20Remarkably%2C%20pretraining%0Aeffectively%20multiplies%20fine-tuning%20datasets%20by%202X%2C%20demonstrating%20strong%20scaling%0Aproperties%20that%20suggest%203D%20VLG%20currently%20operates%20in%20a%20severely%20data-scarce%0Aregime.%20Project%20page%3A%20https%3A//liftgs.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20389v2&entry.124074799=Read"},
{"title": "Hidden in plain sight: VLMs overlook their visual representations", "author": "Stephanie Fu and Tyler Bonnen and Devin Guillory and Trevor Darrell", "abstract": "  Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.\n", "link": "http://arxiv.org/abs/2506.08008v1", "date": "2025-06-09", "relevancy": 3.1148, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hidden%20in%20plain%20sight%3A%20VLMs%20overlook%20their%20visual%20representations&body=Title%3A%20Hidden%20in%20plain%20sight%3A%20VLMs%20overlook%20their%20visual%20representations%0AAuthor%3A%20Stephanie%20Fu%20and%20Tyler%20Bonnen%20and%20Devin%20Guillory%20and%20Trevor%20Darrell%0AAbstract%3A%20%20%20Language%20provides%20a%20natural%20interface%20to%20specify%20and%20evaluate%20performance%20on%0Avisual%20tasks.%20To%20realize%20this%20possibility%2C%20vision%20language%20models%20%28VLMs%29%20must%0Asuccessfully%20integrate%20visual%20and%20linguistic%20information.%20Our%20work%20compares%0AVLMs%20to%20a%20direct%20readout%20of%20their%20visual%20encoders%20to%20understand%20their%20ability%0Ato%20integrate%20across%20these%20modalities.%20Across%20a%20series%20of%20vision-centric%0Abenchmarks%20%28e.g.%2C%20depth%20estimation%2C%20correspondence%29%2C%20we%20find%20that%20VLMs%20perform%0Asubstantially%20worse%20than%20their%20visual%20encoders%2C%20dropping%20to%20near-chance%0Aperformance.%20We%20investigate%20these%20results%20through%20a%20series%20of%20analyses%20across%0Athe%20entire%20VLM%3A%20namely%201%29%20the%20degradation%20of%20vision%20representations%2C%202%29%0Abrittleness%20to%20task%20prompt%2C%20and%203%29%20the%20language%20model%27s%20role%20in%20solving%20the%0Atask.%20We%20find%20that%20the%20bottleneck%20in%20performing%20these%20vision-centric%20tasks%20lies%0Ain%20this%20third%20category%3B%20VLMs%20are%20not%20effectively%20using%20visual%20information%0Aeasily%20accessible%20throughout%20the%20entire%20model%2C%20and%20they%20inherit%20the%20language%0Apriors%20present%20in%20the%20LLM.%20Our%20work%20helps%20diagnose%20the%20failure%20modes%20of%0Aopen-source%20VLMs%2C%20and%20presents%20a%20series%20of%20evaluations%20useful%20for%20future%0Ainvestigations%20into%20visual%20understanding%20within%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHidden%2520in%2520plain%2520sight%253A%2520VLMs%2520overlook%2520their%2520visual%2520representations%26entry.906535625%3DStephanie%2520Fu%2520and%2520Tyler%2520Bonnen%2520and%2520Devin%2520Guillory%2520and%2520Trevor%2520Darrell%26entry.1292438233%3D%2520%2520Language%2520provides%2520a%2520natural%2520interface%2520to%2520specify%2520and%2520evaluate%2520performance%2520on%250Avisual%2520tasks.%2520To%2520realize%2520this%2520possibility%252C%2520vision%2520language%2520models%2520%2528VLMs%2529%2520must%250Asuccessfully%2520integrate%2520visual%2520and%2520linguistic%2520information.%2520Our%2520work%2520compares%250AVLMs%2520to%2520a%2520direct%2520readout%2520of%2520their%2520visual%2520encoders%2520to%2520understand%2520their%2520ability%250Ato%2520integrate%2520across%2520these%2520modalities.%2520Across%2520a%2520series%2520of%2520vision-centric%250Abenchmarks%2520%2528e.g.%252C%2520depth%2520estimation%252C%2520correspondence%2529%252C%2520we%2520find%2520that%2520VLMs%2520perform%250Asubstantially%2520worse%2520than%2520their%2520visual%2520encoders%252C%2520dropping%2520to%2520near-chance%250Aperformance.%2520We%2520investigate%2520these%2520results%2520through%2520a%2520series%2520of%2520analyses%2520across%250Athe%2520entire%2520VLM%253A%2520namely%25201%2529%2520the%2520degradation%2520of%2520vision%2520representations%252C%25202%2529%250Abrittleness%2520to%2520task%2520prompt%252C%2520and%25203%2529%2520the%2520language%2520model%2527s%2520role%2520in%2520solving%2520the%250Atask.%2520We%2520find%2520that%2520the%2520bottleneck%2520in%2520performing%2520these%2520vision-centric%2520tasks%2520lies%250Ain%2520this%2520third%2520category%253B%2520VLMs%2520are%2520not%2520effectively%2520using%2520visual%2520information%250Aeasily%2520accessible%2520throughout%2520the%2520entire%2520model%252C%2520and%2520they%2520inherit%2520the%2520language%250Apriors%2520present%2520in%2520the%2520LLM.%2520Our%2520work%2520helps%2520diagnose%2520the%2520failure%2520modes%2520of%250Aopen-source%2520VLMs%252C%2520and%2520presents%2520a%2520series%2520of%2520evaluations%2520useful%2520for%2520future%250Ainvestigations%2520into%2520visual%2520understanding%2520within%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hidden%20in%20plain%20sight%3A%20VLMs%20overlook%20their%20visual%20representations&entry.906535625=Stephanie%20Fu%20and%20Tyler%20Bonnen%20and%20Devin%20Guillory%20and%20Trevor%20Darrell&entry.1292438233=%20%20Language%20provides%20a%20natural%20interface%20to%20specify%20and%20evaluate%20performance%20on%0Avisual%20tasks.%20To%20realize%20this%20possibility%2C%20vision%20language%20models%20%28VLMs%29%20must%0Asuccessfully%20integrate%20visual%20and%20linguistic%20information.%20Our%20work%20compares%0AVLMs%20to%20a%20direct%20readout%20of%20their%20visual%20encoders%20to%20understand%20their%20ability%0Ato%20integrate%20across%20these%20modalities.%20Across%20a%20series%20of%20vision-centric%0Abenchmarks%20%28e.g.%2C%20depth%20estimation%2C%20correspondence%29%2C%20we%20find%20that%20VLMs%20perform%0Asubstantially%20worse%20than%20their%20visual%20encoders%2C%20dropping%20to%20near-chance%0Aperformance.%20We%20investigate%20these%20results%20through%20a%20series%20of%20analyses%20across%0Athe%20entire%20VLM%3A%20namely%201%29%20the%20degradation%20of%20vision%20representations%2C%202%29%0Abrittleness%20to%20task%20prompt%2C%20and%203%29%20the%20language%20model%27s%20role%20in%20solving%20the%0Atask.%20We%20find%20that%20the%20bottleneck%20in%20performing%20these%20vision-centric%20tasks%20lies%0Ain%20this%20third%20category%3B%20VLMs%20are%20not%20effectively%20using%20visual%20information%0Aeasily%20accessible%20throughout%20the%20entire%20model%2C%20and%20they%20inherit%20the%20language%0Apriors%20present%20in%20the%20LLM.%20Our%20work%20helps%20diagnose%20the%20failure%20modes%20of%0Aopen-source%20VLMs%2C%20and%20presents%20a%20series%20of%20evaluations%20useful%20for%20future%0Ainvestigations%20into%20visual%20understanding%20within%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08008v1&entry.124074799=Read"},
{"title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data", "author": "Ben Moran and Mauro Comi and Arunkumar Byravan and Steven Bohez and Tom Erez and Zhibin Li and Leonard Hasenclever", "abstract": "  Creating accurate, physical simulations directly from real-world robot motion\nholds great value for safe, scalable, and affordable robot learning, yet\nremains exceptionally challenging. Real robot data suffers from occlusions,\nnoisy camera poses, dynamic scene elements, which hinder the creation of\ngeometrically accurate and photorealistic digital twins of unseen objects. We\nintroduce a novel real-to-sim framework tackling all these challenges at once.\nOur key insight is a hybrid scene representation merging the photorealistic\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\nphysics simulation within a single representation. We propose an end-to-end\noptimization pipeline that leverages differentiable rendering and\ndifferentiable physics within MuJoCo to jointly refine all scene components -\nfrom object geometry and appearance to robot poses and physical parameters -\ndirectly from raw and imprecise robot trajectories. This unified optimization\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\ngenerate photorealistic novel views, and perform annotation-free robot pose\ncalibration. We demonstrate the effectiveness of our approach both in\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\nmanipulator, enabling more practical and robust real-to-simulation pipelines.\n", "link": "http://arxiv.org/abs/2506.04120v2", "date": "2025-06-09", "relevancy": 3.1107, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.629}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6246}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splatting%20Physical%20Scenes%3A%20End-to-End%20Real-to-Sim%20from%20Imperfect%20Robot%0A%20%20Data&body=Title%3A%20Splatting%20Physical%20Scenes%3A%20End-to-End%20Real-to-Sim%20from%20Imperfect%20Robot%0A%20%20Data%0AAuthor%3A%20Ben%20Moran%20and%20Mauro%20Comi%20and%20Arunkumar%20Byravan%20and%20Steven%20Bohez%20and%20Tom%20Erez%20and%20Zhibin%20Li%20and%20Leonard%20Hasenclever%0AAbstract%3A%20%20%20Creating%20accurate%2C%20physical%20simulations%20directly%20from%20real-world%20robot%20motion%0Aholds%20great%20value%20for%20safe%2C%20scalable%2C%20and%20affordable%20robot%20learning%2C%20yet%0Aremains%20exceptionally%20challenging.%20Real%20robot%20data%20suffers%20from%20occlusions%2C%0Anoisy%20camera%20poses%2C%20dynamic%20scene%20elements%2C%20which%20hinder%20the%20creation%20of%0Ageometrically%20accurate%20and%20photorealistic%20digital%20twins%20of%20unseen%20objects.%20We%0Aintroduce%20a%20novel%20real-to-sim%20framework%20tackling%20all%20these%20challenges%20at%20once.%0AOur%20key%20insight%20is%20a%20hybrid%20scene%20representation%20merging%20the%20photorealistic%0Arendering%20of%203D%20Gaussian%20Splatting%20with%20explicit%20object%20meshes%20suitable%20for%0Aphysics%20simulation%20within%20a%20single%20representation.%20We%20propose%20an%20end-to-end%0Aoptimization%20pipeline%20that%20leverages%20differentiable%20rendering%20and%0Adifferentiable%20physics%20within%20MuJoCo%20to%20jointly%20refine%20all%20scene%20components%20-%0Afrom%20object%20geometry%20and%20appearance%20to%20robot%20poses%20and%20physical%20parameters%20-%0Adirectly%20from%20raw%20and%20imprecise%20robot%20trajectories.%20This%20unified%20optimization%0Aallows%20us%20to%20simultaneously%20achieve%20high-fidelity%20object%20mesh%20reconstruction%2C%0Agenerate%20photorealistic%20novel%20views%2C%20and%20perform%20annotation-free%20robot%20pose%0Acalibration.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20both%20in%0Asimulation%20and%20on%20challenging%20real-world%20sequences%20using%20an%20ALOHA%202%20bi-manual%0Amanipulator%2C%20enabling%20more%20practical%20and%20robust%20real-to-simulation%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04120v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatting%2520Physical%2520Scenes%253A%2520End-to-End%2520Real-to-Sim%2520from%2520Imperfect%2520Robot%250A%2520%2520Data%26entry.906535625%3DBen%2520Moran%2520and%2520Mauro%2520Comi%2520and%2520Arunkumar%2520Byravan%2520and%2520Steven%2520Bohez%2520and%2520Tom%2520Erez%2520and%2520Zhibin%2520Li%2520and%2520Leonard%2520Hasenclever%26entry.1292438233%3D%2520%2520Creating%2520accurate%252C%2520physical%2520simulations%2520directly%2520from%2520real-world%2520robot%2520motion%250Aholds%2520great%2520value%2520for%2520safe%252C%2520scalable%252C%2520and%2520affordable%2520robot%2520learning%252C%2520yet%250Aremains%2520exceptionally%2520challenging.%2520Real%2520robot%2520data%2520suffers%2520from%2520occlusions%252C%250Anoisy%2520camera%2520poses%252C%2520dynamic%2520scene%2520elements%252C%2520which%2520hinder%2520the%2520creation%2520of%250Ageometrically%2520accurate%2520and%2520photorealistic%2520digital%2520twins%2520of%2520unseen%2520objects.%2520We%250Aintroduce%2520a%2520novel%2520real-to-sim%2520framework%2520tackling%2520all%2520these%2520challenges%2520at%2520once.%250AOur%2520key%2520insight%2520is%2520a%2520hybrid%2520scene%2520representation%2520merging%2520the%2520photorealistic%250Arendering%2520of%25203D%2520Gaussian%2520Splatting%2520with%2520explicit%2520object%2520meshes%2520suitable%2520for%250Aphysics%2520simulation%2520within%2520a%2520single%2520representation.%2520We%2520propose%2520an%2520end-to-end%250Aoptimization%2520pipeline%2520that%2520leverages%2520differentiable%2520rendering%2520and%250Adifferentiable%2520physics%2520within%2520MuJoCo%2520to%2520jointly%2520refine%2520all%2520scene%2520components%2520-%250Afrom%2520object%2520geometry%2520and%2520appearance%2520to%2520robot%2520poses%2520and%2520physical%2520parameters%2520-%250Adirectly%2520from%2520raw%2520and%2520imprecise%2520robot%2520trajectories.%2520This%2520unified%2520optimization%250Aallows%2520us%2520to%2520simultaneously%2520achieve%2520high-fidelity%2520object%2520mesh%2520reconstruction%252C%250Agenerate%2520photorealistic%2520novel%2520views%252C%2520and%2520perform%2520annotation-free%2520robot%2520pose%250Acalibration.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520both%2520in%250Asimulation%2520and%2520on%2520challenging%2520real-world%2520sequences%2520using%2520an%2520ALOHA%25202%2520bi-manual%250Amanipulator%252C%2520enabling%2520more%2520practical%2520and%2520robust%2520real-to-simulation%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04120v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splatting%20Physical%20Scenes%3A%20End-to-End%20Real-to-Sim%20from%20Imperfect%20Robot%0A%20%20Data&entry.906535625=Ben%20Moran%20and%20Mauro%20Comi%20and%20Arunkumar%20Byravan%20and%20Steven%20Bohez%20and%20Tom%20Erez%20and%20Zhibin%20Li%20and%20Leonard%20Hasenclever&entry.1292438233=%20%20Creating%20accurate%2C%20physical%20simulations%20directly%20from%20real-world%20robot%20motion%0Aholds%20great%20value%20for%20safe%2C%20scalable%2C%20and%20affordable%20robot%20learning%2C%20yet%0Aremains%20exceptionally%20challenging.%20Real%20robot%20data%20suffers%20from%20occlusions%2C%0Anoisy%20camera%20poses%2C%20dynamic%20scene%20elements%2C%20which%20hinder%20the%20creation%20of%0Ageometrically%20accurate%20and%20photorealistic%20digital%20twins%20of%20unseen%20objects.%20We%0Aintroduce%20a%20novel%20real-to-sim%20framework%20tackling%20all%20these%20challenges%20at%20once.%0AOur%20key%20insight%20is%20a%20hybrid%20scene%20representation%20merging%20the%20photorealistic%0Arendering%20of%203D%20Gaussian%20Splatting%20with%20explicit%20object%20meshes%20suitable%20for%0Aphysics%20simulation%20within%20a%20single%20representation.%20We%20propose%20an%20end-to-end%0Aoptimization%20pipeline%20that%20leverages%20differentiable%20rendering%20and%0Adifferentiable%20physics%20within%20MuJoCo%20to%20jointly%20refine%20all%20scene%20components%20-%0Afrom%20object%20geometry%20and%20appearance%20to%20robot%20poses%20and%20physical%20parameters%20-%0Adirectly%20from%20raw%20and%20imprecise%20robot%20trajectories.%20This%20unified%20optimization%0Aallows%20us%20to%20simultaneously%20achieve%20high-fidelity%20object%20mesh%20reconstruction%2C%0Agenerate%20photorealistic%20novel%20views%2C%20and%20perform%20annotation-free%20robot%20pose%0Acalibration.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20both%20in%0Asimulation%20and%20on%20challenging%20real-world%20sequences%20using%20an%20ALOHA%202%20bi-manual%0Amanipulator%2C%20enabling%20more%20practical%20and%20robust%20real-to-simulation%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04120v2&entry.124074799=Read"},
{"title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity", "author": "Jinxi Li and Ziyang Song and Siyuan Zhou and Bo Yang", "abstract": "  In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.\n", "link": "http://arxiv.org/abs/2506.07865v1", "date": "2025-06-09", "relevancy": 3.0853, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6313}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6147}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeGave%3A%203D%20Physics%20Learning%20from%20Dynamic%20Videos%20by%20Gaussian%20Velocity&body=Title%3A%20FreeGave%3A%203D%20Physics%20Learning%20from%20Dynamic%20Videos%20by%20Gaussian%20Velocity%0AAuthor%3A%20Jinxi%20Li%20and%20Ziyang%20Song%20and%20Siyuan%20Zhou%20and%20Bo%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20model%203D%20scene%20geometry%2C%20appearance%2C%20and%20the%0Aunderlying%20physics%20purely%20from%20multi-view%20videos.%20By%20applying%20various%20governing%0APDEs%20as%20PINN%20losses%20or%20incorporating%20physics%20simulation%20into%20neural%20networks%2C%0Aexisting%20works%20often%20fail%20to%20learn%20complex%20physical%20motions%20at%20boundaries%20or%0Arequire%20object%20priors%20such%20as%20masks%20or%20types.%20In%20this%20paper%2C%20we%20propose%0AFreeGave%20to%20learn%20the%20physics%20of%20complex%20dynamic%203D%20scenes%20without%20needing%20any%0Aobject%20priors.%20The%20key%20to%20our%20approach%20is%20to%20introduce%20a%20physics%20code%20followed%0Aby%20a%20carefully%20designed%20divergence-free%20module%20for%20estimating%20a%20per-Gaussian%0Avelocity%20field%2C%20without%20relying%20on%20the%20inefficient%20PINN%20losses.%20Extensive%0Aexperiments%20on%20three%20public%20datasets%20and%20a%20newly%20collected%20challenging%0Areal-world%20dataset%20demonstrate%20the%20superior%20performance%20of%20our%20method%20for%0Afuture%20frame%20extrapolation%20and%20motion%20segmentation.%20Most%20notably%2C%20our%0Ainvestigation%20into%20the%20learned%20physics%20codes%20reveals%20that%20they%20truly%20learn%0Ameaningful%203D%20physical%20motion%20patterns%20in%20the%20absence%20of%20any%20human%20labels%20in%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeGave%253A%25203D%2520Physics%2520Learning%2520from%2520Dynamic%2520Videos%2520by%2520Gaussian%2520Velocity%26entry.906535625%3DJinxi%2520Li%2520and%2520Ziyang%2520Song%2520and%2520Siyuan%2520Zhou%2520and%2520Bo%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520model%25203D%2520scene%2520geometry%252C%2520appearance%252C%2520and%2520the%250Aunderlying%2520physics%2520purely%2520from%2520multi-view%2520videos.%2520By%2520applying%2520various%2520governing%250APDEs%2520as%2520PINN%2520losses%2520or%2520incorporating%2520physics%2520simulation%2520into%2520neural%2520networks%252C%250Aexisting%2520works%2520often%2520fail%2520to%2520learn%2520complex%2520physical%2520motions%2520at%2520boundaries%2520or%250Arequire%2520object%2520priors%2520such%2520as%2520masks%2520or%2520types.%2520In%2520this%2520paper%252C%2520we%2520propose%250AFreeGave%2520to%2520learn%2520the%2520physics%2520of%2520complex%2520dynamic%25203D%2520scenes%2520without%2520needing%2520any%250Aobject%2520priors.%2520The%2520key%2520to%2520our%2520approach%2520is%2520to%2520introduce%2520a%2520physics%2520code%2520followed%250Aby%2520a%2520carefully%2520designed%2520divergence-free%2520module%2520for%2520estimating%2520a%2520per-Gaussian%250Avelocity%2520field%252C%2520without%2520relying%2520on%2520the%2520inefficient%2520PINN%2520losses.%2520Extensive%250Aexperiments%2520on%2520three%2520public%2520datasets%2520and%2520a%2520newly%2520collected%2520challenging%250Areal-world%2520dataset%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%2520for%250Afuture%2520frame%2520extrapolation%2520and%2520motion%2520segmentation.%2520Most%2520notably%252C%2520our%250Ainvestigation%2520into%2520the%2520learned%2520physics%2520codes%2520reveals%2520that%2520they%2520truly%2520learn%250Ameaningful%25203D%2520physical%2520motion%2520patterns%2520in%2520the%2520absence%2520of%2520any%2520human%2520labels%2520in%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeGave%3A%203D%20Physics%20Learning%20from%20Dynamic%20Videos%20by%20Gaussian%20Velocity&entry.906535625=Jinxi%20Li%20and%20Ziyang%20Song%20and%20Siyuan%20Zhou%20and%20Bo%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20model%203D%20scene%20geometry%2C%20appearance%2C%20and%20the%0Aunderlying%20physics%20purely%20from%20multi-view%20videos.%20By%20applying%20various%20governing%0APDEs%20as%20PINN%20losses%20or%20incorporating%20physics%20simulation%20into%20neural%20networks%2C%0Aexisting%20works%20often%20fail%20to%20learn%20complex%20physical%20motions%20at%20boundaries%20or%0Arequire%20object%20priors%20such%20as%20masks%20or%20types.%20In%20this%20paper%2C%20we%20propose%0AFreeGave%20to%20learn%20the%20physics%20of%20complex%20dynamic%203D%20scenes%20without%20needing%20any%0Aobject%20priors.%20The%20key%20to%20our%20approach%20is%20to%20introduce%20a%20physics%20code%20followed%0Aby%20a%20carefully%20designed%20divergence-free%20module%20for%20estimating%20a%20per-Gaussian%0Avelocity%20field%2C%20without%20relying%20on%20the%20inefficient%20PINN%20losses.%20Extensive%0Aexperiments%20on%20three%20public%20datasets%20and%20a%20newly%20collected%20challenging%0Areal-world%20dataset%20demonstrate%20the%20superior%20performance%20of%20our%20method%20for%0Afuture%20frame%20extrapolation%20and%20motion%20segmentation.%20Most%20notably%2C%20our%0Ainvestigation%20into%20the%20learned%20physics%20codes%20reveals%20that%20they%20truly%20learn%0Ameaningful%203D%20physical%20motion%20patterns%20in%20the%20absence%20of%20any%20human%20labels%20in%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07865v1&entry.124074799=Read"},
{"title": "Event-Priori-Based Vision-Language Model for Efficient Visual\n  Understanding", "author": "Haotong Qin and Cheng Hu and Michele Magno", "abstract": "  Large Language Model (LLM)-based Vision-Language Models (VLMs) have\nsubstantially extended the boundaries of visual understanding capabilities.\nHowever, their high computational demands hinder deployment on\nresource-constrained edge devices. A key source of inefficiency stems from the\nVLM's need to process dense and redundant visual information. Visual inputs\ncontain significant regions irrelevant to text semantics, rendering the\nassociated computations ineffective for inference. This paper introduces a\nnovel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core\ncontribution is a novel mechanism leveraging motion priors derived from dynamic\nevent vision to enhance VLM efficiency. Inspired by human visual cognition,\nEP-VLM first employs event data to guide the patch-wise sparsification of RGB\nvisual inputs, progressively concentrating VLM computation on salient regions\nof the visual input. Subsequently, we construct a position-preserving\ntokenization strategy for the visual encoder within the VLM architecture. This\nstrategy processes the event-guided, unstructured, sparse visual input while\naccurately preserving positional understanding within the visual input.\nExperimental results demonstrate that EP-VLM achieves significant efficiency\nimprovements while maintaining nearly lossless accuracy compared to baseline\nmodels from the Qwen2-VL series. For instance, against the original\nQwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the\noriginal accuracy on the RealWorldQA dataset. This work demonstrates the\npotential of event-based vision priors for improving VLM inference efficiency,\npaving the way for creating more efficient and deployable VLMs for sustainable\nvisual understanding at the edge.\n", "link": "http://arxiv.org/abs/2506.07627v1", "date": "2025-06-09", "relevancy": 3.082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6465}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Priori-Based%20Vision-Language%20Model%20for%20Efficient%20Visual%0A%20%20Understanding&body=Title%3A%20Event-Priori-Based%20Vision-Language%20Model%20for%20Efficient%20Visual%0A%20%20Understanding%0AAuthor%3A%20Haotong%20Qin%20and%20Cheng%20Hu%20and%20Michele%20Magno%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20Vision-Language%20Models%20%28VLMs%29%20have%0Asubstantially%20extended%20the%20boundaries%20of%20visual%20understanding%20capabilities.%0AHowever%2C%20their%20high%20computational%20demands%20hinder%20deployment%20on%0Aresource-constrained%20edge%20devices.%20A%20key%20source%20of%20inefficiency%20stems%20from%20the%0AVLM%27s%20need%20to%20process%20dense%20and%20redundant%20visual%20information.%20Visual%20inputs%0Acontain%20significant%20regions%20irrelevant%20to%20text%20semantics%2C%20rendering%20the%0Aassociated%20computations%20ineffective%20for%20inference.%20This%20paper%20introduces%20a%0Anovel%20Event-Priori-Based%20Vision-Language%20Model%2C%20termed%20EP-VLM.%20Its%20core%0Acontribution%20is%20a%20novel%20mechanism%20leveraging%20motion%20priors%20derived%20from%20dynamic%0Aevent%20vision%20to%20enhance%20VLM%20efficiency.%20Inspired%20by%20human%20visual%20cognition%2C%0AEP-VLM%20first%20employs%20event%20data%20to%20guide%20the%20patch-wise%20sparsification%20of%20RGB%0Avisual%20inputs%2C%20progressively%20concentrating%20VLM%20computation%20on%20salient%20regions%0Aof%20the%20visual%20input.%20Subsequently%2C%20we%20construct%20a%20position-preserving%0Atokenization%20strategy%20for%20the%20visual%20encoder%20within%20the%20VLM%20architecture.%20This%0Astrategy%20processes%20the%20event-guided%2C%20unstructured%2C%20sparse%20visual%20input%20while%0Aaccurately%20preserving%20positional%20understanding%20within%20the%20visual%20input.%0AExperimental%20results%20demonstrate%20that%20EP-VLM%20achieves%20significant%20efficiency%0Aimprovements%20while%20maintaining%20nearly%20lossless%20accuracy%20compared%20to%20baseline%0Amodels%20from%20the%20Qwen2-VL%20series.%20For%20instance%2C%20against%20the%20original%0AQwen2-VL-2B%2C%20EP-VLM%20achieves%2050%25%20FLOPs%20savings%20while%20retaining%2098%25%20of%20the%0Aoriginal%20accuracy%20on%20the%20RealWorldQA%20dataset.%20This%20work%20demonstrates%20the%0Apotential%20of%20event-based%20vision%20priors%20for%20improving%20VLM%20inference%20efficiency%2C%0Apaving%20the%20way%20for%20creating%20more%20efficient%20and%20deployable%20VLMs%20for%20sustainable%0Avisual%20understanding%20at%20the%20edge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Priori-Based%2520Vision-Language%2520Model%2520for%2520Efficient%2520Visual%250A%2520%2520Understanding%26entry.906535625%3DHaotong%2520Qin%2520and%2520Cheng%2520Hu%2520and%2520Michele%2520Magno%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%250Asubstantially%2520extended%2520the%2520boundaries%2520of%2520visual%2520understanding%2520capabilities.%250AHowever%252C%2520their%2520high%2520computational%2520demands%2520hinder%2520deployment%2520on%250Aresource-constrained%2520edge%2520devices.%2520A%2520key%2520source%2520of%2520inefficiency%2520stems%2520from%2520the%250AVLM%2527s%2520need%2520to%2520process%2520dense%2520and%2520redundant%2520visual%2520information.%2520Visual%2520inputs%250Acontain%2520significant%2520regions%2520irrelevant%2520to%2520text%2520semantics%252C%2520rendering%2520the%250Aassociated%2520computations%2520ineffective%2520for%2520inference.%2520This%2520paper%2520introduces%2520a%250Anovel%2520Event-Priori-Based%2520Vision-Language%2520Model%252C%2520termed%2520EP-VLM.%2520Its%2520core%250Acontribution%2520is%2520a%2520novel%2520mechanism%2520leveraging%2520motion%2520priors%2520derived%2520from%2520dynamic%250Aevent%2520vision%2520to%2520enhance%2520VLM%2520efficiency.%2520Inspired%2520by%2520human%2520visual%2520cognition%252C%250AEP-VLM%2520first%2520employs%2520event%2520data%2520to%2520guide%2520the%2520patch-wise%2520sparsification%2520of%2520RGB%250Avisual%2520inputs%252C%2520progressively%2520concentrating%2520VLM%2520computation%2520on%2520salient%2520regions%250Aof%2520the%2520visual%2520input.%2520Subsequently%252C%2520we%2520construct%2520a%2520position-preserving%250Atokenization%2520strategy%2520for%2520the%2520visual%2520encoder%2520within%2520the%2520VLM%2520architecture.%2520This%250Astrategy%2520processes%2520the%2520event-guided%252C%2520unstructured%252C%2520sparse%2520visual%2520input%2520while%250Aaccurately%2520preserving%2520positional%2520understanding%2520within%2520the%2520visual%2520input.%250AExperimental%2520results%2520demonstrate%2520that%2520EP-VLM%2520achieves%2520significant%2520efficiency%250Aimprovements%2520while%2520maintaining%2520nearly%2520lossless%2520accuracy%2520compared%2520to%2520baseline%250Amodels%2520from%2520the%2520Qwen2-VL%2520series.%2520For%2520instance%252C%2520against%2520the%2520original%250AQwen2-VL-2B%252C%2520EP-VLM%2520achieves%252050%2525%2520FLOPs%2520savings%2520while%2520retaining%252098%2525%2520of%2520the%250Aoriginal%2520accuracy%2520on%2520the%2520RealWorldQA%2520dataset.%2520This%2520work%2520demonstrates%2520the%250Apotential%2520of%2520event-based%2520vision%2520priors%2520for%2520improving%2520VLM%2520inference%2520efficiency%252C%250Apaving%2520the%2520way%2520for%2520creating%2520more%2520efficient%2520and%2520deployable%2520VLMs%2520for%2520sustainable%250Avisual%2520understanding%2520at%2520the%2520edge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Priori-Based%20Vision-Language%20Model%20for%20Efficient%20Visual%0A%20%20Understanding&entry.906535625=Haotong%20Qin%20and%20Cheng%20Hu%20and%20Michele%20Magno&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20Vision-Language%20Models%20%28VLMs%29%20have%0Asubstantially%20extended%20the%20boundaries%20of%20visual%20understanding%20capabilities.%0AHowever%2C%20their%20high%20computational%20demands%20hinder%20deployment%20on%0Aresource-constrained%20edge%20devices.%20A%20key%20source%20of%20inefficiency%20stems%20from%20the%0AVLM%27s%20need%20to%20process%20dense%20and%20redundant%20visual%20information.%20Visual%20inputs%0Acontain%20significant%20regions%20irrelevant%20to%20text%20semantics%2C%20rendering%20the%0Aassociated%20computations%20ineffective%20for%20inference.%20This%20paper%20introduces%20a%0Anovel%20Event-Priori-Based%20Vision-Language%20Model%2C%20termed%20EP-VLM.%20Its%20core%0Acontribution%20is%20a%20novel%20mechanism%20leveraging%20motion%20priors%20derived%20from%20dynamic%0Aevent%20vision%20to%20enhance%20VLM%20efficiency.%20Inspired%20by%20human%20visual%20cognition%2C%0AEP-VLM%20first%20employs%20event%20data%20to%20guide%20the%20patch-wise%20sparsification%20of%20RGB%0Avisual%20inputs%2C%20progressively%20concentrating%20VLM%20computation%20on%20salient%20regions%0Aof%20the%20visual%20input.%20Subsequently%2C%20we%20construct%20a%20position-preserving%0Atokenization%20strategy%20for%20the%20visual%20encoder%20within%20the%20VLM%20architecture.%20This%0Astrategy%20processes%20the%20event-guided%2C%20unstructured%2C%20sparse%20visual%20input%20while%0Aaccurately%20preserving%20positional%20understanding%20within%20the%20visual%20input.%0AExperimental%20results%20demonstrate%20that%20EP-VLM%20achieves%20significant%20efficiency%0Aimprovements%20while%20maintaining%20nearly%20lossless%20accuracy%20compared%20to%20baseline%0Amodels%20from%20the%20Qwen2-VL%20series.%20For%20instance%2C%20against%20the%20original%0AQwen2-VL-2B%2C%20EP-VLM%20achieves%2050%25%20FLOPs%20savings%20while%20retaining%2098%25%20of%20the%0Aoriginal%20accuracy%20on%20the%20RealWorldQA%20dataset.%20This%20work%20demonstrates%20the%0Apotential%20of%20event-based%20vision%20priors%20for%20improving%20VLM%20inference%20efficiency%2C%0Apaving%20the%20way%20for%20creating%20more%20efficient%20and%20deployable%20VLMs%20for%20sustainable%0Avisual%20understanding%20at%20the%20edge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07627v1&entry.124074799=Read"},
{"title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations", "author": "Weijie Guo and Guofeng Zhang and Wufei Ma and Alan Yuille", "abstract": "  Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.\n", "link": "http://arxiv.org/abs/2503.20220v2", "date": "2025-06-09", "relevancy": 3.0259, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINeMo%3A%20Learning%20Neural%20Mesh%20Models%20with%20no%203D%20Annotations&body=Title%3A%20DINeMo%3A%20Learning%20Neural%20Mesh%20Models%20with%20no%203D%20Annotations%0AAuthor%3A%20Weijie%20Guo%20and%20Guofeng%20Zhang%20and%20Wufei%20Ma%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20Category-level%203D/6D%20pose%20estimation%20is%20a%20crucial%20step%20towards%20comprehensive%0A3D%20scene%20understanding%2C%20which%20would%20enable%20a%20broad%20range%20of%20applications%20in%0Arobotics%20and%20embodied%20AI.%20Recent%20works%20explored%20neural%20mesh%20models%20that%0Aapproach%20a%20range%20of%202D%20and%203D%20tasks%20from%20an%20analysis-by-synthesis%20perspective.%0ADespite%20the%20largely%20enhanced%20robustness%20to%20partial%20occlusion%20and%20domain%20shifts%2C%0Athese%20methods%20depended%20heavily%20on%203D%20annotations%20for%20part-contrastive%20learning%2C%0Awhich%20confines%20them%20to%20a%20narrow%20set%20of%20categories%20and%20hinders%20efficient%0Ascaling.%20In%20this%20work%2C%20we%20present%20DINeMo%2C%20a%20novel%20neural%20mesh%20model%20that%20is%0Atrained%20with%20no%203D%20annotations%20by%20leveraging%20pseudo-correspondence%20obtained%0Afrom%20large%20visual%20foundation%20models.%20We%20adopt%20a%20bidirectional%0Apseudo-correspondence%20generation%20method%2C%20which%20produce%20pseudo%20correspondence%0Autilize%20both%20local%20appearance%20features%20and%20global%20context%20information.%0AExperimental%20results%20on%20car%20datasets%20demonstrate%20that%20our%20DINeMo%20outperforms%0Aprevious%20zero-%20and%20few-shot%203D%20pose%20estimation%20by%20a%20wide%20margin%2C%20narrowing%20the%0Agap%20with%20fully-supervised%20methods%20by%2067.3%25.%20Our%20DINeMo%20also%20scales%20effectively%0Aand%20efficiently%20when%20incorporating%20more%20unlabeled%20images%20during%20training%2C%20which%0Ademonstrate%20the%20advantages%20over%20supervised%20learning%20methods%20that%20rely%20on%203D%0Aannotations.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//analysis-by-synthesis.github.io/DINeMo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINeMo%253A%2520Learning%2520Neural%2520Mesh%2520Models%2520with%2520no%25203D%2520Annotations%26entry.906535625%3DWeijie%2520Guo%2520and%2520Guofeng%2520Zhang%2520and%2520Wufei%2520Ma%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520Category-level%25203D/6D%2520pose%2520estimation%2520is%2520a%2520crucial%2520step%2520towards%2520comprehensive%250A3D%2520scene%2520understanding%252C%2520which%2520would%2520enable%2520a%2520broad%2520range%2520of%2520applications%2520in%250Arobotics%2520and%2520embodied%2520AI.%2520Recent%2520works%2520explored%2520neural%2520mesh%2520models%2520that%250Aapproach%2520a%2520range%2520of%25202D%2520and%25203D%2520tasks%2520from%2520an%2520analysis-by-synthesis%2520perspective.%250ADespite%2520the%2520largely%2520enhanced%2520robustness%2520to%2520partial%2520occlusion%2520and%2520domain%2520shifts%252C%250Athese%2520methods%2520depended%2520heavily%2520on%25203D%2520annotations%2520for%2520part-contrastive%2520learning%252C%250Awhich%2520confines%2520them%2520to%2520a%2520narrow%2520set%2520of%2520categories%2520and%2520hinders%2520efficient%250Ascaling.%2520In%2520this%2520work%252C%2520we%2520present%2520DINeMo%252C%2520a%2520novel%2520neural%2520mesh%2520model%2520that%2520is%250Atrained%2520with%2520no%25203D%2520annotations%2520by%2520leveraging%2520pseudo-correspondence%2520obtained%250Afrom%2520large%2520visual%2520foundation%2520models.%2520We%2520adopt%2520a%2520bidirectional%250Apseudo-correspondence%2520generation%2520method%252C%2520which%2520produce%2520pseudo%2520correspondence%250Autilize%2520both%2520local%2520appearance%2520features%2520and%2520global%2520context%2520information.%250AExperimental%2520results%2520on%2520car%2520datasets%2520demonstrate%2520that%2520our%2520DINeMo%2520outperforms%250Aprevious%2520zero-%2520and%2520few-shot%25203D%2520pose%2520estimation%2520by%2520a%2520wide%2520margin%252C%2520narrowing%2520the%250Agap%2520with%2520fully-supervised%2520methods%2520by%252067.3%2525.%2520Our%2520DINeMo%2520also%2520scales%2520effectively%250Aand%2520efficiently%2520when%2520incorporating%2520more%2520unlabeled%2520images%2520during%2520training%252C%2520which%250Ademonstrate%2520the%2520advantages%2520over%2520supervised%2520learning%2520methods%2520that%2520rely%2520on%25203D%250Aannotations.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//analysis-by-synthesis.github.io/DINeMo/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINeMo%3A%20Learning%20Neural%20Mesh%20Models%20with%20no%203D%20Annotations&entry.906535625=Weijie%20Guo%20and%20Guofeng%20Zhang%20and%20Wufei%20Ma%20and%20Alan%20Yuille&entry.1292438233=%20%20Category-level%203D/6D%20pose%20estimation%20is%20a%20crucial%20step%20towards%20comprehensive%0A3D%20scene%20understanding%2C%20which%20would%20enable%20a%20broad%20range%20of%20applications%20in%0Arobotics%20and%20embodied%20AI.%20Recent%20works%20explored%20neural%20mesh%20models%20that%0Aapproach%20a%20range%20of%202D%20and%203D%20tasks%20from%20an%20analysis-by-synthesis%20perspective.%0ADespite%20the%20largely%20enhanced%20robustness%20to%20partial%20occlusion%20and%20domain%20shifts%2C%0Athese%20methods%20depended%20heavily%20on%203D%20annotations%20for%20part-contrastive%20learning%2C%0Awhich%20confines%20them%20to%20a%20narrow%20set%20of%20categories%20and%20hinders%20efficient%0Ascaling.%20In%20this%20work%2C%20we%20present%20DINeMo%2C%20a%20novel%20neural%20mesh%20model%20that%20is%0Atrained%20with%20no%203D%20annotations%20by%20leveraging%20pseudo-correspondence%20obtained%0Afrom%20large%20visual%20foundation%20models.%20We%20adopt%20a%20bidirectional%0Apseudo-correspondence%20generation%20method%2C%20which%20produce%20pseudo%20correspondence%0Autilize%20both%20local%20appearance%20features%20and%20global%20context%20information.%0AExperimental%20results%20on%20car%20datasets%20demonstrate%20that%20our%20DINeMo%20outperforms%0Aprevious%20zero-%20and%20few-shot%203D%20pose%20estimation%20by%20a%20wide%20margin%2C%20narrowing%20the%0Agap%20with%20fully-supervised%20methods%20by%2067.3%25.%20Our%20DINeMo%20also%20scales%20effectively%0Aand%20efficiently%20when%20incorporating%20more%20unlabeled%20images%20during%20training%2C%20which%0Ademonstrate%20the%20advantages%20over%20supervised%20learning%20methods%20that%20rely%20on%203D%0Aannotations.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//analysis-by-synthesis.github.io/DINeMo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20220v2&entry.124074799=Read"},
{"title": "Weakly Supervised Temporal Action Localization via Dual-Prior\n  Collaborative Learning Guided by Multimodal Large Language Models", "author": "Quan Zhang and Jinwei Fang and Rui Yuan and Xi Tang and Yuxin Qi and Ke Zhang and Chun Yuan", "abstract": "  Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained\nsignificant recognition within the deep learning community, where the fusion of\nthe Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven\ninstrumental in constructing robust video understanding systems, effectively\nsurmounting constraints associated with predefined visual tasks. These\nsophisticated MLLMs exhibit remarkable proficiency in comprehending videos,\nswiftly attaining unprecedented performance levels across diverse benchmarks.\nHowever, their operation demands substantial memory and computational\nresources, underscoring the continued importance of traditional models in video\ncomprehension tasks. In this paper, we introduce a novel learning paradigm\ntermed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer\ntemporal action key semantics and complete semantic priors for conventional\nWeakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL\nfacilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves\nthis by integrating two distinct modules: Key Semantic Matching (KSM) and\nComplete Semantic Reconstruction (CSR). These modules work in tandem to\neffectively address prevalent issues like incomplete and over-complete outcomes\ncommon in WTAL methods. Rigorous experiments are conducted to validate the\nefficacy of our proposed approach in augmenting the performance of various\nheterogeneous WTAL models.\n", "link": "http://arxiv.org/abs/2411.08466v2", "date": "2025-06-09", "relevancy": 2.9755, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6237}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Temporal%20Action%20Localization%20via%20Dual-Prior%0A%20%20Collaborative%20Learning%20Guided%20by%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Weakly%20Supervised%20Temporal%20Action%20Localization%20via%20Dual-Prior%0A%20%20Collaborative%20Learning%20Guided%20by%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Quan%20Zhang%20and%20Jinwei%20Fang%20and%20Rui%20Yuan%20and%20Xi%20Tang%20and%20Yuxin%20Qi%20and%20Ke%20Zhang%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20gained%0Asignificant%20recognition%20within%20the%20deep%20learning%20community%2C%20where%20the%20fusion%20of%0Athe%20Video%20Foundation%20Models%20%28VFMs%29%20and%20Large%20Language%20Models%28LLMs%29%20has%20proven%0Ainstrumental%20in%20constructing%20robust%20video%20understanding%20systems%2C%20effectively%0Asurmounting%20constraints%20associated%20with%20predefined%20visual%20tasks.%20These%0Asophisticated%20MLLMs%20exhibit%20remarkable%20proficiency%20in%20comprehending%20videos%2C%0Aswiftly%20attaining%20unprecedented%20performance%20levels%20across%20diverse%20benchmarks.%0AHowever%2C%20their%20operation%20demands%20substantial%20memory%20and%20computational%0Aresources%2C%20underscoring%20the%20continued%20importance%20of%20traditional%20models%20in%20video%0Acomprehension%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20learning%20paradigm%0Atermed%20MLLM4WTAL.%20This%20paradigm%20harnesses%20the%20potential%20of%20MLLM%20to%20offer%0Atemporal%20action%20key%20semantics%20and%20complete%20semantic%20priors%20for%20conventional%0AWeakly-supervised%20Temporal%20Action%20Localization%20%28WTAL%29%20methods.%20MLLM4WTAL%0Afacilitates%20the%20enhancement%20of%20WTAL%20by%20leveraging%20MLLM%20guidance.%20It%20achieves%0Athis%20by%20integrating%20two%20distinct%20modules%3A%20Key%20Semantic%20Matching%20%28KSM%29%20and%0AComplete%20Semantic%20Reconstruction%20%28CSR%29.%20These%20modules%20work%20in%20tandem%20to%0Aeffectively%20address%20prevalent%20issues%20like%20incomplete%20and%20over-complete%20outcomes%0Acommon%20in%20WTAL%20methods.%20Rigorous%20experiments%20are%20conducted%20to%20validate%20the%0Aefficacy%20of%20our%20proposed%20approach%20in%20augmenting%20the%20performance%20of%20various%0Aheterogeneous%20WTAL%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08466v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Temporal%2520Action%2520Localization%2520via%2520Dual-Prior%250A%2520%2520Collaborative%2520Learning%2520Guided%2520by%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DQuan%2520Zhang%2520and%2520Jinwei%2520Fang%2520and%2520Rui%2520Yuan%2520and%2520Xi%2520Tang%2520and%2520Yuxin%2520Qi%2520and%2520Ke%2520Zhang%2520and%2520Chun%2520Yuan%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520gained%250Asignificant%2520recognition%2520within%2520the%2520deep%2520learning%2520community%252C%2520where%2520the%2520fusion%2520of%250Athe%2520Video%2520Foundation%2520Models%2520%2528VFMs%2529%2520and%2520Large%2520Language%2520Models%2528LLMs%2529%2520has%2520proven%250Ainstrumental%2520in%2520constructing%2520robust%2520video%2520understanding%2520systems%252C%2520effectively%250Asurmounting%2520constraints%2520associated%2520with%2520predefined%2520visual%2520tasks.%2520These%250Asophisticated%2520MLLMs%2520exhibit%2520remarkable%2520proficiency%2520in%2520comprehending%2520videos%252C%250Aswiftly%2520attaining%2520unprecedented%2520performance%2520levels%2520across%2520diverse%2520benchmarks.%250AHowever%252C%2520their%2520operation%2520demands%2520substantial%2520memory%2520and%2520computational%250Aresources%252C%2520underscoring%2520the%2520continued%2520importance%2520of%2520traditional%2520models%2520in%2520video%250Acomprehension%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520learning%2520paradigm%250Atermed%2520MLLM4WTAL.%2520This%2520paradigm%2520harnesses%2520the%2520potential%2520of%2520MLLM%2520to%2520offer%250Atemporal%2520action%2520key%2520semantics%2520and%2520complete%2520semantic%2520priors%2520for%2520conventional%250AWeakly-supervised%2520Temporal%2520Action%2520Localization%2520%2528WTAL%2529%2520methods.%2520MLLM4WTAL%250Afacilitates%2520the%2520enhancement%2520of%2520WTAL%2520by%2520leveraging%2520MLLM%2520guidance.%2520It%2520achieves%250Athis%2520by%2520integrating%2520two%2520distinct%2520modules%253A%2520Key%2520Semantic%2520Matching%2520%2528KSM%2529%2520and%250AComplete%2520Semantic%2520Reconstruction%2520%2528CSR%2529.%2520These%2520modules%2520work%2520in%2520tandem%2520to%250Aeffectively%2520address%2520prevalent%2520issues%2520like%2520incomplete%2520and%2520over-complete%2520outcomes%250Acommon%2520in%2520WTAL%2520methods.%2520Rigorous%2520experiments%2520are%2520conducted%2520to%2520validate%2520the%250Aefficacy%2520of%2520our%2520proposed%2520approach%2520in%2520augmenting%2520the%2520performance%2520of%2520various%250Aheterogeneous%2520WTAL%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08466v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Temporal%20Action%20Localization%20via%20Dual-Prior%0A%20%20Collaborative%20Learning%20Guided%20by%20Multimodal%20Large%20Language%20Models&entry.906535625=Quan%20Zhang%20and%20Jinwei%20Fang%20and%20Rui%20Yuan%20and%20Xi%20Tang%20and%20Yuxin%20Qi%20and%20Ke%20Zhang%20and%20Chun%20Yuan&entry.1292438233=%20%20Recent%20breakthroughs%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20gained%0Asignificant%20recognition%20within%20the%20deep%20learning%20community%2C%20where%20the%20fusion%20of%0Athe%20Video%20Foundation%20Models%20%28VFMs%29%20and%20Large%20Language%20Models%28LLMs%29%20has%20proven%0Ainstrumental%20in%20constructing%20robust%20video%20understanding%20systems%2C%20effectively%0Asurmounting%20constraints%20associated%20with%20predefined%20visual%20tasks.%20These%0Asophisticated%20MLLMs%20exhibit%20remarkable%20proficiency%20in%20comprehending%20videos%2C%0Aswiftly%20attaining%20unprecedented%20performance%20levels%20across%20diverse%20benchmarks.%0AHowever%2C%20their%20operation%20demands%20substantial%20memory%20and%20computational%0Aresources%2C%20underscoring%20the%20continued%20importance%20of%20traditional%20models%20in%20video%0Acomprehension%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20learning%20paradigm%0Atermed%20MLLM4WTAL.%20This%20paradigm%20harnesses%20the%20potential%20of%20MLLM%20to%20offer%0Atemporal%20action%20key%20semantics%20and%20complete%20semantic%20priors%20for%20conventional%0AWeakly-supervised%20Temporal%20Action%20Localization%20%28WTAL%29%20methods.%20MLLM4WTAL%0Afacilitates%20the%20enhancement%20of%20WTAL%20by%20leveraging%20MLLM%20guidance.%20It%20achieves%0Athis%20by%20integrating%20two%20distinct%20modules%3A%20Key%20Semantic%20Matching%20%28KSM%29%20and%0AComplete%20Semantic%20Reconstruction%20%28CSR%29.%20These%20modules%20work%20in%20tandem%20to%0Aeffectively%20address%20prevalent%20issues%20like%20incomplete%20and%20over-complete%20outcomes%0Acommon%20in%20WTAL%20methods.%20Rigorous%20experiments%20are%20conducted%20to%20validate%20the%0Aefficacy%20of%20our%20proposed%20approach%20in%20augmenting%20the%20performance%20of%20various%0Aheterogeneous%20WTAL%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08466v2&entry.124074799=Read"},
{"title": "HuSc3D: Human Sculpture dataset for 3D object reconstruction", "author": "Weronika Smolak-Dy\u017cewska and Dawid Malarz and Grzegorz Wilczy\u0144ski and Rafa\u0142 Tobiasz and Joanna Waczy\u0144ska and Piotr Borycki and Przemys\u0142aw Spurek", "abstract": "  3D scene reconstruction from 2D images is one of the most important tasks in\ncomputer graphics. Unfortunately, existing datasets and benchmarks concentrate\non idealized synthetic or meticulously captured realistic data. Such benchmarks\nfail to convey the inherent complexities encountered in newly acquired\nreal-world scenes. In such scenes especially those acquired outside, the\nbackground is often dynamic, and by popular usage of cell phone cameras, there\nmight be discrepancies in, e.g., white balance. To address this gap, we present\nHuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D\nreconstruction models under realistic acquisition challenges. Our dataset\nuniquely features six highly detailed, fully white sculptures characterized by\nintricate perforations and minimal textural and color variation. Furthermore,\nthe number of images per scene varies significantly, introducing the additional\nchallenge of limited training data for some instances alongside scenes with a\nstandard number of views. By evaluating popular 3D reconstruction methods on\nthis diverse dataset, we demonstrate the distinctiveness of HuSc3D in\neffectively differentiating model performance, particularly highlighting the\nsensitivity of methods to fine geometric details, color ambiguity, and varying\ndata availability--limitations often masked by more conventional datasets.\n", "link": "http://arxiv.org/abs/2506.07628v1", "date": "2025-06-09", "relevancy": 2.9674, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5955}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5925}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HuSc3D%3A%20Human%20Sculpture%20dataset%20for%203D%20object%20reconstruction&body=Title%3A%20HuSc3D%3A%20Human%20Sculpture%20dataset%20for%203D%20object%20reconstruction%0AAuthor%3A%20Weronika%20Smolak-Dy%C5%BCewska%20and%20Dawid%20Malarz%20and%20Grzegorz%20Wilczy%C5%84ski%20and%20Rafa%C5%82%20Tobiasz%20and%20Joanna%20Waczy%C5%84ska%20and%20Piotr%20Borycki%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%203D%20scene%20reconstruction%20from%202D%20images%20is%20one%20of%20the%20most%20important%20tasks%20in%0Acomputer%20graphics.%20Unfortunately%2C%20existing%20datasets%20and%20benchmarks%20concentrate%0Aon%20idealized%20synthetic%20or%20meticulously%20captured%20realistic%20data.%20Such%20benchmarks%0Afail%20to%20convey%20the%20inherent%20complexities%20encountered%20in%20newly%20acquired%0Areal-world%20scenes.%20In%20such%20scenes%20especially%20those%20acquired%20outside%2C%20the%0Abackground%20is%20often%20dynamic%2C%20and%20by%20popular%20usage%20of%20cell%20phone%20cameras%2C%20there%0Amight%20be%20discrepancies%20in%2C%20e.g.%2C%20white%20balance.%20To%20address%20this%20gap%2C%20we%20present%0AHuSc3D%2C%20a%20novel%20dataset%20specifically%20designed%20for%20rigorous%20benchmarking%20of%203D%0Areconstruction%20models%20under%20realistic%20acquisition%20challenges.%20Our%20dataset%0Auniquely%20features%20six%20highly%20detailed%2C%20fully%20white%20sculptures%20characterized%20by%0Aintricate%20perforations%20and%20minimal%20textural%20and%20color%20variation.%20Furthermore%2C%0Athe%20number%20of%20images%20per%20scene%20varies%20significantly%2C%20introducing%20the%20additional%0Achallenge%20of%20limited%20training%20data%20for%20some%20instances%20alongside%20scenes%20with%20a%0Astandard%20number%20of%20views.%20By%20evaluating%20popular%203D%20reconstruction%20methods%20on%0Athis%20diverse%20dataset%2C%20we%20demonstrate%20the%20distinctiveness%20of%20HuSc3D%20in%0Aeffectively%20differentiating%20model%20performance%2C%20particularly%20highlighting%20the%0Asensitivity%20of%20methods%20to%20fine%20geometric%20details%2C%20color%20ambiguity%2C%20and%20varying%0Adata%20availability--limitations%20often%20masked%20by%20more%20conventional%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuSc3D%253A%2520Human%2520Sculpture%2520dataset%2520for%25203D%2520object%2520reconstruction%26entry.906535625%3DWeronika%2520Smolak-Dy%25C5%25BCewska%2520and%2520Dawid%2520Malarz%2520and%2520Grzegorz%2520Wilczy%25C5%2584ski%2520and%2520Rafa%25C5%2582%2520Tobiasz%2520and%2520Joanna%2520Waczy%25C5%2584ska%2520and%2520Piotr%2520Borycki%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%25203D%2520scene%2520reconstruction%2520from%25202D%2520images%2520is%2520one%2520of%2520the%2520most%2520important%2520tasks%2520in%250Acomputer%2520graphics.%2520Unfortunately%252C%2520existing%2520datasets%2520and%2520benchmarks%2520concentrate%250Aon%2520idealized%2520synthetic%2520or%2520meticulously%2520captured%2520realistic%2520data.%2520Such%2520benchmarks%250Afail%2520to%2520convey%2520the%2520inherent%2520complexities%2520encountered%2520in%2520newly%2520acquired%250Areal-world%2520scenes.%2520In%2520such%2520scenes%2520especially%2520those%2520acquired%2520outside%252C%2520the%250Abackground%2520is%2520often%2520dynamic%252C%2520and%2520by%2520popular%2520usage%2520of%2520cell%2520phone%2520cameras%252C%2520there%250Amight%2520be%2520discrepancies%2520in%252C%2520e.g.%252C%2520white%2520balance.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%250AHuSc3D%252C%2520a%2520novel%2520dataset%2520specifically%2520designed%2520for%2520rigorous%2520benchmarking%2520of%25203D%250Areconstruction%2520models%2520under%2520realistic%2520acquisition%2520challenges.%2520Our%2520dataset%250Auniquely%2520features%2520six%2520highly%2520detailed%252C%2520fully%2520white%2520sculptures%2520characterized%2520by%250Aintricate%2520perforations%2520and%2520minimal%2520textural%2520and%2520color%2520variation.%2520Furthermore%252C%250Athe%2520number%2520of%2520images%2520per%2520scene%2520varies%2520significantly%252C%2520introducing%2520the%2520additional%250Achallenge%2520of%2520limited%2520training%2520data%2520for%2520some%2520instances%2520alongside%2520scenes%2520with%2520a%250Astandard%2520number%2520of%2520views.%2520By%2520evaluating%2520popular%25203D%2520reconstruction%2520methods%2520on%250Athis%2520diverse%2520dataset%252C%2520we%2520demonstrate%2520the%2520distinctiveness%2520of%2520HuSc3D%2520in%250Aeffectively%2520differentiating%2520model%2520performance%252C%2520particularly%2520highlighting%2520the%250Asensitivity%2520of%2520methods%2520to%2520fine%2520geometric%2520details%252C%2520color%2520ambiguity%252C%2520and%2520varying%250Adata%2520availability--limitations%2520often%2520masked%2520by%2520more%2520conventional%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HuSc3D%3A%20Human%20Sculpture%20dataset%20for%203D%20object%20reconstruction&entry.906535625=Weronika%20Smolak-Dy%C5%BCewska%20and%20Dawid%20Malarz%20and%20Grzegorz%20Wilczy%C5%84ski%20and%20Rafa%C5%82%20Tobiasz%20and%20Joanna%20Waczy%C5%84ska%20and%20Piotr%20Borycki%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%203D%20scene%20reconstruction%20from%202D%20images%20is%20one%20of%20the%20most%20important%20tasks%20in%0Acomputer%20graphics.%20Unfortunately%2C%20existing%20datasets%20and%20benchmarks%20concentrate%0Aon%20idealized%20synthetic%20or%20meticulously%20captured%20realistic%20data.%20Such%20benchmarks%0Afail%20to%20convey%20the%20inherent%20complexities%20encountered%20in%20newly%20acquired%0Areal-world%20scenes.%20In%20such%20scenes%20especially%20those%20acquired%20outside%2C%20the%0Abackground%20is%20often%20dynamic%2C%20and%20by%20popular%20usage%20of%20cell%20phone%20cameras%2C%20there%0Amight%20be%20discrepancies%20in%2C%20e.g.%2C%20white%20balance.%20To%20address%20this%20gap%2C%20we%20present%0AHuSc3D%2C%20a%20novel%20dataset%20specifically%20designed%20for%20rigorous%20benchmarking%20of%203D%0Areconstruction%20models%20under%20realistic%20acquisition%20challenges.%20Our%20dataset%0Auniquely%20features%20six%20highly%20detailed%2C%20fully%20white%20sculptures%20characterized%20by%0Aintricate%20perforations%20and%20minimal%20textural%20and%20color%20variation.%20Furthermore%2C%0Athe%20number%20of%20images%20per%20scene%20varies%20significantly%2C%20introducing%20the%20additional%0Achallenge%20of%20limited%20training%20data%20for%20some%20instances%20alongside%20scenes%20with%20a%0Astandard%20number%20of%20views.%20By%20evaluating%20popular%203D%20reconstruction%20methods%20on%0Athis%20diverse%20dataset%2C%20we%20demonstrate%20the%20distinctiveness%20of%20HuSc3D%20in%0Aeffectively%20differentiating%20model%20performance%2C%20particularly%20highlighting%20the%0Asensitivity%20of%20methods%20to%20fine%20geometric%20details%2C%20color%20ambiguity%2C%20and%20varying%0Adata%20availability--limitations%20often%20masked%20by%20more%20conventional%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07628v1&entry.124074799=Read"},
{"title": "SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object\n  Detection", "author": "Phi Vu Tran", "abstract": "  While modern visual recognition systems have made significant advancements,\nmany continue to struggle with the open problem of learning from few exemplars.\nThis paper focuses on the task of object detection in the setting where object\nclasses follow a natural long-tailed distribution. Existing methods for\nlong-tailed detection resort to external ImageNet labels to augment the\nlow-shot training instances. However, such dependency on a large labeled\ndatabase has limited utility in practical scenarios. We propose a versatile and\nscalable approach to leverage optional unlabeled images, which are easy to\ncollect without the burden of human annotations. Our SimLTD framework is\nstraightforward and intuitive, and consists of three simple steps: (1)\npre-training on abundant head classes; (2) transfer learning on scarce tail\nclasses; and (3) fine-tuning on a sampled set of both head and tail classes.\nOur approach can be viewed as an improved head-to-tail model transfer paradigm\nwithout the added complexities of meta-learning or knowledge distillation, as\nwas required in past research. By harnessing supplementary unlabeled images,\nwithout extra image labels, SimLTD establishes new record results on the\nchallenging LVIS v1 benchmark across both supervised and semi-supervised\nsettings.\n", "link": "http://arxiv.org/abs/2412.20047v3", "date": "2025-06-09", "relevancy": 2.9609, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6087}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5853}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimLTD%3A%20Simple%20Supervised%20and%20Semi-Supervised%20Long-Tailed%20Object%0A%20%20Detection&body=Title%3A%20SimLTD%3A%20Simple%20Supervised%20and%20Semi-Supervised%20Long-Tailed%20Object%0A%20%20Detection%0AAuthor%3A%20Phi%20Vu%20Tran%0AAbstract%3A%20%20%20While%20modern%20visual%20recognition%20systems%20have%20made%20significant%20advancements%2C%0Amany%20continue%20to%20struggle%20with%20the%20open%20problem%20of%20learning%20from%20few%20exemplars.%0AThis%20paper%20focuses%20on%20the%20task%20of%20object%20detection%20in%20the%20setting%20where%20object%0Aclasses%20follow%20a%20natural%20long-tailed%20distribution.%20Existing%20methods%20for%0Along-tailed%20detection%20resort%20to%20external%20ImageNet%20labels%20to%20augment%20the%0Alow-shot%20training%20instances.%20However%2C%20such%20dependency%20on%20a%20large%20labeled%0Adatabase%20has%20limited%20utility%20in%20practical%20scenarios.%20We%20propose%20a%20versatile%20and%0Ascalable%20approach%20to%20leverage%20optional%20unlabeled%20images%2C%20which%20are%20easy%20to%0Acollect%20without%20the%20burden%20of%20human%20annotations.%20Our%20SimLTD%20framework%20is%0Astraightforward%20and%20intuitive%2C%20and%20consists%20of%20three%20simple%20steps%3A%20%281%29%0Apre-training%20on%20abundant%20head%20classes%3B%20%282%29%20transfer%20learning%20on%20scarce%20tail%0Aclasses%3B%20and%20%283%29%20fine-tuning%20on%20a%20sampled%20set%20of%20both%20head%20and%20tail%20classes.%0AOur%20approach%20can%20be%20viewed%20as%20an%20improved%20head-to-tail%20model%20transfer%20paradigm%0Awithout%20the%20added%20complexities%20of%20meta-learning%20or%20knowledge%20distillation%2C%20as%0Awas%20required%20in%20past%20research.%20By%20harnessing%20supplementary%20unlabeled%20images%2C%0Awithout%20extra%20image%20labels%2C%20SimLTD%20establishes%20new%20record%20results%20on%20the%0Achallenging%20LVIS%20v1%20benchmark%20across%20both%20supervised%20and%20semi-supervised%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20047v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimLTD%253A%2520Simple%2520Supervised%2520and%2520Semi-Supervised%2520Long-Tailed%2520Object%250A%2520%2520Detection%26entry.906535625%3DPhi%2520Vu%2520Tran%26entry.1292438233%3D%2520%2520While%2520modern%2520visual%2520recognition%2520systems%2520have%2520made%2520significant%2520advancements%252C%250Amany%2520continue%2520to%2520struggle%2520with%2520the%2520open%2520problem%2520of%2520learning%2520from%2520few%2520exemplars.%250AThis%2520paper%2520focuses%2520on%2520the%2520task%2520of%2520object%2520detection%2520in%2520the%2520setting%2520where%2520object%250Aclasses%2520follow%2520a%2520natural%2520long-tailed%2520distribution.%2520Existing%2520methods%2520for%250Along-tailed%2520detection%2520resort%2520to%2520external%2520ImageNet%2520labels%2520to%2520augment%2520the%250Alow-shot%2520training%2520instances.%2520However%252C%2520such%2520dependency%2520on%2520a%2520large%2520labeled%250Adatabase%2520has%2520limited%2520utility%2520in%2520practical%2520scenarios.%2520We%2520propose%2520a%2520versatile%2520and%250Ascalable%2520approach%2520to%2520leverage%2520optional%2520unlabeled%2520images%252C%2520which%2520are%2520easy%2520to%250Acollect%2520without%2520the%2520burden%2520of%2520human%2520annotations.%2520Our%2520SimLTD%2520framework%2520is%250Astraightforward%2520and%2520intuitive%252C%2520and%2520consists%2520of%2520three%2520simple%2520steps%253A%2520%25281%2529%250Apre-training%2520on%2520abundant%2520head%2520classes%253B%2520%25282%2529%2520transfer%2520learning%2520on%2520scarce%2520tail%250Aclasses%253B%2520and%2520%25283%2529%2520fine-tuning%2520on%2520a%2520sampled%2520set%2520of%2520both%2520head%2520and%2520tail%2520classes.%250AOur%2520approach%2520can%2520be%2520viewed%2520as%2520an%2520improved%2520head-to-tail%2520model%2520transfer%2520paradigm%250Awithout%2520the%2520added%2520complexities%2520of%2520meta-learning%2520or%2520knowledge%2520distillation%252C%2520as%250Awas%2520required%2520in%2520past%2520research.%2520By%2520harnessing%2520supplementary%2520unlabeled%2520images%252C%250Awithout%2520extra%2520image%2520labels%252C%2520SimLTD%2520establishes%2520new%2520record%2520results%2520on%2520the%250Achallenging%2520LVIS%2520v1%2520benchmark%2520across%2520both%2520supervised%2520and%2520semi-supervised%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20047v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimLTD%3A%20Simple%20Supervised%20and%20Semi-Supervised%20Long-Tailed%20Object%0A%20%20Detection&entry.906535625=Phi%20Vu%20Tran&entry.1292438233=%20%20While%20modern%20visual%20recognition%20systems%20have%20made%20significant%20advancements%2C%0Amany%20continue%20to%20struggle%20with%20the%20open%20problem%20of%20learning%20from%20few%20exemplars.%0AThis%20paper%20focuses%20on%20the%20task%20of%20object%20detection%20in%20the%20setting%20where%20object%0Aclasses%20follow%20a%20natural%20long-tailed%20distribution.%20Existing%20methods%20for%0Along-tailed%20detection%20resort%20to%20external%20ImageNet%20labels%20to%20augment%20the%0Alow-shot%20training%20instances.%20However%2C%20such%20dependency%20on%20a%20large%20labeled%0Adatabase%20has%20limited%20utility%20in%20practical%20scenarios.%20We%20propose%20a%20versatile%20and%0Ascalable%20approach%20to%20leverage%20optional%20unlabeled%20images%2C%20which%20are%20easy%20to%0Acollect%20without%20the%20burden%20of%20human%20annotations.%20Our%20SimLTD%20framework%20is%0Astraightforward%20and%20intuitive%2C%20and%20consists%20of%20three%20simple%20steps%3A%20%281%29%0Apre-training%20on%20abundant%20head%20classes%3B%20%282%29%20transfer%20learning%20on%20scarce%20tail%0Aclasses%3B%20and%20%283%29%20fine-tuning%20on%20a%20sampled%20set%20of%20both%20head%20and%20tail%20classes.%0AOur%20approach%20can%20be%20viewed%20as%20an%20improved%20head-to-tail%20model%20transfer%20paradigm%0Awithout%20the%20added%20complexities%20of%20meta-learning%20or%20knowledge%20distillation%2C%20as%0Awas%20required%20in%20past%20research.%20By%20harnessing%20supplementary%20unlabeled%20images%2C%0Awithout%20extra%20image%20labels%2C%20SimLTD%20establishes%20new%20record%20results%20on%20the%0Achallenging%20LVIS%20v1%20benchmark%20across%20both%20supervised%20and%20semi-supervised%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20047v3&entry.124074799=Read"},
{"title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in\n  Vision-Language Models", "author": "Chengyue Huang and Yuchen Zhu and Sichen Zhu and Jingyun Xiao and Moises Andrade and Shivang Chopra and Zsolt Kira", "abstract": "  Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.\n", "link": "http://arxiv.org/abs/2506.07936v1", "date": "2025-06-09", "relevancy": 2.9584, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mimicking%20or%20Reasoning%3A%20Rethinking%20Multi-Modal%20In-Context%20Learning%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Mimicking%20or%20Reasoning%3A%20Rethinking%20Multi-Modal%20In-Context%20Learning%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Chengyue%20Huang%20and%20Yuchen%20Zhu%20and%20Sichen%20Zhu%20and%20Jingyun%20Xiao%20and%20Moises%20Andrade%20and%20Shivang%20Chopra%20and%20Zsolt%20Kira%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20are%20widely%20assumed%20to%20exhibit%20in-context%0Alearning%20%28ICL%29%2C%20a%20property%20similar%20to%20that%20of%20their%20language-only%20counterparts.%0AWhile%20recent%20work%20suggests%20VLMs%20can%20perform%20multimodal%20ICL%20%28MM-ICL%29%2C%20studies%0Ashow%20they%20often%20rely%20on%20shallow%20heuristics%20--%20such%20as%20copying%20or%20majority%0Avoting%20--%20rather%20than%20true%20task%20understanding.%20We%20revisit%20this%20assumption%20by%0Aevaluating%20VLMs%20under%20distribution%20shifts%2C%20where%20support%20examples%20come%20from%20a%0Adataset%20different%20from%20the%20query.%20Surprisingly%2C%20performance%20often%20degrades%20with%0Amore%20demonstrations%2C%20and%20models%20tend%20to%20copy%20answers%20rather%20than%20learn%20from%0Athem.%20To%20investigate%20further%2C%20we%20propose%20a%20new%20MM-ICL%20with%20Reasoning%20pipeline%0Athat%20augments%20each%20demonstration%20with%20a%20generated%20rationale%20alongside%20the%0Aanswer.%20We%20conduct%20extensive%20and%20comprehensive%20experiments%20on%20both%20perception-%0Aand%20reasoning-required%20datasets%20with%20open-source%20VLMs%20ranging%20from%203B%20to%2072B%0Aand%20proprietary%20models%20such%20as%20Gemini%202.0.%20We%20conduct%20controlled%20studies%0Avarying%20shot%20count%2C%20retrieval%20method%2C%20rationale%20quality%2C%20and%20distribution.%20Our%0Aresults%20show%20limited%20performance%20sensitivity%20across%20these%20factors%2C%20suggesting%0Athat%20current%20VLMs%20do%20not%20effectively%20utilize%20demonstration-level%20information%20as%0Aintended%20in%20MM-ICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimicking%2520or%2520Reasoning%253A%2520Rethinking%2520Multi-Modal%2520In-Context%2520Learning%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DChengyue%2520Huang%2520and%2520Yuchen%2520Zhu%2520and%2520Sichen%2520Zhu%2520and%2520Jingyun%2520Xiao%2520and%2520Moises%2520Andrade%2520and%2520Shivang%2520Chopra%2520and%2520Zsolt%2520Kira%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520are%2520widely%2520assumed%2520to%2520exhibit%2520in-context%250Alearning%2520%2528ICL%2529%252C%2520a%2520property%2520similar%2520to%2520that%2520of%2520their%2520language-only%2520counterparts.%250AWhile%2520recent%2520work%2520suggests%2520VLMs%2520can%2520perform%2520multimodal%2520ICL%2520%2528MM-ICL%2529%252C%2520studies%250Ashow%2520they%2520often%2520rely%2520on%2520shallow%2520heuristics%2520--%2520such%2520as%2520copying%2520or%2520majority%250Avoting%2520--%2520rather%2520than%2520true%2520task%2520understanding.%2520We%2520revisit%2520this%2520assumption%2520by%250Aevaluating%2520VLMs%2520under%2520distribution%2520shifts%252C%2520where%2520support%2520examples%2520come%2520from%2520a%250Adataset%2520different%2520from%2520the%2520query.%2520Surprisingly%252C%2520performance%2520often%2520degrades%2520with%250Amore%2520demonstrations%252C%2520and%2520models%2520tend%2520to%2520copy%2520answers%2520rather%2520than%2520learn%2520from%250Athem.%2520To%2520investigate%2520further%252C%2520we%2520propose%2520a%2520new%2520MM-ICL%2520with%2520Reasoning%2520pipeline%250Athat%2520augments%2520each%2520demonstration%2520with%2520a%2520generated%2520rationale%2520alongside%2520the%250Aanswer.%2520We%2520conduct%2520extensive%2520and%2520comprehensive%2520experiments%2520on%2520both%2520perception-%250Aand%2520reasoning-required%2520datasets%2520with%2520open-source%2520VLMs%2520ranging%2520from%25203B%2520to%252072B%250Aand%2520proprietary%2520models%2520such%2520as%2520Gemini%25202.0.%2520We%2520conduct%2520controlled%2520studies%250Avarying%2520shot%2520count%252C%2520retrieval%2520method%252C%2520rationale%2520quality%252C%2520and%2520distribution.%2520Our%250Aresults%2520show%2520limited%2520performance%2520sensitivity%2520across%2520these%2520factors%252C%2520suggesting%250Athat%2520current%2520VLMs%2520do%2520not%2520effectively%2520utilize%2520demonstration-level%2520information%2520as%250Aintended%2520in%2520MM-ICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mimicking%20or%20Reasoning%3A%20Rethinking%20Multi-Modal%20In-Context%20Learning%20in%0A%20%20Vision-Language%20Models&entry.906535625=Chengyue%20Huang%20and%20Yuchen%20Zhu%20and%20Sichen%20Zhu%20and%20Jingyun%20Xiao%20and%20Moises%20Andrade%20and%20Shivang%20Chopra%20and%20Zsolt%20Kira&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20are%20widely%20assumed%20to%20exhibit%20in-context%0Alearning%20%28ICL%29%2C%20a%20property%20similar%20to%20that%20of%20their%20language-only%20counterparts.%0AWhile%20recent%20work%20suggests%20VLMs%20can%20perform%20multimodal%20ICL%20%28MM-ICL%29%2C%20studies%0Ashow%20they%20often%20rely%20on%20shallow%20heuristics%20--%20such%20as%20copying%20or%20majority%0Avoting%20--%20rather%20than%20true%20task%20understanding.%20We%20revisit%20this%20assumption%20by%0Aevaluating%20VLMs%20under%20distribution%20shifts%2C%20where%20support%20examples%20come%20from%20a%0Adataset%20different%20from%20the%20query.%20Surprisingly%2C%20performance%20often%20degrades%20with%0Amore%20demonstrations%2C%20and%20models%20tend%20to%20copy%20answers%20rather%20than%20learn%20from%0Athem.%20To%20investigate%20further%2C%20we%20propose%20a%20new%20MM-ICL%20with%20Reasoning%20pipeline%0Athat%20augments%20each%20demonstration%20with%20a%20generated%20rationale%20alongside%20the%0Aanswer.%20We%20conduct%20extensive%20and%20comprehensive%20experiments%20on%20both%20perception-%0Aand%20reasoning-required%20datasets%20with%20open-source%20VLMs%20ranging%20from%203B%20to%2072B%0Aand%20proprietary%20models%20such%20as%20Gemini%202.0.%20We%20conduct%20controlled%20studies%0Avarying%20shot%20count%2C%20retrieval%20method%2C%20rationale%20quality%2C%20and%20distribution.%20Our%0Aresults%20show%20limited%20performance%20sensitivity%20across%20these%20factors%2C%20suggesting%0Athat%20current%20VLMs%20do%20not%20effectively%20utilize%20demonstration-level%20information%20as%0Aintended%20in%20MM-ICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07936v1&entry.124074799=Read"},
{"title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models", "author": "Peiyan Li and Yixiang Chen and Hongtao Wu and Xiao Ma and Xiangnan Wu and Yan Huang and Liang Wang and Tao Kong and Tieniu Tan", "abstract": "  Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/\n", "link": "http://arxiv.org/abs/2506.07961v1", "date": "2025-06-09", "relevancy": 2.9408, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BridgeVLA%3A%20Input-Output%20Alignment%20for%20Efficient%203D%20Manipulation%20Learning%0A%20%20with%20Vision-Language%20Models&body=Title%3A%20BridgeVLA%3A%20Input-Output%20Alignment%20for%20Efficient%203D%20Manipulation%20Learning%0A%20%20with%20Vision-Language%20Models%0AAuthor%3A%20Peiyan%20Li%20and%20Yixiang%20Chen%20and%20Hongtao%20Wu%20and%20Xiao%20Ma%20and%20Xiangnan%20Wu%20and%20Yan%20Huang%20and%20Liang%20Wang%20and%20Tao%20Kong%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Recently%2C%20leveraging%20pre-trained%20vision-language%20models%20%28VLMs%29%20for%20building%0Avision-language-action%20%28VLA%29%20models%20has%20emerged%20as%20a%20promising%20approach%20to%0Aeffective%20robot%20manipulation%20learning.%20However%2C%20only%20few%20methods%20incorporate%203D%0Asignals%20into%20VLMs%20for%20action%20prediction%2C%20and%20they%20do%20not%20fully%20leverage%20the%0Aspatial%20structure%20inherent%20in%203D%20data%2C%20leading%20to%20low%20sample%20efficiency.%20In%0Athis%20paper%2C%20we%20introduce%20BridgeVLA%2C%20a%20novel%203D%20VLA%20model%20that%20%281%29%20projects%203D%0Ainputs%20to%20multiple%202D%20images%2C%20ensuring%20input%20alignment%20with%20the%20VLM%20backbone%2C%0Aand%20%282%29%20utilizes%202D%20heatmaps%20for%20action%20prediction%2C%20unifying%20the%20input%20and%0Aoutput%20spaces%20within%20a%20consistent%202D%20image%20space.%20In%20addition%2C%20we%20propose%20a%0Ascalable%20pre-training%20method%20that%20equips%20the%20VLM%20backbone%20with%20the%20capability%0Ato%20predict%202D%20heatmaps%20before%20downstream%20policy%20learning.%20Extensive%20experiments%0Ashow%20the%20proposed%20method%20is%20able%20to%20learn%203D%20manipulation%20efficiently%20and%0Aeffectively.%20BridgeVLA%20outperforms%20state-of-the-art%20baseline%20methods%20across%0Athree%20simulation%20benchmarks.%20In%20RLBench%2C%20it%20improves%20the%20average%20success%20rate%0Afrom%2081.4%25%20to%2088.2%25.%20In%20COLOSSEUM%2C%20it%20demonstrates%20significantly%20better%0Aperformance%20in%20challenging%20generalization%20settings%2C%20boosting%20the%20average%0Asuccess%20rate%20from%2056.7%25%20to%2064.0%25.%20In%20GemBench%2C%20it%20surpasses%20all%20the%20comparing%0Abaseline%20methods%20in%20terms%20of%20average%20success%20rate.%20In%20real-robot%20experiments%2C%0ABridgeVLA%20outperforms%20a%20state-of-the-art%20baseline%20method%20by%2032%25%20on%20average.%20It%0Ageneralizes%20robustly%20in%20multiple%20out-of-distribution%20settings%2C%20including%20visual%0Adisturbances%20and%20unseen%20instructions.%20Remarkably%2C%20it%20is%20able%20to%20achieve%20a%0Asuccess%20rate%20of%2096.8%25%20on%2010%2B%20tasks%20with%20only%203%20trajectories%20per%20task%2C%0Ahighlighting%20its%20extraordinary%20sample%20efficiency.%20Project%0AWebsite%3Ahttps%3A//bridgevla.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridgeVLA%253A%2520Input-Output%2520Alignment%2520for%2520Efficient%25203D%2520Manipulation%2520Learning%250A%2520%2520with%2520Vision-Language%2520Models%26entry.906535625%3DPeiyan%2520Li%2520and%2520Yixiang%2520Chen%2520and%2520Hongtao%2520Wu%2520and%2520Xiao%2520Ma%2520and%2520Xiangnan%2520Wu%2520and%2520Yan%2520Huang%2520and%2520Liang%2520Wang%2520and%2520Tao%2520Kong%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Recently%252C%2520leveraging%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520building%250Avision-language-action%2520%2528VLA%2529%2520models%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%250Aeffective%2520robot%2520manipulation%2520learning.%2520However%252C%2520only%2520few%2520methods%2520incorporate%25203D%250Asignals%2520into%2520VLMs%2520for%2520action%2520prediction%252C%2520and%2520they%2520do%2520not%2520fully%2520leverage%2520the%250Aspatial%2520structure%2520inherent%2520in%25203D%2520data%252C%2520leading%2520to%2520low%2520sample%2520efficiency.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520BridgeVLA%252C%2520a%2520novel%25203D%2520VLA%2520model%2520that%2520%25281%2529%2520projects%25203D%250Ainputs%2520to%2520multiple%25202D%2520images%252C%2520ensuring%2520input%2520alignment%2520with%2520the%2520VLM%2520backbone%252C%250Aand%2520%25282%2529%2520utilizes%25202D%2520heatmaps%2520for%2520action%2520prediction%252C%2520unifying%2520the%2520input%2520and%250Aoutput%2520spaces%2520within%2520a%2520consistent%25202D%2520image%2520space.%2520In%2520addition%252C%2520we%2520propose%2520a%250Ascalable%2520pre-training%2520method%2520that%2520equips%2520the%2520VLM%2520backbone%2520with%2520the%2520capability%250Ato%2520predict%25202D%2520heatmaps%2520before%2520downstream%2520policy%2520learning.%2520Extensive%2520experiments%250Ashow%2520the%2520proposed%2520method%2520is%2520able%2520to%2520learn%25203D%2520manipulation%2520efficiently%2520and%250Aeffectively.%2520BridgeVLA%2520outperforms%2520state-of-the-art%2520baseline%2520methods%2520across%250Athree%2520simulation%2520benchmarks.%2520In%2520RLBench%252C%2520it%2520improves%2520the%2520average%2520success%2520rate%250Afrom%252081.4%2525%2520to%252088.2%2525.%2520In%2520COLOSSEUM%252C%2520it%2520demonstrates%2520significantly%2520better%250Aperformance%2520in%2520challenging%2520generalization%2520settings%252C%2520boosting%2520the%2520average%250Asuccess%2520rate%2520from%252056.7%2525%2520to%252064.0%2525.%2520In%2520GemBench%252C%2520it%2520surpasses%2520all%2520the%2520comparing%250Abaseline%2520methods%2520in%2520terms%2520of%2520average%2520success%2520rate.%2520In%2520real-robot%2520experiments%252C%250ABridgeVLA%2520outperforms%2520a%2520state-of-the-art%2520baseline%2520method%2520by%252032%2525%2520on%2520average.%2520It%250Ageneralizes%2520robustly%2520in%2520multiple%2520out-of-distribution%2520settings%252C%2520including%2520visual%250Adisturbances%2520and%2520unseen%2520instructions.%2520Remarkably%252C%2520it%2520is%2520able%2520to%2520achieve%2520a%250Asuccess%2520rate%2520of%252096.8%2525%2520on%252010%252B%2520tasks%2520with%2520only%25203%2520trajectories%2520per%2520task%252C%250Ahighlighting%2520its%2520extraordinary%2520sample%2520efficiency.%2520Project%250AWebsite%253Ahttps%253A//bridgevla.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BridgeVLA%3A%20Input-Output%20Alignment%20for%20Efficient%203D%20Manipulation%20Learning%0A%20%20with%20Vision-Language%20Models&entry.906535625=Peiyan%20Li%20and%20Yixiang%20Chen%20and%20Hongtao%20Wu%20and%20Xiao%20Ma%20and%20Xiangnan%20Wu%20and%20Yan%20Huang%20and%20Liang%20Wang%20and%20Tao%20Kong%20and%20Tieniu%20Tan&entry.1292438233=%20%20Recently%2C%20leveraging%20pre-trained%20vision-language%20models%20%28VLMs%29%20for%20building%0Avision-language-action%20%28VLA%29%20models%20has%20emerged%20as%20a%20promising%20approach%20to%0Aeffective%20robot%20manipulation%20learning.%20However%2C%20only%20few%20methods%20incorporate%203D%0Asignals%20into%20VLMs%20for%20action%20prediction%2C%20and%20they%20do%20not%20fully%20leverage%20the%0Aspatial%20structure%20inherent%20in%203D%20data%2C%20leading%20to%20low%20sample%20efficiency.%20In%0Athis%20paper%2C%20we%20introduce%20BridgeVLA%2C%20a%20novel%203D%20VLA%20model%20that%20%281%29%20projects%203D%0Ainputs%20to%20multiple%202D%20images%2C%20ensuring%20input%20alignment%20with%20the%20VLM%20backbone%2C%0Aand%20%282%29%20utilizes%202D%20heatmaps%20for%20action%20prediction%2C%20unifying%20the%20input%20and%0Aoutput%20spaces%20within%20a%20consistent%202D%20image%20space.%20In%20addition%2C%20we%20propose%20a%0Ascalable%20pre-training%20method%20that%20equips%20the%20VLM%20backbone%20with%20the%20capability%0Ato%20predict%202D%20heatmaps%20before%20downstream%20policy%20learning.%20Extensive%20experiments%0Ashow%20the%20proposed%20method%20is%20able%20to%20learn%203D%20manipulation%20efficiently%20and%0Aeffectively.%20BridgeVLA%20outperforms%20state-of-the-art%20baseline%20methods%20across%0Athree%20simulation%20benchmarks.%20In%20RLBench%2C%20it%20improves%20the%20average%20success%20rate%0Afrom%2081.4%25%20to%2088.2%25.%20In%20COLOSSEUM%2C%20it%20demonstrates%20significantly%20better%0Aperformance%20in%20challenging%20generalization%20settings%2C%20boosting%20the%20average%0Asuccess%20rate%20from%2056.7%25%20to%2064.0%25.%20In%20GemBench%2C%20it%20surpasses%20all%20the%20comparing%0Abaseline%20methods%20in%20terms%20of%20average%20success%20rate.%20In%20real-robot%20experiments%2C%0ABridgeVLA%20outperforms%20a%20state-of-the-art%20baseline%20method%20by%2032%25%20on%20average.%20It%0Ageneralizes%20robustly%20in%20multiple%20out-of-distribution%20settings%2C%20including%20visual%0Adisturbances%20and%20unseen%20instructions.%20Remarkably%2C%20it%20is%20able%20to%20achieve%20a%0Asuccess%20rate%20of%2096.8%25%20on%2010%2B%20tasks%20with%20only%203%20trajectories%20per%20task%2C%0Ahighlighting%20its%20extraordinary%20sample%20efficiency.%20Project%0AWebsite%3Ahttps%3A//bridgevla.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07961v1&entry.124074799=Read"},
{"title": "WeThink: Toward General-purpose Vision-Language Reasoning via\n  Reinforcement Learning", "author": "Jie Yang and Feipeng Ma and Zitian Wang and Dacheng Yin and Kang Rong and Fengyun Rao and Ruimao Zhang", "abstract": "  Building on the success of text-based reasoning models like DeepSeek-R1,\nextending these capabilities to multimodal reasoning holds great promise. While\nrecent works have attempted to adapt DeepSeek-R1-style reinforcement learning\n(RL) training paradigms to multimodal large language models (MLLM), focusing on\ndomain-specific tasks like math and visual perception, a critical question\nremains: How can we achieve the general-purpose visual-language reasoning\nthrough RL? To address this challenge, we make three key efforts: (1) A novel\nScalable Multimodal QA Synthesis pipeline that autonomously generates\ncontext-aware, reasoning-centric question-answer (QA) pairs directly from the\ngiven images. (2) The open-source WeThink dataset containing over 120K\nmultimodal QA pairs with annotated reasoning paths, curated from 18 diverse\ndataset sources and covering various question domains. (3) A comprehensive\nexploration of RL on our dataset, incorporating a hybrid reward mechanism that\ncombines rule-based verification with model-based assessment to optimize RL\ntraining efficiency across various task domains. Across 14 diverse MLLM\nbenchmarks, we demonstrate that our WeThink dataset significantly enhances\nperformance, from mathematical reasoning to diverse general multimodal tasks.\nMoreover, we show that our automated data pipeline can continuously increase\ndata diversity to further improve model performance.\n", "link": "http://arxiv.org/abs/2506.07905v1", "date": "2025-06-09", "relevancy": 2.9167, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeThink%3A%20Toward%20General-purpose%20Vision-Language%20Reasoning%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20WeThink%3A%20Toward%20General-purpose%20Vision-Language%20Reasoning%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Jie%20Yang%20and%20Feipeng%20Ma%20and%20Zitian%20Wang%20and%20Dacheng%20Yin%20and%20Kang%20Rong%20and%20Fengyun%20Rao%20and%20Ruimao%20Zhang%0AAbstract%3A%20%20%20Building%20on%20the%20success%20of%20text-based%20reasoning%20models%20like%20DeepSeek-R1%2C%0Aextending%20these%20capabilities%20to%20multimodal%20reasoning%20holds%20great%20promise.%20While%0Arecent%20works%20have%20attempted%20to%20adapt%20DeepSeek-R1-style%20reinforcement%20learning%0A%28RL%29%20training%20paradigms%20to%20multimodal%20large%20language%20models%20%28MLLM%29%2C%20focusing%20on%0Adomain-specific%20tasks%20like%20math%20and%20visual%20perception%2C%20a%20critical%20question%0Aremains%3A%20How%20can%20we%20achieve%20the%20general-purpose%20visual-language%20reasoning%0Athrough%20RL%3F%20To%20address%20this%20challenge%2C%20we%20make%20three%20key%20efforts%3A%20%281%29%20A%20novel%0AScalable%20Multimodal%20QA%20Synthesis%20pipeline%20that%20autonomously%20generates%0Acontext-aware%2C%20reasoning-centric%20question-answer%20%28QA%29%20pairs%20directly%20from%20the%0Agiven%20images.%20%282%29%20The%20open-source%20WeThink%20dataset%20containing%20over%20120K%0Amultimodal%20QA%20pairs%20with%20annotated%20reasoning%20paths%2C%20curated%20from%2018%20diverse%0Adataset%20sources%20and%20covering%20various%20question%20domains.%20%283%29%20A%20comprehensive%0Aexploration%20of%20RL%20on%20our%20dataset%2C%20incorporating%20a%20hybrid%20reward%20mechanism%20that%0Acombines%20rule-based%20verification%20with%20model-based%20assessment%20to%20optimize%20RL%0Atraining%20efficiency%20across%20various%20task%20domains.%20Across%2014%20diverse%20MLLM%0Abenchmarks%2C%20we%20demonstrate%20that%20our%20WeThink%20dataset%20significantly%20enhances%0Aperformance%2C%20from%20mathematical%20reasoning%20to%20diverse%20general%20multimodal%20tasks.%0AMoreover%2C%20we%20show%20that%20our%20automated%20data%20pipeline%20can%20continuously%20increase%0Adata%20diversity%20to%20further%20improve%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeThink%253A%2520Toward%2520General-purpose%2520Vision-Language%2520Reasoning%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DJie%2520Yang%2520and%2520Feipeng%2520Ma%2520and%2520Zitian%2520Wang%2520and%2520Dacheng%2520Yin%2520and%2520Kang%2520Rong%2520and%2520Fengyun%2520Rao%2520and%2520Ruimao%2520Zhang%26entry.1292438233%3D%2520%2520Building%2520on%2520the%2520success%2520of%2520text-based%2520reasoning%2520models%2520like%2520DeepSeek-R1%252C%250Aextending%2520these%2520capabilities%2520to%2520multimodal%2520reasoning%2520holds%2520great%2520promise.%2520While%250Arecent%2520works%2520have%2520attempted%2520to%2520adapt%2520DeepSeek-R1-style%2520reinforcement%2520learning%250A%2528RL%2529%2520training%2520paradigms%2520to%2520multimodal%2520large%2520language%2520models%2520%2528MLLM%2529%252C%2520focusing%2520on%250Adomain-specific%2520tasks%2520like%2520math%2520and%2520visual%2520perception%252C%2520a%2520critical%2520question%250Aremains%253A%2520How%2520can%2520we%2520achieve%2520the%2520general-purpose%2520visual-language%2520reasoning%250Athrough%2520RL%253F%2520To%2520address%2520this%2520challenge%252C%2520we%2520make%2520three%2520key%2520efforts%253A%2520%25281%2529%2520A%2520novel%250AScalable%2520Multimodal%2520QA%2520Synthesis%2520pipeline%2520that%2520autonomously%2520generates%250Acontext-aware%252C%2520reasoning-centric%2520question-answer%2520%2528QA%2529%2520pairs%2520directly%2520from%2520the%250Agiven%2520images.%2520%25282%2529%2520The%2520open-source%2520WeThink%2520dataset%2520containing%2520over%2520120K%250Amultimodal%2520QA%2520pairs%2520with%2520annotated%2520reasoning%2520paths%252C%2520curated%2520from%252018%2520diverse%250Adataset%2520sources%2520and%2520covering%2520various%2520question%2520domains.%2520%25283%2529%2520A%2520comprehensive%250Aexploration%2520of%2520RL%2520on%2520our%2520dataset%252C%2520incorporating%2520a%2520hybrid%2520reward%2520mechanism%2520that%250Acombines%2520rule-based%2520verification%2520with%2520model-based%2520assessment%2520to%2520optimize%2520RL%250Atraining%2520efficiency%2520across%2520various%2520task%2520domains.%2520Across%252014%2520diverse%2520MLLM%250Abenchmarks%252C%2520we%2520demonstrate%2520that%2520our%2520WeThink%2520dataset%2520significantly%2520enhances%250Aperformance%252C%2520from%2520mathematical%2520reasoning%2520to%2520diverse%2520general%2520multimodal%2520tasks.%250AMoreover%252C%2520we%2520show%2520that%2520our%2520automated%2520data%2520pipeline%2520can%2520continuously%2520increase%250Adata%2520diversity%2520to%2520further%2520improve%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeThink%3A%20Toward%20General-purpose%20Vision-Language%20Reasoning%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Jie%20Yang%20and%20Feipeng%20Ma%20and%20Zitian%20Wang%20and%20Dacheng%20Yin%20and%20Kang%20Rong%20and%20Fengyun%20Rao%20and%20Ruimao%20Zhang&entry.1292438233=%20%20Building%20on%20the%20success%20of%20text-based%20reasoning%20models%20like%20DeepSeek-R1%2C%0Aextending%20these%20capabilities%20to%20multimodal%20reasoning%20holds%20great%20promise.%20While%0Arecent%20works%20have%20attempted%20to%20adapt%20DeepSeek-R1-style%20reinforcement%20learning%0A%28RL%29%20training%20paradigms%20to%20multimodal%20large%20language%20models%20%28MLLM%29%2C%20focusing%20on%0Adomain-specific%20tasks%20like%20math%20and%20visual%20perception%2C%20a%20critical%20question%0Aremains%3A%20How%20can%20we%20achieve%20the%20general-purpose%20visual-language%20reasoning%0Athrough%20RL%3F%20To%20address%20this%20challenge%2C%20we%20make%20three%20key%20efforts%3A%20%281%29%20A%20novel%0AScalable%20Multimodal%20QA%20Synthesis%20pipeline%20that%20autonomously%20generates%0Acontext-aware%2C%20reasoning-centric%20question-answer%20%28QA%29%20pairs%20directly%20from%20the%0Agiven%20images.%20%282%29%20The%20open-source%20WeThink%20dataset%20containing%20over%20120K%0Amultimodal%20QA%20pairs%20with%20annotated%20reasoning%20paths%2C%20curated%20from%2018%20diverse%0Adataset%20sources%20and%20covering%20various%20question%20domains.%20%283%29%20A%20comprehensive%0Aexploration%20of%20RL%20on%20our%20dataset%2C%20incorporating%20a%20hybrid%20reward%20mechanism%20that%0Acombines%20rule-based%20verification%20with%20model-based%20assessment%20to%20optimize%20RL%0Atraining%20efficiency%20across%20various%20task%20domains.%20Across%2014%20diverse%20MLLM%0Abenchmarks%2C%20we%20demonstrate%20that%20our%20WeThink%20dataset%20significantly%20enhances%0Aperformance%2C%20from%20mathematical%20reasoning%20to%20diverse%20general%20multimodal%20tasks.%0AMoreover%2C%20we%20show%20that%20our%20automated%20data%20pipeline%20can%20continuously%20increase%0Adata%20diversity%20to%20further%20improve%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07905v1&entry.124074799=Read"},
{"title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary", "author": "Kevin Qinghong Lin and Mike Zheng Shou", "abstract": "  Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's flexible upgrading over narration\nvocabulary. (ii) A hierarchical vocabulary derived from large-scale video\nnarrations using our narration pair encoding algorithm, enabling efficient\nindexing of specific events (e.g., cutting a tomato) by identifying broader\nscenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand).\n(iii) A vocabulary update strategy leveraging generative models to extend the\nvocabulary for novel events encountered during inference. To validate our\napproach, we introduce VidCap-Eval, a development set requiring concise\nnarrations with reasoning relationships (e.g., before and after). Experiments\non EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog,\nhighlighting its ability to generate concise, contextually accurate, and\nefficient narrations, offering a novel perspective on video understanding.\nCodes are released at https://github.com/showlab/VLog.\n", "link": "http://arxiv.org/abs/2503.09402v2", "date": "2025-06-09", "relevancy": 2.886, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLog%3A%20Video-Language%20Models%20by%20Generative%20Retrieval%20of%20Narration%0A%20%20Vocabulary&body=Title%3A%20VLog%3A%20Video-Language%20Models%20by%20Generative%20Retrieval%20of%20Narration%0A%20%20Vocabulary%0AAuthor%3A%20Kevin%20Qinghong%20Lin%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Human%20daily%20activities%20can%20be%20concisely%20narrated%20as%20sequences%20of%20routine%0Aevents%20%28e.g.%2C%20turning%20off%20an%20alarm%29%20in%20video%20streams%2C%20forming%20an%20event%0Avocabulary.%20Motivated%20by%20this%2C%20we%20introduce%20VLog%2C%20a%20novel%20video%20understanding%0Aframework%20that%20define%20video%20narrations%20as%20vocabulary%2C%20going%20beyond%20the%20typical%0Asubword%20vocabularies%20in%20existing%20generative%20video-language%20models.%20Built%20on%20the%0Alightweight%20language%20model%20GPT-2%2C%20VLog%20feature%20three%20key%20innovations%3A%20%28i%29%20A%0Agenerative%20retrieval%20model%2C%20marrying%20language%20model%27s%20complex%20reasoning%0Acapabilities%20with%20contrastive%20retrieval%27s%20flexible%20upgrading%20over%20narration%0Avocabulary.%20%28ii%29%20A%20hierarchical%20vocabulary%20derived%20from%20large-scale%20video%0Anarrations%20using%20our%20narration%20pair%20encoding%20algorithm%2C%20enabling%20efficient%0Aindexing%20of%20specific%20events%20%28e.g.%2C%20cutting%20a%20tomato%29%20by%20identifying%20broader%0Ascenarios%20%28e.g.%2C%20kitchen%29%20with%20expressive%20postfixes%20%28e.g.%2C%20by%20the%20left%20hand%29.%0A%28iii%29%20A%20vocabulary%20update%20strategy%20leveraging%20generative%20models%20to%20extend%20the%0Avocabulary%20for%20novel%20events%20encountered%20during%20inference.%20To%20validate%20our%0Aapproach%2C%20we%20introduce%20VidCap-Eval%2C%20a%20development%20set%20requiring%20concise%0Anarrations%20with%20reasoning%20relationships%20%28e.g.%2C%20before%20and%20after%29.%20Experiments%0Aon%20EgoSchema%2C%20COIN%2C%20and%20HiREST%20further%20demonstrate%20the%20effectiveness%20of%20VLog%2C%0Ahighlighting%20its%20ability%20to%20generate%20concise%2C%20contextually%20accurate%2C%20and%0Aefficient%20narrations%2C%20offering%20a%20novel%20perspective%20on%20video%20understanding.%0ACodes%20are%20released%20at%20https%3A//github.com/showlab/VLog.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLog%253A%2520Video-Language%2520Models%2520by%2520Generative%2520Retrieval%2520of%2520Narration%250A%2520%2520Vocabulary%26entry.906535625%3DKevin%2520Qinghong%2520Lin%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Human%2520daily%2520activities%2520can%2520be%2520concisely%2520narrated%2520as%2520sequences%2520of%2520routine%250Aevents%2520%2528e.g.%252C%2520turning%2520off%2520an%2520alarm%2529%2520in%2520video%2520streams%252C%2520forming%2520an%2520event%250Avocabulary.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520VLog%252C%2520a%2520novel%2520video%2520understanding%250Aframework%2520that%2520define%2520video%2520narrations%2520as%2520vocabulary%252C%2520going%2520beyond%2520the%2520typical%250Asubword%2520vocabularies%2520in%2520existing%2520generative%2520video-language%2520models.%2520Built%2520on%2520the%250Alightweight%2520language%2520model%2520GPT-2%252C%2520VLog%2520feature%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520A%250Agenerative%2520retrieval%2520model%252C%2520marrying%2520language%2520model%2527s%2520complex%2520reasoning%250Acapabilities%2520with%2520contrastive%2520retrieval%2527s%2520flexible%2520upgrading%2520over%2520narration%250Avocabulary.%2520%2528ii%2529%2520A%2520hierarchical%2520vocabulary%2520derived%2520from%2520large-scale%2520video%250Anarrations%2520using%2520our%2520narration%2520pair%2520encoding%2520algorithm%252C%2520enabling%2520efficient%250Aindexing%2520of%2520specific%2520events%2520%2528e.g.%252C%2520cutting%2520a%2520tomato%2529%2520by%2520identifying%2520broader%250Ascenarios%2520%2528e.g.%252C%2520kitchen%2529%2520with%2520expressive%2520postfixes%2520%2528e.g.%252C%2520by%2520the%2520left%2520hand%2529.%250A%2528iii%2529%2520A%2520vocabulary%2520update%2520strategy%2520leveraging%2520generative%2520models%2520to%2520extend%2520the%250Avocabulary%2520for%2520novel%2520events%2520encountered%2520during%2520inference.%2520To%2520validate%2520our%250Aapproach%252C%2520we%2520introduce%2520VidCap-Eval%252C%2520a%2520development%2520set%2520requiring%2520concise%250Anarrations%2520with%2520reasoning%2520relationships%2520%2528e.g.%252C%2520before%2520and%2520after%2529.%2520Experiments%250Aon%2520EgoSchema%252C%2520COIN%252C%2520and%2520HiREST%2520further%2520demonstrate%2520the%2520effectiveness%2520of%2520VLog%252C%250Ahighlighting%2520its%2520ability%2520to%2520generate%2520concise%252C%2520contextually%2520accurate%252C%2520and%250Aefficient%2520narrations%252C%2520offering%2520a%2520novel%2520perspective%2520on%2520video%2520understanding.%250ACodes%2520are%2520released%2520at%2520https%253A//github.com/showlab/VLog.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLog%3A%20Video-Language%20Models%20by%20Generative%20Retrieval%20of%20Narration%0A%20%20Vocabulary&entry.906535625=Kevin%20Qinghong%20Lin%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Human%20daily%20activities%20can%20be%20concisely%20narrated%20as%20sequences%20of%20routine%0Aevents%20%28e.g.%2C%20turning%20off%20an%20alarm%29%20in%20video%20streams%2C%20forming%20an%20event%0Avocabulary.%20Motivated%20by%20this%2C%20we%20introduce%20VLog%2C%20a%20novel%20video%20understanding%0Aframework%20that%20define%20video%20narrations%20as%20vocabulary%2C%20going%20beyond%20the%20typical%0Asubword%20vocabularies%20in%20existing%20generative%20video-language%20models.%20Built%20on%20the%0Alightweight%20language%20model%20GPT-2%2C%20VLog%20feature%20three%20key%20innovations%3A%20%28i%29%20A%0Agenerative%20retrieval%20model%2C%20marrying%20language%20model%27s%20complex%20reasoning%0Acapabilities%20with%20contrastive%20retrieval%27s%20flexible%20upgrading%20over%20narration%0Avocabulary.%20%28ii%29%20A%20hierarchical%20vocabulary%20derived%20from%20large-scale%20video%0Anarrations%20using%20our%20narration%20pair%20encoding%20algorithm%2C%20enabling%20efficient%0Aindexing%20of%20specific%20events%20%28e.g.%2C%20cutting%20a%20tomato%29%20by%20identifying%20broader%0Ascenarios%20%28e.g.%2C%20kitchen%29%20with%20expressive%20postfixes%20%28e.g.%2C%20by%20the%20left%20hand%29.%0A%28iii%29%20A%20vocabulary%20update%20strategy%20leveraging%20generative%20models%20to%20extend%20the%0Avocabulary%20for%20novel%20events%20encountered%20during%20inference.%20To%20validate%20our%0Aapproach%2C%20we%20introduce%20VidCap-Eval%2C%20a%20development%20set%20requiring%20concise%0Anarrations%20with%20reasoning%20relationships%20%28e.g.%2C%20before%20and%20after%29.%20Experiments%0Aon%20EgoSchema%2C%20COIN%2C%20and%20HiREST%20further%20demonstrate%20the%20effectiveness%20of%20VLog%2C%0Ahighlighting%20its%20ability%20to%20generate%20concise%2C%20contextually%20accurate%2C%20and%0Aefficient%20narrations%2C%20offering%20a%20novel%20perspective%20on%20video%20understanding.%0ACodes%20are%20released%20at%20https%3A//github.com/showlab/VLog.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09402v2&entry.124074799=Read"},
{"title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains", "author": "Chun Wang and Xiaoran Pan and Zihao Pan and Haofan Wang and Yiren Song", "abstract": "  Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.\n", "link": "http://arxiv.org/abs/2505.18700v2", "date": "2025-06-09", "relevancy": 2.8768, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRE%20Suite%3A%20Geo-localization%20Inference%20via%20Fine-Tuned%20Vision-Language%0A%20%20Models%20and%20Enhanced%20Reasoning%20Chains&body=Title%3A%20GRE%20Suite%3A%20Geo-localization%20Inference%20via%20Fine-Tuned%20Vision-Language%0A%20%20Models%20and%20Enhanced%20Reasoning%20Chains%0AAuthor%3A%20Chun%20Wang%20and%20Xiaoran%20Pan%20and%20Zihao%20Pan%20and%20Haofan%20Wang%20and%20Yiren%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20Visual%20Language%20Models%20%28VLMs%29%20have%20demonstrated%0Aexceptional%20performance%20in%20visual%20reasoning%20tasks.%20However%2C%20geo-localization%0Apresents%20unique%20challenges%2C%20requiring%20the%20extraction%20of%20multigranular%20visual%0Acues%20from%20images%20and%20their%20integration%20with%20external%20world%20knowledge%20for%0Asystematic%20reasoning.%20Current%20approaches%20to%20geo-localization%20tasks%20often%20lack%0Arobust%20reasoning%20mechanisms%20and%20explainability%2C%20limiting%20their%20effectiveness.%0ATo%20address%20these%20limitations%2C%20we%20propose%20the%20Geo%20Reason%20Enhancement%20%28GRE%29%0ASuite%2C%20a%20novel%20framework%20that%20augments%20VLMs%20with%20structured%20reasoning%20chains%0Afor%20accurate%20and%20interpretable%20location%20inference.%20The%20GRE%20Suite%20is%0Asystematically%20developed%20across%20three%20key%20dimensions%3A%20dataset%2C%20model%2C%20and%0Abenchmark.%20First%2C%20we%20introduce%20GRE30K%2C%20a%20high-quality%20geo-localization%0Areasoning%20dataset%20designed%20to%20facilitate%20fine-grained%20visual%20and%20contextual%0Aanalysis.%20Next%2C%20we%20present%20the%20GRE%20model%2C%20which%20employs%20a%20multi-stage%20reasoning%0Astrategy%20to%20progressively%20infer%20scene%20attributes%2C%20local%20details%2C%20and%20semantic%0Afeatures%2C%20thereby%20narrowing%20down%20potential%20geographic%20regions%20with%20enhanced%0Aprecision.%20Finally%2C%20we%20construct%20the%20Geo%20Reason%20Evaluation%20Benchmark%0A%28GREval-Bench%29%2C%20a%20comprehensive%20evaluation%20framework%20that%20assesses%20VLMs%20across%0Adiverse%20urban%2C%20natural%2C%20and%20landmark%20scenes%20to%20measure%20both%20coarse-grained%0A%28e.g.%2C%20country%2C%20continent%29%20and%20fine-grained%20%28e.g.%2C%20city%2C%20street%29%20localization%0Aperformance.%20Experimental%20results%20demonstrate%20that%20GRE%20significantly%0Aoutperforms%20existing%20methods%20across%20all%20granularities%20of%20geo-localization%0Atasks%2C%20underscoring%20the%20efficacy%20of%20reasoning-augmented%20VLMs%20in%20complex%0Ageographic%20inference.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//github.com/Thorin215/GRE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRE%2520Suite%253A%2520Geo-localization%2520Inference%2520via%2520Fine-Tuned%2520Vision-Language%250A%2520%2520Models%2520and%2520Enhanced%2520Reasoning%2520Chains%26entry.906535625%3DChun%2520Wang%2520and%2520Xiaoran%2520Pan%2520and%2520Zihao%2520Pan%2520and%2520Haofan%2520Wang%2520and%2520Yiren%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%250Aexceptional%2520performance%2520in%2520visual%2520reasoning%2520tasks.%2520However%252C%2520geo-localization%250Apresents%2520unique%2520challenges%252C%2520requiring%2520the%2520extraction%2520of%2520multigranular%2520visual%250Acues%2520from%2520images%2520and%2520their%2520integration%2520with%2520external%2520world%2520knowledge%2520for%250Asystematic%2520reasoning.%2520Current%2520approaches%2520to%2520geo-localization%2520tasks%2520often%2520lack%250Arobust%2520reasoning%2520mechanisms%2520and%2520explainability%252C%2520limiting%2520their%2520effectiveness.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Geo%2520Reason%2520Enhancement%2520%2528GRE%2529%250ASuite%252C%2520a%2520novel%2520framework%2520that%2520augments%2520VLMs%2520with%2520structured%2520reasoning%2520chains%250Afor%2520accurate%2520and%2520interpretable%2520location%2520inference.%2520The%2520GRE%2520Suite%2520is%250Asystematically%2520developed%2520across%2520three%2520key%2520dimensions%253A%2520dataset%252C%2520model%252C%2520and%250Abenchmark.%2520First%252C%2520we%2520introduce%2520GRE30K%252C%2520a%2520high-quality%2520geo-localization%250Areasoning%2520dataset%2520designed%2520to%2520facilitate%2520fine-grained%2520visual%2520and%2520contextual%250Aanalysis.%2520Next%252C%2520we%2520present%2520the%2520GRE%2520model%252C%2520which%2520employs%2520a%2520multi-stage%2520reasoning%250Astrategy%2520to%2520progressively%2520infer%2520scene%2520attributes%252C%2520local%2520details%252C%2520and%2520semantic%250Afeatures%252C%2520thereby%2520narrowing%2520down%2520potential%2520geographic%2520regions%2520with%2520enhanced%250Aprecision.%2520Finally%252C%2520we%2520construct%2520the%2520Geo%2520Reason%2520Evaluation%2520Benchmark%250A%2528GREval-Bench%2529%252C%2520a%2520comprehensive%2520evaluation%2520framework%2520that%2520assesses%2520VLMs%2520across%250Adiverse%2520urban%252C%2520natural%252C%2520and%2520landmark%2520scenes%2520to%2520measure%2520both%2520coarse-grained%250A%2528e.g.%252C%2520country%252C%2520continent%2529%2520and%2520fine-grained%2520%2528e.g.%252C%2520city%252C%2520street%2529%2520localization%250Aperformance.%2520Experimental%2520results%2520demonstrate%2520that%2520GRE%2520significantly%250Aoutperforms%2520existing%2520methods%2520across%2520all%2520granularities%2520of%2520geo-localization%250Atasks%252C%2520underscoring%2520the%2520efficacy%2520of%2520reasoning-augmented%2520VLMs%2520in%2520complex%250Ageographic%2520inference.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/Thorin215/GRE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRE%20Suite%3A%20Geo-localization%20Inference%20via%20Fine-Tuned%20Vision-Language%0A%20%20Models%20and%20Enhanced%20Reasoning%20Chains&entry.906535625=Chun%20Wang%20and%20Xiaoran%20Pan%20and%20Zihao%20Pan%20and%20Haofan%20Wang%20and%20Yiren%20Song&entry.1292438233=%20%20Recent%20advances%20in%20Visual%20Language%20Models%20%28VLMs%29%20have%20demonstrated%0Aexceptional%20performance%20in%20visual%20reasoning%20tasks.%20However%2C%20geo-localization%0Apresents%20unique%20challenges%2C%20requiring%20the%20extraction%20of%20multigranular%20visual%0Acues%20from%20images%20and%20their%20integration%20with%20external%20world%20knowledge%20for%0Asystematic%20reasoning.%20Current%20approaches%20to%20geo-localization%20tasks%20often%20lack%0Arobust%20reasoning%20mechanisms%20and%20explainability%2C%20limiting%20their%20effectiveness.%0ATo%20address%20these%20limitations%2C%20we%20propose%20the%20Geo%20Reason%20Enhancement%20%28GRE%29%0ASuite%2C%20a%20novel%20framework%20that%20augments%20VLMs%20with%20structured%20reasoning%20chains%0Afor%20accurate%20and%20interpretable%20location%20inference.%20The%20GRE%20Suite%20is%0Asystematically%20developed%20across%20three%20key%20dimensions%3A%20dataset%2C%20model%2C%20and%0Abenchmark.%20First%2C%20we%20introduce%20GRE30K%2C%20a%20high-quality%20geo-localization%0Areasoning%20dataset%20designed%20to%20facilitate%20fine-grained%20visual%20and%20contextual%0Aanalysis.%20Next%2C%20we%20present%20the%20GRE%20model%2C%20which%20employs%20a%20multi-stage%20reasoning%0Astrategy%20to%20progressively%20infer%20scene%20attributes%2C%20local%20details%2C%20and%20semantic%0Afeatures%2C%20thereby%20narrowing%20down%20potential%20geographic%20regions%20with%20enhanced%0Aprecision.%20Finally%2C%20we%20construct%20the%20Geo%20Reason%20Evaluation%20Benchmark%0A%28GREval-Bench%29%2C%20a%20comprehensive%20evaluation%20framework%20that%20assesses%20VLMs%20across%0Adiverse%20urban%2C%20natural%2C%20and%20landmark%20scenes%20to%20measure%20both%20coarse-grained%0A%28e.g.%2C%20country%2C%20continent%29%20and%20fine-grained%20%28e.g.%2C%20city%2C%20street%29%20localization%0Aperformance.%20Experimental%20results%20demonstrate%20that%20GRE%20significantly%0Aoutperforms%20existing%20methods%20across%20all%20granularities%20of%20geo-localization%0Atasks%2C%20underscoring%20the%20efficacy%20of%20reasoning-augmented%20VLMs%20in%20complex%0Ageographic%20inference.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//github.com/Thorin215/GRE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18700v2&entry.124074799=Read"},
{"title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning\n  Segmentation with Digital Twin Representations", "author": "Yizhen Li and Dell Zhang and Xuelong Li and Yiqing Shen", "abstract": "  Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.\n", "link": "http://arxiv.org/abs/2506.07943v1", "date": "2025-06-09", "relevancy": 2.872, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20the%20Image%20Perception%20and%20Multimodal%20Reasoning%20for%20Reasoning%0A%20%20Segmentation%20with%20Digital%20Twin%20Representations&body=Title%3A%20Decoupling%20the%20Image%20Perception%20and%20Multimodal%20Reasoning%20for%20Reasoning%0A%20%20Segmentation%20with%20Digital%20Twin%20Representations%0AAuthor%3A%20Yizhen%20Li%20and%20Dell%20Zhang%20and%20Xuelong%20Li%20and%20Yiqing%20Shen%0AAbstract%3A%20%20%20Reasoning%20Segmentation%20%28RS%29%20is%20a%20multimodal%20vision-text%20task%20that%20requires%0Asegmenting%20objects%20based%20on%20implicit%20text%20queries%2C%20demanding%20both%20precise%0Avisual%20perception%20and%20vision-text%20reasoning%20capabilities.%20Current%20RS%20approaches%0Arely%20on%20fine-tuning%20vision-language%20models%20%28VLMs%29%20for%20both%20perception%20and%0Areasoning%2C%20but%20their%20tokenization%20of%20images%20fundamentally%20disrupts%20continuous%0Aspatial%20relationships%20between%20objects.%20We%20introduce%20DTwinSeger%2C%20a%20novel%20RS%0Aapproach%20that%20leverages%20Digital%20Twin%20%28DT%29%20representation%20as%20an%20intermediate%0Alayer%20to%20decouple%20perception%20from%20reasoning.%20Innovatively%2C%20DTwinSeger%0Areformulates%20RS%20as%20a%20two-stage%20process%2C%20where%20the%20first%20transforms%20the%20image%0Ainto%20a%20structured%20DT%20representation%20that%20preserves%20spatial%20relationships%20and%0Asemantic%20properties%20and%20then%20employs%20a%20Large%20Language%20Model%20%28LLM%29%20to%20perform%0Aexplicit%20reasoning%20over%20this%20representation%20to%20identify%20target%20objects.%20We%0Apropose%20a%20supervised%20fine-tuning%20method%20specifically%20for%20LLM%20with%20DT%0Arepresentation%2C%20together%20with%20a%20corresponding%20fine-tuning%20dataset%20Seg-DT%2C%20to%0Aenhance%20the%20LLM%27s%20reasoning%20capabilities%20with%20DT%20representations.%20Experiments%0Ashow%20that%20our%20method%20can%20achieve%20state-of-the-art%20performance%20on%20two%20image%20RS%0Abenchmarks%20and%20three%20image%20referring%20segmentation%20benchmarks.%20It%20yields%20that%20DT%0Arepresentation%20functions%20as%20an%20effective%20bridge%20between%20vision%20and%20text%2C%0Aenabling%20complex%20multimodal%20reasoning%20tasks%20to%20be%20accomplished%20solely%20with%20an%0ALLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520the%2520Image%2520Perception%2520and%2520Multimodal%2520Reasoning%2520for%2520Reasoning%250A%2520%2520Segmentation%2520with%2520Digital%2520Twin%2520Representations%26entry.906535625%3DYizhen%2520Li%2520and%2520Dell%2520Zhang%2520and%2520Xuelong%2520Li%2520and%2520Yiqing%2520Shen%26entry.1292438233%3D%2520%2520Reasoning%2520Segmentation%2520%2528RS%2529%2520is%2520a%2520multimodal%2520vision-text%2520task%2520that%2520requires%250Asegmenting%2520objects%2520based%2520on%2520implicit%2520text%2520queries%252C%2520demanding%2520both%2520precise%250Avisual%2520perception%2520and%2520vision-text%2520reasoning%2520capabilities.%2520Current%2520RS%2520approaches%250Arely%2520on%2520fine-tuning%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520both%2520perception%2520and%250Areasoning%252C%2520but%2520their%2520tokenization%2520of%2520images%2520fundamentally%2520disrupts%2520continuous%250Aspatial%2520relationships%2520between%2520objects.%2520We%2520introduce%2520DTwinSeger%252C%2520a%2520novel%2520RS%250Aapproach%2520that%2520leverages%2520Digital%2520Twin%2520%2528DT%2529%2520representation%2520as%2520an%2520intermediate%250Alayer%2520to%2520decouple%2520perception%2520from%2520reasoning.%2520Innovatively%252C%2520DTwinSeger%250Areformulates%2520RS%2520as%2520a%2520two-stage%2520process%252C%2520where%2520the%2520first%2520transforms%2520the%2520image%250Ainto%2520a%2520structured%2520DT%2520representation%2520that%2520preserves%2520spatial%2520relationships%2520and%250Asemantic%2520properties%2520and%2520then%2520employs%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520perform%250Aexplicit%2520reasoning%2520over%2520this%2520representation%2520to%2520identify%2520target%2520objects.%2520We%250Apropose%2520a%2520supervised%2520fine-tuning%2520method%2520specifically%2520for%2520LLM%2520with%2520DT%250Arepresentation%252C%2520together%2520with%2520a%2520corresponding%2520fine-tuning%2520dataset%2520Seg-DT%252C%2520to%250Aenhance%2520the%2520LLM%2527s%2520reasoning%2520capabilities%2520with%2520DT%2520representations.%2520Experiments%250Ashow%2520that%2520our%2520method%2520can%2520achieve%2520state-of-the-art%2520performance%2520on%2520two%2520image%2520RS%250Abenchmarks%2520and%2520three%2520image%2520referring%2520segmentation%2520benchmarks.%2520It%2520yields%2520that%2520DT%250Arepresentation%2520functions%2520as%2520an%2520effective%2520bridge%2520between%2520vision%2520and%2520text%252C%250Aenabling%2520complex%2520multimodal%2520reasoning%2520tasks%2520to%2520be%2520accomplished%2520solely%2520with%2520an%250ALLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20the%20Image%20Perception%20and%20Multimodal%20Reasoning%20for%20Reasoning%0A%20%20Segmentation%20with%20Digital%20Twin%20Representations&entry.906535625=Yizhen%20Li%20and%20Dell%20Zhang%20and%20Xuelong%20Li%20and%20Yiqing%20Shen&entry.1292438233=%20%20Reasoning%20Segmentation%20%28RS%29%20is%20a%20multimodal%20vision-text%20task%20that%20requires%0Asegmenting%20objects%20based%20on%20implicit%20text%20queries%2C%20demanding%20both%20precise%0Avisual%20perception%20and%20vision-text%20reasoning%20capabilities.%20Current%20RS%20approaches%0Arely%20on%20fine-tuning%20vision-language%20models%20%28VLMs%29%20for%20both%20perception%20and%0Areasoning%2C%20but%20their%20tokenization%20of%20images%20fundamentally%20disrupts%20continuous%0Aspatial%20relationships%20between%20objects.%20We%20introduce%20DTwinSeger%2C%20a%20novel%20RS%0Aapproach%20that%20leverages%20Digital%20Twin%20%28DT%29%20representation%20as%20an%20intermediate%0Alayer%20to%20decouple%20perception%20from%20reasoning.%20Innovatively%2C%20DTwinSeger%0Areformulates%20RS%20as%20a%20two-stage%20process%2C%20where%20the%20first%20transforms%20the%20image%0Ainto%20a%20structured%20DT%20representation%20that%20preserves%20spatial%20relationships%20and%0Asemantic%20properties%20and%20then%20employs%20a%20Large%20Language%20Model%20%28LLM%29%20to%20perform%0Aexplicit%20reasoning%20over%20this%20representation%20to%20identify%20target%20objects.%20We%0Apropose%20a%20supervised%20fine-tuning%20method%20specifically%20for%20LLM%20with%20DT%0Arepresentation%2C%20together%20with%20a%20corresponding%20fine-tuning%20dataset%20Seg-DT%2C%20to%0Aenhance%20the%20LLM%27s%20reasoning%20capabilities%20with%20DT%20representations.%20Experiments%0Ashow%20that%20our%20method%20can%20achieve%20state-of-the-art%20performance%20on%20two%20image%20RS%0Abenchmarks%20and%20three%20image%20referring%20segmentation%20benchmarks.%20It%20yields%20that%20DT%0Arepresentation%20functions%20as%20an%20effective%20bridge%20between%20vision%20and%20text%2C%0Aenabling%20complex%20multimodal%20reasoning%20tasks%20to%20be%20accomplished%20solely%20with%20an%0ALLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07943v1&entry.124074799=Read"},
{"title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models\n  in Compositional Spatial Intelligence", "author": "Ziyang Gong and Wenhao Li and Oliver Ma and Songyuan Li and Jiayi Ji and Xue Yang and Gen Luo and Junchi Yan and Rongrong Ji", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.\n", "link": "http://arxiv.org/abs/2506.07966v1", "date": "2025-06-09", "relevancy": 2.8675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaCE-10%3A%20A%20Comprehensive%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0A%20%20in%20Compositional%20Spatial%20Intelligence&body=Title%3A%20SpaCE-10%3A%20A%20Comprehensive%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0A%20%20in%20Compositional%20Spatial%20Intelligence%0AAuthor%3A%20Ziyang%20Gong%20and%20Wenhao%20Li%20and%20Oliver%20Ma%20and%20Songyuan%20Li%20and%20Jiayi%20Ji%20and%20Xue%20Yang%20and%20Gen%20Luo%20and%20Junchi%20Yan%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20in%0Avarious%20multimodal%20tasks.%20To%20pursue%20higher%20intelligence%20in%20space%2C%20MLLMs%20require%0Aintegrating%20multiple%20atomic%20spatial%20capabilities%20to%20handle%20complex%20and%20dynamic%0Atasks.%20However%2C%20existing%20benchmarks%20struggle%20to%20comprehensively%20evaluate%20the%0Aspatial%20intelligence%20of%20common%20MLLMs%20from%20the%20atomic%20level%20to%20the%20compositional%0Alevel.%20To%20fill%20this%20gap%2C%20we%20present%20SpaCE-10%2C%20a%20comprehensive%20benchmark%20for%0Acompositional%20spatial%20evaluations.%20In%20SpaCE-10%2C%20we%20define%2010%20atomic%20spatial%0Acapabilities%2C%20which%20are%20combined%20to%20form%208%20compositional%20capabilities.%20Based%20on%0Athese%20definitions%2C%20we%20propose%20a%20novel%20hierarchical%20annotation%20pipeline%20to%0Agenerate%20high-quality%20and%20diverse%20question-answer%20%28QA%29%20pairs.%20With%20over%20150%2B%0Ahours%20of%20human%20expert%20effort%2C%20we%20obtain%20over%205k%20QA%20pairs%20for%20811%20real%20indoor%0Ascenes%20in%20SpaCE-10%2C%20which%20covers%20various%20evaluation%20settings%20like%20point%20cloud%0Ainput%20and%20multi-choice%20QA.%20We%20conduct%20an%20extensive%20evaluation%20of%20common%20MLLMs%0Aon%20SpaCE-10%20and%20find%20that%20even%20the%20most%20advanced%20MLLM%20still%20lags%20behind%20humans%0Aby%20large%20margins.%20Through%20our%20careful%20study%2C%20we%20also%20draw%20several%20significant%0Afindings%20that%20benefit%20the%20MLLM%20community.%20For%20example%2C%20we%20reveal%20that%20the%0Ashortcoming%20of%20counting%20capability%20greatly%20limits%20the%20compositional%20spatial%0Acapabilities%20of%20existing%20MLLMs.%20The%20evaluation%20code%20and%20benchmark%20datasets%20are%0Aavailable%20at%20https%3A//github.com/Cuzyoung/SpaCE-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaCE-10%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Multimodal%2520Large%2520Language%2520Models%250A%2520%2520in%2520Compositional%2520Spatial%2520Intelligence%26entry.906535625%3DZiyang%2520Gong%2520and%2520Wenhao%2520Li%2520and%2520Oliver%2520Ma%2520and%2520Songyuan%2520Li%2520and%2520Jiayi%2520Ji%2520and%2520Xue%2520Yang%2520and%2520Gen%2520Luo%2520and%2520Junchi%2520Yan%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%250Avarious%2520multimodal%2520tasks.%2520To%2520pursue%2520higher%2520intelligence%2520in%2520space%252C%2520MLLMs%2520require%250Aintegrating%2520multiple%2520atomic%2520spatial%2520capabilities%2520to%2520handle%2520complex%2520and%2520dynamic%250Atasks.%2520However%252C%2520existing%2520benchmarks%2520struggle%2520to%2520comprehensively%2520evaluate%2520the%250Aspatial%2520intelligence%2520of%2520common%2520MLLMs%2520from%2520the%2520atomic%2520level%2520to%2520the%2520compositional%250Alevel.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%2520SpaCE-10%252C%2520a%2520comprehensive%2520benchmark%2520for%250Acompositional%2520spatial%2520evaluations.%2520In%2520SpaCE-10%252C%2520we%2520define%252010%2520atomic%2520spatial%250Acapabilities%252C%2520which%2520are%2520combined%2520to%2520form%25208%2520compositional%2520capabilities.%2520Based%2520on%250Athese%2520definitions%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520annotation%2520pipeline%2520to%250Agenerate%2520high-quality%2520and%2520diverse%2520question-answer%2520%2528QA%2529%2520pairs.%2520With%2520over%2520150%252B%250Ahours%2520of%2520human%2520expert%2520effort%252C%2520we%2520obtain%2520over%25205k%2520QA%2520pairs%2520for%2520811%2520real%2520indoor%250Ascenes%2520in%2520SpaCE-10%252C%2520which%2520covers%2520various%2520evaluation%2520settings%2520like%2520point%2520cloud%250Ainput%2520and%2520multi-choice%2520QA.%2520We%2520conduct%2520an%2520extensive%2520evaluation%2520of%2520common%2520MLLMs%250Aon%2520SpaCE-10%2520and%2520find%2520that%2520even%2520the%2520most%2520advanced%2520MLLM%2520still%2520lags%2520behind%2520humans%250Aby%2520large%2520margins.%2520Through%2520our%2520careful%2520study%252C%2520we%2520also%2520draw%2520several%2520significant%250Afindings%2520that%2520benefit%2520the%2520MLLM%2520community.%2520For%2520example%252C%2520we%2520reveal%2520that%2520the%250Ashortcoming%2520of%2520counting%2520capability%2520greatly%2520limits%2520the%2520compositional%2520spatial%250Acapabilities%2520of%2520existing%2520MLLMs.%2520The%2520evaluation%2520code%2520and%2520benchmark%2520datasets%2520are%250Aavailable%2520at%2520https%253A//github.com/Cuzyoung/SpaCE-10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaCE-10%3A%20A%20Comprehensive%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0A%20%20in%20Compositional%20Spatial%20Intelligence&entry.906535625=Ziyang%20Gong%20and%20Wenhao%20Li%20and%20Oliver%20Ma%20and%20Songyuan%20Li%20and%20Jiayi%20Ji%20and%20Xue%20Yang%20and%20Gen%20Luo%20and%20Junchi%20Yan%20and%20Rongrong%20Ji&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20in%0Avarious%20multimodal%20tasks.%20To%20pursue%20higher%20intelligence%20in%20space%2C%20MLLMs%20require%0Aintegrating%20multiple%20atomic%20spatial%20capabilities%20to%20handle%20complex%20and%20dynamic%0Atasks.%20However%2C%20existing%20benchmarks%20struggle%20to%20comprehensively%20evaluate%20the%0Aspatial%20intelligence%20of%20common%20MLLMs%20from%20the%20atomic%20level%20to%20the%20compositional%0Alevel.%20To%20fill%20this%20gap%2C%20we%20present%20SpaCE-10%2C%20a%20comprehensive%20benchmark%20for%0Acompositional%20spatial%20evaluations.%20In%20SpaCE-10%2C%20we%20define%2010%20atomic%20spatial%0Acapabilities%2C%20which%20are%20combined%20to%20form%208%20compositional%20capabilities.%20Based%20on%0Athese%20definitions%2C%20we%20propose%20a%20novel%20hierarchical%20annotation%20pipeline%20to%0Agenerate%20high-quality%20and%20diverse%20question-answer%20%28QA%29%20pairs.%20With%20over%20150%2B%0Ahours%20of%20human%20expert%20effort%2C%20we%20obtain%20over%205k%20QA%20pairs%20for%20811%20real%20indoor%0Ascenes%20in%20SpaCE-10%2C%20which%20covers%20various%20evaluation%20settings%20like%20point%20cloud%0Ainput%20and%20multi-choice%20QA.%20We%20conduct%20an%20extensive%20evaluation%20of%20common%20MLLMs%0Aon%20SpaCE-10%20and%20find%20that%20even%20the%20most%20advanced%20MLLM%20still%20lags%20behind%20humans%0Aby%20large%20margins.%20Through%20our%20careful%20study%2C%20we%20also%20draw%20several%20significant%0Afindings%20that%20benefit%20the%20MLLM%20community.%20For%20example%2C%20we%20reveal%20that%20the%0Ashortcoming%20of%20counting%20capability%20greatly%20limits%20the%20compositional%20spatial%0Acapabilities%20of%20existing%20MLLMs.%20The%20evaluation%20code%20and%20benchmark%20datasets%20are%0Aavailable%20at%20https%3A//github.com/Cuzyoung/SpaCE-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07966v1&entry.124074799=Read"},
{"title": "ViVo: A Dataset for Volumetric Video Reconstruction and Compression", "author": "Adrian Azzarelli and Ge Gao and Ho Man Kwan and Fan Zhang and Nantheera Anantrasirichai and Ollie Moolan-Feroze and David Bull", "abstract": "  As research on neural volumetric video reconstruction and compression\nflourishes, there is a need for diverse and realistic datasets, which can be\nused to develop and validate reconstruction and compression models. However,\nexisting volumetric video datasets lack diverse content in terms of both\nsemantic and low-level features that are commonly present in real-world\nproduction pipelines. In this context, we propose a new dataset, ViVo, for\nVolumetrIc VideO reconstruction and compression. The dataset is faithful to\nreal-world volumetric video production and is the first dataset to extend the\ndefinition of diversity to include both human-centric characteristics (skin,\nhair, etc.) and dynamic visual phenomena (transparent, reflective, liquid,\netc.). Each video sequence in this database contains raw data including\nfourteen multi-view RGB and depth video pairs, synchronized at 30FPS with\nper-frame calibration and audio data, and their associated 2-D foreground masks\nand 3-D point clouds. To demonstrate the use of this database, we have\nbenchmarked three state-of-the-art (SotA) 3-D reconstruction methods and two\nvolumetric video compression algorithms. The obtained results evidence the\nchallenging nature of the proposed dataset and the limitations of existing\ndatasets for both volumetric video reconstruction and compression tasks,\nhighlighting the need to develop more effective algorithms for these\napplications. The database and the associated results are available at\nhttps://vivo-bvicr.github.io/\n", "link": "http://arxiv.org/abs/2506.00558v2", "date": "2025-06-09", "relevancy": 2.8655, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5794}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5794}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViVo%3A%20A%20Dataset%20for%20Volumetric%20Video%20Reconstruction%20and%20Compression&body=Title%3A%20ViVo%3A%20A%20Dataset%20for%20Volumetric%20Video%20Reconstruction%20and%20Compression%0AAuthor%3A%20Adrian%20Azzarelli%20and%20Ge%20Gao%20and%20Ho%20Man%20Kwan%20and%20Fan%20Zhang%20and%20Nantheera%20Anantrasirichai%20and%20Ollie%20Moolan-Feroze%20and%20David%20Bull%0AAbstract%3A%20%20%20As%20research%20on%20neural%20volumetric%20video%20reconstruction%20and%20compression%0Aflourishes%2C%20there%20is%20a%20need%20for%20diverse%20and%20realistic%20datasets%2C%20which%20can%20be%0Aused%20to%20develop%20and%20validate%20reconstruction%20and%20compression%20models.%20However%2C%0Aexisting%20volumetric%20video%20datasets%20lack%20diverse%20content%20in%20terms%20of%20both%0Asemantic%20and%20low-level%20features%20that%20are%20commonly%20present%20in%20real-world%0Aproduction%20pipelines.%20In%20this%20context%2C%20we%20propose%20a%20new%20dataset%2C%20ViVo%2C%20for%0AVolumetrIc%20VideO%20reconstruction%20and%20compression.%20The%20dataset%20is%20faithful%20to%0Areal-world%20volumetric%20video%20production%20and%20is%20the%20first%20dataset%20to%20extend%20the%0Adefinition%20of%20diversity%20to%20include%20both%20human-centric%20characteristics%20%28skin%2C%0Ahair%2C%20etc.%29%20and%20dynamic%20visual%20phenomena%20%28transparent%2C%20reflective%2C%20liquid%2C%0Aetc.%29.%20Each%20video%20sequence%20in%20this%20database%20contains%20raw%20data%20including%0Afourteen%20multi-view%20RGB%20and%20depth%20video%20pairs%2C%20synchronized%20at%2030FPS%20with%0Aper-frame%20calibration%20and%20audio%20data%2C%20and%20their%20associated%202-D%20foreground%20masks%0Aand%203-D%20point%20clouds.%20To%20demonstrate%20the%20use%20of%20this%20database%2C%20we%20have%0Abenchmarked%20three%20state-of-the-art%20%28SotA%29%203-D%20reconstruction%20methods%20and%20two%0Avolumetric%20video%20compression%20algorithms.%20The%20obtained%20results%20evidence%20the%0Achallenging%20nature%20of%20the%20proposed%20dataset%20and%20the%20limitations%20of%20existing%0Adatasets%20for%20both%20volumetric%20video%20reconstruction%20and%20compression%20tasks%2C%0Ahighlighting%20the%20need%20to%20develop%20more%20effective%20algorithms%20for%20these%0Aapplications.%20The%20database%20and%20the%20associated%20results%20are%20available%20at%0Ahttps%3A//vivo-bvicr.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViVo%253A%2520A%2520Dataset%2520for%2520Volumetric%2520Video%2520Reconstruction%2520and%2520Compression%26entry.906535625%3DAdrian%2520Azzarelli%2520and%2520Ge%2520Gao%2520and%2520Ho%2520Man%2520Kwan%2520and%2520Fan%2520Zhang%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520Ollie%2520Moolan-Feroze%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%2520As%2520research%2520on%2520neural%2520volumetric%2520video%2520reconstruction%2520and%2520compression%250Aflourishes%252C%2520there%2520is%2520a%2520need%2520for%2520diverse%2520and%2520realistic%2520datasets%252C%2520which%2520can%2520be%250Aused%2520to%2520develop%2520and%2520validate%2520reconstruction%2520and%2520compression%2520models.%2520However%252C%250Aexisting%2520volumetric%2520video%2520datasets%2520lack%2520diverse%2520content%2520in%2520terms%2520of%2520both%250Asemantic%2520and%2520low-level%2520features%2520that%2520are%2520commonly%2520present%2520in%2520real-world%250Aproduction%2520pipelines.%2520In%2520this%2520context%252C%2520we%2520propose%2520a%2520new%2520dataset%252C%2520ViVo%252C%2520for%250AVolumetrIc%2520VideO%2520reconstruction%2520and%2520compression.%2520The%2520dataset%2520is%2520faithful%2520to%250Areal-world%2520volumetric%2520video%2520production%2520and%2520is%2520the%2520first%2520dataset%2520to%2520extend%2520the%250Adefinition%2520of%2520diversity%2520to%2520include%2520both%2520human-centric%2520characteristics%2520%2528skin%252C%250Ahair%252C%2520etc.%2529%2520and%2520dynamic%2520visual%2520phenomena%2520%2528transparent%252C%2520reflective%252C%2520liquid%252C%250Aetc.%2529.%2520Each%2520video%2520sequence%2520in%2520this%2520database%2520contains%2520raw%2520data%2520including%250Afourteen%2520multi-view%2520RGB%2520and%2520depth%2520video%2520pairs%252C%2520synchronized%2520at%252030FPS%2520with%250Aper-frame%2520calibration%2520and%2520audio%2520data%252C%2520and%2520their%2520associated%25202-D%2520foreground%2520masks%250Aand%25203-D%2520point%2520clouds.%2520To%2520demonstrate%2520the%2520use%2520of%2520this%2520database%252C%2520we%2520have%250Abenchmarked%2520three%2520state-of-the-art%2520%2528SotA%2529%25203-D%2520reconstruction%2520methods%2520and%2520two%250Avolumetric%2520video%2520compression%2520algorithms.%2520The%2520obtained%2520results%2520evidence%2520the%250Achallenging%2520nature%2520of%2520the%2520proposed%2520dataset%2520and%2520the%2520limitations%2520of%2520existing%250Adatasets%2520for%2520both%2520volumetric%2520video%2520reconstruction%2520and%2520compression%2520tasks%252C%250Ahighlighting%2520the%2520need%2520to%2520develop%2520more%2520effective%2520algorithms%2520for%2520these%250Aapplications.%2520The%2520database%2520and%2520the%2520associated%2520results%2520are%2520available%2520at%250Ahttps%253A//vivo-bvicr.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViVo%3A%20A%20Dataset%20for%20Volumetric%20Video%20Reconstruction%20and%20Compression&entry.906535625=Adrian%20Azzarelli%20and%20Ge%20Gao%20and%20Ho%20Man%20Kwan%20and%20Fan%20Zhang%20and%20Nantheera%20Anantrasirichai%20and%20Ollie%20Moolan-Feroze%20and%20David%20Bull&entry.1292438233=%20%20As%20research%20on%20neural%20volumetric%20video%20reconstruction%20and%20compression%0Aflourishes%2C%20there%20is%20a%20need%20for%20diverse%20and%20realistic%20datasets%2C%20which%20can%20be%0Aused%20to%20develop%20and%20validate%20reconstruction%20and%20compression%20models.%20However%2C%0Aexisting%20volumetric%20video%20datasets%20lack%20diverse%20content%20in%20terms%20of%20both%0Asemantic%20and%20low-level%20features%20that%20are%20commonly%20present%20in%20real-world%0Aproduction%20pipelines.%20In%20this%20context%2C%20we%20propose%20a%20new%20dataset%2C%20ViVo%2C%20for%0AVolumetrIc%20VideO%20reconstruction%20and%20compression.%20The%20dataset%20is%20faithful%20to%0Areal-world%20volumetric%20video%20production%20and%20is%20the%20first%20dataset%20to%20extend%20the%0Adefinition%20of%20diversity%20to%20include%20both%20human-centric%20characteristics%20%28skin%2C%0Ahair%2C%20etc.%29%20and%20dynamic%20visual%20phenomena%20%28transparent%2C%20reflective%2C%20liquid%2C%0Aetc.%29.%20Each%20video%20sequence%20in%20this%20database%20contains%20raw%20data%20including%0Afourteen%20multi-view%20RGB%20and%20depth%20video%20pairs%2C%20synchronized%20at%2030FPS%20with%0Aper-frame%20calibration%20and%20audio%20data%2C%20and%20their%20associated%202-D%20foreground%20masks%0Aand%203-D%20point%20clouds.%20To%20demonstrate%20the%20use%20of%20this%20database%2C%20we%20have%0Abenchmarked%20three%20state-of-the-art%20%28SotA%29%203-D%20reconstruction%20methods%20and%20two%0Avolumetric%20video%20compression%20algorithms.%20The%20obtained%20results%20evidence%20the%0Achallenging%20nature%20of%20the%20proposed%20dataset%20and%20the%20limitations%20of%20existing%0Adatasets%20for%20both%20volumetric%20video%20reconstruction%20and%20compression%20tasks%2C%0Ahighlighting%20the%20need%20to%20develop%20more%20effective%20algorithms%20for%20these%0Aapplications.%20The%20database%20and%20the%20associated%20results%20are%20available%20at%0Ahttps%3A//vivo-bvicr.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00558v2&entry.124074799=Read"},
{"title": "Looking Beyond Visible Cues: Implicit Video Question Answering via\n  Dual-Clue Reasoning", "author": "Tieyuan Chen and Huabin Liu and Yi Wang and Chaofan Gan and Mingxi Lyu and Gui Zou and Weiyao Lin", "abstract": "  Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the given video, with prior work primarily focusing on identifying the\nduration of relevant segments, referred to as explicit visual evidence.\nHowever, explicit visual evidence is not always directly available,\nparticularly when questions target symbolic meanings or deeper intentions,\nleading to significant performance degradation. To fill this gap, we introduce\na novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo\n$\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering\nquestions in scenarios where explicit visual evidence is inaccessible. Given an\nimplicit question and its corresponding video, I-VQA requires answering based\non the contextual visual cues present within the video. To tackle I-VQA, we\npropose a novel reasoning framework, IRM (Implicit Reasoning Model),\nincorporating dual-stream modeling of contextual actions and intent clues as\nimplicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the\nVisual Enhancement Module (VEM). AIM deduces and preserves question-related\ndual clues by generating clue candidates and performing relation deduction. VEM\nenhances contextual visual representation by leveraging key contextual clues.\nExtensive experiments validate the effectiveness of our IRM in I-VQA tasks,\noutperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$,\n$1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on\nsimilar implicit advertisement understanding and future prediction in\ntraffic-VQA. Datasets and codes are available for double-blind review in\nanonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.\n", "link": "http://arxiv.org/abs/2506.07811v1", "date": "2025-06-09", "relevancy": 2.8357, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5713}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20Beyond%20Visible%20Cues%3A%20Implicit%20Video%20Question%20Answering%20via%0A%20%20Dual-Clue%20Reasoning&body=Title%3A%20Looking%20Beyond%20Visible%20Cues%3A%20Implicit%20Video%20Question%20Answering%20via%0A%20%20Dual-Clue%20Reasoning%0AAuthor%3A%20Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Yi%20Wang%20and%20Chaofan%20Gan%20and%20Mingxi%20Lyu%20and%20Gui%20Zou%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%20Video%20Question%20Answering%20%28VideoQA%29%20aims%20to%20answer%20natural%20language%20questions%0Abased%20on%20the%20given%20video%2C%20with%20prior%20work%20primarily%20focusing%20on%20identifying%20the%0Aduration%20of%20relevant%20segments%2C%20referred%20to%20as%20explicit%20visual%20evidence.%0AHowever%2C%20explicit%20visual%20evidence%20is%20not%20always%20directly%20available%2C%0Aparticularly%20when%20questions%20target%20symbolic%20meanings%20or%20deeper%20intentions%2C%0Aleading%20to%20significant%20performance%20degradation.%20To%20fill%20this%20gap%2C%20we%20introduce%0Aa%20novel%20task%20and%20dataset%2C%20%24%5Ctextbf%7BI%7D%24mplicit%20%24%5Ctextbf%7BV%7D%24ideo%0A%24%5Ctextbf%7BQ%7D%24uestion%20%24%5Ctextbf%7BA%7D%24nswering%20%28I-VQA%29%2C%20which%20focuses%20on%20answering%0Aquestions%20in%20scenarios%20where%20explicit%20visual%20evidence%20is%20inaccessible.%20Given%20an%0Aimplicit%20question%20and%20its%20corresponding%20video%2C%20I-VQA%20requires%20answering%20based%0Aon%20the%20contextual%20visual%20cues%20present%20within%20the%20video.%20To%20tackle%20I-VQA%2C%20we%0Apropose%20a%20novel%20reasoning%20framework%2C%20IRM%20%28Implicit%20Reasoning%20Model%29%2C%0Aincorporating%20dual-stream%20modeling%20of%20contextual%20actions%20and%20intent%20clues%20as%0Aimplicit%20reasoning%20chains.%20IRM%20comprises%20the%20Action-Intent%20Module%20%28AIM%29%20and%20the%0AVisual%20Enhancement%20Module%20%28VEM%29.%20AIM%20deduces%20and%20preserves%20question-related%0Adual%20clues%20by%20generating%20clue%20candidates%20and%20performing%20relation%20deduction.%20VEM%0Aenhances%20contextual%20visual%20representation%20by%20leveraging%20key%20contextual%20clues.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20IRM%20in%20I-VQA%20tasks%2C%0Aoutperforming%20GPT-4o%2C%20OpenAI-o3%2C%20and%20fine-tuned%20VideoChat2%20by%20%240.76%5C%25%24%2C%0A%241.37%5C%25%24%2C%20and%20%244.87%5C%25%24%2C%20respectively.%20Additionally%2C%20IRM%20performs%20SOTA%20on%0Asimilar%20implicit%20advertisement%20understanding%20and%20future%20prediction%20in%0Atraffic-VQA.%20Datasets%20and%20codes%20are%20available%20for%20double-blind%20review%20in%0Aanonymous%20repo%3A%20https%3A//github.com/tychen-SJTU/Implicit-VideoQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520Beyond%2520Visible%2520Cues%253A%2520Implicit%2520Video%2520Question%2520Answering%2520via%250A%2520%2520Dual-Clue%2520Reasoning%26entry.906535625%3DTieyuan%2520Chen%2520and%2520Huabin%2520Liu%2520and%2520Yi%2520Wang%2520and%2520Chaofan%2520Gan%2520and%2520Mingxi%2520Lyu%2520and%2520Gui%2520Zou%2520and%2520Weiyao%2520Lin%26entry.1292438233%3D%2520%2520Video%2520Question%2520Answering%2520%2528VideoQA%2529%2520aims%2520to%2520answer%2520natural%2520language%2520questions%250Abased%2520on%2520the%2520given%2520video%252C%2520with%2520prior%2520work%2520primarily%2520focusing%2520on%2520identifying%2520the%250Aduration%2520of%2520relevant%2520segments%252C%2520referred%2520to%2520as%2520explicit%2520visual%2520evidence.%250AHowever%252C%2520explicit%2520visual%2520evidence%2520is%2520not%2520always%2520directly%2520available%252C%250Aparticularly%2520when%2520questions%2520target%2520symbolic%2520meanings%2520or%2520deeper%2520intentions%252C%250Aleading%2520to%2520significant%2520performance%2520degradation.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%250Aa%2520novel%2520task%2520and%2520dataset%252C%2520%2524%255Ctextbf%257BI%257D%2524mplicit%2520%2524%255Ctextbf%257BV%257D%2524ideo%250A%2524%255Ctextbf%257BQ%257D%2524uestion%2520%2524%255Ctextbf%257BA%257D%2524nswering%2520%2528I-VQA%2529%252C%2520which%2520focuses%2520on%2520answering%250Aquestions%2520in%2520scenarios%2520where%2520explicit%2520visual%2520evidence%2520is%2520inaccessible.%2520Given%2520an%250Aimplicit%2520question%2520and%2520its%2520corresponding%2520video%252C%2520I-VQA%2520requires%2520answering%2520based%250Aon%2520the%2520contextual%2520visual%2520cues%2520present%2520within%2520the%2520video.%2520To%2520tackle%2520I-VQA%252C%2520we%250Apropose%2520a%2520novel%2520reasoning%2520framework%252C%2520IRM%2520%2528Implicit%2520Reasoning%2520Model%2529%252C%250Aincorporating%2520dual-stream%2520modeling%2520of%2520contextual%2520actions%2520and%2520intent%2520clues%2520as%250Aimplicit%2520reasoning%2520chains.%2520IRM%2520comprises%2520the%2520Action-Intent%2520Module%2520%2528AIM%2529%2520and%2520the%250AVisual%2520Enhancement%2520Module%2520%2528VEM%2529.%2520AIM%2520deduces%2520and%2520preserves%2520question-related%250Adual%2520clues%2520by%2520generating%2520clue%2520candidates%2520and%2520performing%2520relation%2520deduction.%2520VEM%250Aenhances%2520contextual%2520visual%2520representation%2520by%2520leveraging%2520key%2520contextual%2520clues.%250AExtensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520IRM%2520in%2520I-VQA%2520tasks%252C%250Aoutperforming%2520GPT-4o%252C%2520OpenAI-o3%252C%2520and%2520fine-tuned%2520VideoChat2%2520by%2520%25240.76%255C%2525%2524%252C%250A%25241.37%255C%2525%2524%252C%2520and%2520%25244.87%255C%2525%2524%252C%2520respectively.%2520Additionally%252C%2520IRM%2520performs%2520SOTA%2520on%250Asimilar%2520implicit%2520advertisement%2520understanding%2520and%2520future%2520prediction%2520in%250Atraffic-VQA.%2520Datasets%2520and%2520codes%2520are%2520available%2520for%2520double-blind%2520review%2520in%250Aanonymous%2520repo%253A%2520https%253A//github.com/tychen-SJTU/Implicit-VideoQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20Beyond%20Visible%20Cues%3A%20Implicit%20Video%20Question%20Answering%20via%0A%20%20Dual-Clue%20Reasoning&entry.906535625=Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Yi%20Wang%20and%20Chaofan%20Gan%20and%20Mingxi%20Lyu%20and%20Gui%20Zou%20and%20Weiyao%20Lin&entry.1292438233=%20%20Video%20Question%20Answering%20%28VideoQA%29%20aims%20to%20answer%20natural%20language%20questions%0Abased%20on%20the%20given%20video%2C%20with%20prior%20work%20primarily%20focusing%20on%20identifying%20the%0Aduration%20of%20relevant%20segments%2C%20referred%20to%20as%20explicit%20visual%20evidence.%0AHowever%2C%20explicit%20visual%20evidence%20is%20not%20always%20directly%20available%2C%0Aparticularly%20when%20questions%20target%20symbolic%20meanings%20or%20deeper%20intentions%2C%0Aleading%20to%20significant%20performance%20degradation.%20To%20fill%20this%20gap%2C%20we%20introduce%0Aa%20novel%20task%20and%20dataset%2C%20%24%5Ctextbf%7BI%7D%24mplicit%20%24%5Ctextbf%7BV%7D%24ideo%0A%24%5Ctextbf%7BQ%7D%24uestion%20%24%5Ctextbf%7BA%7D%24nswering%20%28I-VQA%29%2C%20which%20focuses%20on%20answering%0Aquestions%20in%20scenarios%20where%20explicit%20visual%20evidence%20is%20inaccessible.%20Given%20an%0Aimplicit%20question%20and%20its%20corresponding%20video%2C%20I-VQA%20requires%20answering%20based%0Aon%20the%20contextual%20visual%20cues%20present%20within%20the%20video.%20To%20tackle%20I-VQA%2C%20we%0Apropose%20a%20novel%20reasoning%20framework%2C%20IRM%20%28Implicit%20Reasoning%20Model%29%2C%0Aincorporating%20dual-stream%20modeling%20of%20contextual%20actions%20and%20intent%20clues%20as%0Aimplicit%20reasoning%20chains.%20IRM%20comprises%20the%20Action-Intent%20Module%20%28AIM%29%20and%20the%0AVisual%20Enhancement%20Module%20%28VEM%29.%20AIM%20deduces%20and%20preserves%20question-related%0Adual%20clues%20by%20generating%20clue%20candidates%20and%20performing%20relation%20deduction.%20VEM%0Aenhances%20contextual%20visual%20representation%20by%20leveraging%20key%20contextual%20clues.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20IRM%20in%20I-VQA%20tasks%2C%0Aoutperforming%20GPT-4o%2C%20OpenAI-o3%2C%20and%20fine-tuned%20VideoChat2%20by%20%240.76%5C%25%24%2C%0A%241.37%5C%25%24%2C%20and%20%244.87%5C%25%24%2C%20respectively.%20Additionally%2C%20IRM%20performs%20SOTA%20on%0Asimilar%20implicit%20advertisement%20understanding%20and%20future%20prediction%20in%0Atraffic-VQA.%20Datasets%20and%20codes%20are%20available%20for%20double-blind%20review%20in%0Aanonymous%20repo%3A%20https%3A//github.com/tychen-SJTU/Implicit-VideoQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07811v1&entry.124074799=Read"},
{"title": "Enhancing Few-Shot Vision-Language Classification with Large Multimodal\n  Model Features", "author": "Chancharik Mitra and Brandon Huang and Tianning Chai and Zhiqiu Lin and Assaf Arbelle and Rogerio Feris and Leonid Karlinsky and Trevor Darrell and Deva Ramanan and Roei Herzig", "abstract": "  Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a\nwide variety of vision-language (VL) tasks. Despite strong performance, LMMs'\ngenerative outputs are not specialized for vision-language classification tasks\n(i.e., tasks with vision-language inputs and discrete labels) such as image\nclassification and multiple-choice VQA. One key challenge in utilizing LMMs for\nthese tasks is the extraction of useful features from generative LMMs. To\novercome this, we propose an approach that leverages multimodal feature\nextraction from the LMM's latent space. Toward this end, we present Sparse\nAttention Vectors (SAVs) -- a finetuning-free method that leverages sparse\nattention head activations (fewer than 5% of the heads) in LMMs as strong\nfeature representations. With only few-shot examples, SAVs demonstrate\nstate-of-the-art performance compared to a variety of few-shot and finetuned\nbaselines on a collection of vision-language classification tasks. Our\nexperiments also imply that SAVs can scale in performance with additional\nexamples and generalize to similar tasks, establishing SAVs as both effective\nand robust multimodal feature representations.\n", "link": "http://arxiv.org/abs/2412.00142v3", "date": "2025-06-09", "relevancy": 2.8317, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Few-Shot%20Vision-Language%20Classification%20with%20Large%20Multimodal%0A%20%20Model%20Features&body=Title%3A%20Enhancing%20Few-Shot%20Vision-Language%20Classification%20with%20Large%20Multimodal%0A%20%20Model%20Features%0AAuthor%3A%20Chancharik%20Mitra%20and%20Brandon%20Huang%20and%20Tianning%20Chai%20and%20Zhiqiu%20Lin%20and%20Assaf%20Arbelle%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%20and%20Trevor%20Darrell%20and%20Deva%20Ramanan%20and%20Roei%20Herzig%0AAbstract%3A%20%20%20Generative%20Large%20Multimodal%20Models%20%28LMMs%29%20like%20LLaVA%20and%20Qwen-VL%20excel%20at%20a%0Awide%20variety%20of%20vision-language%20%28VL%29%20tasks.%20Despite%20strong%20performance%2C%20LMMs%27%0Agenerative%20outputs%20are%20not%20specialized%20for%20vision-language%20classification%20tasks%0A%28i.e.%2C%20tasks%20with%20vision-language%20inputs%20and%20discrete%20labels%29%20such%20as%20image%0Aclassification%20and%20multiple-choice%20VQA.%20One%20key%20challenge%20in%20utilizing%20LMMs%20for%0Athese%20tasks%20is%20the%20extraction%20of%20useful%20features%20from%20generative%20LMMs.%20To%0Aovercome%20this%2C%20we%20propose%20an%20approach%20that%20leverages%20multimodal%20feature%0Aextraction%20from%20the%20LMM%27s%20latent%20space.%20Toward%20this%20end%2C%20we%20present%20Sparse%0AAttention%20Vectors%20%28SAVs%29%20--%20a%20finetuning-free%20method%20that%20leverages%20sparse%0Aattention%20head%20activations%20%28fewer%20than%205%25%20of%20the%20heads%29%20in%20LMMs%20as%20strong%0Afeature%20representations.%20With%20only%20few-shot%20examples%2C%20SAVs%20demonstrate%0Astate-of-the-art%20performance%20compared%20to%20a%20variety%20of%20few-shot%20and%20finetuned%0Abaselines%20on%20a%20collection%20of%20vision-language%20classification%20tasks.%20Our%0Aexperiments%20also%20imply%20that%20SAVs%20can%20scale%20in%20performance%20with%20additional%0Aexamples%20and%20generalize%20to%20similar%20tasks%2C%20establishing%20SAVs%20as%20both%20effective%0Aand%20robust%20multimodal%20feature%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00142v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Few-Shot%2520Vision-Language%2520Classification%2520with%2520Large%2520Multimodal%250A%2520%2520Model%2520Features%26entry.906535625%3DChancharik%2520Mitra%2520and%2520Brandon%2520Huang%2520and%2520Tianning%2520Chai%2520and%2520Zhiqiu%2520Lin%2520and%2520Assaf%2520Arbelle%2520and%2520Rogerio%2520Feris%2520and%2520Leonid%2520Karlinsky%2520and%2520Trevor%2520Darrell%2520and%2520Deva%2520Ramanan%2520and%2520Roei%2520Herzig%26entry.1292438233%3D%2520%2520Generative%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520like%2520LLaVA%2520and%2520Qwen-VL%2520excel%2520at%2520a%250Awide%2520variety%2520of%2520vision-language%2520%2528VL%2529%2520tasks.%2520Despite%2520strong%2520performance%252C%2520LMMs%2527%250Agenerative%2520outputs%2520are%2520not%2520specialized%2520for%2520vision-language%2520classification%2520tasks%250A%2528i.e.%252C%2520tasks%2520with%2520vision-language%2520inputs%2520and%2520discrete%2520labels%2529%2520such%2520as%2520image%250Aclassification%2520and%2520multiple-choice%2520VQA.%2520One%2520key%2520challenge%2520in%2520utilizing%2520LMMs%2520for%250Athese%2520tasks%2520is%2520the%2520extraction%2520of%2520useful%2520features%2520from%2520generative%2520LMMs.%2520To%250Aovercome%2520this%252C%2520we%2520propose%2520an%2520approach%2520that%2520leverages%2520multimodal%2520feature%250Aextraction%2520from%2520the%2520LMM%2527s%2520latent%2520space.%2520Toward%2520this%2520end%252C%2520we%2520present%2520Sparse%250AAttention%2520Vectors%2520%2528SAVs%2529%2520--%2520a%2520finetuning-free%2520method%2520that%2520leverages%2520sparse%250Aattention%2520head%2520activations%2520%2528fewer%2520than%25205%2525%2520of%2520the%2520heads%2529%2520in%2520LMMs%2520as%2520strong%250Afeature%2520representations.%2520With%2520only%2520few-shot%2520examples%252C%2520SAVs%2520demonstrate%250Astate-of-the-art%2520performance%2520compared%2520to%2520a%2520variety%2520of%2520few-shot%2520and%2520finetuned%250Abaselines%2520on%2520a%2520collection%2520of%2520vision-language%2520classification%2520tasks.%2520Our%250Aexperiments%2520also%2520imply%2520that%2520SAVs%2520can%2520scale%2520in%2520performance%2520with%2520additional%250Aexamples%2520and%2520generalize%2520to%2520similar%2520tasks%252C%2520establishing%2520SAVs%2520as%2520both%2520effective%250Aand%2520robust%2520multimodal%2520feature%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00142v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Few-Shot%20Vision-Language%20Classification%20with%20Large%20Multimodal%0A%20%20Model%20Features&entry.906535625=Chancharik%20Mitra%20and%20Brandon%20Huang%20and%20Tianning%20Chai%20and%20Zhiqiu%20Lin%20and%20Assaf%20Arbelle%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%20and%20Trevor%20Darrell%20and%20Deva%20Ramanan%20and%20Roei%20Herzig&entry.1292438233=%20%20Generative%20Large%20Multimodal%20Models%20%28LMMs%29%20like%20LLaVA%20and%20Qwen-VL%20excel%20at%20a%0Awide%20variety%20of%20vision-language%20%28VL%29%20tasks.%20Despite%20strong%20performance%2C%20LMMs%27%0Agenerative%20outputs%20are%20not%20specialized%20for%20vision-language%20classification%20tasks%0A%28i.e.%2C%20tasks%20with%20vision-language%20inputs%20and%20discrete%20labels%29%20such%20as%20image%0Aclassification%20and%20multiple-choice%20VQA.%20One%20key%20challenge%20in%20utilizing%20LMMs%20for%0Athese%20tasks%20is%20the%20extraction%20of%20useful%20features%20from%20generative%20LMMs.%20To%0Aovercome%20this%2C%20we%20propose%20an%20approach%20that%20leverages%20multimodal%20feature%0Aextraction%20from%20the%20LMM%27s%20latent%20space.%20Toward%20this%20end%2C%20we%20present%20Sparse%0AAttention%20Vectors%20%28SAVs%29%20--%20a%20finetuning-free%20method%20that%20leverages%20sparse%0Aattention%20head%20activations%20%28fewer%20than%205%25%20of%20the%20heads%29%20in%20LMMs%20as%20strong%0Afeature%20representations.%20With%20only%20few-shot%20examples%2C%20SAVs%20demonstrate%0Astate-of-the-art%20performance%20compared%20to%20a%20variety%20of%20few-shot%20and%20finetuned%0Abaselines%20on%20a%20collection%20of%20vision-language%20classification%20tasks.%20Our%0Aexperiments%20also%20imply%20that%20SAVs%20can%20scale%20in%20performance%20with%20additional%0Aexamples%20and%20generalize%20to%20similar%20tasks%2C%20establishing%20SAVs%20as%20both%20effective%0Aand%20robust%20multimodal%20feature%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00142v3&entry.124074799=Read"},
{"title": "Feature-Based Lie Group Transformer for Real-World Applications", "author": "Takayuki Komatsu and Yoshiyuki Ohmura and Kayato Nishitsunoi and Yasuo Kuniyoshi", "abstract": "  The main goal of representation learning is to acquire meaningful\nrepresentations from real-world sensory inputs without supervision.\nRepresentation learning explains some aspects of human development. Various\nneural network (NN) models have been proposed that acquire empirically good\nrepresentations. However, the formulation of a good representation has not been\nestablished. We recently proposed a method for categorizing changes between a\npair of sensory inputs. A unique feature of this approach is that\ntransformations between two sensory inputs are learned to satisfy algebraic\nstructural constraints. Conventional representation learning often assumes that\ndisentangled independent feature axes is a good representation; however, we\nfound that such a representation cannot account for conditional independence.\nTo overcome this problem, we proposed a new method using group decomposition in\nGalois algebra theory. Although this method is promising for defining a more\ngeneral representation, it assumes pixel-to-pixel translation without feature\nextraction, and can only process low-resolution images with no background,\nwhich prevents real-world application. In this study, we provide a simple\nmethod to apply our group decomposition theory to a more realistic scenario by\ncombining feature extraction and object segmentation. We replace pixel\ntranslation with feature translation and formulate object segmentation as\ngrouping features under the same transformation. We validated the proposed\nmethod on a practical dataset containing both real-world object and background.\nWe believe that our model will lead to a better understanding of human\ndevelopment of object recognition in the real world.\n", "link": "http://arxiv.org/abs/2506.04668v3", "date": "2025-06-09", "relevancy": 2.8036, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5885}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5484}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-Based%20Lie%20Group%20Transformer%20for%20Real-World%20Applications&body=Title%3A%20Feature-Based%20Lie%20Group%20Transformer%20for%20Real-World%20Applications%0AAuthor%3A%20Takayuki%20Komatsu%20and%20Yoshiyuki%20Ohmura%20and%20Kayato%20Nishitsunoi%20and%20Yasuo%20Kuniyoshi%0AAbstract%3A%20%20%20The%20main%20goal%20of%20representation%20learning%20is%20to%20acquire%20meaningful%0Arepresentations%20from%20real-world%20sensory%20inputs%20without%20supervision.%0ARepresentation%20learning%20explains%20some%20aspects%20of%20human%20development.%20Various%0Aneural%20network%20%28NN%29%20models%20have%20been%20proposed%20that%20acquire%20empirically%20good%0Arepresentations.%20However%2C%20the%20formulation%20of%20a%20good%20representation%20has%20not%20been%0Aestablished.%20We%20recently%20proposed%20a%20method%20for%20categorizing%20changes%20between%20a%0Apair%20of%20sensory%20inputs.%20A%20unique%20feature%20of%20this%20approach%20is%20that%0Atransformations%20between%20two%20sensory%20inputs%20are%20learned%20to%20satisfy%20algebraic%0Astructural%20constraints.%20Conventional%20representation%20learning%20often%20assumes%20that%0Adisentangled%20independent%20feature%20axes%20is%20a%20good%20representation%3B%20however%2C%20we%0Afound%20that%20such%20a%20representation%20cannot%20account%20for%20conditional%20independence.%0ATo%20overcome%20this%20problem%2C%20we%20proposed%20a%20new%20method%20using%20group%20decomposition%20in%0AGalois%20algebra%20theory.%20Although%20this%20method%20is%20promising%20for%20defining%20a%20more%0Ageneral%20representation%2C%20it%20assumes%20pixel-to-pixel%20translation%20without%20feature%0Aextraction%2C%20and%20can%20only%20process%20low-resolution%20images%20with%20no%20background%2C%0Awhich%20prevents%20real-world%20application.%20In%20this%20study%2C%20we%20provide%20a%20simple%0Amethod%20to%20apply%20our%20group%20decomposition%20theory%20to%20a%20more%20realistic%20scenario%20by%0Acombining%20feature%20extraction%20and%20object%20segmentation.%20We%20replace%20pixel%0Atranslation%20with%20feature%20translation%20and%20formulate%20object%20segmentation%20as%0Agrouping%20features%20under%20the%20same%20transformation.%20We%20validated%20the%20proposed%0Amethod%20on%20a%20practical%20dataset%20containing%20both%20real-world%20object%20and%20background.%0AWe%20believe%20that%20our%20model%20will%20lead%20to%20a%20better%20understanding%20of%20human%0Adevelopment%20of%20object%20recognition%20in%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04668v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-Based%2520Lie%2520Group%2520Transformer%2520for%2520Real-World%2520Applications%26entry.906535625%3DTakayuki%2520Komatsu%2520and%2520Yoshiyuki%2520Ohmura%2520and%2520Kayato%2520Nishitsunoi%2520and%2520Yasuo%2520Kuniyoshi%26entry.1292438233%3D%2520%2520The%2520main%2520goal%2520of%2520representation%2520learning%2520is%2520to%2520acquire%2520meaningful%250Arepresentations%2520from%2520real-world%2520sensory%2520inputs%2520without%2520supervision.%250ARepresentation%2520learning%2520explains%2520some%2520aspects%2520of%2520human%2520development.%2520Various%250Aneural%2520network%2520%2528NN%2529%2520models%2520have%2520been%2520proposed%2520that%2520acquire%2520empirically%2520good%250Arepresentations.%2520However%252C%2520the%2520formulation%2520of%2520a%2520good%2520representation%2520has%2520not%2520been%250Aestablished.%2520We%2520recently%2520proposed%2520a%2520method%2520for%2520categorizing%2520changes%2520between%2520a%250Apair%2520of%2520sensory%2520inputs.%2520A%2520unique%2520feature%2520of%2520this%2520approach%2520is%2520that%250Atransformations%2520between%2520two%2520sensory%2520inputs%2520are%2520learned%2520to%2520satisfy%2520algebraic%250Astructural%2520constraints.%2520Conventional%2520representation%2520learning%2520often%2520assumes%2520that%250Adisentangled%2520independent%2520feature%2520axes%2520is%2520a%2520good%2520representation%253B%2520however%252C%2520we%250Afound%2520that%2520such%2520a%2520representation%2520cannot%2520account%2520for%2520conditional%2520independence.%250ATo%2520overcome%2520this%2520problem%252C%2520we%2520proposed%2520a%2520new%2520method%2520using%2520group%2520decomposition%2520in%250AGalois%2520algebra%2520theory.%2520Although%2520this%2520method%2520is%2520promising%2520for%2520defining%2520a%2520more%250Ageneral%2520representation%252C%2520it%2520assumes%2520pixel-to-pixel%2520translation%2520without%2520feature%250Aextraction%252C%2520and%2520can%2520only%2520process%2520low-resolution%2520images%2520with%2520no%2520background%252C%250Awhich%2520prevents%2520real-world%2520application.%2520In%2520this%2520study%252C%2520we%2520provide%2520a%2520simple%250Amethod%2520to%2520apply%2520our%2520group%2520decomposition%2520theory%2520to%2520a%2520more%2520realistic%2520scenario%2520by%250Acombining%2520feature%2520extraction%2520and%2520object%2520segmentation.%2520We%2520replace%2520pixel%250Atranslation%2520with%2520feature%2520translation%2520and%2520formulate%2520object%2520segmentation%2520as%250Agrouping%2520features%2520under%2520the%2520same%2520transformation.%2520We%2520validated%2520the%2520proposed%250Amethod%2520on%2520a%2520practical%2520dataset%2520containing%2520both%2520real-world%2520object%2520and%2520background.%250AWe%2520believe%2520that%2520our%2520model%2520will%2520lead%2520to%2520a%2520better%2520understanding%2520of%2520human%250Adevelopment%2520of%2520object%2520recognition%2520in%2520the%2520real%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04668v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Based%20Lie%20Group%20Transformer%20for%20Real-World%20Applications&entry.906535625=Takayuki%20Komatsu%20and%20Yoshiyuki%20Ohmura%20and%20Kayato%20Nishitsunoi%20and%20Yasuo%20Kuniyoshi&entry.1292438233=%20%20The%20main%20goal%20of%20representation%20learning%20is%20to%20acquire%20meaningful%0Arepresentations%20from%20real-world%20sensory%20inputs%20without%20supervision.%0ARepresentation%20learning%20explains%20some%20aspects%20of%20human%20development.%20Various%0Aneural%20network%20%28NN%29%20models%20have%20been%20proposed%20that%20acquire%20empirically%20good%0Arepresentations.%20However%2C%20the%20formulation%20of%20a%20good%20representation%20has%20not%20been%0Aestablished.%20We%20recently%20proposed%20a%20method%20for%20categorizing%20changes%20between%20a%0Apair%20of%20sensory%20inputs.%20A%20unique%20feature%20of%20this%20approach%20is%20that%0Atransformations%20between%20two%20sensory%20inputs%20are%20learned%20to%20satisfy%20algebraic%0Astructural%20constraints.%20Conventional%20representation%20learning%20often%20assumes%20that%0Adisentangled%20independent%20feature%20axes%20is%20a%20good%20representation%3B%20however%2C%20we%0Afound%20that%20such%20a%20representation%20cannot%20account%20for%20conditional%20independence.%0ATo%20overcome%20this%20problem%2C%20we%20proposed%20a%20new%20method%20using%20group%20decomposition%20in%0AGalois%20algebra%20theory.%20Although%20this%20method%20is%20promising%20for%20defining%20a%20more%0Ageneral%20representation%2C%20it%20assumes%20pixel-to-pixel%20translation%20without%20feature%0Aextraction%2C%20and%20can%20only%20process%20low-resolution%20images%20with%20no%20background%2C%0Awhich%20prevents%20real-world%20application.%20In%20this%20study%2C%20we%20provide%20a%20simple%0Amethod%20to%20apply%20our%20group%20decomposition%20theory%20to%20a%20more%20realistic%20scenario%20by%0Acombining%20feature%20extraction%20and%20object%20segmentation.%20We%20replace%20pixel%0Atranslation%20with%20feature%20translation%20and%20formulate%20object%20segmentation%20as%0Agrouping%20features%20under%20the%20same%20transformation.%20We%20validated%20the%20proposed%0Amethod%20on%20a%20practical%20dataset%20containing%20both%20real-world%20object%20and%20background.%0AWe%20believe%20that%20our%20model%20will%20lead%20to%20a%20better%20understanding%20of%20human%0Adevelopment%20of%20object%20recognition%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04668v3&entry.124074799=Read"},
{"title": "Scaling Human Activity Recognition: A Comparative Evaluation of\n  Synthetic Data Generation and Augmentation Techniques", "author": "Zikang Leng and Archith Iyer and Thomas Pl\u00f6tz", "abstract": "  Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.\n", "link": "http://arxiv.org/abs/2506.07612v1", "date": "2025-06-09", "relevancy": 2.776, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5561}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Human%20Activity%20Recognition%3A%20A%20Comparative%20Evaluation%20of%0A%20%20Synthetic%20Data%20Generation%20and%20Augmentation%20Techniques&body=Title%3A%20Scaling%20Human%20Activity%20Recognition%3A%20A%20Comparative%20Evaluation%20of%0A%20%20Synthetic%20Data%20Generation%20and%20Augmentation%20Techniques%0AAuthor%3A%20Zikang%20Leng%20and%20Archith%20Iyer%20and%20Thomas%20Pl%C3%B6tz%0AAbstract%3A%20%20%20Human%20activity%20recognition%20%28HAR%29%20is%20often%20limited%20by%20the%20scarcity%20of%20labeled%0Adatasets%20due%20to%20the%20high%20cost%20and%20complexity%20of%20real-world%20data%20collection.%20To%0Amitigate%20this%2C%20recent%20work%20has%20explored%20generating%20virtual%20inertial%20measurement%0Aunit%20%28IMU%29%20data%20via%20cross-modality%20transfer.%20While%20video-based%20and%0Alanguage-based%20pipelines%20have%20each%20shown%20promise%2C%20they%20differ%20in%20assumptions%0Aand%20computational%20cost.%20Moreover%2C%20their%20effectiveness%20relative%20to%20traditional%0Asensor-level%20data%20augmentation%20remains%20unclear.%20In%20this%20paper%2C%20we%20present%20a%0Adirect%20comparison%20between%20these%20two%20virtual%20IMU%20generation%20approaches%20against%0Aclassical%20data%20augmentation%20techniques.%20We%20construct%20a%20large-scale%20virtual%20IMU%0Adataset%20spanning%20100%20diverse%20activities%20from%20Kinetics-400%20and%20simulate%20sensor%0Asignals%20at%2022%20body%20locations.%20The%20three%20data%20generation%20strategies%20are%0Aevaluated%20on%20benchmark%20HAR%20datasets%20%28UTD-MHAD%2C%20PAMAP2%2C%20HAD-AW%29%20using%20four%0Apopular%20models.%20Results%20show%20that%20virtual%20IMU%20data%20significantly%20improves%0Aperformance%20over%20real%20or%20augmented%20data%20alone%2C%20particularly%20under%20limited-data%0Aconditions.%20We%20offer%20practical%20guidance%20on%20choosing%20data%20generation%20strategies%0Aand%20highlight%20the%20distinct%20advantages%20and%20disadvantages%20of%20each%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Human%2520Activity%2520Recognition%253A%2520A%2520Comparative%2520Evaluation%2520of%250A%2520%2520Synthetic%2520Data%2520Generation%2520and%2520Augmentation%2520Techniques%26entry.906535625%3DZikang%2520Leng%2520and%2520Archith%2520Iyer%2520and%2520Thomas%2520Pl%25C3%25B6tz%26entry.1292438233%3D%2520%2520Human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520often%2520limited%2520by%2520the%2520scarcity%2520of%2520labeled%250Adatasets%2520due%2520to%2520the%2520high%2520cost%2520and%2520complexity%2520of%2520real-world%2520data%2520collection.%2520To%250Amitigate%2520this%252C%2520recent%2520work%2520has%2520explored%2520generating%2520virtual%2520inertial%2520measurement%250Aunit%2520%2528IMU%2529%2520data%2520via%2520cross-modality%2520transfer.%2520While%2520video-based%2520and%250Alanguage-based%2520pipelines%2520have%2520each%2520shown%2520promise%252C%2520they%2520differ%2520in%2520assumptions%250Aand%2520computational%2520cost.%2520Moreover%252C%2520their%2520effectiveness%2520relative%2520to%2520traditional%250Asensor-level%2520data%2520augmentation%2520remains%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Adirect%2520comparison%2520between%2520these%2520two%2520virtual%2520IMU%2520generation%2520approaches%2520against%250Aclassical%2520data%2520augmentation%2520techniques.%2520We%2520construct%2520a%2520large-scale%2520virtual%2520IMU%250Adataset%2520spanning%2520100%2520diverse%2520activities%2520from%2520Kinetics-400%2520and%2520simulate%2520sensor%250Asignals%2520at%252022%2520body%2520locations.%2520The%2520three%2520data%2520generation%2520strategies%2520are%250Aevaluated%2520on%2520benchmark%2520HAR%2520datasets%2520%2528UTD-MHAD%252C%2520PAMAP2%252C%2520HAD-AW%2529%2520using%2520four%250Apopular%2520models.%2520Results%2520show%2520that%2520virtual%2520IMU%2520data%2520significantly%2520improves%250Aperformance%2520over%2520real%2520or%2520augmented%2520data%2520alone%252C%2520particularly%2520under%2520limited-data%250Aconditions.%2520We%2520offer%2520practical%2520guidance%2520on%2520choosing%2520data%2520generation%2520strategies%250Aand%2520highlight%2520the%2520distinct%2520advantages%2520and%2520disadvantages%2520of%2520each%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Human%20Activity%20Recognition%3A%20A%20Comparative%20Evaluation%20of%0A%20%20Synthetic%20Data%20Generation%20and%20Augmentation%20Techniques&entry.906535625=Zikang%20Leng%20and%20Archith%20Iyer%20and%20Thomas%20Pl%C3%B6tz&entry.1292438233=%20%20Human%20activity%20recognition%20%28HAR%29%20is%20often%20limited%20by%20the%20scarcity%20of%20labeled%0Adatasets%20due%20to%20the%20high%20cost%20and%20complexity%20of%20real-world%20data%20collection.%20To%0Amitigate%20this%2C%20recent%20work%20has%20explored%20generating%20virtual%20inertial%20measurement%0Aunit%20%28IMU%29%20data%20via%20cross-modality%20transfer.%20While%20video-based%20and%0Alanguage-based%20pipelines%20have%20each%20shown%20promise%2C%20they%20differ%20in%20assumptions%0Aand%20computational%20cost.%20Moreover%2C%20their%20effectiveness%20relative%20to%20traditional%0Asensor-level%20data%20augmentation%20remains%20unclear.%20In%20this%20paper%2C%20we%20present%20a%0Adirect%20comparison%20between%20these%20two%20virtual%20IMU%20generation%20approaches%20against%0Aclassical%20data%20augmentation%20techniques.%20We%20construct%20a%20large-scale%20virtual%20IMU%0Adataset%20spanning%20100%20diverse%20activities%20from%20Kinetics-400%20and%20simulate%20sensor%0Asignals%20at%2022%20body%20locations.%20The%20three%20data%20generation%20strategies%20are%0Aevaluated%20on%20benchmark%20HAR%20datasets%20%28UTD-MHAD%2C%20PAMAP2%2C%20HAD-AW%29%20using%20four%0Apopular%20models.%20Results%20show%20that%20virtual%20IMU%20data%20significantly%20improves%0Aperformance%20over%20real%20or%20augmented%20data%20alone%2C%20particularly%20under%20limited-data%0Aconditions.%20We%20offer%20practical%20guidance%20on%20choosing%20data%20generation%20strategies%0Aand%20highlight%20the%20distinct%20advantages%20and%20disadvantages%20of%20each%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07612v1&entry.124074799=Read"},
{"title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "author": "Lev Novitskiy and Viacheslav Vasilev and Maria Kovaleva and Vladimir Arkhipkin and Denis Dimitrov", "abstract": "  Variational Autoencoders (VAEs) remain a cornerstone of generative computer\nvision, yet their training is often plagued by artifacts that degrade\nreconstruction and generation quality. This paper introduces VIVAT, a\nsystematic approach to mitigating common artifacts in KL-VAE training without\nrequiring radical architectural changes. We present a detailed taxonomy of five\nprevalent artifacts - color shift, grid patterns, blur, corner and droplet\nartifacts - and analyze their root causes. Through straightforward\nmodifications, including adjustments to loss weights, padding strategies, and\nthe integration of Spatially Conditional Normalization, we demonstrate\nsignificant improvements in VAE performance. Our method achieves\nstate-of-the-art results in image reconstruction metrics (PSNR and SSIM) across\nmultiple benchmarks and enhances text-to-image generation quality, as evidenced\nby superior CLIP scores. By preserving the simplicity of the KL-VAE framework\nwhile addressing its practical challenges, VIVAT offers actionable insights for\nresearchers and practitioners aiming to optimize VAE training.\n", "link": "http://arxiv.org/abs/2506.07863v1", "date": "2025-06-09", "relevancy": 2.7759, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5722}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIVAT%3A%20Virtuous%20Improving%20VAE%20Training%20through%20Artifact%20Mitigation&body=Title%3A%20VIVAT%3A%20Virtuous%20Improving%20VAE%20Training%20through%20Artifact%20Mitigation%0AAuthor%3A%20Lev%20Novitskiy%20and%20Viacheslav%20Vasilev%20and%20Maria%20Kovaleva%20and%20Vladimir%20Arkhipkin%20and%20Denis%20Dimitrov%0AAbstract%3A%20%20%20Variational%20Autoencoders%20%28VAEs%29%20remain%20a%20cornerstone%20of%20generative%20computer%0Avision%2C%20yet%20their%20training%20is%20often%20plagued%20by%20artifacts%20that%20degrade%0Areconstruction%20and%20generation%20quality.%20This%20paper%20introduces%20VIVAT%2C%20a%0Asystematic%20approach%20to%20mitigating%20common%20artifacts%20in%20KL-VAE%20training%20without%0Arequiring%20radical%20architectural%20changes.%20We%20present%20a%20detailed%20taxonomy%20of%20five%0Aprevalent%20artifacts%20-%20color%20shift%2C%20grid%20patterns%2C%20blur%2C%20corner%20and%20droplet%0Aartifacts%20-%20and%20analyze%20their%20root%20causes.%20Through%20straightforward%0Amodifications%2C%20including%20adjustments%20to%20loss%20weights%2C%20padding%20strategies%2C%20and%0Athe%20integration%20of%20Spatially%20Conditional%20Normalization%2C%20we%20demonstrate%0Asignificant%20improvements%20in%20VAE%20performance.%20Our%20method%20achieves%0Astate-of-the-art%20results%20in%20image%20reconstruction%20metrics%20%28PSNR%20and%20SSIM%29%20across%0Amultiple%20benchmarks%20and%20enhances%20text-to-image%20generation%20quality%2C%20as%20evidenced%0Aby%20superior%20CLIP%20scores.%20By%20preserving%20the%20simplicity%20of%20the%20KL-VAE%20framework%0Awhile%20addressing%20its%20practical%20challenges%2C%20VIVAT%20offers%20actionable%20insights%20for%0Aresearchers%20and%20practitioners%20aiming%20to%20optimize%20VAE%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIVAT%253A%2520Virtuous%2520Improving%2520VAE%2520Training%2520through%2520Artifact%2520Mitigation%26entry.906535625%3DLev%2520Novitskiy%2520and%2520Viacheslav%2520Vasilev%2520and%2520Maria%2520Kovaleva%2520and%2520Vladimir%2520Arkhipkin%2520and%2520Denis%2520Dimitrov%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520remain%2520a%2520cornerstone%2520of%2520generative%2520computer%250Avision%252C%2520yet%2520their%2520training%2520is%2520often%2520plagued%2520by%2520artifacts%2520that%2520degrade%250Areconstruction%2520and%2520generation%2520quality.%2520This%2520paper%2520introduces%2520VIVAT%252C%2520a%250Asystematic%2520approach%2520to%2520mitigating%2520common%2520artifacts%2520in%2520KL-VAE%2520training%2520without%250Arequiring%2520radical%2520architectural%2520changes.%2520We%2520present%2520a%2520detailed%2520taxonomy%2520of%2520five%250Aprevalent%2520artifacts%2520-%2520color%2520shift%252C%2520grid%2520patterns%252C%2520blur%252C%2520corner%2520and%2520droplet%250Aartifacts%2520-%2520and%2520analyze%2520their%2520root%2520causes.%2520Through%2520straightforward%250Amodifications%252C%2520including%2520adjustments%2520to%2520loss%2520weights%252C%2520padding%2520strategies%252C%2520and%250Athe%2520integration%2520of%2520Spatially%2520Conditional%2520Normalization%252C%2520we%2520demonstrate%250Asignificant%2520improvements%2520in%2520VAE%2520performance.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520results%2520in%2520image%2520reconstruction%2520metrics%2520%2528PSNR%2520and%2520SSIM%2529%2520across%250Amultiple%2520benchmarks%2520and%2520enhances%2520text-to-image%2520generation%2520quality%252C%2520as%2520evidenced%250Aby%2520superior%2520CLIP%2520scores.%2520By%2520preserving%2520the%2520simplicity%2520of%2520the%2520KL-VAE%2520framework%250Awhile%2520addressing%2520its%2520practical%2520challenges%252C%2520VIVAT%2520offers%2520actionable%2520insights%2520for%250Aresearchers%2520and%2520practitioners%2520aiming%2520to%2520optimize%2520VAE%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIVAT%3A%20Virtuous%20Improving%20VAE%20Training%20through%20Artifact%20Mitigation&entry.906535625=Lev%20Novitskiy%20and%20Viacheslav%20Vasilev%20and%20Maria%20Kovaleva%20and%20Vladimir%20Arkhipkin%20and%20Denis%20Dimitrov&entry.1292438233=%20%20Variational%20Autoencoders%20%28VAEs%29%20remain%20a%20cornerstone%20of%20generative%20computer%0Avision%2C%20yet%20their%20training%20is%20often%20plagued%20by%20artifacts%20that%20degrade%0Areconstruction%20and%20generation%20quality.%20This%20paper%20introduces%20VIVAT%2C%20a%0Asystematic%20approach%20to%20mitigating%20common%20artifacts%20in%20KL-VAE%20training%20without%0Arequiring%20radical%20architectural%20changes.%20We%20present%20a%20detailed%20taxonomy%20of%20five%0Aprevalent%20artifacts%20-%20color%20shift%2C%20grid%20patterns%2C%20blur%2C%20corner%20and%20droplet%0Aartifacts%20-%20and%20analyze%20their%20root%20causes.%20Through%20straightforward%0Amodifications%2C%20including%20adjustments%20to%20loss%20weights%2C%20padding%20strategies%2C%20and%0Athe%20integration%20of%20Spatially%20Conditional%20Normalization%2C%20we%20demonstrate%0Asignificant%20improvements%20in%20VAE%20performance.%20Our%20method%20achieves%0Astate-of-the-art%20results%20in%20image%20reconstruction%20metrics%20%28PSNR%20and%20SSIM%29%20across%0Amultiple%20benchmarks%20and%20enhances%20text-to-image%20generation%20quality%2C%20as%20evidenced%0Aby%20superior%20CLIP%20scores.%20By%20preserving%20the%20simplicity%20of%20the%20KL-VAE%20framework%0Awhile%20addressing%20its%20practical%20challenges%2C%20VIVAT%20offers%20actionable%20insights%20for%0Aresearchers%20and%20practitioners%20aiming%20to%20optimize%20VAE%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07863v1&entry.124074799=Read"},
{"title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic\n  Segmentation of 3D Point Clouds", "author": "Zihui Zhang and Weisheng Dai and Hongtao Wen and Bo Yang", "abstract": "  We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.\n", "link": "http://arxiv.org/abs/2506.07857v1", "date": "2025-06-09", "relevancy": 2.7717, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5704}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5475}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogoSP%3A%20Local-global%20Grouping%20of%20Superpoints%20for%20Unsupervised%20Semantic%0A%20%20Segmentation%20of%203D%20Point%20Clouds&body=Title%3A%20LogoSP%3A%20Local-global%20Grouping%20of%20Superpoints%20for%20Unsupervised%20Semantic%0A%20%20Segmentation%20of%203D%20Point%20Clouds%0AAuthor%3A%20Zihui%20Zhang%20and%20Weisheng%20Dai%20and%20Hongtao%20Wen%20and%20Bo%20Yang%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20unsupervised%203D%20semantic%20segmentation%20on%20raw%20point%0Aclouds%20without%20needing%20human%20labels%20in%20training.%20Existing%20methods%20usually%0Aformulate%20this%20problem%20into%20learning%20per-point%20local%20features%20followed%20by%20a%0Asimple%20grouping%20strategy%2C%20lacking%20the%20ability%20to%20discover%20additional%20and%0Apossibly%20richer%20semantic%20priors%20beyond%20local%20features.%20In%20this%20paper%2C%20we%0Aintroduce%20LogoSP%20to%20learn%203D%20semantics%20from%20both%20local%20and%20global%20point%0Afeatures.%20The%20key%20to%20our%20approach%20is%20to%20discover%203D%20semantic%20information%20by%0Agrouping%20superpoints%20according%20to%20their%20global%20patterns%20in%20the%20frequency%0Adomain%2C%20thus%20generating%20highly%20accurate%20semantic%20pseudo-labels%20for%20training%20a%0Asegmentation%20network.%20Extensive%20experiments%20on%20two%20indoor%20and%20an%20outdoor%0Adatasets%20show%20that%20our%20LogoSP%20surpasses%20all%20existing%20unsupervised%20methods%20by%0Alarge%20margins%2C%20achieving%20the%20state-of-the-art%20performance%20for%20unsupervised%203D%0Asemantic%20segmentation.%20Notably%2C%20our%20investigation%20into%20the%20learned%20global%0Apatterns%20reveals%20that%20they%20truly%20represent%20meaningful%203D%20semantics%20in%20the%0Aabsence%20of%20human%20labels%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogoSP%253A%2520Local-global%2520Grouping%2520of%2520Superpoints%2520for%2520Unsupervised%2520Semantic%250A%2520%2520Segmentation%2520of%25203D%2520Point%2520Clouds%26entry.906535625%3DZihui%2520Zhang%2520and%2520Weisheng%2520Dai%2520and%2520Hongtao%2520Wen%2520and%2520Bo%2520Yang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520unsupervised%25203D%2520semantic%2520segmentation%2520on%2520raw%2520point%250Aclouds%2520without%2520needing%2520human%2520labels%2520in%2520training.%2520Existing%2520methods%2520usually%250Aformulate%2520this%2520problem%2520into%2520learning%2520per-point%2520local%2520features%2520followed%2520by%2520a%250Asimple%2520grouping%2520strategy%252C%2520lacking%2520the%2520ability%2520to%2520discover%2520additional%2520and%250Apossibly%2520richer%2520semantic%2520priors%2520beyond%2520local%2520features.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520LogoSP%2520to%2520learn%25203D%2520semantics%2520from%2520both%2520local%2520and%2520global%2520point%250Afeatures.%2520The%2520key%2520to%2520our%2520approach%2520is%2520to%2520discover%25203D%2520semantic%2520information%2520by%250Agrouping%2520superpoints%2520according%2520to%2520their%2520global%2520patterns%2520in%2520the%2520frequency%250Adomain%252C%2520thus%2520generating%2520highly%2520accurate%2520semantic%2520pseudo-labels%2520for%2520training%2520a%250Asegmentation%2520network.%2520Extensive%2520experiments%2520on%2520two%2520indoor%2520and%2520an%2520outdoor%250Adatasets%2520show%2520that%2520our%2520LogoSP%2520surpasses%2520all%2520existing%2520unsupervised%2520methods%2520by%250Alarge%2520margins%252C%2520achieving%2520the%2520state-of-the-art%2520performance%2520for%2520unsupervised%25203D%250Asemantic%2520segmentation.%2520Notably%252C%2520our%2520investigation%2520into%2520the%2520learned%2520global%250Apatterns%2520reveals%2520that%2520they%2520truly%2520represent%2520meaningful%25203D%2520semantics%2520in%2520the%250Aabsence%2520of%2520human%2520labels%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogoSP%3A%20Local-global%20Grouping%20of%20Superpoints%20for%20Unsupervised%20Semantic%0A%20%20Segmentation%20of%203D%20Point%20Clouds&entry.906535625=Zihui%20Zhang%20and%20Weisheng%20Dai%20and%20Hongtao%20Wen%20and%20Bo%20Yang&entry.1292438233=%20%20We%20study%20the%20problem%20of%20unsupervised%203D%20semantic%20segmentation%20on%20raw%20point%0Aclouds%20without%20needing%20human%20labels%20in%20training.%20Existing%20methods%20usually%0Aformulate%20this%20problem%20into%20learning%20per-point%20local%20features%20followed%20by%20a%0Asimple%20grouping%20strategy%2C%20lacking%20the%20ability%20to%20discover%20additional%20and%0Apossibly%20richer%20semantic%20priors%20beyond%20local%20features.%20In%20this%20paper%2C%20we%0Aintroduce%20LogoSP%20to%20learn%203D%20semantics%20from%20both%20local%20and%20global%20point%0Afeatures.%20The%20key%20to%20our%20approach%20is%20to%20discover%203D%20semantic%20information%20by%0Agrouping%20superpoints%20according%20to%20their%20global%20patterns%20in%20the%20frequency%0Adomain%2C%20thus%20generating%20highly%20accurate%20semantic%20pseudo-labels%20for%20training%20a%0Asegmentation%20network.%20Extensive%20experiments%20on%20two%20indoor%20and%20an%20outdoor%0Adatasets%20show%20that%20our%20LogoSP%20surpasses%20all%20existing%20unsupervised%20methods%20by%0Alarge%20margins%2C%20achieving%20the%20state-of-the-art%20performance%20for%20unsupervised%203D%0Asemantic%20segmentation.%20Notably%2C%20our%20investigation%20into%20the%20learned%20global%0Apatterns%20reveals%20that%20they%20truly%20represent%20meaningful%203D%20semantics%20in%20the%0Aabsence%20of%20human%20labels%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07857v1&entry.124074799=Read"},
{"title": "CORDIAL: Can Multimodal Large Language Models Effectively Understand\n  Coherence Relationships?", "author": "Aashish Anantha Ramakrishnan and Aadarsh Anantha Ramakrishnan and Dongwon Lee", "abstract": "  Multimodal Large Language Models (MLLMs) are renowned for their superior\ninstruction-following and reasoning capabilities across diverse problem\ndomains. However, existing benchmarks primarily focus on assessing factual and\nlogical correctness in downstream tasks, with limited emphasis on evaluating\nMLLMs' ability to interpret pragmatic cues and intermodal relationships. To\naddress this gap, we assess the competency of MLLMs in performing Multimodal\nDiscourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL,\nencompasses a broad spectrum of Coherence Relations across 3 different\ndiscourse domains at varying levels of granularity. Through our experiments on\n10+ MLLMs employing different prompting strategies, we show that even top\nmodels like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple\nclassifier-based baselines. This study emphasizes the need to move beyond\nsimilarity-based metrics and adopt a discourse-driven framework for evaluating\nMLLMs, providing a more nuanced assessment of their capabilities. The benchmark\nand code are available at: https://aashish2000.github.io/CORDIAL/\n", "link": "http://arxiv.org/abs/2502.11300v2", "date": "2025-06-09", "relevancy": 2.7599, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CORDIAL%3A%20Can%20Multimodal%20Large%20Language%20Models%20Effectively%20Understand%0A%20%20Coherence%20Relationships%3F&body=Title%3A%20CORDIAL%3A%20Can%20Multimodal%20Large%20Language%20Models%20Effectively%20Understand%0A%20%20Coherence%20Relationships%3F%0AAuthor%3A%20Aashish%20Anantha%20Ramakrishnan%20and%20Aadarsh%20Anantha%20Ramakrishnan%20and%20Dongwon%20Lee%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20renowned%20for%20their%20superior%0Ainstruction-following%20and%20reasoning%20capabilities%20across%20diverse%20problem%0Adomains.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20assessing%20factual%20and%0Alogical%20correctness%20in%20downstream%20tasks%2C%20with%20limited%20emphasis%20on%20evaluating%0AMLLMs%27%20ability%20to%20interpret%20pragmatic%20cues%20and%20intermodal%20relationships.%20To%0Aaddress%20this%20gap%2C%20we%20assess%20the%20competency%20of%20MLLMs%20in%20performing%20Multimodal%0ADiscourse%20Analysis%20%28MDA%29%20using%20Coherence%20Relations.%20Our%20benchmark%2C%20CORDIAL%2C%0Aencompasses%20a%20broad%20spectrum%20of%20Coherence%20Relations%20across%203%20different%0Adiscourse%20domains%20at%20varying%20levels%20of%20granularity.%20Through%20our%20experiments%20on%0A10%2B%20MLLMs%20employing%20different%20prompting%20strategies%2C%20we%20show%20that%20even%20top%0Amodels%20like%20Gemini%201.5%20Pro%20and%20GPT-4o%20fail%20to%20match%20the%20performance%20of%20simple%0Aclassifier-based%20baselines.%20This%20study%20emphasizes%20the%20need%20to%20move%20beyond%0Asimilarity-based%20metrics%20and%20adopt%20a%20discourse-driven%20framework%20for%20evaluating%0AMLLMs%2C%20providing%20a%20more%20nuanced%20assessment%20of%20their%20capabilities.%20The%20benchmark%0Aand%20code%20are%20available%20at%3A%20https%3A//aashish2000.github.io/CORDIAL/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCORDIAL%253A%2520Can%2520Multimodal%2520Large%2520Language%2520Models%2520Effectively%2520Understand%250A%2520%2520Coherence%2520Relationships%253F%26entry.906535625%3DAashish%2520Anantha%2520Ramakrishnan%2520and%2520Aadarsh%2520Anantha%2520Ramakrishnan%2520and%2520Dongwon%2520Lee%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520renowned%2520for%2520their%2520superior%250Ainstruction-following%2520and%2520reasoning%2520capabilities%2520across%2520diverse%2520problem%250Adomains.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520assessing%2520factual%2520and%250Alogical%2520correctness%2520in%2520downstream%2520tasks%252C%2520with%2520limited%2520emphasis%2520on%2520evaluating%250AMLLMs%2527%2520ability%2520to%2520interpret%2520pragmatic%2520cues%2520and%2520intermodal%2520relationships.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520assess%2520the%2520competency%2520of%2520MLLMs%2520in%2520performing%2520Multimodal%250ADiscourse%2520Analysis%2520%2528MDA%2529%2520using%2520Coherence%2520Relations.%2520Our%2520benchmark%252C%2520CORDIAL%252C%250Aencompasses%2520a%2520broad%2520spectrum%2520of%2520Coherence%2520Relations%2520across%25203%2520different%250Adiscourse%2520domains%2520at%2520varying%2520levels%2520of%2520granularity.%2520Through%2520our%2520experiments%2520on%250A10%252B%2520MLLMs%2520employing%2520different%2520prompting%2520strategies%252C%2520we%2520show%2520that%2520even%2520top%250Amodels%2520like%2520Gemini%25201.5%2520Pro%2520and%2520GPT-4o%2520fail%2520to%2520match%2520the%2520performance%2520of%2520simple%250Aclassifier-based%2520baselines.%2520This%2520study%2520emphasizes%2520the%2520need%2520to%2520move%2520beyond%250Asimilarity-based%2520metrics%2520and%2520adopt%2520a%2520discourse-driven%2520framework%2520for%2520evaluating%250AMLLMs%252C%2520providing%2520a%2520more%2520nuanced%2520assessment%2520of%2520their%2520capabilities.%2520The%2520benchmark%250Aand%2520code%2520are%2520available%2520at%253A%2520https%253A//aashish2000.github.io/CORDIAL/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CORDIAL%3A%20Can%20Multimodal%20Large%20Language%20Models%20Effectively%20Understand%0A%20%20Coherence%20Relationships%3F&entry.906535625=Aashish%20Anantha%20Ramakrishnan%20and%20Aadarsh%20Anantha%20Ramakrishnan%20and%20Dongwon%20Lee&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20renowned%20for%20their%20superior%0Ainstruction-following%20and%20reasoning%20capabilities%20across%20diverse%20problem%0Adomains.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20assessing%20factual%20and%0Alogical%20correctness%20in%20downstream%20tasks%2C%20with%20limited%20emphasis%20on%20evaluating%0AMLLMs%27%20ability%20to%20interpret%20pragmatic%20cues%20and%20intermodal%20relationships.%20To%0Aaddress%20this%20gap%2C%20we%20assess%20the%20competency%20of%20MLLMs%20in%20performing%20Multimodal%0ADiscourse%20Analysis%20%28MDA%29%20using%20Coherence%20Relations.%20Our%20benchmark%2C%20CORDIAL%2C%0Aencompasses%20a%20broad%20spectrum%20of%20Coherence%20Relations%20across%203%20different%0Adiscourse%20domains%20at%20varying%20levels%20of%20granularity.%20Through%20our%20experiments%20on%0A10%2B%20MLLMs%20employing%20different%20prompting%20strategies%2C%20we%20show%20that%20even%20top%0Amodels%20like%20Gemini%201.5%20Pro%20and%20GPT-4o%20fail%20to%20match%20the%20performance%20of%20simple%0Aclassifier-based%20baselines.%20This%20study%20emphasizes%20the%20need%20to%20move%20beyond%0Asimilarity-based%20metrics%20and%20adopt%20a%20discourse-driven%20framework%20for%20evaluating%0AMLLMs%2C%20providing%20a%20more%20nuanced%20assessment%20of%20their%20capabilities.%20The%20benchmark%0Aand%20code%20are%20available%20at%3A%20https%3A//aashish2000.github.io/CORDIAL/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11300v2&entry.124074799=Read"},
{"title": "F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote\n  Sensing Segmentation", "author": "Hengzhi Chen and Liqian Feng and Wenhua Wu and Xiaogang Zhu and Shawn Leo and Kun Hu", "abstract": "  Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery\nis critical for applications like environmental monitoring and urban planning\nbut faces computational and optimization challenges. Conventional methods\neither lose fine details through downsampling or fragment global context via\npatch processing. While multi-branch networks address this trade-off, they\nsuffer from computational inefficiency and conflicting gradient dynamics during\ntraining. We propose F2Net, a frequency-aware framework that decomposes UHR\nimages into high- and low-frequency components for specialized processing. The\nhigh-frequency branch preserves full-resolution structural details, while the\nlow-frequency branch processes downsampled inputs through dual sub-branches\ncapturing short- and long-range dependencies. A Hybrid-Frequency Fusion module\nintegrates these observations, guided by two novel objectives: Cross-Frequency\nAlignment Loss ensures semantic consistency between frequency components, and\nCross-Frequency Balance Loss regulates gradient magnitudes across branches to\nstabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net\nachieves state-of-the-art performance with mIoU of 80.22 and 83.39,\nrespectively. Our code will be publicly available.\n", "link": "http://arxiv.org/abs/2506.07847v1", "date": "2025-06-09", "relevancy": 2.7588, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F2Net%3A%20A%20Frequency-Fused%20Network%20for%20Ultra-High%20Resolution%20Remote%0A%20%20Sensing%20Segmentation&body=Title%3A%20F2Net%3A%20A%20Frequency-Fused%20Network%20for%20Ultra-High%20Resolution%20Remote%0A%20%20Sensing%20Segmentation%0AAuthor%3A%20Hengzhi%20Chen%20and%20Liqian%20Feng%20and%20Wenhua%20Wu%20and%20Xiaogang%20Zhu%20and%20Shawn%20Leo%20and%20Kun%20Hu%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20ultra-high-resolution%20%28UHR%29%20remote%20sensing%20imagery%0Ais%20critical%20for%20applications%20like%20environmental%20monitoring%20and%20urban%20planning%0Abut%20faces%20computational%20and%20optimization%20challenges.%20Conventional%20methods%0Aeither%20lose%20fine%20details%20through%20downsampling%20or%20fragment%20global%20context%20via%0Apatch%20processing.%20While%20multi-branch%20networks%20address%20this%20trade-off%2C%20they%0Asuffer%20from%20computational%20inefficiency%20and%20conflicting%20gradient%20dynamics%20during%0Atraining.%20We%20propose%20F2Net%2C%20a%20frequency-aware%20framework%20that%20decomposes%20UHR%0Aimages%20into%20high-%20and%20low-frequency%20components%20for%20specialized%20processing.%20The%0Ahigh-frequency%20branch%20preserves%20full-resolution%20structural%20details%2C%20while%20the%0Alow-frequency%20branch%20processes%20downsampled%20inputs%20through%20dual%20sub-branches%0Acapturing%20short-%20and%20long-range%20dependencies.%20A%20Hybrid-Frequency%20Fusion%20module%0Aintegrates%20these%20observations%2C%20guided%20by%20two%20novel%20objectives%3A%20Cross-Frequency%0AAlignment%20Loss%20ensures%20semantic%20consistency%20between%20frequency%20components%2C%20and%0ACross-Frequency%20Balance%20Loss%20regulates%20gradient%20magnitudes%20across%20branches%20to%0Astabilize%20training.%20Evaluated%20on%20DeepGlobe%20and%20Inria%20Aerial%20benchmarks%2C%20F2Net%0Aachieves%20state-of-the-art%20performance%20with%20mIoU%20of%2080.22%20and%2083.39%2C%0Arespectively.%20Our%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF2Net%253A%2520A%2520Frequency-Fused%2520Network%2520for%2520Ultra-High%2520Resolution%2520Remote%250A%2520%2520Sensing%2520Segmentation%26entry.906535625%3DHengzhi%2520Chen%2520and%2520Liqian%2520Feng%2520and%2520Wenhua%2520Wu%2520and%2520Xiaogang%2520Zhu%2520and%2520Shawn%2520Leo%2520and%2520Kun%2520Hu%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520ultra-high-resolution%2520%2528UHR%2529%2520remote%2520sensing%2520imagery%250Ais%2520critical%2520for%2520applications%2520like%2520environmental%2520monitoring%2520and%2520urban%2520planning%250Abut%2520faces%2520computational%2520and%2520optimization%2520challenges.%2520Conventional%2520methods%250Aeither%2520lose%2520fine%2520details%2520through%2520downsampling%2520or%2520fragment%2520global%2520context%2520via%250Apatch%2520processing.%2520While%2520multi-branch%2520networks%2520address%2520this%2520trade-off%252C%2520they%250Asuffer%2520from%2520computational%2520inefficiency%2520and%2520conflicting%2520gradient%2520dynamics%2520during%250Atraining.%2520We%2520propose%2520F2Net%252C%2520a%2520frequency-aware%2520framework%2520that%2520decomposes%2520UHR%250Aimages%2520into%2520high-%2520and%2520low-frequency%2520components%2520for%2520specialized%2520processing.%2520The%250Ahigh-frequency%2520branch%2520preserves%2520full-resolution%2520structural%2520details%252C%2520while%2520the%250Alow-frequency%2520branch%2520processes%2520downsampled%2520inputs%2520through%2520dual%2520sub-branches%250Acapturing%2520short-%2520and%2520long-range%2520dependencies.%2520A%2520Hybrid-Frequency%2520Fusion%2520module%250Aintegrates%2520these%2520observations%252C%2520guided%2520by%2520two%2520novel%2520objectives%253A%2520Cross-Frequency%250AAlignment%2520Loss%2520ensures%2520semantic%2520consistency%2520between%2520frequency%2520components%252C%2520and%250ACross-Frequency%2520Balance%2520Loss%2520regulates%2520gradient%2520magnitudes%2520across%2520branches%2520to%250Astabilize%2520training.%2520Evaluated%2520on%2520DeepGlobe%2520and%2520Inria%2520Aerial%2520benchmarks%252C%2520F2Net%250Aachieves%2520state-of-the-art%2520performance%2520with%2520mIoU%2520of%252080.22%2520and%252083.39%252C%250Arespectively.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F2Net%3A%20A%20Frequency-Fused%20Network%20for%20Ultra-High%20Resolution%20Remote%0A%20%20Sensing%20Segmentation&entry.906535625=Hengzhi%20Chen%20and%20Liqian%20Feng%20and%20Wenhua%20Wu%20and%20Xiaogang%20Zhu%20and%20Shawn%20Leo%20and%20Kun%20Hu&entry.1292438233=%20%20Semantic%20segmentation%20of%20ultra-high-resolution%20%28UHR%29%20remote%20sensing%20imagery%0Ais%20critical%20for%20applications%20like%20environmental%20monitoring%20and%20urban%20planning%0Abut%20faces%20computational%20and%20optimization%20challenges.%20Conventional%20methods%0Aeither%20lose%20fine%20details%20through%20downsampling%20or%20fragment%20global%20context%20via%0Apatch%20processing.%20While%20multi-branch%20networks%20address%20this%20trade-off%2C%20they%0Asuffer%20from%20computational%20inefficiency%20and%20conflicting%20gradient%20dynamics%20during%0Atraining.%20We%20propose%20F2Net%2C%20a%20frequency-aware%20framework%20that%20decomposes%20UHR%0Aimages%20into%20high-%20and%20low-frequency%20components%20for%20specialized%20processing.%20The%0Ahigh-frequency%20branch%20preserves%20full-resolution%20structural%20details%2C%20while%20the%0Alow-frequency%20branch%20processes%20downsampled%20inputs%20through%20dual%20sub-branches%0Acapturing%20short-%20and%20long-range%20dependencies.%20A%20Hybrid-Frequency%20Fusion%20module%0Aintegrates%20these%20observations%2C%20guided%20by%20two%20novel%20objectives%3A%20Cross-Frequency%0AAlignment%20Loss%20ensures%20semantic%20consistency%20between%20frequency%20components%2C%20and%0ACross-Frequency%20Balance%20Loss%20regulates%20gradient%20magnitudes%20across%20branches%20to%0Astabilize%20training.%20Evaluated%20on%20DeepGlobe%20and%20Inria%20Aerial%20benchmarks%2C%20F2Net%0Aachieves%20state-of-the-art%20performance%20with%20mIoU%20of%2080.22%20and%2083.39%2C%0Arespectively.%20Our%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07847v1&entry.124074799=Read"},
{"title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D\n  Generation", "author": "Yuxiao Yang and Peihao Li and Yuhong Zhang and Junzhe Lu and Xianglong He and Minghan Qin and Weitao Wang and Haoqian Wang", "abstract": "  3D AI-generated content (AIGC) has made it increasingly accessible for anyone\nto become a 3D content creator. While recent methods leverage Score\nDistillation Sampling to distill 3D objects from pretrained image diffusion\nmodels, they often suffer from inadequate 3D priors, leading to insufficient\nmulti-view consistency. In this work, we introduce NOVA3D, an innovative\nsingle-image-to-3D generation framework. Our key insight lies in leveraging\nstrong 3D priors from a pretrained video diffusion model and integrating\ngeometric information during multi-view video fine-tuning. To facilitate\ninformation exchange between color and geometric domains, we propose the\nGeometry-Temporal Alignment (GTA) attention mechanism, thereby improving\ngeneralization and multi-view consistency. Moreover, we introduce the\nde-conflict geometry fusion algorithm, which improves texture fidelity by\naddressing multi-view inaccuracies and resolving discrepancies in pose\nalignment. Extensive experiments validate the superiority of NOVA3D over\nexisting baselines.\n", "link": "http://arxiv.org/abs/2506.07698v1", "date": "2025-06-09", "relevancy": 2.7486, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6952}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6952}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOVA3D%3A%20Normal%20Aligned%20Video%20Diffusion%20Model%20for%20Single%20Image%20to%203D%0A%20%20Generation&body=Title%3A%20NOVA3D%3A%20Normal%20Aligned%20Video%20Diffusion%20Model%20for%20Single%20Image%20to%203D%0A%20%20Generation%0AAuthor%3A%20Yuxiao%20Yang%20and%20Peihao%20Li%20and%20Yuhong%20Zhang%20and%20Junzhe%20Lu%20and%20Xianglong%20He%20and%20Minghan%20Qin%20and%20Weitao%20Wang%20and%20Haoqian%20Wang%0AAbstract%3A%20%20%203D%20AI-generated%20content%20%28AIGC%29%20has%20made%20it%20increasingly%20accessible%20for%20anyone%0Ato%20become%20a%203D%20content%20creator.%20While%20recent%20methods%20leverage%20Score%0ADistillation%20Sampling%20to%20distill%203D%20objects%20from%20pretrained%20image%20diffusion%0Amodels%2C%20they%20often%20suffer%20from%20inadequate%203D%20priors%2C%20leading%20to%20insufficient%0Amulti-view%20consistency.%20In%20this%20work%2C%20we%20introduce%20NOVA3D%2C%20an%20innovative%0Asingle-image-to-3D%20generation%20framework.%20Our%20key%20insight%20lies%20in%20leveraging%0Astrong%203D%20priors%20from%20a%20pretrained%20video%20diffusion%20model%20and%20integrating%0Ageometric%20information%20during%20multi-view%20video%20fine-tuning.%20To%20facilitate%0Ainformation%20exchange%20between%20color%20and%20geometric%20domains%2C%20we%20propose%20the%0AGeometry-Temporal%20Alignment%20%28GTA%29%20attention%20mechanism%2C%20thereby%20improving%0Ageneralization%20and%20multi-view%20consistency.%20Moreover%2C%20we%20introduce%20the%0Ade-conflict%20geometry%20fusion%20algorithm%2C%20which%20improves%20texture%20fidelity%20by%0Aaddressing%20multi-view%20inaccuracies%20and%20resolving%20discrepancies%20in%20pose%0Aalignment.%20Extensive%20experiments%20validate%20the%20superiority%20of%20NOVA3D%20over%0Aexisting%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOVA3D%253A%2520Normal%2520Aligned%2520Video%2520Diffusion%2520Model%2520for%2520Single%2520Image%2520to%25203D%250A%2520%2520Generation%26entry.906535625%3DYuxiao%2520Yang%2520and%2520Peihao%2520Li%2520and%2520Yuhong%2520Zhang%2520and%2520Junzhe%2520Lu%2520and%2520Xianglong%2520He%2520and%2520Minghan%2520Qin%2520and%2520Weitao%2520Wang%2520and%2520Haoqian%2520Wang%26entry.1292438233%3D%2520%25203D%2520AI-generated%2520content%2520%2528AIGC%2529%2520has%2520made%2520it%2520increasingly%2520accessible%2520for%2520anyone%250Ato%2520become%2520a%25203D%2520content%2520creator.%2520While%2520recent%2520methods%2520leverage%2520Score%250ADistillation%2520Sampling%2520to%2520distill%25203D%2520objects%2520from%2520pretrained%2520image%2520diffusion%250Amodels%252C%2520they%2520often%2520suffer%2520from%2520inadequate%25203D%2520priors%252C%2520leading%2520to%2520insufficient%250Amulti-view%2520consistency.%2520In%2520this%2520work%252C%2520we%2520introduce%2520NOVA3D%252C%2520an%2520innovative%250Asingle-image-to-3D%2520generation%2520framework.%2520Our%2520key%2520insight%2520lies%2520in%2520leveraging%250Astrong%25203D%2520priors%2520from%2520a%2520pretrained%2520video%2520diffusion%2520model%2520and%2520integrating%250Ageometric%2520information%2520during%2520multi-view%2520video%2520fine-tuning.%2520To%2520facilitate%250Ainformation%2520exchange%2520between%2520color%2520and%2520geometric%2520domains%252C%2520we%2520propose%2520the%250AGeometry-Temporal%2520Alignment%2520%2528GTA%2529%2520attention%2520mechanism%252C%2520thereby%2520improving%250Ageneralization%2520and%2520multi-view%2520consistency.%2520Moreover%252C%2520we%2520introduce%2520the%250Ade-conflict%2520geometry%2520fusion%2520algorithm%252C%2520which%2520improves%2520texture%2520fidelity%2520by%250Aaddressing%2520multi-view%2520inaccuracies%2520and%2520resolving%2520discrepancies%2520in%2520pose%250Aalignment.%2520Extensive%2520experiments%2520validate%2520the%2520superiority%2520of%2520NOVA3D%2520over%250Aexisting%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOVA3D%3A%20Normal%20Aligned%20Video%20Diffusion%20Model%20for%20Single%20Image%20to%203D%0A%20%20Generation&entry.906535625=Yuxiao%20Yang%20and%20Peihao%20Li%20and%20Yuhong%20Zhang%20and%20Junzhe%20Lu%20and%20Xianglong%20He%20and%20Minghan%20Qin%20and%20Weitao%20Wang%20and%20Haoqian%20Wang&entry.1292438233=%20%203D%20AI-generated%20content%20%28AIGC%29%20has%20made%20it%20increasingly%20accessible%20for%20anyone%0Ato%20become%20a%203D%20content%20creator.%20While%20recent%20methods%20leverage%20Score%0ADistillation%20Sampling%20to%20distill%203D%20objects%20from%20pretrained%20image%20diffusion%0Amodels%2C%20they%20often%20suffer%20from%20inadequate%203D%20priors%2C%20leading%20to%20insufficient%0Amulti-view%20consistency.%20In%20this%20work%2C%20we%20introduce%20NOVA3D%2C%20an%20innovative%0Asingle-image-to-3D%20generation%20framework.%20Our%20key%20insight%20lies%20in%20leveraging%0Astrong%203D%20priors%20from%20a%20pretrained%20video%20diffusion%20model%20and%20integrating%0Ageometric%20information%20during%20multi-view%20video%20fine-tuning.%20To%20facilitate%0Ainformation%20exchange%20between%20color%20and%20geometric%20domains%2C%20we%20propose%20the%0AGeometry-Temporal%20Alignment%20%28GTA%29%20attention%20mechanism%2C%20thereby%20improving%0Ageneralization%20and%20multi-view%20consistency.%20Moreover%2C%20we%20introduce%20the%0Ade-conflict%20geometry%20fusion%20algorithm%2C%20which%20improves%20texture%20fidelity%20by%0Aaddressing%20multi-view%20inaccuracies%20and%20resolving%20discrepancies%20in%20pose%0Aalignment.%20Extensive%20experiments%20validate%20the%20superiority%20of%20NOVA3D%20over%0Aexisting%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07698v1&entry.124074799=Read"},
{"title": "GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design\n  and Generic Garment Modeling", "author": "Siran Li and Chen Liu and Ruiyang Liu and Zhendong Wang and Gaofeng He and Yong-Lu Li and Xiaogang Jin and Huamin Wang", "abstract": "  Realistic digital garment modeling remains a labor-intensive task due to the\nintricate process of translating 2D sewing patterns into high-fidelity,\nsimulation-ready 3D garments. We introduce GarmageNet, a unified generative\nframework that automates the creation of 2D sewing patterns, the construction\nof sewing relationships, and the synthesis of 3D garment initializations\ncompatible with physics-based simulation. Central to our approach is Garmage, a\nnovel garment representation that encodes each panel as a structured geometry\nimage, effectively bridging the semantic and geometric gap between 2D\nstructural patterns and 3D garment shapes. GarmageNet employs a latent\ndiffusion transformer to synthesize panel-wise geometry images and integrates\nGarmageJigsaw, a neural module for predicting point-to-point sewing connections\nalong panel contours. To support training and evaluation, we build GarmageSet,\na large-scale dataset comprising over 10,000 professionally designed garments\nwith detailed structural and style annotations. Our method demonstrates\nversatility and efficacy across multiple application scenarios, including\nscalable garment generation from multi-modal design concepts (text prompts,\nsketches, photographs), automatic modeling from raw flat sewing patterns,\npattern recovery from unstructured point clouds, and progressive garment\nediting using conventional instructions-laying the foundation for fully\nautomated, production-ready pipelines in digital fashion. Project page:\nhttps://style3d.github.io/garmagenet.\n", "link": "http://arxiv.org/abs/2504.01483v3", "date": "2025-06-09", "relevancy": 2.7428, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7619}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6378}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarmageNet%3A%20A%20Multimodal%20Generative%20Framework%20for%20Sewing%20Pattern%20Design%0A%20%20and%20Generic%20Garment%20Modeling&body=Title%3A%20GarmageNet%3A%20A%20Multimodal%20Generative%20Framework%20for%20Sewing%20Pattern%20Design%0A%20%20and%20Generic%20Garment%20Modeling%0AAuthor%3A%20Siran%20Li%20and%20Chen%20Liu%20and%20Ruiyang%20Liu%20and%20Zhendong%20Wang%20and%20Gaofeng%20He%20and%20Yong-Lu%20Li%20and%20Xiaogang%20Jin%20and%20Huamin%20Wang%0AAbstract%3A%20%20%20Realistic%20digital%20garment%20modeling%20remains%20a%20labor-intensive%20task%20due%20to%20the%0Aintricate%20process%20of%20translating%202D%20sewing%20patterns%20into%20high-fidelity%2C%0Asimulation-ready%203D%20garments.%20We%20introduce%20GarmageNet%2C%20a%20unified%20generative%0Aframework%20that%20automates%20the%20creation%20of%202D%20sewing%20patterns%2C%20the%20construction%0Aof%20sewing%20relationships%2C%20and%20the%20synthesis%20of%203D%20garment%20initializations%0Acompatible%20with%20physics-based%20simulation.%20Central%20to%20our%20approach%20is%20Garmage%2C%20a%0Anovel%20garment%20representation%20that%20encodes%20each%20panel%20as%20a%20structured%20geometry%0Aimage%2C%20effectively%20bridging%20the%20semantic%20and%20geometric%20gap%20between%202D%0Astructural%20patterns%20and%203D%20garment%20shapes.%20GarmageNet%20employs%20a%20latent%0Adiffusion%20transformer%20to%20synthesize%20panel-wise%20geometry%20images%20and%20integrates%0AGarmageJigsaw%2C%20a%20neural%20module%20for%20predicting%20point-to-point%20sewing%20connections%0Aalong%20panel%20contours.%20To%20support%20training%20and%20evaluation%2C%20we%20build%20GarmageSet%2C%0Aa%20large-scale%20dataset%20comprising%20over%2010%2C000%20professionally%20designed%20garments%0Awith%20detailed%20structural%20and%20style%20annotations.%20Our%20method%20demonstrates%0Aversatility%20and%20efficacy%20across%20multiple%20application%20scenarios%2C%20including%0Ascalable%20garment%20generation%20from%20multi-modal%20design%20concepts%20%28text%20prompts%2C%0Asketches%2C%20photographs%29%2C%20automatic%20modeling%20from%20raw%20flat%20sewing%20patterns%2C%0Apattern%20recovery%20from%20unstructured%20point%20clouds%2C%20and%20progressive%20garment%0Aediting%20using%20conventional%20instructions-laying%20the%20foundation%20for%20fully%0Aautomated%2C%20production-ready%20pipelines%20in%20digital%20fashion.%20Project%20page%3A%0Ahttps%3A//style3d.github.io/garmagenet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01483v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarmageNet%253A%2520A%2520Multimodal%2520Generative%2520Framework%2520for%2520Sewing%2520Pattern%2520Design%250A%2520%2520and%2520Generic%2520Garment%2520Modeling%26entry.906535625%3DSiran%2520Li%2520and%2520Chen%2520Liu%2520and%2520Ruiyang%2520Liu%2520and%2520Zhendong%2520Wang%2520and%2520Gaofeng%2520He%2520and%2520Yong-Lu%2520Li%2520and%2520Xiaogang%2520Jin%2520and%2520Huamin%2520Wang%26entry.1292438233%3D%2520%2520Realistic%2520digital%2520garment%2520modeling%2520remains%2520a%2520labor-intensive%2520task%2520due%2520to%2520the%250Aintricate%2520process%2520of%2520translating%25202D%2520sewing%2520patterns%2520into%2520high-fidelity%252C%250Asimulation-ready%25203D%2520garments.%2520We%2520introduce%2520GarmageNet%252C%2520a%2520unified%2520generative%250Aframework%2520that%2520automates%2520the%2520creation%2520of%25202D%2520sewing%2520patterns%252C%2520the%2520construction%250Aof%2520sewing%2520relationships%252C%2520and%2520the%2520synthesis%2520of%25203D%2520garment%2520initializations%250Acompatible%2520with%2520physics-based%2520simulation.%2520Central%2520to%2520our%2520approach%2520is%2520Garmage%252C%2520a%250Anovel%2520garment%2520representation%2520that%2520encodes%2520each%2520panel%2520as%2520a%2520structured%2520geometry%250Aimage%252C%2520effectively%2520bridging%2520the%2520semantic%2520and%2520geometric%2520gap%2520between%25202D%250Astructural%2520patterns%2520and%25203D%2520garment%2520shapes.%2520GarmageNet%2520employs%2520a%2520latent%250Adiffusion%2520transformer%2520to%2520synthesize%2520panel-wise%2520geometry%2520images%2520and%2520integrates%250AGarmageJigsaw%252C%2520a%2520neural%2520module%2520for%2520predicting%2520point-to-point%2520sewing%2520connections%250Aalong%2520panel%2520contours.%2520To%2520support%2520training%2520and%2520evaluation%252C%2520we%2520build%2520GarmageSet%252C%250Aa%2520large-scale%2520dataset%2520comprising%2520over%252010%252C000%2520professionally%2520designed%2520garments%250Awith%2520detailed%2520structural%2520and%2520style%2520annotations.%2520Our%2520method%2520demonstrates%250Aversatility%2520and%2520efficacy%2520across%2520multiple%2520application%2520scenarios%252C%2520including%250Ascalable%2520garment%2520generation%2520from%2520multi-modal%2520design%2520concepts%2520%2528text%2520prompts%252C%250Asketches%252C%2520photographs%2529%252C%2520automatic%2520modeling%2520from%2520raw%2520flat%2520sewing%2520patterns%252C%250Apattern%2520recovery%2520from%2520unstructured%2520point%2520clouds%252C%2520and%2520progressive%2520garment%250Aediting%2520using%2520conventional%2520instructions-laying%2520the%2520foundation%2520for%2520fully%250Aautomated%252C%2520production-ready%2520pipelines%2520in%2520digital%2520fashion.%2520Project%2520page%253A%250Ahttps%253A//style3d.github.io/garmagenet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01483v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarmageNet%3A%20A%20Multimodal%20Generative%20Framework%20for%20Sewing%20Pattern%20Design%0A%20%20and%20Generic%20Garment%20Modeling&entry.906535625=Siran%20Li%20and%20Chen%20Liu%20and%20Ruiyang%20Liu%20and%20Zhendong%20Wang%20and%20Gaofeng%20He%20and%20Yong-Lu%20Li%20and%20Xiaogang%20Jin%20and%20Huamin%20Wang&entry.1292438233=%20%20Realistic%20digital%20garment%20modeling%20remains%20a%20labor-intensive%20task%20due%20to%20the%0Aintricate%20process%20of%20translating%202D%20sewing%20patterns%20into%20high-fidelity%2C%0Asimulation-ready%203D%20garments.%20We%20introduce%20GarmageNet%2C%20a%20unified%20generative%0Aframework%20that%20automates%20the%20creation%20of%202D%20sewing%20patterns%2C%20the%20construction%0Aof%20sewing%20relationships%2C%20and%20the%20synthesis%20of%203D%20garment%20initializations%0Acompatible%20with%20physics-based%20simulation.%20Central%20to%20our%20approach%20is%20Garmage%2C%20a%0Anovel%20garment%20representation%20that%20encodes%20each%20panel%20as%20a%20structured%20geometry%0Aimage%2C%20effectively%20bridging%20the%20semantic%20and%20geometric%20gap%20between%202D%0Astructural%20patterns%20and%203D%20garment%20shapes.%20GarmageNet%20employs%20a%20latent%0Adiffusion%20transformer%20to%20synthesize%20panel-wise%20geometry%20images%20and%20integrates%0AGarmageJigsaw%2C%20a%20neural%20module%20for%20predicting%20point-to-point%20sewing%20connections%0Aalong%20panel%20contours.%20To%20support%20training%20and%20evaluation%2C%20we%20build%20GarmageSet%2C%0Aa%20large-scale%20dataset%20comprising%20over%2010%2C000%20professionally%20designed%20garments%0Awith%20detailed%20structural%20and%20style%20annotations.%20Our%20method%20demonstrates%0Aversatility%20and%20efficacy%20across%20multiple%20application%20scenarios%2C%20including%0Ascalable%20garment%20generation%20from%20multi-modal%20design%20concepts%20%28text%20prompts%2C%0Asketches%2C%20photographs%29%2C%20automatic%20modeling%20from%20raw%20flat%20sewing%20patterns%2C%0Apattern%20recovery%20from%20unstructured%20point%20clouds%2C%20and%20progressive%20garment%0Aediting%20using%20conventional%20instructions-laying%20the%20foundation%20for%20fully%0Aautomated%2C%20production-ready%20pipelines%20in%20digital%20fashion.%20Project%20page%3A%0Ahttps%3A//style3d.github.io/garmagenet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01483v3&entry.124074799=Read"},
{"title": "Scene Exploration by Vision-Language Models", "author": "Venkatesh Sripada and Samuel Carter and Frank Guerin and Amir Ghalamzan", "abstract": "  Active perception enables robots to dynamically gather information by\nadjusting their viewpoints, a crucial capability for interacting with complex,\npartially observable environments. In this paper, we present AP-VLM, a novel\nframework that combines active perception with a Vision-Language Model (VLM) to\nguide robotic exploration and answer semantic queries. Using a 3D virtual grid\noverlaid on the scene and orientation adjustments, AP-VLM allows a robotic\nmanipulator to intelligently select optimal viewpoints and orientations to\nresolve challenging tasks, such as identifying objects in occluded or inclined\npositions. We evaluate our system on two robotic platforms: a 7-DOF Franka\nPanda and a 6-DOF UR5, across various scenes with differing object\nconfigurations. Our results demonstrate that AP-VLM significantly outperforms\npassive perception methods and baseline models, including Toward Grounded\nCommon Sense Reasoning (TGCSR), particularly in scenarios where fixed camera\nviews are inadequate. The adaptability of AP-VLM in real-world settings shows\npromise for enhancing robotic systems' understanding of complex environments,\nbridging the gap between high-level semantic reasoning and low-level control.\n", "link": "http://arxiv.org/abs/2409.17641v2", "date": "2025-06-09", "relevancy": 2.7038, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene%20Exploration%20by%20Vision-Language%20Models&body=Title%3A%20Scene%20Exploration%20by%20Vision-Language%20Models%0AAuthor%3A%20Venkatesh%20Sripada%20and%20Samuel%20Carter%20and%20Frank%20Guerin%20and%20Amir%20Ghalamzan%0AAbstract%3A%20%20%20Active%20perception%20enables%20robots%20to%20dynamically%20gather%20information%20by%0Aadjusting%20their%20viewpoints%2C%20a%20crucial%20capability%20for%20interacting%20with%20complex%2C%0Apartially%20observable%20environments.%20In%20this%20paper%2C%20we%20present%20AP-VLM%2C%20a%20novel%0Aframework%20that%20combines%20active%20perception%20with%20a%20Vision-Language%20Model%20%28VLM%29%20to%0Aguide%20robotic%20exploration%20and%20answer%20semantic%20queries.%20Using%20a%203D%20virtual%20grid%0Aoverlaid%20on%20the%20scene%20and%20orientation%20adjustments%2C%20AP-VLM%20allows%20a%20robotic%0Amanipulator%20to%20intelligently%20select%20optimal%20viewpoints%20and%20orientations%20to%0Aresolve%20challenging%20tasks%2C%20such%20as%20identifying%20objects%20in%20occluded%20or%20inclined%0Apositions.%20We%20evaluate%20our%20system%20on%20two%20robotic%20platforms%3A%20a%207-DOF%20Franka%0APanda%20and%20a%206-DOF%20UR5%2C%20across%20various%20scenes%20with%20differing%20object%0Aconfigurations.%20Our%20results%20demonstrate%20that%20AP-VLM%20significantly%20outperforms%0Apassive%20perception%20methods%20and%20baseline%20models%2C%20including%20Toward%20Grounded%0ACommon%20Sense%20Reasoning%20%28TGCSR%29%2C%20particularly%20in%20scenarios%20where%20fixed%20camera%0Aviews%20are%20inadequate.%20The%20adaptability%20of%20AP-VLM%20in%20real-world%20settings%20shows%0Apromise%20for%20enhancing%20robotic%20systems%27%20understanding%20of%20complex%20environments%2C%0Abridging%20the%20gap%20between%20high-level%20semantic%20reasoning%20and%20low-level%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene%2520Exploration%2520by%2520Vision-Language%2520Models%26entry.906535625%3DVenkatesh%2520Sripada%2520and%2520Samuel%2520Carter%2520and%2520Frank%2520Guerin%2520and%2520Amir%2520Ghalamzan%26entry.1292438233%3D%2520%2520Active%2520perception%2520enables%2520robots%2520to%2520dynamically%2520gather%2520information%2520by%250Aadjusting%2520their%2520viewpoints%252C%2520a%2520crucial%2520capability%2520for%2520interacting%2520with%2520complex%252C%250Apartially%2520observable%2520environments.%2520In%2520this%2520paper%252C%2520we%2520present%2520AP-VLM%252C%2520a%2520novel%250Aframework%2520that%2520combines%2520active%2520perception%2520with%2520a%2520Vision-Language%2520Model%2520%2528VLM%2529%2520to%250Aguide%2520robotic%2520exploration%2520and%2520answer%2520semantic%2520queries.%2520Using%2520a%25203D%2520virtual%2520grid%250Aoverlaid%2520on%2520the%2520scene%2520and%2520orientation%2520adjustments%252C%2520AP-VLM%2520allows%2520a%2520robotic%250Amanipulator%2520to%2520intelligently%2520select%2520optimal%2520viewpoints%2520and%2520orientations%2520to%250Aresolve%2520challenging%2520tasks%252C%2520such%2520as%2520identifying%2520objects%2520in%2520occluded%2520or%2520inclined%250Apositions.%2520We%2520evaluate%2520our%2520system%2520on%2520two%2520robotic%2520platforms%253A%2520a%25207-DOF%2520Franka%250APanda%2520and%2520a%25206-DOF%2520UR5%252C%2520across%2520various%2520scenes%2520with%2520differing%2520object%250Aconfigurations.%2520Our%2520results%2520demonstrate%2520that%2520AP-VLM%2520significantly%2520outperforms%250Apassive%2520perception%2520methods%2520and%2520baseline%2520models%252C%2520including%2520Toward%2520Grounded%250ACommon%2520Sense%2520Reasoning%2520%2528TGCSR%2529%252C%2520particularly%2520in%2520scenarios%2520where%2520fixed%2520camera%250Aviews%2520are%2520inadequate.%2520The%2520adaptability%2520of%2520AP-VLM%2520in%2520real-world%2520settings%2520shows%250Apromise%2520for%2520enhancing%2520robotic%2520systems%2527%2520understanding%2520of%2520complex%2520environments%252C%250Abridging%2520the%2520gap%2520between%2520high-level%2520semantic%2520reasoning%2520and%2520low-level%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Exploration%20by%20Vision-Language%20Models&entry.906535625=Venkatesh%20Sripada%20and%20Samuel%20Carter%20and%20Frank%20Guerin%20and%20Amir%20Ghalamzan&entry.1292438233=%20%20Active%20perception%20enables%20robots%20to%20dynamically%20gather%20information%20by%0Aadjusting%20their%20viewpoints%2C%20a%20crucial%20capability%20for%20interacting%20with%20complex%2C%0Apartially%20observable%20environments.%20In%20this%20paper%2C%20we%20present%20AP-VLM%2C%20a%20novel%0Aframework%20that%20combines%20active%20perception%20with%20a%20Vision-Language%20Model%20%28VLM%29%20to%0Aguide%20robotic%20exploration%20and%20answer%20semantic%20queries.%20Using%20a%203D%20virtual%20grid%0Aoverlaid%20on%20the%20scene%20and%20orientation%20adjustments%2C%20AP-VLM%20allows%20a%20robotic%0Amanipulator%20to%20intelligently%20select%20optimal%20viewpoints%20and%20orientations%20to%0Aresolve%20challenging%20tasks%2C%20such%20as%20identifying%20objects%20in%20occluded%20or%20inclined%0Apositions.%20We%20evaluate%20our%20system%20on%20two%20robotic%20platforms%3A%20a%207-DOF%20Franka%0APanda%20and%20a%206-DOF%20UR5%2C%20across%20various%20scenes%20with%20differing%20object%0Aconfigurations.%20Our%20results%20demonstrate%20that%20AP-VLM%20significantly%20outperforms%0Apassive%20perception%20methods%20and%20baseline%20models%2C%20including%20Toward%20Grounded%0ACommon%20Sense%20Reasoning%20%28TGCSR%29%2C%20particularly%20in%20scenarios%20where%20fixed%20camera%0Aviews%20are%20inadequate.%20The%20adaptability%20of%20AP-VLM%20in%20real-world%20settings%20shows%0Apromise%20for%20enhancing%20robotic%20systems%27%20understanding%20of%20complex%20environments%2C%0Abridging%20the%20gap%20between%20high-level%20semantic%20reasoning%20and%20low-level%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17641v2&entry.124074799=Read"},
{"title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature\n  fusion for deformation table cell instance segmentation", "author": "Long Liu and Cihui Yang", "abstract": "  Table structure recognition is a key task in document analysis. However, the\ngeometric deformation in deformed tables causes a weak correlation between\ncontent information and structure, resulting in downstream tasks not being able\nto obtain accurate content information. To obtain fine-grained spatial\ncoordinates of cells, we propose the OG-HFYOLO model, which enhances the edge\nresponse by Gradient Orientation-aware Extractor, combines a Heterogeneous\nKernel Cross Fusion module and a scale-aware loss function to adapt to\nmulti-scale objective features, and introduces mask-driven non-maximal\nsuppression in the post-processing, which replaces the traditional bounding box\nsuppression mechanism. Furthermore, we also propose a data generator, filling\nthe gap in the dataset for fine-grained deformation table cell spatial\ncoordinate localization, and derive a large-scale dataset named Deformation\nWired Table (DWTAL). Experiments show that our proposed model demonstrates\nexcellent segmentation accuracy on all mainstream instance segmentation models.\nThe dataset and the source code are open source:\nhttps://github.com/justliulong/OGHFYOLO.\n", "link": "http://arxiv.org/abs/2504.20682v3", "date": "2025-06-09", "relevancy": 2.7029, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5699}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5274}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OG-HFYOLO%20%3AOrientation%20gradient%20guidance%20and%20heterogeneous%20feature%0A%20%20fusion%20for%20deformation%20table%20cell%20instance%20segmentation&body=Title%3A%20OG-HFYOLO%20%3AOrientation%20gradient%20guidance%20and%20heterogeneous%20feature%0A%20%20fusion%20for%20deformation%20table%20cell%20instance%20segmentation%0AAuthor%3A%20Long%20Liu%20and%20Cihui%20Yang%0AAbstract%3A%20%20%20Table%20structure%20recognition%20is%20a%20key%20task%20in%20document%20analysis.%20However%2C%20the%0Ageometric%20deformation%20in%20deformed%20tables%20causes%20a%20weak%20correlation%20between%0Acontent%20information%20and%20structure%2C%20resulting%20in%20downstream%20tasks%20not%20being%20able%0Ato%20obtain%20accurate%20content%20information.%20To%20obtain%20fine-grained%20spatial%0Acoordinates%20of%20cells%2C%20we%20propose%20the%20OG-HFYOLO%20model%2C%20which%20enhances%20the%20edge%0Aresponse%20by%20Gradient%20Orientation-aware%20Extractor%2C%20combines%20a%20Heterogeneous%0AKernel%20Cross%20Fusion%20module%20and%20a%20scale-aware%20loss%20function%20to%20adapt%20to%0Amulti-scale%20objective%20features%2C%20and%20introduces%20mask-driven%20non-maximal%0Asuppression%20in%20the%20post-processing%2C%20which%20replaces%20the%20traditional%20bounding%20box%0Asuppression%20mechanism.%20Furthermore%2C%20we%20also%20propose%20a%20data%20generator%2C%20filling%0Athe%20gap%20in%20the%20dataset%20for%20fine-grained%20deformation%20table%20cell%20spatial%0Acoordinate%20localization%2C%20and%20derive%20a%20large-scale%20dataset%20named%20Deformation%0AWired%20Table%20%28DWTAL%29.%20Experiments%20show%20that%20our%20proposed%20model%20demonstrates%0Aexcellent%20segmentation%20accuracy%20on%20all%20mainstream%20instance%20segmentation%20models.%0AThe%20dataset%20and%20the%20source%20code%20are%20open%20source%3A%0Ahttps%3A//github.com/justliulong/OGHFYOLO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20682v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOG-HFYOLO%2520%253AOrientation%2520gradient%2520guidance%2520and%2520heterogeneous%2520feature%250A%2520%2520fusion%2520for%2520deformation%2520table%2520cell%2520instance%2520segmentation%26entry.906535625%3DLong%2520Liu%2520and%2520Cihui%2520Yang%26entry.1292438233%3D%2520%2520Table%2520structure%2520recognition%2520is%2520a%2520key%2520task%2520in%2520document%2520analysis.%2520However%252C%2520the%250Ageometric%2520deformation%2520in%2520deformed%2520tables%2520causes%2520a%2520weak%2520correlation%2520between%250Acontent%2520information%2520and%2520structure%252C%2520resulting%2520in%2520downstream%2520tasks%2520not%2520being%2520able%250Ato%2520obtain%2520accurate%2520content%2520information.%2520To%2520obtain%2520fine-grained%2520spatial%250Acoordinates%2520of%2520cells%252C%2520we%2520propose%2520the%2520OG-HFYOLO%2520model%252C%2520which%2520enhances%2520the%2520edge%250Aresponse%2520by%2520Gradient%2520Orientation-aware%2520Extractor%252C%2520combines%2520a%2520Heterogeneous%250AKernel%2520Cross%2520Fusion%2520module%2520and%2520a%2520scale-aware%2520loss%2520function%2520to%2520adapt%2520to%250Amulti-scale%2520objective%2520features%252C%2520and%2520introduces%2520mask-driven%2520non-maximal%250Asuppression%2520in%2520the%2520post-processing%252C%2520which%2520replaces%2520the%2520traditional%2520bounding%2520box%250Asuppression%2520mechanism.%2520Furthermore%252C%2520we%2520also%2520propose%2520a%2520data%2520generator%252C%2520filling%250Athe%2520gap%2520in%2520the%2520dataset%2520for%2520fine-grained%2520deformation%2520table%2520cell%2520spatial%250Acoordinate%2520localization%252C%2520and%2520derive%2520a%2520large-scale%2520dataset%2520named%2520Deformation%250AWired%2520Table%2520%2528DWTAL%2529.%2520Experiments%2520show%2520that%2520our%2520proposed%2520model%2520demonstrates%250Aexcellent%2520segmentation%2520accuracy%2520on%2520all%2520mainstream%2520instance%2520segmentation%2520models.%250AThe%2520dataset%2520and%2520the%2520source%2520code%2520are%2520open%2520source%253A%250Ahttps%253A//github.com/justliulong/OGHFYOLO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20682v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OG-HFYOLO%20%3AOrientation%20gradient%20guidance%20and%20heterogeneous%20feature%0A%20%20fusion%20for%20deformation%20table%20cell%20instance%20segmentation&entry.906535625=Long%20Liu%20and%20Cihui%20Yang&entry.1292438233=%20%20Table%20structure%20recognition%20is%20a%20key%20task%20in%20document%20analysis.%20However%2C%20the%0Ageometric%20deformation%20in%20deformed%20tables%20causes%20a%20weak%20correlation%20between%0Acontent%20information%20and%20structure%2C%20resulting%20in%20downstream%20tasks%20not%20being%20able%0Ato%20obtain%20accurate%20content%20information.%20To%20obtain%20fine-grained%20spatial%0Acoordinates%20of%20cells%2C%20we%20propose%20the%20OG-HFYOLO%20model%2C%20which%20enhances%20the%20edge%0Aresponse%20by%20Gradient%20Orientation-aware%20Extractor%2C%20combines%20a%20Heterogeneous%0AKernel%20Cross%20Fusion%20module%20and%20a%20scale-aware%20loss%20function%20to%20adapt%20to%0Amulti-scale%20objective%20features%2C%20and%20introduces%20mask-driven%20non-maximal%0Asuppression%20in%20the%20post-processing%2C%20which%20replaces%20the%20traditional%20bounding%20box%0Asuppression%20mechanism.%20Furthermore%2C%20we%20also%20propose%20a%20data%20generator%2C%20filling%0Athe%20gap%20in%20the%20dataset%20for%20fine-grained%20deformation%20table%20cell%20spatial%0Acoordinate%20localization%2C%20and%20derive%20a%20large-scale%20dataset%20named%20Deformation%0AWired%20Table%20%28DWTAL%29.%20Experiments%20show%20that%20our%20proposed%20model%20demonstrates%0Aexcellent%20segmentation%20accuracy%20on%20all%20mainstream%20instance%20segmentation%20models.%0AThe%20dataset%20and%20the%20source%20code%20are%20open%20source%3A%0Ahttps%3A//github.com/justliulong/OGHFYOLO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20682v3&entry.124074799=Read"},
{"title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular\n  Videos", "author": "Zhen Xu and Zhengqin Li and Zhao Dong and Xiaowei Zhou and Richard Newcombe and Zhaoyang Lv", "abstract": "  We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene\nreconstruction, trained entirely on real-world monocular posed videos. Using 4D\nGaussian as an inductive bias, 4DGT unifies static and dynamic components,\nenabling the modeling of complex, time-varying environments with varying object\nlifespans. We proposed a novel density control strategy in training, which\nenables our 4DGT to handle longer space-time input and remain efficient\nrendering at runtime. Our model processes 64 consecutive posed frames in a\nrolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike\noptimization-based methods, 4DGT performs purely feed-forward inference,\nreducing reconstruction time from hours to seconds and scaling effectively to\nlong video sequences. Trained only on large-scale monocular posed video\ndatasets, 4DGT can outperform prior Gaussian-based networks significantly in\nreal-world videos and achieve on-par accuracy with optimization-based methods\non cross-domain videos. Project page: https://4dgt.github.io\n", "link": "http://arxiv.org/abs/2506.08015v1", "date": "2025-06-09", "relevancy": 2.6851, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6952}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6548}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DGT%3A%20Learning%20a%204D%20Gaussian%20Transformer%20Using%20Real-World%20Monocular%0A%20%20Videos&body=Title%3A%204DGT%3A%20Learning%20a%204D%20Gaussian%20Transformer%20Using%20Real-World%20Monocular%0A%20%20Videos%0AAuthor%3A%20Zhen%20Xu%20and%20Zhengqin%20Li%20and%20Zhao%20Dong%20and%20Xiaowei%20Zhou%20and%20Richard%20Newcombe%20and%20Zhaoyang%20Lv%0AAbstract%3A%20%20%20We%20propose%204DGT%2C%20a%204D%20Gaussian-based%20Transformer%20model%20for%20dynamic%20scene%0Areconstruction%2C%20trained%20entirely%20on%20real-world%20monocular%20posed%20videos.%20Using%204D%0AGaussian%20as%20an%20inductive%20bias%2C%204DGT%20unifies%20static%20and%20dynamic%20components%2C%0Aenabling%20the%20modeling%20of%20complex%2C%20time-varying%20environments%20with%20varying%20object%0Alifespans.%20We%20proposed%20a%20novel%20density%20control%20strategy%20in%20training%2C%20which%0Aenables%20our%204DGT%20to%20handle%20longer%20space-time%20input%20and%20remain%20efficient%0Arendering%20at%20runtime.%20Our%20model%20processes%2064%20consecutive%20posed%20frames%20in%20a%0Arolling-window%20fashion%2C%20predicting%20consistent%204D%20Gaussians%20in%20the%20scene.%20Unlike%0Aoptimization-based%20methods%2C%204DGT%20performs%20purely%20feed-forward%20inference%2C%0Areducing%20reconstruction%20time%20from%20hours%20to%20seconds%20and%20scaling%20effectively%20to%0Along%20video%20sequences.%20Trained%20only%20on%20large-scale%20monocular%20posed%20video%0Adatasets%2C%204DGT%20can%20outperform%20prior%20Gaussian-based%20networks%20significantly%20in%0Areal-world%20videos%20and%20achieve%20on-par%20accuracy%20with%20optimization-based%20methods%0Aon%20cross-domain%20videos.%20Project%20page%3A%20https%3A//4dgt.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DGT%253A%2520Learning%2520a%25204D%2520Gaussian%2520Transformer%2520Using%2520Real-World%2520Monocular%250A%2520%2520Videos%26entry.906535625%3DZhen%2520Xu%2520and%2520Zhengqin%2520Li%2520and%2520Zhao%2520Dong%2520and%2520Xiaowei%2520Zhou%2520and%2520Richard%2520Newcombe%2520and%2520Zhaoyang%2520Lv%26entry.1292438233%3D%2520%2520We%2520propose%25204DGT%252C%2520a%25204D%2520Gaussian-based%2520Transformer%2520model%2520for%2520dynamic%2520scene%250Areconstruction%252C%2520trained%2520entirely%2520on%2520real-world%2520monocular%2520posed%2520videos.%2520Using%25204D%250AGaussian%2520as%2520an%2520inductive%2520bias%252C%25204DGT%2520unifies%2520static%2520and%2520dynamic%2520components%252C%250Aenabling%2520the%2520modeling%2520of%2520complex%252C%2520time-varying%2520environments%2520with%2520varying%2520object%250Alifespans.%2520We%2520proposed%2520a%2520novel%2520density%2520control%2520strategy%2520in%2520training%252C%2520which%250Aenables%2520our%25204DGT%2520to%2520handle%2520longer%2520space-time%2520input%2520and%2520remain%2520efficient%250Arendering%2520at%2520runtime.%2520Our%2520model%2520processes%252064%2520consecutive%2520posed%2520frames%2520in%2520a%250Arolling-window%2520fashion%252C%2520predicting%2520consistent%25204D%2520Gaussians%2520in%2520the%2520scene.%2520Unlike%250Aoptimization-based%2520methods%252C%25204DGT%2520performs%2520purely%2520feed-forward%2520inference%252C%250Areducing%2520reconstruction%2520time%2520from%2520hours%2520to%2520seconds%2520and%2520scaling%2520effectively%2520to%250Along%2520video%2520sequences.%2520Trained%2520only%2520on%2520large-scale%2520monocular%2520posed%2520video%250Adatasets%252C%25204DGT%2520can%2520outperform%2520prior%2520Gaussian-based%2520networks%2520significantly%2520in%250Areal-world%2520videos%2520and%2520achieve%2520on-par%2520accuracy%2520with%2520optimization-based%2520methods%250Aon%2520cross-domain%2520videos.%2520Project%2520page%253A%2520https%253A//4dgt.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DGT%3A%20Learning%20a%204D%20Gaussian%20Transformer%20Using%20Real-World%20Monocular%0A%20%20Videos&entry.906535625=Zhen%20Xu%20and%20Zhengqin%20Li%20and%20Zhao%20Dong%20and%20Xiaowei%20Zhou%20and%20Richard%20Newcombe%20and%20Zhaoyang%20Lv&entry.1292438233=%20%20We%20propose%204DGT%2C%20a%204D%20Gaussian-based%20Transformer%20model%20for%20dynamic%20scene%0Areconstruction%2C%20trained%20entirely%20on%20real-world%20monocular%20posed%20videos.%20Using%204D%0AGaussian%20as%20an%20inductive%20bias%2C%204DGT%20unifies%20static%20and%20dynamic%20components%2C%0Aenabling%20the%20modeling%20of%20complex%2C%20time-varying%20environments%20with%20varying%20object%0Alifespans.%20We%20proposed%20a%20novel%20density%20control%20strategy%20in%20training%2C%20which%0Aenables%20our%204DGT%20to%20handle%20longer%20space-time%20input%20and%20remain%20efficient%0Arendering%20at%20runtime.%20Our%20model%20processes%2064%20consecutive%20posed%20frames%20in%20a%0Arolling-window%20fashion%2C%20predicting%20consistent%204D%20Gaussians%20in%20the%20scene.%20Unlike%0Aoptimization-based%20methods%2C%204DGT%20performs%20purely%20feed-forward%20inference%2C%0Areducing%20reconstruction%20time%20from%20hours%20to%20seconds%20and%20scaling%20effectively%20to%0Along%20video%20sequences.%20Trained%20only%20on%20large-scale%20monocular%20posed%20video%0Adatasets%2C%204DGT%20can%20outperform%20prior%20Gaussian-based%20networks%20significantly%20in%0Areal-world%20videos%20and%20achieve%20on-par%20accuracy%20with%20optimization-based%20methods%0Aon%20cross-domain%20videos.%20Project%20page%3A%20https%3A//4dgt.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08015v1&entry.124074799=Read"},
{"title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement", "author": "Teng Hu and Zhentao Yu and Zhengguang Zhou and Jiangning Zhang and Yuan Zhou and Qinglin Lu and Ran Yi", "abstract": "  Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.\n", "link": "http://arxiv.org/abs/2506.07848v1", "date": "2025-06-09", "relevancy": 2.662, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7374}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6549}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolyVivid%3A%20Vivid%20Multi-Subject%20Video%20Generation%20with%20Cross-Modal%0A%20%20Interaction%20and%20Enhancement&body=Title%3A%20PolyVivid%3A%20Vivid%20Multi-Subject%20Video%20Generation%20with%20Cross-Modal%0A%20%20Interaction%20and%20Enhancement%0AAuthor%3A%20Teng%20Hu%20and%20Zhentao%20Yu%20and%20Zhengguang%20Zhou%20and%20Jiangning%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Ran%20Yi%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20video%20generation%2C%20existing%20models%20still%20lack%0Afine-grained%20controllability%2C%20especially%20for%20multi-subject%20customization%20with%0Aconsistent%20identity%20and%20interaction.%20In%20this%20paper%2C%20we%20propose%20PolyVivid%2C%20a%0Amulti-subject%20video%20customization%20framework%20that%20enables%20flexible%20and%0Aidentity-consistent%20generation.%20To%20establish%20accurate%20correspondences%20between%0Asubject%20images%20and%20textual%20entities%2C%20we%20design%20a%20VLLM-based%20text-image%20fusion%0Amodule%20that%20embeds%20visual%20identities%20into%20the%20textual%20space%20for%20precise%0Agrounding.%20To%20further%20enhance%20identity%20preservation%20and%20subject%20interaction%2C%20we%0Apropose%20a%203D-RoPE-based%20enhancement%20module%20that%20enables%20structured%0Abidirectional%20fusion%20between%20text%20and%20image%20embeddings.%20Moreover%2C%20we%20develop%20an%0Aattention-inherited%20identity%20injection%20module%20to%20effectively%20inject%20fused%0Aidentity%20features%20into%20the%20video%20generation%20process%2C%20mitigating%20identity%20drift.%0AFinally%2C%20we%20construct%20an%20MLLM-based%20data%20pipeline%20that%20combines%20MLLM-based%0Agrounding%2C%20segmentation%2C%20and%20a%20clique-based%20subject%20consolidation%20strategy%20to%0Aproduce%20high-quality%20multi-subject%20data%2C%20effectively%20enhancing%20subject%0Adistinction%20and%20reducing%20ambiguity%20in%20downstream%20video%20generation.%20Extensive%0Aexperiments%20demonstrate%20that%20PolyVivid%20achieves%20superior%20performance%20in%0Aidentity%20fidelity%2C%20video%20realism%2C%20and%20subject%20alignment%2C%20outperforming%20existing%0Aopen-source%20and%20commercial%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyVivid%253A%2520Vivid%2520Multi-Subject%2520Video%2520Generation%2520with%2520Cross-Modal%250A%2520%2520Interaction%2520and%2520Enhancement%26entry.906535625%3DTeng%2520Hu%2520and%2520Zhentao%2520Yu%2520and%2520Zhengguang%2520Zhou%2520and%2520Jiangning%2520Zhang%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%2520and%2520Ran%2520Yi%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520video%2520generation%252C%2520existing%2520models%2520still%2520lack%250Afine-grained%2520controllability%252C%2520especially%2520for%2520multi-subject%2520customization%2520with%250Aconsistent%2520identity%2520and%2520interaction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PolyVivid%252C%2520a%250Amulti-subject%2520video%2520customization%2520framework%2520that%2520enables%2520flexible%2520and%250Aidentity-consistent%2520generation.%2520To%2520establish%2520accurate%2520correspondences%2520between%250Asubject%2520images%2520and%2520textual%2520entities%252C%2520we%2520design%2520a%2520VLLM-based%2520text-image%2520fusion%250Amodule%2520that%2520embeds%2520visual%2520identities%2520into%2520the%2520textual%2520space%2520for%2520precise%250Agrounding.%2520To%2520further%2520enhance%2520identity%2520preservation%2520and%2520subject%2520interaction%252C%2520we%250Apropose%2520a%25203D-RoPE-based%2520enhancement%2520module%2520that%2520enables%2520structured%250Abidirectional%2520fusion%2520between%2520text%2520and%2520image%2520embeddings.%2520Moreover%252C%2520we%2520develop%2520an%250Aattention-inherited%2520identity%2520injection%2520module%2520to%2520effectively%2520inject%2520fused%250Aidentity%2520features%2520into%2520the%2520video%2520generation%2520process%252C%2520mitigating%2520identity%2520drift.%250AFinally%252C%2520we%2520construct%2520an%2520MLLM-based%2520data%2520pipeline%2520that%2520combines%2520MLLM-based%250Agrounding%252C%2520segmentation%252C%2520and%2520a%2520clique-based%2520subject%2520consolidation%2520strategy%2520to%250Aproduce%2520high-quality%2520multi-subject%2520data%252C%2520effectively%2520enhancing%2520subject%250Adistinction%2520and%2520reducing%2520ambiguity%2520in%2520downstream%2520video%2520generation.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520PolyVivid%2520achieves%2520superior%2520performance%2520in%250Aidentity%2520fidelity%252C%2520video%2520realism%252C%2520and%2520subject%2520alignment%252C%2520outperforming%2520existing%250Aopen-source%2520and%2520commercial%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolyVivid%3A%20Vivid%20Multi-Subject%20Video%20Generation%20with%20Cross-Modal%0A%20%20Interaction%20and%20Enhancement&entry.906535625=Teng%20Hu%20and%20Zhentao%20Yu%20and%20Zhengguang%20Zhou%20and%20Jiangning%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Ran%20Yi&entry.1292438233=%20%20Despite%20recent%20advances%20in%20video%20generation%2C%20existing%20models%20still%20lack%0Afine-grained%20controllability%2C%20especially%20for%20multi-subject%20customization%20with%0Aconsistent%20identity%20and%20interaction.%20In%20this%20paper%2C%20we%20propose%20PolyVivid%2C%20a%0Amulti-subject%20video%20customization%20framework%20that%20enables%20flexible%20and%0Aidentity-consistent%20generation.%20To%20establish%20accurate%20correspondences%20between%0Asubject%20images%20and%20textual%20entities%2C%20we%20design%20a%20VLLM-based%20text-image%20fusion%0Amodule%20that%20embeds%20visual%20identities%20into%20the%20textual%20space%20for%20precise%0Agrounding.%20To%20further%20enhance%20identity%20preservation%20and%20subject%20interaction%2C%20we%0Apropose%20a%203D-RoPE-based%20enhancement%20module%20that%20enables%20structured%0Abidirectional%20fusion%20between%20text%20and%20image%20embeddings.%20Moreover%2C%20we%20develop%20an%0Aattention-inherited%20identity%20injection%20module%20to%20effectively%20inject%20fused%0Aidentity%20features%20into%20the%20video%20generation%20process%2C%20mitigating%20identity%20drift.%0AFinally%2C%20we%20construct%20an%20MLLM-based%20data%20pipeline%20that%20combines%20MLLM-based%0Agrounding%2C%20segmentation%2C%20and%20a%20clique-based%20subject%20consolidation%20strategy%20to%0Aproduce%20high-quality%20multi-subject%20data%2C%20effectively%20enhancing%20subject%0Adistinction%20and%20reducing%20ambiguity%20in%20downstream%20video%20generation.%20Extensive%0Aexperiments%20demonstrate%20that%20PolyVivid%20achieves%20superior%20performance%20in%0Aidentity%20fidelity%2C%20video%20realism%2C%20and%20subject%20alignment%2C%20outperforming%20existing%0Aopen-source%20and%20commercial%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07848v1&entry.124074799=Read"},
{"title": "Improving large language models with concept-aware fine-tuning", "author": "Michael K. Chen and Xikun Zhang and Jiaxing Huang and Dacheng Tao", "abstract": "  Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm\n", "link": "http://arxiv.org/abs/2506.07833v1", "date": "2025-06-09", "relevancy": 2.6576, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20large%20language%20models%20with%20concept-aware%20fine-tuning&body=Title%3A%20Improving%20large%20language%20models%20with%20concept-aware%20fine-tuning%0AAuthor%3A%20Michael%20K.%20Chen%20and%20Xikun%20Zhang%20and%20Jiaxing%20Huang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20the%20cornerstone%20of%20modern%20AI.%0AHowever%2C%20the%20existing%20paradigm%20of%20next-token%20prediction%20fundamentally%20limits%0Atheir%20ability%20to%20form%20coherent%2C%20high-level%20concepts%2C%20making%20it%20a%20critical%0Abarrier%20to%20human-like%20understanding%20and%20reasoning.%20Take%20the%20phrase%20%22ribonucleic%0Aacid%22%20as%20an%20example%3A%20an%20LLM%20will%20first%20decompose%20it%20into%20tokens%2C%20i.e.%2C%0Aartificial%20text%20fragments%20%28%22rib%22%2C%20%22on%22%2C%20...%29%2C%20then%20learn%20each%20token%0Asequentially%2C%20rather%20than%20grasping%20the%20phrase%20as%20a%20unified%2C%20coherent%20semantic%0Aentity.%20This%20fragmented%20representation%20hinders%20deeper%20conceptual%20understanding%0Aand%2C%20ultimately%2C%20the%20development%20of%20truly%20intelligent%20systems.%20In%20response%2C%20we%0Aintroduce%20Concept-Aware%20Fine-Tuning%20%28CAFT%29%2C%20a%20novel%20multi-token%20training%20method%0Athat%20redefines%20how%20LLMs%20are%20fine-tuned.%20By%20enabling%20the%20learning%20of%20sequences%0Athat%20span%20multiple%20tokens%2C%20this%20method%20fosters%20stronger%20concept-aware%20learning.%0AOur%20experiments%20demonstrate%20significant%20improvements%20compared%20to%20conventional%0Anext-token%20finetuning%20methods%20across%20diverse%20tasks%2C%20including%20traditional%0Aapplications%20like%20text%20summarization%20and%20domain-specific%20ones%20like%20de%20novo%0Aprotein%20design.%20Multi-token%20prediction%20was%20previously%20only%20possible%20in%20the%0Aprohibitively%20expensive%20pretraining%20phase%3B%20CAFT%2C%20to%20our%20knowledge%2C%20is%20the%20first%0Ato%20bring%20the%20multi-token%20setting%20to%20the%20post-training%20phase%2C%20thus%20effectively%0Ademocratizing%20its%20benefits%20for%20the%20broader%20community%20of%20practitioners%20and%0Aresearchers.%20Finally%2C%20the%20unexpected%20effectiveness%20of%20our%20proposed%20method%0Asuggests%20wider%20implications%20for%20the%20machine%20learning%20research%20community.%20All%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/michaelchen-lab/caft-llm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520large%2520language%2520models%2520with%2520concept-aware%2520fine-tuning%26entry.906535625%3DMichael%2520K.%2520Chen%2520and%2520Xikun%2520Zhang%2520and%2520Jiaxing%2520Huang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520become%2520the%2520cornerstone%2520of%2520modern%2520AI.%250AHowever%252C%2520the%2520existing%2520paradigm%2520of%2520next-token%2520prediction%2520fundamentally%2520limits%250Atheir%2520ability%2520to%2520form%2520coherent%252C%2520high-level%2520concepts%252C%2520making%2520it%2520a%2520critical%250Abarrier%2520to%2520human-like%2520understanding%2520and%2520reasoning.%2520Take%2520the%2520phrase%2520%2522ribonucleic%250Aacid%2522%2520as%2520an%2520example%253A%2520an%2520LLM%2520will%2520first%2520decompose%2520it%2520into%2520tokens%252C%2520i.e.%252C%250Aartificial%2520text%2520fragments%2520%2528%2522rib%2522%252C%2520%2522on%2522%252C%2520...%2529%252C%2520then%2520learn%2520each%2520token%250Asequentially%252C%2520rather%2520than%2520grasping%2520the%2520phrase%2520as%2520a%2520unified%252C%2520coherent%2520semantic%250Aentity.%2520This%2520fragmented%2520representation%2520hinders%2520deeper%2520conceptual%2520understanding%250Aand%252C%2520ultimately%252C%2520the%2520development%2520of%2520truly%2520intelligent%2520systems.%2520In%2520response%252C%2520we%250Aintroduce%2520Concept-Aware%2520Fine-Tuning%2520%2528CAFT%2529%252C%2520a%2520novel%2520multi-token%2520training%2520method%250Athat%2520redefines%2520how%2520LLMs%2520are%2520fine-tuned.%2520By%2520enabling%2520the%2520learning%2520of%2520sequences%250Athat%2520span%2520multiple%2520tokens%252C%2520this%2520method%2520fosters%2520stronger%2520concept-aware%2520learning.%250AOur%2520experiments%2520demonstrate%2520significant%2520improvements%2520compared%2520to%2520conventional%250Anext-token%2520finetuning%2520methods%2520across%2520diverse%2520tasks%252C%2520including%2520traditional%250Aapplications%2520like%2520text%2520summarization%2520and%2520domain-specific%2520ones%2520like%2520de%2520novo%250Aprotein%2520design.%2520Multi-token%2520prediction%2520was%2520previously%2520only%2520possible%2520in%2520the%250Aprohibitively%2520expensive%2520pretraining%2520phase%253B%2520CAFT%252C%2520to%2520our%2520knowledge%252C%2520is%2520the%2520first%250Ato%2520bring%2520the%2520multi-token%2520setting%2520to%2520the%2520post-training%2520phase%252C%2520thus%2520effectively%250Ademocratizing%2520its%2520benefits%2520for%2520the%2520broader%2520community%2520of%2520practitioners%2520and%250Aresearchers.%2520Finally%252C%2520the%2520unexpected%2520effectiveness%2520of%2520our%2520proposed%2520method%250Asuggests%2520wider%2520implications%2520for%2520the%2520machine%2520learning%2520research%2520community.%2520All%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/michaelchen-lab/caft-llm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20large%20language%20models%20with%20concept-aware%20fine-tuning&entry.906535625=Michael%20K.%20Chen%20and%20Xikun%20Zhang%20and%20Jiaxing%20Huang%20and%20Dacheng%20Tao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20the%20cornerstone%20of%20modern%20AI.%0AHowever%2C%20the%20existing%20paradigm%20of%20next-token%20prediction%20fundamentally%20limits%0Atheir%20ability%20to%20form%20coherent%2C%20high-level%20concepts%2C%20making%20it%20a%20critical%0Abarrier%20to%20human-like%20understanding%20and%20reasoning.%20Take%20the%20phrase%20%22ribonucleic%0Aacid%22%20as%20an%20example%3A%20an%20LLM%20will%20first%20decompose%20it%20into%20tokens%2C%20i.e.%2C%0Aartificial%20text%20fragments%20%28%22rib%22%2C%20%22on%22%2C%20...%29%2C%20then%20learn%20each%20token%0Asequentially%2C%20rather%20than%20grasping%20the%20phrase%20as%20a%20unified%2C%20coherent%20semantic%0Aentity.%20This%20fragmented%20representation%20hinders%20deeper%20conceptual%20understanding%0Aand%2C%20ultimately%2C%20the%20development%20of%20truly%20intelligent%20systems.%20In%20response%2C%20we%0Aintroduce%20Concept-Aware%20Fine-Tuning%20%28CAFT%29%2C%20a%20novel%20multi-token%20training%20method%0Athat%20redefines%20how%20LLMs%20are%20fine-tuned.%20By%20enabling%20the%20learning%20of%20sequences%0Athat%20span%20multiple%20tokens%2C%20this%20method%20fosters%20stronger%20concept-aware%20learning.%0AOur%20experiments%20demonstrate%20significant%20improvements%20compared%20to%20conventional%0Anext-token%20finetuning%20methods%20across%20diverse%20tasks%2C%20including%20traditional%0Aapplications%20like%20text%20summarization%20and%20domain-specific%20ones%20like%20de%20novo%0Aprotein%20design.%20Multi-token%20prediction%20was%20previously%20only%20possible%20in%20the%0Aprohibitively%20expensive%20pretraining%20phase%3B%20CAFT%2C%20to%20our%20knowledge%2C%20is%20the%20first%0Ato%20bring%20the%20multi-token%20setting%20to%20the%20post-training%20phase%2C%20thus%20effectively%0Ademocratizing%20its%20benefits%20for%20the%20broader%20community%20of%20practitioners%20and%0Aresearchers.%20Finally%2C%20the%20unexpected%20effectiveness%20of%20our%20proposed%20method%0Asuggests%20wider%20implications%20for%20the%20machine%20learning%20research%20community.%20All%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/michaelchen-lab/caft-llm%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07833v1&entry.124074799=Read"},
{"title": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long\n  Range Tasks", "author": "Ali Hariri and \u00c1lvaro Arroyo and Alessio Gravina and Moshe Eliasof and Carola-Bibiane Sch\u00f6nlieb and Davide Bacciu and Kamyar Azizzadenesheli and Xiaowen Dong and Pierre Vandergheynst", "abstract": "  ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by\nMessage Passing Neural Networks (MPNNs), which gained popularity for their\nsimplicity and effectiveness in capturing local graph structure. Despite their\nsuccess, MPNNs are limited in their ability to capture long-range dependencies\nbetween nodes. This has led researchers to adapt MPNNs through rewiring or make\nuse of Graph Transformers, which compromises the computational efficiency that\ncharacterized early spatial message-passing architectures, and typically\ndisregards the graph structure. Almost a decade after its original\nintroduction, we revisit ChebNet to shed light on its ability to model distant\nnode interactions. We find that out-of-box, ChebNet already shows competitive\nadvantages relative to classical MPNNs and GTs on long-range benchmarks, while\nmaintaining good scalability properties for high-order polynomials. However, we\nuncover that this polynomial expansion leads ChebNet to an unstable regime\nduring training. To address this limitation, we cast ChebNet as a stable and\nnon-dissipative dynamical system, which we coin Stable-ChebNet. Our\nStable-ChebNet model allows for stable information propagation, and has\ncontrollable dynamics which do not require the use of eigendecompositions,\npositional encodings, or graph rewiring. Across several benchmarks,\nStable-ChebNet achieves near state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2506.07624v1", "date": "2025-06-09", "relevancy": 2.6531, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5757}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5287}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Return%20of%20ChebNet%3A%20Understanding%20and%20Improving%20an%20Overlooked%20GNN%20on%20Long%0A%20%20Range%20Tasks&body=Title%3A%20Return%20of%20ChebNet%3A%20Understanding%20and%20Improving%20an%20Overlooked%20GNN%20on%20Long%0A%20%20Range%20Tasks%0AAuthor%3A%20Ali%20Hariri%20and%20%C3%81lvaro%20Arroyo%20and%20Alessio%20Gravina%20and%20Moshe%20Eliasof%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Davide%20Bacciu%20and%20Kamyar%20Azizzadenesheli%20and%20Xiaowen%20Dong%20and%20Pierre%20Vandergheynst%0AAbstract%3A%20%20%20ChebNet%2C%20one%20of%20the%20earliest%20spectral%20GNNs%2C%20has%20largely%20been%20overshadowed%20by%0AMessage%20Passing%20Neural%20Networks%20%28MPNNs%29%2C%20which%20gained%20popularity%20for%20their%0Asimplicity%20and%20effectiveness%20in%20capturing%20local%20graph%20structure.%20Despite%20their%0Asuccess%2C%20MPNNs%20are%20limited%20in%20their%20ability%20to%20capture%20long-range%20dependencies%0Abetween%20nodes.%20This%20has%20led%20researchers%20to%20adapt%20MPNNs%20through%20rewiring%20or%20make%0Ause%20of%20Graph%20Transformers%2C%20which%20compromises%20the%20computational%20efficiency%20that%0Acharacterized%20early%20spatial%20message-passing%20architectures%2C%20and%20typically%0Adisregards%20the%20graph%20structure.%20Almost%20a%20decade%20after%20its%20original%0Aintroduction%2C%20we%20revisit%20ChebNet%20to%20shed%20light%20on%20its%20ability%20to%20model%20distant%0Anode%20interactions.%20We%20find%20that%20out-of-box%2C%20ChebNet%20already%20shows%20competitive%0Aadvantages%20relative%20to%20classical%20MPNNs%20and%20GTs%20on%20long-range%20benchmarks%2C%20while%0Amaintaining%20good%20scalability%20properties%20for%20high-order%20polynomials.%20However%2C%20we%0Auncover%20that%20this%20polynomial%20expansion%20leads%20ChebNet%20to%20an%20unstable%20regime%0Aduring%20training.%20To%20address%20this%20limitation%2C%20we%20cast%20ChebNet%20as%20a%20stable%20and%0Anon-dissipative%20dynamical%20system%2C%20which%20we%20coin%20Stable-ChebNet.%20Our%0AStable-ChebNet%20model%20allows%20for%20stable%20information%20propagation%2C%20and%20has%0Acontrollable%20dynamics%20which%20do%20not%20require%20the%20use%20of%20eigendecompositions%2C%0Apositional%20encodings%2C%20or%20graph%20rewiring.%20Across%20several%20benchmarks%2C%0AStable-ChebNet%20achieves%20near%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReturn%2520of%2520ChebNet%253A%2520Understanding%2520and%2520Improving%2520an%2520Overlooked%2520GNN%2520on%2520Long%250A%2520%2520Range%2520Tasks%26entry.906535625%3DAli%2520Hariri%2520and%2520%25C3%2581lvaro%2520Arroyo%2520and%2520Alessio%2520Gravina%2520and%2520Moshe%2520Eliasof%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Davide%2520Bacciu%2520and%2520Kamyar%2520Azizzadenesheli%2520and%2520Xiaowen%2520Dong%2520and%2520Pierre%2520Vandergheynst%26entry.1292438233%3D%2520%2520ChebNet%252C%2520one%2520of%2520the%2520earliest%2520spectral%2520GNNs%252C%2520has%2520largely%2520been%2520overshadowed%2520by%250AMessage%2520Passing%2520Neural%2520Networks%2520%2528MPNNs%2529%252C%2520which%2520gained%2520popularity%2520for%2520their%250Asimplicity%2520and%2520effectiveness%2520in%2520capturing%2520local%2520graph%2520structure.%2520Despite%2520their%250Asuccess%252C%2520MPNNs%2520are%2520limited%2520in%2520their%2520ability%2520to%2520capture%2520long-range%2520dependencies%250Abetween%2520nodes.%2520This%2520has%2520led%2520researchers%2520to%2520adapt%2520MPNNs%2520through%2520rewiring%2520or%2520make%250Ause%2520of%2520Graph%2520Transformers%252C%2520which%2520compromises%2520the%2520computational%2520efficiency%2520that%250Acharacterized%2520early%2520spatial%2520message-passing%2520architectures%252C%2520and%2520typically%250Adisregards%2520the%2520graph%2520structure.%2520Almost%2520a%2520decade%2520after%2520its%2520original%250Aintroduction%252C%2520we%2520revisit%2520ChebNet%2520to%2520shed%2520light%2520on%2520its%2520ability%2520to%2520model%2520distant%250Anode%2520interactions.%2520We%2520find%2520that%2520out-of-box%252C%2520ChebNet%2520already%2520shows%2520competitive%250Aadvantages%2520relative%2520to%2520classical%2520MPNNs%2520and%2520GTs%2520on%2520long-range%2520benchmarks%252C%2520while%250Amaintaining%2520good%2520scalability%2520properties%2520for%2520high-order%2520polynomials.%2520However%252C%2520we%250Auncover%2520that%2520this%2520polynomial%2520expansion%2520leads%2520ChebNet%2520to%2520an%2520unstable%2520regime%250Aduring%2520training.%2520To%2520address%2520this%2520limitation%252C%2520we%2520cast%2520ChebNet%2520as%2520a%2520stable%2520and%250Anon-dissipative%2520dynamical%2520system%252C%2520which%2520we%2520coin%2520Stable-ChebNet.%2520Our%250AStable-ChebNet%2520model%2520allows%2520for%2520stable%2520information%2520propagation%252C%2520and%2520has%250Acontrollable%2520dynamics%2520which%2520do%2520not%2520require%2520the%2520use%2520of%2520eigendecompositions%252C%250Apositional%2520encodings%252C%2520or%2520graph%2520rewiring.%2520Across%2520several%2520benchmarks%252C%250AStable-ChebNet%2520achieves%2520near%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Return%20of%20ChebNet%3A%20Understanding%20and%20Improving%20an%20Overlooked%20GNN%20on%20Long%0A%20%20Range%20Tasks&entry.906535625=Ali%20Hariri%20and%20%C3%81lvaro%20Arroyo%20and%20Alessio%20Gravina%20and%20Moshe%20Eliasof%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Davide%20Bacciu%20and%20Kamyar%20Azizzadenesheli%20and%20Xiaowen%20Dong%20and%20Pierre%20Vandergheynst&entry.1292438233=%20%20ChebNet%2C%20one%20of%20the%20earliest%20spectral%20GNNs%2C%20has%20largely%20been%20overshadowed%20by%0AMessage%20Passing%20Neural%20Networks%20%28MPNNs%29%2C%20which%20gained%20popularity%20for%20their%0Asimplicity%20and%20effectiveness%20in%20capturing%20local%20graph%20structure.%20Despite%20their%0Asuccess%2C%20MPNNs%20are%20limited%20in%20their%20ability%20to%20capture%20long-range%20dependencies%0Abetween%20nodes.%20This%20has%20led%20researchers%20to%20adapt%20MPNNs%20through%20rewiring%20or%20make%0Ause%20of%20Graph%20Transformers%2C%20which%20compromises%20the%20computational%20efficiency%20that%0Acharacterized%20early%20spatial%20message-passing%20architectures%2C%20and%20typically%0Adisregards%20the%20graph%20structure.%20Almost%20a%20decade%20after%20its%20original%0Aintroduction%2C%20we%20revisit%20ChebNet%20to%20shed%20light%20on%20its%20ability%20to%20model%20distant%0Anode%20interactions.%20We%20find%20that%20out-of-box%2C%20ChebNet%20already%20shows%20competitive%0Aadvantages%20relative%20to%20classical%20MPNNs%20and%20GTs%20on%20long-range%20benchmarks%2C%20while%0Amaintaining%20good%20scalability%20properties%20for%20high-order%20polynomials.%20However%2C%20we%0Auncover%20that%20this%20polynomial%20expansion%20leads%20ChebNet%20to%20an%20unstable%20regime%0Aduring%20training.%20To%20address%20this%20limitation%2C%20we%20cast%20ChebNet%20as%20a%20stable%20and%0Anon-dissipative%20dynamical%20system%2C%20which%20we%20coin%20Stable-ChebNet.%20Our%0AStable-ChebNet%20model%20allows%20for%20stable%20information%20propagation%2C%20and%20has%0Acontrollable%20dynamics%20which%20do%20not%20require%20the%20use%20of%20eigendecompositions%2C%0Apositional%20encodings%2C%20or%20graph%20rewiring.%20Across%20several%20benchmarks%2C%0AStable-ChebNet%20achieves%20near%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07624v1&entry.124074799=Read"},
{"title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from\n  Design", "author": "Wenxin Tang and Jingyu Xiao and Wenxuan Jiang and Xi Xiao and Yuhang Wang and Xuxin Tang and Qing Li and Yuehe Ma and Junliang Liu and Shisong Tang and Michael R. Lyu", "abstract": "  Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.\n", "link": "http://arxiv.org/abs/2506.07964v1", "date": "2025-06-09", "relevancy": 2.6349, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5517}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5184}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlideCoder%3A%20Layout-aware%20RAG-enhanced%20Hierarchical%20Slide%20Generation%20from%0A%20%20Design&body=Title%3A%20SlideCoder%3A%20Layout-aware%20RAG-enhanced%20Hierarchical%20Slide%20Generation%20from%0A%20%20Design%0AAuthor%3A%20Wenxin%20Tang%20and%20Jingyu%20Xiao%20and%20Wenxuan%20Jiang%20and%20Xi%20Xiao%20and%20Yuhang%20Wang%20and%20Xuxin%20Tang%20and%20Qing%20Li%20and%20Yuehe%20Ma%20and%20Junliang%20Liu%20and%20Shisong%20Tang%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Manual%20slide%20creation%20is%20labor-intensive%20and%20requires%20expert%20prior%20knowledge.%0AExisting%20natural%20language-based%20LLM%20generation%20methods%20struggle%20to%20capture%20the%0Avisual%20and%20structural%20nuances%20of%20slide%20designs.%20To%20address%20this%2C%20we%20formalize%0Athe%20Reference%20Image%20to%20Slide%20Generation%20task%20and%20propose%20Slide2Code%2C%20the%20first%0Abenchmark%20with%20difficulty-tiered%20samples%20based%20on%20a%20novel%20Slide%20Complexity%0AMetric.%20We%20introduce%20SlideCoder%2C%20a%20layout-aware%2C%20retrieval-augmented%20framework%0Afor%20generating%20editable%20slides%20from%20reference%20images.%20SlideCoder%20integrates%20a%0AColor%20Gradient-based%20Segmentation%20algorithm%20and%20a%20Hierarchical%0ARetrieval-Augmented%20Generation%20method%20to%20decompose%20complex%20tasks%20and%20enhance%0Acode%20generation.%20We%20also%20release%20SlideMaster%2C%20a%207B%20open-source%20model%20fine-tuned%0Awith%20improved%20reverse-engineered%20data.%20Experiments%20show%20that%20SlideCoder%0Aoutperforms%20state-of-the-art%20baselines%20by%20up%20to%2040.5%20points%2C%20demonstrating%0Astrong%20performance%20across%20layout%20fidelity%2C%20execution%20accuracy%2C%20and%20visual%0Aconsistency.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/vinsontang1/SlideCoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlideCoder%253A%2520Layout-aware%2520RAG-enhanced%2520Hierarchical%2520Slide%2520Generation%2520from%250A%2520%2520Design%26entry.906535625%3DWenxin%2520Tang%2520and%2520Jingyu%2520Xiao%2520and%2520Wenxuan%2520Jiang%2520and%2520Xi%2520Xiao%2520and%2520Yuhang%2520Wang%2520and%2520Xuxin%2520Tang%2520and%2520Qing%2520Li%2520and%2520Yuehe%2520Ma%2520and%2520Junliang%2520Liu%2520and%2520Shisong%2520Tang%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Manual%2520slide%2520creation%2520is%2520labor-intensive%2520and%2520requires%2520expert%2520prior%2520knowledge.%250AExisting%2520natural%2520language-based%2520LLM%2520generation%2520methods%2520struggle%2520to%2520capture%2520the%250Avisual%2520and%2520structural%2520nuances%2520of%2520slide%2520designs.%2520To%2520address%2520this%252C%2520we%2520formalize%250Athe%2520Reference%2520Image%2520to%2520Slide%2520Generation%2520task%2520and%2520propose%2520Slide2Code%252C%2520the%2520first%250Abenchmark%2520with%2520difficulty-tiered%2520samples%2520based%2520on%2520a%2520novel%2520Slide%2520Complexity%250AMetric.%2520We%2520introduce%2520SlideCoder%252C%2520a%2520layout-aware%252C%2520retrieval-augmented%2520framework%250Afor%2520generating%2520editable%2520slides%2520from%2520reference%2520images.%2520SlideCoder%2520integrates%2520a%250AColor%2520Gradient-based%2520Segmentation%2520algorithm%2520and%2520a%2520Hierarchical%250ARetrieval-Augmented%2520Generation%2520method%2520to%2520decompose%2520complex%2520tasks%2520and%2520enhance%250Acode%2520generation.%2520We%2520also%2520release%2520SlideMaster%252C%2520a%25207B%2520open-source%2520model%2520fine-tuned%250Awith%2520improved%2520reverse-engineered%2520data.%2520Experiments%2520show%2520that%2520SlideCoder%250Aoutperforms%2520state-of-the-art%2520baselines%2520by%2520up%2520to%252040.5%2520points%252C%2520demonstrating%250Astrong%2520performance%2520across%2520layout%2520fidelity%252C%2520execution%2520accuracy%252C%2520and%2520visual%250Aconsistency.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/vinsontang1/SlideCoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlideCoder%3A%20Layout-aware%20RAG-enhanced%20Hierarchical%20Slide%20Generation%20from%0A%20%20Design&entry.906535625=Wenxin%20Tang%20and%20Jingyu%20Xiao%20and%20Wenxuan%20Jiang%20and%20Xi%20Xiao%20and%20Yuhang%20Wang%20and%20Xuxin%20Tang%20and%20Qing%20Li%20and%20Yuehe%20Ma%20and%20Junliang%20Liu%20and%20Shisong%20Tang%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Manual%20slide%20creation%20is%20labor-intensive%20and%20requires%20expert%20prior%20knowledge.%0AExisting%20natural%20language-based%20LLM%20generation%20methods%20struggle%20to%20capture%20the%0Avisual%20and%20structural%20nuances%20of%20slide%20designs.%20To%20address%20this%2C%20we%20formalize%0Athe%20Reference%20Image%20to%20Slide%20Generation%20task%20and%20propose%20Slide2Code%2C%20the%20first%0Abenchmark%20with%20difficulty-tiered%20samples%20based%20on%20a%20novel%20Slide%20Complexity%0AMetric.%20We%20introduce%20SlideCoder%2C%20a%20layout-aware%2C%20retrieval-augmented%20framework%0Afor%20generating%20editable%20slides%20from%20reference%20images.%20SlideCoder%20integrates%20a%0AColor%20Gradient-based%20Segmentation%20algorithm%20and%20a%20Hierarchical%0ARetrieval-Augmented%20Generation%20method%20to%20decompose%20complex%20tasks%20and%20enhance%0Acode%20generation.%20We%20also%20release%20SlideMaster%2C%20a%207B%20open-source%20model%20fine-tuned%0Awith%20improved%20reverse-engineered%20data.%20Experiments%20show%20that%20SlideCoder%0Aoutperforms%20state-of-the-art%20baselines%20by%20up%20to%2040.5%20points%2C%20demonstrating%0Astrong%20performance%20across%20layout%20fidelity%2C%20execution%20accuracy%2C%20and%20visual%0Aconsistency.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/vinsontang1/SlideCoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07964v1&entry.124074799=Read"},
{"title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection\n  with Cross-Scale Gated Coding", "author": "Xuemei Chen and Huamin Wang and Hangchi Shen and Shukai Duan and Shiping Wen and Tingwen Huang", "abstract": "  Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.\n", "link": "http://arxiv.org/abs/2506.07737v1", "date": "2025-06-09", "relevancy": 2.61, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.531}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5184}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeSMOKE%3A%20Spiking%20Neural%20Networks%20for%20Monocular%203D%20Object%20Detection%0A%20%20with%20Cross-Scale%20Gated%20Coding&body=Title%3A%20SpikeSMOKE%3A%20Spiking%20Neural%20Networks%20for%20Monocular%203D%20Object%20Detection%0A%20%20with%20Cross-Scale%20Gated%20Coding%0AAuthor%3A%20Xuemei%20Chen%20and%20Huamin%20Wang%20and%20Hangchi%20Shen%20and%20Shukai%20Duan%20and%20Shiping%20Wen%20and%20Tingwen%20Huang%0AAbstract%3A%20%20%20Low%20energy%20consumption%20for%203D%20object%20detection%20is%20an%20important%20research%20area%0Abecause%20of%20the%20increasing%20energy%20consumption%20with%20their%20wide%20application%20in%0Afields%20such%20as%20autonomous%20driving.%20The%20spiking%20neural%20networks%20%28SNNs%29%20with%0Alow-power%20consumption%20characteristics%20can%20provide%20a%20novel%20solution%20for%20this%0Aresearch.%20Therefore%2C%20we%20apply%20SNNs%20to%20monocular%203D%20object%20detection%20and%20propose%0Athe%20SpikeSMOKE%20architecture%20in%20this%20paper%2C%20which%20is%20a%20new%20attempt%20for%20low-power%0Amonocular%203D%20object%20detection.%20As%20we%20all%20know%2C%20discrete%20signals%20of%20SNNs%20will%0Agenerate%20information%20loss%20and%20limit%20their%20feature%20expression%20ability%20compared%0Awith%20the%20artificial%20neural%20networks%20%28ANNs%29.In%20order%20to%20address%20this%20issue%2C%0Ainspired%20by%20the%20filtering%20mechanism%20of%20biological%20neuronal%20synapses%2C%20we%20propose%0Aa%20cross-scale%20gated%20coding%20mechanism%28CSGC%29%2C%20which%20can%20enhance%20feature%0Arepresentation%20by%20combining%20cross-scale%20fusion%20of%20attentional%20methods%20and%20gated%0Afiltering%20mechanisms.In%20addition%2C%20to%20reduce%20the%20computation%20and%20increase%20the%0Aspeed%20of%20training%2C%20we%20present%20a%20novel%20light-weight%20residual%20block%20that%20can%0Amaintain%20spiking%20computing%20paradigm%20and%20the%20highest%20possible%20detection%0Aperformance.%20Compared%20to%20the%20baseline%20SpikeSMOKE%20under%20the%203D%20Object%20Detection%2C%0Athe%20proposed%20SpikeSMOKE%20with%20CSGC%20can%20achieve%2011.78%20%28%2B2.82%2C%20Easy%29%2C%2010.69%20%28%2B3.2%2C%0AModerate%29%2C%20and%2010.48%20%28%2B3.17%2C%20Hard%29%20on%20the%20KITTI%20autonomous%20driving%20dataset%20by%0AAP%7CR11%20at%200.7%20IoU%20threshold%2C%20respectively.%20It%20is%20important%20to%20note%20that%20the%0Aresults%20of%20SpikeSMOKE%20can%20significantly%20reduce%20energy%20consumption%20compared%20to%0Athe%20results%20on%20SMOKE.%20For%20example%2Cthe%20energy%20consumption%20can%20be%20reduced%20by%0A72.2%25%20on%20the%20hard%20category%2C%20while%20the%20detection%20performance%20is%20reduced%20by%20only%0A4%25.%20SpikeSMOKE-L%20%28lightweight%29%20can%20further%20reduce%20the%20amount%20of%20parameters%20by%203%0Atimes%20and%20computation%20by%2010%20times%20compared%20to%20SMOKE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeSMOKE%253A%2520Spiking%2520Neural%2520Networks%2520for%2520Monocular%25203D%2520Object%2520Detection%250A%2520%2520with%2520Cross-Scale%2520Gated%2520Coding%26entry.906535625%3DXuemei%2520Chen%2520and%2520Huamin%2520Wang%2520and%2520Hangchi%2520Shen%2520and%2520Shukai%2520Duan%2520and%2520Shiping%2520Wen%2520and%2520Tingwen%2520Huang%26entry.1292438233%3D%2520%2520Low%2520energy%2520consumption%2520for%25203D%2520object%2520detection%2520is%2520an%2520important%2520research%2520area%250Abecause%2520of%2520the%2520increasing%2520energy%2520consumption%2520with%2520their%2520wide%2520application%2520in%250Afields%2520such%2520as%2520autonomous%2520driving.%2520The%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520with%250Alow-power%2520consumption%2520characteristics%2520can%2520provide%2520a%2520novel%2520solution%2520for%2520this%250Aresearch.%2520Therefore%252C%2520we%2520apply%2520SNNs%2520to%2520monocular%25203D%2520object%2520detection%2520and%2520propose%250Athe%2520SpikeSMOKE%2520architecture%2520in%2520this%2520paper%252C%2520which%2520is%2520a%2520new%2520attempt%2520for%2520low-power%250Amonocular%25203D%2520object%2520detection.%2520As%2520we%2520all%2520know%252C%2520discrete%2520signals%2520of%2520SNNs%2520will%250Agenerate%2520information%2520loss%2520and%2520limit%2520their%2520feature%2520expression%2520ability%2520compared%250Awith%2520the%2520artificial%2520neural%2520networks%2520%2528ANNs%2529.In%2520order%2520to%2520address%2520this%2520issue%252C%250Ainspired%2520by%2520the%2520filtering%2520mechanism%2520of%2520biological%2520neuronal%2520synapses%252C%2520we%2520propose%250Aa%2520cross-scale%2520gated%2520coding%2520mechanism%2528CSGC%2529%252C%2520which%2520can%2520enhance%2520feature%250Arepresentation%2520by%2520combining%2520cross-scale%2520fusion%2520of%2520attentional%2520methods%2520and%2520gated%250Afiltering%2520mechanisms.In%2520addition%252C%2520to%2520reduce%2520the%2520computation%2520and%2520increase%2520the%250Aspeed%2520of%2520training%252C%2520we%2520present%2520a%2520novel%2520light-weight%2520residual%2520block%2520that%2520can%250Amaintain%2520spiking%2520computing%2520paradigm%2520and%2520the%2520highest%2520possible%2520detection%250Aperformance.%2520Compared%2520to%2520the%2520baseline%2520SpikeSMOKE%2520under%2520the%25203D%2520Object%2520Detection%252C%250Athe%2520proposed%2520SpikeSMOKE%2520with%2520CSGC%2520can%2520achieve%252011.78%2520%2528%252B2.82%252C%2520Easy%2529%252C%252010.69%2520%2528%252B3.2%252C%250AModerate%2529%252C%2520and%252010.48%2520%2528%252B3.17%252C%2520Hard%2529%2520on%2520the%2520KITTI%2520autonomous%2520driving%2520dataset%2520by%250AAP%257CR11%2520at%25200.7%2520IoU%2520threshold%252C%2520respectively.%2520It%2520is%2520important%2520to%2520note%2520that%2520the%250Aresults%2520of%2520SpikeSMOKE%2520can%2520significantly%2520reduce%2520energy%2520consumption%2520compared%2520to%250Athe%2520results%2520on%2520SMOKE.%2520For%2520example%252Cthe%2520energy%2520consumption%2520can%2520be%2520reduced%2520by%250A72.2%2525%2520on%2520the%2520hard%2520category%252C%2520while%2520the%2520detection%2520performance%2520is%2520reduced%2520by%2520only%250A4%2525.%2520SpikeSMOKE-L%2520%2528lightweight%2529%2520can%2520further%2520reduce%2520the%2520amount%2520of%2520parameters%2520by%25203%250Atimes%2520and%2520computation%2520by%252010%2520times%2520compared%2520to%2520SMOKE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeSMOKE%3A%20Spiking%20Neural%20Networks%20for%20Monocular%203D%20Object%20Detection%0A%20%20with%20Cross-Scale%20Gated%20Coding&entry.906535625=Xuemei%20Chen%20and%20Huamin%20Wang%20and%20Hangchi%20Shen%20and%20Shukai%20Duan%20and%20Shiping%20Wen%20and%20Tingwen%20Huang&entry.1292438233=%20%20Low%20energy%20consumption%20for%203D%20object%20detection%20is%20an%20important%20research%20area%0Abecause%20of%20the%20increasing%20energy%20consumption%20with%20their%20wide%20application%20in%0Afields%20such%20as%20autonomous%20driving.%20The%20spiking%20neural%20networks%20%28SNNs%29%20with%0Alow-power%20consumption%20characteristics%20can%20provide%20a%20novel%20solution%20for%20this%0Aresearch.%20Therefore%2C%20we%20apply%20SNNs%20to%20monocular%203D%20object%20detection%20and%20propose%0Athe%20SpikeSMOKE%20architecture%20in%20this%20paper%2C%20which%20is%20a%20new%20attempt%20for%20low-power%0Amonocular%203D%20object%20detection.%20As%20we%20all%20know%2C%20discrete%20signals%20of%20SNNs%20will%0Agenerate%20information%20loss%20and%20limit%20their%20feature%20expression%20ability%20compared%0Awith%20the%20artificial%20neural%20networks%20%28ANNs%29.In%20order%20to%20address%20this%20issue%2C%0Ainspired%20by%20the%20filtering%20mechanism%20of%20biological%20neuronal%20synapses%2C%20we%20propose%0Aa%20cross-scale%20gated%20coding%20mechanism%28CSGC%29%2C%20which%20can%20enhance%20feature%0Arepresentation%20by%20combining%20cross-scale%20fusion%20of%20attentional%20methods%20and%20gated%0Afiltering%20mechanisms.In%20addition%2C%20to%20reduce%20the%20computation%20and%20increase%20the%0Aspeed%20of%20training%2C%20we%20present%20a%20novel%20light-weight%20residual%20block%20that%20can%0Amaintain%20spiking%20computing%20paradigm%20and%20the%20highest%20possible%20detection%0Aperformance.%20Compared%20to%20the%20baseline%20SpikeSMOKE%20under%20the%203D%20Object%20Detection%2C%0Athe%20proposed%20SpikeSMOKE%20with%20CSGC%20can%20achieve%2011.78%20%28%2B2.82%2C%20Easy%29%2C%2010.69%20%28%2B3.2%2C%0AModerate%29%2C%20and%2010.48%20%28%2B3.17%2C%20Hard%29%20on%20the%20KITTI%20autonomous%20driving%20dataset%20by%0AAP%7CR11%20at%200.7%20IoU%20threshold%2C%20respectively.%20It%20is%20important%20to%20note%20that%20the%0Aresults%20of%20SpikeSMOKE%20can%20significantly%20reduce%20energy%20consumption%20compared%20to%0Athe%20results%20on%20SMOKE.%20For%20example%2Cthe%20energy%20consumption%20can%20be%20reduced%20by%0A72.2%25%20on%20the%20hard%20category%2C%20while%20the%20detection%20performance%20is%20reduced%20by%20only%0A4%25.%20SpikeSMOKE-L%20%28lightweight%29%20can%20further%20reduce%20the%20amount%20of%20parameters%20by%203%0Atimes%20and%20computation%20by%2010%20times%20compared%20to%20SMOKE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07737v1&entry.124074799=Read"},
{"title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack", "author": "Zhan Li and Mingyu Zhao and Xin Dong and Haibin Ling and Bingyao Huang", "abstract": "  Projector-based adversarial attack aims to project carefully designed light\npatterns (i.e., adversarial projections) onto scenes to deceive deep image\nclassifiers. It has potential applications in privacy protection and the\ndevelopment of more robust classifiers. However, existing approaches primarily\nfocus on individual classifiers and fixed camera poses, often neglecting the\ncomplexities of multi-classifier systems and scenarios with varying camera\nposes. This limitation reduces their effectiveness when introducing new\nclassifiers or camera poses. In this paper, we introduce Classifier-Agnostic\nProjector-Based Adversarial Attack (CAPAA) to address these issues. First, we\ndevelop a novel classifier-agnostic adversarial loss and optimization framework\nthat aggregates adversarial and stealthiness loss gradients from multiple\nclassifiers. Then, we propose an attention-based gradient weighting mechanism\nthat concentrates perturbations on regions of high classification activation,\nthereby improving the robustness of adversarial projections when applied to\nscenes with varying camera poses. Our extensive experimental evaluations\ndemonstrate that CAPAA achieves both a higher attack success rate and greater\nstealthiness compared to existing baselines. Codes are available at:\nhttps://github.com/ZhanLiQxQ/CAPAA.\n", "link": "http://arxiv.org/abs/2506.00978v2", "date": "2025-06-09", "relevancy": 2.6083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5214}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPAA%3A%20Classifier-Agnostic%20Projector-Based%20Adversarial%20Attack&body=Title%3A%20CAPAA%3A%20Classifier-Agnostic%20Projector-Based%20Adversarial%20Attack%0AAuthor%3A%20Zhan%20Li%20and%20Mingyu%20Zhao%20and%20Xin%20Dong%20and%20Haibin%20Ling%20and%20Bingyao%20Huang%0AAbstract%3A%20%20%20Projector-based%20adversarial%20attack%20aims%20to%20project%20carefully%20designed%20light%0Apatterns%20%28i.e.%2C%20adversarial%20projections%29%20onto%20scenes%20to%20deceive%20deep%20image%0Aclassifiers.%20It%20has%20potential%20applications%20in%20privacy%20protection%20and%20the%0Adevelopment%20of%20more%20robust%20classifiers.%20However%2C%20existing%20approaches%20primarily%0Afocus%20on%20individual%20classifiers%20and%20fixed%20camera%20poses%2C%20often%20neglecting%20the%0Acomplexities%20of%20multi-classifier%20systems%20and%20scenarios%20with%20varying%20camera%0Aposes.%20This%20limitation%20reduces%20their%20effectiveness%20when%20introducing%20new%0Aclassifiers%20or%20camera%20poses.%20In%20this%20paper%2C%20we%20introduce%20Classifier-Agnostic%0AProjector-Based%20Adversarial%20Attack%20%28CAPAA%29%20to%20address%20these%20issues.%20First%2C%20we%0Adevelop%20a%20novel%20classifier-agnostic%20adversarial%20loss%20and%20optimization%20framework%0Athat%20aggregates%20adversarial%20and%20stealthiness%20loss%20gradients%20from%20multiple%0Aclassifiers.%20Then%2C%20we%20propose%20an%20attention-based%20gradient%20weighting%20mechanism%0Athat%20concentrates%20perturbations%20on%20regions%20of%20high%20classification%20activation%2C%0Athereby%20improving%20the%20robustness%20of%20adversarial%20projections%20when%20applied%20to%0Ascenes%20with%20varying%20camera%20poses.%20Our%20extensive%20experimental%20evaluations%0Ademonstrate%20that%20CAPAA%20achieves%20both%20a%20higher%20attack%20success%20rate%20and%20greater%0Astealthiness%20compared%20to%20existing%20baselines.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/ZhanLiQxQ/CAPAA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPAA%253A%2520Classifier-Agnostic%2520Projector-Based%2520Adversarial%2520Attack%26entry.906535625%3DZhan%2520Li%2520and%2520Mingyu%2520Zhao%2520and%2520Xin%2520Dong%2520and%2520Haibin%2520Ling%2520and%2520Bingyao%2520Huang%26entry.1292438233%3D%2520%2520Projector-based%2520adversarial%2520attack%2520aims%2520to%2520project%2520carefully%2520designed%2520light%250Apatterns%2520%2528i.e.%252C%2520adversarial%2520projections%2529%2520onto%2520scenes%2520to%2520deceive%2520deep%2520image%250Aclassifiers.%2520It%2520has%2520potential%2520applications%2520in%2520privacy%2520protection%2520and%2520the%250Adevelopment%2520of%2520more%2520robust%2520classifiers.%2520However%252C%2520existing%2520approaches%2520primarily%250Afocus%2520on%2520individual%2520classifiers%2520and%2520fixed%2520camera%2520poses%252C%2520often%2520neglecting%2520the%250Acomplexities%2520of%2520multi-classifier%2520systems%2520and%2520scenarios%2520with%2520varying%2520camera%250Aposes.%2520This%2520limitation%2520reduces%2520their%2520effectiveness%2520when%2520introducing%2520new%250Aclassifiers%2520or%2520camera%2520poses.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Classifier-Agnostic%250AProjector-Based%2520Adversarial%2520Attack%2520%2528CAPAA%2529%2520to%2520address%2520these%2520issues.%2520First%252C%2520we%250Adevelop%2520a%2520novel%2520classifier-agnostic%2520adversarial%2520loss%2520and%2520optimization%2520framework%250Athat%2520aggregates%2520adversarial%2520and%2520stealthiness%2520loss%2520gradients%2520from%2520multiple%250Aclassifiers.%2520Then%252C%2520we%2520propose%2520an%2520attention-based%2520gradient%2520weighting%2520mechanism%250Athat%2520concentrates%2520perturbations%2520on%2520regions%2520of%2520high%2520classification%2520activation%252C%250Athereby%2520improving%2520the%2520robustness%2520of%2520adversarial%2520projections%2520when%2520applied%2520to%250Ascenes%2520with%2520varying%2520camera%2520poses.%2520Our%2520extensive%2520experimental%2520evaluations%250Ademonstrate%2520that%2520CAPAA%2520achieves%2520both%2520a%2520higher%2520attack%2520success%2520rate%2520and%2520greater%250Astealthiness%2520compared%2520to%2520existing%2520baselines.%2520Codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/ZhanLiQxQ/CAPAA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPAA%3A%20Classifier-Agnostic%20Projector-Based%20Adversarial%20Attack&entry.906535625=Zhan%20Li%20and%20Mingyu%20Zhao%20and%20Xin%20Dong%20and%20Haibin%20Ling%20and%20Bingyao%20Huang&entry.1292438233=%20%20Projector-based%20adversarial%20attack%20aims%20to%20project%20carefully%20designed%20light%0Apatterns%20%28i.e.%2C%20adversarial%20projections%29%20onto%20scenes%20to%20deceive%20deep%20image%0Aclassifiers.%20It%20has%20potential%20applications%20in%20privacy%20protection%20and%20the%0Adevelopment%20of%20more%20robust%20classifiers.%20However%2C%20existing%20approaches%20primarily%0Afocus%20on%20individual%20classifiers%20and%20fixed%20camera%20poses%2C%20often%20neglecting%20the%0Acomplexities%20of%20multi-classifier%20systems%20and%20scenarios%20with%20varying%20camera%0Aposes.%20This%20limitation%20reduces%20their%20effectiveness%20when%20introducing%20new%0Aclassifiers%20or%20camera%20poses.%20In%20this%20paper%2C%20we%20introduce%20Classifier-Agnostic%0AProjector-Based%20Adversarial%20Attack%20%28CAPAA%29%20to%20address%20these%20issues.%20First%2C%20we%0Adevelop%20a%20novel%20classifier-agnostic%20adversarial%20loss%20and%20optimization%20framework%0Athat%20aggregates%20adversarial%20and%20stealthiness%20loss%20gradients%20from%20multiple%0Aclassifiers.%20Then%2C%20we%20propose%20an%20attention-based%20gradient%20weighting%20mechanism%0Athat%20concentrates%20perturbations%20on%20regions%20of%20high%20classification%20activation%2C%0Athereby%20improving%20the%20robustness%20of%20adversarial%20projections%20when%20applied%20to%0Ascenes%20with%20varying%20camera%20poses.%20Our%20extensive%20experimental%20evaluations%0Ademonstrate%20that%20CAPAA%20achieves%20both%20a%20higher%20attack%20success%20rate%20and%20greater%0Astealthiness%20compared%20to%20existing%20baselines.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/ZhanLiQxQ/CAPAA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00978v2&entry.124074799=Read"},
{"title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion", "author": "Xun Huang and Zhengqi Li and Guande He and Mingyuan Zhou and Eli Shechtman", "abstract": "  We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/\n", "link": "http://arxiv.org/abs/2506.08009v1", "date": "2025-06-09", "relevancy": 2.582, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6503}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6478}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self%20Forcing%3A%20Bridging%20the%20Train-Test%20Gap%20in%20Autoregressive%20Video%0A%20%20Diffusion&body=Title%3A%20Self%20Forcing%3A%20Bridging%20the%20Train-Test%20Gap%20in%20Autoregressive%20Video%0A%20%20Diffusion%0AAuthor%3A%20Xun%20Huang%20and%20Zhengqi%20Li%20and%20Guande%20He%20and%20Mingyuan%20Zhou%20and%20Eli%20Shechtman%0AAbstract%3A%20%20%20We%20introduce%20Self%20Forcing%2C%20a%20novel%20training%20paradigm%20for%20autoregressive%20video%0Adiffusion%20models.%20It%20addresses%20the%20longstanding%20issue%20of%20exposure%20bias%2C%20where%0Amodels%20trained%20on%20ground-truth%20context%20must%20generate%20sequences%20conditioned%20on%0Atheir%20own%20imperfect%20outputs%20during%20inference.%20Unlike%20prior%20methods%20that%20denoise%0Afuture%20frames%20based%20on%20ground-truth%20context%20frames%2C%20Self%20Forcing%20conditions%0Aeach%20frame%27s%20generation%20on%20previously%20self-generated%20outputs%20by%20performing%0Aautoregressive%20rollout%20with%20key-value%20%28KV%29%20caching%20during%20training.%20This%0Astrategy%20enables%20supervision%20through%20a%20holistic%20loss%20at%20the%20video%20level%20that%0Adirectly%20evaluates%20the%20quality%20of%20the%20entire%20generated%20sequence%2C%20rather%20than%0Arelying%20solely%20on%20traditional%20frame-wise%20objectives.%20To%20ensure%20training%0Aefficiency%2C%20we%20employ%20a%20few-step%20diffusion%20model%20along%20with%20a%20stochastic%0Agradient%20truncation%20strategy%2C%20effectively%20balancing%20computational%20cost%20and%0Aperformance.%20We%20further%20introduce%20a%20rolling%20KV%20cache%20mechanism%20that%20enables%0Aefficient%20autoregressive%20video%20extrapolation.%20Extensive%20experiments%20demonstrate%0Athat%20our%20approach%20achieves%20real-time%20streaming%20video%20generation%20with%20sub-second%0Alatency%20on%20a%20single%20GPU%2C%20while%20matching%20or%20even%20surpassing%20the%20generation%0Aquality%20of%20significantly%20slower%20and%20non-causal%20diffusion%20models.%20Project%0Awebsite%3A%20http%3A//self-forcing.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf%2520Forcing%253A%2520Bridging%2520the%2520Train-Test%2520Gap%2520in%2520Autoregressive%2520Video%250A%2520%2520Diffusion%26entry.906535625%3DXun%2520Huang%2520and%2520Zhengqi%2520Li%2520and%2520Guande%2520He%2520and%2520Mingyuan%2520Zhou%2520and%2520Eli%2520Shechtman%26entry.1292438233%3D%2520%2520We%2520introduce%2520Self%2520Forcing%252C%2520a%2520novel%2520training%2520paradigm%2520for%2520autoregressive%2520video%250Adiffusion%2520models.%2520It%2520addresses%2520the%2520longstanding%2520issue%2520of%2520exposure%2520bias%252C%2520where%250Amodels%2520trained%2520on%2520ground-truth%2520context%2520must%2520generate%2520sequences%2520conditioned%2520on%250Atheir%2520own%2520imperfect%2520outputs%2520during%2520inference.%2520Unlike%2520prior%2520methods%2520that%2520denoise%250Afuture%2520frames%2520based%2520on%2520ground-truth%2520context%2520frames%252C%2520Self%2520Forcing%2520conditions%250Aeach%2520frame%2527s%2520generation%2520on%2520previously%2520self-generated%2520outputs%2520by%2520performing%250Aautoregressive%2520rollout%2520with%2520key-value%2520%2528KV%2529%2520caching%2520during%2520training.%2520This%250Astrategy%2520enables%2520supervision%2520through%2520a%2520holistic%2520loss%2520at%2520the%2520video%2520level%2520that%250Adirectly%2520evaluates%2520the%2520quality%2520of%2520the%2520entire%2520generated%2520sequence%252C%2520rather%2520than%250Arelying%2520solely%2520on%2520traditional%2520frame-wise%2520objectives.%2520To%2520ensure%2520training%250Aefficiency%252C%2520we%2520employ%2520a%2520few-step%2520diffusion%2520model%2520along%2520with%2520a%2520stochastic%250Agradient%2520truncation%2520strategy%252C%2520effectively%2520balancing%2520computational%2520cost%2520and%250Aperformance.%2520We%2520further%2520introduce%2520a%2520rolling%2520KV%2520cache%2520mechanism%2520that%2520enables%250Aefficient%2520autoregressive%2520video%2520extrapolation.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520approach%2520achieves%2520real-time%2520streaming%2520video%2520generation%2520with%2520sub-second%250Alatency%2520on%2520a%2520single%2520GPU%252C%2520while%2520matching%2520or%2520even%2520surpassing%2520the%2520generation%250Aquality%2520of%2520significantly%2520slower%2520and%2520non-causal%2520diffusion%2520models.%2520Project%250Awebsite%253A%2520http%253A//self-forcing.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self%20Forcing%3A%20Bridging%20the%20Train-Test%20Gap%20in%20Autoregressive%20Video%0A%20%20Diffusion&entry.906535625=Xun%20Huang%20and%20Zhengqi%20Li%20and%20Guande%20He%20and%20Mingyuan%20Zhou%20and%20Eli%20Shechtman&entry.1292438233=%20%20We%20introduce%20Self%20Forcing%2C%20a%20novel%20training%20paradigm%20for%20autoregressive%20video%0Adiffusion%20models.%20It%20addresses%20the%20longstanding%20issue%20of%20exposure%20bias%2C%20where%0Amodels%20trained%20on%20ground-truth%20context%20must%20generate%20sequences%20conditioned%20on%0Atheir%20own%20imperfect%20outputs%20during%20inference.%20Unlike%20prior%20methods%20that%20denoise%0Afuture%20frames%20based%20on%20ground-truth%20context%20frames%2C%20Self%20Forcing%20conditions%0Aeach%20frame%27s%20generation%20on%20previously%20self-generated%20outputs%20by%20performing%0Aautoregressive%20rollout%20with%20key-value%20%28KV%29%20caching%20during%20training.%20This%0Astrategy%20enables%20supervision%20through%20a%20holistic%20loss%20at%20the%20video%20level%20that%0Adirectly%20evaluates%20the%20quality%20of%20the%20entire%20generated%20sequence%2C%20rather%20than%0Arelying%20solely%20on%20traditional%20frame-wise%20objectives.%20To%20ensure%20training%0Aefficiency%2C%20we%20employ%20a%20few-step%20diffusion%20model%20along%20with%20a%20stochastic%0Agradient%20truncation%20strategy%2C%20effectively%20balancing%20computational%20cost%20and%0Aperformance.%20We%20further%20introduce%20a%20rolling%20KV%20cache%20mechanism%20that%20enables%0Aefficient%20autoregressive%20video%20extrapolation.%20Extensive%20experiments%20demonstrate%0Athat%20our%20approach%20achieves%20real-time%20streaming%20video%20generation%20with%20sub-second%0Alatency%20on%20a%20single%20GPU%2C%20while%20matching%20or%20even%20surpassing%20the%20generation%0Aquality%20of%20significantly%20slower%20and%20non-causal%20diffusion%20models.%20Project%0Awebsite%3A%20http%3A//self-forcing.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08009v1&entry.124074799=Read"},
{"title": "SoK: Data Reconstruction Attacks Against Machine Learning Models:\n  Definition, Metrics, and Benchmark", "author": "Rui Wen and Yiyong Liu and Michael Backes and Yang Zhang", "abstract": "  Data reconstruction attacks, which aim to recover the training dataset of a\ntarget model with limited access, have gained increasing attention in recent\nyears. However, there is currently no consensus on a formal definition of data\nreconstruction attacks or appropriate evaluation metrics for measuring their\nquality. This lack of rigorous definitions and universal metrics has hindered\nfurther advancement in this field. In this paper, we address this issue in the\nvision domain by proposing a unified attack taxonomy and formal definitions of\ndata reconstruction attacks. We first propose a set of quantitative evaluation\nmetrics that consider important criteria such as quantifiability, consistency,\nprecision, and diversity. Additionally, we leverage large language models\n(LLMs) as a substitute for human judgment, enabling visual evaluation with an\nemphasis on high-quality reconstructions. Using our proposed taxonomy and\nmetrics, we present a unified framework for systematically evaluating the\nstrengths and limitations of existing attacks and establishing a benchmark for\nfuture research. Empirical results, primarily from a memorization perspective,\nnot only validate the effectiveness of our metrics but also offer valuable\ninsights for designing new attacks.\n", "link": "http://arxiv.org/abs/2506.07888v1", "date": "2025-06-09", "relevancy": 2.5544, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20Data%20Reconstruction%20Attacks%20Against%20Machine%20Learning%20Models%3A%0A%20%20Definition%2C%20Metrics%2C%20and%20Benchmark&body=Title%3A%20SoK%3A%20Data%20Reconstruction%20Attacks%20Against%20Machine%20Learning%20Models%3A%0A%20%20Definition%2C%20Metrics%2C%20and%20Benchmark%0AAuthor%3A%20Rui%20Wen%20and%20Yiyong%20Liu%20and%20Michael%20Backes%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Data%20reconstruction%20attacks%2C%20which%20aim%20to%20recover%20the%20training%20dataset%20of%20a%0Atarget%20model%20with%20limited%20access%2C%20have%20gained%20increasing%20attention%20in%20recent%0Ayears.%20However%2C%20there%20is%20currently%20no%20consensus%20on%20a%20formal%20definition%20of%20data%0Areconstruction%20attacks%20or%20appropriate%20evaluation%20metrics%20for%20measuring%20their%0Aquality.%20This%20lack%20of%20rigorous%20definitions%20and%20universal%20metrics%20has%20hindered%0Afurther%20advancement%20in%20this%20field.%20In%20this%20paper%2C%20we%20address%20this%20issue%20in%20the%0Avision%20domain%20by%20proposing%20a%20unified%20attack%20taxonomy%20and%20formal%20definitions%20of%0Adata%20reconstruction%20attacks.%20We%20first%20propose%20a%20set%20of%20quantitative%20evaluation%0Ametrics%20that%20consider%20important%20criteria%20such%20as%20quantifiability%2C%20consistency%2C%0Aprecision%2C%20and%20diversity.%20Additionally%2C%20we%20leverage%20large%20language%20models%0A%28LLMs%29%20as%20a%20substitute%20for%20human%20judgment%2C%20enabling%20visual%20evaluation%20with%20an%0Aemphasis%20on%20high-quality%20reconstructions.%20Using%20our%20proposed%20taxonomy%20and%0Ametrics%2C%20we%20present%20a%20unified%20framework%20for%20systematically%20evaluating%20the%0Astrengths%20and%20limitations%20of%20existing%20attacks%20and%20establishing%20a%20benchmark%20for%0Afuture%20research.%20Empirical%20results%2C%20primarily%20from%20a%20memorization%20perspective%2C%0Anot%20only%20validate%20the%20effectiveness%20of%20our%20metrics%20but%20also%20offer%20valuable%0Ainsights%20for%20designing%20new%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520Data%2520Reconstruction%2520Attacks%2520Against%2520Machine%2520Learning%2520Models%253A%250A%2520%2520Definition%252C%2520Metrics%252C%2520and%2520Benchmark%26entry.906535625%3DRui%2520Wen%2520and%2520Yiyong%2520Liu%2520and%2520Michael%2520Backes%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Data%2520reconstruction%2520attacks%252C%2520which%2520aim%2520to%2520recover%2520the%2520training%2520dataset%2520of%2520a%250Atarget%2520model%2520with%2520limited%2520access%252C%2520have%2520gained%2520increasing%2520attention%2520in%2520recent%250Ayears.%2520However%252C%2520there%2520is%2520currently%2520no%2520consensus%2520on%2520a%2520formal%2520definition%2520of%2520data%250Areconstruction%2520attacks%2520or%2520appropriate%2520evaluation%2520metrics%2520for%2520measuring%2520their%250Aquality.%2520This%2520lack%2520of%2520rigorous%2520definitions%2520and%2520universal%2520metrics%2520has%2520hindered%250Afurther%2520advancement%2520in%2520this%2520field.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%2520in%2520the%250Avision%2520domain%2520by%2520proposing%2520a%2520unified%2520attack%2520taxonomy%2520and%2520formal%2520definitions%2520of%250Adata%2520reconstruction%2520attacks.%2520We%2520first%2520propose%2520a%2520set%2520of%2520quantitative%2520evaluation%250Ametrics%2520that%2520consider%2520important%2520criteria%2520such%2520as%2520quantifiability%252C%2520consistency%252C%250Aprecision%252C%2520and%2520diversity.%2520Additionally%252C%2520we%2520leverage%2520large%2520language%2520models%250A%2528LLMs%2529%2520as%2520a%2520substitute%2520for%2520human%2520judgment%252C%2520enabling%2520visual%2520evaluation%2520with%2520an%250Aemphasis%2520on%2520high-quality%2520reconstructions.%2520Using%2520our%2520proposed%2520taxonomy%2520and%250Ametrics%252C%2520we%2520present%2520a%2520unified%2520framework%2520for%2520systematically%2520evaluating%2520the%250Astrengths%2520and%2520limitations%2520of%2520existing%2520attacks%2520and%2520establishing%2520a%2520benchmark%2520for%250Afuture%2520research.%2520Empirical%2520results%252C%2520primarily%2520from%2520a%2520memorization%2520perspective%252C%250Anot%2520only%2520validate%2520the%2520effectiveness%2520of%2520our%2520metrics%2520but%2520also%2520offer%2520valuable%250Ainsights%2520for%2520designing%2520new%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20Data%20Reconstruction%20Attacks%20Against%20Machine%20Learning%20Models%3A%0A%20%20Definition%2C%20Metrics%2C%20and%20Benchmark&entry.906535625=Rui%20Wen%20and%20Yiyong%20Liu%20and%20Michael%20Backes%20and%20Yang%20Zhang&entry.1292438233=%20%20Data%20reconstruction%20attacks%2C%20which%20aim%20to%20recover%20the%20training%20dataset%20of%20a%0Atarget%20model%20with%20limited%20access%2C%20have%20gained%20increasing%20attention%20in%20recent%0Ayears.%20However%2C%20there%20is%20currently%20no%20consensus%20on%20a%20formal%20definition%20of%20data%0Areconstruction%20attacks%20or%20appropriate%20evaluation%20metrics%20for%20measuring%20their%0Aquality.%20This%20lack%20of%20rigorous%20definitions%20and%20universal%20metrics%20has%20hindered%0Afurther%20advancement%20in%20this%20field.%20In%20this%20paper%2C%20we%20address%20this%20issue%20in%20the%0Avision%20domain%20by%20proposing%20a%20unified%20attack%20taxonomy%20and%20formal%20definitions%20of%0Adata%20reconstruction%20attacks.%20We%20first%20propose%20a%20set%20of%20quantitative%20evaluation%0Ametrics%20that%20consider%20important%20criteria%20such%20as%20quantifiability%2C%20consistency%2C%0Aprecision%2C%20and%20diversity.%20Additionally%2C%20we%20leverage%20large%20language%20models%0A%28LLMs%29%20as%20a%20substitute%20for%20human%20judgment%2C%20enabling%20visual%20evaluation%20with%20an%0Aemphasis%20on%20high-quality%20reconstructions.%20Using%20our%20proposed%20taxonomy%20and%0Ametrics%2C%20we%20present%20a%20unified%20framework%20for%20systematically%20evaluating%20the%0Astrengths%20and%20limitations%20of%20existing%20attacks%20and%20establishing%20a%20benchmark%20for%0Afuture%20research.%20Empirical%20results%2C%20primarily%20from%20a%20memorization%20perspective%2C%0Anot%20only%20validate%20the%20effectiveness%20of%20our%20metrics%20but%20also%20offer%20valuable%0Ainsights%20for%20designing%20new%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07888v1&entry.124074799=Read"},
{"title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models", "author": "Sicheng Mo and Ziyang Leng and Leon Liu and Weizhen Wang and Honglin He and Bolei Zhou", "abstract": "  Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.\n", "link": "http://arxiv.org/abs/2506.08006v1", "date": "2025-06-09", "relevancy": 2.538, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6605}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6376}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dreamland%3A%20Controllable%20World%20Creation%20with%20Simulator%20and%20Generative%0A%20%20Models&body=Title%3A%20Dreamland%3A%20Controllable%20World%20Creation%20with%20Simulator%20and%20Generative%0A%20%20Models%0AAuthor%3A%20Sicheng%20Mo%20and%20Ziyang%20Leng%20and%20Leon%20Liu%20and%20Weizhen%20Wang%20and%20Honglin%20He%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Large-scale%20video%20generative%20models%20can%20synthesize%20diverse%20and%20realistic%0Avisual%20content%20for%20dynamic%20world%20creation%2C%20but%20they%20often%20lack%20element-wise%0Acontrollability%2C%20hindering%20their%20use%20in%20editing%20scenes%20and%20training%20embodied%20AI%0Aagents.%20We%20propose%20Dreamland%2C%20a%20hybrid%20world%20generation%20framework%20combining%20the%0Agranular%20control%20of%20a%20physics-based%20simulator%20and%20the%20photorealistic%20content%0Aoutput%20of%20large-scale%20pretrained%20generative%20models.%20In%20particular%2C%20we%20design%20a%0Alayered%20world%20abstraction%20that%20encodes%20both%20pixel-level%20and%20object-level%0Asemantics%20and%20geometry%20as%20an%20intermediate%20representation%20to%20bridge%20the%0Asimulator%20and%20the%20generative%20model.%20This%20approach%20enhances%20controllability%2C%0Aminimizes%20adaptation%20cost%20through%20early%20alignment%20with%20real-world%0Adistributions%2C%20and%20supports%20off-the-shelf%20use%20of%20existing%20and%20future%20pretrained%0Agenerative%20models.%20We%20further%20construct%20a%20D3Sim%20dataset%20to%20facilitate%20the%0Atraining%20and%20evaluation%20of%20hybrid%20generation%20pipelines.%20Experiments%20demonstrate%0Athat%20Dreamland%20outperforms%20existing%20baselines%20with%2050.8%25%20improved%20image%0Aquality%2C%2017.9%25%20stronger%20controllability%2C%20and%20has%20great%20potential%20to%20enhance%0Aembodied%20agent%20training.%20Code%20and%20data%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamland%253A%2520Controllable%2520World%2520Creation%2520with%2520Simulator%2520and%2520Generative%250A%2520%2520Models%26entry.906535625%3DSicheng%2520Mo%2520and%2520Ziyang%2520Leng%2520and%2520Leon%2520Liu%2520and%2520Weizhen%2520Wang%2520and%2520Honglin%2520He%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520Large-scale%2520video%2520generative%2520models%2520can%2520synthesize%2520diverse%2520and%2520realistic%250Avisual%2520content%2520for%2520dynamic%2520world%2520creation%252C%2520but%2520they%2520often%2520lack%2520element-wise%250Acontrollability%252C%2520hindering%2520their%2520use%2520in%2520editing%2520scenes%2520and%2520training%2520embodied%2520AI%250Aagents.%2520We%2520propose%2520Dreamland%252C%2520a%2520hybrid%2520world%2520generation%2520framework%2520combining%2520the%250Agranular%2520control%2520of%2520a%2520physics-based%2520simulator%2520and%2520the%2520photorealistic%2520content%250Aoutput%2520of%2520large-scale%2520pretrained%2520generative%2520models.%2520In%2520particular%252C%2520we%2520design%2520a%250Alayered%2520world%2520abstraction%2520that%2520encodes%2520both%2520pixel-level%2520and%2520object-level%250Asemantics%2520and%2520geometry%2520as%2520an%2520intermediate%2520representation%2520to%2520bridge%2520the%250Asimulator%2520and%2520the%2520generative%2520model.%2520This%2520approach%2520enhances%2520controllability%252C%250Aminimizes%2520adaptation%2520cost%2520through%2520early%2520alignment%2520with%2520real-world%250Adistributions%252C%2520and%2520supports%2520off-the-shelf%2520use%2520of%2520existing%2520and%2520future%2520pretrained%250Agenerative%2520models.%2520We%2520further%2520construct%2520a%2520D3Sim%2520dataset%2520to%2520facilitate%2520the%250Atraining%2520and%2520evaluation%2520of%2520hybrid%2520generation%2520pipelines.%2520Experiments%2520demonstrate%250Athat%2520Dreamland%2520outperforms%2520existing%2520baselines%2520with%252050.8%2525%2520improved%2520image%250Aquality%252C%252017.9%2525%2520stronger%2520controllability%252C%2520and%2520has%2520great%2520potential%2520to%2520enhance%250Aembodied%2520agent%2520training.%2520Code%2520and%2520data%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dreamland%3A%20Controllable%20World%20Creation%20with%20Simulator%20and%20Generative%0A%20%20Models&entry.906535625=Sicheng%20Mo%20and%20Ziyang%20Leng%20and%20Leon%20Liu%20and%20Weizhen%20Wang%20and%20Honglin%20He%20and%20Bolei%20Zhou&entry.1292438233=%20%20Large-scale%20video%20generative%20models%20can%20synthesize%20diverse%20and%20realistic%0Avisual%20content%20for%20dynamic%20world%20creation%2C%20but%20they%20often%20lack%20element-wise%0Acontrollability%2C%20hindering%20their%20use%20in%20editing%20scenes%20and%20training%20embodied%20AI%0Aagents.%20We%20propose%20Dreamland%2C%20a%20hybrid%20world%20generation%20framework%20combining%20the%0Agranular%20control%20of%20a%20physics-based%20simulator%20and%20the%20photorealistic%20content%0Aoutput%20of%20large-scale%20pretrained%20generative%20models.%20In%20particular%2C%20we%20design%20a%0Alayered%20world%20abstraction%20that%20encodes%20both%20pixel-level%20and%20object-level%0Asemantics%20and%20geometry%20as%20an%20intermediate%20representation%20to%20bridge%20the%0Asimulator%20and%20the%20generative%20model.%20This%20approach%20enhances%20controllability%2C%0Aminimizes%20adaptation%20cost%20through%20early%20alignment%20with%20real-world%0Adistributions%2C%20and%20supports%20off-the-shelf%20use%20of%20existing%20and%20future%20pretrained%0Agenerative%20models.%20We%20further%20construct%20a%20D3Sim%20dataset%20to%20facilitate%20the%0Atraining%20and%20evaluation%20of%20hybrid%20generation%20pipelines.%20Experiments%20demonstrate%0Athat%20Dreamland%20outperforms%20existing%20baselines%20with%2050.8%25%20improved%20image%0Aquality%2C%2017.9%25%20stronger%20controllability%2C%20and%20has%20great%20potential%20to%20enhance%0Aembodied%20agent%20training.%20Code%20and%20data%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08006v1&entry.124074799=Read"},
{"title": "Realistic Urban Traffic Generator using Decentralized Federated Learning\n  for the SUMO simulator", "author": "Alberto Baz\u00e1n-Guill\u00e9n and Carlos Beis-Penedo and Diego Cajaraville-Aboy and Pablo Barbecho-Bautista and Rebeca P. D\u00edaz-Redondo and Luis J. de la Cruz Llopis and Ana Fern\u00e1ndez-Vilas and M\u00f3nica Aguilar Igartua and Manuel Fern\u00e1ndez-Veiga", "abstract": "  Realistic urban traffic simulation is essential for sustainable urban\nplanning and the development of intelligent transportation systems. However,\ngenerating high-fidelity, time-varying traffic profiles that accurately reflect\nreal-world conditions, especially in large-scale scenarios, remains a major\nchallenge. Existing methods often suffer from limitations in accuracy,\nscalability, or raise privacy concerns due to centralized data processing. This\nwork introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a\nnovel framework that integrates Deep Reinforcement Learning (DRL) agents with\nthe SUMO simulator to generate realistic 24-hour traffic patterns. A key\ninnovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),\nwherein each traffic detector and its corresponding urban zone function as an\nindependent learning node. These nodes train local DRL models using minimal\nhistorical data and collaboratively refine their performance by exchanging\nmodel parameters with selected peers (e.g., geographically adjacent zones),\nwithout requiring a central coordinator. Evaluated using real-world data from\nthe city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as\nRouteSampler, as well as other centralized learning approaches, by delivering\nmore accurate and privacy-preserving traffic pattern generation.\n", "link": "http://arxiv.org/abs/2506.07980v1", "date": "2025-06-09", "relevancy": 2.5157, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5078}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Urban%20Traffic%20Generator%20using%20Decentralized%20Federated%20Learning%0A%20%20for%20the%20SUMO%20simulator&body=Title%3A%20Realistic%20Urban%20Traffic%20Generator%20using%20Decentralized%20Federated%20Learning%0A%20%20for%20the%20SUMO%20simulator%0AAuthor%3A%20Alberto%20Baz%C3%A1n-Guill%C3%A9n%20and%20Carlos%20Beis-Penedo%20and%20Diego%20Cajaraville-Aboy%20and%20Pablo%20Barbecho-Bautista%20and%20Rebeca%20P.%20D%C3%ADaz-Redondo%20and%20Luis%20J.%20de%20la%20Cruz%20Llopis%20and%20Ana%20Fern%C3%A1ndez-Vilas%20and%20M%C3%B3nica%20Aguilar%20Igartua%20and%20Manuel%20Fern%C3%A1ndez-Veiga%0AAbstract%3A%20%20%20Realistic%20urban%20traffic%20simulation%20is%20essential%20for%20sustainable%20urban%0Aplanning%20and%20the%20development%20of%20intelligent%20transportation%20systems.%20However%2C%0Agenerating%20high-fidelity%2C%20time-varying%20traffic%20profiles%20that%20accurately%20reflect%0Areal-world%20conditions%2C%20especially%20in%20large-scale%20scenarios%2C%20remains%20a%20major%0Achallenge.%20Existing%20methods%20often%20suffer%20from%20limitations%20in%20accuracy%2C%0Ascalability%2C%20or%20raise%20privacy%20concerns%20due%20to%20centralized%20data%20processing.%20This%0Awork%20introduces%20DesRUTGe%20%28Decentralized%20Realistic%20Urban%20Traffic%20Generator%29%2C%20a%0Anovel%20framework%20that%20integrates%20Deep%20Reinforcement%20Learning%20%28DRL%29%20agents%20with%0Athe%20SUMO%20simulator%20to%20generate%20realistic%2024-hour%20traffic%20patterns.%20A%20key%0Ainnovation%20of%20DesRUTGe%20is%20its%20use%20of%20Decentralized%20Federated%20Learning%20%28DFL%29%2C%0Awherein%20each%20traffic%20detector%20and%20its%20corresponding%20urban%20zone%20function%20as%20an%0Aindependent%20learning%20node.%20These%20nodes%20train%20local%20DRL%20models%20using%20minimal%0Ahistorical%20data%20and%20collaboratively%20refine%20their%20performance%20by%20exchanging%0Amodel%20parameters%20with%20selected%20peers%20%28e.g.%2C%20geographically%20adjacent%20zones%29%2C%0Awithout%20requiring%20a%20central%20coordinator.%20Evaluated%20using%20real-world%20data%20from%0Athe%20city%20of%20Barcelona%2C%20DesRUTGe%20outperforms%20standard%20SUMO-based%20tools%20such%20as%0ARouteSampler%2C%20as%20well%20as%20other%20centralized%20learning%20approaches%2C%20by%20delivering%0Amore%20accurate%20and%20privacy-preserving%20traffic%20pattern%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Urban%2520Traffic%2520Generator%2520using%2520Decentralized%2520Federated%2520Learning%250A%2520%2520for%2520the%2520SUMO%2520simulator%26entry.906535625%3DAlberto%2520Baz%25C3%25A1n-Guill%25C3%25A9n%2520and%2520Carlos%2520Beis-Penedo%2520and%2520Diego%2520Cajaraville-Aboy%2520and%2520Pablo%2520Barbecho-Bautista%2520and%2520Rebeca%2520P.%2520D%25C3%25ADaz-Redondo%2520and%2520Luis%2520J.%2520de%2520la%2520Cruz%2520Llopis%2520and%2520Ana%2520Fern%25C3%25A1ndez-Vilas%2520and%2520M%25C3%25B3nica%2520Aguilar%2520Igartua%2520and%2520Manuel%2520Fern%25C3%25A1ndez-Veiga%26entry.1292438233%3D%2520%2520Realistic%2520urban%2520traffic%2520simulation%2520is%2520essential%2520for%2520sustainable%2520urban%250Aplanning%2520and%2520the%2520development%2520of%2520intelligent%2520transportation%2520systems.%2520However%252C%250Agenerating%2520high-fidelity%252C%2520time-varying%2520traffic%2520profiles%2520that%2520accurately%2520reflect%250Areal-world%2520conditions%252C%2520especially%2520in%2520large-scale%2520scenarios%252C%2520remains%2520a%2520major%250Achallenge.%2520Existing%2520methods%2520often%2520suffer%2520from%2520limitations%2520in%2520accuracy%252C%250Ascalability%252C%2520or%2520raise%2520privacy%2520concerns%2520due%2520to%2520centralized%2520data%2520processing.%2520This%250Awork%2520introduces%2520DesRUTGe%2520%2528Decentralized%2520Realistic%2520Urban%2520Traffic%2520Generator%2529%252C%2520a%250Anovel%2520framework%2520that%2520integrates%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520agents%2520with%250Athe%2520SUMO%2520simulator%2520to%2520generate%2520realistic%252024-hour%2520traffic%2520patterns.%2520A%2520key%250Ainnovation%2520of%2520DesRUTGe%2520is%2520its%2520use%2520of%2520Decentralized%2520Federated%2520Learning%2520%2528DFL%2529%252C%250Awherein%2520each%2520traffic%2520detector%2520and%2520its%2520corresponding%2520urban%2520zone%2520function%2520as%2520an%250Aindependent%2520learning%2520node.%2520These%2520nodes%2520train%2520local%2520DRL%2520models%2520using%2520minimal%250Ahistorical%2520data%2520and%2520collaboratively%2520refine%2520their%2520performance%2520by%2520exchanging%250Amodel%2520parameters%2520with%2520selected%2520peers%2520%2528e.g.%252C%2520geographically%2520adjacent%2520zones%2529%252C%250Awithout%2520requiring%2520a%2520central%2520coordinator.%2520Evaluated%2520using%2520real-world%2520data%2520from%250Athe%2520city%2520of%2520Barcelona%252C%2520DesRUTGe%2520outperforms%2520standard%2520SUMO-based%2520tools%2520such%2520as%250ARouteSampler%252C%2520as%2520well%2520as%2520other%2520centralized%2520learning%2520approaches%252C%2520by%2520delivering%250Amore%2520accurate%2520and%2520privacy-preserving%2520traffic%2520pattern%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Urban%20Traffic%20Generator%20using%20Decentralized%20Federated%20Learning%0A%20%20for%20the%20SUMO%20simulator&entry.906535625=Alberto%20Baz%C3%A1n-Guill%C3%A9n%20and%20Carlos%20Beis-Penedo%20and%20Diego%20Cajaraville-Aboy%20and%20Pablo%20Barbecho-Bautista%20and%20Rebeca%20P.%20D%C3%ADaz-Redondo%20and%20Luis%20J.%20de%20la%20Cruz%20Llopis%20and%20Ana%20Fern%C3%A1ndez-Vilas%20and%20M%C3%B3nica%20Aguilar%20Igartua%20and%20Manuel%20Fern%C3%A1ndez-Veiga&entry.1292438233=%20%20Realistic%20urban%20traffic%20simulation%20is%20essential%20for%20sustainable%20urban%0Aplanning%20and%20the%20development%20of%20intelligent%20transportation%20systems.%20However%2C%0Agenerating%20high-fidelity%2C%20time-varying%20traffic%20profiles%20that%20accurately%20reflect%0Areal-world%20conditions%2C%20especially%20in%20large-scale%20scenarios%2C%20remains%20a%20major%0Achallenge.%20Existing%20methods%20often%20suffer%20from%20limitations%20in%20accuracy%2C%0Ascalability%2C%20or%20raise%20privacy%20concerns%20due%20to%20centralized%20data%20processing.%20This%0Awork%20introduces%20DesRUTGe%20%28Decentralized%20Realistic%20Urban%20Traffic%20Generator%29%2C%20a%0Anovel%20framework%20that%20integrates%20Deep%20Reinforcement%20Learning%20%28DRL%29%20agents%20with%0Athe%20SUMO%20simulator%20to%20generate%20realistic%2024-hour%20traffic%20patterns.%20A%20key%0Ainnovation%20of%20DesRUTGe%20is%20its%20use%20of%20Decentralized%20Federated%20Learning%20%28DFL%29%2C%0Awherein%20each%20traffic%20detector%20and%20its%20corresponding%20urban%20zone%20function%20as%20an%0Aindependent%20learning%20node.%20These%20nodes%20train%20local%20DRL%20models%20using%20minimal%0Ahistorical%20data%20and%20collaboratively%20refine%20their%20performance%20by%20exchanging%0Amodel%20parameters%20with%20selected%20peers%20%28e.g.%2C%20geographically%20adjacent%20zones%29%2C%0Awithout%20requiring%20a%20central%20coordinator.%20Evaluated%20using%20real-world%20data%20from%0Athe%20city%20of%20Barcelona%2C%20DesRUTGe%20outperforms%20standard%20SUMO-based%20tools%20such%20as%0ARouteSampler%2C%20as%20well%20as%20other%20centralized%20learning%20approaches%2C%20by%20delivering%0Amore%20accurate%20and%20privacy-preserving%20traffic%20pattern%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07980v1&entry.124074799=Read"},
{"title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior", "author": "Penghao Wu and Shengnan Ma and Bo Wang and Jiaheng Yu and Lewei Lu and Ziwei Liu", "abstract": "  Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.\n", "link": "http://arxiv.org/abs/2506.08012v1", "date": "2025-06-09", "relevancy": 2.5154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4997}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-Reflection%3A%20Empowering%20Multimodal%20GUI%20Models%20with%20Self-Reflection%0A%20%20Behavior&body=Title%3A%20GUI-Reflection%3A%20Empowering%20Multimodal%20GUI%20Models%20with%20Self-Reflection%0A%20%20Behavior%0AAuthor%3A%20Penghao%20Wu%20and%20Shengnan%20Ma%20and%20Bo%20Wang%20and%20Jiaheng%20Yu%20and%20Lewei%20Lu%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20great%20potential%20in%0Arevolutionizing%20Graphical%20User%20Interface%20%28GUI%29%20automation.%20However%2C%20existing%0AGUI%20models%20mostly%20rely%20on%20learning%20from%20nearly%20error-free%20offline%20trajectories%2C%0Athus%20lacking%20reflection%20and%20error%20recovery%20capabilities.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20GUI-Reflection%2C%20a%20novel%20framework%20that%20explicitly%20integrates%0Aself-reflection%20and%20error%20correction%20capabilities%20into%20end-to-end%20multimodal%0AGUI%20models%20throughout%20dedicated%20training%20stages%3A%20GUI-specific%20pre-training%2C%0Aoffline%20supervised%20fine-tuning%20%28SFT%29%2C%20and%20online%20reflection%20tuning.%0AGUI-reflection%20enables%20self-reflection%20behavior%20emergence%20with%20fully%20automated%0Adata%20generation%20and%20learning%20processes%20without%20requiring%20any%20human%20annotation.%0ASpecifically%2C%201%29%20we%20first%20propose%20scalable%20data%20pipelines%20to%20automatically%0Aconstruct%20reflection%20and%20error%20correction%20data%20from%20existing%20successful%0Atrajectories.%20While%20existing%20GUI%20models%20mainly%20focus%20on%20grounding%20and%20UI%0Aunderstanding%20ability%2C%20we%20propose%20the%20GUI-Reflection%20Task%20Suite%20to%20learn%20and%0Aevaluate%20reflection-oriented%20abilities%20explicitly.%202%29%20Furthermore%2C%20we%20built%20a%0Adiverse%20and%20efficient%20environment%20for%20online%20training%20and%20data%20collection%20of%0AGUI%20models%20on%20mobile%20devices.%203%29%20We%20also%20present%20an%20iterative%20online%20reflection%0Atuning%20algorithm%20leveraging%20the%20proposed%20environment%2C%20enabling%20the%20model%20to%0Acontinuously%20enhance%20its%20reflection%20and%20error%20correction%20abilities.%20Our%0Aframework%20equips%20GUI%20agents%20with%20self-reflection%20and%20correction%20capabilities%2C%0Apaving%20the%20way%20for%20more%20robust%2C%20adaptable%2C%20and%20intelligent%20GUI%20automation%2C%20with%0Aall%20data%2C%20models%2C%20environments%2C%20and%20tools%20to%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-Reflection%253A%2520Empowering%2520Multimodal%2520GUI%2520Models%2520with%2520Self-Reflection%250A%2520%2520Behavior%26entry.906535625%3DPenghao%2520Wu%2520and%2520Shengnan%2520Ma%2520and%2520Bo%2520Wang%2520and%2520Jiaheng%2520Yu%2520and%2520Lewei%2520Lu%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520great%2520potential%2520in%250Arevolutionizing%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520automation.%2520However%252C%2520existing%250AGUI%2520models%2520mostly%2520rely%2520on%2520learning%2520from%2520nearly%2520error-free%2520offline%2520trajectories%252C%250Athus%2520lacking%2520reflection%2520and%2520error%2520recovery%2520capabilities.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Apropose%2520GUI-Reflection%252C%2520a%2520novel%2520framework%2520that%2520explicitly%2520integrates%250Aself-reflection%2520and%2520error%2520correction%2520capabilities%2520into%2520end-to-end%2520multimodal%250AGUI%2520models%2520throughout%2520dedicated%2520training%2520stages%253A%2520GUI-specific%2520pre-training%252C%250Aoffline%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520and%2520online%2520reflection%2520tuning.%250AGUI-reflection%2520enables%2520self-reflection%2520behavior%2520emergence%2520with%2520fully%2520automated%250Adata%2520generation%2520and%2520learning%2520processes%2520without%2520requiring%2520any%2520human%2520annotation.%250ASpecifically%252C%25201%2529%2520we%2520first%2520propose%2520scalable%2520data%2520pipelines%2520to%2520automatically%250Aconstruct%2520reflection%2520and%2520error%2520correction%2520data%2520from%2520existing%2520successful%250Atrajectories.%2520While%2520existing%2520GUI%2520models%2520mainly%2520focus%2520on%2520grounding%2520and%2520UI%250Aunderstanding%2520ability%252C%2520we%2520propose%2520the%2520GUI-Reflection%2520Task%2520Suite%2520to%2520learn%2520and%250Aevaluate%2520reflection-oriented%2520abilities%2520explicitly.%25202%2529%2520Furthermore%252C%2520we%2520built%2520a%250Adiverse%2520and%2520efficient%2520environment%2520for%2520online%2520training%2520and%2520data%2520collection%2520of%250AGUI%2520models%2520on%2520mobile%2520devices.%25203%2529%2520We%2520also%2520present%2520an%2520iterative%2520online%2520reflection%250Atuning%2520algorithm%2520leveraging%2520the%2520proposed%2520environment%252C%2520enabling%2520the%2520model%2520to%250Acontinuously%2520enhance%2520its%2520reflection%2520and%2520error%2520correction%2520abilities.%2520Our%250Aframework%2520equips%2520GUI%2520agents%2520with%2520self-reflection%2520and%2520correction%2520capabilities%252C%250Apaving%2520the%2520way%2520for%2520more%2520robust%252C%2520adaptable%252C%2520and%2520intelligent%2520GUI%2520automation%252C%2520with%250Aall%2520data%252C%2520models%252C%2520environments%252C%2520and%2520tools%2520to%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-Reflection%3A%20Empowering%20Multimodal%20GUI%20Models%20with%20Self-Reflection%0A%20%20Behavior&entry.906535625=Penghao%20Wu%20and%20Shengnan%20Ma%20and%20Bo%20Wang%20and%20Jiaheng%20Yu%20and%20Lewei%20Lu%20and%20Ziwei%20Liu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20great%20potential%20in%0Arevolutionizing%20Graphical%20User%20Interface%20%28GUI%29%20automation.%20However%2C%20existing%0AGUI%20models%20mostly%20rely%20on%20learning%20from%20nearly%20error-free%20offline%20trajectories%2C%0Athus%20lacking%20reflection%20and%20error%20recovery%20capabilities.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20GUI-Reflection%2C%20a%20novel%20framework%20that%20explicitly%20integrates%0Aself-reflection%20and%20error%20correction%20capabilities%20into%20end-to-end%20multimodal%0AGUI%20models%20throughout%20dedicated%20training%20stages%3A%20GUI-specific%20pre-training%2C%0Aoffline%20supervised%20fine-tuning%20%28SFT%29%2C%20and%20online%20reflection%20tuning.%0AGUI-reflection%20enables%20self-reflection%20behavior%20emergence%20with%20fully%20automated%0Adata%20generation%20and%20learning%20processes%20without%20requiring%20any%20human%20annotation.%0ASpecifically%2C%201%29%20we%20first%20propose%20scalable%20data%20pipelines%20to%20automatically%0Aconstruct%20reflection%20and%20error%20correction%20data%20from%20existing%20successful%0Atrajectories.%20While%20existing%20GUI%20models%20mainly%20focus%20on%20grounding%20and%20UI%0Aunderstanding%20ability%2C%20we%20propose%20the%20GUI-Reflection%20Task%20Suite%20to%20learn%20and%0Aevaluate%20reflection-oriented%20abilities%20explicitly.%202%29%20Furthermore%2C%20we%20built%20a%0Adiverse%20and%20efficient%20environment%20for%20online%20training%20and%20data%20collection%20of%0AGUI%20models%20on%20mobile%20devices.%203%29%20We%20also%20present%20an%20iterative%20online%20reflection%0Atuning%20algorithm%20leveraging%20the%20proposed%20environment%2C%20enabling%20the%20model%20to%0Acontinuously%20enhance%20its%20reflection%20and%20error%20correction%20abilities.%20Our%0Aframework%20equips%20GUI%20agents%20with%20self-reflection%20and%20correction%20capabilities%2C%0Apaving%20the%20way%20for%20more%20robust%2C%20adaptable%2C%20and%20intelligent%20GUI%20automation%2C%20with%0Aall%20data%2C%20models%2C%20environments%2C%20and%20tools%20to%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08012v1&entry.124074799=Read"},
{"title": "Can Perplexity Predict Fine-tuning Performance? An Investigation of\n  Tokenization Effects on Sequential Language Models for Nepali", "author": "Nishant Luitel and Nirajan Bekoju and Anand Kumar Sah and Subarna Shakya", "abstract": "  The impact of subword tokenization on language model performance is\nwell-documented for perplexity, with finer granularity consistently reducing\nthis intrinsic metric. However, research on how different tokenization schemes\naffect a model's understanding capabilities remains limited, particularly for\nnon-Latin script languages. Addressing this gap, we conducted a comprehensive\nevaluation of six distinct tokenization strategies by pretraining\ntransformer-based language models for Nepali and evaluating their performance\nacross multiple downstream tasks. While recent prominent models like GPT,\nRoBERTa, Claude, LLaMA, Mistral, Falcon, and MPT have adopted byte-level BPE\ntokenization, our findings demonstrate that for Nepali, SentencePiece\ntokenization consistently yields superior results on understanding-based tasks.\nUnlike previous studies that primarily focused on BERT-based architectures, our\nresearch specifically examines sequential transformer models, providing\nvaluable insights for language model development in low-resource languages and\nhighlighting the importance of tokenization strategy beyond perplexity\nreduction.\n", "link": "http://arxiv.org/abs/2404.18071v2", "date": "2025-06-09", "relevancy": 2.508, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Perplexity%20Predict%20Fine-tuning%20Performance%3F%20An%20Investigation%20of%0A%20%20Tokenization%20Effects%20on%20Sequential%20Language%20Models%20for%20Nepali&body=Title%3A%20Can%20Perplexity%20Predict%20Fine-tuning%20Performance%3F%20An%20Investigation%20of%0A%20%20Tokenization%20Effects%20on%20Sequential%20Language%20Models%20for%20Nepali%0AAuthor%3A%20Nishant%20Luitel%20and%20Nirajan%20Bekoju%20and%20Anand%20Kumar%20Sah%20and%20Subarna%20Shakya%0AAbstract%3A%20%20%20The%20impact%20of%20subword%20tokenization%20on%20language%20model%20performance%20is%0Awell-documented%20for%20perplexity%2C%20with%20finer%20granularity%20consistently%20reducing%0Athis%20intrinsic%20metric.%20However%2C%20research%20on%20how%20different%20tokenization%20schemes%0Aaffect%20a%20model%27s%20understanding%20capabilities%20remains%20limited%2C%20particularly%20for%0Anon-Latin%20script%20languages.%20Addressing%20this%20gap%2C%20we%20conducted%20a%20comprehensive%0Aevaluation%20of%20six%20distinct%20tokenization%20strategies%20by%20pretraining%0Atransformer-based%20language%20models%20for%20Nepali%20and%20evaluating%20their%20performance%0Aacross%20multiple%20downstream%20tasks.%20While%20recent%20prominent%20models%20like%20GPT%2C%0ARoBERTa%2C%20Claude%2C%20LLaMA%2C%20Mistral%2C%20Falcon%2C%20and%20MPT%20have%20adopted%20byte-level%20BPE%0Atokenization%2C%20our%20findings%20demonstrate%20that%20for%20Nepali%2C%20SentencePiece%0Atokenization%20consistently%20yields%20superior%20results%20on%20understanding-based%20tasks.%0AUnlike%20previous%20studies%20that%20primarily%20focused%20on%20BERT-based%20architectures%2C%20our%0Aresearch%20specifically%20examines%20sequential%20transformer%20models%2C%20providing%0Avaluable%20insights%20for%20language%20model%20development%20in%20low-resource%20languages%20and%0Ahighlighting%20the%20importance%20of%20tokenization%20strategy%20beyond%20perplexity%0Areduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18071v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Perplexity%2520Predict%2520Fine-tuning%2520Performance%253F%2520An%2520Investigation%2520of%250A%2520%2520Tokenization%2520Effects%2520on%2520Sequential%2520Language%2520Models%2520for%2520Nepali%26entry.906535625%3DNishant%2520Luitel%2520and%2520Nirajan%2520Bekoju%2520and%2520Anand%2520Kumar%2520Sah%2520and%2520Subarna%2520Shakya%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520subword%2520tokenization%2520on%2520language%2520model%2520performance%2520is%250Awell-documented%2520for%2520perplexity%252C%2520with%2520finer%2520granularity%2520consistently%2520reducing%250Athis%2520intrinsic%2520metric.%2520However%252C%2520research%2520on%2520how%2520different%2520tokenization%2520schemes%250Aaffect%2520a%2520model%2527s%2520understanding%2520capabilities%2520remains%2520limited%252C%2520particularly%2520for%250Anon-Latin%2520script%2520languages.%2520Addressing%2520this%2520gap%252C%2520we%2520conducted%2520a%2520comprehensive%250Aevaluation%2520of%2520six%2520distinct%2520tokenization%2520strategies%2520by%2520pretraining%250Atransformer-based%2520language%2520models%2520for%2520Nepali%2520and%2520evaluating%2520their%2520performance%250Aacross%2520multiple%2520downstream%2520tasks.%2520While%2520recent%2520prominent%2520models%2520like%2520GPT%252C%250ARoBERTa%252C%2520Claude%252C%2520LLaMA%252C%2520Mistral%252C%2520Falcon%252C%2520and%2520MPT%2520have%2520adopted%2520byte-level%2520BPE%250Atokenization%252C%2520our%2520findings%2520demonstrate%2520that%2520for%2520Nepali%252C%2520SentencePiece%250Atokenization%2520consistently%2520yields%2520superior%2520results%2520on%2520understanding-based%2520tasks.%250AUnlike%2520previous%2520studies%2520that%2520primarily%2520focused%2520on%2520BERT-based%2520architectures%252C%2520our%250Aresearch%2520specifically%2520examines%2520sequential%2520transformer%2520models%252C%2520providing%250Avaluable%2520insights%2520for%2520language%2520model%2520development%2520in%2520low-resource%2520languages%2520and%250Ahighlighting%2520the%2520importance%2520of%2520tokenization%2520strategy%2520beyond%2520perplexity%250Areduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18071v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Perplexity%20Predict%20Fine-tuning%20Performance%3F%20An%20Investigation%20of%0A%20%20Tokenization%20Effects%20on%20Sequential%20Language%20Models%20for%20Nepali&entry.906535625=Nishant%20Luitel%20and%20Nirajan%20Bekoju%20and%20Anand%20Kumar%20Sah%20and%20Subarna%20Shakya&entry.1292438233=%20%20The%20impact%20of%20subword%20tokenization%20on%20language%20model%20performance%20is%0Awell-documented%20for%20perplexity%2C%20with%20finer%20granularity%20consistently%20reducing%0Athis%20intrinsic%20metric.%20However%2C%20research%20on%20how%20different%20tokenization%20schemes%0Aaffect%20a%20model%27s%20understanding%20capabilities%20remains%20limited%2C%20particularly%20for%0Anon-Latin%20script%20languages.%20Addressing%20this%20gap%2C%20we%20conducted%20a%20comprehensive%0Aevaluation%20of%20six%20distinct%20tokenization%20strategies%20by%20pretraining%0Atransformer-based%20language%20models%20for%20Nepali%20and%20evaluating%20their%20performance%0Aacross%20multiple%20downstream%20tasks.%20While%20recent%20prominent%20models%20like%20GPT%2C%0ARoBERTa%2C%20Claude%2C%20LLaMA%2C%20Mistral%2C%20Falcon%2C%20and%20MPT%20have%20adopted%20byte-level%20BPE%0Atokenization%2C%20our%20findings%20demonstrate%20that%20for%20Nepali%2C%20SentencePiece%0Atokenization%20consistently%20yields%20superior%20results%20on%20understanding-based%20tasks.%0AUnlike%20previous%20studies%20that%20primarily%20focused%20on%20BERT-based%20architectures%2C%20our%0Aresearch%20specifically%20examines%20sequential%20transformer%20models%2C%20providing%0Avaluable%20insights%20for%20language%20model%20development%20in%20low-resource%20languages%20and%0Ahighlighting%20the%20importance%20of%20tokenization%20strategy%20beyond%20perplexity%0Areduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18071v2&entry.124074799=Read"},
{"title": "Training Superior Sparse Autoencoders for Instruct Models", "author": "Jiaming Li and Haoran Ye and Yukun Chen and Xinyue Li and Lei Zhang and Hamid Alinejad-Rokny and Jimmy Chih-Hsien Peng and Min Yang", "abstract": "  As large language models (LLMs) grow in scale and capability, understanding\ntheir internal mechanisms becomes increasingly critical. Sparse autoencoders\n(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the\nextraction of human-interpretable features from LLMs. However, existing SAE\ntraining methods are primarily designed for base models, resulting in reduced\nreconstruction quality and interpretability when applied to instruct models. To\nbridge this gap, we propose\n$\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned\n$\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining\n($\\textit{FAST}$), a novel training method specifically tailored for instruct\nmodels. $\\textit{FAST}$ aligns the training process with the data distribution\nand activation patterns characteristic of instruct models, resulting in\nsubstantial improvements in both reconstruction and feature interpretability.\nOn Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468\nin token reconstruction, significantly outperforming baseline methods with\nerrors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$\nyields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,\n$21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for\n$\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that\nintervening on the activations of special tokens via the SAEs leads to\nimprovements in output quality, suggesting new opportunities for fine-grained\ncontrol of model behavior. Code, data, and 240 trained SAEs are available at\nhttps://github.com/Geaming2002/FAST.\n", "link": "http://arxiv.org/abs/2506.07691v1", "date": "2025-06-09", "relevancy": 2.4989, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Superior%20Sparse%20Autoencoders%20for%20Instruct%20Models&body=Title%3A%20Training%20Superior%20Sparse%20Autoencoders%20for%20Instruct%20Models%0AAuthor%3A%20Jiaming%20Li%20and%20Haoran%20Ye%20and%20Yukun%20Chen%20and%20Xinyue%20Li%20and%20Lei%20Zhang%20and%20Hamid%20Alinejad-Rokny%20and%20Jimmy%20Chih-Hsien%20Peng%20and%20Min%20Yang%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20grow%20in%20scale%20and%20capability%2C%20understanding%0Atheir%20internal%20mechanisms%20becomes%20increasingly%20critical.%20Sparse%20autoencoders%0A%28SAEs%29%20have%20emerged%20as%20a%20key%20tool%20in%20mechanistic%20interpretability%2C%20enabling%20the%0Aextraction%20of%20human-interpretable%20features%20from%20LLMs.%20However%2C%20existing%20SAE%0Atraining%20methods%20are%20primarily%20designed%20for%20base%20models%2C%20resulting%20in%20reduced%0Areconstruction%20quality%20and%20interpretability%20when%20applied%20to%20instruct%20models.%20To%0Abridge%20this%20gap%2C%20we%20propose%0A%24%5Cunderline%7B%5Ctextbf%7BF%7D%7D%24inetuning-%24%5Cunderline%7B%5Ctextbf%7Ba%7D%7D%24ligned%0A%24%5Cunderline%7B%5Ctextbf%7BS%7D%7D%24equential%20%24%5Cunderline%7B%5Ctextbf%7BT%7D%7D%24raining%0A%28%24%5Ctextit%7BFAST%7D%24%29%2C%20a%20novel%20training%20method%20specifically%20tailored%20for%20instruct%0Amodels.%20%24%5Ctextit%7BFAST%7D%24%20aligns%20the%20training%20process%20with%20the%20data%20distribution%0Aand%20activation%20patterns%20characteristic%20of%20instruct%20models%2C%20resulting%20in%0Asubstantial%20improvements%20in%20both%20reconstruction%20and%20feature%20interpretability.%0AOn%20Qwen2.5-7B-Instruct%2C%20%24%5Ctextit%7BFAST%7D%24%20achieves%20a%20mean%20squared%20error%20of%200.6468%0Ain%20token%20reconstruction%2C%20significantly%20outperforming%20baseline%20methods%20with%0Aerrors%20of%205.1985%20and%201.5096.%20In%20feature%20interpretability%2C%20%24%5Ctextit%7BFAST%7D%24%0Ayields%20a%20higher%20proportion%20of%20high-quality%20features%2C%20for%20Llama3.2-3B-Instruct%2C%0A%2421.1%5C%25%24%20scored%20in%20the%20top%20range%2C%20compared%20to%20%247.0%5C%25%24%20and%20%2410.2%5C%25%24%20for%0A%24%5Ctextit%7BBT%28P%29%7D%24%20and%20%24%5Ctextit%7BBT%28F%29%7D%24.%20Surprisingly%2C%20we%20discover%20that%0Aintervening%20on%20the%20activations%20of%20special%20tokens%20via%20the%20SAEs%20leads%20to%0Aimprovements%20in%20output%20quality%2C%20suggesting%20new%20opportunities%20for%20fine-grained%0Acontrol%20of%20model%20behavior.%20Code%2C%20data%2C%20and%20240%20trained%20SAEs%20are%20available%20at%0Ahttps%3A//github.com/Geaming2002/FAST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Superior%2520Sparse%2520Autoencoders%2520for%2520Instruct%2520Models%26entry.906535625%3DJiaming%2520Li%2520and%2520Haoran%2520Ye%2520and%2520Yukun%2520Chen%2520and%2520Xinyue%2520Li%2520and%2520Lei%2520Zhang%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Jimmy%2520Chih-Hsien%2520Peng%2520and%2520Min%2520Yang%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520grow%2520in%2520scale%2520and%2520capability%252C%2520understanding%250Atheir%2520internal%2520mechanisms%2520becomes%2520increasingly%2520critical.%2520Sparse%2520autoencoders%250A%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520key%2520tool%2520in%2520mechanistic%2520interpretability%252C%2520enabling%2520the%250Aextraction%2520of%2520human-interpretable%2520features%2520from%2520LLMs.%2520However%252C%2520existing%2520SAE%250Atraining%2520methods%2520are%2520primarily%2520designed%2520for%2520base%2520models%252C%2520resulting%2520in%2520reduced%250Areconstruction%2520quality%2520and%2520interpretability%2520when%2520applied%2520to%2520instruct%2520models.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%250A%2524%255Cunderline%257B%255Ctextbf%257BF%257D%257D%2524inetuning-%2524%255Cunderline%257B%255Ctextbf%257Ba%257D%257D%2524ligned%250A%2524%255Cunderline%257B%255Ctextbf%257BS%257D%257D%2524equential%2520%2524%255Cunderline%257B%255Ctextbf%257BT%257D%257D%2524raining%250A%2528%2524%255Ctextit%257BFAST%257D%2524%2529%252C%2520a%2520novel%2520training%2520method%2520specifically%2520tailored%2520for%2520instruct%250Amodels.%2520%2524%255Ctextit%257BFAST%257D%2524%2520aligns%2520the%2520training%2520process%2520with%2520the%2520data%2520distribution%250Aand%2520activation%2520patterns%2520characteristic%2520of%2520instruct%2520models%252C%2520resulting%2520in%250Asubstantial%2520improvements%2520in%2520both%2520reconstruction%2520and%2520feature%2520interpretability.%250AOn%2520Qwen2.5-7B-Instruct%252C%2520%2524%255Ctextit%257BFAST%257D%2524%2520achieves%2520a%2520mean%2520squared%2520error%2520of%25200.6468%250Ain%2520token%2520reconstruction%252C%2520significantly%2520outperforming%2520baseline%2520methods%2520with%250Aerrors%2520of%25205.1985%2520and%25201.5096.%2520In%2520feature%2520interpretability%252C%2520%2524%255Ctextit%257BFAST%257D%2524%250Ayields%2520a%2520higher%2520proportion%2520of%2520high-quality%2520features%252C%2520for%2520Llama3.2-3B-Instruct%252C%250A%252421.1%255C%2525%2524%2520scored%2520in%2520the%2520top%2520range%252C%2520compared%2520to%2520%25247.0%255C%2525%2524%2520and%2520%252410.2%255C%2525%2524%2520for%250A%2524%255Ctextit%257BBT%2528P%2529%257D%2524%2520and%2520%2524%255Ctextit%257BBT%2528F%2529%257D%2524.%2520Surprisingly%252C%2520we%2520discover%2520that%250Aintervening%2520on%2520the%2520activations%2520of%2520special%2520tokens%2520via%2520the%2520SAEs%2520leads%2520to%250Aimprovements%2520in%2520output%2520quality%252C%2520suggesting%2520new%2520opportunities%2520for%2520fine-grained%250Acontrol%2520of%2520model%2520behavior.%2520Code%252C%2520data%252C%2520and%2520240%2520trained%2520SAEs%2520are%2520available%2520at%250Ahttps%253A//github.com/Geaming2002/FAST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Superior%20Sparse%20Autoencoders%20for%20Instruct%20Models&entry.906535625=Jiaming%20Li%20and%20Haoran%20Ye%20and%20Yukun%20Chen%20and%20Xinyue%20Li%20and%20Lei%20Zhang%20and%20Hamid%20Alinejad-Rokny%20and%20Jimmy%20Chih-Hsien%20Peng%20and%20Min%20Yang&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20grow%20in%20scale%20and%20capability%2C%20understanding%0Atheir%20internal%20mechanisms%20becomes%20increasingly%20critical.%20Sparse%20autoencoders%0A%28SAEs%29%20have%20emerged%20as%20a%20key%20tool%20in%20mechanistic%20interpretability%2C%20enabling%20the%0Aextraction%20of%20human-interpretable%20features%20from%20LLMs.%20However%2C%20existing%20SAE%0Atraining%20methods%20are%20primarily%20designed%20for%20base%20models%2C%20resulting%20in%20reduced%0Areconstruction%20quality%20and%20interpretability%20when%20applied%20to%20instruct%20models.%20To%0Abridge%20this%20gap%2C%20we%20propose%0A%24%5Cunderline%7B%5Ctextbf%7BF%7D%7D%24inetuning-%24%5Cunderline%7B%5Ctextbf%7Ba%7D%7D%24ligned%0A%24%5Cunderline%7B%5Ctextbf%7BS%7D%7D%24equential%20%24%5Cunderline%7B%5Ctextbf%7BT%7D%7D%24raining%0A%28%24%5Ctextit%7BFAST%7D%24%29%2C%20a%20novel%20training%20method%20specifically%20tailored%20for%20instruct%0Amodels.%20%24%5Ctextit%7BFAST%7D%24%20aligns%20the%20training%20process%20with%20the%20data%20distribution%0Aand%20activation%20patterns%20characteristic%20of%20instruct%20models%2C%20resulting%20in%0Asubstantial%20improvements%20in%20both%20reconstruction%20and%20feature%20interpretability.%0AOn%20Qwen2.5-7B-Instruct%2C%20%24%5Ctextit%7BFAST%7D%24%20achieves%20a%20mean%20squared%20error%20of%200.6468%0Ain%20token%20reconstruction%2C%20significantly%20outperforming%20baseline%20methods%20with%0Aerrors%20of%205.1985%20and%201.5096.%20In%20feature%20interpretability%2C%20%24%5Ctextit%7BFAST%7D%24%0Ayields%20a%20higher%20proportion%20of%20high-quality%20features%2C%20for%20Llama3.2-3B-Instruct%2C%0A%2421.1%5C%25%24%20scored%20in%20the%20top%20range%2C%20compared%20to%20%247.0%5C%25%24%20and%20%2410.2%5C%25%24%20for%0A%24%5Ctextit%7BBT%28P%29%7D%24%20and%20%24%5Ctextit%7BBT%28F%29%7D%24.%20Surprisingly%2C%20we%20discover%20that%0Aintervening%20on%20the%20activations%20of%20special%20tokens%20via%20the%20SAEs%20leads%20to%0Aimprovements%20in%20output%20quality%2C%20suggesting%20new%20opportunities%20for%20fine-grained%0Acontrol%20of%20model%20behavior.%20Code%2C%20data%2C%20and%20240%20trained%20SAEs%20are%20available%20at%0Ahttps%3A//github.com/Geaming2002/FAST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07691v1&entry.124074799=Read"},
{"title": "Residual Reweighted Conformal Prediction for Graph Neural Networks", "author": "Zheng Zhang and Jie Bao and Zhixin Zhou and Nicolo Colombo and Lixin Cheng and Rui Luo", "abstract": "  Graph Neural Networks (GNNs) excel at modeling relational data but face\nsignificant challenges in high-stakes domains due to unquantified uncertainty.\nConformal prediction (CP) offers statistical coverage guarantees, but existing\nmethods often produce overly conservative prediction intervals that fail to\naccount for graph heteroscedasticity and structural biases. While residual\nreweighting CP variants address some of these limitations, they neglect graph\ntopology, cluster-specific uncertainties, and risk data leakage by reusing\ntraining sets. To address these issues, we propose Residual Reweighted GNN\n(RR-GNN), a framework designed to generate minimal prediction sets with\nprovable marginal coverage guarantees.\n  RR-GNN introduces three major innovations to enhance prediction performance.\nFirst, it employs Graph-Structured Mondrian CP to partition nodes or edges into\ncommunities based on topological features, ensuring cluster-conditional\ncoverage that reflects heterogeneity. Second, it uses Residual-Adaptive\nNonconformity Scores by training a secondary GNN on a held-out calibration set\nto estimate task-specific residuals, dynamically adjusting prediction intervals\naccording to node or edge uncertainty. Third, it adopts a Cross-Training\nProtocol, which alternates the optimization of the primary GNN and the residual\npredictor to prevent information leakage while maintaining graph dependencies.\nWe validate RR-GNN on 15 real-world graphs across diverse tasks, including node\nclassification, regression, and edge weight prediction. Compared to CP\nbaselines, RR-GNN achieves improved efficiency over state-of-the-art methods,\nwith no loss of coverage.\n", "link": "http://arxiv.org/abs/2506.07854v1", "date": "2025-06-09", "relevancy": 2.4985, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5099}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4972}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Reweighted%20Conformal%20Prediction%20for%20Graph%20Neural%20Networks&body=Title%3A%20Residual%20Reweighted%20Conformal%20Prediction%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Zheng%20Zhang%20and%20Jie%20Bao%20and%20Zhixin%20Zhou%20and%20Nicolo%20Colombo%20and%20Lixin%20Cheng%20and%20Rui%20Luo%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%20modeling%20relational%20data%20but%20face%0Asignificant%20challenges%20in%20high-stakes%20domains%20due%20to%20unquantified%20uncertainty.%0AConformal%20prediction%20%28CP%29%20offers%20statistical%20coverage%20guarantees%2C%20but%20existing%0Amethods%20often%20produce%20overly%20conservative%20prediction%20intervals%20that%20fail%20to%0Aaccount%20for%20graph%20heteroscedasticity%20and%20structural%20biases.%20While%20residual%0Areweighting%20CP%20variants%20address%20some%20of%20these%20limitations%2C%20they%20neglect%20graph%0Atopology%2C%20cluster-specific%20uncertainties%2C%20and%20risk%20data%20leakage%20by%20reusing%0Atraining%20sets.%20To%20address%20these%20issues%2C%20we%20propose%20Residual%20Reweighted%20GNN%0A%28RR-GNN%29%2C%20a%20framework%20designed%20to%20generate%20minimal%20prediction%20sets%20with%0Aprovable%20marginal%20coverage%20guarantees.%0A%20%20RR-GNN%20introduces%20three%20major%20innovations%20to%20enhance%20prediction%20performance.%0AFirst%2C%20it%20employs%20Graph-Structured%20Mondrian%20CP%20to%20partition%20nodes%20or%20edges%20into%0Acommunities%20based%20on%20topological%20features%2C%20ensuring%20cluster-conditional%0Acoverage%20that%20reflects%20heterogeneity.%20Second%2C%20it%20uses%20Residual-Adaptive%0ANonconformity%20Scores%20by%20training%20a%20secondary%20GNN%20on%20a%20held-out%20calibration%20set%0Ato%20estimate%20task-specific%20residuals%2C%20dynamically%20adjusting%20prediction%20intervals%0Aaccording%20to%20node%20or%20edge%20uncertainty.%20Third%2C%20it%20adopts%20a%20Cross-Training%0AProtocol%2C%20which%20alternates%20the%20optimization%20of%20the%20primary%20GNN%20and%20the%20residual%0Apredictor%20to%20prevent%20information%20leakage%20while%20maintaining%20graph%20dependencies.%0AWe%20validate%20RR-GNN%20on%2015%20real-world%20graphs%20across%20diverse%20tasks%2C%20including%20node%0Aclassification%2C%20regression%2C%20and%20edge%20weight%20prediction.%20Compared%20to%20CP%0Abaselines%2C%20RR-GNN%20achieves%20improved%20efficiency%20over%20state-of-the-art%20methods%2C%0Awith%20no%20loss%20of%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Reweighted%2520Conformal%2520Prediction%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DZheng%2520Zhang%2520and%2520Jie%2520Bao%2520and%2520Zhixin%2520Zhou%2520and%2520Nicolo%2520Colombo%2520and%2520Lixin%2520Cheng%2520and%2520Rui%2520Luo%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520at%2520modeling%2520relational%2520data%2520but%2520face%250Asignificant%2520challenges%2520in%2520high-stakes%2520domains%2520due%2520to%2520unquantified%2520uncertainty.%250AConformal%2520prediction%2520%2528CP%2529%2520offers%2520statistical%2520coverage%2520guarantees%252C%2520but%2520existing%250Amethods%2520often%2520produce%2520overly%2520conservative%2520prediction%2520intervals%2520that%2520fail%2520to%250Aaccount%2520for%2520graph%2520heteroscedasticity%2520and%2520structural%2520biases.%2520While%2520residual%250Areweighting%2520CP%2520variants%2520address%2520some%2520of%2520these%2520limitations%252C%2520they%2520neglect%2520graph%250Atopology%252C%2520cluster-specific%2520uncertainties%252C%2520and%2520risk%2520data%2520leakage%2520by%2520reusing%250Atraining%2520sets.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Residual%2520Reweighted%2520GNN%250A%2528RR-GNN%2529%252C%2520a%2520framework%2520designed%2520to%2520generate%2520minimal%2520prediction%2520sets%2520with%250Aprovable%2520marginal%2520coverage%2520guarantees.%250A%2520%2520RR-GNN%2520introduces%2520three%2520major%2520innovations%2520to%2520enhance%2520prediction%2520performance.%250AFirst%252C%2520it%2520employs%2520Graph-Structured%2520Mondrian%2520CP%2520to%2520partition%2520nodes%2520or%2520edges%2520into%250Acommunities%2520based%2520on%2520topological%2520features%252C%2520ensuring%2520cluster-conditional%250Acoverage%2520that%2520reflects%2520heterogeneity.%2520Second%252C%2520it%2520uses%2520Residual-Adaptive%250ANonconformity%2520Scores%2520by%2520training%2520a%2520secondary%2520GNN%2520on%2520a%2520held-out%2520calibration%2520set%250Ato%2520estimate%2520task-specific%2520residuals%252C%2520dynamically%2520adjusting%2520prediction%2520intervals%250Aaccording%2520to%2520node%2520or%2520edge%2520uncertainty.%2520Third%252C%2520it%2520adopts%2520a%2520Cross-Training%250AProtocol%252C%2520which%2520alternates%2520the%2520optimization%2520of%2520the%2520primary%2520GNN%2520and%2520the%2520residual%250Apredictor%2520to%2520prevent%2520information%2520leakage%2520while%2520maintaining%2520graph%2520dependencies.%250AWe%2520validate%2520RR-GNN%2520on%252015%2520real-world%2520graphs%2520across%2520diverse%2520tasks%252C%2520including%2520node%250Aclassification%252C%2520regression%252C%2520and%2520edge%2520weight%2520prediction.%2520Compared%2520to%2520CP%250Abaselines%252C%2520RR-GNN%2520achieves%2520improved%2520efficiency%2520over%2520state-of-the-art%2520methods%252C%250Awith%2520no%2520loss%2520of%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Reweighted%20Conformal%20Prediction%20for%20Graph%20Neural%20Networks&entry.906535625=Zheng%20Zhang%20and%20Jie%20Bao%20and%20Zhixin%20Zhou%20and%20Nicolo%20Colombo%20and%20Lixin%20Cheng%20and%20Rui%20Luo&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%20modeling%20relational%20data%20but%20face%0Asignificant%20challenges%20in%20high-stakes%20domains%20due%20to%20unquantified%20uncertainty.%0AConformal%20prediction%20%28CP%29%20offers%20statistical%20coverage%20guarantees%2C%20but%20existing%0Amethods%20often%20produce%20overly%20conservative%20prediction%20intervals%20that%20fail%20to%0Aaccount%20for%20graph%20heteroscedasticity%20and%20structural%20biases.%20While%20residual%0Areweighting%20CP%20variants%20address%20some%20of%20these%20limitations%2C%20they%20neglect%20graph%0Atopology%2C%20cluster-specific%20uncertainties%2C%20and%20risk%20data%20leakage%20by%20reusing%0Atraining%20sets.%20To%20address%20these%20issues%2C%20we%20propose%20Residual%20Reweighted%20GNN%0A%28RR-GNN%29%2C%20a%20framework%20designed%20to%20generate%20minimal%20prediction%20sets%20with%0Aprovable%20marginal%20coverage%20guarantees.%0A%20%20RR-GNN%20introduces%20three%20major%20innovations%20to%20enhance%20prediction%20performance.%0AFirst%2C%20it%20employs%20Graph-Structured%20Mondrian%20CP%20to%20partition%20nodes%20or%20edges%20into%0Acommunities%20based%20on%20topological%20features%2C%20ensuring%20cluster-conditional%0Acoverage%20that%20reflects%20heterogeneity.%20Second%2C%20it%20uses%20Residual-Adaptive%0ANonconformity%20Scores%20by%20training%20a%20secondary%20GNN%20on%20a%20held-out%20calibration%20set%0Ato%20estimate%20task-specific%20residuals%2C%20dynamically%20adjusting%20prediction%20intervals%0Aaccording%20to%20node%20or%20edge%20uncertainty.%20Third%2C%20it%20adopts%20a%20Cross-Training%0AProtocol%2C%20which%20alternates%20the%20optimization%20of%20the%20primary%20GNN%20and%20the%20residual%0Apredictor%20to%20prevent%20information%20leakage%20while%20maintaining%20graph%20dependencies.%0AWe%20validate%20RR-GNN%20on%2015%20real-world%20graphs%20across%20diverse%20tasks%2C%20including%20node%0Aclassification%2C%20regression%2C%20and%20edge%20weight%20prediction.%20Compared%20to%20CP%0Abaselines%2C%20RR-GNN%20achieves%20improved%20efficiency%20over%20state-of-the-art%20methods%2C%0Awith%20no%20loss%20of%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07854v1&entry.124074799=Read"},
{"title": "How Expressive are Knowledge Graph Foundation Models?", "author": "Xingyue Huang and Pablo Barcel\u00f3 and Michael M. Bronstein and \u0130smail \u0130lkan Ceylan and Mikhail Galkin and Juan L Reutter and Miguel Romero Orth", "abstract": "  Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep\nlearning on knowledge graphs (KGs), as they can generalize to completely novel\nknowledge graphs with different relational vocabularies. Despite their\nempirical success, our theoretical understanding of KGFMs remains very limited.\nIn this paper, we conduct a rigorous study of the expressive power of KGFMs.\nSpecifically, we show that the expressive power of KGFMs directly depends on\nthe motifs that are used to learn the relation representations. We then observe\nthat the most typical motifs used in the existing literature are binary, as the\nrepresentations are learned based on how pairs of relations interact, which\nlimits the model's expressiveness. As part of our study, we design more\nexpressive KGFMs using richer motifs, which necessitate learning relation\nrepresentations based on, e.g., how triples of relations interact with each\nother. Finally, we empirically validate our theoretical findings, showing that\nthe use of richer motifs results in better performance on a wide range of\ndatasets drawn from different domains.\n", "link": "http://arxiv.org/abs/2502.13339v2", "date": "2025-06-09", "relevancy": 2.4895, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Expressive%20are%20Knowledge%20Graph%20Foundation%20Models%3F&body=Title%3A%20How%20Expressive%20are%20Knowledge%20Graph%20Foundation%20Models%3F%0AAuthor%3A%20Xingyue%20Huang%20and%20Pablo%20Barcel%C3%B3%20and%20Michael%20M.%20Bronstein%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Mikhail%20Galkin%20and%20Juan%20L%20Reutter%20and%20Miguel%20Romero%20Orth%0AAbstract%3A%20%20%20Knowledge%20Graph%20Foundation%20Models%20%28KGFMs%29%20are%20at%20the%20frontier%20for%20deep%0Alearning%20on%20knowledge%20graphs%20%28KGs%29%2C%20as%20they%20can%20generalize%20to%20completely%20novel%0Aknowledge%20graphs%20with%20different%20relational%20vocabularies.%20Despite%20their%0Aempirical%20success%2C%20our%20theoretical%20understanding%20of%20KGFMs%20remains%20very%20limited.%0AIn%20this%20paper%2C%20we%20conduct%20a%20rigorous%20study%20of%20the%20expressive%20power%20of%20KGFMs.%0ASpecifically%2C%20we%20show%20that%20the%20expressive%20power%20of%20KGFMs%20directly%20depends%20on%0Athe%20motifs%20that%20are%20used%20to%20learn%20the%20relation%20representations.%20We%20then%20observe%0Athat%20the%20most%20typical%20motifs%20used%20in%20the%20existing%20literature%20are%20binary%2C%20as%20the%0Arepresentations%20are%20learned%20based%20on%20how%20pairs%20of%20relations%20interact%2C%20which%0Alimits%20the%20model%27s%20expressiveness.%20As%20part%20of%20our%20study%2C%20we%20design%20more%0Aexpressive%20KGFMs%20using%20richer%20motifs%2C%20which%20necessitate%20learning%20relation%0Arepresentations%20based%20on%2C%20e.g.%2C%20how%20triples%20of%20relations%20interact%20with%20each%0Aother.%20Finally%2C%20we%20empirically%20validate%20our%20theoretical%20findings%2C%20showing%20that%0Athe%20use%20of%20richer%20motifs%20results%20in%20better%20performance%20on%20a%20wide%20range%20of%0Adatasets%20drawn%20from%20different%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Expressive%2520are%2520Knowledge%2520Graph%2520Foundation%2520Models%253F%26entry.906535625%3DXingyue%2520Huang%2520and%2520Pablo%2520Barcel%25C3%25B3%2520and%2520Michael%2520M.%2520Bronstein%2520and%2520%25C4%25B0smail%2520%25C4%25B0lkan%2520Ceylan%2520and%2520Mikhail%2520Galkin%2520and%2520Juan%2520L%2520Reutter%2520and%2520Miguel%2520Romero%2520Orth%26entry.1292438233%3D%2520%2520Knowledge%2520Graph%2520Foundation%2520Models%2520%2528KGFMs%2529%2520are%2520at%2520the%2520frontier%2520for%2520deep%250Alearning%2520on%2520knowledge%2520graphs%2520%2528KGs%2529%252C%2520as%2520they%2520can%2520generalize%2520to%2520completely%2520novel%250Aknowledge%2520graphs%2520with%2520different%2520relational%2520vocabularies.%2520Despite%2520their%250Aempirical%2520success%252C%2520our%2520theoretical%2520understanding%2520of%2520KGFMs%2520remains%2520very%2520limited.%250AIn%2520this%2520paper%252C%2520we%2520conduct%2520a%2520rigorous%2520study%2520of%2520the%2520expressive%2520power%2520of%2520KGFMs.%250ASpecifically%252C%2520we%2520show%2520that%2520the%2520expressive%2520power%2520of%2520KGFMs%2520directly%2520depends%2520on%250Athe%2520motifs%2520that%2520are%2520used%2520to%2520learn%2520the%2520relation%2520representations.%2520We%2520then%2520observe%250Athat%2520the%2520most%2520typical%2520motifs%2520used%2520in%2520the%2520existing%2520literature%2520are%2520binary%252C%2520as%2520the%250Arepresentations%2520are%2520learned%2520based%2520on%2520how%2520pairs%2520of%2520relations%2520interact%252C%2520which%250Alimits%2520the%2520model%2527s%2520expressiveness.%2520As%2520part%2520of%2520our%2520study%252C%2520we%2520design%2520more%250Aexpressive%2520KGFMs%2520using%2520richer%2520motifs%252C%2520which%2520necessitate%2520learning%2520relation%250Arepresentations%2520based%2520on%252C%2520e.g.%252C%2520how%2520triples%2520of%2520relations%2520interact%2520with%2520each%250Aother.%2520Finally%252C%2520we%2520empirically%2520validate%2520our%2520theoretical%2520findings%252C%2520showing%2520that%250Athe%2520use%2520of%2520richer%2520motifs%2520results%2520in%2520better%2520performance%2520on%2520a%2520wide%2520range%2520of%250Adatasets%2520drawn%2520from%2520different%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Expressive%20are%20Knowledge%20Graph%20Foundation%20Models%3F&entry.906535625=Xingyue%20Huang%20and%20Pablo%20Barcel%C3%B3%20and%20Michael%20M.%20Bronstein%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Mikhail%20Galkin%20and%20Juan%20L%20Reutter%20and%20Miguel%20Romero%20Orth&entry.1292438233=%20%20Knowledge%20Graph%20Foundation%20Models%20%28KGFMs%29%20are%20at%20the%20frontier%20for%20deep%0Alearning%20on%20knowledge%20graphs%20%28KGs%29%2C%20as%20they%20can%20generalize%20to%20completely%20novel%0Aknowledge%20graphs%20with%20different%20relational%20vocabularies.%20Despite%20their%0Aempirical%20success%2C%20our%20theoretical%20understanding%20of%20KGFMs%20remains%20very%20limited.%0AIn%20this%20paper%2C%20we%20conduct%20a%20rigorous%20study%20of%20the%20expressive%20power%20of%20KGFMs.%0ASpecifically%2C%20we%20show%20that%20the%20expressive%20power%20of%20KGFMs%20directly%20depends%20on%0Athe%20motifs%20that%20are%20used%20to%20learn%20the%20relation%20representations.%20We%20then%20observe%0Athat%20the%20most%20typical%20motifs%20used%20in%20the%20existing%20literature%20are%20binary%2C%20as%20the%0Arepresentations%20are%20learned%20based%20on%20how%20pairs%20of%20relations%20interact%2C%20which%0Alimits%20the%20model%27s%20expressiveness.%20As%20part%20of%20our%20study%2C%20we%20design%20more%0Aexpressive%20KGFMs%20using%20richer%20motifs%2C%20which%20necessitate%20learning%20relation%0Arepresentations%20based%20on%2C%20e.g.%2C%20how%20triples%20of%20relations%20interact%20with%20each%0Aother.%20Finally%2C%20we%20empirically%20validate%20our%20theoretical%20findings%2C%20showing%20that%0Athe%20use%20of%20richer%20motifs%20results%20in%20better%20performance%20on%20a%20wide%20range%20of%0Adatasets%20drawn%20from%20different%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13339v2&entry.124074799=Read"},
{"title": "Image Reconstruction as a Tool for Feature Analysis", "author": "Eduard Allakhverdov and Dmitrii Tarasov and Elizaveta Goncharova and Andrey Kuznetsov", "abstract": "  Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.\n", "link": "http://arxiv.org/abs/2506.07803v1", "date": "2025-06-09", "relevancy": 2.4648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6331}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6331}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Reconstruction%20as%20a%20Tool%20for%20Feature%20Analysis&body=Title%3A%20Image%20Reconstruction%20as%20a%20Tool%20for%20Feature%20Analysis%0AAuthor%3A%20Eduard%20Allakhverdov%20and%20Dmitrii%20Tarasov%20and%20Elizaveta%20Goncharova%20and%20Andrey%20Kuznetsov%0AAbstract%3A%20%20%20Vision%20encoders%20are%20increasingly%20used%20in%20modern%20applications%2C%20from%0Avision-only%20models%20to%20multimodal%20systems%20such%20as%20vision-language%20models.%0ADespite%20their%20remarkable%20success%2C%20it%20remains%20unclear%20how%20these%20architectures%0Arepresent%20features%20internally.%20Here%2C%20we%20propose%20a%20novel%20approach%20for%0Ainterpreting%20vision%20features%20via%20image%20reconstruction.%20We%20compare%20two%20related%0Amodel%20families%2C%20SigLIP%20and%20SigLIP2%2C%20which%20differ%20only%20in%20their%20training%0Aobjective%2C%20and%20show%20that%20encoders%20pre-trained%20on%20image-based%20tasks%20retain%0Asignificantly%20more%20image%20information%20than%20those%20trained%20on%20non-image%20tasks%20such%0Aas%20contrastive%20learning.%20We%20further%20apply%20our%20method%20to%20a%20range%20of%20vision%0Aencoders%2C%20ranking%20them%20by%20the%20informativeness%20of%20their%20feature%20representations.%0AFinally%2C%20we%20demonstrate%20that%20manipulating%20the%20feature%20space%20yields%20predictable%0Achanges%20in%20reconstructed%20images%2C%20revealing%20that%20orthogonal%20rotations%20%28rather%0Athan%20spatial%20transformations%29%20control%20color%20encoding.%20Our%20approach%20can%20be%0Aapplied%20to%20any%20vision%20encoder%2C%20shedding%20light%20on%20the%20inner%20structure%20of%20its%0Afeature%20space.%20The%20code%20and%20model%20weights%20to%20reproduce%20the%20experiments%20are%0Aavailable%20in%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Reconstruction%2520as%2520a%2520Tool%2520for%2520Feature%2520Analysis%26entry.906535625%3DEduard%2520Allakhverdov%2520and%2520Dmitrii%2520Tarasov%2520and%2520Elizaveta%2520Goncharova%2520and%2520Andrey%2520Kuznetsov%26entry.1292438233%3D%2520%2520Vision%2520encoders%2520are%2520increasingly%2520used%2520in%2520modern%2520applications%252C%2520from%250Avision-only%2520models%2520to%2520multimodal%2520systems%2520such%2520as%2520vision-language%2520models.%250ADespite%2520their%2520remarkable%2520success%252C%2520it%2520remains%2520unclear%2520how%2520these%2520architectures%250Arepresent%2520features%2520internally.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%250Ainterpreting%2520vision%2520features%2520via%2520image%2520reconstruction.%2520We%2520compare%2520two%2520related%250Amodel%2520families%252C%2520SigLIP%2520and%2520SigLIP2%252C%2520which%2520differ%2520only%2520in%2520their%2520training%250Aobjective%252C%2520and%2520show%2520that%2520encoders%2520pre-trained%2520on%2520image-based%2520tasks%2520retain%250Asignificantly%2520more%2520image%2520information%2520than%2520those%2520trained%2520on%2520non-image%2520tasks%2520such%250Aas%2520contrastive%2520learning.%2520We%2520further%2520apply%2520our%2520method%2520to%2520a%2520range%2520of%2520vision%250Aencoders%252C%2520ranking%2520them%2520by%2520the%2520informativeness%2520of%2520their%2520feature%2520representations.%250AFinally%252C%2520we%2520demonstrate%2520that%2520manipulating%2520the%2520feature%2520space%2520yields%2520predictable%250Achanges%2520in%2520reconstructed%2520images%252C%2520revealing%2520that%2520orthogonal%2520rotations%2520%2528rather%250Athan%2520spatial%2520transformations%2529%2520control%2520color%2520encoding.%2520Our%2520approach%2520can%2520be%250Aapplied%2520to%2520any%2520vision%2520encoder%252C%2520shedding%2520light%2520on%2520the%2520inner%2520structure%2520of%2520its%250Afeature%2520space.%2520The%2520code%2520and%2520model%2520weights%2520to%2520reproduce%2520the%2520experiments%2520are%250Aavailable%2520in%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Reconstruction%20as%20a%20Tool%20for%20Feature%20Analysis&entry.906535625=Eduard%20Allakhverdov%20and%20Dmitrii%20Tarasov%20and%20Elizaveta%20Goncharova%20and%20Andrey%20Kuznetsov&entry.1292438233=%20%20Vision%20encoders%20are%20increasingly%20used%20in%20modern%20applications%2C%20from%0Avision-only%20models%20to%20multimodal%20systems%20such%20as%20vision-language%20models.%0ADespite%20their%20remarkable%20success%2C%20it%20remains%20unclear%20how%20these%20architectures%0Arepresent%20features%20internally.%20Here%2C%20we%20propose%20a%20novel%20approach%20for%0Ainterpreting%20vision%20features%20via%20image%20reconstruction.%20We%20compare%20two%20related%0Amodel%20families%2C%20SigLIP%20and%20SigLIP2%2C%20which%20differ%20only%20in%20their%20training%0Aobjective%2C%20and%20show%20that%20encoders%20pre-trained%20on%20image-based%20tasks%20retain%0Asignificantly%20more%20image%20information%20than%20those%20trained%20on%20non-image%20tasks%20such%0Aas%20contrastive%20learning.%20We%20further%20apply%20our%20method%20to%20a%20range%20of%20vision%0Aencoders%2C%20ranking%20them%20by%20the%20informativeness%20of%20their%20feature%20representations.%0AFinally%2C%20we%20demonstrate%20that%20manipulating%20the%20feature%20space%20yields%20predictable%0Achanges%20in%20reconstructed%20images%2C%20revealing%20that%20orthogonal%20rotations%20%28rather%0Athan%20spatial%20transformations%29%20control%20color%20encoding.%20Our%20approach%20can%20be%0Aapplied%20to%20any%20vision%20encoder%2C%20shedding%20light%20on%20the%20inner%20structure%20of%20its%0Afeature%20space.%20The%20code%20and%20model%20weights%20to%20reproduce%20the%20experiments%20are%0Aavailable%20in%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07803v1&entry.124074799=Read"},
{"title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation", "author": "Ge Wang and Songlin Fan and Hangxu Liu and Quanjian Song and Hewei Wang and Jinfeng Xu", "abstract": "  With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.\n", "link": "http://arxiv.org/abs/2506.07713v1", "date": "2025-06-09", "relevancy": 2.4591, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6401}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.617}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Video%20Editing%20as%20Flow-Driven%20Image-to-Video%20Generation&body=Title%3A%20Consistent%20Video%20Editing%20as%20Flow-Driven%20Image-to-Video%20Generation%0AAuthor%3A%20Ge%20Wang%20and%20Songlin%20Fan%20and%20Hangxu%20Liu%20and%20Quanjian%20Song%20and%20Hewei%20Wang%20and%20Jinfeng%20Xu%0AAbstract%3A%20%20%20With%20the%20prosper%20of%20video%20diffusion%20models%2C%20down-stream%20applications%20like%0Avideo%20editing%20have%20been%20significantly%20promoted%20without%20consuming%20much%0Acomputational%20cost.%20One%20particular%20challenge%20in%20this%20task%20lies%20at%20the%20motion%0Atransfer%20process%20from%20the%20source%20video%20to%20the%20edited%20one%2C%20where%20it%20requires%20the%0Aconsideration%20of%20the%20shape%20deformation%20in%20between%2C%20meanwhile%20maintaining%20the%0Atemporal%20consistency%20in%20the%20generated%20video%20sequence.%20However%2C%20existing%20methods%0Afail%20to%20model%20complicated%20motion%20patterns%20for%20video%20editing%2C%20and%20are%0Afundamentally%20limited%20to%20object%20replacement%2C%20where%20tasks%20with%20non-rigid%20object%0Amotions%20like%20multi-object%20and%20portrait%20editing%20are%20largely%20neglected.%20In%20this%0Apaper%2C%20we%20observe%20that%20optical%20flows%20offer%20a%20promising%20alternative%20in%20complex%0Amotion%20modeling%2C%20and%20present%20FlowV2V%20to%20re-investigate%20video%20editing%20as%20a%20task%0Aof%20flow-driven%20Image-to-Video%20%28I2V%29%20generation.%20Specifically%2C%20FlowV2V%0Adecomposes%20the%20entire%20pipeline%20into%20first-frame%20editing%20and%20conditional%20I2V%0Ageneration%2C%20and%20simulates%20pseudo%20flow%20sequence%20that%20aligns%20with%20the%20deformed%0Ashape%2C%20thus%20ensuring%20the%20consistency%20during%20editing.%20Experimental%20results%20on%0ADAVIS-EDIT%20with%20improvements%20of%2013.67%25%20and%2050.66%25%20on%20DOVER%20and%20warping%20error%0Aillustrate%20the%20superior%20temporal%20consistency%20and%20sample%20quality%20of%20FlowV2V%0Acompared%20to%20existing%20state-of-the-art%20ones.%20Furthermore%2C%20we%20conduct%0Acomprehensive%20ablation%20studies%20to%20analyze%20the%20internal%20functionalities%20of%20the%0Afirst-frame%20paradigm%20and%20flow%20alignment%20in%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Video%2520Editing%2520as%2520Flow-Driven%2520Image-to-Video%2520Generation%26entry.906535625%3DGe%2520Wang%2520and%2520Songlin%2520Fan%2520and%2520Hangxu%2520Liu%2520and%2520Quanjian%2520Song%2520and%2520Hewei%2520Wang%2520and%2520Jinfeng%2520Xu%26entry.1292438233%3D%2520%2520With%2520the%2520prosper%2520of%2520video%2520diffusion%2520models%252C%2520down-stream%2520applications%2520like%250Avideo%2520editing%2520have%2520been%2520significantly%2520promoted%2520without%2520consuming%2520much%250Acomputational%2520cost.%2520One%2520particular%2520challenge%2520in%2520this%2520task%2520lies%2520at%2520the%2520motion%250Atransfer%2520process%2520from%2520the%2520source%2520video%2520to%2520the%2520edited%2520one%252C%2520where%2520it%2520requires%2520the%250Aconsideration%2520of%2520the%2520shape%2520deformation%2520in%2520between%252C%2520meanwhile%2520maintaining%2520the%250Atemporal%2520consistency%2520in%2520the%2520generated%2520video%2520sequence.%2520However%252C%2520existing%2520methods%250Afail%2520to%2520model%2520complicated%2520motion%2520patterns%2520for%2520video%2520editing%252C%2520and%2520are%250Afundamentally%2520limited%2520to%2520object%2520replacement%252C%2520where%2520tasks%2520with%2520non-rigid%2520object%250Amotions%2520like%2520multi-object%2520and%2520portrait%2520editing%2520are%2520largely%2520neglected.%2520In%2520this%250Apaper%252C%2520we%2520observe%2520that%2520optical%2520flows%2520offer%2520a%2520promising%2520alternative%2520in%2520complex%250Amotion%2520modeling%252C%2520and%2520present%2520FlowV2V%2520to%2520re-investigate%2520video%2520editing%2520as%2520a%2520task%250Aof%2520flow-driven%2520Image-to-Video%2520%2528I2V%2529%2520generation.%2520Specifically%252C%2520FlowV2V%250Adecomposes%2520the%2520entire%2520pipeline%2520into%2520first-frame%2520editing%2520and%2520conditional%2520I2V%250Ageneration%252C%2520and%2520simulates%2520pseudo%2520flow%2520sequence%2520that%2520aligns%2520with%2520the%2520deformed%250Ashape%252C%2520thus%2520ensuring%2520the%2520consistency%2520during%2520editing.%2520Experimental%2520results%2520on%250ADAVIS-EDIT%2520with%2520improvements%2520of%252013.67%2525%2520and%252050.66%2525%2520on%2520DOVER%2520and%2520warping%2520error%250Aillustrate%2520the%2520superior%2520temporal%2520consistency%2520and%2520sample%2520quality%2520of%2520FlowV2V%250Acompared%2520to%2520existing%2520state-of-the-art%2520ones.%2520Furthermore%252C%2520we%2520conduct%250Acomprehensive%2520ablation%2520studies%2520to%2520analyze%2520the%2520internal%2520functionalities%2520of%2520the%250Afirst-frame%2520paradigm%2520and%2520flow%2520alignment%2520in%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Video%20Editing%20as%20Flow-Driven%20Image-to-Video%20Generation&entry.906535625=Ge%20Wang%20and%20Songlin%20Fan%20and%20Hangxu%20Liu%20and%20Quanjian%20Song%20and%20Hewei%20Wang%20and%20Jinfeng%20Xu&entry.1292438233=%20%20With%20the%20prosper%20of%20video%20diffusion%20models%2C%20down-stream%20applications%20like%0Avideo%20editing%20have%20been%20significantly%20promoted%20without%20consuming%20much%0Acomputational%20cost.%20One%20particular%20challenge%20in%20this%20task%20lies%20at%20the%20motion%0Atransfer%20process%20from%20the%20source%20video%20to%20the%20edited%20one%2C%20where%20it%20requires%20the%0Aconsideration%20of%20the%20shape%20deformation%20in%20between%2C%20meanwhile%20maintaining%20the%0Atemporal%20consistency%20in%20the%20generated%20video%20sequence.%20However%2C%20existing%20methods%0Afail%20to%20model%20complicated%20motion%20patterns%20for%20video%20editing%2C%20and%20are%0Afundamentally%20limited%20to%20object%20replacement%2C%20where%20tasks%20with%20non-rigid%20object%0Amotions%20like%20multi-object%20and%20portrait%20editing%20are%20largely%20neglected.%20In%20this%0Apaper%2C%20we%20observe%20that%20optical%20flows%20offer%20a%20promising%20alternative%20in%20complex%0Amotion%20modeling%2C%20and%20present%20FlowV2V%20to%20re-investigate%20video%20editing%20as%20a%20task%0Aof%20flow-driven%20Image-to-Video%20%28I2V%29%20generation.%20Specifically%2C%20FlowV2V%0Adecomposes%20the%20entire%20pipeline%20into%20first-frame%20editing%20and%20conditional%20I2V%0Ageneration%2C%20and%20simulates%20pseudo%20flow%20sequence%20that%20aligns%20with%20the%20deformed%0Ashape%2C%20thus%20ensuring%20the%20consistency%20during%20editing.%20Experimental%20results%20on%0ADAVIS-EDIT%20with%20improvements%20of%2013.67%25%20and%2050.66%25%20on%20DOVER%20and%20warping%20error%0Aillustrate%20the%20superior%20temporal%20consistency%20and%20sample%20quality%20of%20FlowV2V%0Acompared%20to%20existing%20state-of-the-art%20ones.%20Furthermore%2C%20we%20conduct%0Acomprehensive%20ablation%20studies%20to%20analyze%20the%20internal%20functionalities%20of%20the%0Afirst-frame%20paradigm%20and%20flow%20alignment%20in%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07713v1&entry.124074799=Read"},
{"title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape\n  Reasoning in Large Language Models", "author": "Cheonbok Park and Jeonghoon Kim and Joosung Lee and Sanghwan Bae and Jaegul Choo and Kang Min Yoo", "abstract": "  We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which the\nchain-of-thought (CoT) of a multilingual language model reverts to its dominant\npre-training language even when the prompt is expressed in a different\nlanguage. Recent large language models (LLMs) with reinforcement learning with\nverifiable reward (RLVR) have achieved strong logical reasoning performances by\nexposing their intermediate reasoning traces, giving rise to large reasoning\nmodels (LRMs). However, the mechanism behind multilingual reasoning in LRMs is\nnot yet fully explored. To investigate the issue, we fine-tune multilingual\nLRMs with Group-Relative Policy Optimization (GRPO) on translated versions of\nthe GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,\nKorean, and Ukrainian. During training, we monitor both task accuracy and\nlanguage consistency of the reasoning chains. Our experiments reveal three key\nfindings: (i) GRPO rapidly amplifies pre-training language imbalances, leading\nto the erosion of low-resource languages within just a few hundred updates;\n(ii) language consistency reward mitigates this drift but does so at the\nexpense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting\nlanguage collapse is severely damaging and largely irreversible, as subsequent\nfine-tuning struggles to steer the model back toward its original\ntarget-language reasoning capabilities. Together, these findings point to a\nremarkable conclusion: \\textit{not all languages are trained equally for\nreasoning}. Furthermore, our paper sheds light on the roles of reward shaping,\ndata difficulty, and pre-training priors in eliciting multilingual reasoning.\n", "link": "http://arxiv.org/abs/2506.05850v2", "date": "2025-06-09", "relevancy": 2.4577, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-lingual%20Collapse%3A%20How%20Language-Centric%20Foundation%20Models%20Shape%0A%20%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Cross-lingual%20Collapse%3A%20How%20Language-Centric%20Foundation%20Models%20Shape%0A%20%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Cheonbok%20Park%20and%20Jeonghoon%20Kim%20and%20Joosung%20Lee%20and%20Sanghwan%20Bae%20and%20Jaegul%20Choo%20and%20Kang%20Min%20Yoo%0AAbstract%3A%20%20%20We%20identify%20%5Ctextbf%7BCross-lingual%20Collapse%7D%2C%20a%20systematic%20drift%20in%20which%20the%0Achain-of-thought%20%28CoT%29%20of%20a%20multilingual%20language%20model%20reverts%20to%20its%20dominant%0Apre-training%20language%20even%20when%20the%20prompt%20is%20expressed%20in%20a%20different%0Alanguage.%20Recent%20large%20language%20models%20%28LLMs%29%20with%20reinforcement%20learning%20with%0Averifiable%20reward%20%28RLVR%29%20have%20achieved%20strong%20logical%20reasoning%20performances%20by%0Aexposing%20their%20intermediate%20reasoning%20traces%2C%20giving%20rise%20to%20large%20reasoning%0Amodels%20%28LRMs%29.%20However%2C%20the%20mechanism%20behind%20multilingual%20reasoning%20in%20LRMs%20is%0Anot%20yet%20fully%20explored.%20To%20investigate%20the%20issue%2C%20we%20fine-tune%20multilingual%0ALRMs%20with%20Group-Relative%20Policy%20Optimization%20%28GRPO%29%20on%20translated%20versions%20of%0Athe%20GSM%248%24K%20and%20SimpleRL-Zoo%20datasets%20in%20three%20different%20languages%3A%20Chinese%2C%0AKorean%2C%20and%20Ukrainian.%20During%20training%2C%20we%20monitor%20both%20task%20accuracy%20and%0Alanguage%20consistency%20of%20the%20reasoning%20chains.%20Our%20experiments%20reveal%20three%20key%0Afindings%3A%20%28i%29%20GRPO%20rapidly%20amplifies%20pre-training%20language%20imbalances%2C%20leading%0Ato%20the%20erosion%20of%20low-resource%20languages%20within%20just%20a%20few%20hundred%20updates%3B%0A%28ii%29%20language%20consistency%20reward%20mitigates%20this%20drift%20but%20does%20so%20at%20the%0Aexpense%20of%20an%20almost%205%20-%2010%20pp%20drop%20in%20accuracy.%20and%20%28iii%29%20the%20resulting%0Alanguage%20collapse%20is%20severely%20damaging%20and%20largely%20irreversible%2C%20as%20subsequent%0Afine-tuning%20struggles%20to%20steer%20the%20model%20back%20toward%20its%20original%0Atarget-language%20reasoning%20capabilities.%20Together%2C%20these%20findings%20point%20to%20a%0Aremarkable%20conclusion%3A%20%5Ctextit%7Bnot%20all%20languages%20are%20trained%20equally%20for%0Areasoning%7D.%20Furthermore%2C%20our%20paper%20sheds%20light%20on%20the%20roles%20of%20reward%20shaping%2C%0Adata%20difficulty%2C%20and%20pre-training%20priors%20in%20eliciting%20multilingual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-lingual%2520Collapse%253A%2520How%2520Language-Centric%2520Foundation%2520Models%2520Shape%250A%2520%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DCheonbok%2520Park%2520and%2520Jeonghoon%2520Kim%2520and%2520Joosung%2520Lee%2520and%2520Sanghwan%2520Bae%2520and%2520Jaegul%2520Choo%2520and%2520Kang%2520Min%2520Yoo%26entry.1292438233%3D%2520%2520We%2520identify%2520%255Ctextbf%257BCross-lingual%2520Collapse%257D%252C%2520a%2520systematic%2520drift%2520in%2520which%2520the%250Achain-of-thought%2520%2528CoT%2529%2520of%2520a%2520multilingual%2520language%2520model%2520reverts%2520to%2520its%2520dominant%250Apre-training%2520language%2520even%2520when%2520the%2520prompt%2520is%2520expressed%2520in%2520a%2520different%250Alanguage.%2520Recent%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520reinforcement%2520learning%2520with%250Averifiable%2520reward%2520%2528RLVR%2529%2520have%2520achieved%2520strong%2520logical%2520reasoning%2520performances%2520by%250Aexposing%2520their%2520intermediate%2520reasoning%2520traces%252C%2520giving%2520rise%2520to%2520large%2520reasoning%250Amodels%2520%2528LRMs%2529.%2520However%252C%2520the%2520mechanism%2520behind%2520multilingual%2520reasoning%2520in%2520LRMs%2520is%250Anot%2520yet%2520fully%2520explored.%2520To%2520investigate%2520the%2520issue%252C%2520we%2520fine-tune%2520multilingual%250ALRMs%2520with%2520Group-Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520on%2520translated%2520versions%2520of%250Athe%2520GSM%25248%2524K%2520and%2520SimpleRL-Zoo%2520datasets%2520in%2520three%2520different%2520languages%253A%2520Chinese%252C%250AKorean%252C%2520and%2520Ukrainian.%2520During%2520training%252C%2520we%2520monitor%2520both%2520task%2520accuracy%2520and%250Alanguage%2520consistency%2520of%2520the%2520reasoning%2520chains.%2520Our%2520experiments%2520reveal%2520three%2520key%250Afindings%253A%2520%2528i%2529%2520GRPO%2520rapidly%2520amplifies%2520pre-training%2520language%2520imbalances%252C%2520leading%250Ato%2520the%2520erosion%2520of%2520low-resource%2520languages%2520within%2520just%2520a%2520few%2520hundred%2520updates%253B%250A%2528ii%2529%2520language%2520consistency%2520reward%2520mitigates%2520this%2520drift%2520but%2520does%2520so%2520at%2520the%250Aexpense%2520of%2520an%2520almost%25205%2520-%252010%2520pp%2520drop%2520in%2520accuracy.%2520and%2520%2528iii%2529%2520the%2520resulting%250Alanguage%2520collapse%2520is%2520severely%2520damaging%2520and%2520largely%2520irreversible%252C%2520as%2520subsequent%250Afine-tuning%2520struggles%2520to%2520steer%2520the%2520model%2520back%2520toward%2520its%2520original%250Atarget-language%2520reasoning%2520capabilities.%2520Together%252C%2520these%2520findings%2520point%2520to%2520a%250Aremarkable%2520conclusion%253A%2520%255Ctextit%257Bnot%2520all%2520languages%2520are%2520trained%2520equally%2520for%250Areasoning%257D.%2520Furthermore%252C%2520our%2520paper%2520sheds%2520light%2520on%2520the%2520roles%2520of%2520reward%2520shaping%252C%250Adata%2520difficulty%252C%2520and%2520pre-training%2520priors%2520in%2520eliciting%2520multilingual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-lingual%20Collapse%3A%20How%20Language-Centric%20Foundation%20Models%20Shape%0A%20%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Cheonbok%20Park%20and%20Jeonghoon%20Kim%20and%20Joosung%20Lee%20and%20Sanghwan%20Bae%20and%20Jaegul%20Choo%20and%20Kang%20Min%20Yoo&entry.1292438233=%20%20We%20identify%20%5Ctextbf%7BCross-lingual%20Collapse%7D%2C%20a%20systematic%20drift%20in%20which%20the%0Achain-of-thought%20%28CoT%29%20of%20a%20multilingual%20language%20model%20reverts%20to%20its%20dominant%0Apre-training%20language%20even%20when%20the%20prompt%20is%20expressed%20in%20a%20different%0Alanguage.%20Recent%20large%20language%20models%20%28LLMs%29%20with%20reinforcement%20learning%20with%0Averifiable%20reward%20%28RLVR%29%20have%20achieved%20strong%20logical%20reasoning%20performances%20by%0Aexposing%20their%20intermediate%20reasoning%20traces%2C%20giving%20rise%20to%20large%20reasoning%0Amodels%20%28LRMs%29.%20However%2C%20the%20mechanism%20behind%20multilingual%20reasoning%20in%20LRMs%20is%0Anot%20yet%20fully%20explored.%20To%20investigate%20the%20issue%2C%20we%20fine-tune%20multilingual%0ALRMs%20with%20Group-Relative%20Policy%20Optimization%20%28GRPO%29%20on%20translated%20versions%20of%0Athe%20GSM%248%24K%20and%20SimpleRL-Zoo%20datasets%20in%20three%20different%20languages%3A%20Chinese%2C%0AKorean%2C%20and%20Ukrainian.%20During%20training%2C%20we%20monitor%20both%20task%20accuracy%20and%0Alanguage%20consistency%20of%20the%20reasoning%20chains.%20Our%20experiments%20reveal%20three%20key%0Afindings%3A%20%28i%29%20GRPO%20rapidly%20amplifies%20pre-training%20language%20imbalances%2C%20leading%0Ato%20the%20erosion%20of%20low-resource%20languages%20within%20just%20a%20few%20hundred%20updates%3B%0A%28ii%29%20language%20consistency%20reward%20mitigates%20this%20drift%20but%20does%20so%20at%20the%0Aexpense%20of%20an%20almost%205%20-%2010%20pp%20drop%20in%20accuracy.%20and%20%28iii%29%20the%20resulting%0Alanguage%20collapse%20is%20severely%20damaging%20and%20largely%20irreversible%2C%20as%20subsequent%0Afine-tuning%20struggles%20to%20steer%20the%20model%20back%20toward%20its%20original%0Atarget-language%20reasoning%20capabilities.%20Together%2C%20these%20findings%20point%20to%20a%0Aremarkable%20conclusion%3A%20%5Ctextit%7Bnot%20all%20languages%20are%20trained%20equally%20for%0Areasoning%7D.%20Furthermore%2C%20our%20paper%20sheds%20light%20on%20the%20roles%20of%20reward%20shaping%2C%0Adata%20difficulty%2C%20and%20pre-training%20priors%20in%20eliciting%20multilingual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05850v2&entry.124074799=Read"},
{"title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories\n  in LLMs", "author": "Yao Yan", "abstract": "  Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.\n", "link": "http://arxiv.org/abs/2506.07824v1", "date": "2025-06-09", "relevancy": 2.4475, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addition%20in%20Four%20Movements%3A%20Mapping%20Layer-wise%20Information%20Trajectories%0A%20%20in%20LLMs&body=Title%3A%20Addition%20in%20Four%20Movements%3A%20Mapping%20Layer-wise%20Information%20Trajectories%0A%20%20in%20LLMs%0AAuthor%3A%20Yao%20Yan%0AAbstract%3A%20%20%20Multi-digit%20addition%20is%20a%20clear%20probe%20of%20the%20computational%20power%20of%20large%0Alanguage%20models.%20To%20dissect%20the%20internal%20arithmetic%20processes%20in%0ALLaMA-3-8B-Instruct%2C%20we%20combine%20linear%20probing%20with%20logit-lens%20inspection.%0AInspired%20by%20the%20step-by-step%20manner%20in%20which%20humans%20perform%20addition%2C%20we%0Apropose%20and%20analyze%20a%20coherent%20four-stage%20trajectory%20in%20the%20forward%0Apass%3AFormula-structure%20representations%20become%20linearly%20decodable%20first%2C%20while%0Athe%20answer%20token%20is%20still%20far%20down%20the%20candidate%20list.Core%20computational%0Afeatures%20then%20emerge%20prominently.At%20deeper%20activation%20layers%2C%20numerical%0Aabstractions%20of%20the%20result%20become%20clearer%2C%20enabling%20near-perfect%20detection%20and%0Adecoding%20of%20the%20individual%20digits%20in%20the%20sum.Near%20the%20output%2C%20the%20model%0Aorganizes%20and%20generates%20the%20final%20content%2C%20with%20the%20correct%20token%20reliably%0Aoccupying%20the%20top%20rank.This%20trajectory%20suggests%20a%20hierarchical%20process%20that%0Afavors%20internal%20computation%20over%20rote%20memorization.%20We%20release%20our%20code%20and%0Adata%20to%20facilitate%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddition%2520in%2520Four%2520Movements%253A%2520Mapping%2520Layer-wise%2520Information%2520Trajectories%250A%2520%2520in%2520LLMs%26entry.906535625%3DYao%2520Yan%26entry.1292438233%3D%2520%2520Multi-digit%2520addition%2520is%2520a%2520clear%2520probe%2520of%2520the%2520computational%2520power%2520of%2520large%250Alanguage%2520models.%2520To%2520dissect%2520the%2520internal%2520arithmetic%2520processes%2520in%250ALLaMA-3-8B-Instruct%252C%2520we%2520combine%2520linear%2520probing%2520with%2520logit-lens%2520inspection.%250AInspired%2520by%2520the%2520step-by-step%2520manner%2520in%2520which%2520humans%2520perform%2520addition%252C%2520we%250Apropose%2520and%2520analyze%2520a%2520coherent%2520four-stage%2520trajectory%2520in%2520the%2520forward%250Apass%253AFormula-structure%2520representations%2520become%2520linearly%2520decodable%2520first%252C%2520while%250Athe%2520answer%2520token%2520is%2520still%2520far%2520down%2520the%2520candidate%2520list.Core%2520computational%250Afeatures%2520then%2520emerge%2520prominently.At%2520deeper%2520activation%2520layers%252C%2520numerical%250Aabstractions%2520of%2520the%2520result%2520become%2520clearer%252C%2520enabling%2520near-perfect%2520detection%2520and%250Adecoding%2520of%2520the%2520individual%2520digits%2520in%2520the%2520sum.Near%2520the%2520output%252C%2520the%2520model%250Aorganizes%2520and%2520generates%2520the%2520final%2520content%252C%2520with%2520the%2520correct%2520token%2520reliably%250Aoccupying%2520the%2520top%2520rank.This%2520trajectory%2520suggests%2520a%2520hierarchical%2520process%2520that%250Afavors%2520internal%2520computation%2520over%2520rote%2520memorization.%2520We%2520release%2520our%2520code%2520and%250Adata%2520to%2520facilitate%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addition%20in%20Four%20Movements%3A%20Mapping%20Layer-wise%20Information%20Trajectories%0A%20%20in%20LLMs&entry.906535625=Yao%20Yan&entry.1292438233=%20%20Multi-digit%20addition%20is%20a%20clear%20probe%20of%20the%20computational%20power%20of%20large%0Alanguage%20models.%20To%20dissect%20the%20internal%20arithmetic%20processes%20in%0ALLaMA-3-8B-Instruct%2C%20we%20combine%20linear%20probing%20with%20logit-lens%20inspection.%0AInspired%20by%20the%20step-by-step%20manner%20in%20which%20humans%20perform%20addition%2C%20we%0Apropose%20and%20analyze%20a%20coherent%20four-stage%20trajectory%20in%20the%20forward%0Apass%3AFormula-structure%20representations%20become%20linearly%20decodable%20first%2C%20while%0Athe%20answer%20token%20is%20still%20far%20down%20the%20candidate%20list.Core%20computational%0Afeatures%20then%20emerge%20prominently.At%20deeper%20activation%20layers%2C%20numerical%0Aabstractions%20of%20the%20result%20become%20clearer%2C%20enabling%20near-perfect%20detection%20and%0Adecoding%20of%20the%20individual%20digits%20in%20the%20sum.Near%20the%20output%2C%20the%20model%0Aorganizes%20and%20generates%20the%20final%20content%2C%20with%20the%20correct%20token%20reliably%0Aoccupying%20the%20top%20rank.This%20trajectory%20suggests%20a%20hierarchical%20process%20that%0Afavors%20internal%20computation%20over%20rote%20memorization.%20We%20release%20our%20code%20and%0Adata%20to%20facilitate%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07824v1&entry.124074799=Read"},
{"title": "Synthetic Visual Genome", "author": "Jae Sung Park and Zixian Ma and Linjie Li and Chenhao Zheng and Cheng-Yu Hsieh and Ximing Lu and Khyathi Chandu and Quan Kong and Norimasa Kobori and Ali Farhadi and Yejin Choi and Ranjay Krishna", "abstract": "  Reasoning over visual relationships-spatial, functional, interactional,\nsocial, etc.-is considered to be a fundamental component of human cognition.\nYet, despite the major advances in visual comprehension in multimodal language\nmodels (MLMs), precise reasoning over relationships and their generations\nremains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely\nannotated relationships capable of constructing high-quality dense scene graphs\nat scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by\ncompleting the missing relations of selected objects in existing scene graphs\nusing a teacher MLM and a carefully designed filtering process to ensure\nhigh-quality. To generate more accurate and rich scene graphs at scale for any\nimage, we introduce SG-EDIT: a self-distillation framework where GPT-4o further\nrefines ROBIN's predicted scene graphs by removing unlikely relations and/or\nsuggesting relevant ones. In total, our dataset contains 146K images and 5.6M\nrelationships for 2.6M objects. Results show that our ROBIN-3B model, despite\nbeing trained on less than 3 million instances, outperforms similar-size models\ntrained on over 300 million instances on relationship understanding benchmarks,\nand even surpasses larger models up to 13B parameters. Notably, it achieves\nstate-of-the-art performance in referring expression comprehension with a score\nof 88.9, surpassing the previous best of 87.4. Our results suggest that\ntraining on the refined scene graph data is crucial to maintaining high\nperformance across diverse visual reasoning task.\n", "link": "http://arxiv.org/abs/2506.07643v1", "date": "2025-06-09", "relevancy": 2.4405, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Visual%20Genome&body=Title%3A%20Synthetic%20Visual%20Genome%0AAuthor%3A%20Jae%20Sung%20Park%20and%20Zixian%20Ma%20and%20Linjie%20Li%20and%20Chenhao%20Zheng%20and%20Cheng-Yu%20Hsieh%20and%20Ximing%20Lu%20and%20Khyathi%20Chandu%20and%20Quan%20Kong%20and%20Norimasa%20Kobori%20and%20Ali%20Farhadi%20and%20Yejin%20Choi%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Reasoning%20over%20visual%20relationships-spatial%2C%20functional%2C%20interactional%2C%0Asocial%2C%20etc.-is%20considered%20to%20be%20a%20fundamental%20component%20of%20human%20cognition.%0AYet%2C%20despite%20the%20major%20advances%20in%20visual%20comprehension%20in%20multimodal%20language%0Amodels%20%28MLMs%29%2C%20precise%20reasoning%20over%20relationships%20and%20their%20generations%0Aremains%20a%20challenge.%20We%20introduce%20ROBIN%3A%20an%20MLM%20instruction-tuned%20with%20densely%0Aannotated%20relationships%20capable%20of%20constructing%20high-quality%20dense%20scene%20graphs%0Aat%20scale.%20To%20train%20ROBIN%2C%20we%20curate%20SVG%2C%20a%20synthetic%20scene%20graph%20dataset%20by%0Acompleting%20the%20missing%20relations%20of%20selected%20objects%20in%20existing%20scene%20graphs%0Ausing%20a%20teacher%20MLM%20and%20a%20carefully%20designed%20filtering%20process%20to%20ensure%0Ahigh-quality.%20To%20generate%20more%20accurate%20and%20rich%20scene%20graphs%20at%20scale%20for%20any%0Aimage%2C%20we%20introduce%20SG-EDIT%3A%20a%20self-distillation%20framework%20where%20GPT-4o%20further%0Arefines%20ROBIN%27s%20predicted%20scene%20graphs%20by%20removing%20unlikely%20relations%20and/or%0Asuggesting%20relevant%20ones.%20In%20total%2C%20our%20dataset%20contains%20146K%20images%20and%205.6M%0Arelationships%20for%202.6M%20objects.%20Results%20show%20that%20our%20ROBIN-3B%20model%2C%20despite%0Abeing%20trained%20on%20less%20than%203%20million%20instances%2C%20outperforms%20similar-size%20models%0Atrained%20on%20over%20300%20million%20instances%20on%20relationship%20understanding%20benchmarks%2C%0Aand%20even%20surpasses%20larger%20models%20up%20to%2013B%20parameters.%20Notably%2C%20it%20achieves%0Astate-of-the-art%20performance%20in%20referring%20expression%20comprehension%20with%20a%20score%0Aof%2088.9%2C%20surpassing%20the%20previous%20best%20of%2087.4.%20Our%20results%20suggest%20that%0Atraining%20on%20the%20refined%20scene%20graph%20data%20is%20crucial%20to%20maintaining%20high%0Aperformance%20across%20diverse%20visual%20reasoning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Visual%2520Genome%26entry.906535625%3DJae%2520Sung%2520Park%2520and%2520Zixian%2520Ma%2520and%2520Linjie%2520Li%2520and%2520Chenhao%2520Zheng%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Ximing%2520Lu%2520and%2520Khyathi%2520Chandu%2520and%2520Quan%2520Kong%2520and%2520Norimasa%2520Kobori%2520and%2520Ali%2520Farhadi%2520and%2520Yejin%2520Choi%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Reasoning%2520over%2520visual%2520relationships-spatial%252C%2520functional%252C%2520interactional%252C%250Asocial%252C%2520etc.-is%2520considered%2520to%2520be%2520a%2520fundamental%2520component%2520of%2520human%2520cognition.%250AYet%252C%2520despite%2520the%2520major%2520advances%2520in%2520visual%2520comprehension%2520in%2520multimodal%2520language%250Amodels%2520%2528MLMs%2529%252C%2520precise%2520reasoning%2520over%2520relationships%2520and%2520their%2520generations%250Aremains%2520a%2520challenge.%2520We%2520introduce%2520ROBIN%253A%2520an%2520MLM%2520instruction-tuned%2520with%2520densely%250Aannotated%2520relationships%2520capable%2520of%2520constructing%2520high-quality%2520dense%2520scene%2520graphs%250Aat%2520scale.%2520To%2520train%2520ROBIN%252C%2520we%2520curate%2520SVG%252C%2520a%2520synthetic%2520scene%2520graph%2520dataset%2520by%250Acompleting%2520the%2520missing%2520relations%2520of%2520selected%2520objects%2520in%2520existing%2520scene%2520graphs%250Ausing%2520a%2520teacher%2520MLM%2520and%2520a%2520carefully%2520designed%2520filtering%2520process%2520to%2520ensure%250Ahigh-quality.%2520To%2520generate%2520more%2520accurate%2520and%2520rich%2520scene%2520graphs%2520at%2520scale%2520for%2520any%250Aimage%252C%2520we%2520introduce%2520SG-EDIT%253A%2520a%2520self-distillation%2520framework%2520where%2520GPT-4o%2520further%250Arefines%2520ROBIN%2527s%2520predicted%2520scene%2520graphs%2520by%2520removing%2520unlikely%2520relations%2520and/or%250Asuggesting%2520relevant%2520ones.%2520In%2520total%252C%2520our%2520dataset%2520contains%2520146K%2520images%2520and%25205.6M%250Arelationships%2520for%25202.6M%2520objects.%2520Results%2520show%2520that%2520our%2520ROBIN-3B%2520model%252C%2520despite%250Abeing%2520trained%2520on%2520less%2520than%25203%2520million%2520instances%252C%2520outperforms%2520similar-size%2520models%250Atrained%2520on%2520over%2520300%2520million%2520instances%2520on%2520relationship%2520understanding%2520benchmarks%252C%250Aand%2520even%2520surpasses%2520larger%2520models%2520up%2520to%252013B%2520parameters.%2520Notably%252C%2520it%2520achieves%250Astate-of-the-art%2520performance%2520in%2520referring%2520expression%2520comprehension%2520with%2520a%2520score%250Aof%252088.9%252C%2520surpassing%2520the%2520previous%2520best%2520of%252087.4.%2520Our%2520results%2520suggest%2520that%250Atraining%2520on%2520the%2520refined%2520scene%2520graph%2520data%2520is%2520crucial%2520to%2520maintaining%2520high%250Aperformance%2520across%2520diverse%2520visual%2520reasoning%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Visual%20Genome&entry.906535625=Jae%20Sung%20Park%20and%20Zixian%20Ma%20and%20Linjie%20Li%20and%20Chenhao%20Zheng%20and%20Cheng-Yu%20Hsieh%20and%20Ximing%20Lu%20and%20Khyathi%20Chandu%20and%20Quan%20Kong%20and%20Norimasa%20Kobori%20and%20Ali%20Farhadi%20and%20Yejin%20Choi%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Reasoning%20over%20visual%20relationships-spatial%2C%20functional%2C%20interactional%2C%0Asocial%2C%20etc.-is%20considered%20to%20be%20a%20fundamental%20component%20of%20human%20cognition.%0AYet%2C%20despite%20the%20major%20advances%20in%20visual%20comprehension%20in%20multimodal%20language%0Amodels%20%28MLMs%29%2C%20precise%20reasoning%20over%20relationships%20and%20their%20generations%0Aremains%20a%20challenge.%20We%20introduce%20ROBIN%3A%20an%20MLM%20instruction-tuned%20with%20densely%0Aannotated%20relationships%20capable%20of%20constructing%20high-quality%20dense%20scene%20graphs%0Aat%20scale.%20To%20train%20ROBIN%2C%20we%20curate%20SVG%2C%20a%20synthetic%20scene%20graph%20dataset%20by%0Acompleting%20the%20missing%20relations%20of%20selected%20objects%20in%20existing%20scene%20graphs%0Ausing%20a%20teacher%20MLM%20and%20a%20carefully%20designed%20filtering%20process%20to%20ensure%0Ahigh-quality.%20To%20generate%20more%20accurate%20and%20rich%20scene%20graphs%20at%20scale%20for%20any%0Aimage%2C%20we%20introduce%20SG-EDIT%3A%20a%20self-distillation%20framework%20where%20GPT-4o%20further%0Arefines%20ROBIN%27s%20predicted%20scene%20graphs%20by%20removing%20unlikely%20relations%20and/or%0Asuggesting%20relevant%20ones.%20In%20total%2C%20our%20dataset%20contains%20146K%20images%20and%205.6M%0Arelationships%20for%202.6M%20objects.%20Results%20show%20that%20our%20ROBIN-3B%20model%2C%20despite%0Abeing%20trained%20on%20less%20than%203%20million%20instances%2C%20outperforms%20similar-size%20models%0Atrained%20on%20over%20300%20million%20instances%20on%20relationship%20understanding%20benchmarks%2C%0Aand%20even%20surpasses%20larger%20models%20up%20to%2013B%20parameters.%20Notably%2C%20it%20achieves%0Astate-of-the-art%20performance%20in%20referring%20expression%20comprehension%20with%20a%20score%0Aof%2088.9%2C%20surpassing%20the%20previous%20best%20of%2087.4.%20Our%20results%20suggest%20that%0Atraining%20on%20the%20refined%20scene%20graph%20data%20is%20crucial%20to%20maintaining%20high%0Aperformance%20across%20diverse%20visual%20reasoning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07643v1&entry.124074799=Read"},
{"title": "FunDiff: Diffusion Models over Function Spaces for Physics-Informed\n  Generative Modeling", "author": "Sifan Wang and Zehao Dou and Tong-Rui Liu and Lu Lu", "abstract": "  Recent advances in generative modeling -- particularly diffusion models and\nflow matching -- have achieved remarkable success in synthesizing discrete data\nsuch as images and videos. However, adapting these models to physical\napplications remains challenging, as the quantities of interest are continuous\nfunctions governed by complex physical laws. Here, we introduce\n$\\textbf{FunDiff}$, a novel framework for generative modeling in function\nspaces. FunDiff combines a latent diffusion process with a function autoencoder\narchitecture to handle input functions with varying discretizations, generate\ncontinuous functions evaluable at arbitrary locations, and seamlessly\nincorporate physical priors. These priors are enforced through architectural\nconstraints or physics-informed loss functions, ensuring that generated samples\nsatisfy fundamental physical laws. We theoretically establish minimax\noptimality guarantees for density estimation in function spaces, showing that\ndiffusion-based estimators achieve optimal convergence rates under suitable\nregularity conditions. We demonstrate the practical effectiveness of FunDiff\nacross diverse applications in fluid dynamics and solid mechanics. Empirical\nresults show that our method generates physically consistent samples with high\nfidelity to the target distribution and exhibits robustness to noisy and\nlow-resolution data. Code and datasets are publicly available at\nhttps://github.com/sifanexisted/fundiff.\n", "link": "http://arxiv.org/abs/2506.07902v1", "date": "2025-06-09", "relevancy": 2.4391, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6204}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6085}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FunDiff%3A%20Diffusion%20Models%20over%20Function%20Spaces%20for%20Physics-Informed%0A%20%20Generative%20Modeling&body=Title%3A%20FunDiff%3A%20Diffusion%20Models%20over%20Function%20Spaces%20for%20Physics-Informed%0A%20%20Generative%20Modeling%0AAuthor%3A%20Sifan%20Wang%20and%20Zehao%20Dou%20and%20Tong-Rui%20Liu%20and%20Lu%20Lu%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20modeling%20--%20particularly%20diffusion%20models%20and%0Aflow%20matching%20--%20have%20achieved%20remarkable%20success%20in%20synthesizing%20discrete%20data%0Asuch%20as%20images%20and%20videos.%20However%2C%20adapting%20these%20models%20to%20physical%0Aapplications%20remains%20challenging%2C%20as%20the%20quantities%20of%20interest%20are%20continuous%0Afunctions%20governed%20by%20complex%20physical%20laws.%20Here%2C%20we%20introduce%0A%24%5Ctextbf%7BFunDiff%7D%24%2C%20a%20novel%20framework%20for%20generative%20modeling%20in%20function%0Aspaces.%20FunDiff%20combines%20a%20latent%20diffusion%20process%20with%20a%20function%20autoencoder%0Aarchitecture%20to%20handle%20input%20functions%20with%20varying%20discretizations%2C%20generate%0Acontinuous%20functions%20evaluable%20at%20arbitrary%20locations%2C%20and%20seamlessly%0Aincorporate%20physical%20priors.%20These%20priors%20are%20enforced%20through%20architectural%0Aconstraints%20or%20physics-informed%20loss%20functions%2C%20ensuring%20that%20generated%20samples%0Asatisfy%20fundamental%20physical%20laws.%20We%20theoretically%20establish%20minimax%0Aoptimality%20guarantees%20for%20density%20estimation%20in%20function%20spaces%2C%20showing%20that%0Adiffusion-based%20estimators%20achieve%20optimal%20convergence%20rates%20under%20suitable%0Aregularity%20conditions.%20We%20demonstrate%20the%20practical%20effectiveness%20of%20FunDiff%0Aacross%20diverse%20applications%20in%20fluid%20dynamics%20and%20solid%20mechanics.%20Empirical%0Aresults%20show%20that%20our%20method%20generates%20physically%20consistent%20samples%20with%20high%0Afidelity%20to%20the%20target%20distribution%20and%20exhibits%20robustness%20to%20noisy%20and%0Alow-resolution%20data.%20Code%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/sifanexisted/fundiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunDiff%253A%2520Diffusion%2520Models%2520over%2520Function%2520Spaces%2520for%2520Physics-Informed%250A%2520%2520Generative%2520Modeling%26entry.906535625%3DSifan%2520Wang%2520and%2520Zehao%2520Dou%2520and%2520Tong-Rui%2520Liu%2520and%2520Lu%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520modeling%2520--%2520particularly%2520diffusion%2520models%2520and%250Aflow%2520matching%2520--%2520have%2520achieved%2520remarkable%2520success%2520in%2520synthesizing%2520discrete%2520data%250Asuch%2520as%2520images%2520and%2520videos.%2520However%252C%2520adapting%2520these%2520models%2520to%2520physical%250Aapplications%2520remains%2520challenging%252C%2520as%2520the%2520quantities%2520of%2520interest%2520are%2520continuous%250Afunctions%2520governed%2520by%2520complex%2520physical%2520laws.%2520Here%252C%2520we%2520introduce%250A%2524%255Ctextbf%257BFunDiff%257D%2524%252C%2520a%2520novel%2520framework%2520for%2520generative%2520modeling%2520in%2520function%250Aspaces.%2520FunDiff%2520combines%2520a%2520latent%2520diffusion%2520process%2520with%2520a%2520function%2520autoencoder%250Aarchitecture%2520to%2520handle%2520input%2520functions%2520with%2520varying%2520discretizations%252C%2520generate%250Acontinuous%2520functions%2520evaluable%2520at%2520arbitrary%2520locations%252C%2520and%2520seamlessly%250Aincorporate%2520physical%2520priors.%2520These%2520priors%2520are%2520enforced%2520through%2520architectural%250Aconstraints%2520or%2520physics-informed%2520loss%2520functions%252C%2520ensuring%2520that%2520generated%2520samples%250Asatisfy%2520fundamental%2520physical%2520laws.%2520We%2520theoretically%2520establish%2520minimax%250Aoptimality%2520guarantees%2520for%2520density%2520estimation%2520in%2520function%2520spaces%252C%2520showing%2520that%250Adiffusion-based%2520estimators%2520achieve%2520optimal%2520convergence%2520rates%2520under%2520suitable%250Aregularity%2520conditions.%2520We%2520demonstrate%2520the%2520practical%2520effectiveness%2520of%2520FunDiff%250Aacross%2520diverse%2520applications%2520in%2520fluid%2520dynamics%2520and%2520solid%2520mechanics.%2520Empirical%250Aresults%2520show%2520that%2520our%2520method%2520generates%2520physically%2520consistent%2520samples%2520with%2520high%250Afidelity%2520to%2520the%2520target%2520distribution%2520and%2520exhibits%2520robustness%2520to%2520noisy%2520and%250Alow-resolution%2520data.%2520Code%2520and%2520datasets%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/sifanexisted/fundiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FunDiff%3A%20Diffusion%20Models%20over%20Function%20Spaces%20for%20Physics-Informed%0A%20%20Generative%20Modeling&entry.906535625=Sifan%20Wang%20and%20Zehao%20Dou%20and%20Tong-Rui%20Liu%20and%20Lu%20Lu&entry.1292438233=%20%20Recent%20advances%20in%20generative%20modeling%20--%20particularly%20diffusion%20models%20and%0Aflow%20matching%20--%20have%20achieved%20remarkable%20success%20in%20synthesizing%20discrete%20data%0Asuch%20as%20images%20and%20videos.%20However%2C%20adapting%20these%20models%20to%20physical%0Aapplications%20remains%20challenging%2C%20as%20the%20quantities%20of%20interest%20are%20continuous%0Afunctions%20governed%20by%20complex%20physical%20laws.%20Here%2C%20we%20introduce%0A%24%5Ctextbf%7BFunDiff%7D%24%2C%20a%20novel%20framework%20for%20generative%20modeling%20in%20function%0Aspaces.%20FunDiff%20combines%20a%20latent%20diffusion%20process%20with%20a%20function%20autoencoder%0Aarchitecture%20to%20handle%20input%20functions%20with%20varying%20discretizations%2C%20generate%0Acontinuous%20functions%20evaluable%20at%20arbitrary%20locations%2C%20and%20seamlessly%0Aincorporate%20physical%20priors.%20These%20priors%20are%20enforced%20through%20architectural%0Aconstraints%20or%20physics-informed%20loss%20functions%2C%20ensuring%20that%20generated%20samples%0Asatisfy%20fundamental%20physical%20laws.%20We%20theoretically%20establish%20minimax%0Aoptimality%20guarantees%20for%20density%20estimation%20in%20function%20spaces%2C%20showing%20that%0Adiffusion-based%20estimators%20achieve%20optimal%20convergence%20rates%20under%20suitable%0Aregularity%20conditions.%20We%20demonstrate%20the%20practical%20effectiveness%20of%20FunDiff%0Aacross%20diverse%20applications%20in%20fluid%20dynamics%20and%20solid%20mechanics.%20Empirical%0Aresults%20show%20that%20our%20method%20generates%20physically%20consistent%20samples%20with%20high%0Afidelity%20to%20the%20target%20distribution%20and%20exhibits%20robustness%20to%20noisy%20and%0Alow-resolution%20data.%20Code%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/sifanexisted/fundiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07902v1&entry.124074799=Read"},
{"title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "author": "Zeju Qiu and Simon Buchholz and Tim Z. Xiao and Maximilian Dax and Bernhard Sch\u00f6lkopf and Weiyang Liu", "abstract": "  While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.\n", "link": "http://arxiv.org/abs/2506.08001v1", "date": "2025-06-09", "relevancy": 2.4266, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reparameterized%20LLM%20Training%20via%20Orthogonal%20Equivalence%20Transformation&body=Title%3A%20Reparameterized%20LLM%20Training%20via%20Orthogonal%20Equivalence%20Transformation%0AAuthor%3A%20Zeju%20Qiu%20and%20Simon%20Buchholz%20and%20Tim%20Z.%20Xiao%20and%20Maximilian%20Dax%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Weiyang%20Liu%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20are%20driving%20the%20rapid%20advancement%20of%0Aartificial%20intelligence%2C%20effectively%20and%20reliably%20training%20these%20large%20models%0Aremains%20one%20of%20the%20field%27s%20most%20significant%20challenges.%20To%20address%20this%0Achallenge%2C%20we%20propose%20POET%2C%20a%20novel%20reParameterized%20training%20algorithm%20that%0Auses%20Orthogonal%20Equivalence%20Transformation%20to%20optimize%20neurons.%20Specifically%2C%0APOET%20reparameterizes%20each%20neuron%20with%20two%20learnable%20orthogonal%20matrices%20and%20a%0Afixed%20random%20weight%20matrix.%20Because%20of%20its%20provable%20preservation%20of%20spectral%0Aproperties%20of%20weight%20matrices%2C%20POET%20can%20stably%20optimize%20the%20objective%20function%0Awith%20improved%20generalization.%20We%20further%20develop%20efficient%20approximations%20that%0Amake%20POET%20flexible%20and%20scalable%20for%20training%20large-scale%20neural%20networks.%0AExtensive%20experiments%20validate%20the%20effectiveness%20and%20scalability%20of%20POET%20in%0Atraining%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReparameterized%2520LLM%2520Training%2520via%2520Orthogonal%2520Equivalence%2520Transformation%26entry.906535625%3DZeju%2520Qiu%2520and%2520Simon%2520Buchholz%2520and%2520Tim%2520Z.%2520Xiao%2520and%2520Maximilian%2520Dax%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Weiyang%2520Liu%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520driving%2520the%2520rapid%2520advancement%2520of%250Aartificial%2520intelligence%252C%2520effectively%2520and%2520reliably%2520training%2520these%2520large%2520models%250Aremains%2520one%2520of%2520the%2520field%2527s%2520most%2520significant%2520challenges.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520POET%252C%2520a%2520novel%2520reParameterized%2520training%2520algorithm%2520that%250Auses%2520Orthogonal%2520Equivalence%2520Transformation%2520to%2520optimize%2520neurons.%2520Specifically%252C%250APOET%2520reparameterizes%2520each%2520neuron%2520with%2520two%2520learnable%2520orthogonal%2520matrices%2520and%2520a%250Afixed%2520random%2520weight%2520matrix.%2520Because%2520of%2520its%2520provable%2520preservation%2520of%2520spectral%250Aproperties%2520of%2520weight%2520matrices%252C%2520POET%2520can%2520stably%2520optimize%2520the%2520objective%2520function%250Awith%2520improved%2520generalization.%2520We%2520further%2520develop%2520efficient%2520approximations%2520that%250Amake%2520POET%2520flexible%2520and%2520scalable%2520for%2520training%2520large-scale%2520neural%2520networks.%250AExtensive%2520experiments%2520validate%2520the%2520effectiveness%2520and%2520scalability%2520of%2520POET%2520in%250Atraining%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reparameterized%20LLM%20Training%20via%20Orthogonal%20Equivalence%20Transformation&entry.906535625=Zeju%20Qiu%20and%20Simon%20Buchholz%20and%20Tim%20Z.%20Xiao%20and%20Maximilian%20Dax%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Weiyang%20Liu&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20are%20driving%20the%20rapid%20advancement%20of%0Aartificial%20intelligence%2C%20effectively%20and%20reliably%20training%20these%20large%20models%0Aremains%20one%20of%20the%20field%27s%20most%20significant%20challenges.%20To%20address%20this%0Achallenge%2C%20we%20propose%20POET%2C%20a%20novel%20reParameterized%20training%20algorithm%20that%0Auses%20Orthogonal%20Equivalence%20Transformation%20to%20optimize%20neurons.%20Specifically%2C%0APOET%20reparameterizes%20each%20neuron%20with%20two%20learnable%20orthogonal%20matrices%20and%20a%0Afixed%20random%20weight%20matrix.%20Because%20of%20its%20provable%20preservation%20of%20spectral%0Aproperties%20of%20weight%20matrices%2C%20POET%20can%20stably%20optimize%20the%20objective%20function%0Awith%20improved%20generalization.%20We%20further%20develop%20efficient%20approximations%20that%0Amake%20POET%20flexible%20and%20scalable%20for%20training%20large-scale%20neural%20networks.%0AExtensive%20experiments%20validate%20the%20effectiveness%20and%20scalability%20of%20POET%20in%0Atraining%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08001v1&entry.124074799=Read"},
{"title": "Identifiable Object Representations under Spatial Ambiguities", "author": "Avinash Kori and Francesca Toni and Ben Glocker", "abstract": "  Modular object-centric representations are essential for *human-like\nreasoning* but are challenging to obtain under spatial ambiguities, *e.g. due\nto occlusions and view ambiguities*. However, addressing challenges presents\nboth theoretical and practical difficulties. We introduce a novel multi-view\nprobabilistic approach that aggregates view-specific slots to capture\n*invariant content* information while simultaneously learning disentangled\nglobal *viewpoint-level* information. Unlike prior single-view methods, our\napproach resolves spatial ambiguities, provides theoretical guarantees for\nidentifiability, and requires *no viewpoint annotations*. Extensive experiments\non standard benchmarks and novel complex datasets validate our method's\nrobustness and scalability.\n", "link": "http://arxiv.org/abs/2506.07806v1", "date": "2025-06-09", "relevancy": 2.4072, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6119}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5987}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20Object%20Representations%20under%20Spatial%20Ambiguities&body=Title%3A%20Identifiable%20Object%20Representations%20under%20Spatial%20Ambiguities%0AAuthor%3A%20Avinash%20Kori%20and%20Francesca%20Toni%20and%20Ben%20Glocker%0AAbstract%3A%20%20%20Modular%20object-centric%20representations%20are%20essential%20for%20%2Ahuman-like%0Areasoning%2A%20but%20are%20challenging%20to%20obtain%20under%20spatial%20ambiguities%2C%20%2Ae.g.%20due%0Ato%20occlusions%20and%20view%20ambiguities%2A.%20However%2C%20addressing%20challenges%20presents%0Aboth%20theoretical%20and%20practical%20difficulties.%20We%20introduce%20a%20novel%20multi-view%0Aprobabilistic%20approach%20that%20aggregates%20view-specific%20slots%20to%20capture%0A%2Ainvariant%20content%2A%20information%20while%20simultaneously%20learning%20disentangled%0Aglobal%20%2Aviewpoint-level%2A%20information.%20Unlike%20prior%20single-view%20methods%2C%20our%0Aapproach%20resolves%20spatial%20ambiguities%2C%20provides%20theoretical%20guarantees%20for%0Aidentifiability%2C%20and%20requires%20%2Ano%20viewpoint%20annotations%2A.%20Extensive%20experiments%0Aon%20standard%20benchmarks%20and%20novel%20complex%20datasets%20validate%20our%20method%27s%0Arobustness%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520Object%2520Representations%2520under%2520Spatial%2520Ambiguities%26entry.906535625%3DAvinash%2520Kori%2520and%2520Francesca%2520Toni%2520and%2520Ben%2520Glocker%26entry.1292438233%3D%2520%2520Modular%2520object-centric%2520representations%2520are%2520essential%2520for%2520%252Ahuman-like%250Areasoning%252A%2520but%2520are%2520challenging%2520to%2520obtain%2520under%2520spatial%2520ambiguities%252C%2520%252Ae.g.%2520due%250Ato%2520occlusions%2520and%2520view%2520ambiguities%252A.%2520However%252C%2520addressing%2520challenges%2520presents%250Aboth%2520theoretical%2520and%2520practical%2520difficulties.%2520We%2520introduce%2520a%2520novel%2520multi-view%250Aprobabilistic%2520approach%2520that%2520aggregates%2520view-specific%2520slots%2520to%2520capture%250A%252Ainvariant%2520content%252A%2520information%2520while%2520simultaneously%2520learning%2520disentangled%250Aglobal%2520%252Aviewpoint-level%252A%2520information.%2520Unlike%2520prior%2520single-view%2520methods%252C%2520our%250Aapproach%2520resolves%2520spatial%2520ambiguities%252C%2520provides%2520theoretical%2520guarantees%2520for%250Aidentifiability%252C%2520and%2520requires%2520%252Ano%2520viewpoint%2520annotations%252A.%2520Extensive%2520experiments%250Aon%2520standard%2520benchmarks%2520and%2520novel%2520complex%2520datasets%2520validate%2520our%2520method%2527s%250Arobustness%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20Object%20Representations%20under%20Spatial%20Ambiguities&entry.906535625=Avinash%20Kori%20and%20Francesca%20Toni%20and%20Ben%20Glocker&entry.1292438233=%20%20Modular%20object-centric%20representations%20are%20essential%20for%20%2Ahuman-like%0Areasoning%2A%20but%20are%20challenging%20to%20obtain%20under%20spatial%20ambiguities%2C%20%2Ae.g.%20due%0Ato%20occlusions%20and%20view%20ambiguities%2A.%20However%2C%20addressing%20challenges%20presents%0Aboth%20theoretical%20and%20practical%20difficulties.%20We%20introduce%20a%20novel%20multi-view%0Aprobabilistic%20approach%20that%20aggregates%20view-specific%20slots%20to%20capture%0A%2Ainvariant%20content%2A%20information%20while%20simultaneously%20learning%20disentangled%0Aglobal%20%2Aviewpoint-level%2A%20information.%20Unlike%20prior%20single-view%20methods%2C%20our%0Aapproach%20resolves%20spatial%20ambiguities%2C%20provides%20theoretical%20guarantees%20for%0Aidentifiability%2C%20and%20requires%20%2Ano%20viewpoint%20annotations%2A.%20Extensive%20experiments%0Aon%20standard%20benchmarks%20and%20novel%20complex%20datasets%20validate%20our%20method%27s%0Arobustness%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07806v1&entry.124074799=Read"},
{"title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object\n  Completion with Partial References", "author": "Ming-Feng Li and Xin Yang and Fu-En Wang and Hritam Basak and Yuyin Sun and Shreekant Gayaka and Min Sun and Cheng-Hao Kuo", "abstract": "  6D object pose estimation has shown strong generalizability to novel objects.\nHowever, existing methods often require either a complete, well-reconstructed\n3D model or numerous reference images that fully cover the object. Estimating\n6D poses from partial references, which capture only fragments of an object's\nappearance and geometry, remains challenging. To address this, we propose\nUA-Pose, an uncertainty-aware approach for 6D object pose estimation and online\nobject completion specifically designed for partial references. We assume\naccess to either (1) a limited set of RGBD images with known poses or (2) a\nsingle 2D image. For the first case, we initialize a partial object 3D model\nbased on the provided images and poses, while for the second, we use\nimage-to-3D techniques to generate an initial object 3D model. Our method\nintegrates uncertainty into the incomplete 3D model, distinguishing between\nseen and unseen regions. This uncertainty enables confidence assessment in pose\nestimation and guides an uncertainty-aware sampling strategy for online object\ncompletion, enhancing robustness in pose estimation accuracy and improving\nobject completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and\nHO3D datasets, including RGBD sequences of YCB objects manipulated by robots\nand human hands. Experimental results demonstrate significant performance\nimprovements over existing methods, particularly when object observations are\nincomplete or partially captured. Project page:\nhttps://minfenli.github.io/UA-Pose/\n", "link": "http://arxiv.org/abs/2506.07996v1", "date": "2025-06-09", "relevancy": 2.407, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6003}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UA-Pose%3A%20Uncertainty-Aware%206D%20Object%20Pose%20Estimation%20and%20Online%20Object%0A%20%20Completion%20with%20Partial%20References&body=Title%3A%20UA-Pose%3A%20Uncertainty-Aware%206D%20Object%20Pose%20Estimation%20and%20Online%20Object%0A%20%20Completion%20with%20Partial%20References%0AAuthor%3A%20Ming-Feng%20Li%20and%20Xin%20Yang%20and%20Fu-En%20Wang%20and%20Hritam%20Basak%20and%20Yuyin%20Sun%20and%20Shreekant%20Gayaka%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo%0AAbstract%3A%20%20%206D%20object%20pose%20estimation%20has%20shown%20strong%20generalizability%20to%20novel%20objects.%0AHowever%2C%20existing%20methods%20often%20require%20either%20a%20complete%2C%20well-reconstructed%0A3D%20model%20or%20numerous%20reference%20images%20that%20fully%20cover%20the%20object.%20Estimating%0A6D%20poses%20from%20partial%20references%2C%20which%20capture%20only%20fragments%20of%20an%20object%27s%0Aappearance%20and%20geometry%2C%20remains%20challenging.%20To%20address%20this%2C%20we%20propose%0AUA-Pose%2C%20an%20uncertainty-aware%20approach%20for%206D%20object%20pose%20estimation%20and%20online%0Aobject%20completion%20specifically%20designed%20for%20partial%20references.%20We%20assume%0Aaccess%20to%20either%20%281%29%20a%20limited%20set%20of%20RGBD%20images%20with%20known%20poses%20or%20%282%29%20a%0Asingle%202D%20image.%20For%20the%20first%20case%2C%20we%20initialize%20a%20partial%20object%203D%20model%0Abased%20on%20the%20provided%20images%20and%20poses%2C%20while%20for%20the%20second%2C%20we%20use%0Aimage-to-3D%20techniques%20to%20generate%20an%20initial%20object%203D%20model.%20Our%20method%0Aintegrates%20uncertainty%20into%20the%20incomplete%203D%20model%2C%20distinguishing%20between%0Aseen%20and%20unseen%20regions.%20This%20uncertainty%20enables%20confidence%20assessment%20in%20pose%0Aestimation%20and%20guides%20an%20uncertainty-aware%20sampling%20strategy%20for%20online%20object%0Acompletion%2C%20enhancing%20robustness%20in%20pose%20estimation%20accuracy%20and%20improving%0Aobject%20completeness.%20We%20evaluate%20our%20method%20on%20the%20YCB-Video%2C%20YCBInEOAT%2C%20and%0AHO3D%20datasets%2C%20including%20RGBD%20sequences%20of%20YCB%20objects%20manipulated%20by%20robots%0Aand%20human%20hands.%20Experimental%20results%20demonstrate%20significant%20performance%0Aimprovements%20over%20existing%20methods%2C%20particularly%20when%20object%20observations%20are%0Aincomplete%20or%20partially%20captured.%20Project%20page%3A%0Ahttps%3A//minfenli.github.io/UA-Pose/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUA-Pose%253A%2520Uncertainty-Aware%25206D%2520Object%2520Pose%2520Estimation%2520and%2520Online%2520Object%250A%2520%2520Completion%2520with%2520Partial%2520References%26entry.906535625%3DMing-Feng%2520Li%2520and%2520Xin%2520Yang%2520and%2520Fu-En%2520Wang%2520and%2520Hritam%2520Basak%2520and%2520Yuyin%2520Sun%2520and%2520Shreekant%2520Gayaka%2520and%2520Min%2520Sun%2520and%2520Cheng-Hao%2520Kuo%26entry.1292438233%3D%2520%25206D%2520object%2520pose%2520estimation%2520has%2520shown%2520strong%2520generalizability%2520to%2520novel%2520objects.%250AHowever%252C%2520existing%2520methods%2520often%2520require%2520either%2520a%2520complete%252C%2520well-reconstructed%250A3D%2520model%2520or%2520numerous%2520reference%2520images%2520that%2520fully%2520cover%2520the%2520object.%2520Estimating%250A6D%2520poses%2520from%2520partial%2520references%252C%2520which%2520capture%2520only%2520fragments%2520of%2520an%2520object%2527s%250Aappearance%2520and%2520geometry%252C%2520remains%2520challenging.%2520To%2520address%2520this%252C%2520we%2520propose%250AUA-Pose%252C%2520an%2520uncertainty-aware%2520approach%2520for%25206D%2520object%2520pose%2520estimation%2520and%2520online%250Aobject%2520completion%2520specifically%2520designed%2520for%2520partial%2520references.%2520We%2520assume%250Aaccess%2520to%2520either%2520%25281%2529%2520a%2520limited%2520set%2520of%2520RGBD%2520images%2520with%2520known%2520poses%2520or%2520%25282%2529%2520a%250Asingle%25202D%2520image.%2520For%2520the%2520first%2520case%252C%2520we%2520initialize%2520a%2520partial%2520object%25203D%2520model%250Abased%2520on%2520the%2520provided%2520images%2520and%2520poses%252C%2520while%2520for%2520the%2520second%252C%2520we%2520use%250Aimage-to-3D%2520techniques%2520to%2520generate%2520an%2520initial%2520object%25203D%2520model.%2520Our%2520method%250Aintegrates%2520uncertainty%2520into%2520the%2520incomplete%25203D%2520model%252C%2520distinguishing%2520between%250Aseen%2520and%2520unseen%2520regions.%2520This%2520uncertainty%2520enables%2520confidence%2520assessment%2520in%2520pose%250Aestimation%2520and%2520guides%2520an%2520uncertainty-aware%2520sampling%2520strategy%2520for%2520online%2520object%250Acompletion%252C%2520enhancing%2520robustness%2520in%2520pose%2520estimation%2520accuracy%2520and%2520improving%250Aobject%2520completeness.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520YCB-Video%252C%2520YCBInEOAT%252C%2520and%250AHO3D%2520datasets%252C%2520including%2520RGBD%2520sequences%2520of%2520YCB%2520objects%2520manipulated%2520by%2520robots%250Aand%2520human%2520hands.%2520Experimental%2520results%2520demonstrate%2520significant%2520performance%250Aimprovements%2520over%2520existing%2520methods%252C%2520particularly%2520when%2520object%2520observations%2520are%250Aincomplete%2520or%2520partially%2520captured.%2520Project%2520page%253A%250Ahttps%253A//minfenli.github.io/UA-Pose/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UA-Pose%3A%20Uncertainty-Aware%206D%20Object%20Pose%20Estimation%20and%20Online%20Object%0A%20%20Completion%20with%20Partial%20References&entry.906535625=Ming-Feng%20Li%20and%20Xin%20Yang%20and%20Fu-En%20Wang%20and%20Hritam%20Basak%20and%20Yuyin%20Sun%20and%20Shreekant%20Gayaka%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo&entry.1292438233=%20%206D%20object%20pose%20estimation%20has%20shown%20strong%20generalizability%20to%20novel%20objects.%0AHowever%2C%20existing%20methods%20often%20require%20either%20a%20complete%2C%20well-reconstructed%0A3D%20model%20or%20numerous%20reference%20images%20that%20fully%20cover%20the%20object.%20Estimating%0A6D%20poses%20from%20partial%20references%2C%20which%20capture%20only%20fragments%20of%20an%20object%27s%0Aappearance%20and%20geometry%2C%20remains%20challenging.%20To%20address%20this%2C%20we%20propose%0AUA-Pose%2C%20an%20uncertainty-aware%20approach%20for%206D%20object%20pose%20estimation%20and%20online%0Aobject%20completion%20specifically%20designed%20for%20partial%20references.%20We%20assume%0Aaccess%20to%20either%20%281%29%20a%20limited%20set%20of%20RGBD%20images%20with%20known%20poses%20or%20%282%29%20a%0Asingle%202D%20image.%20For%20the%20first%20case%2C%20we%20initialize%20a%20partial%20object%203D%20model%0Abased%20on%20the%20provided%20images%20and%20poses%2C%20while%20for%20the%20second%2C%20we%20use%0Aimage-to-3D%20techniques%20to%20generate%20an%20initial%20object%203D%20model.%20Our%20method%0Aintegrates%20uncertainty%20into%20the%20incomplete%203D%20model%2C%20distinguishing%20between%0Aseen%20and%20unseen%20regions.%20This%20uncertainty%20enables%20confidence%20assessment%20in%20pose%0Aestimation%20and%20guides%20an%20uncertainty-aware%20sampling%20strategy%20for%20online%20object%0Acompletion%2C%20enhancing%20robustness%20in%20pose%20estimation%20accuracy%20and%20improving%0Aobject%20completeness.%20We%20evaluate%20our%20method%20on%20the%20YCB-Video%2C%20YCBInEOAT%2C%20and%0AHO3D%20datasets%2C%20including%20RGBD%20sequences%20of%20YCB%20objects%20manipulated%20by%20robots%0Aand%20human%20hands.%20Experimental%20results%20demonstrate%20significant%20performance%0Aimprovements%20over%20existing%20methods%2C%20particularly%20when%20object%20observations%20are%0Aincomplete%20or%20partially%20captured.%20Project%20page%3A%0Ahttps%3A//minfenli.github.io/UA-Pose/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07996v1&entry.124074799=Read"},
{"title": "Fine-grained Hierarchical Crop Type Classification from Integrated\n  Hyperspectral EnMAP Data and Multispectral Sentinel-2 Time Series: A\n  Large-scale Dataset and Dual-stream Transformer Method", "author": "Wenyuan Li and Shunlin Liang and Yuxiang Zhang and Liqin Liu and Keyan Chen and Yongzhe Chen and Han Ma and Jianglei Xu and Yichuan Ma and Shikang Guan and Zhenwei Shi", "abstract": "  Fine-grained crop type classification serves as the fundamental basis for\nlarge-scale crop mapping and plays a vital role in ensuring food security. It\nrequires simultaneous capture of both phenological dynamics (obtained from\nmulti-temporal satellite data like Sentinel-2) and subtle spectral variations\n(demanding nanometer-scale spectral resolution from hyperspectral imagery).\nResearch combining these two modalities remains scarce currently due to\nchallenges in hyperspectral data acquisition and crop types annotation costs.\nTo address these issues, we construct a hierarchical hyperspectral crop dataset\n(H2Crop) by integrating 30m-resolution EnMAP hyperspectral data with Sentinel-2\ntime series. With over one million annotated field parcels organized in a\nfour-tier crop taxonomy, H2Crop establishes a vital benchmark for fine-grained\nagricultural crop classification and hyperspectral image processing. We propose\na dual-stream Transformer architecture that synergistically processes these\nmodalities. It coordinates two specialized pathways: a spectral-spatial\nTransformer extracts fine-grained signatures from hyperspectral EnMAP data,\nwhile a temporal Swin Transformer extracts crop growth patterns from Sentinel-2\ntime series. The designed hierarchical classification head with hierarchical\nfusion then simultaneously delivers multi-level crop type classification across\nall taxonomic tiers. Experiments demonstrate that adding hyperspectral EnMAP\ndata to Sentinel-2 time series yields a 4.2% average F1-scores improvement\n(peaking at 6.3%). Extensive comparisons also confirm our method's higher\naccuracy over existing deep learning approaches for crop type classification\nand the consistent benefits of hyperspectral data across varying temporal\nwindows and crop change scenarios. Codes and dataset are available at\nhttps://github.com/flyakon/H2Crop.\n", "link": "http://arxiv.org/abs/2506.06155v2", "date": "2025-06-09", "relevancy": 2.4014, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4813}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Hierarchical%20Crop%20Type%20Classification%20from%20Integrated%0A%20%20Hyperspectral%20EnMAP%20Data%20and%20Multispectral%20Sentinel-2%20Time%20Series%3A%20A%0A%20%20Large-scale%20Dataset%20and%20Dual-stream%20Transformer%20Method&body=Title%3A%20Fine-grained%20Hierarchical%20Crop%20Type%20Classification%20from%20Integrated%0A%20%20Hyperspectral%20EnMAP%20Data%20and%20Multispectral%20Sentinel-2%20Time%20Series%3A%20A%0A%20%20Large-scale%20Dataset%20and%20Dual-stream%20Transformer%20Method%0AAuthor%3A%20Wenyuan%20Li%20and%20Shunlin%20Liang%20and%20Yuxiang%20Zhang%20and%20Liqin%20Liu%20and%20Keyan%20Chen%20and%20Yongzhe%20Chen%20and%20Han%20Ma%20and%20Jianglei%20Xu%20and%20Yichuan%20Ma%20and%20Shikang%20Guan%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Fine-grained%20crop%20type%20classification%20serves%20as%20the%20fundamental%20basis%20for%0Alarge-scale%20crop%20mapping%20and%20plays%20a%20vital%20role%20in%20ensuring%20food%20security.%20It%0Arequires%20simultaneous%20capture%20of%20both%20phenological%20dynamics%20%28obtained%20from%0Amulti-temporal%20satellite%20data%20like%20Sentinel-2%29%20and%20subtle%20spectral%20variations%0A%28demanding%20nanometer-scale%20spectral%20resolution%20from%20hyperspectral%20imagery%29.%0AResearch%20combining%20these%20two%20modalities%20remains%20scarce%20currently%20due%20to%0Achallenges%20in%20hyperspectral%20data%20acquisition%20and%20crop%20types%20annotation%20costs.%0ATo%20address%20these%20issues%2C%20we%20construct%20a%20hierarchical%20hyperspectral%20crop%20dataset%0A%28H2Crop%29%20by%20integrating%2030m-resolution%20EnMAP%20hyperspectral%20data%20with%20Sentinel-2%0Atime%20series.%20With%20over%20one%20million%20annotated%20field%20parcels%20organized%20in%20a%0Afour-tier%20crop%20taxonomy%2C%20H2Crop%20establishes%20a%20vital%20benchmark%20for%20fine-grained%0Aagricultural%20crop%20classification%20and%20hyperspectral%20image%20processing.%20We%20propose%0Aa%20dual-stream%20Transformer%20architecture%20that%20synergistically%20processes%20these%0Amodalities.%20It%20coordinates%20two%20specialized%20pathways%3A%20a%20spectral-spatial%0ATransformer%20extracts%20fine-grained%20signatures%20from%20hyperspectral%20EnMAP%20data%2C%0Awhile%20a%20temporal%20Swin%20Transformer%20extracts%20crop%20growth%20patterns%20from%20Sentinel-2%0Atime%20series.%20The%20designed%20hierarchical%20classification%20head%20with%20hierarchical%0Afusion%20then%20simultaneously%20delivers%20multi-level%20crop%20type%20classification%20across%0Aall%20taxonomic%20tiers.%20Experiments%20demonstrate%20that%20adding%20hyperspectral%20EnMAP%0Adata%20to%20Sentinel-2%20time%20series%20yields%20a%204.2%25%20average%20F1-scores%20improvement%0A%28peaking%20at%206.3%25%29.%20Extensive%20comparisons%20also%20confirm%20our%20method%27s%20higher%0Aaccuracy%20over%20existing%20deep%20learning%20approaches%20for%20crop%20type%20classification%0Aand%20the%20consistent%20benefits%20of%20hyperspectral%20data%20across%20varying%20temporal%0Awindows%20and%20crop%20change%20scenarios.%20Codes%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/flyakon/H2Crop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520Hierarchical%2520Crop%2520Type%2520Classification%2520from%2520Integrated%250A%2520%2520Hyperspectral%2520EnMAP%2520Data%2520and%2520Multispectral%2520Sentinel-2%2520Time%2520Series%253A%2520A%250A%2520%2520Large-scale%2520Dataset%2520and%2520Dual-stream%2520Transformer%2520Method%26entry.906535625%3DWenyuan%2520Li%2520and%2520Shunlin%2520Liang%2520and%2520Yuxiang%2520Zhang%2520and%2520Liqin%2520Liu%2520and%2520Keyan%2520Chen%2520and%2520Yongzhe%2520Chen%2520and%2520Han%2520Ma%2520and%2520Jianglei%2520Xu%2520and%2520Yichuan%2520Ma%2520and%2520Shikang%2520Guan%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Fine-grained%2520crop%2520type%2520classification%2520serves%2520as%2520the%2520fundamental%2520basis%2520for%250Alarge-scale%2520crop%2520mapping%2520and%2520plays%2520a%2520vital%2520role%2520in%2520ensuring%2520food%2520security.%2520It%250Arequires%2520simultaneous%2520capture%2520of%2520both%2520phenological%2520dynamics%2520%2528obtained%2520from%250Amulti-temporal%2520satellite%2520data%2520like%2520Sentinel-2%2529%2520and%2520subtle%2520spectral%2520variations%250A%2528demanding%2520nanometer-scale%2520spectral%2520resolution%2520from%2520hyperspectral%2520imagery%2529.%250AResearch%2520combining%2520these%2520two%2520modalities%2520remains%2520scarce%2520currently%2520due%2520to%250Achallenges%2520in%2520hyperspectral%2520data%2520acquisition%2520and%2520crop%2520types%2520annotation%2520costs.%250ATo%2520address%2520these%2520issues%252C%2520we%2520construct%2520a%2520hierarchical%2520hyperspectral%2520crop%2520dataset%250A%2528H2Crop%2529%2520by%2520integrating%252030m-resolution%2520EnMAP%2520hyperspectral%2520data%2520with%2520Sentinel-2%250Atime%2520series.%2520With%2520over%2520one%2520million%2520annotated%2520field%2520parcels%2520organized%2520in%2520a%250Afour-tier%2520crop%2520taxonomy%252C%2520H2Crop%2520establishes%2520a%2520vital%2520benchmark%2520for%2520fine-grained%250Aagricultural%2520crop%2520classification%2520and%2520hyperspectral%2520image%2520processing.%2520We%2520propose%250Aa%2520dual-stream%2520Transformer%2520architecture%2520that%2520synergistically%2520processes%2520these%250Amodalities.%2520It%2520coordinates%2520two%2520specialized%2520pathways%253A%2520a%2520spectral-spatial%250ATransformer%2520extracts%2520fine-grained%2520signatures%2520from%2520hyperspectral%2520EnMAP%2520data%252C%250Awhile%2520a%2520temporal%2520Swin%2520Transformer%2520extracts%2520crop%2520growth%2520patterns%2520from%2520Sentinel-2%250Atime%2520series.%2520The%2520designed%2520hierarchical%2520classification%2520head%2520with%2520hierarchical%250Afusion%2520then%2520simultaneously%2520delivers%2520multi-level%2520crop%2520type%2520classification%2520across%250Aall%2520taxonomic%2520tiers.%2520Experiments%2520demonstrate%2520that%2520adding%2520hyperspectral%2520EnMAP%250Adata%2520to%2520Sentinel-2%2520time%2520series%2520yields%2520a%25204.2%2525%2520average%2520F1-scores%2520improvement%250A%2528peaking%2520at%25206.3%2525%2529.%2520Extensive%2520comparisons%2520also%2520confirm%2520our%2520method%2527s%2520higher%250Aaccuracy%2520over%2520existing%2520deep%2520learning%2520approaches%2520for%2520crop%2520type%2520classification%250Aand%2520the%2520consistent%2520benefits%2520of%2520hyperspectral%2520data%2520across%2520varying%2520temporal%250Awindows%2520and%2520crop%2520change%2520scenarios.%2520Codes%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/flyakon/H2Crop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Hierarchical%20Crop%20Type%20Classification%20from%20Integrated%0A%20%20Hyperspectral%20EnMAP%20Data%20and%20Multispectral%20Sentinel-2%20Time%20Series%3A%20A%0A%20%20Large-scale%20Dataset%20and%20Dual-stream%20Transformer%20Method&entry.906535625=Wenyuan%20Li%20and%20Shunlin%20Liang%20and%20Yuxiang%20Zhang%20and%20Liqin%20Liu%20and%20Keyan%20Chen%20and%20Yongzhe%20Chen%20and%20Han%20Ma%20and%20Jianglei%20Xu%20and%20Yichuan%20Ma%20and%20Shikang%20Guan%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Fine-grained%20crop%20type%20classification%20serves%20as%20the%20fundamental%20basis%20for%0Alarge-scale%20crop%20mapping%20and%20plays%20a%20vital%20role%20in%20ensuring%20food%20security.%20It%0Arequires%20simultaneous%20capture%20of%20both%20phenological%20dynamics%20%28obtained%20from%0Amulti-temporal%20satellite%20data%20like%20Sentinel-2%29%20and%20subtle%20spectral%20variations%0A%28demanding%20nanometer-scale%20spectral%20resolution%20from%20hyperspectral%20imagery%29.%0AResearch%20combining%20these%20two%20modalities%20remains%20scarce%20currently%20due%20to%0Achallenges%20in%20hyperspectral%20data%20acquisition%20and%20crop%20types%20annotation%20costs.%0ATo%20address%20these%20issues%2C%20we%20construct%20a%20hierarchical%20hyperspectral%20crop%20dataset%0A%28H2Crop%29%20by%20integrating%2030m-resolution%20EnMAP%20hyperspectral%20data%20with%20Sentinel-2%0Atime%20series.%20With%20over%20one%20million%20annotated%20field%20parcels%20organized%20in%20a%0Afour-tier%20crop%20taxonomy%2C%20H2Crop%20establishes%20a%20vital%20benchmark%20for%20fine-grained%0Aagricultural%20crop%20classification%20and%20hyperspectral%20image%20processing.%20We%20propose%0Aa%20dual-stream%20Transformer%20architecture%20that%20synergistically%20processes%20these%0Amodalities.%20It%20coordinates%20two%20specialized%20pathways%3A%20a%20spectral-spatial%0ATransformer%20extracts%20fine-grained%20signatures%20from%20hyperspectral%20EnMAP%20data%2C%0Awhile%20a%20temporal%20Swin%20Transformer%20extracts%20crop%20growth%20patterns%20from%20Sentinel-2%0Atime%20series.%20The%20designed%20hierarchical%20classification%20head%20with%20hierarchical%0Afusion%20then%20simultaneously%20delivers%20multi-level%20crop%20type%20classification%20across%0Aall%20taxonomic%20tiers.%20Experiments%20demonstrate%20that%20adding%20hyperspectral%20EnMAP%0Adata%20to%20Sentinel-2%20time%20series%20yields%20a%204.2%25%20average%20F1-scores%20improvement%0A%28peaking%20at%206.3%25%29.%20Extensive%20comparisons%20also%20confirm%20our%20method%27s%20higher%0Aaccuracy%20over%20existing%20deep%20learning%20approaches%20for%20crop%20type%20classification%0Aand%20the%20consistent%20benefits%20of%20hyperspectral%20data%20across%20varying%20temporal%0Awindows%20and%20crop%20change%20scenarios.%20Codes%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/flyakon/H2Crop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06155v2&entry.124074799=Read"},
{"title": "APE: Selective Fine-tuning with Acceptance Criteria for Language Model\n  Adaptation", "author": "Javier Mar\u00edn", "abstract": "  We present Adjacent Possible Exploration (APE), a selective fine-tuning\nmethod for adapting large language models that systematically explores\nparameter modifications while maintaining model stability. Inspired by\nevolutionary optimization principles, APE evaluates multiple candidate\nparameter updates through fine-tuning on small data subsets and accepts only\nthose exceeding a performance threshold. Unlike standard fine-tuning that\nfollows single gradient directions, APE implements a filtered selection process\nthat prevents destabilizing parameter changes while enabling systematic\nimprovement. Our method achieves 33.9\\% BLEU improvement and 36.2\\% perplexity\nreduction on news summarization tasks while using minimal computational\nresources. The approach provides a practical framework for controlled model\nadaptation that balances performance gains with representational stability.\n", "link": "http://arxiv.org/abs/2505.19912v2", "date": "2025-06-09", "relevancy": 2.4003, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APE%3A%20Selective%20Fine-tuning%20with%20Acceptance%20Criteria%20for%20Language%20Model%0A%20%20Adaptation&body=Title%3A%20APE%3A%20Selective%20Fine-tuning%20with%20Acceptance%20Criteria%20for%20Language%20Model%0A%20%20Adaptation%0AAuthor%3A%20Javier%20Mar%C3%ADn%0AAbstract%3A%20%20%20We%20present%20Adjacent%20Possible%20Exploration%20%28APE%29%2C%20a%20selective%20fine-tuning%0Amethod%20for%20adapting%20large%20language%20models%20that%20systematically%20explores%0Aparameter%20modifications%20while%20maintaining%20model%20stability.%20Inspired%20by%0Aevolutionary%20optimization%20principles%2C%20APE%20evaluates%20multiple%20candidate%0Aparameter%20updates%20through%20fine-tuning%20on%20small%20data%20subsets%20and%20accepts%20only%0Athose%20exceeding%20a%20performance%20threshold.%20Unlike%20standard%20fine-tuning%20that%0Afollows%20single%20gradient%20directions%2C%20APE%20implements%20a%20filtered%20selection%20process%0Athat%20prevents%20destabilizing%20parameter%20changes%20while%20enabling%20systematic%0Aimprovement.%20Our%20method%20achieves%2033.9%5C%25%20BLEU%20improvement%20and%2036.2%5C%25%20perplexity%0Areduction%20on%20news%20summarization%20tasks%20while%20using%20minimal%20computational%0Aresources.%20The%20approach%20provides%20a%20practical%20framework%20for%20controlled%20model%0Aadaptation%20that%20balances%20performance%20gains%20with%20representational%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPE%253A%2520Selective%2520Fine-tuning%2520with%2520Acceptance%2520Criteria%2520for%2520Language%2520Model%250A%2520%2520Adaptation%26entry.906535625%3DJavier%2520Mar%25C3%25ADn%26entry.1292438233%3D%2520%2520We%2520present%2520Adjacent%2520Possible%2520Exploration%2520%2528APE%2529%252C%2520a%2520selective%2520fine-tuning%250Amethod%2520for%2520adapting%2520large%2520language%2520models%2520that%2520systematically%2520explores%250Aparameter%2520modifications%2520while%2520maintaining%2520model%2520stability.%2520Inspired%2520by%250Aevolutionary%2520optimization%2520principles%252C%2520APE%2520evaluates%2520multiple%2520candidate%250Aparameter%2520updates%2520through%2520fine-tuning%2520on%2520small%2520data%2520subsets%2520and%2520accepts%2520only%250Athose%2520exceeding%2520a%2520performance%2520threshold.%2520Unlike%2520standard%2520fine-tuning%2520that%250Afollows%2520single%2520gradient%2520directions%252C%2520APE%2520implements%2520a%2520filtered%2520selection%2520process%250Athat%2520prevents%2520destabilizing%2520parameter%2520changes%2520while%2520enabling%2520systematic%250Aimprovement.%2520Our%2520method%2520achieves%252033.9%255C%2525%2520BLEU%2520improvement%2520and%252036.2%255C%2525%2520perplexity%250Areduction%2520on%2520news%2520summarization%2520tasks%2520while%2520using%2520minimal%2520computational%250Aresources.%2520The%2520approach%2520provides%2520a%2520practical%2520framework%2520for%2520controlled%2520model%250Aadaptation%2520that%2520balances%2520performance%2520gains%2520with%2520representational%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APE%3A%20Selective%20Fine-tuning%20with%20Acceptance%20Criteria%20for%20Language%20Model%0A%20%20Adaptation&entry.906535625=Javier%20Mar%C3%ADn&entry.1292438233=%20%20We%20present%20Adjacent%20Possible%20Exploration%20%28APE%29%2C%20a%20selective%20fine-tuning%0Amethod%20for%20adapting%20large%20language%20models%20that%20systematically%20explores%0Aparameter%20modifications%20while%20maintaining%20model%20stability.%20Inspired%20by%0Aevolutionary%20optimization%20principles%2C%20APE%20evaluates%20multiple%20candidate%0Aparameter%20updates%20through%20fine-tuning%20on%20small%20data%20subsets%20and%20accepts%20only%0Athose%20exceeding%20a%20performance%20threshold.%20Unlike%20standard%20fine-tuning%20that%0Afollows%20single%20gradient%20directions%2C%20APE%20implements%20a%20filtered%20selection%20process%0Athat%20prevents%20destabilizing%20parameter%20changes%20while%20enabling%20systematic%0Aimprovement.%20Our%20method%20achieves%2033.9%5C%25%20BLEU%20improvement%20and%2036.2%5C%25%20perplexity%0Areduction%20on%20news%20summarization%20tasks%20while%20using%20minimal%20computational%0Aresources.%20The%20approach%20provides%20a%20practical%20framework%20for%20controlled%20model%0Aadaptation%20that%20balances%20performance%20gains%20with%20representational%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19912v2&entry.124074799=Read"},
{"title": "Video Unlearning via Low-Rank Refusal Vector", "author": "Simone Facchiano and Stefano Saravalle and Matteo Migliarini and Edoardo De Matteis and Alessio Sampieri and Andrea Pilzer and Emanuele Rodol\u00e0 and Indro Spinelli and Luca Franco and Fabio Galasso", "abstract": "  Video generative models democratize the creation of visual content through\nintuitive instruction following, but they also inherit the biases and harmful\nconcepts embedded within their web-scale training data. This inheritance\ncreates a significant risk, as users can readily generate undesirable and even\nillegal content. This work introduces the first unlearning technique tailored\nexplicitly for video diffusion models to address this critical issue. Our\nmethod requires 5 multi-modal prompt pairs only. Each pair contains a \"safe\"\nand an \"unsafe\" example that differ only by the target concept. Averaging their\nper-layer latent differences produces a \"refusal vector\", which, once\nsubtracted from the model parameters, neutralizes the unsafe concept. We\nintroduce a novel low-rank factorization approach on the covariance difference\nof embeddings that yields robust refusal vectors. This isolates the target\nconcept while minimizing collateral unlearning of other semantics, thus\npreserving the visual quality of the generated video. Our method preserves the\nmodel's generation quality while operating without retraining or access to the\noriginal training data. By embedding the refusal direction directly into the\nmodel's weights, the suppression mechanism becomes inherently more robust\nagainst adversarial bypass attempts compared to surface-level input-output\nfilters. In a thorough qualitative and quantitative evaluation, we show that we\ncan neutralize a variety of harmful contents, including explicit nudity,\ngraphic violence, copyrights, and trademarks. Project page:\nhttps://www.pinlab.org/video-unlearning.\n", "link": "http://arxiv.org/abs/2506.07891v1", "date": "2025-06-09", "relevancy": 2.3972, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6076}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6026}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Unlearning%20via%20Low-Rank%20Refusal%20Vector&body=Title%3A%20Video%20Unlearning%20via%20Low-Rank%20Refusal%20Vector%0AAuthor%3A%20Simone%20Facchiano%20and%20Stefano%20Saravalle%20and%20Matteo%20Migliarini%20and%20Edoardo%20De%20Matteis%20and%20Alessio%20Sampieri%20and%20Andrea%20Pilzer%20and%20Emanuele%20Rodol%C3%A0%20and%20Indro%20Spinelli%20and%20Luca%20Franco%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20Video%20generative%20models%20democratize%20the%20creation%20of%20visual%20content%20through%0Aintuitive%20instruction%20following%2C%20but%20they%20also%20inherit%20the%20biases%20and%20harmful%0Aconcepts%20embedded%20within%20their%20web-scale%20training%20data.%20This%20inheritance%0Acreates%20a%20significant%20risk%2C%20as%20users%20can%20readily%20generate%20undesirable%20and%20even%0Aillegal%20content.%20This%20work%20introduces%20the%20first%20unlearning%20technique%20tailored%0Aexplicitly%20for%20video%20diffusion%20models%20to%20address%20this%20critical%20issue.%20Our%0Amethod%20requires%205%20multi-modal%20prompt%20pairs%20only.%20Each%20pair%20contains%20a%20%22safe%22%0Aand%20an%20%22unsafe%22%20example%20that%20differ%20only%20by%20the%20target%20concept.%20Averaging%20their%0Aper-layer%20latent%20differences%20produces%20a%20%22refusal%20vector%22%2C%20which%2C%20once%0Asubtracted%20from%20the%20model%20parameters%2C%20neutralizes%20the%20unsafe%20concept.%20We%0Aintroduce%20a%20novel%20low-rank%20factorization%20approach%20on%20the%20covariance%20difference%0Aof%20embeddings%20that%20yields%20robust%20refusal%20vectors.%20This%20isolates%20the%20target%0Aconcept%20while%20minimizing%20collateral%20unlearning%20of%20other%20semantics%2C%20thus%0Apreserving%20the%20visual%20quality%20of%20the%20generated%20video.%20Our%20method%20preserves%20the%0Amodel%27s%20generation%20quality%20while%20operating%20without%20retraining%20or%20access%20to%20the%0Aoriginal%20training%20data.%20By%20embedding%20the%20refusal%20direction%20directly%20into%20the%0Amodel%27s%20weights%2C%20the%20suppression%20mechanism%20becomes%20inherently%20more%20robust%0Aagainst%20adversarial%20bypass%20attempts%20compared%20to%20surface-level%20input-output%0Afilters.%20In%20a%20thorough%20qualitative%20and%20quantitative%20evaluation%2C%20we%20show%20that%20we%0Acan%20neutralize%20a%20variety%20of%20harmful%20contents%2C%20including%20explicit%20nudity%2C%0Agraphic%20violence%2C%20copyrights%2C%20and%20trademarks.%20Project%20page%3A%0Ahttps%3A//www.pinlab.org/video-unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Unlearning%2520via%2520Low-Rank%2520Refusal%2520Vector%26entry.906535625%3DSimone%2520Facchiano%2520and%2520Stefano%2520Saravalle%2520and%2520Matteo%2520Migliarini%2520and%2520Edoardo%2520De%2520Matteis%2520and%2520Alessio%2520Sampieri%2520and%2520Andrea%2520Pilzer%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Indro%2520Spinelli%2520and%2520Luca%2520Franco%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520Video%2520generative%2520models%2520democratize%2520the%2520creation%2520of%2520visual%2520content%2520through%250Aintuitive%2520instruction%2520following%252C%2520but%2520they%2520also%2520inherit%2520the%2520biases%2520and%2520harmful%250Aconcepts%2520embedded%2520within%2520their%2520web-scale%2520training%2520data.%2520This%2520inheritance%250Acreates%2520a%2520significant%2520risk%252C%2520as%2520users%2520can%2520readily%2520generate%2520undesirable%2520and%2520even%250Aillegal%2520content.%2520This%2520work%2520introduces%2520the%2520first%2520unlearning%2520technique%2520tailored%250Aexplicitly%2520for%2520video%2520diffusion%2520models%2520to%2520address%2520this%2520critical%2520issue.%2520Our%250Amethod%2520requires%25205%2520multi-modal%2520prompt%2520pairs%2520only.%2520Each%2520pair%2520contains%2520a%2520%2522safe%2522%250Aand%2520an%2520%2522unsafe%2522%2520example%2520that%2520differ%2520only%2520by%2520the%2520target%2520concept.%2520Averaging%2520their%250Aper-layer%2520latent%2520differences%2520produces%2520a%2520%2522refusal%2520vector%2522%252C%2520which%252C%2520once%250Asubtracted%2520from%2520the%2520model%2520parameters%252C%2520neutralizes%2520the%2520unsafe%2520concept.%2520We%250Aintroduce%2520a%2520novel%2520low-rank%2520factorization%2520approach%2520on%2520the%2520covariance%2520difference%250Aof%2520embeddings%2520that%2520yields%2520robust%2520refusal%2520vectors.%2520This%2520isolates%2520the%2520target%250Aconcept%2520while%2520minimizing%2520collateral%2520unlearning%2520of%2520other%2520semantics%252C%2520thus%250Apreserving%2520the%2520visual%2520quality%2520of%2520the%2520generated%2520video.%2520Our%2520method%2520preserves%2520the%250Amodel%2527s%2520generation%2520quality%2520while%2520operating%2520without%2520retraining%2520or%2520access%2520to%2520the%250Aoriginal%2520training%2520data.%2520By%2520embedding%2520the%2520refusal%2520direction%2520directly%2520into%2520the%250Amodel%2527s%2520weights%252C%2520the%2520suppression%2520mechanism%2520becomes%2520inherently%2520more%2520robust%250Aagainst%2520adversarial%2520bypass%2520attempts%2520compared%2520to%2520surface-level%2520input-output%250Afilters.%2520In%2520a%2520thorough%2520qualitative%2520and%2520quantitative%2520evaluation%252C%2520we%2520show%2520that%2520we%250Acan%2520neutralize%2520a%2520variety%2520of%2520harmful%2520contents%252C%2520including%2520explicit%2520nudity%252C%250Agraphic%2520violence%252C%2520copyrights%252C%2520and%2520trademarks.%2520Project%2520page%253A%250Ahttps%253A//www.pinlab.org/video-unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Unlearning%20via%20Low-Rank%20Refusal%20Vector&entry.906535625=Simone%20Facchiano%20and%20Stefano%20Saravalle%20and%20Matteo%20Migliarini%20and%20Edoardo%20De%20Matteis%20and%20Alessio%20Sampieri%20and%20Andrea%20Pilzer%20and%20Emanuele%20Rodol%C3%A0%20and%20Indro%20Spinelli%20and%20Luca%20Franco%20and%20Fabio%20Galasso&entry.1292438233=%20%20Video%20generative%20models%20democratize%20the%20creation%20of%20visual%20content%20through%0Aintuitive%20instruction%20following%2C%20but%20they%20also%20inherit%20the%20biases%20and%20harmful%0Aconcepts%20embedded%20within%20their%20web-scale%20training%20data.%20This%20inheritance%0Acreates%20a%20significant%20risk%2C%20as%20users%20can%20readily%20generate%20undesirable%20and%20even%0Aillegal%20content.%20This%20work%20introduces%20the%20first%20unlearning%20technique%20tailored%0Aexplicitly%20for%20video%20diffusion%20models%20to%20address%20this%20critical%20issue.%20Our%0Amethod%20requires%205%20multi-modal%20prompt%20pairs%20only.%20Each%20pair%20contains%20a%20%22safe%22%0Aand%20an%20%22unsafe%22%20example%20that%20differ%20only%20by%20the%20target%20concept.%20Averaging%20their%0Aper-layer%20latent%20differences%20produces%20a%20%22refusal%20vector%22%2C%20which%2C%20once%0Asubtracted%20from%20the%20model%20parameters%2C%20neutralizes%20the%20unsafe%20concept.%20We%0Aintroduce%20a%20novel%20low-rank%20factorization%20approach%20on%20the%20covariance%20difference%0Aof%20embeddings%20that%20yields%20robust%20refusal%20vectors.%20This%20isolates%20the%20target%0Aconcept%20while%20minimizing%20collateral%20unlearning%20of%20other%20semantics%2C%20thus%0Apreserving%20the%20visual%20quality%20of%20the%20generated%20video.%20Our%20method%20preserves%20the%0Amodel%27s%20generation%20quality%20while%20operating%20without%20retraining%20or%20access%20to%20the%0Aoriginal%20training%20data.%20By%20embedding%20the%20refusal%20direction%20directly%20into%20the%0Amodel%27s%20weights%2C%20the%20suppression%20mechanism%20becomes%20inherently%20more%20robust%0Aagainst%20adversarial%20bypass%20attempts%20compared%20to%20surface-level%20input-output%0Afilters.%20In%20a%20thorough%20qualitative%20and%20quantitative%20evaluation%2C%20we%20show%20that%20we%0Acan%20neutralize%20a%20variety%20of%20harmful%20contents%2C%20including%20explicit%20nudity%2C%0Agraphic%20violence%2C%20copyrights%2C%20and%20trademarks.%20Project%20page%3A%0Ahttps%3A//www.pinlab.org/video-unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07891v1&entry.124074799=Read"},
{"title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs", "author": "Harsh Bihany and Shubham Patel and Ashutosh Modi", "abstract": "  Large Language Models have shown remarkable capabilities in the NLP domain.\nTheir effectiveness can mainly be attributed to their ability to adapt to an\narray of downstream tasks. However, generally, full fine-tuning is a\ncomputationally expensive job. To mitigate this, many techniques have been\ndeveloped that prime efficiency, a prominent one being Low-Rank Adaptation\n(LoRA). However, LoRA and its variants employ re-parametrized additive updates.\nIn this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which\nshifts the paradigm of additive updates to a richer space of matrix\nmultiplicative transformations. We tackle challenges such as computational\ncomplexity and rank bottleneck of matrix multiplication by effectively\nre-ordering operations and introducing rank inflation strategies. We conduct\nextensive experiments to demonstrate the effectiveness of our approach in terms\nof various evaluation metrics.\n", "link": "http://arxiv.org/abs/2506.07621v1", "date": "2025-06-09", "relevancy": 2.3868, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRMA%3A%20Low-Rank%20Multiplicative%20Adaptation%20for%20LLMs&body=Title%3A%20LoRMA%3A%20Low-Rank%20Multiplicative%20Adaptation%20for%20LLMs%0AAuthor%3A%20Harsh%20Bihany%20and%20Shubham%20Patel%20and%20Ashutosh%20Modi%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20shown%20remarkable%20capabilities%20in%20the%20NLP%20domain.%0ATheir%20effectiveness%20can%20mainly%20be%20attributed%20to%20their%20ability%20to%20adapt%20to%20an%0Aarray%20of%20downstream%20tasks.%20However%2C%20generally%2C%20full%20fine-tuning%20is%20a%0Acomputationally%20expensive%20job.%20To%20mitigate%20this%2C%20many%20techniques%20have%20been%0Adeveloped%20that%20prime%20efficiency%2C%20a%20prominent%20one%20being%20Low-Rank%20Adaptation%0A%28LoRA%29.%20However%2C%20LoRA%20and%20its%20variants%20employ%20re-parametrized%20additive%20updates.%0AIn%20this%20paper%2C%20we%20propose%20Low-Rank%20Multiplicative%20Adaptation%20%28LoRMA%29%2C%20which%0Ashifts%20the%20paradigm%20of%20additive%20updates%20to%20a%20richer%20space%20of%20matrix%0Amultiplicative%20transformations.%20We%20tackle%20challenges%20such%20as%20computational%0Acomplexity%20and%20rank%20bottleneck%20of%20matrix%20multiplication%20by%20effectively%0Are-ordering%20operations%20and%20introducing%20rank%20inflation%20strategies.%20We%20conduct%0Aextensive%20experiments%20to%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20terms%0Aof%20various%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRMA%253A%2520Low-Rank%2520Multiplicative%2520Adaptation%2520for%2520LLMs%26entry.906535625%3DHarsh%2520Bihany%2520and%2520Shubham%2520Patel%2520and%2520Ashutosh%2520Modi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520the%2520NLP%2520domain.%250ATheir%2520effectiveness%2520can%2520mainly%2520be%2520attributed%2520to%2520their%2520ability%2520to%2520adapt%2520to%2520an%250Aarray%2520of%2520downstream%2520tasks.%2520However%252C%2520generally%252C%2520full%2520fine-tuning%2520is%2520a%250Acomputationally%2520expensive%2520job.%2520To%2520mitigate%2520this%252C%2520many%2520techniques%2520have%2520been%250Adeveloped%2520that%2520prime%2520efficiency%252C%2520a%2520prominent%2520one%2520being%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529.%2520However%252C%2520LoRA%2520and%2520its%2520variants%2520employ%2520re-parametrized%2520additive%2520updates.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520Low-Rank%2520Multiplicative%2520Adaptation%2520%2528LoRMA%2529%252C%2520which%250Ashifts%2520the%2520paradigm%2520of%2520additive%2520updates%2520to%2520a%2520richer%2520space%2520of%2520matrix%250Amultiplicative%2520transformations.%2520We%2520tackle%2520challenges%2520such%2520as%2520computational%250Acomplexity%2520and%2520rank%2520bottleneck%2520of%2520matrix%2520multiplication%2520by%2520effectively%250Are-ordering%2520operations%2520and%2520introducing%2520rank%2520inflation%2520strategies.%2520We%2520conduct%250Aextensive%2520experiments%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520terms%250Aof%2520various%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRMA%3A%20Low-Rank%20Multiplicative%20Adaptation%20for%20LLMs&entry.906535625=Harsh%20Bihany%20and%20Shubham%20Patel%20and%20Ashutosh%20Modi&entry.1292438233=%20%20Large%20Language%20Models%20have%20shown%20remarkable%20capabilities%20in%20the%20NLP%20domain.%0ATheir%20effectiveness%20can%20mainly%20be%20attributed%20to%20their%20ability%20to%20adapt%20to%20an%0Aarray%20of%20downstream%20tasks.%20However%2C%20generally%2C%20full%20fine-tuning%20is%20a%0Acomputationally%20expensive%20job.%20To%20mitigate%20this%2C%20many%20techniques%20have%20been%0Adeveloped%20that%20prime%20efficiency%2C%20a%20prominent%20one%20being%20Low-Rank%20Adaptation%0A%28LoRA%29.%20However%2C%20LoRA%20and%20its%20variants%20employ%20re-parametrized%20additive%20updates.%0AIn%20this%20paper%2C%20we%20propose%20Low-Rank%20Multiplicative%20Adaptation%20%28LoRMA%29%2C%20which%0Ashifts%20the%20paradigm%20of%20additive%20updates%20to%20a%20richer%20space%20of%20matrix%0Amultiplicative%20transformations.%20We%20tackle%20challenges%20such%20as%20computational%0Acomplexity%20and%20rank%20bottleneck%20of%20matrix%20multiplication%20by%20effectively%0Are-ordering%20operations%20and%20introducing%20rank%20inflation%20strategies.%20We%20conduct%0Aextensive%20experiments%20to%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20terms%0Aof%20various%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07621v1&entry.124074799=Read"},
{"title": "Audio-Sync Video Generation with Multi-Stream Temporal Control", "author": "Shuchen Weng and Haojie Zheng and Zheng Chang and Si Li and Boxin Shi and Xinlong Wang", "abstract": "  Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.\n", "link": "http://arxiv.org/abs/2506.08003v1", "date": "2025-06-09", "relevancy": 2.3807, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6011}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5985}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Sync%20Video%20Generation%20with%20Multi-Stream%20Temporal%20Control&body=Title%3A%20Audio-Sync%20Video%20Generation%20with%20Multi-Stream%20Temporal%20Control%0AAuthor%3A%20Shuchen%20Weng%20and%20Haojie%20Zheng%20and%20Zheng%20Chang%20and%20Si%20Li%20and%20Boxin%20Shi%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20Audio%20is%20inherently%20temporal%20and%20closely%20synchronized%20with%20the%20visual%20world%2C%0Amaking%20it%20a%20naturally%20aligned%20and%20expressive%20control%20signal%20for%20controllable%0Avideo%20generation%20%28e.g.%2C%20movies%29.%20Beyond%20control%2C%20directly%20translating%20audio%0Ainto%20video%20is%20essential%20for%20understanding%20and%20visualizing%20rich%20audio%20narratives%0A%28e.g.%2C%20Podcasts%20or%20historical%20recordings%29.%20However%2C%20existing%20approaches%20fall%0Ashort%20in%20generating%20high-quality%20videos%20with%20precise%20audio-visual%0Asynchronization%2C%20especially%20across%20diverse%20and%20complex%20audio%20types.%20In%20this%0Awork%2C%20we%20introduce%20MTV%2C%20a%20versatile%20framework%20for%20audio-sync%20video%20generation.%0AMTV%20explicitly%20separates%20audios%20into%20speech%2C%20effects%2C%20and%20music%20tracks%2C%0Aenabling%20disentangled%20control%20over%20lip%20motion%2C%20event%20timing%2C%20and%20visual%20mood%2C%0Arespectively%20--%20resulting%20in%20fine-grained%20and%20semantically%20aligned%20video%0Ageneration.%20To%20support%20the%20framework%2C%20we%20additionally%20present%20DEMIX%2C%20a%20dataset%0Acomprising%20high-quality%20cinematic%20videos%20and%20demixed%20audio%20tracks.%20DEMIX%20is%0Astructured%20into%20five%20overlapped%20subsets%2C%20enabling%20scalable%20multi-stage%20training%0Afor%20diverse%20generation%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20MTV%0Aachieves%20state-of-the-art%20performance%20across%20six%20standard%20metrics%20spanning%0Avideo%20quality%2C%20text-video%20consistency%2C%20and%20audio-video%20alignment.%20Project%20page%3A%0Ahttps%3A//hjzheng.net/projects/MTV/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Sync%2520Video%2520Generation%2520with%2520Multi-Stream%2520Temporal%2520Control%26entry.906535625%3DShuchen%2520Weng%2520and%2520Haojie%2520Zheng%2520and%2520Zheng%2520Chang%2520and%2520Si%2520Li%2520and%2520Boxin%2520Shi%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520Audio%2520is%2520inherently%2520temporal%2520and%2520closely%2520synchronized%2520with%2520the%2520visual%2520world%252C%250Amaking%2520it%2520a%2520naturally%2520aligned%2520and%2520expressive%2520control%2520signal%2520for%2520controllable%250Avideo%2520generation%2520%2528e.g.%252C%2520movies%2529.%2520Beyond%2520control%252C%2520directly%2520translating%2520audio%250Ainto%2520video%2520is%2520essential%2520for%2520understanding%2520and%2520visualizing%2520rich%2520audio%2520narratives%250A%2528e.g.%252C%2520Podcasts%2520or%2520historical%2520recordings%2529.%2520However%252C%2520existing%2520approaches%2520fall%250Ashort%2520in%2520generating%2520high-quality%2520videos%2520with%2520precise%2520audio-visual%250Asynchronization%252C%2520especially%2520across%2520diverse%2520and%2520complex%2520audio%2520types.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520MTV%252C%2520a%2520versatile%2520framework%2520for%2520audio-sync%2520video%2520generation.%250AMTV%2520explicitly%2520separates%2520audios%2520into%2520speech%252C%2520effects%252C%2520and%2520music%2520tracks%252C%250Aenabling%2520disentangled%2520control%2520over%2520lip%2520motion%252C%2520event%2520timing%252C%2520and%2520visual%2520mood%252C%250Arespectively%2520--%2520resulting%2520in%2520fine-grained%2520and%2520semantically%2520aligned%2520video%250Ageneration.%2520To%2520support%2520the%2520framework%252C%2520we%2520additionally%2520present%2520DEMIX%252C%2520a%2520dataset%250Acomprising%2520high-quality%2520cinematic%2520videos%2520and%2520demixed%2520audio%2520tracks.%2520DEMIX%2520is%250Astructured%2520into%2520five%2520overlapped%2520subsets%252C%2520enabling%2520scalable%2520multi-stage%2520training%250Afor%2520diverse%2520generation%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MTV%250Aachieves%2520state-of-the-art%2520performance%2520across%2520six%2520standard%2520metrics%2520spanning%250Avideo%2520quality%252C%2520text-video%2520consistency%252C%2520and%2520audio-video%2520alignment.%2520Project%2520page%253A%250Ahttps%253A//hjzheng.net/projects/MTV/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Sync%20Video%20Generation%20with%20Multi-Stream%20Temporal%20Control&entry.906535625=Shuchen%20Weng%20and%20Haojie%20Zheng%20and%20Zheng%20Chang%20and%20Si%20Li%20and%20Boxin%20Shi%20and%20Xinlong%20Wang&entry.1292438233=%20%20Audio%20is%20inherently%20temporal%20and%20closely%20synchronized%20with%20the%20visual%20world%2C%0Amaking%20it%20a%20naturally%20aligned%20and%20expressive%20control%20signal%20for%20controllable%0Avideo%20generation%20%28e.g.%2C%20movies%29.%20Beyond%20control%2C%20directly%20translating%20audio%0Ainto%20video%20is%20essential%20for%20understanding%20and%20visualizing%20rich%20audio%20narratives%0A%28e.g.%2C%20Podcasts%20or%20historical%20recordings%29.%20However%2C%20existing%20approaches%20fall%0Ashort%20in%20generating%20high-quality%20videos%20with%20precise%20audio-visual%0Asynchronization%2C%20especially%20across%20diverse%20and%20complex%20audio%20types.%20In%20this%0Awork%2C%20we%20introduce%20MTV%2C%20a%20versatile%20framework%20for%20audio-sync%20video%20generation.%0AMTV%20explicitly%20separates%20audios%20into%20speech%2C%20effects%2C%20and%20music%20tracks%2C%0Aenabling%20disentangled%20control%20over%20lip%20motion%2C%20event%20timing%2C%20and%20visual%20mood%2C%0Arespectively%20--%20resulting%20in%20fine-grained%20and%20semantically%20aligned%20video%0Ageneration.%20To%20support%20the%20framework%2C%20we%20additionally%20present%20DEMIX%2C%20a%20dataset%0Acomprising%20high-quality%20cinematic%20videos%20and%20demixed%20audio%20tracks.%20DEMIX%20is%0Astructured%20into%20five%20overlapped%20subsets%2C%20enabling%20scalable%20multi-stage%20training%0Afor%20diverse%20generation%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20MTV%0Aachieves%20state-of-the-art%20performance%20across%20six%20standard%20metrics%20spanning%0Avideo%20quality%2C%20text-video%20consistency%2C%20and%20audio-video%20alignment.%20Project%20page%3A%0Ahttps%3A//hjzheng.net/projects/MTV/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08003v1&entry.124074799=Read"},
{"title": "Adversaries With Incentives: A Strategic Alternative to Adversarial\n  Robustness", "author": "Maayan Ehrenberg and Roy Ganz and Nir Rosenfeld", "abstract": "  Adversarial training aims to defend against adversaries: malicious opponents\nwhose sole aim is to harm predictive performance in any way possible. This\npresents a rather harsh perspective, which we assert results in unnecessarily\nconservative training. As an alternative, we propose to model opponents as\nsimply pursuing their own goals--rather than working directly against the\nclassifier. Employing tools from strategic modeling, our approach enables\nknowledge or beliefs regarding the opponent's possible incentives to be used as\ninductive bias for learning. Accordingly, our method of strategic training is\ndesigned to defend against all opponents within an 'incentive uncertainty set'.\nThis resorts to adversarial learning when the set is maximal, but offers\npotential gains when the set can be appropriately reduced. We conduct a series\nof experiments that show how even mild knowledge regarding the opponent's\nincentives can be useful, and that the degree of potential gains depends on how\nthese incentives relate to the structure of the learning task.\n", "link": "http://arxiv.org/abs/2406.11458v3", "date": "2025-06-09", "relevancy": 2.3641, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4868}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4684}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness&body=Title%3A%20Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness%0AAuthor%3A%20Maayan%20Ehrenberg%20and%20Roy%20Ganz%20and%20Nir%20Rosenfeld%0AAbstract%3A%20%20%20Adversarial%20training%20aims%20to%20defend%20against%20adversaries%3A%20malicious%20opponents%0Awhose%20sole%20aim%20is%20to%20harm%20predictive%20performance%20in%20any%20way%20possible.%20This%0Apresents%20a%20rather%20harsh%20perspective%2C%20which%20we%20assert%20results%20in%20unnecessarily%0Aconservative%20training.%20As%20an%20alternative%2C%20we%20propose%20to%20model%20opponents%20as%0Asimply%20pursuing%20their%20own%20goals--rather%20than%20working%20directly%20against%20the%0Aclassifier.%20Employing%20tools%20from%20strategic%20modeling%2C%20our%20approach%20enables%0Aknowledge%20or%20beliefs%20regarding%20the%20opponent%27s%20possible%20incentives%20to%20be%20used%20as%0Ainductive%20bias%20for%20learning.%20Accordingly%2C%20our%20method%20of%20strategic%20training%20is%0Adesigned%20to%20defend%20against%20all%20opponents%20within%20an%20%27incentive%20uncertainty%20set%27.%0AThis%20resorts%20to%20adversarial%20learning%20when%20the%20set%20is%20maximal%2C%20but%20offers%0Apotential%20gains%20when%20the%20set%20can%20be%20appropriately%20reduced.%20We%20conduct%20a%20series%0Aof%20experiments%20that%20show%20how%20even%20mild%20knowledge%20regarding%20the%20opponent%27s%0Aincentives%20can%20be%20useful%2C%20and%20that%20the%20degree%20of%20potential%20gains%20depends%20on%20how%0Athese%20incentives%20relate%20to%20the%20structure%20of%20the%20learning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11458v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversaries%2520With%2520Incentives%253A%2520A%2520Strategic%2520Alternative%2520to%2520Adversarial%250A%2520%2520Robustness%26entry.906535625%3DMaayan%2520Ehrenberg%2520and%2520Roy%2520Ganz%2520and%2520Nir%2520Rosenfeld%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520aims%2520to%2520defend%2520against%2520adversaries%253A%2520malicious%2520opponents%250Awhose%2520sole%2520aim%2520is%2520to%2520harm%2520predictive%2520performance%2520in%2520any%2520way%2520possible.%2520This%250Apresents%2520a%2520rather%2520harsh%2520perspective%252C%2520which%2520we%2520assert%2520results%2520in%2520unnecessarily%250Aconservative%2520training.%2520As%2520an%2520alternative%252C%2520we%2520propose%2520to%2520model%2520opponents%2520as%250Asimply%2520pursuing%2520their%2520own%2520goals--rather%2520than%2520working%2520directly%2520against%2520the%250Aclassifier.%2520Employing%2520tools%2520from%2520strategic%2520modeling%252C%2520our%2520approach%2520enables%250Aknowledge%2520or%2520beliefs%2520regarding%2520the%2520opponent%2527s%2520possible%2520incentives%2520to%2520be%2520used%2520as%250Ainductive%2520bias%2520for%2520learning.%2520Accordingly%252C%2520our%2520method%2520of%2520strategic%2520training%2520is%250Adesigned%2520to%2520defend%2520against%2520all%2520opponents%2520within%2520an%2520%2527incentive%2520uncertainty%2520set%2527.%250AThis%2520resorts%2520to%2520adversarial%2520learning%2520when%2520the%2520set%2520is%2520maximal%252C%2520but%2520offers%250Apotential%2520gains%2520when%2520the%2520set%2520can%2520be%2520appropriately%2520reduced.%2520We%2520conduct%2520a%2520series%250Aof%2520experiments%2520that%2520show%2520how%2520even%2520mild%2520knowledge%2520regarding%2520the%2520opponent%2527s%250Aincentives%2520can%2520be%2520useful%252C%2520and%2520that%2520the%2520degree%2520of%2520potential%2520gains%2520depends%2520on%2520how%250Athese%2520incentives%2520relate%2520to%2520the%2520structure%2520of%2520the%2520learning%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11458v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness&entry.906535625=Maayan%20Ehrenberg%20and%20Roy%20Ganz%20and%20Nir%20Rosenfeld&entry.1292438233=%20%20Adversarial%20training%20aims%20to%20defend%20against%20adversaries%3A%20malicious%20opponents%0Awhose%20sole%20aim%20is%20to%20harm%20predictive%20performance%20in%20any%20way%20possible.%20This%0Apresents%20a%20rather%20harsh%20perspective%2C%20which%20we%20assert%20results%20in%20unnecessarily%0Aconservative%20training.%20As%20an%20alternative%2C%20we%20propose%20to%20model%20opponents%20as%0Asimply%20pursuing%20their%20own%20goals--rather%20than%20working%20directly%20against%20the%0Aclassifier.%20Employing%20tools%20from%20strategic%20modeling%2C%20our%20approach%20enables%0Aknowledge%20or%20beliefs%20regarding%20the%20opponent%27s%20possible%20incentives%20to%20be%20used%20as%0Ainductive%20bias%20for%20learning.%20Accordingly%2C%20our%20method%20of%20strategic%20training%20is%0Adesigned%20to%20defend%20against%20all%20opponents%20within%20an%20%27incentive%20uncertainty%20set%27.%0AThis%20resorts%20to%20adversarial%20learning%20when%20the%20set%20is%20maximal%2C%20but%20offers%0Apotential%20gains%20when%20the%20set%20can%20be%20appropriately%20reduced.%20We%20conduct%20a%20series%0Aof%20experiments%20that%20show%20how%20even%20mild%20knowledge%20regarding%20the%20opponent%27s%0Aincentives%20can%20be%20useful%2C%20and%20that%20the%20degree%20of%20potential%20gains%20depends%20on%20how%0Athese%20incentives%20relate%20to%20the%20structure%20of%20the%20learning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11458v3&entry.124074799=Read"},
{"title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding", "author": "Jiahao Meng and Shuyang Sun and Yue Tan and Lu Qi and Yunhai Tong and Xiangtai Li and Longyin Wen", "abstract": "  Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.\n", "link": "http://arxiv.org/abs/2506.07971v1", "date": "2025-06-09", "relevancy": 2.3639, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CyberV%3A%20Cybernetics%20for%20Test-time%20Scaling%20in%20Video%20Understanding&body=Title%3A%20CyberV%3A%20Cybernetics%20for%20Test-time%20Scaling%20in%20Video%20Understanding%0AAuthor%3A%20Jiahao%20Meng%20and%20Shuyang%20Sun%20and%20Yue%20Tan%20and%20Lu%20Qi%20and%20Yunhai%20Tong%20and%20Xiangtai%20Li%20and%20Longyin%20Wen%0AAbstract%3A%20%20%20Current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20may%20struggle%20with%0Aunderstanding%20long%20or%20complex%20videos%20due%20to%20computational%20demands%20at%20test%20time%2C%0Alack%20of%20robustness%2C%20and%20limited%20accuracy%2C%20primarily%20stemming%20from%20their%0Afeed-forward%20processing%20nature.%20These%20limitations%20could%20be%20more%20severe%20for%0Amodels%20with%20fewer%20parameters.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Aframework%20inspired%20by%20cybernetic%20principles%2C%20redesigning%20video%20MLLMs%20as%0Aadaptive%20systems%20capable%20of%20self-monitoring%2C%20self-correction%2C%20and%20dynamic%0Aresource%20allocation%20during%20inference.%20Our%20approach%2C%20CyberV%2C%20introduces%20a%0Acybernetic%20loop%20consisting%20of%20an%20MLLM%20Inference%20System%2C%20a%20Sensor%2C%20and%20a%0AController.%20Specifically%2C%20the%20sensor%20monitors%20forward%20processes%20of%20the%20MLLM%20and%0Acollects%20intermediate%20interpretations%2C%20such%20as%20attention%20drift%2C%20then%20the%0Acontroller%20determines%20when%20and%20how%20to%20trigger%20self-correction%20and%20generate%0Afeedback%20to%20guide%20the%20next%20round.%20This%20test-time%20adaptive%20scaling%20framework%0Aenhances%20frozen%20MLLMs%20without%20requiring%20retraining%20or%20additional%20components.%0AExperiments%20demonstrate%20significant%20improvements%3A%20CyberV%20boosts%20Qwen2.5-VL-7B%0Aby%208.3%25%20and%20InternVL3-8B%20by%205.5%25%20on%20VideoMMMU%2C%20surpassing%20the%20competitive%0Aproprietary%20model%20GPT-4o.%20When%20applied%20to%20Qwen2.5-VL-72B%2C%20it%20yields%20a%2010.0%25%0Aimprovement%2C%20achieving%20performance%20even%20comparable%20to%20human%20experts.%0AFurthermore%2C%20our%20method%20demonstrates%20consistent%20gains%20on%20general-purpose%0Abenchmarks%2C%20such%20as%20VideoMME%20and%20WorldSense%2C%20highlighting%20its%20effectiveness%20and%0Ageneralization%20capabilities%20in%20making%20MLLMs%20more%20robust%20and%20accurate%20for%0Adynamic%20video%20understanding.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/marinero4972/CyberV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyberV%253A%2520Cybernetics%2520for%2520Test-time%2520Scaling%2520in%2520Video%2520Understanding%26entry.906535625%3DJiahao%2520Meng%2520and%2520Shuyang%2520Sun%2520and%2520Yue%2520Tan%2520and%2520Lu%2520Qi%2520and%2520Yunhai%2520Tong%2520and%2520Xiangtai%2520Li%2520and%2520Longyin%2520Wen%26entry.1292438233%3D%2520%2520Current%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520may%2520struggle%2520with%250Aunderstanding%2520long%2520or%2520complex%2520videos%2520due%2520to%2520computational%2520demands%2520at%2520test%2520time%252C%250Alack%2520of%2520robustness%252C%2520and%2520limited%2520accuracy%252C%2520primarily%2520stemming%2520from%2520their%250Afeed-forward%2520processing%2520nature.%2520These%2520limitations%2520could%2520be%2520more%2520severe%2520for%250Amodels%2520with%2520fewer%2520parameters.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520inspired%2520by%2520cybernetic%2520principles%252C%2520redesigning%2520video%2520MLLMs%2520as%250Aadaptive%2520systems%2520capable%2520of%2520self-monitoring%252C%2520self-correction%252C%2520and%2520dynamic%250Aresource%2520allocation%2520during%2520inference.%2520Our%2520approach%252C%2520CyberV%252C%2520introduces%2520a%250Acybernetic%2520loop%2520consisting%2520of%2520an%2520MLLM%2520Inference%2520System%252C%2520a%2520Sensor%252C%2520and%2520a%250AController.%2520Specifically%252C%2520the%2520sensor%2520monitors%2520forward%2520processes%2520of%2520the%2520MLLM%2520and%250Acollects%2520intermediate%2520interpretations%252C%2520such%2520as%2520attention%2520drift%252C%2520then%2520the%250Acontroller%2520determines%2520when%2520and%2520how%2520to%2520trigger%2520self-correction%2520and%2520generate%250Afeedback%2520to%2520guide%2520the%2520next%2520round.%2520This%2520test-time%2520adaptive%2520scaling%2520framework%250Aenhances%2520frozen%2520MLLMs%2520without%2520requiring%2520retraining%2520or%2520additional%2520components.%250AExperiments%2520demonstrate%2520significant%2520improvements%253A%2520CyberV%2520boosts%2520Qwen2.5-VL-7B%250Aby%25208.3%2525%2520and%2520InternVL3-8B%2520by%25205.5%2525%2520on%2520VideoMMMU%252C%2520surpassing%2520the%2520competitive%250Aproprietary%2520model%2520GPT-4o.%2520When%2520applied%2520to%2520Qwen2.5-VL-72B%252C%2520it%2520yields%2520a%252010.0%2525%250Aimprovement%252C%2520achieving%2520performance%2520even%2520comparable%2520to%2520human%2520experts.%250AFurthermore%252C%2520our%2520method%2520demonstrates%2520consistent%2520gains%2520on%2520general-purpose%250Abenchmarks%252C%2520such%2520as%2520VideoMME%2520and%2520WorldSense%252C%2520highlighting%2520its%2520effectiveness%2520and%250Ageneralization%2520capabilities%2520in%2520making%2520MLLMs%2520more%2520robust%2520and%2520accurate%2520for%250Adynamic%2520video%2520understanding.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/marinero4972/CyberV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CyberV%3A%20Cybernetics%20for%20Test-time%20Scaling%20in%20Video%20Understanding&entry.906535625=Jiahao%20Meng%20and%20Shuyang%20Sun%20and%20Yue%20Tan%20and%20Lu%20Qi%20and%20Yunhai%20Tong%20and%20Xiangtai%20Li%20and%20Longyin%20Wen&entry.1292438233=%20%20Current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20may%20struggle%20with%0Aunderstanding%20long%20or%20complex%20videos%20due%20to%20computational%20demands%20at%20test%20time%2C%0Alack%20of%20robustness%2C%20and%20limited%20accuracy%2C%20primarily%20stemming%20from%20their%0Afeed-forward%20processing%20nature.%20These%20limitations%20could%20be%20more%20severe%20for%0Amodels%20with%20fewer%20parameters.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Aframework%20inspired%20by%20cybernetic%20principles%2C%20redesigning%20video%20MLLMs%20as%0Aadaptive%20systems%20capable%20of%20self-monitoring%2C%20self-correction%2C%20and%20dynamic%0Aresource%20allocation%20during%20inference.%20Our%20approach%2C%20CyberV%2C%20introduces%20a%0Acybernetic%20loop%20consisting%20of%20an%20MLLM%20Inference%20System%2C%20a%20Sensor%2C%20and%20a%0AController.%20Specifically%2C%20the%20sensor%20monitors%20forward%20processes%20of%20the%20MLLM%20and%0Acollects%20intermediate%20interpretations%2C%20such%20as%20attention%20drift%2C%20then%20the%0Acontroller%20determines%20when%20and%20how%20to%20trigger%20self-correction%20and%20generate%0Afeedback%20to%20guide%20the%20next%20round.%20This%20test-time%20adaptive%20scaling%20framework%0Aenhances%20frozen%20MLLMs%20without%20requiring%20retraining%20or%20additional%20components.%0AExperiments%20demonstrate%20significant%20improvements%3A%20CyberV%20boosts%20Qwen2.5-VL-7B%0Aby%208.3%25%20and%20InternVL3-8B%20by%205.5%25%20on%20VideoMMMU%2C%20surpassing%20the%20competitive%0Aproprietary%20model%20GPT-4o.%20When%20applied%20to%20Qwen2.5-VL-72B%2C%20it%20yields%20a%2010.0%25%0Aimprovement%2C%20achieving%20performance%20even%20comparable%20to%20human%20experts.%0AFurthermore%2C%20our%20method%20demonstrates%20consistent%20gains%20on%20general-purpose%0Abenchmarks%2C%20such%20as%20VideoMME%20and%20WorldSense%2C%20highlighting%20its%20effectiveness%20and%0Ageneralization%20capabilities%20in%20making%20MLLMs%20more%20robust%20and%20accurate%20for%0Adynamic%20video%20understanding.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/marinero4972/CyberV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07971v1&entry.124074799=Read"},
{"title": "C3T: Cross-modal Transfer Through Time for Sensor-based Human Activity\n  Recognition", "author": "Abhi Kamboj and Anh Duy Nguyen and Minh N. Do", "abstract": "  In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between time-series modalities using a multimodal\n\\textit{temporal} representation space for Human Activity Recognition (HAR).\nSpecifically, we explore the setting where the modality used in testing has no\nlabeled data during training, which we refer to as Unsupervised Modality\nAdaptation (UMA). We categorize existing UMA approaches as Student-Teacher or\nContrastive Alignment methods. These methods typically compress continuous-time\ndata samples into single latent vectors during alignment, inhibiting their\nability to transfer temporal information through real-world temporal\ndistortions. To address this, we introduce Cross-modal Transfer Through Time\n(C3T), which preserves temporal information during alignment to handle dynamic\nsensor data better. C3T achieves this by aligning a set of temporal latent\nvectors across sensing modalities. Our extensive experiments on various\ncamera+IMU datasets demonstrate that C3T outperforms existing methods in UMA by\nat least 8% in accuracy and shows superior robustness to temporal distortions\nsuch as time-shift, misalignment, and dilation. Our findings suggest that C3T\nhas significant potential for developing generalizable models for time-series\nsensor data, opening new avenues for various multimodal applications.\n", "link": "http://arxiv.org/abs/2407.16803v3", "date": "2025-06-09", "relevancy": 2.3607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6324}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5661}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3T%3A%20Cross-modal%20Transfer%20Through%20Time%20for%20Sensor-based%20Human%20Activity%0A%20%20Recognition&body=Title%3A%20C3T%3A%20Cross-modal%20Transfer%20Through%20Time%20for%20Sensor-based%20Human%20Activity%0A%20%20Recognition%0AAuthor%3A%20Abhi%20Kamboj%20and%20Anh%20Duy%20Nguyen%20and%20Minh%20N.%20Do%0AAbstract%3A%20%20%20In%20order%20to%20unlock%20the%20potential%20of%20diverse%20sensors%2C%20we%20investigate%20a%20method%0Ato%20transfer%20knowledge%20between%20time-series%20modalities%20using%20a%20multimodal%0A%5Ctextit%7Btemporal%7D%20representation%20space%20for%20Human%20Activity%20Recognition%20%28HAR%29.%0ASpecifically%2C%20we%20explore%20the%20setting%20where%20the%20modality%20used%20in%20testing%20has%20no%0Alabeled%20data%20during%20training%2C%20which%20we%20refer%20to%20as%20Unsupervised%20Modality%0AAdaptation%20%28UMA%29.%20We%20categorize%20existing%20UMA%20approaches%20as%20Student-Teacher%20or%0AContrastive%20Alignment%20methods.%20These%20methods%20typically%20compress%20continuous-time%0Adata%20samples%20into%20single%20latent%20vectors%20during%20alignment%2C%20inhibiting%20their%0Aability%20to%20transfer%20temporal%20information%20through%20real-world%20temporal%0Adistortions.%20To%20address%20this%2C%20we%20introduce%20Cross-modal%20Transfer%20Through%20Time%0A%28C3T%29%2C%20which%20preserves%20temporal%20information%20during%20alignment%20to%20handle%20dynamic%0Asensor%20data%20better.%20C3T%20achieves%20this%20by%20aligning%20a%20set%20of%20temporal%20latent%0Avectors%20across%20sensing%20modalities.%20Our%20extensive%20experiments%20on%20various%0Acamera%2BIMU%20datasets%20demonstrate%20that%20C3T%20outperforms%20existing%20methods%20in%20UMA%20by%0Aat%20least%208%25%20in%20accuracy%20and%20shows%20superior%20robustness%20to%20temporal%20distortions%0Asuch%20as%20time-shift%2C%20misalignment%2C%20and%20dilation.%20Our%20findings%20suggest%20that%20C3T%0Ahas%20significant%20potential%20for%20developing%20generalizable%20models%20for%20time-series%0Asensor%20data%2C%20opening%20new%20avenues%20for%20various%20multimodal%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16803v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3T%253A%2520Cross-modal%2520Transfer%2520Through%2520Time%2520for%2520Sensor-based%2520Human%2520Activity%250A%2520%2520Recognition%26entry.906535625%3DAbhi%2520Kamboj%2520and%2520Anh%2520Duy%2520Nguyen%2520and%2520Minh%2520N.%2520Do%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520unlock%2520the%2520potential%2520of%2520diverse%2520sensors%252C%2520we%2520investigate%2520a%2520method%250Ato%2520transfer%2520knowledge%2520between%2520time-series%2520modalities%2520using%2520a%2520multimodal%250A%255Ctextit%257Btemporal%257D%2520representation%2520space%2520for%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529.%250ASpecifically%252C%2520we%2520explore%2520the%2520setting%2520where%2520the%2520modality%2520used%2520in%2520testing%2520has%2520no%250Alabeled%2520data%2520during%2520training%252C%2520which%2520we%2520refer%2520to%2520as%2520Unsupervised%2520Modality%250AAdaptation%2520%2528UMA%2529.%2520We%2520categorize%2520existing%2520UMA%2520approaches%2520as%2520Student-Teacher%2520or%250AContrastive%2520Alignment%2520methods.%2520These%2520methods%2520typically%2520compress%2520continuous-time%250Adata%2520samples%2520into%2520single%2520latent%2520vectors%2520during%2520alignment%252C%2520inhibiting%2520their%250Aability%2520to%2520transfer%2520temporal%2520information%2520through%2520real-world%2520temporal%250Adistortions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Cross-modal%2520Transfer%2520Through%2520Time%250A%2528C3T%2529%252C%2520which%2520preserves%2520temporal%2520information%2520during%2520alignment%2520to%2520handle%2520dynamic%250Asensor%2520data%2520better.%2520C3T%2520achieves%2520this%2520by%2520aligning%2520a%2520set%2520of%2520temporal%2520latent%250Avectors%2520across%2520sensing%2520modalities.%2520Our%2520extensive%2520experiments%2520on%2520various%250Acamera%252BIMU%2520datasets%2520demonstrate%2520that%2520C3T%2520outperforms%2520existing%2520methods%2520in%2520UMA%2520by%250Aat%2520least%25208%2525%2520in%2520accuracy%2520and%2520shows%2520superior%2520robustness%2520to%2520temporal%2520distortions%250Asuch%2520as%2520time-shift%252C%2520misalignment%252C%2520and%2520dilation.%2520Our%2520findings%2520suggest%2520that%2520C3T%250Ahas%2520significant%2520potential%2520for%2520developing%2520generalizable%2520models%2520for%2520time-series%250Asensor%2520data%252C%2520opening%2520new%2520avenues%2520for%2520various%2520multimodal%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16803v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3T%3A%20Cross-modal%20Transfer%20Through%20Time%20for%20Sensor-based%20Human%20Activity%0A%20%20Recognition&entry.906535625=Abhi%20Kamboj%20and%20Anh%20Duy%20Nguyen%20and%20Minh%20N.%20Do&entry.1292438233=%20%20In%20order%20to%20unlock%20the%20potential%20of%20diverse%20sensors%2C%20we%20investigate%20a%20method%0Ato%20transfer%20knowledge%20between%20time-series%20modalities%20using%20a%20multimodal%0A%5Ctextit%7Btemporal%7D%20representation%20space%20for%20Human%20Activity%20Recognition%20%28HAR%29.%0ASpecifically%2C%20we%20explore%20the%20setting%20where%20the%20modality%20used%20in%20testing%20has%20no%0Alabeled%20data%20during%20training%2C%20which%20we%20refer%20to%20as%20Unsupervised%20Modality%0AAdaptation%20%28UMA%29.%20We%20categorize%20existing%20UMA%20approaches%20as%20Student-Teacher%20or%0AContrastive%20Alignment%20methods.%20These%20methods%20typically%20compress%20continuous-time%0Adata%20samples%20into%20single%20latent%20vectors%20during%20alignment%2C%20inhibiting%20their%0Aability%20to%20transfer%20temporal%20information%20through%20real-world%20temporal%0Adistortions.%20To%20address%20this%2C%20we%20introduce%20Cross-modal%20Transfer%20Through%20Time%0A%28C3T%29%2C%20which%20preserves%20temporal%20information%20during%20alignment%20to%20handle%20dynamic%0Asensor%20data%20better.%20C3T%20achieves%20this%20by%20aligning%20a%20set%20of%20temporal%20latent%0Avectors%20across%20sensing%20modalities.%20Our%20extensive%20experiments%20on%20various%0Acamera%2BIMU%20datasets%20demonstrate%20that%20C3T%20outperforms%20existing%20methods%20in%20UMA%20by%0Aat%20least%208%25%20in%20accuracy%20and%20shows%20superior%20robustness%20to%20temporal%20distortions%0Asuch%20as%20time-shift%2C%20misalignment%2C%20and%20dilation.%20Our%20findings%20suggest%20that%20C3T%0Ahas%20significant%20potential%20for%20developing%20generalizable%20models%20for%20time-series%0Asensor%20data%2C%20opening%20new%20avenues%20for%20various%20multimodal%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16803v3&entry.124074799=Read"},
{"title": "Uncovering the Functional Roles of Nonlinearity in Memory", "author": "Manuel Brenner and Georgia Koppe", "abstract": "  Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems.\n", "link": "http://arxiv.org/abs/2506.07919v1", "date": "2025-06-09", "relevancy": 2.3585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20the%20Functional%20Roles%20of%20Nonlinearity%20in%20Memory&body=Title%3A%20Uncovering%20the%20Functional%20Roles%20of%20Nonlinearity%20in%20Memory%0AAuthor%3A%20Manuel%20Brenner%20and%20Georgia%20Koppe%0AAbstract%3A%20%20%20Memory%20and%20long-range%20temporal%20processing%20are%20core%20requirements%20for%20sequence%0Amodeling%20tasks%20across%20natural%20language%20processing%2C%20time-series%20forecasting%2C%0Aspeech%20recognition%2C%20and%20control.%20While%20nonlinear%20recurrence%20has%20long%20been%0Aviewed%20as%20essential%20for%20enabling%20such%20mechanisms%2C%20recent%20work%20suggests%20that%0Alinear%20dynamics%20may%20often%20suffice.%20In%20this%20study%2C%20we%20go%20beyond%20performance%0Acomparisons%20to%20systematically%20dissect%20the%20functional%20role%20of%20nonlinearity%20in%0Arecurrent%20networks--identifying%20both%20when%20it%20is%20computationally%20necessary%2C%20and%0Awhat%20mechanisms%20it%20enables.%20We%20use%20Almost%20Linear%20Recurrent%20Neural%20Networks%0A%28AL-RNNs%29%2C%20which%20allow%20fine-grained%20control%20over%20nonlinearity%2C%20as%20both%20a%0Aflexible%20modeling%20tool%20and%20a%20probe%20into%20the%20internal%20mechanisms%20of%20memory.%0AAcross%20a%20range%20of%20classic%20sequence%20modeling%20tasks%20and%20a%20real-world%20stimulus%0Aselection%20task%2C%20we%20find%20that%20minimal%20nonlinearity%20is%20not%20only%20sufficient%20but%0Aoften%20optimal%2C%20yielding%20models%20that%20are%20simpler%2C%20more%20robust%2C%20and%20more%0Ainterpretable%20than%20their%20fully%20nonlinear%20or%20linear%20counterparts.%20Our%20results%0Aprovide%20a%20principled%20framework%20for%20selectively%20introducing%20nonlinearity%2C%0Abridging%20dynamical%20systems%20theory%20with%20the%20functional%20demands%20of%20long-range%0Amemory%20and%20structured%20computation%20in%20recurrent%20neural%20networks%2C%20with%0Aimplications%20for%20both%20artificial%20and%20biological%20neural%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520the%2520Functional%2520Roles%2520of%2520Nonlinearity%2520in%2520Memory%26entry.906535625%3DManuel%2520Brenner%2520and%2520Georgia%2520Koppe%26entry.1292438233%3D%2520%2520Memory%2520and%2520long-range%2520temporal%2520processing%2520are%2520core%2520requirements%2520for%2520sequence%250Amodeling%2520tasks%2520across%2520natural%2520language%2520processing%252C%2520time-series%2520forecasting%252C%250Aspeech%2520recognition%252C%2520and%2520control.%2520While%2520nonlinear%2520recurrence%2520has%2520long%2520been%250Aviewed%2520as%2520essential%2520for%2520enabling%2520such%2520mechanisms%252C%2520recent%2520work%2520suggests%2520that%250Alinear%2520dynamics%2520may%2520often%2520suffice.%2520In%2520this%2520study%252C%2520we%2520go%2520beyond%2520performance%250Acomparisons%2520to%2520systematically%2520dissect%2520the%2520functional%2520role%2520of%2520nonlinearity%2520in%250Arecurrent%2520networks--identifying%2520both%2520when%2520it%2520is%2520computationally%2520necessary%252C%2520and%250Awhat%2520mechanisms%2520it%2520enables.%2520We%2520use%2520Almost%2520Linear%2520Recurrent%2520Neural%2520Networks%250A%2528AL-RNNs%2529%252C%2520which%2520allow%2520fine-grained%2520control%2520over%2520nonlinearity%252C%2520as%2520both%2520a%250Aflexible%2520modeling%2520tool%2520and%2520a%2520probe%2520into%2520the%2520internal%2520mechanisms%2520of%2520memory.%250AAcross%2520a%2520range%2520of%2520classic%2520sequence%2520modeling%2520tasks%2520and%2520a%2520real-world%2520stimulus%250Aselection%2520task%252C%2520we%2520find%2520that%2520minimal%2520nonlinearity%2520is%2520not%2520only%2520sufficient%2520but%250Aoften%2520optimal%252C%2520yielding%2520models%2520that%2520are%2520simpler%252C%2520more%2520robust%252C%2520and%2520more%250Ainterpretable%2520than%2520their%2520fully%2520nonlinear%2520or%2520linear%2520counterparts.%2520Our%2520results%250Aprovide%2520a%2520principled%2520framework%2520for%2520selectively%2520introducing%2520nonlinearity%252C%250Abridging%2520dynamical%2520systems%2520theory%2520with%2520the%2520functional%2520demands%2520of%2520long-range%250Amemory%2520and%2520structured%2520computation%2520in%2520recurrent%2520neural%2520networks%252C%2520with%250Aimplications%2520for%2520both%2520artificial%2520and%2520biological%2520neural%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20the%20Functional%20Roles%20of%20Nonlinearity%20in%20Memory&entry.906535625=Manuel%20Brenner%20and%20Georgia%20Koppe&entry.1292438233=%20%20Memory%20and%20long-range%20temporal%20processing%20are%20core%20requirements%20for%20sequence%0Amodeling%20tasks%20across%20natural%20language%20processing%2C%20time-series%20forecasting%2C%0Aspeech%20recognition%2C%20and%20control.%20While%20nonlinear%20recurrence%20has%20long%20been%0Aviewed%20as%20essential%20for%20enabling%20such%20mechanisms%2C%20recent%20work%20suggests%20that%0Alinear%20dynamics%20may%20often%20suffice.%20In%20this%20study%2C%20we%20go%20beyond%20performance%0Acomparisons%20to%20systematically%20dissect%20the%20functional%20role%20of%20nonlinearity%20in%0Arecurrent%20networks--identifying%20both%20when%20it%20is%20computationally%20necessary%2C%20and%0Awhat%20mechanisms%20it%20enables.%20We%20use%20Almost%20Linear%20Recurrent%20Neural%20Networks%0A%28AL-RNNs%29%2C%20which%20allow%20fine-grained%20control%20over%20nonlinearity%2C%20as%20both%20a%0Aflexible%20modeling%20tool%20and%20a%20probe%20into%20the%20internal%20mechanisms%20of%20memory.%0AAcross%20a%20range%20of%20classic%20sequence%20modeling%20tasks%20and%20a%20real-world%20stimulus%0Aselection%20task%2C%20we%20find%20that%20minimal%20nonlinearity%20is%20not%20only%20sufficient%20but%0Aoften%20optimal%2C%20yielding%20models%20that%20are%20simpler%2C%20more%20robust%2C%20and%20more%0Ainterpretable%20than%20their%20fully%20nonlinear%20or%20linear%20counterparts.%20Our%20results%0Aprovide%20a%20principled%20framework%20for%20selectively%20introducing%20nonlinearity%2C%0Abridging%20dynamical%20systems%20theory%20with%20the%20functional%20demands%20of%20long-range%0Amemory%20and%20structured%20computation%20in%20recurrent%20neural%20networks%2C%20with%0Aimplications%20for%20both%20artificial%20and%20biological%20neural%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07919v1&entry.124074799=Read"},
{"title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning\n  from Partially Annotated Synthetic Datasets", "author": "Anh-Quan Cao and Ivan Lopes and Raoul de Charette", "abstract": "  Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.\n", "link": "http://arxiv.org/abs/2506.08013v1", "date": "2025-06-09", "relevancy": 2.3537, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.61}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableMTL%3A%20Repurposing%20Latent%20Diffusion%20Models%20for%20Multi-Task%20Learning%0A%20%20from%20Partially%20Annotated%20Synthetic%20Datasets&body=Title%3A%20StableMTL%3A%20Repurposing%20Latent%20Diffusion%20Models%20for%20Multi-Task%20Learning%0A%20%20from%20Partially%20Annotated%20Synthetic%20Datasets%0AAuthor%3A%20Anh-Quan%20Cao%20and%20Ivan%20Lopes%20and%20Raoul%20de%20Charette%0AAbstract%3A%20%20%20Multi-task%20learning%20for%20dense%20prediction%20is%20limited%20by%20the%20need%20for%20extensive%0Aannotation%20for%20every%20task%2C%20though%20recent%20works%20have%20explored%20training%20with%0Apartial%20task%20labels.%20Leveraging%20the%20generalization%20power%20of%20diffusion%20models%2C%0Awe%20extend%20the%20partial%20learning%20setup%20to%20a%20zero-shot%20setting%2C%20training%20a%0Amulti-task%20model%20on%20multiple%20synthetic%20datasets%2C%20each%20labeled%20for%20only%20a%20subset%0Aof%20tasks.%20Our%20method%2C%20StableMTL%2C%20repurposes%20image%20generators%20for%20latent%0Aregression.%20Adapting%20a%20denoising%20framework%20with%20task%20encoding%2C%20per-task%0Aconditioning%20and%20a%20tailored%20training%20scheme.%20Instead%20of%20per-task%20losses%0Arequiring%20careful%20balancing%2C%20a%20unified%20latent%20loss%20is%20adopted%2C%20enabling%0Aseamless%20scaling%20to%20more%20tasks.%20To%20encourage%20inter-task%20synergy%2C%20we%20introduce%20a%0Amulti-stream%20model%20with%20a%20task-attention%20mechanism%20that%20converts%20N-to-N%20task%0Ainteractions%20into%20efficient%201-to-N%20attention%2C%20promoting%20effective%20cross-task%0Asharing.%20StableMTL%20outperforms%20baselines%20on%207%20tasks%20across%208%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableMTL%253A%2520Repurposing%2520Latent%2520Diffusion%2520Models%2520for%2520Multi-Task%2520Learning%250A%2520%2520from%2520Partially%2520Annotated%2520Synthetic%2520Datasets%26entry.906535625%3DAnh-Quan%2520Cao%2520and%2520Ivan%2520Lopes%2520and%2520Raoul%2520de%2520Charette%26entry.1292438233%3D%2520%2520Multi-task%2520learning%2520for%2520dense%2520prediction%2520is%2520limited%2520by%2520the%2520need%2520for%2520extensive%250Aannotation%2520for%2520every%2520task%252C%2520though%2520recent%2520works%2520have%2520explored%2520training%2520with%250Apartial%2520task%2520labels.%2520Leveraging%2520the%2520generalization%2520power%2520of%2520diffusion%2520models%252C%250Awe%2520extend%2520the%2520partial%2520learning%2520setup%2520to%2520a%2520zero-shot%2520setting%252C%2520training%2520a%250Amulti-task%2520model%2520on%2520multiple%2520synthetic%2520datasets%252C%2520each%2520labeled%2520for%2520only%2520a%2520subset%250Aof%2520tasks.%2520Our%2520method%252C%2520StableMTL%252C%2520repurposes%2520image%2520generators%2520for%2520latent%250Aregression.%2520Adapting%2520a%2520denoising%2520framework%2520with%2520task%2520encoding%252C%2520per-task%250Aconditioning%2520and%2520a%2520tailored%2520training%2520scheme.%2520Instead%2520of%2520per-task%2520losses%250Arequiring%2520careful%2520balancing%252C%2520a%2520unified%2520latent%2520loss%2520is%2520adopted%252C%2520enabling%250Aseamless%2520scaling%2520to%2520more%2520tasks.%2520To%2520encourage%2520inter-task%2520synergy%252C%2520we%2520introduce%2520a%250Amulti-stream%2520model%2520with%2520a%2520task-attention%2520mechanism%2520that%2520converts%2520N-to-N%2520task%250Ainteractions%2520into%2520efficient%25201-to-N%2520attention%252C%2520promoting%2520effective%2520cross-task%250Asharing.%2520StableMTL%2520outperforms%2520baselines%2520on%25207%2520tasks%2520across%25208%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableMTL%3A%20Repurposing%20Latent%20Diffusion%20Models%20for%20Multi-Task%20Learning%0A%20%20from%20Partially%20Annotated%20Synthetic%20Datasets&entry.906535625=Anh-Quan%20Cao%20and%20Ivan%20Lopes%20and%20Raoul%20de%20Charette&entry.1292438233=%20%20Multi-task%20learning%20for%20dense%20prediction%20is%20limited%20by%20the%20need%20for%20extensive%0Aannotation%20for%20every%20task%2C%20though%20recent%20works%20have%20explored%20training%20with%0Apartial%20task%20labels.%20Leveraging%20the%20generalization%20power%20of%20diffusion%20models%2C%0Awe%20extend%20the%20partial%20learning%20setup%20to%20a%20zero-shot%20setting%2C%20training%20a%0Amulti-task%20model%20on%20multiple%20synthetic%20datasets%2C%20each%20labeled%20for%20only%20a%20subset%0Aof%20tasks.%20Our%20method%2C%20StableMTL%2C%20repurposes%20image%20generators%20for%20latent%0Aregression.%20Adapting%20a%20denoising%20framework%20with%20task%20encoding%2C%20per-task%0Aconditioning%20and%20a%20tailored%20training%20scheme.%20Instead%20of%20per-task%20losses%0Arequiring%20careful%20balancing%2C%20a%20unified%20latent%20loss%20is%20adopted%2C%20enabling%0Aseamless%20scaling%20to%20more%20tasks.%20To%20encourage%20inter-task%20synergy%2C%20we%20introduce%20a%0Amulti-stream%20model%20with%20a%20task-attention%20mechanism%20that%20converts%20N-to-N%20task%0Ainteractions%20into%20efficient%201-to-N%20attention%2C%20promoting%20effective%20cross-task%0Asharing.%20StableMTL%20outperforms%20baselines%20on%207%20tasks%20across%208%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08013v1&entry.124074799=Read"},
{"title": "Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the\n  Wild", "author": "Junhyeong Cho and Kim Youwang and Hunmin Yang and Tae-Hyun Oh", "abstract": "  Recent monocular 3D shape reconstruction methods have shown promising\nzero-shot results on object-segmented images without any occlusions. However,\ntheir effectiveness is significantly compromised in real-world conditions, due\nto imperfect object segmentation by off-the-shelf models and the prevalence of\nocclusions. To effectively address these issues, we propose a unified\nregression model that integrates segmentation and reconstruction, specifically\ndesigned for occlusion-aware 3D shape reconstruction. To facilitate its\nreconstruction in the wild, we also introduce a scalable data synthesis\npipeline that simulates a wide range of variations in objects, occluders, and\nbackgrounds. Training on our synthetic data enables the proposed model to\nachieve state-of-the-art zero-shot results on real-world images, using\nsignificantly fewer parameters than competing approaches.\n", "link": "http://arxiv.org/abs/2403.14539v3", "date": "2025-06-09", "relevancy": 2.3531, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5947}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.584}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%203D%20Shape%20Reconstruction%20in%20Zero-Shot%20from%20a%20Single%20Image%20in%20the%0A%20%20Wild&body=Title%3A%20Robust%203D%20Shape%20Reconstruction%20in%20Zero-Shot%20from%20a%20Single%20Image%20in%20the%0A%20%20Wild%0AAuthor%3A%20Junhyeong%20Cho%20and%20Kim%20Youwang%20and%20Hunmin%20Yang%20and%20Tae-Hyun%20Oh%0AAbstract%3A%20%20%20Recent%20monocular%203D%20shape%20reconstruction%20methods%20have%20shown%20promising%0Azero-shot%20results%20on%20object-segmented%20images%20without%20any%20occlusions.%20However%2C%0Atheir%20effectiveness%20is%20significantly%20compromised%20in%20real-world%20conditions%2C%20due%0Ato%20imperfect%20object%20segmentation%20by%20off-the-shelf%20models%20and%20the%20prevalence%20of%0Aocclusions.%20To%20effectively%20address%20these%20issues%2C%20we%20propose%20a%20unified%0Aregression%20model%20that%20integrates%20segmentation%20and%20reconstruction%2C%20specifically%0Adesigned%20for%20occlusion-aware%203D%20shape%20reconstruction.%20To%20facilitate%20its%0Areconstruction%20in%20the%20wild%2C%20we%20also%20introduce%20a%20scalable%20data%20synthesis%0Apipeline%20that%20simulates%20a%20wide%20range%20of%20variations%20in%20objects%2C%20occluders%2C%20and%0Abackgrounds.%20Training%20on%20our%20synthetic%20data%20enables%20the%20proposed%20model%20to%0Aachieve%20state-of-the-art%20zero-shot%20results%20on%20real-world%20images%2C%20using%0Asignificantly%20fewer%20parameters%20than%20competing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%25203D%2520Shape%2520Reconstruction%2520in%2520Zero-Shot%2520from%2520a%2520Single%2520Image%2520in%2520the%250A%2520%2520Wild%26entry.906535625%3DJunhyeong%2520Cho%2520and%2520Kim%2520Youwang%2520and%2520Hunmin%2520Yang%2520and%2520Tae-Hyun%2520Oh%26entry.1292438233%3D%2520%2520Recent%2520monocular%25203D%2520shape%2520reconstruction%2520methods%2520have%2520shown%2520promising%250Azero-shot%2520results%2520on%2520object-segmented%2520images%2520without%2520any%2520occlusions.%2520However%252C%250Atheir%2520effectiveness%2520is%2520significantly%2520compromised%2520in%2520real-world%2520conditions%252C%2520due%250Ato%2520imperfect%2520object%2520segmentation%2520by%2520off-the-shelf%2520models%2520and%2520the%2520prevalence%2520of%250Aocclusions.%2520To%2520effectively%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520unified%250Aregression%2520model%2520that%2520integrates%2520segmentation%2520and%2520reconstruction%252C%2520specifically%250Adesigned%2520for%2520occlusion-aware%25203D%2520shape%2520reconstruction.%2520To%2520facilitate%2520its%250Areconstruction%2520in%2520the%2520wild%252C%2520we%2520also%2520introduce%2520a%2520scalable%2520data%2520synthesis%250Apipeline%2520that%2520simulates%2520a%2520wide%2520range%2520of%2520variations%2520in%2520objects%252C%2520occluders%252C%2520and%250Abackgrounds.%2520Training%2520on%2520our%2520synthetic%2520data%2520enables%2520the%2520proposed%2520model%2520to%250Aachieve%2520state-of-the-art%2520zero-shot%2520results%2520on%2520real-world%2520images%252C%2520using%250Asignificantly%2520fewer%2520parameters%2520than%2520competing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%203D%20Shape%20Reconstruction%20in%20Zero-Shot%20from%20a%20Single%20Image%20in%20the%0A%20%20Wild&entry.906535625=Junhyeong%20Cho%20and%20Kim%20Youwang%20and%20Hunmin%20Yang%20and%20Tae-Hyun%20Oh&entry.1292438233=%20%20Recent%20monocular%203D%20shape%20reconstruction%20methods%20have%20shown%20promising%0Azero-shot%20results%20on%20object-segmented%20images%20without%20any%20occlusions.%20However%2C%0Atheir%20effectiveness%20is%20significantly%20compromised%20in%20real-world%20conditions%2C%20due%0Ato%20imperfect%20object%20segmentation%20by%20off-the-shelf%20models%20and%20the%20prevalence%20of%0Aocclusions.%20To%20effectively%20address%20these%20issues%2C%20we%20propose%20a%20unified%0Aregression%20model%20that%20integrates%20segmentation%20and%20reconstruction%2C%20specifically%0Adesigned%20for%20occlusion-aware%203D%20shape%20reconstruction.%20To%20facilitate%20its%0Areconstruction%20in%20the%20wild%2C%20we%20also%20introduce%20a%20scalable%20data%20synthesis%0Apipeline%20that%20simulates%20a%20wide%20range%20of%20variations%20in%20objects%2C%20occluders%2C%20and%0Abackgrounds.%20Training%20on%20our%20synthetic%20data%20enables%20the%20proposed%20model%20to%0Aachieve%20state-of-the-art%20zero-shot%20results%20on%20real-world%20images%2C%20using%0Asignificantly%20fewer%20parameters%20than%20competing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14539v3&entry.124074799=Read"},
{"title": "LUCIFER: Language Understanding and Context-Infused Framework for\n  Exploration and Behavior Refinement", "author": "Dimitris Panagopoulos and Adolfo Perrusquia and Weisi Guo", "abstract": "  In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.\n", "link": "http://arxiv.org/abs/2506.07915v1", "date": "2025-06-09", "relevancy": 2.3486, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUCIFER%3A%20Language%20Understanding%20and%20Context-Infused%20Framework%20for%0A%20%20Exploration%20and%20Behavior%20Refinement&body=Title%3A%20LUCIFER%3A%20Language%20Understanding%20and%20Context-Infused%20Framework%20for%0A%20%20Exploration%20and%20Behavior%20Refinement%0AAuthor%3A%20Dimitris%20Panagopoulos%20and%20Adolfo%20Perrusquia%20and%20Weisi%20Guo%0AAbstract%3A%20%20%20In%20dynamic%20environments%2C%20the%20rapid%20obsolescence%20of%20pre-existing%20environmental%0Aknowledge%20creates%20a%20gap%20between%20an%20agent%27s%20internal%20model%20and%20the%20evolving%0Areality%20of%20its%20operational%20context.%20This%20disparity%20between%20prior%20and%20updated%0Aenvironmental%20valuations%20fundamentally%20limits%20the%20effectiveness%20of%20autonomous%0Adecision-making.%20To%20bridge%20this%20gap%2C%20the%20contextual%20bias%20of%20human%20domain%0Astakeholders%2C%20who%20naturally%20accumulate%20insights%20through%20direct%2C%20real-time%0Aobservation%2C%20becomes%20indispensable.%20However%2C%20translating%20their%20nuanced%2C%20and%0Acontext-rich%20input%20into%20actionable%20intelligence%20for%20autonomous%20systems%20remains%0Aan%20open%20challenge.%20To%20address%20this%2C%20we%20propose%20LUCIFER%20%28Language%20Understanding%0Aand%20Context-Infused%20Framework%20for%20Exploration%20and%20Behavior%20Refinement%29%2C%20a%0Adomain-agnostic%20framework%20that%20integrates%20a%20hierarchical%20decision-making%0Aarchitecture%20with%20reinforcement%20learning%20%28RL%29%20and%20large%20language%20models%20%28LLMs%29%0Ainto%20a%20unified%20system.%20This%20architecture%20mirrors%20how%20humans%20decompose%20complex%0Atasks%2C%20enabling%20a%20high-level%20planner%20to%20coordinate%20specialised%20sub-agents%2C%20each%0Afocused%20on%20distinct%20objectives%20and%20temporally%20interdependent%20actions.%20Unlike%0Atraditional%20applications%20where%20LLMs%20are%20limited%20to%20single%20role%2C%20LUCIFER%0Aintegrates%20them%20in%20two%20synergistic%20roles%3A%20as%20context%20extractors%2C%20structuring%0Averbal%20stakeholder%20input%20into%20domain-aware%20representations%20that%20influence%0Adecision-making%20through%20an%20attention%20space%20mechanism%20aligning%20LLM-derived%0Ainsights%20with%20the%20agent%27s%20learning%20process%2C%20and%20as%20zero-shot%20exploration%0Afacilitators%20guiding%20the%20agent%27s%20action%20selection%20process%20during%20exploration.%0AWe%20benchmark%20various%20LLMs%20in%20both%20roles%20and%20demonstrate%20that%20LUCIFER%20improves%0Aexploration%20efficiency%20and%20decision%20quality%2C%20outperforming%20flat%2C%0Agoal-conditioned%20policies.%20Our%20findings%20show%20the%20potential%20of%20context-driven%0Adecision-making%2C%20where%20autonomous%20systems%20leverage%20human%20contextual%20knowledge%0Afor%20operational%20success.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUCIFER%253A%2520Language%2520Understanding%2520and%2520Context-Infused%2520Framework%2520for%250A%2520%2520Exploration%2520and%2520Behavior%2520Refinement%26entry.906535625%3DDimitris%2520Panagopoulos%2520and%2520Adolfo%2520Perrusquia%2520and%2520Weisi%2520Guo%26entry.1292438233%3D%2520%2520In%2520dynamic%2520environments%252C%2520the%2520rapid%2520obsolescence%2520of%2520pre-existing%2520environmental%250Aknowledge%2520creates%2520a%2520gap%2520between%2520an%2520agent%2527s%2520internal%2520model%2520and%2520the%2520evolving%250Areality%2520of%2520its%2520operational%2520context.%2520This%2520disparity%2520between%2520prior%2520and%2520updated%250Aenvironmental%2520valuations%2520fundamentally%2520limits%2520the%2520effectiveness%2520of%2520autonomous%250Adecision-making.%2520To%2520bridge%2520this%2520gap%252C%2520the%2520contextual%2520bias%2520of%2520human%2520domain%250Astakeholders%252C%2520who%2520naturally%2520accumulate%2520insights%2520through%2520direct%252C%2520real-time%250Aobservation%252C%2520becomes%2520indispensable.%2520However%252C%2520translating%2520their%2520nuanced%252C%2520and%250Acontext-rich%2520input%2520into%2520actionable%2520intelligence%2520for%2520autonomous%2520systems%2520remains%250Aan%2520open%2520challenge.%2520To%2520address%2520this%252C%2520we%2520propose%2520LUCIFER%2520%2528Language%2520Understanding%250Aand%2520Context-Infused%2520Framework%2520for%2520Exploration%2520and%2520Behavior%2520Refinement%2529%252C%2520a%250Adomain-agnostic%2520framework%2520that%2520integrates%2520a%2520hierarchical%2520decision-making%250Aarchitecture%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%250Ainto%2520a%2520unified%2520system.%2520This%2520architecture%2520mirrors%2520how%2520humans%2520decompose%2520complex%250Atasks%252C%2520enabling%2520a%2520high-level%2520planner%2520to%2520coordinate%2520specialised%2520sub-agents%252C%2520each%250Afocused%2520on%2520distinct%2520objectives%2520and%2520temporally%2520interdependent%2520actions.%2520Unlike%250Atraditional%2520applications%2520where%2520LLMs%2520are%2520limited%2520to%2520single%2520role%252C%2520LUCIFER%250Aintegrates%2520them%2520in%2520two%2520synergistic%2520roles%253A%2520as%2520context%2520extractors%252C%2520structuring%250Averbal%2520stakeholder%2520input%2520into%2520domain-aware%2520representations%2520that%2520influence%250Adecision-making%2520through%2520an%2520attention%2520space%2520mechanism%2520aligning%2520LLM-derived%250Ainsights%2520with%2520the%2520agent%2527s%2520learning%2520process%252C%2520and%2520as%2520zero-shot%2520exploration%250Afacilitators%2520guiding%2520the%2520agent%2527s%2520action%2520selection%2520process%2520during%2520exploration.%250AWe%2520benchmark%2520various%2520LLMs%2520in%2520both%2520roles%2520and%2520demonstrate%2520that%2520LUCIFER%2520improves%250Aexploration%2520efficiency%2520and%2520decision%2520quality%252C%2520outperforming%2520flat%252C%250Agoal-conditioned%2520policies.%2520Our%2520findings%2520show%2520the%2520potential%2520of%2520context-driven%250Adecision-making%252C%2520where%2520autonomous%2520systems%2520leverage%2520human%2520contextual%2520knowledge%250Afor%2520operational%2520success.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUCIFER%3A%20Language%20Understanding%20and%20Context-Infused%20Framework%20for%0A%20%20Exploration%20and%20Behavior%20Refinement&entry.906535625=Dimitris%20Panagopoulos%20and%20Adolfo%20Perrusquia%20and%20Weisi%20Guo&entry.1292438233=%20%20In%20dynamic%20environments%2C%20the%20rapid%20obsolescence%20of%20pre-existing%20environmental%0Aknowledge%20creates%20a%20gap%20between%20an%20agent%27s%20internal%20model%20and%20the%20evolving%0Areality%20of%20its%20operational%20context.%20This%20disparity%20between%20prior%20and%20updated%0Aenvironmental%20valuations%20fundamentally%20limits%20the%20effectiveness%20of%20autonomous%0Adecision-making.%20To%20bridge%20this%20gap%2C%20the%20contextual%20bias%20of%20human%20domain%0Astakeholders%2C%20who%20naturally%20accumulate%20insights%20through%20direct%2C%20real-time%0Aobservation%2C%20becomes%20indispensable.%20However%2C%20translating%20their%20nuanced%2C%20and%0Acontext-rich%20input%20into%20actionable%20intelligence%20for%20autonomous%20systems%20remains%0Aan%20open%20challenge.%20To%20address%20this%2C%20we%20propose%20LUCIFER%20%28Language%20Understanding%0Aand%20Context-Infused%20Framework%20for%20Exploration%20and%20Behavior%20Refinement%29%2C%20a%0Adomain-agnostic%20framework%20that%20integrates%20a%20hierarchical%20decision-making%0Aarchitecture%20with%20reinforcement%20learning%20%28RL%29%20and%20large%20language%20models%20%28LLMs%29%0Ainto%20a%20unified%20system.%20This%20architecture%20mirrors%20how%20humans%20decompose%20complex%0Atasks%2C%20enabling%20a%20high-level%20planner%20to%20coordinate%20specialised%20sub-agents%2C%20each%0Afocused%20on%20distinct%20objectives%20and%20temporally%20interdependent%20actions.%20Unlike%0Atraditional%20applications%20where%20LLMs%20are%20limited%20to%20single%20role%2C%20LUCIFER%0Aintegrates%20them%20in%20two%20synergistic%20roles%3A%20as%20context%20extractors%2C%20structuring%0Averbal%20stakeholder%20input%20into%20domain-aware%20representations%20that%20influence%0Adecision-making%20through%20an%20attention%20space%20mechanism%20aligning%20LLM-derived%0Ainsights%20with%20the%20agent%27s%20learning%20process%2C%20and%20as%20zero-shot%20exploration%0Afacilitators%20guiding%20the%20agent%27s%20action%20selection%20process%20during%20exploration.%0AWe%20benchmark%20various%20LLMs%20in%20both%20roles%20and%20demonstrate%20that%20LUCIFER%20improves%0Aexploration%20efficiency%20and%20decision%20quality%2C%20outperforming%20flat%2C%0Agoal-conditioned%20policies.%20Our%20findings%20show%20the%20potential%20of%20context-driven%0Adecision-making%2C%20where%20autonomous%20systems%20leverage%20human%20contextual%20knowledge%0Afor%20operational%20success.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07915v1&entry.124074799=Read"},
{"title": "Data-driven inventory management for new products: An adjusted Dyna-$Q$\n  approach with transfer learning", "author": "Xinye Qu and Longxiao Liu and Wenjie Huang", "abstract": "  In this paper, we propose a novel reinforcement learning algorithm for\ninventory management of newly launched products with no historical demand\ninformation. The algorithm follows the classic Dyna-$Q$ structure, balancing\nthe model-free and model-based approaches, while accelerating the training\nprocess of Dyna-$Q$ and mitigating the model discrepancy generated by the\nmodel-based feedback. Based on the idea of transfer learning, warm-start\ninformation from the demand data of existing similar products can be\nincorporated into the algorithm to further stabilize the early-stage training\nand reduce the variance of the estimated optimal policy. Our approach is\nvalidated through a case study of bakery inventory management with real data.\nThe adjusted Dyna-$Q$ shows up to a 23.7\\% reduction in average daily cost\ncompared with $Q$-learning, and up to a 77.5\\% reduction in training time\nwithin the same horizon compared with classic Dyna-$Q$. By using transfer\nlearning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost,\nlowest variance in total cost, and relatively low shortage percentages among\nall the benchmarking algorithms under a 30-day testing.\n", "link": "http://arxiv.org/abs/2501.08109v4", "date": "2025-06-09", "relevancy": 2.3459, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4499}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20inventory%20management%20for%20new%20products%3A%20An%20adjusted%20Dyna-%24Q%24%0A%20%20approach%20with%20transfer%20learning&body=Title%3A%20Data-driven%20inventory%20management%20for%20new%20products%3A%20An%20adjusted%20Dyna-%24Q%24%0A%20%20approach%20with%20transfer%20learning%0AAuthor%3A%20Xinye%20Qu%20and%20Longxiao%20Liu%20and%20Wenjie%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20reinforcement%20learning%20algorithm%20for%0Ainventory%20management%20of%20newly%20launched%20products%20with%20no%20historical%20demand%0Ainformation.%20The%20algorithm%20follows%20the%20classic%20Dyna-%24Q%24%20structure%2C%20balancing%0Athe%20model-free%20and%20model-based%20approaches%2C%20while%20accelerating%20the%20training%0Aprocess%20of%20Dyna-%24Q%24%20and%20mitigating%20the%20model%20discrepancy%20generated%20by%20the%0Amodel-based%20feedback.%20Based%20on%20the%20idea%20of%20transfer%20learning%2C%20warm-start%0Ainformation%20from%20the%20demand%20data%20of%20existing%20similar%20products%20can%20be%0Aincorporated%20into%20the%20algorithm%20to%20further%20stabilize%20the%20early-stage%20training%0Aand%20reduce%20the%20variance%20of%20the%20estimated%20optimal%20policy.%20Our%20approach%20is%0Avalidated%20through%20a%20case%20study%20of%20bakery%20inventory%20management%20with%20real%20data.%0AThe%20adjusted%20Dyna-%24Q%24%20shows%20up%20to%20a%2023.7%5C%25%20reduction%20in%20average%20daily%20cost%0Acompared%20with%20%24Q%24-learning%2C%20and%20up%20to%20a%2077.5%5C%25%20reduction%20in%20training%20time%0Awithin%20the%20same%20horizon%20compared%20with%20classic%20Dyna-%24Q%24.%20By%20using%20transfer%0Alearning%2C%20it%20can%20be%20found%20that%20the%20adjusted%20Dyna-%24Q%24%20has%20the%20lowest%20total%20cost%2C%0Alowest%20variance%20in%20total%20cost%2C%20and%20relatively%20low%20shortage%20percentages%20among%0Aall%20the%20benchmarking%20algorithms%20under%20a%2030-day%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08109v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520inventory%2520management%2520for%2520new%2520products%253A%2520An%2520adjusted%2520Dyna-%2524Q%2524%250A%2520%2520approach%2520with%2520transfer%2520learning%26entry.906535625%3DXinye%2520Qu%2520and%2520Longxiao%2520Liu%2520and%2520Wenjie%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520reinforcement%2520learning%2520algorithm%2520for%250Ainventory%2520management%2520of%2520newly%2520launched%2520products%2520with%2520no%2520historical%2520demand%250Ainformation.%2520The%2520algorithm%2520follows%2520the%2520classic%2520Dyna-%2524Q%2524%2520structure%252C%2520balancing%250Athe%2520model-free%2520and%2520model-based%2520approaches%252C%2520while%2520accelerating%2520the%2520training%250Aprocess%2520of%2520Dyna-%2524Q%2524%2520and%2520mitigating%2520the%2520model%2520discrepancy%2520generated%2520by%2520the%250Amodel-based%2520feedback.%2520Based%2520on%2520the%2520idea%2520of%2520transfer%2520learning%252C%2520warm-start%250Ainformation%2520from%2520the%2520demand%2520data%2520of%2520existing%2520similar%2520products%2520can%2520be%250Aincorporated%2520into%2520the%2520algorithm%2520to%2520further%2520stabilize%2520the%2520early-stage%2520training%250Aand%2520reduce%2520the%2520variance%2520of%2520the%2520estimated%2520optimal%2520policy.%2520Our%2520approach%2520is%250Avalidated%2520through%2520a%2520case%2520study%2520of%2520bakery%2520inventory%2520management%2520with%2520real%2520data.%250AThe%2520adjusted%2520Dyna-%2524Q%2524%2520shows%2520up%2520to%2520a%252023.7%255C%2525%2520reduction%2520in%2520average%2520daily%2520cost%250Acompared%2520with%2520%2524Q%2524-learning%252C%2520and%2520up%2520to%2520a%252077.5%255C%2525%2520reduction%2520in%2520training%2520time%250Awithin%2520the%2520same%2520horizon%2520compared%2520with%2520classic%2520Dyna-%2524Q%2524.%2520By%2520using%2520transfer%250Alearning%252C%2520it%2520can%2520be%2520found%2520that%2520the%2520adjusted%2520Dyna-%2524Q%2524%2520has%2520the%2520lowest%2520total%2520cost%252C%250Alowest%2520variance%2520in%2520total%2520cost%252C%2520and%2520relatively%2520low%2520shortage%2520percentages%2520among%250Aall%2520the%2520benchmarking%2520algorithms%2520under%2520a%252030-day%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08109v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20inventory%20management%20for%20new%20products%3A%20An%20adjusted%20Dyna-%24Q%24%0A%20%20approach%20with%20transfer%20learning&entry.906535625=Xinye%20Qu%20and%20Longxiao%20Liu%20and%20Wenjie%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20reinforcement%20learning%20algorithm%20for%0Ainventory%20management%20of%20newly%20launched%20products%20with%20no%20historical%20demand%0Ainformation.%20The%20algorithm%20follows%20the%20classic%20Dyna-%24Q%24%20structure%2C%20balancing%0Athe%20model-free%20and%20model-based%20approaches%2C%20while%20accelerating%20the%20training%0Aprocess%20of%20Dyna-%24Q%24%20and%20mitigating%20the%20model%20discrepancy%20generated%20by%20the%0Amodel-based%20feedback.%20Based%20on%20the%20idea%20of%20transfer%20learning%2C%20warm-start%0Ainformation%20from%20the%20demand%20data%20of%20existing%20similar%20products%20can%20be%0Aincorporated%20into%20the%20algorithm%20to%20further%20stabilize%20the%20early-stage%20training%0Aand%20reduce%20the%20variance%20of%20the%20estimated%20optimal%20policy.%20Our%20approach%20is%0Avalidated%20through%20a%20case%20study%20of%20bakery%20inventory%20management%20with%20real%20data.%0AThe%20adjusted%20Dyna-%24Q%24%20shows%20up%20to%20a%2023.7%5C%25%20reduction%20in%20average%20daily%20cost%0Acompared%20with%20%24Q%24-learning%2C%20and%20up%20to%20a%2077.5%5C%25%20reduction%20in%20training%20time%0Awithin%20the%20same%20horizon%20compared%20with%20classic%20Dyna-%24Q%24.%20By%20using%20transfer%0Alearning%2C%20it%20can%20be%20found%20that%20the%20adjusted%20Dyna-%24Q%24%20has%20the%20lowest%20total%20cost%2C%0Alowest%20variance%20in%20total%20cost%2C%20and%20relatively%20low%20shortage%20percentages%20among%0Aall%20the%20benchmarking%20algorithms%20under%20a%2030-day%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08109v4&entry.124074799=Read"},
{"title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video\n  Understanding", "author": "Nianbo Zeng and Haowen Hou and Fei Richard Yu and Si Shi and Ying Tiffany He", "abstract": "  Despite recent advances in retrieval-augmented generation (RAG) for video\nunderstanding, effectively understanding long-form video content remains\nunderexplored due to the vast scale and high complexity of video data. Current\nRAG approaches typically segment videos into fixed-length chunks, which often\ndisrupts the continuity of contextual information and fails to capture\nauthentic scene boundaries. Inspired by the human ability to naturally organize\ncontinuous experiences into coherent scenes, we present SceneRAG, a unified\nframework that leverages large language models to segment videos into\nnarrative-consistent scenes by processing ASR transcripts alongside temporal\nmetadata. SceneRAG further sharpens these initial boundaries through\nlightweight heuristics and iterative correction. For each scene, the framework\nfuses information from both visual and textual modalities to extract entity\nrelations and dynamically builds a knowledge graph, enabling robust multi-hop\nretrieval and generation that account for long-range dependencies. Experiments\non the LongerVideos benchmark, featuring over 134 hours of diverse content,\nconfirm that SceneRAG substantially outperforms prior baselines, achieving a\nwin rate of up to 72.5 percent on generation tasks.\n", "link": "http://arxiv.org/abs/2506.07600v1", "date": "2025-06-09", "relevancy": 2.3264, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneRAG%3A%20Scene-level%20Retrieval-Augmented%20Generation%20for%20Video%0A%20%20Understanding&body=Title%3A%20SceneRAG%3A%20Scene-level%20Retrieval-Augmented%20Generation%20for%20Video%0A%20%20Understanding%0AAuthor%3A%20Nianbo%20Zeng%20and%20Haowen%20Hou%20and%20Fei%20Richard%20Yu%20and%20Si%20Shi%20and%20Ying%20Tiffany%20He%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20retrieval-augmented%20generation%20%28RAG%29%20for%20video%0Aunderstanding%2C%20effectively%20understanding%20long-form%20video%20content%20remains%0Aunderexplored%20due%20to%20the%20vast%20scale%20and%20high%20complexity%20of%20video%20data.%20Current%0ARAG%20approaches%20typically%20segment%20videos%20into%20fixed-length%20chunks%2C%20which%20often%0Adisrupts%20the%20continuity%20of%20contextual%20information%20and%20fails%20to%20capture%0Aauthentic%20scene%20boundaries.%20Inspired%20by%20the%20human%20ability%20to%20naturally%20organize%0Acontinuous%20experiences%20into%20coherent%20scenes%2C%20we%20present%20SceneRAG%2C%20a%20unified%0Aframework%20that%20leverages%20large%20language%20models%20to%20segment%20videos%20into%0Anarrative-consistent%20scenes%20by%20processing%20ASR%20transcripts%20alongside%20temporal%0Ametadata.%20SceneRAG%20further%20sharpens%20these%20initial%20boundaries%20through%0Alightweight%20heuristics%20and%20iterative%20correction.%20For%20each%20scene%2C%20the%20framework%0Afuses%20information%20from%20both%20visual%20and%20textual%20modalities%20to%20extract%20entity%0Arelations%20and%20dynamically%20builds%20a%20knowledge%20graph%2C%20enabling%20robust%20multi-hop%0Aretrieval%20and%20generation%20that%20account%20for%20long-range%20dependencies.%20Experiments%0Aon%20the%20LongerVideos%20benchmark%2C%20featuring%20over%20134%20hours%20of%20diverse%20content%2C%0Aconfirm%20that%20SceneRAG%20substantially%20outperforms%20prior%20baselines%2C%20achieving%20a%0Awin%20rate%20of%20up%20to%2072.5%20percent%20on%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneRAG%253A%2520Scene-level%2520Retrieval-Augmented%2520Generation%2520for%2520Video%250A%2520%2520Understanding%26entry.906535625%3DNianbo%2520Zeng%2520and%2520Haowen%2520Hou%2520and%2520Fei%2520Richard%2520Yu%2520and%2520Si%2520Shi%2520and%2520Ying%2520Tiffany%2520He%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520for%2520video%250Aunderstanding%252C%2520effectively%2520understanding%2520long-form%2520video%2520content%2520remains%250Aunderexplored%2520due%2520to%2520the%2520vast%2520scale%2520and%2520high%2520complexity%2520of%2520video%2520data.%2520Current%250ARAG%2520approaches%2520typically%2520segment%2520videos%2520into%2520fixed-length%2520chunks%252C%2520which%2520often%250Adisrupts%2520the%2520continuity%2520of%2520contextual%2520information%2520and%2520fails%2520to%2520capture%250Aauthentic%2520scene%2520boundaries.%2520Inspired%2520by%2520the%2520human%2520ability%2520to%2520naturally%2520organize%250Acontinuous%2520experiences%2520into%2520coherent%2520scenes%252C%2520we%2520present%2520SceneRAG%252C%2520a%2520unified%250Aframework%2520that%2520leverages%2520large%2520language%2520models%2520to%2520segment%2520videos%2520into%250Anarrative-consistent%2520scenes%2520by%2520processing%2520ASR%2520transcripts%2520alongside%2520temporal%250Ametadata.%2520SceneRAG%2520further%2520sharpens%2520these%2520initial%2520boundaries%2520through%250Alightweight%2520heuristics%2520and%2520iterative%2520correction.%2520For%2520each%2520scene%252C%2520the%2520framework%250Afuses%2520information%2520from%2520both%2520visual%2520and%2520textual%2520modalities%2520to%2520extract%2520entity%250Arelations%2520and%2520dynamically%2520builds%2520a%2520knowledge%2520graph%252C%2520enabling%2520robust%2520multi-hop%250Aretrieval%2520and%2520generation%2520that%2520account%2520for%2520long-range%2520dependencies.%2520Experiments%250Aon%2520the%2520LongerVideos%2520benchmark%252C%2520featuring%2520over%2520134%2520hours%2520of%2520diverse%2520content%252C%250Aconfirm%2520that%2520SceneRAG%2520substantially%2520outperforms%2520prior%2520baselines%252C%2520achieving%2520a%250Awin%2520rate%2520of%2520up%2520to%252072.5%2520percent%2520on%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneRAG%3A%20Scene-level%20Retrieval-Augmented%20Generation%20for%20Video%0A%20%20Understanding&entry.906535625=Nianbo%20Zeng%20and%20Haowen%20Hou%20and%20Fei%20Richard%20Yu%20and%20Si%20Shi%20and%20Ying%20Tiffany%20He&entry.1292438233=%20%20Despite%20recent%20advances%20in%20retrieval-augmented%20generation%20%28RAG%29%20for%20video%0Aunderstanding%2C%20effectively%20understanding%20long-form%20video%20content%20remains%0Aunderexplored%20due%20to%20the%20vast%20scale%20and%20high%20complexity%20of%20video%20data.%20Current%0ARAG%20approaches%20typically%20segment%20videos%20into%20fixed-length%20chunks%2C%20which%20often%0Adisrupts%20the%20continuity%20of%20contextual%20information%20and%20fails%20to%20capture%0Aauthentic%20scene%20boundaries.%20Inspired%20by%20the%20human%20ability%20to%20naturally%20organize%0Acontinuous%20experiences%20into%20coherent%20scenes%2C%20we%20present%20SceneRAG%2C%20a%20unified%0Aframework%20that%20leverages%20large%20language%20models%20to%20segment%20videos%20into%0Anarrative-consistent%20scenes%20by%20processing%20ASR%20transcripts%20alongside%20temporal%0Ametadata.%20SceneRAG%20further%20sharpens%20these%20initial%20boundaries%20through%0Alightweight%20heuristics%20and%20iterative%20correction.%20For%20each%20scene%2C%20the%20framework%0Afuses%20information%20from%20both%20visual%20and%20textual%20modalities%20to%20extract%20entity%0Arelations%20and%20dynamically%20builds%20a%20knowledge%20graph%2C%20enabling%20robust%20multi-hop%0Aretrieval%20and%20generation%20that%20account%20for%20long-range%20dependencies.%20Experiments%0Aon%20the%20LongerVideos%20benchmark%2C%20featuring%20over%20134%20hours%20of%20diverse%20content%2C%0Aconfirm%20that%20SceneRAG%20substantially%20outperforms%20prior%20baselines%2C%20achieving%20a%0Awin%20rate%20of%20up%20to%2072.5%20percent%20on%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07600v1&entry.124074799=Read"},
{"title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining\n  AutoRater and Critic-and-Revise Pipeline", "author": "Brian Gordon and Yonatan Bitton and Andreea Marzoca and Yasumasa Onoe and Xiao Wang and Daniel Cohen-Or and Idan Szpektor", "abstract": "  Large Vision-Language Models (VLMs) now generate highly detailed,\nparagraphlength image captions, yet evaluating their factual accuracy remains\nchallenging. Current methods often miss fine-grained errors, being designed for\nshorter texts or lacking datasets with verified inaccuracies. We introduce\nDOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100\nimages, 14 VLMs) featuring over 10,216 sentence-level human annotations of\nfactual correctness and explanatory rationales for errors, all within paragraph\ncontext. Building on this, we develop VNLI-Critique, a model for automated\nsentence-level factuality classification and critique generation. We highlight\nthree key applications: (1) VNLI-Critique demonstrates robust generalization,\nvalidated by state-of-the-art performance on the M-HalDetect benchmark and\nstrong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven\nAutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent\nalignment with human factuality judgments (e.g., 0.98 Spearman). (3) An\ninnovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide\nLLM-based corrections, achieves substantial improvements in caption factuality\n(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark\nalongside practical tools, designed to significantly elevate the standards for\nfine-grained evaluation and foster the improvement of VLM image understanding.\nProject page: https://google.github.io/unblocking-detail-caption\n", "link": "http://arxiv.org/abs/2506.07631v1", "date": "2025-06-09", "relevancy": 2.3233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5898}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unblocking%20Fine-Grained%20Evaluation%20of%20Detailed%20Captions%3A%20An%20Explaining%0A%20%20AutoRater%20and%20Critic-and-Revise%20Pipeline&body=Title%3A%20Unblocking%20Fine-Grained%20Evaluation%20of%20Detailed%20Captions%3A%20An%20Explaining%0A%20%20AutoRater%20and%20Critic-and-Revise%20Pipeline%0AAuthor%3A%20Brian%20Gordon%20and%20Yonatan%20Bitton%20and%20Andreea%20Marzoca%20and%20Yasumasa%20Onoe%20and%20Xiao%20Wang%20and%20Daniel%20Cohen-Or%20and%20Idan%20Szpektor%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20now%20generate%20highly%20detailed%2C%0Aparagraphlength%20image%20captions%2C%20yet%20evaluating%20their%20factual%20accuracy%20remains%0Achallenging.%20Current%20methods%20often%20miss%20fine-grained%20errors%2C%20being%20designed%20for%0Ashorter%20texts%20or%20lacking%20datasets%20with%20verified%20inaccuracies.%20We%20introduce%0ADOCCI-Critique%2C%20a%20benchmark%20with%201%2C400%20VLM-generated%20paragraph%20captions%20%28100%0Aimages%2C%2014%20VLMs%29%20featuring%20over%2010%2C216%20sentence-level%20human%20annotations%20of%0Afactual%20correctness%20and%20explanatory%20rationales%20for%20errors%2C%20all%20within%20paragraph%0Acontext.%20Building%20on%20this%2C%20we%20develop%20VNLI-Critique%2C%20a%20model%20for%20automated%0Asentence-level%20factuality%20classification%20and%20critique%20generation.%20We%20highlight%0Athree%20key%20applications%3A%20%281%29%20VNLI-Critique%20demonstrates%20robust%20generalization%2C%0Avalidated%20by%20state-of-the-art%20performance%20on%20the%20M-HalDetect%20benchmark%20and%0Astrong%20results%20in%20CHOCOLATE%20claim%20verification.%20%282%29%20The%20VNLI-Critique%20driven%0AAutoRater%20for%20DOCCI-Critique%20provides%20reliable%20VLM%20rankings%2C%20showing%20excellent%0Aalignment%20with%20human%20factuality%20judgments%20%28e.g.%2C%200.98%20Spearman%29.%20%283%29%20An%0Ainnovative%20Critic-and-Revise%20pipeline%2C%20where%20critiques%20from%20VNLI-Critique%20guide%0ALLM-based%20corrections%2C%20achieves%20substantial%20improvements%20in%20caption%20factuality%0A%28e.g.%2C%20a%2046%25%20gain%20on%20DetailCaps-4870%29.%20Our%20work%20offers%20a%20crucial%20benchmark%0Aalongside%20practical%20tools%2C%20designed%20to%20significantly%20elevate%20the%20standards%20for%0Afine-grained%20evaluation%20and%20foster%20the%20improvement%20of%20VLM%20image%20understanding.%0AProject%20page%3A%20https%3A//google.github.io/unblocking-detail-caption%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnblocking%2520Fine-Grained%2520Evaluation%2520of%2520Detailed%2520Captions%253A%2520An%2520Explaining%250A%2520%2520AutoRater%2520and%2520Critic-and-Revise%2520Pipeline%26entry.906535625%3DBrian%2520Gordon%2520and%2520Yonatan%2520Bitton%2520and%2520Andreea%2520Marzoca%2520and%2520Yasumasa%2520Onoe%2520and%2520Xiao%2520Wang%2520and%2520Daniel%2520Cohen-Or%2520and%2520Idan%2520Szpektor%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520now%2520generate%2520highly%2520detailed%252C%250Aparagraphlength%2520image%2520captions%252C%2520yet%2520evaluating%2520their%2520factual%2520accuracy%2520remains%250Achallenging.%2520Current%2520methods%2520often%2520miss%2520fine-grained%2520errors%252C%2520being%2520designed%2520for%250Ashorter%2520texts%2520or%2520lacking%2520datasets%2520with%2520verified%2520inaccuracies.%2520We%2520introduce%250ADOCCI-Critique%252C%2520a%2520benchmark%2520with%25201%252C400%2520VLM-generated%2520paragraph%2520captions%2520%2528100%250Aimages%252C%252014%2520VLMs%2529%2520featuring%2520over%252010%252C216%2520sentence-level%2520human%2520annotations%2520of%250Afactual%2520correctness%2520and%2520explanatory%2520rationales%2520for%2520errors%252C%2520all%2520within%2520paragraph%250Acontext.%2520Building%2520on%2520this%252C%2520we%2520develop%2520VNLI-Critique%252C%2520a%2520model%2520for%2520automated%250Asentence-level%2520factuality%2520classification%2520and%2520critique%2520generation.%2520We%2520highlight%250Athree%2520key%2520applications%253A%2520%25281%2529%2520VNLI-Critique%2520demonstrates%2520robust%2520generalization%252C%250Avalidated%2520by%2520state-of-the-art%2520performance%2520on%2520the%2520M-HalDetect%2520benchmark%2520and%250Astrong%2520results%2520in%2520CHOCOLATE%2520claim%2520verification.%2520%25282%2529%2520The%2520VNLI-Critique%2520driven%250AAutoRater%2520for%2520DOCCI-Critique%2520provides%2520reliable%2520VLM%2520rankings%252C%2520showing%2520excellent%250Aalignment%2520with%2520human%2520factuality%2520judgments%2520%2528e.g.%252C%25200.98%2520Spearman%2529.%2520%25283%2529%2520An%250Ainnovative%2520Critic-and-Revise%2520pipeline%252C%2520where%2520critiques%2520from%2520VNLI-Critique%2520guide%250ALLM-based%2520corrections%252C%2520achieves%2520substantial%2520improvements%2520in%2520caption%2520factuality%250A%2528e.g.%252C%2520a%252046%2525%2520gain%2520on%2520DetailCaps-4870%2529.%2520Our%2520work%2520offers%2520a%2520crucial%2520benchmark%250Aalongside%2520practical%2520tools%252C%2520designed%2520to%2520significantly%2520elevate%2520the%2520standards%2520for%250Afine-grained%2520evaluation%2520and%2520foster%2520the%2520improvement%2520of%2520VLM%2520image%2520understanding.%250AProject%2520page%253A%2520https%253A//google.github.io/unblocking-detail-caption%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unblocking%20Fine-Grained%20Evaluation%20of%20Detailed%20Captions%3A%20An%20Explaining%0A%20%20AutoRater%20and%20Critic-and-Revise%20Pipeline&entry.906535625=Brian%20Gordon%20and%20Yonatan%20Bitton%20and%20Andreea%20Marzoca%20and%20Yasumasa%20Onoe%20and%20Xiao%20Wang%20and%20Daniel%20Cohen-Or%20and%20Idan%20Szpektor&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20now%20generate%20highly%20detailed%2C%0Aparagraphlength%20image%20captions%2C%20yet%20evaluating%20their%20factual%20accuracy%20remains%0Achallenging.%20Current%20methods%20often%20miss%20fine-grained%20errors%2C%20being%20designed%20for%0Ashorter%20texts%20or%20lacking%20datasets%20with%20verified%20inaccuracies.%20We%20introduce%0ADOCCI-Critique%2C%20a%20benchmark%20with%201%2C400%20VLM-generated%20paragraph%20captions%20%28100%0Aimages%2C%2014%20VLMs%29%20featuring%20over%2010%2C216%20sentence-level%20human%20annotations%20of%0Afactual%20correctness%20and%20explanatory%20rationales%20for%20errors%2C%20all%20within%20paragraph%0Acontext.%20Building%20on%20this%2C%20we%20develop%20VNLI-Critique%2C%20a%20model%20for%20automated%0Asentence-level%20factuality%20classification%20and%20critique%20generation.%20We%20highlight%0Athree%20key%20applications%3A%20%281%29%20VNLI-Critique%20demonstrates%20robust%20generalization%2C%0Avalidated%20by%20state-of-the-art%20performance%20on%20the%20M-HalDetect%20benchmark%20and%0Astrong%20results%20in%20CHOCOLATE%20claim%20verification.%20%282%29%20The%20VNLI-Critique%20driven%0AAutoRater%20for%20DOCCI-Critique%20provides%20reliable%20VLM%20rankings%2C%20showing%20excellent%0Aalignment%20with%20human%20factuality%20judgments%20%28e.g.%2C%200.98%20Spearman%29.%20%283%29%20An%0Ainnovative%20Critic-and-Revise%20pipeline%2C%20where%20critiques%20from%20VNLI-Critique%20guide%0ALLM-based%20corrections%2C%20achieves%20substantial%20improvements%20in%20caption%20factuality%0A%28e.g.%2C%20a%2046%25%20gain%20on%20DetailCaps-4870%29.%20Our%20work%20offers%20a%20crucial%20benchmark%0Aalongside%20practical%20tools%2C%20designed%20to%20significantly%20elevate%20the%20standards%20for%0Afine-grained%20evaluation%20and%20foster%20the%20improvement%20of%20VLM%20image%20understanding.%0AProject%20page%3A%20https%3A//google.github.io/unblocking-detail-caption%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07631v1&entry.124074799=Read"},
{"title": "Evaluating Large Language Models on the Frame and Symbol Grounding\n  Problems: A Zero-shot Benchmark", "author": "Shoko Oka", "abstract": "  Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.\n", "link": "http://arxiv.org/abs/2506.07896v1", "date": "2025-06-09", "relevancy": 2.3228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Language%20Models%20on%20the%20Frame%20and%20Symbol%20Grounding%0A%20%20Problems%3A%20A%20Zero-shot%20Benchmark&body=Title%3A%20Evaluating%20Large%20Language%20Models%20on%20the%20Frame%20and%20Symbol%20Grounding%0A%20%20Problems%3A%20A%20Zero-shot%20Benchmark%0AAuthor%3A%20Shoko%20Oka%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revitalized%0Aphilosophical%20debates%20surrounding%20artificial%20intelligence.%20Two%20of%20the%20most%0Afundamental%20challenges%20-%20namely%2C%20the%20Frame%20Problem%20and%20the%20Symbol%20Grounding%0AProblem%20-%20have%20historically%20been%20viewed%20as%20unsolvable%20within%20traditional%0Asymbolic%20AI%20systems.%20This%20study%20investigates%20whether%20modern%20LLMs%20possess%20the%0Acognitive%20capacities%20required%20to%20address%20these%20problems.%20To%20do%20so%2C%20I%20designed%0Atwo%20benchmark%20tasks%20reflecting%20the%20philosophical%20core%20of%20each%20problem%2C%0Aadministered%20them%20under%20zero-shot%20conditions%20to%2013%20prominent%20LLMs%20%28both%20closed%0Aand%20open-source%29%2C%20and%20assessed%20the%20quality%20of%20the%20models%27%20outputs%20across%20five%0Atrials%20each.%20Responses%20were%20scored%20along%20multiple%20criteria%2C%20including%0Acontextual%20reasoning%2C%20semantic%20coherence%2C%20and%20information%20filtering.%20The%0Aresults%20demonstrate%20that%20while%20open-source%20models%20showed%20variability%20in%0Aperformance%20due%20to%20differences%20in%20model%20size%2C%20quantization%2C%20and%20instruction%0Atuning%2C%20several%20closed%20models%20consistently%20achieved%20high%20scores.%20These%20findings%0Asuggest%20that%20select%20modern%20LLMs%20may%20be%20acquiring%20capacities%20sufficient%20to%0Aproduce%20meaningful%20and%20stable%20responses%20to%20these%20long-standing%20theoretical%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Language%2520Models%2520on%2520the%2520Frame%2520and%2520Symbol%2520Grounding%250A%2520%2520Problems%253A%2520A%2520Zero-shot%2520Benchmark%26entry.906535625%3DShoko%2520Oka%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revitalized%250Aphilosophical%2520debates%2520surrounding%2520artificial%2520intelligence.%2520Two%2520of%2520the%2520most%250Afundamental%2520challenges%2520-%2520namely%252C%2520the%2520Frame%2520Problem%2520and%2520the%2520Symbol%2520Grounding%250AProblem%2520-%2520have%2520historically%2520been%2520viewed%2520as%2520unsolvable%2520within%2520traditional%250Asymbolic%2520AI%2520systems.%2520This%2520study%2520investigates%2520whether%2520modern%2520LLMs%2520possess%2520the%250Acognitive%2520capacities%2520required%2520to%2520address%2520these%2520problems.%2520To%2520do%2520so%252C%2520I%2520designed%250Atwo%2520benchmark%2520tasks%2520reflecting%2520the%2520philosophical%2520core%2520of%2520each%2520problem%252C%250Aadministered%2520them%2520under%2520zero-shot%2520conditions%2520to%252013%2520prominent%2520LLMs%2520%2528both%2520closed%250Aand%2520open-source%2529%252C%2520and%2520assessed%2520the%2520quality%2520of%2520the%2520models%2527%2520outputs%2520across%2520five%250Atrials%2520each.%2520Responses%2520were%2520scored%2520along%2520multiple%2520criteria%252C%2520including%250Acontextual%2520reasoning%252C%2520semantic%2520coherence%252C%2520and%2520information%2520filtering.%2520The%250Aresults%2520demonstrate%2520that%2520while%2520open-source%2520models%2520showed%2520variability%2520in%250Aperformance%2520due%2520to%2520differences%2520in%2520model%2520size%252C%2520quantization%252C%2520and%2520instruction%250Atuning%252C%2520several%2520closed%2520models%2520consistently%2520achieved%2520high%2520scores.%2520These%2520findings%250Asuggest%2520that%2520select%2520modern%2520LLMs%2520may%2520be%2520acquiring%2520capacities%2520sufficient%2520to%250Aproduce%2520meaningful%2520and%2520stable%2520responses%2520to%2520these%2520long-standing%2520theoretical%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Language%20Models%20on%20the%20Frame%20and%20Symbol%20Grounding%0A%20%20Problems%3A%20A%20Zero-shot%20Benchmark&entry.906535625=Shoko%20Oka&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revitalized%0Aphilosophical%20debates%20surrounding%20artificial%20intelligence.%20Two%20of%20the%20most%0Afundamental%20challenges%20-%20namely%2C%20the%20Frame%20Problem%20and%20the%20Symbol%20Grounding%0AProblem%20-%20have%20historically%20been%20viewed%20as%20unsolvable%20within%20traditional%0Asymbolic%20AI%20systems.%20This%20study%20investigates%20whether%20modern%20LLMs%20possess%20the%0Acognitive%20capacities%20required%20to%20address%20these%20problems.%20To%20do%20so%2C%20I%20designed%0Atwo%20benchmark%20tasks%20reflecting%20the%20philosophical%20core%20of%20each%20problem%2C%0Aadministered%20them%20under%20zero-shot%20conditions%20to%2013%20prominent%20LLMs%20%28both%20closed%0Aand%20open-source%29%2C%20and%20assessed%20the%20quality%20of%20the%20models%27%20outputs%20across%20five%0Atrials%20each.%20Responses%20were%20scored%20along%20multiple%20criteria%2C%20including%0Acontextual%20reasoning%2C%20semantic%20coherence%2C%20and%20information%20filtering.%20The%0Aresults%20demonstrate%20that%20while%20open-source%20models%20showed%20variability%20in%0Aperformance%20due%20to%20differences%20in%20model%20size%2C%20quantization%2C%20and%20instruction%0Atuning%2C%20several%20closed%20models%20consistently%20achieved%20high%20scores.%20These%20findings%0Asuggest%20that%20select%20modern%20LLMs%20may%20be%20acquiring%20capacities%20sufficient%20to%0Aproduce%20meaningful%20and%20stable%20responses%20to%20these%20long-standing%20theoretical%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07896v1&entry.124074799=Read"},
{"title": "Correlated Errors in Large Language Models", "author": "Elliot Kim and Avi Garg and Kenny Peng and Nikhil Garg", "abstract": "  Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.\n", "link": "http://arxiv.org/abs/2506.07962v1", "date": "2025-06-09", "relevancy": 2.32, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4674}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correlated%20Errors%20in%20Large%20Language%20Models&body=Title%3A%20Correlated%20Errors%20in%20Large%20Language%20Models%0AAuthor%3A%20Elliot%20Kim%20and%20Avi%20Garg%20and%20Kenny%20Peng%20and%20Nikhil%20Garg%0AAbstract%3A%20%20%20Diversity%20in%20training%20data%2C%20architecture%2C%20and%20providers%20is%20assumed%20to%0Amitigate%20homogeneity%20in%20LLMs.%20However%2C%20we%20lack%20empirical%20evidence%20on%20whether%0Adifferent%20LLMs%20differ%20meaningfully.%20We%20conduct%20a%20large-scale%20empirical%0Aevaluation%20on%20over%20350%20LLMs%20overall%2C%20using%20two%20popular%20leaderboards%20and%20a%0Aresume-screening%20task.%20We%20find%20substantial%20correlation%20in%20model%20errors%20--%20on%0Aone%20leaderboard%20dataset%2C%20models%20agree%2060%25%20of%20the%20time%20when%20both%20models%20err.%20We%0Aidentify%20factors%20driving%20model%20correlation%2C%20including%20shared%20architectures%20and%0Aproviders.%20Crucially%2C%20however%2C%20larger%20and%20more%20accurate%20models%20have%20highly%0Acorrelated%20errors%2C%20even%20with%20distinct%20architectures%20and%20providers.%20Finally%2C%20we%0Ashow%20the%20effects%20of%20correlation%20in%20two%20downstream%20tasks%3A%20LLM-as-judge%0Aevaluation%20and%20hiring%20--%20the%20latter%20reflecting%20theoretical%20predictions%0Aregarding%20algorithmic%20monoculture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrelated%2520Errors%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DElliot%2520Kim%2520and%2520Avi%2520Garg%2520and%2520Kenny%2520Peng%2520and%2520Nikhil%2520Garg%26entry.1292438233%3D%2520%2520Diversity%2520in%2520training%2520data%252C%2520architecture%252C%2520and%2520providers%2520is%2520assumed%2520to%250Amitigate%2520homogeneity%2520in%2520LLMs.%2520However%252C%2520we%2520lack%2520empirical%2520evidence%2520on%2520whether%250Adifferent%2520LLMs%2520differ%2520meaningfully.%2520We%2520conduct%2520a%2520large-scale%2520empirical%250Aevaluation%2520on%2520over%2520350%2520LLMs%2520overall%252C%2520using%2520two%2520popular%2520leaderboards%2520and%2520a%250Aresume-screening%2520task.%2520We%2520find%2520substantial%2520correlation%2520in%2520model%2520errors%2520--%2520on%250Aone%2520leaderboard%2520dataset%252C%2520models%2520agree%252060%2525%2520of%2520the%2520time%2520when%2520both%2520models%2520err.%2520We%250Aidentify%2520factors%2520driving%2520model%2520correlation%252C%2520including%2520shared%2520architectures%2520and%250Aproviders.%2520Crucially%252C%2520however%252C%2520larger%2520and%2520more%2520accurate%2520models%2520have%2520highly%250Acorrelated%2520errors%252C%2520even%2520with%2520distinct%2520architectures%2520and%2520providers.%2520Finally%252C%2520we%250Ashow%2520the%2520effects%2520of%2520correlation%2520in%2520two%2520downstream%2520tasks%253A%2520LLM-as-judge%250Aevaluation%2520and%2520hiring%2520--%2520the%2520latter%2520reflecting%2520theoretical%2520predictions%250Aregarding%2520algorithmic%2520monoculture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correlated%20Errors%20in%20Large%20Language%20Models&entry.906535625=Elliot%20Kim%20and%20Avi%20Garg%20and%20Kenny%20Peng%20and%20Nikhil%20Garg&entry.1292438233=%20%20Diversity%20in%20training%20data%2C%20architecture%2C%20and%20providers%20is%20assumed%20to%0Amitigate%20homogeneity%20in%20LLMs.%20However%2C%20we%20lack%20empirical%20evidence%20on%20whether%0Adifferent%20LLMs%20differ%20meaningfully.%20We%20conduct%20a%20large-scale%20empirical%0Aevaluation%20on%20over%20350%20LLMs%20overall%2C%20using%20two%20popular%20leaderboards%20and%20a%0Aresume-screening%20task.%20We%20find%20substantial%20correlation%20in%20model%20errors%20--%20on%0Aone%20leaderboard%20dataset%2C%20models%20agree%2060%25%20of%20the%20time%20when%20both%20models%20err.%20We%0Aidentify%20factors%20driving%20model%20correlation%2C%20including%20shared%20architectures%20and%0Aproviders.%20Crucially%2C%20however%2C%20larger%20and%20more%20accurate%20models%20have%20highly%0Acorrelated%20errors%2C%20even%20with%20distinct%20architectures%20and%20providers.%20Finally%2C%20we%0Ashow%20the%20effects%20of%20correlation%20in%20two%20downstream%20tasks%3A%20LLM-as-judge%0Aevaluation%20and%20hiring%20--%20the%20latter%20reflecting%20theoretical%20predictions%0Aregarding%20algorithmic%20monoculture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07962v1&entry.124074799=Read"},
{"title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance", "author": "Xuanfan Ni and Liyan Xu and Chenyang Lyu and Longyue Wang and Mo Yu and Lemao Liu and Fandong Meng and Jie Zhou and Piji Li", "abstract": "  To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.\n", "link": "http://arxiv.org/abs/2502.16886v2", "date": "2025-06-09", "relevancy": 2.3194, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4603}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBudgetKV%3A%20Dynamic%20Budget%20in%20KV%20Cache%20Compression%20for%20Ensuring%20Optimal%0A%20%20Performance&body=Title%3A%20DBudgetKV%3A%20Dynamic%20Budget%20in%20KV%20Cache%20Compression%20for%20Ensuring%20Optimal%0A%20%20Performance%0AAuthor%3A%20Xuanfan%20Ni%20and%20Liyan%20Xu%20and%20Chenyang%20Lyu%20and%20Longyue%20Wang%20and%20Mo%20Yu%20and%20Lemao%20Liu%20and%20Fandong%20Meng%20and%20Jie%20Zhou%20and%20Piji%20Li%0AAbstract%3A%20%20%20To%20alleviate%20memory%20burden%20during%20inference%20of%20large%20language%20models%20%28LLMs%29%2C%0Anumerous%20studies%20have%20focused%20on%20compressing%20the%20KV%20cache%20by%20exploring%20aspects%0Asuch%20as%20attention%20sparsity.%20These%20techniques%20are%20often%20designed%20with%20a%0Apre-defined%20KV%20budget%3B%20however%2C%20as%20the%20optimal%20budget%20varies%20by%20different%20input%0Alengths%20and%20task%20types%2C%20the%20existence%20of%20a%20fixed%20budget%20could%20result%20in%0Ainconsistent%20performance%20accepting%20inputs%20of%20diverse%20domains.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20new%20KV%20cache%20compression%20objective%3A%20to%20always%20ensure%0Athe%20full-cache%20performance%20regardless%20of%20specific%20inputs%2C%20while%20maximizing%20KV%0Acache%20pruning%20as%20much%20as%20possible.%20To%20achieve%20this%20goal%2C%20we%20introduce%20a%20novel%0AKV%20cache%20compression%20method%20dubbed%20DBudgetKV%2C%20which%20features%20an%20attention-based%0Ametric%20to%20signal%20when%20the%20remaining%20KV%20cache%20is%20unlikely%20to%20match%20the%0Afull-cache%20performance%2C%20then%20halting%20the%20pruning%20process.%20Empirical%20evaluation%0Aspanning%20diverse%20context%20lengths%2C%20task%20types%2C%20and%20model%20sizes%20suggests%20that%20our%0Amethod%20achieves%20lossless%20KV%20pruning%20effectively%20and%20robustly%2C%20exceeding%2025%25%0Acompression%20ratio%20on%20average.%20Furthermore%2C%20our%20method%20is%20easy%20to%20integrate%0Awithin%20LLM%20inference%2C%20not%20only%20optimizing%20memory%20space%2C%20but%20also%20showing%0Areduced%20inference%20time%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBudgetKV%253A%2520Dynamic%2520Budget%2520in%2520KV%2520Cache%2520Compression%2520for%2520Ensuring%2520Optimal%250A%2520%2520Performance%26entry.906535625%3DXuanfan%2520Ni%2520and%2520Liyan%2520Xu%2520and%2520Chenyang%2520Lyu%2520and%2520Longyue%2520Wang%2520and%2520Mo%2520Yu%2520and%2520Lemao%2520Liu%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%2520and%2520Piji%2520Li%26entry.1292438233%3D%2520%2520To%2520alleviate%2520memory%2520burden%2520during%2520inference%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Anumerous%2520studies%2520have%2520focused%2520on%2520compressing%2520the%2520KV%2520cache%2520by%2520exploring%2520aspects%250Asuch%2520as%2520attention%2520sparsity.%2520These%2520techniques%2520are%2520often%2520designed%2520with%2520a%250Apre-defined%2520KV%2520budget%253B%2520however%252C%2520as%2520the%2520optimal%2520budget%2520varies%2520by%2520different%2520input%250Alengths%2520and%2520task%2520types%252C%2520the%2520existence%2520of%2520a%2520fixed%2520budget%2520could%2520result%2520in%250Ainconsistent%2520performance%2520accepting%2520inputs%2520of%2520diverse%2520domains.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520new%2520KV%2520cache%2520compression%2520objective%253A%2520to%2520always%2520ensure%250Athe%2520full-cache%2520performance%2520regardless%2520of%2520specific%2520inputs%252C%2520while%2520maximizing%2520KV%250Acache%2520pruning%2520as%2520much%2520as%2520possible.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520introduce%2520a%2520novel%250AKV%2520cache%2520compression%2520method%2520dubbed%2520DBudgetKV%252C%2520which%2520features%2520an%2520attention-based%250Ametric%2520to%2520signal%2520when%2520the%2520remaining%2520KV%2520cache%2520is%2520unlikely%2520to%2520match%2520the%250Afull-cache%2520performance%252C%2520then%2520halting%2520the%2520pruning%2520process.%2520Empirical%2520evaluation%250Aspanning%2520diverse%2520context%2520lengths%252C%2520task%2520types%252C%2520and%2520model%2520sizes%2520suggests%2520that%2520our%250Amethod%2520achieves%2520lossless%2520KV%2520pruning%2520effectively%2520and%2520robustly%252C%2520exceeding%252025%2525%250Acompression%2520ratio%2520on%2520average.%2520Furthermore%252C%2520our%2520method%2520is%2520easy%2520to%2520integrate%250Awithin%2520LLM%2520inference%252C%2520not%2520only%2520optimizing%2520memory%2520space%252C%2520but%2520also%2520showing%250Areduced%2520inference%2520time%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBudgetKV%3A%20Dynamic%20Budget%20in%20KV%20Cache%20Compression%20for%20Ensuring%20Optimal%0A%20%20Performance&entry.906535625=Xuanfan%20Ni%20and%20Liyan%20Xu%20and%20Chenyang%20Lyu%20and%20Longyue%20Wang%20and%20Mo%20Yu%20and%20Lemao%20Liu%20and%20Fandong%20Meng%20and%20Jie%20Zhou%20and%20Piji%20Li&entry.1292438233=%20%20To%20alleviate%20memory%20burden%20during%20inference%20of%20large%20language%20models%20%28LLMs%29%2C%0Anumerous%20studies%20have%20focused%20on%20compressing%20the%20KV%20cache%20by%20exploring%20aspects%0Asuch%20as%20attention%20sparsity.%20These%20techniques%20are%20often%20designed%20with%20a%0Apre-defined%20KV%20budget%3B%20however%2C%20as%20the%20optimal%20budget%20varies%20by%20different%20input%0Alengths%20and%20task%20types%2C%20the%20existence%20of%20a%20fixed%20budget%20could%20result%20in%0Ainconsistent%20performance%20accepting%20inputs%20of%20diverse%20domains.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20new%20KV%20cache%20compression%20objective%3A%20to%20always%20ensure%0Athe%20full-cache%20performance%20regardless%20of%20specific%20inputs%2C%20while%20maximizing%20KV%0Acache%20pruning%20as%20much%20as%20possible.%20To%20achieve%20this%20goal%2C%20we%20introduce%20a%20novel%0AKV%20cache%20compression%20method%20dubbed%20DBudgetKV%2C%20which%20features%20an%20attention-based%0Ametric%20to%20signal%20when%20the%20remaining%20KV%20cache%20is%20unlikely%20to%20match%20the%0Afull-cache%20performance%2C%20then%20halting%20the%20pruning%20process.%20Empirical%20evaluation%0Aspanning%20diverse%20context%20lengths%2C%20task%20types%2C%20and%20model%20sizes%20suggests%20that%20our%0Amethod%20achieves%20lossless%20KV%20pruning%20effectively%20and%20robustly%2C%20exceeding%2025%25%0Acompression%20ratio%20on%20average.%20Furthermore%2C%20our%20method%20is%20easy%20to%20integrate%0Awithin%20LLM%20inference%2C%20not%20only%20optimizing%20memory%20space%2C%20but%20also%20showing%0Areduced%20inference%20time%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16886v2&entry.124074799=Read"},
{"title": "Dynamic View Synthesis as an Inverse Problem", "author": "Hidir Yesiltepe and Pinar Yanardag", "abstract": "  In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.\n", "link": "http://arxiv.org/abs/2506.08004v1", "date": "2025-06-09", "relevancy": 2.3135, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6137}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5534}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20View%20Synthesis%20as%20an%20Inverse%20Problem&body=Title%3A%20Dynamic%20View%20Synthesis%20as%20an%20Inverse%20Problem%0AAuthor%3A%20Hidir%20Yesiltepe%20and%20Pinar%20Yanardag%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20dynamic%20view%20synthesis%20from%20monocular%20videos%20as%20an%0Ainverse%20problem%20in%20a%20training-free%20setting.%20By%20redesigning%20the%20noise%0Ainitialization%20phase%20of%20a%20pre-trained%20video%20diffusion%20model%2C%20we%20enable%0Ahigh-fidelity%20dynamic%20view%20synthesis%20without%20any%20weight%20updates%20or%20auxiliary%0Amodules.%20We%20begin%20by%20identifying%20a%20fundamental%20obstacle%20to%20deterministic%0Ainversion%20arising%20from%20zero-terminal%20signal-to-noise%20ratio%20%28SNR%29%20schedules%20and%0Aresolve%20it%20by%20introducing%20a%20novel%20noise%20representation%2C%20termed%20K-order%0ARecursive%20Noise%20Representation.%20We%20derive%20a%20closed%20form%20expression%20for%20this%0Arepresentation%2C%20enabling%20precise%20and%20efficient%20alignment%20between%20the%0AVAE-encoded%20and%20the%20DDIM%20inverted%20latents.%20To%20synthesize%20newly%20visible%20regions%0Aresulting%20from%20camera%20motion%2C%20we%20introduce%20Stochastic%20Latent%20Modulation%2C%20which%0Aperforms%20visibility%20aware%20sampling%20over%20the%20latent%20space%20to%20complete%20occluded%0Aregions.%20Comprehensive%20experiments%20demonstrate%20that%20dynamic%20view%20synthesis%20can%0Abe%20effectively%20performed%20through%20structured%20latent%20manipulation%20in%20the%20noise%0Ainitialization%20phase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520View%2520Synthesis%2520as%2520an%2520Inverse%2520Problem%26entry.906535625%3DHidir%2520Yesiltepe%2520and%2520Pinar%2520Yanardag%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520dynamic%2520view%2520synthesis%2520from%2520monocular%2520videos%2520as%2520an%250Ainverse%2520problem%2520in%2520a%2520training-free%2520setting.%2520By%2520redesigning%2520the%2520noise%250Ainitialization%2520phase%2520of%2520a%2520pre-trained%2520video%2520diffusion%2520model%252C%2520we%2520enable%250Ahigh-fidelity%2520dynamic%2520view%2520synthesis%2520without%2520any%2520weight%2520updates%2520or%2520auxiliary%250Amodules.%2520We%2520begin%2520by%2520identifying%2520a%2520fundamental%2520obstacle%2520to%2520deterministic%250Ainversion%2520arising%2520from%2520zero-terminal%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520schedules%2520and%250Aresolve%2520it%2520by%2520introducing%2520a%2520novel%2520noise%2520representation%252C%2520termed%2520K-order%250ARecursive%2520Noise%2520Representation.%2520We%2520derive%2520a%2520closed%2520form%2520expression%2520for%2520this%250Arepresentation%252C%2520enabling%2520precise%2520and%2520efficient%2520alignment%2520between%2520the%250AVAE-encoded%2520and%2520the%2520DDIM%2520inverted%2520latents.%2520To%2520synthesize%2520newly%2520visible%2520regions%250Aresulting%2520from%2520camera%2520motion%252C%2520we%2520introduce%2520Stochastic%2520Latent%2520Modulation%252C%2520which%250Aperforms%2520visibility%2520aware%2520sampling%2520over%2520the%2520latent%2520space%2520to%2520complete%2520occluded%250Aregions.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520dynamic%2520view%2520synthesis%2520can%250Abe%2520effectively%2520performed%2520through%2520structured%2520latent%2520manipulation%2520in%2520the%2520noise%250Ainitialization%2520phase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20View%20Synthesis%20as%20an%20Inverse%20Problem&entry.906535625=Hidir%20Yesiltepe%20and%20Pinar%20Yanardag&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20dynamic%20view%20synthesis%20from%20monocular%20videos%20as%20an%0Ainverse%20problem%20in%20a%20training-free%20setting.%20By%20redesigning%20the%20noise%0Ainitialization%20phase%20of%20a%20pre-trained%20video%20diffusion%20model%2C%20we%20enable%0Ahigh-fidelity%20dynamic%20view%20synthesis%20without%20any%20weight%20updates%20or%20auxiliary%0Amodules.%20We%20begin%20by%20identifying%20a%20fundamental%20obstacle%20to%20deterministic%0Ainversion%20arising%20from%20zero-terminal%20signal-to-noise%20ratio%20%28SNR%29%20schedules%20and%0Aresolve%20it%20by%20introducing%20a%20novel%20noise%20representation%2C%20termed%20K-order%0ARecursive%20Noise%20Representation.%20We%20derive%20a%20closed%20form%20expression%20for%20this%0Arepresentation%2C%20enabling%20precise%20and%20efficient%20alignment%20between%20the%0AVAE-encoded%20and%20the%20DDIM%20inverted%20latents.%20To%20synthesize%20newly%20visible%20regions%0Aresulting%20from%20camera%20motion%2C%20we%20introduce%20Stochastic%20Latent%20Modulation%2C%20which%0Aperforms%20visibility%20aware%20sampling%20over%20the%20latent%20space%20to%20complete%20occluded%0Aregions.%20Comprehensive%20experiments%20demonstrate%20that%20dynamic%20view%20synthesis%20can%0Abe%20effectively%20performed%20through%20structured%20latent%20manipulation%20in%20the%20noise%0Ainitialization%20phase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08004v1&entry.124074799=Read"},
{"title": "ZeroVO: Visual Odometry with Minimal Assumptions", "author": "Lei Lai and Zekai Yin and Eshed Ohn-Bar", "abstract": "  We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves\nzero-shot generalization across diverse cameras and environments, overcoming\nlimitations in existing methods that depend on predefined or static camera\ncalibration setups. Our approach incorporates three main innovations. First, we\ndesign a calibration-free, geometry-aware network structure capable of handling\nnoise in estimated depth and camera parameters. Second, we introduce a\nlanguage-based prior that infuses semantic information to enhance robust\nfeature extraction and generalization to previously unseen domains. Third, we\ndevelop a flexible, semi-supervised training paradigm that iteratively adapts\nto new scenes using unlabeled data, further boosting the models' ability to\ngeneralize across diverse real-world scenarios. We analyze complex autonomous\ndriving contexts, demonstrating over 30% improvement against prior methods on\nthree standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly\nintroduced, high-fidelity synthetic dataset derived from Grand Theft Auto\n(GTA). By not requiring fine-tuning or camera calibration, our work broadens\nthe applicability of VO, providing a versatile solution for real-world\ndeployment at scale.\n", "link": "http://arxiv.org/abs/2506.08005v1", "date": "2025-06-09", "relevancy": 2.3071, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5868}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.58}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroVO%3A%20Visual%20Odometry%20with%20Minimal%20Assumptions&body=Title%3A%20ZeroVO%3A%20Visual%20Odometry%20with%20Minimal%20Assumptions%0AAuthor%3A%20Lei%20Lai%20and%20Zekai%20Yin%20and%20Eshed%20Ohn-Bar%0AAbstract%3A%20%20%20We%20introduce%20ZeroVO%2C%20a%20novel%20visual%20odometry%20%28VO%29%20algorithm%20that%20achieves%0Azero-shot%20generalization%20across%20diverse%20cameras%20and%20environments%2C%20overcoming%0Alimitations%20in%20existing%20methods%20that%20depend%20on%20predefined%20or%20static%20camera%0Acalibration%20setups.%20Our%20approach%20incorporates%20three%20main%20innovations.%20First%2C%20we%0Adesign%20a%20calibration-free%2C%20geometry-aware%20network%20structure%20capable%20of%20handling%0Anoise%20in%20estimated%20depth%20and%20camera%20parameters.%20Second%2C%20we%20introduce%20a%0Alanguage-based%20prior%20that%20infuses%20semantic%20information%20to%20enhance%20robust%0Afeature%20extraction%20and%20generalization%20to%20previously%20unseen%20domains.%20Third%2C%20we%0Adevelop%20a%20flexible%2C%20semi-supervised%20training%20paradigm%20that%20iteratively%20adapts%0Ato%20new%20scenes%20using%20unlabeled%20data%2C%20further%20boosting%20the%20models%27%20ability%20to%0Ageneralize%20across%20diverse%20real-world%20scenarios.%20We%20analyze%20complex%20autonomous%0Adriving%20contexts%2C%20demonstrating%20over%2030%25%20improvement%20against%20prior%20methods%20on%0Athree%20standard%20benchmarks%2C%20KITTI%2C%20nuScenes%2C%20and%20Argoverse%202%2C%20as%20well%20as%20a%20newly%0Aintroduced%2C%20high-fidelity%20synthetic%20dataset%20derived%20from%20Grand%20Theft%20Auto%0A%28GTA%29.%20By%20not%20requiring%20fine-tuning%20or%20camera%20calibration%2C%20our%20work%20broadens%0Athe%20applicability%20of%20VO%2C%20providing%20a%20versatile%20solution%20for%20real-world%0Adeployment%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroVO%253A%2520Visual%2520Odometry%2520with%2520Minimal%2520Assumptions%26entry.906535625%3DLei%2520Lai%2520and%2520Zekai%2520Yin%2520and%2520Eshed%2520Ohn-Bar%26entry.1292438233%3D%2520%2520We%2520introduce%2520ZeroVO%252C%2520a%2520novel%2520visual%2520odometry%2520%2528VO%2529%2520algorithm%2520that%2520achieves%250Azero-shot%2520generalization%2520across%2520diverse%2520cameras%2520and%2520environments%252C%2520overcoming%250Alimitations%2520in%2520existing%2520methods%2520that%2520depend%2520on%2520predefined%2520or%2520static%2520camera%250Acalibration%2520setups.%2520Our%2520approach%2520incorporates%2520three%2520main%2520innovations.%2520First%252C%2520we%250Adesign%2520a%2520calibration-free%252C%2520geometry-aware%2520network%2520structure%2520capable%2520of%2520handling%250Anoise%2520in%2520estimated%2520depth%2520and%2520camera%2520parameters.%2520Second%252C%2520we%2520introduce%2520a%250Alanguage-based%2520prior%2520that%2520infuses%2520semantic%2520information%2520to%2520enhance%2520robust%250Afeature%2520extraction%2520and%2520generalization%2520to%2520previously%2520unseen%2520domains.%2520Third%252C%2520we%250Adevelop%2520a%2520flexible%252C%2520semi-supervised%2520training%2520paradigm%2520that%2520iteratively%2520adapts%250Ato%2520new%2520scenes%2520using%2520unlabeled%2520data%252C%2520further%2520boosting%2520the%2520models%2527%2520ability%2520to%250Ageneralize%2520across%2520diverse%2520real-world%2520scenarios.%2520We%2520analyze%2520complex%2520autonomous%250Adriving%2520contexts%252C%2520demonstrating%2520over%252030%2525%2520improvement%2520against%2520prior%2520methods%2520on%250Athree%2520standard%2520benchmarks%252C%2520KITTI%252C%2520nuScenes%252C%2520and%2520Argoverse%25202%252C%2520as%2520well%2520as%2520a%2520newly%250Aintroduced%252C%2520high-fidelity%2520synthetic%2520dataset%2520derived%2520from%2520Grand%2520Theft%2520Auto%250A%2528GTA%2529.%2520By%2520not%2520requiring%2520fine-tuning%2520or%2520camera%2520calibration%252C%2520our%2520work%2520broadens%250Athe%2520applicability%2520of%2520VO%252C%2520providing%2520a%2520versatile%2520solution%2520for%2520real-world%250Adeployment%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroVO%3A%20Visual%20Odometry%20with%20Minimal%20Assumptions&entry.906535625=Lei%20Lai%20and%20Zekai%20Yin%20and%20Eshed%20Ohn-Bar&entry.1292438233=%20%20We%20introduce%20ZeroVO%2C%20a%20novel%20visual%20odometry%20%28VO%29%20algorithm%20that%20achieves%0Azero-shot%20generalization%20across%20diverse%20cameras%20and%20environments%2C%20overcoming%0Alimitations%20in%20existing%20methods%20that%20depend%20on%20predefined%20or%20static%20camera%0Acalibration%20setups.%20Our%20approach%20incorporates%20three%20main%20innovations.%20First%2C%20we%0Adesign%20a%20calibration-free%2C%20geometry-aware%20network%20structure%20capable%20of%20handling%0Anoise%20in%20estimated%20depth%20and%20camera%20parameters.%20Second%2C%20we%20introduce%20a%0Alanguage-based%20prior%20that%20infuses%20semantic%20information%20to%20enhance%20robust%0Afeature%20extraction%20and%20generalization%20to%20previously%20unseen%20domains.%20Third%2C%20we%0Adevelop%20a%20flexible%2C%20semi-supervised%20training%20paradigm%20that%20iteratively%20adapts%0Ato%20new%20scenes%20using%20unlabeled%20data%2C%20further%20boosting%20the%20models%27%20ability%20to%0Ageneralize%20across%20diverse%20real-world%20scenarios.%20We%20analyze%20complex%20autonomous%0Adriving%20contexts%2C%20demonstrating%20over%2030%25%20improvement%20against%20prior%20methods%20on%0Athree%20standard%20benchmarks%2C%20KITTI%2C%20nuScenes%2C%20and%20Argoverse%202%2C%20as%20well%20as%20a%20newly%0Aintroduced%2C%20high-fidelity%20synthetic%20dataset%20derived%20from%20Grand%20Theft%20Auto%0A%28GTA%29.%20By%20not%20requiring%20fine-tuning%20or%20camera%20calibration%2C%20our%20work%20broadens%0Athe%20applicability%20of%20VO%2C%20providing%20a%20versatile%20solution%20for%20real-world%0Adeployment%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08005v1&entry.124074799=Read"},
{"title": "Re-ranking Reasoning Context with Tree Search Makes Large\n  Vision-Language Models Stronger", "author": "Qi Yang and Chenghao Zhang and Lubin Fan and Kun Ding and Jieping Ye and Shiming Xiang", "abstract": "  Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.\n", "link": "http://arxiv.org/abs/2506.07785v1", "date": "2025-06-09", "relevancy": 2.3052, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-ranking%20Reasoning%20Context%20with%20Tree%20Search%20Makes%20Large%0A%20%20Vision-Language%20Models%20Stronger&body=Title%3A%20Re-ranking%20Reasoning%20Context%20with%20Tree%20Search%20Makes%20Large%0A%20%20Vision-Language%20Models%20Stronger%0AAuthor%3A%20Qi%20Yang%20and%20Chenghao%20Zhang%20and%20Lubin%20Fan%20and%20Kun%20Ding%20and%20Jieping%20Ye%20and%20Shiming%20Xiang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%0Asignificantly%20improved%20performance%20in%20Visual%20Question%20Answering%20%28VQA%29%20tasks%0Athrough%20multimodal%20Retrieval-Augmented%20Generation%20%28RAG%29.%20However%2C%20existing%0Amethods%20still%20face%20challenges%2C%20such%20as%20the%20scarcity%20of%20knowledge%20with%20reasoning%0Aexamples%20and%20erratic%20responses%20from%20retrieved%20knowledge.%20To%20address%20these%0Aissues%2C%20in%20this%20study%2C%20we%20propose%20a%20multimodal%20RAG%20framework%2C%20termed%20RCTS%2C%0Awhich%20enhances%20LVLMs%20by%20constructing%20a%20Reasoning%20Context-enriched%20knowledge%0Abase%20and%20a%20Tree%20Search%20re-ranking%20method.%20Specifically%2C%20we%20introduce%20a%0Aself-consistent%20evaluation%20mechanism%20to%20enrich%20the%20knowledge%20base%20with%0Aintrinsic%20reasoning%20patterns.%20We%20further%20propose%20a%20Monte%20Carlo%20Tree%20Search%20with%0AHeuristic%20Rewards%20%28MCTS-HR%29%20to%20prioritize%20the%20most%20relevant%20examples.%20This%0Aensures%20that%20LVLMs%20can%20leverage%20high-quality%20contextual%20reasoning%20for%20better%0Aand%20more%20consistent%20responses.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20achieves%20state-of-the-art%20performance%20on%20multiple%20VQA%20datasets%2C%0Asignificantly%20outperforming%20In-Context%20Learning%20%28ICL%29%20and%20Vanilla-RAG%20methods.%0AIt%20highlights%20the%20effectiveness%20of%20our%20knowledge%20base%20and%20re-ranking%20method%20in%0Aimproving%20LVLMs.%20Our%20code%20is%20available%20at%20https%3A//github.com/yannqi/RCTS-RAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-ranking%2520Reasoning%2520Context%2520with%2520Tree%2520Search%2520Makes%2520Large%250A%2520%2520Vision-Language%2520Models%2520Stronger%26entry.906535625%3DQi%2520Yang%2520and%2520Chenghao%2520Zhang%2520and%2520Lubin%2520Fan%2520and%2520Kun%2520Ding%2520and%2520Jieping%2520Ye%2520and%2520Shiming%2520Xiang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%250Asignificantly%2520improved%2520performance%2520in%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520tasks%250Athrough%2520multimodal%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529.%2520However%252C%2520existing%250Amethods%2520still%2520face%2520challenges%252C%2520such%2520as%2520the%2520scarcity%2520of%2520knowledge%2520with%2520reasoning%250Aexamples%2520and%2520erratic%2520responses%2520from%2520retrieved%2520knowledge.%2520To%2520address%2520these%250Aissues%252C%2520in%2520this%2520study%252C%2520we%2520propose%2520a%2520multimodal%2520RAG%2520framework%252C%2520termed%2520RCTS%252C%250Awhich%2520enhances%2520LVLMs%2520by%2520constructing%2520a%2520Reasoning%2520Context-enriched%2520knowledge%250Abase%2520and%2520a%2520Tree%2520Search%2520re-ranking%2520method.%2520Specifically%252C%2520we%2520introduce%2520a%250Aself-consistent%2520evaluation%2520mechanism%2520to%2520enrich%2520the%2520knowledge%2520base%2520with%250Aintrinsic%2520reasoning%2520patterns.%2520We%2520further%2520propose%2520a%2520Monte%2520Carlo%2520Tree%2520Search%2520with%250AHeuristic%2520Rewards%2520%2528MCTS-HR%2529%2520to%2520prioritize%2520the%2520most%2520relevant%2520examples.%2520This%250Aensures%2520that%2520LVLMs%2520can%2520leverage%2520high-quality%2520contextual%2520reasoning%2520for%2520better%250Aand%2520more%2520consistent%2520responses.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aframework%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520VQA%2520datasets%252C%250Asignificantly%2520outperforming%2520In-Context%2520Learning%2520%2528ICL%2529%2520and%2520Vanilla-RAG%2520methods.%250AIt%2520highlights%2520the%2520effectiveness%2520of%2520our%2520knowledge%2520base%2520and%2520re-ranking%2520method%2520in%250Aimproving%2520LVLMs.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/yannqi/RCTS-RAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-ranking%20Reasoning%20Context%20with%20Tree%20Search%20Makes%20Large%0A%20%20Vision-Language%20Models%20Stronger&entry.906535625=Qi%20Yang%20and%20Chenghao%20Zhang%20and%20Lubin%20Fan%20and%20Kun%20Ding%20and%20Jieping%20Ye%20and%20Shiming%20Xiang&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%0Asignificantly%20improved%20performance%20in%20Visual%20Question%20Answering%20%28VQA%29%20tasks%0Athrough%20multimodal%20Retrieval-Augmented%20Generation%20%28RAG%29.%20However%2C%20existing%0Amethods%20still%20face%20challenges%2C%20such%20as%20the%20scarcity%20of%20knowledge%20with%20reasoning%0Aexamples%20and%20erratic%20responses%20from%20retrieved%20knowledge.%20To%20address%20these%0Aissues%2C%20in%20this%20study%2C%20we%20propose%20a%20multimodal%20RAG%20framework%2C%20termed%20RCTS%2C%0Awhich%20enhances%20LVLMs%20by%20constructing%20a%20Reasoning%20Context-enriched%20knowledge%0Abase%20and%20a%20Tree%20Search%20re-ranking%20method.%20Specifically%2C%20we%20introduce%20a%0Aself-consistent%20evaluation%20mechanism%20to%20enrich%20the%20knowledge%20base%20with%0Aintrinsic%20reasoning%20patterns.%20We%20further%20propose%20a%20Monte%20Carlo%20Tree%20Search%20with%0AHeuristic%20Rewards%20%28MCTS-HR%29%20to%20prioritize%20the%20most%20relevant%20examples.%20This%0Aensures%20that%20LVLMs%20can%20leverage%20high-quality%20contextual%20reasoning%20for%20better%0Aand%20more%20consistent%20responses.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20achieves%20state-of-the-art%20performance%20on%20multiple%20VQA%20datasets%2C%0Asignificantly%20outperforming%20In-Context%20Learning%20%28ICL%29%20and%20Vanilla-RAG%20methods.%0AIt%20highlights%20the%20effectiveness%20of%20our%20knowledge%20base%20and%20re-ranking%20method%20in%0Aimproving%20LVLMs.%20Our%20code%20is%20available%20at%20https%3A//github.com/yannqi/RCTS-RAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07785v1&entry.124074799=Read"},
{"title": "Real-time Localization of a Soccer Ball from a Single Camera", "author": "Dmitrii Vorobev and Artem Prosvetov and Karim Elhadji Daou", "abstract": "  We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.\n", "link": "http://arxiv.org/abs/2506.07981v1", "date": "2025-06-09", "relevancy": 2.3032, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5949}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5924}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Localization%20of%20a%20Soccer%20Ball%20from%20a%20Single%20Camera&body=Title%3A%20Real-time%20Localization%20of%20a%20Soccer%20Ball%20from%20a%20Single%20Camera%0AAuthor%3A%20Dmitrii%20Vorobev%20and%20Artem%20Prosvetov%20and%20Karim%20Elhadji%20Daou%0AAbstract%3A%20%20%20We%20propose%20a%20computationally%20efficient%20method%20for%20real-time%20three-dimensional%0Afootball%20trajectory%20reconstruction%20from%20a%20single%20broadcast%20camera.%20In%20contrast%0Ato%20previous%20work%2C%20our%20approach%20introduces%20a%20multi-mode%20state%20model%20with%20%24W%24%0Adiscrete%20modes%20to%20significantly%20accelerate%20optimization%20while%20preserving%0Acentimeter-level%20accuracy%20--%20even%20in%20cases%20of%20severe%20occlusion%2C%20motion%20blur%2C%0Aand%20complex%20backgrounds.%20The%20system%20operates%20on%20standard%20CPUs%20and%20achieves%20low%0Alatency%20suitable%20for%20live%20broadcast%20settings.%20Extensive%20evaluation%20on%20a%0Aproprietary%20dataset%20of%206K-resolution%20Russian%20Premier%20League%20matches%0Ademonstrates%20performance%20comparable%20to%20multi-camera%20systems%2C%20without%20the%20need%0Afor%20specialized%20or%20costly%20infrastructure.%20This%20work%20provides%20a%20practical%20method%0Afor%20accessible%20and%20accurate%203D%20ball%20tracking%20in%20professional%20football%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Localization%2520of%2520a%2520Soccer%2520Ball%2520from%2520a%2520Single%2520Camera%26entry.906535625%3DDmitrii%2520Vorobev%2520and%2520Artem%2520Prosvetov%2520and%2520Karim%2520Elhadji%2520Daou%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520computationally%2520efficient%2520method%2520for%2520real-time%2520three-dimensional%250Afootball%2520trajectory%2520reconstruction%2520from%2520a%2520single%2520broadcast%2520camera.%2520In%2520contrast%250Ato%2520previous%2520work%252C%2520our%2520approach%2520introduces%2520a%2520multi-mode%2520state%2520model%2520with%2520%2524W%2524%250Adiscrete%2520modes%2520to%2520significantly%2520accelerate%2520optimization%2520while%2520preserving%250Acentimeter-level%2520accuracy%2520--%2520even%2520in%2520cases%2520of%2520severe%2520occlusion%252C%2520motion%2520blur%252C%250Aand%2520complex%2520backgrounds.%2520The%2520system%2520operates%2520on%2520standard%2520CPUs%2520and%2520achieves%2520low%250Alatency%2520suitable%2520for%2520live%2520broadcast%2520settings.%2520Extensive%2520evaluation%2520on%2520a%250Aproprietary%2520dataset%2520of%25206K-resolution%2520Russian%2520Premier%2520League%2520matches%250Ademonstrates%2520performance%2520comparable%2520to%2520multi-camera%2520systems%252C%2520without%2520the%2520need%250Afor%2520specialized%2520or%2520costly%2520infrastructure.%2520This%2520work%2520provides%2520a%2520practical%2520method%250Afor%2520accessible%2520and%2520accurate%25203D%2520ball%2520tracking%2520in%2520professional%2520football%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Localization%20of%20a%20Soccer%20Ball%20from%20a%20Single%20Camera&entry.906535625=Dmitrii%20Vorobev%20and%20Artem%20Prosvetov%20and%20Karim%20Elhadji%20Daou&entry.1292438233=%20%20We%20propose%20a%20computationally%20efficient%20method%20for%20real-time%20three-dimensional%0Afootball%20trajectory%20reconstruction%20from%20a%20single%20broadcast%20camera.%20In%20contrast%0Ato%20previous%20work%2C%20our%20approach%20introduces%20a%20multi-mode%20state%20model%20with%20%24W%24%0Adiscrete%20modes%20to%20significantly%20accelerate%20optimization%20while%20preserving%0Acentimeter-level%20accuracy%20--%20even%20in%20cases%20of%20severe%20occlusion%2C%20motion%20blur%2C%0Aand%20complex%20backgrounds.%20The%20system%20operates%20on%20standard%20CPUs%20and%20achieves%20low%0Alatency%20suitable%20for%20live%20broadcast%20settings.%20Extensive%20evaluation%20on%20a%0Aproprietary%20dataset%20of%206K-resolution%20Russian%20Premier%20League%20matches%0Ademonstrates%20performance%20comparable%20to%20multi-camera%20systems%2C%20without%20the%20need%0Afor%20specialized%20or%20costly%20infrastructure.%20This%20work%20provides%20a%20practical%20method%0Afor%20accessible%20and%20accurate%203D%20ball%20tracking%20in%20professional%20football%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07981v1&entry.124074799=Read"},
{"title": "Hyperpruning: Efficient Search through Pruned Variants of Recurrent\n  Neural Networks Leveraging Lyapunov Spectrum", "author": "Caleb Zheng and Eli Shlizerman", "abstract": "  A variety of pruning methods have been introduced for over-parameterized\nRecurrent Neural Networks to improve efficiency in terms of power consumption\nand storage utilization. These advances motivate a new paradigm, termed\n`hyperpruning', which seeks to identify the most suitable pruning strategy for\na given network architecture and application. Unlike conventional\nhyperparameter search, where the optimal configuration's accuracy remains\nuncertain, in the context of network pruning, the accuracy of the dense model\nsets the target for the accuracy of the pruned one. The goal, therefore, is to\ndiscover pruned variants that match or even surpass this established accuracy.\nHowever, exhaustive search over pruning configurations is computationally\nexpensive and lacks early performance guarantees. To address this challenge, we\npropose a novel Lyapunov Spectrum (LS)-based distance metric that enables early\ncomparison between pruned and dense networks, allowing accurate prediction of\npost-training performance. By integrating this LS-based distance with standard\nhyperparameter optimization algorithms, we introduce an efficient hyperpruning\nframework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an\norder of magnitude compared to conventional approaches relying on full\ntraining. Experiments on stacked LSTM and RHN architectures using the Penn\nTreebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under\nfixed training budgets and target pruning ratios, LSH consistently identifies\nsuperior pruned models. Remarkably, these pruned variants not only outperform\nthose selected by loss-based baseline but also exceed the performance of their\ndense counterpart.\n", "link": "http://arxiv.org/abs/2506.07975v1", "date": "2025-06-09", "relevancy": 2.2946, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4734}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4545}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperpruning%3A%20Efficient%20Search%20through%20Pruned%20Variants%20of%20Recurrent%0A%20%20Neural%20Networks%20Leveraging%20Lyapunov%20Spectrum&body=Title%3A%20Hyperpruning%3A%20Efficient%20Search%20through%20Pruned%20Variants%20of%20Recurrent%0A%20%20Neural%20Networks%20Leveraging%20Lyapunov%20Spectrum%0AAuthor%3A%20Caleb%20Zheng%20and%20Eli%20Shlizerman%0AAbstract%3A%20%20%20A%20variety%20of%20pruning%20methods%20have%20been%20introduced%20for%20over-parameterized%0ARecurrent%20Neural%20Networks%20to%20improve%20efficiency%20in%20terms%20of%20power%20consumption%0Aand%20storage%20utilization.%20These%20advances%20motivate%20a%20new%20paradigm%2C%20termed%0A%60hyperpruning%27%2C%20which%20seeks%20to%20identify%20the%20most%20suitable%20pruning%20strategy%20for%0Aa%20given%20network%20architecture%20and%20application.%20Unlike%20conventional%0Ahyperparameter%20search%2C%20where%20the%20optimal%20configuration%27s%20accuracy%20remains%0Auncertain%2C%20in%20the%20context%20of%20network%20pruning%2C%20the%20accuracy%20of%20the%20dense%20model%0Asets%20the%20target%20for%20the%20accuracy%20of%20the%20pruned%20one.%20The%20goal%2C%20therefore%2C%20is%20to%0Adiscover%20pruned%20variants%20that%20match%20or%20even%20surpass%20this%20established%20accuracy.%0AHowever%2C%20exhaustive%20search%20over%20pruning%20configurations%20is%20computationally%0Aexpensive%20and%20lacks%20early%20performance%20guarantees.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20Lyapunov%20Spectrum%20%28LS%29-based%20distance%20metric%20that%20enables%20early%0Acomparison%20between%20pruned%20and%20dense%20networks%2C%20allowing%20accurate%20prediction%20of%0Apost-training%20performance.%20By%20integrating%20this%20LS-based%20distance%20with%20standard%0Ahyperparameter%20optimization%20algorithms%2C%20we%20introduce%20an%20efficient%20hyperpruning%0Aframework%2C%20termed%20LS-based%20Hyperpruning%20%28LSH%29.%20LSH%20reduces%20search%20time%20by%20an%0Aorder%20of%20magnitude%20compared%20to%20conventional%20approaches%20relying%20on%20full%0Atraining.%20Experiments%20on%20stacked%20LSTM%20and%20RHN%20architectures%20using%20the%20Penn%0ATreebank%20dataset%2C%20and%20on%20AWD-LSTM-MoS%20using%20WikiText-2%2C%20demonstrate%20that%20under%0Afixed%20training%20budgets%20and%20target%20pruning%20ratios%2C%20LSH%20consistently%20identifies%0Asuperior%20pruned%20models.%20Remarkably%2C%20these%20pruned%20variants%20not%20only%20outperform%0Athose%20selected%20by%20loss-based%20baseline%20but%20also%20exceed%20the%20performance%20of%20their%0Adense%20counterpart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperpruning%253A%2520Efficient%2520Search%2520through%2520Pruned%2520Variants%2520of%2520Recurrent%250A%2520%2520Neural%2520Networks%2520Leveraging%2520Lyapunov%2520Spectrum%26entry.906535625%3DCaleb%2520Zheng%2520and%2520Eli%2520Shlizerman%26entry.1292438233%3D%2520%2520A%2520variety%2520of%2520pruning%2520methods%2520have%2520been%2520introduced%2520for%2520over-parameterized%250ARecurrent%2520Neural%2520Networks%2520to%2520improve%2520efficiency%2520in%2520terms%2520of%2520power%2520consumption%250Aand%2520storage%2520utilization.%2520These%2520advances%2520motivate%2520a%2520new%2520paradigm%252C%2520termed%250A%2560hyperpruning%2527%252C%2520which%2520seeks%2520to%2520identify%2520the%2520most%2520suitable%2520pruning%2520strategy%2520for%250Aa%2520given%2520network%2520architecture%2520and%2520application.%2520Unlike%2520conventional%250Ahyperparameter%2520search%252C%2520where%2520the%2520optimal%2520configuration%2527s%2520accuracy%2520remains%250Auncertain%252C%2520in%2520the%2520context%2520of%2520network%2520pruning%252C%2520the%2520accuracy%2520of%2520the%2520dense%2520model%250Asets%2520the%2520target%2520for%2520the%2520accuracy%2520of%2520the%2520pruned%2520one.%2520The%2520goal%252C%2520therefore%252C%2520is%2520to%250Adiscover%2520pruned%2520variants%2520that%2520match%2520or%2520even%2520surpass%2520this%2520established%2520accuracy.%250AHowever%252C%2520exhaustive%2520search%2520over%2520pruning%2520configurations%2520is%2520computationally%250Aexpensive%2520and%2520lacks%2520early%2520performance%2520guarantees.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520novel%2520Lyapunov%2520Spectrum%2520%2528LS%2529-based%2520distance%2520metric%2520that%2520enables%2520early%250Acomparison%2520between%2520pruned%2520and%2520dense%2520networks%252C%2520allowing%2520accurate%2520prediction%2520of%250Apost-training%2520performance.%2520By%2520integrating%2520this%2520LS-based%2520distance%2520with%2520standard%250Ahyperparameter%2520optimization%2520algorithms%252C%2520we%2520introduce%2520an%2520efficient%2520hyperpruning%250Aframework%252C%2520termed%2520LS-based%2520Hyperpruning%2520%2528LSH%2529.%2520LSH%2520reduces%2520search%2520time%2520by%2520an%250Aorder%2520of%2520magnitude%2520compared%2520to%2520conventional%2520approaches%2520relying%2520on%2520full%250Atraining.%2520Experiments%2520on%2520stacked%2520LSTM%2520and%2520RHN%2520architectures%2520using%2520the%2520Penn%250ATreebank%2520dataset%252C%2520and%2520on%2520AWD-LSTM-MoS%2520using%2520WikiText-2%252C%2520demonstrate%2520that%2520under%250Afixed%2520training%2520budgets%2520and%2520target%2520pruning%2520ratios%252C%2520LSH%2520consistently%2520identifies%250Asuperior%2520pruned%2520models.%2520Remarkably%252C%2520these%2520pruned%2520variants%2520not%2520only%2520outperform%250Athose%2520selected%2520by%2520loss-based%2520baseline%2520but%2520also%2520exceed%2520the%2520performance%2520of%2520their%250Adense%2520counterpart.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperpruning%3A%20Efficient%20Search%20through%20Pruned%20Variants%20of%20Recurrent%0A%20%20Neural%20Networks%20Leveraging%20Lyapunov%20Spectrum&entry.906535625=Caleb%20Zheng%20and%20Eli%20Shlizerman&entry.1292438233=%20%20A%20variety%20of%20pruning%20methods%20have%20been%20introduced%20for%20over-parameterized%0ARecurrent%20Neural%20Networks%20to%20improve%20efficiency%20in%20terms%20of%20power%20consumption%0Aand%20storage%20utilization.%20These%20advances%20motivate%20a%20new%20paradigm%2C%20termed%0A%60hyperpruning%27%2C%20which%20seeks%20to%20identify%20the%20most%20suitable%20pruning%20strategy%20for%0Aa%20given%20network%20architecture%20and%20application.%20Unlike%20conventional%0Ahyperparameter%20search%2C%20where%20the%20optimal%20configuration%27s%20accuracy%20remains%0Auncertain%2C%20in%20the%20context%20of%20network%20pruning%2C%20the%20accuracy%20of%20the%20dense%20model%0Asets%20the%20target%20for%20the%20accuracy%20of%20the%20pruned%20one.%20The%20goal%2C%20therefore%2C%20is%20to%0Adiscover%20pruned%20variants%20that%20match%20or%20even%20surpass%20this%20established%20accuracy.%0AHowever%2C%20exhaustive%20search%20over%20pruning%20configurations%20is%20computationally%0Aexpensive%20and%20lacks%20early%20performance%20guarantees.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20Lyapunov%20Spectrum%20%28LS%29-based%20distance%20metric%20that%20enables%20early%0Acomparison%20between%20pruned%20and%20dense%20networks%2C%20allowing%20accurate%20prediction%20of%0Apost-training%20performance.%20By%20integrating%20this%20LS-based%20distance%20with%20standard%0Ahyperparameter%20optimization%20algorithms%2C%20we%20introduce%20an%20efficient%20hyperpruning%0Aframework%2C%20termed%20LS-based%20Hyperpruning%20%28LSH%29.%20LSH%20reduces%20search%20time%20by%20an%0Aorder%20of%20magnitude%20compared%20to%20conventional%20approaches%20relying%20on%20full%0Atraining.%20Experiments%20on%20stacked%20LSTM%20and%20RHN%20architectures%20using%20the%20Penn%0ATreebank%20dataset%2C%20and%20on%20AWD-LSTM-MoS%20using%20WikiText-2%2C%20demonstrate%20that%20under%0Afixed%20training%20budgets%20and%20target%20pruning%20ratios%2C%20LSH%20consistently%20identifies%0Asuperior%20pruned%20models.%20Remarkably%2C%20these%20pruned%20variants%20not%20only%20outperform%0Athose%20selected%20by%20loss-based%20baseline%20but%20also%20exceed%20the%20performance%20of%20their%0Adense%20counterpart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07975v1&entry.124074799=Read"},
{"title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning", "author": "Yichang Xu and Gaowen Liu and Ramana Rao Kompella and Sihao Hu and Tiansheng Huang and Fatih Ilhan and Selim Furkan Tekin and Zachary Yahn and Ling Liu", "abstract": "  The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal visual-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date suffer from\ngeneralization performance. Inspired by recent development in LLMs for visual\nreasoning, this paper presents VLAgent, an AI system that can create a\nstep-by-step visual reasoning plan with an easy-to-understand script and\nexecute each step of the plan in real time by integrating planning script with\nexecution verifications via an automated process supported by VLAgent. In the\ntask planning phase, VLAgent fine-tunes an LLM through in-context learning to\ngenerate a step-by-step planner for each user-submitted text-visual reasoning\ntask. During the plan execution phase, VLAgent progressively refines the\ncomposition of neuro-symbolic executable modules to generate high-confidence\nreasoning results. VLAgent has three unique design characteristics: First, we\nimprove the quality of plan generation through in-context learning, improving\nlogic reasoning by reducing erroneous logic steps, incorrect programs, and LLM\nhallucinations. Second, we design a syntax-semantics parser to identify and\ncorrect additional logic errors of the LLM-generated planning script prior to\nlaunching the plan executor. Finally, we employ the ensemble method to improve\nthe generalization performance of our step-executor. Extensive experiments with\nfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent\nachieves significant performance enhancement for multimodal text-visual\nreasoning applications, compared to the exiting representative VLMs and LLM\nbased visual composition approaches like ViperGPT and VisProg, thanks to the\nnovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,\nOutput Verifiers). Code and data will be made available upon paper acceptance.\n", "link": "http://arxiv.org/abs/2506.07778v1", "date": "2025-06-09", "relevancy": 2.281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Vision%20Planner%20and%20Executor%20for%20Text-to-Visual%20Reasoning&body=Title%3A%20Language-Vision%20Planner%20and%20Executor%20for%20Text-to-Visual%20Reasoning%0AAuthor%3A%20Yichang%20Xu%20and%20Gaowen%20Liu%20and%20Ramana%20Rao%20Kompella%20and%20Sihao%20Hu%20and%20Tiansheng%20Huang%20and%20Fatih%20Ilhan%20and%20Selim%20Furkan%20Tekin%20and%20Zachary%20Yahn%20and%20Ling%20Liu%0AAbstract%3A%20%20%20The%20advancement%20in%20large%20language%20models%20%28LLMs%29%20and%20large%20vision%20models%20has%0Afueled%20the%20rapid%20progress%20in%20multi-modal%20visual-text%20reasoning%20capabilities.%0AHowever%2C%20existing%20vision-language%20models%20%28VLMs%29%20to%20date%20suffer%20from%0Ageneralization%20performance.%20Inspired%20by%20recent%20development%20in%20LLMs%20for%20visual%0Areasoning%2C%20this%20paper%20presents%20VLAgent%2C%20an%20AI%20system%20that%20can%20create%20a%0Astep-by-step%20visual%20reasoning%20plan%20with%20an%20easy-to-understand%20script%20and%0Aexecute%20each%20step%20of%20the%20plan%20in%20real%20time%20by%20integrating%20planning%20script%20with%0Aexecution%20verifications%20via%20an%20automated%20process%20supported%20by%20VLAgent.%20In%20the%0Atask%20planning%20phase%2C%20VLAgent%20fine-tunes%20an%20LLM%20through%20in-context%20learning%20to%0Agenerate%20a%20step-by-step%20planner%20for%20each%20user-submitted%20text-visual%20reasoning%0Atask.%20During%20the%20plan%20execution%20phase%2C%20VLAgent%20progressively%20refines%20the%0Acomposition%20of%20neuro-symbolic%20executable%20modules%20to%20generate%20high-confidence%0Areasoning%20results.%20VLAgent%20has%20three%20unique%20design%20characteristics%3A%20First%2C%20we%0Aimprove%20the%20quality%20of%20plan%20generation%20through%20in-context%20learning%2C%20improving%0Alogic%20reasoning%20by%20reducing%20erroneous%20logic%20steps%2C%20incorrect%20programs%2C%20and%20LLM%0Ahallucinations.%20Second%2C%20we%20design%20a%20syntax-semantics%20parser%20to%20identify%20and%0Acorrect%20additional%20logic%20errors%20of%20the%20LLM-generated%20planning%20script%20prior%20to%0Alaunching%20the%20plan%20executor.%20Finally%2C%20we%20employ%20the%20ensemble%20method%20to%20improve%0Athe%20generalization%20performance%20of%20our%20step-executor.%20Extensive%20experiments%20with%0Afour%20visual%20reasoning%20benchmarks%20%28GQA%2C%20MME%2C%20NLVR2%2C%20VQAv2%29%20show%20that%20VLAgent%0Aachieves%20significant%20performance%20enhancement%20for%20multimodal%20text-visual%0Areasoning%20applications%2C%20compared%20to%20the%20exiting%20representative%20VLMs%20and%20LLM%0Abased%20visual%20composition%20approaches%20like%20ViperGPT%20and%20VisProg%2C%20thanks%20to%20the%0Anovel%20optimization%20modules%20of%20VLAgent%20back-engine%20%28SS-Parser%2C%20Plan%20Repairer%2C%0AOutput%20Verifiers%29.%20Code%20and%20data%20will%20be%20made%20available%20upon%20paper%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Vision%2520Planner%2520and%2520Executor%2520for%2520Text-to-Visual%2520Reasoning%26entry.906535625%3DYichang%2520Xu%2520and%2520Gaowen%2520Liu%2520and%2520Ramana%2520Rao%2520Kompella%2520and%2520Sihao%2520Hu%2520and%2520Tiansheng%2520Huang%2520and%2520Fatih%2520Ilhan%2520and%2520Selim%2520Furkan%2520Tekin%2520and%2520Zachary%2520Yahn%2520and%2520Ling%2520Liu%26entry.1292438233%3D%2520%2520The%2520advancement%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520large%2520vision%2520models%2520has%250Afueled%2520the%2520rapid%2520progress%2520in%2520multi-modal%2520visual-text%2520reasoning%2520capabilities.%250AHowever%252C%2520existing%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520date%2520suffer%2520from%250Ageneralization%2520performance.%2520Inspired%2520by%2520recent%2520development%2520in%2520LLMs%2520for%2520visual%250Areasoning%252C%2520this%2520paper%2520presents%2520VLAgent%252C%2520an%2520AI%2520system%2520that%2520can%2520create%2520a%250Astep-by-step%2520visual%2520reasoning%2520plan%2520with%2520an%2520easy-to-understand%2520script%2520and%250Aexecute%2520each%2520step%2520of%2520the%2520plan%2520in%2520real%2520time%2520by%2520integrating%2520planning%2520script%2520with%250Aexecution%2520verifications%2520via%2520an%2520automated%2520process%2520supported%2520by%2520VLAgent.%2520In%2520the%250Atask%2520planning%2520phase%252C%2520VLAgent%2520fine-tunes%2520an%2520LLM%2520through%2520in-context%2520learning%2520to%250Agenerate%2520a%2520step-by-step%2520planner%2520for%2520each%2520user-submitted%2520text-visual%2520reasoning%250Atask.%2520During%2520the%2520plan%2520execution%2520phase%252C%2520VLAgent%2520progressively%2520refines%2520the%250Acomposition%2520of%2520neuro-symbolic%2520executable%2520modules%2520to%2520generate%2520high-confidence%250Areasoning%2520results.%2520VLAgent%2520has%2520three%2520unique%2520design%2520characteristics%253A%2520First%252C%2520we%250Aimprove%2520the%2520quality%2520of%2520plan%2520generation%2520through%2520in-context%2520learning%252C%2520improving%250Alogic%2520reasoning%2520by%2520reducing%2520erroneous%2520logic%2520steps%252C%2520incorrect%2520programs%252C%2520and%2520LLM%250Ahallucinations.%2520Second%252C%2520we%2520design%2520a%2520syntax-semantics%2520parser%2520to%2520identify%2520and%250Acorrect%2520additional%2520logic%2520errors%2520of%2520the%2520LLM-generated%2520planning%2520script%2520prior%2520to%250Alaunching%2520the%2520plan%2520executor.%2520Finally%252C%2520we%2520employ%2520the%2520ensemble%2520method%2520to%2520improve%250Athe%2520generalization%2520performance%2520of%2520our%2520step-executor.%2520Extensive%2520experiments%2520with%250Afour%2520visual%2520reasoning%2520benchmarks%2520%2528GQA%252C%2520MME%252C%2520NLVR2%252C%2520VQAv2%2529%2520show%2520that%2520VLAgent%250Aachieves%2520significant%2520performance%2520enhancement%2520for%2520multimodal%2520text-visual%250Areasoning%2520applications%252C%2520compared%2520to%2520the%2520exiting%2520representative%2520VLMs%2520and%2520LLM%250Abased%2520visual%2520composition%2520approaches%2520like%2520ViperGPT%2520and%2520VisProg%252C%2520thanks%2520to%2520the%250Anovel%2520optimization%2520modules%2520of%2520VLAgent%2520back-engine%2520%2528SS-Parser%252C%2520Plan%2520Repairer%252C%250AOutput%2520Verifiers%2529.%2520Code%2520and%2520data%2520will%2520be%2520made%2520available%2520upon%2520paper%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Vision%20Planner%20and%20Executor%20for%20Text-to-Visual%20Reasoning&entry.906535625=Yichang%20Xu%20and%20Gaowen%20Liu%20and%20Ramana%20Rao%20Kompella%20and%20Sihao%20Hu%20and%20Tiansheng%20Huang%20and%20Fatih%20Ilhan%20and%20Selim%20Furkan%20Tekin%20and%20Zachary%20Yahn%20and%20Ling%20Liu&entry.1292438233=%20%20The%20advancement%20in%20large%20language%20models%20%28LLMs%29%20and%20large%20vision%20models%20has%0Afueled%20the%20rapid%20progress%20in%20multi-modal%20visual-text%20reasoning%20capabilities.%0AHowever%2C%20existing%20vision-language%20models%20%28VLMs%29%20to%20date%20suffer%20from%0Ageneralization%20performance.%20Inspired%20by%20recent%20development%20in%20LLMs%20for%20visual%0Areasoning%2C%20this%20paper%20presents%20VLAgent%2C%20an%20AI%20system%20that%20can%20create%20a%0Astep-by-step%20visual%20reasoning%20plan%20with%20an%20easy-to-understand%20script%20and%0Aexecute%20each%20step%20of%20the%20plan%20in%20real%20time%20by%20integrating%20planning%20script%20with%0Aexecution%20verifications%20via%20an%20automated%20process%20supported%20by%20VLAgent.%20In%20the%0Atask%20planning%20phase%2C%20VLAgent%20fine-tunes%20an%20LLM%20through%20in-context%20learning%20to%0Agenerate%20a%20step-by-step%20planner%20for%20each%20user-submitted%20text-visual%20reasoning%0Atask.%20During%20the%20plan%20execution%20phase%2C%20VLAgent%20progressively%20refines%20the%0Acomposition%20of%20neuro-symbolic%20executable%20modules%20to%20generate%20high-confidence%0Areasoning%20results.%20VLAgent%20has%20three%20unique%20design%20characteristics%3A%20First%2C%20we%0Aimprove%20the%20quality%20of%20plan%20generation%20through%20in-context%20learning%2C%20improving%0Alogic%20reasoning%20by%20reducing%20erroneous%20logic%20steps%2C%20incorrect%20programs%2C%20and%20LLM%0Ahallucinations.%20Second%2C%20we%20design%20a%20syntax-semantics%20parser%20to%20identify%20and%0Acorrect%20additional%20logic%20errors%20of%20the%20LLM-generated%20planning%20script%20prior%20to%0Alaunching%20the%20plan%20executor.%20Finally%2C%20we%20employ%20the%20ensemble%20method%20to%20improve%0Athe%20generalization%20performance%20of%20our%20step-executor.%20Extensive%20experiments%20with%0Afour%20visual%20reasoning%20benchmarks%20%28GQA%2C%20MME%2C%20NLVR2%2C%20VQAv2%29%20show%20that%20VLAgent%0Aachieves%20significant%20performance%20enhancement%20for%20multimodal%20text-visual%0Areasoning%20applications%2C%20compared%20to%20the%20exiting%20representative%20VLMs%20and%20LLM%0Abased%20visual%20composition%20approaches%20like%20ViperGPT%20and%20VisProg%2C%20thanks%20to%20the%0Anovel%20optimization%20modules%20of%20VLAgent%20back-engine%20%28SS-Parser%2C%20Plan%20Repairer%2C%0AOutput%20Verifiers%29.%20Code%20and%20data%20will%20be%20made%20available%20upon%20paper%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07778v1&entry.124074799=Read"},
{"title": "ArchiLense: A Framework for Quantitative Analysis of Architectural\n  Styles Based on Vision Large Language Models", "author": "Jing Zhong and Jun Yin and Peilin Li and Pengyu Zeng and Miao Zhang and Shuai Lu and Ran Luo", "abstract": "  Architectural cultures across regions are characterized by stylistic\ndiversity, shaped by historical, social, and technological contexts in addition\nto geograph-ical conditions. Understanding architectural styles requires the\nability to describe and analyze the stylistic features of different architects\nfrom various regions through visual observations of architectural imagery.\nHowever, traditional studies of architectural culture have largely relied on\nsubjective expert interpretations and historical literature reviews, often\nsuffering from regional biases and limited ex-planatory scope. To address these\nchallenges, this study proposes three core contributions: (1) We construct a\nprofessional architectural style dataset named ArchDiffBench, which comprises\n1,765 high-quality architectural images and their corresponding style\nannotations, collected from different regions and historical periods. (2) We\npropose ArchiLense, an analytical framework grounded in Vision-Language Models\nand constructed using the ArchDiffBench dataset. By integrating ad-vanced\ncomputer vision techniques, deep learning, and machine learning algo-rithms,\nArchiLense enables automatic recognition, comparison, and precise\nclassi-fication of architectural imagery, producing descriptive language\noutputs that ar-ticulate stylistic differences. (3) Extensive evaluations show\nthat ArchiLense achieves strong performance in architectural style recognition,\nwith a 92.4% con-sistency rate with expert annotations and 84.5% classification\naccuracy, effec-tively capturing stylistic distinctions across images. The\nproposed approach transcends the subjectivity inherent in traditional analyses\nand offers a more objective and accurate perspective for comparative studies of\narchitectural culture.\n", "link": "http://arxiv.org/abs/2506.07739v1", "date": "2025-06-09", "relevancy": 2.2695, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArchiLense%3A%20A%20Framework%20for%20Quantitative%20Analysis%20of%20Architectural%0A%20%20Styles%20Based%20on%20Vision%20Large%20Language%20Models&body=Title%3A%20ArchiLense%3A%20A%20Framework%20for%20Quantitative%20Analysis%20of%20Architectural%0A%20%20Styles%20Based%20on%20Vision%20Large%20Language%20Models%0AAuthor%3A%20Jing%20Zhong%20and%20Jun%20Yin%20and%20Peilin%20Li%20and%20Pengyu%20Zeng%20and%20Miao%20Zhang%20and%20Shuai%20Lu%20and%20Ran%20Luo%0AAbstract%3A%20%20%20Architectural%20cultures%20across%20regions%20are%20characterized%20by%20stylistic%0Adiversity%2C%20shaped%20by%20historical%2C%20social%2C%20and%20technological%20contexts%20in%20addition%0Ato%20geograph-ical%20conditions.%20Understanding%20architectural%20styles%20requires%20the%0Aability%20to%20describe%20and%20analyze%20the%20stylistic%20features%20of%20different%20architects%0Afrom%20various%20regions%20through%20visual%20observations%20of%20architectural%20imagery.%0AHowever%2C%20traditional%20studies%20of%20architectural%20culture%20have%20largely%20relied%20on%0Asubjective%20expert%20interpretations%20and%20historical%20literature%20reviews%2C%20often%0Asuffering%20from%20regional%20biases%20and%20limited%20ex-planatory%20scope.%20To%20address%20these%0Achallenges%2C%20this%20study%20proposes%20three%20core%20contributions%3A%20%281%29%20We%20construct%20a%0Aprofessional%20architectural%20style%20dataset%20named%20ArchDiffBench%2C%20which%20comprises%0A1%2C765%20high-quality%20architectural%20images%20and%20their%20corresponding%20style%0Aannotations%2C%20collected%20from%20different%20regions%20and%20historical%20periods.%20%282%29%20We%0Apropose%20ArchiLense%2C%20an%20analytical%20framework%20grounded%20in%20Vision-Language%20Models%0Aand%20constructed%20using%20the%20ArchDiffBench%20dataset.%20By%20integrating%20ad-vanced%0Acomputer%20vision%20techniques%2C%20deep%20learning%2C%20and%20machine%20learning%20algo-rithms%2C%0AArchiLense%20enables%20automatic%20recognition%2C%20comparison%2C%20and%20precise%0Aclassi-fication%20of%20architectural%20imagery%2C%20producing%20descriptive%20language%0Aoutputs%20that%20ar-ticulate%20stylistic%20differences.%20%283%29%20Extensive%20evaluations%20show%0Athat%20ArchiLense%20achieves%20strong%20performance%20in%20architectural%20style%20recognition%2C%0Awith%20a%2092.4%25%20con-sistency%20rate%20with%20expert%20annotations%20and%2084.5%25%20classification%0Aaccuracy%2C%20effec-tively%20capturing%20stylistic%20distinctions%20across%20images.%20The%0Aproposed%20approach%20transcends%20the%20subjectivity%20inherent%20in%20traditional%20analyses%0Aand%20offers%20a%20more%20objective%20and%20accurate%20perspective%20for%20comparative%20studies%20of%0Aarchitectural%20culture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchiLense%253A%2520A%2520Framework%2520for%2520Quantitative%2520Analysis%2520of%2520Architectural%250A%2520%2520Styles%2520Based%2520on%2520Vision%2520Large%2520Language%2520Models%26entry.906535625%3DJing%2520Zhong%2520and%2520Jun%2520Yin%2520and%2520Peilin%2520Li%2520and%2520Pengyu%2520Zeng%2520and%2520Miao%2520Zhang%2520and%2520Shuai%2520Lu%2520and%2520Ran%2520Luo%26entry.1292438233%3D%2520%2520Architectural%2520cultures%2520across%2520regions%2520are%2520characterized%2520by%2520stylistic%250Adiversity%252C%2520shaped%2520by%2520historical%252C%2520social%252C%2520and%2520technological%2520contexts%2520in%2520addition%250Ato%2520geograph-ical%2520conditions.%2520Understanding%2520architectural%2520styles%2520requires%2520the%250Aability%2520to%2520describe%2520and%2520analyze%2520the%2520stylistic%2520features%2520of%2520different%2520architects%250Afrom%2520various%2520regions%2520through%2520visual%2520observations%2520of%2520architectural%2520imagery.%250AHowever%252C%2520traditional%2520studies%2520of%2520architectural%2520culture%2520have%2520largely%2520relied%2520on%250Asubjective%2520expert%2520interpretations%2520and%2520historical%2520literature%2520reviews%252C%2520often%250Asuffering%2520from%2520regional%2520biases%2520and%2520limited%2520ex-planatory%2520scope.%2520To%2520address%2520these%250Achallenges%252C%2520this%2520study%2520proposes%2520three%2520core%2520contributions%253A%2520%25281%2529%2520We%2520construct%2520a%250Aprofessional%2520architectural%2520style%2520dataset%2520named%2520ArchDiffBench%252C%2520which%2520comprises%250A1%252C765%2520high-quality%2520architectural%2520images%2520and%2520their%2520corresponding%2520style%250Aannotations%252C%2520collected%2520from%2520different%2520regions%2520and%2520historical%2520periods.%2520%25282%2529%2520We%250Apropose%2520ArchiLense%252C%2520an%2520analytical%2520framework%2520grounded%2520in%2520Vision-Language%2520Models%250Aand%2520constructed%2520using%2520the%2520ArchDiffBench%2520dataset.%2520By%2520integrating%2520ad-vanced%250Acomputer%2520vision%2520techniques%252C%2520deep%2520learning%252C%2520and%2520machine%2520learning%2520algo-rithms%252C%250AArchiLense%2520enables%2520automatic%2520recognition%252C%2520comparison%252C%2520and%2520precise%250Aclassi-fication%2520of%2520architectural%2520imagery%252C%2520producing%2520descriptive%2520language%250Aoutputs%2520that%2520ar-ticulate%2520stylistic%2520differences.%2520%25283%2529%2520Extensive%2520evaluations%2520show%250Athat%2520ArchiLense%2520achieves%2520strong%2520performance%2520in%2520architectural%2520style%2520recognition%252C%250Awith%2520a%252092.4%2525%2520con-sistency%2520rate%2520with%2520expert%2520annotations%2520and%252084.5%2525%2520classification%250Aaccuracy%252C%2520effec-tively%2520capturing%2520stylistic%2520distinctions%2520across%2520images.%2520The%250Aproposed%2520approach%2520transcends%2520the%2520subjectivity%2520inherent%2520in%2520traditional%2520analyses%250Aand%2520offers%2520a%2520more%2520objective%2520and%2520accurate%2520perspective%2520for%2520comparative%2520studies%2520of%250Aarchitectural%2520culture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArchiLense%3A%20A%20Framework%20for%20Quantitative%20Analysis%20of%20Architectural%0A%20%20Styles%20Based%20on%20Vision%20Large%20Language%20Models&entry.906535625=Jing%20Zhong%20and%20Jun%20Yin%20and%20Peilin%20Li%20and%20Pengyu%20Zeng%20and%20Miao%20Zhang%20and%20Shuai%20Lu%20and%20Ran%20Luo&entry.1292438233=%20%20Architectural%20cultures%20across%20regions%20are%20characterized%20by%20stylistic%0Adiversity%2C%20shaped%20by%20historical%2C%20social%2C%20and%20technological%20contexts%20in%20addition%0Ato%20geograph-ical%20conditions.%20Understanding%20architectural%20styles%20requires%20the%0Aability%20to%20describe%20and%20analyze%20the%20stylistic%20features%20of%20different%20architects%0Afrom%20various%20regions%20through%20visual%20observations%20of%20architectural%20imagery.%0AHowever%2C%20traditional%20studies%20of%20architectural%20culture%20have%20largely%20relied%20on%0Asubjective%20expert%20interpretations%20and%20historical%20literature%20reviews%2C%20often%0Asuffering%20from%20regional%20biases%20and%20limited%20ex-planatory%20scope.%20To%20address%20these%0Achallenges%2C%20this%20study%20proposes%20three%20core%20contributions%3A%20%281%29%20We%20construct%20a%0Aprofessional%20architectural%20style%20dataset%20named%20ArchDiffBench%2C%20which%20comprises%0A1%2C765%20high-quality%20architectural%20images%20and%20their%20corresponding%20style%0Aannotations%2C%20collected%20from%20different%20regions%20and%20historical%20periods.%20%282%29%20We%0Apropose%20ArchiLense%2C%20an%20analytical%20framework%20grounded%20in%20Vision-Language%20Models%0Aand%20constructed%20using%20the%20ArchDiffBench%20dataset.%20By%20integrating%20ad-vanced%0Acomputer%20vision%20techniques%2C%20deep%20learning%2C%20and%20machine%20learning%20algo-rithms%2C%0AArchiLense%20enables%20automatic%20recognition%2C%20comparison%2C%20and%20precise%0Aclassi-fication%20of%20architectural%20imagery%2C%20producing%20descriptive%20language%0Aoutputs%20that%20ar-ticulate%20stylistic%20differences.%20%283%29%20Extensive%20evaluations%20show%0Athat%20ArchiLense%20achieves%20strong%20performance%20in%20architectural%20style%20recognition%2C%0Awith%20a%2092.4%25%20con-sistency%20rate%20with%20expert%20annotations%20and%2084.5%25%20classification%0Aaccuracy%2C%20effec-tively%20capturing%20stylistic%20distinctions%20across%20images.%20The%0Aproposed%20approach%20transcends%20the%20subjectivity%20inherent%20in%20traditional%20analyses%0Aand%20offers%20a%20more%20objective%20and%20accurate%20perspective%20for%20comparative%20studies%20of%0Aarchitectural%20culture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07739v1&entry.124074799=Read"},
{"title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor", "author": "Rishit Dagli and Yushi Guan and Sankeerth Durvasula and Mohammadreza Mofayezi and Nandita Vijaykumar", "abstract": "  We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.\n", "link": "http://arxiv.org/abs/2506.07932v1", "date": "2025-06-09", "relevancy": 2.2689, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5903}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Squeeze3D%3A%20Your%203D%20Generation%20Model%20is%20Secretly%20an%20Extreme%20Neural%0A%20%20Compressor&body=Title%3A%20Squeeze3D%3A%20Your%203D%20Generation%20Model%20is%20Secretly%20an%20Extreme%20Neural%0A%20%20Compressor%0AAuthor%3A%20Rishit%20Dagli%20and%20Yushi%20Guan%20and%20Sankeerth%20Durvasula%20and%20Mohammadreza%20Mofayezi%20and%20Nandita%20Vijaykumar%0AAbstract%3A%20%20%20We%20propose%20Squeeze3D%2C%20a%20novel%20framework%20that%20leverages%20implicit%20prior%0Aknowledge%20learnt%20by%20existing%20pre-trained%203D%20generative%20models%20to%20compress%203D%0Adata%20at%20extremely%20high%20compression%20ratios.%20Our%20approach%20bridges%20the%20latent%0Aspaces%20between%20a%20pre-trained%20encoder%20and%20a%20pre-trained%20generation%20model%20through%0Atrainable%20mapping%20networks.%20Any%203D%20model%20represented%20as%20a%20mesh%2C%20point%20cloud%2C%20or%0Aa%20radiance%20field%20is%20first%20encoded%20by%20the%20pre-trained%20encoder%20and%20then%0Atransformed%20%28i.e.%20compressed%29%20into%20a%20highly%20compact%20latent%20code.%20This%20latent%0Acode%20can%20effectively%20be%20used%20as%20an%20extremely%20compressed%20representation%20of%20the%0Amesh%20or%20point%20cloud.%20A%20mapping%20network%20transforms%20the%20compressed%20latent%20code%0Ainto%20the%20latent%20space%20of%20a%20powerful%20generative%20model%2C%20which%20is%20then%20conditioned%0Ato%20recreate%20the%20original%203D%20model%20%28i.e.%20decompression%29.%20Squeeze3D%20is%20trained%0Aentirely%20on%20generated%20synthetic%20data%20and%20does%20not%20require%20any%203D%20datasets.%20The%0ASqueeze3D%20architecture%20can%20be%20flexibly%20used%20with%20existing%20pre-trained%203D%0Aencoders%20and%20existing%20generative%20models.%20It%20can%20flexibly%20support%20different%0Aformats%2C%20including%20meshes%2C%20point%20clouds%2C%20and%20radiance%20fields.%20Our%20experiments%0Ademonstrate%20that%20Squeeze3D%20achieves%20compression%20ratios%20of%20up%20to%202187x%20for%0Atextured%20meshes%2C%2055x%20for%20point%20clouds%2C%20and%20619x%20for%20radiance%20fields%20while%0Amaintaining%20visual%20quality%20comparable%20to%20many%20existing%20methods.%20Squeeze3D%20only%0Aincurs%20a%20small%20compression%20and%20decompression%20latency%20since%20it%20does%20not%20involve%0Atraining%20object-specific%20networks%20to%20compress%20an%20object.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSqueeze3D%253A%2520Your%25203D%2520Generation%2520Model%2520is%2520Secretly%2520an%2520Extreme%2520Neural%250A%2520%2520Compressor%26entry.906535625%3DRishit%2520Dagli%2520and%2520Yushi%2520Guan%2520and%2520Sankeerth%2520Durvasula%2520and%2520Mohammadreza%2520Mofayezi%2520and%2520Nandita%2520Vijaykumar%26entry.1292438233%3D%2520%2520We%2520propose%2520Squeeze3D%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520implicit%2520prior%250Aknowledge%2520learnt%2520by%2520existing%2520pre-trained%25203D%2520generative%2520models%2520to%2520compress%25203D%250Adata%2520at%2520extremely%2520high%2520compression%2520ratios.%2520Our%2520approach%2520bridges%2520the%2520latent%250Aspaces%2520between%2520a%2520pre-trained%2520encoder%2520and%2520a%2520pre-trained%2520generation%2520model%2520through%250Atrainable%2520mapping%2520networks.%2520Any%25203D%2520model%2520represented%2520as%2520a%2520mesh%252C%2520point%2520cloud%252C%2520or%250Aa%2520radiance%2520field%2520is%2520first%2520encoded%2520by%2520the%2520pre-trained%2520encoder%2520and%2520then%250Atransformed%2520%2528i.e.%2520compressed%2529%2520into%2520a%2520highly%2520compact%2520latent%2520code.%2520This%2520latent%250Acode%2520can%2520effectively%2520be%2520used%2520as%2520an%2520extremely%2520compressed%2520representation%2520of%2520the%250Amesh%2520or%2520point%2520cloud.%2520A%2520mapping%2520network%2520transforms%2520the%2520compressed%2520latent%2520code%250Ainto%2520the%2520latent%2520space%2520of%2520a%2520powerful%2520generative%2520model%252C%2520which%2520is%2520then%2520conditioned%250Ato%2520recreate%2520the%2520original%25203D%2520model%2520%2528i.e.%2520decompression%2529.%2520Squeeze3D%2520is%2520trained%250Aentirely%2520on%2520generated%2520synthetic%2520data%2520and%2520does%2520not%2520require%2520any%25203D%2520datasets.%2520The%250ASqueeze3D%2520architecture%2520can%2520be%2520flexibly%2520used%2520with%2520existing%2520pre-trained%25203D%250Aencoders%2520and%2520existing%2520generative%2520models.%2520It%2520can%2520flexibly%2520support%2520different%250Aformats%252C%2520including%2520meshes%252C%2520point%2520clouds%252C%2520and%2520radiance%2520fields.%2520Our%2520experiments%250Ademonstrate%2520that%2520Squeeze3D%2520achieves%2520compression%2520ratios%2520of%2520up%2520to%25202187x%2520for%250Atextured%2520meshes%252C%252055x%2520for%2520point%2520clouds%252C%2520and%2520619x%2520for%2520radiance%2520fields%2520while%250Amaintaining%2520visual%2520quality%2520comparable%2520to%2520many%2520existing%2520methods.%2520Squeeze3D%2520only%250Aincurs%2520a%2520small%2520compression%2520and%2520decompression%2520latency%2520since%2520it%2520does%2520not%2520involve%250Atraining%2520object-specific%2520networks%2520to%2520compress%2520an%2520object.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Squeeze3D%3A%20Your%203D%20Generation%20Model%20is%20Secretly%20an%20Extreme%20Neural%0A%20%20Compressor&entry.906535625=Rishit%20Dagli%20and%20Yushi%20Guan%20and%20Sankeerth%20Durvasula%20and%20Mohammadreza%20Mofayezi%20and%20Nandita%20Vijaykumar&entry.1292438233=%20%20We%20propose%20Squeeze3D%2C%20a%20novel%20framework%20that%20leverages%20implicit%20prior%0Aknowledge%20learnt%20by%20existing%20pre-trained%203D%20generative%20models%20to%20compress%203D%0Adata%20at%20extremely%20high%20compression%20ratios.%20Our%20approach%20bridges%20the%20latent%0Aspaces%20between%20a%20pre-trained%20encoder%20and%20a%20pre-trained%20generation%20model%20through%0Atrainable%20mapping%20networks.%20Any%203D%20model%20represented%20as%20a%20mesh%2C%20point%20cloud%2C%20or%0Aa%20radiance%20field%20is%20first%20encoded%20by%20the%20pre-trained%20encoder%20and%20then%0Atransformed%20%28i.e.%20compressed%29%20into%20a%20highly%20compact%20latent%20code.%20This%20latent%0Acode%20can%20effectively%20be%20used%20as%20an%20extremely%20compressed%20representation%20of%20the%0Amesh%20or%20point%20cloud.%20A%20mapping%20network%20transforms%20the%20compressed%20latent%20code%0Ainto%20the%20latent%20space%20of%20a%20powerful%20generative%20model%2C%20which%20is%20then%20conditioned%0Ato%20recreate%20the%20original%203D%20model%20%28i.e.%20decompression%29.%20Squeeze3D%20is%20trained%0Aentirely%20on%20generated%20synthetic%20data%20and%20does%20not%20require%20any%203D%20datasets.%20The%0ASqueeze3D%20architecture%20can%20be%20flexibly%20used%20with%20existing%20pre-trained%203D%0Aencoders%20and%20existing%20generative%20models.%20It%20can%20flexibly%20support%20different%0Aformats%2C%20including%20meshes%2C%20point%20clouds%2C%20and%20radiance%20fields.%20Our%20experiments%0Ademonstrate%20that%20Squeeze3D%20achieves%20compression%20ratios%20of%20up%20to%202187x%20for%0Atextured%20meshes%2C%2055x%20for%20point%20clouds%2C%20and%20619x%20for%20radiance%20fields%20while%0Amaintaining%20visual%20quality%20comparable%20to%20many%20existing%20methods.%20Squeeze3D%20only%0Aincurs%20a%20small%20compression%20and%20decompression%20latency%20since%20it%20does%20not%20involve%0Atraining%20object-specific%20networks%20to%20compress%20an%20object.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07932v1&entry.124074799=Read"},
{"title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "author": "Gen Li and Yutong Chen and Yiqian Wu and Kaifeng Zhao and Marc Pollefeys and Siyu Tang", "abstract": "  Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/\n", "link": "http://arxiv.org/abs/2506.07886v1", "date": "2025-06-09", "relevancy": 2.2652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5855}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoM2P%3A%20Egocentric%20Multimodal%20Multitask%20Pretraining&body=Title%3A%20EgoM2P%3A%20Egocentric%20Multimodal%20Multitask%20Pretraining%0AAuthor%3A%20Gen%20Li%20and%20Yutong%20Chen%20and%20Yiqian%20Wu%20and%20Kaifeng%20Zhao%20and%20Marc%20Pollefeys%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20Understanding%20multimodal%20signals%20in%20egocentric%20vision%2C%20such%20as%20RGB%20video%2C%0Adepth%2C%20camera%20poses%2C%20and%20gaze%2C%20is%20essential%20for%20applications%20in%20augmented%0Areality%2C%20robotics%2C%20and%20human-computer%20interaction.%20These%20capabilities%20enable%0Asystems%20to%20better%20interpret%20the%20camera%20wearer%27s%20actions%2C%20intentions%2C%20and%0Asurrounding%20environment.%20However%2C%20building%20large-scale%20egocentric%20multimodal%0Aand%20multitask%20models%20presents%20unique%20challenges.%20Egocentric%20data%20are%20inherently%0Aheterogeneous%2C%20with%20large%20variations%20in%20modality%20coverage%20across%20devices%20and%0Asettings.%20Generating%20pseudo-labels%20for%20missing%20modalities%2C%20such%20as%20gaze%20or%0Ahead-mounted%20camera%20trajectories%2C%20is%20often%20infeasible%2C%20making%20standard%0Asupervised%20learning%20approaches%20difficult%20to%20scale.%20Furthermore%2C%20dynamic%20camera%0Amotion%20and%20the%20complex%20temporal%20and%20spatial%20structure%20of%20first-person%20video%0Apose%20additional%20challenges%20for%20the%20direct%20application%20of%20existing%20multimodal%0Afoundation%20models.%0A%20%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20set%20of%20efficient%20temporal%0Atokenizers%20and%20propose%20EgoM2P%2C%20a%20masked%20modeling%20framework%20that%20learns%20from%0Atemporally%20aware%20multimodal%20tokens%20to%20train%20a%20large%2C%20general-purpose%20model%20for%0Aegocentric%204D%20understanding.%20This%20unified%20design%20supports%20multitasking%20across%0Adiverse%20egocentric%20perception%20and%20synthesis%20tasks%2C%20including%20gaze%20prediction%2C%0Aegocentric%20camera%20tracking%2C%20and%20monocular%20depth%20estimation%20from%20egocentric%0Avideo.%20EgoM2P%20also%20serves%20as%20a%20generative%20model%20for%20conditional%20egocentric%0Avideo%20synthesis.%20Across%20these%20tasks%2C%20EgoM2P%20matches%20or%20outperforms%20specialist%0Amodels%20while%20being%20an%20order%20of%20magnitude%20faster.%20We%20will%20fully%20open-source%0AEgoM2P%20to%20support%20the%20community%20and%20advance%20egocentric%20vision%20research.%20Project%0Apage%3A%20https%3A//egom2p.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoM2P%253A%2520Egocentric%2520Multimodal%2520Multitask%2520Pretraining%26entry.906535625%3DGen%2520Li%2520and%2520Yutong%2520Chen%2520and%2520Yiqian%2520Wu%2520and%2520Kaifeng%2520Zhao%2520and%2520Marc%2520Pollefeys%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520Understanding%2520multimodal%2520signals%2520in%2520egocentric%2520vision%252C%2520such%2520as%2520RGB%2520video%252C%250Adepth%252C%2520camera%2520poses%252C%2520and%2520gaze%252C%2520is%2520essential%2520for%2520applications%2520in%2520augmented%250Areality%252C%2520robotics%252C%2520and%2520human-computer%2520interaction.%2520These%2520capabilities%2520enable%250Asystems%2520to%2520better%2520interpret%2520the%2520camera%2520wearer%2527s%2520actions%252C%2520intentions%252C%2520and%250Asurrounding%2520environment.%2520However%252C%2520building%2520large-scale%2520egocentric%2520multimodal%250Aand%2520multitask%2520models%2520presents%2520unique%2520challenges.%2520Egocentric%2520data%2520are%2520inherently%250Aheterogeneous%252C%2520with%2520large%2520variations%2520in%2520modality%2520coverage%2520across%2520devices%2520and%250Asettings.%2520Generating%2520pseudo-labels%2520for%2520missing%2520modalities%252C%2520such%2520as%2520gaze%2520or%250Ahead-mounted%2520camera%2520trajectories%252C%2520is%2520often%2520infeasible%252C%2520making%2520standard%250Asupervised%2520learning%2520approaches%2520difficult%2520to%2520scale.%2520Furthermore%252C%2520dynamic%2520camera%250Amotion%2520and%2520the%2520complex%2520temporal%2520and%2520spatial%2520structure%2520of%2520first-person%2520video%250Apose%2520additional%2520challenges%2520for%2520the%2520direct%2520application%2520of%2520existing%2520multimodal%250Afoundation%2520models.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520set%2520of%2520efficient%2520temporal%250Atokenizers%2520and%2520propose%2520EgoM2P%252C%2520a%2520masked%2520modeling%2520framework%2520that%2520learns%2520from%250Atemporally%2520aware%2520multimodal%2520tokens%2520to%2520train%2520a%2520large%252C%2520general-purpose%2520model%2520for%250Aegocentric%25204D%2520understanding.%2520This%2520unified%2520design%2520supports%2520multitasking%2520across%250Adiverse%2520egocentric%2520perception%2520and%2520synthesis%2520tasks%252C%2520including%2520gaze%2520prediction%252C%250Aegocentric%2520camera%2520tracking%252C%2520and%2520monocular%2520depth%2520estimation%2520from%2520egocentric%250Avideo.%2520EgoM2P%2520also%2520serves%2520as%2520a%2520generative%2520model%2520for%2520conditional%2520egocentric%250Avideo%2520synthesis.%2520Across%2520these%2520tasks%252C%2520EgoM2P%2520matches%2520or%2520outperforms%2520specialist%250Amodels%2520while%2520being%2520an%2520order%2520of%2520magnitude%2520faster.%2520We%2520will%2520fully%2520open-source%250AEgoM2P%2520to%2520support%2520the%2520community%2520and%2520advance%2520egocentric%2520vision%2520research.%2520Project%250Apage%253A%2520https%253A//egom2p.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoM2P%3A%20Egocentric%20Multimodal%20Multitask%20Pretraining&entry.906535625=Gen%20Li%20and%20Yutong%20Chen%20and%20Yiqian%20Wu%20and%20Kaifeng%20Zhao%20and%20Marc%20Pollefeys%20and%20Siyu%20Tang&entry.1292438233=%20%20Understanding%20multimodal%20signals%20in%20egocentric%20vision%2C%20such%20as%20RGB%20video%2C%0Adepth%2C%20camera%20poses%2C%20and%20gaze%2C%20is%20essential%20for%20applications%20in%20augmented%0Areality%2C%20robotics%2C%20and%20human-computer%20interaction.%20These%20capabilities%20enable%0Asystems%20to%20better%20interpret%20the%20camera%20wearer%27s%20actions%2C%20intentions%2C%20and%0Asurrounding%20environment.%20However%2C%20building%20large-scale%20egocentric%20multimodal%0Aand%20multitask%20models%20presents%20unique%20challenges.%20Egocentric%20data%20are%20inherently%0Aheterogeneous%2C%20with%20large%20variations%20in%20modality%20coverage%20across%20devices%20and%0Asettings.%20Generating%20pseudo-labels%20for%20missing%20modalities%2C%20such%20as%20gaze%20or%0Ahead-mounted%20camera%20trajectories%2C%20is%20often%20infeasible%2C%20making%20standard%0Asupervised%20learning%20approaches%20difficult%20to%20scale.%20Furthermore%2C%20dynamic%20camera%0Amotion%20and%20the%20complex%20temporal%20and%20spatial%20structure%20of%20first-person%20video%0Apose%20additional%20challenges%20for%20the%20direct%20application%20of%20existing%20multimodal%0Afoundation%20models.%0A%20%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20set%20of%20efficient%20temporal%0Atokenizers%20and%20propose%20EgoM2P%2C%20a%20masked%20modeling%20framework%20that%20learns%20from%0Atemporally%20aware%20multimodal%20tokens%20to%20train%20a%20large%2C%20general-purpose%20model%20for%0Aegocentric%204D%20understanding.%20This%20unified%20design%20supports%20multitasking%20across%0Adiverse%20egocentric%20perception%20and%20synthesis%20tasks%2C%20including%20gaze%20prediction%2C%0Aegocentric%20camera%20tracking%2C%20and%20monocular%20depth%20estimation%20from%20egocentric%0Avideo.%20EgoM2P%20also%20serves%20as%20a%20generative%20model%20for%20conditional%20egocentric%0Avideo%20synthesis.%20Across%20these%20tasks%2C%20EgoM2P%20matches%20or%20outperforms%20specialist%0Amodels%20while%20being%20an%20order%20of%20magnitude%20faster.%20We%20will%20fully%20open-source%0AEgoM2P%20to%20support%20the%20community%20and%20advance%20egocentric%20vision%20research.%20Project%0Apage%3A%20https%3A//egom2p.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07886v1&entry.124074799=Read"},
{"title": "Reliably detecting model failures in deployment without labels", "author": "Viet Nguyen and Changjian Shui and Vijay Giri and Siddarth Arya and Amol Verma and Fahad Razak and Rahul G. Krishnan", "abstract": "  The distribution of data changes over time; models operating operating in\ndynamic environments need retraining. But knowing when to retrain, without\naccess to labels, is an open challenge since some, but not all shifts degrade\nmodel performance. This paper formalizes and addresses the problem of\npost-deployment deterioration (PDD) monitoring. We propose D3M, a practical and\nefficient monitoring algorithm based on the disagreement of predictive models,\nachieving low false positive rates under non-deteriorating shifts and provides\nsample complexity bounds for high true positive rates under deteriorating\nshifts. Empirical results on both standard benchmark and a real-world\nlarge-scale internal medicine dataset demonstrate the effectiveness of the\nframework and highlight its viability as an alert mechanism for high-stakes\nmachine learning pipelines.\n", "link": "http://arxiv.org/abs/2506.05047v2", "date": "2025-06-09", "relevancy": 1.4624, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5453}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4844}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliably%20detecting%20model%20failures%20in%20deployment%20without%20labels&body=Title%3A%20Reliably%20detecting%20model%20failures%20in%20deployment%20without%20labels%0AAuthor%3A%20Viet%20Nguyen%20and%20Changjian%20Shui%20and%20Vijay%20Giri%20and%20Siddarth%20Arya%20and%20Amol%20Verma%20and%20Fahad%20Razak%20and%20Rahul%20G.%20Krishnan%0AAbstract%3A%20%20%20The%20distribution%20of%20data%20changes%20over%20time%3B%20models%20operating%20operating%20in%0Adynamic%20environments%20need%20retraining.%20But%20knowing%20when%20to%20retrain%2C%20without%0Aaccess%20to%20labels%2C%20is%20an%20open%20challenge%20since%20some%2C%20but%20not%20all%20shifts%20degrade%0Amodel%20performance.%20This%20paper%20formalizes%20and%20addresses%20the%20problem%20of%0Apost-deployment%20deterioration%20%28PDD%29%20monitoring.%20We%20propose%20D3M%2C%20a%20practical%20and%0Aefficient%20monitoring%20algorithm%20based%20on%20the%20disagreement%20of%20predictive%20models%2C%0Aachieving%20low%20false%20positive%20rates%20under%20non-deteriorating%20shifts%20and%20provides%0Asample%20complexity%20bounds%20for%20high%20true%20positive%20rates%20under%20deteriorating%0Ashifts.%20Empirical%20results%20on%20both%20standard%20benchmark%20and%20a%20real-world%0Alarge-scale%20internal%20medicine%20dataset%20demonstrate%20the%20effectiveness%20of%20the%0Aframework%20and%20highlight%20its%20viability%20as%20an%20alert%20mechanism%20for%20high-stakes%0Amachine%20learning%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliably%2520detecting%2520model%2520failures%2520in%2520deployment%2520without%2520labels%26entry.906535625%3DViet%2520Nguyen%2520and%2520Changjian%2520Shui%2520and%2520Vijay%2520Giri%2520and%2520Siddarth%2520Arya%2520and%2520Amol%2520Verma%2520and%2520Fahad%2520Razak%2520and%2520Rahul%2520G.%2520Krishnan%26entry.1292438233%3D%2520%2520The%2520distribution%2520of%2520data%2520changes%2520over%2520time%253B%2520models%2520operating%2520operating%2520in%250Adynamic%2520environments%2520need%2520retraining.%2520But%2520knowing%2520when%2520to%2520retrain%252C%2520without%250Aaccess%2520to%2520labels%252C%2520is%2520an%2520open%2520challenge%2520since%2520some%252C%2520but%2520not%2520all%2520shifts%2520degrade%250Amodel%2520performance.%2520This%2520paper%2520formalizes%2520and%2520addresses%2520the%2520problem%2520of%250Apost-deployment%2520deterioration%2520%2528PDD%2529%2520monitoring.%2520We%2520propose%2520D3M%252C%2520a%2520practical%2520and%250Aefficient%2520monitoring%2520algorithm%2520based%2520on%2520the%2520disagreement%2520of%2520predictive%2520models%252C%250Aachieving%2520low%2520false%2520positive%2520rates%2520under%2520non-deteriorating%2520shifts%2520and%2520provides%250Asample%2520complexity%2520bounds%2520for%2520high%2520true%2520positive%2520rates%2520under%2520deteriorating%250Ashifts.%2520Empirical%2520results%2520on%2520both%2520standard%2520benchmark%2520and%2520a%2520real-world%250Alarge-scale%2520internal%2520medicine%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aframework%2520and%2520highlight%2520its%2520viability%2520as%2520an%2520alert%2520mechanism%2520for%2520high-stakes%250Amachine%2520learning%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliably%20detecting%20model%20failures%20in%20deployment%20without%20labels&entry.906535625=Viet%20Nguyen%20and%20Changjian%20Shui%20and%20Vijay%20Giri%20and%20Siddarth%20Arya%20and%20Amol%20Verma%20and%20Fahad%20Razak%20and%20Rahul%20G.%20Krishnan&entry.1292438233=%20%20The%20distribution%20of%20data%20changes%20over%20time%3B%20models%20operating%20operating%20in%0Adynamic%20environments%20need%20retraining.%20But%20knowing%20when%20to%20retrain%2C%20without%0Aaccess%20to%20labels%2C%20is%20an%20open%20challenge%20since%20some%2C%20but%20not%20all%20shifts%20degrade%0Amodel%20performance.%20This%20paper%20formalizes%20and%20addresses%20the%20problem%20of%0Apost-deployment%20deterioration%20%28PDD%29%20monitoring.%20We%20propose%20D3M%2C%20a%20practical%20and%0Aefficient%20monitoring%20algorithm%20based%20on%20the%20disagreement%20of%20predictive%20models%2C%0Aachieving%20low%20false%20positive%20rates%20under%20non-deteriorating%20shifts%20and%20provides%0Asample%20complexity%20bounds%20for%20high%20true%20positive%20rates%20under%20deteriorating%0Ashifts.%20Empirical%20results%20on%20both%20standard%20benchmark%20and%20a%20real-world%0Alarge-scale%20internal%20medicine%20dataset%20demonstrate%20the%20effectiveness%20of%20the%0Aframework%20and%20highlight%20its%20viability%20as%20an%20alert%20mechanism%20for%20high-stakes%0Amachine%20learning%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05047v2&entry.124074799=Read"},
{"title": "M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for\n  All-in-One Image Restoration", "author": "Yongzhen Wang and Yongjun Li and Zhuoran Zheng and Xiao-Ping Zhang and Mingqiang Wei", "abstract": "  Natural images are often degraded by complex, composite degradations such as\nrain, snow, and haze, which adversely impact downstream vision applications.\nWhile existing image restoration efforts have achieved notable success, they\nare still hindered by two critical challenges: limited generalization across\ndynamically varying degradation scenarios and a suboptimal balance between\npreserving local details and modeling global dependencies. To overcome these\nchallenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based\nMamba-CNN fusion framework for efficient and robust all-in-one image\nrestoration. M2Restore introduces three key contributions: First, to boost the\nmodel's generalization across diverse degradation conditions, we exploit a\nCLIP-guided MoE gating mechanism that fuses task-conditioned prompts with\nCLIP-derived semantic priors. This mechanism is further refined via cross-modal\nfeature calibration, which enables precise expert selection for various\ndegradation types. Second, to jointly capture global contextual dependencies\nand fine-grained local details, we design a dual-stream architecture that\nintegrates the localized representational strength of CNNs with the long-range\nmodeling efficiency of Mamba. This integration enables collaborative\noptimization of global semantic relationships and local structural fidelity,\npreserving global coherence while enhancing detail restoration. Third, we\nintroduce an edge-aware dynamic gating mechanism that adaptively balances\nglobal modeling and local enhancement by reallocating computational attention\nto degradation-sensitive regions. This targeted focus leads to more efficient\nand precise restoration. Extensive experiments across multiple image\nrestoration benchmarks validate the superiority of M2Restore in both visual\nquality and quantitative performance.\n", "link": "http://arxiv.org/abs/2506.07814v1", "date": "2025-06-09", "relevancy": 2.1015, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5418}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5223}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M2Restore%3A%20Mixture-of-Experts-based%20Mamba-CNN%20Fusion%20Framework%20for%0A%20%20All-in-One%20Image%20Restoration&body=Title%3A%20M2Restore%3A%20Mixture-of-Experts-based%20Mamba-CNN%20Fusion%20Framework%20for%0A%20%20All-in-One%20Image%20Restoration%0AAuthor%3A%20Yongzhen%20Wang%20and%20Yongjun%20Li%20and%20Zhuoran%20Zheng%20and%20Xiao-Ping%20Zhang%20and%20Mingqiang%20Wei%0AAbstract%3A%20%20%20Natural%20images%20are%20often%20degraded%20by%20complex%2C%20composite%20degradations%20such%20as%0Arain%2C%20snow%2C%20and%20haze%2C%20which%20adversely%20impact%20downstream%20vision%20applications.%0AWhile%20existing%20image%20restoration%20efforts%20have%20achieved%20notable%20success%2C%20they%0Aare%20still%20hindered%20by%20two%20critical%20challenges%3A%20limited%20generalization%20across%0Adynamically%20varying%20degradation%20scenarios%20and%20a%20suboptimal%20balance%20between%0Apreserving%20local%20details%20and%20modeling%20global%20dependencies.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20M2Restore%2C%20a%20novel%20Mixture-of-Experts%20%28MoE%29-based%0AMamba-CNN%20fusion%20framework%20for%20efficient%20and%20robust%20all-in-one%20image%0Arestoration.%20M2Restore%20introduces%20three%20key%20contributions%3A%20First%2C%20to%20boost%20the%0Amodel%27s%20generalization%20across%20diverse%20degradation%20conditions%2C%20we%20exploit%20a%0ACLIP-guided%20MoE%20gating%20mechanism%20that%20fuses%20task-conditioned%20prompts%20with%0ACLIP-derived%20semantic%20priors.%20This%20mechanism%20is%20further%20refined%20via%20cross-modal%0Afeature%20calibration%2C%20which%20enables%20precise%20expert%20selection%20for%20various%0Adegradation%20types.%20Second%2C%20to%20jointly%20capture%20global%20contextual%20dependencies%0Aand%20fine-grained%20local%20details%2C%20we%20design%20a%20dual-stream%20architecture%20that%0Aintegrates%20the%20localized%20representational%20strength%20of%20CNNs%20with%20the%20long-range%0Amodeling%20efficiency%20of%20Mamba.%20This%20integration%20enables%20collaborative%0Aoptimization%20of%20global%20semantic%20relationships%20and%20local%20structural%20fidelity%2C%0Apreserving%20global%20coherence%20while%20enhancing%20detail%20restoration.%20Third%2C%20we%0Aintroduce%20an%20edge-aware%20dynamic%20gating%20mechanism%20that%20adaptively%20balances%0Aglobal%20modeling%20and%20local%20enhancement%20by%20reallocating%20computational%20attention%0Ato%20degradation-sensitive%20regions.%20This%20targeted%20focus%20leads%20to%20more%20efficient%0Aand%20precise%20restoration.%20Extensive%20experiments%20across%20multiple%20image%0Arestoration%20benchmarks%20validate%20the%20superiority%20of%20M2Restore%20in%20both%20visual%0Aquality%20and%20quantitative%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM2Restore%253A%2520Mixture-of-Experts-based%2520Mamba-CNN%2520Fusion%2520Framework%2520for%250A%2520%2520All-in-One%2520Image%2520Restoration%26entry.906535625%3DYongzhen%2520Wang%2520and%2520Yongjun%2520Li%2520and%2520Zhuoran%2520Zheng%2520and%2520Xiao-Ping%2520Zhang%2520and%2520Mingqiang%2520Wei%26entry.1292438233%3D%2520%2520Natural%2520images%2520are%2520often%2520degraded%2520by%2520complex%252C%2520composite%2520degradations%2520such%2520as%250Arain%252C%2520snow%252C%2520and%2520haze%252C%2520which%2520adversely%2520impact%2520downstream%2520vision%2520applications.%250AWhile%2520existing%2520image%2520restoration%2520efforts%2520have%2520achieved%2520notable%2520success%252C%2520they%250Aare%2520still%2520hindered%2520by%2520two%2520critical%2520challenges%253A%2520limited%2520generalization%2520across%250Adynamically%2520varying%2520degradation%2520scenarios%2520and%2520a%2520suboptimal%2520balance%2520between%250Apreserving%2520local%2520details%2520and%2520modeling%2520global%2520dependencies.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520M2Restore%252C%2520a%2520novel%2520Mixture-of-Experts%2520%2528MoE%2529-based%250AMamba-CNN%2520fusion%2520framework%2520for%2520efficient%2520and%2520robust%2520all-in-one%2520image%250Arestoration.%2520M2Restore%2520introduces%2520three%2520key%2520contributions%253A%2520First%252C%2520to%2520boost%2520the%250Amodel%2527s%2520generalization%2520across%2520diverse%2520degradation%2520conditions%252C%2520we%2520exploit%2520a%250ACLIP-guided%2520MoE%2520gating%2520mechanism%2520that%2520fuses%2520task-conditioned%2520prompts%2520with%250ACLIP-derived%2520semantic%2520priors.%2520This%2520mechanism%2520is%2520further%2520refined%2520via%2520cross-modal%250Afeature%2520calibration%252C%2520which%2520enables%2520precise%2520expert%2520selection%2520for%2520various%250Adegradation%2520types.%2520Second%252C%2520to%2520jointly%2520capture%2520global%2520contextual%2520dependencies%250Aand%2520fine-grained%2520local%2520details%252C%2520we%2520design%2520a%2520dual-stream%2520architecture%2520that%250Aintegrates%2520the%2520localized%2520representational%2520strength%2520of%2520CNNs%2520with%2520the%2520long-range%250Amodeling%2520efficiency%2520of%2520Mamba.%2520This%2520integration%2520enables%2520collaborative%250Aoptimization%2520of%2520global%2520semantic%2520relationships%2520and%2520local%2520structural%2520fidelity%252C%250Apreserving%2520global%2520coherence%2520while%2520enhancing%2520detail%2520restoration.%2520Third%252C%2520we%250Aintroduce%2520an%2520edge-aware%2520dynamic%2520gating%2520mechanism%2520that%2520adaptively%2520balances%250Aglobal%2520modeling%2520and%2520local%2520enhancement%2520by%2520reallocating%2520computational%2520attention%250Ato%2520degradation-sensitive%2520regions.%2520This%2520targeted%2520focus%2520leads%2520to%2520more%2520efficient%250Aand%2520precise%2520restoration.%2520Extensive%2520experiments%2520across%2520multiple%2520image%250Arestoration%2520benchmarks%2520validate%2520the%2520superiority%2520of%2520M2Restore%2520in%2520both%2520visual%250Aquality%2520and%2520quantitative%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M2Restore%3A%20Mixture-of-Experts-based%20Mamba-CNN%20Fusion%20Framework%20for%0A%20%20All-in-One%20Image%20Restoration&entry.906535625=Yongzhen%20Wang%20and%20Yongjun%20Li%20and%20Zhuoran%20Zheng%20and%20Xiao-Ping%20Zhang%20and%20Mingqiang%20Wei&entry.1292438233=%20%20Natural%20images%20are%20often%20degraded%20by%20complex%2C%20composite%20degradations%20such%20as%0Arain%2C%20snow%2C%20and%20haze%2C%20which%20adversely%20impact%20downstream%20vision%20applications.%0AWhile%20existing%20image%20restoration%20efforts%20have%20achieved%20notable%20success%2C%20they%0Aare%20still%20hindered%20by%20two%20critical%20challenges%3A%20limited%20generalization%20across%0Adynamically%20varying%20degradation%20scenarios%20and%20a%20suboptimal%20balance%20between%0Apreserving%20local%20details%20and%20modeling%20global%20dependencies.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20M2Restore%2C%20a%20novel%20Mixture-of-Experts%20%28MoE%29-based%0AMamba-CNN%20fusion%20framework%20for%20efficient%20and%20robust%20all-in-one%20image%0Arestoration.%20M2Restore%20introduces%20three%20key%20contributions%3A%20First%2C%20to%20boost%20the%0Amodel%27s%20generalization%20across%20diverse%20degradation%20conditions%2C%20we%20exploit%20a%0ACLIP-guided%20MoE%20gating%20mechanism%20that%20fuses%20task-conditioned%20prompts%20with%0ACLIP-derived%20semantic%20priors.%20This%20mechanism%20is%20further%20refined%20via%20cross-modal%0Afeature%20calibration%2C%20which%20enables%20precise%20expert%20selection%20for%20various%0Adegradation%20types.%20Second%2C%20to%20jointly%20capture%20global%20contextual%20dependencies%0Aand%20fine-grained%20local%20details%2C%20we%20design%20a%20dual-stream%20architecture%20that%0Aintegrates%20the%20localized%20representational%20strength%20of%20CNNs%20with%20the%20long-range%0Amodeling%20efficiency%20of%20Mamba.%20This%20integration%20enables%20collaborative%0Aoptimization%20of%20global%20semantic%20relationships%20and%20local%20structural%20fidelity%2C%0Apreserving%20global%20coherence%20while%20enhancing%20detail%20restoration.%20Third%2C%20we%0Aintroduce%20an%20edge-aware%20dynamic%20gating%20mechanism%20that%20adaptively%20balances%0Aglobal%20modeling%20and%20local%20enhancement%20by%20reallocating%20computational%20attention%0Ato%20degradation-sensitive%20regions.%20This%20targeted%20focus%20leads%20to%20more%20efficient%0Aand%20precise%20restoration.%20Extensive%20experiments%20across%20multiple%20image%0Arestoration%20benchmarks%20validate%20the%20superiority%20of%20M2Restore%20in%20both%20visual%0Aquality%20and%20quantitative%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07814v1&entry.124074799=Read"},
{"title": "Generalized Interpolating Discrete Diffusion", "author": "Dimitri von R\u00fctte and Janis Fluri and Yuhui Ding and Antonio Orvieto and Bernhard Sch\u00f6lkopf and Thomas Hofmann", "abstract": "  While state-of-the-art language models achieve impressive results through\nnext-token prediction, they have inherent limitations such as the inability to\nrevise already generated tokens. This has prompted exploration of alternative\napproaches such as discrete diffusion. However, masked diffusion, which has\nemerged as a popular choice due to its simplicity and effectiveness,\nreintroduces this inability to revise words. To overcome this, we generalize\nmasked diffusion, deriving a new family of general interpolating discrete\ndiffusion (GIDD) which offers greater flexibility in the design of the noising\nprocesses. Leveraging a novel diffusion ELBO, we achieve compute-matched\nstate-of-the-art performance in diffusion language modeling. Exploiting GIDD's\nflexibility, we explore a hybrid approach combining masking and uniform noise,\nleading to improved sample quality and unlocking the ability for the model to\ncorrect its own mistakes, an area where autoregressive models notoriously have\nstruggled. Code: https://github.com/dvruette/gidd/\n", "link": "http://arxiv.org/abs/2503.04482v2", "date": "2025-06-09", "relevancy": 1.1753, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6185}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5825}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Interpolating%20Discrete%20Diffusion&body=Title%3A%20Generalized%20Interpolating%20Discrete%20Diffusion%0AAuthor%3A%20Dimitri%20von%20R%C3%BCtte%20and%20Janis%20Fluri%20and%20Yuhui%20Ding%20and%20Antonio%20Orvieto%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Thomas%20Hofmann%0AAbstract%3A%20%20%20While%20state-of-the-art%20language%20models%20achieve%20impressive%20results%20through%0Anext-token%20prediction%2C%20they%20have%20inherent%20limitations%20such%20as%20the%20inability%20to%0Arevise%20already%20generated%20tokens.%20This%20has%20prompted%20exploration%20of%20alternative%0Aapproaches%20such%20as%20discrete%20diffusion.%20However%2C%20masked%20diffusion%2C%20which%20has%0Aemerged%20as%20a%20popular%20choice%20due%20to%20its%20simplicity%20and%20effectiveness%2C%0Areintroduces%20this%20inability%20to%20revise%20words.%20To%20overcome%20this%2C%20we%20generalize%0Amasked%20diffusion%2C%20deriving%20a%20new%20family%20of%20general%20interpolating%20discrete%0Adiffusion%20%28GIDD%29%20which%20offers%20greater%20flexibility%20in%20the%20design%20of%20the%20noising%0Aprocesses.%20Leveraging%20a%20novel%20diffusion%20ELBO%2C%20we%20achieve%20compute-matched%0Astate-of-the-art%20performance%20in%20diffusion%20language%20modeling.%20Exploiting%20GIDD%27s%0Aflexibility%2C%20we%20explore%20a%20hybrid%20approach%20combining%20masking%20and%20uniform%20noise%2C%0Aleading%20to%20improved%20sample%20quality%20and%20unlocking%20the%20ability%20for%20the%20model%20to%0Acorrect%20its%20own%20mistakes%2C%20an%20area%20where%20autoregressive%20models%20notoriously%20have%0Astruggled.%20Code%3A%20https%3A//github.com/dvruette/gidd/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Interpolating%2520Discrete%2520Diffusion%26entry.906535625%3DDimitri%2520von%2520R%25C3%25BCtte%2520and%2520Janis%2520Fluri%2520and%2520Yuhui%2520Ding%2520and%2520Antonio%2520Orvieto%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Thomas%2520Hofmann%26entry.1292438233%3D%2520%2520While%2520state-of-the-art%2520language%2520models%2520achieve%2520impressive%2520results%2520through%250Anext-token%2520prediction%252C%2520they%2520have%2520inherent%2520limitations%2520such%2520as%2520the%2520inability%2520to%250Arevise%2520already%2520generated%2520tokens.%2520This%2520has%2520prompted%2520exploration%2520of%2520alternative%250Aapproaches%2520such%2520as%2520discrete%2520diffusion.%2520However%252C%2520masked%2520diffusion%252C%2520which%2520has%250Aemerged%2520as%2520a%2520popular%2520choice%2520due%2520to%2520its%2520simplicity%2520and%2520effectiveness%252C%250Areintroduces%2520this%2520inability%2520to%2520revise%2520words.%2520To%2520overcome%2520this%252C%2520we%2520generalize%250Amasked%2520diffusion%252C%2520deriving%2520a%2520new%2520family%2520of%2520general%2520interpolating%2520discrete%250Adiffusion%2520%2528GIDD%2529%2520which%2520offers%2520greater%2520flexibility%2520in%2520the%2520design%2520of%2520the%2520noising%250Aprocesses.%2520Leveraging%2520a%2520novel%2520diffusion%2520ELBO%252C%2520we%2520achieve%2520compute-matched%250Astate-of-the-art%2520performance%2520in%2520diffusion%2520language%2520modeling.%2520Exploiting%2520GIDD%2527s%250Aflexibility%252C%2520we%2520explore%2520a%2520hybrid%2520approach%2520combining%2520masking%2520and%2520uniform%2520noise%252C%250Aleading%2520to%2520improved%2520sample%2520quality%2520and%2520unlocking%2520the%2520ability%2520for%2520the%2520model%2520to%250Acorrect%2520its%2520own%2520mistakes%252C%2520an%2520area%2520where%2520autoregressive%2520models%2520notoriously%2520have%250Astruggled.%2520Code%253A%2520https%253A//github.com/dvruette/gidd/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Interpolating%20Discrete%20Diffusion&entry.906535625=Dimitri%20von%20R%C3%BCtte%20and%20Janis%20Fluri%20and%20Yuhui%20Ding%20and%20Antonio%20Orvieto%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Thomas%20Hofmann&entry.1292438233=%20%20While%20state-of-the-art%20language%20models%20achieve%20impressive%20results%20through%0Anext-token%20prediction%2C%20they%20have%20inherent%20limitations%20such%20as%20the%20inability%20to%0Arevise%20already%20generated%20tokens.%20This%20has%20prompted%20exploration%20of%20alternative%0Aapproaches%20such%20as%20discrete%20diffusion.%20However%2C%20masked%20diffusion%2C%20which%20has%0Aemerged%20as%20a%20popular%20choice%20due%20to%20its%20simplicity%20and%20effectiveness%2C%0Areintroduces%20this%20inability%20to%20revise%20words.%20To%20overcome%20this%2C%20we%20generalize%0Amasked%20diffusion%2C%20deriving%20a%20new%20family%20of%20general%20interpolating%20discrete%0Adiffusion%20%28GIDD%29%20which%20offers%20greater%20flexibility%20in%20the%20design%20of%20the%20noising%0Aprocesses.%20Leveraging%20a%20novel%20diffusion%20ELBO%2C%20we%20achieve%20compute-matched%0Astate-of-the-art%20performance%20in%20diffusion%20language%20modeling.%20Exploiting%20GIDD%27s%0Aflexibility%2C%20we%20explore%20a%20hybrid%20approach%20combining%20masking%20and%20uniform%20noise%2C%0Aleading%20to%20improved%20sample%20quality%20and%20unlocking%20the%20ability%20for%20the%20model%20to%0Acorrect%20its%20own%20mistakes%2C%20an%20area%20where%20autoregressive%20models%20notoriously%20have%0Astruggled.%20Code%3A%20https%3A//github.com/dvruette/gidd/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04482v2&entry.124074799=Read"},
{"title": "FedX: Adaptive Model Decomposition and Quantization for IoT Federated\n  Learning", "author": "Phung Lai and Xiaopeng Jiang and Hai Phan and Cristian Borcea and Khang Tran and An Chen and Vijaya Datta Mayyuri and Ruoming Jin", "abstract": "  Federated Learning (FL) allows collaborative training among multiple devices\nwithout data sharing, thus enabling privacy-sensitive applications on mobile or\nInternet of Things (IoT) devices, such as mobile health and asset tracking.\nHowever, designing an FL system with good model utility that works with low\ncomputation/communication overhead on heterogeneous, resource-constrained\nmobile/IoT devices is challenging. To address this problem, this paper proposes\nFedX, a novel adaptive model decomposition and quantization FL system for IoT.\nTo balance utility with resource constraints on IoT devices, FedX decomposes a\nglobal FL model into different sub-networks with adaptive numbers of quantized\nbits for different devices. The key idea is that a device with fewer resources\nreceives a smaller sub-network for lower overhead but utilizes a larger number\nof quantized bits for higher model utility, and vice versa. The quantization\noperations in FedX are done at the server to reduce the computational load on\ndevices. FedX iteratively minimizes the losses in the devices' local data and\nin the server's public data using quantized sub-networks under a regularization\nterm, and thus it maximizes the benefits of combining FL with model\nquantization through knowledge sharing among the server and devices in a\ncost-effective training process. Extensive experiments show that FedX\nsignificantly improves quantization times by up to 8.43X, on-device computation\ntime by 1.5X, and total end-to-end training time by 1.36X, compared with\nbaseline FL systems. We guarantee the global model convergence theoretically\nand validate local model convergence empirically, highlighting FedX's\noptimization efficiency.\n", "link": "http://arxiv.org/abs/2504.12849v3", "date": "2025-06-09", "relevancy": 1.5669, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5306}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5225}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedX%3A%20Adaptive%20Model%20Decomposition%20and%20Quantization%20for%20IoT%20Federated%0A%20%20Learning&body=Title%3A%20FedX%3A%20Adaptive%20Model%20Decomposition%20and%20Quantization%20for%20IoT%20Federated%0A%20%20Learning%0AAuthor%3A%20Phung%20Lai%20and%20Xiaopeng%20Jiang%20and%20Hai%20Phan%20and%20Cristian%20Borcea%20and%20Khang%20Tran%20and%20An%20Chen%20and%20Vijaya%20Datta%20Mayyuri%20and%20Ruoming%20Jin%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20allows%20collaborative%20training%20among%20multiple%20devices%0Awithout%20data%20sharing%2C%20thus%20enabling%20privacy-sensitive%20applications%20on%20mobile%20or%0AInternet%20of%20Things%20%28IoT%29%20devices%2C%20such%20as%20mobile%20health%20and%20asset%20tracking.%0AHowever%2C%20designing%20an%20FL%20system%20with%20good%20model%20utility%20that%20works%20with%20low%0Acomputation/communication%20overhead%20on%20heterogeneous%2C%20resource-constrained%0Amobile/IoT%20devices%20is%20challenging.%20To%20address%20this%20problem%2C%20this%20paper%20proposes%0AFedX%2C%20a%20novel%20adaptive%20model%20decomposition%20and%20quantization%20FL%20system%20for%20IoT.%0ATo%20balance%20utility%20with%20resource%20constraints%20on%20IoT%20devices%2C%20FedX%20decomposes%20a%0Aglobal%20FL%20model%20into%20different%20sub-networks%20with%20adaptive%20numbers%20of%20quantized%0Abits%20for%20different%20devices.%20The%20key%20idea%20is%20that%20a%20device%20with%20fewer%20resources%0Areceives%20a%20smaller%20sub-network%20for%20lower%20overhead%20but%20utilizes%20a%20larger%20number%0Aof%20quantized%20bits%20for%20higher%20model%20utility%2C%20and%20vice%20versa.%20The%20quantization%0Aoperations%20in%20FedX%20are%20done%20at%20the%20server%20to%20reduce%20the%20computational%20load%20on%0Adevices.%20FedX%20iteratively%20minimizes%20the%20losses%20in%20the%20devices%27%20local%20data%20and%0Ain%20the%20server%27s%20public%20data%20using%20quantized%20sub-networks%20under%20a%20regularization%0Aterm%2C%20and%20thus%20it%20maximizes%20the%20benefits%20of%20combining%20FL%20with%20model%0Aquantization%20through%20knowledge%20sharing%20among%20the%20server%20and%20devices%20in%20a%0Acost-effective%20training%20process.%20Extensive%20experiments%20show%20that%20FedX%0Asignificantly%20improves%20quantization%20times%20by%20up%20to%208.43X%2C%20on-device%20computation%0Atime%20by%201.5X%2C%20and%20total%20end-to-end%20training%20time%20by%201.36X%2C%20compared%20with%0Abaseline%20FL%20systems.%20We%20guarantee%20the%20global%20model%20convergence%20theoretically%0Aand%20validate%20local%20model%20convergence%20empirically%2C%20highlighting%20FedX%27s%0Aoptimization%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12849v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedX%253A%2520Adaptive%2520Model%2520Decomposition%2520and%2520Quantization%2520for%2520IoT%2520Federated%250A%2520%2520Learning%26entry.906535625%3DPhung%2520Lai%2520and%2520Xiaopeng%2520Jiang%2520and%2520Hai%2520Phan%2520and%2520Cristian%2520Borcea%2520and%2520Khang%2520Tran%2520and%2520An%2520Chen%2520and%2520Vijaya%2520Datta%2520Mayyuri%2520and%2520Ruoming%2520Jin%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520allows%2520collaborative%2520training%2520among%2520multiple%2520devices%250Awithout%2520data%2520sharing%252C%2520thus%2520enabling%2520privacy-sensitive%2520applications%2520on%2520mobile%2520or%250AInternet%2520of%2520Things%2520%2528IoT%2529%2520devices%252C%2520such%2520as%2520mobile%2520health%2520and%2520asset%2520tracking.%250AHowever%252C%2520designing%2520an%2520FL%2520system%2520with%2520good%2520model%2520utility%2520that%2520works%2520with%2520low%250Acomputation/communication%2520overhead%2520on%2520heterogeneous%252C%2520resource-constrained%250Amobile/IoT%2520devices%2520is%2520challenging.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%2520proposes%250AFedX%252C%2520a%2520novel%2520adaptive%2520model%2520decomposition%2520and%2520quantization%2520FL%2520system%2520for%2520IoT.%250ATo%2520balance%2520utility%2520with%2520resource%2520constraints%2520on%2520IoT%2520devices%252C%2520FedX%2520decomposes%2520a%250Aglobal%2520FL%2520model%2520into%2520different%2520sub-networks%2520with%2520adaptive%2520numbers%2520of%2520quantized%250Abits%2520for%2520different%2520devices.%2520The%2520key%2520idea%2520is%2520that%2520a%2520device%2520with%2520fewer%2520resources%250Areceives%2520a%2520smaller%2520sub-network%2520for%2520lower%2520overhead%2520but%2520utilizes%2520a%2520larger%2520number%250Aof%2520quantized%2520bits%2520for%2520higher%2520model%2520utility%252C%2520and%2520vice%2520versa.%2520The%2520quantization%250Aoperations%2520in%2520FedX%2520are%2520done%2520at%2520the%2520server%2520to%2520reduce%2520the%2520computational%2520load%2520on%250Adevices.%2520FedX%2520iteratively%2520minimizes%2520the%2520losses%2520in%2520the%2520devices%2527%2520local%2520data%2520and%250Ain%2520the%2520server%2527s%2520public%2520data%2520using%2520quantized%2520sub-networks%2520under%2520a%2520regularization%250Aterm%252C%2520and%2520thus%2520it%2520maximizes%2520the%2520benefits%2520of%2520combining%2520FL%2520with%2520model%250Aquantization%2520through%2520knowledge%2520sharing%2520among%2520the%2520server%2520and%2520devices%2520in%2520a%250Acost-effective%2520training%2520process.%2520Extensive%2520experiments%2520show%2520that%2520FedX%250Asignificantly%2520improves%2520quantization%2520times%2520by%2520up%2520to%25208.43X%252C%2520on-device%2520computation%250Atime%2520by%25201.5X%252C%2520and%2520total%2520end-to-end%2520training%2520time%2520by%25201.36X%252C%2520compared%2520with%250Abaseline%2520FL%2520systems.%2520We%2520guarantee%2520the%2520global%2520model%2520convergence%2520theoretically%250Aand%2520validate%2520local%2520model%2520convergence%2520empirically%252C%2520highlighting%2520FedX%2527s%250Aoptimization%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12849v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedX%3A%20Adaptive%20Model%20Decomposition%20and%20Quantization%20for%20IoT%20Federated%0A%20%20Learning&entry.906535625=Phung%20Lai%20and%20Xiaopeng%20Jiang%20and%20Hai%20Phan%20and%20Cristian%20Borcea%20and%20Khang%20Tran%20and%20An%20Chen%20and%20Vijaya%20Datta%20Mayyuri%20and%20Ruoming%20Jin&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20allows%20collaborative%20training%20among%20multiple%20devices%0Awithout%20data%20sharing%2C%20thus%20enabling%20privacy-sensitive%20applications%20on%20mobile%20or%0AInternet%20of%20Things%20%28IoT%29%20devices%2C%20such%20as%20mobile%20health%20and%20asset%20tracking.%0AHowever%2C%20designing%20an%20FL%20system%20with%20good%20model%20utility%20that%20works%20with%20low%0Acomputation/communication%20overhead%20on%20heterogeneous%2C%20resource-constrained%0Amobile/IoT%20devices%20is%20challenging.%20To%20address%20this%20problem%2C%20this%20paper%20proposes%0AFedX%2C%20a%20novel%20adaptive%20model%20decomposition%20and%20quantization%20FL%20system%20for%20IoT.%0ATo%20balance%20utility%20with%20resource%20constraints%20on%20IoT%20devices%2C%20FedX%20decomposes%20a%0Aglobal%20FL%20model%20into%20different%20sub-networks%20with%20adaptive%20numbers%20of%20quantized%0Abits%20for%20different%20devices.%20The%20key%20idea%20is%20that%20a%20device%20with%20fewer%20resources%0Areceives%20a%20smaller%20sub-network%20for%20lower%20overhead%20but%20utilizes%20a%20larger%20number%0Aof%20quantized%20bits%20for%20higher%20model%20utility%2C%20and%20vice%20versa.%20The%20quantization%0Aoperations%20in%20FedX%20are%20done%20at%20the%20server%20to%20reduce%20the%20computational%20load%20on%0Adevices.%20FedX%20iteratively%20minimizes%20the%20losses%20in%20the%20devices%27%20local%20data%20and%0Ain%20the%20server%27s%20public%20data%20using%20quantized%20sub-networks%20under%20a%20regularization%0Aterm%2C%20and%20thus%20it%20maximizes%20the%20benefits%20of%20combining%20FL%20with%20model%0Aquantization%20through%20knowledge%20sharing%20among%20the%20server%20and%20devices%20in%20a%0Acost-effective%20training%20process.%20Extensive%20experiments%20show%20that%20FedX%0Asignificantly%20improves%20quantization%20times%20by%20up%20to%208.43X%2C%20on-device%20computation%0Atime%20by%201.5X%2C%20and%20total%20end-to-end%20training%20time%20by%201.36X%2C%20compared%20with%0Abaseline%20FL%20systems.%20We%20guarantee%20the%20global%20model%20convergence%20theoretically%0Aand%20validate%20local%20model%20convergence%20empirically%2C%20highlighting%20FedX%27s%0Aoptimization%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12849v3&entry.124074799=Read"},
{"title": "MIB: A Mechanistic Interpretability Benchmark", "author": "Aaron Mueller and Atticus Geiger and Sarah Wiegreffe and Dana Arad and Iv\u00e1n Arcuschin and Adam Belfki and Yik Siu Chan and Jaden Fiotto-Kaufman and Tal Haklay and Michael Hanna and Jing Huang and Rohan Gupta and Yaniv Nikankin and Hadas Orgad and Nikhil Prakash and Anja Reusch and Aruna Sankaranarayanan and Shun Shao and Alessandro Stolfo and Martin Tutek and Amir Zur and David Bau and Yonatan Belinkov", "abstract": "  How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of lasting evaluation standards, we propose MIB, a\nMechanistic Interpretability Benchmark, with two tracks spanning four tasks and\nfive models. MIB favors methods that precisely and concisely recover relevant\ncausal pathways or causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and align\nthose features to a task-relevant causal variable. Using MIB, we find that\nattribution and mask optimization methods perform best on circuit localization.\nFor causal variable localization, we find that the supervised DAS method\nperforms best, while SAE features are not better than neurons, i.e.,\nnon-featurized hidden vectors. These findings illustrate that MIB enables\nmeaningful comparisons, and increases our confidence that there has been real\nprogress in the field.\n", "link": "http://arxiv.org/abs/2504.13151v2", "date": "2025-06-09", "relevancy": 2.0924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIB%3A%20A%20Mechanistic%20Interpretability%20Benchmark&body=Title%3A%20MIB%3A%20A%20Mechanistic%20Interpretability%20Benchmark%0AAuthor%3A%20Aaron%20Mueller%20and%20Atticus%20Geiger%20and%20Sarah%20Wiegreffe%20and%20Dana%20Arad%20and%20Iv%C3%A1n%20Arcuschin%20and%20Adam%20Belfki%20and%20Yik%20Siu%20Chan%20and%20Jaden%20Fiotto-Kaufman%20and%20Tal%20Haklay%20and%20Michael%20Hanna%20and%20Jing%20Huang%20and%20Rohan%20Gupta%20and%20Yaniv%20Nikankin%20and%20Hadas%20Orgad%20and%20Nikhil%20Prakash%20and%20Anja%20Reusch%20and%20Aruna%20Sankaranarayanan%20and%20Shun%20Shao%20and%20Alessandro%20Stolfo%20and%20Martin%20Tutek%20and%20Amir%20Zur%20and%20David%20Bau%20and%20Yonatan%20Belinkov%0AAbstract%3A%20%20%20How%20can%20we%20know%20whether%20new%20mechanistic%20interpretability%20methods%20achieve%20real%0Aimprovements%3F%20In%20pursuit%20of%20lasting%20evaluation%20standards%2C%20we%20propose%20MIB%2C%20a%0AMechanistic%20Interpretability%20Benchmark%2C%20with%20two%20tracks%20spanning%20four%20tasks%20and%0Afive%20models.%20MIB%20favors%20methods%20that%20precisely%20and%20concisely%20recover%20relevant%0Acausal%20pathways%20or%20causal%20variables%20in%20neural%20language%20models.%20The%20circuit%0Alocalization%20track%20compares%20methods%20that%20locate%20the%20model%20components%20-%20and%0Aconnections%20between%20them%20-%20most%20important%20for%20performing%20a%20task%20%28e.g.%2C%0Aattribution%20patching%20or%20information%20flow%20routes%29.%20The%20causal%20variable%0Alocalization%20track%20compares%20methods%20that%20featurize%20a%20hidden%20vector%2C%20e.g.%2C%0Asparse%20autoencoders%20%28SAEs%29%20or%20distributed%20alignment%20search%20%28DAS%29%2C%20and%20align%0Athose%20features%20to%20a%20task-relevant%20causal%20variable.%20Using%20MIB%2C%20we%20find%20that%0Aattribution%20and%20mask%20optimization%20methods%20perform%20best%20on%20circuit%20localization.%0AFor%20causal%20variable%20localization%2C%20we%20find%20that%20the%20supervised%20DAS%20method%0Aperforms%20best%2C%20while%20SAE%20features%20are%20not%20better%20than%20neurons%2C%20i.e.%2C%0Anon-featurized%20hidden%20vectors.%20These%20findings%20illustrate%20that%20MIB%20enables%0Ameaningful%20comparisons%2C%20and%20increases%20our%20confidence%20that%20there%20has%20been%20real%0Aprogress%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIB%253A%2520A%2520Mechanistic%2520Interpretability%2520Benchmark%26entry.906535625%3DAaron%2520Mueller%2520and%2520Atticus%2520Geiger%2520and%2520Sarah%2520Wiegreffe%2520and%2520Dana%2520Arad%2520and%2520Iv%25C3%25A1n%2520Arcuschin%2520and%2520Adam%2520Belfki%2520and%2520Yik%2520Siu%2520Chan%2520and%2520Jaden%2520Fiotto-Kaufman%2520and%2520Tal%2520Haklay%2520and%2520Michael%2520Hanna%2520and%2520Jing%2520Huang%2520and%2520Rohan%2520Gupta%2520and%2520Yaniv%2520Nikankin%2520and%2520Hadas%2520Orgad%2520and%2520Nikhil%2520Prakash%2520and%2520Anja%2520Reusch%2520and%2520Aruna%2520Sankaranarayanan%2520and%2520Shun%2520Shao%2520and%2520Alessandro%2520Stolfo%2520and%2520Martin%2520Tutek%2520and%2520Amir%2520Zur%2520and%2520David%2520Bau%2520and%2520Yonatan%2520Belinkov%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520know%2520whether%2520new%2520mechanistic%2520interpretability%2520methods%2520achieve%2520real%250Aimprovements%253F%2520In%2520pursuit%2520of%2520lasting%2520evaluation%2520standards%252C%2520we%2520propose%2520MIB%252C%2520a%250AMechanistic%2520Interpretability%2520Benchmark%252C%2520with%2520two%2520tracks%2520spanning%2520four%2520tasks%2520and%250Afive%2520models.%2520MIB%2520favors%2520methods%2520that%2520precisely%2520and%2520concisely%2520recover%2520relevant%250Acausal%2520pathways%2520or%2520causal%2520variables%2520in%2520neural%2520language%2520models.%2520The%2520circuit%250Alocalization%2520track%2520compares%2520methods%2520that%2520locate%2520the%2520model%2520components%2520-%2520and%250Aconnections%2520between%2520them%2520-%2520most%2520important%2520for%2520performing%2520a%2520task%2520%2528e.g.%252C%250Aattribution%2520patching%2520or%2520information%2520flow%2520routes%2529.%2520The%2520causal%2520variable%250Alocalization%2520track%2520compares%2520methods%2520that%2520featurize%2520a%2520hidden%2520vector%252C%2520e.g.%252C%250Asparse%2520autoencoders%2520%2528SAEs%2529%2520or%2520distributed%2520alignment%2520search%2520%2528DAS%2529%252C%2520and%2520align%250Athose%2520features%2520to%2520a%2520task-relevant%2520causal%2520variable.%2520Using%2520MIB%252C%2520we%2520find%2520that%250Aattribution%2520and%2520mask%2520optimization%2520methods%2520perform%2520best%2520on%2520circuit%2520localization.%250AFor%2520causal%2520variable%2520localization%252C%2520we%2520find%2520that%2520the%2520supervised%2520DAS%2520method%250Aperforms%2520best%252C%2520while%2520SAE%2520features%2520are%2520not%2520better%2520than%2520neurons%252C%2520i.e.%252C%250Anon-featurized%2520hidden%2520vectors.%2520These%2520findings%2520illustrate%2520that%2520MIB%2520enables%250Ameaningful%2520comparisons%252C%2520and%2520increases%2520our%2520confidence%2520that%2520there%2520has%2520been%2520real%250Aprogress%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIB%3A%20A%20Mechanistic%20Interpretability%20Benchmark&entry.906535625=Aaron%20Mueller%20and%20Atticus%20Geiger%20and%20Sarah%20Wiegreffe%20and%20Dana%20Arad%20and%20Iv%C3%A1n%20Arcuschin%20and%20Adam%20Belfki%20and%20Yik%20Siu%20Chan%20and%20Jaden%20Fiotto-Kaufman%20and%20Tal%20Haklay%20and%20Michael%20Hanna%20and%20Jing%20Huang%20and%20Rohan%20Gupta%20and%20Yaniv%20Nikankin%20and%20Hadas%20Orgad%20and%20Nikhil%20Prakash%20and%20Anja%20Reusch%20and%20Aruna%20Sankaranarayanan%20and%20Shun%20Shao%20and%20Alessandro%20Stolfo%20and%20Martin%20Tutek%20and%20Amir%20Zur%20and%20David%20Bau%20and%20Yonatan%20Belinkov&entry.1292438233=%20%20How%20can%20we%20know%20whether%20new%20mechanistic%20interpretability%20methods%20achieve%20real%0Aimprovements%3F%20In%20pursuit%20of%20lasting%20evaluation%20standards%2C%20we%20propose%20MIB%2C%20a%0AMechanistic%20Interpretability%20Benchmark%2C%20with%20two%20tracks%20spanning%20four%20tasks%20and%0Afive%20models.%20MIB%20favors%20methods%20that%20precisely%20and%20concisely%20recover%20relevant%0Acausal%20pathways%20or%20causal%20variables%20in%20neural%20language%20models.%20The%20circuit%0Alocalization%20track%20compares%20methods%20that%20locate%20the%20model%20components%20-%20and%0Aconnections%20between%20them%20-%20most%20important%20for%20performing%20a%20task%20%28e.g.%2C%0Aattribution%20patching%20or%20information%20flow%20routes%29.%20The%20causal%20variable%0Alocalization%20track%20compares%20methods%20that%20featurize%20a%20hidden%20vector%2C%20e.g.%2C%0Asparse%20autoencoders%20%28SAEs%29%20or%20distributed%20alignment%20search%20%28DAS%29%2C%20and%20align%0Athose%20features%20to%20a%20task-relevant%20causal%20variable.%20Using%20MIB%2C%20we%20find%20that%0Aattribution%20and%20mask%20optimization%20methods%20perform%20best%20on%20circuit%20localization.%0AFor%20causal%20variable%20localization%2C%20we%20find%20that%20the%20supervised%20DAS%20method%0Aperforms%20best%2C%20while%20SAE%20features%20are%20not%20better%20than%20neurons%2C%20i.e.%2C%0Anon-featurized%20hidden%20vectors.%20These%20findings%20illustrate%20that%20MIB%20enables%0Ameaningful%20comparisons%2C%20and%20increases%20our%20confidence%20that%20there%20has%20been%20real%0Aprogress%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13151v2&entry.124074799=Read"},
{"title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated\n  Image Detectors", "author": "Hicham Eddoubi and Jonas Ricker and Federico Cocchi and Lorenzo Baraldi and Angelo Sotgiu and Maura Pintor and Marcella Cornia and Lorenzo Baraldi and Asja Fischer and Rita Cucchiara and Battista Biggio", "abstract": "  AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustness. Our findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID.\n", "link": "http://arxiv.org/abs/2506.03988v3", "date": "2025-06-09", "relevancy": 1.981, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5216}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4925}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAID%3A%20A%20Dataset%20for%20Testing%20the%20Adversarial%20Robustness%20of%20AI-Generated%0A%20%20Image%20Detectors&body=Title%3A%20RAID%3A%20A%20Dataset%20for%20Testing%20the%20Adversarial%20Robustness%20of%20AI-Generated%0A%20%20Image%20Detectors%0AAuthor%3A%20Hicham%20Eddoubi%20and%20Jonas%20Ricker%20and%20Federico%20Cocchi%20and%20Lorenzo%20Baraldi%20and%20Angelo%20Sotgiu%20and%20Maura%20Pintor%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Asja%20Fischer%20and%20Rita%20Cucchiara%20and%20Battista%20Biggio%0AAbstract%3A%20%20%20AI-generated%20images%20have%20reached%20a%20quality%20level%20at%20which%20humans%20are%0Aincapable%20of%20reliably%20distinguishing%20them%20from%20real%20images.%20To%20counteract%20the%0Ainherent%20risk%20of%20fraud%20and%20disinformation%2C%20the%20detection%20of%20AI-generated%20images%0Ais%20a%20pressing%20challenge%20and%20an%20active%20research%20topic.%20While%20many%20of%20the%0Apresented%20methods%20claim%20to%20achieve%20high%20detection%20accuracy%2C%20they%20are%20usually%0Aevaluated%20under%20idealized%20conditions.%20In%20particular%2C%20the%20adversarial%20robustness%0Ais%20often%20neglected%2C%20potentially%20due%20to%20a%20lack%20of%20awareness%20or%20the%20substantial%0Aeffort%20required%20to%20conduct%20a%20comprehensive%20robustness%20analysis.%20In%20this%20work%2C%0Awe%20tackle%20this%20problem%20by%20providing%20a%20simpler%20means%20to%20assess%20the%20robustness%20of%0AAI-generated%20image%20detectors.%20We%20present%20RAID%20%28Robust%20evaluation%20of%0AAI-generated%20image%20Detectors%29%2C%20a%20dataset%20of%2072k%20diverse%20and%20highly%20transferable%0Aadversarial%20examples.%20The%20dataset%20is%20created%20by%20running%20attacks%20against%20an%0Aensemble%20of%20seven%20state-of-the-art%20detectors%20and%20images%20generated%20by%20four%0Adifferent%20text-to-image%20models.%20Extensive%20experiments%20show%20that%20our%20methodology%0Agenerates%20adversarial%20images%20that%20transfer%20with%20a%20high%20success%20rate%20to%20unseen%0Adetectors%2C%20which%20can%20be%20used%20to%20quickly%20provide%20an%20approximate%20yet%20still%0Areliable%20estimate%20of%20a%20detector%27s%20adversarial%20robustness.%20Our%20findings%20indicate%0Athat%20current%20state-of-the-art%20AI-generated%20image%20detectors%20can%20be%20easily%0Adeceived%20by%20adversarial%20examples%2C%20highlighting%20the%20critical%20need%20for%20the%0Adevelopment%20of%20more%20robust%20methods.%20We%20release%20our%20dataset%20at%0Ahttps%3A//huggingface.co/datasets/aimagelab/RAID%20and%20evaluation%20code%20at%0Ahttps%3A//github.com/pralab/RAID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03988v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAID%253A%2520A%2520Dataset%2520for%2520Testing%2520the%2520Adversarial%2520Robustness%2520of%2520AI-Generated%250A%2520%2520Image%2520Detectors%26entry.906535625%3DHicham%2520Eddoubi%2520and%2520Jonas%2520Ricker%2520and%2520Federico%2520Cocchi%2520and%2520Lorenzo%2520Baraldi%2520and%2520Angelo%2520Sotgiu%2520and%2520Maura%2520Pintor%2520and%2520Marcella%2520Cornia%2520and%2520Lorenzo%2520Baraldi%2520and%2520Asja%2520Fischer%2520and%2520Rita%2520Cucchiara%2520and%2520Battista%2520Biggio%26entry.1292438233%3D%2520%2520AI-generated%2520images%2520have%2520reached%2520a%2520quality%2520level%2520at%2520which%2520humans%2520are%250Aincapable%2520of%2520reliably%2520distinguishing%2520them%2520from%2520real%2520images.%2520To%2520counteract%2520the%250Ainherent%2520risk%2520of%2520fraud%2520and%2520disinformation%252C%2520the%2520detection%2520of%2520AI-generated%2520images%250Ais%2520a%2520pressing%2520challenge%2520and%2520an%2520active%2520research%2520topic.%2520While%2520many%2520of%2520the%250Apresented%2520methods%2520claim%2520to%2520achieve%2520high%2520detection%2520accuracy%252C%2520they%2520are%2520usually%250Aevaluated%2520under%2520idealized%2520conditions.%2520In%2520particular%252C%2520the%2520adversarial%2520robustness%250Ais%2520often%2520neglected%252C%2520potentially%2520due%2520to%2520a%2520lack%2520of%2520awareness%2520or%2520the%2520substantial%250Aeffort%2520required%2520to%2520conduct%2520a%2520comprehensive%2520robustness%2520analysis.%2520In%2520this%2520work%252C%250Awe%2520tackle%2520this%2520problem%2520by%2520providing%2520a%2520simpler%2520means%2520to%2520assess%2520the%2520robustness%2520of%250AAI-generated%2520image%2520detectors.%2520We%2520present%2520RAID%2520%2528Robust%2520evaluation%2520of%250AAI-generated%2520image%2520Detectors%2529%252C%2520a%2520dataset%2520of%252072k%2520diverse%2520and%2520highly%2520transferable%250Aadversarial%2520examples.%2520The%2520dataset%2520is%2520created%2520by%2520running%2520attacks%2520against%2520an%250Aensemble%2520of%2520seven%2520state-of-the-art%2520detectors%2520and%2520images%2520generated%2520by%2520four%250Adifferent%2520text-to-image%2520models.%2520Extensive%2520experiments%2520show%2520that%2520our%2520methodology%250Agenerates%2520adversarial%2520images%2520that%2520transfer%2520with%2520a%2520high%2520success%2520rate%2520to%2520unseen%250Adetectors%252C%2520which%2520can%2520be%2520used%2520to%2520quickly%2520provide%2520an%2520approximate%2520yet%2520still%250Areliable%2520estimate%2520of%2520a%2520detector%2527s%2520adversarial%2520robustness.%2520Our%2520findings%2520indicate%250Athat%2520current%2520state-of-the-art%2520AI-generated%2520image%2520detectors%2520can%2520be%2520easily%250Adeceived%2520by%2520adversarial%2520examples%252C%2520highlighting%2520the%2520critical%2520need%2520for%2520the%250Adevelopment%2520of%2520more%2520robust%2520methods.%2520We%2520release%2520our%2520dataset%2520at%250Ahttps%253A//huggingface.co/datasets/aimagelab/RAID%2520and%2520evaluation%2520code%2520at%250Ahttps%253A//github.com/pralab/RAID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03988v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAID%3A%20A%20Dataset%20for%20Testing%20the%20Adversarial%20Robustness%20of%20AI-Generated%0A%20%20Image%20Detectors&entry.906535625=Hicham%20Eddoubi%20and%20Jonas%20Ricker%20and%20Federico%20Cocchi%20and%20Lorenzo%20Baraldi%20and%20Angelo%20Sotgiu%20and%20Maura%20Pintor%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Asja%20Fischer%20and%20Rita%20Cucchiara%20and%20Battista%20Biggio&entry.1292438233=%20%20AI-generated%20images%20have%20reached%20a%20quality%20level%20at%20which%20humans%20are%0Aincapable%20of%20reliably%20distinguishing%20them%20from%20real%20images.%20To%20counteract%20the%0Ainherent%20risk%20of%20fraud%20and%20disinformation%2C%20the%20detection%20of%20AI-generated%20images%0Ais%20a%20pressing%20challenge%20and%20an%20active%20research%20topic.%20While%20many%20of%20the%0Apresented%20methods%20claim%20to%20achieve%20high%20detection%20accuracy%2C%20they%20are%20usually%0Aevaluated%20under%20idealized%20conditions.%20In%20particular%2C%20the%20adversarial%20robustness%0Ais%20often%20neglected%2C%20potentially%20due%20to%20a%20lack%20of%20awareness%20or%20the%20substantial%0Aeffort%20required%20to%20conduct%20a%20comprehensive%20robustness%20analysis.%20In%20this%20work%2C%0Awe%20tackle%20this%20problem%20by%20providing%20a%20simpler%20means%20to%20assess%20the%20robustness%20of%0AAI-generated%20image%20detectors.%20We%20present%20RAID%20%28Robust%20evaluation%20of%0AAI-generated%20image%20Detectors%29%2C%20a%20dataset%20of%2072k%20diverse%20and%20highly%20transferable%0Aadversarial%20examples.%20The%20dataset%20is%20created%20by%20running%20attacks%20against%20an%0Aensemble%20of%20seven%20state-of-the-art%20detectors%20and%20images%20generated%20by%20four%0Adifferent%20text-to-image%20models.%20Extensive%20experiments%20show%20that%20our%20methodology%0Agenerates%20adversarial%20images%20that%20transfer%20with%20a%20high%20success%20rate%20to%20unseen%0Adetectors%2C%20which%20can%20be%20used%20to%20quickly%20provide%20an%20approximate%20yet%20still%0Areliable%20estimate%20of%20a%20detector%27s%20adversarial%20robustness.%20Our%20findings%20indicate%0Athat%20current%20state-of-the-art%20AI-generated%20image%20detectors%20can%20be%20easily%0Adeceived%20by%20adversarial%20examples%2C%20highlighting%20the%20critical%20need%20for%20the%0Adevelopment%20of%20more%20robust%20methods.%20We%20release%20our%20dataset%20at%0Ahttps%3A//huggingface.co/datasets/aimagelab/RAID%20and%20evaluation%20code%20at%0Ahttps%3A//github.com/pralab/RAID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03988v3&entry.124074799=Read"},
{"title": "PropEnc: A Property Encoder for Graph Neural Networks", "author": "Anwar Said and Waseem Abbas and Xenofon Koutsoukos", "abstract": "  Graph machine learning, particularly using graph neural networks, heavily\nrelies on node features. However, many real-world systems, such as social and\nbiological networks, lack node features due to privacy concerns, incomplete\ndata, or collection limitations. Structural and positional encoding are\ncommonly used to address this but are constrained by the maximum values of the\nencoded properties, such as the highest node degree. This limitation makes them\nimpractical for scale-free networks and applications involving large or\nnon-categorical properties. This paper introduces PropEnc, a novel and\nversatile encoder to generate expressive node embedding from any graph metric.\nBy combining histogram construction with reversed index encoding, PropEnc\noffers a flexible solution that supports low-dimensional representations and\ndiverse input types, effectively mitigating sparsity issues while improving\ncomputational efficiency. Additionally, it replicates one-hot encoding or\napproximates indices with high accuracy, making it adaptable to a wide range of\ngraph applications. We validate PropEnc through extensive experiments on graph\nclassification task across several social networks lacking node features. The\nempirical results demonstrate that PropEnc offers an efficient mechanism for\nconstructing node features from various graph metrics.\n", "link": "http://arxiv.org/abs/2409.11554v3", "date": "2025-06-09", "relevancy": 1.8917, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PropEnc%3A%20A%20Property%20Encoder%20for%20Graph%20Neural%20Networks&body=Title%3A%20PropEnc%3A%20A%20Property%20Encoder%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Anwar%20Said%20and%20Waseem%20Abbas%20and%20Xenofon%20Koutsoukos%0AAbstract%3A%20%20%20Graph%20machine%20learning%2C%20particularly%20using%20graph%20neural%20networks%2C%20heavily%0Arelies%20on%20node%20features.%20However%2C%20many%20real-world%20systems%2C%20such%20as%20social%20and%0Abiological%20networks%2C%20lack%20node%20features%20due%20to%20privacy%20concerns%2C%20incomplete%0Adata%2C%20or%20collection%20limitations.%20Structural%20and%20positional%20encoding%20are%0Acommonly%20used%20to%20address%20this%20but%20are%20constrained%20by%20the%20maximum%20values%20of%20the%0Aencoded%20properties%2C%20such%20as%20the%20highest%20node%20degree.%20This%20limitation%20makes%20them%0Aimpractical%20for%20scale-free%20networks%20and%20applications%20involving%20large%20or%0Anon-categorical%20properties.%20This%20paper%20introduces%20PropEnc%2C%20a%20novel%20and%0Aversatile%20encoder%20to%20generate%20expressive%20node%20embedding%20from%20any%20graph%20metric.%0ABy%20combining%20histogram%20construction%20with%20reversed%20index%20encoding%2C%20PropEnc%0Aoffers%20a%20flexible%20solution%20that%20supports%20low-dimensional%20representations%20and%0Adiverse%20input%20types%2C%20effectively%20mitigating%20sparsity%20issues%20while%20improving%0Acomputational%20efficiency.%20Additionally%2C%20it%20replicates%20one-hot%20encoding%20or%0Aapproximates%20indices%20with%20high%20accuracy%2C%20making%20it%20adaptable%20to%20a%20wide%20range%20of%0Agraph%20applications.%20We%20validate%20PropEnc%20through%20extensive%20experiments%20on%20graph%0Aclassification%20task%20across%20several%20social%20networks%20lacking%20node%20features.%20The%0Aempirical%20results%20demonstrate%20that%20PropEnc%20offers%20an%20efficient%20mechanism%20for%0Aconstructing%20node%20features%20from%20various%20graph%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11554v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPropEnc%253A%2520A%2520Property%2520Encoder%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DAnwar%2520Said%2520and%2520Waseem%2520Abbas%2520and%2520Xenofon%2520Koutsoukos%26entry.1292438233%3D%2520%2520Graph%2520machine%2520learning%252C%2520particularly%2520using%2520graph%2520neural%2520networks%252C%2520heavily%250Arelies%2520on%2520node%2520features.%2520However%252C%2520many%2520real-world%2520systems%252C%2520such%2520as%2520social%2520and%250Abiological%2520networks%252C%2520lack%2520node%2520features%2520due%2520to%2520privacy%2520concerns%252C%2520incomplete%250Adata%252C%2520or%2520collection%2520limitations.%2520Structural%2520and%2520positional%2520encoding%2520are%250Acommonly%2520used%2520to%2520address%2520this%2520but%2520are%2520constrained%2520by%2520the%2520maximum%2520values%2520of%2520the%250Aencoded%2520properties%252C%2520such%2520as%2520the%2520highest%2520node%2520degree.%2520This%2520limitation%2520makes%2520them%250Aimpractical%2520for%2520scale-free%2520networks%2520and%2520applications%2520involving%2520large%2520or%250Anon-categorical%2520properties.%2520This%2520paper%2520introduces%2520PropEnc%252C%2520a%2520novel%2520and%250Aversatile%2520encoder%2520to%2520generate%2520expressive%2520node%2520embedding%2520from%2520any%2520graph%2520metric.%250ABy%2520combining%2520histogram%2520construction%2520with%2520reversed%2520index%2520encoding%252C%2520PropEnc%250Aoffers%2520a%2520flexible%2520solution%2520that%2520supports%2520low-dimensional%2520representations%2520and%250Adiverse%2520input%2520types%252C%2520effectively%2520mitigating%2520sparsity%2520issues%2520while%2520improving%250Acomputational%2520efficiency.%2520Additionally%252C%2520it%2520replicates%2520one-hot%2520encoding%2520or%250Aapproximates%2520indices%2520with%2520high%2520accuracy%252C%2520making%2520it%2520adaptable%2520to%2520a%2520wide%2520range%2520of%250Agraph%2520applications.%2520We%2520validate%2520PropEnc%2520through%2520extensive%2520experiments%2520on%2520graph%250Aclassification%2520task%2520across%2520several%2520social%2520networks%2520lacking%2520node%2520features.%2520The%250Aempirical%2520results%2520demonstrate%2520that%2520PropEnc%2520offers%2520an%2520efficient%2520mechanism%2520for%250Aconstructing%2520node%2520features%2520from%2520various%2520graph%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11554v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PropEnc%3A%20A%20Property%20Encoder%20for%20Graph%20Neural%20Networks&entry.906535625=Anwar%20Said%20and%20Waseem%20Abbas%20and%20Xenofon%20Koutsoukos&entry.1292438233=%20%20Graph%20machine%20learning%2C%20particularly%20using%20graph%20neural%20networks%2C%20heavily%0Arelies%20on%20node%20features.%20However%2C%20many%20real-world%20systems%2C%20such%20as%20social%20and%0Abiological%20networks%2C%20lack%20node%20features%20due%20to%20privacy%20concerns%2C%20incomplete%0Adata%2C%20or%20collection%20limitations.%20Structural%20and%20positional%20encoding%20are%0Acommonly%20used%20to%20address%20this%20but%20are%20constrained%20by%20the%20maximum%20values%20of%20the%0Aencoded%20properties%2C%20such%20as%20the%20highest%20node%20degree.%20This%20limitation%20makes%20them%0Aimpractical%20for%20scale-free%20networks%20and%20applications%20involving%20large%20or%0Anon-categorical%20properties.%20This%20paper%20introduces%20PropEnc%2C%20a%20novel%20and%0Aversatile%20encoder%20to%20generate%20expressive%20node%20embedding%20from%20any%20graph%20metric.%0ABy%20combining%20histogram%20construction%20with%20reversed%20index%20encoding%2C%20PropEnc%0Aoffers%20a%20flexible%20solution%20that%20supports%20low-dimensional%20representations%20and%0Adiverse%20input%20types%2C%20effectively%20mitigating%20sparsity%20issues%20while%20improving%0Acomputational%20efficiency.%20Additionally%2C%20it%20replicates%20one-hot%20encoding%20or%0Aapproximates%20indices%20with%20high%20accuracy%2C%20making%20it%20adaptable%20to%20a%20wide%20range%20of%0Agraph%20applications.%20We%20validate%20PropEnc%20through%20extensive%20experiments%20on%20graph%0Aclassification%20task%20across%20several%20social%20networks%20lacking%20node%20features.%20The%0Aempirical%20results%20demonstrate%20that%20PropEnc%20offers%20an%20efficient%20mechanism%20for%0Aconstructing%20node%20features%20from%20various%20graph%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11554v3&entry.124074799=Read"},
{"title": "Distillation Robustifies Unlearning", "author": "Bruce W. Lee and Addie Foote and Alex Infanger and Leni Shor and Harish Kamath and Jacob Goldman-Wetzler and Bryce Woodworth and Alex Cloud and Alexander Matt Turner", "abstract": "  Current LLM unlearning methods are not robust: they can be reverted easily\nwith a few steps of finetuning. This is true even for the idealized unlearning\nmethod of training to imitate an oracle model that was never exposed to\nunwanted information, suggesting that output-based finetuning is insufficient\nto achieve robust unlearning. In a similar vein, we find that training a\nrandomly initialized student to imitate an unlearned model transfers desired\nbehaviors while leaving undesired capabilities behind. In other words,\ndistillation robustifies unlearning. Building on this insight, we propose\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\nunlearned model into a partially noised copy of itself. UNDO introduces a\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\nUNDO matches the robustness of a model retrained from scratch with perfect data\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\nthe pretraining data to be labeled. We also show that UNDO robustifies\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\nbenchmark. Since distillation is widely used in practice, incorporating an\nunlearning step beforehand offers a convenient path to robust capability\nremoval.\n", "link": "http://arxiv.org/abs/2506.06278v2", "date": "2025-06-09", "relevancy": 1.4149, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4814}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4704}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distillation%20Robustifies%20Unlearning&body=Title%3A%20Distillation%20Robustifies%20Unlearning%0AAuthor%3A%20Bruce%20W.%20Lee%20and%20Addie%20Foote%20and%20Alex%20Infanger%20and%20Leni%20Shor%20and%20Harish%20Kamath%20and%20Jacob%20Goldman-Wetzler%20and%20Bryce%20Woodworth%20and%20Alex%20Cloud%20and%20Alexander%20Matt%20Turner%0AAbstract%3A%20%20%20Current%20LLM%20unlearning%20methods%20are%20not%20robust%3A%20they%20can%20be%20reverted%20easily%0Awith%20a%20few%20steps%20of%20finetuning.%20This%20is%20true%20even%20for%20the%20idealized%20unlearning%0Amethod%20of%20training%20to%20imitate%20an%20oracle%20model%20that%20was%20never%20exposed%20to%0Aunwanted%20information%2C%20suggesting%20that%20output-based%20finetuning%20is%20insufficient%0Ato%20achieve%20robust%20unlearning.%20In%20a%20similar%20vein%2C%20we%20find%20that%20training%20a%0Arandomly%20initialized%20student%20to%20imitate%20an%20unlearned%20model%20transfers%20desired%0Abehaviors%20while%20leaving%20undesired%20capabilities%20behind.%20In%20other%20words%2C%0Adistillation%20robustifies%20unlearning.%20Building%20on%20this%20insight%2C%20we%20propose%0AUnlearn-Noise-Distill-on-Outputs%20%28UNDO%29%2C%20a%20scalable%20method%20that%20distills%20an%0Aunlearned%20model%20into%20a%20partially%20noised%20copy%20of%20itself.%20UNDO%20introduces%20a%0Atunable%20tradeoff%20between%20compute%20cost%20and%20robustness%2C%20establishing%20a%20new%20Pareto%0Afrontier%20on%20synthetic%20language%20and%20arithmetic%20tasks.%20At%20its%20strongest%20setting%2C%0AUNDO%20matches%20the%20robustness%20of%20a%20model%20retrained%20from%20scratch%20with%20perfect%20data%0Afiltering%20while%20using%20only%2060-80%25%20of%20the%20compute%20and%20requiring%20only%200.01%25%20of%0Athe%20pretraining%20data%20to%20be%20labeled.%20We%20also%20show%20that%20UNDO%20robustifies%0Aunlearning%20on%20the%20more%20realistic%20Weapons%20of%20Mass%20Destruction%20Proxy%20%28WMDP%29%0Abenchmark.%20Since%20distillation%20is%20widely%20used%20in%20practice%2C%20incorporating%20an%0Aunlearning%20step%20beforehand%20offers%20a%20convenient%20path%20to%20robust%20capability%0Aremoval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistillation%2520Robustifies%2520Unlearning%26entry.906535625%3DBruce%2520W.%2520Lee%2520and%2520Addie%2520Foote%2520and%2520Alex%2520Infanger%2520and%2520Leni%2520Shor%2520and%2520Harish%2520Kamath%2520and%2520Jacob%2520Goldman-Wetzler%2520and%2520Bryce%2520Woodworth%2520and%2520Alex%2520Cloud%2520and%2520Alexander%2520Matt%2520Turner%26entry.1292438233%3D%2520%2520Current%2520LLM%2520unlearning%2520methods%2520are%2520not%2520robust%253A%2520they%2520can%2520be%2520reverted%2520easily%250Awith%2520a%2520few%2520steps%2520of%2520finetuning.%2520This%2520is%2520true%2520even%2520for%2520the%2520idealized%2520unlearning%250Amethod%2520of%2520training%2520to%2520imitate%2520an%2520oracle%2520model%2520that%2520was%2520never%2520exposed%2520to%250Aunwanted%2520information%252C%2520suggesting%2520that%2520output-based%2520finetuning%2520is%2520insufficient%250Ato%2520achieve%2520robust%2520unlearning.%2520In%2520a%2520similar%2520vein%252C%2520we%2520find%2520that%2520training%2520a%250Arandomly%2520initialized%2520student%2520to%2520imitate%2520an%2520unlearned%2520model%2520transfers%2520desired%250Abehaviors%2520while%2520leaving%2520undesired%2520capabilities%2520behind.%2520In%2520other%2520words%252C%250Adistillation%2520robustifies%2520unlearning.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%250AUnlearn-Noise-Distill-on-Outputs%2520%2528UNDO%2529%252C%2520a%2520scalable%2520method%2520that%2520distills%2520an%250Aunlearned%2520model%2520into%2520a%2520partially%2520noised%2520copy%2520of%2520itself.%2520UNDO%2520introduces%2520a%250Atunable%2520tradeoff%2520between%2520compute%2520cost%2520and%2520robustness%252C%2520establishing%2520a%2520new%2520Pareto%250Afrontier%2520on%2520synthetic%2520language%2520and%2520arithmetic%2520tasks.%2520At%2520its%2520strongest%2520setting%252C%250AUNDO%2520matches%2520the%2520robustness%2520of%2520a%2520model%2520retrained%2520from%2520scratch%2520with%2520perfect%2520data%250Afiltering%2520while%2520using%2520only%252060-80%2525%2520of%2520the%2520compute%2520and%2520requiring%2520only%25200.01%2525%2520of%250Athe%2520pretraining%2520data%2520to%2520be%2520labeled.%2520We%2520also%2520show%2520that%2520UNDO%2520robustifies%250Aunlearning%2520on%2520the%2520more%2520realistic%2520Weapons%2520of%2520Mass%2520Destruction%2520Proxy%2520%2528WMDP%2529%250Abenchmark.%2520Since%2520distillation%2520is%2520widely%2520used%2520in%2520practice%252C%2520incorporating%2520an%250Aunlearning%2520step%2520beforehand%2520offers%2520a%2520convenient%2520path%2520to%2520robust%2520capability%250Aremoval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distillation%20Robustifies%20Unlearning&entry.906535625=Bruce%20W.%20Lee%20and%20Addie%20Foote%20and%20Alex%20Infanger%20and%20Leni%20Shor%20and%20Harish%20Kamath%20and%20Jacob%20Goldman-Wetzler%20and%20Bryce%20Woodworth%20and%20Alex%20Cloud%20and%20Alexander%20Matt%20Turner&entry.1292438233=%20%20Current%20LLM%20unlearning%20methods%20are%20not%20robust%3A%20they%20can%20be%20reverted%20easily%0Awith%20a%20few%20steps%20of%20finetuning.%20This%20is%20true%20even%20for%20the%20idealized%20unlearning%0Amethod%20of%20training%20to%20imitate%20an%20oracle%20model%20that%20was%20never%20exposed%20to%0Aunwanted%20information%2C%20suggesting%20that%20output-based%20finetuning%20is%20insufficient%0Ato%20achieve%20robust%20unlearning.%20In%20a%20similar%20vein%2C%20we%20find%20that%20training%20a%0Arandomly%20initialized%20student%20to%20imitate%20an%20unlearned%20model%20transfers%20desired%0Abehaviors%20while%20leaving%20undesired%20capabilities%20behind.%20In%20other%20words%2C%0Adistillation%20robustifies%20unlearning.%20Building%20on%20this%20insight%2C%20we%20propose%0AUnlearn-Noise-Distill-on-Outputs%20%28UNDO%29%2C%20a%20scalable%20method%20that%20distills%20an%0Aunlearned%20model%20into%20a%20partially%20noised%20copy%20of%20itself.%20UNDO%20introduces%20a%0Atunable%20tradeoff%20between%20compute%20cost%20and%20robustness%2C%20establishing%20a%20new%20Pareto%0Afrontier%20on%20synthetic%20language%20and%20arithmetic%20tasks.%20At%20its%20strongest%20setting%2C%0AUNDO%20matches%20the%20robustness%20of%20a%20model%20retrained%20from%20scratch%20with%20perfect%20data%0Afiltering%20while%20using%20only%2060-80%25%20of%20the%20compute%20and%20requiring%20only%200.01%25%20of%0Athe%20pretraining%20data%20to%20be%20labeled.%20We%20also%20show%20that%20UNDO%20robustifies%0Aunlearning%20on%20the%20more%20realistic%20Weapons%20of%20Mass%20Destruction%20Proxy%20%28WMDP%29%0Abenchmark.%20Since%20distillation%20is%20widely%20used%20in%20practice%2C%20incorporating%20an%0Aunlearning%20step%20beforehand%20offers%20a%20convenient%20path%20to%20robust%20capability%0Aremoval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06278v2&entry.124074799=Read"},
{"title": "Profiling Electric Vehicles via Early Charging Voltage Patterns", "author": "Francesco Marchiori and Denis Donadel and Alessandro Brighente and Mauro Conti", "abstract": "  Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable\nalternative to fuel-powered vehicles, making secure charging infrastructure\nessential. Despite traditional authentication protocols, recent results showed\nthat attackers may steal energy through tailored relay attacks. One\ncountermeasure is leveraging the EV's fingerprint on the current exchanged\nduring charging. However, existing methods focus on the final charging stage,\nallowing malicious actors to consume substantial energy before being detected\nand repudiated. This underscores the need for earlier and more effective\nauthentication methods to prevent unauthorized charging. Meanwhile, profiling\nraises privacy concerns, as uniquely identifying EVs through charging patterns\ncould enable user tracking.\n  In this paper, we propose a framework for uniquely identifying EVs using\nphysical measurements from the early charging stages. We hypothesize that\nvoltage behavior early in the process exhibits similar characteristics to\ncurrent behavior in later stages. By extracting features from early voltage\nmeasurements, we demonstrate the feasibility of EV profiling. Our approach\nimproves existing methods by enabling faster and more reliable vehicle\nidentification. We test our solution on a dataset of 7408 usable charges from\n49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that\nnear-optimal performance is possible with just 10 key features, improving\nefficiency alongside our lightweight models. This research lays the foundation\nfor a novel authentication factor while exposing potential privacy risks from\nunauthorized access to charging data.\n", "link": "http://arxiv.org/abs/2506.07714v1", "date": "2025-06-09", "relevancy": 1.8907, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3951}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3713}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Profiling%20Electric%20Vehicles%20via%20Early%20Charging%20Voltage%20Patterns&body=Title%3A%20Profiling%20Electric%20Vehicles%20via%20Early%20Charging%20Voltage%20Patterns%0AAuthor%3A%20Francesco%20Marchiori%20and%20Denis%20Donadel%20and%20Alessandro%20Brighente%20and%20Mauro%20Conti%0AAbstract%3A%20%20%20Electric%20Vehicles%20%28EVs%29%20are%20rapidly%20gaining%20adoption%20as%20a%20sustainable%0Aalternative%20to%20fuel-powered%20vehicles%2C%20making%20secure%20charging%20infrastructure%0Aessential.%20Despite%20traditional%20authentication%20protocols%2C%20recent%20results%20showed%0Athat%20attackers%20may%20steal%20energy%20through%20tailored%20relay%20attacks.%20One%0Acountermeasure%20is%20leveraging%20the%20EV%27s%20fingerprint%20on%20the%20current%20exchanged%0Aduring%20charging.%20However%2C%20existing%20methods%20focus%20on%20the%20final%20charging%20stage%2C%0Aallowing%20malicious%20actors%20to%20consume%20substantial%20energy%20before%20being%20detected%0Aand%20repudiated.%20This%20underscores%20the%20need%20for%20earlier%20and%20more%20effective%0Aauthentication%20methods%20to%20prevent%20unauthorized%20charging.%20Meanwhile%2C%20profiling%0Araises%20privacy%20concerns%2C%20as%20uniquely%20identifying%20EVs%20through%20charging%20patterns%0Acould%20enable%20user%20tracking.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20uniquely%20identifying%20EVs%20using%0Aphysical%20measurements%20from%20the%20early%20charging%20stages.%20We%20hypothesize%20that%0Avoltage%20behavior%20early%20in%20the%20process%20exhibits%20similar%20characteristics%20to%0Acurrent%20behavior%20in%20later%20stages.%20By%20extracting%20features%20from%20early%20voltage%0Ameasurements%2C%20we%20demonstrate%20the%20feasibility%20of%20EV%20profiling.%20Our%20approach%0Aimproves%20existing%20methods%20by%20enabling%20faster%20and%20more%20reliable%20vehicle%0Aidentification.%20We%20test%20our%20solution%20on%20a%20dataset%20of%207408%20usable%20charges%20from%0A49%20EVs%2C%20achieving%20up%20to%200.86%20accuracy.%20Feature%20importance%20analysis%20shows%20that%0Anear-optimal%20performance%20is%20possible%20with%20just%2010%20key%20features%2C%20improving%0Aefficiency%20alongside%20our%20lightweight%20models.%20This%20research%20lays%20the%20foundation%0Afor%20a%20novel%20authentication%20factor%20while%20exposing%20potential%20privacy%20risks%20from%0Aunauthorized%20access%20to%20charging%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProfiling%2520Electric%2520Vehicles%2520via%2520Early%2520Charging%2520Voltage%2520Patterns%26entry.906535625%3DFrancesco%2520Marchiori%2520and%2520Denis%2520Donadel%2520and%2520Alessandro%2520Brighente%2520and%2520Mauro%2520Conti%26entry.1292438233%3D%2520%2520Electric%2520Vehicles%2520%2528EVs%2529%2520are%2520rapidly%2520gaining%2520adoption%2520as%2520a%2520sustainable%250Aalternative%2520to%2520fuel-powered%2520vehicles%252C%2520making%2520secure%2520charging%2520infrastructure%250Aessential.%2520Despite%2520traditional%2520authentication%2520protocols%252C%2520recent%2520results%2520showed%250Athat%2520attackers%2520may%2520steal%2520energy%2520through%2520tailored%2520relay%2520attacks.%2520One%250Acountermeasure%2520is%2520leveraging%2520the%2520EV%2527s%2520fingerprint%2520on%2520the%2520current%2520exchanged%250Aduring%2520charging.%2520However%252C%2520existing%2520methods%2520focus%2520on%2520the%2520final%2520charging%2520stage%252C%250Aallowing%2520malicious%2520actors%2520to%2520consume%2520substantial%2520energy%2520before%2520being%2520detected%250Aand%2520repudiated.%2520This%2520underscores%2520the%2520need%2520for%2520earlier%2520and%2520more%2520effective%250Aauthentication%2520methods%2520to%2520prevent%2520unauthorized%2520charging.%2520Meanwhile%252C%2520profiling%250Araises%2520privacy%2520concerns%252C%2520as%2520uniquely%2520identifying%2520EVs%2520through%2520charging%2520patterns%250Acould%2520enable%2520user%2520tracking.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520for%2520uniquely%2520identifying%2520EVs%2520using%250Aphysical%2520measurements%2520from%2520the%2520early%2520charging%2520stages.%2520We%2520hypothesize%2520that%250Avoltage%2520behavior%2520early%2520in%2520the%2520process%2520exhibits%2520similar%2520characteristics%2520to%250Acurrent%2520behavior%2520in%2520later%2520stages.%2520By%2520extracting%2520features%2520from%2520early%2520voltage%250Ameasurements%252C%2520we%2520demonstrate%2520the%2520feasibility%2520of%2520EV%2520profiling.%2520Our%2520approach%250Aimproves%2520existing%2520methods%2520by%2520enabling%2520faster%2520and%2520more%2520reliable%2520vehicle%250Aidentification.%2520We%2520test%2520our%2520solution%2520on%2520a%2520dataset%2520of%25207408%2520usable%2520charges%2520from%250A49%2520EVs%252C%2520achieving%2520up%2520to%25200.86%2520accuracy.%2520Feature%2520importance%2520analysis%2520shows%2520that%250Anear-optimal%2520performance%2520is%2520possible%2520with%2520just%252010%2520key%2520features%252C%2520improving%250Aefficiency%2520alongside%2520our%2520lightweight%2520models.%2520This%2520research%2520lays%2520the%2520foundation%250Afor%2520a%2520novel%2520authentication%2520factor%2520while%2520exposing%2520potential%2520privacy%2520risks%2520from%250Aunauthorized%2520access%2520to%2520charging%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Profiling%20Electric%20Vehicles%20via%20Early%20Charging%20Voltage%20Patterns&entry.906535625=Francesco%20Marchiori%20and%20Denis%20Donadel%20and%20Alessandro%20Brighente%20and%20Mauro%20Conti&entry.1292438233=%20%20Electric%20Vehicles%20%28EVs%29%20are%20rapidly%20gaining%20adoption%20as%20a%20sustainable%0Aalternative%20to%20fuel-powered%20vehicles%2C%20making%20secure%20charging%20infrastructure%0Aessential.%20Despite%20traditional%20authentication%20protocols%2C%20recent%20results%20showed%0Athat%20attackers%20may%20steal%20energy%20through%20tailored%20relay%20attacks.%20One%0Acountermeasure%20is%20leveraging%20the%20EV%27s%20fingerprint%20on%20the%20current%20exchanged%0Aduring%20charging.%20However%2C%20existing%20methods%20focus%20on%20the%20final%20charging%20stage%2C%0Aallowing%20malicious%20actors%20to%20consume%20substantial%20energy%20before%20being%20detected%0Aand%20repudiated.%20This%20underscores%20the%20need%20for%20earlier%20and%20more%20effective%0Aauthentication%20methods%20to%20prevent%20unauthorized%20charging.%20Meanwhile%2C%20profiling%0Araises%20privacy%20concerns%2C%20as%20uniquely%20identifying%20EVs%20through%20charging%20patterns%0Acould%20enable%20user%20tracking.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20uniquely%20identifying%20EVs%20using%0Aphysical%20measurements%20from%20the%20early%20charging%20stages.%20We%20hypothesize%20that%0Avoltage%20behavior%20early%20in%20the%20process%20exhibits%20similar%20characteristics%20to%0Acurrent%20behavior%20in%20later%20stages.%20By%20extracting%20features%20from%20early%20voltage%0Ameasurements%2C%20we%20demonstrate%20the%20feasibility%20of%20EV%20profiling.%20Our%20approach%0Aimproves%20existing%20methods%20by%20enabling%20faster%20and%20more%20reliable%20vehicle%0Aidentification.%20We%20test%20our%20solution%20on%20a%20dataset%20of%207408%20usable%20charges%20from%0A49%20EVs%2C%20achieving%20up%20to%200.86%20accuracy.%20Feature%20importance%20analysis%20shows%20that%0Anear-optimal%20performance%20is%20possible%20with%20just%2010%20key%20features%2C%20improving%0Aefficiency%20alongside%20our%20lightweight%20models.%20This%20research%20lays%20the%20foundation%0Afor%20a%20novel%20authentication%20factor%20while%20exposing%20potential%20privacy%20risks%20from%0Aunauthorized%20access%20to%20charging%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07714v1&entry.124074799=Read"},
{"title": "Can Hessian-Based Insights Support Fault Diagnosis in Attention-based\n  Models?", "author": "Sigma Jahan and Mohammad Masudur Rahman", "abstract": "  As attention-based deep learning models scale in size and complexity,\ndiagnosing their faults becomes increasingly challenging. In this work, we\nconduct an empirical study to evaluate the potential of Hessian-based analysis\nfor diagnosing faults in attention-based models. Specifically, we use\nHessian-derived insights to identify fragile regions (via curvature analysis)\nand parameter interdependencies (via parameter interaction analysis) within\nattention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,\nDistilBERT), we show that Hessian-based metrics can localize instability and\npinpoint fault sources more effectively than gradients alone. Our empirical\nfindings suggest that these metrics could significantly improve fault diagnosis\nin complex neural architectures, potentially improving software debugging\npractices.\n", "link": "http://arxiv.org/abs/2506.07871v1", "date": "2025-06-09", "relevancy": 1.9766, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Hessian-Based%20Insights%20Support%20Fault%20Diagnosis%20in%20Attention-based%0A%20%20Models%3F&body=Title%3A%20Can%20Hessian-Based%20Insights%20Support%20Fault%20Diagnosis%20in%20Attention-based%0A%20%20Models%3F%0AAuthor%3A%20Sigma%20Jahan%20and%20Mohammad%20Masudur%20Rahman%0AAbstract%3A%20%20%20As%20attention-based%20deep%20learning%20models%20scale%20in%20size%20and%20complexity%2C%0Adiagnosing%20their%20faults%20becomes%20increasingly%20challenging.%20In%20this%20work%2C%20we%0Aconduct%20an%20empirical%20study%20to%20evaluate%20the%20potential%20of%20Hessian-based%20analysis%0Afor%20diagnosing%20faults%20in%20attention-based%20models.%20Specifically%2C%20we%20use%0AHessian-derived%20insights%20to%20identify%20fragile%20regions%20%28via%20curvature%20analysis%29%0Aand%20parameter%20interdependencies%20%28via%20parameter%20interaction%20analysis%29%20within%0Aattention%20mechanisms.%20Through%20experiments%20on%20three%20diverse%20models%20%28HAN%2C%203D-CNN%2C%0ADistilBERT%29%2C%20we%20show%20that%20Hessian-based%20metrics%20can%20localize%20instability%20and%0Apinpoint%20fault%20sources%20more%20effectively%20than%20gradients%20alone.%20Our%20empirical%0Afindings%20suggest%20that%20these%20metrics%20could%20significantly%20improve%20fault%20diagnosis%0Ain%20complex%20neural%20architectures%2C%20potentially%20improving%20software%20debugging%0Apractices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Hessian-Based%2520Insights%2520Support%2520Fault%2520Diagnosis%2520in%2520Attention-based%250A%2520%2520Models%253F%26entry.906535625%3DSigma%2520Jahan%2520and%2520Mohammad%2520Masudur%2520Rahman%26entry.1292438233%3D%2520%2520As%2520attention-based%2520deep%2520learning%2520models%2520scale%2520in%2520size%2520and%2520complexity%252C%250Adiagnosing%2520their%2520faults%2520becomes%2520increasingly%2520challenging.%2520In%2520this%2520work%252C%2520we%250Aconduct%2520an%2520empirical%2520study%2520to%2520evaluate%2520the%2520potential%2520of%2520Hessian-based%2520analysis%250Afor%2520diagnosing%2520faults%2520in%2520attention-based%2520models.%2520Specifically%252C%2520we%2520use%250AHessian-derived%2520insights%2520to%2520identify%2520fragile%2520regions%2520%2528via%2520curvature%2520analysis%2529%250Aand%2520parameter%2520interdependencies%2520%2528via%2520parameter%2520interaction%2520analysis%2529%2520within%250Aattention%2520mechanisms.%2520Through%2520experiments%2520on%2520three%2520diverse%2520models%2520%2528HAN%252C%25203D-CNN%252C%250ADistilBERT%2529%252C%2520we%2520show%2520that%2520Hessian-based%2520metrics%2520can%2520localize%2520instability%2520and%250Apinpoint%2520fault%2520sources%2520more%2520effectively%2520than%2520gradients%2520alone.%2520Our%2520empirical%250Afindings%2520suggest%2520that%2520these%2520metrics%2520could%2520significantly%2520improve%2520fault%2520diagnosis%250Ain%2520complex%2520neural%2520architectures%252C%2520potentially%2520improving%2520software%2520debugging%250Apractices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Hessian-Based%20Insights%20Support%20Fault%20Diagnosis%20in%20Attention-based%0A%20%20Models%3F&entry.906535625=Sigma%20Jahan%20and%20Mohammad%20Masudur%20Rahman&entry.1292438233=%20%20As%20attention-based%20deep%20learning%20models%20scale%20in%20size%20and%20complexity%2C%0Adiagnosing%20their%20faults%20becomes%20increasingly%20challenging.%20In%20this%20work%2C%20we%0Aconduct%20an%20empirical%20study%20to%20evaluate%20the%20potential%20of%20Hessian-based%20analysis%0Afor%20diagnosing%20faults%20in%20attention-based%20models.%20Specifically%2C%20we%20use%0AHessian-derived%20insights%20to%20identify%20fragile%20regions%20%28via%20curvature%20analysis%29%0Aand%20parameter%20interdependencies%20%28via%20parameter%20interaction%20analysis%29%20within%0Aattention%20mechanisms.%20Through%20experiments%20on%20three%20diverse%20models%20%28HAN%2C%203D-CNN%2C%0ADistilBERT%29%2C%20we%20show%20that%20Hessian-based%20metrics%20can%20localize%20instability%20and%0Apinpoint%20fault%20sources%20more%20effectively%20than%20gradients%20alone.%20Our%20empirical%0Afindings%20suggest%20that%20these%20metrics%20could%20significantly%20improve%20fault%20diagnosis%0Ain%20complex%20neural%20architectures%2C%20potentially%20improving%20software%20debugging%0Apractices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07871v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


