<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250128.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian\n  Splatting scenes", "author": "Juliette Marrie and Romain Menegaux and Michael Arbel and Diane Larlus and Julien Mairal", "abstract": "  We address the problem of extending the capabilities of vision foundation\nmodels such as DINO, SAM, and CLIP, to 3D tasks. Specifically, we introduce a\nnovel method to uplift 2D image features into Gaussian Splatting\nrepresentations of 3D scenes. Unlike traditional approaches that rely on\nminimizing a reconstruction loss, our method employs a simpler and more\nefficient feature aggregation technique, augmented by a graph diffusion\nmechanism. Graph diffusion refines 3D features, such as coarse segmentation\nmasks, by leveraging 3D geometry and pairwise similarities induced by DINOv2.\nOur approach achieves performance comparable to the state of the art on\nmultiple downstream tasks while delivering significant speed-ups. Notably, we\nobtain competitive segmentation results using generic DINOv2 features, despite\nDINOv2 not being trained on millions of annotated segmentation masks like SAM.\nWhen applied to CLIP features, our method demonstrates strong performance in\nopen-vocabulary object localization tasks, highlighting the versatility of our\napproach.\n", "link": "http://arxiv.org/abs/2410.14462v4", "date": "2025-01-28", "relevancy": 3.3416, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.692}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6836}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUDVIG%3A%20Learning-free%20Uplifting%20of%202D%20Visual%20features%20to%20Gaussian%0A%20%20Splatting%20scenes&body=Title%3A%20LUDVIG%3A%20Learning-free%20Uplifting%20of%202D%20Visual%20features%20to%20Gaussian%0A%20%20Splatting%20scenes%0AAuthor%3A%20Juliette%20Marrie%20and%20Romain%20Menegaux%20and%20Michael%20Arbel%20and%20Diane%20Larlus%20and%20Julien%20Mairal%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20extending%20the%20capabilities%20of%20vision%20foundation%0Amodels%20such%20as%20DINO%2C%20SAM%2C%20and%20CLIP%2C%20to%203D%20tasks.%20Specifically%2C%20we%20introduce%20a%0Anovel%20method%20to%20uplift%202D%20image%20features%20into%20Gaussian%20Splatting%0Arepresentations%20of%203D%20scenes.%20Unlike%20traditional%20approaches%20that%20rely%20on%0Aminimizing%20a%20reconstruction%20loss%2C%20our%20method%20employs%20a%20simpler%20and%20more%0Aefficient%20feature%20aggregation%20technique%2C%20augmented%20by%20a%20graph%20diffusion%0Amechanism.%20Graph%20diffusion%20refines%203D%20features%2C%20such%20as%20coarse%20segmentation%0Amasks%2C%20by%20leveraging%203D%20geometry%20and%20pairwise%20similarities%20induced%20by%20DINOv2.%0AOur%20approach%20achieves%20performance%20comparable%20to%20the%20state%20of%20the%20art%20on%0Amultiple%20downstream%20tasks%20while%20delivering%20significant%20speed-ups.%20Notably%2C%20we%0Aobtain%20competitive%20segmentation%20results%20using%20generic%20DINOv2%20features%2C%20despite%0ADINOv2%20not%20being%20trained%20on%20millions%20of%20annotated%20segmentation%20masks%20like%20SAM.%0AWhen%20applied%20to%20CLIP%20features%2C%20our%20method%20demonstrates%20strong%20performance%20in%0Aopen-vocabulary%20object%20localization%20tasks%2C%20highlighting%20the%20versatility%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14462v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUDVIG%253A%2520Learning-free%2520Uplifting%2520of%25202D%2520Visual%2520features%2520to%2520Gaussian%250A%2520%2520Splatting%2520scenes%26entry.906535625%3DJuliette%2520Marrie%2520and%2520Romain%2520Menegaux%2520and%2520Michael%2520Arbel%2520and%2520Diane%2520Larlus%2520and%2520Julien%2520Mairal%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520extending%2520the%2520capabilities%2520of%2520vision%2520foundation%250Amodels%2520such%2520as%2520DINO%252C%2520SAM%252C%2520and%2520CLIP%252C%2520to%25203D%2520tasks.%2520Specifically%252C%2520we%2520introduce%2520a%250Anovel%2520method%2520to%2520uplift%25202D%2520image%2520features%2520into%2520Gaussian%2520Splatting%250Arepresentations%2520of%25203D%2520scenes.%2520Unlike%2520traditional%2520approaches%2520that%2520rely%2520on%250Aminimizing%2520a%2520reconstruction%2520loss%252C%2520our%2520method%2520employs%2520a%2520simpler%2520and%2520more%250Aefficient%2520feature%2520aggregation%2520technique%252C%2520augmented%2520by%2520a%2520graph%2520diffusion%250Amechanism.%2520Graph%2520diffusion%2520refines%25203D%2520features%252C%2520such%2520as%2520coarse%2520segmentation%250Amasks%252C%2520by%2520leveraging%25203D%2520geometry%2520and%2520pairwise%2520similarities%2520induced%2520by%2520DINOv2.%250AOur%2520approach%2520achieves%2520performance%2520comparable%2520to%2520the%2520state%2520of%2520the%2520art%2520on%250Amultiple%2520downstream%2520tasks%2520while%2520delivering%2520significant%2520speed-ups.%2520Notably%252C%2520we%250Aobtain%2520competitive%2520segmentation%2520results%2520using%2520generic%2520DINOv2%2520features%252C%2520despite%250ADINOv2%2520not%2520being%2520trained%2520on%2520millions%2520of%2520annotated%2520segmentation%2520masks%2520like%2520SAM.%250AWhen%2520applied%2520to%2520CLIP%2520features%252C%2520our%2520method%2520demonstrates%2520strong%2520performance%2520in%250Aopen-vocabulary%2520object%2520localization%2520tasks%252C%2520highlighting%2520the%2520versatility%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14462v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUDVIG%3A%20Learning-free%20Uplifting%20of%202D%20Visual%20features%20to%20Gaussian%0A%20%20Splatting%20scenes&entry.906535625=Juliette%20Marrie%20and%20Romain%20Menegaux%20and%20Michael%20Arbel%20and%20Diane%20Larlus%20and%20Julien%20Mairal&entry.1292438233=%20%20We%20address%20the%20problem%20of%20extending%20the%20capabilities%20of%20vision%20foundation%0Amodels%20such%20as%20DINO%2C%20SAM%2C%20and%20CLIP%2C%20to%203D%20tasks.%20Specifically%2C%20we%20introduce%20a%0Anovel%20method%20to%20uplift%202D%20image%20features%20into%20Gaussian%20Splatting%0Arepresentations%20of%203D%20scenes.%20Unlike%20traditional%20approaches%20that%20rely%20on%0Aminimizing%20a%20reconstruction%20loss%2C%20our%20method%20employs%20a%20simpler%20and%20more%0Aefficient%20feature%20aggregation%20technique%2C%20augmented%20by%20a%20graph%20diffusion%0Amechanism.%20Graph%20diffusion%20refines%203D%20features%2C%20such%20as%20coarse%20segmentation%0Amasks%2C%20by%20leveraging%203D%20geometry%20and%20pairwise%20similarities%20induced%20by%20DINOv2.%0AOur%20approach%20achieves%20performance%20comparable%20to%20the%20state%20of%20the%20art%20on%0Amultiple%20downstream%20tasks%20while%20delivering%20significant%20speed-ups.%20Notably%2C%20we%0Aobtain%20competitive%20segmentation%20results%20using%20generic%20DINOv2%20features%2C%20despite%0ADINOv2%20not%20being%20trained%20on%20millions%20of%20annotated%20segmentation%20masks%20like%20SAM.%0AWhen%20applied%20to%20CLIP%20features%2C%20our%20method%20demonstrates%20strong%20performance%20in%0Aopen-vocabulary%20object%20localization%20tasks%2C%20highlighting%20the%20versatility%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14462v4&entry.124074799=Read"},
{"title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding", "author": "Boqiang Zhang and Kehan Li and Zesen Cheng and Zhiqiang Hu and Yuqian Yuan and Guanzheng Chen and Sicong Leng and Yuming Jiang and Hang Zhang and Xin Li and Peng Jin and Wenqi Zhang and Fan Wang and Lidong Bing and Deli Zhao", "abstract": "  In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.\n", "link": "http://arxiv.org/abs/2501.13106v3", "date": "2025-01-28", "relevancy": 3.2904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLLaMA%203%3A%20Frontier%20Multimodal%20Foundation%20Models%20for%20Image%20and%20Video%0A%20%20Understanding&body=Title%3A%20VideoLLaMA%203%3A%20Frontier%20Multimodal%20Foundation%20Models%20for%20Image%20and%20Video%0A%20%20Understanding%0AAuthor%3A%20Boqiang%20Zhang%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Zhiqiang%20Hu%20and%20Yuqian%20Yuan%20and%20Guanzheng%20Chen%20and%20Sicong%20Leng%20and%20Yuming%20Jiang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Peng%20Jin%20and%20Wenqi%20Zhang%20and%20Fan%20Wang%20and%20Lidong%20Bing%20and%20Deli%20Zhao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20VideoLLaMA3%2C%20a%20more%20advanced%20multimodal%20foundation%0Amodel%20for%20image%20and%20video%20understanding.%20The%20core%20design%20philosophy%20of%0AVideoLLaMA3%20is%20vision-centric.%20The%20meaning%20of%20%22vision-centric%22%20is%20two-fold%3A%20the%0Avision-centric%20training%20paradigm%20and%20vision-centric%20framework%20design.%20The%20key%0Ainsight%20of%20our%20vision-centric%20training%20paradigm%20is%20that%20high-quality%20image-text%0Adata%20is%20crucial%20for%20both%20image%20and%20video%20understanding.%20Instead%20of%20preparing%0Amassive%20video-text%20datasets%2C%20we%20focus%20on%20constructing%20large-scale%20and%0Ahigh-quality%20image-text%20datasets.%20VideoLLaMA3%20has%20four%20training%20stages%3A%201%29%0AVision%20Encoder%20Adaptation%2C%20which%20enables%20vision%20encoder%20to%20accept%20images%20of%0Avariable%20resolutions%20as%20input%3B%202%29%20Vision-Language%20Alignment%2C%20which%20jointly%0Atunes%20the%20vision%20encoder%2C%20projector%2C%20and%20LLM%20with%20large-scale%20image-text%20data%0Acovering%20multiple%20types%20%28including%20scene%20images%2C%20documents%2C%20charts%29%20as%20well%20as%0Atext-only%20data.%203%29%20Multi-task%20Fine-tuning%2C%20which%20incorporates%20image-text%20SFT%0Adata%20for%20downstream%20tasks%20and%20video-text%20data%20to%20establish%20a%20foundation%20for%0Avideo%20understanding.%204%29%20Video-centric%20Fine-tuning%2C%20which%20further%20improves%20the%0Amodel%27s%20capability%20in%20video%20understanding.%20As%20for%20the%20framework%20design%2C%20to%0Abetter%20capture%20fine-grained%20details%20in%20images%2C%20the%20pretrained%20vision%20encoder%20is%0Aadapted%20to%20encode%20images%20of%20varying%20sizes%20into%20vision%20tokens%20with%20corresponding%0Anumbers%2C%20rather%20than%20a%20fixed%20number%20of%20tokens.%20For%20video%20inputs%2C%20we%20reduce%20the%0Anumber%20of%20vision%20tokens%20according%20to%20their%20similarity%20so%20that%20the%0Arepresentation%20of%20videos%20will%20be%20more%20precise%20and%20compact.%20Benefit%20from%0Avision-centric%20designs%2C%20VideoLLaMA3%20achieves%20compelling%20performances%20in%20both%0Aimage%20and%20video%20understanding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13106v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLLaMA%25203%253A%2520Frontier%2520Multimodal%2520Foundation%2520Models%2520for%2520Image%2520and%2520Video%250A%2520%2520Understanding%26entry.906535625%3DBoqiang%2520Zhang%2520and%2520Kehan%2520Li%2520and%2520Zesen%2520Cheng%2520and%2520Zhiqiang%2520Hu%2520and%2520Yuqian%2520Yuan%2520and%2520Guanzheng%2520Chen%2520and%2520Sicong%2520Leng%2520and%2520Yuming%2520Jiang%2520and%2520Hang%2520Zhang%2520and%2520Xin%2520Li%2520and%2520Peng%2520Jin%2520and%2520Wenqi%2520Zhang%2520and%2520Fan%2520Wang%2520and%2520Lidong%2520Bing%2520and%2520Deli%2520Zhao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520VideoLLaMA3%252C%2520a%2520more%2520advanced%2520multimodal%2520foundation%250Amodel%2520for%2520image%2520and%2520video%2520understanding.%2520The%2520core%2520design%2520philosophy%2520of%250AVideoLLaMA3%2520is%2520vision-centric.%2520The%2520meaning%2520of%2520%2522vision-centric%2522%2520is%2520two-fold%253A%2520the%250Avision-centric%2520training%2520paradigm%2520and%2520vision-centric%2520framework%2520design.%2520The%2520key%250Ainsight%2520of%2520our%2520vision-centric%2520training%2520paradigm%2520is%2520that%2520high-quality%2520image-text%250Adata%2520is%2520crucial%2520for%2520both%2520image%2520and%2520video%2520understanding.%2520Instead%2520of%2520preparing%250Amassive%2520video-text%2520datasets%252C%2520we%2520focus%2520on%2520constructing%2520large-scale%2520and%250Ahigh-quality%2520image-text%2520datasets.%2520VideoLLaMA3%2520has%2520four%2520training%2520stages%253A%25201%2529%250AVision%2520Encoder%2520Adaptation%252C%2520which%2520enables%2520vision%2520encoder%2520to%2520accept%2520images%2520of%250Avariable%2520resolutions%2520as%2520input%253B%25202%2529%2520Vision-Language%2520Alignment%252C%2520which%2520jointly%250Atunes%2520the%2520vision%2520encoder%252C%2520projector%252C%2520and%2520LLM%2520with%2520large-scale%2520image-text%2520data%250Acovering%2520multiple%2520types%2520%2528including%2520scene%2520images%252C%2520documents%252C%2520charts%2529%2520as%2520well%2520as%250Atext-only%2520data.%25203%2529%2520Multi-task%2520Fine-tuning%252C%2520which%2520incorporates%2520image-text%2520SFT%250Adata%2520for%2520downstream%2520tasks%2520and%2520video-text%2520data%2520to%2520establish%2520a%2520foundation%2520for%250Avideo%2520understanding.%25204%2529%2520Video-centric%2520Fine-tuning%252C%2520which%2520further%2520improves%2520the%250Amodel%2527s%2520capability%2520in%2520video%2520understanding.%2520As%2520for%2520the%2520framework%2520design%252C%2520to%250Abetter%2520capture%2520fine-grained%2520details%2520in%2520images%252C%2520the%2520pretrained%2520vision%2520encoder%2520is%250Aadapted%2520to%2520encode%2520images%2520of%2520varying%2520sizes%2520into%2520vision%2520tokens%2520with%2520corresponding%250Anumbers%252C%2520rather%2520than%2520a%2520fixed%2520number%2520of%2520tokens.%2520For%2520video%2520inputs%252C%2520we%2520reduce%2520the%250Anumber%2520of%2520vision%2520tokens%2520according%2520to%2520their%2520similarity%2520so%2520that%2520the%250Arepresentation%2520of%2520videos%2520will%2520be%2520more%2520precise%2520and%2520compact.%2520Benefit%2520from%250Avision-centric%2520designs%252C%2520VideoLLaMA3%2520achieves%2520compelling%2520performances%2520in%2520both%250Aimage%2520and%2520video%2520understanding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13106v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLLaMA%203%3A%20Frontier%20Multimodal%20Foundation%20Models%20for%20Image%20and%20Video%0A%20%20Understanding&entry.906535625=Boqiang%20Zhang%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Zhiqiang%20Hu%20and%20Yuqian%20Yuan%20and%20Guanzheng%20Chen%20and%20Sicong%20Leng%20and%20Yuming%20Jiang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Peng%20Jin%20and%20Wenqi%20Zhang%20and%20Fan%20Wang%20and%20Lidong%20Bing%20and%20Deli%20Zhao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20VideoLLaMA3%2C%20a%20more%20advanced%20multimodal%20foundation%0Amodel%20for%20image%20and%20video%20understanding.%20The%20core%20design%20philosophy%20of%0AVideoLLaMA3%20is%20vision-centric.%20The%20meaning%20of%20%22vision-centric%22%20is%20two-fold%3A%20the%0Avision-centric%20training%20paradigm%20and%20vision-centric%20framework%20design.%20The%20key%0Ainsight%20of%20our%20vision-centric%20training%20paradigm%20is%20that%20high-quality%20image-text%0Adata%20is%20crucial%20for%20both%20image%20and%20video%20understanding.%20Instead%20of%20preparing%0Amassive%20video-text%20datasets%2C%20we%20focus%20on%20constructing%20large-scale%20and%0Ahigh-quality%20image-text%20datasets.%20VideoLLaMA3%20has%20four%20training%20stages%3A%201%29%0AVision%20Encoder%20Adaptation%2C%20which%20enables%20vision%20encoder%20to%20accept%20images%20of%0Avariable%20resolutions%20as%20input%3B%202%29%20Vision-Language%20Alignment%2C%20which%20jointly%0Atunes%20the%20vision%20encoder%2C%20projector%2C%20and%20LLM%20with%20large-scale%20image-text%20data%0Acovering%20multiple%20types%20%28including%20scene%20images%2C%20documents%2C%20charts%29%20as%20well%20as%0Atext-only%20data.%203%29%20Multi-task%20Fine-tuning%2C%20which%20incorporates%20image-text%20SFT%0Adata%20for%20downstream%20tasks%20and%20video-text%20data%20to%20establish%20a%20foundation%20for%0Avideo%20understanding.%204%29%20Video-centric%20Fine-tuning%2C%20which%20further%20improves%20the%0Amodel%27s%20capability%20in%20video%20understanding.%20As%20for%20the%20framework%20design%2C%20to%0Abetter%20capture%20fine-grained%20details%20in%20images%2C%20the%20pretrained%20vision%20encoder%20is%0Aadapted%20to%20encode%20images%20of%20varying%20sizes%20into%20vision%20tokens%20with%20corresponding%0Anumbers%2C%20rather%20than%20a%20fixed%20number%20of%20tokens.%20For%20video%20inputs%2C%20we%20reduce%20the%0Anumber%20of%20vision%20tokens%20according%20to%20their%20similarity%20so%20that%20the%0Arepresentation%20of%20videos%20will%20be%20more%20precise%20and%20compact.%20Benefit%20from%0Avision-centric%20designs%2C%20VideoLLaMA3%20achieves%20compelling%20performances%20in%20both%0Aimage%20and%20video%20understanding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13106v3&entry.124074799=Read"},
{"title": "Scenario Understanding of Traffic Scenes Through Large Visual Language\n  Models", "author": "Rivera Esteban and L\u00fcbberstedt Jannik and Nico Uhlemann and Markus Lienkamp", "abstract": "  Deep learning models for autonomous driving, encompassing perception,\nplanning, and control, depend on vast datasets to achieve their high\nperformance. However, their generalization often suffers due to domain-specific\ndata distributions, making an effective scene-based categorization of samples\nnecessary to improve their reliability across diverse domains. Manual\ncaptioning, though valuable, is both labor-intensive and time-consuming,\ncreating a bottleneck in the data annotation process. Large Visual Language\nModels (LVLMs) present a compelling solution by automating image analysis and\ncategorization through contextual queries, often without requiring retraining\nfor new categories. In this study, we evaluate the capabilities of LVLMs,\nincluding GPT-4 and LLaVA, to understand and classify urban traffic scenes on\nboth an in-house dataset and the BDD100K. We propose a scalable captioning\npipeline that integrates state-of-the-art models, enabling a flexible\ndeployment on new datasets. Our analysis, combining quantitative metrics with\nqualitative insights, demonstrates the effectiveness of LVLMs to understand\nurban traffic scenarios and highlights their potential as an efficient tool for\ndata-driven advancements in autonomous driving.\n", "link": "http://arxiv.org/abs/2501.17131v1", "date": "2025-01-28", "relevancy": 3.0748, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6379}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scenario%20Understanding%20of%20Traffic%20Scenes%20Through%20Large%20Visual%20Language%0A%20%20Models&body=Title%3A%20Scenario%20Understanding%20of%20Traffic%20Scenes%20Through%20Large%20Visual%20Language%0A%20%20Models%0AAuthor%3A%20Rivera%20Esteban%20and%20L%C3%BCbberstedt%20Jannik%20and%20Nico%20Uhlemann%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20Deep%20learning%20models%20for%20autonomous%20driving%2C%20encompassing%20perception%2C%0Aplanning%2C%20and%20control%2C%20depend%20on%20vast%20datasets%20to%20achieve%20their%20high%0Aperformance.%20However%2C%20their%20generalization%20often%20suffers%20due%20to%20domain-specific%0Adata%20distributions%2C%20making%20an%20effective%20scene-based%20categorization%20of%20samples%0Anecessary%20to%20improve%20their%20reliability%20across%20diverse%20domains.%20Manual%0Acaptioning%2C%20though%20valuable%2C%20is%20both%20labor-intensive%20and%20time-consuming%2C%0Acreating%20a%20bottleneck%20in%20the%20data%20annotation%20process.%20Large%20Visual%20Language%0AModels%20%28LVLMs%29%20present%20a%20compelling%20solution%20by%20automating%20image%20analysis%20and%0Acategorization%20through%20contextual%20queries%2C%20often%20without%20requiring%20retraining%0Afor%20new%20categories.%20In%20this%20study%2C%20we%20evaluate%20the%20capabilities%20of%20LVLMs%2C%0Aincluding%20GPT-4%20and%20LLaVA%2C%20to%20understand%20and%20classify%20urban%20traffic%20scenes%20on%0Aboth%20an%20in-house%20dataset%20and%20the%20BDD100K.%20We%20propose%20a%20scalable%20captioning%0Apipeline%20that%20integrates%20state-of-the-art%20models%2C%20enabling%20a%20flexible%0Adeployment%20on%20new%20datasets.%20Our%20analysis%2C%20combining%20quantitative%20metrics%20with%0Aqualitative%20insights%2C%20demonstrates%20the%20effectiveness%20of%20LVLMs%20to%20understand%0Aurban%20traffic%20scenarios%20and%20highlights%20their%20potential%20as%20an%20efficient%20tool%20for%0Adata-driven%20advancements%20in%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenario%2520Understanding%2520of%2520Traffic%2520Scenes%2520Through%2520Large%2520Visual%2520Language%250A%2520%2520Models%26entry.906535625%3DRivera%2520Esteban%2520and%2520L%25C3%25BCbberstedt%2520Jannik%2520and%2520Nico%2520Uhlemann%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520for%2520autonomous%2520driving%252C%2520encompassing%2520perception%252C%250Aplanning%252C%2520and%2520control%252C%2520depend%2520on%2520vast%2520datasets%2520to%2520achieve%2520their%2520high%250Aperformance.%2520However%252C%2520their%2520generalization%2520often%2520suffers%2520due%2520to%2520domain-specific%250Adata%2520distributions%252C%2520making%2520an%2520effective%2520scene-based%2520categorization%2520of%2520samples%250Anecessary%2520to%2520improve%2520their%2520reliability%2520across%2520diverse%2520domains.%2520Manual%250Acaptioning%252C%2520though%2520valuable%252C%2520is%2520both%2520labor-intensive%2520and%2520time-consuming%252C%250Acreating%2520a%2520bottleneck%2520in%2520the%2520data%2520annotation%2520process.%2520Large%2520Visual%2520Language%250AModels%2520%2528LVLMs%2529%2520present%2520a%2520compelling%2520solution%2520by%2520automating%2520image%2520analysis%2520and%250Acategorization%2520through%2520contextual%2520queries%252C%2520often%2520without%2520requiring%2520retraining%250Afor%2520new%2520categories.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520the%2520capabilities%2520of%2520LVLMs%252C%250Aincluding%2520GPT-4%2520and%2520LLaVA%252C%2520to%2520understand%2520and%2520classify%2520urban%2520traffic%2520scenes%2520on%250Aboth%2520an%2520in-house%2520dataset%2520and%2520the%2520BDD100K.%2520We%2520propose%2520a%2520scalable%2520captioning%250Apipeline%2520that%2520integrates%2520state-of-the-art%2520models%252C%2520enabling%2520a%2520flexible%250Adeployment%2520on%2520new%2520datasets.%2520Our%2520analysis%252C%2520combining%2520quantitative%2520metrics%2520with%250Aqualitative%2520insights%252C%2520demonstrates%2520the%2520effectiveness%2520of%2520LVLMs%2520to%2520understand%250Aurban%2520traffic%2520scenarios%2520and%2520highlights%2520their%2520potential%2520as%2520an%2520efficient%2520tool%2520for%250Adata-driven%2520advancements%2520in%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scenario%20Understanding%20of%20Traffic%20Scenes%20Through%20Large%20Visual%20Language%0A%20%20Models&entry.906535625=Rivera%20Esteban%20and%20L%C3%BCbberstedt%20Jannik%20and%20Nico%20Uhlemann%20and%20Markus%20Lienkamp&entry.1292438233=%20%20Deep%20learning%20models%20for%20autonomous%20driving%2C%20encompassing%20perception%2C%0Aplanning%2C%20and%20control%2C%20depend%20on%20vast%20datasets%20to%20achieve%20their%20high%0Aperformance.%20However%2C%20their%20generalization%20often%20suffers%20due%20to%20domain-specific%0Adata%20distributions%2C%20making%20an%20effective%20scene-based%20categorization%20of%20samples%0Anecessary%20to%20improve%20their%20reliability%20across%20diverse%20domains.%20Manual%0Acaptioning%2C%20though%20valuable%2C%20is%20both%20labor-intensive%20and%20time-consuming%2C%0Acreating%20a%20bottleneck%20in%20the%20data%20annotation%20process.%20Large%20Visual%20Language%0AModels%20%28LVLMs%29%20present%20a%20compelling%20solution%20by%20automating%20image%20analysis%20and%0Acategorization%20through%20contextual%20queries%2C%20often%20without%20requiring%20retraining%0Afor%20new%20categories.%20In%20this%20study%2C%20we%20evaluate%20the%20capabilities%20of%20LVLMs%2C%0Aincluding%20GPT-4%20and%20LLaVA%2C%20to%20understand%20and%20classify%20urban%20traffic%20scenes%20on%0Aboth%20an%20in-house%20dataset%20and%20the%20BDD100K.%20We%20propose%20a%20scalable%20captioning%0Apipeline%20that%20integrates%20state-of-the-art%20models%2C%20enabling%20a%20flexible%0Adeployment%20on%20new%20datasets.%20Our%20analysis%2C%20combining%20quantitative%20metrics%20with%0Aqualitative%20insights%2C%20demonstrates%20the%20effectiveness%20of%20LVLMs%20to%20understand%0Aurban%20traffic%20scenarios%20and%20highlights%20their%20potential%20as%20an%20efficient%20tool%20for%0Adata-driven%20advancements%20in%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17131v1&entry.124074799=Read"},
{"title": "Image-based Geo-localization for Robotics: Are Black-box Vision-Language\n  Models there yet?", "author": "Sania Waheed and Bruno Ferrarini and Michael Milford and Sarvapali D. Ramchurn and Shoaib Ehsan", "abstract": "  The advances in Vision-Language models (VLMs) offer exciting opportunities\nfor robotic applications involving image geo-localization, the problem of\nidentifying the geo-coordinates of a place based on visual data only. Recent\nresearch works have focused on using a VLM as embeddings extractor for\ngeo-localization, however, the most sophisticated VLMs may only be available as\nblack boxes that are accessible through an API, and come with a number of\nlimitations: there is no access to training data, model features and gradients;\nretraining is not possible; the number of predictions may be limited by the\nAPI; training on model outputs is often prohibited; and queries are open-ended.\nThe utilization of a VLM as a stand-alone, zero-shot geo-localization system\nusing a single text-based prompt is largely unexplored. To bridge this gap,\nthis paper undertakes the first systematic study, to the best of our knowledge,\nto investigate the potential of some of the state-of-the-art VLMs as\nstand-alone, zero-shot geo-localization systems in a black-box setting with\nrealistic constraints. We consider three main scenarios for this thorough\ninvestigation: a) fixed text-based prompt; b) semantically-equivalent\ntext-based prompts; and c) semantically-equivalent query images. We also take\ninto account the auto-regressive and probabilistic generation process of the\nVLMs when investigating their utility for geo-localization task by using model\nconsistency as a metric in addition to traditional accuracy. Our work provides\nnew insights in the capabilities of different VLMs for the above-mentioned\nscenarios.\n", "link": "http://arxiv.org/abs/2501.16947v1", "date": "2025-01-28", "relevancy": 2.9709, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-based%20Geo-localization%20for%20Robotics%3A%20Are%20Black-box%20Vision-Language%0A%20%20Models%20there%20yet%3F&body=Title%3A%20Image-based%20Geo-localization%20for%20Robotics%3A%20Are%20Black-box%20Vision-Language%0A%20%20Models%20there%20yet%3F%0AAuthor%3A%20Sania%20Waheed%20and%20Bruno%20Ferrarini%20and%20Michael%20Milford%20and%20Sarvapali%20D.%20Ramchurn%20and%20Shoaib%20Ehsan%0AAbstract%3A%20%20%20The%20advances%20in%20Vision-Language%20models%20%28VLMs%29%20offer%20exciting%20opportunities%0Afor%20robotic%20applications%20involving%20image%20geo-localization%2C%20the%20problem%20of%0Aidentifying%20the%20geo-coordinates%20of%20a%20place%20based%20on%20visual%20data%20only.%20Recent%0Aresearch%20works%20have%20focused%20on%20using%20a%20VLM%20as%20embeddings%20extractor%20for%0Ageo-localization%2C%20however%2C%20the%20most%20sophisticated%20VLMs%20may%20only%20be%20available%20as%0Ablack%20boxes%20that%20are%20accessible%20through%20an%20API%2C%20and%20come%20with%20a%20number%20of%0Alimitations%3A%20there%20is%20no%20access%20to%20training%20data%2C%20model%20features%20and%20gradients%3B%0Aretraining%20is%20not%20possible%3B%20the%20number%20of%20predictions%20may%20be%20limited%20by%20the%0AAPI%3B%20training%20on%20model%20outputs%20is%20often%20prohibited%3B%20and%20queries%20are%20open-ended.%0AThe%20utilization%20of%20a%20VLM%20as%20a%20stand-alone%2C%20zero-shot%20geo-localization%20system%0Ausing%20a%20single%20text-based%20prompt%20is%20largely%20unexplored.%20To%20bridge%20this%20gap%2C%0Athis%20paper%20undertakes%20the%20first%20systematic%20study%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Ato%20investigate%20the%20potential%20of%20some%20of%20the%20state-of-the-art%20VLMs%20as%0Astand-alone%2C%20zero-shot%20geo-localization%20systems%20in%20a%20black-box%20setting%20with%0Arealistic%20constraints.%20We%20consider%20three%20main%20scenarios%20for%20this%20thorough%0Ainvestigation%3A%20a%29%20fixed%20text-based%20prompt%3B%20b%29%20semantically-equivalent%0Atext-based%20prompts%3B%20and%20c%29%20semantically-equivalent%20query%20images.%20We%20also%20take%0Ainto%20account%20the%20auto-regressive%20and%20probabilistic%20generation%20process%20of%20the%0AVLMs%20when%20investigating%20their%20utility%20for%20geo-localization%20task%20by%20using%20model%0Aconsistency%20as%20a%20metric%20in%20addition%20to%20traditional%20accuracy.%20Our%20work%20provides%0Anew%20insights%20in%20the%20capabilities%20of%20different%20VLMs%20for%20the%20above-mentioned%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-based%2520Geo-localization%2520for%2520Robotics%253A%2520Are%2520Black-box%2520Vision-Language%250A%2520%2520Models%2520there%2520yet%253F%26entry.906535625%3DSania%2520Waheed%2520and%2520Bruno%2520Ferrarini%2520and%2520Michael%2520Milford%2520and%2520Sarvapali%2520D.%2520Ramchurn%2520and%2520Shoaib%2520Ehsan%26entry.1292438233%3D%2520%2520The%2520advances%2520in%2520Vision-Language%2520models%2520%2528VLMs%2529%2520offer%2520exciting%2520opportunities%250Afor%2520robotic%2520applications%2520involving%2520image%2520geo-localization%252C%2520the%2520problem%2520of%250Aidentifying%2520the%2520geo-coordinates%2520of%2520a%2520place%2520based%2520on%2520visual%2520data%2520only.%2520Recent%250Aresearch%2520works%2520have%2520focused%2520on%2520using%2520a%2520VLM%2520as%2520embeddings%2520extractor%2520for%250Ageo-localization%252C%2520however%252C%2520the%2520most%2520sophisticated%2520VLMs%2520may%2520only%2520be%2520available%2520as%250Ablack%2520boxes%2520that%2520are%2520accessible%2520through%2520an%2520API%252C%2520and%2520come%2520with%2520a%2520number%2520of%250Alimitations%253A%2520there%2520is%2520no%2520access%2520to%2520training%2520data%252C%2520model%2520features%2520and%2520gradients%253B%250Aretraining%2520is%2520not%2520possible%253B%2520the%2520number%2520of%2520predictions%2520may%2520be%2520limited%2520by%2520the%250AAPI%253B%2520training%2520on%2520model%2520outputs%2520is%2520often%2520prohibited%253B%2520and%2520queries%2520are%2520open-ended.%250AThe%2520utilization%2520of%2520a%2520VLM%2520as%2520a%2520stand-alone%252C%2520zero-shot%2520geo-localization%2520system%250Ausing%2520a%2520single%2520text-based%2520prompt%2520is%2520largely%2520unexplored.%2520To%2520bridge%2520this%2520gap%252C%250Athis%2520paper%2520undertakes%2520the%2520first%2520systematic%2520study%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%250Ato%2520investigate%2520the%2520potential%2520of%2520some%2520of%2520the%2520state-of-the-art%2520VLMs%2520as%250Astand-alone%252C%2520zero-shot%2520geo-localization%2520systems%2520in%2520a%2520black-box%2520setting%2520with%250Arealistic%2520constraints.%2520We%2520consider%2520three%2520main%2520scenarios%2520for%2520this%2520thorough%250Ainvestigation%253A%2520a%2529%2520fixed%2520text-based%2520prompt%253B%2520b%2529%2520semantically-equivalent%250Atext-based%2520prompts%253B%2520and%2520c%2529%2520semantically-equivalent%2520query%2520images.%2520We%2520also%2520take%250Ainto%2520account%2520the%2520auto-regressive%2520and%2520probabilistic%2520generation%2520process%2520of%2520the%250AVLMs%2520when%2520investigating%2520their%2520utility%2520for%2520geo-localization%2520task%2520by%2520using%2520model%250Aconsistency%2520as%2520a%2520metric%2520in%2520addition%2520to%2520traditional%2520accuracy.%2520Our%2520work%2520provides%250Anew%2520insights%2520in%2520the%2520capabilities%2520of%2520different%2520VLMs%2520for%2520the%2520above-mentioned%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-based%20Geo-localization%20for%20Robotics%3A%20Are%20Black-box%20Vision-Language%0A%20%20Models%20there%20yet%3F&entry.906535625=Sania%20Waheed%20and%20Bruno%20Ferrarini%20and%20Michael%20Milford%20and%20Sarvapali%20D.%20Ramchurn%20and%20Shoaib%20Ehsan&entry.1292438233=%20%20The%20advances%20in%20Vision-Language%20models%20%28VLMs%29%20offer%20exciting%20opportunities%0Afor%20robotic%20applications%20involving%20image%20geo-localization%2C%20the%20problem%20of%0Aidentifying%20the%20geo-coordinates%20of%20a%20place%20based%20on%20visual%20data%20only.%20Recent%0Aresearch%20works%20have%20focused%20on%20using%20a%20VLM%20as%20embeddings%20extractor%20for%0Ageo-localization%2C%20however%2C%20the%20most%20sophisticated%20VLMs%20may%20only%20be%20available%20as%0Ablack%20boxes%20that%20are%20accessible%20through%20an%20API%2C%20and%20come%20with%20a%20number%20of%0Alimitations%3A%20there%20is%20no%20access%20to%20training%20data%2C%20model%20features%20and%20gradients%3B%0Aretraining%20is%20not%20possible%3B%20the%20number%20of%20predictions%20may%20be%20limited%20by%20the%0AAPI%3B%20training%20on%20model%20outputs%20is%20often%20prohibited%3B%20and%20queries%20are%20open-ended.%0AThe%20utilization%20of%20a%20VLM%20as%20a%20stand-alone%2C%20zero-shot%20geo-localization%20system%0Ausing%20a%20single%20text-based%20prompt%20is%20largely%20unexplored.%20To%20bridge%20this%20gap%2C%0Athis%20paper%20undertakes%20the%20first%20systematic%20study%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Ato%20investigate%20the%20potential%20of%20some%20of%20the%20state-of-the-art%20VLMs%20as%0Astand-alone%2C%20zero-shot%20geo-localization%20systems%20in%20a%20black-box%20setting%20with%0Arealistic%20constraints.%20We%20consider%20three%20main%20scenarios%20for%20this%20thorough%0Ainvestigation%3A%20a%29%20fixed%20text-based%20prompt%3B%20b%29%20semantically-equivalent%0Atext-based%20prompts%3B%20and%20c%29%20semantically-equivalent%20query%20images.%20We%20also%20take%0Ainto%20account%20the%20auto-regressive%20and%20probabilistic%20generation%20process%20of%20the%0AVLMs%20when%20investigating%20their%20utility%20for%20geo-localization%20task%20by%20using%20model%0Aconsistency%20as%20a%20metric%20in%20addition%20to%20traditional%20accuracy.%20Our%20work%20provides%0Anew%20insights%20in%20the%20capabilities%20of%20different%20VLMs%20for%20the%20above-mentioned%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16947v1&entry.124074799=Read"},
{"title": "What Really Matters for Learning-based LiDAR-Camera Calibration", "author": "Shujuan Huang and Chunyu Lin and Yao Zhao", "abstract": "  Calibration is an essential prerequisite for the accurate data fusion of\nLiDAR and camera sensors. Traditional calibration techniques often require\nspecific targets or suitable scenes to obtain reliable 2D-3D correspondences.\nTo tackle the challenge of target-less and online calibration, deep neural\nnetworks have been introduced to solve the problem in a data-driven manner.\nWhile previous learning-based methods have achieved impressive performance on\nspecific datasets, they still struggle in complex real-world scenarios. Most\nexisting works focus on improving calibration accuracy but overlook the\nunderlying mechanisms. In this paper, we revisit the development of\nlearning-based LiDAR-Camera calibration and encourage the community to pay more\nattention to the underlying principles to advance practical applications. We\nsystematically analyze the paradigm of mainstream learning-based methods, and\nidentify the critical limitations of regression-based methods with the widely\nused data generation pipeline. Our findings reveal that most learning-based\nmethods inadvertently operate as retrieval networks, focusing more on\nsingle-modality distributions rather than cross-modality correspondences. We\nalso investigate how the input data format and preprocessing operations impact\nnetwork performance and summarize the regression clues to inform further\nimprovements.\n", "link": "http://arxiv.org/abs/2501.16969v1", "date": "2025-01-28", "relevancy": 2.9436, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6036}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5817}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Really%20Matters%20for%20Learning-based%20LiDAR-Camera%20Calibration&body=Title%3A%20What%20Really%20Matters%20for%20Learning-based%20LiDAR-Camera%20Calibration%0AAuthor%3A%20Shujuan%20Huang%20and%20Chunyu%20Lin%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Calibration%20is%20an%20essential%20prerequisite%20for%20the%20accurate%20data%20fusion%20of%0ALiDAR%20and%20camera%20sensors.%20Traditional%20calibration%20techniques%20often%20require%0Aspecific%20targets%20or%20suitable%20scenes%20to%20obtain%20reliable%202D-3D%20correspondences.%0ATo%20tackle%20the%20challenge%20of%20target-less%20and%20online%20calibration%2C%20deep%20neural%0Anetworks%20have%20been%20introduced%20to%20solve%20the%20problem%20in%20a%20data-driven%20manner.%0AWhile%20previous%20learning-based%20methods%20have%20achieved%20impressive%20performance%20on%0Aspecific%20datasets%2C%20they%20still%20struggle%20in%20complex%20real-world%20scenarios.%20Most%0Aexisting%20works%20focus%20on%20improving%20calibration%20accuracy%20but%20overlook%20the%0Aunderlying%20mechanisms.%20In%20this%20paper%2C%20we%20revisit%20the%20development%20of%0Alearning-based%20LiDAR-Camera%20calibration%20and%20encourage%20the%20community%20to%20pay%20more%0Aattention%20to%20the%20underlying%20principles%20to%20advance%20practical%20applications.%20We%0Asystematically%20analyze%20the%20paradigm%20of%20mainstream%20learning-based%20methods%2C%20and%0Aidentify%20the%20critical%20limitations%20of%20regression-based%20methods%20with%20the%20widely%0Aused%20data%20generation%20pipeline.%20Our%20findings%20reveal%20that%20most%20learning-based%0Amethods%20inadvertently%20operate%20as%20retrieval%20networks%2C%20focusing%20more%20on%0Asingle-modality%20distributions%20rather%20than%20cross-modality%20correspondences.%20We%0Aalso%20investigate%20how%20the%20input%20data%20format%20and%20preprocessing%20operations%20impact%0Anetwork%20performance%20and%20summarize%20the%20regression%20clues%20to%20inform%20further%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Really%2520Matters%2520for%2520Learning-based%2520LiDAR-Camera%2520Calibration%26entry.906535625%3DShujuan%2520Huang%2520and%2520Chunyu%2520Lin%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Calibration%2520is%2520an%2520essential%2520prerequisite%2520for%2520the%2520accurate%2520data%2520fusion%2520of%250ALiDAR%2520and%2520camera%2520sensors.%2520Traditional%2520calibration%2520techniques%2520often%2520require%250Aspecific%2520targets%2520or%2520suitable%2520scenes%2520to%2520obtain%2520reliable%25202D-3D%2520correspondences.%250ATo%2520tackle%2520the%2520challenge%2520of%2520target-less%2520and%2520online%2520calibration%252C%2520deep%2520neural%250Anetworks%2520have%2520been%2520introduced%2520to%2520solve%2520the%2520problem%2520in%2520a%2520data-driven%2520manner.%250AWhile%2520previous%2520learning-based%2520methods%2520have%2520achieved%2520impressive%2520performance%2520on%250Aspecific%2520datasets%252C%2520they%2520still%2520struggle%2520in%2520complex%2520real-world%2520scenarios.%2520Most%250Aexisting%2520works%2520focus%2520on%2520improving%2520calibration%2520accuracy%2520but%2520overlook%2520the%250Aunderlying%2520mechanisms.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520the%2520development%2520of%250Alearning-based%2520LiDAR-Camera%2520calibration%2520and%2520encourage%2520the%2520community%2520to%2520pay%2520more%250Aattention%2520to%2520the%2520underlying%2520principles%2520to%2520advance%2520practical%2520applications.%2520We%250Asystematically%2520analyze%2520the%2520paradigm%2520of%2520mainstream%2520learning-based%2520methods%252C%2520and%250Aidentify%2520the%2520critical%2520limitations%2520of%2520regression-based%2520methods%2520with%2520the%2520widely%250Aused%2520data%2520generation%2520pipeline.%2520Our%2520findings%2520reveal%2520that%2520most%2520learning-based%250Amethods%2520inadvertently%2520operate%2520as%2520retrieval%2520networks%252C%2520focusing%2520more%2520on%250Asingle-modality%2520distributions%2520rather%2520than%2520cross-modality%2520correspondences.%2520We%250Aalso%2520investigate%2520how%2520the%2520input%2520data%2520format%2520and%2520preprocessing%2520operations%2520impact%250Anetwork%2520performance%2520and%2520summarize%2520the%2520regression%2520clues%2520to%2520inform%2520further%250Aimprovements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Really%20Matters%20for%20Learning-based%20LiDAR-Camera%20Calibration&entry.906535625=Shujuan%20Huang%20and%20Chunyu%20Lin%20and%20Yao%20Zhao&entry.1292438233=%20%20Calibration%20is%20an%20essential%20prerequisite%20for%20the%20accurate%20data%20fusion%20of%0ALiDAR%20and%20camera%20sensors.%20Traditional%20calibration%20techniques%20often%20require%0Aspecific%20targets%20or%20suitable%20scenes%20to%20obtain%20reliable%202D-3D%20correspondences.%0ATo%20tackle%20the%20challenge%20of%20target-less%20and%20online%20calibration%2C%20deep%20neural%0Anetworks%20have%20been%20introduced%20to%20solve%20the%20problem%20in%20a%20data-driven%20manner.%0AWhile%20previous%20learning-based%20methods%20have%20achieved%20impressive%20performance%20on%0Aspecific%20datasets%2C%20they%20still%20struggle%20in%20complex%20real-world%20scenarios.%20Most%0Aexisting%20works%20focus%20on%20improving%20calibration%20accuracy%20but%20overlook%20the%0Aunderlying%20mechanisms.%20In%20this%20paper%2C%20we%20revisit%20the%20development%20of%0Alearning-based%20LiDAR-Camera%20calibration%20and%20encourage%20the%20community%20to%20pay%20more%0Aattention%20to%20the%20underlying%20principles%20to%20advance%20practical%20applications.%20We%0Asystematically%20analyze%20the%20paradigm%20of%20mainstream%20learning-based%20methods%2C%20and%0Aidentify%20the%20critical%20limitations%20of%20regression-based%20methods%20with%20the%20widely%0Aused%20data%20generation%20pipeline.%20Our%20findings%20reveal%20that%20most%20learning-based%0Amethods%20inadvertently%20operate%20as%20retrieval%20networks%2C%20focusing%20more%20on%0Asingle-modality%20distributions%20rather%20than%20cross-modality%20correspondences.%20We%0Aalso%20investigate%20how%20the%20input%20data%20format%20and%20preprocessing%20operations%20impact%0Anetwork%20performance%20and%20summarize%20the%20regression%20clues%20to%20inform%20further%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16969v1&entry.124074799=Read"},
{"title": "Acquiring Submillimeter-Accurate Multi-Task Vision Datasets for\n  Computer-Assisted Orthopedic Surgery", "author": "Emma Most and Jonas Hein and Fr\u00e9d\u00e9ric Giraud and Nicola A. Cavalcanti and Lukas Zingg and Baptiste Brument and Nino Louman and Fabio Carrillo and Philipp F\u00fcrnstahl and Lilian Calvet", "abstract": "  Advances in computer vision, particularly in optical image-based 3D\nreconstruction and feature matching, enable applications like marker-less\nsurgical navigation and digitization of surgery. However, their development is\nhindered by a lack of suitable datasets with 3D ground truth. This work\nexplores an approach to generating realistic and accurate ex vivo datasets\ntailored for 3D reconstruction and feature matching in open orthopedic surgery.\nA set of posed images and an accurately registered ground truth surface mesh of\nthe scene are required to develop vision-based 3D reconstruction and matching\nmethods suitable for surgery. We propose a framework consisting of three core\nsteps and compare different methods for each step: 3D scanning, calibration of\nviewpoints for a set of high-resolution RGB images, and an optical-based method\nfor scene registration. We evaluate each step of this framework on an ex vivo\nscoliosis surgery using a pig spine, conducted under real operating room\nconditions. A mean 3D Euclidean error of 0.35 mm is achieved with respect to\nthe 3D ground truth. The proposed method results in submillimeter accurate 3D\nground truths and surgical images with a spatial resolution of 0.1 mm. This\nopens the door to acquiring future surgical datasets for high-precision\napplications.\n", "link": "http://arxiv.org/abs/2501.15371v2", "date": "2025-01-28", "relevancy": 2.9254, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5893}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5893}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Acquiring%20Submillimeter-Accurate%20Multi-Task%20Vision%20Datasets%20for%0A%20%20Computer-Assisted%20Orthopedic%20Surgery&body=Title%3A%20Acquiring%20Submillimeter-Accurate%20Multi-Task%20Vision%20Datasets%20for%0A%20%20Computer-Assisted%20Orthopedic%20Surgery%0AAuthor%3A%20Emma%20Most%20and%20Jonas%20Hein%20and%20Fr%C3%A9d%C3%A9ric%20Giraud%20and%20Nicola%20A.%20Cavalcanti%20and%20Lukas%20Zingg%20and%20Baptiste%20Brument%20and%20Nino%20Louman%20and%20Fabio%20Carrillo%20and%20Philipp%20F%C3%BCrnstahl%20and%20Lilian%20Calvet%0AAbstract%3A%20%20%20Advances%20in%20computer%20vision%2C%20particularly%20in%20optical%20image-based%203D%0Areconstruction%20and%20feature%20matching%2C%20enable%20applications%20like%20marker-less%0Asurgical%20navigation%20and%20digitization%20of%20surgery.%20However%2C%20their%20development%20is%0Ahindered%20by%20a%20lack%20of%20suitable%20datasets%20with%203D%20ground%20truth.%20This%20work%0Aexplores%20an%20approach%20to%20generating%20realistic%20and%20accurate%20ex%20vivo%20datasets%0Atailored%20for%203D%20reconstruction%20and%20feature%20matching%20in%20open%20orthopedic%20surgery.%0AA%20set%20of%20posed%20images%20and%20an%20accurately%20registered%20ground%20truth%20surface%20mesh%20of%0Athe%20scene%20are%20required%20to%20develop%20vision-based%203D%20reconstruction%20and%20matching%0Amethods%20suitable%20for%20surgery.%20We%20propose%20a%20framework%20consisting%20of%20three%20core%0Asteps%20and%20compare%20different%20methods%20for%20each%20step%3A%203D%20scanning%2C%20calibration%20of%0Aviewpoints%20for%20a%20set%20of%20high-resolution%20RGB%20images%2C%20and%20an%20optical-based%20method%0Afor%20scene%20registration.%20We%20evaluate%20each%20step%20of%20this%20framework%20on%20an%20ex%20vivo%0Ascoliosis%20surgery%20using%20a%20pig%20spine%2C%20conducted%20under%20real%20operating%20room%0Aconditions.%20A%20mean%203D%20Euclidean%20error%20of%200.35%20mm%20is%20achieved%20with%20respect%20to%0Athe%203D%20ground%20truth.%20The%20proposed%20method%20results%20in%20submillimeter%20accurate%203D%0Aground%20truths%20and%20surgical%20images%20with%20a%20spatial%20resolution%20of%200.1%20mm.%20This%0Aopens%20the%20door%20to%20acquiring%20future%20surgical%20datasets%20for%20high-precision%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAcquiring%2520Submillimeter-Accurate%2520Multi-Task%2520Vision%2520Datasets%2520for%250A%2520%2520Computer-Assisted%2520Orthopedic%2520Surgery%26entry.906535625%3DEmma%2520Most%2520and%2520Jonas%2520Hein%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Giraud%2520and%2520Nicola%2520A.%2520Cavalcanti%2520and%2520Lukas%2520Zingg%2520and%2520Baptiste%2520Brument%2520and%2520Nino%2520Louman%2520and%2520Fabio%2520Carrillo%2520and%2520Philipp%2520F%25C3%25BCrnstahl%2520and%2520Lilian%2520Calvet%26entry.1292438233%3D%2520%2520Advances%2520in%2520computer%2520vision%252C%2520particularly%2520in%2520optical%2520image-based%25203D%250Areconstruction%2520and%2520feature%2520matching%252C%2520enable%2520applications%2520like%2520marker-less%250Asurgical%2520navigation%2520and%2520digitization%2520of%2520surgery.%2520However%252C%2520their%2520development%2520is%250Ahindered%2520by%2520a%2520lack%2520of%2520suitable%2520datasets%2520with%25203D%2520ground%2520truth.%2520This%2520work%250Aexplores%2520an%2520approach%2520to%2520generating%2520realistic%2520and%2520accurate%2520ex%2520vivo%2520datasets%250Atailored%2520for%25203D%2520reconstruction%2520and%2520feature%2520matching%2520in%2520open%2520orthopedic%2520surgery.%250AA%2520set%2520of%2520posed%2520images%2520and%2520an%2520accurately%2520registered%2520ground%2520truth%2520surface%2520mesh%2520of%250Athe%2520scene%2520are%2520required%2520to%2520develop%2520vision-based%25203D%2520reconstruction%2520and%2520matching%250Amethods%2520suitable%2520for%2520surgery.%2520We%2520propose%2520a%2520framework%2520consisting%2520of%2520three%2520core%250Asteps%2520and%2520compare%2520different%2520methods%2520for%2520each%2520step%253A%25203D%2520scanning%252C%2520calibration%2520of%250Aviewpoints%2520for%2520a%2520set%2520of%2520high-resolution%2520RGB%2520images%252C%2520and%2520an%2520optical-based%2520method%250Afor%2520scene%2520registration.%2520We%2520evaluate%2520each%2520step%2520of%2520this%2520framework%2520on%2520an%2520ex%2520vivo%250Ascoliosis%2520surgery%2520using%2520a%2520pig%2520spine%252C%2520conducted%2520under%2520real%2520operating%2520room%250Aconditions.%2520A%2520mean%25203D%2520Euclidean%2520error%2520of%25200.35%2520mm%2520is%2520achieved%2520with%2520respect%2520to%250Athe%25203D%2520ground%2520truth.%2520The%2520proposed%2520method%2520results%2520in%2520submillimeter%2520accurate%25203D%250Aground%2520truths%2520and%2520surgical%2520images%2520with%2520a%2520spatial%2520resolution%2520of%25200.1%2520mm.%2520This%250Aopens%2520the%2520door%2520to%2520acquiring%2520future%2520surgical%2520datasets%2520for%2520high-precision%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Acquiring%20Submillimeter-Accurate%20Multi-Task%20Vision%20Datasets%20for%0A%20%20Computer-Assisted%20Orthopedic%20Surgery&entry.906535625=Emma%20Most%20and%20Jonas%20Hein%20and%20Fr%C3%A9d%C3%A9ric%20Giraud%20and%20Nicola%20A.%20Cavalcanti%20and%20Lukas%20Zingg%20and%20Baptiste%20Brument%20and%20Nino%20Louman%20and%20Fabio%20Carrillo%20and%20Philipp%20F%C3%BCrnstahl%20and%20Lilian%20Calvet&entry.1292438233=%20%20Advances%20in%20computer%20vision%2C%20particularly%20in%20optical%20image-based%203D%0Areconstruction%20and%20feature%20matching%2C%20enable%20applications%20like%20marker-less%0Asurgical%20navigation%20and%20digitization%20of%20surgery.%20However%2C%20their%20development%20is%0Ahindered%20by%20a%20lack%20of%20suitable%20datasets%20with%203D%20ground%20truth.%20This%20work%0Aexplores%20an%20approach%20to%20generating%20realistic%20and%20accurate%20ex%20vivo%20datasets%0Atailored%20for%203D%20reconstruction%20and%20feature%20matching%20in%20open%20orthopedic%20surgery.%0AA%20set%20of%20posed%20images%20and%20an%20accurately%20registered%20ground%20truth%20surface%20mesh%20of%0Athe%20scene%20are%20required%20to%20develop%20vision-based%203D%20reconstruction%20and%20matching%0Amethods%20suitable%20for%20surgery.%20We%20propose%20a%20framework%20consisting%20of%20three%20core%0Asteps%20and%20compare%20different%20methods%20for%20each%20step%3A%203D%20scanning%2C%20calibration%20of%0Aviewpoints%20for%20a%20set%20of%20high-resolution%20RGB%20images%2C%20and%20an%20optical-based%20method%0Afor%20scene%20registration.%20We%20evaluate%20each%20step%20of%20this%20framework%20on%20an%20ex%20vivo%0Ascoliosis%20surgery%20using%20a%20pig%20spine%2C%20conducted%20under%20real%20operating%20room%0Aconditions.%20A%20mean%203D%20Euclidean%20error%20of%200.35%20mm%20is%20achieved%20with%20respect%20to%0Athe%203D%20ground%20truth.%20The%20proposed%20method%20results%20in%20submillimeter%20accurate%203D%0Aground%20truths%20and%20surgical%20images%20with%20a%20spatial%20resolution%20of%200.1%20mm.%20This%0Aopens%20the%20door%20to%20acquiring%20future%20surgical%20datasets%20for%20high-precision%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15371v2&entry.124074799=Read"},
{"title": "DINOSTAR: Deep Iterative Neural Object Detector Self-Supervised Training\n  for Roadside LiDAR Applications", "author": "Muhammad Shahbaz and Shaurya Agarwal", "abstract": "  Recent advancements in deep-learning methods for object detection in\npoint-cloud data have enabled numerous roadside applications, fostering\nimprovements in transportation safety and management. However, the intricate\nnature of point-cloud data poses significant challenges for human-supervised\nlabeling, resulting in substantial expenditures of time and capital. This paper\naddresses the issue by developing an end-to-end, scalable, and self-supervised\nframework for training deep object detectors tailored for roadside point-cloud\ndata. The proposed framework leverages self-supervised, statistically modeled\nteachers to train off-the-shelf deep object detectors, thus circumventing the\nneed for human supervision. The teacher models follow fine-tuned set standard\npractices of background filtering, object clustering, bounding-box fitting, and\nclassification to generate noisy labels. It is presented that by training the\nstudent model over the combined noisy annotations from multitude of teachers\nenhances its capacity to discern background/foreground more effectively and\nforces it to learn diverse point-cloud-representations for object categories of\ninterest. The evaluations, involving publicly available roadside datasets and\nstate-of-art deep object detectors, demonstrate that the proposed framework\nachieves comparable performance to deep object detectors trained on\nhuman-annotated labels, despite not utilizing such human-annotations in its\ntraining process.\n", "link": "http://arxiv.org/abs/2501.17076v1", "date": "2025-01-28", "relevancy": 2.8877, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5909}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINOSTAR%3A%20Deep%20Iterative%20Neural%20Object%20Detector%20Self-Supervised%20Training%0A%20%20for%20Roadside%20LiDAR%20Applications&body=Title%3A%20DINOSTAR%3A%20Deep%20Iterative%20Neural%20Object%20Detector%20Self-Supervised%20Training%0A%20%20for%20Roadside%20LiDAR%20Applications%0AAuthor%3A%20Muhammad%20Shahbaz%20and%20Shaurya%20Agarwal%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep-learning%20methods%20for%20object%20detection%20in%0Apoint-cloud%20data%20have%20enabled%20numerous%20roadside%20applications%2C%20fostering%0Aimprovements%20in%20transportation%20safety%20and%20management.%20However%2C%20the%20intricate%0Anature%20of%20point-cloud%20data%20poses%20significant%20challenges%20for%20human-supervised%0Alabeling%2C%20resulting%20in%20substantial%20expenditures%20of%20time%20and%20capital.%20This%20paper%0Aaddresses%20the%20issue%20by%20developing%20an%20end-to-end%2C%20scalable%2C%20and%20self-supervised%0Aframework%20for%20training%20deep%20object%20detectors%20tailored%20for%20roadside%20point-cloud%0Adata.%20The%20proposed%20framework%20leverages%20self-supervised%2C%20statistically%20modeled%0Ateachers%20to%20train%20off-the-shelf%20deep%20object%20detectors%2C%20thus%20circumventing%20the%0Aneed%20for%20human%20supervision.%20The%20teacher%20models%20follow%20fine-tuned%20set%20standard%0Apractices%20of%20background%20filtering%2C%20object%20clustering%2C%20bounding-box%20fitting%2C%20and%0Aclassification%20to%20generate%20noisy%20labels.%20It%20is%20presented%20that%20by%20training%20the%0Astudent%20model%20over%20the%20combined%20noisy%20annotations%20from%20multitude%20of%20teachers%0Aenhances%20its%20capacity%20to%20discern%20background/foreground%20more%20effectively%20and%0Aforces%20it%20to%20learn%20diverse%20point-cloud-representations%20for%20object%20categories%20of%0Ainterest.%20The%20evaluations%2C%20involving%20publicly%20available%20roadside%20datasets%20and%0Astate-of-art%20deep%20object%20detectors%2C%20demonstrate%20that%20the%20proposed%20framework%0Aachieves%20comparable%20performance%20to%20deep%20object%20detectors%20trained%20on%0Ahuman-annotated%20labels%2C%20despite%20not%20utilizing%20such%20human-annotations%20in%20its%0Atraining%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINOSTAR%253A%2520Deep%2520Iterative%2520Neural%2520Object%2520Detector%2520Self-Supervised%2520Training%250A%2520%2520for%2520Roadside%2520LiDAR%2520Applications%26entry.906535625%3DMuhammad%2520Shahbaz%2520and%2520Shaurya%2520Agarwal%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep-learning%2520methods%2520for%2520object%2520detection%2520in%250Apoint-cloud%2520data%2520have%2520enabled%2520numerous%2520roadside%2520applications%252C%2520fostering%250Aimprovements%2520in%2520transportation%2520safety%2520and%2520management.%2520However%252C%2520the%2520intricate%250Anature%2520of%2520point-cloud%2520data%2520poses%2520significant%2520challenges%2520for%2520human-supervised%250Alabeling%252C%2520resulting%2520in%2520substantial%2520expenditures%2520of%2520time%2520and%2520capital.%2520This%2520paper%250Aaddresses%2520the%2520issue%2520by%2520developing%2520an%2520end-to-end%252C%2520scalable%252C%2520and%2520self-supervised%250Aframework%2520for%2520training%2520deep%2520object%2520detectors%2520tailored%2520for%2520roadside%2520point-cloud%250Adata.%2520The%2520proposed%2520framework%2520leverages%2520self-supervised%252C%2520statistically%2520modeled%250Ateachers%2520to%2520train%2520off-the-shelf%2520deep%2520object%2520detectors%252C%2520thus%2520circumventing%2520the%250Aneed%2520for%2520human%2520supervision.%2520The%2520teacher%2520models%2520follow%2520fine-tuned%2520set%2520standard%250Apractices%2520of%2520background%2520filtering%252C%2520object%2520clustering%252C%2520bounding-box%2520fitting%252C%2520and%250Aclassification%2520to%2520generate%2520noisy%2520labels.%2520It%2520is%2520presented%2520that%2520by%2520training%2520the%250Astudent%2520model%2520over%2520the%2520combined%2520noisy%2520annotations%2520from%2520multitude%2520of%2520teachers%250Aenhances%2520its%2520capacity%2520to%2520discern%2520background/foreground%2520more%2520effectively%2520and%250Aforces%2520it%2520to%2520learn%2520diverse%2520point-cloud-representations%2520for%2520object%2520categories%2520of%250Ainterest.%2520The%2520evaluations%252C%2520involving%2520publicly%2520available%2520roadside%2520datasets%2520and%250Astate-of-art%2520deep%2520object%2520detectors%252C%2520demonstrate%2520that%2520the%2520proposed%2520framework%250Aachieves%2520comparable%2520performance%2520to%2520deep%2520object%2520detectors%2520trained%2520on%250Ahuman-annotated%2520labels%252C%2520despite%2520not%2520utilizing%2520such%2520human-annotations%2520in%2520its%250Atraining%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINOSTAR%3A%20Deep%20Iterative%20Neural%20Object%20Detector%20Self-Supervised%20Training%0A%20%20for%20Roadside%20LiDAR%20Applications&entry.906535625=Muhammad%20Shahbaz%20and%20Shaurya%20Agarwal&entry.1292438233=%20%20Recent%20advancements%20in%20deep-learning%20methods%20for%20object%20detection%20in%0Apoint-cloud%20data%20have%20enabled%20numerous%20roadside%20applications%2C%20fostering%0Aimprovements%20in%20transportation%20safety%20and%20management.%20However%2C%20the%20intricate%0Anature%20of%20point-cloud%20data%20poses%20significant%20challenges%20for%20human-supervised%0Alabeling%2C%20resulting%20in%20substantial%20expenditures%20of%20time%20and%20capital.%20This%20paper%0Aaddresses%20the%20issue%20by%20developing%20an%20end-to-end%2C%20scalable%2C%20and%20self-supervised%0Aframework%20for%20training%20deep%20object%20detectors%20tailored%20for%20roadside%20point-cloud%0Adata.%20The%20proposed%20framework%20leverages%20self-supervised%2C%20statistically%20modeled%0Ateachers%20to%20train%20off-the-shelf%20deep%20object%20detectors%2C%20thus%20circumventing%20the%0Aneed%20for%20human%20supervision.%20The%20teacher%20models%20follow%20fine-tuned%20set%20standard%0Apractices%20of%20background%20filtering%2C%20object%20clustering%2C%20bounding-box%20fitting%2C%20and%0Aclassification%20to%20generate%20noisy%20labels.%20It%20is%20presented%20that%20by%20training%20the%0Astudent%20model%20over%20the%20combined%20noisy%20annotations%20from%20multitude%20of%20teachers%0Aenhances%20its%20capacity%20to%20discern%20background/foreground%20more%20effectively%20and%0Aforces%20it%20to%20learn%20diverse%20point-cloud-representations%20for%20object%20categories%20of%0Ainterest.%20The%20evaluations%2C%20involving%20publicly%20available%20roadside%20datasets%20and%0Astate-of-art%20deep%20object%20detectors%2C%20demonstrate%20that%20the%20proposed%20framework%0Aachieves%20comparable%20performance%20to%20deep%20object%20detectors%20trained%20on%0Ahuman-annotated%20labels%2C%20despite%20not%20utilizing%20such%20human-annotations%20in%20its%0Atraining%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17076v1&entry.124074799=Read"},
{"title": "Modulating CNN Features with Pre-Trained ViT Representations for\n  Open-Vocabulary Object Detection", "author": "Xiangyu Gao and Yu Dai and Benliu Qiu and Hongliang Li", "abstract": "  Owing to large-scale image-text contrastive training, pre-trained vision\nlanguage model (VLM) like CLIP shows superior open-vocabulary recognition\nability. Most existing open-vocabulary object detectors attempt to utilize the\npre-trained VLM to attain generative representation. F-ViT uses the pre-trained\nvisual encoder as the backbone network and freezes it during training. However,\nthe frozen backbone doesn't benefit from the labeled data to strengthen the\nrepresentation. Therefore, we propose a novel two-branch backbone network\ndesign, named as ViT-Feature-Modulated Multi-Scale Convolutional network\n(VMCNet). VMCNet consists of a trainable convolutional branch, a frozen\npre-trained ViT branch and a feature modulation module. The trainable CNN\nbranch could be optimized with labeled data while the frozen pre-trained ViT\nbranch could keep the representation ability derived from large-scale\npre-training. Then, the proposed feature modulation module could modulate the\nmulti-scale CNN features with the representations from ViT branch. With the\nproposed mixed structure, detector is more likely to discover novel categories.\nEvaluated on two popular benchmarks, our method boosts the detection\nperformance on novel category and outperforms the baseline. On OV-COCO, the\nproposed method achieves 44.3 AP$_{50}^{\\mathrm{novel}}$ with ViT-B/16 and 48.5\nAP$_{50}^{\\mathrm{novel}}$ with ViT-L/14. On OV-LVIS, VMCNet with ViT-B/16 and\nViT-L/14 reaches 27.8 and 38.4 mAP$_{r}$.\n", "link": "http://arxiv.org/abs/2501.16981v1", "date": "2025-01-28", "relevancy": 2.8704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modulating%20CNN%20Features%20with%20Pre-Trained%20ViT%20Representations%20for%0A%20%20Open-Vocabulary%20Object%20Detection&body=Title%3A%20Modulating%20CNN%20Features%20with%20Pre-Trained%20ViT%20Representations%20for%0A%20%20Open-Vocabulary%20Object%20Detection%0AAuthor%3A%20Xiangyu%20Gao%20and%20Yu%20Dai%20and%20Benliu%20Qiu%20and%20Hongliang%20Li%0AAbstract%3A%20%20%20Owing%20to%20large-scale%20image-text%20contrastive%20training%2C%20pre-trained%20vision%0Alanguage%20model%20%28VLM%29%20like%20CLIP%20shows%20superior%20open-vocabulary%20recognition%0Aability.%20Most%20existing%20open-vocabulary%20object%20detectors%20attempt%20to%20utilize%20the%0Apre-trained%20VLM%20to%20attain%20generative%20representation.%20F-ViT%20uses%20the%20pre-trained%0Avisual%20encoder%20as%20the%20backbone%20network%20and%20freezes%20it%20during%20training.%20However%2C%0Athe%20frozen%20backbone%20doesn%27t%20benefit%20from%20the%20labeled%20data%20to%20strengthen%20the%0Arepresentation.%20Therefore%2C%20we%20propose%20a%20novel%20two-branch%20backbone%20network%0Adesign%2C%20named%20as%20ViT-Feature-Modulated%20Multi-Scale%20Convolutional%20network%0A%28VMCNet%29.%20VMCNet%20consists%20of%20a%20trainable%20convolutional%20branch%2C%20a%20frozen%0Apre-trained%20ViT%20branch%20and%20a%20feature%20modulation%20module.%20The%20trainable%20CNN%0Abranch%20could%20be%20optimized%20with%20labeled%20data%20while%20the%20frozen%20pre-trained%20ViT%0Abranch%20could%20keep%20the%20representation%20ability%20derived%20from%20large-scale%0Apre-training.%20Then%2C%20the%20proposed%20feature%20modulation%20module%20could%20modulate%20the%0Amulti-scale%20CNN%20features%20with%20the%20representations%20from%20ViT%20branch.%20With%20the%0Aproposed%20mixed%20structure%2C%20detector%20is%20more%20likely%20to%20discover%20novel%20categories.%0AEvaluated%20on%20two%20popular%20benchmarks%2C%20our%20method%20boosts%20the%20detection%0Aperformance%20on%20novel%20category%20and%20outperforms%20the%20baseline.%20On%20OV-COCO%2C%20the%0Aproposed%20method%20achieves%2044.3%20AP%24_%7B50%7D%5E%7B%5Cmathrm%7Bnovel%7D%7D%24%20with%20ViT-B/16%20and%2048.5%0AAP%24_%7B50%7D%5E%7B%5Cmathrm%7Bnovel%7D%7D%24%20with%20ViT-L/14.%20On%20OV-LVIS%2C%20VMCNet%20with%20ViT-B/16%20and%0AViT-L/14%20reaches%2027.8%20and%2038.4%20mAP%24_%7Br%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModulating%2520CNN%2520Features%2520with%2520Pre-Trained%2520ViT%2520Representations%2520for%250A%2520%2520Open-Vocabulary%2520Object%2520Detection%26entry.906535625%3DXiangyu%2520Gao%2520and%2520Yu%2520Dai%2520and%2520Benliu%2520Qiu%2520and%2520Hongliang%2520Li%26entry.1292438233%3D%2520%2520Owing%2520to%2520large-scale%2520image-text%2520contrastive%2520training%252C%2520pre-trained%2520vision%250Alanguage%2520model%2520%2528VLM%2529%2520like%2520CLIP%2520shows%2520superior%2520open-vocabulary%2520recognition%250Aability.%2520Most%2520existing%2520open-vocabulary%2520object%2520detectors%2520attempt%2520to%2520utilize%2520the%250Apre-trained%2520VLM%2520to%2520attain%2520generative%2520representation.%2520F-ViT%2520uses%2520the%2520pre-trained%250Avisual%2520encoder%2520as%2520the%2520backbone%2520network%2520and%2520freezes%2520it%2520during%2520training.%2520However%252C%250Athe%2520frozen%2520backbone%2520doesn%2527t%2520benefit%2520from%2520the%2520labeled%2520data%2520to%2520strengthen%2520the%250Arepresentation.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520two-branch%2520backbone%2520network%250Adesign%252C%2520named%2520as%2520ViT-Feature-Modulated%2520Multi-Scale%2520Convolutional%2520network%250A%2528VMCNet%2529.%2520VMCNet%2520consists%2520of%2520a%2520trainable%2520convolutional%2520branch%252C%2520a%2520frozen%250Apre-trained%2520ViT%2520branch%2520and%2520a%2520feature%2520modulation%2520module.%2520The%2520trainable%2520CNN%250Abranch%2520could%2520be%2520optimized%2520with%2520labeled%2520data%2520while%2520the%2520frozen%2520pre-trained%2520ViT%250Abranch%2520could%2520keep%2520the%2520representation%2520ability%2520derived%2520from%2520large-scale%250Apre-training.%2520Then%252C%2520the%2520proposed%2520feature%2520modulation%2520module%2520could%2520modulate%2520the%250Amulti-scale%2520CNN%2520features%2520with%2520the%2520representations%2520from%2520ViT%2520branch.%2520With%2520the%250Aproposed%2520mixed%2520structure%252C%2520detector%2520is%2520more%2520likely%2520to%2520discover%2520novel%2520categories.%250AEvaluated%2520on%2520two%2520popular%2520benchmarks%252C%2520our%2520method%2520boosts%2520the%2520detection%250Aperformance%2520on%2520novel%2520category%2520and%2520outperforms%2520the%2520baseline.%2520On%2520OV-COCO%252C%2520the%250Aproposed%2520method%2520achieves%252044.3%2520AP%2524_%257B50%257D%255E%257B%255Cmathrm%257Bnovel%257D%257D%2524%2520with%2520ViT-B/16%2520and%252048.5%250AAP%2524_%257B50%257D%255E%257B%255Cmathrm%257Bnovel%257D%257D%2524%2520with%2520ViT-L/14.%2520On%2520OV-LVIS%252C%2520VMCNet%2520with%2520ViT-B/16%2520and%250AViT-L/14%2520reaches%252027.8%2520and%252038.4%2520mAP%2524_%257Br%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modulating%20CNN%20Features%20with%20Pre-Trained%20ViT%20Representations%20for%0A%20%20Open-Vocabulary%20Object%20Detection&entry.906535625=Xiangyu%20Gao%20and%20Yu%20Dai%20and%20Benliu%20Qiu%20and%20Hongliang%20Li&entry.1292438233=%20%20Owing%20to%20large-scale%20image-text%20contrastive%20training%2C%20pre-trained%20vision%0Alanguage%20model%20%28VLM%29%20like%20CLIP%20shows%20superior%20open-vocabulary%20recognition%0Aability.%20Most%20existing%20open-vocabulary%20object%20detectors%20attempt%20to%20utilize%20the%0Apre-trained%20VLM%20to%20attain%20generative%20representation.%20F-ViT%20uses%20the%20pre-trained%0Avisual%20encoder%20as%20the%20backbone%20network%20and%20freezes%20it%20during%20training.%20However%2C%0Athe%20frozen%20backbone%20doesn%27t%20benefit%20from%20the%20labeled%20data%20to%20strengthen%20the%0Arepresentation.%20Therefore%2C%20we%20propose%20a%20novel%20two-branch%20backbone%20network%0Adesign%2C%20named%20as%20ViT-Feature-Modulated%20Multi-Scale%20Convolutional%20network%0A%28VMCNet%29.%20VMCNet%20consists%20of%20a%20trainable%20convolutional%20branch%2C%20a%20frozen%0Apre-trained%20ViT%20branch%20and%20a%20feature%20modulation%20module.%20The%20trainable%20CNN%0Abranch%20could%20be%20optimized%20with%20labeled%20data%20while%20the%20frozen%20pre-trained%20ViT%0Abranch%20could%20keep%20the%20representation%20ability%20derived%20from%20large-scale%0Apre-training.%20Then%2C%20the%20proposed%20feature%20modulation%20module%20could%20modulate%20the%0Amulti-scale%20CNN%20features%20with%20the%20representations%20from%20ViT%20branch.%20With%20the%0Aproposed%20mixed%20structure%2C%20detector%20is%20more%20likely%20to%20discover%20novel%20categories.%0AEvaluated%20on%20two%20popular%20benchmarks%2C%20our%20method%20boosts%20the%20detection%0Aperformance%20on%20novel%20category%20and%20outperforms%20the%20baseline.%20On%20OV-COCO%2C%20the%0Aproposed%20method%20achieves%2044.3%20AP%24_%7B50%7D%5E%7B%5Cmathrm%7Bnovel%7D%7D%24%20with%20ViT-B/16%20and%2048.5%0AAP%24_%7B50%7D%5E%7B%5Cmathrm%7Bnovel%7D%7D%24%20with%20ViT-L/14.%20On%20OV-LVIS%2C%20VMCNet%20with%20ViT-B/16%20and%0AViT-L/14%20reaches%2027.8%20and%2038.4%20mAP%24_%7Br%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16981v1&entry.124074799=Read"},
{"title": "Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds", "author": "Xiaohan Sun and Yinghan Xu and John Dingliana and Carol O'Sullivan", "abstract": "  Efficient and realistic crowd rendering is an important element of many\nreal-time graphics applications such as Virtual Reality (VR) and games. To this\nend, Levels of Detail (LOD) avatar representations such as polygonal meshes,\nimage-based impostors, and point clouds have been proposed and evaluated. More\nrecently, 3D Gaussian Splatting has been explored as a potential method for\nreal-time crowd rendering. In this paper, we present a two-alternative forced\nchoice (2AFC) experiment that aims to determine the perceived quality of 3D\nGaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians),\nand the avatar height in Pixels (corresponding to the viewing distance).\nParticipants viewed pairs of animated 3D Gaussian avatars and were tasked with\nchoosing the most detailed one. Our findings can inform the optimization of LOD\nstrategies in Gaussian-based crowd rendering, thereby helping to achieve\nefficient rendering while maintaining visual quality in real-time applications.\n", "link": "http://arxiv.org/abs/2501.17085v1", "date": "2025-01-28", "relevancy": 2.866, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5885}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5681}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20CrowdSplat%3A%20Perceived%20Level%20of%20Detail%20for%20Gaussian%20Crowds&body=Title%3A%20Evaluating%20CrowdSplat%3A%20Perceived%20Level%20of%20Detail%20for%20Gaussian%20Crowds%0AAuthor%3A%20Xiaohan%20Sun%20and%20Yinghan%20Xu%20and%20John%20Dingliana%20and%20Carol%20O%27Sullivan%0AAbstract%3A%20%20%20Efficient%20and%20realistic%20crowd%20rendering%20is%20an%20important%20element%20of%20many%0Areal-time%20graphics%20applications%20such%20as%20Virtual%20Reality%20%28VR%29%20and%20games.%20To%20this%0Aend%2C%20Levels%20of%20Detail%20%28LOD%29%20avatar%20representations%20such%20as%20polygonal%20meshes%2C%0Aimage-based%20impostors%2C%20and%20point%20clouds%20have%20been%20proposed%20and%20evaluated.%20More%0Arecently%2C%203D%20Gaussian%20Splatting%20has%20been%20explored%20as%20a%20potential%20method%20for%0Areal-time%20crowd%20rendering.%20In%20this%20paper%2C%20we%20present%20a%20two-alternative%20forced%0Achoice%20%282AFC%29%20experiment%20that%20aims%20to%20determine%20the%20perceived%20quality%20of%203D%0AGaussian%20avatars.%20Three%20factors%20were%20explored%3A%20Motion%2C%20LOD%20%28i.e.%2C%20%23Gaussians%29%2C%0Aand%20the%20avatar%20height%20in%20Pixels%20%28corresponding%20to%20the%20viewing%20distance%29.%0AParticipants%20viewed%20pairs%20of%20animated%203D%20Gaussian%20avatars%20and%20were%20tasked%20with%0Achoosing%20the%20most%20detailed%20one.%20Our%20findings%20can%20inform%20the%20optimization%20of%20LOD%0Astrategies%20in%20Gaussian-based%20crowd%20rendering%2C%20thereby%20helping%20to%20achieve%0Aefficient%20rendering%20while%20maintaining%20visual%20quality%20in%20real-time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520CrowdSplat%253A%2520Perceived%2520Level%2520of%2520Detail%2520for%2520Gaussian%2520Crowds%26entry.906535625%3DXiaohan%2520Sun%2520and%2520Yinghan%2520Xu%2520and%2520John%2520Dingliana%2520and%2520Carol%2520O%2527Sullivan%26entry.1292438233%3D%2520%2520Efficient%2520and%2520realistic%2520crowd%2520rendering%2520is%2520an%2520important%2520element%2520of%2520many%250Areal-time%2520graphics%2520applications%2520such%2520as%2520Virtual%2520Reality%2520%2528VR%2529%2520and%2520games.%2520To%2520this%250Aend%252C%2520Levels%2520of%2520Detail%2520%2528LOD%2529%2520avatar%2520representations%2520such%2520as%2520polygonal%2520meshes%252C%250Aimage-based%2520impostors%252C%2520and%2520point%2520clouds%2520have%2520been%2520proposed%2520and%2520evaluated.%2520More%250Arecently%252C%25203D%2520Gaussian%2520Splatting%2520has%2520been%2520explored%2520as%2520a%2520potential%2520method%2520for%250Areal-time%2520crowd%2520rendering.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520two-alternative%2520forced%250Achoice%2520%25282AFC%2529%2520experiment%2520that%2520aims%2520to%2520determine%2520the%2520perceived%2520quality%2520of%25203D%250AGaussian%2520avatars.%2520Three%2520factors%2520were%2520explored%253A%2520Motion%252C%2520LOD%2520%2528i.e.%252C%2520%2523Gaussians%2529%252C%250Aand%2520the%2520avatar%2520height%2520in%2520Pixels%2520%2528corresponding%2520to%2520the%2520viewing%2520distance%2529.%250AParticipants%2520viewed%2520pairs%2520of%2520animated%25203D%2520Gaussian%2520avatars%2520and%2520were%2520tasked%2520with%250Achoosing%2520the%2520most%2520detailed%2520one.%2520Our%2520findings%2520can%2520inform%2520the%2520optimization%2520of%2520LOD%250Astrategies%2520in%2520Gaussian-based%2520crowd%2520rendering%252C%2520thereby%2520helping%2520to%2520achieve%250Aefficient%2520rendering%2520while%2520maintaining%2520visual%2520quality%2520in%2520real-time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20CrowdSplat%3A%20Perceived%20Level%20of%20Detail%20for%20Gaussian%20Crowds&entry.906535625=Xiaohan%20Sun%20and%20Yinghan%20Xu%20and%20John%20Dingliana%20and%20Carol%20O%27Sullivan&entry.1292438233=%20%20Efficient%20and%20realistic%20crowd%20rendering%20is%20an%20important%20element%20of%20many%0Areal-time%20graphics%20applications%20such%20as%20Virtual%20Reality%20%28VR%29%20and%20games.%20To%20this%0Aend%2C%20Levels%20of%20Detail%20%28LOD%29%20avatar%20representations%20such%20as%20polygonal%20meshes%2C%0Aimage-based%20impostors%2C%20and%20point%20clouds%20have%20been%20proposed%20and%20evaluated.%20More%0Arecently%2C%203D%20Gaussian%20Splatting%20has%20been%20explored%20as%20a%20potential%20method%20for%0Areal-time%20crowd%20rendering.%20In%20this%20paper%2C%20we%20present%20a%20two-alternative%20forced%0Achoice%20%282AFC%29%20experiment%20that%20aims%20to%20determine%20the%20perceived%20quality%20of%203D%0AGaussian%20avatars.%20Three%20factors%20were%20explored%3A%20Motion%2C%20LOD%20%28i.e.%2C%20%23Gaussians%29%2C%0Aand%20the%20avatar%20height%20in%20Pixels%20%28corresponding%20to%20the%20viewing%20distance%29.%0AParticipants%20viewed%20pairs%20of%20animated%203D%20Gaussian%20avatars%20and%20were%20tasked%20with%0Achoosing%20the%20most%20detailed%20one.%20Our%20findings%20can%20inform%20the%20optimization%20of%20LOD%0Astrategies%20in%20Gaussian-based%20crowd%20rendering%2C%20thereby%20helping%20to%20achieve%0Aefficient%20rendering%20while%20maintaining%20visual%20quality%20in%20real-time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17085v1&entry.124074799=Read"},
{"title": "PokeFlex: A Real-World Dataset of Volumetric Deformable Objects for\n  Robotics", "author": "Jan Obrist and Miguel Zamora and Hehui Zheng and Ronan Hinchet and Firat Ozdemir and Juan Zarate and Robert K. Katzschmann and Stelian Coros", "abstract": "  Data-driven methods have shown great potential in solving challenging\nmanipulation tasks; however, their application in the domain of deformable\nobjects has been constrained, in part, by the lack of data. To address this\nlack, we propose PokeFlex, a dataset featuring real-world multimodal data that\nis paired and annotated. The modalities include 3D textured meshes, point\nclouds, RGB images, and depth maps. Such data can be leveraged for several\ndownstream tasks, such as online 3D mesh reconstruction, and it can potentially\nenable underexplored applications such as the real-world deployment of\ntraditional control methods based on mesh simulations. To deal with the\nchallenges posed by real-world 3D mesh reconstruction, we leverage a\nprofessional volumetric capture system that allows complete 360{\\deg}\nreconstruction. PokeFlex consists of 18 deformable objects with varying\nstiffness and shapes. Deformations are generated by dropping objects onto a\nflat surface or by poking the objects with a robot arm. Interaction wrenches\nand contact locations are also reported for the latter case. Using different\ndata modalities, we demonstrated a use case for our dataset training models\nthat, given the novelty of the multimodal nature of Pokeflex, constitute the\nstate-of-the-art in multi-object online template-based mesh reconstruction from\nmultimodal data, to the best of our knowledge. We refer the reader to our\nwebsite ( https://pokeflex-dataset.github.io/ ) for further demos and examples.\n", "link": "http://arxiv.org/abs/2410.07688v2", "date": "2025-01-28", "relevancy": 2.8293, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5776}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5611}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PokeFlex%3A%20A%20Real-World%20Dataset%20of%20Volumetric%20Deformable%20Objects%20for%0A%20%20Robotics&body=Title%3A%20PokeFlex%3A%20A%20Real-World%20Dataset%20of%20Volumetric%20Deformable%20Objects%20for%0A%20%20Robotics%0AAuthor%3A%20Jan%20Obrist%20and%20Miguel%20Zamora%20and%20Hehui%20Zheng%20and%20Ronan%20Hinchet%20and%20Firat%20Ozdemir%20and%20Juan%20Zarate%20and%20Robert%20K.%20Katzschmann%20and%20Stelian%20Coros%0AAbstract%3A%20%20%20Data-driven%20methods%20have%20shown%20great%20potential%20in%20solving%20challenging%0Amanipulation%20tasks%3B%20however%2C%20their%20application%20in%20the%20domain%20of%20deformable%0Aobjects%20has%20been%20constrained%2C%20in%20part%2C%20by%20the%20lack%20of%20data.%20To%20address%20this%0Alack%2C%20we%20propose%20PokeFlex%2C%20a%20dataset%20featuring%20real-world%20multimodal%20data%20that%0Ais%20paired%20and%20annotated.%20The%20modalities%20include%203D%20textured%20meshes%2C%20point%0Aclouds%2C%20RGB%20images%2C%20and%20depth%20maps.%20Such%20data%20can%20be%20leveraged%20for%20several%0Adownstream%20tasks%2C%20such%20as%20online%203D%20mesh%20reconstruction%2C%20and%20it%20can%20potentially%0Aenable%20underexplored%20applications%20such%20as%20the%20real-world%20deployment%20of%0Atraditional%20control%20methods%20based%20on%20mesh%20simulations.%20To%20deal%20with%20the%0Achallenges%20posed%20by%20real-world%203D%20mesh%20reconstruction%2C%20we%20leverage%20a%0Aprofessional%20volumetric%20capture%20system%20that%20allows%20complete%20360%7B%5Cdeg%7D%0Areconstruction.%20PokeFlex%20consists%20of%2018%20deformable%20objects%20with%20varying%0Astiffness%20and%20shapes.%20Deformations%20are%20generated%20by%20dropping%20objects%20onto%20a%0Aflat%20surface%20or%20by%20poking%20the%20objects%20with%20a%20robot%20arm.%20Interaction%20wrenches%0Aand%20contact%20locations%20are%20also%20reported%20for%20the%20latter%20case.%20Using%20different%0Adata%20modalities%2C%20we%20demonstrated%20a%20use%20case%20for%20our%20dataset%20training%20models%0Athat%2C%20given%20the%20novelty%20of%20the%20multimodal%20nature%20of%20Pokeflex%2C%20constitute%20the%0Astate-of-the-art%20in%20multi-object%20online%20template-based%20mesh%20reconstruction%20from%0Amultimodal%20data%2C%20to%20the%20best%20of%20our%20knowledge.%20We%20refer%20the%20reader%20to%20our%0Awebsite%20%28%20https%3A//pokeflex-dataset.github.io/%20%29%20for%20further%20demos%20and%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPokeFlex%253A%2520A%2520Real-World%2520Dataset%2520of%2520Volumetric%2520Deformable%2520Objects%2520for%250A%2520%2520Robotics%26entry.906535625%3DJan%2520Obrist%2520and%2520Miguel%2520Zamora%2520and%2520Hehui%2520Zheng%2520and%2520Ronan%2520Hinchet%2520and%2520Firat%2520Ozdemir%2520and%2520Juan%2520Zarate%2520and%2520Robert%2520K.%2520Katzschmann%2520and%2520Stelian%2520Coros%26entry.1292438233%3D%2520%2520Data-driven%2520methods%2520have%2520shown%2520great%2520potential%2520in%2520solving%2520challenging%250Amanipulation%2520tasks%253B%2520however%252C%2520their%2520application%2520in%2520the%2520domain%2520of%2520deformable%250Aobjects%2520has%2520been%2520constrained%252C%2520in%2520part%252C%2520by%2520the%2520lack%2520of%2520data.%2520To%2520address%2520this%250Alack%252C%2520we%2520propose%2520PokeFlex%252C%2520a%2520dataset%2520featuring%2520real-world%2520multimodal%2520data%2520that%250Ais%2520paired%2520and%2520annotated.%2520The%2520modalities%2520include%25203D%2520textured%2520meshes%252C%2520point%250Aclouds%252C%2520RGB%2520images%252C%2520and%2520depth%2520maps.%2520Such%2520data%2520can%2520be%2520leveraged%2520for%2520several%250Adownstream%2520tasks%252C%2520such%2520as%2520online%25203D%2520mesh%2520reconstruction%252C%2520and%2520it%2520can%2520potentially%250Aenable%2520underexplored%2520applications%2520such%2520as%2520the%2520real-world%2520deployment%2520of%250Atraditional%2520control%2520methods%2520based%2520on%2520mesh%2520simulations.%2520To%2520deal%2520with%2520the%250Achallenges%2520posed%2520by%2520real-world%25203D%2520mesh%2520reconstruction%252C%2520we%2520leverage%2520a%250Aprofessional%2520volumetric%2520capture%2520system%2520that%2520allows%2520complete%2520360%257B%255Cdeg%257D%250Areconstruction.%2520PokeFlex%2520consists%2520of%252018%2520deformable%2520objects%2520with%2520varying%250Astiffness%2520and%2520shapes.%2520Deformations%2520are%2520generated%2520by%2520dropping%2520objects%2520onto%2520a%250Aflat%2520surface%2520or%2520by%2520poking%2520the%2520objects%2520with%2520a%2520robot%2520arm.%2520Interaction%2520wrenches%250Aand%2520contact%2520locations%2520are%2520also%2520reported%2520for%2520the%2520latter%2520case.%2520Using%2520different%250Adata%2520modalities%252C%2520we%2520demonstrated%2520a%2520use%2520case%2520for%2520our%2520dataset%2520training%2520models%250Athat%252C%2520given%2520the%2520novelty%2520of%2520the%2520multimodal%2520nature%2520of%2520Pokeflex%252C%2520constitute%2520the%250Astate-of-the-art%2520in%2520multi-object%2520online%2520template-based%2520mesh%2520reconstruction%2520from%250Amultimodal%2520data%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge.%2520We%2520refer%2520the%2520reader%2520to%2520our%250Awebsite%2520%2528%2520https%253A//pokeflex-dataset.github.io/%2520%2529%2520for%2520further%2520demos%2520and%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PokeFlex%3A%20A%20Real-World%20Dataset%20of%20Volumetric%20Deformable%20Objects%20for%0A%20%20Robotics&entry.906535625=Jan%20Obrist%20and%20Miguel%20Zamora%20and%20Hehui%20Zheng%20and%20Ronan%20Hinchet%20and%20Firat%20Ozdemir%20and%20Juan%20Zarate%20and%20Robert%20K.%20Katzschmann%20and%20Stelian%20Coros&entry.1292438233=%20%20Data-driven%20methods%20have%20shown%20great%20potential%20in%20solving%20challenging%0Amanipulation%20tasks%3B%20however%2C%20their%20application%20in%20the%20domain%20of%20deformable%0Aobjects%20has%20been%20constrained%2C%20in%20part%2C%20by%20the%20lack%20of%20data.%20To%20address%20this%0Alack%2C%20we%20propose%20PokeFlex%2C%20a%20dataset%20featuring%20real-world%20multimodal%20data%20that%0Ais%20paired%20and%20annotated.%20The%20modalities%20include%203D%20textured%20meshes%2C%20point%0Aclouds%2C%20RGB%20images%2C%20and%20depth%20maps.%20Such%20data%20can%20be%20leveraged%20for%20several%0Adownstream%20tasks%2C%20such%20as%20online%203D%20mesh%20reconstruction%2C%20and%20it%20can%20potentially%0Aenable%20underexplored%20applications%20such%20as%20the%20real-world%20deployment%20of%0Atraditional%20control%20methods%20based%20on%20mesh%20simulations.%20To%20deal%20with%20the%0Achallenges%20posed%20by%20real-world%203D%20mesh%20reconstruction%2C%20we%20leverage%20a%0Aprofessional%20volumetric%20capture%20system%20that%20allows%20complete%20360%7B%5Cdeg%7D%0Areconstruction.%20PokeFlex%20consists%20of%2018%20deformable%20objects%20with%20varying%0Astiffness%20and%20shapes.%20Deformations%20are%20generated%20by%20dropping%20objects%20onto%20a%0Aflat%20surface%20or%20by%20poking%20the%20objects%20with%20a%20robot%20arm.%20Interaction%20wrenches%0Aand%20contact%20locations%20are%20also%20reported%20for%20the%20latter%20case.%20Using%20different%0Adata%20modalities%2C%20we%20demonstrated%20a%20use%20case%20for%20our%20dataset%20training%20models%0Athat%2C%20given%20the%20novelty%20of%20the%20multimodal%20nature%20of%20Pokeflex%2C%20constitute%20the%0Astate-of-the-art%20in%20multi-object%20online%20template-based%20mesh%20reconstruction%20from%0Amultimodal%20data%2C%20to%20the%20best%20of%20our%20knowledge.%20We%20refer%20the%20reader%20to%20our%0Awebsite%20%28%20https%3A//pokeflex-dataset.github.io/%20%29%20for%20further%20demos%20and%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07688v2&entry.124074799=Read"},
{"title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders", "author": "Zhengxuan Wu and Aryaman Arora and Atticus Geiger and Zheng Wang and Jing Huang and Dan Jurafsky and Christopher D. Manning and Christopher Potts", "abstract": "  Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.\n", "link": "http://arxiv.org/abs/2501.17148v1", "date": "2025-01-28", "relevancy": 2.8169, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5821}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AxBench%3A%20Steering%20LLMs%3F%20Even%20Simple%20Baselines%20Outperform%20Sparse%0A%20%20Autoencoders&body=Title%3A%20AxBench%3A%20Steering%20LLMs%3F%20Even%20Simple%20Baselines%20Outperform%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20Zhengxuan%20Wu%20and%20Aryaman%20Arora%20and%20Atticus%20Geiger%20and%20Zheng%20Wang%20and%20Jing%20Huang%20and%20Dan%20Jurafsky%20and%20Christopher%20D.%20Manning%20and%20Christopher%20Potts%0AAbstract%3A%20%20%20Fine-grained%20steering%20of%20language%20model%20outputs%20is%20essential%20for%20safety%20and%0Areliability.%20Prompting%20and%20finetuning%20are%20widely%20used%20to%20achieve%20these%20goals%2C%0Abut%20interpretability%20researchers%20have%20proposed%20a%20variety%20of%0Arepresentation-based%20techniques%20as%20well%2C%20including%20sparse%20autoencoders%20%28SAEs%29%2C%0Alinear%20artificial%20tomography%2C%20supervised%20steering%20vectors%2C%20linear%20probes%2C%20and%0Arepresentation%20finetuning.%20At%20present%2C%20there%20is%20no%20benchmark%20for%20making%20direct%0Acomparisons%20between%20these%20proposals.%20Therefore%2C%20we%20introduce%20AxBench%2C%20a%0Alarge-scale%20benchmark%20for%20steering%20and%20concept%20detection%2C%20and%20report%0Aexperiments%20on%20Gemma-2-2B%20and%209B.%20For%20steering%2C%20we%20find%20that%20prompting%0Aoutperforms%20all%20existing%20methods%2C%20followed%20by%20finetuning.%20For%20concept%0Adetection%2C%20representation-based%20methods%20such%20as%20difference-in-means%2C%20perform%0Athe%20best.%20On%20both%20evaluations%2C%20SAEs%20are%20not%20competitive.%20We%20introduce%20a%20novel%0Aweakly-supervised%20representational%20method%20%28Rank-1%20Representation%20Finetuning%3B%0AReFT-r1%29%2C%20which%20is%20competitive%20on%20both%20tasks%20while%20providing%20the%0Ainterpretability%20advantages%20that%20prompting%20lacks.%20Along%20with%20AxBench%2C%20we%20train%0Aand%20publicly%20release%20SAE-scale%20feature%20dictionaries%20for%20ReFT-r1%20and%20DiffMean.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAxBench%253A%2520Steering%2520LLMs%253F%2520Even%2520Simple%2520Baselines%2520Outperform%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DZhengxuan%2520Wu%2520and%2520Aryaman%2520Arora%2520and%2520Atticus%2520Geiger%2520and%2520Zheng%2520Wang%2520and%2520Jing%2520Huang%2520and%2520Dan%2520Jurafsky%2520and%2520Christopher%2520D.%2520Manning%2520and%2520Christopher%2520Potts%26entry.1292438233%3D%2520%2520Fine-grained%2520steering%2520of%2520language%2520model%2520outputs%2520is%2520essential%2520for%2520safety%2520and%250Areliability.%2520Prompting%2520and%2520finetuning%2520are%2520widely%2520used%2520to%2520achieve%2520these%2520goals%252C%250Abut%2520interpretability%2520researchers%2520have%2520proposed%2520a%2520variety%2520of%250Arepresentation-based%2520techniques%2520as%2520well%252C%2520including%2520sparse%2520autoencoders%2520%2528SAEs%2529%252C%250Alinear%2520artificial%2520tomography%252C%2520supervised%2520steering%2520vectors%252C%2520linear%2520probes%252C%2520and%250Arepresentation%2520finetuning.%2520At%2520present%252C%2520there%2520is%2520no%2520benchmark%2520for%2520making%2520direct%250Acomparisons%2520between%2520these%2520proposals.%2520Therefore%252C%2520we%2520introduce%2520AxBench%252C%2520a%250Alarge-scale%2520benchmark%2520for%2520steering%2520and%2520concept%2520detection%252C%2520and%2520report%250Aexperiments%2520on%2520Gemma-2-2B%2520and%25209B.%2520For%2520steering%252C%2520we%2520find%2520that%2520prompting%250Aoutperforms%2520all%2520existing%2520methods%252C%2520followed%2520by%2520finetuning.%2520For%2520concept%250Adetection%252C%2520representation-based%2520methods%2520such%2520as%2520difference-in-means%252C%2520perform%250Athe%2520best.%2520On%2520both%2520evaluations%252C%2520SAEs%2520are%2520not%2520competitive.%2520We%2520introduce%2520a%2520novel%250Aweakly-supervised%2520representational%2520method%2520%2528Rank-1%2520Representation%2520Finetuning%253B%250AReFT-r1%2529%252C%2520which%2520is%2520competitive%2520on%2520both%2520tasks%2520while%2520providing%2520the%250Ainterpretability%2520advantages%2520that%2520prompting%2520lacks.%2520Along%2520with%2520AxBench%252C%2520we%2520train%250Aand%2520publicly%2520release%2520SAE-scale%2520feature%2520dictionaries%2520for%2520ReFT-r1%2520and%2520DiffMean.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AxBench%3A%20Steering%20LLMs%3F%20Even%20Simple%20Baselines%20Outperform%20Sparse%0A%20%20Autoencoders&entry.906535625=Zhengxuan%20Wu%20and%20Aryaman%20Arora%20and%20Atticus%20Geiger%20and%20Zheng%20Wang%20and%20Jing%20Huang%20and%20Dan%20Jurafsky%20and%20Christopher%20D.%20Manning%20and%20Christopher%20Potts&entry.1292438233=%20%20Fine-grained%20steering%20of%20language%20model%20outputs%20is%20essential%20for%20safety%20and%0Areliability.%20Prompting%20and%20finetuning%20are%20widely%20used%20to%20achieve%20these%20goals%2C%0Abut%20interpretability%20researchers%20have%20proposed%20a%20variety%20of%0Arepresentation-based%20techniques%20as%20well%2C%20including%20sparse%20autoencoders%20%28SAEs%29%2C%0Alinear%20artificial%20tomography%2C%20supervised%20steering%20vectors%2C%20linear%20probes%2C%20and%0Arepresentation%20finetuning.%20At%20present%2C%20there%20is%20no%20benchmark%20for%20making%20direct%0Acomparisons%20between%20these%20proposals.%20Therefore%2C%20we%20introduce%20AxBench%2C%20a%0Alarge-scale%20benchmark%20for%20steering%20and%20concept%20detection%2C%20and%20report%0Aexperiments%20on%20Gemma-2-2B%20and%209B.%20For%20steering%2C%20we%20find%20that%20prompting%0Aoutperforms%20all%20existing%20methods%2C%20followed%20by%20finetuning.%20For%20concept%0Adetection%2C%20representation-based%20methods%20such%20as%20difference-in-means%2C%20perform%0Athe%20best.%20On%20both%20evaluations%2C%20SAEs%20are%20not%20competitive.%20We%20introduce%20a%20novel%0Aweakly-supervised%20representational%20method%20%28Rank-1%20Representation%20Finetuning%3B%0AReFT-r1%29%2C%20which%20is%20competitive%20on%20both%20tasks%20while%20providing%20the%0Ainterpretability%20advantages%20that%20prompting%20lacks.%20Along%20with%20AxBench%2C%20we%20train%0Aand%20publicly%20release%20SAE-scale%20feature%20dictionaries%20for%20ReFT-r1%20and%20DiffMean.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17148v1&entry.124074799=Read"},
{"title": "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama\n  Generation", "author": "Nikolai Kalischek and Michael Oechsle and Fabian Manhardt and Philipp Henzler and Konrad Schindler and Federico Tombari", "abstract": "  We introduce a novel method for generating 360{\\deg} panoramas from text\nprompts or images. Our approach leverages recent advances in 3D generation by\nemploying multi-view diffusion models to jointly synthesize the six faces of a\ncubemap. Unlike previous methods that rely on processing equirectangular\nprojections or autoregressive generation, our method treats each face as a\nstandard perspective image, simplifying the generation process and enabling the\nuse of existing multi-view diffusion models. We demonstrate that these models\ncan be adapted to produce high-quality cubemaps without requiring\ncorrespondence-aware attention layers. Our model allows for fine-grained text\ncontrol, generates high resolution panorama images and generalizes well beyond\nits training set, whilst achieving state-of-the-art results, both qualitatively\nand quantitatively. Project page: https://cubediff.github.io/\n", "link": "http://arxiv.org/abs/2501.17162v1", "date": "2025-01-28", "relevancy": 2.7469, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6989}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6989}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CubeDiff%3A%20Repurposing%20Diffusion-Based%20Image%20Models%20for%20Panorama%0A%20%20Generation&body=Title%3A%20CubeDiff%3A%20Repurposing%20Diffusion-Based%20Image%20Models%20for%20Panorama%0A%20%20Generation%0AAuthor%3A%20Nikolai%20Kalischek%20and%20Michael%20Oechsle%20and%20Fabian%20Manhardt%20and%20Philipp%20Henzler%20and%20Konrad%20Schindler%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20generating%20360%7B%5Cdeg%7D%20panoramas%20from%20text%0Aprompts%20or%20images.%20Our%20approach%20leverages%20recent%20advances%20in%203D%20generation%20by%0Aemploying%20multi-view%20diffusion%20models%20to%20jointly%20synthesize%20the%20six%20faces%20of%20a%0Acubemap.%20Unlike%20previous%20methods%20that%20rely%20on%20processing%20equirectangular%0Aprojections%20or%20autoregressive%20generation%2C%20our%20method%20treats%20each%20face%20as%20a%0Astandard%20perspective%20image%2C%20simplifying%20the%20generation%20process%20and%20enabling%20the%0Ause%20of%20existing%20multi-view%20diffusion%20models.%20We%20demonstrate%20that%20these%20models%0Acan%20be%20adapted%20to%20produce%20high-quality%20cubemaps%20without%20requiring%0Acorrespondence-aware%20attention%20layers.%20Our%20model%20allows%20for%20fine-grained%20text%0Acontrol%2C%20generates%20high%20resolution%20panorama%20images%20and%20generalizes%20well%20beyond%0Aits%20training%20set%2C%20whilst%20achieving%20state-of-the-art%20results%2C%20both%20qualitatively%0Aand%20quantitatively.%20Project%20page%3A%20https%3A//cubediff.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCubeDiff%253A%2520Repurposing%2520Diffusion-Based%2520Image%2520Models%2520for%2520Panorama%250A%2520%2520Generation%26entry.906535625%3DNikolai%2520Kalischek%2520and%2520Michael%2520Oechsle%2520and%2520Fabian%2520Manhardt%2520and%2520Philipp%2520Henzler%2520and%2520Konrad%2520Schindler%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520generating%2520360%257B%255Cdeg%257D%2520panoramas%2520from%2520text%250Aprompts%2520or%2520images.%2520Our%2520approach%2520leverages%2520recent%2520advances%2520in%25203D%2520generation%2520by%250Aemploying%2520multi-view%2520diffusion%2520models%2520to%2520jointly%2520synthesize%2520the%2520six%2520faces%2520of%2520a%250Acubemap.%2520Unlike%2520previous%2520methods%2520that%2520rely%2520on%2520processing%2520equirectangular%250Aprojections%2520or%2520autoregressive%2520generation%252C%2520our%2520method%2520treats%2520each%2520face%2520as%2520a%250Astandard%2520perspective%2520image%252C%2520simplifying%2520the%2520generation%2520process%2520and%2520enabling%2520the%250Ause%2520of%2520existing%2520multi-view%2520diffusion%2520models.%2520We%2520demonstrate%2520that%2520these%2520models%250Acan%2520be%2520adapted%2520to%2520produce%2520high-quality%2520cubemaps%2520without%2520requiring%250Acorrespondence-aware%2520attention%2520layers.%2520Our%2520model%2520allows%2520for%2520fine-grained%2520text%250Acontrol%252C%2520generates%2520high%2520resolution%2520panorama%2520images%2520and%2520generalizes%2520well%2520beyond%250Aits%2520training%2520set%252C%2520whilst%2520achieving%2520state-of-the-art%2520results%252C%2520both%2520qualitatively%250Aand%2520quantitatively.%2520Project%2520page%253A%2520https%253A//cubediff.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CubeDiff%3A%20Repurposing%20Diffusion-Based%20Image%20Models%20for%20Panorama%0A%20%20Generation&entry.906535625=Nikolai%20Kalischek%20and%20Michael%20Oechsle%20and%20Fabian%20Manhardt%20and%20Philipp%20Henzler%20and%20Konrad%20Schindler%20and%20Federico%20Tombari&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20generating%20360%7B%5Cdeg%7D%20panoramas%20from%20text%0Aprompts%20or%20images.%20Our%20approach%20leverages%20recent%20advances%20in%203D%20generation%20by%0Aemploying%20multi-view%20diffusion%20models%20to%20jointly%20synthesize%20the%20six%20faces%20of%20a%0Acubemap.%20Unlike%20previous%20methods%20that%20rely%20on%20processing%20equirectangular%0Aprojections%20or%20autoregressive%20generation%2C%20our%20method%20treats%20each%20face%20as%20a%0Astandard%20perspective%20image%2C%20simplifying%20the%20generation%20process%20and%20enabling%20the%0Ause%20of%20existing%20multi-view%20diffusion%20models.%20We%20demonstrate%20that%20these%20models%0Acan%20be%20adapted%20to%20produce%20high-quality%20cubemaps%20without%20requiring%0Acorrespondence-aware%20attention%20layers.%20Our%20model%20allows%20for%20fine-grained%20text%0Acontrol%2C%20generates%20high%20resolution%20panorama%20images%20and%20generalizes%20well%20beyond%0Aits%20training%20set%2C%20whilst%20achieving%20state-of-the-art%20results%2C%20both%20qualitatively%0Aand%20quantitatively.%20Project%20page%3A%20https%3A//cubediff.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17162v1&entry.124074799=Read"},
{"title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model\n  Post-training", "author": "Tianzhe Chu and Yuexiang Zhai and Jihan Yang and Shengbang Tong and Saining Xie and Dale Schuurmans and Quoc V. Le and Sergey Levine and Yi Ma", "abstract": "  Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used\npost-training techniques for foundation models. However, their roles in\nenhancing model generalization capabilities remain unclear. This paper studies\nthe difference between SFT and RL on generalization and memorization, focusing\non text-based rule variants and visual variants. We introduce GeneralPoints, an\narithmetic reasoning card game, and adopt V-IRL, a real-world navigation\nenvironment, to assess how models trained with SFT and RL generalize to unseen\nvariants in both textual and visual domains. We show that RL, especially when\ntrained with an outcome-based reward, generalizes across both rule-based\ntextual and visual variants. SFT, in contrast, tends to memorize training data\nand struggles to generalize out-of-distribution scenarios. Further analysis\nreveals that RL improves the model's underlying visual recognition\ncapabilities, contributing to its enhanced generalization in the visual domain.\nDespite RL's superior generalization, we show that SFT remains essential for\neffective RL training; SFT stabilizes the model's output format, enabling\nsubsequent RL to achieve its performance gains. These findings demonstrates the\ncapability of RL for acquiring generalizable knowledge in complex, multi-modal\ntasks.\n", "link": "http://arxiv.org/abs/2501.17161v1", "date": "2025-01-28", "relevancy": 2.7221, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFT%20Memorizes%2C%20RL%20Generalizes%3A%20A%20Comparative%20Study%20of%20Foundation%20Model%0A%20%20Post-training&body=Title%3A%20SFT%20Memorizes%2C%20RL%20Generalizes%3A%20A%20Comparative%20Study%20of%20Foundation%20Model%0A%20%20Post-training%0AAuthor%3A%20Tianzhe%20Chu%20and%20Yuexiang%20Zhai%20and%20Jihan%20Yang%20and%20Shengbang%20Tong%20and%20Saining%20Xie%20and%20Dale%20Schuurmans%20and%20Quoc%20V.%20Le%20and%20Sergey%20Levine%20and%20Yi%20Ma%0AAbstract%3A%20%20%20Supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29%20are%20widely%20used%0Apost-training%20techniques%20for%20foundation%20models.%20However%2C%20their%20roles%20in%0Aenhancing%20model%20generalization%20capabilities%20remain%20unclear.%20This%20paper%20studies%0Athe%20difference%20between%20SFT%20and%20RL%20on%20generalization%20and%20memorization%2C%20focusing%0Aon%20text-based%20rule%20variants%20and%20visual%20variants.%20We%20introduce%20GeneralPoints%2C%20an%0Aarithmetic%20reasoning%20card%20game%2C%20and%20adopt%20V-IRL%2C%20a%20real-world%20navigation%0Aenvironment%2C%20to%20assess%20how%20models%20trained%20with%20SFT%20and%20RL%20generalize%20to%20unseen%0Avariants%20in%20both%20textual%20and%20visual%20domains.%20We%20show%20that%20RL%2C%20especially%20when%0Atrained%20with%20an%20outcome-based%20reward%2C%20generalizes%20across%20both%20rule-based%0Atextual%20and%20visual%20variants.%20SFT%2C%20in%20contrast%2C%20tends%20to%20memorize%20training%20data%0Aand%20struggles%20to%20generalize%20out-of-distribution%20scenarios.%20Further%20analysis%0Areveals%20that%20RL%20improves%20the%20model%27s%20underlying%20visual%20recognition%0Acapabilities%2C%20contributing%20to%20its%20enhanced%20generalization%20in%20the%20visual%20domain.%0ADespite%20RL%27s%20superior%20generalization%2C%20we%20show%20that%20SFT%20remains%20essential%20for%0Aeffective%20RL%20training%3B%20SFT%20stabilizes%20the%20model%27s%20output%20format%2C%20enabling%0Asubsequent%20RL%20to%20achieve%20its%20performance%20gains.%20These%20findings%20demonstrates%20the%0Acapability%20of%20RL%20for%20acquiring%20generalizable%20knowledge%20in%20complex%2C%20multi-modal%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFT%2520Memorizes%252C%2520RL%2520Generalizes%253A%2520A%2520Comparative%2520Study%2520of%2520Foundation%2520Model%250A%2520%2520Post-training%26entry.906535625%3DTianzhe%2520Chu%2520and%2520Yuexiang%2520Zhai%2520and%2520Jihan%2520Yang%2520and%2520Shengbang%2520Tong%2520and%2520Saining%2520Xie%2520and%2520Dale%2520Schuurmans%2520and%2520Quoc%2520V.%2520Le%2520and%2520Sergey%2520Levine%2520and%2520Yi%2520Ma%26entry.1292438233%3D%2520%2520Supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520are%2520widely%2520used%250Apost-training%2520techniques%2520for%2520foundation%2520models.%2520However%252C%2520their%2520roles%2520in%250Aenhancing%2520model%2520generalization%2520capabilities%2520remain%2520unclear.%2520This%2520paper%2520studies%250Athe%2520difference%2520between%2520SFT%2520and%2520RL%2520on%2520generalization%2520and%2520memorization%252C%2520focusing%250Aon%2520text-based%2520rule%2520variants%2520and%2520visual%2520variants.%2520We%2520introduce%2520GeneralPoints%252C%2520an%250Aarithmetic%2520reasoning%2520card%2520game%252C%2520and%2520adopt%2520V-IRL%252C%2520a%2520real-world%2520navigation%250Aenvironment%252C%2520to%2520assess%2520how%2520models%2520trained%2520with%2520SFT%2520and%2520RL%2520generalize%2520to%2520unseen%250Avariants%2520in%2520both%2520textual%2520and%2520visual%2520domains.%2520We%2520show%2520that%2520RL%252C%2520especially%2520when%250Atrained%2520with%2520an%2520outcome-based%2520reward%252C%2520generalizes%2520across%2520both%2520rule-based%250Atextual%2520and%2520visual%2520variants.%2520SFT%252C%2520in%2520contrast%252C%2520tends%2520to%2520memorize%2520training%2520data%250Aand%2520struggles%2520to%2520generalize%2520out-of-distribution%2520scenarios.%2520Further%2520analysis%250Areveals%2520that%2520RL%2520improves%2520the%2520model%2527s%2520underlying%2520visual%2520recognition%250Acapabilities%252C%2520contributing%2520to%2520its%2520enhanced%2520generalization%2520in%2520the%2520visual%2520domain.%250ADespite%2520RL%2527s%2520superior%2520generalization%252C%2520we%2520show%2520that%2520SFT%2520remains%2520essential%2520for%250Aeffective%2520RL%2520training%253B%2520SFT%2520stabilizes%2520the%2520model%2527s%2520output%2520format%252C%2520enabling%250Asubsequent%2520RL%2520to%2520achieve%2520its%2520performance%2520gains.%2520These%2520findings%2520demonstrates%2520the%250Acapability%2520of%2520RL%2520for%2520acquiring%2520generalizable%2520knowledge%2520in%2520complex%252C%2520multi-modal%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFT%20Memorizes%2C%20RL%20Generalizes%3A%20A%20Comparative%20Study%20of%20Foundation%20Model%0A%20%20Post-training&entry.906535625=Tianzhe%20Chu%20and%20Yuexiang%20Zhai%20and%20Jihan%20Yang%20and%20Shengbang%20Tong%20and%20Saining%20Xie%20and%20Dale%20Schuurmans%20and%20Quoc%20V.%20Le%20and%20Sergey%20Levine%20and%20Yi%20Ma&entry.1292438233=%20%20Supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29%20are%20widely%20used%0Apost-training%20techniques%20for%20foundation%20models.%20However%2C%20their%20roles%20in%0Aenhancing%20model%20generalization%20capabilities%20remain%20unclear.%20This%20paper%20studies%0Athe%20difference%20between%20SFT%20and%20RL%20on%20generalization%20and%20memorization%2C%20focusing%0Aon%20text-based%20rule%20variants%20and%20visual%20variants.%20We%20introduce%20GeneralPoints%2C%20an%0Aarithmetic%20reasoning%20card%20game%2C%20and%20adopt%20V-IRL%2C%20a%20real-world%20navigation%0Aenvironment%2C%20to%20assess%20how%20models%20trained%20with%20SFT%20and%20RL%20generalize%20to%20unseen%0Avariants%20in%20both%20textual%20and%20visual%20domains.%20We%20show%20that%20RL%2C%20especially%20when%0Atrained%20with%20an%20outcome-based%20reward%2C%20generalizes%20across%20both%20rule-based%0Atextual%20and%20visual%20variants.%20SFT%2C%20in%20contrast%2C%20tends%20to%20memorize%20training%20data%0Aand%20struggles%20to%20generalize%20out-of-distribution%20scenarios.%20Further%20analysis%0Areveals%20that%20RL%20improves%20the%20model%27s%20underlying%20visual%20recognition%0Acapabilities%2C%20contributing%20to%20its%20enhanced%20generalization%20in%20the%20visual%20domain.%0ADespite%20RL%27s%20superior%20generalization%2C%20we%20show%20that%20SFT%20remains%20essential%20for%0Aeffective%20RL%20training%3B%20SFT%20stabilizes%20the%20model%27s%20output%20format%2C%20enabling%0Asubsequent%20RL%20to%20achieve%20its%20performance%20gains.%20These%20findings%20demonstrates%20the%0Acapability%20of%20RL%20for%20acquiring%20generalizable%20knowledge%20in%20complex%2C%20multi-modal%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17161v1&entry.124074799=Read"},
{"title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering", "author": "Nicolas von L\u00fctzow and Matthias Nie\u00dfner", "abstract": "  Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives-octahedra and tetrahedra-both of which define\nhomogeneous volumes bounded by triangular faces. This formulation aligns\nnaturally with standard mesh-based tools, minimizing overhead for downstream\napplications. To optimize these primitives, we present a differentiable\nrasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based\noptimization while maintaining realtime rendering capabilities. Through\nexperiments on real-world datasets, we demonstrate comparable performance to\nstate-of-the-art volumetric methods while requiring fewer primitives to achieve\nsimilar reconstruction fidelity. Our findings provide insights into the\ngeometry of volumetric rendering and suggest that adopting explicit polyhedra\ncan expand the design space of scene representations.\n", "link": "http://arxiv.org/abs/2501.16312v2", "date": "2025-01-28", "relevancy": 2.6818, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5544}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5392}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering&body=Title%3A%20LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering%0AAuthor%3A%20Nicolas%20von%20L%C3%BCtzow%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20Volumetric%20rendering%20has%20become%20central%20to%20modern%20novel%20view%20synthesis%0Amethods%2C%20which%20use%20differentiable%20rendering%20to%20optimize%203D%20scene%0Arepresentations%20directly%20from%20observed%20views.%20While%20many%20recent%20works%20build%20on%0ANeRF%20or%203D%20Gaussians%2C%20we%20explore%20an%20alternative%20volumetric%20scene%0Arepresentation.%20More%20specifically%2C%20we%20introduce%20two%20new%20scene%20representations%0Abased%20on%20linear%20primitives-octahedra%20and%20tetrahedra-both%20of%20which%20define%0Ahomogeneous%20volumes%20bounded%20by%20triangular%20faces.%20This%20formulation%20aligns%0Anaturally%20with%20standard%20mesh-based%20tools%2C%20minimizing%20overhead%20for%20downstream%0Aapplications.%20To%20optimize%20these%20primitives%2C%20we%20present%20a%20differentiable%0Arasterizer%20that%20runs%20efficiently%20on%20GPUs%2C%20allowing%20end-to-end%20gradient-based%0Aoptimization%20while%20maintaining%20realtime%20rendering%20capabilities.%20Through%0Aexperiments%20on%20real-world%20datasets%2C%20we%20demonstrate%20comparable%20performance%20to%0Astate-of-the-art%20volumetric%20methods%20while%20requiring%20fewer%20primitives%20to%20achieve%0Asimilar%20reconstruction%20fidelity.%20Our%20findings%20provide%20insights%20into%20the%0Ageometry%20of%20volumetric%20rendering%20and%20suggest%20that%20adopting%20explicit%20polyhedra%0Acan%20expand%20the%20design%20space%20of%20scene%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinPrim%253A%2520Linear%2520Primitives%2520for%2520Differentiable%2520Volumetric%2520Rendering%26entry.906535625%3DNicolas%2520von%2520L%25C3%25BCtzow%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520Volumetric%2520rendering%2520has%2520become%2520central%2520to%2520modern%2520novel%2520view%2520synthesis%250Amethods%252C%2520which%2520use%2520differentiable%2520rendering%2520to%2520optimize%25203D%2520scene%250Arepresentations%2520directly%2520from%2520observed%2520views.%2520While%2520many%2520recent%2520works%2520build%2520on%250ANeRF%2520or%25203D%2520Gaussians%252C%2520we%2520explore%2520an%2520alternative%2520volumetric%2520scene%250Arepresentation.%2520More%2520specifically%252C%2520we%2520introduce%2520two%2520new%2520scene%2520representations%250Abased%2520on%2520linear%2520primitives-octahedra%2520and%2520tetrahedra-both%2520of%2520which%2520define%250Ahomogeneous%2520volumes%2520bounded%2520by%2520triangular%2520faces.%2520This%2520formulation%2520aligns%250Anaturally%2520with%2520standard%2520mesh-based%2520tools%252C%2520minimizing%2520overhead%2520for%2520downstream%250Aapplications.%2520To%2520optimize%2520these%2520primitives%252C%2520we%2520present%2520a%2520differentiable%250Arasterizer%2520that%2520runs%2520efficiently%2520on%2520GPUs%252C%2520allowing%2520end-to-end%2520gradient-based%250Aoptimization%2520while%2520maintaining%2520realtime%2520rendering%2520capabilities.%2520Through%250Aexperiments%2520on%2520real-world%2520datasets%252C%2520we%2520demonstrate%2520comparable%2520performance%2520to%250Astate-of-the-art%2520volumetric%2520methods%2520while%2520requiring%2520fewer%2520primitives%2520to%2520achieve%250Asimilar%2520reconstruction%2520fidelity.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%250Ageometry%2520of%2520volumetric%2520rendering%2520and%2520suggest%2520that%2520adopting%2520explicit%2520polyhedra%250Acan%2520expand%2520the%2520design%2520space%2520of%2520scene%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering&entry.906535625=Nicolas%20von%20L%C3%BCtzow%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20Volumetric%20rendering%20has%20become%20central%20to%20modern%20novel%20view%20synthesis%0Amethods%2C%20which%20use%20differentiable%20rendering%20to%20optimize%203D%20scene%0Arepresentations%20directly%20from%20observed%20views.%20While%20many%20recent%20works%20build%20on%0ANeRF%20or%203D%20Gaussians%2C%20we%20explore%20an%20alternative%20volumetric%20scene%0Arepresentation.%20More%20specifically%2C%20we%20introduce%20two%20new%20scene%20representations%0Abased%20on%20linear%20primitives-octahedra%20and%20tetrahedra-both%20of%20which%20define%0Ahomogeneous%20volumes%20bounded%20by%20triangular%20faces.%20This%20formulation%20aligns%0Anaturally%20with%20standard%20mesh-based%20tools%2C%20minimizing%20overhead%20for%20downstream%0Aapplications.%20To%20optimize%20these%20primitives%2C%20we%20present%20a%20differentiable%0Arasterizer%20that%20runs%20efficiently%20on%20GPUs%2C%20allowing%20end-to-end%20gradient-based%0Aoptimization%20while%20maintaining%20realtime%20rendering%20capabilities.%20Through%0Aexperiments%20on%20real-world%20datasets%2C%20we%20demonstrate%20comparable%20performance%20to%0Astate-of-the-art%20volumetric%20methods%20while%20requiring%20fewer%20primitives%20to%20achieve%0Asimilar%20reconstruction%20fidelity.%20Our%20findings%20provide%20insights%20into%20the%0Ageometry%20of%20volumetric%20rendering%20and%20suggest%20that%20adopting%20explicit%20polyhedra%0Acan%20expand%20the%20design%20space%20of%20scene%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16312v2&entry.124074799=Read"},
{"title": "Large Language Models for cross-language code clone detection", "author": "Micheline B\u00e9n\u00e9dicte Moumoula and Abdoul Kader Kabore and Jacques Klein and Tegawend\u00e9 Bissyande", "abstract": "  With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.\n", "link": "http://arxiv.org/abs/2408.04430v2", "date": "2025-01-28", "relevancy": 2.6064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20cross-language%20code%20clone%20detection&body=Title%3A%20Large%20Language%20Models%20for%20cross-language%20code%20clone%20detection%0AAuthor%3A%20Micheline%20B%C3%A9n%C3%A9dicte%20Moumoula%20and%20Abdoul%20Kader%20Kabore%20and%20Jacques%20Klein%20and%20Tegawend%C3%A9%20Bissyande%0AAbstract%3A%20%20%20With%20the%20involvement%20of%20multiple%20programming%20languages%20in%20modern%20software%0Adevelopment%2C%20cross-lingual%20code%20clone%20detection%20has%20gained%20traction%20within%20the%0Asoftware%20engineering%20community.%20Numerous%20studies%20have%20explored%20this%20topic%2C%0Aproposing%20various%20promising%20approaches.%20Inspired%20by%20the%20significant%20advances%20in%0Amachine%20learning%20in%20recent%20years%2C%20particularly%20Large%20Language%20Models%20%28LLMs%29%2C%0Awhich%20have%20demonstrated%20their%20ability%20to%20tackle%20various%20tasks%2C%20this%20paper%0Arevisits%20cross-lingual%20code%20clone%20detection.%20We%20evaluate%20the%20performance%20of%0Afive%20%2805%29%20LLMs%20and%20eight%20prompts%20%2808%29%20for%20the%20identification%20of%20cross-lingual%0Acode%20clones.%20Additionally%2C%20we%20compare%20these%20results%20against%20two%20baseline%0Amethods.%20Finally%2C%20we%20evaluate%20a%20pre-trained%20embedding%20model%20to%20assess%20the%0Aeffectiveness%20of%20the%20generated%20representations%20for%20classifying%20clone%20and%0Anon-clone%20pairs.%20The%20studies%20involving%20LLMs%20and%20Embedding%20models%20are%20evaluated%0Ausing%20two%20widely%20used%20cross-lingual%20datasets%2C%20XLCoST%20and%20CodeNet.%20Our%20results%0Ashow%20that%20LLMs%20can%20achieve%20high%20F1%20scores%2C%20up%20to%200.99%2C%20for%20straightforward%0Aprogramming%20examples.%20However%2C%20they%20not%20only%20perform%20less%20well%20on%20programs%0Aassociated%20with%20complex%20programming%20challenges%20but%20also%20do%20not%20necessarily%0Aunderstand%20the%20meaning%20of%20%22code%20clones%22%20in%20a%20cross-lingual%20setting.%20We%20show%0Athat%20embedding%20models%20used%20to%20represent%20code%20fragments%20from%20different%0Aprogramming%20languages%20in%20the%20same%20representation%20space%20enable%20the%20training%20of%20a%0Abasic%20classifier%20that%20outperforms%20all%20LLMs%20by%20~1%20and%20~20%20percentage%20points%20on%0Athe%20XLCoST%20and%20CodeNet%20datasets%2C%20respectively.%20This%20finding%20suggests%20that%2C%0Adespite%20the%20apparent%20capabilities%20of%20LLMs%2C%20embeddings%20provided%20by%20embedding%0Amodels%20offer%20suitable%20representations%20to%20achieve%20state-of-the-art%20performance%0Ain%20cross-lingual%20code%20clone%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04430v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520cross-language%2520code%2520clone%2520detection%26entry.906535625%3DMicheline%2520B%25C3%25A9n%25C3%25A9dicte%2520Moumoula%2520and%2520Abdoul%2520Kader%2520Kabore%2520and%2520Jacques%2520Klein%2520and%2520Tegawend%25C3%25A9%2520Bissyande%26entry.1292438233%3D%2520%2520With%2520the%2520involvement%2520of%2520multiple%2520programming%2520languages%2520in%2520modern%2520software%250Adevelopment%252C%2520cross-lingual%2520code%2520clone%2520detection%2520has%2520gained%2520traction%2520within%2520the%250Asoftware%2520engineering%2520community.%2520Numerous%2520studies%2520have%2520explored%2520this%2520topic%252C%250Aproposing%2520various%2520promising%2520approaches.%2520Inspired%2520by%2520the%2520significant%2520advances%2520in%250Amachine%2520learning%2520in%2520recent%2520years%252C%2520particularly%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Awhich%2520have%2520demonstrated%2520their%2520ability%2520to%2520tackle%2520various%2520tasks%252C%2520this%2520paper%250Arevisits%2520cross-lingual%2520code%2520clone%2520detection.%2520We%2520evaluate%2520the%2520performance%2520of%250Afive%2520%252805%2529%2520LLMs%2520and%2520eight%2520prompts%2520%252808%2529%2520for%2520the%2520identification%2520of%2520cross-lingual%250Acode%2520clones.%2520Additionally%252C%2520we%2520compare%2520these%2520results%2520against%2520two%2520baseline%250Amethods.%2520Finally%252C%2520we%2520evaluate%2520a%2520pre-trained%2520embedding%2520model%2520to%2520assess%2520the%250Aeffectiveness%2520of%2520the%2520generated%2520representations%2520for%2520classifying%2520clone%2520and%250Anon-clone%2520pairs.%2520The%2520studies%2520involving%2520LLMs%2520and%2520Embedding%2520models%2520are%2520evaluated%250Ausing%2520two%2520widely%2520used%2520cross-lingual%2520datasets%252C%2520XLCoST%2520and%2520CodeNet.%2520Our%2520results%250Ashow%2520that%2520LLMs%2520can%2520achieve%2520high%2520F1%2520scores%252C%2520up%2520to%25200.99%252C%2520for%2520straightforward%250Aprogramming%2520examples.%2520However%252C%2520they%2520not%2520only%2520perform%2520less%2520well%2520on%2520programs%250Aassociated%2520with%2520complex%2520programming%2520challenges%2520but%2520also%2520do%2520not%2520necessarily%250Aunderstand%2520the%2520meaning%2520of%2520%2522code%2520clones%2522%2520in%2520a%2520cross-lingual%2520setting.%2520We%2520show%250Athat%2520embedding%2520models%2520used%2520to%2520represent%2520code%2520fragments%2520from%2520different%250Aprogramming%2520languages%2520in%2520the%2520same%2520representation%2520space%2520enable%2520the%2520training%2520of%2520a%250Abasic%2520classifier%2520that%2520outperforms%2520all%2520LLMs%2520by%2520~1%2520and%2520~20%2520percentage%2520points%2520on%250Athe%2520XLCoST%2520and%2520CodeNet%2520datasets%252C%2520respectively.%2520This%2520finding%2520suggests%2520that%252C%250Adespite%2520the%2520apparent%2520capabilities%2520of%2520LLMs%252C%2520embeddings%2520provided%2520by%2520embedding%250Amodels%2520offer%2520suitable%2520representations%2520to%2520achieve%2520state-of-the-art%2520performance%250Ain%2520cross-lingual%2520code%2520clone%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04430v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20cross-language%20code%20clone%20detection&entry.906535625=Micheline%20B%C3%A9n%C3%A9dicte%20Moumoula%20and%20Abdoul%20Kader%20Kabore%20and%20Jacques%20Klein%20and%20Tegawend%C3%A9%20Bissyande&entry.1292438233=%20%20With%20the%20involvement%20of%20multiple%20programming%20languages%20in%20modern%20software%0Adevelopment%2C%20cross-lingual%20code%20clone%20detection%20has%20gained%20traction%20within%20the%0Asoftware%20engineering%20community.%20Numerous%20studies%20have%20explored%20this%20topic%2C%0Aproposing%20various%20promising%20approaches.%20Inspired%20by%20the%20significant%20advances%20in%0Amachine%20learning%20in%20recent%20years%2C%20particularly%20Large%20Language%20Models%20%28LLMs%29%2C%0Awhich%20have%20demonstrated%20their%20ability%20to%20tackle%20various%20tasks%2C%20this%20paper%0Arevisits%20cross-lingual%20code%20clone%20detection.%20We%20evaluate%20the%20performance%20of%0Afive%20%2805%29%20LLMs%20and%20eight%20prompts%20%2808%29%20for%20the%20identification%20of%20cross-lingual%0Acode%20clones.%20Additionally%2C%20we%20compare%20these%20results%20against%20two%20baseline%0Amethods.%20Finally%2C%20we%20evaluate%20a%20pre-trained%20embedding%20model%20to%20assess%20the%0Aeffectiveness%20of%20the%20generated%20representations%20for%20classifying%20clone%20and%0Anon-clone%20pairs.%20The%20studies%20involving%20LLMs%20and%20Embedding%20models%20are%20evaluated%0Ausing%20two%20widely%20used%20cross-lingual%20datasets%2C%20XLCoST%20and%20CodeNet.%20Our%20results%0Ashow%20that%20LLMs%20can%20achieve%20high%20F1%20scores%2C%20up%20to%200.99%2C%20for%20straightforward%0Aprogramming%20examples.%20However%2C%20they%20not%20only%20perform%20less%20well%20on%20programs%0Aassociated%20with%20complex%20programming%20challenges%20but%20also%20do%20not%20necessarily%0Aunderstand%20the%20meaning%20of%20%22code%20clones%22%20in%20a%20cross-lingual%20setting.%20We%20show%0Athat%20embedding%20models%20used%20to%20represent%20code%20fragments%20from%20different%0Aprogramming%20languages%20in%20the%20same%20representation%20space%20enable%20the%20training%20of%20a%0Abasic%20classifier%20that%20outperforms%20all%20LLMs%20by%20~1%20and%20~20%20percentage%20points%20on%0Athe%20XLCoST%20and%20CodeNet%20datasets%2C%20respectively.%20This%20finding%20suggests%20that%2C%0Adespite%20the%20apparent%20capabilities%20of%20LLMs%2C%20embeddings%20provided%20by%20embedding%0Amodels%20offer%20suitable%20representations%20to%20achieve%20state-of-the-art%20performance%0Ain%20cross-lingual%20code%20clone%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04430v2&entry.124074799=Read"},
{"title": "NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields", "author": "Amandine Brunetto and Sascha Hornauer and Fabien Moutarde", "abstract": "  Sound plays a major role in human perception. Along with vision, it provides\nessential information for understanding our surroundings. Despite advances in\nneural implicit representations, learning acoustics that align with visual\nscenes remains a challenge. We propose NeRAF, a method that jointly learns\nacoustic and radiance fields. NeRAF synthesizes both novel views and\nspatialized room impulse responses (RIR) at new positions by conditioning the\nacoustic field on 3D scene geometric and appearance priors from the radiance\nfield. The generated RIR can be applied to auralize any audio signal. Each\nmodality can be rendered independently and at spatially distinct positions,\noffering greater versatility. We demonstrate that NeRAF generates high-quality\naudio on SoundSpaces and RAF datasets, achieving significant performance\nimprovements over prior methods while being more data-efficient. Additionally,\nNeRAF enhances novel view synthesis of complex scenes trained with sparse data\nthrough cross-modal learning. NeRAF is designed as a Nerfstudio module,\nproviding convenient access to realistic audio-visual generation.\n", "link": "http://arxiv.org/abs/2405.18213v3", "date": "2025-01-28", "relevancy": 2.6056, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5381}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&body=Title%3A%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields%0AAuthor%3A%20Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde%0AAbstract%3A%20%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception.%20Along%20with%20vision%2C%20it%20provides%0Aessential%20information%20for%20understanding%20our%20surroundings.%20Despite%20advances%20in%0Aneural%20implicit%20representations%2C%20learning%20acoustics%20that%20align%20with%20visual%0Ascenes%20remains%20a%20challenge.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%20learns%0Aacoustic%20and%20radiance%20fields.%20NeRAF%20synthesizes%20both%20novel%20views%20and%0Aspatialized%20room%20impulse%20responses%20%28RIR%29%20at%20new%20positions%20by%20conditioning%20the%0Aacoustic%20field%20on%203D%20scene%20geometric%20and%20appearance%20priors%20from%20the%20radiance%0Afield.%20The%20generated%20RIR%20can%20be%20applied%20to%20auralize%20any%20audio%20signal.%20Each%0Amodality%20can%20be%20rendered%20independently%20and%20at%20spatially%20distinct%20positions%2C%0Aoffering%20greater%20versatility.%20We%20demonstrate%20that%20NeRAF%20generates%20high-quality%0Aaudio%20on%20SoundSpaces%20and%20RAF%20datasets%2C%20achieving%20significant%20performance%0Aimprovements%20over%20prior%20methods%20while%20being%20more%20data-efficient.%20Additionally%2C%0ANeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%20sparse%20data%0Athrough%20cross-modal%20learning.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%2C%0Aproviding%20convenient%20access%20to%20realistic%20audio-visual%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18213v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRAF%253A%25203D%2520Scene%2520Infused%2520Neural%2520Radiance%2520and%2520Acoustic%2520Fields%26entry.906535625%3DAmandine%2520Brunetto%2520and%2520Sascha%2520Hornauer%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3D%2520%2520Sound%2520plays%2520a%2520major%2520role%2520in%2520human%2520perception.%2520Along%2520with%2520vision%252C%2520it%2520provides%250Aessential%2520information%2520for%2520understanding%2520our%2520surroundings.%2520Despite%2520advances%2520in%250Aneural%2520implicit%2520representations%252C%2520learning%2520acoustics%2520that%2520align%2520with%2520visual%250Ascenes%2520remains%2520a%2520challenge.%2520We%2520propose%2520NeRAF%252C%2520a%2520method%2520that%2520jointly%2520learns%250Aacoustic%2520and%2520radiance%2520fields.%2520NeRAF%2520synthesizes%2520both%2520novel%2520views%2520and%250Aspatialized%2520room%2520impulse%2520responses%2520%2528RIR%2529%2520at%2520new%2520positions%2520by%2520conditioning%2520the%250Aacoustic%2520field%2520on%25203D%2520scene%2520geometric%2520and%2520appearance%2520priors%2520from%2520the%2520radiance%250Afield.%2520The%2520generated%2520RIR%2520can%2520be%2520applied%2520to%2520auralize%2520any%2520audio%2520signal.%2520Each%250Amodality%2520can%2520be%2520rendered%2520independently%2520and%2520at%2520spatially%2520distinct%2520positions%252C%250Aoffering%2520greater%2520versatility.%2520We%2520demonstrate%2520that%2520NeRAF%2520generates%2520high-quality%250Aaudio%2520on%2520SoundSpaces%2520and%2520RAF%2520datasets%252C%2520achieving%2520significant%2520performance%250Aimprovements%2520over%2520prior%2520methods%2520while%2520being%2520more%2520data-efficient.%2520Additionally%252C%250ANeRAF%2520enhances%2520novel%2520view%2520synthesis%2520of%2520complex%2520scenes%2520trained%2520with%2520sparse%2520data%250Athrough%2520cross-modal%2520learning.%2520NeRAF%2520is%2520designed%2520as%2520a%2520Nerfstudio%2520module%252C%250Aproviding%2520convenient%2520access%2520to%2520realistic%2520audio-visual%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18213v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&entry.906535625=Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde&entry.1292438233=%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception.%20Along%20with%20vision%2C%20it%20provides%0Aessential%20information%20for%20understanding%20our%20surroundings.%20Despite%20advances%20in%0Aneural%20implicit%20representations%2C%20learning%20acoustics%20that%20align%20with%20visual%0Ascenes%20remains%20a%20challenge.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%20learns%0Aacoustic%20and%20radiance%20fields.%20NeRAF%20synthesizes%20both%20novel%20views%20and%0Aspatialized%20room%20impulse%20responses%20%28RIR%29%20at%20new%20positions%20by%20conditioning%20the%0Aacoustic%20field%20on%203D%20scene%20geometric%20and%20appearance%20priors%20from%20the%20radiance%0Afield.%20The%20generated%20RIR%20can%20be%20applied%20to%20auralize%20any%20audio%20signal.%20Each%0Amodality%20can%20be%20rendered%20independently%20and%20at%20spatially%20distinct%20positions%2C%0Aoffering%20greater%20versatility.%20We%20demonstrate%20that%20NeRAF%20generates%20high-quality%0Aaudio%20on%20SoundSpaces%20and%20RAF%20datasets%2C%20achieving%20significant%20performance%0Aimprovements%20over%20prior%20methods%20while%20being%20more%20data-efficient.%20Additionally%2C%0ANeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%20sparse%20data%0Athrough%20cross-modal%20learning.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%2C%0Aproviding%20convenient%20access%20to%20realistic%20audio-visual%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18213v3&entry.124074799=Read"},
{"title": "Text-to-Image Generation for Vocabulary Learning Using the Keyword\n  Method", "author": "Nuwan T. Attygalle and Matja\u017e Kljun and Aaron Quigley and Klen \u010dOpi\u010d Pucihar and Jens Grubert and Verena Biener and Luis A. Leiva and Juri Yoneyama and Alice Toniolo and Angela Miguel and Hirokazu Kato and Maheshya Weerasinghe", "abstract": "  The 'keyword method' is an effective technique for learning vocabulary of a\nforeign language. It involves creating a memorable visual link between what a\nword means and what its pronunciation in a foreign language sounds like in the\nlearner's native language. However, these memorable visual links remain\nimplicit in the people's mind and are not easy to remember for a large set of\nwords. To enhance the memorisation and recall of the vocabulary, we developed\nan application that combines the keyword method with text-to-image generators\nto externalise the memorable visual links into visuals. These visuals represent\nadditional stimuli during the memorisation process. To explore the\neffectiveness of this approach we first run a pilot study to investigate how\ndifficult it is to externalise the descriptions of mental visualisations of\nmemorable links, by asking participants to write them down. We used these\ndescriptions as prompts for text-to-image generator (DALL-E2) to convert them\ninto images and asked participants to select their favourites. Next, we\ncompared different text-to-image generators (DALL-E2, Midjourney, Stable and\nLatent Diffusion) to evaluate the perceived quality of the generated images by\neach. Despite heterogeneous results, participants mostly preferred images\ngenerated by DALL-E2, which was used also for the final study. In this study,\nwe investigated whether providing such images enhances the retention of\nvocabulary being learned, compared to the keyword method only. Our results\nindicate that people did not encounter difficulties describing their\nvisualisations of memorable links and that providing corresponding images\nsignificantly improves memory retention.\n", "link": "http://arxiv.org/abs/2501.17099v1", "date": "2025-01-28", "relevancy": 2.5863, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5408}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5056}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Image%20Generation%20for%20Vocabulary%20Learning%20Using%20the%20Keyword%0A%20%20Method&body=Title%3A%20Text-to-Image%20Generation%20for%20Vocabulary%20Learning%20Using%20the%20Keyword%0A%20%20Method%0AAuthor%3A%20Nuwan%20T.%20Attygalle%20and%20Matja%C5%BE%20Kljun%20and%20Aaron%20Quigley%20and%20Klen%20%C4%8DOpi%C4%8D%20Pucihar%20and%20Jens%20Grubert%20and%20Verena%20Biener%20and%20Luis%20A.%20Leiva%20and%20Juri%20Yoneyama%20and%20Alice%20Toniolo%20and%20Angela%20Miguel%20and%20Hirokazu%20Kato%20and%20Maheshya%20Weerasinghe%0AAbstract%3A%20%20%20The%20%27keyword%20method%27%20is%20an%20effective%20technique%20for%20learning%20vocabulary%20of%20a%0Aforeign%20language.%20It%20involves%20creating%20a%20memorable%20visual%20link%20between%20what%20a%0Aword%20means%20and%20what%20its%20pronunciation%20in%20a%20foreign%20language%20sounds%20like%20in%20the%0Alearner%27s%20native%20language.%20However%2C%20these%20memorable%20visual%20links%20remain%0Aimplicit%20in%20the%20people%27s%20mind%20and%20are%20not%20easy%20to%20remember%20for%20a%20large%20set%20of%0Awords.%20To%20enhance%20the%20memorisation%20and%20recall%20of%20the%20vocabulary%2C%20we%20developed%0Aan%20application%20that%20combines%20the%20keyword%20method%20with%20text-to-image%20generators%0Ato%20externalise%20the%20memorable%20visual%20links%20into%20visuals.%20These%20visuals%20represent%0Aadditional%20stimuli%20during%20the%20memorisation%20process.%20To%20explore%20the%0Aeffectiveness%20of%20this%20approach%20we%20first%20run%20a%20pilot%20study%20to%20investigate%20how%0Adifficult%20it%20is%20to%20externalise%20the%20descriptions%20of%20mental%20visualisations%20of%0Amemorable%20links%2C%20by%20asking%20participants%20to%20write%20them%20down.%20We%20used%20these%0Adescriptions%20as%20prompts%20for%20text-to-image%20generator%20%28DALL-E2%29%20to%20convert%20them%0Ainto%20images%20and%20asked%20participants%20to%20select%20their%20favourites.%20Next%2C%20we%0Acompared%20different%20text-to-image%20generators%20%28DALL-E2%2C%20Midjourney%2C%20Stable%20and%0ALatent%20Diffusion%29%20to%20evaluate%20the%20perceived%20quality%20of%20the%20generated%20images%20by%0Aeach.%20Despite%20heterogeneous%20results%2C%20participants%20mostly%20preferred%20images%0Agenerated%20by%20DALL-E2%2C%20which%20was%20used%20also%20for%20the%20final%20study.%20In%20this%20study%2C%0Awe%20investigated%20whether%20providing%20such%20images%20enhances%20the%20retention%20of%0Avocabulary%20being%20learned%2C%20compared%20to%20the%20keyword%20method%20only.%20Our%20results%0Aindicate%20that%20people%20did%20not%20encounter%20difficulties%20describing%20their%0Avisualisations%20of%20memorable%20links%20and%20that%20providing%20corresponding%20images%0Asignificantly%20improves%20memory%20retention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Image%2520Generation%2520for%2520Vocabulary%2520Learning%2520Using%2520the%2520Keyword%250A%2520%2520Method%26entry.906535625%3DNuwan%2520T.%2520Attygalle%2520and%2520Matja%25C5%25BE%2520Kljun%2520and%2520Aaron%2520Quigley%2520and%2520Klen%2520%25C4%258DOpi%25C4%258D%2520Pucihar%2520and%2520Jens%2520Grubert%2520and%2520Verena%2520Biener%2520and%2520Luis%2520A.%2520Leiva%2520and%2520Juri%2520Yoneyama%2520and%2520Alice%2520Toniolo%2520and%2520Angela%2520Miguel%2520and%2520Hirokazu%2520Kato%2520and%2520Maheshya%2520Weerasinghe%26entry.1292438233%3D%2520%2520The%2520%2527keyword%2520method%2527%2520is%2520an%2520effective%2520technique%2520for%2520learning%2520vocabulary%2520of%2520a%250Aforeign%2520language.%2520It%2520involves%2520creating%2520a%2520memorable%2520visual%2520link%2520between%2520what%2520a%250Aword%2520means%2520and%2520what%2520its%2520pronunciation%2520in%2520a%2520foreign%2520language%2520sounds%2520like%2520in%2520the%250Alearner%2527s%2520native%2520language.%2520However%252C%2520these%2520memorable%2520visual%2520links%2520remain%250Aimplicit%2520in%2520the%2520people%2527s%2520mind%2520and%2520are%2520not%2520easy%2520to%2520remember%2520for%2520a%2520large%2520set%2520of%250Awords.%2520To%2520enhance%2520the%2520memorisation%2520and%2520recall%2520of%2520the%2520vocabulary%252C%2520we%2520developed%250Aan%2520application%2520that%2520combines%2520the%2520keyword%2520method%2520with%2520text-to-image%2520generators%250Ato%2520externalise%2520the%2520memorable%2520visual%2520links%2520into%2520visuals.%2520These%2520visuals%2520represent%250Aadditional%2520stimuli%2520during%2520the%2520memorisation%2520process.%2520To%2520explore%2520the%250Aeffectiveness%2520of%2520this%2520approach%2520we%2520first%2520run%2520a%2520pilot%2520study%2520to%2520investigate%2520how%250Adifficult%2520it%2520is%2520to%2520externalise%2520the%2520descriptions%2520of%2520mental%2520visualisations%2520of%250Amemorable%2520links%252C%2520by%2520asking%2520participants%2520to%2520write%2520them%2520down.%2520We%2520used%2520these%250Adescriptions%2520as%2520prompts%2520for%2520text-to-image%2520generator%2520%2528DALL-E2%2529%2520to%2520convert%2520them%250Ainto%2520images%2520and%2520asked%2520participants%2520to%2520select%2520their%2520favourites.%2520Next%252C%2520we%250Acompared%2520different%2520text-to-image%2520generators%2520%2528DALL-E2%252C%2520Midjourney%252C%2520Stable%2520and%250ALatent%2520Diffusion%2529%2520to%2520evaluate%2520the%2520perceived%2520quality%2520of%2520the%2520generated%2520images%2520by%250Aeach.%2520Despite%2520heterogeneous%2520results%252C%2520participants%2520mostly%2520preferred%2520images%250Agenerated%2520by%2520DALL-E2%252C%2520which%2520was%2520used%2520also%2520for%2520the%2520final%2520study.%2520In%2520this%2520study%252C%250Awe%2520investigated%2520whether%2520providing%2520such%2520images%2520enhances%2520the%2520retention%2520of%250Avocabulary%2520being%2520learned%252C%2520compared%2520to%2520the%2520keyword%2520method%2520only.%2520Our%2520results%250Aindicate%2520that%2520people%2520did%2520not%2520encounter%2520difficulties%2520describing%2520their%250Avisualisations%2520of%2520memorable%2520links%2520and%2520that%2520providing%2520corresponding%2520images%250Asignificantly%2520improves%2520memory%2520retention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Image%20Generation%20for%20Vocabulary%20Learning%20Using%20the%20Keyword%0A%20%20Method&entry.906535625=Nuwan%20T.%20Attygalle%20and%20Matja%C5%BE%20Kljun%20and%20Aaron%20Quigley%20and%20Klen%20%C4%8DOpi%C4%8D%20Pucihar%20and%20Jens%20Grubert%20and%20Verena%20Biener%20and%20Luis%20A.%20Leiva%20and%20Juri%20Yoneyama%20and%20Alice%20Toniolo%20and%20Angela%20Miguel%20and%20Hirokazu%20Kato%20and%20Maheshya%20Weerasinghe&entry.1292438233=%20%20The%20%27keyword%20method%27%20is%20an%20effective%20technique%20for%20learning%20vocabulary%20of%20a%0Aforeign%20language.%20It%20involves%20creating%20a%20memorable%20visual%20link%20between%20what%20a%0Aword%20means%20and%20what%20its%20pronunciation%20in%20a%20foreign%20language%20sounds%20like%20in%20the%0Alearner%27s%20native%20language.%20However%2C%20these%20memorable%20visual%20links%20remain%0Aimplicit%20in%20the%20people%27s%20mind%20and%20are%20not%20easy%20to%20remember%20for%20a%20large%20set%20of%0Awords.%20To%20enhance%20the%20memorisation%20and%20recall%20of%20the%20vocabulary%2C%20we%20developed%0Aan%20application%20that%20combines%20the%20keyword%20method%20with%20text-to-image%20generators%0Ato%20externalise%20the%20memorable%20visual%20links%20into%20visuals.%20These%20visuals%20represent%0Aadditional%20stimuli%20during%20the%20memorisation%20process.%20To%20explore%20the%0Aeffectiveness%20of%20this%20approach%20we%20first%20run%20a%20pilot%20study%20to%20investigate%20how%0Adifficult%20it%20is%20to%20externalise%20the%20descriptions%20of%20mental%20visualisations%20of%0Amemorable%20links%2C%20by%20asking%20participants%20to%20write%20them%20down.%20We%20used%20these%0Adescriptions%20as%20prompts%20for%20text-to-image%20generator%20%28DALL-E2%29%20to%20convert%20them%0Ainto%20images%20and%20asked%20participants%20to%20select%20their%20favourites.%20Next%2C%20we%0Acompared%20different%20text-to-image%20generators%20%28DALL-E2%2C%20Midjourney%2C%20Stable%20and%0ALatent%20Diffusion%29%20to%20evaluate%20the%20perceived%20quality%20of%20the%20generated%20images%20by%0Aeach.%20Despite%20heterogeneous%20results%2C%20participants%20mostly%20preferred%20images%0Agenerated%20by%20DALL-E2%2C%20which%20was%20used%20also%20for%20the%20final%20study.%20In%20this%20study%2C%0Awe%20investigated%20whether%20providing%20such%20images%20enhances%20the%20retention%20of%0Avocabulary%20being%20learned%2C%20compared%20to%20the%20keyword%20method%20only.%20Our%20results%0Aindicate%20that%20people%20did%20not%20encounter%20difficulties%20describing%20their%0Avisualisations%20of%20memorable%20links%20and%20that%20providing%20corresponding%20images%0Asignificantly%20improves%20memory%20retention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17099v1&entry.124074799=Read"},
{"title": "Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on\n  Advanced Mathematical Problem-Solving", "author": "Evgenii Evstafev", "abstract": "  Large language models (LLMs) excel in many natural language tasks, yet they\nstruggle with complex mathemat-ical problem-solving, particularly in symbolic\nreasoning and maintaining consistent output. This study evalu-ates 10 LLMs with\n7 to 8 billion parameters using 945 competition-level problems from the MATH\ndataset. The focus is on their ability to generate executable Python code as a\nstep in their reasoning process, involving over 9,450 code executions. The\nresearch introduces an evaluation framework using mistral-large-2411 to rate\nanswers on a 5-point scale, which helps address inconsistencies in mathematical\nnotation. It also examines the impact of regenerating output token-by-token on\nrefining results. The findings reveal a significant 34.5% per-formance gap\nbetween the top commercial model (gpt-4o-mini, scoring 83.7%) and the least\neffective open-source model (open-codestral-mamba:v0.1, scoring 49.2%). This\ndisparity is especially noticeable in complex areas like Number Theory. While\ntoken-by-token regeneration slightly improved accuracy (+0.8%) for the model\nllama3.1:8b, it also reduced code execution time by 36.7%, highlighting a\ntrade-off between efficiency and precision. The study also noted a consistent\ntrend where harder problems correlated with lower accuracy across all models.\nDespite using controlled execution environments, less than 1% of the generated\ncode was unsafe, and 3.17% of problems remained unsolved after 10 attempts,\nsuggesting that hybrid reasoning methods may be beneficial.\n", "link": "http://arxiv.org/abs/2501.17084v1", "date": "2025-01-28", "relevancy": 2.4603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-by-Token%20Regeneration%20and%20Domain%20Biases%3A%20A%20Benchmark%20of%20LLMs%20on%0A%20%20Advanced%20Mathematical%20Problem-Solving&body=Title%3A%20Token-by-Token%20Regeneration%20and%20Domain%20Biases%3A%20A%20Benchmark%20of%20LLMs%20on%0A%20%20Advanced%20Mathematical%20Problem-Solving%0AAuthor%3A%20Evgenii%20Evstafev%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20many%20natural%20language%20tasks%2C%20yet%20they%0Astruggle%20with%20complex%20mathemat-ical%20problem-solving%2C%20particularly%20in%20symbolic%0Areasoning%20and%20maintaining%20consistent%20output.%20This%20study%20evalu-ates%2010%20LLMs%20with%0A7%20to%208%20billion%20parameters%20using%20945%20competition-level%20problems%20from%20the%20MATH%0Adataset.%20The%20focus%20is%20on%20their%20ability%20to%20generate%20executable%20Python%20code%20as%20a%0Astep%20in%20their%20reasoning%20process%2C%20involving%20over%209%2C450%20code%20executions.%20The%0Aresearch%20introduces%20an%20evaluation%20framework%20using%20mistral-large-2411%20to%20rate%0Aanswers%20on%20a%205-point%20scale%2C%20which%20helps%20address%20inconsistencies%20in%20mathematical%0Anotation.%20It%20also%20examines%20the%20impact%20of%20regenerating%20output%20token-by-token%20on%0Arefining%20results.%20The%20findings%20reveal%20a%20significant%2034.5%25%20per-formance%20gap%0Abetween%20the%20top%20commercial%20model%20%28gpt-4o-mini%2C%20scoring%2083.7%25%29%20and%20the%20least%0Aeffective%20open-source%20model%20%28open-codestral-mamba%3Av0.1%2C%20scoring%2049.2%25%29.%20This%0Adisparity%20is%20especially%20noticeable%20in%20complex%20areas%20like%20Number%20Theory.%20While%0Atoken-by-token%20regeneration%20slightly%20improved%20accuracy%20%28%2B0.8%25%29%20for%20the%20model%0Allama3.1%3A8b%2C%20it%20also%20reduced%20code%20execution%20time%20by%2036.7%25%2C%20highlighting%20a%0Atrade-off%20between%20efficiency%20and%20precision.%20The%20study%20also%20noted%20a%20consistent%0Atrend%20where%20harder%20problems%20correlated%20with%20lower%20accuracy%20across%20all%20models.%0ADespite%20using%20controlled%20execution%20environments%2C%20less%20than%201%25%20of%20the%20generated%0Acode%20was%20unsafe%2C%20and%203.17%25%20of%20problems%20remained%20unsolved%20after%2010%20attempts%2C%0Asuggesting%20that%20hybrid%20reasoning%20methods%20may%20be%20beneficial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-by-Token%2520Regeneration%2520and%2520Domain%2520Biases%253A%2520A%2520Benchmark%2520of%2520LLMs%2520on%250A%2520%2520Advanced%2520Mathematical%2520Problem-Solving%26entry.906535625%3DEvgenii%2520Evstafev%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520many%2520natural%2520language%2520tasks%252C%2520yet%2520they%250Astruggle%2520with%2520complex%2520mathemat-ical%2520problem-solving%252C%2520particularly%2520in%2520symbolic%250Areasoning%2520and%2520maintaining%2520consistent%2520output.%2520This%2520study%2520evalu-ates%252010%2520LLMs%2520with%250A7%2520to%25208%2520billion%2520parameters%2520using%2520945%2520competition-level%2520problems%2520from%2520the%2520MATH%250Adataset.%2520The%2520focus%2520is%2520on%2520their%2520ability%2520to%2520generate%2520executable%2520Python%2520code%2520as%2520a%250Astep%2520in%2520their%2520reasoning%2520process%252C%2520involving%2520over%25209%252C450%2520code%2520executions.%2520The%250Aresearch%2520introduces%2520an%2520evaluation%2520framework%2520using%2520mistral-large-2411%2520to%2520rate%250Aanswers%2520on%2520a%25205-point%2520scale%252C%2520which%2520helps%2520address%2520inconsistencies%2520in%2520mathematical%250Anotation.%2520It%2520also%2520examines%2520the%2520impact%2520of%2520regenerating%2520output%2520token-by-token%2520on%250Arefining%2520results.%2520The%2520findings%2520reveal%2520a%2520significant%252034.5%2525%2520per-formance%2520gap%250Abetween%2520the%2520top%2520commercial%2520model%2520%2528gpt-4o-mini%252C%2520scoring%252083.7%2525%2529%2520and%2520the%2520least%250Aeffective%2520open-source%2520model%2520%2528open-codestral-mamba%253Av0.1%252C%2520scoring%252049.2%2525%2529.%2520This%250Adisparity%2520is%2520especially%2520noticeable%2520in%2520complex%2520areas%2520like%2520Number%2520Theory.%2520While%250Atoken-by-token%2520regeneration%2520slightly%2520improved%2520accuracy%2520%2528%252B0.8%2525%2529%2520for%2520the%2520model%250Allama3.1%253A8b%252C%2520it%2520also%2520reduced%2520code%2520execution%2520time%2520by%252036.7%2525%252C%2520highlighting%2520a%250Atrade-off%2520between%2520efficiency%2520and%2520precision.%2520The%2520study%2520also%2520noted%2520a%2520consistent%250Atrend%2520where%2520harder%2520problems%2520correlated%2520with%2520lower%2520accuracy%2520across%2520all%2520models.%250ADespite%2520using%2520controlled%2520execution%2520environments%252C%2520less%2520than%25201%2525%2520of%2520the%2520generated%250Acode%2520was%2520unsafe%252C%2520and%25203.17%2525%2520of%2520problems%2520remained%2520unsolved%2520after%252010%2520attempts%252C%250Asuggesting%2520that%2520hybrid%2520reasoning%2520methods%2520may%2520be%2520beneficial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-by-Token%20Regeneration%20and%20Domain%20Biases%3A%20A%20Benchmark%20of%20LLMs%20on%0A%20%20Advanced%20Mathematical%20Problem-Solving&entry.906535625=Evgenii%20Evstafev&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20many%20natural%20language%20tasks%2C%20yet%20they%0Astruggle%20with%20complex%20mathemat-ical%20problem-solving%2C%20particularly%20in%20symbolic%0Areasoning%20and%20maintaining%20consistent%20output.%20This%20study%20evalu-ates%2010%20LLMs%20with%0A7%20to%208%20billion%20parameters%20using%20945%20competition-level%20problems%20from%20the%20MATH%0Adataset.%20The%20focus%20is%20on%20their%20ability%20to%20generate%20executable%20Python%20code%20as%20a%0Astep%20in%20their%20reasoning%20process%2C%20involving%20over%209%2C450%20code%20executions.%20The%0Aresearch%20introduces%20an%20evaluation%20framework%20using%20mistral-large-2411%20to%20rate%0Aanswers%20on%20a%205-point%20scale%2C%20which%20helps%20address%20inconsistencies%20in%20mathematical%0Anotation.%20It%20also%20examines%20the%20impact%20of%20regenerating%20output%20token-by-token%20on%0Arefining%20results.%20The%20findings%20reveal%20a%20significant%2034.5%25%20per-formance%20gap%0Abetween%20the%20top%20commercial%20model%20%28gpt-4o-mini%2C%20scoring%2083.7%25%29%20and%20the%20least%0Aeffective%20open-source%20model%20%28open-codestral-mamba%3Av0.1%2C%20scoring%2049.2%25%29.%20This%0Adisparity%20is%20especially%20noticeable%20in%20complex%20areas%20like%20Number%20Theory.%20While%0Atoken-by-token%20regeneration%20slightly%20improved%20accuracy%20%28%2B0.8%25%29%20for%20the%20model%0Allama3.1%3A8b%2C%20it%20also%20reduced%20code%20execution%20time%20by%2036.7%25%2C%20highlighting%20a%0Atrade-off%20between%20efficiency%20and%20precision.%20The%20study%20also%20noted%20a%20consistent%0Atrend%20where%20harder%20problems%20correlated%20with%20lower%20accuracy%20across%20all%20models.%0ADespite%20using%20controlled%20execution%20environments%2C%20less%20than%201%25%20of%20the%20generated%0Acode%20was%20unsafe%2C%20and%203.17%25%20of%20problems%20remained%20unsolved%20after%2010%20attempts%2C%0Asuggesting%20that%20hybrid%20reasoning%20methods%20may%20be%20beneficial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17084v1&entry.124074799=Read"},
{"title": "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction", "author": "Carl-Leander Henneking and Claas Beger", "abstract": "  Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning.\n", "link": "http://arxiv.org/abs/2501.17112v1", "date": "2025-01-28", "relevancy": 2.4523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Transparent%20Alignment%20Through%20Enhanced%20Inverse%20Constitutional%0A%20%20AI%20for%20Principle%20Extraction&body=Title%3A%20Unlocking%20Transparent%20Alignment%20Through%20Enhanced%20Inverse%20Constitutional%0A%20%20AI%20for%20Principle%20Extraction%0AAuthor%3A%20Carl-Leander%20Henneking%20and%20Claas%20Beger%0AAbstract%3A%20%20%20Traditional%20methods%20for%20aligning%20Large%20Language%20Models%20%28LLMs%29%2C%20such%20as%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20rely%20on%20implicit%20principles%2C%20limiting%20interpretability.%0AConstitutional%20AI%20%28CAI%29%20offers%20an%20explicit%2C%20rule-based%20framework%20for%20guiding%0Amodel%20outputs.%20Building%20on%20this%2C%20we%20refine%20the%20Inverse%20Constitutional%20AI%20%28ICAI%29%0Aalgorithm%2C%20which%20extracts%20constitutions%20from%20preference%20datasets.%20By%20improving%0Aprinciple%20generation%2C%20clustering%2C%20and%20embedding%20processes%2C%20our%20approach%0Aenhances%20the%20accuracy%20and%20generalizability%20of%20extracted%20principles%20across%0Asynthetic%20and%20real-world%20datasets.%20While%20in-context%20alignment%20yields%20modest%0Aimprovements%2C%20our%20results%20highlight%20the%20potential%20of%20these%20principles%20to%20foster%0Amore%20transparent%20and%20adaptable%20alignment%20methods%2C%20offering%20a%20promising%0Adirection%20for%20future%20advancements%20beyond%20traditional%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Transparent%2520Alignment%2520Through%2520Enhanced%2520Inverse%2520Constitutional%250A%2520%2520AI%2520for%2520Principle%2520Extraction%26entry.906535625%3DCarl-Leander%2520Henneking%2520and%2520Claas%2520Beger%26entry.1292438233%3D%2520%2520Traditional%2520methods%2520for%2520aligning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520such%2520as%250AReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520rely%2520on%2520implicit%2520principles%252C%2520limiting%2520interpretability.%250AConstitutional%2520AI%2520%2528CAI%2529%2520offers%2520an%2520explicit%252C%2520rule-based%2520framework%2520for%2520guiding%250Amodel%2520outputs.%2520Building%2520on%2520this%252C%2520we%2520refine%2520the%2520Inverse%2520Constitutional%2520AI%2520%2528ICAI%2529%250Aalgorithm%252C%2520which%2520extracts%2520constitutions%2520from%2520preference%2520datasets.%2520By%2520improving%250Aprinciple%2520generation%252C%2520clustering%252C%2520and%2520embedding%2520processes%252C%2520our%2520approach%250Aenhances%2520the%2520accuracy%2520and%2520generalizability%2520of%2520extracted%2520principles%2520across%250Asynthetic%2520and%2520real-world%2520datasets.%2520While%2520in-context%2520alignment%2520yields%2520modest%250Aimprovements%252C%2520our%2520results%2520highlight%2520the%2520potential%2520of%2520these%2520principles%2520to%2520foster%250Amore%2520transparent%2520and%2520adaptable%2520alignment%2520methods%252C%2520offering%2520a%2520promising%250Adirection%2520for%2520future%2520advancements%2520beyond%2520traditional%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Transparent%20Alignment%20Through%20Enhanced%20Inverse%20Constitutional%0A%20%20AI%20for%20Principle%20Extraction&entry.906535625=Carl-Leander%20Henneking%20and%20Claas%20Beger&entry.1292438233=%20%20Traditional%20methods%20for%20aligning%20Large%20Language%20Models%20%28LLMs%29%2C%20such%20as%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20and%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20rely%20on%20implicit%20principles%2C%20limiting%20interpretability.%0AConstitutional%20AI%20%28CAI%29%20offers%20an%20explicit%2C%20rule-based%20framework%20for%20guiding%0Amodel%20outputs.%20Building%20on%20this%2C%20we%20refine%20the%20Inverse%20Constitutional%20AI%20%28ICAI%29%0Aalgorithm%2C%20which%20extracts%20constitutions%20from%20preference%20datasets.%20By%20improving%0Aprinciple%20generation%2C%20clustering%2C%20and%20embedding%20processes%2C%20our%20approach%0Aenhances%20the%20accuracy%20and%20generalizability%20of%20extracted%20principles%20across%0Asynthetic%20and%20real-world%20datasets.%20While%20in-context%20alignment%20yields%20modest%0Aimprovements%2C%20our%20results%20highlight%20the%20potential%20of%20these%20principles%20to%20foster%0Amore%20transparent%20and%20adaptable%20alignment%20methods%2C%20offering%20a%20promising%0Adirection%20for%20future%20advancements%20beyond%20traditional%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17112v1&entry.124074799=Read"},
{"title": "Few Edges Are Enough: Few-Shot Network Attack Detection with Graph\n  Neural Networks", "author": "Tristan Bilot and Nour El Madhoun and Khaldoun Al Agha and Anis Zouaoui", "abstract": "  Detecting cyberattacks using Graph Neural Networks (GNNs) has seen promising\nresults recently. Most of the state-of-the-art models that leverage these\ntechniques require labeled examples, hard to obtain in many real-world\nscenarios. To address this issue, unsupervised learning and Self-Supervised\nLearning (SSL) have emerged as interesting approaches to reduce the dependency\non labeled data. Nonetheless, these methods tend to yield more anomalous\ndetection algorithms rather than effective attack detection systems. This paper\nintroduces Few Edges Are Enough (FEAE), a GNN-based architecture trained with\nSSL and Few-Shot Learning (FSL) to better distinguish between false positive\nanomalies and actual attacks. To maximize the potential of few-shot examples,\nour model employs a hybrid self-supervised objective that combines the\nadvantages of contrastive-based and reconstruction-based SSL. By leveraging\nonly a minimal number of labeled attack events, represented as attack edges,\nFEAE achieves competitive performance on two well-known network datasets\ncompared to both supervised and unsupervised methods. Remarkably, our\nexperimental results unveil that employing only 1 malicious event for each\nattack type in the dataset is sufficient to achieve substantial improvements.\nFEAE not only outperforms self-supervised GNN baselines but also surpasses some\nsupervised approaches on one of the datasets.\n", "link": "http://arxiv.org/abs/2501.16964v1", "date": "2025-01-28", "relevancy": 2.4124, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5236}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4681}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few%20Edges%20Are%20Enough%3A%20Few-Shot%20Network%20Attack%20Detection%20with%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Few%20Edges%20Are%20Enough%3A%20Few-Shot%20Network%20Attack%20Detection%20with%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Tristan%20Bilot%20and%20Nour%20El%20Madhoun%20and%20Khaldoun%20Al%20Agha%20and%20Anis%20Zouaoui%0AAbstract%3A%20%20%20Detecting%20cyberattacks%20using%20Graph%20Neural%20Networks%20%28GNNs%29%20has%20seen%20promising%0Aresults%20recently.%20Most%20of%20the%20state-of-the-art%20models%20that%20leverage%20these%0Atechniques%20require%20labeled%20examples%2C%20hard%20to%20obtain%20in%20many%20real-world%0Ascenarios.%20To%20address%20this%20issue%2C%20unsupervised%20learning%20and%20Self-Supervised%0ALearning%20%28SSL%29%20have%20emerged%20as%20interesting%20approaches%20to%20reduce%20the%20dependency%0Aon%20labeled%20data.%20Nonetheless%2C%20these%20methods%20tend%20to%20yield%20more%20anomalous%0Adetection%20algorithms%20rather%20than%20effective%20attack%20detection%20systems.%20This%20paper%0Aintroduces%20Few%20Edges%20Are%20Enough%20%28FEAE%29%2C%20a%20GNN-based%20architecture%20trained%20with%0ASSL%20and%20Few-Shot%20Learning%20%28FSL%29%20to%20better%20distinguish%20between%20false%20positive%0Aanomalies%20and%20actual%20attacks.%20To%20maximize%20the%20potential%20of%20few-shot%20examples%2C%0Aour%20model%20employs%20a%20hybrid%20self-supervised%20objective%20that%20combines%20the%0Aadvantages%20of%20contrastive-based%20and%20reconstruction-based%20SSL.%20By%20leveraging%0Aonly%20a%20minimal%20number%20of%20labeled%20attack%20events%2C%20represented%20as%20attack%20edges%2C%0AFEAE%20achieves%20competitive%20performance%20on%20two%20well-known%20network%20datasets%0Acompared%20to%20both%20supervised%20and%20unsupervised%20methods.%20Remarkably%2C%20our%0Aexperimental%20results%20unveil%20that%20employing%20only%201%20malicious%20event%20for%20each%0Aattack%20type%20in%20the%20dataset%20is%20sufficient%20to%20achieve%20substantial%20improvements.%0AFEAE%20not%20only%20outperforms%20self-supervised%20GNN%20baselines%20but%20also%20surpasses%20some%0Asupervised%20approaches%20on%20one%20of%20the%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew%2520Edges%2520Are%2520Enough%253A%2520Few-Shot%2520Network%2520Attack%2520Detection%2520with%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DTristan%2520Bilot%2520and%2520Nour%2520El%2520Madhoun%2520and%2520Khaldoun%2520Al%2520Agha%2520and%2520Anis%2520Zouaoui%26entry.1292438233%3D%2520%2520Detecting%2520cyberattacks%2520using%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520has%2520seen%2520promising%250Aresults%2520recently.%2520Most%2520of%2520the%2520state-of-the-art%2520models%2520that%2520leverage%2520these%250Atechniques%2520require%2520labeled%2520examples%252C%2520hard%2520to%2520obtain%2520in%2520many%2520real-world%250Ascenarios.%2520To%2520address%2520this%2520issue%252C%2520unsupervised%2520learning%2520and%2520Self-Supervised%250ALearning%2520%2528SSL%2529%2520have%2520emerged%2520as%2520interesting%2520approaches%2520to%2520reduce%2520the%2520dependency%250Aon%2520labeled%2520data.%2520Nonetheless%252C%2520these%2520methods%2520tend%2520to%2520yield%2520more%2520anomalous%250Adetection%2520algorithms%2520rather%2520than%2520effective%2520attack%2520detection%2520systems.%2520This%2520paper%250Aintroduces%2520Few%2520Edges%2520Are%2520Enough%2520%2528FEAE%2529%252C%2520a%2520GNN-based%2520architecture%2520trained%2520with%250ASSL%2520and%2520Few-Shot%2520Learning%2520%2528FSL%2529%2520to%2520better%2520distinguish%2520between%2520false%2520positive%250Aanomalies%2520and%2520actual%2520attacks.%2520To%2520maximize%2520the%2520potential%2520of%2520few-shot%2520examples%252C%250Aour%2520model%2520employs%2520a%2520hybrid%2520self-supervised%2520objective%2520that%2520combines%2520the%250Aadvantages%2520of%2520contrastive-based%2520and%2520reconstruction-based%2520SSL.%2520By%2520leveraging%250Aonly%2520a%2520minimal%2520number%2520of%2520labeled%2520attack%2520events%252C%2520represented%2520as%2520attack%2520edges%252C%250AFEAE%2520achieves%2520competitive%2520performance%2520on%2520two%2520well-known%2520network%2520datasets%250Acompared%2520to%2520both%2520supervised%2520and%2520unsupervised%2520methods.%2520Remarkably%252C%2520our%250Aexperimental%2520results%2520unveil%2520that%2520employing%2520only%25201%2520malicious%2520event%2520for%2520each%250Aattack%2520type%2520in%2520the%2520dataset%2520is%2520sufficient%2520to%2520achieve%2520substantial%2520improvements.%250AFEAE%2520not%2520only%2520outperforms%2520self-supervised%2520GNN%2520baselines%2520but%2520also%2520surpasses%2520some%250Asupervised%2520approaches%2520on%2520one%2520of%2520the%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few%20Edges%20Are%20Enough%3A%20Few-Shot%20Network%20Attack%20Detection%20with%20Graph%0A%20%20Neural%20Networks&entry.906535625=Tristan%20Bilot%20and%20Nour%20El%20Madhoun%20and%20Khaldoun%20Al%20Agha%20and%20Anis%20Zouaoui&entry.1292438233=%20%20Detecting%20cyberattacks%20using%20Graph%20Neural%20Networks%20%28GNNs%29%20has%20seen%20promising%0Aresults%20recently.%20Most%20of%20the%20state-of-the-art%20models%20that%20leverage%20these%0Atechniques%20require%20labeled%20examples%2C%20hard%20to%20obtain%20in%20many%20real-world%0Ascenarios.%20To%20address%20this%20issue%2C%20unsupervised%20learning%20and%20Self-Supervised%0ALearning%20%28SSL%29%20have%20emerged%20as%20interesting%20approaches%20to%20reduce%20the%20dependency%0Aon%20labeled%20data.%20Nonetheless%2C%20these%20methods%20tend%20to%20yield%20more%20anomalous%0Adetection%20algorithms%20rather%20than%20effective%20attack%20detection%20systems.%20This%20paper%0Aintroduces%20Few%20Edges%20Are%20Enough%20%28FEAE%29%2C%20a%20GNN-based%20architecture%20trained%20with%0ASSL%20and%20Few-Shot%20Learning%20%28FSL%29%20to%20better%20distinguish%20between%20false%20positive%0Aanomalies%20and%20actual%20attacks.%20To%20maximize%20the%20potential%20of%20few-shot%20examples%2C%0Aour%20model%20employs%20a%20hybrid%20self-supervised%20objective%20that%20combines%20the%0Aadvantages%20of%20contrastive-based%20and%20reconstruction-based%20SSL.%20By%20leveraging%0Aonly%20a%20minimal%20number%20of%20labeled%20attack%20events%2C%20represented%20as%20attack%20edges%2C%0AFEAE%20achieves%20competitive%20performance%20on%20two%20well-known%20network%20datasets%0Acompared%20to%20both%20supervised%20and%20unsupervised%20methods.%20Remarkably%2C%20our%0Aexperimental%20results%20unveil%20that%20employing%20only%201%20malicious%20event%20for%20each%0Aattack%20type%20in%20the%20dataset%20is%20sufficient%20to%20achieve%20substantial%20improvements.%0AFEAE%20not%20only%20outperforms%20self-supervised%20GNN%20baselines%20but%20also%20surpasses%20some%0Asupervised%20approaches%20on%20one%20of%20the%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16964v1&entry.124074799=Read"},
{"title": "Optimizing Large Language Model Training Using FP4 Quantization", "author": "Ruizhe Wang and Yeyun Gong and Xiao Liu and Guoshuai Zhao and Ziyue Yang and Baining Guo and Zhengjun Zha and Peng Cheng", "abstract": "  The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.\n", "link": "http://arxiv.org/abs/2501.17116v1", "date": "2025-01-28", "relevancy": 2.4041, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Large%20Language%20Model%20Training%20Using%20FP4%20Quantization&body=Title%3A%20Optimizing%20Large%20Language%20Model%20Training%20Using%20FP4%20Quantization%0AAuthor%3A%20Ruizhe%20Wang%20and%20Yeyun%20Gong%20and%20Xiao%20Liu%20and%20Guoshuai%20Zhao%20and%20Ziyue%20Yang%20and%20Baining%20Guo%20and%20Zhengjun%20Zha%20and%20Peng%20Cheng%0AAbstract%3A%20%20%20The%20growing%20computational%20demands%20of%20training%20large%20language%20models%20%28LLMs%29%0Anecessitate%20more%20efficient%20methods.%20Quantized%20training%20presents%20a%20promising%0Asolution%20by%20enabling%20low-bit%20arithmetic%20operations%20to%20reduce%20these%20costs.%20While%0AFP8%20precision%20has%20demonstrated%20feasibility%2C%20leveraging%20FP4%20remains%20a%20challenge%0Adue%20to%20significant%20quantization%20errors%20and%20limited%20representational%20capacity.%0AThis%20work%20introduces%20the%20first%20FP4%20training%20framework%20for%20LLMs%2C%20addressing%0Athese%20challenges%20with%20two%20key%20innovations%3A%20a%20differentiable%20quantization%0Aestimator%20for%20precise%20weight%20updates%20and%20an%20outlier%20clamping%20and%20compensation%0Astrategy%20to%20prevent%20activation%20collapse.%20To%20ensure%20stability%2C%20the%20framework%0Aintegrates%20a%20mixed-precision%20training%20scheme%20and%20vector-wise%20quantization.%0AExperimental%20results%20demonstrate%20that%20our%20FP4%20framework%20achieves%20accuracy%0Acomparable%20to%20BF16%20and%20FP8%2C%20with%20minimal%20degradation%2C%20scaling%20effectively%20to%0A13B-parameter%20LLMs%20trained%20on%20up%20to%20100B%20tokens.%20With%20the%20emergence%20of%0Anext-generation%20hardware%20supporting%20FP4%2C%20our%20framework%20sets%20a%20foundation%20for%0Aefficient%20ultra-low%20precision%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Large%2520Language%2520Model%2520Training%2520Using%2520FP4%2520Quantization%26entry.906535625%3DRuizhe%2520Wang%2520and%2520Yeyun%2520Gong%2520and%2520Xiao%2520Liu%2520and%2520Guoshuai%2520Zhao%2520and%2520Ziyue%2520Yang%2520and%2520Baining%2520Guo%2520and%2520Zhengjun%2520Zha%2520and%2520Peng%2520Cheng%26entry.1292438233%3D%2520%2520The%2520growing%2520computational%2520demands%2520of%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%250Anecessitate%2520more%2520efficient%2520methods.%2520Quantized%2520training%2520presents%2520a%2520promising%250Asolution%2520by%2520enabling%2520low-bit%2520arithmetic%2520operations%2520to%2520reduce%2520these%2520costs.%2520While%250AFP8%2520precision%2520has%2520demonstrated%2520feasibility%252C%2520leveraging%2520FP4%2520remains%2520a%2520challenge%250Adue%2520to%2520significant%2520quantization%2520errors%2520and%2520limited%2520representational%2520capacity.%250AThis%2520work%2520introduces%2520the%2520first%2520FP4%2520training%2520framework%2520for%2520LLMs%252C%2520addressing%250Athese%2520challenges%2520with%2520two%2520key%2520innovations%253A%2520a%2520differentiable%2520quantization%250Aestimator%2520for%2520precise%2520weight%2520updates%2520and%2520an%2520outlier%2520clamping%2520and%2520compensation%250Astrategy%2520to%2520prevent%2520activation%2520collapse.%2520To%2520ensure%2520stability%252C%2520the%2520framework%250Aintegrates%2520a%2520mixed-precision%2520training%2520scheme%2520and%2520vector-wise%2520quantization.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520FP4%2520framework%2520achieves%2520accuracy%250Acomparable%2520to%2520BF16%2520and%2520FP8%252C%2520with%2520minimal%2520degradation%252C%2520scaling%2520effectively%2520to%250A13B-parameter%2520LLMs%2520trained%2520on%2520up%2520to%2520100B%2520tokens.%2520With%2520the%2520emergence%2520of%250Anext-generation%2520hardware%2520supporting%2520FP4%252C%2520our%2520framework%2520sets%2520a%2520foundation%2520for%250Aefficient%2520ultra-low%2520precision%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Large%20Language%20Model%20Training%20Using%20FP4%20Quantization&entry.906535625=Ruizhe%20Wang%20and%20Yeyun%20Gong%20and%20Xiao%20Liu%20and%20Guoshuai%20Zhao%20and%20Ziyue%20Yang%20and%20Baining%20Guo%20and%20Zhengjun%20Zha%20and%20Peng%20Cheng&entry.1292438233=%20%20The%20growing%20computational%20demands%20of%20training%20large%20language%20models%20%28LLMs%29%0Anecessitate%20more%20efficient%20methods.%20Quantized%20training%20presents%20a%20promising%0Asolution%20by%20enabling%20low-bit%20arithmetic%20operations%20to%20reduce%20these%20costs.%20While%0AFP8%20precision%20has%20demonstrated%20feasibility%2C%20leveraging%20FP4%20remains%20a%20challenge%0Adue%20to%20significant%20quantization%20errors%20and%20limited%20representational%20capacity.%0AThis%20work%20introduces%20the%20first%20FP4%20training%20framework%20for%20LLMs%2C%20addressing%0Athese%20challenges%20with%20two%20key%20innovations%3A%20a%20differentiable%20quantization%0Aestimator%20for%20precise%20weight%20updates%20and%20an%20outlier%20clamping%20and%20compensation%0Astrategy%20to%20prevent%20activation%20collapse.%20To%20ensure%20stability%2C%20the%20framework%0Aintegrates%20a%20mixed-precision%20training%20scheme%20and%20vector-wise%20quantization.%0AExperimental%20results%20demonstrate%20that%20our%20FP4%20framework%20achieves%20accuracy%0Acomparable%20to%20BF16%20and%20FP8%2C%20with%20minimal%20degradation%2C%20scaling%20effectively%20to%0A13B-parameter%20LLMs%20trained%20on%20up%20to%20100B%20tokens.%20With%20the%20emergence%20of%0Anext-generation%20hardware%20supporting%20FP4%2C%20our%20framework%20sets%20a%20foundation%20for%0Aefficient%20ultra-low%20precision%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17116v1&entry.124074799=Read"},
{"title": "Ultra-high resolution multimodal MRI dense labelled holistic brain atlas", "author": "Jos\u00e9 V. Manj\u00f3n and Sergio Morell-Ortega and Marina Ruiz-Perez and Boris Mansencal and Edern Le Bot and Marien Gadea and Enrique Lanuza and Gwenaelle Catheline and Thomas Tourdias and Vincent Planche and R\u00e9mi Giraud and Denis Rivi\u00e8re and Jean-Fran\u00e7ois Mangin and Nicole Labra-Avila and Roberto Vivo-Hernando and Gregorio Rubio and Fernando Aparici and Maria de la Iglesia-Vaya and Pierrick Coup\u00e9", "abstract": "  In this paper, we introduce holiAtlas, a holistic, multimodal and\nhigh-resolution human brain atlas. This atlas covers different levels of\ndetails of the human brain anatomy, from the organ to the substructure level,\nusing a new dense labelled protocol generated from the fusion of multiple local\nprotocols at different scales. This atlas has been constructed averaging images\nand segmentations of 75 healthy subjects from the Human Connectome Project\ndatabase. Specifically, MR images of T1, T2 and WMn (White Matter nulled)\ncontrasts at 0.125 $mm^{3}$ resolution that were nonlinearly registered and\naveraged using symmetric group-wise normalisation to construct the atlas. At\nthe finest level, the holiAtlas protocol has 350 different labels derived from\n10 different delineation protocols. These labels were grouped at different\nscales to provide a holistic view of the brain at different levels in a\ncoherent and consistent manner. This multiscale and multimodal atlas can be\nused for the development of new ultra-high resolution segmentation methods that\ncan potentially leverage the early detection of neurological disorders.\n", "link": "http://arxiv.org/abs/2501.16879v1", "date": "2025-01-28", "relevancy": 2.384, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4818}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-high%20resolution%20multimodal%20MRI%20dense%20labelled%20holistic%20brain%20atlas&body=Title%3A%20Ultra-high%20resolution%20multimodal%20MRI%20dense%20labelled%20holistic%20brain%20atlas%0AAuthor%3A%20Jos%C3%A9%20V.%20Manj%C3%B3n%20and%20Sergio%20Morell-Ortega%20and%20Marina%20Ruiz-Perez%20and%20Boris%20Mansencal%20and%20Edern%20Le%20Bot%20and%20Marien%20Gadea%20and%20Enrique%20Lanuza%20and%20Gwenaelle%20Catheline%20and%20Thomas%20Tourdias%20and%20Vincent%20Planche%20and%20R%C3%A9mi%20Giraud%20and%20Denis%20Rivi%C3%A8re%20and%20Jean-Fran%C3%A7ois%20Mangin%20and%20Nicole%20Labra-Avila%20and%20Roberto%20Vivo-Hernando%20and%20Gregorio%20Rubio%20and%20Fernando%20Aparici%20and%20Maria%20de%20la%20Iglesia-Vaya%20and%20Pierrick%20Coup%C3%A9%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20holiAtlas%2C%20a%20holistic%2C%20multimodal%20and%0Ahigh-resolution%20human%20brain%20atlas.%20This%20atlas%20covers%20different%20levels%20of%0Adetails%20of%20the%20human%20brain%20anatomy%2C%20from%20the%20organ%20to%20the%20substructure%20level%2C%0Ausing%20a%20new%20dense%20labelled%20protocol%20generated%20from%20the%20fusion%20of%20multiple%20local%0Aprotocols%20at%20different%20scales.%20This%20atlas%20has%20been%20constructed%20averaging%20images%0Aand%20segmentations%20of%2075%20healthy%20subjects%20from%20the%20Human%20Connectome%20Project%0Adatabase.%20Specifically%2C%20MR%20images%20of%20T1%2C%20T2%20and%20WMn%20%28White%20Matter%20nulled%29%0Acontrasts%20at%200.125%20%24mm%5E%7B3%7D%24%20resolution%20that%20were%20nonlinearly%20registered%20and%0Aaveraged%20using%20symmetric%20group-wise%20normalisation%20to%20construct%20the%20atlas.%20At%0Athe%20finest%20level%2C%20the%20holiAtlas%20protocol%20has%20350%20different%20labels%20derived%20from%0A10%20different%20delineation%20protocols.%20These%20labels%20were%20grouped%20at%20different%0Ascales%20to%20provide%20a%20holistic%20view%20of%20the%20brain%20at%20different%20levels%20in%20a%0Acoherent%20and%20consistent%20manner.%20This%20multiscale%20and%20multimodal%20atlas%20can%20be%0Aused%20for%20the%20development%20of%20new%20ultra-high%20resolution%20segmentation%20methods%20that%0Acan%20potentially%20leverage%20the%20early%20detection%20of%20neurological%20disorders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-high%2520resolution%2520multimodal%2520MRI%2520dense%2520labelled%2520holistic%2520brain%2520atlas%26entry.906535625%3DJos%25C3%25A9%2520V.%2520Manj%25C3%25B3n%2520and%2520Sergio%2520Morell-Ortega%2520and%2520Marina%2520Ruiz-Perez%2520and%2520Boris%2520Mansencal%2520and%2520Edern%2520Le%2520Bot%2520and%2520Marien%2520Gadea%2520and%2520Enrique%2520Lanuza%2520and%2520Gwenaelle%2520Catheline%2520and%2520Thomas%2520Tourdias%2520and%2520Vincent%2520Planche%2520and%2520R%25C3%25A9mi%2520Giraud%2520and%2520Denis%2520Rivi%25C3%25A8re%2520and%2520Jean-Fran%25C3%25A7ois%2520Mangin%2520and%2520Nicole%2520Labra-Avila%2520and%2520Roberto%2520Vivo-Hernando%2520and%2520Gregorio%2520Rubio%2520and%2520Fernando%2520Aparici%2520and%2520Maria%2520de%2520la%2520Iglesia-Vaya%2520and%2520Pierrick%2520Coup%25C3%25A9%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520holiAtlas%252C%2520a%2520holistic%252C%2520multimodal%2520and%250Ahigh-resolution%2520human%2520brain%2520atlas.%2520This%2520atlas%2520covers%2520different%2520levels%2520of%250Adetails%2520of%2520the%2520human%2520brain%2520anatomy%252C%2520from%2520the%2520organ%2520to%2520the%2520substructure%2520level%252C%250Ausing%2520a%2520new%2520dense%2520labelled%2520protocol%2520generated%2520from%2520the%2520fusion%2520of%2520multiple%2520local%250Aprotocols%2520at%2520different%2520scales.%2520This%2520atlas%2520has%2520been%2520constructed%2520averaging%2520images%250Aand%2520segmentations%2520of%252075%2520healthy%2520subjects%2520from%2520the%2520Human%2520Connectome%2520Project%250Adatabase.%2520Specifically%252C%2520MR%2520images%2520of%2520T1%252C%2520T2%2520and%2520WMn%2520%2528White%2520Matter%2520nulled%2529%250Acontrasts%2520at%25200.125%2520%2524mm%255E%257B3%257D%2524%2520resolution%2520that%2520were%2520nonlinearly%2520registered%2520and%250Aaveraged%2520using%2520symmetric%2520group-wise%2520normalisation%2520to%2520construct%2520the%2520atlas.%2520At%250Athe%2520finest%2520level%252C%2520the%2520holiAtlas%2520protocol%2520has%2520350%2520different%2520labels%2520derived%2520from%250A10%2520different%2520delineation%2520protocols.%2520These%2520labels%2520were%2520grouped%2520at%2520different%250Ascales%2520to%2520provide%2520a%2520holistic%2520view%2520of%2520the%2520brain%2520at%2520different%2520levels%2520in%2520a%250Acoherent%2520and%2520consistent%2520manner.%2520This%2520multiscale%2520and%2520multimodal%2520atlas%2520can%2520be%250Aused%2520for%2520the%2520development%2520of%2520new%2520ultra-high%2520resolution%2520segmentation%2520methods%2520that%250Acan%2520potentially%2520leverage%2520the%2520early%2520detection%2520of%2520neurological%2520disorders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-high%20resolution%20multimodal%20MRI%20dense%20labelled%20holistic%20brain%20atlas&entry.906535625=Jos%C3%A9%20V.%20Manj%C3%B3n%20and%20Sergio%20Morell-Ortega%20and%20Marina%20Ruiz-Perez%20and%20Boris%20Mansencal%20and%20Edern%20Le%20Bot%20and%20Marien%20Gadea%20and%20Enrique%20Lanuza%20and%20Gwenaelle%20Catheline%20and%20Thomas%20Tourdias%20and%20Vincent%20Planche%20and%20R%C3%A9mi%20Giraud%20and%20Denis%20Rivi%C3%A8re%20and%20Jean-Fran%C3%A7ois%20Mangin%20and%20Nicole%20Labra-Avila%20and%20Roberto%20Vivo-Hernando%20and%20Gregorio%20Rubio%20and%20Fernando%20Aparici%20and%20Maria%20de%20la%20Iglesia-Vaya%20and%20Pierrick%20Coup%C3%A9&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20holiAtlas%2C%20a%20holistic%2C%20multimodal%20and%0Ahigh-resolution%20human%20brain%20atlas.%20This%20atlas%20covers%20different%20levels%20of%0Adetails%20of%20the%20human%20brain%20anatomy%2C%20from%20the%20organ%20to%20the%20substructure%20level%2C%0Ausing%20a%20new%20dense%20labelled%20protocol%20generated%20from%20the%20fusion%20of%20multiple%20local%0Aprotocols%20at%20different%20scales.%20This%20atlas%20has%20been%20constructed%20averaging%20images%0Aand%20segmentations%20of%2075%20healthy%20subjects%20from%20the%20Human%20Connectome%20Project%0Adatabase.%20Specifically%2C%20MR%20images%20of%20T1%2C%20T2%20and%20WMn%20%28White%20Matter%20nulled%29%0Acontrasts%20at%200.125%20%24mm%5E%7B3%7D%24%20resolution%20that%20were%20nonlinearly%20registered%20and%0Aaveraged%20using%20symmetric%20group-wise%20normalisation%20to%20construct%20the%20atlas.%20At%0Athe%20finest%20level%2C%20the%20holiAtlas%20protocol%20has%20350%20different%20labels%20derived%20from%0A10%20different%20delineation%20protocols.%20These%20labels%20were%20grouped%20at%20different%0Ascales%20to%20provide%20a%20holistic%20view%20of%20the%20brain%20at%20different%20levels%20in%20a%0Acoherent%20and%20consistent%20manner.%20This%20multiscale%20and%20multimodal%20atlas%20can%20be%0Aused%20for%20the%20development%20of%20new%20ultra-high%20resolution%20segmentation%20methods%20that%0Acan%20potentially%20leverage%20the%20early%20detection%20of%20neurological%20disorders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16879v1&entry.124074799=Read"},
{"title": "Synthesizing 3D Abstractions by Inverting Procedural Buildings with\n  Transformers", "author": "Max Dax and Jordi Berbel and Jan Stria and Leonidas Guibas and Urs Bergmann", "abstract": "  We generate abstractions of buildings, reflecting the essential aspects of\ntheir geometry and structure, by learning to invert procedural models. We first\nbuild a dataset of abstract procedural building models paired with simulated\npoint clouds and then learn the inverse mapping through a transformer. Given a\npoint cloud, the trained transformer then infers the corresponding abstracted\nbuilding in terms of a programmatic language description. This approach\nleverages expressive procedural models developed for gaming and animation, and\nthereby retains desirable properties such as efficient rendering of the\ninferred abstractions and strong priors for regularity and symmetry. Our\napproach achieves good reconstruction accuracy in terms of geometry and\nstructure, as well as structurally consistent inpainting.\n", "link": "http://arxiv.org/abs/2501.17044v1", "date": "2025-01-28", "relevancy": 2.373, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.609}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.606}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%203D%20Abstractions%20by%20Inverting%20Procedural%20Buildings%20with%0A%20%20Transformers&body=Title%3A%20Synthesizing%203D%20Abstractions%20by%20Inverting%20Procedural%20Buildings%20with%0A%20%20Transformers%0AAuthor%3A%20Max%20Dax%20and%20Jordi%20Berbel%20and%20Jan%20Stria%20and%20Leonidas%20Guibas%20and%20Urs%20Bergmann%0AAbstract%3A%20%20%20We%20generate%20abstractions%20of%20buildings%2C%20reflecting%20the%20essential%20aspects%20of%0Atheir%20geometry%20and%20structure%2C%20by%20learning%20to%20invert%20procedural%20models.%20We%20first%0Abuild%20a%20dataset%20of%20abstract%20procedural%20building%20models%20paired%20with%20simulated%0Apoint%20clouds%20and%20then%20learn%20the%20inverse%20mapping%20through%20a%20transformer.%20Given%20a%0Apoint%20cloud%2C%20the%20trained%20transformer%20then%20infers%20the%20corresponding%20abstracted%0Abuilding%20in%20terms%20of%20a%20programmatic%20language%20description.%20This%20approach%0Aleverages%20expressive%20procedural%20models%20developed%20for%20gaming%20and%20animation%2C%20and%0Athereby%20retains%20desirable%20properties%20such%20as%20efficient%20rendering%20of%20the%0Ainferred%20abstractions%20and%20strong%20priors%20for%20regularity%20and%20symmetry.%20Our%0Aapproach%20achieves%20good%20reconstruction%20accuracy%20in%20terms%20of%20geometry%20and%0Astructure%2C%20as%20well%20as%20structurally%20consistent%20inpainting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%25203D%2520Abstractions%2520by%2520Inverting%2520Procedural%2520Buildings%2520with%250A%2520%2520Transformers%26entry.906535625%3DMax%2520Dax%2520and%2520Jordi%2520Berbel%2520and%2520Jan%2520Stria%2520and%2520Leonidas%2520Guibas%2520and%2520Urs%2520Bergmann%26entry.1292438233%3D%2520%2520We%2520generate%2520abstractions%2520of%2520buildings%252C%2520reflecting%2520the%2520essential%2520aspects%2520of%250Atheir%2520geometry%2520and%2520structure%252C%2520by%2520learning%2520to%2520invert%2520procedural%2520models.%2520We%2520first%250Abuild%2520a%2520dataset%2520of%2520abstract%2520procedural%2520building%2520models%2520paired%2520with%2520simulated%250Apoint%2520clouds%2520and%2520then%2520learn%2520the%2520inverse%2520mapping%2520through%2520a%2520transformer.%2520Given%2520a%250Apoint%2520cloud%252C%2520the%2520trained%2520transformer%2520then%2520infers%2520the%2520corresponding%2520abstracted%250Abuilding%2520in%2520terms%2520of%2520a%2520programmatic%2520language%2520description.%2520This%2520approach%250Aleverages%2520expressive%2520procedural%2520models%2520developed%2520for%2520gaming%2520and%2520animation%252C%2520and%250Athereby%2520retains%2520desirable%2520properties%2520such%2520as%2520efficient%2520rendering%2520of%2520the%250Ainferred%2520abstractions%2520and%2520strong%2520priors%2520for%2520regularity%2520and%2520symmetry.%2520Our%250Aapproach%2520achieves%2520good%2520reconstruction%2520accuracy%2520in%2520terms%2520of%2520geometry%2520and%250Astructure%252C%2520as%2520well%2520as%2520structurally%2520consistent%2520inpainting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%203D%20Abstractions%20by%20Inverting%20Procedural%20Buildings%20with%0A%20%20Transformers&entry.906535625=Max%20Dax%20and%20Jordi%20Berbel%20and%20Jan%20Stria%20and%20Leonidas%20Guibas%20and%20Urs%20Bergmann&entry.1292438233=%20%20We%20generate%20abstractions%20of%20buildings%2C%20reflecting%20the%20essential%20aspects%20of%0Atheir%20geometry%20and%20structure%2C%20by%20learning%20to%20invert%20procedural%20models.%20We%20first%0Abuild%20a%20dataset%20of%20abstract%20procedural%20building%20models%20paired%20with%20simulated%0Apoint%20clouds%20and%20then%20learn%20the%20inverse%20mapping%20through%20a%20transformer.%20Given%20a%0Apoint%20cloud%2C%20the%20trained%20transformer%20then%20infers%20the%20corresponding%20abstracted%0Abuilding%20in%20terms%20of%20a%20programmatic%20language%20description.%20This%20approach%0Aleverages%20expressive%20procedural%20models%20developed%20for%20gaming%20and%20animation%2C%20and%0Athereby%20retains%20desirable%20properties%20such%20as%20efficient%20rendering%20of%20the%0Ainferred%20abstractions%20and%20strong%20priors%20for%20regularity%20and%20symmetry.%20Our%0Aapproach%20achieves%20good%20reconstruction%20accuracy%20in%20terms%20of%20geometry%20and%0Astructure%2C%20as%20well%20as%20structurally%20consistent%20inpainting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17044v1&entry.124074799=Read"},
{"title": "Knowledge Discovery using Unsupervised Cognition", "author": "Alfredo Ibias and Hector Antona and Guillem Ramirez-Miranda and Enric Guinovart", "abstract": "  Knowledge discovery is key to understand and interpret a dataset, as well as\nto find the underlying relationships between its components. Unsupervised\nCognition is a novel unsupervised learning algorithm that focus on modelling\nthe learned data. This paper presents three techniques to perform knowledge\ndiscovery over an already trained Unsupervised Cognition model. Specifically,\nwe present a technique for pattern mining, a technique for feature selection\nbased on the previous pattern mining technique, and a technique for\ndimensionality reduction based on the previous feature selection technique. The\nfinal goal is to distinguish between relevant and irrelevant features and use\nthem to build a model from which to extract meaningful patterns. We evaluated\nour proposals with empirical experiments and found that they overcome the\nstate-of-the-art in knowledge discovery.\n", "link": "http://arxiv.org/abs/2409.20064v2", "date": "2025-01-28", "relevancy": 2.3525, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Discovery%20using%20Unsupervised%20Cognition&body=Title%3A%20Knowledge%20Discovery%20using%20Unsupervised%20Cognition%0AAuthor%3A%20Alfredo%20Ibias%20and%20Hector%20Antona%20and%20Guillem%20Ramirez-Miranda%20and%20Enric%20Guinovart%0AAbstract%3A%20%20%20Knowledge%20discovery%20is%20key%20to%20understand%20and%20interpret%20a%20dataset%2C%20as%20well%20as%0Ato%20find%20the%20underlying%20relationships%20between%20its%20components.%20Unsupervised%0ACognition%20is%20a%20novel%20unsupervised%20learning%20algorithm%20that%20focus%20on%20modelling%0Athe%20learned%20data.%20This%20paper%20presents%20three%20techniques%20to%20perform%20knowledge%0Adiscovery%20over%20an%20already%20trained%20Unsupervised%20Cognition%20model.%20Specifically%2C%0Awe%20present%20a%20technique%20for%20pattern%20mining%2C%20a%20technique%20for%20feature%20selection%0Abased%20on%20the%20previous%20pattern%20mining%20technique%2C%20and%20a%20technique%20for%0Adimensionality%20reduction%20based%20on%20the%20previous%20feature%20selection%20technique.%20The%0Afinal%20goal%20is%20to%20distinguish%20between%20relevant%20and%20irrelevant%20features%20and%20use%0Athem%20to%20build%20a%20model%20from%20which%20to%20extract%20meaningful%20patterns.%20We%20evaluated%0Aour%20proposals%20with%20empirical%20experiments%20and%20found%20that%20they%20overcome%20the%0Astate-of-the-art%20in%20knowledge%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Discovery%2520using%2520Unsupervised%2520Cognition%26entry.906535625%3DAlfredo%2520Ibias%2520and%2520Hector%2520Antona%2520and%2520Guillem%2520Ramirez-Miranda%2520and%2520Enric%2520Guinovart%26entry.1292438233%3D%2520%2520Knowledge%2520discovery%2520is%2520key%2520to%2520understand%2520and%2520interpret%2520a%2520dataset%252C%2520as%2520well%2520as%250Ato%2520find%2520the%2520underlying%2520relationships%2520between%2520its%2520components.%2520Unsupervised%250ACognition%2520is%2520a%2520novel%2520unsupervised%2520learning%2520algorithm%2520that%2520focus%2520on%2520modelling%250Athe%2520learned%2520data.%2520This%2520paper%2520presents%2520three%2520techniques%2520to%2520perform%2520knowledge%250Adiscovery%2520over%2520an%2520already%2520trained%2520Unsupervised%2520Cognition%2520model.%2520Specifically%252C%250Awe%2520present%2520a%2520technique%2520for%2520pattern%2520mining%252C%2520a%2520technique%2520for%2520feature%2520selection%250Abased%2520on%2520the%2520previous%2520pattern%2520mining%2520technique%252C%2520and%2520a%2520technique%2520for%250Adimensionality%2520reduction%2520based%2520on%2520the%2520previous%2520feature%2520selection%2520technique.%2520The%250Afinal%2520goal%2520is%2520to%2520distinguish%2520between%2520relevant%2520and%2520irrelevant%2520features%2520and%2520use%250Athem%2520to%2520build%2520a%2520model%2520from%2520which%2520to%2520extract%2520meaningful%2520patterns.%2520We%2520evaluated%250Aour%2520proposals%2520with%2520empirical%2520experiments%2520and%2520found%2520that%2520they%2520overcome%2520the%250Astate-of-the-art%2520in%2520knowledge%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Discovery%20using%20Unsupervised%20Cognition&entry.906535625=Alfredo%20Ibias%20and%20Hector%20Antona%20and%20Guillem%20Ramirez-Miranda%20and%20Enric%20Guinovart&entry.1292438233=%20%20Knowledge%20discovery%20is%20key%20to%20understand%20and%20interpret%20a%20dataset%2C%20as%20well%20as%0Ato%20find%20the%20underlying%20relationships%20between%20its%20components.%20Unsupervised%0ACognition%20is%20a%20novel%20unsupervised%20learning%20algorithm%20that%20focus%20on%20modelling%0Athe%20learned%20data.%20This%20paper%20presents%20three%20techniques%20to%20perform%20knowledge%0Adiscovery%20over%20an%20already%20trained%20Unsupervised%20Cognition%20model.%20Specifically%2C%0Awe%20present%20a%20technique%20for%20pattern%20mining%2C%20a%20technique%20for%20feature%20selection%0Abased%20on%20the%20previous%20pattern%20mining%20technique%2C%20and%20a%20technique%20for%0Adimensionality%20reduction%20based%20on%20the%20previous%20feature%20selection%20technique.%20The%0Afinal%20goal%20is%20to%20distinguish%20between%20relevant%20and%20irrelevant%20features%20and%20use%0Athem%20to%20build%20a%20model%20from%20which%20to%20extract%20meaningful%20patterns.%20We%20evaluated%0Aour%20proposals%20with%20empirical%20experiments%20and%20found%20that%20they%20overcome%20the%0Astate-of-the-art%20in%20knowledge%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20064v2&entry.124074799=Read"},
{"title": "Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal\n  Video Grounding", "author": "Akash Kumar and Zsolt Kira and Yogesh Singh Rawat", "abstract": "  In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding\n(WSTVG). It is a multimodal task aimed at localizing specific subjects\nspatio-temporally based on textual queries without bounding box supervision.\nMotivated by recent advancements in multi-modal foundation models for grounding\ntasks, we first explore the potential of state-of-the-art object detection\nmodels for WSTVG. Despite their robust zero-shot capabilities, our adaptation\nreveals significant limitations, including inconsistent temporal predictions,\ninadequate understanding of complex queries, and challenges in adapting to\ndifficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a\nnovel approach which is designed to overcome these limitations. CoSPaL\nintegrates three core components: (1) Tubelet Phrase Grounding (TPG), which\nintroduces spatio-temporal prediction by linking textual queries to tubelets;\n(2) Contextual Referral Grounding (CRG), which improves comprehension of\ncomplex queries by extracting contextual information to refine object\nidentification over time; and (3) Self-Paced Scene Understanding (SPS), a\ntraining paradigm that progressively increases task difficulty, enabling the\nmodel to adapt to complex scenarios by transitioning from coarse to\nfine-grained understanding.\n", "link": "http://arxiv.org/abs/2501.17053v1", "date": "2025-01-28", "relevancy": 2.3226, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6114}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5663}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Self-paced%20Learning%20for%20Weakly%20Supervised%20Spatio-Temporal%0A%20%20Video%20Grounding&body=Title%3A%20Contextual%20Self-paced%20Learning%20for%20Weakly%20Supervised%20Spatio-Temporal%0A%20%20Video%20Grounding%0AAuthor%3A%20Akash%20Kumar%20and%20Zsolt%20Kira%20and%20Yogesh%20Singh%20Rawat%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20focus%20on%20Weakly%20Supervised%20Spatio-Temporal%20Video%20Grounding%0A%28WSTVG%29.%20It%20is%20a%20multimodal%20task%20aimed%20at%20localizing%20specific%20subjects%0Aspatio-temporally%20based%20on%20textual%20queries%20without%20bounding%20box%20supervision.%0AMotivated%20by%20recent%20advancements%20in%20multi-modal%20foundation%20models%20for%20grounding%0Atasks%2C%20we%20first%20explore%20the%20potential%20of%20state-of-the-art%20object%20detection%0Amodels%20for%20WSTVG.%20Despite%20their%20robust%20zero-shot%20capabilities%2C%20our%20adaptation%0Areveals%20significant%20limitations%2C%20including%20inconsistent%20temporal%20predictions%2C%0Ainadequate%20understanding%20of%20complex%20queries%2C%20and%20challenges%20in%20adapting%20to%0Adifficult%20scenarios.%20We%20propose%20CoSPaL%20%28Contextual%20Self-Paced%20Learning%29%2C%20a%0Anovel%20approach%20which%20is%20designed%20to%20overcome%20these%20limitations.%20CoSPaL%0Aintegrates%20three%20core%20components%3A%20%281%29%20Tubelet%20Phrase%20Grounding%20%28TPG%29%2C%20which%0Aintroduces%20spatio-temporal%20prediction%20by%20linking%20textual%20queries%20to%20tubelets%3B%0A%282%29%20Contextual%20Referral%20Grounding%20%28CRG%29%2C%20which%20improves%20comprehension%20of%0Acomplex%20queries%20by%20extracting%20contextual%20information%20to%20refine%20object%0Aidentification%20over%20time%3B%20and%20%283%29%20Self-Paced%20Scene%20Understanding%20%28SPS%29%2C%20a%0Atraining%20paradigm%20that%20progressively%20increases%20task%20difficulty%2C%20enabling%20the%0Amodel%20to%20adapt%20to%20complex%20scenarios%20by%20transitioning%20from%20coarse%20to%0Afine-grained%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Self-paced%2520Learning%2520for%2520Weakly%2520Supervised%2520Spatio-Temporal%250A%2520%2520Video%2520Grounding%26entry.906535625%3DAkash%2520Kumar%2520and%2520Zsolt%2520Kira%2520and%2520Yogesh%2520Singh%2520Rawat%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520Weakly%2520Supervised%2520Spatio-Temporal%2520Video%2520Grounding%250A%2528WSTVG%2529.%2520It%2520is%2520a%2520multimodal%2520task%2520aimed%2520at%2520localizing%2520specific%2520subjects%250Aspatio-temporally%2520based%2520on%2520textual%2520queries%2520without%2520bounding%2520box%2520supervision.%250AMotivated%2520by%2520recent%2520advancements%2520in%2520multi-modal%2520foundation%2520models%2520for%2520grounding%250Atasks%252C%2520we%2520first%2520explore%2520the%2520potential%2520of%2520state-of-the-art%2520object%2520detection%250Amodels%2520for%2520WSTVG.%2520Despite%2520their%2520robust%2520zero-shot%2520capabilities%252C%2520our%2520adaptation%250Areveals%2520significant%2520limitations%252C%2520including%2520inconsistent%2520temporal%2520predictions%252C%250Ainadequate%2520understanding%2520of%2520complex%2520queries%252C%2520and%2520challenges%2520in%2520adapting%2520to%250Adifficult%2520scenarios.%2520We%2520propose%2520CoSPaL%2520%2528Contextual%2520Self-Paced%2520Learning%2529%252C%2520a%250Anovel%2520approach%2520which%2520is%2520designed%2520to%2520overcome%2520these%2520limitations.%2520CoSPaL%250Aintegrates%2520three%2520core%2520components%253A%2520%25281%2529%2520Tubelet%2520Phrase%2520Grounding%2520%2528TPG%2529%252C%2520which%250Aintroduces%2520spatio-temporal%2520prediction%2520by%2520linking%2520textual%2520queries%2520to%2520tubelets%253B%250A%25282%2529%2520Contextual%2520Referral%2520Grounding%2520%2528CRG%2529%252C%2520which%2520improves%2520comprehension%2520of%250Acomplex%2520queries%2520by%2520extracting%2520contextual%2520information%2520to%2520refine%2520object%250Aidentification%2520over%2520time%253B%2520and%2520%25283%2529%2520Self-Paced%2520Scene%2520Understanding%2520%2528SPS%2529%252C%2520a%250Atraining%2520paradigm%2520that%2520progressively%2520increases%2520task%2520difficulty%252C%2520enabling%2520the%250Amodel%2520to%2520adapt%2520to%2520complex%2520scenarios%2520by%2520transitioning%2520from%2520coarse%2520to%250Afine-grained%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Self-paced%20Learning%20for%20Weakly%20Supervised%20Spatio-Temporal%0A%20%20Video%20Grounding&entry.906535625=Akash%20Kumar%20and%20Zsolt%20Kira%20and%20Yogesh%20Singh%20Rawat&entry.1292438233=%20%20In%20this%20work%2C%20we%20focus%20on%20Weakly%20Supervised%20Spatio-Temporal%20Video%20Grounding%0A%28WSTVG%29.%20It%20is%20a%20multimodal%20task%20aimed%20at%20localizing%20specific%20subjects%0Aspatio-temporally%20based%20on%20textual%20queries%20without%20bounding%20box%20supervision.%0AMotivated%20by%20recent%20advancements%20in%20multi-modal%20foundation%20models%20for%20grounding%0Atasks%2C%20we%20first%20explore%20the%20potential%20of%20state-of-the-art%20object%20detection%0Amodels%20for%20WSTVG.%20Despite%20their%20robust%20zero-shot%20capabilities%2C%20our%20adaptation%0Areveals%20significant%20limitations%2C%20including%20inconsistent%20temporal%20predictions%2C%0Ainadequate%20understanding%20of%20complex%20queries%2C%20and%20challenges%20in%20adapting%20to%0Adifficult%20scenarios.%20We%20propose%20CoSPaL%20%28Contextual%20Self-Paced%20Learning%29%2C%20a%0Anovel%20approach%20which%20is%20designed%20to%20overcome%20these%20limitations.%20CoSPaL%0Aintegrates%20three%20core%20components%3A%20%281%29%20Tubelet%20Phrase%20Grounding%20%28TPG%29%2C%20which%0Aintroduces%20spatio-temporal%20prediction%20by%20linking%20textual%20queries%20to%20tubelets%3B%0A%282%29%20Contextual%20Referral%20Grounding%20%28CRG%29%2C%20which%20improves%20comprehension%20of%0Acomplex%20queries%20by%20extracting%20contextual%20information%20to%20refine%20object%0Aidentification%20over%20time%3B%20and%20%283%29%20Self-Paced%20Scene%20Understanding%20%28SPS%29%2C%20a%0Atraining%20paradigm%20that%20progressively%20increases%20task%20difficulty%2C%20enabling%20the%0Amodel%20to%20adapt%20to%20complex%20scenarios%20by%20transitioning%20from%20coarse%20to%0Afine-grained%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17053v1&entry.124074799=Read"},
{"title": "Solving Roughly Forced Nonlinear PDEs via Misspecified Kernel Methods\n  and Neural Networks", "author": "Matthieu Darcy and Edoardo Calvello and Ricardo Baptista and Houman Owhadi and Andrew M. Stuart and Xianjin Yang", "abstract": "  We consider the use of Gaussian Processes (GPs) or Neural Networks (NNs) to\nnumerically approximate the solutions to nonlinear partial differential\nequations (PDEs) with rough forcing or source terms, which commonly arise as\npathwise solutions to stochastic PDEs. Kernel methods have recently been\ngeneralized to solve nonlinear PDEs by approximating their solutions as the\nmaximum a posteriori estimator of GPs that are conditioned to satisfy the PDE\nat a finite set of collocation points. The convergence and error guarantees of\nthese methods, however, rely on the PDE being defined in a classical sense and\nits solution possessing sufficient regularity to belong to the associated\nreproducing kernel Hilbert space. We propose a generalization of these methods\nto handle roughly forced nonlinear PDEs while preserving convergence guarantees\nwith an oversmoothing GP kernel that is misspecified relative to the true\nsolution's regularity. This is achieved by conditioning a regular GP to satisfy\nthe PDE with a modified source term in a weak sense (when integrated against a\nfinite number of test functions). This is equivalent to replacing the empirical\n$L^2$-loss on the PDE constraint by an empirical negative-Sobolev norm. We\nfurther show that this loss function can be used to extend physics-informed\nneural networks (PINNs) to stochastic equations, thereby resulting in a new\nNN-based variant termed Negative Sobolev Norm-PINN (NeS-PINN).\n", "link": "http://arxiv.org/abs/2501.17110v1", "date": "2025-01-28", "relevancy": 2.2757, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4561}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Roughly%20Forced%20Nonlinear%20PDEs%20via%20Misspecified%20Kernel%20Methods%0A%20%20and%20Neural%20Networks&body=Title%3A%20Solving%20Roughly%20Forced%20Nonlinear%20PDEs%20via%20Misspecified%20Kernel%20Methods%0A%20%20and%20Neural%20Networks%0AAuthor%3A%20Matthieu%20Darcy%20and%20Edoardo%20Calvello%20and%20Ricardo%20Baptista%20and%20Houman%20Owhadi%20and%20Andrew%20M.%20Stuart%20and%20Xianjin%20Yang%0AAbstract%3A%20%20%20We%20consider%20the%20use%20of%20Gaussian%20Processes%20%28GPs%29%20or%20Neural%20Networks%20%28NNs%29%20to%0Anumerically%20approximate%20the%20solutions%20to%20nonlinear%20partial%20differential%0Aequations%20%28PDEs%29%20with%20rough%20forcing%20or%20source%20terms%2C%20which%20commonly%20arise%20as%0Apathwise%20solutions%20to%20stochastic%20PDEs.%20Kernel%20methods%20have%20recently%20been%0Ageneralized%20to%20solve%20nonlinear%20PDEs%20by%20approximating%20their%20solutions%20as%20the%0Amaximum%20a%20posteriori%20estimator%20of%20GPs%20that%20are%20conditioned%20to%20satisfy%20the%20PDE%0Aat%20a%20finite%20set%20of%20collocation%20points.%20The%20convergence%20and%20error%20guarantees%20of%0Athese%20methods%2C%20however%2C%20rely%20on%20the%20PDE%20being%20defined%20in%20a%20classical%20sense%20and%0Aits%20solution%20possessing%20sufficient%20regularity%20to%20belong%20to%20the%20associated%0Areproducing%20kernel%20Hilbert%20space.%20We%20propose%20a%20generalization%20of%20these%20methods%0Ato%20handle%20roughly%20forced%20nonlinear%20PDEs%20while%20preserving%20convergence%20guarantees%0Awith%20an%20oversmoothing%20GP%20kernel%20that%20is%20misspecified%20relative%20to%20the%20true%0Asolution%27s%20regularity.%20This%20is%20achieved%20by%20conditioning%20a%20regular%20GP%20to%20satisfy%0Athe%20PDE%20with%20a%20modified%20source%20term%20in%20a%20weak%20sense%20%28when%20integrated%20against%20a%0Afinite%20number%20of%20test%20functions%29.%20This%20is%20equivalent%20to%20replacing%20the%20empirical%0A%24L%5E2%24-loss%20on%20the%20PDE%20constraint%20by%20an%20empirical%20negative-Sobolev%20norm.%20We%0Afurther%20show%20that%20this%20loss%20function%20can%20be%20used%20to%20extend%20physics-informed%0Aneural%20networks%20%28PINNs%29%20to%20stochastic%20equations%2C%20thereby%20resulting%20in%20a%20new%0ANN-based%20variant%20termed%20Negative%20Sobolev%20Norm-PINN%20%28NeS-PINN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Roughly%2520Forced%2520Nonlinear%2520PDEs%2520via%2520Misspecified%2520Kernel%2520Methods%250A%2520%2520and%2520Neural%2520Networks%26entry.906535625%3DMatthieu%2520Darcy%2520and%2520Edoardo%2520Calvello%2520and%2520Ricardo%2520Baptista%2520and%2520Houman%2520Owhadi%2520and%2520Andrew%2520M.%2520Stuart%2520and%2520Xianjin%2520Yang%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520use%2520of%2520Gaussian%2520Processes%2520%2528GPs%2529%2520or%2520Neural%2520Networks%2520%2528NNs%2529%2520to%250Anumerically%2520approximate%2520the%2520solutions%2520to%2520nonlinear%2520partial%2520differential%250Aequations%2520%2528PDEs%2529%2520with%2520rough%2520forcing%2520or%2520source%2520terms%252C%2520which%2520commonly%2520arise%2520as%250Apathwise%2520solutions%2520to%2520stochastic%2520PDEs.%2520Kernel%2520methods%2520have%2520recently%2520been%250Ageneralized%2520to%2520solve%2520nonlinear%2520PDEs%2520by%2520approximating%2520their%2520solutions%2520as%2520the%250Amaximum%2520a%2520posteriori%2520estimator%2520of%2520GPs%2520that%2520are%2520conditioned%2520to%2520satisfy%2520the%2520PDE%250Aat%2520a%2520finite%2520set%2520of%2520collocation%2520points.%2520The%2520convergence%2520and%2520error%2520guarantees%2520of%250Athese%2520methods%252C%2520however%252C%2520rely%2520on%2520the%2520PDE%2520being%2520defined%2520in%2520a%2520classical%2520sense%2520and%250Aits%2520solution%2520possessing%2520sufficient%2520regularity%2520to%2520belong%2520to%2520the%2520associated%250Areproducing%2520kernel%2520Hilbert%2520space.%2520We%2520propose%2520a%2520generalization%2520of%2520these%2520methods%250Ato%2520handle%2520roughly%2520forced%2520nonlinear%2520PDEs%2520while%2520preserving%2520convergence%2520guarantees%250Awith%2520an%2520oversmoothing%2520GP%2520kernel%2520that%2520is%2520misspecified%2520relative%2520to%2520the%2520true%250Asolution%2527s%2520regularity.%2520This%2520is%2520achieved%2520by%2520conditioning%2520a%2520regular%2520GP%2520to%2520satisfy%250Athe%2520PDE%2520with%2520a%2520modified%2520source%2520term%2520in%2520a%2520weak%2520sense%2520%2528when%2520integrated%2520against%2520a%250Afinite%2520number%2520of%2520test%2520functions%2529.%2520This%2520is%2520equivalent%2520to%2520replacing%2520the%2520empirical%250A%2524L%255E2%2524-loss%2520on%2520the%2520PDE%2520constraint%2520by%2520an%2520empirical%2520negative-Sobolev%2520norm.%2520We%250Afurther%2520show%2520that%2520this%2520loss%2520function%2520can%2520be%2520used%2520to%2520extend%2520physics-informed%250Aneural%2520networks%2520%2528PINNs%2529%2520to%2520stochastic%2520equations%252C%2520thereby%2520resulting%2520in%2520a%2520new%250ANN-based%2520variant%2520termed%2520Negative%2520Sobolev%2520Norm-PINN%2520%2528NeS-PINN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Roughly%20Forced%20Nonlinear%20PDEs%20via%20Misspecified%20Kernel%20Methods%0A%20%20and%20Neural%20Networks&entry.906535625=Matthieu%20Darcy%20and%20Edoardo%20Calvello%20and%20Ricardo%20Baptista%20and%20Houman%20Owhadi%20and%20Andrew%20M.%20Stuart%20and%20Xianjin%20Yang&entry.1292438233=%20%20We%20consider%20the%20use%20of%20Gaussian%20Processes%20%28GPs%29%20or%20Neural%20Networks%20%28NNs%29%20to%0Anumerically%20approximate%20the%20solutions%20to%20nonlinear%20partial%20differential%0Aequations%20%28PDEs%29%20with%20rough%20forcing%20or%20source%20terms%2C%20which%20commonly%20arise%20as%0Apathwise%20solutions%20to%20stochastic%20PDEs.%20Kernel%20methods%20have%20recently%20been%0Ageneralized%20to%20solve%20nonlinear%20PDEs%20by%20approximating%20their%20solutions%20as%20the%0Amaximum%20a%20posteriori%20estimator%20of%20GPs%20that%20are%20conditioned%20to%20satisfy%20the%20PDE%0Aat%20a%20finite%20set%20of%20collocation%20points.%20The%20convergence%20and%20error%20guarantees%20of%0Athese%20methods%2C%20however%2C%20rely%20on%20the%20PDE%20being%20defined%20in%20a%20classical%20sense%20and%0Aits%20solution%20possessing%20sufficient%20regularity%20to%20belong%20to%20the%20associated%0Areproducing%20kernel%20Hilbert%20space.%20We%20propose%20a%20generalization%20of%20these%20methods%0Ato%20handle%20roughly%20forced%20nonlinear%20PDEs%20while%20preserving%20convergence%20guarantees%0Awith%20an%20oversmoothing%20GP%20kernel%20that%20is%20misspecified%20relative%20to%20the%20true%0Asolution%27s%20regularity.%20This%20is%20achieved%20by%20conditioning%20a%20regular%20GP%20to%20satisfy%0Athe%20PDE%20with%20a%20modified%20source%20term%20in%20a%20weak%20sense%20%28when%20integrated%20against%20a%0Afinite%20number%20of%20test%20functions%29.%20This%20is%20equivalent%20to%20replacing%20the%20empirical%0A%24L%5E2%24-loss%20on%20the%20PDE%20constraint%20by%20an%20empirical%20negative-Sobolev%20norm.%20We%0Afurther%20show%20that%20this%20loss%20function%20can%20be%20used%20to%20extend%20physics-informed%0Aneural%20networks%20%28PINNs%29%20to%20stochastic%20equations%2C%20thereby%20resulting%20in%20a%20new%0ANN-based%20variant%20termed%20Negative%20Sobolev%20Norm-PINN%20%28NeS-PINN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17110v1&entry.124074799=Read"},
{"title": "Extending Information Bottleneck Attribution to Video Sequences", "author": "Veronika Solopova and Lucas Schmidt and Dorothea Kolossa", "abstract": "  We introduce VIBA, a novel approach for explainable video classification by\nadapting Information Bottlenecks for Attribution (IBA) to video sequences.\nWhile most traditional explainability methods are designed for image models,\nour IBA framework addresses the need for explainability in temporal models used\nfor video analysis. To demonstrate its effectiveness, we apply VIBA to video\ndeepfake detection, testing it on two architectures: the Xception model for\nspatial features and a VGG11-based model for capturing motion dynamics through\noptical flow. Using a custom dataset that reflects recent deepfake generation\ntechniques, we adapt IBA to create relevance and optical flow maps, visually\nhighlighting manipulated regions and motion inconsistencies. Our results show\nthat VIBA generates temporally and spatially consistent explanations, which\nalign closely with human annotations, thus providing interpretability for video\nclassification and particularly for deepfake detection.\n", "link": "http://arxiv.org/abs/2501.16889v1", "date": "2025-01-28", "relevancy": 2.2622, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5779}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5737}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20Information%20Bottleneck%20Attribution%20to%20Video%20Sequences&body=Title%3A%20Extending%20Information%20Bottleneck%20Attribution%20to%20Video%20Sequences%0AAuthor%3A%20Veronika%20Solopova%20and%20Lucas%20Schmidt%20and%20Dorothea%20Kolossa%0AAbstract%3A%20%20%20We%20introduce%20VIBA%2C%20a%20novel%20approach%20for%20explainable%20video%20classification%20by%0Aadapting%20Information%20Bottlenecks%20for%20Attribution%20%28IBA%29%20to%20video%20sequences.%0AWhile%20most%20traditional%20explainability%20methods%20are%20designed%20for%20image%20models%2C%0Aour%20IBA%20framework%20addresses%20the%20need%20for%20explainability%20in%20temporal%20models%20used%0Afor%20video%20analysis.%20To%20demonstrate%20its%20effectiveness%2C%20we%20apply%20VIBA%20to%20video%0Adeepfake%20detection%2C%20testing%20it%20on%20two%20architectures%3A%20the%20Xception%20model%20for%0Aspatial%20features%20and%20a%20VGG11-based%20model%20for%20capturing%20motion%20dynamics%20through%0Aoptical%20flow.%20Using%20a%20custom%20dataset%20that%20reflects%20recent%20deepfake%20generation%0Atechniques%2C%20we%20adapt%20IBA%20to%20create%20relevance%20and%20optical%20flow%20maps%2C%20visually%0Ahighlighting%20manipulated%20regions%20and%20motion%20inconsistencies.%20Our%20results%20show%0Athat%20VIBA%20generates%20temporally%20and%20spatially%20consistent%20explanations%2C%20which%0Aalign%20closely%20with%20human%20annotations%2C%20thus%20providing%20interpretability%20for%20video%0Aclassification%20and%20particularly%20for%20deepfake%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520Information%2520Bottleneck%2520Attribution%2520to%2520Video%2520Sequences%26entry.906535625%3DVeronika%2520Solopova%2520and%2520Lucas%2520Schmidt%2520and%2520Dorothea%2520Kolossa%26entry.1292438233%3D%2520%2520We%2520introduce%2520VIBA%252C%2520a%2520novel%2520approach%2520for%2520explainable%2520video%2520classification%2520by%250Aadapting%2520Information%2520Bottlenecks%2520for%2520Attribution%2520%2528IBA%2529%2520to%2520video%2520sequences.%250AWhile%2520most%2520traditional%2520explainability%2520methods%2520are%2520designed%2520for%2520image%2520models%252C%250Aour%2520IBA%2520framework%2520addresses%2520the%2520need%2520for%2520explainability%2520in%2520temporal%2520models%2520used%250Afor%2520video%2520analysis.%2520To%2520demonstrate%2520its%2520effectiveness%252C%2520we%2520apply%2520VIBA%2520to%2520video%250Adeepfake%2520detection%252C%2520testing%2520it%2520on%2520two%2520architectures%253A%2520the%2520Xception%2520model%2520for%250Aspatial%2520features%2520and%2520a%2520VGG11-based%2520model%2520for%2520capturing%2520motion%2520dynamics%2520through%250Aoptical%2520flow.%2520Using%2520a%2520custom%2520dataset%2520that%2520reflects%2520recent%2520deepfake%2520generation%250Atechniques%252C%2520we%2520adapt%2520IBA%2520to%2520create%2520relevance%2520and%2520optical%2520flow%2520maps%252C%2520visually%250Ahighlighting%2520manipulated%2520regions%2520and%2520motion%2520inconsistencies.%2520Our%2520results%2520show%250Athat%2520VIBA%2520generates%2520temporally%2520and%2520spatially%2520consistent%2520explanations%252C%2520which%250Aalign%2520closely%2520with%2520human%2520annotations%252C%2520thus%2520providing%2520interpretability%2520for%2520video%250Aclassification%2520and%2520particularly%2520for%2520deepfake%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20Information%20Bottleneck%20Attribution%20to%20Video%20Sequences&entry.906535625=Veronika%20Solopova%20and%20Lucas%20Schmidt%20and%20Dorothea%20Kolossa&entry.1292438233=%20%20We%20introduce%20VIBA%2C%20a%20novel%20approach%20for%20explainable%20video%20classification%20by%0Aadapting%20Information%20Bottlenecks%20for%20Attribution%20%28IBA%29%20to%20video%20sequences.%0AWhile%20most%20traditional%20explainability%20methods%20are%20designed%20for%20image%20models%2C%0Aour%20IBA%20framework%20addresses%20the%20need%20for%20explainability%20in%20temporal%20models%20used%0Afor%20video%20analysis.%20To%20demonstrate%20its%20effectiveness%2C%20we%20apply%20VIBA%20to%20video%0Adeepfake%20detection%2C%20testing%20it%20on%20two%20architectures%3A%20the%20Xception%20model%20for%0Aspatial%20features%20and%20a%20VGG11-based%20model%20for%20capturing%20motion%20dynamics%20through%0Aoptical%20flow.%20Using%20a%20custom%20dataset%20that%20reflects%20recent%20deepfake%20generation%0Atechniques%2C%20we%20adapt%20IBA%20to%20create%20relevance%20and%20optical%20flow%20maps%2C%20visually%0Ahighlighting%20manipulated%20regions%20and%20motion%20inconsistencies.%20Our%20results%20show%0Athat%20VIBA%20generates%20temporally%20and%20spatially%20consistent%20explanations%2C%20which%0Aalign%20closely%20with%20human%20annotations%2C%20thus%20providing%20interpretability%20for%20video%0Aclassification%20and%20particularly%20for%20deepfake%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16889v1&entry.124074799=Read"},
{"title": "Multi-View Spectral Clustering for Graphs with Multiple View Structures", "author": "Yorgos Tsitsikas and Evangelos E. Papalexakis", "abstract": "  Despite the fundamental importance of clustering, to this day, much of the\nrelevant research is still based on ambiguous foundations, leading to an\nunclear understanding of whether or how the various clustering methods are\nconnected with each other. In this work, we provide an additional stepping\nstone towards resolving such ambiguities by presenting a general clustering\nframework that subsumes a series of seemingly disparate clustering methods,\nincluding various methods belonging to the widely popular spectral clustering\nframework. In fact, the generality of the proposed framework is additionally\ncapable of shedding light to the largely unexplored area of multi-view graphs\nwhere each view may have differently clustered nodes. In turn, we propose\nGenClus: a method that is simultaneously an instance of this framework and a\ngeneralization of spectral clustering, while also being closely related to\nk-means as well. This results in a principled alternative to the few existing\nmethods studying this special type of multi-view graphs. Then, we conduct\nin-depth experiments, which demonstrate that GenClus is more computationally\nefficient than existing methods, while also attaining similar or better\nclustering performance. Lastly, a qualitative real-world case-study further\ndemonstrates the ability of GenClus to produce meaningful clusterings.\n", "link": "http://arxiv.org/abs/2501.11422v2", "date": "2025-01-28", "relevancy": 2.2576, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Spectral%20Clustering%20for%20Graphs%20with%20Multiple%20View%20Structures&body=Title%3A%20Multi-View%20Spectral%20Clustering%20for%20Graphs%20with%20Multiple%20View%20Structures%0AAuthor%3A%20Yorgos%20Tsitsikas%20and%20Evangelos%20E.%20Papalexakis%0AAbstract%3A%20%20%20Despite%20the%20fundamental%20importance%20of%20clustering%2C%20to%20this%20day%2C%20much%20of%20the%0Arelevant%20research%20is%20still%20based%20on%20ambiguous%20foundations%2C%20leading%20to%20an%0Aunclear%20understanding%20of%20whether%20or%20how%20the%20various%20clustering%20methods%20are%0Aconnected%20with%20each%20other.%20In%20this%20work%2C%20we%20provide%20an%20additional%20stepping%0Astone%20towards%20resolving%20such%20ambiguities%20by%20presenting%20a%20general%20clustering%0Aframework%20that%20subsumes%20a%20series%20of%20seemingly%20disparate%20clustering%20methods%2C%0Aincluding%20various%20methods%20belonging%20to%20the%20widely%20popular%20spectral%20clustering%0Aframework.%20In%20fact%2C%20the%20generality%20of%20the%20proposed%20framework%20is%20additionally%0Acapable%20of%20shedding%20light%20to%20the%20largely%20unexplored%20area%20of%20multi-view%20graphs%0Awhere%20each%20view%20may%20have%20differently%20clustered%20nodes.%20In%20turn%2C%20we%20propose%0AGenClus%3A%20a%20method%20that%20is%20simultaneously%20an%20instance%20of%20this%20framework%20and%20a%0Ageneralization%20of%20spectral%20clustering%2C%20while%20also%20being%20closely%20related%20to%0Ak-means%20as%20well.%20This%20results%20in%20a%20principled%20alternative%20to%20the%20few%20existing%0Amethods%20studying%20this%20special%20type%20of%20multi-view%20graphs.%20Then%2C%20we%20conduct%0Ain-depth%20experiments%2C%20which%20demonstrate%20that%20GenClus%20is%20more%20computationally%0Aefficient%20than%20existing%20methods%2C%20while%20also%20attaining%20similar%20or%20better%0Aclustering%20performance.%20Lastly%2C%20a%20qualitative%20real-world%20case-study%20further%0Ademonstrates%20the%20ability%20of%20GenClus%20to%20produce%20meaningful%20clusterings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Spectral%2520Clustering%2520for%2520Graphs%2520with%2520Multiple%2520View%2520Structures%26entry.906535625%3DYorgos%2520Tsitsikas%2520and%2520Evangelos%2520E.%2520Papalexakis%26entry.1292438233%3D%2520%2520Despite%2520the%2520fundamental%2520importance%2520of%2520clustering%252C%2520to%2520this%2520day%252C%2520much%2520of%2520the%250Arelevant%2520research%2520is%2520still%2520based%2520on%2520ambiguous%2520foundations%252C%2520leading%2520to%2520an%250Aunclear%2520understanding%2520of%2520whether%2520or%2520how%2520the%2520various%2520clustering%2520methods%2520are%250Aconnected%2520with%2520each%2520other.%2520In%2520this%2520work%252C%2520we%2520provide%2520an%2520additional%2520stepping%250Astone%2520towards%2520resolving%2520such%2520ambiguities%2520by%2520presenting%2520a%2520general%2520clustering%250Aframework%2520that%2520subsumes%2520a%2520series%2520of%2520seemingly%2520disparate%2520clustering%2520methods%252C%250Aincluding%2520various%2520methods%2520belonging%2520to%2520the%2520widely%2520popular%2520spectral%2520clustering%250Aframework.%2520In%2520fact%252C%2520the%2520generality%2520of%2520the%2520proposed%2520framework%2520is%2520additionally%250Acapable%2520of%2520shedding%2520light%2520to%2520the%2520largely%2520unexplored%2520area%2520of%2520multi-view%2520graphs%250Awhere%2520each%2520view%2520may%2520have%2520differently%2520clustered%2520nodes.%2520In%2520turn%252C%2520we%2520propose%250AGenClus%253A%2520a%2520method%2520that%2520is%2520simultaneously%2520an%2520instance%2520of%2520this%2520framework%2520and%2520a%250Ageneralization%2520of%2520spectral%2520clustering%252C%2520while%2520also%2520being%2520closely%2520related%2520to%250Ak-means%2520as%2520well.%2520This%2520results%2520in%2520a%2520principled%2520alternative%2520to%2520the%2520few%2520existing%250Amethods%2520studying%2520this%2520special%2520type%2520of%2520multi-view%2520graphs.%2520Then%252C%2520we%2520conduct%250Ain-depth%2520experiments%252C%2520which%2520demonstrate%2520that%2520GenClus%2520is%2520more%2520computationally%250Aefficient%2520than%2520existing%2520methods%252C%2520while%2520also%2520attaining%2520similar%2520or%2520better%250Aclustering%2520performance.%2520Lastly%252C%2520a%2520qualitative%2520real-world%2520case-study%2520further%250Ademonstrates%2520the%2520ability%2520of%2520GenClus%2520to%2520produce%2520meaningful%2520clusterings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Spectral%20Clustering%20for%20Graphs%20with%20Multiple%20View%20Structures&entry.906535625=Yorgos%20Tsitsikas%20and%20Evangelos%20E.%20Papalexakis&entry.1292438233=%20%20Despite%20the%20fundamental%20importance%20of%20clustering%2C%20to%20this%20day%2C%20much%20of%20the%0Arelevant%20research%20is%20still%20based%20on%20ambiguous%20foundations%2C%20leading%20to%20an%0Aunclear%20understanding%20of%20whether%20or%20how%20the%20various%20clustering%20methods%20are%0Aconnected%20with%20each%20other.%20In%20this%20work%2C%20we%20provide%20an%20additional%20stepping%0Astone%20towards%20resolving%20such%20ambiguities%20by%20presenting%20a%20general%20clustering%0Aframework%20that%20subsumes%20a%20series%20of%20seemingly%20disparate%20clustering%20methods%2C%0Aincluding%20various%20methods%20belonging%20to%20the%20widely%20popular%20spectral%20clustering%0Aframework.%20In%20fact%2C%20the%20generality%20of%20the%20proposed%20framework%20is%20additionally%0Acapable%20of%20shedding%20light%20to%20the%20largely%20unexplored%20area%20of%20multi-view%20graphs%0Awhere%20each%20view%20may%20have%20differently%20clustered%20nodes.%20In%20turn%2C%20we%20propose%0AGenClus%3A%20a%20method%20that%20is%20simultaneously%20an%20instance%20of%20this%20framework%20and%20a%0Ageneralization%20of%20spectral%20clustering%2C%20while%20also%20being%20closely%20related%20to%0Ak-means%20as%20well.%20This%20results%20in%20a%20principled%20alternative%20to%20the%20few%20existing%0Amethods%20studying%20this%20special%20type%20of%20multi-view%20graphs.%20Then%2C%20we%20conduct%0Ain-depth%20experiments%2C%20which%20demonstrate%20that%20GenClus%20is%20more%20computationally%0Aefficient%20than%20existing%20methods%2C%20while%20also%20attaining%20similar%20or%20better%0Aclustering%20performance.%20Lastly%2C%20a%20qualitative%20real-world%20case-study%20further%0Ademonstrates%20the%20ability%20of%20GenClus%20to%20produce%20meaningful%20clusterings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11422v2&entry.124074799=Read"},
{"title": "Scaling laws for decoding images from brain activity", "author": "Hubert Banville and Yohann Benchetrit and St\u00e9phane d'Ascoli and J\u00e9r\u00e9my Rapin and Jean-R\u00e9mi King", "abstract": "  Generative AI has recently propelled the decoding of images from brain\nactivity. How do these approaches scale with the amount and type of neural\nrecordings? Here, we systematically compare image decoding from four types of\nnon-invasive devices: electroencephalography (EEG), magnetoencephalography\n(MEG), high-field functional Magnetic Resonance Imaging (3T fMRI) and\nultra-high field (7T) fMRI. For this, we evaluate decoding models on the\nlargest benchmark to date, encompassing 8 public datasets, 84 volunteers, 498\nhours of brain recording and 2.3 million brain responses to natural images.\nUnlike previous work, we focus on single-trial decoding performance to simulate\nreal-time settings. This systematic comparison reveals three main findings.\nFirst, the most precise neuroimaging devices tend to yield the best decoding\nperformances, when the size of the training sets are similar. However, the gain\nenabled by deep learning - in comparison to linear models - is obtained with\nthe noisiest devices. Second, we do not observe any plateau of decoding\nperformance as the amount of training data increases. Rather, decoding\nperformance scales log-linearly with the amount of brain recording. Third, this\nscaling law primarily depends on the amount of data per subject. However,\nlittle decoding gain is observed by increasing the number of subjects. Overall,\nthese findings delineate the path most suitable to scale the decoding of images\nfrom non-invasive brain recordings.\n", "link": "http://arxiv.org/abs/2501.15322v2", "date": "2025-01-28", "relevancy": 2.2235, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20laws%20for%20decoding%20images%20from%20brain%20activity&body=Title%3A%20Scaling%20laws%20for%20decoding%20images%20from%20brain%20activity%0AAuthor%3A%20Hubert%20Banville%20and%20Yohann%20Benchetrit%20and%20St%C3%A9phane%20d%27Ascoli%20and%20J%C3%A9r%C3%A9my%20Rapin%20and%20Jean-R%C3%A9mi%20King%0AAbstract%3A%20%20%20Generative%20AI%20has%20recently%20propelled%20the%20decoding%20of%20images%20from%20brain%0Aactivity.%20How%20do%20these%20approaches%20scale%20with%20the%20amount%20and%20type%20of%20neural%0Arecordings%3F%20Here%2C%20we%20systematically%20compare%20image%20decoding%20from%20four%20types%20of%0Anon-invasive%20devices%3A%20electroencephalography%20%28EEG%29%2C%20magnetoencephalography%0A%28MEG%29%2C%20high-field%20functional%20Magnetic%20Resonance%20Imaging%20%283T%20fMRI%29%20and%0Aultra-high%20field%20%287T%29%20fMRI.%20For%20this%2C%20we%20evaluate%20decoding%20models%20on%20the%0Alargest%20benchmark%20to%20date%2C%20encompassing%208%20public%20datasets%2C%2084%20volunteers%2C%20498%0Ahours%20of%20brain%20recording%20and%202.3%20million%20brain%20responses%20to%20natural%20images.%0AUnlike%20previous%20work%2C%20we%20focus%20on%20single-trial%20decoding%20performance%20to%20simulate%0Areal-time%20settings.%20This%20systematic%20comparison%20reveals%20three%20main%20findings.%0AFirst%2C%20the%20most%20precise%20neuroimaging%20devices%20tend%20to%20yield%20the%20best%20decoding%0Aperformances%2C%20when%20the%20size%20of%20the%20training%20sets%20are%20similar.%20However%2C%20the%20gain%0Aenabled%20by%20deep%20learning%20-%20in%20comparison%20to%20linear%20models%20-%20is%20obtained%20with%0Athe%20noisiest%20devices.%20Second%2C%20we%20do%20not%20observe%20any%20plateau%20of%20decoding%0Aperformance%20as%20the%20amount%20of%20training%20data%20increases.%20Rather%2C%20decoding%0Aperformance%20scales%20log-linearly%20with%20the%20amount%20of%20brain%20recording.%20Third%2C%20this%0Ascaling%20law%20primarily%20depends%20on%20the%20amount%20of%20data%20per%20subject.%20However%2C%0Alittle%20decoding%20gain%20is%20observed%20by%20increasing%20the%20number%20of%20subjects.%20Overall%2C%0Athese%20findings%20delineate%20the%20path%20most%20suitable%20to%20scale%20the%20decoding%20of%20images%0Afrom%20non-invasive%20brain%20recordings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520laws%2520for%2520decoding%2520images%2520from%2520brain%2520activity%26entry.906535625%3DHubert%2520Banville%2520and%2520Yohann%2520Benchetrit%2520and%2520St%25C3%25A9phane%2520d%2527Ascoli%2520and%2520J%25C3%25A9r%25C3%25A9my%2520Rapin%2520and%2520Jean-R%25C3%25A9mi%2520King%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520recently%2520propelled%2520the%2520decoding%2520of%2520images%2520from%2520brain%250Aactivity.%2520How%2520do%2520these%2520approaches%2520scale%2520with%2520the%2520amount%2520and%2520type%2520of%2520neural%250Arecordings%253F%2520Here%252C%2520we%2520systematically%2520compare%2520image%2520decoding%2520from%2520four%2520types%2520of%250Anon-invasive%2520devices%253A%2520electroencephalography%2520%2528EEG%2529%252C%2520magnetoencephalography%250A%2528MEG%2529%252C%2520high-field%2520functional%2520Magnetic%2520Resonance%2520Imaging%2520%25283T%2520fMRI%2529%2520and%250Aultra-high%2520field%2520%25287T%2529%2520fMRI.%2520For%2520this%252C%2520we%2520evaluate%2520decoding%2520models%2520on%2520the%250Alargest%2520benchmark%2520to%2520date%252C%2520encompassing%25208%2520public%2520datasets%252C%252084%2520volunteers%252C%2520498%250Ahours%2520of%2520brain%2520recording%2520and%25202.3%2520million%2520brain%2520responses%2520to%2520natural%2520images.%250AUnlike%2520previous%2520work%252C%2520we%2520focus%2520on%2520single-trial%2520decoding%2520performance%2520to%2520simulate%250Areal-time%2520settings.%2520This%2520systematic%2520comparison%2520reveals%2520three%2520main%2520findings.%250AFirst%252C%2520the%2520most%2520precise%2520neuroimaging%2520devices%2520tend%2520to%2520yield%2520the%2520best%2520decoding%250Aperformances%252C%2520when%2520the%2520size%2520of%2520the%2520training%2520sets%2520are%2520similar.%2520However%252C%2520the%2520gain%250Aenabled%2520by%2520deep%2520learning%2520-%2520in%2520comparison%2520to%2520linear%2520models%2520-%2520is%2520obtained%2520with%250Athe%2520noisiest%2520devices.%2520Second%252C%2520we%2520do%2520not%2520observe%2520any%2520plateau%2520of%2520decoding%250Aperformance%2520as%2520the%2520amount%2520of%2520training%2520data%2520increases.%2520Rather%252C%2520decoding%250Aperformance%2520scales%2520log-linearly%2520with%2520the%2520amount%2520of%2520brain%2520recording.%2520Third%252C%2520this%250Ascaling%2520law%2520primarily%2520depends%2520on%2520the%2520amount%2520of%2520data%2520per%2520subject.%2520However%252C%250Alittle%2520decoding%2520gain%2520is%2520observed%2520by%2520increasing%2520the%2520number%2520of%2520subjects.%2520Overall%252C%250Athese%2520findings%2520delineate%2520the%2520path%2520most%2520suitable%2520to%2520scale%2520the%2520decoding%2520of%2520images%250Afrom%2520non-invasive%2520brain%2520recordings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20laws%20for%20decoding%20images%20from%20brain%20activity&entry.906535625=Hubert%20Banville%20and%20Yohann%20Benchetrit%20and%20St%C3%A9phane%20d%27Ascoli%20and%20J%C3%A9r%C3%A9my%20Rapin%20and%20Jean-R%C3%A9mi%20King&entry.1292438233=%20%20Generative%20AI%20has%20recently%20propelled%20the%20decoding%20of%20images%20from%20brain%0Aactivity.%20How%20do%20these%20approaches%20scale%20with%20the%20amount%20and%20type%20of%20neural%0Arecordings%3F%20Here%2C%20we%20systematically%20compare%20image%20decoding%20from%20four%20types%20of%0Anon-invasive%20devices%3A%20electroencephalography%20%28EEG%29%2C%20magnetoencephalography%0A%28MEG%29%2C%20high-field%20functional%20Magnetic%20Resonance%20Imaging%20%283T%20fMRI%29%20and%0Aultra-high%20field%20%287T%29%20fMRI.%20For%20this%2C%20we%20evaluate%20decoding%20models%20on%20the%0Alargest%20benchmark%20to%20date%2C%20encompassing%208%20public%20datasets%2C%2084%20volunteers%2C%20498%0Ahours%20of%20brain%20recording%20and%202.3%20million%20brain%20responses%20to%20natural%20images.%0AUnlike%20previous%20work%2C%20we%20focus%20on%20single-trial%20decoding%20performance%20to%20simulate%0Areal-time%20settings.%20This%20systematic%20comparison%20reveals%20three%20main%20findings.%0AFirst%2C%20the%20most%20precise%20neuroimaging%20devices%20tend%20to%20yield%20the%20best%20decoding%0Aperformances%2C%20when%20the%20size%20of%20the%20training%20sets%20are%20similar.%20However%2C%20the%20gain%0Aenabled%20by%20deep%20learning%20-%20in%20comparison%20to%20linear%20models%20-%20is%20obtained%20with%0Athe%20noisiest%20devices.%20Second%2C%20we%20do%20not%20observe%20any%20plateau%20of%20decoding%0Aperformance%20as%20the%20amount%20of%20training%20data%20increases.%20Rather%2C%20decoding%0Aperformance%20scales%20log-linearly%20with%20the%20amount%20of%20brain%20recording.%20Third%2C%20this%0Ascaling%20law%20primarily%20depends%20on%20the%20amount%20of%20data%20per%20subject.%20However%2C%0Alittle%20decoding%20gain%20is%20observed%20by%20increasing%20the%20number%20of%20subjects.%20Overall%2C%0Athese%20findings%20delineate%20the%20path%20most%20suitable%20to%20scale%20the%20decoding%20of%20images%0Afrom%20non-invasive%20brain%20recordings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15322v2&entry.124074799=Read"},
{"title": "Graph Transformers for inverse physics: reconstructing flows around\n  arbitrary 2D airfoils", "author": "Gregory Duth\u00e9 and Imad Abdallah and Eleni Chatzi", "abstract": "  We introduce a Graph Transformer framework that serves as a general inverse\nphysics engine on meshes, demonstrated through the challenging task of\nreconstructing aerodynamic flow fields from sparse surface measurements. While\ndeep learning has shown promising results in forward physics simulation,\ninverse problems remain particularly challenging due to their ill-posed nature\nand the difficulty of propagating information from limited boundary\nobservations. Our approach addresses these challenges by combining the\ngeometric expressiveness of message-passing neural networks with the global\nreasoning of Transformers, enabling efficient learning of inverse mappings from\nboundary conditions to complete states. We evaluate this framework on a\ncomprehensive dataset of steady-state RANS simulations around diverse airfoil\ngeometries, where the task is to reconstruct full pressure and velocity fields\nfrom surface pressure measurements alone. The architecture achieves high\nreconstruction accuracy while maintaining fast inference times. We conduct\nexperiments and provide insights into the relative importance of local\ngeometric processing and global attention mechanisms in mesh-based inverse\nproblems. We also find that the framework is robust to reduced sensor coverage.\nThese results suggest that Graph Transformers can serve as effective inverse\nphysics engines across a broader range of applications where complete system\nstates must be reconstructed from limited boundary observations.\n", "link": "http://arxiv.org/abs/2501.17081v1", "date": "2025-01-28", "relevancy": 2.2058, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6717}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5349}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Transformers%20for%20inverse%20physics%3A%20reconstructing%20flows%20around%0A%20%20arbitrary%202D%20airfoils&body=Title%3A%20Graph%20Transformers%20for%20inverse%20physics%3A%20reconstructing%20flows%20around%0A%20%20arbitrary%202D%20airfoils%0AAuthor%3A%20Gregory%20Duth%C3%A9%20and%20Imad%20Abdallah%20and%20Eleni%20Chatzi%0AAbstract%3A%20%20%20We%20introduce%20a%20Graph%20Transformer%20framework%20that%20serves%20as%20a%20general%20inverse%0Aphysics%20engine%20on%20meshes%2C%20demonstrated%20through%20the%20challenging%20task%20of%0Areconstructing%20aerodynamic%20flow%20fields%20from%20sparse%20surface%20measurements.%20While%0Adeep%20learning%20has%20shown%20promising%20results%20in%20forward%20physics%20simulation%2C%0Ainverse%20problems%20remain%20particularly%20challenging%20due%20to%20their%20ill-posed%20nature%0Aand%20the%20difficulty%20of%20propagating%20information%20from%20limited%20boundary%0Aobservations.%20Our%20approach%20addresses%20these%20challenges%20by%20combining%20the%0Ageometric%20expressiveness%20of%20message-passing%20neural%20networks%20with%20the%20global%0Areasoning%20of%20Transformers%2C%20enabling%20efficient%20learning%20of%20inverse%20mappings%20from%0Aboundary%20conditions%20to%20complete%20states.%20We%20evaluate%20this%20framework%20on%20a%0Acomprehensive%20dataset%20of%20steady-state%20RANS%20simulations%20around%20diverse%20airfoil%0Ageometries%2C%20where%20the%20task%20is%20to%20reconstruct%20full%20pressure%20and%20velocity%20fields%0Afrom%20surface%20pressure%20measurements%20alone.%20The%20architecture%20achieves%20high%0Areconstruction%20accuracy%20while%20maintaining%20fast%20inference%20times.%20We%20conduct%0Aexperiments%20and%20provide%20insights%20into%20the%20relative%20importance%20of%20local%0Ageometric%20processing%20and%20global%20attention%20mechanisms%20in%20mesh-based%20inverse%0Aproblems.%20We%20also%20find%20that%20the%20framework%20is%20robust%20to%20reduced%20sensor%20coverage.%0AThese%20results%20suggest%20that%20Graph%20Transformers%20can%20serve%20as%20effective%20inverse%0Aphysics%20engines%20across%20a%20broader%20range%20of%20applications%20where%20complete%20system%0Astates%20must%20be%20reconstructed%20from%20limited%20boundary%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Transformers%2520for%2520inverse%2520physics%253A%2520reconstructing%2520flows%2520around%250A%2520%2520arbitrary%25202D%2520airfoils%26entry.906535625%3DGregory%2520Duth%25C3%25A9%2520and%2520Imad%2520Abdallah%2520and%2520Eleni%2520Chatzi%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520Graph%2520Transformer%2520framework%2520that%2520serves%2520as%2520a%2520general%2520inverse%250Aphysics%2520engine%2520on%2520meshes%252C%2520demonstrated%2520through%2520the%2520challenging%2520task%2520of%250Areconstructing%2520aerodynamic%2520flow%2520fields%2520from%2520sparse%2520surface%2520measurements.%2520While%250Adeep%2520learning%2520has%2520shown%2520promising%2520results%2520in%2520forward%2520physics%2520simulation%252C%250Ainverse%2520problems%2520remain%2520particularly%2520challenging%2520due%2520to%2520their%2520ill-posed%2520nature%250Aand%2520the%2520difficulty%2520of%2520propagating%2520information%2520from%2520limited%2520boundary%250Aobservations.%2520Our%2520approach%2520addresses%2520these%2520challenges%2520by%2520combining%2520the%250Ageometric%2520expressiveness%2520of%2520message-passing%2520neural%2520networks%2520with%2520the%2520global%250Areasoning%2520of%2520Transformers%252C%2520enabling%2520efficient%2520learning%2520of%2520inverse%2520mappings%2520from%250Aboundary%2520conditions%2520to%2520complete%2520states.%2520We%2520evaluate%2520this%2520framework%2520on%2520a%250Acomprehensive%2520dataset%2520of%2520steady-state%2520RANS%2520simulations%2520around%2520diverse%2520airfoil%250Ageometries%252C%2520where%2520the%2520task%2520is%2520to%2520reconstruct%2520full%2520pressure%2520and%2520velocity%2520fields%250Afrom%2520surface%2520pressure%2520measurements%2520alone.%2520The%2520architecture%2520achieves%2520high%250Areconstruction%2520accuracy%2520while%2520maintaining%2520fast%2520inference%2520times.%2520We%2520conduct%250Aexperiments%2520and%2520provide%2520insights%2520into%2520the%2520relative%2520importance%2520of%2520local%250Ageometric%2520processing%2520and%2520global%2520attention%2520mechanisms%2520in%2520mesh-based%2520inverse%250Aproblems.%2520We%2520also%2520find%2520that%2520the%2520framework%2520is%2520robust%2520to%2520reduced%2520sensor%2520coverage.%250AThese%2520results%2520suggest%2520that%2520Graph%2520Transformers%2520can%2520serve%2520as%2520effective%2520inverse%250Aphysics%2520engines%2520across%2520a%2520broader%2520range%2520of%2520applications%2520where%2520complete%2520system%250Astates%2520must%2520be%2520reconstructed%2520from%2520limited%2520boundary%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Transformers%20for%20inverse%20physics%3A%20reconstructing%20flows%20around%0A%20%20arbitrary%202D%20airfoils&entry.906535625=Gregory%20Duth%C3%A9%20and%20Imad%20Abdallah%20and%20Eleni%20Chatzi&entry.1292438233=%20%20We%20introduce%20a%20Graph%20Transformer%20framework%20that%20serves%20as%20a%20general%20inverse%0Aphysics%20engine%20on%20meshes%2C%20demonstrated%20through%20the%20challenging%20task%20of%0Areconstructing%20aerodynamic%20flow%20fields%20from%20sparse%20surface%20measurements.%20While%0Adeep%20learning%20has%20shown%20promising%20results%20in%20forward%20physics%20simulation%2C%0Ainverse%20problems%20remain%20particularly%20challenging%20due%20to%20their%20ill-posed%20nature%0Aand%20the%20difficulty%20of%20propagating%20information%20from%20limited%20boundary%0Aobservations.%20Our%20approach%20addresses%20these%20challenges%20by%20combining%20the%0Ageometric%20expressiveness%20of%20message-passing%20neural%20networks%20with%20the%20global%0Areasoning%20of%20Transformers%2C%20enabling%20efficient%20learning%20of%20inverse%20mappings%20from%0Aboundary%20conditions%20to%20complete%20states.%20We%20evaluate%20this%20framework%20on%20a%0Acomprehensive%20dataset%20of%20steady-state%20RANS%20simulations%20around%20diverse%20airfoil%0Ageometries%2C%20where%20the%20task%20is%20to%20reconstruct%20full%20pressure%20and%20velocity%20fields%0Afrom%20surface%20pressure%20measurements%20alone.%20The%20architecture%20achieves%20high%0Areconstruction%20accuracy%20while%20maintaining%20fast%20inference%20times.%20We%20conduct%0Aexperiments%20and%20provide%20insights%20into%20the%20relative%20importance%20of%20local%0Ageometric%20processing%20and%20global%20attention%20mechanisms%20in%20mesh-based%20inverse%0Aproblems.%20We%20also%20find%20that%20the%20framework%20is%20robust%20to%20reduced%20sensor%20coverage.%0AThese%20results%20suggest%20that%20Graph%20Transformers%20can%20serve%20as%20effective%20inverse%0Aphysics%20engines%20across%20a%20broader%20range%20of%20applications%20where%20complete%20system%0Astates%20must%20be%20reconstructed%20from%20limited%20boundary%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17081v1&entry.124074799=Read"},
{"title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning", "author": "Peiling Yi and Yuhan Xia", "abstract": "  Irony is a powerful figurative language (FL) on social media that can\npotentially mislead various NLP tasks, such as recommendation systems,\nmisinformation checks, and sentiment analysis. Understanding the implicit\nmeaning of this kind of subtle language is essential to mitigate irony's\nnegative impact on NLP tasks. However, building models to understand irony\npresents a unique set of challenges, because irony is a complex form of\nlanguage that often relies on context, tone, and subtle cues to convey meaning\nthat is opposite or different from the literal interpretation. Large language\nmodels, such as ChatGPT, are increasingly able to capture implicit and\ncontextual information. In this study, we investigate the generalization,\nreasoning and understanding ability of ChatGPT on irony detection across six\ndifferent genre irony detection datasets. Our findings suggest that ChatGPT\nappears to show an enhanced language understanding and reasoning ability. But\nit needs to be very careful in prompt engineering design. Thus, we propose a\nprompt engineering design framework IDADP to achieve higher irony detection\naccuracy, improved understanding of irony, and more effective explanations\ncompared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain\nvia experiments that the practice generated under the framework is likely to be\nthe promised solution to resolve the generalization issues of LLMs.\n", "link": "http://arxiv.org/abs/2501.16884v1", "date": "2025-01-28", "relevancy": 2.1973, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4428}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Irony%20Detection%2C%20Reasoning%20and%20Understanding%20in%20Zero-shot%20Learning&body=Title%3A%20Irony%20Detection%2C%20Reasoning%20and%20Understanding%20in%20Zero-shot%20Learning%0AAuthor%3A%20Peiling%20Yi%20and%20Yuhan%20Xia%0AAbstract%3A%20%20%20Irony%20is%20a%20powerful%20figurative%20language%20%28FL%29%20on%20social%20media%20that%20can%0Apotentially%20mislead%20various%20NLP%20tasks%2C%20such%20as%20recommendation%20systems%2C%0Amisinformation%20checks%2C%20and%20sentiment%20analysis.%20Understanding%20the%20implicit%0Ameaning%20of%20this%20kind%20of%20subtle%20language%20is%20essential%20to%20mitigate%20irony%27s%0Anegative%20impact%20on%20NLP%20tasks.%20However%2C%20building%20models%20to%20understand%20irony%0Apresents%20a%20unique%20set%20of%20challenges%2C%20because%20irony%20is%20a%20complex%20form%20of%0Alanguage%20that%20often%20relies%20on%20context%2C%20tone%2C%20and%20subtle%20cues%20to%20convey%20meaning%0Athat%20is%20opposite%20or%20different%20from%20the%20literal%20interpretation.%20Large%20language%0Amodels%2C%20such%20as%20ChatGPT%2C%20are%20increasingly%20able%20to%20capture%20implicit%20and%0Acontextual%20information.%20In%20this%20study%2C%20we%20investigate%20the%20generalization%2C%0Areasoning%20and%20understanding%20ability%20of%20ChatGPT%20on%20irony%20detection%20across%20six%0Adifferent%20genre%20irony%20detection%20datasets.%20Our%20findings%20suggest%20that%20ChatGPT%0Aappears%20to%20show%20an%20enhanced%20language%20understanding%20and%20reasoning%20ability.%20But%0Ait%20needs%20to%20be%20very%20careful%20in%20prompt%20engineering%20design.%20Thus%2C%20we%20propose%20a%0Aprompt%20engineering%20design%20framework%20IDADP%20to%20achieve%20higher%20irony%20detection%0Aaccuracy%2C%20improved%20understanding%20of%20irony%2C%20and%20more%20effective%20explanations%0Acompared%20to%20other%20state-of-the-art%20ChatGPT%20zero-shot%20approaches.%20And%20ascertain%0Avia%20experiments%20that%20the%20practice%20generated%20under%20the%20framework%20is%20likely%20to%20be%0Athe%20promised%20solution%20to%20resolve%20the%20generalization%20issues%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIrony%2520Detection%252C%2520Reasoning%2520and%2520Understanding%2520in%2520Zero-shot%2520Learning%26entry.906535625%3DPeiling%2520Yi%2520and%2520Yuhan%2520Xia%26entry.1292438233%3D%2520%2520Irony%2520is%2520a%2520powerful%2520figurative%2520language%2520%2528FL%2529%2520on%2520social%2520media%2520that%2520can%250Apotentially%2520mislead%2520various%2520NLP%2520tasks%252C%2520such%2520as%2520recommendation%2520systems%252C%250Amisinformation%2520checks%252C%2520and%2520sentiment%2520analysis.%2520Understanding%2520the%2520implicit%250Ameaning%2520of%2520this%2520kind%2520of%2520subtle%2520language%2520is%2520essential%2520to%2520mitigate%2520irony%2527s%250Anegative%2520impact%2520on%2520NLP%2520tasks.%2520However%252C%2520building%2520models%2520to%2520understand%2520irony%250Apresents%2520a%2520unique%2520set%2520of%2520challenges%252C%2520because%2520irony%2520is%2520a%2520complex%2520form%2520of%250Alanguage%2520that%2520often%2520relies%2520on%2520context%252C%2520tone%252C%2520and%2520subtle%2520cues%2520to%2520convey%2520meaning%250Athat%2520is%2520opposite%2520or%2520different%2520from%2520the%2520literal%2520interpretation.%2520Large%2520language%250Amodels%252C%2520such%2520as%2520ChatGPT%252C%2520are%2520increasingly%2520able%2520to%2520capture%2520implicit%2520and%250Acontextual%2520information.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520generalization%252C%250Areasoning%2520and%2520understanding%2520ability%2520of%2520ChatGPT%2520on%2520irony%2520detection%2520across%2520six%250Adifferent%2520genre%2520irony%2520detection%2520datasets.%2520Our%2520findings%2520suggest%2520that%2520ChatGPT%250Aappears%2520to%2520show%2520an%2520enhanced%2520language%2520understanding%2520and%2520reasoning%2520ability.%2520But%250Ait%2520needs%2520to%2520be%2520very%2520careful%2520in%2520prompt%2520engineering%2520design.%2520Thus%252C%2520we%2520propose%2520a%250Aprompt%2520engineering%2520design%2520framework%2520IDADP%2520to%2520achieve%2520higher%2520irony%2520detection%250Aaccuracy%252C%2520improved%2520understanding%2520of%2520irony%252C%2520and%2520more%2520effective%2520explanations%250Acompared%2520to%2520other%2520state-of-the-art%2520ChatGPT%2520zero-shot%2520approaches.%2520And%2520ascertain%250Avia%2520experiments%2520that%2520the%2520practice%2520generated%2520under%2520the%2520framework%2520is%2520likely%2520to%2520be%250Athe%2520promised%2520solution%2520to%2520resolve%2520the%2520generalization%2520issues%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Irony%20Detection%2C%20Reasoning%20and%20Understanding%20in%20Zero-shot%20Learning&entry.906535625=Peiling%20Yi%20and%20Yuhan%20Xia&entry.1292438233=%20%20Irony%20is%20a%20powerful%20figurative%20language%20%28FL%29%20on%20social%20media%20that%20can%0Apotentially%20mislead%20various%20NLP%20tasks%2C%20such%20as%20recommendation%20systems%2C%0Amisinformation%20checks%2C%20and%20sentiment%20analysis.%20Understanding%20the%20implicit%0Ameaning%20of%20this%20kind%20of%20subtle%20language%20is%20essential%20to%20mitigate%20irony%27s%0Anegative%20impact%20on%20NLP%20tasks.%20However%2C%20building%20models%20to%20understand%20irony%0Apresents%20a%20unique%20set%20of%20challenges%2C%20because%20irony%20is%20a%20complex%20form%20of%0Alanguage%20that%20often%20relies%20on%20context%2C%20tone%2C%20and%20subtle%20cues%20to%20convey%20meaning%0Athat%20is%20opposite%20or%20different%20from%20the%20literal%20interpretation.%20Large%20language%0Amodels%2C%20such%20as%20ChatGPT%2C%20are%20increasingly%20able%20to%20capture%20implicit%20and%0Acontextual%20information.%20In%20this%20study%2C%20we%20investigate%20the%20generalization%2C%0Areasoning%20and%20understanding%20ability%20of%20ChatGPT%20on%20irony%20detection%20across%20six%0Adifferent%20genre%20irony%20detection%20datasets.%20Our%20findings%20suggest%20that%20ChatGPT%0Aappears%20to%20show%20an%20enhanced%20language%20understanding%20and%20reasoning%20ability.%20But%0Ait%20needs%20to%20be%20very%20careful%20in%20prompt%20engineering%20design.%20Thus%2C%20we%20propose%20a%0Aprompt%20engineering%20design%20framework%20IDADP%20to%20achieve%20higher%20irony%20detection%0Aaccuracy%2C%20improved%20understanding%20of%20irony%2C%20and%20more%20effective%20explanations%0Acompared%20to%20other%20state-of-the-art%20ChatGPT%20zero-shot%20approaches.%20And%20ascertain%0Avia%20experiments%20that%20the%20practice%20generated%20under%20the%20framework%20is%20likely%20to%20be%0Athe%20promised%20solution%20to%20resolve%20the%20generalization%20issues%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16884v1&entry.124074799=Read"},
{"title": "Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile\n  Compensation Using Deep Energy Model", "author": "Reza Ghorbani and Jyothi Rikhab Chand and Chu-Yu Lee and Mathews Jacob and Merry Mani", "abstract": "  Three-dimensional (3D) multi-slab acquisition is a technique frequently\nemployed in high-resolution diffusion-weighted MRI in order to achieve the best\nsignal-to-noise ratio (SNR) efficiency. However, this technique is limited by\nslab boundary artifacts that cause intensity fluctuations and aliasing between\nslabs which reduces the accuracy of anatomical imaging. Addressing this issue\nis crucial for advancing diffusion MRI quality and making high-resolution\nimaging more feasible for clinical and research applications. In this work, we\npropose a regularized slab profile encoding (PEN) method within a Plug-and-Play\nADMM framework, incorporating multi-scale energy (MuSE) regularization to\neffectively improve the slab combined reconstruction. Experimental results\ndemonstrate that the proposed method significantly improves image quality\ncompared to non-regularized and TV-regularized PEN approaches. The regularized\nPEN framework provides a more robust and efficient solution for high-resolution\n3D diffusion MRI, potentially enabling clearer, more reliable anatomical\nimaging across various applications.\n", "link": "http://arxiv.org/abs/2501.17152v1", "date": "2025-01-28", "relevancy": 2.1854, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5516}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5453}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-Dimensional%20Diffusion-Weighted%20Multi-Slab%20MRI%20With%20Slice%20Profile%0A%20%20Compensation%20Using%20Deep%20Energy%20Model&body=Title%3A%20Three-Dimensional%20Diffusion-Weighted%20Multi-Slab%20MRI%20With%20Slice%20Profile%0A%20%20Compensation%20Using%20Deep%20Energy%20Model%0AAuthor%3A%20Reza%20Ghorbani%20and%20Jyothi%20Rikhab%20Chand%20and%20Chu-Yu%20Lee%20and%20Mathews%20Jacob%20and%20Merry%20Mani%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20multi-slab%20acquisition%20is%20a%20technique%20frequently%0Aemployed%20in%20high-resolution%20diffusion-weighted%20MRI%20in%20order%20to%20achieve%20the%20best%0Asignal-to-noise%20ratio%20%28SNR%29%20efficiency.%20However%2C%20this%20technique%20is%20limited%20by%0Aslab%20boundary%20artifacts%20that%20cause%20intensity%20fluctuations%20and%20aliasing%20between%0Aslabs%20which%20reduces%20the%20accuracy%20of%20anatomical%20imaging.%20Addressing%20this%20issue%0Ais%20crucial%20for%20advancing%20diffusion%20MRI%20quality%20and%20making%20high-resolution%0Aimaging%20more%20feasible%20for%20clinical%20and%20research%20applications.%20In%20this%20work%2C%20we%0Apropose%20a%20regularized%20slab%20profile%20encoding%20%28PEN%29%20method%20within%20a%20Plug-and-Play%0AADMM%20framework%2C%20incorporating%20multi-scale%20energy%20%28MuSE%29%20regularization%20to%0Aeffectively%20improve%20the%20slab%20combined%20reconstruction.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20improves%20image%20quality%0Acompared%20to%20non-regularized%20and%20TV-regularized%20PEN%20approaches.%20The%20regularized%0APEN%20framework%20provides%20a%20more%20robust%20and%20efficient%20solution%20for%20high-resolution%0A3D%20diffusion%20MRI%2C%20potentially%20enabling%20clearer%2C%20more%20reliable%20anatomical%0Aimaging%20across%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-Dimensional%2520Diffusion-Weighted%2520Multi-Slab%2520MRI%2520With%2520Slice%2520Profile%250A%2520%2520Compensation%2520Using%2520Deep%2520Energy%2520Model%26entry.906535625%3DReza%2520Ghorbani%2520and%2520Jyothi%2520Rikhab%2520Chand%2520and%2520Chu-Yu%2520Lee%2520and%2520Mathews%2520Jacob%2520and%2520Merry%2520Mani%26entry.1292438233%3D%2520%2520Three-dimensional%2520%25283D%2529%2520multi-slab%2520acquisition%2520is%2520a%2520technique%2520frequently%250Aemployed%2520in%2520high-resolution%2520diffusion-weighted%2520MRI%2520in%2520order%2520to%2520achieve%2520the%2520best%250Asignal-to-noise%2520ratio%2520%2528SNR%2529%2520efficiency.%2520However%252C%2520this%2520technique%2520is%2520limited%2520by%250Aslab%2520boundary%2520artifacts%2520that%2520cause%2520intensity%2520fluctuations%2520and%2520aliasing%2520between%250Aslabs%2520which%2520reduces%2520the%2520accuracy%2520of%2520anatomical%2520imaging.%2520Addressing%2520this%2520issue%250Ais%2520crucial%2520for%2520advancing%2520diffusion%2520MRI%2520quality%2520and%2520making%2520high-resolution%250Aimaging%2520more%2520feasible%2520for%2520clinical%2520and%2520research%2520applications.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520regularized%2520slab%2520profile%2520encoding%2520%2528PEN%2529%2520method%2520within%2520a%2520Plug-and-Play%250AADMM%2520framework%252C%2520incorporating%2520multi-scale%2520energy%2520%2528MuSE%2529%2520regularization%2520to%250Aeffectively%2520improve%2520the%2520slab%2520combined%2520reconstruction.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520improves%2520image%2520quality%250Acompared%2520to%2520non-regularized%2520and%2520TV-regularized%2520PEN%2520approaches.%2520The%2520regularized%250APEN%2520framework%2520provides%2520a%2520more%2520robust%2520and%2520efficient%2520solution%2520for%2520high-resolution%250A3D%2520diffusion%2520MRI%252C%2520potentially%2520enabling%2520clearer%252C%2520more%2520reliable%2520anatomical%250Aimaging%2520across%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-Dimensional%20Diffusion-Weighted%20Multi-Slab%20MRI%20With%20Slice%20Profile%0A%20%20Compensation%20Using%20Deep%20Energy%20Model&entry.906535625=Reza%20Ghorbani%20and%20Jyothi%20Rikhab%20Chand%20and%20Chu-Yu%20Lee%20and%20Mathews%20Jacob%20and%20Merry%20Mani&entry.1292438233=%20%20Three-dimensional%20%283D%29%20multi-slab%20acquisition%20is%20a%20technique%20frequently%0Aemployed%20in%20high-resolution%20diffusion-weighted%20MRI%20in%20order%20to%20achieve%20the%20best%0Asignal-to-noise%20ratio%20%28SNR%29%20efficiency.%20However%2C%20this%20technique%20is%20limited%20by%0Aslab%20boundary%20artifacts%20that%20cause%20intensity%20fluctuations%20and%20aliasing%20between%0Aslabs%20which%20reduces%20the%20accuracy%20of%20anatomical%20imaging.%20Addressing%20this%20issue%0Ais%20crucial%20for%20advancing%20diffusion%20MRI%20quality%20and%20making%20high-resolution%0Aimaging%20more%20feasible%20for%20clinical%20and%20research%20applications.%20In%20this%20work%2C%20we%0Apropose%20a%20regularized%20slab%20profile%20encoding%20%28PEN%29%20method%20within%20a%20Plug-and-Play%0AADMM%20framework%2C%20incorporating%20multi-scale%20energy%20%28MuSE%29%20regularization%20to%0Aeffectively%20improve%20the%20slab%20combined%20reconstruction.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20improves%20image%20quality%0Acompared%20to%20non-regularized%20and%20TV-regularized%20PEN%20approaches.%20The%20regularized%0APEN%20framework%20provides%20a%20more%20robust%20and%20efficient%20solution%20for%20high-resolution%0A3D%20diffusion%20MRI%2C%20potentially%20enabling%20clearer%2C%20more%20reliable%20anatomical%0Aimaging%20across%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17152v1&entry.124074799=Read"},
{"title": "Upside Down Reinforcement Learning with Policy Generators", "author": "Jacopo Di Ventura and Dylan R. Ashley and Vincent Herrmann and Francesco Faccio and J\u00fcrgen Schmidhuber", "abstract": "  Upside Down Reinforcement Learning (UDRL) is a promising framework for\nsolving reinforcement learning problems which focuses on learning\ncommand-conditioned policies. In this work, we extend UDRL to the task of\nlearning a command-conditioned generator of deep neural network policies. We\naccomplish this using Hypernetworks - a variant of Fast Weight Programmers,\nwhich learn to decode input commands representing a desired expected return\ninto command-specific weight matrices. Our method, dubbed Upside Down\nReinforcement Learning with Policy Generators (UDRLPG), streamlines comparable\ntechniques by removing the need for an evaluator or critic to update the\nweights of the generator. To counteract the increased variance in last returns\ncaused by not having an evaluator, we decouple the sampling probability of the\nbuffer from the absolute number of policies in it, which, together with a\nsimple weighting strategy, improves the empirical convergence of the algorithm.\nCompared with existing algorithms, UDRLPG achieves competitive performance and\nhigh returns, sometimes outperforming more complex architectures. Our\nexperiments show that a trained generator can generalize to create policies\nthat achieve unseen returns zero-shot. The proposed method appears to be\neffective in mitigating some of the challenges associated with learning highly\nmultimodal functions. Altogether, we believe that UDRLPG represents a promising\nstep forward in achieving greater empirical sample efficiency in RL. A full\nimplementation of UDRLPG is publicly available at\nhttps://github.com/JacopoD/udrlpg_\n", "link": "http://arxiv.org/abs/2501.16288v2", "date": "2025-01-28", "relevancy": 2.1728, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5721}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5385}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upside%20Down%20Reinforcement%20Learning%20with%20Policy%20Generators&body=Title%3A%20Upside%20Down%20Reinforcement%20Learning%20with%20Policy%20Generators%0AAuthor%3A%20Jacopo%20Di%20Ventura%20and%20Dylan%20R.%20Ashley%20and%20Vincent%20Herrmann%20and%20Francesco%20Faccio%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Upside%20Down%20Reinforcement%20Learning%20%28UDRL%29%20is%20a%20promising%20framework%20for%0Asolving%20reinforcement%20learning%20problems%20which%20focuses%20on%20learning%0Acommand-conditioned%20policies.%20In%20this%20work%2C%20we%20extend%20UDRL%20to%20the%20task%20of%0Alearning%20a%20command-conditioned%20generator%20of%20deep%20neural%20network%20policies.%20We%0Aaccomplish%20this%20using%20Hypernetworks%20-%20a%20variant%20of%20Fast%20Weight%20Programmers%2C%0Awhich%20learn%20to%20decode%20input%20commands%20representing%20a%20desired%20expected%20return%0Ainto%20command-specific%20weight%20matrices.%20Our%20method%2C%20dubbed%20Upside%20Down%0AReinforcement%20Learning%20with%20Policy%20Generators%20%28UDRLPG%29%2C%20streamlines%20comparable%0Atechniques%20by%20removing%20the%20need%20for%20an%20evaluator%20or%20critic%20to%20update%20the%0Aweights%20of%20the%20generator.%20To%20counteract%20the%20increased%20variance%20in%20last%20returns%0Acaused%20by%20not%20having%20an%20evaluator%2C%20we%20decouple%20the%20sampling%20probability%20of%20the%0Abuffer%20from%20the%20absolute%20number%20of%20policies%20in%20it%2C%20which%2C%20together%20with%20a%0Asimple%20weighting%20strategy%2C%20improves%20the%20empirical%20convergence%20of%20the%20algorithm.%0ACompared%20with%20existing%20algorithms%2C%20UDRLPG%20achieves%20competitive%20performance%20and%0Ahigh%20returns%2C%20sometimes%20outperforming%20more%20complex%20architectures.%20Our%0Aexperiments%20show%20that%20a%20trained%20generator%20can%20generalize%20to%20create%20policies%0Athat%20achieve%20unseen%20returns%20zero-shot.%20The%20proposed%20method%20appears%20to%20be%0Aeffective%20in%20mitigating%20some%20of%20the%20challenges%20associated%20with%20learning%20highly%0Amultimodal%20functions.%20Altogether%2C%20we%20believe%20that%20UDRLPG%20represents%20a%20promising%0Astep%20forward%20in%20achieving%20greater%20empirical%20sample%20efficiency%20in%20RL.%20A%20full%0Aimplementation%20of%20UDRLPG%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JacopoD/udrlpg_%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpside%2520Down%2520Reinforcement%2520Learning%2520with%2520Policy%2520Generators%26entry.906535625%3DJacopo%2520Di%2520Ventura%2520and%2520Dylan%2520R.%2520Ashley%2520and%2520Vincent%2520Herrmann%2520and%2520Francesco%2520Faccio%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Upside%2520Down%2520Reinforcement%2520Learning%2520%2528UDRL%2529%2520is%2520a%2520promising%2520framework%2520for%250Asolving%2520reinforcement%2520learning%2520problems%2520which%2520focuses%2520on%2520learning%250Acommand-conditioned%2520policies.%2520In%2520this%2520work%252C%2520we%2520extend%2520UDRL%2520to%2520the%2520task%2520of%250Alearning%2520a%2520command-conditioned%2520generator%2520of%2520deep%2520neural%2520network%2520policies.%2520We%250Aaccomplish%2520this%2520using%2520Hypernetworks%2520-%2520a%2520variant%2520of%2520Fast%2520Weight%2520Programmers%252C%250Awhich%2520learn%2520to%2520decode%2520input%2520commands%2520representing%2520a%2520desired%2520expected%2520return%250Ainto%2520command-specific%2520weight%2520matrices.%2520Our%2520method%252C%2520dubbed%2520Upside%2520Down%250AReinforcement%2520Learning%2520with%2520Policy%2520Generators%2520%2528UDRLPG%2529%252C%2520streamlines%2520comparable%250Atechniques%2520by%2520removing%2520the%2520need%2520for%2520an%2520evaluator%2520or%2520critic%2520to%2520update%2520the%250Aweights%2520of%2520the%2520generator.%2520To%2520counteract%2520the%2520increased%2520variance%2520in%2520last%2520returns%250Acaused%2520by%2520not%2520having%2520an%2520evaluator%252C%2520we%2520decouple%2520the%2520sampling%2520probability%2520of%2520the%250Abuffer%2520from%2520the%2520absolute%2520number%2520of%2520policies%2520in%2520it%252C%2520which%252C%2520together%2520with%2520a%250Asimple%2520weighting%2520strategy%252C%2520improves%2520the%2520empirical%2520convergence%2520of%2520the%2520algorithm.%250ACompared%2520with%2520existing%2520algorithms%252C%2520UDRLPG%2520achieves%2520competitive%2520performance%2520and%250Ahigh%2520returns%252C%2520sometimes%2520outperforming%2520more%2520complex%2520architectures.%2520Our%250Aexperiments%2520show%2520that%2520a%2520trained%2520generator%2520can%2520generalize%2520to%2520create%2520policies%250Athat%2520achieve%2520unseen%2520returns%2520zero-shot.%2520The%2520proposed%2520method%2520appears%2520to%2520be%250Aeffective%2520in%2520mitigating%2520some%2520of%2520the%2520challenges%2520associated%2520with%2520learning%2520highly%250Amultimodal%2520functions.%2520Altogether%252C%2520we%2520believe%2520that%2520UDRLPG%2520represents%2520a%2520promising%250Astep%2520forward%2520in%2520achieving%2520greater%2520empirical%2520sample%2520efficiency%2520in%2520RL.%2520A%2520full%250Aimplementation%2520of%2520UDRLPG%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JacopoD/udrlpg_%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upside%20Down%20Reinforcement%20Learning%20with%20Policy%20Generators&entry.906535625=Jacopo%20Di%20Ventura%20and%20Dylan%20R.%20Ashley%20and%20Vincent%20Herrmann%20and%20Francesco%20Faccio%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Upside%20Down%20Reinforcement%20Learning%20%28UDRL%29%20is%20a%20promising%20framework%20for%0Asolving%20reinforcement%20learning%20problems%20which%20focuses%20on%20learning%0Acommand-conditioned%20policies.%20In%20this%20work%2C%20we%20extend%20UDRL%20to%20the%20task%20of%0Alearning%20a%20command-conditioned%20generator%20of%20deep%20neural%20network%20policies.%20We%0Aaccomplish%20this%20using%20Hypernetworks%20-%20a%20variant%20of%20Fast%20Weight%20Programmers%2C%0Awhich%20learn%20to%20decode%20input%20commands%20representing%20a%20desired%20expected%20return%0Ainto%20command-specific%20weight%20matrices.%20Our%20method%2C%20dubbed%20Upside%20Down%0AReinforcement%20Learning%20with%20Policy%20Generators%20%28UDRLPG%29%2C%20streamlines%20comparable%0Atechniques%20by%20removing%20the%20need%20for%20an%20evaluator%20or%20critic%20to%20update%20the%0Aweights%20of%20the%20generator.%20To%20counteract%20the%20increased%20variance%20in%20last%20returns%0Acaused%20by%20not%20having%20an%20evaluator%2C%20we%20decouple%20the%20sampling%20probability%20of%20the%0Abuffer%20from%20the%20absolute%20number%20of%20policies%20in%20it%2C%20which%2C%20together%20with%20a%0Asimple%20weighting%20strategy%2C%20improves%20the%20empirical%20convergence%20of%20the%20algorithm.%0ACompared%20with%20existing%20algorithms%2C%20UDRLPG%20achieves%20competitive%20performance%20and%0Ahigh%20returns%2C%20sometimes%20outperforming%20more%20complex%20architectures.%20Our%0Aexperiments%20show%20that%20a%20trained%20generator%20can%20generalize%20to%20create%20policies%0Athat%20achieve%20unseen%20returns%20zero-shot.%20The%20proposed%20method%20appears%20to%20be%0Aeffective%20in%20mitigating%20some%20of%20the%20challenges%20associated%20with%20learning%20highly%0Amultimodal%20functions.%20Altogether%2C%20we%20believe%20that%20UDRLPG%20represents%20a%20promising%0Astep%20forward%20in%20achieving%20greater%20empirical%20sample%20efficiency%20in%20RL.%20A%20full%0Aimplementation%20of%20UDRLPG%20is%20publicly%20available%20at%0Ahttps%3A//github.com/JacopoD/udrlpg_%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16288v2&entry.124074799=Read"},
{"title": "Effective Interplay between Sparsity and Quantization: From Theory to\n  Practice", "author": "Simla Burcu Harma and Ayan Chakraborty and Elizaveta Kostenok and Danila Mishin and Dongho Ha and Babak Falsafi and Martin Jaggi and Ming Liu and Yunho Oh and Suvinay Subramanian and Amir Yazdanbakhsh", "abstract": "  The increasing size of deep neural networks (DNNs) necessitates effective\nmodel compression to reduce their computational and memory footprints. Sparsity\nand quantization are two prominent compression methods that have been shown to\nreduce DNNs' computational and memory footprints significantly while preserving\nmodel accuracy. However, how these two methods interact when combined together\nremains a key question for developers, as many tacitly assume that they are\northogonal, meaning that their combined use does not introduce additional\nerrors beyond those introduced by each method independently. In this paper, we\nprovide the first mathematical proof that sparsity and quantization are\nnon-orthogonal. We corroborate these results with experiments spanning a range\nof large language models, including the OPT and LLaMA model families (with 125M\nto 8B parameters), and vision models like ViT and ResNet. We show that the\norder in which we apply these methods matters because applying quantization\nbefore sparsity may disrupt the relative importance of tensor elements, which\nmay inadvertently remove significant elements from a tensor. More importantly,\nwe show that even if applied in the correct order, the compounded errors from\nsparsity and quantization can significantly harm accuracy. Our findings extend\nto the efficient deployment of large models in resource-constrained compute\nplatforms to reduce serving cost, offering insights into best practices for\napplying these compression methods to maximize hardware resource efficiency\nwithout compromising accuracy.\n", "link": "http://arxiv.org/abs/2405.20935v2", "date": "2025-01-28", "relevancy": 2.1488, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5535}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Interplay%20between%20Sparsity%20and%20Quantization%3A%20From%20Theory%20to%0A%20%20Practice&body=Title%3A%20Effective%20Interplay%20between%20Sparsity%20and%20Quantization%3A%20From%20Theory%20to%0A%20%20Practice%0AAuthor%3A%20Simla%20Burcu%20Harma%20and%20Ayan%20Chakraborty%20and%20Elizaveta%20Kostenok%20and%20Danila%20Mishin%20and%20Dongho%20Ha%20and%20Babak%20Falsafi%20and%20Martin%20Jaggi%20and%20Ming%20Liu%20and%20Yunho%20Oh%20and%20Suvinay%20Subramanian%20and%20Amir%20Yazdanbakhsh%0AAbstract%3A%20%20%20The%20increasing%20size%20of%20deep%20neural%20networks%20%28DNNs%29%20necessitates%20effective%0Amodel%20compression%20to%20reduce%20their%20computational%20and%20memory%20footprints.%20Sparsity%0Aand%20quantization%20are%20two%20prominent%20compression%20methods%20that%20have%20been%20shown%20to%0Areduce%20DNNs%27%20computational%20and%20memory%20footprints%20significantly%20while%20preserving%0Amodel%20accuracy.%20However%2C%20how%20these%20two%20methods%20interact%20when%20combined%20together%0Aremains%20a%20key%20question%20for%20developers%2C%20as%20many%20tacitly%20assume%20that%20they%20are%0Aorthogonal%2C%20meaning%20that%20their%20combined%20use%20does%20not%20introduce%20additional%0Aerrors%20beyond%20those%20introduced%20by%20each%20method%20independently.%20In%20this%20paper%2C%20we%0Aprovide%20the%20first%20mathematical%20proof%20that%20sparsity%20and%20quantization%20are%0Anon-orthogonal.%20We%20corroborate%20these%20results%20with%20experiments%20spanning%20a%20range%0Aof%20large%20language%20models%2C%20including%20the%20OPT%20and%20LLaMA%20model%20families%20%28with%20125M%0Ato%208B%20parameters%29%2C%20and%20vision%20models%20like%20ViT%20and%20ResNet.%20We%20show%20that%20the%0Aorder%20in%20which%20we%20apply%20these%20methods%20matters%20because%20applying%20quantization%0Abefore%20sparsity%20may%20disrupt%20the%20relative%20importance%20of%20tensor%20elements%2C%20which%0Amay%20inadvertently%20remove%20significant%20elements%20from%20a%20tensor.%20More%20importantly%2C%0Awe%20show%20that%20even%20if%20applied%20in%20the%20correct%20order%2C%20the%20compounded%20errors%20from%0Asparsity%20and%20quantization%20can%20significantly%20harm%20accuracy.%20Our%20findings%20extend%0Ato%20the%20efficient%20deployment%20of%20large%20models%20in%20resource-constrained%20compute%0Aplatforms%20to%20reduce%20serving%20cost%2C%20offering%20insights%20into%20best%20practices%20for%0Aapplying%20these%20compression%20methods%20to%20maximize%20hardware%20resource%20efficiency%0Awithout%20compromising%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Interplay%2520between%2520Sparsity%2520and%2520Quantization%253A%2520From%2520Theory%2520to%250A%2520%2520Practice%26entry.906535625%3DSimla%2520Burcu%2520Harma%2520and%2520Ayan%2520Chakraborty%2520and%2520Elizaveta%2520Kostenok%2520and%2520Danila%2520Mishin%2520and%2520Dongho%2520Ha%2520and%2520Babak%2520Falsafi%2520and%2520Martin%2520Jaggi%2520and%2520Ming%2520Liu%2520and%2520Yunho%2520Oh%2520and%2520Suvinay%2520Subramanian%2520and%2520Amir%2520Yazdanbakhsh%26entry.1292438233%3D%2520%2520The%2520increasing%2520size%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520necessitates%2520effective%250Amodel%2520compression%2520to%2520reduce%2520their%2520computational%2520and%2520memory%2520footprints.%2520Sparsity%250Aand%2520quantization%2520are%2520two%2520prominent%2520compression%2520methods%2520that%2520have%2520been%2520shown%2520to%250Areduce%2520DNNs%2527%2520computational%2520and%2520memory%2520footprints%2520significantly%2520while%2520preserving%250Amodel%2520accuracy.%2520However%252C%2520how%2520these%2520two%2520methods%2520interact%2520when%2520combined%2520together%250Aremains%2520a%2520key%2520question%2520for%2520developers%252C%2520as%2520many%2520tacitly%2520assume%2520that%2520they%2520are%250Aorthogonal%252C%2520meaning%2520that%2520their%2520combined%2520use%2520does%2520not%2520introduce%2520additional%250Aerrors%2520beyond%2520those%2520introduced%2520by%2520each%2520method%2520independently.%2520In%2520this%2520paper%252C%2520we%250Aprovide%2520the%2520first%2520mathematical%2520proof%2520that%2520sparsity%2520and%2520quantization%2520are%250Anon-orthogonal.%2520We%2520corroborate%2520these%2520results%2520with%2520experiments%2520spanning%2520a%2520range%250Aof%2520large%2520language%2520models%252C%2520including%2520the%2520OPT%2520and%2520LLaMA%2520model%2520families%2520%2528with%2520125M%250Ato%25208B%2520parameters%2529%252C%2520and%2520vision%2520models%2520like%2520ViT%2520and%2520ResNet.%2520We%2520show%2520that%2520the%250Aorder%2520in%2520which%2520we%2520apply%2520these%2520methods%2520matters%2520because%2520applying%2520quantization%250Abefore%2520sparsity%2520may%2520disrupt%2520the%2520relative%2520importance%2520of%2520tensor%2520elements%252C%2520which%250Amay%2520inadvertently%2520remove%2520significant%2520elements%2520from%2520a%2520tensor.%2520More%2520importantly%252C%250Awe%2520show%2520that%2520even%2520if%2520applied%2520in%2520the%2520correct%2520order%252C%2520the%2520compounded%2520errors%2520from%250Asparsity%2520and%2520quantization%2520can%2520significantly%2520harm%2520accuracy.%2520Our%2520findings%2520extend%250Ato%2520the%2520efficient%2520deployment%2520of%2520large%2520models%2520in%2520resource-constrained%2520compute%250Aplatforms%2520to%2520reduce%2520serving%2520cost%252C%2520offering%2520insights%2520into%2520best%2520practices%2520for%250Aapplying%2520these%2520compression%2520methods%2520to%2520maximize%2520hardware%2520resource%2520efficiency%250Awithout%2520compromising%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Interplay%20between%20Sparsity%20and%20Quantization%3A%20From%20Theory%20to%0A%20%20Practice&entry.906535625=Simla%20Burcu%20Harma%20and%20Ayan%20Chakraborty%20and%20Elizaveta%20Kostenok%20and%20Danila%20Mishin%20and%20Dongho%20Ha%20and%20Babak%20Falsafi%20and%20Martin%20Jaggi%20and%20Ming%20Liu%20and%20Yunho%20Oh%20and%20Suvinay%20Subramanian%20and%20Amir%20Yazdanbakhsh&entry.1292438233=%20%20The%20increasing%20size%20of%20deep%20neural%20networks%20%28DNNs%29%20necessitates%20effective%0Amodel%20compression%20to%20reduce%20their%20computational%20and%20memory%20footprints.%20Sparsity%0Aand%20quantization%20are%20two%20prominent%20compression%20methods%20that%20have%20been%20shown%20to%0Areduce%20DNNs%27%20computational%20and%20memory%20footprints%20significantly%20while%20preserving%0Amodel%20accuracy.%20However%2C%20how%20these%20two%20methods%20interact%20when%20combined%20together%0Aremains%20a%20key%20question%20for%20developers%2C%20as%20many%20tacitly%20assume%20that%20they%20are%0Aorthogonal%2C%20meaning%20that%20their%20combined%20use%20does%20not%20introduce%20additional%0Aerrors%20beyond%20those%20introduced%20by%20each%20method%20independently.%20In%20this%20paper%2C%20we%0Aprovide%20the%20first%20mathematical%20proof%20that%20sparsity%20and%20quantization%20are%0Anon-orthogonal.%20We%20corroborate%20these%20results%20with%20experiments%20spanning%20a%20range%0Aof%20large%20language%20models%2C%20including%20the%20OPT%20and%20LLaMA%20model%20families%20%28with%20125M%0Ato%208B%20parameters%29%2C%20and%20vision%20models%20like%20ViT%20and%20ResNet.%20We%20show%20that%20the%0Aorder%20in%20which%20we%20apply%20these%20methods%20matters%20because%20applying%20quantization%0Abefore%20sparsity%20may%20disrupt%20the%20relative%20importance%20of%20tensor%20elements%2C%20which%0Amay%20inadvertently%20remove%20significant%20elements%20from%20a%20tensor.%20More%20importantly%2C%0Awe%20show%20that%20even%20if%20applied%20in%20the%20correct%20order%2C%20the%20compounded%20errors%20from%0Asparsity%20and%20quantization%20can%20significantly%20harm%20accuracy.%20Our%20findings%20extend%0Ato%20the%20efficient%20deployment%20of%20large%20models%20in%20resource-constrained%20compute%0Aplatforms%20to%20reduce%20serving%20cost%2C%20offering%20insights%20into%20best%20practices%20for%0Aapplying%20these%20compression%20methods%20to%20maximize%20hardware%20resource%20efficiency%0Awithout%20compromising%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20935v2&entry.124074799=Read"},
{"title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation", "author": "Karishma Thakrar", "abstract": "  Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.\n", "link": "http://arxiv.org/abs/2412.18644v3", "date": "2025-01-28", "relevancy": 2.1484, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5777}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.517}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaGRAG%20%7C%20Exploring%20the%20Topology%20of%20Information%20for%20Advancing%20Language%0A%20%20Understanding%20and%20Generation%20in%20Graph%20Retrieval-Augmented%20Generation&body=Title%3A%20DynaGRAG%20%7C%20Exploring%20the%20Topology%20of%20Information%20for%20Advancing%20Language%0A%20%20Understanding%20and%20Generation%20in%20Graph%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Karishma%20Thakrar%0AAbstract%3A%20%20%20Graph%20Retrieval-Augmented%20Generation%20%28GRAG%20or%20Graph%20RAG%29%20architectures%20aim%20to%0Aenhance%20language%20understanding%20and%20generation%20by%20leveraging%20external%20knowledge.%0AHowever%2C%20effectively%20capturing%20and%20integrating%20the%20rich%20semantic%20information%0Apresent%20in%20textual%20and%20structured%20data%20remains%20a%20challenge.%20To%20address%20this%2C%20a%0Anovel%20GRAG%20framework%2C%20Dynamic%20Graph%20Retrieval-Agumented%20Generation%20%28DynaGRAG%29%2C%0Ais%20proposed%20to%20focus%20on%20enhancing%20subgraph%20representation%20and%20diversity%20within%0Athe%20knowledge%20graph.%20By%20improving%20graph%20density%2C%20capturing%20entity%20and%20relation%0Ainformation%20more%20effectively%2C%20and%20dynamically%20prioritizing%20relevant%20and%20diverse%0Asubgraphs%20and%20information%20within%20them%2C%20the%20proposed%20approach%20enables%20a%20more%0Acomprehensive%20understanding%20of%20the%20underlying%20semantic%20structure.%20This%20is%0Aachieved%20through%20a%20combination%20of%20de-duplication%20processes%2C%20two-step%20mean%0Apooling%20of%20embeddings%2C%20query-aware%20retrieval%20considering%20unique%20nodes%2C%20and%20a%0ADynamic%20Similarity-Aware%20BFS%20%28DSA-BFS%29%20traversal%20algorithm.%20Integrating%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Large%20Language%20Models%20%28LLMs%29%20through%20hard%0Aprompting%20further%20enhances%20the%20learning%20of%20rich%20node%20and%20edge%20representations%0Awhile%20preserving%20the%20hierarchical%20subgraph%20structure.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20DynaGRAG%2C%20showcasing%20the%20significance%20of%0Aenhanced%20subgraph%20representation%20and%20diversity%20for%20improved%20language%0Aunderstanding%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18644v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaGRAG%2520%257C%2520Exploring%2520the%2520Topology%2520of%2520Information%2520for%2520Advancing%2520Language%250A%2520%2520Understanding%2520and%2520Generation%2520in%2520Graph%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DKarishma%2520Thakrar%26entry.1292438233%3D%2520%2520Graph%2520Retrieval-Augmented%2520Generation%2520%2528GRAG%2520or%2520Graph%2520RAG%2529%2520architectures%2520aim%2520to%250Aenhance%2520language%2520understanding%2520and%2520generation%2520by%2520leveraging%2520external%2520knowledge.%250AHowever%252C%2520effectively%2520capturing%2520and%2520integrating%2520the%2520rich%2520semantic%2520information%250Apresent%2520in%2520textual%2520and%2520structured%2520data%2520remains%2520a%2520challenge.%2520To%2520address%2520this%252C%2520a%250Anovel%2520GRAG%2520framework%252C%2520Dynamic%2520Graph%2520Retrieval-Agumented%2520Generation%2520%2528DynaGRAG%2529%252C%250Ais%2520proposed%2520to%2520focus%2520on%2520enhancing%2520subgraph%2520representation%2520and%2520diversity%2520within%250Athe%2520knowledge%2520graph.%2520By%2520improving%2520graph%2520density%252C%2520capturing%2520entity%2520and%2520relation%250Ainformation%2520more%2520effectively%252C%2520and%2520dynamically%2520prioritizing%2520relevant%2520and%2520diverse%250Asubgraphs%2520and%2520information%2520within%2520them%252C%2520the%2520proposed%2520approach%2520enables%2520a%2520more%250Acomprehensive%2520understanding%2520of%2520the%2520underlying%2520semantic%2520structure.%2520This%2520is%250Aachieved%2520through%2520a%2520combination%2520of%2520de-duplication%2520processes%252C%2520two-step%2520mean%250Apooling%2520of%2520embeddings%252C%2520query-aware%2520retrieval%2520considering%2520unique%2520nodes%252C%2520and%2520a%250ADynamic%2520Similarity-Aware%2520BFS%2520%2528DSA-BFS%2529%2520traversal%2520algorithm.%2520Integrating%2520Graph%250AConvolutional%2520Networks%2520%2528GCNs%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520through%2520hard%250Aprompting%2520further%2520enhances%2520the%2520learning%2520of%2520rich%2520node%2520and%2520edge%2520representations%250Awhile%2520preserving%2520the%2520hierarchical%2520subgraph%2520structure.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520DynaGRAG%252C%2520showcasing%2520the%2520significance%2520of%250Aenhanced%2520subgraph%2520representation%2520and%2520diversity%2520for%2520improved%2520language%250Aunderstanding%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18644v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaGRAG%20%7C%20Exploring%20the%20Topology%20of%20Information%20for%20Advancing%20Language%0A%20%20Understanding%20and%20Generation%20in%20Graph%20Retrieval-Augmented%20Generation&entry.906535625=Karishma%20Thakrar&entry.1292438233=%20%20Graph%20Retrieval-Augmented%20Generation%20%28GRAG%20or%20Graph%20RAG%29%20architectures%20aim%20to%0Aenhance%20language%20understanding%20and%20generation%20by%20leveraging%20external%20knowledge.%0AHowever%2C%20effectively%20capturing%20and%20integrating%20the%20rich%20semantic%20information%0Apresent%20in%20textual%20and%20structured%20data%20remains%20a%20challenge.%20To%20address%20this%2C%20a%0Anovel%20GRAG%20framework%2C%20Dynamic%20Graph%20Retrieval-Agumented%20Generation%20%28DynaGRAG%29%2C%0Ais%20proposed%20to%20focus%20on%20enhancing%20subgraph%20representation%20and%20diversity%20within%0Athe%20knowledge%20graph.%20By%20improving%20graph%20density%2C%20capturing%20entity%20and%20relation%0Ainformation%20more%20effectively%2C%20and%20dynamically%20prioritizing%20relevant%20and%20diverse%0Asubgraphs%20and%20information%20within%20them%2C%20the%20proposed%20approach%20enables%20a%20more%0Acomprehensive%20understanding%20of%20the%20underlying%20semantic%20structure.%20This%20is%0Aachieved%20through%20a%20combination%20of%20de-duplication%20processes%2C%20two-step%20mean%0Apooling%20of%20embeddings%2C%20query-aware%20retrieval%20considering%20unique%20nodes%2C%20and%20a%0ADynamic%20Similarity-Aware%20BFS%20%28DSA-BFS%29%20traversal%20algorithm.%20Integrating%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Large%20Language%20Models%20%28LLMs%29%20through%20hard%0Aprompting%20further%20enhances%20the%20learning%20of%20rich%20node%20and%20edge%20representations%0Awhile%20preserving%20the%20hierarchical%20subgraph%20structure.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20DynaGRAG%2C%20showcasing%20the%20significance%20of%0Aenhanced%20subgraph%20representation%20and%20diversity%20for%20improved%20language%0Aunderstanding%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18644v3&entry.124074799=Read"},
{"title": "FedEFM: Federated Endovascular Foundation Model with Unseen Data", "author": "Tuong Do and Nghia Vu and Tudor Jianu and Baoru Huang and Minh Vu and Jionglong Su and Erman Tjiputra and Quang D. Tran and Te-Chuan Chiu and Anh Nguyen", "abstract": "  In endovascular surgery, the precise identification of catheters and\nguidewires in X-ray images is essential for reducing intervention risks.\nHowever, accurately segmenting catheter and guidewire structures is challenging\ndue to the limited availability of labeled data. Foundation models offer a\npromising solution by enabling the collection of similar domain data to train\nmodels whose weights can be fine-tuned for downstream tasks. Nonetheless,\nlarge-scale data collection for training is constrained by the necessity of\nmaintaining patient privacy. This paper proposes a new method to train a\nfoundation model in a decentralized federated learning setting for endovascular\nintervention. To ensure the feasibility of the training, we tackle the unseen\ndata issue using differentiable Earth Mover's Distance within a knowledge\ndistillation framework. Once trained, our foundation model's weights provide\nvaluable initialization for downstream tasks, thereby enhancing task-specific\nperformance. Intensive experiments show that our approach achieves new\nstate-of-the-art results, contributing to advancements in endovascular\nintervention and robotic-assisted endovascular surgery, while addressing the\ncritical issue of data sharing in the medical domain.\n", "link": "http://arxiv.org/abs/2501.16992v1", "date": "2025-01-28", "relevancy": 2.1265, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedEFM%3A%20Federated%20Endovascular%20Foundation%20Model%20with%20Unseen%20Data&body=Title%3A%20FedEFM%3A%20Federated%20Endovascular%20Foundation%20Model%20with%20Unseen%20Data%0AAuthor%3A%20Tuong%20Do%20and%20Nghia%20Vu%20and%20Tudor%20Jianu%20and%20Baoru%20Huang%20and%20Minh%20Vu%20and%20Jionglong%20Su%20and%20Erman%20Tjiputra%20and%20Quang%20D.%20Tran%20and%20Te-Chuan%20Chiu%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20In%20endovascular%20surgery%2C%20the%20precise%20identification%20of%20catheters%20and%0Aguidewires%20in%20X-ray%20images%20is%20essential%20for%20reducing%20intervention%20risks.%0AHowever%2C%20accurately%20segmenting%20catheter%20and%20guidewire%20structures%20is%20challenging%0Adue%20to%20the%20limited%20availability%20of%20labeled%20data.%20Foundation%20models%20offer%20a%0Apromising%20solution%20by%20enabling%20the%20collection%20of%20similar%20domain%20data%20to%20train%0Amodels%20whose%20weights%20can%20be%20fine-tuned%20for%20downstream%20tasks.%20Nonetheless%2C%0Alarge-scale%20data%20collection%20for%20training%20is%20constrained%20by%20the%20necessity%20of%0Amaintaining%20patient%20privacy.%20This%20paper%20proposes%20a%20new%20method%20to%20train%20a%0Afoundation%20model%20in%20a%20decentralized%20federated%20learning%20setting%20for%20endovascular%0Aintervention.%20To%20ensure%20the%20feasibility%20of%20the%20training%2C%20we%20tackle%20the%20unseen%0Adata%20issue%20using%20differentiable%20Earth%20Mover%27s%20Distance%20within%20a%20knowledge%0Adistillation%20framework.%20Once%20trained%2C%20our%20foundation%20model%27s%20weights%20provide%0Avaluable%20initialization%20for%20downstream%20tasks%2C%20thereby%20enhancing%20task-specific%0Aperformance.%20Intensive%20experiments%20show%20that%20our%20approach%20achieves%20new%0Astate-of-the-art%20results%2C%20contributing%20to%20advancements%20in%20endovascular%0Aintervention%20and%20robotic-assisted%20endovascular%20surgery%2C%20while%20addressing%20the%0Acritical%20issue%20of%20data%20sharing%20in%20the%20medical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedEFM%253A%2520Federated%2520Endovascular%2520Foundation%2520Model%2520with%2520Unseen%2520Data%26entry.906535625%3DTuong%2520Do%2520and%2520Nghia%2520Vu%2520and%2520Tudor%2520Jianu%2520and%2520Baoru%2520Huang%2520and%2520Minh%2520Vu%2520and%2520Jionglong%2520Su%2520and%2520Erman%2520Tjiputra%2520and%2520Quang%2520D.%2520Tran%2520and%2520Te-Chuan%2520Chiu%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520In%2520endovascular%2520surgery%252C%2520the%2520precise%2520identification%2520of%2520catheters%2520and%250Aguidewires%2520in%2520X-ray%2520images%2520is%2520essential%2520for%2520reducing%2520intervention%2520risks.%250AHowever%252C%2520accurately%2520segmenting%2520catheter%2520and%2520guidewire%2520structures%2520is%2520challenging%250Adue%2520to%2520the%2520limited%2520availability%2520of%2520labeled%2520data.%2520Foundation%2520models%2520offer%2520a%250Apromising%2520solution%2520by%2520enabling%2520the%2520collection%2520of%2520similar%2520domain%2520data%2520to%2520train%250Amodels%2520whose%2520weights%2520can%2520be%2520fine-tuned%2520for%2520downstream%2520tasks.%2520Nonetheless%252C%250Alarge-scale%2520data%2520collection%2520for%2520training%2520is%2520constrained%2520by%2520the%2520necessity%2520of%250Amaintaining%2520patient%2520privacy.%2520This%2520paper%2520proposes%2520a%2520new%2520method%2520to%2520train%2520a%250Afoundation%2520model%2520in%2520a%2520decentralized%2520federated%2520learning%2520setting%2520for%2520endovascular%250Aintervention.%2520To%2520ensure%2520the%2520feasibility%2520of%2520the%2520training%252C%2520we%2520tackle%2520the%2520unseen%250Adata%2520issue%2520using%2520differentiable%2520Earth%2520Mover%2527s%2520Distance%2520within%2520a%2520knowledge%250Adistillation%2520framework.%2520Once%2520trained%252C%2520our%2520foundation%2520model%2527s%2520weights%2520provide%250Avaluable%2520initialization%2520for%2520downstream%2520tasks%252C%2520thereby%2520enhancing%2520task-specific%250Aperformance.%2520Intensive%2520experiments%2520show%2520that%2520our%2520approach%2520achieves%2520new%250Astate-of-the-art%2520results%252C%2520contributing%2520to%2520advancements%2520in%2520endovascular%250Aintervention%2520and%2520robotic-assisted%2520endovascular%2520surgery%252C%2520while%2520addressing%2520the%250Acritical%2520issue%2520of%2520data%2520sharing%2520in%2520the%2520medical%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedEFM%3A%20Federated%20Endovascular%20Foundation%20Model%20with%20Unseen%20Data&entry.906535625=Tuong%20Do%20and%20Nghia%20Vu%20and%20Tudor%20Jianu%20and%20Baoru%20Huang%20and%20Minh%20Vu%20and%20Jionglong%20Su%20and%20Erman%20Tjiputra%20and%20Quang%20D.%20Tran%20and%20Te-Chuan%20Chiu%20and%20Anh%20Nguyen&entry.1292438233=%20%20In%20endovascular%20surgery%2C%20the%20precise%20identification%20of%20catheters%20and%0Aguidewires%20in%20X-ray%20images%20is%20essential%20for%20reducing%20intervention%20risks.%0AHowever%2C%20accurately%20segmenting%20catheter%20and%20guidewire%20structures%20is%20challenging%0Adue%20to%20the%20limited%20availability%20of%20labeled%20data.%20Foundation%20models%20offer%20a%0Apromising%20solution%20by%20enabling%20the%20collection%20of%20similar%20domain%20data%20to%20train%0Amodels%20whose%20weights%20can%20be%20fine-tuned%20for%20downstream%20tasks.%20Nonetheless%2C%0Alarge-scale%20data%20collection%20for%20training%20is%20constrained%20by%20the%20necessity%20of%0Amaintaining%20patient%20privacy.%20This%20paper%20proposes%20a%20new%20method%20to%20train%20a%0Afoundation%20model%20in%20a%20decentralized%20federated%20learning%20setting%20for%20endovascular%0Aintervention.%20To%20ensure%20the%20feasibility%20of%20the%20training%2C%20we%20tackle%20the%20unseen%0Adata%20issue%20using%20differentiable%20Earth%20Mover%27s%20Distance%20within%20a%20knowledge%0Adistillation%20framework.%20Once%20trained%2C%20our%20foundation%20model%27s%20weights%20provide%0Avaluable%20initialization%20for%20downstream%20tasks%2C%20thereby%20enhancing%20task-specific%0Aperformance.%20Intensive%20experiments%20show%20that%20our%20approach%20achieves%20new%0Astate-of-the-art%20results%2C%20contributing%20to%20advancements%20in%20endovascular%0Aintervention%20and%20robotic-assisted%20endovascular%20surgery%2C%20while%20addressing%20the%0Acritical%20issue%20of%20data%20sharing%20in%20the%20medical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16992v1&entry.124074799=Read"},
{"title": "MIDI-GPT: A Controllable Generative Model for Computer-Assisted\n  Multitrack Music Composition", "author": "Philippe Pasquier and Jeff Ens and Nathan Fradet and Paul Triana and Davide Rizzotti and Jean-Baptiste Rolland and Maryam Safi", "abstract": "  We present and release MIDI-GPT, a generative system based on the Transformer\narchitecture that is designed for computer-assisted music composition\nworkflows. MIDI-GPT supports the infilling of musical material at the track and\nbar level, and can condition generation on attributes including: instrument\ntype, musical style, note density, polyphony level, and note duration. In order\nto integrate these features, we employ an alternative representation for\nmusical material, creating a time-ordered sequence of musical events for each\ntrack and concatenating several tracks into a single sequence, rather than\nusing a single time-ordered sequence where the musical events corresponding to\ndifferent tracks are interleaved. We also propose a variation of our\nrepresentation allowing for expressiveness. We present experimental results\nthat demonstrate that MIDI-GPT is able to consistently avoid duplicating the\nmusical material it was trained on, generate music that is stylistically\nsimilar to the training dataset, and that attribute controls allow enforcing\nvarious constraints on the generated material. We also outline several\nreal-world applications of MIDI-GPT, including collaborations with industry\npartners that explore the integration and evaluation of MIDI-GPT into\ncommercial products, as well as several artistic works produced using it.\n", "link": "http://arxiv.org/abs/2501.17011v1", "date": "2025-01-28", "relevancy": 2.1248, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5433}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5389}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIDI-GPT%3A%20A%20Controllable%20Generative%20Model%20for%20Computer-Assisted%0A%20%20Multitrack%20Music%20Composition&body=Title%3A%20MIDI-GPT%3A%20A%20Controllable%20Generative%20Model%20for%20Computer-Assisted%0A%20%20Multitrack%20Music%20Composition%0AAuthor%3A%20Philippe%20Pasquier%20and%20Jeff%20Ens%20and%20Nathan%20Fradet%20and%20Paul%20Triana%20and%20Davide%20Rizzotti%20and%20Jean-Baptiste%20Rolland%20and%20Maryam%20Safi%0AAbstract%3A%20%20%20We%20present%20and%20release%20MIDI-GPT%2C%20a%20generative%20system%20based%20on%20the%20Transformer%0Aarchitecture%20that%20is%20designed%20for%20computer-assisted%20music%20composition%0Aworkflows.%20MIDI-GPT%20supports%20the%20infilling%20of%20musical%20material%20at%20the%20track%20and%0Abar%20level%2C%20and%20can%20condition%20generation%20on%20attributes%20including%3A%20instrument%0Atype%2C%20musical%20style%2C%20note%20density%2C%20polyphony%20level%2C%20and%20note%20duration.%20In%20order%0Ato%20integrate%20these%20features%2C%20we%20employ%20an%20alternative%20representation%20for%0Amusical%20material%2C%20creating%20a%20time-ordered%20sequence%20of%20musical%20events%20for%20each%0Atrack%20and%20concatenating%20several%20tracks%20into%20a%20single%20sequence%2C%20rather%20than%0Ausing%20a%20single%20time-ordered%20sequence%20where%20the%20musical%20events%20corresponding%20to%0Adifferent%20tracks%20are%20interleaved.%20We%20also%20propose%20a%20variation%20of%20our%0Arepresentation%20allowing%20for%20expressiveness.%20We%20present%20experimental%20results%0Athat%20demonstrate%20that%20MIDI-GPT%20is%20able%20to%20consistently%20avoid%20duplicating%20the%0Amusical%20material%20it%20was%20trained%20on%2C%20generate%20music%20that%20is%20stylistically%0Asimilar%20to%20the%20training%20dataset%2C%20and%20that%20attribute%20controls%20allow%20enforcing%0Avarious%20constraints%20on%20the%20generated%20material.%20We%20also%20outline%20several%0Areal-world%20applications%20of%20MIDI-GPT%2C%20including%20collaborations%20with%20industry%0Apartners%20that%20explore%20the%20integration%20and%20evaluation%20of%20MIDI-GPT%20into%0Acommercial%20products%2C%20as%20well%20as%20several%20artistic%20works%20produced%20using%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIDI-GPT%253A%2520A%2520Controllable%2520Generative%2520Model%2520for%2520Computer-Assisted%250A%2520%2520Multitrack%2520Music%2520Composition%26entry.906535625%3DPhilippe%2520Pasquier%2520and%2520Jeff%2520Ens%2520and%2520Nathan%2520Fradet%2520and%2520Paul%2520Triana%2520and%2520Davide%2520Rizzotti%2520and%2520Jean-Baptiste%2520Rolland%2520and%2520Maryam%2520Safi%26entry.1292438233%3D%2520%2520We%2520present%2520and%2520release%2520MIDI-GPT%252C%2520a%2520generative%2520system%2520based%2520on%2520the%2520Transformer%250Aarchitecture%2520that%2520is%2520designed%2520for%2520computer-assisted%2520music%2520composition%250Aworkflows.%2520MIDI-GPT%2520supports%2520the%2520infilling%2520of%2520musical%2520material%2520at%2520the%2520track%2520and%250Abar%2520level%252C%2520and%2520can%2520condition%2520generation%2520on%2520attributes%2520including%253A%2520instrument%250Atype%252C%2520musical%2520style%252C%2520note%2520density%252C%2520polyphony%2520level%252C%2520and%2520note%2520duration.%2520In%2520order%250Ato%2520integrate%2520these%2520features%252C%2520we%2520employ%2520an%2520alternative%2520representation%2520for%250Amusical%2520material%252C%2520creating%2520a%2520time-ordered%2520sequence%2520of%2520musical%2520events%2520for%2520each%250Atrack%2520and%2520concatenating%2520several%2520tracks%2520into%2520a%2520single%2520sequence%252C%2520rather%2520than%250Ausing%2520a%2520single%2520time-ordered%2520sequence%2520where%2520the%2520musical%2520events%2520corresponding%2520to%250Adifferent%2520tracks%2520are%2520interleaved.%2520We%2520also%2520propose%2520a%2520variation%2520of%2520our%250Arepresentation%2520allowing%2520for%2520expressiveness.%2520We%2520present%2520experimental%2520results%250Athat%2520demonstrate%2520that%2520MIDI-GPT%2520is%2520able%2520to%2520consistently%2520avoid%2520duplicating%2520the%250Amusical%2520material%2520it%2520was%2520trained%2520on%252C%2520generate%2520music%2520that%2520is%2520stylistically%250Asimilar%2520to%2520the%2520training%2520dataset%252C%2520and%2520that%2520attribute%2520controls%2520allow%2520enforcing%250Avarious%2520constraints%2520on%2520the%2520generated%2520material.%2520We%2520also%2520outline%2520several%250Areal-world%2520applications%2520of%2520MIDI-GPT%252C%2520including%2520collaborations%2520with%2520industry%250Apartners%2520that%2520explore%2520the%2520integration%2520and%2520evaluation%2520of%2520MIDI-GPT%2520into%250Acommercial%2520products%252C%2520as%2520well%2520as%2520several%2520artistic%2520works%2520produced%2520using%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIDI-GPT%3A%20A%20Controllable%20Generative%20Model%20for%20Computer-Assisted%0A%20%20Multitrack%20Music%20Composition&entry.906535625=Philippe%20Pasquier%20and%20Jeff%20Ens%20and%20Nathan%20Fradet%20and%20Paul%20Triana%20and%20Davide%20Rizzotti%20and%20Jean-Baptiste%20Rolland%20and%20Maryam%20Safi&entry.1292438233=%20%20We%20present%20and%20release%20MIDI-GPT%2C%20a%20generative%20system%20based%20on%20the%20Transformer%0Aarchitecture%20that%20is%20designed%20for%20computer-assisted%20music%20composition%0Aworkflows.%20MIDI-GPT%20supports%20the%20infilling%20of%20musical%20material%20at%20the%20track%20and%0Abar%20level%2C%20and%20can%20condition%20generation%20on%20attributes%20including%3A%20instrument%0Atype%2C%20musical%20style%2C%20note%20density%2C%20polyphony%20level%2C%20and%20note%20duration.%20In%20order%0Ato%20integrate%20these%20features%2C%20we%20employ%20an%20alternative%20representation%20for%0Amusical%20material%2C%20creating%20a%20time-ordered%20sequence%20of%20musical%20events%20for%20each%0Atrack%20and%20concatenating%20several%20tracks%20into%20a%20single%20sequence%2C%20rather%20than%0Ausing%20a%20single%20time-ordered%20sequence%20where%20the%20musical%20events%20corresponding%20to%0Adifferent%20tracks%20are%20interleaved.%20We%20also%20propose%20a%20variation%20of%20our%0Arepresentation%20allowing%20for%20expressiveness.%20We%20present%20experimental%20results%0Athat%20demonstrate%20that%20MIDI-GPT%20is%20able%20to%20consistently%20avoid%20duplicating%20the%0Amusical%20material%20it%20was%20trained%20on%2C%20generate%20music%20that%20is%20stylistically%0Asimilar%20to%20the%20training%20dataset%2C%20and%20that%20attribute%20controls%20allow%20enforcing%0Avarious%20constraints%20on%20the%20generated%20material.%20We%20also%20outline%20several%0Areal-world%20applications%20of%20MIDI-GPT%2C%20including%20collaborations%20with%20industry%0Apartners%20that%20explore%20the%20integration%20and%20evaluation%20of%20MIDI-GPT%20into%0Acommercial%20products%2C%20as%20well%20as%20several%20artistic%20works%20produced%20using%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17011v1&entry.124074799=Read"},
{"title": "Event-Based Adaptive Koopman Framework for Optic Flow-Guided Landing on\n  Moving Platforms", "author": "Bazeela Banday and Chandan Kumar Sah and Jishnu Keshavan", "abstract": "  This paper presents an optic flow-guided approach for achieving soft landings\nby resource-constrained unmanned aerial vehicles (UAVs) on dynamic platforms.\nAn offline data-driven linear model based on Koopman operator theory is\ndeveloped to describe the underlying (nonlinear) dynamics of optic flow output\nobtained from a single monocular camera that maps to vehicle acceleration as\nthe control input. Moreover, a novel adaptation scheme within the Koopman\nframework is introduced online to handle uncertainties such as unknown platform\nmotion and ground effect, which exert a significant influence during the\nterminal stage of the descent process. Further, to minimize computational\noverhead, an event-based adaptation trigger is incorporated into an\nevent-driven Model Predictive Control (MPC) strategy to regulate optic flow and\ntrack a desired reference. A detailed convergence analysis ensures global\nconvergence of the tracking error to a uniform ultimate bound while ensuring\nZeno-free behavior. Simulation results demonstrate the algorithm's robustness\nand effectiveness in landing on dynamic platforms under ground effect and\nsensor noise, which compares favorably to non-adaptive event-triggered and\ntime-triggered adaptive schemes.\n", "link": "http://arxiv.org/abs/2501.16868v1", "date": "2025-01-28", "relevancy": 2.1194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Based%20Adaptive%20Koopman%20Framework%20for%20Optic%20Flow-Guided%20Landing%20on%0A%20%20Moving%20Platforms&body=Title%3A%20Event-Based%20Adaptive%20Koopman%20Framework%20for%20Optic%20Flow-Guided%20Landing%20on%0A%20%20Moving%20Platforms%0AAuthor%3A%20Bazeela%20Banday%20and%20Chandan%20Kumar%20Sah%20and%20Jishnu%20Keshavan%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20optic%20flow-guided%20approach%20for%20achieving%20soft%20landings%0Aby%20resource-constrained%20unmanned%20aerial%20vehicles%20%28UAVs%29%20on%20dynamic%20platforms.%0AAn%20offline%20data-driven%20linear%20model%20based%20on%20Koopman%20operator%20theory%20is%0Adeveloped%20to%20describe%20the%20underlying%20%28nonlinear%29%20dynamics%20of%20optic%20flow%20output%0Aobtained%20from%20a%20single%20monocular%20camera%20that%20maps%20to%20vehicle%20acceleration%20as%0Athe%20control%20input.%20Moreover%2C%20a%20novel%20adaptation%20scheme%20within%20the%20Koopman%0Aframework%20is%20introduced%20online%20to%20handle%20uncertainties%20such%20as%20unknown%20platform%0Amotion%20and%20ground%20effect%2C%20which%20exert%20a%20significant%20influence%20during%20the%0Aterminal%20stage%20of%20the%20descent%20process.%20Further%2C%20to%20minimize%20computational%0Aoverhead%2C%20an%20event-based%20adaptation%20trigger%20is%20incorporated%20into%20an%0Aevent-driven%20Model%20Predictive%20Control%20%28MPC%29%20strategy%20to%20regulate%20optic%20flow%20and%0Atrack%20a%20desired%20reference.%20A%20detailed%20convergence%20analysis%20ensures%20global%0Aconvergence%20of%20the%20tracking%20error%20to%20a%20uniform%20ultimate%20bound%20while%20ensuring%0AZeno-free%20behavior.%20Simulation%20results%20demonstrate%20the%20algorithm%27s%20robustness%0Aand%20effectiveness%20in%20landing%20on%20dynamic%20platforms%20under%20ground%20effect%20and%0Asensor%20noise%2C%20which%20compares%20favorably%20to%20non-adaptive%20event-triggered%20and%0Atime-triggered%20adaptive%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Based%2520Adaptive%2520Koopman%2520Framework%2520for%2520Optic%2520Flow-Guided%2520Landing%2520on%250A%2520%2520Moving%2520Platforms%26entry.906535625%3DBazeela%2520Banday%2520and%2520Chandan%2520Kumar%2520Sah%2520and%2520Jishnu%2520Keshavan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520optic%2520flow-guided%2520approach%2520for%2520achieving%2520soft%2520landings%250Aby%2520resource-constrained%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520on%2520dynamic%2520platforms.%250AAn%2520offline%2520data-driven%2520linear%2520model%2520based%2520on%2520Koopman%2520operator%2520theory%2520is%250Adeveloped%2520to%2520describe%2520the%2520underlying%2520%2528nonlinear%2529%2520dynamics%2520of%2520optic%2520flow%2520output%250Aobtained%2520from%2520a%2520single%2520monocular%2520camera%2520that%2520maps%2520to%2520vehicle%2520acceleration%2520as%250Athe%2520control%2520input.%2520Moreover%252C%2520a%2520novel%2520adaptation%2520scheme%2520within%2520the%2520Koopman%250Aframework%2520is%2520introduced%2520online%2520to%2520handle%2520uncertainties%2520such%2520as%2520unknown%2520platform%250Amotion%2520and%2520ground%2520effect%252C%2520which%2520exert%2520a%2520significant%2520influence%2520during%2520the%250Aterminal%2520stage%2520of%2520the%2520descent%2520process.%2520Further%252C%2520to%2520minimize%2520computational%250Aoverhead%252C%2520an%2520event-based%2520adaptation%2520trigger%2520is%2520incorporated%2520into%2520an%250Aevent-driven%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520strategy%2520to%2520regulate%2520optic%2520flow%2520and%250Atrack%2520a%2520desired%2520reference.%2520A%2520detailed%2520convergence%2520analysis%2520ensures%2520global%250Aconvergence%2520of%2520the%2520tracking%2520error%2520to%2520a%2520uniform%2520ultimate%2520bound%2520while%2520ensuring%250AZeno-free%2520behavior.%2520Simulation%2520results%2520demonstrate%2520the%2520algorithm%2527s%2520robustness%250Aand%2520effectiveness%2520in%2520landing%2520on%2520dynamic%2520platforms%2520under%2520ground%2520effect%2520and%250Asensor%2520noise%252C%2520which%2520compares%2520favorably%2520to%2520non-adaptive%2520event-triggered%2520and%250Atime-triggered%2520adaptive%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Based%20Adaptive%20Koopman%20Framework%20for%20Optic%20Flow-Guided%20Landing%20on%0A%20%20Moving%20Platforms&entry.906535625=Bazeela%20Banday%20and%20Chandan%20Kumar%20Sah%20and%20Jishnu%20Keshavan&entry.1292438233=%20%20This%20paper%20presents%20an%20optic%20flow-guided%20approach%20for%20achieving%20soft%20landings%0Aby%20resource-constrained%20unmanned%20aerial%20vehicles%20%28UAVs%29%20on%20dynamic%20platforms.%0AAn%20offline%20data-driven%20linear%20model%20based%20on%20Koopman%20operator%20theory%20is%0Adeveloped%20to%20describe%20the%20underlying%20%28nonlinear%29%20dynamics%20of%20optic%20flow%20output%0Aobtained%20from%20a%20single%20monocular%20camera%20that%20maps%20to%20vehicle%20acceleration%20as%0Athe%20control%20input.%20Moreover%2C%20a%20novel%20adaptation%20scheme%20within%20the%20Koopman%0Aframework%20is%20introduced%20online%20to%20handle%20uncertainties%20such%20as%20unknown%20platform%0Amotion%20and%20ground%20effect%2C%20which%20exert%20a%20significant%20influence%20during%20the%0Aterminal%20stage%20of%20the%20descent%20process.%20Further%2C%20to%20minimize%20computational%0Aoverhead%2C%20an%20event-based%20adaptation%20trigger%20is%20incorporated%20into%20an%0Aevent-driven%20Model%20Predictive%20Control%20%28MPC%29%20strategy%20to%20regulate%20optic%20flow%20and%0Atrack%20a%20desired%20reference.%20A%20detailed%20convergence%20analysis%20ensures%20global%0Aconvergence%20of%20the%20tracking%20error%20to%20a%20uniform%20ultimate%20bound%20while%20ensuring%0AZeno-free%20behavior.%20Simulation%20results%20demonstrate%20the%20algorithm%27s%20robustness%0Aand%20effectiveness%20in%20landing%20on%20dynamic%20platforms%20under%20ground%20effect%20and%0Asensor%20noise%2C%20which%20compares%20favorably%20to%20non-adaptive%20event-triggered%20and%0Atime-triggered%20adaptive%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16868v1&entry.124074799=Read"},
{"title": "Optimization and Learning in Open Multi-Agent Systems", "author": "Diego Deplano and Nicola Bastianello and Mauro Franceschelli and Karl H. Johansson", "abstract": "  Modern artificial intelligence relies on networks of agents that collect\ndata, process information, and exchange it with neighbors to collaboratively\nsolve optimization and learning problems. This article introduces a novel\ndistributed algorithm to address a broad class of these problems in \"open\nnetworks\", where the number of participating agents may vary due to several\nfactors, such as autonomous decisions, heterogeneous resource availability, or\nDoS attacks. Extending the current literature, the convergence analysis of the\nproposed algorithm is based on the newly developed \"Theory of Open Operators\",\nwhich characterizes an operator as open when the set of components to be\nupdated changes over time, yielding to time-varying operators acting on\nsequences of points of different dimensions and compositions. The mathematical\ntools and convergence results developed here provide a general framework for\nevaluating distributed algorithms in open networks, allowing to characterize\ntheir performance in terms of the punctual distance from the optimal solution,\nin contrast with regret-based metrics that assess cumulative performance over a\nfinite-time horizon. As illustrative examples, the proposed algorithm is used\nto solve dynamic consensus or tracking problems on different metrics of\ninterest, such as average, median, and min/max value, as well as classification\nproblems with logistic loss functions.\n", "link": "http://arxiv.org/abs/2501.16847v1", "date": "2025-01-28", "relevancy": 2.0908, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.537}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20and%20Learning%20in%20Open%20Multi-Agent%20Systems&body=Title%3A%20Optimization%20and%20Learning%20in%20Open%20Multi-Agent%20Systems%0AAuthor%3A%20Diego%20Deplano%20and%20Nicola%20Bastianello%20and%20Mauro%20Franceschelli%20and%20Karl%20H.%20Johansson%0AAbstract%3A%20%20%20Modern%20artificial%20intelligence%20relies%20on%20networks%20of%20agents%20that%20collect%0Adata%2C%20process%20information%2C%20and%20exchange%20it%20with%20neighbors%20to%20collaboratively%0Asolve%20optimization%20and%20learning%20problems.%20This%20article%20introduces%20a%20novel%0Adistributed%20algorithm%20to%20address%20a%20broad%20class%20of%20these%20problems%20in%20%22open%0Anetworks%22%2C%20where%20the%20number%20of%20participating%20agents%20may%20vary%20due%20to%20several%0Afactors%2C%20such%20as%20autonomous%20decisions%2C%20heterogeneous%20resource%20availability%2C%20or%0ADoS%20attacks.%20Extending%20the%20current%20literature%2C%20the%20convergence%20analysis%20of%20the%0Aproposed%20algorithm%20is%20based%20on%20the%20newly%20developed%20%22Theory%20of%20Open%20Operators%22%2C%0Awhich%20characterizes%20an%20operator%20as%20open%20when%20the%20set%20of%20components%20to%20be%0Aupdated%20changes%20over%20time%2C%20yielding%20to%20time-varying%20operators%20acting%20on%0Asequences%20of%20points%20of%20different%20dimensions%20and%20compositions.%20The%20mathematical%0Atools%20and%20convergence%20results%20developed%20here%20provide%20a%20general%20framework%20for%0Aevaluating%20distributed%20algorithms%20in%20open%20networks%2C%20allowing%20to%20characterize%0Atheir%20performance%20in%20terms%20of%20the%20punctual%20distance%20from%20the%20optimal%20solution%2C%0Ain%20contrast%20with%20regret-based%20metrics%20that%20assess%20cumulative%20performance%20over%20a%0Afinite-time%20horizon.%20As%20illustrative%20examples%2C%20the%20proposed%20algorithm%20is%20used%0Ato%20solve%20dynamic%20consensus%20or%20tracking%20problems%20on%20different%20metrics%20of%0Ainterest%2C%20such%20as%20average%2C%20median%2C%20and%20min/max%20value%2C%20as%20well%20as%20classification%0Aproblems%20with%20logistic%20loss%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520and%2520Learning%2520in%2520Open%2520Multi-Agent%2520Systems%26entry.906535625%3DDiego%2520Deplano%2520and%2520Nicola%2520Bastianello%2520and%2520Mauro%2520Franceschelli%2520and%2520Karl%2520H.%2520Johansson%26entry.1292438233%3D%2520%2520Modern%2520artificial%2520intelligence%2520relies%2520on%2520networks%2520of%2520agents%2520that%2520collect%250Adata%252C%2520process%2520information%252C%2520and%2520exchange%2520it%2520with%2520neighbors%2520to%2520collaboratively%250Asolve%2520optimization%2520and%2520learning%2520problems.%2520This%2520article%2520introduces%2520a%2520novel%250Adistributed%2520algorithm%2520to%2520address%2520a%2520broad%2520class%2520of%2520these%2520problems%2520in%2520%2522open%250Anetworks%2522%252C%2520where%2520the%2520number%2520of%2520participating%2520agents%2520may%2520vary%2520due%2520to%2520several%250Afactors%252C%2520such%2520as%2520autonomous%2520decisions%252C%2520heterogeneous%2520resource%2520availability%252C%2520or%250ADoS%2520attacks.%2520Extending%2520the%2520current%2520literature%252C%2520the%2520convergence%2520analysis%2520of%2520the%250Aproposed%2520algorithm%2520is%2520based%2520on%2520the%2520newly%2520developed%2520%2522Theory%2520of%2520Open%2520Operators%2522%252C%250Awhich%2520characterizes%2520an%2520operator%2520as%2520open%2520when%2520the%2520set%2520of%2520components%2520to%2520be%250Aupdated%2520changes%2520over%2520time%252C%2520yielding%2520to%2520time-varying%2520operators%2520acting%2520on%250Asequences%2520of%2520points%2520of%2520different%2520dimensions%2520and%2520compositions.%2520The%2520mathematical%250Atools%2520and%2520convergence%2520results%2520developed%2520here%2520provide%2520a%2520general%2520framework%2520for%250Aevaluating%2520distributed%2520algorithms%2520in%2520open%2520networks%252C%2520allowing%2520to%2520characterize%250Atheir%2520performance%2520in%2520terms%2520of%2520the%2520punctual%2520distance%2520from%2520the%2520optimal%2520solution%252C%250Ain%2520contrast%2520with%2520regret-based%2520metrics%2520that%2520assess%2520cumulative%2520performance%2520over%2520a%250Afinite-time%2520horizon.%2520As%2520illustrative%2520examples%252C%2520the%2520proposed%2520algorithm%2520is%2520used%250Ato%2520solve%2520dynamic%2520consensus%2520or%2520tracking%2520problems%2520on%2520different%2520metrics%2520of%250Ainterest%252C%2520such%2520as%2520average%252C%2520median%252C%2520and%2520min/max%2520value%252C%2520as%2520well%2520as%2520classification%250Aproblems%2520with%2520logistic%2520loss%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20and%20Learning%20in%20Open%20Multi-Agent%20Systems&entry.906535625=Diego%20Deplano%20and%20Nicola%20Bastianello%20and%20Mauro%20Franceschelli%20and%20Karl%20H.%20Johansson&entry.1292438233=%20%20Modern%20artificial%20intelligence%20relies%20on%20networks%20of%20agents%20that%20collect%0Adata%2C%20process%20information%2C%20and%20exchange%20it%20with%20neighbors%20to%20collaboratively%0Asolve%20optimization%20and%20learning%20problems.%20This%20article%20introduces%20a%20novel%0Adistributed%20algorithm%20to%20address%20a%20broad%20class%20of%20these%20problems%20in%20%22open%0Anetworks%22%2C%20where%20the%20number%20of%20participating%20agents%20may%20vary%20due%20to%20several%0Afactors%2C%20such%20as%20autonomous%20decisions%2C%20heterogeneous%20resource%20availability%2C%20or%0ADoS%20attacks.%20Extending%20the%20current%20literature%2C%20the%20convergence%20analysis%20of%20the%0Aproposed%20algorithm%20is%20based%20on%20the%20newly%20developed%20%22Theory%20of%20Open%20Operators%22%2C%0Awhich%20characterizes%20an%20operator%20as%20open%20when%20the%20set%20of%20components%20to%20be%0Aupdated%20changes%20over%20time%2C%20yielding%20to%20time-varying%20operators%20acting%20on%0Asequences%20of%20points%20of%20different%20dimensions%20and%20compositions.%20The%20mathematical%0Atools%20and%20convergence%20results%20developed%20here%20provide%20a%20general%20framework%20for%0Aevaluating%20distributed%20algorithms%20in%20open%20networks%2C%20allowing%20to%20characterize%0Atheir%20performance%20in%20terms%20of%20the%20punctual%20distance%20from%20the%20optimal%20solution%2C%0Ain%20contrast%20with%20regret-based%20metrics%20that%20assess%20cumulative%20performance%20over%20a%0Afinite-time%20horizon.%20As%20illustrative%20examples%2C%20the%20proposed%20algorithm%20is%20used%0Ato%20solve%20dynamic%20consensus%20or%20tracking%20problems%20on%20different%20metrics%20of%0Ainterest%2C%20such%20as%20average%2C%20median%2C%20and%20min/max%20value%2C%20as%20well%20as%20classification%0Aproblems%20with%20logistic%20loss%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16847v1&entry.124074799=Read"},
{"title": "COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via\n  Language Models", "author": "Tobias Materzok", "abstract": "  We present COS(M+O)S, a System 2-inspired framework for open-ended plot\ndevelopment that systematically explores the vast space of possible story\nexpansions, enabling a 3B-parameter language model to approach the plot quality\nof a 70B model on select short-story tasks. The method accomplishes this by\ncombining Monte Carlo Tree Search (MCTS), guided by a step-level value model\nthat rewards moderate surprisal (curiosity) while penalizing incoherence, and\nOdds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value\nplot expansions. This iterative reinforcement learning loop systematically\nexplores multiple candidate plot branches, backpropagates quality signals, and\nadapts the policy for faster convergence, notably shifting the policy from\npuzzle-based Chain-of-Thought to more character-driven storytelling. In\nsmall-scale tests with short-story prompts, 67%-77% of participants favored\nCOS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our\nlearned value function aligns. GPT-4o ratings further show that COS(M+O)S\nsurpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming\nwithin 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise\ncomparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no\nstatistically significant gap from 70B. Nevertheless, absolute story quality\nremains modest, constrained by the small model's capacity and limited training\ndata.\n", "link": "http://arxiv.org/abs/2501.17104v1", "date": "2025-01-28", "relevancy": 2.0883, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COS%28M%2BO%29S%3A%20Curiosity%20and%20RL-Enhanced%20MCTS%20for%20Exploring%20Story%20Space%20via%0A%20%20Language%20Models&body=Title%3A%20COS%28M%2BO%29S%3A%20Curiosity%20and%20RL-Enhanced%20MCTS%20for%20Exploring%20Story%20Space%20via%0A%20%20Language%20Models%0AAuthor%3A%20Tobias%20Materzok%0AAbstract%3A%20%20%20We%20present%20COS%28M%2BO%29S%2C%20a%20System%202-inspired%20framework%20for%20open-ended%20plot%0Adevelopment%20that%20systematically%20explores%20the%20vast%20space%20of%20possible%20story%0Aexpansions%2C%20enabling%20a%203B-parameter%20language%20model%20to%20approach%20the%20plot%20quality%0Aof%20a%2070B%20model%20on%20select%20short-story%20tasks.%20The%20method%20accomplishes%20this%20by%0Acombining%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20guided%20by%20a%20step-level%20value%20model%0Athat%20rewards%20moderate%20surprisal%20%28curiosity%29%20while%20penalizing%20incoherence%2C%20and%0AOdds%20Ratio%20Preference%20Optimization%20%28ORPO%29%20to%20fine-tune%20the%20policy%20on%20high-value%0Aplot%20expansions.%20This%20iterative%20reinforcement%20learning%20loop%20systematically%0Aexplores%20multiple%20candidate%20plot%20branches%2C%20backpropagates%20quality%20signals%2C%20and%0Aadapts%20the%20policy%20for%20faster%20convergence%2C%20notably%20shifting%20the%20policy%20from%0Apuzzle-based%20Chain-of-Thought%20to%20more%20character-driven%20storytelling.%20In%0Asmall-scale%20tests%20with%20short-story%20prompts%2C%2067%25-77%25%20of%20participants%20favored%0ACOS%28M%2BO%29S%27s%20highest-rated%20expansions%20over%20lower-rated%20ones%2C%20suggesting%20that%20our%0Alearned%20value%20function%20aligns.%20GPT-4o%20ratings%20further%20show%20that%20COS%28M%2BO%29S%0Asurpasses%20naive%20single-pass%20decoding%20from%20Llama%203.2%203B%20by%200.59%20SD%2C%20coming%0Awithin%200.06%20SD%20of%20Llama%203.1%2070B%20%28no%20significant%20difference%2C%20p%3D0.93%29.%20Pairwise%0Acomparisons%20with%20o1%20place%20COS%28M%2BO%29S%201.5%20SD%20above%20the%203B%20baseline%20and%20find%20no%0Astatistically%20significant%20gap%20from%2070B.%20Nevertheless%2C%20absolute%20story%20quality%0Aremains%20modest%2C%20constrained%20by%20the%20small%20model%27s%20capacity%20and%20limited%20training%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOS%2528M%252BO%2529S%253A%2520Curiosity%2520and%2520RL-Enhanced%2520MCTS%2520for%2520Exploring%2520Story%2520Space%2520via%250A%2520%2520Language%2520Models%26entry.906535625%3DTobias%2520Materzok%26entry.1292438233%3D%2520%2520We%2520present%2520COS%2528M%252BO%2529S%252C%2520a%2520System%25202-inspired%2520framework%2520for%2520open-ended%2520plot%250Adevelopment%2520that%2520systematically%2520explores%2520the%2520vast%2520space%2520of%2520possible%2520story%250Aexpansions%252C%2520enabling%2520a%25203B-parameter%2520language%2520model%2520to%2520approach%2520the%2520plot%2520quality%250Aof%2520a%252070B%2520model%2520on%2520select%2520short-story%2520tasks.%2520The%2520method%2520accomplishes%2520this%2520by%250Acombining%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%252C%2520guided%2520by%2520a%2520step-level%2520value%2520model%250Athat%2520rewards%2520moderate%2520surprisal%2520%2528curiosity%2529%2520while%2520penalizing%2520incoherence%252C%2520and%250AOdds%2520Ratio%2520Preference%2520Optimization%2520%2528ORPO%2529%2520to%2520fine-tune%2520the%2520policy%2520on%2520high-value%250Aplot%2520expansions.%2520This%2520iterative%2520reinforcement%2520learning%2520loop%2520systematically%250Aexplores%2520multiple%2520candidate%2520plot%2520branches%252C%2520backpropagates%2520quality%2520signals%252C%2520and%250Aadapts%2520the%2520policy%2520for%2520faster%2520convergence%252C%2520notably%2520shifting%2520the%2520policy%2520from%250Apuzzle-based%2520Chain-of-Thought%2520to%2520more%2520character-driven%2520storytelling.%2520In%250Asmall-scale%2520tests%2520with%2520short-story%2520prompts%252C%252067%2525-77%2525%2520of%2520participants%2520favored%250ACOS%2528M%252BO%2529S%2527s%2520highest-rated%2520expansions%2520over%2520lower-rated%2520ones%252C%2520suggesting%2520that%2520our%250Alearned%2520value%2520function%2520aligns.%2520GPT-4o%2520ratings%2520further%2520show%2520that%2520COS%2528M%252BO%2529S%250Asurpasses%2520naive%2520single-pass%2520decoding%2520from%2520Llama%25203.2%25203B%2520by%25200.59%2520SD%252C%2520coming%250Awithin%25200.06%2520SD%2520of%2520Llama%25203.1%252070B%2520%2528no%2520significant%2520difference%252C%2520p%253D0.93%2529.%2520Pairwise%250Acomparisons%2520with%2520o1%2520place%2520COS%2528M%252BO%2529S%25201.5%2520SD%2520above%2520the%25203B%2520baseline%2520and%2520find%2520no%250Astatistically%2520significant%2520gap%2520from%252070B.%2520Nevertheless%252C%2520absolute%2520story%2520quality%250Aremains%2520modest%252C%2520constrained%2520by%2520the%2520small%2520model%2527s%2520capacity%2520and%2520limited%2520training%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COS%28M%2BO%29S%3A%20Curiosity%20and%20RL-Enhanced%20MCTS%20for%20Exploring%20Story%20Space%20via%0A%20%20Language%20Models&entry.906535625=Tobias%20Materzok&entry.1292438233=%20%20We%20present%20COS%28M%2BO%29S%2C%20a%20System%202-inspired%20framework%20for%20open-ended%20plot%0Adevelopment%20that%20systematically%20explores%20the%20vast%20space%20of%20possible%20story%0Aexpansions%2C%20enabling%20a%203B-parameter%20language%20model%20to%20approach%20the%20plot%20quality%0Aof%20a%2070B%20model%20on%20select%20short-story%20tasks.%20The%20method%20accomplishes%20this%20by%0Acombining%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20guided%20by%20a%20step-level%20value%20model%0Athat%20rewards%20moderate%20surprisal%20%28curiosity%29%20while%20penalizing%20incoherence%2C%20and%0AOdds%20Ratio%20Preference%20Optimization%20%28ORPO%29%20to%20fine-tune%20the%20policy%20on%20high-value%0Aplot%20expansions.%20This%20iterative%20reinforcement%20learning%20loop%20systematically%0Aexplores%20multiple%20candidate%20plot%20branches%2C%20backpropagates%20quality%20signals%2C%20and%0Aadapts%20the%20policy%20for%20faster%20convergence%2C%20notably%20shifting%20the%20policy%20from%0Apuzzle-based%20Chain-of-Thought%20to%20more%20character-driven%20storytelling.%20In%0Asmall-scale%20tests%20with%20short-story%20prompts%2C%2067%25-77%25%20of%20participants%20favored%0ACOS%28M%2BO%29S%27s%20highest-rated%20expansions%20over%20lower-rated%20ones%2C%20suggesting%20that%20our%0Alearned%20value%20function%20aligns.%20GPT-4o%20ratings%20further%20show%20that%20COS%28M%2BO%29S%0Asurpasses%20naive%20single-pass%20decoding%20from%20Llama%203.2%203B%20by%200.59%20SD%2C%20coming%0Awithin%200.06%20SD%20of%20Llama%203.1%2070B%20%28no%20significant%20difference%2C%20p%3D0.93%29.%20Pairwise%0Acomparisons%20with%20o1%20place%20COS%28M%2BO%29S%201.5%20SD%20above%20the%203B%20baseline%20and%20find%20no%0Astatistically%20significant%20gap%20from%2070B.%20Nevertheless%2C%20absolute%20story%20quality%0Aremains%20modest%2C%20constrained%20by%20the%20small%20model%27s%20capacity%20and%20limited%20training%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17104v1&entry.124074799=Read"},
{"title": "Accelerated Training through Iterative Gradient Propagation Along the\n  Residual Path", "author": "Erwan Fagnou and Paul Caillon and Blaise Delattre and Alexandre Allauzen", "abstract": "  Despite being the cornerstone of deep learning, backpropagation is criticized\nfor its inherent sequentiality, which can limit the scalability of very deep\nmodels. Such models faced convergence issues due to vanishing gradient, later\nresolved using residual connections. Variants of these are now widely used in\nmodern architecture. However, the computational cost of backpropagation remains\na major burden, accounting for most of the training time. Taking advantage of\nresidual-like architectural designs, we introduce Highway backpropagation, a\nparallelizable iterative algorithm that approximates backpropagation, by\nalternatively i) accumulating the gradient estimates along the residual path,\nand ii) backpropagating them through every layer in parallel. This algorithm is\nnaturally derived from a decomposition of the gradient as the sum of gradients\nflowing through all paths and is adaptable to a diverse set of common\narchitectures, ranging from ResNets and Transformers to recurrent neural\nnetworks. Through an extensive empirical study on a large selection of tasks\nand models, we evaluate Highway-BP and show that major speedups can be achieved\nwith minimal performance degradation.\n", "link": "http://arxiv.org/abs/2501.17086v1", "date": "2025-01-28", "relevancy": 2.0871, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5317}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5283}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Training%20through%20Iterative%20Gradient%20Propagation%20Along%20the%0A%20%20Residual%20Path&body=Title%3A%20Accelerated%20Training%20through%20Iterative%20Gradient%20Propagation%20Along%20the%0A%20%20Residual%20Path%0AAuthor%3A%20Erwan%20Fagnou%20and%20Paul%20Caillon%20and%20Blaise%20Delattre%20and%20Alexandre%20Allauzen%0AAbstract%3A%20%20%20Despite%20being%20the%20cornerstone%20of%20deep%20learning%2C%20backpropagation%20is%20criticized%0Afor%20its%20inherent%20sequentiality%2C%20which%20can%20limit%20the%20scalability%20of%20very%20deep%0Amodels.%20Such%20models%20faced%20convergence%20issues%20due%20to%20vanishing%20gradient%2C%20later%0Aresolved%20using%20residual%20connections.%20Variants%20of%20these%20are%20now%20widely%20used%20in%0Amodern%20architecture.%20However%2C%20the%20computational%20cost%20of%20backpropagation%20remains%0Aa%20major%20burden%2C%20accounting%20for%20most%20of%20the%20training%20time.%20Taking%20advantage%20of%0Aresidual-like%20architectural%20designs%2C%20we%20introduce%20Highway%20backpropagation%2C%20a%0Aparallelizable%20iterative%20algorithm%20that%20approximates%20backpropagation%2C%20by%0Aalternatively%20i%29%20accumulating%20the%20gradient%20estimates%20along%20the%20residual%20path%2C%0Aand%20ii%29%20backpropagating%20them%20through%20every%20layer%20in%20parallel.%20This%20algorithm%20is%0Anaturally%20derived%20from%20a%20decomposition%20of%20the%20gradient%20as%20the%20sum%20of%20gradients%0Aflowing%20through%20all%20paths%20and%20is%20adaptable%20to%20a%20diverse%20set%20of%20common%0Aarchitectures%2C%20ranging%20from%20ResNets%20and%20Transformers%20to%20recurrent%20neural%0Anetworks.%20Through%20an%20extensive%20empirical%20study%20on%20a%20large%20selection%20of%20tasks%0Aand%20models%2C%20we%20evaluate%20Highway-BP%20and%20show%20that%20major%20speedups%20can%20be%20achieved%0Awith%20minimal%20performance%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Training%2520through%2520Iterative%2520Gradient%2520Propagation%2520Along%2520the%250A%2520%2520Residual%2520Path%26entry.906535625%3DErwan%2520Fagnou%2520and%2520Paul%2520Caillon%2520and%2520Blaise%2520Delattre%2520and%2520Alexandre%2520Allauzen%26entry.1292438233%3D%2520%2520Despite%2520being%2520the%2520cornerstone%2520of%2520deep%2520learning%252C%2520backpropagation%2520is%2520criticized%250Afor%2520its%2520inherent%2520sequentiality%252C%2520which%2520can%2520limit%2520the%2520scalability%2520of%2520very%2520deep%250Amodels.%2520Such%2520models%2520faced%2520convergence%2520issues%2520due%2520to%2520vanishing%2520gradient%252C%2520later%250Aresolved%2520using%2520residual%2520connections.%2520Variants%2520of%2520these%2520are%2520now%2520widely%2520used%2520in%250Amodern%2520architecture.%2520However%252C%2520the%2520computational%2520cost%2520of%2520backpropagation%2520remains%250Aa%2520major%2520burden%252C%2520accounting%2520for%2520most%2520of%2520the%2520training%2520time.%2520Taking%2520advantage%2520of%250Aresidual-like%2520architectural%2520designs%252C%2520we%2520introduce%2520Highway%2520backpropagation%252C%2520a%250Aparallelizable%2520iterative%2520algorithm%2520that%2520approximates%2520backpropagation%252C%2520by%250Aalternatively%2520i%2529%2520accumulating%2520the%2520gradient%2520estimates%2520along%2520the%2520residual%2520path%252C%250Aand%2520ii%2529%2520backpropagating%2520them%2520through%2520every%2520layer%2520in%2520parallel.%2520This%2520algorithm%2520is%250Anaturally%2520derived%2520from%2520a%2520decomposition%2520of%2520the%2520gradient%2520as%2520the%2520sum%2520of%2520gradients%250Aflowing%2520through%2520all%2520paths%2520and%2520is%2520adaptable%2520to%2520a%2520diverse%2520set%2520of%2520common%250Aarchitectures%252C%2520ranging%2520from%2520ResNets%2520and%2520Transformers%2520to%2520recurrent%2520neural%250Anetworks.%2520Through%2520an%2520extensive%2520empirical%2520study%2520on%2520a%2520large%2520selection%2520of%2520tasks%250Aand%2520models%252C%2520we%2520evaluate%2520Highway-BP%2520and%2520show%2520that%2520major%2520speedups%2520can%2520be%2520achieved%250Awith%2520minimal%2520performance%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Training%20through%20Iterative%20Gradient%20Propagation%20Along%20the%0A%20%20Residual%20Path&entry.906535625=Erwan%20Fagnou%20and%20Paul%20Caillon%20and%20Blaise%20Delattre%20and%20Alexandre%20Allauzen&entry.1292438233=%20%20Despite%20being%20the%20cornerstone%20of%20deep%20learning%2C%20backpropagation%20is%20criticized%0Afor%20its%20inherent%20sequentiality%2C%20which%20can%20limit%20the%20scalability%20of%20very%20deep%0Amodels.%20Such%20models%20faced%20convergence%20issues%20due%20to%20vanishing%20gradient%2C%20later%0Aresolved%20using%20residual%20connections.%20Variants%20of%20these%20are%20now%20widely%20used%20in%0Amodern%20architecture.%20However%2C%20the%20computational%20cost%20of%20backpropagation%20remains%0Aa%20major%20burden%2C%20accounting%20for%20most%20of%20the%20training%20time.%20Taking%20advantage%20of%0Aresidual-like%20architectural%20designs%2C%20we%20introduce%20Highway%20backpropagation%2C%20a%0Aparallelizable%20iterative%20algorithm%20that%20approximates%20backpropagation%2C%20by%0Aalternatively%20i%29%20accumulating%20the%20gradient%20estimates%20along%20the%20residual%20path%2C%0Aand%20ii%29%20backpropagating%20them%20through%20every%20layer%20in%20parallel.%20This%20algorithm%20is%0Anaturally%20derived%20from%20a%20decomposition%20of%20the%20gradient%20as%20the%20sum%20of%20gradients%0Aflowing%20through%20all%20paths%20and%20is%20adaptable%20to%20a%20diverse%20set%20of%20common%0Aarchitectures%2C%20ranging%20from%20ResNets%20and%20Transformers%20to%20recurrent%20neural%0Anetworks.%20Through%20an%20extensive%20empirical%20study%20on%20a%20large%20selection%20of%20tasks%0Aand%20models%2C%20we%20evaluate%20Highway-BP%20and%20show%20that%20major%20speedups%20can%20be%20achieved%0Awith%20minimal%20performance%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17086v1&entry.124074799=Read"},
{"title": "GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via\n  Generative Feature Extraction from MCI", "author": "Zhaojie Fang and Shenghao Zhu and Yifei Chen and Binfeng Zou and Fan Jia and Linwei Qiu and Chang Liu and Xiang Feng and Changmiao Wang and Feiwei Qin and Jin Fan and Changbiao Chu", "abstract": "  Alzheimer's Disease (AD) is a progressive, irreversible neurodegenerative\ndisorder that often originates from Mild Cognitive Impairment (MCI). This\nprogression results in significant memory loss and severely affects patients'\nquality of life. Clinical trials have consistently shown that early and\ntargeted interventions for individuals with MCI may slow or even prevent the\nadvancement of AD. Research indicates that accurate medical classification\nrequires diverse multimodal data, including detailed assessment scales and\nneuroimaging techniques like Magnetic Resonance Imaging (MRI) and Positron\nEmission Tomography (PET). However, simultaneously collecting the\naforementioned three modalities for training presents substantial challenges.\nTo tackle these difficulties, we propose GFE-Mamba, a multimodal classifier\nfounded on Generative Feature Extractor. The intermediate features provided by\nthis Extractor can compensate for the shortcomings of PET and achieve profound\nmultimodal fusion in the classifier. The Mamba block, as the backbone of the\nclassifier, enables it to efficiently extract information from long-sequence\nscale information. Pixel-level Bi-cross Attention supplements pixel-level\ninformation from MRI and PET. We provide our rationale for developing this\ncross-temporal progression prediction dataset and the pre-trained Extractor\nweights. Our experimental findings reveal that the GFE-Mamba model effectively\npredicts the progression from MCI to AD and surpasses several leading methods\nin the field. Our source code is available at\nhttps://github.com/Tinysqua/GFE-Mamba.\n", "link": "http://arxiv.org/abs/2407.15719v2", "date": "2025-01-28", "relevancy": 2.0837, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5233}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GFE-Mamba%3A%20Mamba-based%20AD%20Multi-modal%20Progression%20Assessment%20via%0A%20%20Generative%20Feature%20Extraction%20from%20MCI&body=Title%3A%20GFE-Mamba%3A%20Mamba-based%20AD%20Multi-modal%20Progression%20Assessment%20via%0A%20%20Generative%20Feature%20Extraction%20from%20MCI%0AAuthor%3A%20Zhaojie%20Fang%20and%20Shenghao%20Zhu%20and%20Yifei%20Chen%20and%20Binfeng%20Zou%20and%20Fan%20Jia%20and%20Linwei%20Qiu%20and%20Chang%20Liu%20and%20Xiang%20Feng%20and%20Changmiao%20Wang%20and%20Feiwei%20Qin%20and%20Jin%20Fan%20and%20Changbiao%20Chu%0AAbstract%3A%20%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20a%20progressive%2C%20irreversible%20neurodegenerative%0Adisorder%20that%20often%20originates%20from%20Mild%20Cognitive%20Impairment%20%28MCI%29.%20This%0Aprogression%20results%20in%20significant%20memory%20loss%20and%20severely%20affects%20patients%27%0Aquality%20of%20life.%20Clinical%20trials%20have%20consistently%20shown%20that%20early%20and%0Atargeted%20interventions%20for%20individuals%20with%20MCI%20may%20slow%20or%20even%20prevent%20the%0Aadvancement%20of%20AD.%20Research%20indicates%20that%20accurate%20medical%20classification%0Arequires%20diverse%20multimodal%20data%2C%20including%20detailed%20assessment%20scales%20and%0Aneuroimaging%20techniques%20like%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20and%20Positron%0AEmission%20Tomography%20%28PET%29.%20However%2C%20simultaneously%20collecting%20the%0Aaforementioned%20three%20modalities%20for%20training%20presents%20substantial%20challenges.%0ATo%20tackle%20these%20difficulties%2C%20we%20propose%20GFE-Mamba%2C%20a%20multimodal%20classifier%0Afounded%20on%20Generative%20Feature%20Extractor.%20The%20intermediate%20features%20provided%20by%0Athis%20Extractor%20can%20compensate%20for%20the%20shortcomings%20of%20PET%20and%20achieve%20profound%0Amultimodal%20fusion%20in%20the%20classifier.%20The%20Mamba%20block%2C%20as%20the%20backbone%20of%20the%0Aclassifier%2C%20enables%20it%20to%20efficiently%20extract%20information%20from%20long-sequence%0Ascale%20information.%20Pixel-level%20Bi-cross%20Attention%20supplements%20pixel-level%0Ainformation%20from%20MRI%20and%20PET.%20We%20provide%20our%20rationale%20for%20developing%20this%0Across-temporal%20progression%20prediction%20dataset%20and%20the%20pre-trained%20Extractor%0Aweights.%20Our%20experimental%20findings%20reveal%20that%20the%20GFE-Mamba%20model%20effectively%0Apredicts%20the%20progression%20from%20MCI%20to%20AD%20and%20surpasses%20several%20leading%20methods%0Ain%20the%20field.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Tinysqua/GFE-Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGFE-Mamba%253A%2520Mamba-based%2520AD%2520Multi-modal%2520Progression%2520Assessment%2520via%250A%2520%2520Generative%2520Feature%2520Extraction%2520from%2520MCI%26entry.906535625%3DZhaojie%2520Fang%2520and%2520Shenghao%2520Zhu%2520and%2520Yifei%2520Chen%2520and%2520Binfeng%2520Zou%2520and%2520Fan%2520Jia%2520and%2520Linwei%2520Qiu%2520and%2520Chang%2520Liu%2520and%2520Xiang%2520Feng%2520and%2520Changmiao%2520Wang%2520and%2520Feiwei%2520Qin%2520and%2520Jin%2520Fan%2520and%2520Changbiao%2520Chu%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%2520a%2520progressive%252C%2520irreversible%2520neurodegenerative%250Adisorder%2520that%2520often%2520originates%2520from%2520Mild%2520Cognitive%2520Impairment%2520%2528MCI%2529.%2520This%250Aprogression%2520results%2520in%2520significant%2520memory%2520loss%2520and%2520severely%2520affects%2520patients%2527%250Aquality%2520of%2520life.%2520Clinical%2520trials%2520have%2520consistently%2520shown%2520that%2520early%2520and%250Atargeted%2520interventions%2520for%2520individuals%2520with%2520MCI%2520may%2520slow%2520or%2520even%2520prevent%2520the%250Aadvancement%2520of%2520AD.%2520Research%2520indicates%2520that%2520accurate%2520medical%2520classification%250Arequires%2520diverse%2520multimodal%2520data%252C%2520including%2520detailed%2520assessment%2520scales%2520and%250Aneuroimaging%2520techniques%2520like%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520and%2520Positron%250AEmission%2520Tomography%2520%2528PET%2529.%2520However%252C%2520simultaneously%2520collecting%2520the%250Aaforementioned%2520three%2520modalities%2520for%2520training%2520presents%2520substantial%2520challenges.%250ATo%2520tackle%2520these%2520difficulties%252C%2520we%2520propose%2520GFE-Mamba%252C%2520a%2520multimodal%2520classifier%250Afounded%2520on%2520Generative%2520Feature%2520Extractor.%2520The%2520intermediate%2520features%2520provided%2520by%250Athis%2520Extractor%2520can%2520compensate%2520for%2520the%2520shortcomings%2520of%2520PET%2520and%2520achieve%2520profound%250Amultimodal%2520fusion%2520in%2520the%2520classifier.%2520The%2520Mamba%2520block%252C%2520as%2520the%2520backbone%2520of%2520the%250Aclassifier%252C%2520enables%2520it%2520to%2520efficiently%2520extract%2520information%2520from%2520long-sequence%250Ascale%2520information.%2520Pixel-level%2520Bi-cross%2520Attention%2520supplements%2520pixel-level%250Ainformation%2520from%2520MRI%2520and%2520PET.%2520We%2520provide%2520our%2520rationale%2520for%2520developing%2520this%250Across-temporal%2520progression%2520prediction%2520dataset%2520and%2520the%2520pre-trained%2520Extractor%250Aweights.%2520Our%2520experimental%2520findings%2520reveal%2520that%2520the%2520GFE-Mamba%2520model%2520effectively%250Apredicts%2520the%2520progression%2520from%2520MCI%2520to%2520AD%2520and%2520surpasses%2520several%2520leading%2520methods%250Ain%2520the%2520field.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Tinysqua/GFE-Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GFE-Mamba%3A%20Mamba-based%20AD%20Multi-modal%20Progression%20Assessment%20via%0A%20%20Generative%20Feature%20Extraction%20from%20MCI&entry.906535625=Zhaojie%20Fang%20and%20Shenghao%20Zhu%20and%20Yifei%20Chen%20and%20Binfeng%20Zou%20and%20Fan%20Jia%20and%20Linwei%20Qiu%20and%20Chang%20Liu%20and%20Xiang%20Feng%20and%20Changmiao%20Wang%20and%20Feiwei%20Qin%20and%20Jin%20Fan%20and%20Changbiao%20Chu&entry.1292438233=%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20a%20progressive%2C%20irreversible%20neurodegenerative%0Adisorder%20that%20often%20originates%20from%20Mild%20Cognitive%20Impairment%20%28MCI%29.%20This%0Aprogression%20results%20in%20significant%20memory%20loss%20and%20severely%20affects%20patients%27%0Aquality%20of%20life.%20Clinical%20trials%20have%20consistently%20shown%20that%20early%20and%0Atargeted%20interventions%20for%20individuals%20with%20MCI%20may%20slow%20or%20even%20prevent%20the%0Aadvancement%20of%20AD.%20Research%20indicates%20that%20accurate%20medical%20classification%0Arequires%20diverse%20multimodal%20data%2C%20including%20detailed%20assessment%20scales%20and%0Aneuroimaging%20techniques%20like%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20and%20Positron%0AEmission%20Tomography%20%28PET%29.%20However%2C%20simultaneously%20collecting%20the%0Aaforementioned%20three%20modalities%20for%20training%20presents%20substantial%20challenges.%0ATo%20tackle%20these%20difficulties%2C%20we%20propose%20GFE-Mamba%2C%20a%20multimodal%20classifier%0Afounded%20on%20Generative%20Feature%20Extractor.%20The%20intermediate%20features%20provided%20by%0Athis%20Extractor%20can%20compensate%20for%20the%20shortcomings%20of%20PET%20and%20achieve%20profound%0Amultimodal%20fusion%20in%20the%20classifier.%20The%20Mamba%20block%2C%20as%20the%20backbone%20of%20the%0Aclassifier%2C%20enables%20it%20to%20efficiently%20extract%20information%20from%20long-sequence%0Ascale%20information.%20Pixel-level%20Bi-cross%20Attention%20supplements%20pixel-level%0Ainformation%20from%20MRI%20and%20PET.%20We%20provide%20our%20rationale%20for%20developing%20this%0Across-temporal%20progression%20prediction%20dataset%20and%20the%20pre-trained%20Extractor%0Aweights.%20Our%20experimental%20findings%20reveal%20that%20the%20GFE-Mamba%20model%20effectively%0Apredicts%20the%20progression%20from%20MCI%20to%20AD%20and%20surpasses%20several%20leading%20methods%0Ain%20the%20field.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Tinysqua/GFE-Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15719v2&entry.124074799=Read"},
{"title": "Random-Set Neural Networks (RS-NN)", "author": "Shireen Kudukkil Manchingal and Muhammad Mubashar and Kaizheng Wang and Keivan Shariatmadar and Fabio Cuzzolin", "abstract": "  Machine learning is increasingly deployed in safety-critical domains where\nerroneous predictions may lead to potentially catastrophic consequences,\nhighlighting the need for learning systems to be aware of how confident they\nare in their own predictions: in other words, 'to know when they do not know'.\nIn this paper, we propose a novel Random-Set Neural Network (RS-NN) approach to\nclassification which predicts belief functions (rather than classical\nprobability vectors) over the class list using the mathematics of random sets,\ni.e., distributions over the collection of sets of classes. RS-NN encodes the\n'epistemic' uncertainty induced by training sets that are insufficiently\nrepresentative or limited in size via the size of the convex set of probability\nvectors associated with a predicted belief function. Our approach outperforms\nstate-of-the-art Bayesian and Ensemble methods in terms of accuracy,\nuncertainty estimation and out-of-distribution (OoD) detection on multiple\nbenchmarks (CIFAR-10 vs SVHN/Intel-Image, MNIST vs FMNIST/KMNIST, ImageNet vs\nImageNet-O). RS-NN also scales up effectively to large-scale architectures\n(e.g. WideResNet-28-10, VGG16, Inception V3, EfficientNetB2 and ViT-Base-16),\nexhibits remarkable robustness to adversarial attacks and can provide\nstatistical guarantees in a conformal learning setting.\n", "link": "http://arxiv.org/abs/2307.05772v4", "date": "2025-01-28", "relevancy": 2.0757, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random-Set%20Neural%20Networks%20%28RS-NN%29&body=Title%3A%20Random-Set%20Neural%20Networks%20%28RS-NN%29%0AAuthor%3A%20Shireen%20Kudukkil%20Manchingal%20and%20Muhammad%20Mubashar%20and%20Kaizheng%20Wang%20and%20Keivan%20Shariatmadar%20and%20Fabio%20Cuzzolin%0AAbstract%3A%20%20%20Machine%20learning%20is%20increasingly%20deployed%20in%20safety-critical%20domains%20where%0Aerroneous%20predictions%20may%20lead%20to%20potentially%20catastrophic%20consequences%2C%0Ahighlighting%20the%20need%20for%20learning%20systems%20to%20be%20aware%20of%20how%20confident%20they%0Aare%20in%20their%20own%20predictions%3A%20in%20other%20words%2C%20%27to%20know%20when%20they%20do%20not%20know%27.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20Random-Set%20Neural%20Network%20%28RS-NN%29%20approach%20to%0Aclassification%20which%20predicts%20belief%20functions%20%28rather%20than%20classical%0Aprobability%20vectors%29%20over%20the%20class%20list%20using%20the%20mathematics%20of%20random%20sets%2C%0Ai.e.%2C%20distributions%20over%20the%20collection%20of%20sets%20of%20classes.%20RS-NN%20encodes%20the%0A%27epistemic%27%20uncertainty%20induced%20by%20training%20sets%20that%20are%20insufficiently%0Arepresentative%20or%20limited%20in%20size%20via%20the%20size%20of%20the%20convex%20set%20of%20probability%0Avectors%20associated%20with%20a%20predicted%20belief%20function.%20Our%20approach%20outperforms%0Astate-of-the-art%20Bayesian%20and%20Ensemble%20methods%20in%20terms%20of%20accuracy%2C%0Auncertainty%20estimation%20and%20out-of-distribution%20%28OoD%29%20detection%20on%20multiple%0Abenchmarks%20%28CIFAR-10%20vs%20SVHN/Intel-Image%2C%20MNIST%20vs%20FMNIST/KMNIST%2C%20ImageNet%20vs%0AImageNet-O%29.%20RS-NN%20also%20scales%20up%20effectively%20to%20large-scale%20architectures%0A%28e.g.%20WideResNet-28-10%2C%20VGG16%2C%20Inception%20V3%2C%20EfficientNetB2%20and%20ViT-Base-16%29%2C%0Aexhibits%20remarkable%20robustness%20to%20adversarial%20attacks%20and%20can%20provide%0Astatistical%20guarantees%20in%20a%20conformal%20learning%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05772v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom-Set%2520Neural%2520Networks%2520%2528RS-NN%2529%26entry.906535625%3DShireen%2520Kudukkil%2520Manchingal%2520and%2520Muhammad%2520Mubashar%2520and%2520Kaizheng%2520Wang%2520and%2520Keivan%2520Shariatmadar%2520and%2520Fabio%2520Cuzzolin%26entry.1292438233%3D%2520%2520Machine%2520learning%2520is%2520increasingly%2520deployed%2520in%2520safety-critical%2520domains%2520where%250Aerroneous%2520predictions%2520may%2520lead%2520to%2520potentially%2520catastrophic%2520consequences%252C%250Ahighlighting%2520the%2520need%2520for%2520learning%2520systems%2520to%2520be%2520aware%2520of%2520how%2520confident%2520they%250Aare%2520in%2520their%2520own%2520predictions%253A%2520in%2520other%2520words%252C%2520%2527to%2520know%2520when%2520they%2520do%2520not%2520know%2527.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Random-Set%2520Neural%2520Network%2520%2528RS-NN%2529%2520approach%2520to%250Aclassification%2520which%2520predicts%2520belief%2520functions%2520%2528rather%2520than%2520classical%250Aprobability%2520vectors%2529%2520over%2520the%2520class%2520list%2520using%2520the%2520mathematics%2520of%2520random%2520sets%252C%250Ai.e.%252C%2520distributions%2520over%2520the%2520collection%2520of%2520sets%2520of%2520classes.%2520RS-NN%2520encodes%2520the%250A%2527epistemic%2527%2520uncertainty%2520induced%2520by%2520training%2520sets%2520that%2520are%2520insufficiently%250Arepresentative%2520or%2520limited%2520in%2520size%2520via%2520the%2520size%2520of%2520the%2520convex%2520set%2520of%2520probability%250Avectors%2520associated%2520with%2520a%2520predicted%2520belief%2520function.%2520Our%2520approach%2520outperforms%250Astate-of-the-art%2520Bayesian%2520and%2520Ensemble%2520methods%2520in%2520terms%2520of%2520accuracy%252C%250Auncertainty%2520estimation%2520and%2520out-of-distribution%2520%2528OoD%2529%2520detection%2520on%2520multiple%250Abenchmarks%2520%2528CIFAR-10%2520vs%2520SVHN/Intel-Image%252C%2520MNIST%2520vs%2520FMNIST/KMNIST%252C%2520ImageNet%2520vs%250AImageNet-O%2529.%2520RS-NN%2520also%2520scales%2520up%2520effectively%2520to%2520large-scale%2520architectures%250A%2528e.g.%2520WideResNet-28-10%252C%2520VGG16%252C%2520Inception%2520V3%252C%2520EfficientNetB2%2520and%2520ViT-Base-16%2529%252C%250Aexhibits%2520remarkable%2520robustness%2520to%2520adversarial%2520attacks%2520and%2520can%2520provide%250Astatistical%2520guarantees%2520in%2520a%2520conformal%2520learning%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.05772v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random-Set%20Neural%20Networks%20%28RS-NN%29&entry.906535625=Shireen%20Kudukkil%20Manchingal%20and%20Muhammad%20Mubashar%20and%20Kaizheng%20Wang%20and%20Keivan%20Shariatmadar%20and%20Fabio%20Cuzzolin&entry.1292438233=%20%20Machine%20learning%20is%20increasingly%20deployed%20in%20safety-critical%20domains%20where%0Aerroneous%20predictions%20may%20lead%20to%20potentially%20catastrophic%20consequences%2C%0Ahighlighting%20the%20need%20for%20learning%20systems%20to%20be%20aware%20of%20how%20confident%20they%0Aare%20in%20their%20own%20predictions%3A%20in%20other%20words%2C%20%27to%20know%20when%20they%20do%20not%20know%27.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20Random-Set%20Neural%20Network%20%28RS-NN%29%20approach%20to%0Aclassification%20which%20predicts%20belief%20functions%20%28rather%20than%20classical%0Aprobability%20vectors%29%20over%20the%20class%20list%20using%20the%20mathematics%20of%20random%20sets%2C%0Ai.e.%2C%20distributions%20over%20the%20collection%20of%20sets%20of%20classes.%20RS-NN%20encodes%20the%0A%27epistemic%27%20uncertainty%20induced%20by%20training%20sets%20that%20are%20insufficiently%0Arepresentative%20or%20limited%20in%20size%20via%20the%20size%20of%20the%20convex%20set%20of%20probability%0Avectors%20associated%20with%20a%20predicted%20belief%20function.%20Our%20approach%20outperforms%0Astate-of-the-art%20Bayesian%20and%20Ensemble%20methods%20in%20terms%20of%20accuracy%2C%0Auncertainty%20estimation%20and%20out-of-distribution%20%28OoD%29%20detection%20on%20multiple%0Abenchmarks%20%28CIFAR-10%20vs%20SVHN/Intel-Image%2C%20MNIST%20vs%20FMNIST/KMNIST%2C%20ImageNet%20vs%0AImageNet-O%29.%20RS-NN%20also%20scales%20up%20effectively%20to%20large-scale%20architectures%0A%28e.g.%20WideResNet-28-10%2C%20VGG16%2C%20Inception%20V3%2C%20EfficientNetB2%20and%20ViT-Base-16%29%2C%0Aexhibits%20remarkable%20robustness%20to%20adversarial%20attacks%20and%20can%20provide%0Astatistical%20guarantees%20in%20a%20conformal%20learning%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05772v4&entry.124074799=Read"},
{"title": "Conditional Distribution Learning on Graphs", "author": "Jie Chen and Hua Mao and Yuanbiao Gou and Zhu Wang and Xi Peng", "abstract": "  Leveraging the diversity and quantity of data provided by various\ngraph-structured data augmentations while preserving intrinsic semantic\ninformation is challenging. Additionally, successive layers in graph neural\nnetwork (GNN) tend to produce more similar node embeddings, while graph\ncontrastive learning aims to increase the dissimilarity between negative pairs\nof node embeddings. This inevitably results in a conflict between the\nmessage-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of\nnegative pairs via intraviews. In this paper, we propose a conditional\ndistribution learning (CDL) method that learns graph representations from\ngraph-structured data for semisupervised graph classification. Specifically, we\npresent an end-to-end graph representation learning model to align the\nconditional distributions of weakly and strongly augmented features over the\noriginal features. This alignment enables the CDL model to effectively preserve\nintrinsic semantic information when both weak and strong augmentations are\napplied to graph-structured data. To avoid the conflict between the MPM and the\nCL of negative pairs, positive pairs of node representations are retained for\nmeasuring the similarity between the original features and the corresponding\nweakly augmented features. Extensive experiments with several benchmark graph\ndatasets demonstrate the effectiveness of the proposed CDL method.\n", "link": "http://arxiv.org/abs/2411.15206v2", "date": "2025-01-28", "relevancy": 2.0703, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5375}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Distribution%20Learning%20on%20Graphs&body=Title%3A%20Conditional%20Distribution%20Learning%20on%20Graphs%0AAuthor%3A%20Jie%20Chen%20and%20Hua%20Mao%20and%20Yuanbiao%20Gou%20and%20Zhu%20Wang%20and%20Xi%20Peng%0AAbstract%3A%20%20%20Leveraging%20the%20diversity%20and%20quantity%20of%20data%20provided%20by%20various%0Agraph-structured%20data%20augmentations%20while%20preserving%20intrinsic%20semantic%0Ainformation%20is%20challenging.%20Additionally%2C%20successive%20layers%20in%20graph%20neural%0Anetwork%20%28GNN%29%20tend%20to%20produce%20more%20similar%20node%20embeddings%2C%20while%20graph%0Acontrastive%20learning%20aims%20to%20increase%20the%20dissimilarity%20between%20negative%20pairs%0Aof%20node%20embeddings.%20This%20inevitably%20results%20in%20a%20conflict%20between%20the%0Amessage-passing%20mechanism%20%28MPM%29%20of%20GNNs%20and%20the%20contrastive%20learning%20%28CL%29%20of%0Anegative%20pairs%20via%20intraviews.%20In%20this%20paper%2C%20we%20propose%20a%20conditional%0Adistribution%20learning%20%28CDL%29%20method%20that%20learns%20graph%20representations%20from%0Agraph-structured%20data%20for%20semisupervised%20graph%20classification.%20Specifically%2C%20we%0Apresent%20an%20end-to-end%20graph%20representation%20learning%20model%20to%20align%20the%0Aconditional%20distributions%20of%20weakly%20and%20strongly%20augmented%20features%20over%20the%0Aoriginal%20features.%20This%20alignment%20enables%20the%20CDL%20model%20to%20effectively%20preserve%0Aintrinsic%20semantic%20information%20when%20both%20weak%20and%20strong%20augmentations%20are%0Aapplied%20to%20graph-structured%20data.%20To%20avoid%20the%20conflict%20between%20the%20MPM%20and%20the%0ACL%20of%20negative%20pairs%2C%20positive%20pairs%20of%20node%20representations%20are%20retained%20for%0Ameasuring%20the%20similarity%20between%20the%20original%20features%20and%20the%20corresponding%0Aweakly%20augmented%20features.%20Extensive%20experiments%20with%20several%20benchmark%20graph%0Adatasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20CDL%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Distribution%2520Learning%2520on%2520Graphs%26entry.906535625%3DJie%2520Chen%2520and%2520Hua%2520Mao%2520and%2520Yuanbiao%2520Gou%2520and%2520Zhu%2520Wang%2520and%2520Xi%2520Peng%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520diversity%2520and%2520quantity%2520of%2520data%2520provided%2520by%2520various%250Agraph-structured%2520data%2520augmentations%2520while%2520preserving%2520intrinsic%2520semantic%250Ainformation%2520is%2520challenging.%2520Additionally%252C%2520successive%2520layers%2520in%2520graph%2520neural%250Anetwork%2520%2528GNN%2529%2520tend%2520to%2520produce%2520more%2520similar%2520node%2520embeddings%252C%2520while%2520graph%250Acontrastive%2520learning%2520aims%2520to%2520increase%2520the%2520dissimilarity%2520between%2520negative%2520pairs%250Aof%2520node%2520embeddings.%2520This%2520inevitably%2520results%2520in%2520a%2520conflict%2520between%2520the%250Amessage-passing%2520mechanism%2520%2528MPM%2529%2520of%2520GNNs%2520and%2520the%2520contrastive%2520learning%2520%2528CL%2529%2520of%250Anegative%2520pairs%2520via%2520intraviews.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520conditional%250Adistribution%2520learning%2520%2528CDL%2529%2520method%2520that%2520learns%2520graph%2520representations%2520from%250Agraph-structured%2520data%2520for%2520semisupervised%2520graph%2520classification.%2520Specifically%252C%2520we%250Apresent%2520an%2520end-to-end%2520graph%2520representation%2520learning%2520model%2520to%2520align%2520the%250Aconditional%2520distributions%2520of%2520weakly%2520and%2520strongly%2520augmented%2520features%2520over%2520the%250Aoriginal%2520features.%2520This%2520alignment%2520enables%2520the%2520CDL%2520model%2520to%2520effectively%2520preserve%250Aintrinsic%2520semantic%2520information%2520when%2520both%2520weak%2520and%2520strong%2520augmentations%2520are%250Aapplied%2520to%2520graph-structured%2520data.%2520To%2520avoid%2520the%2520conflict%2520between%2520the%2520MPM%2520and%2520the%250ACL%2520of%2520negative%2520pairs%252C%2520positive%2520pairs%2520of%2520node%2520representations%2520are%2520retained%2520for%250Ameasuring%2520the%2520similarity%2520between%2520the%2520original%2520features%2520and%2520the%2520corresponding%250Aweakly%2520augmented%2520features.%2520Extensive%2520experiments%2520with%2520several%2520benchmark%2520graph%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CDL%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Distribution%20Learning%20on%20Graphs&entry.906535625=Jie%20Chen%20and%20Hua%20Mao%20and%20Yuanbiao%20Gou%20and%20Zhu%20Wang%20and%20Xi%20Peng&entry.1292438233=%20%20Leveraging%20the%20diversity%20and%20quantity%20of%20data%20provided%20by%20various%0Agraph-structured%20data%20augmentations%20while%20preserving%20intrinsic%20semantic%0Ainformation%20is%20challenging.%20Additionally%2C%20successive%20layers%20in%20graph%20neural%0Anetwork%20%28GNN%29%20tend%20to%20produce%20more%20similar%20node%20embeddings%2C%20while%20graph%0Acontrastive%20learning%20aims%20to%20increase%20the%20dissimilarity%20between%20negative%20pairs%0Aof%20node%20embeddings.%20This%20inevitably%20results%20in%20a%20conflict%20between%20the%0Amessage-passing%20mechanism%20%28MPM%29%20of%20GNNs%20and%20the%20contrastive%20learning%20%28CL%29%20of%0Anegative%20pairs%20via%20intraviews.%20In%20this%20paper%2C%20we%20propose%20a%20conditional%0Adistribution%20learning%20%28CDL%29%20method%20that%20learns%20graph%20representations%20from%0Agraph-structured%20data%20for%20semisupervised%20graph%20classification.%20Specifically%2C%20we%0Apresent%20an%20end-to-end%20graph%20representation%20learning%20model%20to%20align%20the%0Aconditional%20distributions%20of%20weakly%20and%20strongly%20augmented%20features%20over%20the%0Aoriginal%20features.%20This%20alignment%20enables%20the%20CDL%20model%20to%20effectively%20preserve%0Aintrinsic%20semantic%20information%20when%20both%20weak%20and%20strong%20augmentations%20are%0Aapplied%20to%20graph-structured%20data.%20To%20avoid%20the%20conflict%20between%20the%20MPM%20and%20the%0ACL%20of%20negative%20pairs%2C%20positive%20pairs%20of%20node%20representations%20are%20retained%20for%0Ameasuring%20the%20similarity%20between%20the%20original%20features%20and%20the%20corresponding%0Aweakly%20augmented%20features.%20Extensive%20experiments%20with%20several%20benchmark%20graph%0Adatasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20CDL%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15206v2&entry.124074799=Read"},
{"title": "Six-Degree-of-Freedom Motion Emulation for Data-Driven Modeling of\n  Underwater Vehicles", "author": "Juliana Danesi Ruiz and Michael Swafford and Austin Krebill and Rachel Vitali and Casey Harwood", "abstract": "  This article presents a collaborative research effort aimed at developing a\nnovel six-degree-of-freedom (6-DOF) motion platform for the empirical\ncharacterization of hydrodynamic forces crucial for the control and stability\nof surface and subsurface vehicles. Traditional experimental methods, such as\nthe Planar Motion Mechanism (PMM), are limited by the number of simultaneously\narticulated DOFs and are limited to single-frequency testing, making such\nsystems impractical for resolving frequency-dependent added mass or damping\nmatrices. The 6 DOF platform, termed a hexapod, overcomes these limitations by\noffering enhanced maneuverability and the ability to test broad-banded\nfrequency spectra in multiple degrees of freedom in a single experiment.\n", "link": "http://arxiv.org/abs/2501.17018v1", "date": "2025-01-28", "relevancy": 2.0644, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Six-Degree-of-Freedom%20Motion%20Emulation%20for%20Data-Driven%20Modeling%20of%0A%20%20Underwater%20Vehicles&body=Title%3A%20Six-Degree-of-Freedom%20Motion%20Emulation%20for%20Data-Driven%20Modeling%20of%0A%20%20Underwater%20Vehicles%0AAuthor%3A%20Juliana%20Danesi%20Ruiz%20and%20Michael%20Swafford%20and%20Austin%20Krebill%20and%20Rachel%20Vitali%20and%20Casey%20Harwood%0AAbstract%3A%20%20%20This%20article%20presents%20a%20collaborative%20research%20effort%20aimed%20at%20developing%20a%0Anovel%20six-degree-of-freedom%20%286-DOF%29%20motion%20platform%20for%20the%20empirical%0Acharacterization%20of%20hydrodynamic%20forces%20crucial%20for%20the%20control%20and%20stability%0Aof%20surface%20and%20subsurface%20vehicles.%20Traditional%20experimental%20methods%2C%20such%20as%0Athe%20Planar%20Motion%20Mechanism%20%28PMM%29%2C%20are%20limited%20by%20the%20number%20of%20simultaneously%0Aarticulated%20DOFs%20and%20are%20limited%20to%20single-frequency%20testing%2C%20making%20such%0Asystems%20impractical%20for%20resolving%20frequency-dependent%20added%20mass%20or%20damping%0Amatrices.%20The%206%20DOF%20platform%2C%20termed%20a%20hexapod%2C%20overcomes%20these%20limitations%20by%0Aoffering%20enhanced%20maneuverability%20and%20the%20ability%20to%20test%20broad-banded%0Afrequency%20spectra%20in%20multiple%20degrees%20of%20freedom%20in%20a%20single%20experiment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSix-Degree-of-Freedom%2520Motion%2520Emulation%2520for%2520Data-Driven%2520Modeling%2520of%250A%2520%2520Underwater%2520Vehicles%26entry.906535625%3DJuliana%2520Danesi%2520Ruiz%2520and%2520Michael%2520Swafford%2520and%2520Austin%2520Krebill%2520and%2520Rachel%2520Vitali%2520and%2520Casey%2520Harwood%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520collaborative%2520research%2520effort%2520aimed%2520at%2520developing%2520a%250Anovel%2520six-degree-of-freedom%2520%25286-DOF%2529%2520motion%2520platform%2520for%2520the%2520empirical%250Acharacterization%2520of%2520hydrodynamic%2520forces%2520crucial%2520for%2520the%2520control%2520and%2520stability%250Aof%2520surface%2520and%2520subsurface%2520vehicles.%2520Traditional%2520experimental%2520methods%252C%2520such%2520as%250Athe%2520Planar%2520Motion%2520Mechanism%2520%2528PMM%2529%252C%2520are%2520limited%2520by%2520the%2520number%2520of%2520simultaneously%250Aarticulated%2520DOFs%2520and%2520are%2520limited%2520to%2520single-frequency%2520testing%252C%2520making%2520such%250Asystems%2520impractical%2520for%2520resolving%2520frequency-dependent%2520added%2520mass%2520or%2520damping%250Amatrices.%2520The%25206%2520DOF%2520platform%252C%2520termed%2520a%2520hexapod%252C%2520overcomes%2520these%2520limitations%2520by%250Aoffering%2520enhanced%2520maneuverability%2520and%2520the%2520ability%2520to%2520test%2520broad-banded%250Afrequency%2520spectra%2520in%2520multiple%2520degrees%2520of%2520freedom%2520in%2520a%2520single%2520experiment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Six-Degree-of-Freedom%20Motion%20Emulation%20for%20Data-Driven%20Modeling%20of%0A%20%20Underwater%20Vehicles&entry.906535625=Juliana%20Danesi%20Ruiz%20and%20Michael%20Swafford%20and%20Austin%20Krebill%20and%20Rachel%20Vitali%20and%20Casey%20Harwood&entry.1292438233=%20%20This%20article%20presents%20a%20collaborative%20research%20effort%20aimed%20at%20developing%20a%0Anovel%20six-degree-of-freedom%20%286-DOF%29%20motion%20platform%20for%20the%20empirical%0Acharacterization%20of%20hydrodynamic%20forces%20crucial%20for%20the%20control%20and%20stability%0Aof%20surface%20and%20subsurface%20vehicles.%20Traditional%20experimental%20methods%2C%20such%20as%0Athe%20Planar%20Motion%20Mechanism%20%28PMM%29%2C%20are%20limited%20by%20the%20number%20of%20simultaneously%0Aarticulated%20DOFs%20and%20are%20limited%20to%20single-frequency%20testing%2C%20making%20such%0Asystems%20impractical%20for%20resolving%20frequency-dependent%20added%20mass%20or%20damping%0Amatrices.%20The%206%20DOF%20platform%2C%20termed%20a%20hexapod%2C%20overcomes%20these%20limitations%20by%0Aoffering%20enhanced%20maneuverability%20and%20the%20ability%20to%20test%20broad-banded%0Afrequency%20spectra%20in%20multiple%20degrees%20of%20freedom%20in%20a%20single%20experiment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17018v1&entry.124074799=Read"},
{"title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling", "author": "Hongzhi Huang and Defa Zhu and Banggu Wu and Yutao Zeng and Ya Wang and Qiyang Min and Xun Zhou", "abstract": "  Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.\n", "link": "http://arxiv.org/abs/2501.16975v1", "date": "2025-01-28", "relevancy": 2.0454, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5716}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Over-Tokenized%20Transformer%3A%20Vocabulary%20is%20Generally%20Worth%20Scaling&body=Title%3A%20Over-Tokenized%20Transformer%3A%20Vocabulary%20is%20Generally%20Worth%20Scaling%0AAuthor%3A%20Hongzhi%20Huang%20and%20Defa%20Zhu%20and%20Banggu%20Wu%20and%20Yutao%20Zeng%20and%20Ya%20Wang%20and%20Qiyang%20Min%20and%20Xun%20Zhou%0AAbstract%3A%20%20%20Tokenization%20is%20a%20fundamental%20component%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%0Aits%20influence%20on%20model%20scaling%20and%20performance%20is%20not%20fully%20explored.%20In%20this%0Apaper%2C%20we%20introduce%20Over-Tokenized%20Transformers%2C%20a%20novel%20framework%20that%0Adecouples%20input%20and%20output%20vocabularies%20to%20improve%20language%20modeling%0Aperformance.%20Specifically%2C%20our%20approach%20scales%20up%20input%20vocabularies%20to%0Aleverage%20multi-gram%20tokens.%20Through%20extensive%20experiments%2C%20we%20uncover%20a%0Alog-linear%20relationship%20between%20input%20vocabulary%20size%20and%20training%20loss%2C%0Ademonstrating%20that%20larger%20input%20vocabularies%20consistently%20enhance%20model%0Aperformance%2C%20regardless%20of%20model%20size.%20Using%20a%20large%20input%20vocabulary%2C%20we%0Aachieve%20performance%20comparable%20to%20double-sized%20baselines%20with%20no%20additional%0Acost.%20Our%20findings%20highlight%20the%20importance%20of%20tokenization%20in%20scaling%20laws%20and%0Aprovide%20practical%20insight%20for%20tokenizer%20design%2C%20paving%20the%20way%20for%20more%0Aefficient%20and%20powerful%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOver-Tokenized%2520Transformer%253A%2520Vocabulary%2520is%2520Generally%2520Worth%2520Scaling%26entry.906535625%3DHongzhi%2520Huang%2520and%2520Defa%2520Zhu%2520and%2520Banggu%2520Wu%2520and%2520Yutao%2520Zeng%2520and%2520Ya%2520Wang%2520and%2520Qiyang%2520Min%2520and%2520Xun%2520Zhou%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520a%2520fundamental%2520component%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%250Aits%2520influence%2520on%2520model%2520scaling%2520and%2520performance%2520is%2520not%2520fully%2520explored.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520Over-Tokenized%2520Transformers%252C%2520a%2520novel%2520framework%2520that%250Adecouples%2520input%2520and%2520output%2520vocabularies%2520to%2520improve%2520language%2520modeling%250Aperformance.%2520Specifically%252C%2520our%2520approach%2520scales%2520up%2520input%2520vocabularies%2520to%250Aleverage%2520multi-gram%2520tokens.%2520Through%2520extensive%2520experiments%252C%2520we%2520uncover%2520a%250Alog-linear%2520relationship%2520between%2520input%2520vocabulary%2520size%2520and%2520training%2520loss%252C%250Ademonstrating%2520that%2520larger%2520input%2520vocabularies%2520consistently%2520enhance%2520model%250Aperformance%252C%2520regardless%2520of%2520model%2520size.%2520Using%2520a%2520large%2520input%2520vocabulary%252C%2520we%250Aachieve%2520performance%2520comparable%2520to%2520double-sized%2520baselines%2520with%2520no%2520additional%250Acost.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520tokenization%2520in%2520scaling%2520laws%2520and%250Aprovide%2520practical%2520insight%2520for%2520tokenizer%2520design%252C%2520paving%2520the%2520way%2520for%2520more%250Aefficient%2520and%2520powerful%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over-Tokenized%20Transformer%3A%20Vocabulary%20is%20Generally%20Worth%20Scaling&entry.906535625=Hongzhi%20Huang%20and%20Defa%20Zhu%20and%20Banggu%20Wu%20and%20Yutao%20Zeng%20and%20Ya%20Wang%20and%20Qiyang%20Min%20and%20Xun%20Zhou&entry.1292438233=%20%20Tokenization%20is%20a%20fundamental%20component%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%0Aits%20influence%20on%20model%20scaling%20and%20performance%20is%20not%20fully%20explored.%20In%20this%0Apaper%2C%20we%20introduce%20Over-Tokenized%20Transformers%2C%20a%20novel%20framework%20that%0Adecouples%20input%20and%20output%20vocabularies%20to%20improve%20language%20modeling%0Aperformance.%20Specifically%2C%20our%20approach%20scales%20up%20input%20vocabularies%20to%0Aleverage%20multi-gram%20tokens.%20Through%20extensive%20experiments%2C%20we%20uncover%20a%0Alog-linear%20relationship%20between%20input%20vocabulary%20size%20and%20training%20loss%2C%0Ademonstrating%20that%20larger%20input%20vocabularies%20consistently%20enhance%20model%0Aperformance%2C%20regardless%20of%20model%20size.%20Using%20a%20large%20input%20vocabulary%2C%20we%0Aachieve%20performance%20comparable%20to%20double-sized%20baselines%20with%20no%20additional%0Acost.%20Our%20findings%20highlight%20the%20importance%20of%20tokenization%20in%20scaling%20laws%20and%0Aprovide%20practical%20insight%20for%20tokenizer%20design%2C%20paving%20the%20way%20for%20more%0Aefficient%20and%20powerful%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16975v1&entry.124074799=Read"},
{"title": "Generative quantum combinatorial optimization by means of a novel\n  conditional generative quantum eigensolver", "author": "Shunya Minami and Kouhei Nakaji and Yohichi Suzuki and Al\u00e1n Aspuru-Guzik and Tadashi Kadowaki", "abstract": "  Quantum computing is entering a transformative phase with the emergence of\nlogical quantum processors, which hold the potential to tackle complex problems\nbeyond classical capabilities. While significant progress has been made,\napplying quantum algorithms to real-world problems remains challenging. Hybrid\nquantum-classical techniques have been explored to bridge this gap, but they\noften face limitations in expressiveness, trainability, or scalability. In this\nwork, we introduce conditional Generative Quantum Eigensolver\n(conditional-GQE), a context-aware quantum circuit generator powered by an\nencoder-decoder Transformer. Focusing on combinatorial optimization, we train\nour generator for solving problems with up to 10 qubits, exhibiting nearly\nperfect performance on new problems. By leveraging the high expressiveness and\nflexibility of classical generative models, along with an efficient\npreference-based training scheme, conditional-GQE provides a generalizable and\nscalable framework for quantum circuit generation. Our approach advances hybrid\nquantum-classical computing and contributes to accelerate the transition toward\nfault-tolerant quantum computing.\n", "link": "http://arxiv.org/abs/2501.16986v1", "date": "2025-01-28", "relevancy": 2.0444, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5507}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5297}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20quantum%20combinatorial%20optimization%20by%20means%20of%20a%20novel%0A%20%20conditional%20generative%20quantum%20eigensolver&body=Title%3A%20Generative%20quantum%20combinatorial%20optimization%20by%20means%20of%20a%20novel%0A%20%20conditional%20generative%20quantum%20eigensolver%0AAuthor%3A%20Shunya%20Minami%20and%20Kouhei%20Nakaji%20and%20Yohichi%20Suzuki%20and%20Al%C3%A1n%20Aspuru-Guzik%20and%20Tadashi%20Kadowaki%0AAbstract%3A%20%20%20Quantum%20computing%20is%20entering%20a%20transformative%20phase%20with%20the%20emergence%20of%0Alogical%20quantum%20processors%2C%20which%20hold%20the%20potential%20to%20tackle%20complex%20problems%0Abeyond%20classical%20capabilities.%20While%20significant%20progress%20has%20been%20made%2C%0Aapplying%20quantum%20algorithms%20to%20real-world%20problems%20remains%20challenging.%20Hybrid%0Aquantum-classical%20techniques%20have%20been%20explored%20to%20bridge%20this%20gap%2C%20but%20they%0Aoften%20face%20limitations%20in%20expressiveness%2C%20trainability%2C%20or%20scalability.%20In%20this%0Awork%2C%20we%20introduce%20conditional%20Generative%20Quantum%20Eigensolver%0A%28conditional-GQE%29%2C%20a%20context-aware%20quantum%20circuit%20generator%20powered%20by%20an%0Aencoder-decoder%20Transformer.%20Focusing%20on%20combinatorial%20optimization%2C%20we%20train%0Aour%20generator%20for%20solving%20problems%20with%20up%20to%2010%20qubits%2C%20exhibiting%20nearly%0Aperfect%20performance%20on%20new%20problems.%20By%20leveraging%20the%20high%20expressiveness%20and%0Aflexibility%20of%20classical%20generative%20models%2C%20along%20with%20an%20efficient%0Apreference-based%20training%20scheme%2C%20conditional-GQE%20provides%20a%20generalizable%20and%0Ascalable%20framework%20for%20quantum%20circuit%20generation.%20Our%20approach%20advances%20hybrid%0Aquantum-classical%20computing%20and%20contributes%20to%20accelerate%20the%20transition%20toward%0Afault-tolerant%20quantum%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520quantum%2520combinatorial%2520optimization%2520by%2520means%2520of%2520a%2520novel%250A%2520%2520conditional%2520generative%2520quantum%2520eigensolver%26entry.906535625%3DShunya%2520Minami%2520and%2520Kouhei%2520Nakaji%2520and%2520Yohichi%2520Suzuki%2520and%2520Al%25C3%25A1n%2520Aspuru-Guzik%2520and%2520Tadashi%2520Kadowaki%26entry.1292438233%3D%2520%2520Quantum%2520computing%2520is%2520entering%2520a%2520transformative%2520phase%2520with%2520the%2520emergence%2520of%250Alogical%2520quantum%2520processors%252C%2520which%2520hold%2520the%2520potential%2520to%2520tackle%2520complex%2520problems%250Abeyond%2520classical%2520capabilities.%2520While%2520significant%2520progress%2520has%2520been%2520made%252C%250Aapplying%2520quantum%2520algorithms%2520to%2520real-world%2520problems%2520remains%2520challenging.%2520Hybrid%250Aquantum-classical%2520techniques%2520have%2520been%2520explored%2520to%2520bridge%2520this%2520gap%252C%2520but%2520they%250Aoften%2520face%2520limitations%2520in%2520expressiveness%252C%2520trainability%252C%2520or%2520scalability.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520conditional%2520Generative%2520Quantum%2520Eigensolver%250A%2528conditional-GQE%2529%252C%2520a%2520context-aware%2520quantum%2520circuit%2520generator%2520powered%2520by%2520an%250Aencoder-decoder%2520Transformer.%2520Focusing%2520on%2520combinatorial%2520optimization%252C%2520we%2520train%250Aour%2520generator%2520for%2520solving%2520problems%2520with%2520up%2520to%252010%2520qubits%252C%2520exhibiting%2520nearly%250Aperfect%2520performance%2520on%2520new%2520problems.%2520By%2520leveraging%2520the%2520high%2520expressiveness%2520and%250Aflexibility%2520of%2520classical%2520generative%2520models%252C%2520along%2520with%2520an%2520efficient%250Apreference-based%2520training%2520scheme%252C%2520conditional-GQE%2520provides%2520a%2520generalizable%2520and%250Ascalable%2520framework%2520for%2520quantum%2520circuit%2520generation.%2520Our%2520approach%2520advances%2520hybrid%250Aquantum-classical%2520computing%2520and%2520contributes%2520to%2520accelerate%2520the%2520transition%2520toward%250Afault-tolerant%2520quantum%2520computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20quantum%20combinatorial%20optimization%20by%20means%20of%20a%20novel%0A%20%20conditional%20generative%20quantum%20eigensolver&entry.906535625=Shunya%20Minami%20and%20Kouhei%20Nakaji%20and%20Yohichi%20Suzuki%20and%20Al%C3%A1n%20Aspuru-Guzik%20and%20Tadashi%20Kadowaki&entry.1292438233=%20%20Quantum%20computing%20is%20entering%20a%20transformative%20phase%20with%20the%20emergence%20of%0Alogical%20quantum%20processors%2C%20which%20hold%20the%20potential%20to%20tackle%20complex%20problems%0Abeyond%20classical%20capabilities.%20While%20significant%20progress%20has%20been%20made%2C%0Aapplying%20quantum%20algorithms%20to%20real-world%20problems%20remains%20challenging.%20Hybrid%0Aquantum-classical%20techniques%20have%20been%20explored%20to%20bridge%20this%20gap%2C%20but%20they%0Aoften%20face%20limitations%20in%20expressiveness%2C%20trainability%2C%20or%20scalability.%20In%20this%0Awork%2C%20we%20introduce%20conditional%20Generative%20Quantum%20Eigensolver%0A%28conditional-GQE%29%2C%20a%20context-aware%20quantum%20circuit%20generator%20powered%20by%20an%0Aencoder-decoder%20Transformer.%20Focusing%20on%20combinatorial%20optimization%2C%20we%20train%0Aour%20generator%20for%20solving%20problems%20with%20up%20to%2010%20qubits%2C%20exhibiting%20nearly%0Aperfect%20performance%20on%20new%20problems.%20By%20leveraging%20the%20high%20expressiveness%20and%0Aflexibility%20of%20classical%20generative%20models%2C%20along%20with%20an%20efficient%0Apreference-based%20training%20scheme%2C%20conditional-GQE%20provides%20a%20generalizable%20and%0Ascalable%20framework%20for%20quantum%20circuit%20generation.%20Our%20approach%20advances%20hybrid%0Aquantum-classical%20computing%20and%20contributes%20to%20accelerate%20the%20transition%20toward%0Afault-tolerant%20quantum%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16986v1&entry.124074799=Read"},
{"title": "CNMBERT: A Model for Converting Hanyu Pinyin Abbreviations to Chinese\n  Characters", "author": "Zishuo Feng and Feng Cao", "abstract": "  The task of converting Hanyu Pinyin abbreviations to Chinese characters is a\nsignificant branch within the domain of Chinese Spelling Correction (CSC). It\nplays an important role in many downstream applications such as named entity\nrecognition and sentiment analysis. This task typically involves text-length\nalignment and seems easy to solve; however, due to the limited information\ncontent in pinyin abbreviations, achieving accurate conversion is challenging.\nIn this paper, we treat this as a fill-mask task and propose CNMBERT, which\nstands for zh-CN Pinyin Multi-mask BERT Model, as a solution to this issue. By\nintroducing a multi-mask strategy and Mixture of Experts (MoE) layers, CNMBERT\noutperforms fine-tuned GPT models and ChatGPT-4o with a 61.53% MRR score and\n51.86% accuracy on a 10,373-sample test dataset.\n", "link": "http://arxiv.org/abs/2411.11770v4", "date": "2025-01-28", "relevancy": 2.0382, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4119}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNMBERT%3A%20A%20Model%20for%20Converting%20Hanyu%20Pinyin%20Abbreviations%20to%20Chinese%0A%20%20Characters&body=Title%3A%20CNMBERT%3A%20A%20Model%20for%20Converting%20Hanyu%20Pinyin%20Abbreviations%20to%20Chinese%0A%20%20Characters%0AAuthor%3A%20Zishuo%20Feng%20and%20Feng%20Cao%0AAbstract%3A%20%20%20The%20task%20of%20converting%20Hanyu%20Pinyin%20abbreviations%20to%20Chinese%20characters%20is%20a%0Asignificant%20branch%20within%20the%20domain%20of%20Chinese%20Spelling%20Correction%20%28CSC%29.%20It%0Aplays%20an%20important%20role%20in%20many%20downstream%20applications%20such%20as%20named%20entity%0Arecognition%20and%20sentiment%20analysis.%20This%20task%20typically%20involves%20text-length%0Aalignment%20and%20seems%20easy%20to%20solve%3B%20however%2C%20due%20to%20the%20limited%20information%0Acontent%20in%20pinyin%20abbreviations%2C%20achieving%20accurate%20conversion%20is%20challenging.%0AIn%20this%20paper%2C%20we%20treat%20this%20as%20a%20fill-mask%20task%20and%20propose%20CNMBERT%2C%20which%0Astands%20for%20zh-CN%20Pinyin%20Multi-mask%20BERT%20Model%2C%20as%20a%20solution%20to%20this%20issue.%20By%0Aintroducing%20a%20multi-mask%20strategy%20and%20Mixture%20of%20Experts%20%28MoE%29%20layers%2C%20CNMBERT%0Aoutperforms%20fine-tuned%20GPT%20models%20and%20ChatGPT-4o%20with%20a%2061.53%25%20MRR%20score%20and%0A51.86%25%20accuracy%20on%20a%2010%2C373-sample%20test%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11770v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNMBERT%253A%2520A%2520Model%2520for%2520Converting%2520Hanyu%2520Pinyin%2520Abbreviations%2520to%2520Chinese%250A%2520%2520Characters%26entry.906535625%3DZishuo%2520Feng%2520and%2520Feng%2520Cao%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520converting%2520Hanyu%2520Pinyin%2520abbreviations%2520to%2520Chinese%2520characters%2520is%2520a%250Asignificant%2520branch%2520within%2520the%2520domain%2520of%2520Chinese%2520Spelling%2520Correction%2520%2528CSC%2529.%2520It%250Aplays%2520an%2520important%2520role%2520in%2520many%2520downstream%2520applications%2520such%2520as%2520named%2520entity%250Arecognition%2520and%2520sentiment%2520analysis.%2520This%2520task%2520typically%2520involves%2520text-length%250Aalignment%2520and%2520seems%2520easy%2520to%2520solve%253B%2520however%252C%2520due%2520to%2520the%2520limited%2520information%250Acontent%2520in%2520pinyin%2520abbreviations%252C%2520achieving%2520accurate%2520conversion%2520is%2520challenging.%250AIn%2520this%2520paper%252C%2520we%2520treat%2520this%2520as%2520a%2520fill-mask%2520task%2520and%2520propose%2520CNMBERT%252C%2520which%250Astands%2520for%2520zh-CN%2520Pinyin%2520Multi-mask%2520BERT%2520Model%252C%2520as%2520a%2520solution%2520to%2520this%2520issue.%2520By%250Aintroducing%2520a%2520multi-mask%2520strategy%2520and%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520layers%252C%2520CNMBERT%250Aoutperforms%2520fine-tuned%2520GPT%2520models%2520and%2520ChatGPT-4o%2520with%2520a%252061.53%2525%2520MRR%2520score%2520and%250A51.86%2525%2520accuracy%2520on%2520a%252010%252C373-sample%2520test%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11770v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNMBERT%3A%20A%20Model%20for%20Converting%20Hanyu%20Pinyin%20Abbreviations%20to%20Chinese%0A%20%20Characters&entry.906535625=Zishuo%20Feng%20and%20Feng%20Cao&entry.1292438233=%20%20The%20task%20of%20converting%20Hanyu%20Pinyin%20abbreviations%20to%20Chinese%20characters%20is%20a%0Asignificant%20branch%20within%20the%20domain%20of%20Chinese%20Spelling%20Correction%20%28CSC%29.%20It%0Aplays%20an%20important%20role%20in%20many%20downstream%20applications%20such%20as%20named%20entity%0Arecognition%20and%20sentiment%20analysis.%20This%20task%20typically%20involves%20text-length%0Aalignment%20and%20seems%20easy%20to%20solve%3B%20however%2C%20due%20to%20the%20limited%20information%0Acontent%20in%20pinyin%20abbreviations%2C%20achieving%20accurate%20conversion%20is%20challenging.%0AIn%20this%20paper%2C%20we%20treat%20this%20as%20a%20fill-mask%20task%20and%20propose%20CNMBERT%2C%20which%0Astands%20for%20zh-CN%20Pinyin%20Multi-mask%20BERT%20Model%2C%20as%20a%20solution%20to%20this%20issue.%20By%0Aintroducing%20a%20multi-mask%20strategy%20and%20Mixture%20of%20Experts%20%28MoE%29%20layers%2C%20CNMBERT%0Aoutperforms%20fine-tuned%20GPT%20models%20and%20ChatGPT-4o%20with%20a%2061.53%25%20MRR%20score%20and%0A51.86%25%20accuracy%20on%20a%2010%2C373-sample%20test%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11770v4&entry.124074799=Read"},
{"title": "Misspellings in Natural Language Processing: A survey", "author": "Gianluca Sperduti and Alejandro Moreo", "abstract": "  This survey provides an overview of the challenges of misspellings in natural\nlanguage processing (NLP). While often unintentional, misspellings have become\nubiquitous in digital communication, especially with the proliferation of Web\n2.0, user-generated content, and informal text mediums such as social media,\nblogs, and forums. Even if humans can generally interpret misspelled text, NLP\nmodels frequently struggle to handle it: this causes a decline in performance\nin common tasks like text classification and machine translation. In this\npaper, we reconstruct a history of misspellings as a scientific problem. We\nthen discuss the latest advancements to address the challenge of misspellings\nin NLP. Main strategies to mitigate the effect of misspellings include data\naugmentation, double step, character-order agnostic, and tuple-based methods,\namong others. This survey also examines dedicated data challenges and\ncompetitions to spur progress in the field. Critical safety and ethical\nconcerns are also examined, for example, the voluntary use of misspellings to\ninject malicious messages and hate speech on social networks. Furthermore, the\nsurvey explores psycholinguistic perspectives on how humans process\nmisspellings, potentially informing innovative computational techniques for\ntext normalization and representation. Finally, the misspelling-related\nchallenges and opportunities associated with modern large language models are\nalso analyzed, including benchmarks, datasets, and performances of the most\nprominent language models against misspellings. This survey aims to be an\nexhaustive resource for researchers seeking to mitigate the impact of\nmisspellings in the rapidly evolving landscape of NLP.\n", "link": "http://arxiv.org/abs/2501.16836v1", "date": "2025-01-28", "relevancy": 2.0322, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Misspellings%20in%20Natural%20Language%20Processing%3A%20A%20survey&body=Title%3A%20Misspellings%20in%20Natural%20Language%20Processing%3A%20A%20survey%0AAuthor%3A%20Gianluca%20Sperduti%20and%20Alejandro%20Moreo%0AAbstract%3A%20%20%20This%20survey%20provides%20an%20overview%20of%20the%20challenges%20of%20misspellings%20in%20natural%0Alanguage%20processing%20%28NLP%29.%20While%20often%20unintentional%2C%20misspellings%20have%20become%0Aubiquitous%20in%20digital%20communication%2C%20especially%20with%20the%20proliferation%20of%20Web%0A2.0%2C%20user-generated%20content%2C%20and%20informal%20text%20mediums%20such%20as%20social%20media%2C%0Ablogs%2C%20and%20forums.%20Even%20if%20humans%20can%20generally%20interpret%20misspelled%20text%2C%20NLP%0Amodels%20frequently%20struggle%20to%20handle%20it%3A%20this%20causes%20a%20decline%20in%20performance%0Ain%20common%20tasks%20like%20text%20classification%20and%20machine%20translation.%20In%20this%0Apaper%2C%20we%20reconstruct%20a%20history%20of%20misspellings%20as%20a%20scientific%20problem.%20We%0Athen%20discuss%20the%20latest%20advancements%20to%20address%20the%20challenge%20of%20misspellings%0Ain%20NLP.%20Main%20strategies%20to%20mitigate%20the%20effect%20of%20misspellings%20include%20data%0Aaugmentation%2C%20double%20step%2C%20character-order%20agnostic%2C%20and%20tuple-based%20methods%2C%0Aamong%20others.%20This%20survey%20also%20examines%20dedicated%20data%20challenges%20and%0Acompetitions%20to%20spur%20progress%20in%20the%20field.%20Critical%20safety%20and%20ethical%0Aconcerns%20are%20also%20examined%2C%20for%20example%2C%20the%20voluntary%20use%20of%20misspellings%20to%0Ainject%20malicious%20messages%20and%20hate%20speech%20on%20social%20networks.%20Furthermore%2C%20the%0Asurvey%20explores%20psycholinguistic%20perspectives%20on%20how%20humans%20process%0Amisspellings%2C%20potentially%20informing%20innovative%20computational%20techniques%20for%0Atext%20normalization%20and%20representation.%20Finally%2C%20the%20misspelling-related%0Achallenges%20and%20opportunities%20associated%20with%20modern%20large%20language%20models%20are%0Aalso%20analyzed%2C%20including%20benchmarks%2C%20datasets%2C%20and%20performances%20of%20the%20most%0Aprominent%20language%20models%20against%20misspellings.%20This%20survey%20aims%20to%20be%20an%0Aexhaustive%20resource%20for%20researchers%20seeking%20to%20mitigate%20the%20impact%20of%0Amisspellings%20in%20the%20rapidly%20evolving%20landscape%20of%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMisspellings%2520in%2520Natural%2520Language%2520Processing%253A%2520A%2520survey%26entry.906535625%3DGianluca%2520Sperduti%2520and%2520Alejandro%2520Moreo%26entry.1292438233%3D%2520%2520This%2520survey%2520provides%2520an%2520overview%2520of%2520the%2520challenges%2520of%2520misspellings%2520in%2520natural%250Alanguage%2520processing%2520%2528NLP%2529.%2520While%2520often%2520unintentional%252C%2520misspellings%2520have%2520become%250Aubiquitous%2520in%2520digital%2520communication%252C%2520especially%2520with%2520the%2520proliferation%2520of%2520Web%250A2.0%252C%2520user-generated%2520content%252C%2520and%2520informal%2520text%2520mediums%2520such%2520as%2520social%2520media%252C%250Ablogs%252C%2520and%2520forums.%2520Even%2520if%2520humans%2520can%2520generally%2520interpret%2520misspelled%2520text%252C%2520NLP%250Amodels%2520frequently%2520struggle%2520to%2520handle%2520it%253A%2520this%2520causes%2520a%2520decline%2520in%2520performance%250Ain%2520common%2520tasks%2520like%2520text%2520classification%2520and%2520machine%2520translation.%2520In%2520this%250Apaper%252C%2520we%2520reconstruct%2520a%2520history%2520of%2520misspellings%2520as%2520a%2520scientific%2520problem.%2520We%250Athen%2520discuss%2520the%2520latest%2520advancements%2520to%2520address%2520the%2520challenge%2520of%2520misspellings%250Ain%2520NLP.%2520Main%2520strategies%2520to%2520mitigate%2520the%2520effect%2520of%2520misspellings%2520include%2520data%250Aaugmentation%252C%2520double%2520step%252C%2520character-order%2520agnostic%252C%2520and%2520tuple-based%2520methods%252C%250Aamong%2520others.%2520This%2520survey%2520also%2520examines%2520dedicated%2520data%2520challenges%2520and%250Acompetitions%2520to%2520spur%2520progress%2520in%2520the%2520field.%2520Critical%2520safety%2520and%2520ethical%250Aconcerns%2520are%2520also%2520examined%252C%2520for%2520example%252C%2520the%2520voluntary%2520use%2520of%2520misspellings%2520to%250Ainject%2520malicious%2520messages%2520and%2520hate%2520speech%2520on%2520social%2520networks.%2520Furthermore%252C%2520the%250Asurvey%2520explores%2520psycholinguistic%2520perspectives%2520on%2520how%2520humans%2520process%250Amisspellings%252C%2520potentially%2520informing%2520innovative%2520computational%2520techniques%2520for%250Atext%2520normalization%2520and%2520representation.%2520Finally%252C%2520the%2520misspelling-related%250Achallenges%2520and%2520opportunities%2520associated%2520with%2520modern%2520large%2520language%2520models%2520are%250Aalso%2520analyzed%252C%2520including%2520benchmarks%252C%2520datasets%252C%2520and%2520performances%2520of%2520the%2520most%250Aprominent%2520language%2520models%2520against%2520misspellings.%2520This%2520survey%2520aims%2520to%2520be%2520an%250Aexhaustive%2520resource%2520for%2520researchers%2520seeking%2520to%2520mitigate%2520the%2520impact%2520of%250Amisspellings%2520in%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Misspellings%20in%20Natural%20Language%20Processing%3A%20A%20survey&entry.906535625=Gianluca%20Sperduti%20and%20Alejandro%20Moreo&entry.1292438233=%20%20This%20survey%20provides%20an%20overview%20of%20the%20challenges%20of%20misspellings%20in%20natural%0Alanguage%20processing%20%28NLP%29.%20While%20often%20unintentional%2C%20misspellings%20have%20become%0Aubiquitous%20in%20digital%20communication%2C%20especially%20with%20the%20proliferation%20of%20Web%0A2.0%2C%20user-generated%20content%2C%20and%20informal%20text%20mediums%20such%20as%20social%20media%2C%0Ablogs%2C%20and%20forums.%20Even%20if%20humans%20can%20generally%20interpret%20misspelled%20text%2C%20NLP%0Amodels%20frequently%20struggle%20to%20handle%20it%3A%20this%20causes%20a%20decline%20in%20performance%0Ain%20common%20tasks%20like%20text%20classification%20and%20machine%20translation.%20In%20this%0Apaper%2C%20we%20reconstruct%20a%20history%20of%20misspellings%20as%20a%20scientific%20problem.%20We%0Athen%20discuss%20the%20latest%20advancements%20to%20address%20the%20challenge%20of%20misspellings%0Ain%20NLP.%20Main%20strategies%20to%20mitigate%20the%20effect%20of%20misspellings%20include%20data%0Aaugmentation%2C%20double%20step%2C%20character-order%20agnostic%2C%20and%20tuple-based%20methods%2C%0Aamong%20others.%20This%20survey%20also%20examines%20dedicated%20data%20challenges%20and%0Acompetitions%20to%20spur%20progress%20in%20the%20field.%20Critical%20safety%20and%20ethical%0Aconcerns%20are%20also%20examined%2C%20for%20example%2C%20the%20voluntary%20use%20of%20misspellings%20to%0Ainject%20malicious%20messages%20and%20hate%20speech%20on%20social%20networks.%20Furthermore%2C%20the%0Asurvey%20explores%20psycholinguistic%20perspectives%20on%20how%20humans%20process%0Amisspellings%2C%20potentially%20informing%20innovative%20computational%20techniques%20for%0Atext%20normalization%20and%20representation.%20Finally%2C%20the%20misspelling-related%0Achallenges%20and%20opportunities%20associated%20with%20modern%20large%20language%20models%20are%0Aalso%20analyzed%2C%20including%20benchmarks%2C%20datasets%2C%20and%20performances%20of%20the%20most%0Aprominent%20language%20models%20against%20misspellings.%20This%20survey%20aims%20to%20be%20an%0Aexhaustive%20resource%20for%20researchers%20seeking%20to%20mitigate%20the%20impact%20of%0Amisspellings%20in%20the%20rapidly%20evolving%20landscape%20of%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16836v1&entry.124074799=Read"},
{"title": "Heterogeneity-aware Personalized Federated Learning via Adaptive\n  Dual-Agent Reinforcement Learning", "author": "Xi Chen and Qin Li and Haibin Cai and Ting Wang", "abstract": "  Federated Learning (FL) empowers multiple clients to collaboratively train\nmachine learning models without sharing local data, making it highly applicable\nin heterogeneous Internet of Things (IoT) environments. However, intrinsic\nheterogeneity in clients' model architectures and computing capabilities often\nresults in model accuracy loss and the intractable straggler problem, which\nsignificantly impairs training effectiveness. To tackle these challenges, this\npaper proposes a novel Heterogeneity-aware Personalized Federated Learning\nmethod, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms.\nHAPFL optimizes the training process by incorporating three strategic\ncomponents: 1) An RL-based heterogeneous model allocation mechanism. The\nparameter server employs a Proximal Policy Optimization (PPO)-based RL agent to\nadaptively allocate appropriately sized, differentiated models to clients based\non their performance, effectively mitigating performance disparities. 2) An\nRL-based training intensity adjustment scheme. The parameter server leverages\nanother PPO-based RL agent to dynamically fine-tune the training intensity for\neach client to further enhance training efficiency and reduce straggling\nlatency. 3) A knowledge distillation-based mutual learning mechanism. Each\nclient deploys both a heterogeneous local model and a homogeneous lightweight\nmodel named LiteModel, where these models undergo mutual learning through\nknowledge distillation. This uniform LiteModel plays a pivotal role in\naggregating and sharing global knowledge, significantly enhancing the\neffectiveness of personalized local training. Experimental results across\nmultiple benchmark datasets demonstrate that HAPFL not only achieves high\naccuracy but also substantially reduces the overall training time by\n20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to\nexisting solutions.\n", "link": "http://arxiv.org/abs/2501.16966v1", "date": "2025-01-28", "relevancy": 2.0085, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-aware%20Personalized%20Federated%20Learning%20via%20Adaptive%0A%20%20Dual-Agent%20Reinforcement%20Learning&body=Title%3A%20Heterogeneity-aware%20Personalized%20Federated%20Learning%20via%20Adaptive%0A%20%20Dual-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Xi%20Chen%20and%20Qin%20Li%20and%20Haibin%20Cai%20and%20Ting%20Wang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20empowers%20multiple%20clients%20to%20collaboratively%20train%0Amachine%20learning%20models%20without%20sharing%20local%20data%2C%20making%20it%20highly%20applicable%0Ain%20heterogeneous%20Internet%20of%20Things%20%28IoT%29%20environments.%20However%2C%20intrinsic%0Aheterogeneity%20in%20clients%27%20model%20architectures%20and%20computing%20capabilities%20often%0Aresults%20in%20model%20accuracy%20loss%20and%20the%20intractable%20straggler%20problem%2C%20which%0Asignificantly%20impairs%20training%20effectiveness.%20To%20tackle%20these%20challenges%2C%20this%0Apaper%20proposes%20a%20novel%20Heterogeneity-aware%20Personalized%20Federated%20Learning%0Amethod%2C%20named%20HAPFL%2C%20via%20multi-level%20Reinforcement%20Learning%20%28RL%29%20mechanisms.%0AHAPFL%20optimizes%20the%20training%20process%20by%20incorporating%20three%20strategic%0Acomponents%3A%201%29%20An%20RL-based%20heterogeneous%20model%20allocation%20mechanism.%20The%0Aparameter%20server%20employs%20a%20Proximal%20Policy%20Optimization%20%28PPO%29-based%20RL%20agent%20to%0Aadaptively%20allocate%20appropriately%20sized%2C%20differentiated%20models%20to%20clients%20based%0Aon%20their%20performance%2C%20effectively%20mitigating%20performance%20disparities.%202%29%20An%0ARL-based%20training%20intensity%20adjustment%20scheme.%20The%20parameter%20server%20leverages%0Aanother%20PPO-based%20RL%20agent%20to%20dynamically%20fine-tune%20the%20training%20intensity%20for%0Aeach%20client%20to%20further%20enhance%20training%20efficiency%20and%20reduce%20straggling%0Alatency.%203%29%20A%20knowledge%20distillation-based%20mutual%20learning%20mechanism.%20Each%0Aclient%20deploys%20both%20a%20heterogeneous%20local%20model%20and%20a%20homogeneous%20lightweight%0Amodel%20named%20LiteModel%2C%20where%20these%20models%20undergo%20mutual%20learning%20through%0Aknowledge%20distillation.%20This%20uniform%20LiteModel%20plays%20a%20pivotal%20role%20in%0Aaggregating%20and%20sharing%20global%20knowledge%2C%20significantly%20enhancing%20the%0Aeffectiveness%20of%20personalized%20local%20training.%20Experimental%20results%20across%0Amultiple%20benchmark%20datasets%20demonstrate%20that%20HAPFL%20not%20only%20achieves%20high%0Aaccuracy%20but%20also%20substantially%20reduces%20the%20overall%20training%20time%20by%0A20.9%25-40.4%25%20and%20decreases%20straggling%20latency%20by%2019.0%25-48.0%25%20compared%20to%0Aexisting%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-aware%2520Personalized%2520Federated%2520Learning%2520via%2520Adaptive%250A%2520%2520Dual-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DXi%2520Chen%2520and%2520Qin%2520Li%2520and%2520Haibin%2520Cai%2520and%2520Ting%2520Wang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520empowers%2520multiple%2520clients%2520to%2520collaboratively%2520train%250Amachine%2520learning%2520models%2520without%2520sharing%2520local%2520data%252C%2520making%2520it%2520highly%2520applicable%250Ain%2520heterogeneous%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520environments.%2520However%252C%2520intrinsic%250Aheterogeneity%2520in%2520clients%2527%2520model%2520architectures%2520and%2520computing%2520capabilities%2520often%250Aresults%2520in%2520model%2520accuracy%2520loss%2520and%2520the%2520intractable%2520straggler%2520problem%252C%2520which%250Asignificantly%2520impairs%2520training%2520effectiveness.%2520To%2520tackle%2520these%2520challenges%252C%2520this%250Apaper%2520proposes%2520a%2520novel%2520Heterogeneity-aware%2520Personalized%2520Federated%2520Learning%250Amethod%252C%2520named%2520HAPFL%252C%2520via%2520multi-level%2520Reinforcement%2520Learning%2520%2528RL%2529%2520mechanisms.%250AHAPFL%2520optimizes%2520the%2520training%2520process%2520by%2520incorporating%2520three%2520strategic%250Acomponents%253A%25201%2529%2520An%2520RL-based%2520heterogeneous%2520model%2520allocation%2520mechanism.%2520The%250Aparameter%2520server%2520employs%2520a%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529-based%2520RL%2520agent%2520to%250Aadaptively%2520allocate%2520appropriately%2520sized%252C%2520differentiated%2520models%2520to%2520clients%2520based%250Aon%2520their%2520performance%252C%2520effectively%2520mitigating%2520performance%2520disparities.%25202%2529%2520An%250ARL-based%2520training%2520intensity%2520adjustment%2520scheme.%2520The%2520parameter%2520server%2520leverages%250Aanother%2520PPO-based%2520RL%2520agent%2520to%2520dynamically%2520fine-tune%2520the%2520training%2520intensity%2520for%250Aeach%2520client%2520to%2520further%2520enhance%2520training%2520efficiency%2520and%2520reduce%2520straggling%250Alatency.%25203%2529%2520A%2520knowledge%2520distillation-based%2520mutual%2520learning%2520mechanism.%2520Each%250Aclient%2520deploys%2520both%2520a%2520heterogeneous%2520local%2520model%2520and%2520a%2520homogeneous%2520lightweight%250Amodel%2520named%2520LiteModel%252C%2520where%2520these%2520models%2520undergo%2520mutual%2520learning%2520through%250Aknowledge%2520distillation.%2520This%2520uniform%2520LiteModel%2520plays%2520a%2520pivotal%2520role%2520in%250Aaggregating%2520and%2520sharing%2520global%2520knowledge%252C%2520significantly%2520enhancing%2520the%250Aeffectiveness%2520of%2520personalized%2520local%2520training.%2520Experimental%2520results%2520across%250Amultiple%2520benchmark%2520datasets%2520demonstrate%2520that%2520HAPFL%2520not%2520only%2520achieves%2520high%250Aaccuracy%2520but%2520also%2520substantially%2520reduces%2520the%2520overall%2520training%2520time%2520by%250A20.9%2525-40.4%2525%2520and%2520decreases%2520straggling%2520latency%2520by%252019.0%2525-48.0%2525%2520compared%2520to%250Aexisting%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-aware%20Personalized%20Federated%20Learning%20via%20Adaptive%0A%20%20Dual-Agent%20Reinforcement%20Learning&entry.906535625=Xi%20Chen%20and%20Qin%20Li%20and%20Haibin%20Cai%20and%20Ting%20Wang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20empowers%20multiple%20clients%20to%20collaboratively%20train%0Amachine%20learning%20models%20without%20sharing%20local%20data%2C%20making%20it%20highly%20applicable%0Ain%20heterogeneous%20Internet%20of%20Things%20%28IoT%29%20environments.%20However%2C%20intrinsic%0Aheterogeneity%20in%20clients%27%20model%20architectures%20and%20computing%20capabilities%20often%0Aresults%20in%20model%20accuracy%20loss%20and%20the%20intractable%20straggler%20problem%2C%20which%0Asignificantly%20impairs%20training%20effectiveness.%20To%20tackle%20these%20challenges%2C%20this%0Apaper%20proposes%20a%20novel%20Heterogeneity-aware%20Personalized%20Federated%20Learning%0Amethod%2C%20named%20HAPFL%2C%20via%20multi-level%20Reinforcement%20Learning%20%28RL%29%20mechanisms.%0AHAPFL%20optimizes%20the%20training%20process%20by%20incorporating%20three%20strategic%0Acomponents%3A%201%29%20An%20RL-based%20heterogeneous%20model%20allocation%20mechanism.%20The%0Aparameter%20server%20employs%20a%20Proximal%20Policy%20Optimization%20%28PPO%29-based%20RL%20agent%20to%0Aadaptively%20allocate%20appropriately%20sized%2C%20differentiated%20models%20to%20clients%20based%0Aon%20their%20performance%2C%20effectively%20mitigating%20performance%20disparities.%202%29%20An%0ARL-based%20training%20intensity%20adjustment%20scheme.%20The%20parameter%20server%20leverages%0Aanother%20PPO-based%20RL%20agent%20to%20dynamically%20fine-tune%20the%20training%20intensity%20for%0Aeach%20client%20to%20further%20enhance%20training%20efficiency%20and%20reduce%20straggling%0Alatency.%203%29%20A%20knowledge%20distillation-based%20mutual%20learning%20mechanism.%20Each%0Aclient%20deploys%20both%20a%20heterogeneous%20local%20model%20and%20a%20homogeneous%20lightweight%0Amodel%20named%20LiteModel%2C%20where%20these%20models%20undergo%20mutual%20learning%20through%0Aknowledge%20distillation.%20This%20uniform%20LiteModel%20plays%20a%20pivotal%20role%20in%0Aaggregating%20and%20sharing%20global%20knowledge%2C%20significantly%20enhancing%20the%0Aeffectiveness%20of%20personalized%20local%20training.%20Experimental%20results%20across%0Amultiple%20benchmark%20datasets%20demonstrate%20that%20HAPFL%20not%20only%20achieves%20high%0Aaccuracy%20but%20also%20substantially%20reduces%20the%20overall%20training%20time%20by%0A20.9%25-40.4%25%20and%20decreases%20straggling%20latency%20by%2019.0%25-48.0%25%20compared%20to%0Aexisting%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16966v1&entry.124074799=Read"},
{"title": "Instantiation-based Formalization of Logical Reasoning Tasks using\n  Language Models and Logical Solvers", "author": "Mohammad Raza and Natasa Milic-Frayling", "abstract": "  Robustness of reasoning remains a significant challenge for large language\nmodels, and addressing it is essential for the practical applicability of\nAI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a\nnovel approach that addresses the key challenge in combining language models\nwith the rigor of logical solvers: to accurately formulate the reasoning\nproblem from natural language to the formal language of the solver. SSV uses a\nconsistency-based approach to produce strong abstract formalizations of\nproblems using concrete instantiations that are generated by the model and\nverified by the solver. In addition to significantly advancing the overall\nreasoning accuracy over the state-of-the-art, a key novelty that this approach\npresents is a feature of verification that has near-perfect precision over a\nsignificant coverage of cases, as we demonstrate on open reasoning benchmarks.\nWe propose such *near-certain reasoning* as a new approach to reduce the need\nfor manual verification in many cases, taking us closer to more dependable and\nautonomous AI reasoning systems.\n", "link": "http://arxiv.org/abs/2501.16961v1", "date": "2025-01-28", "relevancy": 2.004, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instantiation-based%20Formalization%20of%20Logical%20Reasoning%20Tasks%20using%0A%20%20Language%20Models%20and%20Logical%20Solvers&body=Title%3A%20Instantiation-based%20Formalization%20of%20Logical%20Reasoning%20Tasks%20using%0A%20%20Language%20Models%20and%20Logical%20Solvers%0AAuthor%3A%20Mohammad%20Raza%20and%20Natasa%20Milic-Frayling%0AAbstract%3A%20%20%20Robustness%20of%20reasoning%20remains%20a%20significant%20challenge%20for%20large%20language%0Amodels%2C%20and%20addressing%20it%20is%20essential%20for%20the%20practical%20applicability%20of%0AAI-driven%20reasoning%20systems.%20We%20introduce%20Semantic%20Self-Verification%20%28SSV%29%2C%20a%0Anovel%20approach%20that%20addresses%20the%20key%20challenge%20in%20combining%20language%20models%0Awith%20the%20rigor%20of%20logical%20solvers%3A%20to%20accurately%20formulate%20the%20reasoning%0Aproblem%20from%20natural%20language%20to%20the%20formal%20language%20of%20the%20solver.%20SSV%20uses%20a%0Aconsistency-based%20approach%20to%20produce%20strong%20abstract%20formalizations%20of%0Aproblems%20using%20concrete%20instantiations%20that%20are%20generated%20by%20the%20model%20and%0Averified%20by%20the%20solver.%20In%20addition%20to%20significantly%20advancing%20the%20overall%0Areasoning%20accuracy%20over%20the%20state-of-the-art%2C%20a%20key%20novelty%20that%20this%20approach%0Apresents%20is%20a%20feature%20of%20verification%20that%20has%20near-perfect%20precision%20over%20a%0Asignificant%20coverage%20of%20cases%2C%20as%20we%20demonstrate%20on%20open%20reasoning%20benchmarks.%0AWe%20propose%20such%20%2Anear-certain%20reasoning%2A%20as%20a%20new%20approach%20to%20reduce%20the%20need%0Afor%20manual%20verification%20in%20many%20cases%2C%20taking%20us%20closer%20to%20more%20dependable%20and%0Aautonomous%20AI%20reasoning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstantiation-based%2520Formalization%2520of%2520Logical%2520Reasoning%2520Tasks%2520using%250A%2520%2520Language%2520Models%2520and%2520Logical%2520Solvers%26entry.906535625%3DMohammad%2520Raza%2520and%2520Natasa%2520Milic-Frayling%26entry.1292438233%3D%2520%2520Robustness%2520of%2520reasoning%2520remains%2520a%2520significant%2520challenge%2520for%2520large%2520language%250Amodels%252C%2520and%2520addressing%2520it%2520is%2520essential%2520for%2520the%2520practical%2520applicability%2520of%250AAI-driven%2520reasoning%2520systems.%2520We%2520introduce%2520Semantic%2520Self-Verification%2520%2528SSV%2529%252C%2520a%250Anovel%2520approach%2520that%2520addresses%2520the%2520key%2520challenge%2520in%2520combining%2520language%2520models%250Awith%2520the%2520rigor%2520of%2520logical%2520solvers%253A%2520to%2520accurately%2520formulate%2520the%2520reasoning%250Aproblem%2520from%2520natural%2520language%2520to%2520the%2520formal%2520language%2520of%2520the%2520solver.%2520SSV%2520uses%2520a%250Aconsistency-based%2520approach%2520to%2520produce%2520strong%2520abstract%2520formalizations%2520of%250Aproblems%2520using%2520concrete%2520instantiations%2520that%2520are%2520generated%2520by%2520the%2520model%2520and%250Averified%2520by%2520the%2520solver.%2520In%2520addition%2520to%2520significantly%2520advancing%2520the%2520overall%250Areasoning%2520accuracy%2520over%2520the%2520state-of-the-art%252C%2520a%2520key%2520novelty%2520that%2520this%2520approach%250Apresents%2520is%2520a%2520feature%2520of%2520verification%2520that%2520has%2520near-perfect%2520precision%2520over%2520a%250Asignificant%2520coverage%2520of%2520cases%252C%2520as%2520we%2520demonstrate%2520on%2520open%2520reasoning%2520benchmarks.%250AWe%2520propose%2520such%2520%252Anear-certain%2520reasoning%252A%2520as%2520a%2520new%2520approach%2520to%2520reduce%2520the%2520need%250Afor%2520manual%2520verification%2520in%2520many%2520cases%252C%2520taking%2520us%2520closer%2520to%2520more%2520dependable%2520and%250Aautonomous%2520AI%2520reasoning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instantiation-based%20Formalization%20of%20Logical%20Reasoning%20Tasks%20using%0A%20%20Language%20Models%20and%20Logical%20Solvers&entry.906535625=Mohammad%20Raza%20and%20Natasa%20Milic-Frayling&entry.1292438233=%20%20Robustness%20of%20reasoning%20remains%20a%20significant%20challenge%20for%20large%20language%0Amodels%2C%20and%20addressing%20it%20is%20essential%20for%20the%20practical%20applicability%20of%0AAI-driven%20reasoning%20systems.%20We%20introduce%20Semantic%20Self-Verification%20%28SSV%29%2C%20a%0Anovel%20approach%20that%20addresses%20the%20key%20challenge%20in%20combining%20language%20models%0Awith%20the%20rigor%20of%20logical%20solvers%3A%20to%20accurately%20formulate%20the%20reasoning%0Aproblem%20from%20natural%20language%20to%20the%20formal%20language%20of%20the%20solver.%20SSV%20uses%20a%0Aconsistency-based%20approach%20to%20produce%20strong%20abstract%20formalizations%20of%0Aproblems%20using%20concrete%20instantiations%20that%20are%20generated%20by%20the%20model%20and%0Averified%20by%20the%20solver.%20In%20addition%20to%20significantly%20advancing%20the%20overall%0Areasoning%20accuracy%20over%20the%20state-of-the-art%2C%20a%20key%20novelty%20that%20this%20approach%0Apresents%20is%20a%20feature%20of%20verification%20that%20has%20near-perfect%20precision%20over%20a%0Asignificant%20coverage%20of%20cases%2C%20as%20we%20demonstrate%20on%20open%20reasoning%20benchmarks.%0AWe%20propose%20such%20%2Anear-certain%20reasoning%2A%20as%20a%20new%20approach%20to%20reduce%20the%20need%0Afor%20manual%20verification%20in%20many%20cases%2C%20taking%20us%20closer%20to%20more%20dependable%20and%0Aautonomous%20AI%20reasoning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16961v1&entry.124074799=Read"},
{"title": "CoRe-Net: Co-Operational Regressor Network with Progressive Transfer\n  Learning for Blind Radar Signal Restoration", "author": "Muhammad Uzair Zahid and Serkan Kiranyaz and Alper Yildirim and Moncef Gabbouj", "abstract": "  Real-world radar signals are frequently corrupted by various artifacts,\nincluding sensor noise, echoes, interference, and intentional jamming,\ndiffering in type, severity, and duration. This pilot study introduces a novel\nmodel, called Co-Operational Regressor Network (CoRe-Net) for blind radar\nsignal restoration, designed to address such limitations and drawbacks.\nCoRe-Net replaces adversarial training with a novel cooperative learning\nstrategy, leveraging the complementary roles of its Apprentice Regressor (AR)\nand Master Regressor (MR). The AR restores radar signals corrupted by various\nartifacts, while the MR evaluates the quality of the restoration and provides\nimmediate and task-specific feedback, ensuring stable and efficient learning.\nThe AR, therefore, has the advantage of both self-learning and assistive\nlearning by the MR. The proposed model has been extensively evaluated over the\nbenchmark Blind Radar Signal Restoration (BRSR) dataset, which simulates\ndiverse real-world artifact scenarios. Under the fair experimental setup, this\nstudy shows that the CoRe-Net surpasses the Op-GANs over a 1 dB mean SNR\nimprovement. To further boost the performance gain, this study proposes\nmulti-pass restoration by cascaded CoRe-Nets trained with a novel paradigm\ncalled Progressive Transfer Learning (PTL), which enables iterative refinement,\nthus achieving an additional 2 dB mean SNR enhancement. Multi-pass CoRe-Net\ntraining by PTL consistently yields incremental performance improvements\nthrough successive restoration passes whilst highlighting CoRe-Net ability to\nhandle such a complex and varying blend of artifacts.\n", "link": "http://arxiv.org/abs/2501.17125v1", "date": "2025-01-28", "relevancy": 1.9985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5026}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4984}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRe-Net%3A%20Co-Operational%20Regressor%20Network%20with%20Progressive%20Transfer%0A%20%20Learning%20for%20Blind%20Radar%20Signal%20Restoration&body=Title%3A%20CoRe-Net%3A%20Co-Operational%20Regressor%20Network%20with%20Progressive%20Transfer%0A%20%20Learning%20for%20Blind%20Radar%20Signal%20Restoration%0AAuthor%3A%20Muhammad%20Uzair%20Zahid%20and%20Serkan%20Kiranyaz%20and%20Alper%20Yildirim%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20Real-world%20radar%20signals%20are%20frequently%20corrupted%20by%20various%20artifacts%2C%0Aincluding%20sensor%20noise%2C%20echoes%2C%20interference%2C%20and%20intentional%20jamming%2C%0Adiffering%20in%20type%2C%20severity%2C%20and%20duration.%20This%20pilot%20study%20introduces%20a%20novel%0Amodel%2C%20called%20Co-Operational%20Regressor%20Network%20%28CoRe-Net%29%20for%20blind%20radar%0Asignal%20restoration%2C%20designed%20to%20address%20such%20limitations%20and%20drawbacks.%0ACoRe-Net%20replaces%20adversarial%20training%20with%20a%20novel%20cooperative%20learning%0Astrategy%2C%20leveraging%20the%20complementary%20roles%20of%20its%20Apprentice%20Regressor%20%28AR%29%0Aand%20Master%20Regressor%20%28MR%29.%20The%20AR%20restores%20radar%20signals%20corrupted%20by%20various%0Aartifacts%2C%20while%20the%20MR%20evaluates%20the%20quality%20of%20the%20restoration%20and%20provides%0Aimmediate%20and%20task-specific%20feedback%2C%20ensuring%20stable%20and%20efficient%20learning.%0AThe%20AR%2C%20therefore%2C%20has%20the%20advantage%20of%20both%20self-learning%20and%20assistive%0Alearning%20by%20the%20MR.%20The%20proposed%20model%20has%20been%20extensively%20evaluated%20over%20the%0Abenchmark%20Blind%20Radar%20Signal%20Restoration%20%28BRSR%29%20dataset%2C%20which%20simulates%0Adiverse%20real-world%20artifact%20scenarios.%20Under%20the%20fair%20experimental%20setup%2C%20this%0Astudy%20shows%20that%20the%20CoRe-Net%20surpasses%20the%20Op-GANs%20over%20a%201%20dB%20mean%20SNR%0Aimprovement.%20To%20further%20boost%20the%20performance%20gain%2C%20this%20study%20proposes%0Amulti-pass%20restoration%20by%20cascaded%20CoRe-Nets%20trained%20with%20a%20novel%20paradigm%0Acalled%20Progressive%20Transfer%20Learning%20%28PTL%29%2C%20which%20enables%20iterative%20refinement%2C%0Athus%20achieving%20an%20additional%202%20dB%20mean%20SNR%20enhancement.%20Multi-pass%20CoRe-Net%0Atraining%20by%20PTL%20consistently%20yields%20incremental%20performance%20improvements%0Athrough%20successive%20restoration%20passes%20whilst%20highlighting%20CoRe-Net%20ability%20to%0Ahandle%20such%20a%20complex%20and%20varying%20blend%20of%20artifacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRe-Net%253A%2520Co-Operational%2520Regressor%2520Network%2520with%2520Progressive%2520Transfer%250A%2520%2520Learning%2520for%2520Blind%2520Radar%2520Signal%2520Restoration%26entry.906535625%3DMuhammad%2520Uzair%2520Zahid%2520and%2520Serkan%2520Kiranyaz%2520and%2520Alper%2520Yildirim%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520Real-world%2520radar%2520signals%2520are%2520frequently%2520corrupted%2520by%2520various%2520artifacts%252C%250Aincluding%2520sensor%2520noise%252C%2520echoes%252C%2520interference%252C%2520and%2520intentional%2520jamming%252C%250Adiffering%2520in%2520type%252C%2520severity%252C%2520and%2520duration.%2520This%2520pilot%2520study%2520introduces%2520a%2520novel%250Amodel%252C%2520called%2520Co-Operational%2520Regressor%2520Network%2520%2528CoRe-Net%2529%2520for%2520blind%2520radar%250Asignal%2520restoration%252C%2520designed%2520to%2520address%2520such%2520limitations%2520and%2520drawbacks.%250ACoRe-Net%2520replaces%2520adversarial%2520training%2520with%2520a%2520novel%2520cooperative%2520learning%250Astrategy%252C%2520leveraging%2520the%2520complementary%2520roles%2520of%2520its%2520Apprentice%2520Regressor%2520%2528AR%2529%250Aand%2520Master%2520Regressor%2520%2528MR%2529.%2520The%2520AR%2520restores%2520radar%2520signals%2520corrupted%2520by%2520various%250Aartifacts%252C%2520while%2520the%2520MR%2520evaluates%2520the%2520quality%2520of%2520the%2520restoration%2520and%2520provides%250Aimmediate%2520and%2520task-specific%2520feedback%252C%2520ensuring%2520stable%2520and%2520efficient%2520learning.%250AThe%2520AR%252C%2520therefore%252C%2520has%2520the%2520advantage%2520of%2520both%2520self-learning%2520and%2520assistive%250Alearning%2520by%2520the%2520MR.%2520The%2520proposed%2520model%2520has%2520been%2520extensively%2520evaluated%2520over%2520the%250Abenchmark%2520Blind%2520Radar%2520Signal%2520Restoration%2520%2528BRSR%2529%2520dataset%252C%2520which%2520simulates%250Adiverse%2520real-world%2520artifact%2520scenarios.%2520Under%2520the%2520fair%2520experimental%2520setup%252C%2520this%250Astudy%2520shows%2520that%2520the%2520CoRe-Net%2520surpasses%2520the%2520Op-GANs%2520over%2520a%25201%2520dB%2520mean%2520SNR%250Aimprovement.%2520To%2520further%2520boost%2520the%2520performance%2520gain%252C%2520this%2520study%2520proposes%250Amulti-pass%2520restoration%2520by%2520cascaded%2520CoRe-Nets%2520trained%2520with%2520a%2520novel%2520paradigm%250Acalled%2520Progressive%2520Transfer%2520Learning%2520%2528PTL%2529%252C%2520which%2520enables%2520iterative%2520refinement%252C%250Athus%2520achieving%2520an%2520additional%25202%2520dB%2520mean%2520SNR%2520enhancement.%2520Multi-pass%2520CoRe-Net%250Atraining%2520by%2520PTL%2520consistently%2520yields%2520incremental%2520performance%2520improvements%250Athrough%2520successive%2520restoration%2520passes%2520whilst%2520highlighting%2520CoRe-Net%2520ability%2520to%250Ahandle%2520such%2520a%2520complex%2520and%2520varying%2520blend%2520of%2520artifacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRe-Net%3A%20Co-Operational%20Regressor%20Network%20with%20Progressive%20Transfer%0A%20%20Learning%20for%20Blind%20Radar%20Signal%20Restoration&entry.906535625=Muhammad%20Uzair%20Zahid%20and%20Serkan%20Kiranyaz%20and%20Alper%20Yildirim%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20Real-world%20radar%20signals%20are%20frequently%20corrupted%20by%20various%20artifacts%2C%0Aincluding%20sensor%20noise%2C%20echoes%2C%20interference%2C%20and%20intentional%20jamming%2C%0Adiffering%20in%20type%2C%20severity%2C%20and%20duration.%20This%20pilot%20study%20introduces%20a%20novel%0Amodel%2C%20called%20Co-Operational%20Regressor%20Network%20%28CoRe-Net%29%20for%20blind%20radar%0Asignal%20restoration%2C%20designed%20to%20address%20such%20limitations%20and%20drawbacks.%0ACoRe-Net%20replaces%20adversarial%20training%20with%20a%20novel%20cooperative%20learning%0Astrategy%2C%20leveraging%20the%20complementary%20roles%20of%20its%20Apprentice%20Regressor%20%28AR%29%0Aand%20Master%20Regressor%20%28MR%29.%20The%20AR%20restores%20radar%20signals%20corrupted%20by%20various%0Aartifacts%2C%20while%20the%20MR%20evaluates%20the%20quality%20of%20the%20restoration%20and%20provides%0Aimmediate%20and%20task-specific%20feedback%2C%20ensuring%20stable%20and%20efficient%20learning.%0AThe%20AR%2C%20therefore%2C%20has%20the%20advantage%20of%20both%20self-learning%20and%20assistive%0Alearning%20by%20the%20MR.%20The%20proposed%20model%20has%20been%20extensively%20evaluated%20over%20the%0Abenchmark%20Blind%20Radar%20Signal%20Restoration%20%28BRSR%29%20dataset%2C%20which%20simulates%0Adiverse%20real-world%20artifact%20scenarios.%20Under%20the%20fair%20experimental%20setup%2C%20this%0Astudy%20shows%20that%20the%20CoRe-Net%20surpasses%20the%20Op-GANs%20over%20a%201%20dB%20mean%20SNR%0Aimprovement.%20To%20further%20boost%20the%20performance%20gain%2C%20this%20study%20proposes%0Amulti-pass%20restoration%20by%20cascaded%20CoRe-Nets%20trained%20with%20a%20novel%20paradigm%0Acalled%20Progressive%20Transfer%20Learning%20%28PTL%29%2C%20which%20enables%20iterative%20refinement%2C%0Athus%20achieving%20an%20additional%202%20dB%20mean%20SNR%20enhancement.%20Multi-pass%20CoRe-Net%0Atraining%20by%20PTL%20consistently%20yields%20incremental%20performance%20improvements%0Athrough%20successive%20restoration%20passes%20whilst%20highlighting%20CoRe-Net%20ability%20to%0Ahandle%20such%20a%20complex%20and%20varying%20blend%20of%20artifacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17125v1&entry.124074799=Read"},
{"title": "Scanning Trojaned Models Using Out-of-Distribution Samples", "author": "Hossein Mirzaei and Ali Ansari and Bahar Dibaei Nia and Mojtaba Nafez and Moein Madadi and Sepehr Rezaee and Zeinab Sadat Taghavi and Arad Maleki and Kian Shamsaie and Mahdi Hajialilue and Jafar Habibi and Mohammad Sabokrou and Mohammad Hossein Rohban", "abstract": "  Scanning for trojan (backdoor) in deep neural networks is crucial due to\ntheir significant real-world applications. There has been an increasing focus\non developing effective general trojan scanning methods across various trojan\nattacks. Despite advancements, there remains a shortage of methods that perform\neffectively without preconceived assumptions about the backdoor attack method.\nAdditionally, we have observed that current methods struggle to identify\nclassifiers trojaned using adversarial training. Motivated by these challenges,\nour study introduces a novel scanning method named TRODO (TROjan scanning by\nDetection of adversarial shifts in Out-of-distribution samples). TRODO\nleverages the concept of \"blind spots\"--regions where trojaned classifiers\nerroneously identify out-of-distribution (OOD) samples as in-distribution (ID).\nWe scan for these blind spots by adversarially shifting OOD samples towards\nin-distribution. The increased likelihood of perturbed OOD samples being\nclassified as ID serves as a signature for trojan detection. TRODO is both\ntrojan and label mapping agnostic, effective even against adversarially trained\ntrojaned classifiers. It is applicable even in scenarios where training data is\nabsent, demonstrating high accuracy and adaptability across various scenarios\nand datasets, highlighting its potential as a robust trojan scanning strategy.\n", "link": "http://arxiv.org/abs/2501.17151v1", "date": "2025-01-28", "relevancy": 1.9878, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5034}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scanning%20Trojaned%20Models%20Using%20Out-of-Distribution%20Samples&body=Title%3A%20Scanning%20Trojaned%20Models%20Using%20Out-of-Distribution%20Samples%0AAuthor%3A%20Hossein%20Mirzaei%20and%20Ali%20Ansari%20and%20Bahar%20Dibaei%20Nia%20and%20Mojtaba%20Nafez%20and%20Moein%20Madadi%20and%20Sepehr%20Rezaee%20and%20Zeinab%20Sadat%20Taghavi%20and%20Arad%20Maleki%20and%20Kian%20Shamsaie%20and%20Mahdi%20Hajialilue%20and%20Jafar%20Habibi%20and%20Mohammad%20Sabokrou%20and%20Mohammad%20Hossein%20Rohban%0AAbstract%3A%20%20%20Scanning%20for%20trojan%20%28backdoor%29%20in%20deep%20neural%20networks%20is%20crucial%20due%20to%0Atheir%20significant%20real-world%20applications.%20There%20has%20been%20an%20increasing%20focus%0Aon%20developing%20effective%20general%20trojan%20scanning%20methods%20across%20various%20trojan%0Aattacks.%20Despite%20advancements%2C%20there%20remains%20a%20shortage%20of%20methods%20that%20perform%0Aeffectively%20without%20preconceived%20assumptions%20about%20the%20backdoor%20attack%20method.%0AAdditionally%2C%20we%20have%20observed%20that%20current%20methods%20struggle%20to%20identify%0Aclassifiers%20trojaned%20using%20adversarial%20training.%20Motivated%20by%20these%20challenges%2C%0Aour%20study%20introduces%20a%20novel%20scanning%20method%20named%20TRODO%20%28TROjan%20scanning%20by%0ADetection%20of%20adversarial%20shifts%20in%20Out-of-distribution%20samples%29.%20TRODO%0Aleverages%20the%20concept%20of%20%22blind%20spots%22--regions%20where%20trojaned%20classifiers%0Aerroneously%20identify%20out-of-distribution%20%28OOD%29%20samples%20as%20in-distribution%20%28ID%29.%0AWe%20scan%20for%20these%20blind%20spots%20by%20adversarially%20shifting%20OOD%20samples%20towards%0Ain-distribution.%20The%20increased%20likelihood%20of%20perturbed%20OOD%20samples%20being%0Aclassified%20as%20ID%20serves%20as%20a%20signature%20for%20trojan%20detection.%20TRODO%20is%20both%0Atrojan%20and%20label%20mapping%20agnostic%2C%20effective%20even%20against%20adversarially%20trained%0Atrojaned%20classifiers.%20It%20is%20applicable%20even%20in%20scenarios%20where%20training%20data%20is%0Aabsent%2C%20demonstrating%20high%20accuracy%20and%20adaptability%20across%20various%20scenarios%0Aand%20datasets%2C%20highlighting%20its%20potential%20as%20a%20robust%20trojan%20scanning%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScanning%2520Trojaned%2520Models%2520Using%2520Out-of-Distribution%2520Samples%26entry.906535625%3DHossein%2520Mirzaei%2520and%2520Ali%2520Ansari%2520and%2520Bahar%2520Dibaei%2520Nia%2520and%2520Mojtaba%2520Nafez%2520and%2520Moein%2520Madadi%2520and%2520Sepehr%2520Rezaee%2520and%2520Zeinab%2520Sadat%2520Taghavi%2520and%2520Arad%2520Maleki%2520and%2520Kian%2520Shamsaie%2520and%2520Mahdi%2520Hajialilue%2520and%2520Jafar%2520Habibi%2520and%2520Mohammad%2520Sabokrou%2520and%2520Mohammad%2520Hossein%2520Rohban%26entry.1292438233%3D%2520%2520Scanning%2520for%2520trojan%2520%2528backdoor%2529%2520in%2520deep%2520neural%2520networks%2520is%2520crucial%2520due%2520to%250Atheir%2520significant%2520real-world%2520applications.%2520There%2520has%2520been%2520an%2520increasing%2520focus%250Aon%2520developing%2520effective%2520general%2520trojan%2520scanning%2520methods%2520across%2520various%2520trojan%250Aattacks.%2520Despite%2520advancements%252C%2520there%2520remains%2520a%2520shortage%2520of%2520methods%2520that%2520perform%250Aeffectively%2520without%2520preconceived%2520assumptions%2520about%2520the%2520backdoor%2520attack%2520method.%250AAdditionally%252C%2520we%2520have%2520observed%2520that%2520current%2520methods%2520struggle%2520to%2520identify%250Aclassifiers%2520trojaned%2520using%2520adversarial%2520training.%2520Motivated%2520by%2520these%2520challenges%252C%250Aour%2520study%2520introduces%2520a%2520novel%2520scanning%2520method%2520named%2520TRODO%2520%2528TROjan%2520scanning%2520by%250ADetection%2520of%2520adversarial%2520shifts%2520in%2520Out-of-distribution%2520samples%2529.%2520TRODO%250Aleverages%2520the%2520concept%2520of%2520%2522blind%2520spots%2522--regions%2520where%2520trojaned%2520classifiers%250Aerroneously%2520identify%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520as%2520in-distribution%2520%2528ID%2529.%250AWe%2520scan%2520for%2520these%2520blind%2520spots%2520by%2520adversarially%2520shifting%2520OOD%2520samples%2520towards%250Ain-distribution.%2520The%2520increased%2520likelihood%2520of%2520perturbed%2520OOD%2520samples%2520being%250Aclassified%2520as%2520ID%2520serves%2520as%2520a%2520signature%2520for%2520trojan%2520detection.%2520TRODO%2520is%2520both%250Atrojan%2520and%2520label%2520mapping%2520agnostic%252C%2520effective%2520even%2520against%2520adversarially%2520trained%250Atrojaned%2520classifiers.%2520It%2520is%2520applicable%2520even%2520in%2520scenarios%2520where%2520training%2520data%2520is%250Aabsent%252C%2520demonstrating%2520high%2520accuracy%2520and%2520adaptability%2520across%2520various%2520scenarios%250Aand%2520datasets%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520robust%2520trojan%2520scanning%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scanning%20Trojaned%20Models%20Using%20Out-of-Distribution%20Samples&entry.906535625=Hossein%20Mirzaei%20and%20Ali%20Ansari%20and%20Bahar%20Dibaei%20Nia%20and%20Mojtaba%20Nafez%20and%20Moein%20Madadi%20and%20Sepehr%20Rezaee%20and%20Zeinab%20Sadat%20Taghavi%20and%20Arad%20Maleki%20and%20Kian%20Shamsaie%20and%20Mahdi%20Hajialilue%20and%20Jafar%20Habibi%20and%20Mohammad%20Sabokrou%20and%20Mohammad%20Hossein%20Rohban&entry.1292438233=%20%20Scanning%20for%20trojan%20%28backdoor%29%20in%20deep%20neural%20networks%20is%20crucial%20due%20to%0Atheir%20significant%20real-world%20applications.%20There%20has%20been%20an%20increasing%20focus%0Aon%20developing%20effective%20general%20trojan%20scanning%20methods%20across%20various%20trojan%0Aattacks.%20Despite%20advancements%2C%20there%20remains%20a%20shortage%20of%20methods%20that%20perform%0Aeffectively%20without%20preconceived%20assumptions%20about%20the%20backdoor%20attack%20method.%0AAdditionally%2C%20we%20have%20observed%20that%20current%20methods%20struggle%20to%20identify%0Aclassifiers%20trojaned%20using%20adversarial%20training.%20Motivated%20by%20these%20challenges%2C%0Aour%20study%20introduces%20a%20novel%20scanning%20method%20named%20TRODO%20%28TROjan%20scanning%20by%0ADetection%20of%20adversarial%20shifts%20in%20Out-of-distribution%20samples%29.%20TRODO%0Aleverages%20the%20concept%20of%20%22blind%20spots%22--regions%20where%20trojaned%20classifiers%0Aerroneously%20identify%20out-of-distribution%20%28OOD%29%20samples%20as%20in-distribution%20%28ID%29.%0AWe%20scan%20for%20these%20blind%20spots%20by%20adversarially%20shifting%20OOD%20samples%20towards%0Ain-distribution.%20The%20increased%20likelihood%20of%20perturbed%20OOD%20samples%20being%0Aclassified%20as%20ID%20serves%20as%20a%20signature%20for%20trojan%20detection.%20TRODO%20is%20both%0Atrojan%20and%20label%20mapping%20agnostic%2C%20effective%20even%20against%20adversarially%20trained%0Atrojaned%20classifiers.%20It%20is%20applicable%20even%20in%20scenarios%20where%20training%20data%20is%0Aabsent%2C%20demonstrating%20high%20accuracy%20and%20adaptability%20across%20various%20scenarios%0Aand%20datasets%2C%20highlighting%20its%20potential%20as%20a%20robust%20trojan%20scanning%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17151v1&entry.124074799=Read"},
{"title": "Online-BLS: An Accurate and Efficient Online Broad Learning System for\n  Data Stream Classification", "author": "Chunyu Lei and Guang-Ze Chen and C. L. Philip Chen and Tong Zhang", "abstract": "  The state-of-the-art online learning models generally conduct a single online\ngradient descent when a new sample arrives and thus suffer from suboptimal\nmodel weights. To this end, we introduce an online broad learning system\nframework with closed-form solutions for each online update. Different from\nemploying existing incremental broad learning algorithms for online learning\ntasks, which tend to incur degraded accuracy and expensive online update\noverhead, we design an effective weight estimation algorithm and an efficient\nonline updating strategy to remedy the above two deficiencies, respectively.\nSpecifically, an effective weight estimation algorithm is first developed by\nreplacing notorious matrix inverse operations with Cholesky decomposition and\nforward-backward substitution to improve model accuracy. Second, we devise an\nefficient online updating strategy that dramatically reduces online update\ntime. Theoretical analysis exhibits the splendid error bound and low time\ncomplexity of our model. The most popular test-then-training evaluation\nexperiments on various real-world datasets prove its superiority and\nefficiency. Furthermore, our framework is naturally extended to data stream\nscenarios with concept drift and exceeds state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2501.16932v1", "date": "2025-01-28", "relevancy": 1.9729, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4997}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4892}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online-BLS%3A%20An%20Accurate%20and%20Efficient%20Online%20Broad%20Learning%20System%20for%0A%20%20Data%20Stream%20Classification&body=Title%3A%20Online-BLS%3A%20An%20Accurate%20and%20Efficient%20Online%20Broad%20Learning%20System%20for%0A%20%20Data%20Stream%20Classification%0AAuthor%3A%20Chunyu%20Lei%20and%20Guang-Ze%20Chen%20and%20C.%20L.%20Philip%20Chen%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20The%20state-of-the-art%20online%20learning%20models%20generally%20conduct%20a%20single%20online%0Agradient%20descent%20when%20a%20new%20sample%20arrives%20and%20thus%20suffer%20from%20suboptimal%0Amodel%20weights.%20To%20this%20end%2C%20we%20introduce%20an%20online%20broad%20learning%20system%0Aframework%20with%20closed-form%20solutions%20for%20each%20online%20update.%20Different%20from%0Aemploying%20existing%20incremental%20broad%20learning%20algorithms%20for%20online%20learning%0Atasks%2C%20which%20tend%20to%20incur%20degraded%20accuracy%20and%20expensive%20online%20update%0Aoverhead%2C%20we%20design%20an%20effective%20weight%20estimation%20algorithm%20and%20an%20efficient%0Aonline%20updating%20strategy%20to%20remedy%20the%20above%20two%20deficiencies%2C%20respectively.%0ASpecifically%2C%20an%20effective%20weight%20estimation%20algorithm%20is%20first%20developed%20by%0Areplacing%20notorious%20matrix%20inverse%20operations%20with%20Cholesky%20decomposition%20and%0Aforward-backward%20substitution%20to%20improve%20model%20accuracy.%20Second%2C%20we%20devise%20an%0Aefficient%20online%20updating%20strategy%20that%20dramatically%20reduces%20online%20update%0Atime.%20Theoretical%20analysis%20exhibits%20the%20splendid%20error%20bound%20and%20low%20time%0Acomplexity%20of%20our%20model.%20The%20most%20popular%20test-then-training%20evaluation%0Aexperiments%20on%20various%20real-world%20datasets%20prove%20its%20superiority%20and%0Aefficiency.%20Furthermore%2C%20our%20framework%20is%20naturally%20extended%20to%20data%20stream%0Ascenarios%20with%20concept%20drift%20and%20exceeds%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline-BLS%253A%2520An%2520Accurate%2520and%2520Efficient%2520Online%2520Broad%2520Learning%2520System%2520for%250A%2520%2520Data%2520Stream%2520Classification%26entry.906535625%3DChunyu%2520Lei%2520and%2520Guang-Ze%2520Chen%2520and%2520C.%2520L.%2520Philip%2520Chen%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520The%2520state-of-the-art%2520online%2520learning%2520models%2520generally%2520conduct%2520a%2520single%2520online%250Agradient%2520descent%2520when%2520a%2520new%2520sample%2520arrives%2520and%2520thus%2520suffer%2520from%2520suboptimal%250Amodel%2520weights.%2520To%2520this%2520end%252C%2520we%2520introduce%2520an%2520online%2520broad%2520learning%2520system%250Aframework%2520with%2520closed-form%2520solutions%2520for%2520each%2520online%2520update.%2520Different%2520from%250Aemploying%2520existing%2520incremental%2520broad%2520learning%2520algorithms%2520for%2520online%2520learning%250Atasks%252C%2520which%2520tend%2520to%2520incur%2520degraded%2520accuracy%2520and%2520expensive%2520online%2520update%250Aoverhead%252C%2520we%2520design%2520an%2520effective%2520weight%2520estimation%2520algorithm%2520and%2520an%2520efficient%250Aonline%2520updating%2520strategy%2520to%2520remedy%2520the%2520above%2520two%2520deficiencies%252C%2520respectively.%250ASpecifically%252C%2520an%2520effective%2520weight%2520estimation%2520algorithm%2520is%2520first%2520developed%2520by%250Areplacing%2520notorious%2520matrix%2520inverse%2520operations%2520with%2520Cholesky%2520decomposition%2520and%250Aforward-backward%2520substitution%2520to%2520improve%2520model%2520accuracy.%2520Second%252C%2520we%2520devise%2520an%250Aefficient%2520online%2520updating%2520strategy%2520that%2520dramatically%2520reduces%2520online%2520update%250Atime.%2520Theoretical%2520analysis%2520exhibits%2520the%2520splendid%2520error%2520bound%2520and%2520low%2520time%250Acomplexity%2520of%2520our%2520model.%2520The%2520most%2520popular%2520test-then-training%2520evaluation%250Aexperiments%2520on%2520various%2520real-world%2520datasets%2520prove%2520its%2520superiority%2520and%250Aefficiency.%2520Furthermore%252C%2520our%2520framework%2520is%2520naturally%2520extended%2520to%2520data%2520stream%250Ascenarios%2520with%2520concept%2520drift%2520and%2520exceeds%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online-BLS%3A%20An%20Accurate%20and%20Efficient%20Online%20Broad%20Learning%20System%20for%0A%20%20Data%20Stream%20Classification&entry.906535625=Chunyu%20Lei%20and%20Guang-Ze%20Chen%20and%20C.%20L.%20Philip%20Chen%20and%20Tong%20Zhang&entry.1292438233=%20%20The%20state-of-the-art%20online%20learning%20models%20generally%20conduct%20a%20single%20online%0Agradient%20descent%20when%20a%20new%20sample%20arrives%20and%20thus%20suffer%20from%20suboptimal%0Amodel%20weights.%20To%20this%20end%2C%20we%20introduce%20an%20online%20broad%20learning%20system%0Aframework%20with%20closed-form%20solutions%20for%20each%20online%20update.%20Different%20from%0Aemploying%20existing%20incremental%20broad%20learning%20algorithms%20for%20online%20learning%0Atasks%2C%20which%20tend%20to%20incur%20degraded%20accuracy%20and%20expensive%20online%20update%0Aoverhead%2C%20we%20design%20an%20effective%20weight%20estimation%20algorithm%20and%20an%20efficient%0Aonline%20updating%20strategy%20to%20remedy%20the%20above%20two%20deficiencies%2C%20respectively.%0ASpecifically%2C%20an%20effective%20weight%20estimation%20algorithm%20is%20first%20developed%20by%0Areplacing%20notorious%20matrix%20inverse%20operations%20with%20Cholesky%20decomposition%20and%0Aforward-backward%20substitution%20to%20improve%20model%20accuracy.%20Second%2C%20we%20devise%20an%0Aefficient%20online%20updating%20strategy%20that%20dramatically%20reduces%20online%20update%0Atime.%20Theoretical%20analysis%20exhibits%20the%20splendid%20error%20bound%20and%20low%20time%0Acomplexity%20of%20our%20model.%20The%20most%20popular%20test-then-training%20evaluation%0Aexperiments%20on%20various%20real-world%20datasets%20prove%20its%20superiority%20and%0Aefficiency.%20Furthermore%2C%20our%20framework%20is%20naturally%20extended%20to%20data%20stream%0Ascenarios%20with%20concept%20drift%20and%20exceeds%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16932v1&entry.124074799=Read"},
{"title": "Adversarial Masked Autoencoder Purifier with Defense Transferability", "author": "Yuan-Chih Chen and Chun-Shien Lu", "abstract": "  The study of adversarial defense still struggles to combat with advanced\nadversarial attacks. In contrast to most prior studies that rely on the\ndiffusion model for test-time defense to remarkably increase the inference\ntime, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked\nAutoEncoder (MAE) into an adversarial purifier framework for test-time\npurification. While MAEP achieves promising adversarial robustness, it\nparticularly features model defense transferability and attack generalization\nwithout relying on using additional data that is different from the training\ndataset. To our knowledge, MAEP is the first study of adversarial purifier\nbased on MAE. Extensive experimental results demonstrate that our method can\nnot only maintain clear accuracy with only a slight drop but also exhibit a\nclose gap between the clean and robust accuracy. Notably, MAEP trained on\nCIFAR10 achieves state-of-the-art performance even when tested directly on\nImageNet, outperforming existing diffusion-based models trained specifically on\nImageNet.\n", "link": "http://arxiv.org/abs/2501.16904v1", "date": "2025-01-28", "relevancy": 1.9669, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5122}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4913}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Masked%20Autoencoder%20Purifier%20with%20Defense%20Transferability&body=Title%3A%20Adversarial%20Masked%20Autoencoder%20Purifier%20with%20Defense%20Transferability%0AAuthor%3A%20Yuan-Chih%20Chen%20and%20Chun-Shien%20Lu%0AAbstract%3A%20%20%20The%20study%20of%20adversarial%20defense%20still%20struggles%20to%20combat%20with%20advanced%0Aadversarial%20attacks.%20In%20contrast%20to%20most%20prior%20studies%20that%20rely%20on%20the%0Adiffusion%20model%20for%20test-time%20defense%20to%20remarkably%20increase%20the%20inference%0Atime%2C%20we%20propose%20Masked%20AutoEncoder%20Purifier%20%28MAEP%29%2C%20which%20integrates%20Masked%0AAutoEncoder%20%28MAE%29%20into%20an%20adversarial%20purifier%20framework%20for%20test-time%0Apurification.%20While%20MAEP%20achieves%20promising%20adversarial%20robustness%2C%20it%0Aparticularly%20features%20model%20defense%20transferability%20and%20attack%20generalization%0Awithout%20relying%20on%20using%20additional%20data%20that%20is%20different%20from%20the%20training%0Adataset.%20To%20our%20knowledge%2C%20MAEP%20is%20the%20first%20study%20of%20adversarial%20purifier%0Abased%20on%20MAE.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%20can%0Anot%20only%20maintain%20clear%20accuracy%20with%20only%20a%20slight%20drop%20but%20also%20exhibit%20a%0Aclose%20gap%20between%20the%20clean%20and%20robust%20accuracy.%20Notably%2C%20MAEP%20trained%20on%0ACIFAR10%20achieves%20state-of-the-art%20performance%20even%20when%20tested%20directly%20on%0AImageNet%2C%20outperforming%20existing%20diffusion-based%20models%20trained%20specifically%20on%0AImageNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Masked%2520Autoencoder%2520Purifier%2520with%2520Defense%2520Transferability%26entry.906535625%3DYuan-Chih%2520Chen%2520and%2520Chun-Shien%2520Lu%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520adversarial%2520defense%2520still%2520struggles%2520to%2520combat%2520with%2520advanced%250Aadversarial%2520attacks.%2520In%2520contrast%2520to%2520most%2520prior%2520studies%2520that%2520rely%2520on%2520the%250Adiffusion%2520model%2520for%2520test-time%2520defense%2520to%2520remarkably%2520increase%2520the%2520inference%250Atime%252C%2520we%2520propose%2520Masked%2520AutoEncoder%2520Purifier%2520%2528MAEP%2529%252C%2520which%2520integrates%2520Masked%250AAutoEncoder%2520%2528MAE%2529%2520into%2520an%2520adversarial%2520purifier%2520framework%2520for%2520test-time%250Apurification.%2520While%2520MAEP%2520achieves%2520promising%2520adversarial%2520robustness%252C%2520it%250Aparticularly%2520features%2520model%2520defense%2520transferability%2520and%2520attack%2520generalization%250Awithout%2520relying%2520on%2520using%2520additional%2520data%2520that%2520is%2520different%2520from%2520the%2520training%250Adataset.%2520To%2520our%2520knowledge%252C%2520MAEP%2520is%2520the%2520first%2520study%2520of%2520adversarial%2520purifier%250Abased%2520on%2520MAE.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520can%250Anot%2520only%2520maintain%2520clear%2520accuracy%2520with%2520only%2520a%2520slight%2520drop%2520but%2520also%2520exhibit%2520a%250Aclose%2520gap%2520between%2520the%2520clean%2520and%2520robust%2520accuracy.%2520Notably%252C%2520MAEP%2520trained%2520on%250ACIFAR10%2520achieves%2520state-of-the-art%2520performance%2520even%2520when%2520tested%2520directly%2520on%250AImageNet%252C%2520outperforming%2520existing%2520diffusion-based%2520models%2520trained%2520specifically%2520on%250AImageNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Masked%20Autoencoder%20Purifier%20with%20Defense%20Transferability&entry.906535625=Yuan-Chih%20Chen%20and%20Chun-Shien%20Lu&entry.1292438233=%20%20The%20study%20of%20adversarial%20defense%20still%20struggles%20to%20combat%20with%20advanced%0Aadversarial%20attacks.%20In%20contrast%20to%20most%20prior%20studies%20that%20rely%20on%20the%0Adiffusion%20model%20for%20test-time%20defense%20to%20remarkably%20increase%20the%20inference%0Atime%2C%20we%20propose%20Masked%20AutoEncoder%20Purifier%20%28MAEP%29%2C%20which%20integrates%20Masked%0AAutoEncoder%20%28MAE%29%20into%20an%20adversarial%20purifier%20framework%20for%20test-time%0Apurification.%20While%20MAEP%20achieves%20promising%20adversarial%20robustness%2C%20it%0Aparticularly%20features%20model%20defense%20transferability%20and%20attack%20generalization%0Awithout%20relying%20on%20using%20additional%20data%20that%20is%20different%20from%20the%20training%0Adataset.%20To%20our%20knowledge%2C%20MAEP%20is%20the%20first%20study%20of%20adversarial%20purifier%0Abased%20on%20MAE.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%20can%0Anot%20only%20maintain%20clear%20accuracy%20with%20only%20a%20slight%20drop%20but%20also%20exhibit%20a%0Aclose%20gap%20between%20the%20clean%20and%20robust%20accuracy.%20Notably%2C%20MAEP%20trained%20on%0ACIFAR10%20achieves%20state-of-the-art%20performance%20even%20when%20tested%20directly%20on%0AImageNet%2C%20outperforming%20existing%20diffusion-based%20models%20trained%20specifically%20on%0AImageNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16904v1&entry.124074799=Read"},
{"title": "On AI-Inspired UI-Design", "author": "Jialiang Wei and Anne-Lise Courbis and Thomas Lambolais and G\u00e9rard Dray and Walid Maalej", "abstract": "  Graphical User Interface (or simply UI) is a primary mean of interaction\nbetween users and their devices. In this paper, we discuss three complementary\nArtificial Intelligence (AI) approaches for triggering the creativity of app\ndesigners and inspiring them create better and more diverse UI designs. First,\ndesigners can prompt a Large Language Model (LLM) to directly generate and\nadjust UIs. Second, a Vision-Language Model (VLM) enables designers to\neffectively search a large screenshot dataset, e.g. from apps published in app\nstores. Third, a Diffusion Model (DM) can be trained to specifically generate\nUIs as inspirational images. We present an AI-inspired design process and\ndiscuss the implications and limitations of the approaches.\n", "link": "http://arxiv.org/abs/2406.13631v2", "date": "2025-01-28", "relevancy": 1.9596, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5209}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4917}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20AI-Inspired%20UI-Design&body=Title%3A%20On%20AI-Inspired%20UI-Design%0AAuthor%3A%20Jialiang%20Wei%20and%20Anne-Lise%20Courbis%20and%20Thomas%20Lambolais%20and%20G%C3%A9rard%20Dray%20and%20Walid%20Maalej%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28or%20simply%20UI%29%20is%20a%20primary%20mean%20of%20interaction%0Abetween%20users%20and%20their%20devices.%20In%20this%20paper%2C%20we%20discuss%20three%20complementary%0AArtificial%20Intelligence%20%28AI%29%20approaches%20for%20triggering%20the%20creativity%20of%20app%0Adesigners%20and%20inspiring%20them%20create%20better%20and%20more%20diverse%20UI%20designs.%20First%2C%0Adesigners%20can%20prompt%20a%20Large%20Language%20Model%20%28LLM%29%20to%20directly%20generate%20and%0Aadjust%20UIs.%20Second%2C%20a%20Vision-Language%20Model%20%28VLM%29%20enables%20designers%20to%0Aeffectively%20search%20a%20large%20screenshot%20dataset%2C%20e.g.%20from%20apps%20published%20in%20app%0Astores.%20Third%2C%20a%20Diffusion%20Model%20%28DM%29%20can%20be%20trained%20to%20specifically%20generate%0AUIs%20as%20inspirational%20images.%20We%20present%20an%20AI-inspired%20design%20process%20and%0Adiscuss%20the%20implications%20and%20limitations%20of%20the%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520AI-Inspired%2520UI-Design%26entry.906535625%3DJialiang%2520Wei%2520and%2520Anne-Lise%2520Courbis%2520and%2520Thomas%2520Lambolais%2520and%2520G%25C3%25A9rard%2520Dray%2520and%2520Walid%2520Maalej%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528or%2520simply%2520UI%2529%2520is%2520a%2520primary%2520mean%2520of%2520interaction%250Abetween%2520users%2520and%2520their%2520devices.%2520In%2520this%2520paper%252C%2520we%2520discuss%2520three%2520complementary%250AArtificial%2520Intelligence%2520%2528AI%2529%2520approaches%2520for%2520triggering%2520the%2520creativity%2520of%2520app%250Adesigners%2520and%2520inspiring%2520them%2520create%2520better%2520and%2520more%2520diverse%2520UI%2520designs.%2520First%252C%250Adesigners%2520can%2520prompt%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520directly%2520generate%2520and%250Aadjust%2520UIs.%2520Second%252C%2520a%2520Vision-Language%2520Model%2520%2528VLM%2529%2520enables%2520designers%2520to%250Aeffectively%2520search%2520a%2520large%2520screenshot%2520dataset%252C%2520e.g.%2520from%2520apps%2520published%2520in%2520app%250Astores.%2520Third%252C%2520a%2520Diffusion%2520Model%2520%2528DM%2529%2520can%2520be%2520trained%2520to%2520specifically%2520generate%250AUIs%2520as%2520inspirational%2520images.%2520We%2520present%2520an%2520AI-inspired%2520design%2520process%2520and%250Adiscuss%2520the%2520implications%2520and%2520limitations%2520of%2520the%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20AI-Inspired%20UI-Design&entry.906535625=Jialiang%20Wei%20and%20Anne-Lise%20Courbis%20and%20Thomas%20Lambolais%20and%20G%C3%A9rard%20Dray%20and%20Walid%20Maalej&entry.1292438233=%20%20Graphical%20User%20Interface%20%28or%20simply%20UI%29%20is%20a%20primary%20mean%20of%20interaction%0Abetween%20users%20and%20their%20devices.%20In%20this%20paper%2C%20we%20discuss%20three%20complementary%0AArtificial%20Intelligence%20%28AI%29%20approaches%20for%20triggering%20the%20creativity%20of%20app%0Adesigners%20and%20inspiring%20them%20create%20better%20and%20more%20diverse%20UI%20designs.%20First%2C%0Adesigners%20can%20prompt%20a%20Large%20Language%20Model%20%28LLM%29%20to%20directly%20generate%20and%0Aadjust%20UIs.%20Second%2C%20a%20Vision-Language%20Model%20%28VLM%29%20enables%20designers%20to%0Aeffectively%20search%20a%20large%20screenshot%20dataset%2C%20e.g.%20from%20apps%20published%20in%20app%0Astores.%20Third%2C%20a%20Diffusion%20Model%20%28DM%29%20can%20be%20trained%20to%20specifically%20generate%0AUIs%20as%20inspirational%20images.%20We%20present%20an%20AI-inspired%20design%20process%20and%0Adiscuss%20the%20implications%20and%20limitations%20of%20the%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13631v2&entry.124074799=Read"},
{"title": "Abstract Operations Research Modeling Using Natural Language Inputs", "author": "Junxuan Li and Ryan Wickman and Sahil Bhatnagar and Raj Kumar Maity and Arko Mukherjee", "abstract": "  Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems.\n", "link": "http://arxiv.org/abs/2408.07272v2", "date": "2025-01-28", "relevancy": 1.948, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstract%20Operations%20Research%20Modeling%20Using%20Natural%20Language%20Inputs&body=Title%3A%20Abstract%20Operations%20Research%20Modeling%20Using%20Natural%20Language%20Inputs%0AAuthor%3A%20Junxuan%20Li%20and%20Ryan%20Wickman%20and%20Sahil%20Bhatnagar%20and%20Raj%20Kumar%20Maity%20and%20Arko%20Mukherjee%0AAbstract%3A%20%20%20Operations%20research%20%28OR%29%20uses%20mathematical%20models%20to%20enhance%20decision-making%2C%0Abut%20developing%20these%20models%20requires%20expert%20knowledge%20and%20can%20be%0Atime-consuming.%20Automated%20mathematical%20programming%20%28AMP%29%20has%20emerged%20to%0Asimplify%20this%20process%2C%20but%20existing%20systems%20have%20limitations.%20This%20paper%0Aintroduces%20a%20novel%20methodology%20that%20uses%20recent%20advances%20in%20Large%20Language%0AModel%20%28LLM%29%20to%20create%20and%20edit%20OR%20solutions%20from%20non-expert%20user%20queries%0Aexpressed%20using%20Natural%20Language.%20This%20reduces%20the%20need%20for%20domain%20expertise%0Aand%20the%20time%20to%20formulate%20a%20problem.%20The%20paper%20presents%20an%20end-to-end%20pipeline%2C%0Anamed%20NL2OR%2C%20that%20generates%20solutions%20to%20OR%20problems%20from%20natural%20language%0Ainput%2C%20and%20shares%20experimental%20results%20on%20several%20important%20OR%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstract%2520Operations%2520Research%2520Modeling%2520Using%2520Natural%2520Language%2520Inputs%26entry.906535625%3DJunxuan%2520Li%2520and%2520Ryan%2520Wickman%2520and%2520Sahil%2520Bhatnagar%2520and%2520Raj%2520Kumar%2520Maity%2520and%2520Arko%2520Mukherjee%26entry.1292438233%3D%2520%2520Operations%2520research%2520%2528OR%2529%2520uses%2520mathematical%2520models%2520to%2520enhance%2520decision-making%252C%250Abut%2520developing%2520these%2520models%2520requires%2520expert%2520knowledge%2520and%2520can%2520be%250Atime-consuming.%2520Automated%2520mathematical%2520programming%2520%2528AMP%2529%2520has%2520emerged%2520to%250Asimplify%2520this%2520process%252C%2520but%2520existing%2520systems%2520have%2520limitations.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520methodology%2520that%2520uses%2520recent%2520advances%2520in%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520to%2520create%2520and%2520edit%2520OR%2520solutions%2520from%2520non-expert%2520user%2520queries%250Aexpressed%2520using%2520Natural%2520Language.%2520This%2520reduces%2520the%2520need%2520for%2520domain%2520expertise%250Aand%2520the%2520time%2520to%2520formulate%2520a%2520problem.%2520The%2520paper%2520presents%2520an%2520end-to-end%2520pipeline%252C%250Anamed%2520NL2OR%252C%2520that%2520generates%2520solutions%2520to%2520OR%2520problems%2520from%2520natural%2520language%250Ainput%252C%2520and%2520shares%2520experimental%2520results%2520on%2520several%2520important%2520OR%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstract%20Operations%20Research%20Modeling%20Using%20Natural%20Language%20Inputs&entry.906535625=Junxuan%20Li%20and%20Ryan%20Wickman%20and%20Sahil%20Bhatnagar%20and%20Raj%20Kumar%20Maity%20and%20Arko%20Mukherjee&entry.1292438233=%20%20Operations%20research%20%28OR%29%20uses%20mathematical%20models%20to%20enhance%20decision-making%2C%0Abut%20developing%20these%20models%20requires%20expert%20knowledge%20and%20can%20be%0Atime-consuming.%20Automated%20mathematical%20programming%20%28AMP%29%20has%20emerged%20to%0Asimplify%20this%20process%2C%20but%20existing%20systems%20have%20limitations.%20This%20paper%0Aintroduces%20a%20novel%20methodology%20that%20uses%20recent%20advances%20in%20Large%20Language%0AModel%20%28LLM%29%20to%20create%20and%20edit%20OR%20solutions%20from%20non-expert%20user%20queries%0Aexpressed%20using%20Natural%20Language.%20This%20reduces%20the%20need%20for%20domain%20expertise%0Aand%20the%20time%20to%20formulate%20a%20problem.%20The%20paper%20presents%20an%20end-to-end%20pipeline%2C%0Anamed%20NL2OR%2C%20that%20generates%20solutions%20to%20OR%20problems%20from%20natural%20language%0Ainput%2C%20and%20shares%20experimental%20results%20on%20several%20important%20OR%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07272v2&entry.124074799=Read"},
{"title": "IC-Portrait: In-Context Matching for View-Consistent Personalized\n  Portrait", "author": "Han Yang and Enis Simsar and Sotiris Anagnostidi and Yanlong Zang and Thomas Hofmann and Ziwei Liu", "abstract": "  Existing diffusion models show great potential for identity-preserving\ngeneration. However, personalized portrait generation remains challenging due\nto the diversity in user profiles, including variations in appearance and\nlighting conditions. To address these challenges, we propose IC-Portrait, a\nnovel framework designed to accurately encode individual identities for\npersonalized portrait generation. Our key insight is that pre-trained diffusion\nmodels are fast learners (e.g.,100 ~ 200 steps) for in-context dense\ncorrespondence matching, which motivates the two major designs of our\nIC-Portrait framework. Specifically, we reformulate portrait generation into\ntwo sub-tasks: 1) Lighting-Aware Stitching: we find that masking a high\nproportion of the input image, e.g., 80%, yields a highly effective\nself-supervisory representation learning of reference image lighting. 2)\nView-Consistent Adaptation: we leverage a synthetic view-consistent profile\ndataset to learn the in-context correspondence. The reference profile can then\nbe warped into arbitrary poses for strong spatial-aligned view conditioning.\nCoupling these two designs by simply concatenating latents to form\nControlNet-like supervision and modeling, enables us to significantly enhance\nthe identity preservation fidelity and stability. Extensive evaluations\ndemonstrate that IC-Portrait consistently outperforms existing state-of-the-art\nmethods both quantitatively and qualitatively, with particularly notable\nimprovements in visual qualities. Furthermore, IC-Portrait even demonstrates\n3D-aware relighting capabilities.\n", "link": "http://arxiv.org/abs/2501.17159v1", "date": "2025-01-28", "relevancy": 1.9371, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6564}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6421}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IC-Portrait%3A%20In-Context%20Matching%20for%20View-Consistent%20Personalized%0A%20%20Portrait&body=Title%3A%20IC-Portrait%3A%20In-Context%20Matching%20for%20View-Consistent%20Personalized%0A%20%20Portrait%0AAuthor%3A%20Han%20Yang%20and%20Enis%20Simsar%20and%20Sotiris%20Anagnostidi%20and%20Yanlong%20Zang%20and%20Thomas%20Hofmann%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Existing%20diffusion%20models%20show%20great%20potential%20for%20identity-preserving%0Ageneration.%20However%2C%20personalized%20portrait%20generation%20remains%20challenging%20due%0Ato%20the%20diversity%20in%20user%20profiles%2C%20including%20variations%20in%20appearance%20and%0Alighting%20conditions.%20To%20address%20these%20challenges%2C%20we%20propose%20IC-Portrait%2C%20a%0Anovel%20framework%20designed%20to%20accurately%20encode%20individual%20identities%20for%0Apersonalized%20portrait%20generation.%20Our%20key%20insight%20is%20that%20pre-trained%20diffusion%0Amodels%20are%20fast%20learners%20%28e.g.%2C100%20~%20200%20steps%29%20for%20in-context%20dense%0Acorrespondence%20matching%2C%20which%20motivates%20the%20two%20major%20designs%20of%20our%0AIC-Portrait%20framework.%20Specifically%2C%20we%20reformulate%20portrait%20generation%20into%0Atwo%20sub-tasks%3A%201%29%20Lighting-Aware%20Stitching%3A%20we%20find%20that%20masking%20a%20high%0Aproportion%20of%20the%20input%20image%2C%20e.g.%2C%2080%25%2C%20yields%20a%20highly%20effective%0Aself-supervisory%20representation%20learning%20of%20reference%20image%20lighting.%202%29%0AView-Consistent%20Adaptation%3A%20we%20leverage%20a%20synthetic%20view-consistent%20profile%0Adataset%20to%20learn%20the%20in-context%20correspondence.%20The%20reference%20profile%20can%20then%0Abe%20warped%20into%20arbitrary%20poses%20for%20strong%20spatial-aligned%20view%20conditioning.%0ACoupling%20these%20two%20designs%20by%20simply%20concatenating%20latents%20to%20form%0AControlNet-like%20supervision%20and%20modeling%2C%20enables%20us%20to%20significantly%20enhance%0Athe%20identity%20preservation%20fidelity%20and%20stability.%20Extensive%20evaluations%0Ademonstrate%20that%20IC-Portrait%20consistently%20outperforms%20existing%20state-of-the-art%0Amethods%20both%20quantitatively%20and%20qualitatively%2C%20with%20particularly%20notable%0Aimprovements%20in%20visual%20qualities.%20Furthermore%2C%20IC-Portrait%20even%20demonstrates%0A3D-aware%20relighting%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIC-Portrait%253A%2520In-Context%2520Matching%2520for%2520View-Consistent%2520Personalized%250A%2520%2520Portrait%26entry.906535625%3DHan%2520Yang%2520and%2520Enis%2520Simsar%2520and%2520Sotiris%2520Anagnostidi%2520and%2520Yanlong%2520Zang%2520and%2520Thomas%2520Hofmann%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520diffusion%2520models%2520show%2520great%2520potential%2520for%2520identity-preserving%250Ageneration.%2520However%252C%2520personalized%2520portrait%2520generation%2520remains%2520challenging%2520due%250Ato%2520the%2520diversity%2520in%2520user%2520profiles%252C%2520including%2520variations%2520in%2520appearance%2520and%250Alighting%2520conditions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520IC-Portrait%252C%2520a%250Anovel%2520framework%2520designed%2520to%2520accurately%2520encode%2520individual%2520identities%2520for%250Apersonalized%2520portrait%2520generation.%2520Our%2520key%2520insight%2520is%2520that%2520pre-trained%2520diffusion%250Amodels%2520are%2520fast%2520learners%2520%2528e.g.%252C100%2520~%2520200%2520steps%2529%2520for%2520in-context%2520dense%250Acorrespondence%2520matching%252C%2520which%2520motivates%2520the%2520two%2520major%2520designs%2520of%2520our%250AIC-Portrait%2520framework.%2520Specifically%252C%2520we%2520reformulate%2520portrait%2520generation%2520into%250Atwo%2520sub-tasks%253A%25201%2529%2520Lighting-Aware%2520Stitching%253A%2520we%2520find%2520that%2520masking%2520a%2520high%250Aproportion%2520of%2520the%2520input%2520image%252C%2520e.g.%252C%252080%2525%252C%2520yields%2520a%2520highly%2520effective%250Aself-supervisory%2520representation%2520learning%2520of%2520reference%2520image%2520lighting.%25202%2529%250AView-Consistent%2520Adaptation%253A%2520we%2520leverage%2520a%2520synthetic%2520view-consistent%2520profile%250Adataset%2520to%2520learn%2520the%2520in-context%2520correspondence.%2520The%2520reference%2520profile%2520can%2520then%250Abe%2520warped%2520into%2520arbitrary%2520poses%2520for%2520strong%2520spatial-aligned%2520view%2520conditioning.%250ACoupling%2520these%2520two%2520designs%2520by%2520simply%2520concatenating%2520latents%2520to%2520form%250AControlNet-like%2520supervision%2520and%2520modeling%252C%2520enables%2520us%2520to%2520significantly%2520enhance%250Athe%2520identity%2520preservation%2520fidelity%2520and%2520stability.%2520Extensive%2520evaluations%250Ademonstrate%2520that%2520IC-Portrait%2520consistently%2520outperforms%2520existing%2520state-of-the-art%250Amethods%2520both%2520quantitatively%2520and%2520qualitatively%252C%2520with%2520particularly%2520notable%250Aimprovements%2520in%2520visual%2520qualities.%2520Furthermore%252C%2520IC-Portrait%2520even%2520demonstrates%250A3D-aware%2520relighting%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IC-Portrait%3A%20In-Context%20Matching%20for%20View-Consistent%20Personalized%0A%20%20Portrait&entry.906535625=Han%20Yang%20and%20Enis%20Simsar%20and%20Sotiris%20Anagnostidi%20and%20Yanlong%20Zang%20and%20Thomas%20Hofmann%20and%20Ziwei%20Liu&entry.1292438233=%20%20Existing%20diffusion%20models%20show%20great%20potential%20for%20identity-preserving%0Ageneration.%20However%2C%20personalized%20portrait%20generation%20remains%20challenging%20due%0Ato%20the%20diversity%20in%20user%20profiles%2C%20including%20variations%20in%20appearance%20and%0Alighting%20conditions.%20To%20address%20these%20challenges%2C%20we%20propose%20IC-Portrait%2C%20a%0Anovel%20framework%20designed%20to%20accurately%20encode%20individual%20identities%20for%0Apersonalized%20portrait%20generation.%20Our%20key%20insight%20is%20that%20pre-trained%20diffusion%0Amodels%20are%20fast%20learners%20%28e.g.%2C100%20~%20200%20steps%29%20for%20in-context%20dense%0Acorrespondence%20matching%2C%20which%20motivates%20the%20two%20major%20designs%20of%20our%0AIC-Portrait%20framework.%20Specifically%2C%20we%20reformulate%20portrait%20generation%20into%0Atwo%20sub-tasks%3A%201%29%20Lighting-Aware%20Stitching%3A%20we%20find%20that%20masking%20a%20high%0Aproportion%20of%20the%20input%20image%2C%20e.g.%2C%2080%25%2C%20yields%20a%20highly%20effective%0Aself-supervisory%20representation%20learning%20of%20reference%20image%20lighting.%202%29%0AView-Consistent%20Adaptation%3A%20we%20leverage%20a%20synthetic%20view-consistent%20profile%0Adataset%20to%20learn%20the%20in-context%20correspondence.%20The%20reference%20profile%20can%20then%0Abe%20warped%20into%20arbitrary%20poses%20for%20strong%20spatial-aligned%20view%20conditioning.%0ACoupling%20these%20two%20designs%20by%20simply%20concatenating%20latents%20to%20form%0AControlNet-like%20supervision%20and%20modeling%2C%20enables%20us%20to%20significantly%20enhance%0Athe%20identity%20preservation%20fidelity%20and%20stability.%20Extensive%20evaluations%0Ademonstrate%20that%20IC-Portrait%20consistently%20outperforms%20existing%20state-of-the-art%0Amethods%20both%20quantitatively%20and%20qualitatively%2C%20with%20particularly%20notable%0Aimprovements%20in%20visual%20qualities.%20Furthermore%2C%20IC-Portrait%20even%20demonstrates%0A3D-aware%20relighting%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17159v1&entry.124074799=Read"},
{"title": "Multiple Abstraction Level Retrieve Augment Generation", "author": "Zheng Zheng and Xinyi Ni and Pengyu Hong", "abstract": "  A Retrieval-Augmented Generation (RAG) model powered by a large language\nmodel (LLM) provides a faster and more cost-effective solution for adapting to\nnew data and knowledge. It also delivers more specialized responses compared to\npre-trained LLMs. However, most existing approaches rely on retrieving\nprefix-sized chunks as references to support question-answering (Q/A). This\napproach is often deployed to address information needs at a single level of\nabstraction, as it struggles to generate answers across multiple levels of\nabstraction. In an RAG setting, while LLMs can summarize and answer questions\neffectively when provided with sufficient details, retrieving excessive\ninformation often leads to the 'lost in the middle' problem and exceeds token\nlimitations. We propose a novel RAG approach that uses chunks of multiple\nabstraction levels (MAL), including multi-sentence-level, paragraph-level,\nsection-level, and document-level. The effectiveness of our approach is\ndemonstrated in an under-explored scientific domain of Glycoscience. Compared\nto traditional single-level RAG approaches, our approach improves AI evaluated\nanswer correctness of Q/A by 25.739\\% on Glyco-related papers.\n", "link": "http://arxiv.org/abs/2501.16952v1", "date": "2025-01-28", "relevancy": 1.9287, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.495}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20Abstraction%20Level%20Retrieve%20Augment%20Generation&body=Title%3A%20Multiple%20Abstraction%20Level%20Retrieve%20Augment%20Generation%0AAuthor%3A%20Zheng%20Zheng%20and%20Xinyi%20Ni%20and%20Pengyu%20Hong%0AAbstract%3A%20%20%20A%20Retrieval-Augmented%20Generation%20%28RAG%29%20model%20powered%20by%20a%20large%20language%0Amodel%20%28LLM%29%20provides%20a%20faster%20and%20more%20cost-effective%20solution%20for%20adapting%20to%0Anew%20data%20and%20knowledge.%20It%20also%20delivers%20more%20specialized%20responses%20compared%20to%0Apre-trained%20LLMs.%20However%2C%20most%20existing%20approaches%20rely%20on%20retrieving%0Aprefix-sized%20chunks%20as%20references%20to%20support%20question-answering%20%28Q/A%29.%20This%0Aapproach%20is%20often%20deployed%20to%20address%20information%20needs%20at%20a%20single%20level%20of%0Aabstraction%2C%20as%20it%20struggles%20to%20generate%20answers%20across%20multiple%20levels%20of%0Aabstraction.%20In%20an%20RAG%20setting%2C%20while%20LLMs%20can%20summarize%20and%20answer%20questions%0Aeffectively%20when%20provided%20with%20sufficient%20details%2C%20retrieving%20excessive%0Ainformation%20often%20leads%20to%20the%20%27lost%20in%20the%20middle%27%20problem%20and%20exceeds%20token%0Alimitations.%20We%20propose%20a%20novel%20RAG%20approach%20that%20uses%20chunks%20of%20multiple%0Aabstraction%20levels%20%28MAL%29%2C%20including%20multi-sentence-level%2C%20paragraph-level%2C%0Asection-level%2C%20and%20document-level.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20in%20an%20under-explored%20scientific%20domain%20of%20Glycoscience.%20Compared%0Ato%20traditional%20single-level%20RAG%20approaches%2C%20our%20approach%20improves%20AI%20evaluated%0Aanswer%20correctness%20of%20Q/A%20by%2025.739%5C%25%20on%20Glyco-related%20papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520Abstraction%2520Level%2520Retrieve%2520Augment%2520Generation%26entry.906535625%3DZheng%2520Zheng%2520and%2520Xinyi%2520Ni%2520and%2520Pengyu%2520Hong%26entry.1292438233%3D%2520%2520A%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520model%2520powered%2520by%2520a%2520large%2520language%250Amodel%2520%2528LLM%2529%2520provides%2520a%2520faster%2520and%2520more%2520cost-effective%2520solution%2520for%2520adapting%2520to%250Anew%2520data%2520and%2520knowledge.%2520It%2520also%2520delivers%2520more%2520specialized%2520responses%2520compared%2520to%250Apre-trained%2520LLMs.%2520However%252C%2520most%2520existing%2520approaches%2520rely%2520on%2520retrieving%250Aprefix-sized%2520chunks%2520as%2520references%2520to%2520support%2520question-answering%2520%2528Q/A%2529.%2520This%250Aapproach%2520is%2520often%2520deployed%2520to%2520address%2520information%2520needs%2520at%2520a%2520single%2520level%2520of%250Aabstraction%252C%2520as%2520it%2520struggles%2520to%2520generate%2520answers%2520across%2520multiple%2520levels%2520of%250Aabstraction.%2520In%2520an%2520RAG%2520setting%252C%2520while%2520LLMs%2520can%2520summarize%2520and%2520answer%2520questions%250Aeffectively%2520when%2520provided%2520with%2520sufficient%2520details%252C%2520retrieving%2520excessive%250Ainformation%2520often%2520leads%2520to%2520the%2520%2527lost%2520in%2520the%2520middle%2527%2520problem%2520and%2520exceeds%2520token%250Alimitations.%2520We%2520propose%2520a%2520novel%2520RAG%2520approach%2520that%2520uses%2520chunks%2520of%2520multiple%250Aabstraction%2520levels%2520%2528MAL%2529%252C%2520including%2520multi-sentence-level%252C%2520paragraph-level%252C%250Asection-level%252C%2520and%2520document-level.%2520The%2520effectiveness%2520of%2520our%2520approach%2520is%250Ademonstrated%2520in%2520an%2520under-explored%2520scientific%2520domain%2520of%2520Glycoscience.%2520Compared%250Ato%2520traditional%2520single-level%2520RAG%2520approaches%252C%2520our%2520approach%2520improves%2520AI%2520evaluated%250Aanswer%2520correctness%2520of%2520Q/A%2520by%252025.739%255C%2525%2520on%2520Glyco-related%2520papers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20Abstraction%20Level%20Retrieve%20Augment%20Generation&entry.906535625=Zheng%20Zheng%20and%20Xinyi%20Ni%20and%20Pengyu%20Hong&entry.1292438233=%20%20A%20Retrieval-Augmented%20Generation%20%28RAG%29%20model%20powered%20by%20a%20large%20language%0Amodel%20%28LLM%29%20provides%20a%20faster%20and%20more%20cost-effective%20solution%20for%20adapting%20to%0Anew%20data%20and%20knowledge.%20It%20also%20delivers%20more%20specialized%20responses%20compared%20to%0Apre-trained%20LLMs.%20However%2C%20most%20existing%20approaches%20rely%20on%20retrieving%0Aprefix-sized%20chunks%20as%20references%20to%20support%20question-answering%20%28Q/A%29.%20This%0Aapproach%20is%20often%20deployed%20to%20address%20information%20needs%20at%20a%20single%20level%20of%0Aabstraction%2C%20as%20it%20struggles%20to%20generate%20answers%20across%20multiple%20levels%20of%0Aabstraction.%20In%20an%20RAG%20setting%2C%20while%20LLMs%20can%20summarize%20and%20answer%20questions%0Aeffectively%20when%20provided%20with%20sufficient%20details%2C%20retrieving%20excessive%0Ainformation%20often%20leads%20to%20the%20%27lost%20in%20the%20middle%27%20problem%20and%20exceeds%20token%0Alimitations.%20We%20propose%20a%20novel%20RAG%20approach%20that%20uses%20chunks%20of%20multiple%0Aabstraction%20levels%20%28MAL%29%2C%20including%20multi-sentence-level%2C%20paragraph-level%2C%0Asection-level%2C%20and%20document-level.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20in%20an%20under-explored%20scientific%20domain%20of%20Glycoscience.%20Compared%0Ato%20traditional%20single-level%20RAG%20approaches%2C%20our%20approach%20improves%20AI%20evaluated%0Aanswer%20correctness%20of%20Q/A%20by%2025.739%5C%25%20on%20Glyco-related%20papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16952v1&entry.124074799=Read"},
{"title": "Q-learning with temporal memory to navigate turbulence", "author": "Marco Rando and Martin James and Alessandro Verri and Lorenzo Rosasco and Agnese Seminara", "abstract": "  We consider the problem of olfactory searches in a turbulent environment. We\nfocus on agents that respond solely to odor stimuli, with no access to spatial\nperception nor prior information about the odor. We ask whether navigation to a\ntarget can be learned robustly within a sequential decision making framework.\nWe develop a reinforcement learning algorithm using a small set of\ninterpretable olfactory states and train it with realistic turbulent odor cues.\nBy introducing a temporal memory, we demonstrate that two salient features of\nodor traces, discretized in few olfactory states, are sufficient to learn\nnavigation in a realistic odor plume. Performance is dictated by the sparse\nnature of turbulent odors. An optimal memory exists which ignores blanks within\nthe plume and activates a recovery strategy outside the plume. We obtain the\nbest performance by letting agents learn their recovery strategy and show that\nit is mostly casting cross wind, similar to behavior observed in flying\ninsects. The optimal strategy is robust to substantial changes in the odor\nplumes, suggesting minor parameter tuning may be sufficient to adapt to\ndifferent environments.\n", "link": "http://arxiv.org/abs/2404.17495v2", "date": "2025-01-28", "relevancy": 1.9224, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4848}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-learning%20with%20temporal%20memory%20to%20navigate%20turbulence&body=Title%3A%20Q-learning%20with%20temporal%20memory%20to%20navigate%20turbulence%0AAuthor%3A%20Marco%20Rando%20and%20Martin%20James%20and%20Alessandro%20Verri%20and%20Lorenzo%20Rosasco%20and%20Agnese%20Seminara%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20olfactory%20searches%20in%20a%20turbulent%20environment.%20We%0Afocus%20on%20agents%20that%20respond%20solely%20to%20odor%20stimuli%2C%20with%20no%20access%20to%20spatial%0Aperception%20nor%20prior%20information%20about%20the%20odor.%20We%20ask%20whether%20navigation%20to%20a%0Atarget%20can%20be%20learned%20robustly%20within%20a%20sequential%20decision%20making%20framework.%0AWe%20develop%20a%20reinforcement%20learning%20algorithm%20using%20a%20small%20set%20of%0Ainterpretable%20olfactory%20states%20and%20train%20it%20with%20realistic%20turbulent%20odor%20cues.%0ABy%20introducing%20a%20temporal%20memory%2C%20we%20demonstrate%20that%20two%20salient%20features%20of%0Aodor%20traces%2C%20discretized%20in%20few%20olfactory%20states%2C%20are%20sufficient%20to%20learn%0Anavigation%20in%20a%20realistic%20odor%20plume.%20Performance%20is%20dictated%20by%20the%20sparse%0Anature%20of%20turbulent%20odors.%20An%20optimal%20memory%20exists%20which%20ignores%20blanks%20within%0Athe%20plume%20and%20activates%20a%20recovery%20strategy%20outside%20the%20plume.%20We%20obtain%20the%0Abest%20performance%20by%20letting%20agents%20learn%20their%20recovery%20strategy%20and%20show%20that%0Ait%20is%20mostly%20casting%20cross%20wind%2C%20similar%20to%20behavior%20observed%20in%20flying%0Ainsects.%20The%20optimal%20strategy%20is%20robust%20to%20substantial%20changes%20in%20the%20odor%0Aplumes%2C%20suggesting%20minor%20parameter%20tuning%20may%20be%20sufficient%20to%20adapt%20to%0Adifferent%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-learning%2520with%2520temporal%2520memory%2520to%2520navigate%2520turbulence%26entry.906535625%3DMarco%2520Rando%2520and%2520Martin%2520James%2520and%2520Alessandro%2520Verri%2520and%2520Lorenzo%2520Rosasco%2520and%2520Agnese%2520Seminara%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520olfactory%2520searches%2520in%2520a%2520turbulent%2520environment.%2520We%250Afocus%2520on%2520agents%2520that%2520respond%2520solely%2520to%2520odor%2520stimuli%252C%2520with%2520no%2520access%2520to%2520spatial%250Aperception%2520nor%2520prior%2520information%2520about%2520the%2520odor.%2520We%2520ask%2520whether%2520navigation%2520to%2520a%250Atarget%2520can%2520be%2520learned%2520robustly%2520within%2520a%2520sequential%2520decision%2520making%2520framework.%250AWe%2520develop%2520a%2520reinforcement%2520learning%2520algorithm%2520using%2520a%2520small%2520set%2520of%250Ainterpretable%2520olfactory%2520states%2520and%2520train%2520it%2520with%2520realistic%2520turbulent%2520odor%2520cues.%250ABy%2520introducing%2520a%2520temporal%2520memory%252C%2520we%2520demonstrate%2520that%2520two%2520salient%2520features%2520of%250Aodor%2520traces%252C%2520discretized%2520in%2520few%2520olfactory%2520states%252C%2520are%2520sufficient%2520to%2520learn%250Anavigation%2520in%2520a%2520realistic%2520odor%2520plume.%2520Performance%2520is%2520dictated%2520by%2520the%2520sparse%250Anature%2520of%2520turbulent%2520odors.%2520An%2520optimal%2520memory%2520exists%2520which%2520ignores%2520blanks%2520within%250Athe%2520plume%2520and%2520activates%2520a%2520recovery%2520strategy%2520outside%2520the%2520plume.%2520We%2520obtain%2520the%250Abest%2520performance%2520by%2520letting%2520agents%2520learn%2520their%2520recovery%2520strategy%2520and%2520show%2520that%250Ait%2520is%2520mostly%2520casting%2520cross%2520wind%252C%2520similar%2520to%2520behavior%2520observed%2520in%2520flying%250Ainsects.%2520The%2520optimal%2520strategy%2520is%2520robust%2520to%2520substantial%2520changes%2520in%2520the%2520odor%250Aplumes%252C%2520suggesting%2520minor%2520parameter%2520tuning%2520may%2520be%2520sufficient%2520to%2520adapt%2520to%250Adifferent%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-learning%20with%20temporal%20memory%20to%20navigate%20turbulence&entry.906535625=Marco%20Rando%20and%20Martin%20James%20and%20Alessandro%20Verri%20and%20Lorenzo%20Rosasco%20and%20Agnese%20Seminara&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20olfactory%20searches%20in%20a%20turbulent%20environment.%20We%0Afocus%20on%20agents%20that%20respond%20solely%20to%20odor%20stimuli%2C%20with%20no%20access%20to%20spatial%0Aperception%20nor%20prior%20information%20about%20the%20odor.%20We%20ask%20whether%20navigation%20to%20a%0Atarget%20can%20be%20learned%20robustly%20within%20a%20sequential%20decision%20making%20framework.%0AWe%20develop%20a%20reinforcement%20learning%20algorithm%20using%20a%20small%20set%20of%0Ainterpretable%20olfactory%20states%20and%20train%20it%20with%20realistic%20turbulent%20odor%20cues.%0ABy%20introducing%20a%20temporal%20memory%2C%20we%20demonstrate%20that%20two%20salient%20features%20of%0Aodor%20traces%2C%20discretized%20in%20few%20olfactory%20states%2C%20are%20sufficient%20to%20learn%0Anavigation%20in%20a%20realistic%20odor%20plume.%20Performance%20is%20dictated%20by%20the%20sparse%0Anature%20of%20turbulent%20odors.%20An%20optimal%20memory%20exists%20which%20ignores%20blanks%20within%0Athe%20plume%20and%20activates%20a%20recovery%20strategy%20outside%20the%20plume.%20We%20obtain%20the%0Abest%20performance%20by%20letting%20agents%20learn%20their%20recovery%20strategy%20and%20show%20that%0Ait%20is%20mostly%20casting%20cross%20wind%2C%20similar%20to%20behavior%20observed%20in%20flying%0Ainsects.%20The%20optimal%20strategy%20is%20robust%20to%20substantial%20changes%20in%20the%20odor%0Aplumes%2C%20suggesting%20minor%20parameter%20tuning%20may%20be%20sufficient%20to%20adapt%20to%0Adifferent%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17495v2&entry.124074799=Read"},
{"title": "B-FPGM: Lightweight Face Detection via Bayesian-Optimized Soft FPGM\n  Pruning", "author": "Nikolaos Kaparinos and Vasileios Mezaris", "abstract": "  Face detection is a computer vision application that increasingly demands\nlightweight models to facilitate deployment on devices with limited\ncomputational resources. Neural network pruning is a promising technique that\ncan effectively reduce network size without significantly affecting\nperformance. In this work, we propose a novel face detection pruning pipeline\nthat leverages Filter Pruning via Geometric Median (FPGM) pruning, Soft Filter\nPruning (SFP) and Bayesian optimization in order to achieve a superior\ntrade-off between size and performance compared to existing approaches. FPGM\npruning is a structured pruning technique that allows pruning the least\nsignificant filters in each layer, while SFP iteratively prunes the filters and\nallows them to be updated in any subsequent training step. Bayesian\noptimization is employed in order to optimize the pruning rates of each layer,\nrather than relying on engineering expertise to determine the optimal pruning\nrates for each layer. In our experiments across all three subsets of the WIDER\nFACE dataset, our proposed approach B-FPGM consistently outperforms existing\nones in balancing model size and performance. All our experiments were applied\nto EResFD, the currently smallest (in number of parameters) well-performing\nface detector of the literature; a small ablation study with a second small\nface detector, EXTD, is also reported. The source code and trained pruned face\ndetection models can be found at: https://github.com/IDTITI/B-FPGM.\n", "link": "http://arxiv.org/abs/2501.16917v1", "date": "2025-01-28", "relevancy": 1.9173, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4819}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4797}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B-FPGM%3A%20Lightweight%20Face%20Detection%20via%20Bayesian-Optimized%20Soft%20FPGM%0A%20%20Pruning&body=Title%3A%20B-FPGM%3A%20Lightweight%20Face%20Detection%20via%20Bayesian-Optimized%20Soft%20FPGM%0A%20%20Pruning%0AAuthor%3A%20Nikolaos%20Kaparinos%20and%20Vasileios%20Mezaris%0AAbstract%3A%20%20%20Face%20detection%20is%20a%20computer%20vision%20application%20that%20increasingly%20demands%0Alightweight%20models%20to%20facilitate%20deployment%20on%20devices%20with%20limited%0Acomputational%20resources.%20Neural%20network%20pruning%20is%20a%20promising%20technique%20that%0Acan%20effectively%20reduce%20network%20size%20without%20significantly%20affecting%0Aperformance.%20In%20this%20work%2C%20we%20propose%20a%20novel%20face%20detection%20pruning%20pipeline%0Athat%20leverages%20Filter%20Pruning%20via%20Geometric%20Median%20%28FPGM%29%20pruning%2C%20Soft%20Filter%0APruning%20%28SFP%29%20and%20Bayesian%20optimization%20in%20order%20to%20achieve%20a%20superior%0Atrade-off%20between%20size%20and%20performance%20compared%20to%20existing%20approaches.%20FPGM%0Apruning%20is%20a%20structured%20pruning%20technique%20that%20allows%20pruning%20the%20least%0Asignificant%20filters%20in%20each%20layer%2C%20while%20SFP%20iteratively%20prunes%20the%20filters%20and%0Aallows%20them%20to%20be%20updated%20in%20any%20subsequent%20training%20step.%20Bayesian%0Aoptimization%20is%20employed%20in%20order%20to%20optimize%20the%20pruning%20rates%20of%20each%20layer%2C%0Arather%20than%20relying%20on%20engineering%20expertise%20to%20determine%20the%20optimal%20pruning%0Arates%20for%20each%20layer.%20In%20our%20experiments%20across%20all%20three%20subsets%20of%20the%20WIDER%0AFACE%20dataset%2C%20our%20proposed%20approach%20B-FPGM%20consistently%20outperforms%20existing%0Aones%20in%20balancing%20model%20size%20and%20performance.%20All%20our%20experiments%20were%20applied%0Ato%20EResFD%2C%20the%20currently%20smallest%20%28in%20number%20of%20parameters%29%20well-performing%0Aface%20detector%20of%20the%20literature%3B%20a%20small%20ablation%20study%20with%20a%20second%20small%0Aface%20detector%2C%20EXTD%2C%20is%20also%20reported.%20The%20source%20code%20and%20trained%20pruned%20face%0Adetection%20models%20can%20be%20found%20at%3A%20https%3A//github.com/IDTITI/B-FPGM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB-FPGM%253A%2520Lightweight%2520Face%2520Detection%2520via%2520Bayesian-Optimized%2520Soft%2520FPGM%250A%2520%2520Pruning%26entry.906535625%3DNikolaos%2520Kaparinos%2520and%2520Vasileios%2520Mezaris%26entry.1292438233%3D%2520%2520Face%2520detection%2520is%2520a%2520computer%2520vision%2520application%2520that%2520increasingly%2520demands%250Alightweight%2520models%2520to%2520facilitate%2520deployment%2520on%2520devices%2520with%2520limited%250Acomputational%2520resources.%2520Neural%2520network%2520pruning%2520is%2520a%2520promising%2520technique%2520that%250Acan%2520effectively%2520reduce%2520network%2520size%2520without%2520significantly%2520affecting%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520face%2520detection%2520pruning%2520pipeline%250Athat%2520leverages%2520Filter%2520Pruning%2520via%2520Geometric%2520Median%2520%2528FPGM%2529%2520pruning%252C%2520Soft%2520Filter%250APruning%2520%2528SFP%2529%2520and%2520Bayesian%2520optimization%2520in%2520order%2520to%2520achieve%2520a%2520superior%250Atrade-off%2520between%2520size%2520and%2520performance%2520compared%2520to%2520existing%2520approaches.%2520FPGM%250Apruning%2520is%2520a%2520structured%2520pruning%2520technique%2520that%2520allows%2520pruning%2520the%2520least%250Asignificant%2520filters%2520in%2520each%2520layer%252C%2520while%2520SFP%2520iteratively%2520prunes%2520the%2520filters%2520and%250Aallows%2520them%2520to%2520be%2520updated%2520in%2520any%2520subsequent%2520training%2520step.%2520Bayesian%250Aoptimization%2520is%2520employed%2520in%2520order%2520to%2520optimize%2520the%2520pruning%2520rates%2520of%2520each%2520layer%252C%250Arather%2520than%2520relying%2520on%2520engineering%2520expertise%2520to%2520determine%2520the%2520optimal%2520pruning%250Arates%2520for%2520each%2520layer.%2520In%2520our%2520experiments%2520across%2520all%2520three%2520subsets%2520of%2520the%2520WIDER%250AFACE%2520dataset%252C%2520our%2520proposed%2520approach%2520B-FPGM%2520consistently%2520outperforms%2520existing%250Aones%2520in%2520balancing%2520model%2520size%2520and%2520performance.%2520All%2520our%2520experiments%2520were%2520applied%250Ato%2520EResFD%252C%2520the%2520currently%2520smallest%2520%2528in%2520number%2520of%2520parameters%2529%2520well-performing%250Aface%2520detector%2520of%2520the%2520literature%253B%2520a%2520small%2520ablation%2520study%2520with%2520a%2520second%2520small%250Aface%2520detector%252C%2520EXTD%252C%2520is%2520also%2520reported.%2520The%2520source%2520code%2520and%2520trained%2520pruned%2520face%250Adetection%2520models%2520can%2520be%2520found%2520at%253A%2520https%253A//github.com/IDTITI/B-FPGM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B-FPGM%3A%20Lightweight%20Face%20Detection%20via%20Bayesian-Optimized%20Soft%20FPGM%0A%20%20Pruning&entry.906535625=Nikolaos%20Kaparinos%20and%20Vasileios%20Mezaris&entry.1292438233=%20%20Face%20detection%20is%20a%20computer%20vision%20application%20that%20increasingly%20demands%0Alightweight%20models%20to%20facilitate%20deployment%20on%20devices%20with%20limited%0Acomputational%20resources.%20Neural%20network%20pruning%20is%20a%20promising%20technique%20that%0Acan%20effectively%20reduce%20network%20size%20without%20significantly%20affecting%0Aperformance.%20In%20this%20work%2C%20we%20propose%20a%20novel%20face%20detection%20pruning%20pipeline%0Athat%20leverages%20Filter%20Pruning%20via%20Geometric%20Median%20%28FPGM%29%20pruning%2C%20Soft%20Filter%0APruning%20%28SFP%29%20and%20Bayesian%20optimization%20in%20order%20to%20achieve%20a%20superior%0Atrade-off%20between%20size%20and%20performance%20compared%20to%20existing%20approaches.%20FPGM%0Apruning%20is%20a%20structured%20pruning%20technique%20that%20allows%20pruning%20the%20least%0Asignificant%20filters%20in%20each%20layer%2C%20while%20SFP%20iteratively%20prunes%20the%20filters%20and%0Aallows%20them%20to%20be%20updated%20in%20any%20subsequent%20training%20step.%20Bayesian%0Aoptimization%20is%20employed%20in%20order%20to%20optimize%20the%20pruning%20rates%20of%20each%20layer%2C%0Arather%20than%20relying%20on%20engineering%20expertise%20to%20determine%20the%20optimal%20pruning%0Arates%20for%20each%20layer.%20In%20our%20experiments%20across%20all%20three%20subsets%20of%20the%20WIDER%0AFACE%20dataset%2C%20our%20proposed%20approach%20B-FPGM%20consistently%20outperforms%20existing%0Aones%20in%20balancing%20model%20size%20and%20performance.%20All%20our%20experiments%20were%20applied%0Ato%20EResFD%2C%20the%20currently%20smallest%20%28in%20number%20of%20parameters%29%20well-performing%0Aface%20detector%20of%20the%20literature%3B%20a%20small%20ablation%20study%20with%20a%20second%20small%0Aface%20detector%2C%20EXTD%2C%20is%20also%20reported.%20The%20source%20code%20and%20trained%20pruned%20face%0Adetection%20models%20can%20be%20found%20at%3A%20https%3A//github.com/IDTITI/B-FPGM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16917v1&entry.124074799=Read"},
{"title": "Giving Sense to Inputs: Toward an Accessible Control Framework for\n  Shared Autonomy", "author": "Shalutha Rajapakshe and Jean-Marc Odobez and Emmanuel Senft", "abstract": "  While shared autonomy offers significant potential for assistive robotics,\nkey questions remain about how to effectively map 2D control inputs to 6D robot\nmotions. An intuitive framework should allow users to input commands\neffortlessly, with the robot responding as expected, without users needing to\nanticipate the impact of their inputs. In this article, we propose a dynamic\ninput mapping framework that links joystick movements to motions on control\nframes defined along a trajectory encoded with canal surfaces. We evaluate our\nmethod in a user study with 20 participants, demonstrating that our input\nmapping framework reduces the workload and improves usability compared to a\nbaseline mapping with similar motion encoding. To prepare for deployment in\nassistive scenarios, we built on the development from the accessible gaming\ncommunity to select an accessible control interface. We then tested the system\nin an exploratory study, where three wheelchair users controlled the robot for\nboth daily living activities and a creative painting task, demonstrating its\nfeasibility for users closer to our target population.\n", "link": "http://arxiv.org/abs/2501.16929v1", "date": "2025-01-28", "relevancy": 1.9117, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6802}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.588}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Giving%20Sense%20to%20Inputs%3A%20Toward%20an%20Accessible%20Control%20Framework%20for%0A%20%20Shared%20Autonomy&body=Title%3A%20Giving%20Sense%20to%20Inputs%3A%20Toward%20an%20Accessible%20Control%20Framework%20for%0A%20%20Shared%20Autonomy%0AAuthor%3A%20Shalutha%20Rajapakshe%20and%20Jean-Marc%20Odobez%20and%20Emmanuel%20Senft%0AAbstract%3A%20%20%20While%20shared%20autonomy%20offers%20significant%20potential%20for%20assistive%20robotics%2C%0Akey%20questions%20remain%20about%20how%20to%20effectively%20map%202D%20control%20inputs%20to%206D%20robot%0Amotions.%20An%20intuitive%20framework%20should%20allow%20users%20to%20input%20commands%0Aeffortlessly%2C%20with%20the%20robot%20responding%20as%20expected%2C%20without%20users%20needing%20to%0Aanticipate%20the%20impact%20of%20their%20inputs.%20In%20this%20article%2C%20we%20propose%20a%20dynamic%0Ainput%20mapping%20framework%20that%20links%20joystick%20movements%20to%20motions%20on%20control%0Aframes%20defined%20along%20a%20trajectory%20encoded%20with%20canal%20surfaces.%20We%20evaluate%20our%0Amethod%20in%20a%20user%20study%20with%2020%20participants%2C%20demonstrating%20that%20our%20input%0Amapping%20framework%20reduces%20the%20workload%20and%20improves%20usability%20compared%20to%20a%0Abaseline%20mapping%20with%20similar%20motion%20encoding.%20To%20prepare%20for%20deployment%20in%0Aassistive%20scenarios%2C%20we%20built%20on%20the%20development%20from%20the%20accessible%20gaming%0Acommunity%20to%20select%20an%20accessible%20control%20interface.%20We%20then%20tested%20the%20system%0Ain%20an%20exploratory%20study%2C%20where%20three%20wheelchair%20users%20controlled%20the%20robot%20for%0Aboth%20daily%20living%20activities%20and%20a%20creative%20painting%20task%2C%20demonstrating%20its%0Afeasibility%20for%20users%20closer%20to%20our%20target%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGiving%2520Sense%2520to%2520Inputs%253A%2520Toward%2520an%2520Accessible%2520Control%2520Framework%2520for%250A%2520%2520Shared%2520Autonomy%26entry.906535625%3DShalutha%2520Rajapakshe%2520and%2520Jean-Marc%2520Odobez%2520and%2520Emmanuel%2520Senft%26entry.1292438233%3D%2520%2520While%2520shared%2520autonomy%2520offers%2520significant%2520potential%2520for%2520assistive%2520robotics%252C%250Akey%2520questions%2520remain%2520about%2520how%2520to%2520effectively%2520map%25202D%2520control%2520inputs%2520to%25206D%2520robot%250Amotions.%2520An%2520intuitive%2520framework%2520should%2520allow%2520users%2520to%2520input%2520commands%250Aeffortlessly%252C%2520with%2520the%2520robot%2520responding%2520as%2520expected%252C%2520without%2520users%2520needing%2520to%250Aanticipate%2520the%2520impact%2520of%2520their%2520inputs.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%2520dynamic%250Ainput%2520mapping%2520framework%2520that%2520links%2520joystick%2520movements%2520to%2520motions%2520on%2520control%250Aframes%2520defined%2520along%2520a%2520trajectory%2520encoded%2520with%2520canal%2520surfaces.%2520We%2520evaluate%2520our%250Amethod%2520in%2520a%2520user%2520study%2520with%252020%2520participants%252C%2520demonstrating%2520that%2520our%2520input%250Amapping%2520framework%2520reduces%2520the%2520workload%2520and%2520improves%2520usability%2520compared%2520to%2520a%250Abaseline%2520mapping%2520with%2520similar%2520motion%2520encoding.%2520To%2520prepare%2520for%2520deployment%2520in%250Aassistive%2520scenarios%252C%2520we%2520built%2520on%2520the%2520development%2520from%2520the%2520accessible%2520gaming%250Acommunity%2520to%2520select%2520an%2520accessible%2520control%2520interface.%2520We%2520then%2520tested%2520the%2520system%250Ain%2520an%2520exploratory%2520study%252C%2520where%2520three%2520wheelchair%2520users%2520controlled%2520the%2520robot%2520for%250Aboth%2520daily%2520living%2520activities%2520and%2520a%2520creative%2520painting%2520task%252C%2520demonstrating%2520its%250Afeasibility%2520for%2520users%2520closer%2520to%2520our%2520target%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Giving%20Sense%20to%20Inputs%3A%20Toward%20an%20Accessible%20Control%20Framework%20for%0A%20%20Shared%20Autonomy&entry.906535625=Shalutha%20Rajapakshe%20and%20Jean-Marc%20Odobez%20and%20Emmanuel%20Senft&entry.1292438233=%20%20While%20shared%20autonomy%20offers%20significant%20potential%20for%20assistive%20robotics%2C%0Akey%20questions%20remain%20about%20how%20to%20effectively%20map%202D%20control%20inputs%20to%206D%20robot%0Amotions.%20An%20intuitive%20framework%20should%20allow%20users%20to%20input%20commands%0Aeffortlessly%2C%20with%20the%20robot%20responding%20as%20expected%2C%20without%20users%20needing%20to%0Aanticipate%20the%20impact%20of%20their%20inputs.%20In%20this%20article%2C%20we%20propose%20a%20dynamic%0Ainput%20mapping%20framework%20that%20links%20joystick%20movements%20to%20motions%20on%20control%0Aframes%20defined%20along%20a%20trajectory%20encoded%20with%20canal%20surfaces.%20We%20evaluate%20our%0Amethod%20in%20a%20user%20study%20with%2020%20participants%2C%20demonstrating%20that%20our%20input%0Amapping%20framework%20reduces%20the%20workload%20and%20improves%20usability%20compared%20to%20a%0Abaseline%20mapping%20with%20similar%20motion%20encoding.%20To%20prepare%20for%20deployment%20in%0Aassistive%20scenarios%2C%20we%20built%20on%20the%20development%20from%20the%20accessible%20gaming%0Acommunity%20to%20select%20an%20accessible%20control%20interface.%20We%20then%20tested%20the%20system%0Ain%20an%20exploratory%20study%2C%20where%20three%20wheelchair%20users%20controlled%20the%20robot%20for%0Aboth%20daily%20living%20activities%20and%20a%20creative%20painting%20task%2C%20demonstrating%20its%0Afeasibility%20for%20users%20closer%20to%20our%20target%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16929v1&entry.124074799=Read"},
{"title": "Convergence of two-timescale gradient descent ascent dynamics:\n  finite-dimensional and mean-field perspectives", "author": "Jing An and Jianfeng Lu", "abstract": "  The two-timescale gradient descent-ascent (GDA) is a canonical gradient\nalgorithm designed to find Nash equilibria in min-max games. We analyze the\ntwo-timescale GDA by investigating the effects of learning rate ratios on\nconvergence behavior in both finite-dimensional and mean-field settings. In\nparticular, for finite-dimensional quadratic min-max games, we obtain long-time\nconvergence in near quasi-static regimes through the hypocoercivity method. For\nmean-field GDA dynamics, we investigate convergence under a finite-scale ratio\nusing a mixed synchronous-reflection coupling technique.\n", "link": "http://arxiv.org/abs/2501.17122v1", "date": "2025-01-28", "relevancy": 1.9034, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20two-timescale%20gradient%20descent%20ascent%20dynamics%3A%0A%20%20finite-dimensional%20and%20mean-field%20perspectives&body=Title%3A%20Convergence%20of%20two-timescale%20gradient%20descent%20ascent%20dynamics%3A%0A%20%20finite-dimensional%20and%20mean-field%20perspectives%0AAuthor%3A%20Jing%20An%20and%20Jianfeng%20Lu%0AAbstract%3A%20%20%20The%20two-timescale%20gradient%20descent-ascent%20%28GDA%29%20is%20a%20canonical%20gradient%0Aalgorithm%20designed%20to%20find%20Nash%20equilibria%20in%20min-max%20games.%20We%20analyze%20the%0Atwo-timescale%20GDA%20by%20investigating%20the%20effects%20of%20learning%20rate%20ratios%20on%0Aconvergence%20behavior%20in%20both%20finite-dimensional%20and%20mean-field%20settings.%20In%0Aparticular%2C%20for%20finite-dimensional%20quadratic%20min-max%20games%2C%20we%20obtain%20long-time%0Aconvergence%20in%20near%20quasi-static%20regimes%20through%20the%20hypocoercivity%20method.%20For%0Amean-field%20GDA%20dynamics%2C%20we%20investigate%20convergence%20under%20a%20finite-scale%20ratio%0Ausing%20a%20mixed%20synchronous-reflection%20coupling%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520two-timescale%2520gradient%2520descent%2520ascent%2520dynamics%253A%250A%2520%2520finite-dimensional%2520and%2520mean-field%2520perspectives%26entry.906535625%3DJing%2520An%2520and%2520Jianfeng%2520Lu%26entry.1292438233%3D%2520%2520The%2520two-timescale%2520gradient%2520descent-ascent%2520%2528GDA%2529%2520is%2520a%2520canonical%2520gradient%250Aalgorithm%2520designed%2520to%2520find%2520Nash%2520equilibria%2520in%2520min-max%2520games.%2520We%2520analyze%2520the%250Atwo-timescale%2520GDA%2520by%2520investigating%2520the%2520effects%2520of%2520learning%2520rate%2520ratios%2520on%250Aconvergence%2520behavior%2520in%2520both%2520finite-dimensional%2520and%2520mean-field%2520settings.%2520In%250Aparticular%252C%2520for%2520finite-dimensional%2520quadratic%2520min-max%2520games%252C%2520we%2520obtain%2520long-time%250Aconvergence%2520in%2520near%2520quasi-static%2520regimes%2520through%2520the%2520hypocoercivity%2520method.%2520For%250Amean-field%2520GDA%2520dynamics%252C%2520we%2520investigate%2520convergence%2520under%2520a%2520finite-scale%2520ratio%250Ausing%2520a%2520mixed%2520synchronous-reflection%2520coupling%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20two-timescale%20gradient%20descent%20ascent%20dynamics%3A%0A%20%20finite-dimensional%20and%20mean-field%20perspectives&entry.906535625=Jing%20An%20and%20Jianfeng%20Lu&entry.1292438233=%20%20The%20two-timescale%20gradient%20descent-ascent%20%28GDA%29%20is%20a%20canonical%20gradient%0Aalgorithm%20designed%20to%20find%20Nash%20equilibria%20in%20min-max%20games.%20We%20analyze%20the%0Atwo-timescale%20GDA%20by%20investigating%20the%20effects%20of%20learning%20rate%20ratios%20on%0Aconvergence%20behavior%20in%20both%20finite-dimensional%20and%20mean-field%20settings.%20In%0Aparticular%2C%20for%20finite-dimensional%20quadratic%20min-max%20games%2C%20we%20obtain%20long-time%0Aconvergence%20in%20near%20quasi-static%20regimes%20through%20the%20hypocoercivity%20method.%20For%0Amean-field%20GDA%20dynamics%2C%20we%20investigate%20convergence%20under%20a%20finite-scale%20ratio%0Ausing%20a%20mixed%20synchronous-reflection%20coupling%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17122v1&entry.124074799=Read"},
{"title": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings\n  of Reinforcement Learning Strategies", "author": "Manojkumar Parmar and Yuvaraj Govindarajulu", "abstract": "  Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented.\n", "link": "http://arxiv.org/abs/2501.17030v1", "date": "2025-01-28", "relevancy": 1.9026, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20in%20Ensuring%20AI%20Safety%20in%20DeepSeek-R1%20Models%3A%20The%20Shortcomings%0A%20%20of%20Reinforcement%20Learning%20Strategies&body=Title%3A%20Challenges%20in%20Ensuring%20AI%20Safety%20in%20DeepSeek-R1%20Models%3A%20The%20Shortcomings%0A%20%20of%20Reinforcement%20Learning%20Strategies%0AAuthor%3A%20Manojkumar%20Parmar%20and%20Yuvaraj%20Govindarajulu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20in%20reasoning%2C%0Aalignment%2C%20and%20task-specific%20performance.%20However%2C%20ensuring%20harmlessness%20in%0Athese%20systems%20remains%20a%20critical%20challenge%2C%20particularly%20in%20advanced%20models%0Alike%20DeepSeek-R1.%20This%20paper%20examines%20the%20limitations%20of%20Reinforcement%20Learning%0A%28RL%29%20as%20the%20primary%20approach%20for%20reducing%20harmful%20outputs%20in%20DeepSeek-R1%20and%0Acompares%20it%20with%20Supervised%20Fine-Tuning%20%28SFT%29.%20While%20RL%20improves%20reasoning%0Acapabilities%2C%20it%20faces%20challenges%20such%20as%20reward%20hacking%2C%20generalization%0Afailures%2C%20language%20mixing%2C%20and%20high%20computational%20costs.%20We%20propose%20hybrid%0Atraining%20approaches%20combining%20RL%20and%20SFT%20to%20achieve%20robust%20harmlessness%0Areduction.%20Usage%20recommendations%20and%20future%20directions%20for%20deploying%0ADeepSeek-R1%20responsibly%20are%20also%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520in%2520Ensuring%2520AI%2520Safety%2520in%2520DeepSeek-R1%2520Models%253A%2520The%2520Shortcomings%250A%2520%2520of%2520Reinforcement%2520Learning%2520Strategies%26entry.906535625%3DManojkumar%2520Parmar%2520and%2520Yuvaraj%2520Govindarajulu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520reasoning%252C%250Aalignment%252C%2520and%2520task-specific%2520performance.%2520However%252C%2520ensuring%2520harmlessness%2520in%250Athese%2520systems%2520remains%2520a%2520critical%2520challenge%252C%2520particularly%2520in%2520advanced%2520models%250Alike%2520DeepSeek-R1.%2520This%2520paper%2520examines%2520the%2520limitations%2520of%2520Reinforcement%2520Learning%250A%2528RL%2529%2520as%2520the%2520primary%2520approach%2520for%2520reducing%2520harmful%2520outputs%2520in%2520DeepSeek-R1%2520and%250Acompares%2520it%2520with%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529.%2520While%2520RL%2520improves%2520reasoning%250Acapabilities%252C%2520it%2520faces%2520challenges%2520such%2520as%2520reward%2520hacking%252C%2520generalization%250Afailures%252C%2520language%2520mixing%252C%2520and%2520high%2520computational%2520costs.%2520We%2520propose%2520hybrid%250Atraining%2520approaches%2520combining%2520RL%2520and%2520SFT%2520to%2520achieve%2520robust%2520harmlessness%250Areduction.%2520Usage%2520recommendations%2520and%2520future%2520directions%2520for%2520deploying%250ADeepSeek-R1%2520responsibly%2520are%2520also%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20in%20Ensuring%20AI%20Safety%20in%20DeepSeek-R1%20Models%3A%20The%20Shortcomings%0A%20%20of%20Reinforcement%20Learning%20Strategies&entry.906535625=Manojkumar%20Parmar%20and%20Yuvaraj%20Govindarajulu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20in%20reasoning%2C%0Aalignment%2C%20and%20task-specific%20performance.%20However%2C%20ensuring%20harmlessness%20in%0Athese%20systems%20remains%20a%20critical%20challenge%2C%20particularly%20in%20advanced%20models%0Alike%20DeepSeek-R1.%20This%20paper%20examines%20the%20limitations%20of%20Reinforcement%20Learning%0A%28RL%29%20as%20the%20primary%20approach%20for%20reducing%20harmful%20outputs%20in%20DeepSeek-R1%20and%0Acompares%20it%20with%20Supervised%20Fine-Tuning%20%28SFT%29.%20While%20RL%20improves%20reasoning%0Acapabilities%2C%20it%20faces%20challenges%20such%20as%20reward%20hacking%2C%20generalization%0Afailures%2C%20language%20mixing%2C%20and%20high%20computational%20costs.%20We%20propose%20hybrid%0Atraining%20approaches%20combining%20RL%20and%20SFT%20to%20achieve%20robust%20harmlessness%0Areduction.%20Usage%20recommendations%20and%20future%20directions%20for%20deploying%0ADeepSeek-R1%20responsibly%20are%20also%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17030v1&entry.124074799=Read"},
{"title": "A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from\n  Computed Tomography (CT) Scan Images", "author": "Suresh Babu Nettur and Shanthi Karpurapu and Unnati Nettur and Likhit Sagar Gajja and Sravanthy Myneni and Akhil Dusi and Lalithya Posham", "abstract": "  Early detection of COVID-19 is crucial for effective treatment and\ncontrolling its spread. This study proposes a novel hybrid deep learning model\nfor detecting COVID-19 from CT scan images, designed to assist overburdened\nmedical professionals. Our proposed model leverages the strengths of VGG16,\nDenseNet121, and MobileNetV2 to extract features, followed by Principal\nComponent Analysis (PCA) for dimensionality reduction, after which the features\nare stacked and classified using a Support Vector Classifier (SVC). We\nconducted comparative analysis between the proposed hybrid model and individual\npre-trained CNN models, using a dataset of 2,108 training images and 373 test\nimages comprising both COVID-positive and non-COVID images. Our proposed hybrid\nmodel achieved an accuracy of 98.93%, outperforming the individual models in\nterms of precision, recall, F1 scores, and ROC curve performance.\n", "link": "http://arxiv.org/abs/2501.17160v1", "date": "2025-01-28", "relevancy": 1.8914, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4678}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Deep%20Learning%20CNN%20Model%20for%20Enhanced%20COVID-19%20Detection%20from%0A%20%20Computed%20Tomography%20%28CT%29%20Scan%20Images&body=Title%3A%20A%20Hybrid%20Deep%20Learning%20CNN%20Model%20for%20Enhanced%20COVID-19%20Detection%20from%0A%20%20Computed%20Tomography%20%28CT%29%20Scan%20Images%0AAuthor%3A%20Suresh%20Babu%20Nettur%20and%20Shanthi%20Karpurapu%20and%20Unnati%20Nettur%20and%20Likhit%20Sagar%20Gajja%20and%20Sravanthy%20Myneni%20and%20Akhil%20Dusi%20and%20Lalithya%20Posham%0AAbstract%3A%20%20%20Early%20detection%20of%20COVID-19%20is%20crucial%20for%20effective%20treatment%20and%0Acontrolling%20its%20spread.%20This%20study%20proposes%20a%20novel%20hybrid%20deep%20learning%20model%0Afor%20detecting%20COVID-19%20from%20CT%20scan%20images%2C%20designed%20to%20assist%20overburdened%0Amedical%20professionals.%20Our%20proposed%20model%20leverages%20the%20strengths%20of%20VGG16%2C%0ADenseNet121%2C%20and%20MobileNetV2%20to%20extract%20features%2C%20followed%20by%20Principal%0AComponent%20Analysis%20%28PCA%29%20for%20dimensionality%20reduction%2C%20after%20which%20the%20features%0Aare%20stacked%20and%20classified%20using%20a%20Support%20Vector%20Classifier%20%28SVC%29.%20We%0Aconducted%20comparative%20analysis%20between%20the%20proposed%20hybrid%20model%20and%20individual%0Apre-trained%20CNN%20models%2C%20using%20a%20dataset%20of%202%2C108%20training%20images%20and%20373%20test%0Aimages%20comprising%20both%20COVID-positive%20and%20non-COVID%20images.%20Our%20proposed%20hybrid%0Amodel%20achieved%20an%20accuracy%20of%2098.93%25%2C%20outperforming%20the%20individual%20models%20in%0Aterms%20of%20precision%2C%20recall%2C%20F1%20scores%2C%20and%20ROC%20curve%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Deep%2520Learning%2520CNN%2520Model%2520for%2520Enhanced%2520COVID-19%2520Detection%2520from%250A%2520%2520Computed%2520Tomography%2520%2528CT%2529%2520Scan%2520Images%26entry.906535625%3DSuresh%2520Babu%2520Nettur%2520and%2520Shanthi%2520Karpurapu%2520and%2520Unnati%2520Nettur%2520and%2520Likhit%2520Sagar%2520Gajja%2520and%2520Sravanthy%2520Myneni%2520and%2520Akhil%2520Dusi%2520and%2520Lalithya%2520Posham%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520COVID-19%2520is%2520crucial%2520for%2520effective%2520treatment%2520and%250Acontrolling%2520its%2520spread.%2520This%2520study%2520proposes%2520a%2520novel%2520hybrid%2520deep%2520learning%2520model%250Afor%2520detecting%2520COVID-19%2520from%2520CT%2520scan%2520images%252C%2520designed%2520to%2520assist%2520overburdened%250Amedical%2520professionals.%2520Our%2520proposed%2520model%2520leverages%2520the%2520strengths%2520of%2520VGG16%252C%250ADenseNet121%252C%2520and%2520MobileNetV2%2520to%2520extract%2520features%252C%2520followed%2520by%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529%2520for%2520dimensionality%2520reduction%252C%2520after%2520which%2520the%2520features%250Aare%2520stacked%2520and%2520classified%2520using%2520a%2520Support%2520Vector%2520Classifier%2520%2528SVC%2529.%2520We%250Aconducted%2520comparative%2520analysis%2520between%2520the%2520proposed%2520hybrid%2520model%2520and%2520individual%250Apre-trained%2520CNN%2520models%252C%2520using%2520a%2520dataset%2520of%25202%252C108%2520training%2520images%2520and%2520373%2520test%250Aimages%2520comprising%2520both%2520COVID-positive%2520and%2520non-COVID%2520images.%2520Our%2520proposed%2520hybrid%250Amodel%2520achieved%2520an%2520accuracy%2520of%252098.93%2525%252C%2520outperforming%2520the%2520individual%2520models%2520in%250Aterms%2520of%2520precision%252C%2520recall%252C%2520F1%2520scores%252C%2520and%2520ROC%2520curve%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Deep%20Learning%20CNN%20Model%20for%20Enhanced%20COVID-19%20Detection%20from%0A%20%20Computed%20Tomography%20%28CT%29%20Scan%20Images&entry.906535625=Suresh%20Babu%20Nettur%20and%20Shanthi%20Karpurapu%20and%20Unnati%20Nettur%20and%20Likhit%20Sagar%20Gajja%20and%20Sravanthy%20Myneni%20and%20Akhil%20Dusi%20and%20Lalithya%20Posham&entry.1292438233=%20%20Early%20detection%20of%20COVID-19%20is%20crucial%20for%20effective%20treatment%20and%0Acontrolling%20its%20spread.%20This%20study%20proposes%20a%20novel%20hybrid%20deep%20learning%20model%0Afor%20detecting%20COVID-19%20from%20CT%20scan%20images%2C%20designed%20to%20assist%20overburdened%0Amedical%20professionals.%20Our%20proposed%20model%20leverages%20the%20strengths%20of%20VGG16%2C%0ADenseNet121%2C%20and%20MobileNetV2%20to%20extract%20features%2C%20followed%20by%20Principal%0AComponent%20Analysis%20%28PCA%29%20for%20dimensionality%20reduction%2C%20after%20which%20the%20features%0Aare%20stacked%20and%20classified%20using%20a%20Support%20Vector%20Classifier%20%28SVC%29.%20We%0Aconducted%20comparative%20analysis%20between%20the%20proposed%20hybrid%20model%20and%20individual%0Apre-trained%20CNN%20models%2C%20using%20a%20dataset%20of%202%2C108%20training%20images%20and%20373%20test%0Aimages%20comprising%20both%20COVID-positive%20and%20non-COVID%20images.%20Our%20proposed%20hybrid%0Amodel%20achieved%20an%20accuracy%20of%2098.93%25%2C%20outperforming%20the%20individual%20models%20in%0Aterms%20of%20precision%2C%20recall%2C%20F1%20scores%2C%20and%20ROC%20curve%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17160v1&entry.124074799=Read"},
{"title": "Quantifying Uncertainty and Variability in Machine Learning: Confidence\n  Intervals for Quantiles in Performance Metric Distributions", "author": "Christoph Lehmann and Yahor Paromau", "abstract": "  Machine learning models are widely used in applications where reliability and\nrobustness are critical. Model evaluation often relies on single-point\nestimates of performance metrics such as accuracy, F1 score, or mean squared\nerror, that fail to capture the inherent variability in model performance. This\nvariability arises from multiple sources, including train-test split, weights\ninitialization, and hyperparameter tuning. Investigating the characteristics of\nperformance metric distributions, rather than focusing on a single point only,\nis essential for informed decision-making during model selection and\noptimization, especially in high-stakes settings.\n  How does the performance metric vary due to intrinsic uncertainty in the\nselected modeling approach? For example, train-test split is modified, initial\nweights for optimization are modified or hyperparameter tuning is done using an\nalgorithm with probabilistic nature?\n  This is shifting the focus from identifying a single best model to\nunderstanding a distribution of the performance metric that captures\nvariability across different training conditions. By running multiple\nexperiments with varied settings, empirical distributions of performance\nmetrics can be generated. Analyzing these distributions can lead to more robust\nmodels that generalize well across diverse scenarios.\n  This contribution explores the use of quantiles and confidence intervals to\nanalyze such distributions, providing a more complete understanding of model\nperformance and its uncertainty. Aimed at a statistically interested audience\nwithin the machine learning community, the suggested approaches are easy to\nimplement and apply to various performance metrics for classification and\nregression problems. Given the often long training times in ML, particular\nattention is given to small sample sizes (in the order of 10-25).\n", "link": "http://arxiv.org/abs/2501.16931v1", "date": "2025-01-28", "relevancy": 1.8904, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5049}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4703}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Uncertainty%20and%20Variability%20in%20Machine%20Learning%3A%20Confidence%0A%20%20Intervals%20for%20Quantiles%20in%20Performance%20Metric%20Distributions&body=Title%3A%20Quantifying%20Uncertainty%20and%20Variability%20in%20Machine%20Learning%3A%20Confidence%0A%20%20Intervals%20for%20Quantiles%20in%20Performance%20Metric%20Distributions%0AAuthor%3A%20Christoph%20Lehmann%20and%20Yahor%20Paromau%0AAbstract%3A%20%20%20Machine%20learning%20models%20are%20widely%20used%20in%20applications%20where%20reliability%20and%0Arobustness%20are%20critical.%20Model%20evaluation%20often%20relies%20on%20single-point%0Aestimates%20of%20performance%20metrics%20such%20as%20accuracy%2C%20F1%20score%2C%20or%20mean%20squared%0Aerror%2C%20that%20fail%20to%20capture%20the%20inherent%20variability%20in%20model%20performance.%20This%0Avariability%20arises%20from%20multiple%20sources%2C%20including%20train-test%20split%2C%20weights%0Ainitialization%2C%20and%20hyperparameter%20tuning.%20Investigating%20the%20characteristics%20of%0Aperformance%20metric%20distributions%2C%20rather%20than%20focusing%20on%20a%20single%20point%20only%2C%0Ais%20essential%20for%20informed%20decision-making%20during%20model%20selection%20and%0Aoptimization%2C%20especially%20in%20high-stakes%20settings.%0A%20%20How%20does%20the%20performance%20metric%20vary%20due%20to%20intrinsic%20uncertainty%20in%20the%0Aselected%20modeling%20approach%3F%20For%20example%2C%20train-test%20split%20is%20modified%2C%20initial%0Aweights%20for%20optimization%20are%20modified%20or%20hyperparameter%20tuning%20is%20done%20using%20an%0Aalgorithm%20with%20probabilistic%20nature%3F%0A%20%20This%20is%20shifting%20the%20focus%20from%20identifying%20a%20single%20best%20model%20to%0Aunderstanding%20a%20distribution%20of%20the%20performance%20metric%20that%20captures%0Avariability%20across%20different%20training%20conditions.%20By%20running%20multiple%0Aexperiments%20with%20varied%20settings%2C%20empirical%20distributions%20of%20performance%0Ametrics%20can%20be%20generated.%20Analyzing%20these%20distributions%20can%20lead%20to%20more%20robust%0Amodels%20that%20generalize%20well%20across%20diverse%20scenarios.%0A%20%20This%20contribution%20explores%20the%20use%20of%20quantiles%20and%20confidence%20intervals%20to%0Aanalyze%20such%20distributions%2C%20providing%20a%20more%20complete%20understanding%20of%20model%0Aperformance%20and%20its%20uncertainty.%20Aimed%20at%20a%20statistically%20interested%20audience%0Awithin%20the%20machine%20learning%20community%2C%20the%20suggested%20approaches%20are%20easy%20to%0Aimplement%20and%20apply%20to%20various%20performance%20metrics%20for%20classification%20and%0Aregression%20problems.%20Given%20the%20often%20long%20training%20times%20in%20ML%2C%20particular%0Aattention%20is%20given%20to%20small%20sample%20sizes%20%28in%20the%20order%20of%2010-25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Uncertainty%2520and%2520Variability%2520in%2520Machine%2520Learning%253A%2520Confidence%250A%2520%2520Intervals%2520for%2520Quantiles%2520in%2520Performance%2520Metric%2520Distributions%26entry.906535625%3DChristoph%2520Lehmann%2520and%2520Yahor%2520Paromau%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520are%2520widely%2520used%2520in%2520applications%2520where%2520reliability%2520and%250Arobustness%2520are%2520critical.%2520Model%2520evaluation%2520often%2520relies%2520on%2520single-point%250Aestimates%2520of%2520performance%2520metrics%2520such%2520as%2520accuracy%252C%2520F1%2520score%252C%2520or%2520mean%2520squared%250Aerror%252C%2520that%2520fail%2520to%2520capture%2520the%2520inherent%2520variability%2520in%2520model%2520performance.%2520This%250Avariability%2520arises%2520from%2520multiple%2520sources%252C%2520including%2520train-test%2520split%252C%2520weights%250Ainitialization%252C%2520and%2520hyperparameter%2520tuning.%2520Investigating%2520the%2520characteristics%2520of%250Aperformance%2520metric%2520distributions%252C%2520rather%2520than%2520focusing%2520on%2520a%2520single%2520point%2520only%252C%250Ais%2520essential%2520for%2520informed%2520decision-making%2520during%2520model%2520selection%2520and%250Aoptimization%252C%2520especially%2520in%2520high-stakes%2520settings.%250A%2520%2520How%2520does%2520the%2520performance%2520metric%2520vary%2520due%2520to%2520intrinsic%2520uncertainty%2520in%2520the%250Aselected%2520modeling%2520approach%253F%2520For%2520example%252C%2520train-test%2520split%2520is%2520modified%252C%2520initial%250Aweights%2520for%2520optimization%2520are%2520modified%2520or%2520hyperparameter%2520tuning%2520is%2520done%2520using%2520an%250Aalgorithm%2520with%2520probabilistic%2520nature%253F%250A%2520%2520This%2520is%2520shifting%2520the%2520focus%2520from%2520identifying%2520a%2520single%2520best%2520model%2520to%250Aunderstanding%2520a%2520distribution%2520of%2520the%2520performance%2520metric%2520that%2520captures%250Avariability%2520across%2520different%2520training%2520conditions.%2520By%2520running%2520multiple%250Aexperiments%2520with%2520varied%2520settings%252C%2520empirical%2520distributions%2520of%2520performance%250Ametrics%2520can%2520be%2520generated.%2520Analyzing%2520these%2520distributions%2520can%2520lead%2520to%2520more%2520robust%250Amodels%2520that%2520generalize%2520well%2520across%2520diverse%2520scenarios.%250A%2520%2520This%2520contribution%2520explores%2520the%2520use%2520of%2520quantiles%2520and%2520confidence%2520intervals%2520to%250Aanalyze%2520such%2520distributions%252C%2520providing%2520a%2520more%2520complete%2520understanding%2520of%2520model%250Aperformance%2520and%2520its%2520uncertainty.%2520Aimed%2520at%2520a%2520statistically%2520interested%2520audience%250Awithin%2520the%2520machine%2520learning%2520community%252C%2520the%2520suggested%2520approaches%2520are%2520easy%2520to%250Aimplement%2520and%2520apply%2520to%2520various%2520performance%2520metrics%2520for%2520classification%2520and%250Aregression%2520problems.%2520Given%2520the%2520often%2520long%2520training%2520times%2520in%2520ML%252C%2520particular%250Aattention%2520is%2520given%2520to%2520small%2520sample%2520sizes%2520%2528in%2520the%2520order%2520of%252010-25%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Uncertainty%20and%20Variability%20in%20Machine%20Learning%3A%20Confidence%0A%20%20Intervals%20for%20Quantiles%20in%20Performance%20Metric%20Distributions&entry.906535625=Christoph%20Lehmann%20and%20Yahor%20Paromau&entry.1292438233=%20%20Machine%20learning%20models%20are%20widely%20used%20in%20applications%20where%20reliability%20and%0Arobustness%20are%20critical.%20Model%20evaluation%20often%20relies%20on%20single-point%0Aestimates%20of%20performance%20metrics%20such%20as%20accuracy%2C%20F1%20score%2C%20or%20mean%20squared%0Aerror%2C%20that%20fail%20to%20capture%20the%20inherent%20variability%20in%20model%20performance.%20This%0Avariability%20arises%20from%20multiple%20sources%2C%20including%20train-test%20split%2C%20weights%0Ainitialization%2C%20and%20hyperparameter%20tuning.%20Investigating%20the%20characteristics%20of%0Aperformance%20metric%20distributions%2C%20rather%20than%20focusing%20on%20a%20single%20point%20only%2C%0Ais%20essential%20for%20informed%20decision-making%20during%20model%20selection%20and%0Aoptimization%2C%20especially%20in%20high-stakes%20settings.%0A%20%20How%20does%20the%20performance%20metric%20vary%20due%20to%20intrinsic%20uncertainty%20in%20the%0Aselected%20modeling%20approach%3F%20For%20example%2C%20train-test%20split%20is%20modified%2C%20initial%0Aweights%20for%20optimization%20are%20modified%20or%20hyperparameter%20tuning%20is%20done%20using%20an%0Aalgorithm%20with%20probabilistic%20nature%3F%0A%20%20This%20is%20shifting%20the%20focus%20from%20identifying%20a%20single%20best%20model%20to%0Aunderstanding%20a%20distribution%20of%20the%20performance%20metric%20that%20captures%0Avariability%20across%20different%20training%20conditions.%20By%20running%20multiple%0Aexperiments%20with%20varied%20settings%2C%20empirical%20distributions%20of%20performance%0Ametrics%20can%20be%20generated.%20Analyzing%20these%20distributions%20can%20lead%20to%20more%20robust%0Amodels%20that%20generalize%20well%20across%20diverse%20scenarios.%0A%20%20This%20contribution%20explores%20the%20use%20of%20quantiles%20and%20confidence%20intervals%20to%0Aanalyze%20such%20distributions%2C%20providing%20a%20more%20complete%20understanding%20of%20model%0Aperformance%20and%20its%20uncertainty.%20Aimed%20at%20a%20statistically%20interested%20audience%0Awithin%20the%20machine%20learning%20community%2C%20the%20suggested%20approaches%20are%20easy%20to%0Aimplement%20and%20apply%20to%20various%20performance%20metrics%20for%20classification%20and%0Aregression%20problems.%20Given%20the%20often%20long%20training%20times%20in%20ML%2C%20particular%0Aattention%20is%20given%20to%20small%20sample%20sizes%20%28in%20the%20order%20of%2010-25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16931v1&entry.124074799=Read"},
{"title": "Sources of Uncertainty in Supervised Machine Learning -- A\n  Statisticians' View", "author": "Cornelia Gruber and Patrick Oliver Schenk and Malte Schierholz and Frauke Kreuter and G\u00f6ran Kauermann", "abstract": "  Supervised machine learning and predictive models have achieved an impressive\nstandard today, enabling us to answer questions that were inconceivable a few\nyears ago. Besides these successes, it becomes clear, that beyond pure\nprediction, which is the primary strength of most supervised machine learning\nalgorithms, the quantification of uncertainty is relevant and necessary as\nwell. However, before quantification is possible, types and sources of\nuncertainty need to be defined precisely. While first concepts and ideas in\nthis direction have emerged in recent years, this paper adopts a conceptual,\nbasic science perspective and examines possible sources of uncertainty. By\nadopting the viewpoint of a statistician, we discuss the concepts of aleatoric\nand epistemic uncertainty, which are more commonly associated with machine\nlearning. The paper aims to formalize the two types of uncertainty and\ndemonstrates that sources of uncertainty are miscellaneous and can not always\nbe decomposed into aleatoric and epistemic. Drawing parallels between\nstatistical concepts and uncertainty in machine learning, we emphasise the role\nof data and their influence on uncertainty.\n", "link": "http://arxiv.org/abs/2305.16703v3", "date": "2025-01-28", "relevancy": 1.8799, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4971}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4686}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sources%20of%20Uncertainty%20in%20Supervised%20Machine%20Learning%20--%20A%0A%20%20Statisticians%27%20View&body=Title%3A%20Sources%20of%20Uncertainty%20in%20Supervised%20Machine%20Learning%20--%20A%0A%20%20Statisticians%27%20View%0AAuthor%3A%20Cornelia%20Gruber%20and%20Patrick%20Oliver%20Schenk%20and%20Malte%20Schierholz%20and%20Frauke%20Kreuter%20and%20G%C3%B6ran%20Kauermann%0AAbstract%3A%20%20%20Supervised%20machine%20learning%20and%20predictive%20models%20have%20achieved%20an%20impressive%0Astandard%20today%2C%20enabling%20us%20to%20answer%20questions%20that%20were%20inconceivable%20a%20few%0Ayears%20ago.%20Besides%20these%20successes%2C%20it%20becomes%20clear%2C%20that%20beyond%20pure%0Aprediction%2C%20which%20is%20the%20primary%20strength%20of%20most%20supervised%20machine%20learning%0Aalgorithms%2C%20the%20quantification%20of%20uncertainty%20is%20relevant%20and%20necessary%20as%0Awell.%20However%2C%20before%20quantification%20is%20possible%2C%20types%20and%20sources%20of%0Auncertainty%20need%20to%20be%20defined%20precisely.%20While%20first%20concepts%20and%20ideas%20in%0Athis%20direction%20have%20emerged%20in%20recent%20years%2C%20this%20paper%20adopts%20a%20conceptual%2C%0Abasic%20science%20perspective%20and%20examines%20possible%20sources%20of%20uncertainty.%20By%0Aadopting%20the%20viewpoint%20of%20a%20statistician%2C%20we%20discuss%20the%20concepts%20of%20aleatoric%0Aand%20epistemic%20uncertainty%2C%20which%20are%20more%20commonly%20associated%20with%20machine%0Alearning.%20The%20paper%20aims%20to%20formalize%20the%20two%20types%20of%20uncertainty%20and%0Ademonstrates%20that%20sources%20of%20uncertainty%20are%20miscellaneous%20and%20can%20not%20always%0Abe%20decomposed%20into%20aleatoric%20and%20epistemic.%20Drawing%20parallels%20between%0Astatistical%20concepts%20and%20uncertainty%20in%20machine%20learning%2C%20we%20emphasise%20the%20role%0Aof%20data%20and%20their%20influence%20on%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16703v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSources%2520of%2520Uncertainty%2520in%2520Supervised%2520Machine%2520Learning%2520--%2520A%250A%2520%2520Statisticians%2527%2520View%26entry.906535625%3DCornelia%2520Gruber%2520and%2520Patrick%2520Oliver%2520Schenk%2520and%2520Malte%2520Schierholz%2520and%2520Frauke%2520Kreuter%2520and%2520G%25C3%25B6ran%2520Kauermann%26entry.1292438233%3D%2520%2520Supervised%2520machine%2520learning%2520and%2520predictive%2520models%2520have%2520achieved%2520an%2520impressive%250Astandard%2520today%252C%2520enabling%2520us%2520to%2520answer%2520questions%2520that%2520were%2520inconceivable%2520a%2520few%250Ayears%2520ago.%2520Besides%2520these%2520successes%252C%2520it%2520becomes%2520clear%252C%2520that%2520beyond%2520pure%250Aprediction%252C%2520which%2520is%2520the%2520primary%2520strength%2520of%2520most%2520supervised%2520machine%2520learning%250Aalgorithms%252C%2520the%2520quantification%2520of%2520uncertainty%2520is%2520relevant%2520and%2520necessary%2520as%250Awell.%2520However%252C%2520before%2520quantification%2520is%2520possible%252C%2520types%2520and%2520sources%2520of%250Auncertainty%2520need%2520to%2520be%2520defined%2520precisely.%2520While%2520first%2520concepts%2520and%2520ideas%2520in%250Athis%2520direction%2520have%2520emerged%2520in%2520recent%2520years%252C%2520this%2520paper%2520adopts%2520a%2520conceptual%252C%250Abasic%2520science%2520perspective%2520and%2520examines%2520possible%2520sources%2520of%2520uncertainty.%2520By%250Aadopting%2520the%2520viewpoint%2520of%2520a%2520statistician%252C%2520we%2520discuss%2520the%2520concepts%2520of%2520aleatoric%250Aand%2520epistemic%2520uncertainty%252C%2520which%2520are%2520more%2520commonly%2520associated%2520with%2520machine%250Alearning.%2520The%2520paper%2520aims%2520to%2520formalize%2520the%2520two%2520types%2520of%2520uncertainty%2520and%250Ademonstrates%2520that%2520sources%2520of%2520uncertainty%2520are%2520miscellaneous%2520and%2520can%2520not%2520always%250Abe%2520decomposed%2520into%2520aleatoric%2520and%2520epistemic.%2520Drawing%2520parallels%2520between%250Astatistical%2520concepts%2520and%2520uncertainty%2520in%2520machine%2520learning%252C%2520we%2520emphasise%2520the%2520role%250Aof%2520data%2520and%2520their%2520influence%2520on%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16703v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sources%20of%20Uncertainty%20in%20Supervised%20Machine%20Learning%20--%20A%0A%20%20Statisticians%27%20View&entry.906535625=Cornelia%20Gruber%20and%20Patrick%20Oliver%20Schenk%20and%20Malte%20Schierholz%20and%20Frauke%20Kreuter%20and%20G%C3%B6ran%20Kauermann&entry.1292438233=%20%20Supervised%20machine%20learning%20and%20predictive%20models%20have%20achieved%20an%20impressive%0Astandard%20today%2C%20enabling%20us%20to%20answer%20questions%20that%20were%20inconceivable%20a%20few%0Ayears%20ago.%20Besides%20these%20successes%2C%20it%20becomes%20clear%2C%20that%20beyond%20pure%0Aprediction%2C%20which%20is%20the%20primary%20strength%20of%20most%20supervised%20machine%20learning%0Aalgorithms%2C%20the%20quantification%20of%20uncertainty%20is%20relevant%20and%20necessary%20as%0Awell.%20However%2C%20before%20quantification%20is%20possible%2C%20types%20and%20sources%20of%0Auncertainty%20need%20to%20be%20defined%20precisely.%20While%20first%20concepts%20and%20ideas%20in%0Athis%20direction%20have%20emerged%20in%20recent%20years%2C%20this%20paper%20adopts%20a%20conceptual%2C%0Abasic%20science%20perspective%20and%20examines%20possible%20sources%20of%20uncertainty.%20By%0Aadopting%20the%20viewpoint%20of%20a%20statistician%2C%20we%20discuss%20the%20concepts%20of%20aleatoric%0Aand%20epistemic%20uncertainty%2C%20which%20are%20more%20commonly%20associated%20with%20machine%0Alearning.%20The%20paper%20aims%20to%20formalize%20the%20two%20types%20of%20uncertainty%20and%0Ademonstrates%20that%20sources%20of%20uncertainty%20are%20miscellaneous%20and%20can%20not%20always%0Abe%20decomposed%20into%20aleatoric%20and%20epistemic.%20Drawing%20parallels%20between%0Astatistical%20concepts%20and%20uncertainty%20in%20machine%20learning%2C%20we%20emphasise%20the%20role%0Aof%20data%20and%20their%20influence%20on%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16703v3&entry.124074799=Read"},
{"title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "author": "Sara Abdali and Can Goksen and Saeed Amizadeh and Kazuhito Koishida", "abstract": "  Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research.\n", "link": "http://arxiv.org/abs/2501.14917v2", "date": "2025-01-28", "relevancy": 1.8763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4719}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-reflecting%20Large%20Language%20Models%3A%20A%20Hegelian%20Dialectical%20Approach&body=Title%3A%20Self-reflecting%20Large%20Language%20Models%3A%20A%20Hegelian%20Dialectical%20Approach%0AAuthor%3A%20Sara%20Abdali%20and%20Can%20Goksen%20and%20Saeed%20Amizadeh%20and%20Kazuhito%20Koishida%0AAbstract%3A%20%20%20Investigating%20NLP%20through%20a%20philosophical%20lens%20has%20recently%20caught%0Aresearcher%27s%20eyes%20as%20it%20connects%20computational%20methods%20with%20classical%20schools%0Aof%20philosophy.%20This%20paper%20introduces%20a%20philosophical%20approach%20inspired%20by%20the%0AHegelian%20Dialectic%20for%20LLMs%27%20self-reflection%2C%20utilizing%20a%20self-dialectical%0Aapproach%20to%20emulate%20internal%20critiques%20and%20then%20synthesize%20new%20ideas%20by%0Aresolving%20the%20contradicting%20points.%20Moreover%2C%20this%20paper%20investigates%20the%0Aeffect%20of%20LLMs%27%20temperature%20for%20generation%20by%20establishing%20a%20dynamic%20annealing%0Aapproach%2C%20which%20promotes%20the%20creativity%20in%20the%20early%20stages%20and%20gradually%0Arefines%20it%20by%20focusing%20on%20the%20nuances%2C%20as%20well%20as%20a%20fixed%20temperature%20strategy%0Afor%20generation.%20Our%20proposed%20approach%20is%20examined%20to%20determine%20its%20ability%20to%0Agenerate%20novel%20ideas%20from%20an%20initial%20proposition.%20Additionally%2C%20a%20Multi%20Agent%0AMajority%20Voting%20%28MAMV%29%20strategy%20is%20leveraged%20to%20assess%20the%20validity%20and%20novelty%0Aof%20the%20generated%20ideas%2C%20which%20proves%20beneficial%20in%20the%20absence%20of%20domain%0Aexperts.%20Our%20experiments%20show%20promise%20in%20generating%20new%20ideas%20and%20provide%20a%0Astepping%20stone%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-reflecting%2520Large%2520Language%2520Models%253A%2520A%2520Hegelian%2520Dialectical%2520Approach%26entry.906535625%3DSara%2520Abdali%2520and%2520Can%2520Goksen%2520and%2520Saeed%2520Amizadeh%2520and%2520Kazuhito%2520Koishida%26entry.1292438233%3D%2520%2520Investigating%2520NLP%2520through%2520a%2520philosophical%2520lens%2520has%2520recently%2520caught%250Aresearcher%2527s%2520eyes%2520as%2520it%2520connects%2520computational%2520methods%2520with%2520classical%2520schools%250Aof%2520philosophy.%2520This%2520paper%2520introduces%2520a%2520philosophical%2520approach%2520inspired%2520by%2520the%250AHegelian%2520Dialectic%2520for%2520LLMs%2527%2520self-reflection%252C%2520utilizing%2520a%2520self-dialectical%250Aapproach%2520to%2520emulate%2520internal%2520critiques%2520and%2520then%2520synthesize%2520new%2520ideas%2520by%250Aresolving%2520the%2520contradicting%2520points.%2520Moreover%252C%2520this%2520paper%2520investigates%2520the%250Aeffect%2520of%2520LLMs%2527%2520temperature%2520for%2520generation%2520by%2520establishing%2520a%2520dynamic%2520annealing%250Aapproach%252C%2520which%2520promotes%2520the%2520creativity%2520in%2520the%2520early%2520stages%2520and%2520gradually%250Arefines%2520it%2520by%2520focusing%2520on%2520the%2520nuances%252C%2520as%2520well%2520as%2520a%2520fixed%2520temperature%2520strategy%250Afor%2520generation.%2520Our%2520proposed%2520approach%2520is%2520examined%2520to%2520determine%2520its%2520ability%2520to%250Agenerate%2520novel%2520ideas%2520from%2520an%2520initial%2520proposition.%2520Additionally%252C%2520a%2520Multi%2520Agent%250AMajority%2520Voting%2520%2528MAMV%2529%2520strategy%2520is%2520leveraged%2520to%2520assess%2520the%2520validity%2520and%2520novelty%250Aof%2520the%2520generated%2520ideas%252C%2520which%2520proves%2520beneficial%2520in%2520the%2520absence%2520of%2520domain%250Aexperts.%2520Our%2520experiments%2520show%2520promise%2520in%2520generating%2520new%2520ideas%2520and%2520provide%2520a%250Astepping%2520stone%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-reflecting%20Large%20Language%20Models%3A%20A%20Hegelian%20Dialectical%20Approach&entry.906535625=Sara%20Abdali%20and%20Can%20Goksen%20and%20Saeed%20Amizadeh%20and%20Kazuhito%20Koishida&entry.1292438233=%20%20Investigating%20NLP%20through%20a%20philosophical%20lens%20has%20recently%20caught%0Aresearcher%27s%20eyes%20as%20it%20connects%20computational%20methods%20with%20classical%20schools%0Aof%20philosophy.%20This%20paper%20introduces%20a%20philosophical%20approach%20inspired%20by%20the%0AHegelian%20Dialectic%20for%20LLMs%27%20self-reflection%2C%20utilizing%20a%20self-dialectical%0Aapproach%20to%20emulate%20internal%20critiques%20and%20then%20synthesize%20new%20ideas%20by%0Aresolving%20the%20contradicting%20points.%20Moreover%2C%20this%20paper%20investigates%20the%0Aeffect%20of%20LLMs%27%20temperature%20for%20generation%20by%20establishing%20a%20dynamic%20annealing%0Aapproach%2C%20which%20promotes%20the%20creativity%20in%20the%20early%20stages%20and%20gradually%0Arefines%20it%20by%20focusing%20on%20the%20nuances%2C%20as%20well%20as%20a%20fixed%20temperature%20strategy%0Afor%20generation.%20Our%20proposed%20approach%20is%20examined%20to%20determine%20its%20ability%20to%0Agenerate%20novel%20ideas%20from%20an%20initial%20proposition.%20Additionally%2C%20a%20Multi%20Agent%0AMajority%20Voting%20%28MAMV%29%20strategy%20is%20leveraged%20to%20assess%20the%20validity%20and%20novelty%0Aof%20the%20generated%20ideas%2C%20which%20proves%20beneficial%20in%20the%20absence%20of%20domain%0Aexperts.%20Our%20experiments%20show%20promise%20in%20generating%20new%20ideas%20and%20provide%20a%0Astepping%20stone%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14917v2&entry.124074799=Read"},
{"title": "Interpretability in Parameter Space: Minimizing Mechanistic Description\n  Length with Attribution-based Parameter Decomposition", "author": "Dan Braun and Lucius Bushnaq and Stefan Heimersheim and Jake Mendel and Lee Sharkey", "abstract": "  Mechanistic interpretability aims to understand the internal mechanisms\nlearned by neural networks. Despite recent progress toward this goal, it\nremains unclear how best to decompose neural network parameters into\nmechanistic components. We introduce Attribution-based Parameter Decomposition\n(APD), a method that directly decomposes a neural network's parameters into\ncomponents that (i) are faithful to the parameters of the original network,\n(ii) require a minimal number of components to process any input, and (iii) are\nmaximally simple. Our approach thus optimizes for a minimal length description\nof the network's mechanisms. We demonstrate APD's effectiveness by successfully\nidentifying ground truth mechanisms in multiple toy experimental settings:\nRecovering features from superposition; separating compressed computations; and\nidentifying cross-layer distributed representations. While challenges remain to\nscaling APD to non-toy models, our results suggest solutions to several open\nproblems in mechanistic interpretability, including identifying minimal\ncircuits in superposition, offering a conceptual foundation for 'features', and\nproviding an architecture-agnostic framework for neural network decomposition.\n", "link": "http://arxiv.org/abs/2501.14926v2", "date": "2025-01-28", "relevancy": 1.8721, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretability%20in%20Parameter%20Space%3A%20Minimizing%20Mechanistic%20Description%0A%20%20Length%20with%20Attribution-based%20Parameter%20Decomposition&body=Title%3A%20Interpretability%20in%20Parameter%20Space%3A%20Minimizing%20Mechanistic%20Description%0A%20%20Length%20with%20Attribution-based%20Parameter%20Decomposition%0AAuthor%3A%20Dan%20Braun%20and%20Lucius%20Bushnaq%20and%20Stefan%20Heimersheim%20and%20Jake%20Mendel%20and%20Lee%20Sharkey%0AAbstract%3A%20%20%20Mechanistic%20interpretability%20aims%20to%20understand%20the%20internal%20mechanisms%0Alearned%20by%20neural%20networks.%20Despite%20recent%20progress%20toward%20this%20goal%2C%20it%0Aremains%20unclear%20how%20best%20to%20decompose%20neural%20network%20parameters%20into%0Amechanistic%20components.%20We%20introduce%20Attribution-based%20Parameter%20Decomposition%0A%28APD%29%2C%20a%20method%20that%20directly%20decomposes%20a%20neural%20network%27s%20parameters%20into%0Acomponents%20that%20%28i%29%20are%20faithful%20to%20the%20parameters%20of%20the%20original%20network%2C%0A%28ii%29%20require%20a%20minimal%20number%20of%20components%20to%20process%20any%20input%2C%20and%20%28iii%29%20are%0Amaximally%20simple.%20Our%20approach%20thus%20optimizes%20for%20a%20minimal%20length%20description%0Aof%20the%20network%27s%20mechanisms.%20We%20demonstrate%20APD%27s%20effectiveness%20by%20successfully%0Aidentifying%20ground%20truth%20mechanisms%20in%20multiple%20toy%20experimental%20settings%3A%0ARecovering%20features%20from%20superposition%3B%20separating%20compressed%20computations%3B%20and%0Aidentifying%20cross-layer%20distributed%20representations.%20While%20challenges%20remain%20to%0Ascaling%20APD%20to%20non-toy%20models%2C%20our%20results%20suggest%20solutions%20to%20several%20open%0Aproblems%20in%20mechanistic%20interpretability%2C%20including%20identifying%20minimal%0Acircuits%20in%20superposition%2C%20offering%20a%20conceptual%20foundation%20for%20%27features%27%2C%20and%0Aproviding%20an%20architecture-agnostic%20framework%20for%20neural%20network%20decomposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretability%2520in%2520Parameter%2520Space%253A%2520Minimizing%2520Mechanistic%2520Description%250A%2520%2520Length%2520with%2520Attribution-based%2520Parameter%2520Decomposition%26entry.906535625%3DDan%2520Braun%2520and%2520Lucius%2520Bushnaq%2520and%2520Stefan%2520Heimersheim%2520and%2520Jake%2520Mendel%2520and%2520Lee%2520Sharkey%26entry.1292438233%3D%2520%2520Mechanistic%2520interpretability%2520aims%2520to%2520understand%2520the%2520internal%2520mechanisms%250Alearned%2520by%2520neural%2520networks.%2520Despite%2520recent%2520progress%2520toward%2520this%2520goal%252C%2520it%250Aremains%2520unclear%2520how%2520best%2520to%2520decompose%2520neural%2520network%2520parameters%2520into%250Amechanistic%2520components.%2520We%2520introduce%2520Attribution-based%2520Parameter%2520Decomposition%250A%2528APD%2529%252C%2520a%2520method%2520that%2520directly%2520decomposes%2520a%2520neural%2520network%2527s%2520parameters%2520into%250Acomponents%2520that%2520%2528i%2529%2520are%2520faithful%2520to%2520the%2520parameters%2520of%2520the%2520original%2520network%252C%250A%2528ii%2529%2520require%2520a%2520minimal%2520number%2520of%2520components%2520to%2520process%2520any%2520input%252C%2520and%2520%2528iii%2529%2520are%250Amaximally%2520simple.%2520Our%2520approach%2520thus%2520optimizes%2520for%2520a%2520minimal%2520length%2520description%250Aof%2520the%2520network%2527s%2520mechanisms.%2520We%2520demonstrate%2520APD%2527s%2520effectiveness%2520by%2520successfully%250Aidentifying%2520ground%2520truth%2520mechanisms%2520in%2520multiple%2520toy%2520experimental%2520settings%253A%250ARecovering%2520features%2520from%2520superposition%253B%2520separating%2520compressed%2520computations%253B%2520and%250Aidentifying%2520cross-layer%2520distributed%2520representations.%2520While%2520challenges%2520remain%2520to%250Ascaling%2520APD%2520to%2520non-toy%2520models%252C%2520our%2520results%2520suggest%2520solutions%2520to%2520several%2520open%250Aproblems%2520in%2520mechanistic%2520interpretability%252C%2520including%2520identifying%2520minimal%250Acircuits%2520in%2520superposition%252C%2520offering%2520a%2520conceptual%2520foundation%2520for%2520%2527features%2527%252C%2520and%250Aproviding%2520an%2520architecture-agnostic%2520framework%2520for%2520neural%2520network%2520decomposition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretability%20in%20Parameter%20Space%3A%20Minimizing%20Mechanistic%20Description%0A%20%20Length%20with%20Attribution-based%20Parameter%20Decomposition&entry.906535625=Dan%20Braun%20and%20Lucius%20Bushnaq%20and%20Stefan%20Heimersheim%20and%20Jake%20Mendel%20and%20Lee%20Sharkey&entry.1292438233=%20%20Mechanistic%20interpretability%20aims%20to%20understand%20the%20internal%20mechanisms%0Alearned%20by%20neural%20networks.%20Despite%20recent%20progress%20toward%20this%20goal%2C%20it%0Aremains%20unclear%20how%20best%20to%20decompose%20neural%20network%20parameters%20into%0Amechanistic%20components.%20We%20introduce%20Attribution-based%20Parameter%20Decomposition%0A%28APD%29%2C%20a%20method%20that%20directly%20decomposes%20a%20neural%20network%27s%20parameters%20into%0Acomponents%20that%20%28i%29%20are%20faithful%20to%20the%20parameters%20of%20the%20original%20network%2C%0A%28ii%29%20require%20a%20minimal%20number%20of%20components%20to%20process%20any%20input%2C%20and%20%28iii%29%20are%0Amaximally%20simple.%20Our%20approach%20thus%20optimizes%20for%20a%20minimal%20length%20description%0Aof%20the%20network%27s%20mechanisms.%20We%20demonstrate%20APD%27s%20effectiveness%20by%20successfully%0Aidentifying%20ground%20truth%20mechanisms%20in%20multiple%20toy%20experimental%20settings%3A%0ARecovering%20features%20from%20superposition%3B%20separating%20compressed%20computations%3B%20and%0Aidentifying%20cross-layer%20distributed%20representations.%20While%20challenges%20remain%20to%0Ascaling%20APD%20to%20non-toy%20models%2C%20our%20results%20suggest%20solutions%20to%20several%20open%0Aproblems%20in%20mechanistic%20interpretability%2C%20including%20identifying%20minimal%0Acircuits%20in%20superposition%2C%20offering%20a%20conceptual%20foundation%20for%20%27features%27%2C%20and%0Aproviding%20an%20architecture-agnostic%20framework%20for%20neural%20network%20decomposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14926v2&entry.124074799=Read"},
{"title": "Reinforcement Learning for Control of Non-Markovian Cellular Population\n  Dynamics", "author": "Josiah C. Kratz and Jacob Adamczyk", "abstract": "  Many organisms and cell types, from bacteria to cancer cells, exhibit a\nremarkable ability to adapt to fluctuating environments. Additionally, cells\ncan leverage a memory of past environments to better survive\npreviously-encountered stressors. From a control perspective, this adaptability\nposes significant challenges in driving cell populations toward extinction, and\nthus poses an open question with great clinical significance. In this work, we\nfocus on drug dosing in cell populations exhibiting phenotypic plasticity. For\nspecific dynamical models switching between resistant and susceptible states,\nexact solutions are known. However, when the underlying system parameters are\nunknown, and for complex memory-based systems, obtaining the optimal solution\nis currently intractable. To address this challenge, we apply reinforcement\nlearning (RL) to identify informed dosing strategies to control cell\npopulations evolving under novel non-Markovian dynamics. We find that\nmodel-free deep RL is able to recover exact solutions and control cell\npopulations even in the presence of long-range temporal dynamics. To further\ntest our approach in more realistic settings, we demonstrate robust RL-based\ncontrol strategies in environments with measurement noise and dynamic memory\nstrength.\n", "link": "http://arxiv.org/abs/2410.08439v3", "date": "2025-01-28", "relevancy": 1.8627, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4549}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20for%20Control%20of%20Non-Markovian%20Cellular%20Population%0A%20%20Dynamics&body=Title%3A%20Reinforcement%20Learning%20for%20Control%20of%20Non-Markovian%20Cellular%20Population%0A%20%20Dynamics%0AAuthor%3A%20Josiah%20C.%20Kratz%20and%20Jacob%20Adamczyk%0AAbstract%3A%20%20%20Many%20organisms%20and%20cell%20types%2C%20from%20bacteria%20to%20cancer%20cells%2C%20exhibit%20a%0Aremarkable%20ability%20to%20adapt%20to%20fluctuating%20environments.%20Additionally%2C%20cells%0Acan%20leverage%20a%20memory%20of%20past%20environments%20to%20better%20survive%0Apreviously-encountered%20stressors.%20From%20a%20control%20perspective%2C%20this%20adaptability%0Aposes%20significant%20challenges%20in%20driving%20cell%20populations%20toward%20extinction%2C%20and%0Athus%20poses%20an%20open%20question%20with%20great%20clinical%20significance.%20In%20this%20work%2C%20we%0Afocus%20on%20drug%20dosing%20in%20cell%20populations%20exhibiting%20phenotypic%20plasticity.%20For%0Aspecific%20dynamical%20models%20switching%20between%20resistant%20and%20susceptible%20states%2C%0Aexact%20solutions%20are%20known.%20However%2C%20when%20the%20underlying%20system%20parameters%20are%0Aunknown%2C%20and%20for%20complex%20memory-based%20systems%2C%20obtaining%20the%20optimal%20solution%0Ais%20currently%20intractable.%20To%20address%20this%20challenge%2C%20we%20apply%20reinforcement%0Alearning%20%28RL%29%20to%20identify%20informed%20dosing%20strategies%20to%20control%20cell%0Apopulations%20evolving%20under%20novel%20non-Markovian%20dynamics.%20We%20find%20that%0Amodel-free%20deep%20RL%20is%20able%20to%20recover%20exact%20solutions%20and%20control%20cell%0Apopulations%20even%20in%20the%20presence%20of%20long-range%20temporal%20dynamics.%20To%20further%0Atest%20our%20approach%20in%20more%20realistic%20settings%2C%20we%20demonstrate%20robust%20RL-based%0Acontrol%20strategies%20in%20environments%20with%20measurement%20noise%20and%20dynamic%20memory%0Astrength.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08439v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520for%2520Control%2520of%2520Non-Markovian%2520Cellular%2520Population%250A%2520%2520Dynamics%26entry.906535625%3DJosiah%2520C.%2520Kratz%2520and%2520Jacob%2520Adamczyk%26entry.1292438233%3D%2520%2520Many%2520organisms%2520and%2520cell%2520types%252C%2520from%2520bacteria%2520to%2520cancer%2520cells%252C%2520exhibit%2520a%250Aremarkable%2520ability%2520to%2520adapt%2520to%2520fluctuating%2520environments.%2520Additionally%252C%2520cells%250Acan%2520leverage%2520a%2520memory%2520of%2520past%2520environments%2520to%2520better%2520survive%250Apreviously-encountered%2520stressors.%2520From%2520a%2520control%2520perspective%252C%2520this%2520adaptability%250Aposes%2520significant%2520challenges%2520in%2520driving%2520cell%2520populations%2520toward%2520extinction%252C%2520and%250Athus%2520poses%2520an%2520open%2520question%2520with%2520great%2520clinical%2520significance.%2520In%2520this%2520work%252C%2520we%250Afocus%2520on%2520drug%2520dosing%2520in%2520cell%2520populations%2520exhibiting%2520phenotypic%2520plasticity.%2520For%250Aspecific%2520dynamical%2520models%2520switching%2520between%2520resistant%2520and%2520susceptible%2520states%252C%250Aexact%2520solutions%2520are%2520known.%2520However%252C%2520when%2520the%2520underlying%2520system%2520parameters%2520are%250Aunknown%252C%2520and%2520for%2520complex%2520memory-based%2520systems%252C%2520obtaining%2520the%2520optimal%2520solution%250Ais%2520currently%2520intractable.%2520To%2520address%2520this%2520challenge%252C%2520we%2520apply%2520reinforcement%250Alearning%2520%2528RL%2529%2520to%2520identify%2520informed%2520dosing%2520strategies%2520to%2520control%2520cell%250Apopulations%2520evolving%2520under%2520novel%2520non-Markovian%2520dynamics.%2520We%2520find%2520that%250Amodel-free%2520deep%2520RL%2520is%2520able%2520to%2520recover%2520exact%2520solutions%2520and%2520control%2520cell%250Apopulations%2520even%2520in%2520the%2520presence%2520of%2520long-range%2520temporal%2520dynamics.%2520To%2520further%250Atest%2520our%2520approach%2520in%2520more%2520realistic%2520settings%252C%2520we%2520demonstrate%2520robust%2520RL-based%250Acontrol%2520strategies%2520in%2520environments%2520with%2520measurement%2520noise%2520and%2520dynamic%2520memory%250Astrength.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08439v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20for%20Control%20of%20Non-Markovian%20Cellular%20Population%0A%20%20Dynamics&entry.906535625=Josiah%20C.%20Kratz%20and%20Jacob%20Adamczyk&entry.1292438233=%20%20Many%20organisms%20and%20cell%20types%2C%20from%20bacteria%20to%20cancer%20cells%2C%20exhibit%20a%0Aremarkable%20ability%20to%20adapt%20to%20fluctuating%20environments.%20Additionally%2C%20cells%0Acan%20leverage%20a%20memory%20of%20past%20environments%20to%20better%20survive%0Apreviously-encountered%20stressors.%20From%20a%20control%20perspective%2C%20this%20adaptability%0Aposes%20significant%20challenges%20in%20driving%20cell%20populations%20toward%20extinction%2C%20and%0Athus%20poses%20an%20open%20question%20with%20great%20clinical%20significance.%20In%20this%20work%2C%20we%0Afocus%20on%20drug%20dosing%20in%20cell%20populations%20exhibiting%20phenotypic%20plasticity.%20For%0Aspecific%20dynamical%20models%20switching%20between%20resistant%20and%20susceptible%20states%2C%0Aexact%20solutions%20are%20known.%20However%2C%20when%20the%20underlying%20system%20parameters%20are%0Aunknown%2C%20and%20for%20complex%20memory-based%20systems%2C%20obtaining%20the%20optimal%20solution%0Ais%20currently%20intractable.%20To%20address%20this%20challenge%2C%20we%20apply%20reinforcement%0Alearning%20%28RL%29%20to%20identify%20informed%20dosing%20strategies%20to%20control%20cell%0Apopulations%20evolving%20under%20novel%20non-Markovian%20dynamics.%20We%20find%20that%0Amodel-free%20deep%20RL%20is%20able%20to%20recover%20exact%20solutions%20and%20control%20cell%0Apopulations%20even%20in%20the%20presence%20of%20long-range%20temporal%20dynamics.%20To%20further%0Atest%20our%20approach%20in%20more%20realistic%20settings%2C%20we%20demonstrate%20robust%20RL-based%0Acontrol%20strategies%20in%20environments%20with%20measurement%20noise%20and%20dynamic%20memory%0Astrength.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08439v3&entry.124074799=Read"},
{"title": "RAINER: A Robust Ensemble Learning Grid Search-Tuned Framework for\n  Rainfall Patterns Prediction", "author": "Zhenqi Li and Junhao Zhong and Hewei Wang and Jinfeng Xu and Yijie Li and Jinjiang You and Jiayi Zhang and Runzhi Wu and Soumyabrata Dev", "abstract": "  Rainfall prediction remains a persistent challenge due to the highly\nnonlinear and complex nature of meteorological data. Existing approaches lack\nsystematic utilization of grid search for optimal hyperparameter tuning,\nrelying instead on heuristic or manual selection, frequently resulting in\nsub-optimal results. Additionally, these methods rarely incorporate newly\nconstructed meteorological features such as differences between temperature and\nhumidity to capture critical weather dynamics. Furthermore, there is a lack of\nsystematic evaluation of ensemble learning techniques and limited exploration\nof diverse advanced models introduced in the past one or two years. To address\nthese limitations, we propose a robust ensemble learning grid search-tuned\nframework (RAINER) for rainfall prediction. RAINER incorporates a comprehensive\nfeature engineering pipeline, including outlier removal, imputation of missing\nvalues, feature reconstruction, and dimensionality reduction via Principal\nComponent Analysis (PCA). The framework integrates novel meteorological\nfeatures to capture dynamic weather patterns and systematically evaluates\nnon-learning mathematical-based methods and a variety of machine learning\nmodels, from weak classifiers to advanced neural networks such as\nKolmogorov-Arnold Networks (KAN). By leveraging grid search for hyperparameter\ntuning and ensemble voting techniques, RAINER achieves promising results within\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2501.16900v1", "date": "2025-01-28", "relevancy": 1.8614, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAINER%3A%20A%20Robust%20Ensemble%20Learning%20Grid%20Search-Tuned%20Framework%20for%0A%20%20Rainfall%20Patterns%20Prediction&body=Title%3A%20RAINER%3A%20A%20Robust%20Ensemble%20Learning%20Grid%20Search-Tuned%20Framework%20for%0A%20%20Rainfall%20Patterns%20Prediction%0AAuthor%3A%20Zhenqi%20Li%20and%20Junhao%20Zhong%20and%20Hewei%20Wang%20and%20Jinfeng%20Xu%20and%20Yijie%20Li%20and%20Jinjiang%20You%20and%20Jiayi%20Zhang%20and%20Runzhi%20Wu%20and%20Soumyabrata%20Dev%0AAbstract%3A%20%20%20Rainfall%20prediction%20remains%20a%20persistent%20challenge%20due%20to%20the%20highly%0Anonlinear%20and%20complex%20nature%20of%20meteorological%20data.%20Existing%20approaches%20lack%0Asystematic%20utilization%20of%20grid%20search%20for%20optimal%20hyperparameter%20tuning%2C%0Arelying%20instead%20on%20heuristic%20or%20manual%20selection%2C%20frequently%20resulting%20in%0Asub-optimal%20results.%20Additionally%2C%20these%20methods%20rarely%20incorporate%20newly%0Aconstructed%20meteorological%20features%20such%20as%20differences%20between%20temperature%20and%0Ahumidity%20to%20capture%20critical%20weather%20dynamics.%20Furthermore%2C%20there%20is%20a%20lack%20of%0Asystematic%20evaluation%20of%20ensemble%20learning%20techniques%20and%20limited%20exploration%0Aof%20diverse%20advanced%20models%20introduced%20in%20the%20past%20one%20or%20two%20years.%20To%20address%0Athese%20limitations%2C%20we%20propose%20a%20robust%20ensemble%20learning%20grid%20search-tuned%0Aframework%20%28RAINER%29%20for%20rainfall%20prediction.%20RAINER%20incorporates%20a%20comprehensive%0Afeature%20engineering%20pipeline%2C%20including%20outlier%20removal%2C%20imputation%20of%20missing%0Avalues%2C%20feature%20reconstruction%2C%20and%20dimensionality%20reduction%20via%20Principal%0AComponent%20Analysis%20%28PCA%29.%20The%20framework%20integrates%20novel%20meteorological%0Afeatures%20to%20capture%20dynamic%20weather%20patterns%20and%20systematically%20evaluates%0Anon-learning%20mathematical-based%20methods%20and%20a%20variety%20of%20machine%20learning%0Amodels%2C%20from%20weak%20classifiers%20to%20advanced%20neural%20networks%20such%20as%0AKolmogorov-Arnold%20Networks%20%28KAN%29.%20By%20leveraging%20grid%20search%20for%20hyperparameter%0Atuning%20and%20ensemble%20voting%20techniques%2C%20RAINER%20achieves%20promising%20results%20within%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAINER%253A%2520A%2520Robust%2520Ensemble%2520Learning%2520Grid%2520Search-Tuned%2520Framework%2520for%250A%2520%2520Rainfall%2520Patterns%2520Prediction%26entry.906535625%3DZhenqi%2520Li%2520and%2520Junhao%2520Zhong%2520and%2520Hewei%2520Wang%2520and%2520Jinfeng%2520Xu%2520and%2520Yijie%2520Li%2520and%2520Jinjiang%2520You%2520and%2520Jiayi%2520Zhang%2520and%2520Runzhi%2520Wu%2520and%2520Soumyabrata%2520Dev%26entry.1292438233%3D%2520%2520Rainfall%2520prediction%2520remains%2520a%2520persistent%2520challenge%2520due%2520to%2520the%2520highly%250Anonlinear%2520and%2520complex%2520nature%2520of%2520meteorological%2520data.%2520Existing%2520approaches%2520lack%250Asystematic%2520utilization%2520of%2520grid%2520search%2520for%2520optimal%2520hyperparameter%2520tuning%252C%250Arelying%2520instead%2520on%2520heuristic%2520or%2520manual%2520selection%252C%2520frequently%2520resulting%2520in%250Asub-optimal%2520results.%2520Additionally%252C%2520these%2520methods%2520rarely%2520incorporate%2520newly%250Aconstructed%2520meteorological%2520features%2520such%2520as%2520differences%2520between%2520temperature%2520and%250Ahumidity%2520to%2520capture%2520critical%2520weather%2520dynamics.%2520Furthermore%252C%2520there%2520is%2520a%2520lack%2520of%250Asystematic%2520evaluation%2520of%2520ensemble%2520learning%2520techniques%2520and%2520limited%2520exploration%250Aof%2520diverse%2520advanced%2520models%2520introduced%2520in%2520the%2520past%2520one%2520or%2520two%2520years.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520a%2520robust%2520ensemble%2520learning%2520grid%2520search-tuned%250Aframework%2520%2528RAINER%2529%2520for%2520rainfall%2520prediction.%2520RAINER%2520incorporates%2520a%2520comprehensive%250Afeature%2520engineering%2520pipeline%252C%2520including%2520outlier%2520removal%252C%2520imputation%2520of%2520missing%250Avalues%252C%2520feature%2520reconstruction%252C%2520and%2520dimensionality%2520reduction%2520via%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529.%2520The%2520framework%2520integrates%2520novel%2520meteorological%250Afeatures%2520to%2520capture%2520dynamic%2520weather%2520patterns%2520and%2520systematically%2520evaluates%250Anon-learning%2520mathematical-based%2520methods%2520and%2520a%2520variety%2520of%2520machine%2520learning%250Amodels%252C%2520from%2520weak%2520classifiers%2520to%2520advanced%2520neural%2520networks%2520such%2520as%250AKolmogorov-Arnold%2520Networks%2520%2528KAN%2529.%2520By%2520leveraging%2520grid%2520search%2520for%2520hyperparameter%250Atuning%2520and%2520ensemble%2520voting%2520techniques%252C%2520RAINER%2520achieves%2520promising%2520results%2520within%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAINER%3A%20A%20Robust%20Ensemble%20Learning%20Grid%20Search-Tuned%20Framework%20for%0A%20%20Rainfall%20Patterns%20Prediction&entry.906535625=Zhenqi%20Li%20and%20Junhao%20Zhong%20and%20Hewei%20Wang%20and%20Jinfeng%20Xu%20and%20Yijie%20Li%20and%20Jinjiang%20You%20and%20Jiayi%20Zhang%20and%20Runzhi%20Wu%20and%20Soumyabrata%20Dev&entry.1292438233=%20%20Rainfall%20prediction%20remains%20a%20persistent%20challenge%20due%20to%20the%20highly%0Anonlinear%20and%20complex%20nature%20of%20meteorological%20data.%20Existing%20approaches%20lack%0Asystematic%20utilization%20of%20grid%20search%20for%20optimal%20hyperparameter%20tuning%2C%0Arelying%20instead%20on%20heuristic%20or%20manual%20selection%2C%20frequently%20resulting%20in%0Asub-optimal%20results.%20Additionally%2C%20these%20methods%20rarely%20incorporate%20newly%0Aconstructed%20meteorological%20features%20such%20as%20differences%20between%20temperature%20and%0Ahumidity%20to%20capture%20critical%20weather%20dynamics.%20Furthermore%2C%20there%20is%20a%20lack%20of%0Asystematic%20evaluation%20of%20ensemble%20learning%20techniques%20and%20limited%20exploration%0Aof%20diverse%20advanced%20models%20introduced%20in%20the%20past%20one%20or%20two%20years.%20To%20address%0Athese%20limitations%2C%20we%20propose%20a%20robust%20ensemble%20learning%20grid%20search-tuned%0Aframework%20%28RAINER%29%20for%20rainfall%20prediction.%20RAINER%20incorporates%20a%20comprehensive%0Afeature%20engineering%20pipeline%2C%20including%20outlier%20removal%2C%20imputation%20of%20missing%0Avalues%2C%20feature%20reconstruction%2C%20and%20dimensionality%20reduction%20via%20Principal%0AComponent%20Analysis%20%28PCA%29.%20The%20framework%20integrates%20novel%20meteorological%0Afeatures%20to%20capture%20dynamic%20weather%20patterns%20and%20systematically%20evaluates%0Anon-learning%20mathematical-based%20methods%20and%20a%20variety%20of%20machine%20learning%0Amodels%2C%20from%20weak%20classifiers%20to%20advanced%20neural%20networks%20such%20as%0AKolmogorov-Arnold%20Networks%20%28KAN%29.%20By%20leveraging%20grid%20search%20for%20hyperparameter%0Atuning%20and%20ensemble%20voting%20techniques%2C%20RAINER%20achieves%20promising%20results%20within%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16900v1&entry.124074799=Read"},
{"title": "Exact Computation of Any-Order Shapley Interactions for Graph Neural\n  Networks", "author": "Fabian Fumagalli and Maximilian Muschalik and Paolo Frazzetto and Janine Strotherm and Luca Hermes and Alessandro Sperduti and Eyke H\u00fcllermeier and Barbara Hammer", "abstract": "  Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning\n(ML) prediction tasks involving graph-structured data, their interpretability\nremains challenging. In explainable artificial intelligence (XAI), the Shapley\nValue (SV) is the predominant method to quantify contributions of individual\nfeatures to a ML model's output. Addressing the limitations of SVs in complex\nprediction models, Shapley Interactions (SIs) extend the SV to groups of\nfeatures. In this work, we explain single graph predictions of GNNs with SIs\nthat quantify node contributions and interactions among multiple nodes. By\nexploiting the GNN architecture, we show that the structure of interactions in\nnode embeddings are preserved for graph prediction. As a result, the\nexponential complexity of SIs depends only on the receptive fields, i.e. the\nmessage-passing ranges determined by the connectivity of the graph and the\nnumber of convolutional layers. Based on our theoretical results, we introduce\nGraphSHAP-IQ, an efficient approach to compute any-order SIs exactly.\nGraphSHAP-IQ is applicable to popular message passing techniques in conjunction\nwith a linear global pooling and output layer. We showcase that GraphSHAP-IQ\nsubstantially reduces the exponential complexity of computing exact SIs on\nmultiple benchmark datasets. Beyond exact computation, we evaluate\nGraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare\nwith existing baselines. Lastly, we visualize SIs of real-world water\ndistribution networks and molecule structures using a SI-Graph.\n", "link": "http://arxiv.org/abs/2501.16944v1", "date": "2025-01-28", "relevancy": 1.859, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4699}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4616}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Computation%20of%20Any-Order%20Shapley%20Interactions%20for%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Exact%20Computation%20of%20Any-Order%20Shapley%20Interactions%20for%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Fabian%20Fumagalli%20and%20Maximilian%20Muschalik%20and%20Paolo%20Frazzetto%20and%20Janine%20Strotherm%20and%20Luca%20Hermes%20and%20Alessandro%20Sperduti%20and%20Eyke%20H%C3%BCllermeier%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Albeit%20the%20ubiquitous%20use%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20machine%20learning%0A%28ML%29%20prediction%20tasks%20involving%20graph-structured%20data%2C%20their%20interpretability%0Aremains%20challenging.%20In%20explainable%20artificial%20intelligence%20%28XAI%29%2C%20the%20Shapley%0AValue%20%28SV%29%20is%20the%20predominant%20method%20to%20quantify%20contributions%20of%20individual%0Afeatures%20to%20a%20ML%20model%27s%20output.%20Addressing%20the%20limitations%20of%20SVs%20in%20complex%0Aprediction%20models%2C%20Shapley%20Interactions%20%28SIs%29%20extend%20the%20SV%20to%20groups%20of%0Afeatures.%20In%20this%20work%2C%20we%20explain%20single%20graph%20predictions%20of%20GNNs%20with%20SIs%0Athat%20quantify%20node%20contributions%20and%20interactions%20among%20multiple%20nodes.%20By%0Aexploiting%20the%20GNN%20architecture%2C%20we%20show%20that%20the%20structure%20of%20interactions%20in%0Anode%20embeddings%20are%20preserved%20for%20graph%20prediction.%20As%20a%20result%2C%20the%0Aexponential%20complexity%20of%20SIs%20depends%20only%20on%20the%20receptive%20fields%2C%20i.e.%20the%0Amessage-passing%20ranges%20determined%20by%20the%20connectivity%20of%20the%20graph%20and%20the%0Anumber%20of%20convolutional%20layers.%20Based%20on%20our%20theoretical%20results%2C%20we%20introduce%0AGraphSHAP-IQ%2C%20an%20efficient%20approach%20to%20compute%20any-order%20SIs%20exactly.%0AGraphSHAP-IQ%20is%20applicable%20to%20popular%20message%20passing%20techniques%20in%20conjunction%0Awith%20a%20linear%20global%20pooling%20and%20output%20layer.%20We%20showcase%20that%20GraphSHAP-IQ%0Asubstantially%20reduces%20the%20exponential%20complexity%20of%20computing%20exact%20SIs%20on%0Amultiple%20benchmark%20datasets.%20Beyond%20exact%20computation%2C%20we%20evaluate%0AGraphSHAP-IQ%27s%20approximation%20of%20SIs%20on%20popular%20GNN%20architectures%20and%20compare%0Awith%20existing%20baselines.%20Lastly%2C%20we%20visualize%20SIs%20of%20real-world%20water%0Adistribution%20networks%20and%20molecule%20structures%20using%20a%20SI-Graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Computation%2520of%2520Any-Order%2520Shapley%2520Interactions%2520for%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DFabian%2520Fumagalli%2520and%2520Maximilian%2520Muschalik%2520and%2520Paolo%2520Frazzetto%2520and%2520Janine%2520Strotherm%2520and%2520Luca%2520Hermes%2520and%2520Alessandro%2520Sperduti%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Albeit%2520the%2520ubiquitous%2520use%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520in%2520machine%2520learning%250A%2528ML%2529%2520prediction%2520tasks%2520involving%2520graph-structured%2520data%252C%2520their%2520interpretability%250Aremains%2520challenging.%2520In%2520explainable%2520artificial%2520intelligence%2520%2528XAI%2529%252C%2520the%2520Shapley%250AValue%2520%2528SV%2529%2520is%2520the%2520predominant%2520method%2520to%2520quantify%2520contributions%2520of%2520individual%250Afeatures%2520to%2520a%2520ML%2520model%2527s%2520output.%2520Addressing%2520the%2520limitations%2520of%2520SVs%2520in%2520complex%250Aprediction%2520models%252C%2520Shapley%2520Interactions%2520%2528SIs%2529%2520extend%2520the%2520SV%2520to%2520groups%2520of%250Afeatures.%2520In%2520this%2520work%252C%2520we%2520explain%2520single%2520graph%2520predictions%2520of%2520GNNs%2520with%2520SIs%250Athat%2520quantify%2520node%2520contributions%2520and%2520interactions%2520among%2520multiple%2520nodes.%2520By%250Aexploiting%2520the%2520GNN%2520architecture%252C%2520we%2520show%2520that%2520the%2520structure%2520of%2520interactions%2520in%250Anode%2520embeddings%2520are%2520preserved%2520for%2520graph%2520prediction.%2520As%2520a%2520result%252C%2520the%250Aexponential%2520complexity%2520of%2520SIs%2520depends%2520only%2520on%2520the%2520receptive%2520fields%252C%2520i.e.%2520the%250Amessage-passing%2520ranges%2520determined%2520by%2520the%2520connectivity%2520of%2520the%2520graph%2520and%2520the%250Anumber%2520of%2520convolutional%2520layers.%2520Based%2520on%2520our%2520theoretical%2520results%252C%2520we%2520introduce%250AGraphSHAP-IQ%252C%2520an%2520efficient%2520approach%2520to%2520compute%2520any-order%2520SIs%2520exactly.%250AGraphSHAP-IQ%2520is%2520applicable%2520to%2520popular%2520message%2520passing%2520techniques%2520in%2520conjunction%250Awith%2520a%2520linear%2520global%2520pooling%2520and%2520output%2520layer.%2520We%2520showcase%2520that%2520GraphSHAP-IQ%250Asubstantially%2520reduces%2520the%2520exponential%2520complexity%2520of%2520computing%2520exact%2520SIs%2520on%250Amultiple%2520benchmark%2520datasets.%2520Beyond%2520exact%2520computation%252C%2520we%2520evaluate%250AGraphSHAP-IQ%2527s%2520approximation%2520of%2520SIs%2520on%2520popular%2520GNN%2520architectures%2520and%2520compare%250Awith%2520existing%2520baselines.%2520Lastly%252C%2520we%2520visualize%2520SIs%2520of%2520real-world%2520water%250Adistribution%2520networks%2520and%2520molecule%2520structures%2520using%2520a%2520SI-Graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Computation%20of%20Any-Order%20Shapley%20Interactions%20for%20Graph%20Neural%0A%20%20Networks&entry.906535625=Fabian%20Fumagalli%20and%20Maximilian%20Muschalik%20and%20Paolo%20Frazzetto%20and%20Janine%20Strotherm%20and%20Luca%20Hermes%20and%20Alessandro%20Sperduti%20and%20Eyke%20H%C3%BCllermeier%20and%20Barbara%20Hammer&entry.1292438233=%20%20Albeit%20the%20ubiquitous%20use%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20machine%20learning%0A%28ML%29%20prediction%20tasks%20involving%20graph-structured%20data%2C%20their%20interpretability%0Aremains%20challenging.%20In%20explainable%20artificial%20intelligence%20%28XAI%29%2C%20the%20Shapley%0AValue%20%28SV%29%20is%20the%20predominant%20method%20to%20quantify%20contributions%20of%20individual%0Afeatures%20to%20a%20ML%20model%27s%20output.%20Addressing%20the%20limitations%20of%20SVs%20in%20complex%0Aprediction%20models%2C%20Shapley%20Interactions%20%28SIs%29%20extend%20the%20SV%20to%20groups%20of%0Afeatures.%20In%20this%20work%2C%20we%20explain%20single%20graph%20predictions%20of%20GNNs%20with%20SIs%0Athat%20quantify%20node%20contributions%20and%20interactions%20among%20multiple%20nodes.%20By%0Aexploiting%20the%20GNN%20architecture%2C%20we%20show%20that%20the%20structure%20of%20interactions%20in%0Anode%20embeddings%20are%20preserved%20for%20graph%20prediction.%20As%20a%20result%2C%20the%0Aexponential%20complexity%20of%20SIs%20depends%20only%20on%20the%20receptive%20fields%2C%20i.e.%20the%0Amessage-passing%20ranges%20determined%20by%20the%20connectivity%20of%20the%20graph%20and%20the%0Anumber%20of%20convolutional%20layers.%20Based%20on%20our%20theoretical%20results%2C%20we%20introduce%0AGraphSHAP-IQ%2C%20an%20efficient%20approach%20to%20compute%20any-order%20SIs%20exactly.%0AGraphSHAP-IQ%20is%20applicable%20to%20popular%20message%20passing%20techniques%20in%20conjunction%0Awith%20a%20linear%20global%20pooling%20and%20output%20layer.%20We%20showcase%20that%20GraphSHAP-IQ%0Asubstantially%20reduces%20the%20exponential%20complexity%20of%20computing%20exact%20SIs%20on%0Amultiple%20benchmark%20datasets.%20Beyond%20exact%20computation%2C%20we%20evaluate%0AGraphSHAP-IQ%27s%20approximation%20of%20SIs%20on%20popular%20GNN%20architectures%20and%20compare%0Awith%20existing%20baselines.%20Lastly%2C%20we%20visualize%20SIs%20of%20real-world%20water%0Adistribution%20networks%20and%20molecule%20structures%20using%20a%20SI-Graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16944v1&entry.124074799=Read"},
{"title": "Representation Learning with Parameterised Quantum Circuits for\n  Advancing Speech Emotion Recognition", "author": "Thejan Rajapakshe and Rajib Rana and Farina Riaz and Sara Khalifa and Bj\u00f6rn W. Schuller", "abstract": "  Speech Emotion Recognition (SER) is a complex and challenging task in\nhuman-computer interaction due to the intricate dependencies of features and\nthe overlapping nature of emotional expressions conveyed through speech.\nAlthough traditional deep learning methods have shown effectiveness, they often\nstruggle to capture subtle emotional variations and overlapping states. This\npaper introduces a hybrid classical-quantum framework that integrates\nParameterised Quantum Circuits (PQCs) with conventional Convolutional Neural\nNetwork (CNN) architectures. By leveraging quantum properties such as\nsuperposition and entanglement, the proposed model enhances feature\nrepresentation and captures complex dependencies more effectively than\nclassical methods. Experimental evaluations conducted on benchmark datasets,\nincluding IEMOCAP, RECOLA, and MSP-Improv, demonstrate that the hybrid model\nachieves higher accuracy in both binary and multi-class emotion classification\nwhile significantly reducing the number of trainable parameters. While a few\nexisting studies have explored the feasibility of using Quantum Circuits to\nreduce model complexity, none have successfully shown how they can enhance\naccuracy. This study is the first to demonstrate that Quantum Circuits has the\npotential to improve the accuracy of SER. The findings highlight the promise of\nQML to transform SER, suggesting a promising direction for future research and\npractical applications in emotion-aware systems.\n", "link": "http://arxiv.org/abs/2501.12050v2", "date": "2025-01-28", "relevancy": 1.8517, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20with%20Parameterised%20Quantum%20Circuits%20for%0A%20%20Advancing%20Speech%20Emotion%20Recognition&body=Title%3A%20Representation%20Learning%20with%20Parameterised%20Quantum%20Circuits%20for%0A%20%20Advancing%20Speech%20Emotion%20Recognition%0AAuthor%3A%20Thejan%20Rajapakshe%20and%20Rajib%20Rana%20and%20Farina%20Riaz%20and%20Sara%20Khalifa%20and%20Bj%C3%B6rn%20W.%20Schuller%0AAbstract%3A%20%20%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20a%20complex%20and%20challenging%20task%20in%0Ahuman-computer%20interaction%20due%20to%20the%20intricate%20dependencies%20of%20features%20and%0Athe%20overlapping%20nature%20of%20emotional%20expressions%20conveyed%20through%20speech.%0AAlthough%20traditional%20deep%20learning%20methods%20have%20shown%20effectiveness%2C%20they%20often%0Astruggle%20to%20capture%20subtle%20emotional%20variations%20and%20overlapping%20states.%20This%0Apaper%20introduces%20a%20hybrid%20classical-quantum%20framework%20that%20integrates%0AParameterised%20Quantum%20Circuits%20%28PQCs%29%20with%20conventional%20Convolutional%20Neural%0ANetwork%20%28CNN%29%20architectures.%20By%20leveraging%20quantum%20properties%20such%20as%0Asuperposition%20and%20entanglement%2C%20the%20proposed%20model%20enhances%20feature%0Arepresentation%20and%20captures%20complex%20dependencies%20more%20effectively%20than%0Aclassical%20methods.%20Experimental%20evaluations%20conducted%20on%20benchmark%20datasets%2C%0Aincluding%20IEMOCAP%2C%20RECOLA%2C%20and%20MSP-Improv%2C%20demonstrate%20that%20the%20hybrid%20model%0Aachieves%20higher%20accuracy%20in%20both%20binary%20and%20multi-class%20emotion%20classification%0Awhile%20significantly%20reducing%20the%20number%20of%20trainable%20parameters.%20While%20a%20few%0Aexisting%20studies%20have%20explored%20the%20feasibility%20of%20using%20Quantum%20Circuits%20to%0Areduce%20model%20complexity%2C%20none%20have%20successfully%20shown%20how%20they%20can%20enhance%0Aaccuracy.%20This%20study%20is%20the%20first%20to%20demonstrate%20that%20Quantum%20Circuits%20has%20the%0Apotential%20to%20improve%20the%20accuracy%20of%20SER.%20The%20findings%20highlight%20the%20promise%20of%0AQML%20to%20transform%20SER%2C%20suggesting%20a%20promising%20direction%20for%20future%20research%20and%0Apractical%20applications%20in%20emotion-aware%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520with%2520Parameterised%2520Quantum%2520Circuits%2520for%250A%2520%2520Advancing%2520Speech%2520Emotion%2520Recognition%26entry.906535625%3DThejan%2520Rajapakshe%2520and%2520Rajib%2520Rana%2520and%2520Farina%2520Riaz%2520and%2520Sara%2520Khalifa%2520and%2520Bj%25C3%25B6rn%2520W.%2520Schuller%26entry.1292438233%3D%2520%2520Speech%2520Emotion%2520Recognition%2520%2528SER%2529%2520is%2520a%2520complex%2520and%2520challenging%2520task%2520in%250Ahuman-computer%2520interaction%2520due%2520to%2520the%2520intricate%2520dependencies%2520of%2520features%2520and%250Athe%2520overlapping%2520nature%2520of%2520emotional%2520expressions%2520conveyed%2520through%2520speech.%250AAlthough%2520traditional%2520deep%2520learning%2520methods%2520have%2520shown%2520effectiveness%252C%2520they%2520often%250Astruggle%2520to%2520capture%2520subtle%2520emotional%2520variations%2520and%2520overlapping%2520states.%2520This%250Apaper%2520introduces%2520a%2520hybrid%2520classical-quantum%2520framework%2520that%2520integrates%250AParameterised%2520Quantum%2520Circuits%2520%2528PQCs%2529%2520with%2520conventional%2520Convolutional%2520Neural%250ANetwork%2520%2528CNN%2529%2520architectures.%2520By%2520leveraging%2520quantum%2520properties%2520such%2520as%250Asuperposition%2520and%2520entanglement%252C%2520the%2520proposed%2520model%2520enhances%2520feature%250Arepresentation%2520and%2520captures%2520complex%2520dependencies%2520more%2520effectively%2520than%250Aclassical%2520methods.%2520Experimental%2520evaluations%2520conducted%2520on%2520benchmark%2520datasets%252C%250Aincluding%2520IEMOCAP%252C%2520RECOLA%252C%2520and%2520MSP-Improv%252C%2520demonstrate%2520that%2520the%2520hybrid%2520model%250Aachieves%2520higher%2520accuracy%2520in%2520both%2520binary%2520and%2520multi-class%2520emotion%2520classification%250Awhile%2520significantly%2520reducing%2520the%2520number%2520of%2520trainable%2520parameters.%2520While%2520a%2520few%250Aexisting%2520studies%2520have%2520explored%2520the%2520feasibility%2520of%2520using%2520Quantum%2520Circuits%2520to%250Areduce%2520model%2520complexity%252C%2520none%2520have%2520successfully%2520shown%2520how%2520they%2520can%2520enhance%250Aaccuracy.%2520This%2520study%2520is%2520the%2520first%2520to%2520demonstrate%2520that%2520Quantum%2520Circuits%2520has%2520the%250Apotential%2520to%2520improve%2520the%2520accuracy%2520of%2520SER.%2520The%2520findings%2520highlight%2520the%2520promise%2520of%250AQML%2520to%2520transform%2520SER%252C%2520suggesting%2520a%2520promising%2520direction%2520for%2520future%2520research%2520and%250Apractical%2520applications%2520in%2520emotion-aware%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20with%20Parameterised%20Quantum%20Circuits%20for%0A%20%20Advancing%20Speech%20Emotion%20Recognition&entry.906535625=Thejan%20Rajapakshe%20and%20Rajib%20Rana%20and%20Farina%20Riaz%20and%20Sara%20Khalifa%20and%20Bj%C3%B6rn%20W.%20Schuller&entry.1292438233=%20%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20a%20complex%20and%20challenging%20task%20in%0Ahuman-computer%20interaction%20due%20to%20the%20intricate%20dependencies%20of%20features%20and%0Athe%20overlapping%20nature%20of%20emotional%20expressions%20conveyed%20through%20speech.%0AAlthough%20traditional%20deep%20learning%20methods%20have%20shown%20effectiveness%2C%20they%20often%0Astruggle%20to%20capture%20subtle%20emotional%20variations%20and%20overlapping%20states.%20This%0Apaper%20introduces%20a%20hybrid%20classical-quantum%20framework%20that%20integrates%0AParameterised%20Quantum%20Circuits%20%28PQCs%29%20with%20conventional%20Convolutional%20Neural%0ANetwork%20%28CNN%29%20architectures.%20By%20leveraging%20quantum%20properties%20such%20as%0Asuperposition%20and%20entanglement%2C%20the%20proposed%20model%20enhances%20feature%0Arepresentation%20and%20captures%20complex%20dependencies%20more%20effectively%20than%0Aclassical%20methods.%20Experimental%20evaluations%20conducted%20on%20benchmark%20datasets%2C%0Aincluding%20IEMOCAP%2C%20RECOLA%2C%20and%20MSP-Improv%2C%20demonstrate%20that%20the%20hybrid%20model%0Aachieves%20higher%20accuracy%20in%20both%20binary%20and%20multi-class%20emotion%20classification%0Awhile%20significantly%20reducing%20the%20number%20of%20trainable%20parameters.%20While%20a%20few%0Aexisting%20studies%20have%20explored%20the%20feasibility%20of%20using%20Quantum%20Circuits%20to%0Areduce%20model%20complexity%2C%20none%20have%20successfully%20shown%20how%20they%20can%20enhance%0Aaccuracy.%20This%20study%20is%20the%20first%20to%20demonstrate%20that%20Quantum%20Circuits%20has%20the%0Apotential%20to%20improve%20the%20accuracy%20of%20SER.%20The%20findings%20highlight%20the%20promise%20of%0AQML%20to%20transform%20SER%2C%20suggesting%20a%20promising%20direction%20for%20future%20research%20and%0Apractical%20applications%20in%20emotion-aware%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12050v2&entry.124074799=Read"},
{"title": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting", "author": "Fuqiang Liu and Sicong Jiang and Luis Miranda-Moreno and Seongjin Choi and Lijun Sun", "abstract": "  Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications.\n", "link": "http://arxiv.org/abs/2412.08099v3", "date": "2025-01-28", "relevancy": 1.8437, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4704}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4546}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Vulnerabilities%20in%20Large%20Language%20Models%20for%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Adversarial%20Vulnerabilities%20in%20Large%20Language%20Models%20for%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Fuqiang%20Liu%20and%20Sicong%20Jiang%20and%20Luis%20Miranda-Moreno%20and%20Seongjin%20Choi%20and%20Lijun%20Sun%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20demonstrated%20significant%20potential%0Ain%20the%20field%20of%20time%20series%20forecasting%2C%20offering%20impressive%20capabilities%20in%0Ahandling%20complex%20temporal%20data.%20However%2C%20their%20robustness%20and%20reliability%20in%0Areal-world%20applications%20remain%20under-explored%2C%20particularly%20concerning%20their%0Asusceptibility%20to%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20introduce%20a%20targeted%0Aadversarial%20attack%20framework%20for%20LLM-based%20time%20series%20forecasting.%20By%0Aemploying%20both%20gradient-free%20and%20black-box%20optimization%20methods%2C%20we%20generate%0Aminimal%20yet%20highly%20effective%20perturbations%20that%20significantly%20degrade%20the%0Aforecasting%20accuracy%20across%20multiple%20datasets%20and%20LLM%20architectures.%20Our%0Aexperiments%2C%20which%20include%20models%20like%20TimeGPT%20and%20LLM-Time%20with%20GPT-3.5%2C%0AGPT-4%2C%20LLaMa%2C%20and%20Mistral%2C%20show%20that%20adversarial%20attacks%20lead%20to%20much%20more%0Asevere%20performance%20degradation%20than%20random%20noise%2C%20and%20demonstrate%20the%20broad%0Aeffectiveness%20of%20our%20attacks%20across%20different%20LLMs.%20The%20results%20underscore%20the%0Acritical%20vulnerabilities%20of%20LLMs%20in%20time%20series%20forecasting%2C%20highlighting%20the%0Aneed%20for%20robust%20defense%20mechanisms%20to%20ensure%20their%20reliable%20deployment%20in%0Apractical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08099v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Vulnerabilities%2520in%2520Large%2520Language%2520Models%2520for%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DFuqiang%2520Liu%2520and%2520Sicong%2520Jiang%2520and%2520Luis%2520Miranda-Moreno%2520and%2520Seongjin%2520Choi%2520and%2520Lijun%2520Sun%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520significant%2520potential%250Ain%2520the%2520field%2520of%2520time%2520series%2520forecasting%252C%2520offering%2520impressive%2520capabilities%2520in%250Ahandling%2520complex%2520temporal%2520data.%2520However%252C%2520their%2520robustness%2520and%2520reliability%2520in%250Areal-world%2520applications%2520remain%2520under-explored%252C%2520particularly%2520concerning%2520their%250Asusceptibility%2520to%2520adversarial%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520targeted%250Aadversarial%2520attack%2520framework%2520for%2520LLM-based%2520time%2520series%2520forecasting.%2520By%250Aemploying%2520both%2520gradient-free%2520and%2520black-box%2520optimization%2520methods%252C%2520we%2520generate%250Aminimal%2520yet%2520highly%2520effective%2520perturbations%2520that%2520significantly%2520degrade%2520the%250Aforecasting%2520accuracy%2520across%2520multiple%2520datasets%2520and%2520LLM%2520architectures.%2520Our%250Aexperiments%252C%2520which%2520include%2520models%2520like%2520TimeGPT%2520and%2520LLM-Time%2520with%2520GPT-3.5%252C%250AGPT-4%252C%2520LLaMa%252C%2520and%2520Mistral%252C%2520show%2520that%2520adversarial%2520attacks%2520lead%2520to%2520much%2520more%250Asevere%2520performance%2520degradation%2520than%2520random%2520noise%252C%2520and%2520demonstrate%2520the%2520broad%250Aeffectiveness%2520of%2520our%2520attacks%2520across%2520different%2520LLMs.%2520The%2520results%2520underscore%2520the%250Acritical%2520vulnerabilities%2520of%2520LLMs%2520in%2520time%2520series%2520forecasting%252C%2520highlighting%2520the%250Aneed%2520for%2520robust%2520defense%2520mechanisms%2520to%2520ensure%2520their%2520reliable%2520deployment%2520in%250Apractical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08099v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Vulnerabilities%20in%20Large%20Language%20Models%20for%20Time%20Series%0A%20%20Forecasting&entry.906535625=Fuqiang%20Liu%20and%20Sicong%20Jiang%20and%20Luis%20Miranda-Moreno%20and%20Seongjin%20Choi%20and%20Lijun%20Sun&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20demonstrated%20significant%20potential%0Ain%20the%20field%20of%20time%20series%20forecasting%2C%20offering%20impressive%20capabilities%20in%0Ahandling%20complex%20temporal%20data.%20However%2C%20their%20robustness%20and%20reliability%20in%0Areal-world%20applications%20remain%20under-explored%2C%20particularly%20concerning%20their%0Asusceptibility%20to%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20introduce%20a%20targeted%0Aadversarial%20attack%20framework%20for%20LLM-based%20time%20series%20forecasting.%20By%0Aemploying%20both%20gradient-free%20and%20black-box%20optimization%20methods%2C%20we%20generate%0Aminimal%20yet%20highly%20effective%20perturbations%20that%20significantly%20degrade%20the%0Aforecasting%20accuracy%20across%20multiple%20datasets%20and%20LLM%20architectures.%20Our%0Aexperiments%2C%20which%20include%20models%20like%20TimeGPT%20and%20LLM-Time%20with%20GPT-3.5%2C%0AGPT-4%2C%20LLaMa%2C%20and%20Mistral%2C%20show%20that%20adversarial%20attacks%20lead%20to%20much%20more%0Asevere%20performance%20degradation%20than%20random%20noise%2C%20and%20demonstrate%20the%20broad%0Aeffectiveness%20of%20our%20attacks%20across%20different%20LLMs.%20The%20results%20underscore%20the%0Acritical%20vulnerabilities%20of%20LLMs%20in%20time%20series%20forecasting%2C%20highlighting%20the%0Aneed%20for%20robust%20defense%20mechanisms%20to%20ensure%20their%20reliable%20deployment%20in%0Apractical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08099v3&entry.124074799=Read"},
{"title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability\n  of Correct Reasoning", "author": "Zeyu Gan and Yun Liao and Yong Liu", "abstract": "  Test-time scaling, which is also often referred to as slow-thinking, has been\ndemonstrated to enhance multi-step reasoning in large language models (LLMs).\nHowever, despite its widespread utilization, the mechanisms underlying\nslow-thinking methods remain poorly understood. This paper explores the\nmechanisms of external slow-thinking from a theoretical standpoint. We begin by\nexamining the snowball error effect within the LLM reasoning process and\nconnect it to the likelihood of correct reasoning using information theory.\nBuilding on this, we show that external slow-thinking methods can be\ninterpreted as strategies to mitigate the error probability. We further provide\na comparative analysis of popular external slow-thinking approaches, ranging\nfrom simple to complex, highlighting their differences and interrelationships.\nOur findings suggest that the efficacy of these methods is not primarily\ndetermined by the specific framework employed, and that expanding the search\nscope or the model's internal reasoning capacity may yield more sustained\nimprovements in the long term. We open-source our code at\nhttps://github.com/ZyGan1999/Snowball-Errors-and-Probability.\n", "link": "http://arxiv.org/abs/2501.15602v2", "date": "2025-01-28", "relevancy": 1.8401, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4598}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20External%20Slow-Thinking%3A%20From%20Snowball%20Errors%20to%20Probability%0A%20%20of%20Correct%20Reasoning&body=Title%3A%20Rethinking%20External%20Slow-Thinking%3A%20From%20Snowball%20Errors%20to%20Probability%0A%20%20of%20Correct%20Reasoning%0AAuthor%3A%20Zeyu%20Gan%20and%20Yun%20Liao%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Test-time%20scaling%2C%20which%20is%20also%20often%20referred%20to%20as%20slow-thinking%2C%20has%20been%0Ademonstrated%20to%20enhance%20multi-step%20reasoning%20in%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20despite%20its%20widespread%20utilization%2C%20the%20mechanisms%20underlying%0Aslow-thinking%20methods%20remain%20poorly%20understood.%20This%20paper%20explores%20the%0Amechanisms%20of%20external%20slow-thinking%20from%20a%20theoretical%20standpoint.%20We%20begin%20by%0Aexamining%20the%20snowball%20error%20effect%20within%20the%20LLM%20reasoning%20process%20and%0Aconnect%20it%20to%20the%20likelihood%20of%20correct%20reasoning%20using%20information%20theory.%0ABuilding%20on%20this%2C%20we%20show%20that%20external%20slow-thinking%20methods%20can%20be%0Ainterpreted%20as%20strategies%20to%20mitigate%20the%20error%20probability.%20We%20further%20provide%0Aa%20comparative%20analysis%20of%20popular%20external%20slow-thinking%20approaches%2C%20ranging%0Afrom%20simple%20to%20complex%2C%20highlighting%20their%20differences%20and%20interrelationships.%0AOur%20findings%20suggest%20that%20the%20efficacy%20of%20these%20methods%20is%20not%20primarily%0Adetermined%20by%20the%20specific%20framework%20employed%2C%20and%20that%20expanding%20the%20search%0Ascope%20or%20the%20model%27s%20internal%20reasoning%20capacity%20may%20yield%20more%20sustained%0Aimprovements%20in%20the%20long%20term.%20We%20open-source%20our%20code%20at%0Ahttps%3A//github.com/ZyGan1999/Snowball-Errors-and-Probability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520External%2520Slow-Thinking%253A%2520From%2520Snowball%2520Errors%2520to%2520Probability%250A%2520%2520of%2520Correct%2520Reasoning%26entry.906535625%3DZeyu%2520Gan%2520and%2520Yun%2520Liao%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Test-time%2520scaling%252C%2520which%2520is%2520also%2520often%2520referred%2520to%2520as%2520slow-thinking%252C%2520has%2520been%250Ademonstrated%2520to%2520enhance%2520multi-step%2520reasoning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%250AHowever%252C%2520despite%2520its%2520widespread%2520utilization%252C%2520the%2520mechanisms%2520underlying%250Aslow-thinking%2520methods%2520remain%2520poorly%2520understood.%2520This%2520paper%2520explores%2520the%250Amechanisms%2520of%2520external%2520slow-thinking%2520from%2520a%2520theoretical%2520standpoint.%2520We%2520begin%2520by%250Aexamining%2520the%2520snowball%2520error%2520effect%2520within%2520the%2520LLM%2520reasoning%2520process%2520and%250Aconnect%2520it%2520to%2520the%2520likelihood%2520of%2520correct%2520reasoning%2520using%2520information%2520theory.%250ABuilding%2520on%2520this%252C%2520we%2520show%2520that%2520external%2520slow-thinking%2520methods%2520can%2520be%250Ainterpreted%2520as%2520strategies%2520to%2520mitigate%2520the%2520error%2520probability.%2520We%2520further%2520provide%250Aa%2520comparative%2520analysis%2520of%2520popular%2520external%2520slow-thinking%2520approaches%252C%2520ranging%250Afrom%2520simple%2520to%2520complex%252C%2520highlighting%2520their%2520differences%2520and%2520interrelationships.%250AOur%2520findings%2520suggest%2520that%2520the%2520efficacy%2520of%2520these%2520methods%2520is%2520not%2520primarily%250Adetermined%2520by%2520the%2520specific%2520framework%2520employed%252C%2520and%2520that%2520expanding%2520the%2520search%250Ascope%2520or%2520the%2520model%2527s%2520internal%2520reasoning%2520capacity%2520may%2520yield%2520more%2520sustained%250Aimprovements%2520in%2520the%2520long%2520term.%2520We%2520open-source%2520our%2520code%2520at%250Ahttps%253A//github.com/ZyGan1999/Snowball-Errors-and-Probability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20External%20Slow-Thinking%3A%20From%20Snowball%20Errors%20to%20Probability%0A%20%20of%20Correct%20Reasoning&entry.906535625=Zeyu%20Gan%20and%20Yun%20Liao%20and%20Yong%20Liu&entry.1292438233=%20%20Test-time%20scaling%2C%20which%20is%20also%20often%20referred%20to%20as%20slow-thinking%2C%20has%20been%0Ademonstrated%20to%20enhance%20multi-step%20reasoning%20in%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20despite%20its%20widespread%20utilization%2C%20the%20mechanisms%20underlying%0Aslow-thinking%20methods%20remain%20poorly%20understood.%20This%20paper%20explores%20the%0Amechanisms%20of%20external%20slow-thinking%20from%20a%20theoretical%20standpoint.%20We%20begin%20by%0Aexamining%20the%20snowball%20error%20effect%20within%20the%20LLM%20reasoning%20process%20and%0Aconnect%20it%20to%20the%20likelihood%20of%20correct%20reasoning%20using%20information%20theory.%0ABuilding%20on%20this%2C%20we%20show%20that%20external%20slow-thinking%20methods%20can%20be%0Ainterpreted%20as%20strategies%20to%20mitigate%20the%20error%20probability.%20We%20further%20provide%0Aa%20comparative%20analysis%20of%20popular%20external%20slow-thinking%20approaches%2C%20ranging%0Afrom%20simple%20to%20complex%2C%20highlighting%20their%20differences%20and%20interrelationships.%0AOur%20findings%20suggest%20that%20the%20efficacy%20of%20these%20methods%20is%20not%20primarily%0Adetermined%20by%20the%20specific%20framework%20employed%2C%20and%20that%20expanding%20the%20search%0Ascope%20or%20the%20model%27s%20internal%20reasoning%20capacity%20may%20yield%20more%20sustained%0Aimprovements%20in%20the%20long%20term.%20We%20open-source%20our%20code%20at%0Ahttps%3A//github.com/ZyGan1999/Snowball-Errors-and-Probability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15602v2&entry.124074799=Read"},
{"title": "Steerable Conditional Diffusion for Out-of-Distribution Adaptation in\n  Medical Image Reconstruction", "author": "Riccardo Barbano and Alexander Denker and Hyungjin Chung and Tae Hoon Roh and Simon Arridge and Peter Maass and Bangti Jin and Jong Chul Ye", "abstract": "  Denoising diffusion models have emerged as the go-to generative framework for\nsolving inverse problems in imaging. A critical concern regarding these models\nis their performance on out-of-distribution tasks, which remains an\nunder-explored challenge. Using a diffusion model on an out-of-distribution\ndataset, realistic reconstructions can be generated, but with hallucinating\nimage features that are uniquely present in the training dataset. To address\nthis discrepancy during train-test time and improve reconstruction accuracy, we\nintroduce a novel sampling framework called Steerable Conditional Diffusion.\nSpecifically, this framework adapts the diffusion model, concurrently with\nimage reconstruction, based solely on the information provided by the available\nmeasurement. Utilising our proposed method, we achieve substantial enhancements\nin out-of-distribution performance across diverse imaging modalities, advancing\nthe robust deployment of denoising diffusion models in real-world applications.\n", "link": "http://arxiv.org/abs/2308.14409v3", "date": "2025-01-28", "relevancy": 1.8345, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6676}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5961}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steerable%20Conditional%20Diffusion%20for%20Out-of-Distribution%20Adaptation%20in%0A%20%20Medical%20Image%20Reconstruction&body=Title%3A%20Steerable%20Conditional%20Diffusion%20for%20Out-of-Distribution%20Adaptation%20in%0A%20%20Medical%20Image%20Reconstruction%0AAuthor%3A%20Riccardo%20Barbano%20and%20Alexander%20Denker%20and%20Hyungjin%20Chung%20and%20Tae%20Hoon%20Roh%20and%20Simon%20Arridge%20and%20Peter%20Maass%20and%20Bangti%20Jin%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20Denoising%20diffusion%20models%20have%20emerged%20as%20the%20go-to%20generative%20framework%20for%0Asolving%20inverse%20problems%20in%20imaging.%20A%20critical%20concern%20regarding%20these%20models%0Ais%20their%20performance%20on%20out-of-distribution%20tasks%2C%20which%20remains%20an%0Aunder-explored%20challenge.%20Using%20a%20diffusion%20model%20on%20an%20out-of-distribution%0Adataset%2C%20realistic%20reconstructions%20can%20be%20generated%2C%20but%20with%20hallucinating%0Aimage%20features%20that%20are%20uniquely%20present%20in%20the%20training%20dataset.%20To%20address%0Athis%20discrepancy%20during%20train-test%20time%20and%20improve%20reconstruction%20accuracy%2C%20we%0Aintroduce%20a%20novel%20sampling%20framework%20called%20Steerable%20Conditional%20Diffusion.%0ASpecifically%2C%20this%20framework%20adapts%20the%20diffusion%20model%2C%20concurrently%20with%0Aimage%20reconstruction%2C%20based%20solely%20on%20the%20information%20provided%20by%20the%20available%0Ameasurement.%20Utilising%20our%20proposed%20method%2C%20we%20achieve%20substantial%20enhancements%0Ain%20out-of-distribution%20performance%20across%20diverse%20imaging%20modalities%2C%20advancing%0Athe%20robust%20deployment%20of%20denoising%20diffusion%20models%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14409v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteerable%2520Conditional%2520Diffusion%2520for%2520Out-of-Distribution%2520Adaptation%2520in%250A%2520%2520Medical%2520Image%2520Reconstruction%26entry.906535625%3DRiccardo%2520Barbano%2520and%2520Alexander%2520Denker%2520and%2520Hyungjin%2520Chung%2520and%2520Tae%2520Hoon%2520Roh%2520and%2520Simon%2520Arridge%2520and%2520Peter%2520Maass%2520and%2520Bangti%2520Jin%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520Denoising%2520diffusion%2520models%2520have%2520emerged%2520as%2520the%2520go-to%2520generative%2520framework%2520for%250Asolving%2520inverse%2520problems%2520in%2520imaging.%2520A%2520critical%2520concern%2520regarding%2520these%2520models%250Ais%2520their%2520performance%2520on%2520out-of-distribution%2520tasks%252C%2520which%2520remains%2520an%250Aunder-explored%2520challenge.%2520Using%2520a%2520diffusion%2520model%2520on%2520an%2520out-of-distribution%250Adataset%252C%2520realistic%2520reconstructions%2520can%2520be%2520generated%252C%2520but%2520with%2520hallucinating%250Aimage%2520features%2520that%2520are%2520uniquely%2520present%2520in%2520the%2520training%2520dataset.%2520To%2520address%250Athis%2520discrepancy%2520during%2520train-test%2520time%2520and%2520improve%2520reconstruction%2520accuracy%252C%2520we%250Aintroduce%2520a%2520novel%2520sampling%2520framework%2520called%2520Steerable%2520Conditional%2520Diffusion.%250ASpecifically%252C%2520this%2520framework%2520adapts%2520the%2520diffusion%2520model%252C%2520concurrently%2520with%250Aimage%2520reconstruction%252C%2520based%2520solely%2520on%2520the%2520information%2520provided%2520by%2520the%2520available%250Ameasurement.%2520Utilising%2520our%2520proposed%2520method%252C%2520we%2520achieve%2520substantial%2520enhancements%250Ain%2520out-of-distribution%2520performance%2520across%2520diverse%2520imaging%2520modalities%252C%2520advancing%250Athe%2520robust%2520deployment%2520of%2520denoising%2520diffusion%2520models%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14409v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steerable%20Conditional%20Diffusion%20for%20Out-of-Distribution%20Adaptation%20in%0A%20%20Medical%20Image%20Reconstruction&entry.906535625=Riccardo%20Barbano%20and%20Alexander%20Denker%20and%20Hyungjin%20Chung%20and%20Tae%20Hoon%20Roh%20and%20Simon%20Arridge%20and%20Peter%20Maass%20and%20Bangti%20Jin%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20Denoising%20diffusion%20models%20have%20emerged%20as%20the%20go-to%20generative%20framework%20for%0Asolving%20inverse%20problems%20in%20imaging.%20A%20critical%20concern%20regarding%20these%20models%0Ais%20their%20performance%20on%20out-of-distribution%20tasks%2C%20which%20remains%20an%0Aunder-explored%20challenge.%20Using%20a%20diffusion%20model%20on%20an%20out-of-distribution%0Adataset%2C%20realistic%20reconstructions%20can%20be%20generated%2C%20but%20with%20hallucinating%0Aimage%20features%20that%20are%20uniquely%20present%20in%20the%20training%20dataset.%20To%20address%0Athis%20discrepancy%20during%20train-test%20time%20and%20improve%20reconstruction%20accuracy%2C%20we%0Aintroduce%20a%20novel%20sampling%20framework%20called%20Steerable%20Conditional%20Diffusion.%0ASpecifically%2C%20this%20framework%20adapts%20the%20diffusion%20model%2C%20concurrently%20with%0Aimage%20reconstruction%2C%20based%20solely%20on%20the%20information%20provided%20by%20the%20available%0Ameasurement.%20Utilising%20our%20proposed%20method%2C%20we%20achieve%20substantial%20enhancements%0Ain%20out-of-distribution%20performance%20across%20diverse%20imaging%20modalities%2C%20advancing%0Athe%20robust%20deployment%20of%20denoising%20diffusion%20models%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14409v3&entry.124074799=Read"},
{"title": "Learning Curves for Decision Making in Supervised Machine Learning: A\n  Survey", "author": "Felix Mohr and Jan N. van Rijn", "abstract": "  Learning curves are a concept from social sciences that has been adopted in\nthe context of machine learning to assess the performance of a learning\nalgorithm with respect to a certain resource, e.g., the number of training\nexamples or the number of training iterations. Learning curves have important\napplications in several machine learning contexts, most notably in data\nacquisition, early stopping of model training, and model selection. For\ninstance, learning curves can be used to model the performance of the\ncombination of an algorithm and its hyperparameter configuration, providing\ninsights into their potential suitability at an early stage and often\nexpediting the algorithm selection process. Various learning curve models have\nbeen proposed to use learning curves for decision making. Some of these models\nanswer the binary decision question of whether a given algorithm at a certain\nbudget will outperform a certain reference performance, whereas more complex\nmodels predict the entire learning curve of an algorithm. We contribute a\nframework that categorises learning curve approaches using three criteria: the\ndecision-making situation they address, the intrinsic learning curve question\nthey answer and the type of resources they use. We survey papers from the\nliterature and classify them into this framework.\n", "link": "http://arxiv.org/abs/2201.12150v2", "date": "2025-01-28", "relevancy": 1.818, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4645}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Curves%20for%20Decision%20Making%20in%20Supervised%20Machine%20Learning%3A%20A%0A%20%20Survey&body=Title%3A%20Learning%20Curves%20for%20Decision%20Making%20in%20Supervised%20Machine%20Learning%3A%20A%0A%20%20Survey%0AAuthor%3A%20Felix%20Mohr%20and%20Jan%20N.%20van%20Rijn%0AAbstract%3A%20%20%20Learning%20curves%20are%20a%20concept%20from%20social%20sciences%20that%20has%20been%20adopted%20in%0Athe%20context%20of%20machine%20learning%20to%20assess%20the%20performance%20of%20a%20learning%0Aalgorithm%20with%20respect%20to%20a%20certain%20resource%2C%20e.g.%2C%20the%20number%20of%20training%0Aexamples%20or%20the%20number%20of%20training%20iterations.%20Learning%20curves%20have%20important%0Aapplications%20in%20several%20machine%20learning%20contexts%2C%20most%20notably%20in%20data%0Aacquisition%2C%20early%20stopping%20of%20model%20training%2C%20and%20model%20selection.%20For%0Ainstance%2C%20learning%20curves%20can%20be%20used%20to%20model%20the%20performance%20of%20the%0Acombination%20of%20an%20algorithm%20and%20its%20hyperparameter%20configuration%2C%20providing%0Ainsights%20into%20their%20potential%20suitability%20at%20an%20early%20stage%20and%20often%0Aexpediting%20the%20algorithm%20selection%20process.%20Various%20learning%20curve%20models%20have%0Abeen%20proposed%20to%20use%20learning%20curves%20for%20decision%20making.%20Some%20of%20these%20models%0Aanswer%20the%20binary%20decision%20question%20of%20whether%20a%20given%20algorithm%20at%20a%20certain%0Abudget%20will%20outperform%20a%20certain%20reference%20performance%2C%20whereas%20more%20complex%0Amodels%20predict%20the%20entire%20learning%20curve%20of%20an%20algorithm.%20We%20contribute%20a%0Aframework%20that%20categorises%20learning%20curve%20approaches%20using%20three%20criteria%3A%20the%0Adecision-making%20situation%20they%20address%2C%20the%20intrinsic%20learning%20curve%20question%0Athey%20answer%20and%20the%20type%20of%20resources%20they%20use.%20We%20survey%20papers%20from%20the%0Aliterature%20and%20classify%20them%20into%20this%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.12150v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Curves%2520for%2520Decision%2520Making%2520in%2520Supervised%2520Machine%2520Learning%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DFelix%2520Mohr%2520and%2520Jan%2520N.%2520van%2520Rijn%26entry.1292438233%3D%2520%2520Learning%2520curves%2520are%2520a%2520concept%2520from%2520social%2520sciences%2520that%2520has%2520been%2520adopted%2520in%250Athe%2520context%2520of%2520machine%2520learning%2520to%2520assess%2520the%2520performance%2520of%2520a%2520learning%250Aalgorithm%2520with%2520respect%2520to%2520a%2520certain%2520resource%252C%2520e.g.%252C%2520the%2520number%2520of%2520training%250Aexamples%2520or%2520the%2520number%2520of%2520training%2520iterations.%2520Learning%2520curves%2520have%2520important%250Aapplications%2520in%2520several%2520machine%2520learning%2520contexts%252C%2520most%2520notably%2520in%2520data%250Aacquisition%252C%2520early%2520stopping%2520of%2520model%2520training%252C%2520and%2520model%2520selection.%2520For%250Ainstance%252C%2520learning%2520curves%2520can%2520be%2520used%2520to%2520model%2520the%2520performance%2520of%2520the%250Acombination%2520of%2520an%2520algorithm%2520and%2520its%2520hyperparameter%2520configuration%252C%2520providing%250Ainsights%2520into%2520their%2520potential%2520suitability%2520at%2520an%2520early%2520stage%2520and%2520often%250Aexpediting%2520the%2520algorithm%2520selection%2520process.%2520Various%2520learning%2520curve%2520models%2520have%250Abeen%2520proposed%2520to%2520use%2520learning%2520curves%2520for%2520decision%2520making.%2520Some%2520of%2520these%2520models%250Aanswer%2520the%2520binary%2520decision%2520question%2520of%2520whether%2520a%2520given%2520algorithm%2520at%2520a%2520certain%250Abudget%2520will%2520outperform%2520a%2520certain%2520reference%2520performance%252C%2520whereas%2520more%2520complex%250Amodels%2520predict%2520the%2520entire%2520learning%2520curve%2520of%2520an%2520algorithm.%2520We%2520contribute%2520a%250Aframework%2520that%2520categorises%2520learning%2520curve%2520approaches%2520using%2520three%2520criteria%253A%2520the%250Adecision-making%2520situation%2520they%2520address%252C%2520the%2520intrinsic%2520learning%2520curve%2520question%250Athey%2520answer%2520and%2520the%2520type%2520of%2520resources%2520they%2520use.%2520We%2520survey%2520papers%2520from%2520the%250Aliterature%2520and%2520classify%2520them%2520into%2520this%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.12150v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Curves%20for%20Decision%20Making%20in%20Supervised%20Machine%20Learning%3A%20A%0A%20%20Survey&entry.906535625=Felix%20Mohr%20and%20Jan%20N.%20van%20Rijn&entry.1292438233=%20%20Learning%20curves%20are%20a%20concept%20from%20social%20sciences%20that%20has%20been%20adopted%20in%0Athe%20context%20of%20machine%20learning%20to%20assess%20the%20performance%20of%20a%20learning%0Aalgorithm%20with%20respect%20to%20a%20certain%20resource%2C%20e.g.%2C%20the%20number%20of%20training%0Aexamples%20or%20the%20number%20of%20training%20iterations.%20Learning%20curves%20have%20important%0Aapplications%20in%20several%20machine%20learning%20contexts%2C%20most%20notably%20in%20data%0Aacquisition%2C%20early%20stopping%20of%20model%20training%2C%20and%20model%20selection.%20For%0Ainstance%2C%20learning%20curves%20can%20be%20used%20to%20model%20the%20performance%20of%20the%0Acombination%20of%20an%20algorithm%20and%20its%20hyperparameter%20configuration%2C%20providing%0Ainsights%20into%20their%20potential%20suitability%20at%20an%20early%20stage%20and%20often%0Aexpediting%20the%20algorithm%20selection%20process.%20Various%20learning%20curve%20models%20have%0Abeen%20proposed%20to%20use%20learning%20curves%20for%20decision%20making.%20Some%20of%20these%20models%0Aanswer%20the%20binary%20decision%20question%20of%20whether%20a%20given%20algorithm%20at%20a%20certain%0Abudget%20will%20outperform%20a%20certain%20reference%20performance%2C%20whereas%20more%20complex%0Amodels%20predict%20the%20entire%20learning%20curve%20of%20an%20algorithm.%20We%20contribute%20a%0Aframework%20that%20categorises%20learning%20curve%20approaches%20using%20three%20criteria%3A%20the%0Adecision-making%20situation%20they%20address%2C%20the%20intrinsic%20learning%20curve%20question%0Athey%20answer%20and%20the%20type%20of%20resources%20they%20use.%20We%20survey%20papers%20from%20the%0Aliterature%20and%20classify%20them%20into%20this%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.12150v2&entry.124074799=Read"},
{"title": "CantorNet: A Sandbox for Testing Geometrical and Topological Complexity\n  Measures", "author": "Michal Lewandowski and Hamid Eghbalzadeh and Bernhard A. Moser", "abstract": "  Many natural phenomena are characterized by self-similarity, for example the\nsymmetry of human faces, or a repetitive motif of a song. Studying of such\nsymmetries will allow us to gain deeper insights into the underlying mechanisms\nof complex systems. Recognizing the importance of understanding these patterns,\nwe propose a geometrically inspired framework to study such phenomena in\nartificial neural networks. To this end, we introduce \\emph{CantorNet},\ninspired by the triadic construction of the Cantor set, which was introduced by\nGeorg Cantor in the $19^\\text{th}$ century. In mathematics, the Cantor set is a\nset of points lying on a single line that is self-similar and has a counter\nintuitive property of being an uncountably infinite null set. Similarly, we\nintroduce CantorNet as a sandbox for studying self-similarity by means of novel\ntopological and geometrical complexity measures. CantorNet constitutes a family\nof ReLU neural networks that spans the whole spectrum of possible Kolmogorov\ncomplexities, including the two opposite descriptions (linear and exponential\nas measured by the description length). CantorNet's decision boundaries can be\narbitrarily ragged, yet are analytically known. Besides serving as a testing\nground for complexity measures, our work may serve to illustrate potential\npitfalls in geometry-ignorant data augmentation techniques and adversarial\nattacks.\n", "link": "http://arxiv.org/abs/2411.19713v3", "date": "2025-01-28", "relevancy": 1.809, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.459}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CantorNet%3A%20A%20Sandbox%20for%20Testing%20Geometrical%20and%20Topological%20Complexity%0A%20%20Measures&body=Title%3A%20CantorNet%3A%20A%20Sandbox%20for%20Testing%20Geometrical%20and%20Topological%20Complexity%0A%20%20Measures%0AAuthor%3A%20Michal%20Lewandowski%20and%20Hamid%20Eghbalzadeh%20and%20Bernhard%20A.%20Moser%0AAbstract%3A%20%20%20Many%20natural%20phenomena%20are%20characterized%20by%20self-similarity%2C%20for%20example%20the%0Asymmetry%20of%20human%20faces%2C%20or%20a%20repetitive%20motif%20of%20a%20song.%20Studying%20of%20such%0Asymmetries%20will%20allow%20us%20to%20gain%20deeper%20insights%20into%20the%20underlying%20mechanisms%0Aof%20complex%20systems.%20Recognizing%20the%20importance%20of%20understanding%20these%20patterns%2C%0Awe%20propose%20a%20geometrically%20inspired%20framework%20to%20study%20such%20phenomena%20in%0Aartificial%20neural%20networks.%20To%20this%20end%2C%20we%20introduce%20%5Cemph%7BCantorNet%7D%2C%0Ainspired%20by%20the%20triadic%20construction%20of%20the%20Cantor%20set%2C%20which%20was%20introduced%20by%0AGeorg%20Cantor%20in%20the%20%2419%5E%5Ctext%7Bth%7D%24%20century.%20In%20mathematics%2C%20the%20Cantor%20set%20is%20a%0Aset%20of%20points%20lying%20on%20a%20single%20line%20that%20is%20self-similar%20and%20has%20a%20counter%0Aintuitive%20property%20of%20being%20an%20uncountably%20infinite%20null%20set.%20Similarly%2C%20we%0Aintroduce%20CantorNet%20as%20a%20sandbox%20for%20studying%20self-similarity%20by%20means%20of%20novel%0Atopological%20and%20geometrical%20complexity%20measures.%20CantorNet%20constitutes%20a%20family%0Aof%20ReLU%20neural%20networks%20that%20spans%20the%20whole%20spectrum%20of%20possible%20Kolmogorov%0Acomplexities%2C%20including%20the%20two%20opposite%20descriptions%20%28linear%20and%20exponential%0Aas%20measured%20by%20the%20description%20length%29.%20CantorNet%27s%20decision%20boundaries%20can%20be%0Aarbitrarily%20ragged%2C%20yet%20are%20analytically%20known.%20Besides%20serving%20as%20a%20testing%0Aground%20for%20complexity%20measures%2C%20our%20work%20may%20serve%20to%20illustrate%20potential%0Apitfalls%20in%20geometry-ignorant%20data%20augmentation%20techniques%20and%20adversarial%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19713v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCantorNet%253A%2520A%2520Sandbox%2520for%2520Testing%2520Geometrical%2520and%2520Topological%2520Complexity%250A%2520%2520Measures%26entry.906535625%3DMichal%2520Lewandowski%2520and%2520Hamid%2520Eghbalzadeh%2520and%2520Bernhard%2520A.%2520Moser%26entry.1292438233%3D%2520%2520Many%2520natural%2520phenomena%2520are%2520characterized%2520by%2520self-similarity%252C%2520for%2520example%2520the%250Asymmetry%2520of%2520human%2520faces%252C%2520or%2520a%2520repetitive%2520motif%2520of%2520a%2520song.%2520Studying%2520of%2520such%250Asymmetries%2520will%2520allow%2520us%2520to%2520gain%2520deeper%2520insights%2520into%2520the%2520underlying%2520mechanisms%250Aof%2520complex%2520systems.%2520Recognizing%2520the%2520importance%2520of%2520understanding%2520these%2520patterns%252C%250Awe%2520propose%2520a%2520geometrically%2520inspired%2520framework%2520to%2520study%2520such%2520phenomena%2520in%250Aartificial%2520neural%2520networks.%2520To%2520this%2520end%252C%2520we%2520introduce%2520%255Cemph%257BCantorNet%257D%252C%250Ainspired%2520by%2520the%2520triadic%2520construction%2520of%2520the%2520Cantor%2520set%252C%2520which%2520was%2520introduced%2520by%250AGeorg%2520Cantor%2520in%2520the%2520%252419%255E%255Ctext%257Bth%257D%2524%2520century.%2520In%2520mathematics%252C%2520the%2520Cantor%2520set%2520is%2520a%250Aset%2520of%2520points%2520lying%2520on%2520a%2520single%2520line%2520that%2520is%2520self-similar%2520and%2520has%2520a%2520counter%250Aintuitive%2520property%2520of%2520being%2520an%2520uncountably%2520infinite%2520null%2520set.%2520Similarly%252C%2520we%250Aintroduce%2520CantorNet%2520as%2520a%2520sandbox%2520for%2520studying%2520self-similarity%2520by%2520means%2520of%2520novel%250Atopological%2520and%2520geometrical%2520complexity%2520measures.%2520CantorNet%2520constitutes%2520a%2520family%250Aof%2520ReLU%2520neural%2520networks%2520that%2520spans%2520the%2520whole%2520spectrum%2520of%2520possible%2520Kolmogorov%250Acomplexities%252C%2520including%2520the%2520two%2520opposite%2520descriptions%2520%2528linear%2520and%2520exponential%250Aas%2520measured%2520by%2520the%2520description%2520length%2529.%2520CantorNet%2527s%2520decision%2520boundaries%2520can%2520be%250Aarbitrarily%2520ragged%252C%2520yet%2520are%2520analytically%2520known.%2520Besides%2520serving%2520as%2520a%2520testing%250Aground%2520for%2520complexity%2520measures%252C%2520our%2520work%2520may%2520serve%2520to%2520illustrate%2520potential%250Apitfalls%2520in%2520geometry-ignorant%2520data%2520augmentation%2520techniques%2520and%2520adversarial%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19713v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CantorNet%3A%20A%20Sandbox%20for%20Testing%20Geometrical%20and%20Topological%20Complexity%0A%20%20Measures&entry.906535625=Michal%20Lewandowski%20and%20Hamid%20Eghbalzadeh%20and%20Bernhard%20A.%20Moser&entry.1292438233=%20%20Many%20natural%20phenomena%20are%20characterized%20by%20self-similarity%2C%20for%20example%20the%0Asymmetry%20of%20human%20faces%2C%20or%20a%20repetitive%20motif%20of%20a%20song.%20Studying%20of%20such%0Asymmetries%20will%20allow%20us%20to%20gain%20deeper%20insights%20into%20the%20underlying%20mechanisms%0Aof%20complex%20systems.%20Recognizing%20the%20importance%20of%20understanding%20these%20patterns%2C%0Awe%20propose%20a%20geometrically%20inspired%20framework%20to%20study%20such%20phenomena%20in%0Aartificial%20neural%20networks.%20To%20this%20end%2C%20we%20introduce%20%5Cemph%7BCantorNet%7D%2C%0Ainspired%20by%20the%20triadic%20construction%20of%20the%20Cantor%20set%2C%20which%20was%20introduced%20by%0AGeorg%20Cantor%20in%20the%20%2419%5E%5Ctext%7Bth%7D%24%20century.%20In%20mathematics%2C%20the%20Cantor%20set%20is%20a%0Aset%20of%20points%20lying%20on%20a%20single%20line%20that%20is%20self-similar%20and%20has%20a%20counter%0Aintuitive%20property%20of%20being%20an%20uncountably%20infinite%20null%20set.%20Similarly%2C%20we%0Aintroduce%20CantorNet%20as%20a%20sandbox%20for%20studying%20self-similarity%20by%20means%20of%20novel%0Atopological%20and%20geometrical%20complexity%20measures.%20CantorNet%20constitutes%20a%20family%0Aof%20ReLU%20neural%20networks%20that%20spans%20the%20whole%20spectrum%20of%20possible%20Kolmogorov%0Acomplexities%2C%20including%20the%20two%20opposite%20descriptions%20%28linear%20and%20exponential%0Aas%20measured%20by%20the%20description%20length%29.%20CantorNet%27s%20decision%20boundaries%20can%20be%0Aarbitrarily%20ragged%2C%20yet%20are%20analytically%20known.%20Besides%20serving%20as%20a%20testing%0Aground%20for%20complexity%20measures%2C%20our%20work%20may%20serve%20to%20illustrate%20potential%0Apitfalls%20in%20geometry-ignorant%20data%20augmentation%20techniques%20and%20adversarial%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19713v3&entry.124074799=Read"},
{"title": "Frequency Matters: Explaining Biases of Face Recognition in the\n  Frequency Domain", "author": "Marco Huber and Fadi Boutros and Naser Damer", "abstract": "  Face recognition (FR) models are vulnerable to performance variations across\ndemographic groups. The causes for these performance differences are unclear\ndue to the highly complex deep learning-based structure of face recognition\nmodels. Several works aimed at exploring possible roots of gender and ethnicity\nbias, identifying semantic reasons such as hairstyle, make-up, or facial hair\nas possible sources. Motivated by recent discoveries of the importance of\nfrequency patterns in convolutional neural networks, we explain bias in face\nrecognition using state-of-the-art frequency-based explanations. Our extensive\nresults show that different frequencies are important to FR models depending on\nthe ethnicity of the samples.\n", "link": "http://arxiv.org/abs/2501.16896v1", "date": "2025-01-28", "relevancy": 1.8035, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4735}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4433}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency%20Matters%3A%20Explaining%20Biases%20of%20Face%20Recognition%20in%20the%0A%20%20Frequency%20Domain&body=Title%3A%20Frequency%20Matters%3A%20Explaining%20Biases%20of%20Face%20Recognition%20in%20the%0A%20%20Frequency%20Domain%0AAuthor%3A%20Marco%20Huber%20and%20Fadi%20Boutros%20and%20Naser%20Damer%0AAbstract%3A%20%20%20Face%20recognition%20%28FR%29%20models%20are%20vulnerable%20to%20performance%20variations%20across%0Ademographic%20groups.%20The%20causes%20for%20these%20performance%20differences%20are%20unclear%0Adue%20to%20the%20highly%20complex%20deep%20learning-based%20structure%20of%20face%20recognition%0Amodels.%20Several%20works%20aimed%20at%20exploring%20possible%20roots%20of%20gender%20and%20ethnicity%0Abias%2C%20identifying%20semantic%20reasons%20such%20as%20hairstyle%2C%20make-up%2C%20or%20facial%20hair%0Aas%20possible%20sources.%20Motivated%20by%20recent%20discoveries%20of%20the%20importance%20of%0Afrequency%20patterns%20in%20convolutional%20neural%20networks%2C%20we%20explain%20bias%20in%20face%0Arecognition%20using%20state-of-the-art%20frequency-based%20explanations.%20Our%20extensive%0Aresults%20show%20that%20different%20frequencies%20are%20important%20to%20FR%20models%20depending%20on%0Athe%20ethnicity%20of%20the%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency%2520Matters%253A%2520Explaining%2520Biases%2520of%2520Face%2520Recognition%2520in%2520the%250A%2520%2520Frequency%2520Domain%26entry.906535625%3DMarco%2520Huber%2520and%2520Fadi%2520Boutros%2520and%2520Naser%2520Damer%26entry.1292438233%3D%2520%2520Face%2520recognition%2520%2528FR%2529%2520models%2520are%2520vulnerable%2520to%2520performance%2520variations%2520across%250Ademographic%2520groups.%2520The%2520causes%2520for%2520these%2520performance%2520differences%2520are%2520unclear%250Adue%2520to%2520the%2520highly%2520complex%2520deep%2520learning-based%2520structure%2520of%2520face%2520recognition%250Amodels.%2520Several%2520works%2520aimed%2520at%2520exploring%2520possible%2520roots%2520of%2520gender%2520and%2520ethnicity%250Abias%252C%2520identifying%2520semantic%2520reasons%2520such%2520as%2520hairstyle%252C%2520make-up%252C%2520or%2520facial%2520hair%250Aas%2520possible%2520sources.%2520Motivated%2520by%2520recent%2520discoveries%2520of%2520the%2520importance%2520of%250Afrequency%2520patterns%2520in%2520convolutional%2520neural%2520networks%252C%2520we%2520explain%2520bias%2520in%2520face%250Arecognition%2520using%2520state-of-the-art%2520frequency-based%2520explanations.%2520Our%2520extensive%250Aresults%2520show%2520that%2520different%2520frequencies%2520are%2520important%2520to%2520FR%2520models%2520depending%2520on%250Athe%2520ethnicity%2520of%2520the%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency%20Matters%3A%20Explaining%20Biases%20of%20Face%20Recognition%20in%20the%0A%20%20Frequency%20Domain&entry.906535625=Marco%20Huber%20and%20Fadi%20Boutros%20and%20Naser%20Damer&entry.1292438233=%20%20Face%20recognition%20%28FR%29%20models%20are%20vulnerable%20to%20performance%20variations%20across%0Ademographic%20groups.%20The%20causes%20for%20these%20performance%20differences%20are%20unclear%0Adue%20to%20the%20highly%20complex%20deep%20learning-based%20structure%20of%20face%20recognition%0Amodels.%20Several%20works%20aimed%20at%20exploring%20possible%20roots%20of%20gender%20and%20ethnicity%0Abias%2C%20identifying%20semantic%20reasons%20such%20as%20hairstyle%2C%20make-up%2C%20or%20facial%20hair%0Aas%20possible%20sources.%20Motivated%20by%20recent%20discoveries%20of%20the%20importance%20of%0Afrequency%20patterns%20in%20convolutional%20neural%20networks%2C%20we%20explain%20bias%20in%20face%0Arecognition%20using%20state-of-the-art%20frequency-based%20explanations.%20Our%20extensive%0Aresults%20show%20that%20different%20frequencies%20are%20important%20to%20FR%20models%20depending%20on%0Athe%20ethnicity%20of%20the%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16896v1&entry.124074799=Read"},
{"title": "Generalized Distribution Prediction for Asset Returns", "author": "\u00cdsak P\u00e9tursson and Mar\u00eda \u00d3skarsd\u00f3ttir", "abstract": "  We present a novel approach for predicting the distribution of asset returns\nusing a quantile-based method with Long Short-Term Memory (LSTM) networks. Our\nmodel is designed in two stages: the first focuses on predicting the quantiles\nof normalized asset returns using asset-specific features, while the second\nstage incorporates market data to adjust these predictions for broader economic\nconditions. This results in a generalized model that can be applied across\nvarious asset classes, including commodities, cryptocurrencies, as well as\nsynthetic datasets. The predicted quantiles are then converted into full\nprobability distributions through kernel density estimation, allowing for more\nprecise return distribution predictions and inferencing. The LSTM model\nsignificantly outperforms a linear quantile regression baseline by 98% and a\ndense neural network model by over 50%, showcasing its ability to capture\ncomplex patterns in financial return distributions across both synthetic and\nreal-world data. By using exclusively asset-class-neutral features, our model\nachieves robust, generalizable results.\n", "link": "http://arxiv.org/abs/2410.23296v2", "date": "2025-01-28", "relevancy": 1.7852, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.456}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Distribution%20Prediction%20for%20Asset%20Returns&body=Title%3A%20Generalized%20Distribution%20Prediction%20for%20Asset%20Returns%0AAuthor%3A%20%C3%8Dsak%20P%C3%A9tursson%20and%20Mar%C3%ADa%20%C3%93skarsd%C3%B3ttir%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20predicting%20the%20distribution%20of%20asset%20returns%0Ausing%20a%20quantile-based%20method%20with%20Long%20Short-Term%20Memory%20%28LSTM%29%20networks.%20Our%0Amodel%20is%20designed%20in%20two%20stages%3A%20the%20first%20focuses%20on%20predicting%20the%20quantiles%0Aof%20normalized%20asset%20returns%20using%20asset-specific%20features%2C%20while%20the%20second%0Astage%20incorporates%20market%20data%20to%20adjust%20these%20predictions%20for%20broader%20economic%0Aconditions.%20This%20results%20in%20a%20generalized%20model%20that%20can%20be%20applied%20across%0Avarious%20asset%20classes%2C%20including%20commodities%2C%20cryptocurrencies%2C%20as%20well%20as%0Asynthetic%20datasets.%20The%20predicted%20quantiles%20are%20then%20converted%20into%20full%0Aprobability%20distributions%20through%20kernel%20density%20estimation%2C%20allowing%20for%20more%0Aprecise%20return%20distribution%20predictions%20and%20inferencing.%20The%20LSTM%20model%0Asignificantly%20outperforms%20a%20linear%20quantile%20regression%20baseline%20by%2098%25%20and%20a%0Adense%20neural%20network%20model%20by%20over%2050%25%2C%20showcasing%20its%20ability%20to%20capture%0Acomplex%20patterns%20in%20financial%20return%20distributions%20across%20both%20synthetic%20and%0Areal-world%20data.%20By%20using%20exclusively%20asset-class-neutral%20features%2C%20our%20model%0Aachieves%20robust%2C%20generalizable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23296v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Distribution%2520Prediction%2520for%2520Asset%2520Returns%26entry.906535625%3D%25C3%258Dsak%2520P%25C3%25A9tursson%2520and%2520Mar%25C3%25ADa%2520%25C3%2593skarsd%25C3%25B3ttir%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520predicting%2520the%2520distribution%2520of%2520asset%2520returns%250Ausing%2520a%2520quantile-based%2520method%2520with%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520networks.%2520Our%250Amodel%2520is%2520designed%2520in%2520two%2520stages%253A%2520the%2520first%2520focuses%2520on%2520predicting%2520the%2520quantiles%250Aof%2520normalized%2520asset%2520returns%2520using%2520asset-specific%2520features%252C%2520while%2520the%2520second%250Astage%2520incorporates%2520market%2520data%2520to%2520adjust%2520these%2520predictions%2520for%2520broader%2520economic%250Aconditions.%2520This%2520results%2520in%2520a%2520generalized%2520model%2520that%2520can%2520be%2520applied%2520across%250Avarious%2520asset%2520classes%252C%2520including%2520commodities%252C%2520cryptocurrencies%252C%2520as%2520well%2520as%250Asynthetic%2520datasets.%2520The%2520predicted%2520quantiles%2520are%2520then%2520converted%2520into%2520full%250Aprobability%2520distributions%2520through%2520kernel%2520density%2520estimation%252C%2520allowing%2520for%2520more%250Aprecise%2520return%2520distribution%2520predictions%2520and%2520inferencing.%2520The%2520LSTM%2520model%250Asignificantly%2520outperforms%2520a%2520linear%2520quantile%2520regression%2520baseline%2520by%252098%2525%2520and%2520a%250Adense%2520neural%2520network%2520model%2520by%2520over%252050%2525%252C%2520showcasing%2520its%2520ability%2520to%2520capture%250Acomplex%2520patterns%2520in%2520financial%2520return%2520distributions%2520across%2520both%2520synthetic%2520and%250Areal-world%2520data.%2520By%2520using%2520exclusively%2520asset-class-neutral%2520features%252C%2520our%2520model%250Aachieves%2520robust%252C%2520generalizable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23296v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Distribution%20Prediction%20for%20Asset%20Returns&entry.906535625=%C3%8Dsak%20P%C3%A9tursson%20and%20Mar%C3%ADa%20%C3%93skarsd%C3%B3ttir&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20predicting%20the%20distribution%20of%20asset%20returns%0Ausing%20a%20quantile-based%20method%20with%20Long%20Short-Term%20Memory%20%28LSTM%29%20networks.%20Our%0Amodel%20is%20designed%20in%20two%20stages%3A%20the%20first%20focuses%20on%20predicting%20the%20quantiles%0Aof%20normalized%20asset%20returns%20using%20asset-specific%20features%2C%20while%20the%20second%0Astage%20incorporates%20market%20data%20to%20adjust%20these%20predictions%20for%20broader%20economic%0Aconditions.%20This%20results%20in%20a%20generalized%20model%20that%20can%20be%20applied%20across%0Avarious%20asset%20classes%2C%20including%20commodities%2C%20cryptocurrencies%2C%20as%20well%20as%0Asynthetic%20datasets.%20The%20predicted%20quantiles%20are%20then%20converted%20into%20full%0Aprobability%20distributions%20through%20kernel%20density%20estimation%2C%20allowing%20for%20more%0Aprecise%20return%20distribution%20predictions%20and%20inferencing.%20The%20LSTM%20model%0Asignificantly%20outperforms%20a%20linear%20quantile%20regression%20baseline%20by%2098%25%20and%20a%0Adense%20neural%20network%20model%20by%20over%2050%25%2C%20showcasing%20its%20ability%20to%20capture%0Acomplex%20patterns%20in%20financial%20return%20distributions%20across%20both%20synthetic%20and%0Areal-world%20data.%20By%20using%20exclusively%20asset-class-neutral%20features%2C%20our%20model%0Aachieves%20robust%2C%20generalizable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23296v2&entry.124074799=Read"},
{"title": "Preferences Evolve And So Should Your Bandits: Bandits with Evolving\n  States for Online Platforms", "author": "Khashayar Khosravi and Renato Paes Leme and Chara Podimata and Apostolis Tsorvantzis", "abstract": "  We propose a model for learning with bandit feedback while accounting for\ndeterministically evolving and unobservable states that we call Bandits with\nDeterministically Evolving States ($B$-$DES$). The workhorse applications of\nour model are learning for recommendation systems and learning for online ads.\nIn both cases, the reward that the algorithm obtains at each round is a\nfunction of the short-term reward of the action chosen and how \"healthy\" the\nsystem is (i.e., as measured by its state). For example, in recommendation\nsystems, the reward that the platform obtains from a user's engagement with a\nparticular type of content depends not only on the inherent features of the\nspecific content, but also on how the user's preferences have evolved as a\nresult of interacting with other types of content on the platform. Our general\nmodel accounts for the different rate $\\lambda \\in [0,1]$ at which the state\nevolves (e.g., how fast a user's preferences shift as a result of previous\ncontent consumption) and encompasses standard multi-armed bandits as a special\ncase. The goal of the algorithm is to minimize a notion of regret against the\nbest-fixed sequence of arms pulled, which is significantly harder to attain\ncompared to standard benchmark of the best-fixed action in hindsight. We\npresent online learning algorithms for any possible value of the evolution rate\n$\\lambda$ and we show the robustness of our results to various model\nmisspecifications.\n", "link": "http://arxiv.org/abs/2307.11655v5", "date": "2025-01-28", "relevancy": 1.78, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4416}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preferences%20Evolve%20And%20So%20Should%20Your%20Bandits%3A%20Bandits%20with%20Evolving%0A%20%20States%20for%20Online%20Platforms&body=Title%3A%20Preferences%20Evolve%20And%20So%20Should%20Your%20Bandits%3A%20Bandits%20with%20Evolving%0A%20%20States%20for%20Online%20Platforms%0AAuthor%3A%20Khashayar%20Khosravi%20and%20Renato%20Paes%20Leme%20and%20Chara%20Podimata%20and%20Apostolis%20Tsorvantzis%0AAbstract%3A%20%20%20We%20propose%20a%20model%20for%20learning%20with%20bandit%20feedback%20while%20accounting%20for%0Adeterministically%20evolving%20and%20unobservable%20states%20that%20we%20call%20Bandits%20with%0ADeterministically%20Evolving%20States%20%28%24B%24-%24DES%24%29.%20The%20workhorse%20applications%20of%0Aour%20model%20are%20learning%20for%20recommendation%20systems%20and%20learning%20for%20online%20ads.%0AIn%20both%20cases%2C%20the%20reward%20that%20the%20algorithm%20obtains%20at%20each%20round%20is%20a%0Afunction%20of%20the%20short-term%20reward%20of%20the%20action%20chosen%20and%20how%20%22healthy%22%20the%0Asystem%20is%20%28i.e.%2C%20as%20measured%20by%20its%20state%29.%20For%20example%2C%20in%20recommendation%0Asystems%2C%20the%20reward%20that%20the%20platform%20obtains%20from%20a%20user%27s%20engagement%20with%20a%0Aparticular%20type%20of%20content%20depends%20not%20only%20on%20the%20inherent%20features%20of%20the%0Aspecific%20content%2C%20but%20also%20on%20how%20the%20user%27s%20preferences%20have%20evolved%20as%20a%0Aresult%20of%20interacting%20with%20other%20types%20of%20content%20on%20the%20platform.%20Our%20general%0Amodel%20accounts%20for%20the%20different%20rate%20%24%5Clambda%20%5Cin%20%5B0%2C1%5D%24%20at%20which%20the%20state%0Aevolves%20%28e.g.%2C%20how%20fast%20a%20user%27s%20preferences%20shift%20as%20a%20result%20of%20previous%0Acontent%20consumption%29%20and%20encompasses%20standard%20multi-armed%20bandits%20as%20a%20special%0Acase.%20The%20goal%20of%20the%20algorithm%20is%20to%20minimize%20a%20notion%20of%20regret%20against%20the%0Abest-fixed%20sequence%20of%20arms%20pulled%2C%20which%20is%20significantly%20harder%20to%20attain%0Acompared%20to%20standard%20benchmark%20of%20the%20best-fixed%20action%20in%20hindsight.%20We%0Apresent%20online%20learning%20algorithms%20for%20any%20possible%20value%20of%20the%20evolution%20rate%0A%24%5Clambda%24%20and%20we%20show%20the%20robustness%20of%20our%20results%20to%20various%20model%0Amisspecifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.11655v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreferences%2520Evolve%2520And%2520So%2520Should%2520Your%2520Bandits%253A%2520Bandits%2520with%2520Evolving%250A%2520%2520States%2520for%2520Online%2520Platforms%26entry.906535625%3DKhashayar%2520Khosravi%2520and%2520Renato%2520Paes%2520Leme%2520and%2520Chara%2520Podimata%2520and%2520Apostolis%2520Tsorvantzis%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520model%2520for%2520learning%2520with%2520bandit%2520feedback%2520while%2520accounting%2520for%250Adeterministically%2520evolving%2520and%2520unobservable%2520states%2520that%2520we%2520call%2520Bandits%2520with%250ADeterministically%2520Evolving%2520States%2520%2528%2524B%2524-%2524DES%2524%2529.%2520The%2520workhorse%2520applications%2520of%250Aour%2520model%2520are%2520learning%2520for%2520recommendation%2520systems%2520and%2520learning%2520for%2520online%2520ads.%250AIn%2520both%2520cases%252C%2520the%2520reward%2520that%2520the%2520algorithm%2520obtains%2520at%2520each%2520round%2520is%2520a%250Afunction%2520of%2520the%2520short-term%2520reward%2520of%2520the%2520action%2520chosen%2520and%2520how%2520%2522healthy%2522%2520the%250Asystem%2520is%2520%2528i.e.%252C%2520as%2520measured%2520by%2520its%2520state%2529.%2520For%2520example%252C%2520in%2520recommendation%250Asystems%252C%2520the%2520reward%2520that%2520the%2520platform%2520obtains%2520from%2520a%2520user%2527s%2520engagement%2520with%2520a%250Aparticular%2520type%2520of%2520content%2520depends%2520not%2520only%2520on%2520the%2520inherent%2520features%2520of%2520the%250Aspecific%2520content%252C%2520but%2520also%2520on%2520how%2520the%2520user%2527s%2520preferences%2520have%2520evolved%2520as%2520a%250Aresult%2520of%2520interacting%2520with%2520other%2520types%2520of%2520content%2520on%2520the%2520platform.%2520Our%2520general%250Amodel%2520accounts%2520for%2520the%2520different%2520rate%2520%2524%255Clambda%2520%255Cin%2520%255B0%252C1%255D%2524%2520at%2520which%2520the%2520state%250Aevolves%2520%2528e.g.%252C%2520how%2520fast%2520a%2520user%2527s%2520preferences%2520shift%2520as%2520a%2520result%2520of%2520previous%250Acontent%2520consumption%2529%2520and%2520encompasses%2520standard%2520multi-armed%2520bandits%2520as%2520a%2520special%250Acase.%2520The%2520goal%2520of%2520the%2520algorithm%2520is%2520to%2520minimize%2520a%2520notion%2520of%2520regret%2520against%2520the%250Abest-fixed%2520sequence%2520of%2520arms%2520pulled%252C%2520which%2520is%2520significantly%2520harder%2520to%2520attain%250Acompared%2520to%2520standard%2520benchmark%2520of%2520the%2520best-fixed%2520action%2520in%2520hindsight.%2520We%250Apresent%2520online%2520learning%2520algorithms%2520for%2520any%2520possible%2520value%2520of%2520the%2520evolution%2520rate%250A%2524%255Clambda%2524%2520and%2520we%2520show%2520the%2520robustness%2520of%2520our%2520results%2520to%2520various%2520model%250Amisspecifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.11655v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preferences%20Evolve%20And%20So%20Should%20Your%20Bandits%3A%20Bandits%20with%20Evolving%0A%20%20States%20for%20Online%20Platforms&entry.906535625=Khashayar%20Khosravi%20and%20Renato%20Paes%20Leme%20and%20Chara%20Podimata%20and%20Apostolis%20Tsorvantzis&entry.1292438233=%20%20We%20propose%20a%20model%20for%20learning%20with%20bandit%20feedback%20while%20accounting%20for%0Adeterministically%20evolving%20and%20unobservable%20states%20that%20we%20call%20Bandits%20with%0ADeterministically%20Evolving%20States%20%28%24B%24-%24DES%24%29.%20The%20workhorse%20applications%20of%0Aour%20model%20are%20learning%20for%20recommendation%20systems%20and%20learning%20for%20online%20ads.%0AIn%20both%20cases%2C%20the%20reward%20that%20the%20algorithm%20obtains%20at%20each%20round%20is%20a%0Afunction%20of%20the%20short-term%20reward%20of%20the%20action%20chosen%20and%20how%20%22healthy%22%20the%0Asystem%20is%20%28i.e.%2C%20as%20measured%20by%20its%20state%29.%20For%20example%2C%20in%20recommendation%0Asystems%2C%20the%20reward%20that%20the%20platform%20obtains%20from%20a%20user%27s%20engagement%20with%20a%0Aparticular%20type%20of%20content%20depends%20not%20only%20on%20the%20inherent%20features%20of%20the%0Aspecific%20content%2C%20but%20also%20on%20how%20the%20user%27s%20preferences%20have%20evolved%20as%20a%0Aresult%20of%20interacting%20with%20other%20types%20of%20content%20on%20the%20platform.%20Our%20general%0Amodel%20accounts%20for%20the%20different%20rate%20%24%5Clambda%20%5Cin%20%5B0%2C1%5D%24%20at%20which%20the%20state%0Aevolves%20%28e.g.%2C%20how%20fast%20a%20user%27s%20preferences%20shift%20as%20a%20result%20of%20previous%0Acontent%20consumption%29%20and%20encompasses%20standard%20multi-armed%20bandits%20as%20a%20special%0Acase.%20The%20goal%20of%20the%20algorithm%20is%20to%20minimize%20a%20notion%20of%20regret%20against%20the%0Abest-fixed%20sequence%20of%20arms%20pulled%2C%20which%20is%20significantly%20harder%20to%20attain%0Acompared%20to%20standard%20benchmark%20of%20the%20best-fixed%20action%20in%20hindsight.%20We%0Apresent%20online%20learning%20algorithms%20for%20any%20possible%20value%20of%20the%20evolution%20rate%0A%24%5Clambda%24%20and%20we%20show%20the%20robustness%20of%20our%20results%20to%20various%20model%0Amisspecifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11655v5&entry.124074799=Read"},
{"title": "DBSCAN in domains with periodic boundary conditions", "author": "Xander M. de Wit and Alessandro Gabbana", "abstract": "  Many scientific problems involve data that is embedded in a space with\nperiodic boundary conditions. This can for instance be related to an inherent\ncyclic or rotational symmetry in the data or a spatially extended periodicity.\nWhen analyzing such data, well-tailored methods are needed to obtain efficient\napproaches that obey the periodic boundary conditions of the problem. In this\nwork, we present a method for applying a clustering algorithm to data embedded\nin a periodic domain based on the DBSCAN algorithm, a widely used unsupervised\nmachine learning method that identifies clusters in data. The proposed method\ninternally leverages the conventional DBSCAN algorithm for domains with open\nboundaries, such that it remains compatible with all optimized implementations\nfor neighborhood searches in open domains. In this way, it retains the same\noptimized runtime complexity of $O(N\\log N)$. We demonstrate the workings of\nthe proposed method using synthetic data in one, two and three dimensions and\nalso apply it to a real-world example involving the clustering of bubbles in a\nturbulent flow. The proposed approach is implemented in a ready-to-use Python\npackage that we make publicly available.\n", "link": "http://arxiv.org/abs/2501.16894v1", "date": "2025-01-28", "relevancy": 1.7694, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4483}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4402}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBSCAN%20in%20domains%20with%20periodic%20boundary%20conditions&body=Title%3A%20DBSCAN%20in%20domains%20with%20periodic%20boundary%20conditions%0AAuthor%3A%20Xander%20M.%20de%20Wit%20and%20Alessandro%20Gabbana%0AAbstract%3A%20%20%20Many%20scientific%20problems%20involve%20data%20that%20is%20embedded%20in%20a%20space%20with%0Aperiodic%20boundary%20conditions.%20This%20can%20for%20instance%20be%20related%20to%20an%20inherent%0Acyclic%20or%20rotational%20symmetry%20in%20the%20data%20or%20a%20spatially%20extended%20periodicity.%0AWhen%20analyzing%20such%20data%2C%20well-tailored%20methods%20are%20needed%20to%20obtain%20efficient%0Aapproaches%20that%20obey%20the%20periodic%20boundary%20conditions%20of%20the%20problem.%20In%20this%0Awork%2C%20we%20present%20a%20method%20for%20applying%20a%20clustering%20algorithm%20to%20data%20embedded%0Ain%20a%20periodic%20domain%20based%20on%20the%20DBSCAN%20algorithm%2C%20a%20widely%20used%20unsupervised%0Amachine%20learning%20method%20that%20identifies%20clusters%20in%20data.%20The%20proposed%20method%0Ainternally%20leverages%20the%20conventional%20DBSCAN%20algorithm%20for%20domains%20with%20open%0Aboundaries%2C%20such%20that%20it%20remains%20compatible%20with%20all%20optimized%20implementations%0Afor%20neighborhood%20searches%20in%20open%20domains.%20In%20this%20way%2C%20it%20retains%20the%20same%0Aoptimized%20runtime%20complexity%20of%20%24O%28N%5Clog%20N%29%24.%20We%20demonstrate%20the%20workings%20of%0Athe%20proposed%20method%20using%20synthetic%20data%20in%20one%2C%20two%20and%20three%20dimensions%20and%0Aalso%20apply%20it%20to%20a%20real-world%20example%20involving%20the%20clustering%20of%20bubbles%20in%20a%0Aturbulent%20flow.%20The%20proposed%20approach%20is%20implemented%20in%20a%20ready-to-use%20Python%0Apackage%20that%20we%20make%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBSCAN%2520in%2520domains%2520with%2520periodic%2520boundary%2520conditions%26entry.906535625%3DXander%2520M.%2520de%2520Wit%2520and%2520Alessandro%2520Gabbana%26entry.1292438233%3D%2520%2520Many%2520scientific%2520problems%2520involve%2520data%2520that%2520is%2520embedded%2520in%2520a%2520space%2520with%250Aperiodic%2520boundary%2520conditions.%2520This%2520can%2520for%2520instance%2520be%2520related%2520to%2520an%2520inherent%250Acyclic%2520or%2520rotational%2520symmetry%2520in%2520the%2520data%2520or%2520a%2520spatially%2520extended%2520periodicity.%250AWhen%2520analyzing%2520such%2520data%252C%2520well-tailored%2520methods%2520are%2520needed%2520to%2520obtain%2520efficient%250Aapproaches%2520that%2520obey%2520the%2520periodic%2520boundary%2520conditions%2520of%2520the%2520problem.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520method%2520for%2520applying%2520a%2520clustering%2520algorithm%2520to%2520data%2520embedded%250Ain%2520a%2520periodic%2520domain%2520based%2520on%2520the%2520DBSCAN%2520algorithm%252C%2520a%2520widely%2520used%2520unsupervised%250Amachine%2520learning%2520method%2520that%2520identifies%2520clusters%2520in%2520data.%2520The%2520proposed%2520method%250Ainternally%2520leverages%2520the%2520conventional%2520DBSCAN%2520algorithm%2520for%2520domains%2520with%2520open%250Aboundaries%252C%2520such%2520that%2520it%2520remains%2520compatible%2520with%2520all%2520optimized%2520implementations%250Afor%2520neighborhood%2520searches%2520in%2520open%2520domains.%2520In%2520this%2520way%252C%2520it%2520retains%2520the%2520same%250Aoptimized%2520runtime%2520complexity%2520of%2520%2524O%2528N%255Clog%2520N%2529%2524.%2520We%2520demonstrate%2520the%2520workings%2520of%250Athe%2520proposed%2520method%2520using%2520synthetic%2520data%2520in%2520one%252C%2520two%2520and%2520three%2520dimensions%2520and%250Aalso%2520apply%2520it%2520to%2520a%2520real-world%2520example%2520involving%2520the%2520clustering%2520of%2520bubbles%2520in%2520a%250Aturbulent%2520flow.%2520The%2520proposed%2520approach%2520is%2520implemented%2520in%2520a%2520ready-to-use%2520Python%250Apackage%2520that%2520we%2520make%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBSCAN%20in%20domains%20with%20periodic%20boundary%20conditions&entry.906535625=Xander%20M.%20de%20Wit%20and%20Alessandro%20Gabbana&entry.1292438233=%20%20Many%20scientific%20problems%20involve%20data%20that%20is%20embedded%20in%20a%20space%20with%0Aperiodic%20boundary%20conditions.%20This%20can%20for%20instance%20be%20related%20to%20an%20inherent%0Acyclic%20or%20rotational%20symmetry%20in%20the%20data%20or%20a%20spatially%20extended%20periodicity.%0AWhen%20analyzing%20such%20data%2C%20well-tailored%20methods%20are%20needed%20to%20obtain%20efficient%0Aapproaches%20that%20obey%20the%20periodic%20boundary%20conditions%20of%20the%20problem.%20In%20this%0Awork%2C%20we%20present%20a%20method%20for%20applying%20a%20clustering%20algorithm%20to%20data%20embedded%0Ain%20a%20periodic%20domain%20based%20on%20the%20DBSCAN%20algorithm%2C%20a%20widely%20used%20unsupervised%0Amachine%20learning%20method%20that%20identifies%20clusters%20in%20data.%20The%20proposed%20method%0Ainternally%20leverages%20the%20conventional%20DBSCAN%20algorithm%20for%20domains%20with%20open%0Aboundaries%2C%20such%20that%20it%20remains%20compatible%20with%20all%20optimized%20implementations%0Afor%20neighborhood%20searches%20in%20open%20domains.%20In%20this%20way%2C%20it%20retains%20the%20same%0Aoptimized%20runtime%20complexity%20of%20%24O%28N%5Clog%20N%29%24.%20We%20demonstrate%20the%20workings%20of%0Athe%20proposed%20method%20using%20synthetic%20data%20in%20one%2C%20two%20and%20three%20dimensions%20and%0Aalso%20apply%20it%20to%20a%20real-world%20example%20involving%20the%20clustering%20of%20bubbles%20in%20a%0Aturbulent%20flow.%20The%20proposed%20approach%20is%20implemented%20in%20a%20ready-to-use%20Python%0Apackage%20that%20we%20make%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16894v1&entry.124074799=Read"},
{"title": "StableMaterials: Enhancing Diversity in Material Generation via\n  Semi-Supervised Learning", "author": "Giuseppe Vecchio", "abstract": "  We introduce StableMaterials, a novel approach for generating photorealistic\nphysical-based rendering (PBR) materials that integrate semi-supervised\nlearning with Latent Diffusion Models (LDMs). Our method employs adversarial\ntraining to distill knowledge from existing large-scale image generation\nmodels, minimizing the reliance on annotated data and enhancing the diversity\nin generation. This distillation approach aligns the distribution of the\ngenerated materials with that of image textures from an SDXL model, enabling\nthe generation of novel materials that are not present in the initial training\ndataset. Furthermore, we employ a diffusion-based refiner model to improve the\nvisual quality of the samples and achieve high-resolution generation. Finally,\nwe distill a latent consistency model for fast generation in just four steps\nand propose a new tileability technique that removes visual artifacts typically\nassociated with fewer diffusion steps. We detail the architecture and training\nprocess of StableMaterials, the integration of semi-supervised training within\nexisting LDM frameworks and show the advantages of our approach. Comparative\nevaluations with state-of-the-art methods show the effectiveness of\nStableMaterials, highlighting its potential applications in computer graphics\nand beyond. StableMaterials is publicly available at\nhttps://gvecchio.com/stablematerials.\n", "link": "http://arxiv.org/abs/2406.09293v3", "date": "2025-01-28", "relevancy": 1.7518, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5953}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5791}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableMaterials%3A%20Enhancing%20Diversity%20in%20Material%20Generation%20via%0A%20%20Semi-Supervised%20Learning&body=Title%3A%20StableMaterials%3A%20Enhancing%20Diversity%20in%20Material%20Generation%20via%0A%20%20Semi-Supervised%20Learning%0AAuthor%3A%20Giuseppe%20Vecchio%0AAbstract%3A%20%20%20We%20introduce%20StableMaterials%2C%20a%20novel%20approach%20for%20generating%20photorealistic%0Aphysical-based%20rendering%20%28PBR%29%20materials%20that%20integrate%20semi-supervised%0Alearning%20with%20Latent%20Diffusion%20Models%20%28LDMs%29.%20Our%20method%20employs%20adversarial%0Atraining%20to%20distill%20knowledge%20from%20existing%20large-scale%20image%20generation%0Amodels%2C%20minimizing%20the%20reliance%20on%20annotated%20data%20and%20enhancing%20the%20diversity%0Ain%20generation.%20This%20distillation%20approach%20aligns%20the%20distribution%20of%20the%0Agenerated%20materials%20with%20that%20of%20image%20textures%20from%20an%20SDXL%20model%2C%20enabling%0Athe%20generation%20of%20novel%20materials%20that%20are%20not%20present%20in%20the%20initial%20training%0Adataset.%20Furthermore%2C%20we%20employ%20a%20diffusion-based%20refiner%20model%20to%20improve%20the%0Avisual%20quality%20of%20the%20samples%20and%20achieve%20high-resolution%20generation.%20Finally%2C%0Awe%20distill%20a%20latent%20consistency%20model%20for%20fast%20generation%20in%20just%20four%20steps%0Aand%20propose%20a%20new%20tileability%20technique%20that%20removes%20visual%20artifacts%20typically%0Aassociated%20with%20fewer%20diffusion%20steps.%20We%20detail%20the%20architecture%20and%20training%0Aprocess%20of%20StableMaterials%2C%20the%20integration%20of%20semi-supervised%20training%20within%0Aexisting%20LDM%20frameworks%20and%20show%20the%20advantages%20of%20our%20approach.%20Comparative%0Aevaluations%20with%20state-of-the-art%20methods%20show%20the%20effectiveness%20of%0AStableMaterials%2C%20highlighting%20its%20potential%20applications%20in%20computer%20graphics%0Aand%20beyond.%20StableMaterials%20is%20publicly%20available%20at%0Ahttps%3A//gvecchio.com/stablematerials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09293v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableMaterials%253A%2520Enhancing%2520Diversity%2520in%2520Material%2520Generation%2520via%250A%2520%2520Semi-Supervised%2520Learning%26entry.906535625%3DGiuseppe%2520Vecchio%26entry.1292438233%3D%2520%2520We%2520introduce%2520StableMaterials%252C%2520a%2520novel%2520approach%2520for%2520generating%2520photorealistic%250Aphysical-based%2520rendering%2520%2528PBR%2529%2520materials%2520that%2520integrate%2520semi-supervised%250Alearning%2520with%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529.%2520Our%2520method%2520employs%2520adversarial%250Atraining%2520to%2520distill%2520knowledge%2520from%2520existing%2520large-scale%2520image%2520generation%250Amodels%252C%2520minimizing%2520the%2520reliance%2520on%2520annotated%2520data%2520and%2520enhancing%2520the%2520diversity%250Ain%2520generation.%2520This%2520distillation%2520approach%2520aligns%2520the%2520distribution%2520of%2520the%250Agenerated%2520materials%2520with%2520that%2520of%2520image%2520textures%2520from%2520an%2520SDXL%2520model%252C%2520enabling%250Athe%2520generation%2520of%2520novel%2520materials%2520that%2520are%2520not%2520present%2520in%2520the%2520initial%2520training%250Adataset.%2520Furthermore%252C%2520we%2520employ%2520a%2520diffusion-based%2520refiner%2520model%2520to%2520improve%2520the%250Avisual%2520quality%2520of%2520the%2520samples%2520and%2520achieve%2520high-resolution%2520generation.%2520Finally%252C%250Awe%2520distill%2520a%2520latent%2520consistency%2520model%2520for%2520fast%2520generation%2520in%2520just%2520four%2520steps%250Aand%2520propose%2520a%2520new%2520tileability%2520technique%2520that%2520removes%2520visual%2520artifacts%2520typically%250Aassociated%2520with%2520fewer%2520diffusion%2520steps.%2520We%2520detail%2520the%2520architecture%2520and%2520training%250Aprocess%2520of%2520StableMaterials%252C%2520the%2520integration%2520of%2520semi-supervised%2520training%2520within%250Aexisting%2520LDM%2520frameworks%2520and%2520show%2520the%2520advantages%2520of%2520our%2520approach.%2520Comparative%250Aevaluations%2520with%2520state-of-the-art%2520methods%2520show%2520the%2520effectiveness%2520of%250AStableMaterials%252C%2520highlighting%2520its%2520potential%2520applications%2520in%2520computer%2520graphics%250Aand%2520beyond.%2520StableMaterials%2520is%2520publicly%2520available%2520at%250Ahttps%253A//gvecchio.com/stablematerials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09293v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableMaterials%3A%20Enhancing%20Diversity%20in%20Material%20Generation%20via%0A%20%20Semi-Supervised%20Learning&entry.906535625=Giuseppe%20Vecchio&entry.1292438233=%20%20We%20introduce%20StableMaterials%2C%20a%20novel%20approach%20for%20generating%20photorealistic%0Aphysical-based%20rendering%20%28PBR%29%20materials%20that%20integrate%20semi-supervised%0Alearning%20with%20Latent%20Diffusion%20Models%20%28LDMs%29.%20Our%20method%20employs%20adversarial%0Atraining%20to%20distill%20knowledge%20from%20existing%20large-scale%20image%20generation%0Amodels%2C%20minimizing%20the%20reliance%20on%20annotated%20data%20and%20enhancing%20the%20diversity%0Ain%20generation.%20This%20distillation%20approach%20aligns%20the%20distribution%20of%20the%0Agenerated%20materials%20with%20that%20of%20image%20textures%20from%20an%20SDXL%20model%2C%20enabling%0Athe%20generation%20of%20novel%20materials%20that%20are%20not%20present%20in%20the%20initial%20training%0Adataset.%20Furthermore%2C%20we%20employ%20a%20diffusion-based%20refiner%20model%20to%20improve%20the%0Avisual%20quality%20of%20the%20samples%20and%20achieve%20high-resolution%20generation.%20Finally%2C%0Awe%20distill%20a%20latent%20consistency%20model%20for%20fast%20generation%20in%20just%20four%20steps%0Aand%20propose%20a%20new%20tileability%20technique%20that%20removes%20visual%20artifacts%20typically%0Aassociated%20with%20fewer%20diffusion%20steps.%20We%20detail%20the%20architecture%20and%20training%0Aprocess%20of%20StableMaterials%2C%20the%20integration%20of%20semi-supervised%20training%20within%0Aexisting%20LDM%20frameworks%20and%20show%20the%20advantages%20of%20our%20approach.%20Comparative%0Aevaluations%20with%20state-of-the-art%20methods%20show%20the%20effectiveness%20of%0AStableMaterials%2C%20highlighting%20its%20potential%20applications%20in%20computer%20graphics%0Aand%20beyond.%20StableMaterials%20is%20publicly%20available%20at%0Ahttps%3A//gvecchio.com/stablematerials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09293v3&entry.124074799=Read"},
{"title": "Robust Policy Search for Robot Navigation", "author": "Javier Garcia-Barcos and Ruben Martinez-Cantin", "abstract": "  Complex robot navigation and control problems can be framed as policy search\nproblems. However, interactive learning in uncertain environments can be\nexpensive, requiring the use of data-efficient methods. Bayesian optimization\nis an efficient nonlinear optimization method where queries are carefully\nselected to gather information about the optimum location. This is achieved by\na surrogate model, which encodes past information, and the acquisition function\nfor query selection. Bayesian optimization can be very sensitive to uncertainty\nin the input data or prior assumptions. In this work, we incorporate both\nrobust optimization and statistical robustness, showing that both types of\nrobustness are synergistic. For robust optimization we use an improved version\nof unscented Bayesian optimization which provides safe and repeatable policies\nin the presence of policy uncertainty. We also provide new theoretical\ninsights. For statistical robustness, we use an adaptive surrogate model and we\nintroduce the Boltzmann selection as a stochastic acquisition method to have\nconvergence guarantees and improved performance even with surrogate modeling\nerrors. We present results in several optimization benchmarks and robot tasks.\n", "link": "http://arxiv.org/abs/2003.01000v2", "date": "2025-01-28", "relevancy": 1.7462, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6156}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5839}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Policy%20Search%20for%20Robot%20Navigation&body=Title%3A%20Robust%20Policy%20Search%20for%20Robot%20Navigation%0AAuthor%3A%20Javier%20Garcia-Barcos%20and%20Ruben%20Martinez-Cantin%0AAbstract%3A%20%20%20Complex%20robot%20navigation%20and%20control%20problems%20can%20be%20framed%20as%20policy%20search%0Aproblems.%20However%2C%20interactive%20learning%20in%20uncertain%20environments%20can%20be%0Aexpensive%2C%20requiring%20the%20use%20of%20data-efficient%20methods.%20Bayesian%20optimization%0Ais%20an%20efficient%20nonlinear%20optimization%20method%20where%20queries%20are%20carefully%0Aselected%20to%20gather%20information%20about%20the%20optimum%20location.%20This%20is%20achieved%20by%0Aa%20surrogate%20model%2C%20which%20encodes%20past%20information%2C%20and%20the%20acquisition%20function%0Afor%20query%20selection.%20Bayesian%20optimization%20can%20be%20very%20sensitive%20to%20uncertainty%0Ain%20the%20input%20data%20or%20prior%20assumptions.%20In%20this%20work%2C%20we%20incorporate%20both%0Arobust%20optimization%20and%20statistical%20robustness%2C%20showing%20that%20both%20types%20of%0Arobustness%20are%20synergistic.%20For%20robust%20optimization%20we%20use%20an%20improved%20version%0Aof%20unscented%20Bayesian%20optimization%20which%20provides%20safe%20and%20repeatable%20policies%0Ain%20the%20presence%20of%20policy%20uncertainty.%20We%20also%20provide%20new%20theoretical%0Ainsights.%20For%20statistical%20robustness%2C%20we%20use%20an%20adaptive%20surrogate%20model%20and%20we%0Aintroduce%20the%20Boltzmann%20selection%20as%20a%20stochastic%20acquisition%20method%20to%20have%0Aconvergence%20guarantees%20and%20improved%20performance%20even%20with%20surrogate%20modeling%0Aerrors.%20We%20present%20results%20in%20several%20optimization%20benchmarks%20and%20robot%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2003.01000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Policy%2520Search%2520for%2520Robot%2520Navigation%26entry.906535625%3DJavier%2520Garcia-Barcos%2520and%2520Ruben%2520Martinez-Cantin%26entry.1292438233%3D%2520%2520Complex%2520robot%2520navigation%2520and%2520control%2520problems%2520can%2520be%2520framed%2520as%2520policy%2520search%250Aproblems.%2520However%252C%2520interactive%2520learning%2520in%2520uncertain%2520environments%2520can%2520be%250Aexpensive%252C%2520requiring%2520the%2520use%2520of%2520data-efficient%2520methods.%2520Bayesian%2520optimization%250Ais%2520an%2520efficient%2520nonlinear%2520optimization%2520method%2520where%2520queries%2520are%2520carefully%250Aselected%2520to%2520gather%2520information%2520about%2520the%2520optimum%2520location.%2520This%2520is%2520achieved%2520by%250Aa%2520surrogate%2520model%252C%2520which%2520encodes%2520past%2520information%252C%2520and%2520the%2520acquisition%2520function%250Afor%2520query%2520selection.%2520Bayesian%2520optimization%2520can%2520be%2520very%2520sensitive%2520to%2520uncertainty%250Ain%2520the%2520input%2520data%2520or%2520prior%2520assumptions.%2520In%2520this%2520work%252C%2520we%2520incorporate%2520both%250Arobust%2520optimization%2520and%2520statistical%2520robustness%252C%2520showing%2520that%2520both%2520types%2520of%250Arobustness%2520are%2520synergistic.%2520For%2520robust%2520optimization%2520we%2520use%2520an%2520improved%2520version%250Aof%2520unscented%2520Bayesian%2520optimization%2520which%2520provides%2520safe%2520and%2520repeatable%2520policies%250Ain%2520the%2520presence%2520of%2520policy%2520uncertainty.%2520We%2520also%2520provide%2520new%2520theoretical%250Ainsights.%2520For%2520statistical%2520robustness%252C%2520we%2520use%2520an%2520adaptive%2520surrogate%2520model%2520and%2520we%250Aintroduce%2520the%2520Boltzmann%2520selection%2520as%2520a%2520stochastic%2520acquisition%2520method%2520to%2520have%250Aconvergence%2520guarantees%2520and%2520improved%2520performance%2520even%2520with%2520surrogate%2520modeling%250Aerrors.%2520We%2520present%2520results%2520in%2520several%2520optimization%2520benchmarks%2520and%2520robot%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2003.01000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Policy%20Search%20for%20Robot%20Navigation&entry.906535625=Javier%20Garcia-Barcos%20and%20Ruben%20Martinez-Cantin&entry.1292438233=%20%20Complex%20robot%20navigation%20and%20control%20problems%20can%20be%20framed%20as%20policy%20search%0Aproblems.%20However%2C%20interactive%20learning%20in%20uncertain%20environments%20can%20be%0Aexpensive%2C%20requiring%20the%20use%20of%20data-efficient%20methods.%20Bayesian%20optimization%0Ais%20an%20efficient%20nonlinear%20optimization%20method%20where%20queries%20are%20carefully%0Aselected%20to%20gather%20information%20about%20the%20optimum%20location.%20This%20is%20achieved%20by%0Aa%20surrogate%20model%2C%20which%20encodes%20past%20information%2C%20and%20the%20acquisition%20function%0Afor%20query%20selection.%20Bayesian%20optimization%20can%20be%20very%20sensitive%20to%20uncertainty%0Ain%20the%20input%20data%20or%20prior%20assumptions.%20In%20this%20work%2C%20we%20incorporate%20both%0Arobust%20optimization%20and%20statistical%20robustness%2C%20showing%20that%20both%20types%20of%0Arobustness%20are%20synergistic.%20For%20robust%20optimization%20we%20use%20an%20improved%20version%0Aof%20unscented%20Bayesian%20optimization%20which%20provides%20safe%20and%20repeatable%20policies%0Ain%20the%20presence%20of%20policy%20uncertainty.%20We%20also%20provide%20new%20theoretical%0Ainsights.%20For%20statistical%20robustness%2C%20we%20use%20an%20adaptive%20surrogate%20model%20and%20we%0Aintroduce%20the%20Boltzmann%20selection%20as%20a%20stochastic%20acquisition%20method%20to%20have%0Aconvergence%20guarantees%20and%20improved%20performance%20even%20with%20surrogate%20modeling%0Aerrors.%20We%20present%20results%20in%20several%20optimization%20benchmarks%20and%20robot%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2003.01000v2&entry.124074799=Read"},
{"title": "Benchmarking Quantum Convolutional Neural Networks for Signal\n  Classification in Simulated Gamma-Ray Burst Detection", "author": "Farida Farsian and Nicol\u00f2 Parmiggiani and Alessandro Rizzo and Gabriele Panebianco and Andrea Bulgarelli and Francesco Schillir\u00f2 and Carlo Burigana and Vincenzo Cardone and Luca Cappelli and Massimo Meneghetti and Giuseppe Murante and Giuseppe Sarracino and Roberto Scaramella and Vincenzo Testa and Tiziana Trombetti", "abstract": "  This study evaluates the use of Quantum Convolutional Neural Networks (QCNNs)\nfor identifying signals resembling Gamma-Ray Bursts (GRBs) within simulated\nastrophysical datasets in the form of light curves. The task addressed here\nfocuses on distinguishing GRB-like signals from background noise in simulated\nCherenkov Telescope Array Observatory (CTAO) data, the next-generation\nastrophysical observatory for very high-energy gamma-ray science. QCNNs, a\nquantum counterpart of classical Convolutional Neural Networks (CNNs), leverage\nquantum principles to process and analyze high-dimensional data efficiently. We\nimplemented a hybrid quantum-classical machine learning technique using the\nQiskit framework, with the QCNNs trained on a quantum simulator. Several QCNN\narchitectures were tested, employing different encoding methods such as Data\nReuploading and Amplitude encoding. Key findings include that QCNNs achieved\naccuracy comparable to classical CNNs, often surpassing 90\\%, while using fewer\nparameters, potentially leading to more efficient models in terms of\ncomputational resources. A benchmark study further examined how hyperparameters\nlike the number of qubits and encoding methods affected performance, with more\nqubits and advanced encoding methods generally enhancing accuracy but\nincreasing complexity. QCNNs showed robust performance on time-series datasets,\nsuccessfully detecting GRB signals with high precision. The research is a\npioneering effort in applying QCNNs to astrophysics, offering insights into\ntheir potential and limitations. This work sets the stage for future\ninvestigations to fully realize the advantages of QCNNs in astrophysical data\nanalysis.\n", "link": "http://arxiv.org/abs/2501.17041v1", "date": "2025-01-28", "relevancy": 1.7207, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4372}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4263}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Quantum%20Convolutional%20Neural%20Networks%20for%20Signal%0A%20%20Classification%20in%20Simulated%20Gamma-Ray%20Burst%20Detection&body=Title%3A%20Benchmarking%20Quantum%20Convolutional%20Neural%20Networks%20for%20Signal%0A%20%20Classification%20in%20Simulated%20Gamma-Ray%20Burst%20Detection%0AAuthor%3A%20Farida%20Farsian%20and%20Nicol%C3%B2%20Parmiggiani%20and%20Alessandro%20Rizzo%20and%20Gabriele%20Panebianco%20and%20Andrea%20Bulgarelli%20and%20Francesco%20Schillir%C3%B2%20and%20Carlo%20Burigana%20and%20Vincenzo%20Cardone%20and%20Luca%20Cappelli%20and%20Massimo%20Meneghetti%20and%20Giuseppe%20Murante%20and%20Giuseppe%20Sarracino%20and%20Roberto%20Scaramella%20and%20Vincenzo%20Testa%20and%20Tiziana%20Trombetti%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20use%20of%20Quantum%20Convolutional%20Neural%20Networks%20%28QCNNs%29%0Afor%20identifying%20signals%20resembling%20Gamma-Ray%20Bursts%20%28GRBs%29%20within%20simulated%0Aastrophysical%20datasets%20in%20the%20form%20of%20light%20curves.%20The%20task%20addressed%20here%0Afocuses%20on%20distinguishing%20GRB-like%20signals%20from%20background%20noise%20in%20simulated%0ACherenkov%20Telescope%20Array%20Observatory%20%28CTAO%29%20data%2C%20the%20next-generation%0Aastrophysical%20observatory%20for%20very%20high-energy%20gamma-ray%20science.%20QCNNs%2C%20a%0Aquantum%20counterpart%20of%20classical%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20leverage%0Aquantum%20principles%20to%20process%20and%20analyze%20high-dimensional%20data%20efficiently.%20We%0Aimplemented%20a%20hybrid%20quantum-classical%20machine%20learning%20technique%20using%20the%0AQiskit%20framework%2C%20with%20the%20QCNNs%20trained%20on%20a%20quantum%20simulator.%20Several%20QCNN%0Aarchitectures%20were%20tested%2C%20employing%20different%20encoding%20methods%20such%20as%20Data%0AReuploading%20and%20Amplitude%20encoding.%20Key%20findings%20include%20that%20QCNNs%20achieved%0Aaccuracy%20comparable%20to%20classical%20CNNs%2C%20often%20surpassing%2090%5C%25%2C%20while%20using%20fewer%0Aparameters%2C%20potentially%20leading%20to%20more%20efficient%20models%20in%20terms%20of%0Acomputational%20resources.%20A%20benchmark%20study%20further%20examined%20how%20hyperparameters%0Alike%20the%20number%20of%20qubits%20and%20encoding%20methods%20affected%20performance%2C%20with%20more%0Aqubits%20and%20advanced%20encoding%20methods%20generally%20enhancing%20accuracy%20but%0Aincreasing%20complexity.%20QCNNs%20showed%20robust%20performance%20on%20time-series%20datasets%2C%0Asuccessfully%20detecting%20GRB%20signals%20with%20high%20precision.%20The%20research%20is%20a%0Apioneering%20effort%20in%20applying%20QCNNs%20to%20astrophysics%2C%20offering%20insights%20into%0Atheir%20potential%20and%20limitations.%20This%20work%20sets%20the%20stage%20for%20future%0Ainvestigations%20to%20fully%20realize%20the%20advantages%20of%20QCNNs%20in%20astrophysical%20data%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Quantum%2520Convolutional%2520Neural%2520Networks%2520for%2520Signal%250A%2520%2520Classification%2520in%2520Simulated%2520Gamma-Ray%2520Burst%2520Detection%26entry.906535625%3DFarida%2520Farsian%2520and%2520Nicol%25C3%25B2%2520Parmiggiani%2520and%2520Alessandro%2520Rizzo%2520and%2520Gabriele%2520Panebianco%2520and%2520Andrea%2520Bulgarelli%2520and%2520Francesco%2520Schillir%25C3%25B2%2520and%2520Carlo%2520Burigana%2520and%2520Vincenzo%2520Cardone%2520and%2520Luca%2520Cappelli%2520and%2520Massimo%2520Meneghetti%2520and%2520Giuseppe%2520Murante%2520and%2520Giuseppe%2520Sarracino%2520and%2520Roberto%2520Scaramella%2520and%2520Vincenzo%2520Testa%2520and%2520Tiziana%2520Trombetti%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520use%2520of%2520Quantum%2520Convolutional%2520Neural%2520Networks%2520%2528QCNNs%2529%250Afor%2520identifying%2520signals%2520resembling%2520Gamma-Ray%2520Bursts%2520%2528GRBs%2529%2520within%2520simulated%250Aastrophysical%2520datasets%2520in%2520the%2520form%2520of%2520light%2520curves.%2520The%2520task%2520addressed%2520here%250Afocuses%2520on%2520distinguishing%2520GRB-like%2520signals%2520from%2520background%2520noise%2520in%2520simulated%250ACherenkov%2520Telescope%2520Array%2520Observatory%2520%2528CTAO%2529%2520data%252C%2520the%2520next-generation%250Aastrophysical%2520observatory%2520for%2520very%2520high-energy%2520gamma-ray%2520science.%2520QCNNs%252C%2520a%250Aquantum%2520counterpart%2520of%2520classical%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520leverage%250Aquantum%2520principles%2520to%2520process%2520and%2520analyze%2520high-dimensional%2520data%2520efficiently.%2520We%250Aimplemented%2520a%2520hybrid%2520quantum-classical%2520machine%2520learning%2520technique%2520using%2520the%250AQiskit%2520framework%252C%2520with%2520the%2520QCNNs%2520trained%2520on%2520a%2520quantum%2520simulator.%2520Several%2520QCNN%250Aarchitectures%2520were%2520tested%252C%2520employing%2520different%2520encoding%2520methods%2520such%2520as%2520Data%250AReuploading%2520and%2520Amplitude%2520encoding.%2520Key%2520findings%2520include%2520that%2520QCNNs%2520achieved%250Aaccuracy%2520comparable%2520to%2520classical%2520CNNs%252C%2520often%2520surpassing%252090%255C%2525%252C%2520while%2520using%2520fewer%250Aparameters%252C%2520potentially%2520leading%2520to%2520more%2520efficient%2520models%2520in%2520terms%2520of%250Acomputational%2520resources.%2520A%2520benchmark%2520study%2520further%2520examined%2520how%2520hyperparameters%250Alike%2520the%2520number%2520of%2520qubits%2520and%2520encoding%2520methods%2520affected%2520performance%252C%2520with%2520more%250Aqubits%2520and%2520advanced%2520encoding%2520methods%2520generally%2520enhancing%2520accuracy%2520but%250Aincreasing%2520complexity.%2520QCNNs%2520showed%2520robust%2520performance%2520on%2520time-series%2520datasets%252C%250Asuccessfully%2520detecting%2520GRB%2520signals%2520with%2520high%2520precision.%2520The%2520research%2520is%2520a%250Apioneering%2520effort%2520in%2520applying%2520QCNNs%2520to%2520astrophysics%252C%2520offering%2520insights%2520into%250Atheir%2520potential%2520and%2520limitations.%2520This%2520work%2520sets%2520the%2520stage%2520for%2520future%250Ainvestigations%2520to%2520fully%2520realize%2520the%2520advantages%2520of%2520QCNNs%2520in%2520astrophysical%2520data%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Quantum%20Convolutional%20Neural%20Networks%20for%20Signal%0A%20%20Classification%20in%20Simulated%20Gamma-Ray%20Burst%20Detection&entry.906535625=Farida%20Farsian%20and%20Nicol%C3%B2%20Parmiggiani%20and%20Alessandro%20Rizzo%20and%20Gabriele%20Panebianco%20and%20Andrea%20Bulgarelli%20and%20Francesco%20Schillir%C3%B2%20and%20Carlo%20Burigana%20and%20Vincenzo%20Cardone%20and%20Luca%20Cappelli%20and%20Massimo%20Meneghetti%20and%20Giuseppe%20Murante%20and%20Giuseppe%20Sarracino%20and%20Roberto%20Scaramella%20and%20Vincenzo%20Testa%20and%20Tiziana%20Trombetti&entry.1292438233=%20%20This%20study%20evaluates%20the%20use%20of%20Quantum%20Convolutional%20Neural%20Networks%20%28QCNNs%29%0Afor%20identifying%20signals%20resembling%20Gamma-Ray%20Bursts%20%28GRBs%29%20within%20simulated%0Aastrophysical%20datasets%20in%20the%20form%20of%20light%20curves.%20The%20task%20addressed%20here%0Afocuses%20on%20distinguishing%20GRB-like%20signals%20from%20background%20noise%20in%20simulated%0ACherenkov%20Telescope%20Array%20Observatory%20%28CTAO%29%20data%2C%20the%20next-generation%0Aastrophysical%20observatory%20for%20very%20high-energy%20gamma-ray%20science.%20QCNNs%2C%20a%0Aquantum%20counterpart%20of%20classical%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20leverage%0Aquantum%20principles%20to%20process%20and%20analyze%20high-dimensional%20data%20efficiently.%20We%0Aimplemented%20a%20hybrid%20quantum-classical%20machine%20learning%20technique%20using%20the%0AQiskit%20framework%2C%20with%20the%20QCNNs%20trained%20on%20a%20quantum%20simulator.%20Several%20QCNN%0Aarchitectures%20were%20tested%2C%20employing%20different%20encoding%20methods%20such%20as%20Data%0AReuploading%20and%20Amplitude%20encoding.%20Key%20findings%20include%20that%20QCNNs%20achieved%0Aaccuracy%20comparable%20to%20classical%20CNNs%2C%20often%20surpassing%2090%5C%25%2C%20while%20using%20fewer%0Aparameters%2C%20potentially%20leading%20to%20more%20efficient%20models%20in%20terms%20of%0Acomputational%20resources.%20A%20benchmark%20study%20further%20examined%20how%20hyperparameters%0Alike%20the%20number%20of%20qubits%20and%20encoding%20methods%20affected%20performance%2C%20with%20more%0Aqubits%20and%20advanced%20encoding%20methods%20generally%20enhancing%20accuracy%20but%0Aincreasing%20complexity.%20QCNNs%20showed%20robust%20performance%20on%20time-series%20datasets%2C%0Asuccessfully%20detecting%20GRB%20signals%20with%20high%20precision.%20The%20research%20is%20a%0Apioneering%20effort%20in%20applying%20QCNNs%20to%20astrophysics%2C%20offering%20insights%20into%0Atheir%20potential%20and%20limitations.%20This%20work%20sets%20the%20stage%20for%20future%0Ainvestigations%20to%20fully%20realize%20the%20advantages%20of%20QCNNs%20in%20astrophysical%20data%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17041v1&entry.124074799=Read"},
{"title": "Empirical modeling and hybrid machine learning framework for nucleate\n  pool boiling on microchannel structured surfaces", "author": "Vijay Kuberan and Sateesh Gedupudi", "abstract": "  Micro-structured surfaces influence nucleation characteristics and bubble\ndynamics besides increasing the heat transfer surface area, thus enabling\nefficient nucleate boiling heat transfer. Modeling the pool boiling heat\ntransfer characteristics of these surfaces under varied conditions is essential\nin diverse applications. A new empirical correlation for nucleate boiling on\nmicrochannel structured surfaces has been proposed with the data collected from\nvarious experiments in previous studies since the existing correlations are\nlimited by their accuracy and narrow operating ranges. This study also examines\nvarious Machine Learning (ML) algorithms and Deep Neural Networks (DNN) on the\nmicrochannel structured surfaces dataset to predict the nucleate pool boiling\nHeat Transfer Coefficient (HTC). With the aim to integrate both the ML and\ndomain knowledge, a Physics-Informed Machine Learning Aided Framework (PIMLAF)\nis proposed. The proposed correlation in this study is employed as the prior\nphysics-based model for PIMLAF, and a DNN is employed to model the residuals of\nthe prior model. This hybrid framework achieved the best performance in\ncomparison to the other ML models and DNNs. This framework is able to\ngeneralize well for different datasets because the proposed correlation\nprovides the baseline knowledge of the boiling behavior. Also, SHAP\ninterpretation analysis identifies the critical parameters impacting the model\npredictions and their effect on HTC prediction. This analysis further makes the\nmodel more robust and reliable.\n  Keywords: Pool boiling, Microchannels, Heat transfer coefficient, Correlation\nanalysis, Machine learning, Deep neural network, Physics-informed machine\nlearning aided framework, SHAP analysis\n", "link": "http://arxiv.org/abs/2501.16867v1", "date": "2025-01-28", "relevancy": 1.712, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4404}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4266}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20modeling%20and%20hybrid%20machine%20learning%20framework%20for%20nucleate%0A%20%20pool%20boiling%20on%20microchannel%20structured%20surfaces&body=Title%3A%20Empirical%20modeling%20and%20hybrid%20machine%20learning%20framework%20for%20nucleate%0A%20%20pool%20boiling%20on%20microchannel%20structured%20surfaces%0AAuthor%3A%20Vijay%20Kuberan%20and%20Sateesh%20Gedupudi%0AAbstract%3A%20%20%20Micro-structured%20surfaces%20influence%20nucleation%20characteristics%20and%20bubble%0Adynamics%20besides%20increasing%20the%20heat%20transfer%20surface%20area%2C%20thus%20enabling%0Aefficient%20nucleate%20boiling%20heat%20transfer.%20Modeling%20the%20pool%20boiling%20heat%0Atransfer%20characteristics%20of%20these%20surfaces%20under%20varied%20conditions%20is%20essential%0Ain%20diverse%20applications.%20A%20new%20empirical%20correlation%20for%20nucleate%20boiling%20on%0Amicrochannel%20structured%20surfaces%20has%20been%20proposed%20with%20the%20data%20collected%20from%0Avarious%20experiments%20in%20previous%20studies%20since%20the%20existing%20correlations%20are%0Alimited%20by%20their%20accuracy%20and%20narrow%20operating%20ranges.%20This%20study%20also%20examines%0Avarious%20Machine%20Learning%20%28ML%29%20algorithms%20and%20Deep%20Neural%20Networks%20%28DNN%29%20on%20the%0Amicrochannel%20structured%20surfaces%20dataset%20to%20predict%20the%20nucleate%20pool%20boiling%0AHeat%20Transfer%20Coefficient%20%28HTC%29.%20With%20the%20aim%20to%20integrate%20both%20the%20ML%20and%0Adomain%20knowledge%2C%20a%20Physics-Informed%20Machine%20Learning%20Aided%20Framework%20%28PIMLAF%29%0Ais%20proposed.%20The%20proposed%20correlation%20in%20this%20study%20is%20employed%20as%20the%20prior%0Aphysics-based%20model%20for%20PIMLAF%2C%20and%20a%20DNN%20is%20employed%20to%20model%20the%20residuals%20of%0Athe%20prior%20model.%20This%20hybrid%20framework%20achieved%20the%20best%20performance%20in%0Acomparison%20to%20the%20other%20ML%20models%20and%20DNNs.%20This%20framework%20is%20able%20to%0Ageneralize%20well%20for%20different%20datasets%20because%20the%20proposed%20correlation%0Aprovides%20the%20baseline%20knowledge%20of%20the%20boiling%20behavior.%20Also%2C%20SHAP%0Ainterpretation%20analysis%20identifies%20the%20critical%20parameters%20impacting%20the%20model%0Apredictions%20and%20their%20effect%20on%20HTC%20prediction.%20This%20analysis%20further%20makes%20the%0Amodel%20more%20robust%20and%20reliable.%0A%20%20Keywords%3A%20Pool%20boiling%2C%20Microchannels%2C%20Heat%20transfer%20coefficient%2C%20Correlation%0Aanalysis%2C%20Machine%20learning%2C%20Deep%20neural%20network%2C%20Physics-informed%20machine%0Alearning%20aided%20framework%2C%20SHAP%20analysis%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520modeling%2520and%2520hybrid%2520machine%2520learning%2520framework%2520for%2520nucleate%250A%2520%2520pool%2520boiling%2520on%2520microchannel%2520structured%2520surfaces%26entry.906535625%3DVijay%2520Kuberan%2520and%2520Sateesh%2520Gedupudi%26entry.1292438233%3D%2520%2520Micro-structured%2520surfaces%2520influence%2520nucleation%2520characteristics%2520and%2520bubble%250Adynamics%2520besides%2520increasing%2520the%2520heat%2520transfer%2520surface%2520area%252C%2520thus%2520enabling%250Aefficient%2520nucleate%2520boiling%2520heat%2520transfer.%2520Modeling%2520the%2520pool%2520boiling%2520heat%250Atransfer%2520characteristics%2520of%2520these%2520surfaces%2520under%2520varied%2520conditions%2520is%2520essential%250Ain%2520diverse%2520applications.%2520A%2520new%2520empirical%2520correlation%2520for%2520nucleate%2520boiling%2520on%250Amicrochannel%2520structured%2520surfaces%2520has%2520been%2520proposed%2520with%2520the%2520data%2520collected%2520from%250Avarious%2520experiments%2520in%2520previous%2520studies%2520since%2520the%2520existing%2520correlations%2520are%250Alimited%2520by%2520their%2520accuracy%2520and%2520narrow%2520operating%2520ranges.%2520This%2520study%2520also%2520examines%250Avarious%2520Machine%2520Learning%2520%2528ML%2529%2520algorithms%2520and%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529%2520on%2520the%250Amicrochannel%2520structured%2520surfaces%2520dataset%2520to%2520predict%2520the%2520nucleate%2520pool%2520boiling%250AHeat%2520Transfer%2520Coefficient%2520%2528HTC%2529.%2520With%2520the%2520aim%2520to%2520integrate%2520both%2520the%2520ML%2520and%250Adomain%2520knowledge%252C%2520a%2520Physics-Informed%2520Machine%2520Learning%2520Aided%2520Framework%2520%2528PIMLAF%2529%250Ais%2520proposed.%2520The%2520proposed%2520correlation%2520in%2520this%2520study%2520is%2520employed%2520as%2520the%2520prior%250Aphysics-based%2520model%2520for%2520PIMLAF%252C%2520and%2520a%2520DNN%2520is%2520employed%2520to%2520model%2520the%2520residuals%2520of%250Athe%2520prior%2520model.%2520This%2520hybrid%2520framework%2520achieved%2520the%2520best%2520performance%2520in%250Acomparison%2520to%2520the%2520other%2520ML%2520models%2520and%2520DNNs.%2520This%2520framework%2520is%2520able%2520to%250Ageneralize%2520well%2520for%2520different%2520datasets%2520because%2520the%2520proposed%2520correlation%250Aprovides%2520the%2520baseline%2520knowledge%2520of%2520the%2520boiling%2520behavior.%2520Also%252C%2520SHAP%250Ainterpretation%2520analysis%2520identifies%2520the%2520critical%2520parameters%2520impacting%2520the%2520model%250Apredictions%2520and%2520their%2520effect%2520on%2520HTC%2520prediction.%2520This%2520analysis%2520further%2520makes%2520the%250Amodel%2520more%2520robust%2520and%2520reliable.%250A%2520%2520Keywords%253A%2520Pool%2520boiling%252C%2520Microchannels%252C%2520Heat%2520transfer%2520coefficient%252C%2520Correlation%250Aanalysis%252C%2520Machine%2520learning%252C%2520Deep%2520neural%2520network%252C%2520Physics-informed%2520machine%250Alearning%2520aided%2520framework%252C%2520SHAP%2520analysis%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20modeling%20and%20hybrid%20machine%20learning%20framework%20for%20nucleate%0A%20%20pool%20boiling%20on%20microchannel%20structured%20surfaces&entry.906535625=Vijay%20Kuberan%20and%20Sateesh%20Gedupudi&entry.1292438233=%20%20Micro-structured%20surfaces%20influence%20nucleation%20characteristics%20and%20bubble%0Adynamics%20besides%20increasing%20the%20heat%20transfer%20surface%20area%2C%20thus%20enabling%0Aefficient%20nucleate%20boiling%20heat%20transfer.%20Modeling%20the%20pool%20boiling%20heat%0Atransfer%20characteristics%20of%20these%20surfaces%20under%20varied%20conditions%20is%20essential%0Ain%20diverse%20applications.%20A%20new%20empirical%20correlation%20for%20nucleate%20boiling%20on%0Amicrochannel%20structured%20surfaces%20has%20been%20proposed%20with%20the%20data%20collected%20from%0Avarious%20experiments%20in%20previous%20studies%20since%20the%20existing%20correlations%20are%0Alimited%20by%20their%20accuracy%20and%20narrow%20operating%20ranges.%20This%20study%20also%20examines%0Avarious%20Machine%20Learning%20%28ML%29%20algorithms%20and%20Deep%20Neural%20Networks%20%28DNN%29%20on%20the%0Amicrochannel%20structured%20surfaces%20dataset%20to%20predict%20the%20nucleate%20pool%20boiling%0AHeat%20Transfer%20Coefficient%20%28HTC%29.%20With%20the%20aim%20to%20integrate%20both%20the%20ML%20and%0Adomain%20knowledge%2C%20a%20Physics-Informed%20Machine%20Learning%20Aided%20Framework%20%28PIMLAF%29%0Ais%20proposed.%20The%20proposed%20correlation%20in%20this%20study%20is%20employed%20as%20the%20prior%0Aphysics-based%20model%20for%20PIMLAF%2C%20and%20a%20DNN%20is%20employed%20to%20model%20the%20residuals%20of%0Athe%20prior%20model.%20This%20hybrid%20framework%20achieved%20the%20best%20performance%20in%0Acomparison%20to%20the%20other%20ML%20models%20and%20DNNs.%20This%20framework%20is%20able%20to%0Ageneralize%20well%20for%20different%20datasets%20because%20the%20proposed%20correlation%0Aprovides%20the%20baseline%20knowledge%20of%20the%20boiling%20behavior.%20Also%2C%20SHAP%0Ainterpretation%20analysis%20identifies%20the%20critical%20parameters%20impacting%20the%20model%0Apredictions%20and%20their%20effect%20on%20HTC%20prediction.%20This%20analysis%20further%20makes%20the%0Amodel%20more%20robust%20and%20reliable.%0A%20%20Keywords%3A%20Pool%20boiling%2C%20Microchannels%2C%20Heat%20transfer%20coefficient%2C%20Correlation%0Aanalysis%2C%20Machine%20learning%2C%20Deep%20neural%20network%2C%20Physics-informed%20machine%0Alearning%20aided%20framework%2C%20SHAP%20analysis%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16867v1&entry.124074799=Read"},
{"title": "Histoires Morales: A French Dataset for Assessing Moral Alignment", "author": "Thibaud Leteno and Irina Proskurina and Antoine Gourru and Julien Velcin and Charlotte Laclau and Guillaume Metzler and Christophe Gravier", "abstract": "  Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.\n", "link": "http://arxiv.org/abs/2501.17117v1", "date": "2025-01-28", "relevancy": 1.6969, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histoires%20Morales%3A%20A%20French%20Dataset%20for%20Assessing%20Moral%20Alignment&body=Title%3A%20Histoires%20Morales%3A%20A%20French%20Dataset%20for%20Assessing%20Moral%20Alignment%0AAuthor%3A%20Thibaud%20Leteno%20and%20Irina%20Proskurina%20and%20Antoine%20Gourru%20and%20Julien%20Velcin%20and%20Charlotte%20Laclau%20and%20Guillaume%20Metzler%20and%20Christophe%20Gravier%0AAbstract%3A%20%20%20Aligning%20language%20models%20with%20human%20values%20is%20crucial%2C%20especially%20as%20they%0Abecome%20more%20integrated%20into%20everyday%20life.%20While%20models%20are%20often%20adapted%20to%0Auser%20preferences%2C%20it%20is%20equally%20important%20to%20ensure%20they%20align%20with%20moral%20norms%0Aand%20behaviours%20in%20real-world%20social%20situations.%20Despite%20significant%20progress%20in%0Alanguages%20like%20English%20and%20Chinese%2C%20French%20has%20seen%20little%20attention%20in%20this%0Aarea%2C%20leaving%20a%20gap%20in%20understanding%20how%20LLMs%20handle%20moral%20reasoning%20in%20this%0Alanguage.%20To%20address%20this%20gap%2C%20we%20introduce%20Histoires%20Morales%2C%20a%20French%20dataset%0Aderived%20from%20Moral%20Stories%2C%20created%20through%20translation%20and%20subsequently%0Arefined%20with%20the%20assistance%20of%20native%20speakers%20to%20guarantee%20grammatical%0Aaccuracy%20and%20adaptation%20to%20the%20French%20cultural%20context.%20We%20also%20rely%20on%0Aannotations%20of%20the%20moral%20values%20within%20the%20dataset%20to%20ensure%20their%20alignment%0Awith%20French%20norms.%20Histoires%20Morales%20covers%20a%20wide%20range%20of%20social%20situations%2C%0Aincluding%20differences%20in%20tipping%20practices%2C%20expressions%20of%20honesty%20in%0Arelationships%2C%20and%20responsibilities%20toward%20animals.%20To%20foster%20future%20research%2C%0Awe%20also%20conduct%20preliminary%20experiments%20on%20the%20alignment%20of%20multilingual%20models%0Aon%20French%20and%20English%20data%20and%20the%20robustness%20of%20the%20alignment.%20We%20find%20that%0Awhile%20LLMs%20are%20generally%20aligned%20with%20human%20moral%20norms%20by%20default%2C%20they%20can%20be%0Aeasily%20influenced%20with%20user-preference%20optimization%20for%20both%20moral%20and%20immoral%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistoires%2520Morales%253A%2520A%2520French%2520Dataset%2520for%2520Assessing%2520Moral%2520Alignment%26entry.906535625%3DThibaud%2520Leteno%2520and%2520Irina%2520Proskurina%2520and%2520Antoine%2520Gourru%2520and%2520Julien%2520Velcin%2520and%2520Charlotte%2520Laclau%2520and%2520Guillaume%2520Metzler%2520and%2520Christophe%2520Gravier%26entry.1292438233%3D%2520%2520Aligning%2520language%2520models%2520with%2520human%2520values%2520is%2520crucial%252C%2520especially%2520as%2520they%250Abecome%2520more%2520integrated%2520into%2520everyday%2520life.%2520While%2520models%2520are%2520often%2520adapted%2520to%250Auser%2520preferences%252C%2520it%2520is%2520equally%2520important%2520to%2520ensure%2520they%2520align%2520with%2520moral%2520norms%250Aand%2520behaviours%2520in%2520real-world%2520social%2520situations.%2520Despite%2520significant%2520progress%2520in%250Alanguages%2520like%2520English%2520and%2520Chinese%252C%2520French%2520has%2520seen%2520little%2520attention%2520in%2520this%250Aarea%252C%2520leaving%2520a%2520gap%2520in%2520understanding%2520how%2520LLMs%2520handle%2520moral%2520reasoning%2520in%2520this%250Alanguage.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Histoires%2520Morales%252C%2520a%2520French%2520dataset%250Aderived%2520from%2520Moral%2520Stories%252C%2520created%2520through%2520translation%2520and%2520subsequently%250Arefined%2520with%2520the%2520assistance%2520of%2520native%2520speakers%2520to%2520guarantee%2520grammatical%250Aaccuracy%2520and%2520adaptation%2520to%2520the%2520French%2520cultural%2520context.%2520We%2520also%2520rely%2520on%250Aannotations%2520of%2520the%2520moral%2520values%2520within%2520the%2520dataset%2520to%2520ensure%2520their%2520alignment%250Awith%2520French%2520norms.%2520Histoires%2520Morales%2520covers%2520a%2520wide%2520range%2520of%2520social%2520situations%252C%250Aincluding%2520differences%2520in%2520tipping%2520practices%252C%2520expressions%2520of%2520honesty%2520in%250Arelationships%252C%2520and%2520responsibilities%2520toward%2520animals.%2520To%2520foster%2520future%2520research%252C%250Awe%2520also%2520conduct%2520preliminary%2520experiments%2520on%2520the%2520alignment%2520of%2520multilingual%2520models%250Aon%2520French%2520and%2520English%2520data%2520and%2520the%2520robustness%2520of%2520the%2520alignment.%2520We%2520find%2520that%250Awhile%2520LLMs%2520are%2520generally%2520aligned%2520with%2520human%2520moral%2520norms%2520by%2520default%252C%2520they%2520can%2520be%250Aeasily%2520influenced%2520with%2520user-preference%2520optimization%2520for%2520both%2520moral%2520and%2520immoral%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histoires%20Morales%3A%20A%20French%20Dataset%20for%20Assessing%20Moral%20Alignment&entry.906535625=Thibaud%20Leteno%20and%20Irina%20Proskurina%20and%20Antoine%20Gourru%20and%20Julien%20Velcin%20and%20Charlotte%20Laclau%20and%20Guillaume%20Metzler%20and%20Christophe%20Gravier&entry.1292438233=%20%20Aligning%20language%20models%20with%20human%20values%20is%20crucial%2C%20especially%20as%20they%0Abecome%20more%20integrated%20into%20everyday%20life.%20While%20models%20are%20often%20adapted%20to%0Auser%20preferences%2C%20it%20is%20equally%20important%20to%20ensure%20they%20align%20with%20moral%20norms%0Aand%20behaviours%20in%20real-world%20social%20situations.%20Despite%20significant%20progress%20in%0Alanguages%20like%20English%20and%20Chinese%2C%20French%20has%20seen%20little%20attention%20in%20this%0Aarea%2C%20leaving%20a%20gap%20in%20understanding%20how%20LLMs%20handle%20moral%20reasoning%20in%20this%0Alanguage.%20To%20address%20this%20gap%2C%20we%20introduce%20Histoires%20Morales%2C%20a%20French%20dataset%0Aderived%20from%20Moral%20Stories%2C%20created%20through%20translation%20and%20subsequently%0Arefined%20with%20the%20assistance%20of%20native%20speakers%20to%20guarantee%20grammatical%0Aaccuracy%20and%20adaptation%20to%20the%20French%20cultural%20context.%20We%20also%20rely%20on%0Aannotations%20of%20the%20moral%20values%20within%20the%20dataset%20to%20ensure%20their%20alignment%0Awith%20French%20norms.%20Histoires%20Morales%20covers%20a%20wide%20range%20of%20social%20situations%2C%0Aincluding%20differences%20in%20tipping%20practices%2C%20expressions%20of%20honesty%20in%0Arelationships%2C%20and%20responsibilities%20toward%20animals.%20To%20foster%20future%20research%2C%0Awe%20also%20conduct%20preliminary%20experiments%20on%20the%20alignment%20of%20multilingual%20models%0Aon%20French%20and%20English%20data%20and%20the%20robustness%20of%20the%20alignment.%20We%20find%20that%0Awhile%20LLMs%20are%20generally%20aligned%20with%20human%20moral%20norms%20by%20default%2C%20they%20can%20be%0Aeasily%20influenced%20with%20user-preference%20optimization%20for%20both%20moral%20and%20immoral%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17117v1&entry.124074799=Read"},
{"title": "Mobile Manipulation Instruction Generation from Multiple Images with\n  Automatic Metric Enhancement", "author": "Kei Katsumata and Motonari Kambara and Daichi Yashima and Ryosuke Korekata and Komei Sugiura", "abstract": "  We consider the problem of generating free-form mobile manipulation\ninstructions based on a target object image and receptacle image. Conventional\nimage captioning models are not able to generate appropriate instructions\nbecause their architectures are typically optimized for single-image. In this\nstudy, we propose a model that handles both the target object and receptacle to\ngenerate free-form instruction sentences for mobile manipulation tasks.\nMoreover, we introduce a novel training method that effectively incorporates\nthe scores from both learning-based and n-gram based automatic evaluation\nmetrics as rewards. This method enables the model to learn the co-occurrence\nrelationships between words and appropriate paraphrases. Results demonstrate\nthat our proposed method outperforms baseline methods including representative\nmultimodal large language models on standard automatic evaluation metrics.\nMoreover, physical experiments reveal that using our method to augment data on\nlanguage instructions improves the performance of an existing multimodal\nlanguage understanding model for mobile manipulation.\n", "link": "http://arxiv.org/abs/2501.17022v1", "date": "2025-01-28", "relevancy": 1.6948, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5674}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5633}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mobile%20Manipulation%20Instruction%20Generation%20from%20Multiple%20Images%20with%0A%20%20Automatic%20Metric%20Enhancement&body=Title%3A%20Mobile%20Manipulation%20Instruction%20Generation%20from%20Multiple%20Images%20with%0A%20%20Automatic%20Metric%20Enhancement%0AAuthor%3A%20Kei%20Katsumata%20and%20Motonari%20Kambara%20and%20Daichi%20Yashima%20and%20Ryosuke%20Korekata%20and%20Komei%20Sugiura%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20generating%20free-form%20mobile%20manipulation%0Ainstructions%20based%20on%20a%20target%20object%20image%20and%20receptacle%20image.%20Conventional%0Aimage%20captioning%20models%20are%20not%20able%20to%20generate%20appropriate%20instructions%0Abecause%20their%20architectures%20are%20typically%20optimized%20for%20single-image.%20In%20this%0Astudy%2C%20we%20propose%20a%20model%20that%20handles%20both%20the%20target%20object%20and%20receptacle%20to%0Agenerate%20free-form%20instruction%20sentences%20for%20mobile%20manipulation%20tasks.%0AMoreover%2C%20we%20introduce%20a%20novel%20training%20method%20that%20effectively%20incorporates%0Athe%20scores%20from%20both%20learning-based%20and%20n-gram%20based%20automatic%20evaluation%0Ametrics%20as%20rewards.%20This%20method%20enables%20the%20model%20to%20learn%20the%20co-occurrence%0Arelationships%20between%20words%20and%20appropriate%20paraphrases.%20Results%20demonstrate%0Athat%20our%20proposed%20method%20outperforms%20baseline%20methods%20including%20representative%0Amultimodal%20large%20language%20models%20on%20standard%20automatic%20evaluation%20metrics.%0AMoreover%2C%20physical%20experiments%20reveal%20that%20using%20our%20method%20to%20augment%20data%20on%0Alanguage%20instructions%20improves%20the%20performance%20of%20an%20existing%20multimodal%0Alanguage%20understanding%20model%20for%20mobile%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobile%2520Manipulation%2520Instruction%2520Generation%2520from%2520Multiple%2520Images%2520with%250A%2520%2520Automatic%2520Metric%2520Enhancement%26entry.906535625%3DKei%2520Katsumata%2520and%2520Motonari%2520Kambara%2520and%2520Daichi%2520Yashima%2520and%2520Ryosuke%2520Korekata%2520and%2520Komei%2520Sugiura%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520generating%2520free-form%2520mobile%2520manipulation%250Ainstructions%2520based%2520on%2520a%2520target%2520object%2520image%2520and%2520receptacle%2520image.%2520Conventional%250Aimage%2520captioning%2520models%2520are%2520not%2520able%2520to%2520generate%2520appropriate%2520instructions%250Abecause%2520their%2520architectures%2520are%2520typically%2520optimized%2520for%2520single-image.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520a%2520model%2520that%2520handles%2520both%2520the%2520target%2520object%2520and%2520receptacle%2520to%250Agenerate%2520free-form%2520instruction%2520sentences%2520for%2520mobile%2520manipulation%2520tasks.%250AMoreover%252C%2520we%2520introduce%2520a%2520novel%2520training%2520method%2520that%2520effectively%2520incorporates%250Athe%2520scores%2520from%2520both%2520learning-based%2520and%2520n-gram%2520based%2520automatic%2520evaluation%250Ametrics%2520as%2520rewards.%2520This%2520method%2520enables%2520the%2520model%2520to%2520learn%2520the%2520co-occurrence%250Arelationships%2520between%2520words%2520and%2520appropriate%2520paraphrases.%2520Results%2520demonstrate%250Athat%2520our%2520proposed%2520method%2520outperforms%2520baseline%2520methods%2520including%2520representative%250Amultimodal%2520large%2520language%2520models%2520on%2520standard%2520automatic%2520evaluation%2520metrics.%250AMoreover%252C%2520physical%2520experiments%2520reveal%2520that%2520using%2520our%2520method%2520to%2520augment%2520data%2520on%250Alanguage%2520instructions%2520improves%2520the%2520performance%2520of%2520an%2520existing%2520multimodal%250Alanguage%2520understanding%2520model%2520for%2520mobile%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobile%20Manipulation%20Instruction%20Generation%20from%20Multiple%20Images%20with%0A%20%20Automatic%20Metric%20Enhancement&entry.906535625=Kei%20Katsumata%20and%20Motonari%20Kambara%20and%20Daichi%20Yashima%20and%20Ryosuke%20Korekata%20and%20Komei%20Sugiura&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20generating%20free-form%20mobile%20manipulation%0Ainstructions%20based%20on%20a%20target%20object%20image%20and%20receptacle%20image.%20Conventional%0Aimage%20captioning%20models%20are%20not%20able%20to%20generate%20appropriate%20instructions%0Abecause%20their%20architectures%20are%20typically%20optimized%20for%20single-image.%20In%20this%0Astudy%2C%20we%20propose%20a%20model%20that%20handles%20both%20the%20target%20object%20and%20receptacle%20to%0Agenerate%20free-form%20instruction%20sentences%20for%20mobile%20manipulation%20tasks.%0AMoreover%2C%20we%20introduce%20a%20novel%20training%20method%20that%20effectively%20incorporates%0Athe%20scores%20from%20both%20learning-based%20and%20n-gram%20based%20automatic%20evaluation%0Ametrics%20as%20rewards.%20This%20method%20enables%20the%20model%20to%20learn%20the%20co-occurrence%0Arelationships%20between%20words%20and%20appropriate%20paraphrases.%20Results%20demonstrate%0Athat%20our%20proposed%20method%20outperforms%20baseline%20methods%20including%20representative%0Amultimodal%20large%20language%20models%20on%20standard%20automatic%20evaluation%20metrics.%0AMoreover%2C%20physical%20experiments%20reveal%20that%20using%20our%20method%20to%20augment%20data%20on%0Alanguage%20instructions%20improves%20the%20performance%20of%20an%20existing%20multimodal%0Alanguage%20understanding%20model%20for%20mobile%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17022v1&entry.124074799=Read"},
{"title": "Autonomous Bootstrapping of Quantum Dot Devices", "author": "Anton Zubchenko and Danielle Middlebrooks and Torbj\u00f8rn Rasmussen and Lara Lausen and Ferdinand Kuemmeth and Anasua Chatterjee and Justyna P. Zwolak", "abstract": "  Semiconductor quantum dots (QDs) are a promising platform for multiple\ndifferent qubit implementations, all of which are voltage controlled by\nprogrammable gate electrodes. However, as the QD arrays grow in size and\ncomplexity, tuning procedures that can fully autonomously handle the increasing\nnumber of control parameters are becoming essential for enabling scalability.\nWe propose a bootstrapping algorithm for initializing a depletion-mode QD\ndevice in preparation for subsequent phases of tuning. During bootstrapping,\nthe QD device functionality is validated, all gates are characterized, and the\nQD charge sensor is made operational. We demonstrate the bootstrapping protocol\nin conjunction with a coarse-tuning module, showing that the combined algorithm\ncan efficiently and reliably take a cooled-down QD device to a desired\nglobal-state configuration in under 8 min with a success rate of 96 %. Finally,\nby following heuristic approaches to QD device initialization and combining the\nefficient ray-based measurement with the rapid radio-frequency reflectometry\nmeasurements, the proposed algorithm establishes a reference in terms of\nperformance, reliability, and efficiency against which alternative algorithms\ncan be benchmarked.\n", "link": "http://arxiv.org/abs/2407.20061v2", "date": "2025-01-28", "relevancy": 1.692, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4525}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4226}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Bootstrapping%20of%20Quantum%20Dot%20Devices&body=Title%3A%20Autonomous%20Bootstrapping%20of%20Quantum%20Dot%20Devices%0AAuthor%3A%20Anton%20Zubchenko%20and%20Danielle%20Middlebrooks%20and%20Torbj%C3%B8rn%20Rasmussen%20and%20Lara%20Lausen%20and%20Ferdinand%20Kuemmeth%20and%20Anasua%20Chatterjee%20and%20Justyna%20P.%20Zwolak%0AAbstract%3A%20%20%20Semiconductor%20quantum%20dots%20%28QDs%29%20are%20a%20promising%20platform%20for%20multiple%0Adifferent%20qubit%20implementations%2C%20all%20of%20which%20are%20voltage%20controlled%20by%0Aprogrammable%20gate%20electrodes.%20However%2C%20as%20the%20QD%20arrays%20grow%20in%20size%20and%0Acomplexity%2C%20tuning%20procedures%20that%20can%20fully%20autonomously%20handle%20the%20increasing%0Anumber%20of%20control%20parameters%20are%20becoming%20essential%20for%20enabling%20scalability.%0AWe%20propose%20a%20bootstrapping%20algorithm%20for%20initializing%20a%20depletion-mode%20QD%0Adevice%20in%20preparation%20for%20subsequent%20phases%20of%20tuning.%20During%20bootstrapping%2C%0Athe%20QD%20device%20functionality%20is%20validated%2C%20all%20gates%20are%20characterized%2C%20and%20the%0AQD%20charge%20sensor%20is%20made%20operational.%20We%20demonstrate%20the%20bootstrapping%20protocol%0Ain%20conjunction%20with%20a%20coarse-tuning%20module%2C%20showing%20that%20the%20combined%20algorithm%0Acan%20efficiently%20and%20reliably%20take%20a%20cooled-down%20QD%20device%20to%20a%20desired%0Aglobal-state%20configuration%20in%20under%208%20min%20with%20a%20success%20rate%20of%2096%20%25.%20Finally%2C%0Aby%20following%20heuristic%20approaches%20to%20QD%20device%20initialization%20and%20combining%20the%0Aefficient%20ray-based%20measurement%20with%20the%20rapid%20radio-frequency%20reflectometry%0Ameasurements%2C%20the%20proposed%20algorithm%20establishes%20a%20reference%20in%20terms%20of%0Aperformance%2C%20reliability%2C%20and%20efficiency%20against%20which%20alternative%20algorithms%0Acan%20be%20benchmarked.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Bootstrapping%2520of%2520Quantum%2520Dot%2520Devices%26entry.906535625%3DAnton%2520Zubchenko%2520and%2520Danielle%2520Middlebrooks%2520and%2520Torbj%25C3%25B8rn%2520Rasmussen%2520and%2520Lara%2520Lausen%2520and%2520Ferdinand%2520Kuemmeth%2520and%2520Anasua%2520Chatterjee%2520and%2520Justyna%2520P.%2520Zwolak%26entry.1292438233%3D%2520%2520Semiconductor%2520quantum%2520dots%2520%2528QDs%2529%2520are%2520a%2520promising%2520platform%2520for%2520multiple%250Adifferent%2520qubit%2520implementations%252C%2520all%2520of%2520which%2520are%2520voltage%2520controlled%2520by%250Aprogrammable%2520gate%2520electrodes.%2520However%252C%2520as%2520the%2520QD%2520arrays%2520grow%2520in%2520size%2520and%250Acomplexity%252C%2520tuning%2520procedures%2520that%2520can%2520fully%2520autonomously%2520handle%2520the%2520increasing%250Anumber%2520of%2520control%2520parameters%2520are%2520becoming%2520essential%2520for%2520enabling%2520scalability.%250AWe%2520propose%2520a%2520bootstrapping%2520algorithm%2520for%2520initializing%2520a%2520depletion-mode%2520QD%250Adevice%2520in%2520preparation%2520for%2520subsequent%2520phases%2520of%2520tuning.%2520During%2520bootstrapping%252C%250Athe%2520QD%2520device%2520functionality%2520is%2520validated%252C%2520all%2520gates%2520are%2520characterized%252C%2520and%2520the%250AQD%2520charge%2520sensor%2520is%2520made%2520operational.%2520We%2520demonstrate%2520the%2520bootstrapping%2520protocol%250Ain%2520conjunction%2520with%2520a%2520coarse-tuning%2520module%252C%2520showing%2520that%2520the%2520combined%2520algorithm%250Acan%2520efficiently%2520and%2520reliably%2520take%2520a%2520cooled-down%2520QD%2520device%2520to%2520a%2520desired%250Aglobal-state%2520configuration%2520in%2520under%25208%2520min%2520with%2520a%2520success%2520rate%2520of%252096%2520%2525.%2520Finally%252C%250Aby%2520following%2520heuristic%2520approaches%2520to%2520QD%2520device%2520initialization%2520and%2520combining%2520the%250Aefficient%2520ray-based%2520measurement%2520with%2520the%2520rapid%2520radio-frequency%2520reflectometry%250Ameasurements%252C%2520the%2520proposed%2520algorithm%2520establishes%2520a%2520reference%2520in%2520terms%2520of%250Aperformance%252C%2520reliability%252C%2520and%2520efficiency%2520against%2520which%2520alternative%2520algorithms%250Acan%2520be%2520benchmarked.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Bootstrapping%20of%20Quantum%20Dot%20Devices&entry.906535625=Anton%20Zubchenko%20and%20Danielle%20Middlebrooks%20and%20Torbj%C3%B8rn%20Rasmussen%20and%20Lara%20Lausen%20and%20Ferdinand%20Kuemmeth%20and%20Anasua%20Chatterjee%20and%20Justyna%20P.%20Zwolak&entry.1292438233=%20%20Semiconductor%20quantum%20dots%20%28QDs%29%20are%20a%20promising%20platform%20for%20multiple%0Adifferent%20qubit%20implementations%2C%20all%20of%20which%20are%20voltage%20controlled%20by%0Aprogrammable%20gate%20electrodes.%20However%2C%20as%20the%20QD%20arrays%20grow%20in%20size%20and%0Acomplexity%2C%20tuning%20procedures%20that%20can%20fully%20autonomously%20handle%20the%20increasing%0Anumber%20of%20control%20parameters%20are%20becoming%20essential%20for%20enabling%20scalability.%0AWe%20propose%20a%20bootstrapping%20algorithm%20for%20initializing%20a%20depletion-mode%20QD%0Adevice%20in%20preparation%20for%20subsequent%20phases%20of%20tuning.%20During%20bootstrapping%2C%0Athe%20QD%20device%20functionality%20is%20validated%2C%20all%20gates%20are%20characterized%2C%20and%20the%0AQD%20charge%20sensor%20is%20made%20operational.%20We%20demonstrate%20the%20bootstrapping%20protocol%0Ain%20conjunction%20with%20a%20coarse-tuning%20module%2C%20showing%20that%20the%20combined%20algorithm%0Acan%20efficiently%20and%20reliably%20take%20a%20cooled-down%20QD%20device%20to%20a%20desired%0Aglobal-state%20configuration%20in%20under%208%20min%20with%20a%20success%20rate%20of%2096%20%25.%20Finally%2C%0Aby%20following%20heuristic%20approaches%20to%20QD%20device%20initialization%20and%20combining%20the%0Aefficient%20ray-based%20measurement%20with%20the%20rapid%20radio-frequency%20reflectometry%0Ameasurements%2C%20the%20proposed%20algorithm%20establishes%20a%20reference%20in%20terms%20of%0Aperformance%2C%20reliability%2C%20and%20efficiency%20against%20which%20alternative%20algorithms%0Acan%20be%20benchmarked.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20061v2&entry.124074799=Read"},
{"title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models", "author": "Makoto Shing and Kou Misaki and Han Bao and Sho Yokoi and Takuya Akiba", "abstract": "  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n", "link": "http://arxiv.org/abs/2501.16937v1", "date": "2025-01-28", "relevancy": 1.6872, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5846}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5359}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAID%3A%20Temporally%20Adaptive%20Interpolated%20Distillation%20for%20Efficient%0A%20%20Knowledge%20Transfer%20in%20Language%20Models&body=Title%3A%20TAID%3A%20Temporally%20Adaptive%20Interpolated%20Distillation%20for%20Efficient%0A%20%20Knowledge%20Transfer%20in%20Language%20Models%0AAuthor%3A%20Makoto%20Shing%20and%20Kou%20Misaki%20and%20Han%20Bao%20and%20Sho%20Yokoi%20and%20Takuya%20Akiba%0AAbstract%3A%20%20%20Causal%20language%20models%20have%20demonstrated%20remarkable%20capabilities%2C%20but%20their%0Asize%20poses%20significant%20challenges%20for%20deployment%20in%20resource-constrained%0Aenvironments.%20Knowledge%20distillation%2C%20a%20widely-used%20technique%20for%20transferring%0Aknowledge%20from%20a%20large%20teacher%20model%20to%20a%20small%20student%20model%2C%20presents%20a%0Apromising%20approach%20for%20model%20compression.%20A%20significant%20remaining%20issue%20lies%20in%0Athe%20major%20differences%20between%20teacher%20and%20student%20models%2C%20namely%20the%0Asubstantial%20capacity%20gap%2C%20mode%20averaging%2C%20and%20mode%20collapse%2C%20which%20pose%0Abarriers%20during%20distillation.%20To%20address%20these%20issues%2C%20we%20introduce%0A%24%5Ctextit%7BTemporally%20Adaptive%20Interpolated%20Distillation%20%28TAID%29%7D%24%2C%20a%20novel%0Aknowledge%20distillation%20approach%20that%20dynamically%20interpolates%20student%20and%0Ateacher%20distributions%20through%20an%20adaptive%20intermediate%20distribution%2C%20gradually%0Ashifting%20from%20the%20student%27s%20initial%20distribution%20towards%20the%20teacher%27s%0Adistribution.%20We%20provide%20a%20theoretical%20analysis%20demonstrating%20TAID%27s%20ability%20to%0Aprevent%20mode%20collapse%20and%20empirically%20show%20its%20effectiveness%20in%20addressing%20the%0Acapacity%20gap%20while%20balancing%20mode%20averaging%20and%20mode%20collapse.%20Our%0Acomprehensive%20experiments%20demonstrate%20TAID%27s%20superior%20performance%20across%0Avarious%20model%20sizes%20and%20architectures%20in%20both%20instruction%20tuning%20and%0Apre-training%20scenarios.%20Furthermore%2C%20we%20showcase%20TAID%27s%20practical%20impact%20by%0Adeveloping%20two%20state-of-the-art%20compact%20foundation%20models%3A%0A%24%5Ctexttt%7BTAID-LLM-1.5B%7D%24%20for%20language%20tasks%20and%20%24%5Ctexttt%7BTAID-VLM-2B%7D%24%20for%0Avision-language%20tasks.%20These%20results%20demonstrate%20TAID%27s%20effectiveness%20in%0Acreating%20high-performing%20and%20efficient%20models%2C%20advancing%20the%20development%20of%0Amore%20accessible%20AI%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAID%253A%2520Temporally%2520Adaptive%2520Interpolated%2520Distillation%2520for%2520Efficient%250A%2520%2520Knowledge%2520Transfer%2520in%2520Language%2520Models%26entry.906535625%3DMakoto%2520Shing%2520and%2520Kou%2520Misaki%2520and%2520Han%2520Bao%2520and%2520Sho%2520Yokoi%2520and%2520Takuya%2520Akiba%26entry.1292438233%3D%2520%2520Causal%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%252C%2520but%2520their%250Asize%2520poses%2520significant%2520challenges%2520for%2520deployment%2520in%2520resource-constrained%250Aenvironments.%2520Knowledge%2520distillation%252C%2520a%2520widely-used%2520technique%2520for%2520transferring%250Aknowledge%2520from%2520a%2520large%2520teacher%2520model%2520to%2520a%2520small%2520student%2520model%252C%2520presents%2520a%250Apromising%2520approach%2520for%2520model%2520compression.%2520A%2520significant%2520remaining%2520issue%2520lies%2520in%250Athe%2520major%2520differences%2520between%2520teacher%2520and%2520student%2520models%252C%2520namely%2520the%250Asubstantial%2520capacity%2520gap%252C%2520mode%2520averaging%252C%2520and%2520mode%2520collapse%252C%2520which%2520pose%250Abarriers%2520during%2520distillation.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250A%2524%255Ctextit%257BTemporally%2520Adaptive%2520Interpolated%2520Distillation%2520%2528TAID%2529%257D%2524%252C%2520a%2520novel%250Aknowledge%2520distillation%2520approach%2520that%2520dynamically%2520interpolates%2520student%2520and%250Ateacher%2520distributions%2520through%2520an%2520adaptive%2520intermediate%2520distribution%252C%2520gradually%250Ashifting%2520from%2520the%2520student%2527s%2520initial%2520distribution%2520towards%2520the%2520teacher%2527s%250Adistribution.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520demonstrating%2520TAID%2527s%2520ability%2520to%250Aprevent%2520mode%2520collapse%2520and%2520empirically%2520show%2520its%2520effectiveness%2520in%2520addressing%2520the%250Acapacity%2520gap%2520while%2520balancing%2520mode%2520averaging%2520and%2520mode%2520collapse.%2520Our%250Acomprehensive%2520experiments%2520demonstrate%2520TAID%2527s%2520superior%2520performance%2520across%250Avarious%2520model%2520sizes%2520and%2520architectures%2520in%2520both%2520instruction%2520tuning%2520and%250Apre-training%2520scenarios.%2520Furthermore%252C%2520we%2520showcase%2520TAID%2527s%2520practical%2520impact%2520by%250Adeveloping%2520two%2520state-of-the-art%2520compact%2520foundation%2520models%253A%250A%2524%255Ctexttt%257BTAID-LLM-1.5B%257D%2524%2520for%2520language%2520tasks%2520and%2520%2524%255Ctexttt%257BTAID-VLM-2B%257D%2524%2520for%250Avision-language%2520tasks.%2520These%2520results%2520demonstrate%2520TAID%2527s%2520effectiveness%2520in%250Acreating%2520high-performing%2520and%2520efficient%2520models%252C%2520advancing%2520the%2520development%2520of%250Amore%2520accessible%2520AI%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAID%3A%20Temporally%20Adaptive%20Interpolated%20Distillation%20for%20Efficient%0A%20%20Knowledge%20Transfer%20in%20Language%20Models&entry.906535625=Makoto%20Shing%20and%20Kou%20Misaki%20and%20Han%20Bao%20and%20Sho%20Yokoi%20and%20Takuya%20Akiba&entry.1292438233=%20%20Causal%20language%20models%20have%20demonstrated%20remarkable%20capabilities%2C%20but%20their%0Asize%20poses%20significant%20challenges%20for%20deployment%20in%20resource-constrained%0Aenvironments.%20Knowledge%20distillation%2C%20a%20widely-used%20technique%20for%20transferring%0Aknowledge%20from%20a%20large%20teacher%20model%20to%20a%20small%20student%20model%2C%20presents%20a%0Apromising%20approach%20for%20model%20compression.%20A%20significant%20remaining%20issue%20lies%20in%0Athe%20major%20differences%20between%20teacher%20and%20student%20models%2C%20namely%20the%0Asubstantial%20capacity%20gap%2C%20mode%20averaging%2C%20and%20mode%20collapse%2C%20which%20pose%0Abarriers%20during%20distillation.%20To%20address%20these%20issues%2C%20we%20introduce%0A%24%5Ctextit%7BTemporally%20Adaptive%20Interpolated%20Distillation%20%28TAID%29%7D%24%2C%20a%20novel%0Aknowledge%20distillation%20approach%20that%20dynamically%20interpolates%20student%20and%0Ateacher%20distributions%20through%20an%20adaptive%20intermediate%20distribution%2C%20gradually%0Ashifting%20from%20the%20student%27s%20initial%20distribution%20towards%20the%20teacher%27s%0Adistribution.%20We%20provide%20a%20theoretical%20analysis%20demonstrating%20TAID%27s%20ability%20to%0Aprevent%20mode%20collapse%20and%20empirically%20show%20its%20effectiveness%20in%20addressing%20the%0Acapacity%20gap%20while%20balancing%20mode%20averaging%20and%20mode%20collapse.%20Our%0Acomprehensive%20experiments%20demonstrate%20TAID%27s%20superior%20performance%20across%0Avarious%20model%20sizes%20and%20architectures%20in%20both%20instruction%20tuning%20and%0Apre-training%20scenarios.%20Furthermore%2C%20we%20showcase%20TAID%27s%20practical%20impact%20by%0Adeveloping%20two%20state-of-the-art%20compact%20foundation%20models%3A%0A%24%5Ctexttt%7BTAID-LLM-1.5B%7D%24%20for%20language%20tasks%20and%20%24%5Ctexttt%7BTAID-VLM-2B%7D%24%20for%0Avision-language%20tasks.%20These%20results%20demonstrate%20TAID%27s%20effectiveness%20in%0Acreating%20high-performing%20and%20efficient%20models%2C%20advancing%20the%20development%20of%0Amore%20accessible%20AI%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16937v1&entry.124074799=Read"},
{"title": "HD-CB: The First Exploration of Hyperdimensional Computing for\n  Contextual Bandits Problems", "author": "Marco Angioli and Antonello Rosato and Marcello Barbirotta and Rocco Martino and Francesco Menichelli and Mauro Olivieri", "abstract": "  Hyperdimensional Computing (HDC), also known as Vector Symbolic\nArchitectures, is a computing paradigm that combines the strengths of symbolic\nreasoning with the efficiency and scalability of distributed connectionist\nmodels in artificial intelligence. HDC has recently emerged as a promising\nalternative for performing learning tasks in resource-constrained environments\nthanks to its energy and computational efficiency, inherent parallelism, and\nresilience to noise and hardware faults.\n  This work introduces the Hyperdimensional Contextual Bandits (HD-CB): the\nfirst exploration of HDC to model and automate sequential decision-making\nContextual Bandits (CB) problems. The proposed approach maps environmental\nstates in a high-dimensional space and represents each action with dedicated\nhypervectors (HVs). At each iteration, these HVs are used to select the optimal\naction for the given context and are updated based on the received reward,\nreplacing computationally expensive ridge regression procedures required by\ntraditional linear CB algorithms with simple, highly parallel vector\noperations. We propose four HD-CB variants, demonstrating their flexibility in\nimplementing different exploration strategies, as well as techniques to reduce\nmemory overhead and the number of hyperparameters. Extensive simulations on\nsynthetic datasets and a real-world benchmark reveal that HD-CB consistently\nachieves competitive or superior performance compared to traditional linear CB\nalgorithms, while offering faster convergence time, lower computational\ncomplexity, improved scalability, and high parallelism.\n", "link": "http://arxiv.org/abs/2501.16863v1", "date": "2025-01-28", "relevancy": 1.0107, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HD-CB%3A%20The%20First%20Exploration%20of%20Hyperdimensional%20Computing%20for%0A%20%20Contextual%20Bandits%20Problems&body=Title%3A%20HD-CB%3A%20The%20First%20Exploration%20of%20Hyperdimensional%20Computing%20for%0A%20%20Contextual%20Bandits%20Problems%0AAuthor%3A%20Marco%20Angioli%20and%20Antonello%20Rosato%20and%20Marcello%20Barbirotta%20and%20Rocco%20Martino%20and%20Francesco%20Menichelli%20and%20Mauro%20Olivieri%0AAbstract%3A%20%20%20Hyperdimensional%20Computing%20%28HDC%29%2C%20also%20known%20as%20Vector%20Symbolic%0AArchitectures%2C%20is%20a%20computing%20paradigm%20that%20combines%20the%20strengths%20of%20symbolic%0Areasoning%20with%20the%20efficiency%20and%20scalability%20of%20distributed%20connectionist%0Amodels%20in%20artificial%20intelligence.%20HDC%20has%20recently%20emerged%20as%20a%20promising%0Aalternative%20for%20performing%20learning%20tasks%20in%20resource-constrained%20environments%0Athanks%20to%20its%20energy%20and%20computational%20efficiency%2C%20inherent%20parallelism%2C%20and%0Aresilience%20to%20noise%20and%20hardware%20faults.%0A%20%20This%20work%20introduces%20the%20Hyperdimensional%20Contextual%20Bandits%20%28HD-CB%29%3A%20the%0Afirst%20exploration%20of%20HDC%20to%20model%20and%20automate%20sequential%20decision-making%0AContextual%20Bandits%20%28CB%29%20problems.%20The%20proposed%20approach%20maps%20environmental%0Astates%20in%20a%20high-dimensional%20space%20and%20represents%20each%20action%20with%20dedicated%0Ahypervectors%20%28HVs%29.%20At%20each%20iteration%2C%20these%20HVs%20are%20used%20to%20select%20the%20optimal%0Aaction%20for%20the%20given%20context%20and%20are%20updated%20based%20on%20the%20received%20reward%2C%0Areplacing%20computationally%20expensive%20ridge%20regression%20procedures%20required%20by%0Atraditional%20linear%20CB%20algorithms%20with%20simple%2C%20highly%20parallel%20vector%0Aoperations.%20We%20propose%20four%20HD-CB%20variants%2C%20demonstrating%20their%20flexibility%20in%0Aimplementing%20different%20exploration%20strategies%2C%20as%20well%20as%20techniques%20to%20reduce%0Amemory%20overhead%20and%20the%20number%20of%20hyperparameters.%20Extensive%20simulations%20on%0Asynthetic%20datasets%20and%20a%20real-world%20benchmark%20reveal%20that%20HD-CB%20consistently%0Aachieves%20competitive%20or%20superior%20performance%20compared%20to%20traditional%20linear%20CB%0Aalgorithms%2C%20while%20offering%20faster%20convergence%20time%2C%20lower%20computational%0Acomplexity%2C%20improved%20scalability%2C%20and%20high%20parallelism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHD-CB%253A%2520The%2520First%2520Exploration%2520of%2520Hyperdimensional%2520Computing%2520for%250A%2520%2520Contextual%2520Bandits%2520Problems%26entry.906535625%3DMarco%2520Angioli%2520and%2520Antonello%2520Rosato%2520and%2520Marcello%2520Barbirotta%2520and%2520Rocco%2520Martino%2520and%2520Francesco%2520Menichelli%2520and%2520Mauro%2520Olivieri%26entry.1292438233%3D%2520%2520Hyperdimensional%2520Computing%2520%2528HDC%2529%252C%2520also%2520known%2520as%2520Vector%2520Symbolic%250AArchitectures%252C%2520is%2520a%2520computing%2520paradigm%2520that%2520combines%2520the%2520strengths%2520of%2520symbolic%250Areasoning%2520with%2520the%2520efficiency%2520and%2520scalability%2520of%2520distributed%2520connectionist%250Amodels%2520in%2520artificial%2520intelligence.%2520HDC%2520has%2520recently%2520emerged%2520as%2520a%2520promising%250Aalternative%2520for%2520performing%2520learning%2520tasks%2520in%2520resource-constrained%2520environments%250Athanks%2520to%2520its%2520energy%2520and%2520computational%2520efficiency%252C%2520inherent%2520parallelism%252C%2520and%250Aresilience%2520to%2520noise%2520and%2520hardware%2520faults.%250A%2520%2520This%2520work%2520introduces%2520the%2520Hyperdimensional%2520Contextual%2520Bandits%2520%2528HD-CB%2529%253A%2520the%250Afirst%2520exploration%2520of%2520HDC%2520to%2520model%2520and%2520automate%2520sequential%2520decision-making%250AContextual%2520Bandits%2520%2528CB%2529%2520problems.%2520The%2520proposed%2520approach%2520maps%2520environmental%250Astates%2520in%2520a%2520high-dimensional%2520space%2520and%2520represents%2520each%2520action%2520with%2520dedicated%250Ahypervectors%2520%2528HVs%2529.%2520At%2520each%2520iteration%252C%2520these%2520HVs%2520are%2520used%2520to%2520select%2520the%2520optimal%250Aaction%2520for%2520the%2520given%2520context%2520and%2520are%2520updated%2520based%2520on%2520the%2520received%2520reward%252C%250Areplacing%2520computationally%2520expensive%2520ridge%2520regression%2520procedures%2520required%2520by%250Atraditional%2520linear%2520CB%2520algorithms%2520with%2520simple%252C%2520highly%2520parallel%2520vector%250Aoperations.%2520We%2520propose%2520four%2520HD-CB%2520variants%252C%2520demonstrating%2520their%2520flexibility%2520in%250Aimplementing%2520different%2520exploration%2520strategies%252C%2520as%2520well%2520as%2520techniques%2520to%2520reduce%250Amemory%2520overhead%2520and%2520the%2520number%2520of%2520hyperparameters.%2520Extensive%2520simulations%2520on%250Asynthetic%2520datasets%2520and%2520a%2520real-world%2520benchmark%2520reveal%2520that%2520HD-CB%2520consistently%250Aachieves%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%2520traditional%2520linear%2520CB%250Aalgorithms%252C%2520while%2520offering%2520faster%2520convergence%2520time%252C%2520lower%2520computational%250Acomplexity%252C%2520improved%2520scalability%252C%2520and%2520high%2520parallelism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HD-CB%3A%20The%20First%20Exploration%20of%20Hyperdimensional%20Computing%20for%0A%20%20Contextual%20Bandits%20Problems&entry.906535625=Marco%20Angioli%20and%20Antonello%20Rosato%20and%20Marcello%20Barbirotta%20and%20Rocco%20Martino%20and%20Francesco%20Menichelli%20and%20Mauro%20Olivieri&entry.1292438233=%20%20Hyperdimensional%20Computing%20%28HDC%29%2C%20also%20known%20as%20Vector%20Symbolic%0AArchitectures%2C%20is%20a%20computing%20paradigm%20that%20combines%20the%20strengths%20of%20symbolic%0Areasoning%20with%20the%20efficiency%20and%20scalability%20of%20distributed%20connectionist%0Amodels%20in%20artificial%20intelligence.%20HDC%20has%20recently%20emerged%20as%20a%20promising%0Aalternative%20for%20performing%20learning%20tasks%20in%20resource-constrained%20environments%0Athanks%20to%20its%20energy%20and%20computational%20efficiency%2C%20inherent%20parallelism%2C%20and%0Aresilience%20to%20noise%20and%20hardware%20faults.%0A%20%20This%20work%20introduces%20the%20Hyperdimensional%20Contextual%20Bandits%20%28HD-CB%29%3A%20the%0Afirst%20exploration%20of%20HDC%20to%20model%20and%20automate%20sequential%20decision-making%0AContextual%20Bandits%20%28CB%29%20problems.%20The%20proposed%20approach%20maps%20environmental%0Astates%20in%20a%20high-dimensional%20space%20and%20represents%20each%20action%20with%20dedicated%0Ahypervectors%20%28HVs%29.%20At%20each%20iteration%2C%20these%20HVs%20are%20used%20to%20select%20the%20optimal%0Aaction%20for%20the%20given%20context%20and%20are%20updated%20based%20on%20the%20received%20reward%2C%0Areplacing%20computationally%20expensive%20ridge%20regression%20procedures%20required%20by%0Atraditional%20linear%20CB%20algorithms%20with%20simple%2C%20highly%20parallel%20vector%0Aoperations.%20We%20propose%20four%20HD-CB%20variants%2C%20demonstrating%20their%20flexibility%20in%0Aimplementing%20different%20exploration%20strategies%2C%20as%20well%20as%20techniques%20to%20reduce%0Amemory%20overhead%20and%20the%20number%20of%20hyperparameters.%20Extensive%20simulations%20on%0Asynthetic%20datasets%20and%20a%20real-world%20benchmark%20reveal%20that%20HD-CB%20consistently%0Aachieves%20competitive%20or%20superior%20performance%20compared%20to%20traditional%20linear%20CB%0Aalgorithms%2C%20while%20offering%20faster%20convergence%20time%2C%20lower%20computational%0Acomplexity%2C%20improved%20scalability%2C%20and%20high%20parallelism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16863v1&entry.124074799=Read"},
{"title": "Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy\n  Measures", "author": "Marc Schmitt and Pantelis Koutroumpis", "abstract": "  The digital age, driven by the AI revolution, brings significant\nopportunities but also conceals security threats, which we refer to as cyber\nshadows. These threats pose risks at individual, organizational, and societal\nlevels. This paper examines the systemic impact of these cyber threats and\nproposes a comprehensive cybersecurity strategy that integrates AI-driven\nsolutions, such as Intrusion Detection Systems (IDS), with targeted policy\ninterventions. By combining technological and regulatory measures, we create a\nmultilevel defense capable of addressing both direct threats and indirect\nnegative externalities. We emphasize that the synergy between AI-driven\nsolutions and policy interventions is essential for neutralizing cyber threats\nand mitigating their negative impact on the digital economy. Finally, we\nunderscore the need for continuous adaptation of these strategies, especially\nin response to the rapid advancement of autonomous AI-driven attacks, to ensure\nthe creation of secure and resilient digital ecosystems.\n", "link": "http://arxiv.org/abs/2501.09025v2", "date": "2025-01-28", "relevancy": 1.1653, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4046}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3883}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cyber%20Shadows%3A%20Neutralizing%20Security%20Threats%20with%20AI%20and%20Targeted%20Policy%0A%20%20Measures&body=Title%3A%20Cyber%20Shadows%3A%20Neutralizing%20Security%20Threats%20with%20AI%20and%20Targeted%20Policy%0A%20%20Measures%0AAuthor%3A%20Marc%20Schmitt%20and%20Pantelis%20Koutroumpis%0AAbstract%3A%20%20%20The%20digital%20age%2C%20driven%20by%20the%20AI%20revolution%2C%20brings%20significant%0Aopportunities%20but%20also%20conceals%20security%20threats%2C%20which%20we%20refer%20to%20as%20cyber%0Ashadows.%20These%20threats%20pose%20risks%20at%20individual%2C%20organizational%2C%20and%20societal%0Alevels.%20This%20paper%20examines%20the%20systemic%20impact%20of%20these%20cyber%20threats%20and%0Aproposes%20a%20comprehensive%20cybersecurity%20strategy%20that%20integrates%20AI-driven%0Asolutions%2C%20such%20as%20Intrusion%20Detection%20Systems%20%28IDS%29%2C%20with%20targeted%20policy%0Ainterventions.%20By%20combining%20technological%20and%20regulatory%20measures%2C%20we%20create%20a%0Amultilevel%20defense%20capable%20of%20addressing%20both%20direct%20threats%20and%20indirect%0Anegative%20externalities.%20We%20emphasize%20that%20the%20synergy%20between%20AI-driven%0Asolutions%20and%20policy%20interventions%20is%20essential%20for%20neutralizing%20cyber%20threats%0Aand%20mitigating%20their%20negative%20impact%20on%20the%20digital%20economy.%20Finally%2C%20we%0Aunderscore%20the%20need%20for%20continuous%20adaptation%20of%20these%20strategies%2C%20especially%0Ain%20response%20to%20the%20rapid%20advancement%20of%20autonomous%20AI-driven%20attacks%2C%20to%20ensure%0Athe%20creation%20of%20secure%20and%20resilient%20digital%20ecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyber%2520Shadows%253A%2520Neutralizing%2520Security%2520Threats%2520with%2520AI%2520and%2520Targeted%2520Policy%250A%2520%2520Measures%26entry.906535625%3DMarc%2520Schmitt%2520and%2520Pantelis%2520Koutroumpis%26entry.1292438233%3D%2520%2520The%2520digital%2520age%252C%2520driven%2520by%2520the%2520AI%2520revolution%252C%2520brings%2520significant%250Aopportunities%2520but%2520also%2520conceals%2520security%2520threats%252C%2520which%2520we%2520refer%2520to%2520as%2520cyber%250Ashadows.%2520These%2520threats%2520pose%2520risks%2520at%2520individual%252C%2520organizational%252C%2520and%2520societal%250Alevels.%2520This%2520paper%2520examines%2520the%2520systemic%2520impact%2520of%2520these%2520cyber%2520threats%2520and%250Aproposes%2520a%2520comprehensive%2520cybersecurity%2520strategy%2520that%2520integrates%2520AI-driven%250Asolutions%252C%2520such%2520as%2520Intrusion%2520Detection%2520Systems%2520%2528IDS%2529%252C%2520with%2520targeted%2520policy%250Ainterventions.%2520By%2520combining%2520technological%2520and%2520regulatory%2520measures%252C%2520we%2520create%2520a%250Amultilevel%2520defense%2520capable%2520of%2520addressing%2520both%2520direct%2520threats%2520and%2520indirect%250Anegative%2520externalities.%2520We%2520emphasize%2520that%2520the%2520synergy%2520between%2520AI-driven%250Asolutions%2520and%2520policy%2520interventions%2520is%2520essential%2520for%2520neutralizing%2520cyber%2520threats%250Aand%2520mitigating%2520their%2520negative%2520impact%2520on%2520the%2520digital%2520economy.%2520Finally%252C%2520we%250Aunderscore%2520the%2520need%2520for%2520continuous%2520adaptation%2520of%2520these%2520strategies%252C%2520especially%250Ain%2520response%2520to%2520the%2520rapid%2520advancement%2520of%2520autonomous%2520AI-driven%2520attacks%252C%2520to%2520ensure%250Athe%2520creation%2520of%2520secure%2520and%2520resilient%2520digital%2520ecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyber%20Shadows%3A%20Neutralizing%20Security%20Threats%20with%20AI%20and%20Targeted%20Policy%0A%20%20Measures&entry.906535625=Marc%20Schmitt%20and%20Pantelis%20Koutroumpis&entry.1292438233=%20%20The%20digital%20age%2C%20driven%20by%20the%20AI%20revolution%2C%20brings%20significant%0Aopportunities%20but%20also%20conceals%20security%20threats%2C%20which%20we%20refer%20to%20as%20cyber%0Ashadows.%20These%20threats%20pose%20risks%20at%20individual%2C%20organizational%2C%20and%20societal%0Alevels.%20This%20paper%20examines%20the%20systemic%20impact%20of%20these%20cyber%20threats%20and%0Aproposes%20a%20comprehensive%20cybersecurity%20strategy%20that%20integrates%20AI-driven%0Asolutions%2C%20such%20as%20Intrusion%20Detection%20Systems%20%28IDS%29%2C%20with%20targeted%20policy%0Ainterventions.%20By%20combining%20technological%20and%20regulatory%20measures%2C%20we%20create%20a%0Amultilevel%20defense%20capable%20of%20addressing%20both%20direct%20threats%20and%20indirect%0Anegative%20externalities.%20We%20emphasize%20that%20the%20synergy%20between%20AI-driven%0Asolutions%20and%20policy%20interventions%20is%20essential%20for%20neutralizing%20cyber%20threats%0Aand%20mitigating%20their%20negative%20impact%20on%20the%20digital%20economy.%20Finally%2C%20we%0Aunderscore%20the%20need%20for%20continuous%20adaptation%20of%20these%20strategies%2C%20especially%0Ain%20response%20to%20the%20rapid%20advancement%20of%20autonomous%20AI-driven%20attacks%2C%20to%20ensure%0Athe%20creation%20of%20secure%20and%20resilient%20digital%20ecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09025v2&entry.124074799=Read"},
{"title": "Enhancing Web Service Anomaly Detection via Fine-grained Multi-modal\n  Association and Frequency Domain Analysis", "author": "Xixuan Yang and Xin Huang and Chiming Duan and Tong Jia and Shandong Dong and Ying Li and Gang Huang", "abstract": "  Anomaly detection is crucial for ensuring the stability and reliability of\nweb service systems. Logs and metrics contain multiple information that can\nreflect the system's operational state and potential anomalies. Thus, existing\nanomaly detection methods use logs and metrics to detect web service systems'\nanomalies through data fusion approaches. They associate logs and metrics using\ncoarse-grained time window alignment and capture the normal patterns of system\noperation through reconstruction. However, these methods have two issues that\nlimit their performance in anomaly detection. First, due to asynchrony between\nlogs and metrics, coarse-grained time window alignment cannot achieve a precise\nassociation between the two modalities. Second, reconstruction-based methods\nsuffer from severe overgeneralization problems, resulting in anomalies being\naccurately reconstructed. In this paper, we propose a novel anomaly detection\nmethod named FFAD to address these two issues. On the one hand, FFAD employs\ngraph-based alignment to mine and extract associations between the modalities\nfrom the constructed log-metric relation graph, achieving precise associations\nbetween logs and metrics. On the other hand, we improve the model's fit to\nnormal data distributions through Fourier Frequency Focus, thereby enhancing\nthe effectiveness of anomaly detection. We validated the effectiveness of our\nmodel on two real-world industrial datasets and one open-source dataset. The\nresults show that our method achieves an average anomaly detection F1-score of\n93.6%, representing an 8.8% improvement over previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2501.16875v1", "date": "2025-01-28", "relevancy": 1.4457, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4876}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4761}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Web%20Service%20Anomaly%20Detection%20via%20Fine-grained%20Multi-modal%0A%20%20Association%20and%20Frequency%20Domain%20Analysis&body=Title%3A%20Enhancing%20Web%20Service%20Anomaly%20Detection%20via%20Fine-grained%20Multi-modal%0A%20%20Association%20and%20Frequency%20Domain%20Analysis%0AAuthor%3A%20Xixuan%20Yang%20and%20Xin%20Huang%20and%20Chiming%20Duan%20and%20Tong%20Jia%20and%20Shandong%20Dong%20and%20Ying%20Li%20and%20Gang%20Huang%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20crucial%20for%20ensuring%20the%20stability%20and%20reliability%20of%0Aweb%20service%20systems.%20Logs%20and%20metrics%20contain%20multiple%20information%20that%20can%0Areflect%20the%20system%27s%20operational%20state%20and%20potential%20anomalies.%20Thus%2C%20existing%0Aanomaly%20detection%20methods%20use%20logs%20and%20metrics%20to%20detect%20web%20service%20systems%27%0Aanomalies%20through%20data%20fusion%20approaches.%20They%20associate%20logs%20and%20metrics%20using%0Acoarse-grained%20time%20window%20alignment%20and%20capture%20the%20normal%20patterns%20of%20system%0Aoperation%20through%20reconstruction.%20However%2C%20these%20methods%20have%20two%20issues%20that%0Alimit%20their%20performance%20in%20anomaly%20detection.%20First%2C%20due%20to%20asynchrony%20between%0Alogs%20and%20metrics%2C%20coarse-grained%20time%20window%20alignment%20cannot%20achieve%20a%20precise%0Aassociation%20between%20the%20two%20modalities.%20Second%2C%20reconstruction-based%20methods%0Asuffer%20from%20severe%20overgeneralization%20problems%2C%20resulting%20in%20anomalies%20being%0Aaccurately%20reconstructed.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20anomaly%20detection%0Amethod%20named%20FFAD%20to%20address%20these%20two%20issues.%20On%20the%20one%20hand%2C%20FFAD%20employs%0Agraph-based%20alignment%20to%20mine%20and%20extract%20associations%20between%20the%20modalities%0Afrom%20the%20constructed%20log-metric%20relation%20graph%2C%20achieving%20precise%20associations%0Abetween%20logs%20and%20metrics.%20On%20the%20other%20hand%2C%20we%20improve%20the%20model%27s%20fit%20to%0Anormal%20data%20distributions%20through%20Fourier%20Frequency%20Focus%2C%20thereby%20enhancing%0Athe%20effectiveness%20of%20anomaly%20detection.%20We%20validated%20the%20effectiveness%20of%20our%0Amodel%20on%20two%20real-world%20industrial%20datasets%20and%20one%20open-source%20dataset.%20The%0Aresults%20show%20that%20our%20method%20achieves%20an%20average%20anomaly%20detection%20F1-score%20of%0A93.6%25%2C%20representing%20an%208.8%25%20improvement%20over%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Web%2520Service%2520Anomaly%2520Detection%2520via%2520Fine-grained%2520Multi-modal%250A%2520%2520Association%2520and%2520Frequency%2520Domain%2520Analysis%26entry.906535625%3DXixuan%2520Yang%2520and%2520Xin%2520Huang%2520and%2520Chiming%2520Duan%2520and%2520Tong%2520Jia%2520and%2520Shandong%2520Dong%2520and%2520Ying%2520Li%2520and%2520Gang%2520Huang%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520crucial%2520for%2520ensuring%2520the%2520stability%2520and%2520reliability%2520of%250Aweb%2520service%2520systems.%2520Logs%2520and%2520metrics%2520contain%2520multiple%2520information%2520that%2520can%250Areflect%2520the%2520system%2527s%2520operational%2520state%2520and%2520potential%2520anomalies.%2520Thus%252C%2520existing%250Aanomaly%2520detection%2520methods%2520use%2520logs%2520and%2520metrics%2520to%2520detect%2520web%2520service%2520systems%2527%250Aanomalies%2520through%2520data%2520fusion%2520approaches.%2520They%2520associate%2520logs%2520and%2520metrics%2520using%250Acoarse-grained%2520time%2520window%2520alignment%2520and%2520capture%2520the%2520normal%2520patterns%2520of%2520system%250Aoperation%2520through%2520reconstruction.%2520However%252C%2520these%2520methods%2520have%2520two%2520issues%2520that%250Alimit%2520their%2520performance%2520in%2520anomaly%2520detection.%2520First%252C%2520due%2520to%2520asynchrony%2520between%250Alogs%2520and%2520metrics%252C%2520coarse-grained%2520time%2520window%2520alignment%2520cannot%2520achieve%2520a%2520precise%250Aassociation%2520between%2520the%2520two%2520modalities.%2520Second%252C%2520reconstruction-based%2520methods%250Asuffer%2520from%2520severe%2520overgeneralization%2520problems%252C%2520resulting%2520in%2520anomalies%2520being%250Aaccurately%2520reconstructed.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520anomaly%2520detection%250Amethod%2520named%2520FFAD%2520to%2520address%2520these%2520two%2520issues.%2520On%2520the%2520one%2520hand%252C%2520FFAD%2520employs%250Agraph-based%2520alignment%2520to%2520mine%2520and%2520extract%2520associations%2520between%2520the%2520modalities%250Afrom%2520the%2520constructed%2520log-metric%2520relation%2520graph%252C%2520achieving%2520precise%2520associations%250Abetween%2520logs%2520and%2520metrics.%2520On%2520the%2520other%2520hand%252C%2520we%2520improve%2520the%2520model%2527s%2520fit%2520to%250Anormal%2520data%2520distributions%2520through%2520Fourier%2520Frequency%2520Focus%252C%2520thereby%2520enhancing%250Athe%2520effectiveness%2520of%2520anomaly%2520detection.%2520We%2520validated%2520the%2520effectiveness%2520of%2520our%250Amodel%2520on%2520two%2520real-world%2520industrial%2520datasets%2520and%2520one%2520open-source%2520dataset.%2520The%250Aresults%2520show%2520that%2520our%2520method%2520achieves%2520an%2520average%2520anomaly%2520detection%2520F1-score%2520of%250A93.6%2525%252C%2520representing%2520an%25208.8%2525%2520improvement%2520over%2520previous%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Web%20Service%20Anomaly%20Detection%20via%20Fine-grained%20Multi-modal%0A%20%20Association%20and%20Frequency%20Domain%20Analysis&entry.906535625=Xixuan%20Yang%20and%20Xin%20Huang%20and%20Chiming%20Duan%20and%20Tong%20Jia%20and%20Shandong%20Dong%20and%20Ying%20Li%20and%20Gang%20Huang&entry.1292438233=%20%20Anomaly%20detection%20is%20crucial%20for%20ensuring%20the%20stability%20and%20reliability%20of%0Aweb%20service%20systems.%20Logs%20and%20metrics%20contain%20multiple%20information%20that%20can%0Areflect%20the%20system%27s%20operational%20state%20and%20potential%20anomalies.%20Thus%2C%20existing%0Aanomaly%20detection%20methods%20use%20logs%20and%20metrics%20to%20detect%20web%20service%20systems%27%0Aanomalies%20through%20data%20fusion%20approaches.%20They%20associate%20logs%20and%20metrics%20using%0Acoarse-grained%20time%20window%20alignment%20and%20capture%20the%20normal%20patterns%20of%20system%0Aoperation%20through%20reconstruction.%20However%2C%20these%20methods%20have%20two%20issues%20that%0Alimit%20their%20performance%20in%20anomaly%20detection.%20First%2C%20due%20to%20asynchrony%20between%0Alogs%20and%20metrics%2C%20coarse-grained%20time%20window%20alignment%20cannot%20achieve%20a%20precise%0Aassociation%20between%20the%20two%20modalities.%20Second%2C%20reconstruction-based%20methods%0Asuffer%20from%20severe%20overgeneralization%20problems%2C%20resulting%20in%20anomalies%20being%0Aaccurately%20reconstructed.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20anomaly%20detection%0Amethod%20named%20FFAD%20to%20address%20these%20two%20issues.%20On%20the%20one%20hand%2C%20FFAD%20employs%0Agraph-based%20alignment%20to%20mine%20and%20extract%20associations%20between%20the%20modalities%0Afrom%20the%20constructed%20log-metric%20relation%20graph%2C%20achieving%20precise%20associations%0Abetween%20logs%20and%20metrics.%20On%20the%20other%20hand%2C%20we%20improve%20the%20model%27s%20fit%20to%0Anormal%20data%20distributions%20through%20Fourier%20Frequency%20Focus%2C%20thereby%20enhancing%0Athe%20effectiveness%20of%20anomaly%20detection.%20We%20validated%20the%20effectiveness%20of%20our%0Amodel%20on%20two%20real-world%20industrial%20datasets%20and%20one%20open-source%20dataset.%20The%0Aresults%20show%20that%20our%20method%20achieves%20an%20average%20anomaly%20detection%20F1-score%20of%0A93.6%25%2C%20representing%20an%208.8%25%20improvement%20over%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16875v1&entry.124074799=Read"},
{"title": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data", "author": "Deren Lei and Yaxi Li and Siyao Li and Mengya Hu and Rui Xu and Ken Archer and Mingyu Wang and Emily Ching and Alex Deng", "abstract": "  Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size.\n", "link": "http://arxiv.org/abs/2501.17144v1", "date": "2025-01-28", "relevancy": 0.9204, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4657}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4622}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FactCG%3A%20Enhancing%20Fact%20Checkers%20with%20Graph-Based%20Multi-Hop%20Data&body=Title%3A%20FactCG%3A%20Enhancing%20Fact%20Checkers%20with%20Graph-Based%20Multi-Hop%20Data%0AAuthor%3A%20Deren%20Lei%20and%20Yaxi%20Li%20and%20Siyao%20Li%20and%20Mengya%20Hu%20and%20Rui%20Xu%20and%20Ken%20Archer%20and%20Mingyu%20Wang%20and%20Emily%20Ching%20and%20Alex%20Deng%0AAbstract%3A%20%20%20Prior%20research%20on%20training%20grounded%20factuality%20classification%20models%20to%0Adetect%20hallucinations%20in%20large%20language%20models%20%28LLMs%29%20has%20relied%20on%20public%0Anatural%20language%20inference%20%28NLI%29%20data%20and%20synthetic%20data.%20However%2C%20conventional%0ANLI%20datasets%20are%20not%20well-suited%20for%20document-level%20reasoning%2C%20which%20is%0Acritical%20for%20detecting%20LLM%20hallucinations.%20Recent%20approaches%20to%20document-level%0Asynthetic%20data%20generation%20involve%20iteratively%20removing%20sentences%20from%20documents%0Aand%20annotating%20factuality%20using%20LLM-based%20prompts.%20While%20effective%2C%20this%20method%0Ais%20computationally%20expensive%20for%20long%20documents%20and%20limited%20by%20the%20LLM%27s%0Acapabilities.%20In%20this%20work%2C%20we%20analyze%20the%20differences%20between%20existing%0Asynthetic%20training%20data%20used%20in%20state-of-the-art%20models%20and%20real%20LLM%20output%0Aclaims.%20Based%20on%20our%20findings%2C%20we%20propose%20a%20novel%20approach%20for%20synthetic%20data%0Ageneration%2C%20CG2C%2C%20that%20leverages%20multi-hop%20reasoning%20on%20context%20graphs%0Aextracted%20from%20documents.%20Our%20fact%20checker%20model%2C%20FactCG%2C%20demonstrates%20improved%0Aperformance%20with%20more%20connected%20reasoning%2C%20using%20the%20same%20backbone%20models.%0AExperiments%20show%20it%20even%20outperforms%20GPT-4-o%20on%20the%20LLM-Aggrefact%20benchmark%0Awith%20much%20smaller%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactCG%253A%2520Enhancing%2520Fact%2520Checkers%2520with%2520Graph-Based%2520Multi-Hop%2520Data%26entry.906535625%3DDeren%2520Lei%2520and%2520Yaxi%2520Li%2520and%2520Siyao%2520Li%2520and%2520Mengya%2520Hu%2520and%2520Rui%2520Xu%2520and%2520Ken%2520Archer%2520and%2520Mingyu%2520Wang%2520and%2520Emily%2520Ching%2520and%2520Alex%2520Deng%26entry.1292438233%3D%2520%2520Prior%2520research%2520on%2520training%2520grounded%2520factuality%2520classification%2520models%2520to%250Adetect%2520hallucinations%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520relied%2520on%2520public%250Anatural%2520language%2520inference%2520%2528NLI%2529%2520data%2520and%2520synthetic%2520data.%2520However%252C%2520conventional%250ANLI%2520datasets%2520are%2520not%2520well-suited%2520for%2520document-level%2520reasoning%252C%2520which%2520is%250Acritical%2520for%2520detecting%2520LLM%2520hallucinations.%2520Recent%2520approaches%2520to%2520document-level%250Asynthetic%2520data%2520generation%2520involve%2520iteratively%2520removing%2520sentences%2520from%2520documents%250Aand%2520annotating%2520factuality%2520using%2520LLM-based%2520prompts.%2520While%2520effective%252C%2520this%2520method%250Ais%2520computationally%2520expensive%2520for%2520long%2520documents%2520and%2520limited%2520by%2520the%2520LLM%2527s%250Acapabilities.%2520In%2520this%2520work%252C%2520we%2520analyze%2520the%2520differences%2520between%2520existing%250Asynthetic%2520training%2520data%2520used%2520in%2520state-of-the-art%2520models%2520and%2520real%2520LLM%2520output%250Aclaims.%2520Based%2520on%2520our%2520findings%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520synthetic%2520data%250Ageneration%252C%2520CG2C%252C%2520that%2520leverages%2520multi-hop%2520reasoning%2520on%2520context%2520graphs%250Aextracted%2520from%2520documents.%2520Our%2520fact%2520checker%2520model%252C%2520FactCG%252C%2520demonstrates%2520improved%250Aperformance%2520with%2520more%2520connected%2520reasoning%252C%2520using%2520the%2520same%2520backbone%2520models.%250AExperiments%2520show%2520it%2520even%2520outperforms%2520GPT-4-o%2520on%2520the%2520LLM-Aggrefact%2520benchmark%250Awith%2520much%2520smaller%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FactCG%3A%20Enhancing%20Fact%20Checkers%20with%20Graph-Based%20Multi-Hop%20Data&entry.906535625=Deren%20Lei%20and%20Yaxi%20Li%20and%20Siyao%20Li%20and%20Mengya%20Hu%20and%20Rui%20Xu%20and%20Ken%20Archer%20and%20Mingyu%20Wang%20and%20Emily%20Ching%20and%20Alex%20Deng&entry.1292438233=%20%20Prior%20research%20on%20training%20grounded%20factuality%20classification%20models%20to%0Adetect%20hallucinations%20in%20large%20language%20models%20%28LLMs%29%20has%20relied%20on%20public%0Anatural%20language%20inference%20%28NLI%29%20data%20and%20synthetic%20data.%20However%2C%20conventional%0ANLI%20datasets%20are%20not%20well-suited%20for%20document-level%20reasoning%2C%20which%20is%0Acritical%20for%20detecting%20LLM%20hallucinations.%20Recent%20approaches%20to%20document-level%0Asynthetic%20data%20generation%20involve%20iteratively%20removing%20sentences%20from%20documents%0Aand%20annotating%20factuality%20using%20LLM-based%20prompts.%20While%20effective%2C%20this%20method%0Ais%20computationally%20expensive%20for%20long%20documents%20and%20limited%20by%20the%20LLM%27s%0Acapabilities.%20In%20this%20work%2C%20we%20analyze%20the%20differences%20between%20existing%0Asynthetic%20training%20data%20used%20in%20state-of-the-art%20models%20and%20real%20LLM%20output%0Aclaims.%20Based%20on%20our%20findings%2C%20we%20propose%20a%20novel%20approach%20for%20synthetic%20data%0Ageneration%2C%20CG2C%2C%20that%20leverages%20multi-hop%20reasoning%20on%20context%20graphs%0Aextracted%20from%20documents.%20Our%20fact%20checker%20model%2C%20FactCG%2C%20demonstrates%20improved%0Aperformance%20with%20more%20connected%20reasoning%2C%20using%20the%20same%20backbone%20models.%0AExperiments%20show%20it%20even%20outperforms%20GPT-4-o%20on%20the%20LLM-Aggrefact%20benchmark%0Awith%20much%20smaller%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17144v1&entry.124074799=Read"},
{"title": "Thermodynamic limit in learning period three", "author": "Yuichiro Terasaki and Kohei Nakajima", "abstract": "  A continuous one-dimensional map with period three includes all periods. This\nraises the following question: Can we obtain any types of periodic orbits\nsolely by learning three data points? In this paper, we report the answer to be\nyes. Considering a random neural network in its thermodynamic limit, we first\nshow that almost all learned periods are unstable and each network has its\ncharacteristic attractors (which can even be untrained ones). The latently\nacquired dynamics, which are unstable within the trained network, serve as a\nfoundation for the diversity of characteristic attractors and may even lead to\nthe emergence of attractors of all periods after learning. When the neural\nnetwork interpolation is quadratic, a universal post-learning bifurcation\nscenario appears, which is consistent with a topological conjugacy between the\ntrained network and the classical logistic map. In addition to universality, we\nexplore specific properties of certain networks, including the singular\nbehavior at the infinite scale of weights limit and the symmetry in learning\nperiod three.\n", "link": "http://arxiv.org/abs/2405.08825v3", "date": "2025-01-28", "relevancy": 1.6375, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thermodynamic%20limit%20in%20learning%20period%20three&body=Title%3A%20Thermodynamic%20limit%20in%20learning%20period%20three%0AAuthor%3A%20Yuichiro%20Terasaki%20and%20Kohei%20Nakajima%0AAbstract%3A%20%20%20A%20continuous%20one-dimensional%20map%20with%20period%20three%20includes%20all%20periods.%20This%0Araises%20the%20following%20question%3A%20Can%20we%20obtain%20any%20types%20of%20periodic%20orbits%0Asolely%20by%20learning%20three%20data%20points%3F%20In%20this%20paper%2C%20we%20report%20the%20answer%20to%20be%0Ayes.%20Considering%20a%20random%20neural%20network%20in%20its%20thermodynamic%20limit%2C%20we%20first%0Ashow%20that%20almost%20all%20learned%20periods%20are%20unstable%20and%20each%20network%20has%20its%0Acharacteristic%20attractors%20%28which%20can%20even%20be%20untrained%20ones%29.%20The%20latently%0Aacquired%20dynamics%2C%20which%20are%20unstable%20within%20the%20trained%20network%2C%20serve%20as%20a%0Afoundation%20for%20the%20diversity%20of%20characteristic%20attractors%20and%20may%20even%20lead%20to%0Athe%20emergence%20of%20attractors%20of%20all%20periods%20after%20learning.%20When%20the%20neural%0Anetwork%20interpolation%20is%20quadratic%2C%20a%20universal%20post-learning%20bifurcation%0Ascenario%20appears%2C%20which%20is%20consistent%20with%20a%20topological%20conjugacy%20between%20the%0Atrained%20network%20and%20the%20classical%20logistic%20map.%20In%20addition%20to%20universality%2C%20we%0Aexplore%20specific%20properties%20of%20certain%20networks%2C%20including%20the%20singular%0Abehavior%20at%20the%20infinite%20scale%20of%20weights%20limit%20and%20the%20symmetry%20in%20learning%0Aperiod%20three.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08825v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermodynamic%2520limit%2520in%2520learning%2520period%2520three%26entry.906535625%3DYuichiro%2520Terasaki%2520and%2520Kohei%2520Nakajima%26entry.1292438233%3D%2520%2520A%2520continuous%2520one-dimensional%2520map%2520with%2520period%2520three%2520includes%2520all%2520periods.%2520This%250Araises%2520the%2520following%2520question%253A%2520Can%2520we%2520obtain%2520any%2520types%2520of%2520periodic%2520orbits%250Asolely%2520by%2520learning%2520three%2520data%2520points%253F%2520In%2520this%2520paper%252C%2520we%2520report%2520the%2520answer%2520to%2520be%250Ayes.%2520Considering%2520a%2520random%2520neural%2520network%2520in%2520its%2520thermodynamic%2520limit%252C%2520we%2520first%250Ashow%2520that%2520almost%2520all%2520learned%2520periods%2520are%2520unstable%2520and%2520each%2520network%2520has%2520its%250Acharacteristic%2520attractors%2520%2528which%2520can%2520even%2520be%2520untrained%2520ones%2529.%2520The%2520latently%250Aacquired%2520dynamics%252C%2520which%2520are%2520unstable%2520within%2520the%2520trained%2520network%252C%2520serve%2520as%2520a%250Afoundation%2520for%2520the%2520diversity%2520of%2520characteristic%2520attractors%2520and%2520may%2520even%2520lead%2520to%250Athe%2520emergence%2520of%2520attractors%2520of%2520all%2520periods%2520after%2520learning.%2520When%2520the%2520neural%250Anetwork%2520interpolation%2520is%2520quadratic%252C%2520a%2520universal%2520post-learning%2520bifurcation%250Ascenario%2520appears%252C%2520which%2520is%2520consistent%2520with%2520a%2520topological%2520conjugacy%2520between%2520the%250Atrained%2520network%2520and%2520the%2520classical%2520logistic%2520map.%2520In%2520addition%2520to%2520universality%252C%2520we%250Aexplore%2520specific%2520properties%2520of%2520certain%2520networks%252C%2520including%2520the%2520singular%250Abehavior%2520at%2520the%2520infinite%2520scale%2520of%2520weights%2520limit%2520and%2520the%2520symmetry%2520in%2520learning%250Aperiod%2520three.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08825v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thermodynamic%20limit%20in%20learning%20period%20three&entry.906535625=Yuichiro%20Terasaki%20and%20Kohei%20Nakajima&entry.1292438233=%20%20A%20continuous%20one-dimensional%20map%20with%20period%20three%20includes%20all%20periods.%20This%0Araises%20the%20following%20question%3A%20Can%20we%20obtain%20any%20types%20of%20periodic%20orbits%0Asolely%20by%20learning%20three%20data%20points%3F%20In%20this%20paper%2C%20we%20report%20the%20answer%20to%20be%0Ayes.%20Considering%20a%20random%20neural%20network%20in%20its%20thermodynamic%20limit%2C%20we%20first%0Ashow%20that%20almost%20all%20learned%20periods%20are%20unstable%20and%20each%20network%20has%20its%0Acharacteristic%20attractors%20%28which%20can%20even%20be%20untrained%20ones%29.%20The%20latently%0Aacquired%20dynamics%2C%20which%20are%20unstable%20within%20the%20trained%20network%2C%20serve%20as%20a%0Afoundation%20for%20the%20diversity%20of%20characteristic%20attractors%20and%20may%20even%20lead%20to%0Athe%20emergence%20of%20attractors%20of%20all%20periods%20after%20learning.%20When%20the%20neural%0Anetwork%20interpolation%20is%20quadratic%2C%20a%20universal%20post-learning%20bifurcation%0Ascenario%20appears%2C%20which%20is%20consistent%20with%20a%20topological%20conjugacy%20between%20the%0Atrained%20network%20and%20the%20classical%20logistic%20map.%20In%20addition%20to%20universality%2C%20we%0Aexplore%20specific%20properties%20of%20certain%20networks%2C%20including%20the%20singular%0Abehavior%20at%20the%20infinite%20scale%20of%20weights%20limit%20and%20the%20symmetry%20in%20learning%0Aperiod%20three.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08825v3&entry.124074799=Read"},
{"title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand\n  REST API Documentations", "author": "Xinyi Ni and Qiuyang Wang and Yukun Zhang and Pengyu Hong", "abstract": "  LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows.\n", "link": "http://arxiv.org/abs/2501.16945v1", "date": "2025-01-28", "relevancy": 1.3652, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4932}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4461}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToolFactory%3A%20Automating%20Tool%20Generation%20by%20Leveraging%20LLM%20to%20Understand%0A%20%20REST%20API%20Documentations&body=Title%3A%20ToolFactory%3A%20Automating%20Tool%20Generation%20by%20Leveraging%20LLM%20to%20Understand%0A%20%20REST%20API%20Documentations%0AAuthor%3A%20Xinyi%20Ni%20and%20Qiuyang%20Wang%20and%20Yukun%20Zhang%20and%20Pengyu%20Hong%0AAbstract%3A%20%20%20LLM-based%20tool%20agents%20offer%20natural%20language%20interfaces%2C%20enabling%20users%20to%0Aseamlessly%20interact%20with%20computing%20services.%20While%20REST%20APIs%20are%20valuable%0Aresources%20for%20building%20such%20agents%2C%20they%20must%20first%20be%20transformed%20into%0AAI-compatible%20tools.%20Automatically%20generating%20AI-compatible%20tools%20from%20REST%20API%0Adocuments%20can%20greatly%20streamline%20tool%20agent%20development%20and%20minimize%20user%0Alearning%20curves.%20However%2C%20API%20documentation%20often%20suffers%20from%20a%20lack%20of%0Astandardization%2C%20inconsistent%20schemas%2C%20and%20incomplete%20information.%20To%20address%0Athese%20issues%2C%20we%20developed%20%5Ctextbf%7BToolFactory%7D%2C%20an%20open-source%20pipeline%20for%0Aautomating%20tool%20generation%20from%20unstructured%20API%20documents.%20To%20enhance%20the%0Areliability%20of%20the%20developed%20tools%2C%20we%20implemented%20an%20evaluation%20method%20to%0Adiagnose%20errors.%20Furthermore%2C%20we%20built%20a%20knowledge%20base%20of%20verified%20tools%2C%0Awhich%20we%20leveraged%20to%20infer%20missing%20information%20from%20poorly%20documented%20APIs.%20We%0Adeveloped%20the%20API%20Extraction%20Benchmark%2C%20comprising%20167%20API%20documents%20and%20744%0Aendpoints%20in%20various%20formats%2C%20and%20designed%20a%20JSON%20schema%20to%20annotate%20them.%20This%0Aannotated%20dataset%20was%20utilized%20to%20train%20and%20validate%20ToolFactory.%20The%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20ToolFactory.%20We%20also%0Ademonstrated%20ToolFactory%20by%20creating%20a%20domain-specific%20AI%20agent%20for%0Aglycomaterials%20research.%20ToolFactory%20exhibits%20significant%20potential%20for%0Afacilitating%20the%20seamless%20integration%20of%20scientific%20REST%20APIs%20into%20AI%0Aworkflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToolFactory%253A%2520Automating%2520Tool%2520Generation%2520by%2520Leveraging%2520LLM%2520to%2520Understand%250A%2520%2520REST%2520API%2520Documentations%26entry.906535625%3DXinyi%2520Ni%2520and%2520Qiuyang%2520Wang%2520and%2520Yukun%2520Zhang%2520and%2520Pengyu%2520Hong%26entry.1292438233%3D%2520%2520LLM-based%2520tool%2520agents%2520offer%2520natural%2520language%2520interfaces%252C%2520enabling%2520users%2520to%250Aseamlessly%2520interact%2520with%2520computing%2520services.%2520While%2520REST%2520APIs%2520are%2520valuable%250Aresources%2520for%2520building%2520such%2520agents%252C%2520they%2520must%2520first%2520be%2520transformed%2520into%250AAI-compatible%2520tools.%2520Automatically%2520generating%2520AI-compatible%2520tools%2520from%2520REST%2520API%250Adocuments%2520can%2520greatly%2520streamline%2520tool%2520agent%2520development%2520and%2520minimize%2520user%250Alearning%2520curves.%2520However%252C%2520API%2520documentation%2520often%2520suffers%2520from%2520a%2520lack%2520of%250Astandardization%252C%2520inconsistent%2520schemas%252C%2520and%2520incomplete%2520information.%2520To%2520address%250Athese%2520issues%252C%2520we%2520developed%2520%255Ctextbf%257BToolFactory%257D%252C%2520an%2520open-source%2520pipeline%2520for%250Aautomating%2520tool%2520generation%2520from%2520unstructured%2520API%2520documents.%2520To%2520enhance%2520the%250Areliability%2520of%2520the%2520developed%2520tools%252C%2520we%2520implemented%2520an%2520evaluation%2520method%2520to%250Adiagnose%2520errors.%2520Furthermore%252C%2520we%2520built%2520a%2520knowledge%2520base%2520of%2520verified%2520tools%252C%250Awhich%2520we%2520leveraged%2520to%2520infer%2520missing%2520information%2520from%2520poorly%2520documented%2520APIs.%2520We%250Adeveloped%2520the%2520API%2520Extraction%2520Benchmark%252C%2520comprising%2520167%2520API%2520documents%2520and%2520744%250Aendpoints%2520in%2520various%2520formats%252C%2520and%2520designed%2520a%2520JSON%2520schema%2520to%2520annotate%2520them.%2520This%250Aannotated%2520dataset%2520was%2520utilized%2520to%2520train%2520and%2520validate%2520ToolFactory.%2520The%250Aexperimental%2520results%2520highlight%2520the%2520effectiveness%2520of%2520ToolFactory.%2520We%2520also%250Ademonstrated%2520ToolFactory%2520by%2520creating%2520a%2520domain-specific%2520AI%2520agent%2520for%250Aglycomaterials%2520research.%2520ToolFactory%2520exhibits%2520significant%2520potential%2520for%250Afacilitating%2520the%2520seamless%2520integration%2520of%2520scientific%2520REST%2520APIs%2520into%2520AI%250Aworkflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolFactory%3A%20Automating%20Tool%20Generation%20by%20Leveraging%20LLM%20to%20Understand%0A%20%20REST%20API%20Documentations&entry.906535625=Xinyi%20Ni%20and%20Qiuyang%20Wang%20and%20Yukun%20Zhang%20and%20Pengyu%20Hong&entry.1292438233=%20%20LLM-based%20tool%20agents%20offer%20natural%20language%20interfaces%2C%20enabling%20users%20to%0Aseamlessly%20interact%20with%20computing%20services.%20While%20REST%20APIs%20are%20valuable%0Aresources%20for%20building%20such%20agents%2C%20they%20must%20first%20be%20transformed%20into%0AAI-compatible%20tools.%20Automatically%20generating%20AI-compatible%20tools%20from%20REST%20API%0Adocuments%20can%20greatly%20streamline%20tool%20agent%20development%20and%20minimize%20user%0Alearning%20curves.%20However%2C%20API%20documentation%20often%20suffers%20from%20a%20lack%20of%0Astandardization%2C%20inconsistent%20schemas%2C%20and%20incomplete%20information.%20To%20address%0Athese%20issues%2C%20we%20developed%20%5Ctextbf%7BToolFactory%7D%2C%20an%20open-source%20pipeline%20for%0Aautomating%20tool%20generation%20from%20unstructured%20API%20documents.%20To%20enhance%20the%0Areliability%20of%20the%20developed%20tools%2C%20we%20implemented%20an%20evaluation%20method%20to%0Adiagnose%20errors.%20Furthermore%2C%20we%20built%20a%20knowledge%20base%20of%20verified%20tools%2C%0Awhich%20we%20leveraged%20to%20infer%20missing%20information%20from%20poorly%20documented%20APIs.%20We%0Adeveloped%20the%20API%20Extraction%20Benchmark%2C%20comprising%20167%20API%20documents%20and%20744%0Aendpoints%20in%20various%20formats%2C%20and%20designed%20a%20JSON%20schema%20to%20annotate%20them.%20This%0Aannotated%20dataset%20was%20utilized%20to%20train%20and%20validate%20ToolFactory.%20The%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20ToolFactory.%20We%20also%0Ademonstrated%20ToolFactory%20by%20creating%20a%20domain-specific%20AI%20agent%20for%0Aglycomaterials%20research.%20ToolFactory%20exhibits%20significant%20potential%20for%0Afacilitating%20the%20seamless%20integration%20of%20scientific%20REST%20APIs%20into%20AI%0Aworkflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16945v1&entry.124074799=Read"},
{"title": "Can Transformers Learn Full Bayesian Inference in Context?", "author": "Arik Reuter and Tim G. J. Rudner and Vincent Fortuin and David R\u00fcgamer", "abstract": "  Transformers have emerged as the dominant architecture in the field of deep\nlearning, with a broad range of applications and remarkable in-context learning\n(ICL) capabilities. While not yet fully understood, ICL has already proved to\nbe an intriguing phenomenon, allowing transformers to learn in context --\nwithout requiring further training. In this paper, we further advance the\nunderstanding of ICL by demonstrating that transformers can perform full\nBayesian inference for commonly used statistical models in context. More\nspecifically, we introduce a general framework that builds on ideas from prior\nfitted networks and continuous normalizing flows which enables us to infer\ncomplex posterior distributions for methods such as generalized linear models\nand latent factor models. Extensive experiments on real-world datasets\ndemonstrate that our ICL approach yields posterior samples that are similar in\nquality to state-of-the-art MCMC or variational inference methods not operating\nin context.\n", "link": "http://arxiv.org/abs/2501.16825v1", "date": "2025-01-28", "relevancy": 1.5163, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5268}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Transformers%20Learn%20Full%20Bayesian%20Inference%20in%20Context%3F&body=Title%3A%20Can%20Transformers%20Learn%20Full%20Bayesian%20Inference%20in%20Context%3F%0AAuthor%3A%20Arik%20Reuter%20and%20Tim%20G.%20J.%20Rudner%20and%20Vincent%20Fortuin%20and%20David%20R%C3%BCgamer%0AAbstract%3A%20%20%20Transformers%20have%20emerged%20as%20the%20dominant%20architecture%20in%20the%20field%20of%20deep%0Alearning%2C%20with%20a%20broad%20range%20of%20applications%20and%20remarkable%20in-context%20learning%0A%28ICL%29%20capabilities.%20While%20not%20yet%20fully%20understood%2C%20ICL%20has%20already%20proved%20to%0Abe%20an%20intriguing%20phenomenon%2C%20allowing%20transformers%20to%20learn%20in%20context%20--%0Awithout%20requiring%20further%20training.%20In%20this%20paper%2C%20we%20further%20advance%20the%0Aunderstanding%20of%20ICL%20by%20demonstrating%20that%20transformers%20can%20perform%20full%0ABayesian%20inference%20for%20commonly%20used%20statistical%20models%20in%20context.%20More%0Aspecifically%2C%20we%20introduce%20a%20general%20framework%20that%20builds%20on%20ideas%20from%20prior%0Afitted%20networks%20and%20continuous%20normalizing%20flows%20which%20enables%20us%20to%20infer%0Acomplex%20posterior%20distributions%20for%20methods%20such%20as%20generalized%20linear%20models%0Aand%20latent%20factor%20models.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20that%20our%20ICL%20approach%20yields%20posterior%20samples%20that%20are%20similar%20in%0Aquality%20to%20state-of-the-art%20MCMC%20or%20variational%20inference%20methods%20not%20operating%0Ain%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Transformers%2520Learn%2520Full%2520Bayesian%2520Inference%2520in%2520Context%253F%26entry.906535625%3DArik%2520Reuter%2520and%2520Tim%2520G.%2520J.%2520Rudner%2520and%2520Vincent%2520Fortuin%2520and%2520David%2520R%25C3%25BCgamer%26entry.1292438233%3D%2520%2520Transformers%2520have%2520emerged%2520as%2520the%2520dominant%2520architecture%2520in%2520the%2520field%2520of%2520deep%250Alearning%252C%2520with%2520a%2520broad%2520range%2520of%2520applications%2520and%2520remarkable%2520in-context%2520learning%250A%2528ICL%2529%2520capabilities.%2520While%2520not%2520yet%2520fully%2520understood%252C%2520ICL%2520has%2520already%2520proved%2520to%250Abe%2520an%2520intriguing%2520phenomenon%252C%2520allowing%2520transformers%2520to%2520learn%2520in%2520context%2520--%250Awithout%2520requiring%2520further%2520training.%2520In%2520this%2520paper%252C%2520we%2520further%2520advance%2520the%250Aunderstanding%2520of%2520ICL%2520by%2520demonstrating%2520that%2520transformers%2520can%2520perform%2520full%250ABayesian%2520inference%2520for%2520commonly%2520used%2520statistical%2520models%2520in%2520context.%2520More%250Aspecifically%252C%2520we%2520introduce%2520a%2520general%2520framework%2520that%2520builds%2520on%2520ideas%2520from%2520prior%250Afitted%2520networks%2520and%2520continuous%2520normalizing%2520flows%2520which%2520enables%2520us%2520to%2520infer%250Acomplex%2520posterior%2520distributions%2520for%2520methods%2520such%2520as%2520generalized%2520linear%2520models%250Aand%2520latent%2520factor%2520models.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520that%2520our%2520ICL%2520approach%2520yields%2520posterior%2520samples%2520that%2520are%2520similar%2520in%250Aquality%2520to%2520state-of-the-art%2520MCMC%2520or%2520variational%2520inference%2520methods%2520not%2520operating%250Ain%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Transformers%20Learn%20Full%20Bayesian%20Inference%20in%20Context%3F&entry.906535625=Arik%20Reuter%20and%20Tim%20G.%20J.%20Rudner%20and%20Vincent%20Fortuin%20and%20David%20R%C3%BCgamer&entry.1292438233=%20%20Transformers%20have%20emerged%20as%20the%20dominant%20architecture%20in%20the%20field%20of%20deep%0Alearning%2C%20with%20a%20broad%20range%20of%20applications%20and%20remarkable%20in-context%20learning%0A%28ICL%29%20capabilities.%20While%20not%20yet%20fully%20understood%2C%20ICL%20has%20already%20proved%20to%0Abe%20an%20intriguing%20phenomenon%2C%20allowing%20transformers%20to%20learn%20in%20context%20--%0Awithout%20requiring%20further%20training.%20In%20this%20paper%2C%20we%20further%20advance%20the%0Aunderstanding%20of%20ICL%20by%20demonstrating%20that%20transformers%20can%20perform%20full%0ABayesian%20inference%20for%20commonly%20used%20statistical%20models%20in%20context.%20More%0Aspecifically%2C%20we%20introduce%20a%20general%20framework%20that%20builds%20on%20ideas%20from%20prior%0Afitted%20networks%20and%20continuous%20normalizing%20flows%20which%20enables%20us%20to%20infer%0Acomplex%20posterior%20distributions%20for%20methods%20such%20as%20generalized%20linear%20models%0Aand%20latent%20factor%20models.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20that%20our%20ICL%20approach%20yields%20posterior%20samples%20that%20are%20similar%20in%0Aquality%20to%20state-of-the-art%20MCMC%20or%20variational%20inference%20methods%20not%20operating%0Ain%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16825v1&entry.124074799=Read"},
{"title": "Hellinger-Kantorovich Gradient Flows: Global Exponential Decay of\n  Entropy Functionals", "author": "Alexander Mielke and Jia-Jie Zhu", "abstract": "  We investigate a family of gradient flows of positive and probability\nmeasures, focusing on the Hellinger-Kantorovich (HK) geometry, which unifies\ntransport mechanism of Otto-Wasserstein, and the birth-death mechanism of\nHellinger (or Fisher-Rao). A central contribution is a complete\ncharacterization of global exponential decay behaviors of entropy functionals\n(e.g. KL, $\\chi^2$) under Otto-Wasserstein and Hellinger-type gradient flows.\nIn particular, for the more challenging analysis of HK gradient flows on\npositive measures -- where the typical log-Sobolev arguments fail -- we develop\na specialized shape-mass decomposition that enables new analysis results. Our\napproach also leverages the (Polyak-)\\L{}ojasiewicz-type functional\ninequalities and a careful extension of classical dissipation estimates. These\nfindings provide a unified and complete theoretical framework for gradient\nflows and underpin applications in computational algorithms for statistical\ninference, optimization, and machine learning.\n", "link": "http://arxiv.org/abs/2501.17049v1", "date": "2025-01-28", "relevancy": 1.5946, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4645}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3863}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hellinger-Kantorovich%20Gradient%20Flows%3A%20Global%20Exponential%20Decay%20of%0A%20%20Entropy%20Functionals&body=Title%3A%20Hellinger-Kantorovich%20Gradient%20Flows%3A%20Global%20Exponential%20Decay%20of%0A%20%20Entropy%20Functionals%0AAuthor%3A%20Alexander%20Mielke%20and%20Jia-Jie%20Zhu%0AAbstract%3A%20%20%20We%20investigate%20a%20family%20of%20gradient%20flows%20of%20positive%20and%20probability%0Ameasures%2C%20focusing%20on%20the%20Hellinger-Kantorovich%20%28HK%29%20geometry%2C%20which%20unifies%0Atransport%20mechanism%20of%20Otto-Wasserstein%2C%20and%20the%20birth-death%20mechanism%20of%0AHellinger%20%28or%20Fisher-Rao%29.%20A%20central%20contribution%20is%20a%20complete%0Acharacterization%20of%20global%20exponential%20decay%20behaviors%20of%20entropy%20functionals%0A%28e.g.%20KL%2C%20%24%5Cchi%5E2%24%29%20under%20Otto-Wasserstein%20and%20Hellinger-type%20gradient%20flows.%0AIn%20particular%2C%20for%20the%20more%20challenging%20analysis%20of%20HK%20gradient%20flows%20on%0Apositive%20measures%20--%20where%20the%20typical%20log-Sobolev%20arguments%20fail%20--%20we%20develop%0Aa%20specialized%20shape-mass%20decomposition%20that%20enables%20new%20analysis%20results.%20Our%0Aapproach%20also%20leverages%20the%20%28Polyak-%29%5CL%7B%7Dojasiewicz-type%20functional%0Ainequalities%20and%20a%20careful%20extension%20of%20classical%20dissipation%20estimates.%20These%0Afindings%20provide%20a%20unified%20and%20complete%20theoretical%20framework%20for%20gradient%0Aflows%20and%20underpin%20applications%20in%20computational%20algorithms%20for%20statistical%0Ainference%2C%20optimization%2C%20and%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHellinger-Kantorovich%2520Gradient%2520Flows%253A%2520Global%2520Exponential%2520Decay%2520of%250A%2520%2520Entropy%2520Functionals%26entry.906535625%3DAlexander%2520Mielke%2520and%2520Jia-Jie%2520Zhu%26entry.1292438233%3D%2520%2520We%2520investigate%2520a%2520family%2520of%2520gradient%2520flows%2520of%2520positive%2520and%2520probability%250Ameasures%252C%2520focusing%2520on%2520the%2520Hellinger-Kantorovich%2520%2528HK%2529%2520geometry%252C%2520which%2520unifies%250Atransport%2520mechanism%2520of%2520Otto-Wasserstein%252C%2520and%2520the%2520birth-death%2520mechanism%2520of%250AHellinger%2520%2528or%2520Fisher-Rao%2529.%2520A%2520central%2520contribution%2520is%2520a%2520complete%250Acharacterization%2520of%2520global%2520exponential%2520decay%2520behaviors%2520of%2520entropy%2520functionals%250A%2528e.g.%2520KL%252C%2520%2524%255Cchi%255E2%2524%2529%2520under%2520Otto-Wasserstein%2520and%2520Hellinger-type%2520gradient%2520flows.%250AIn%2520particular%252C%2520for%2520the%2520more%2520challenging%2520analysis%2520of%2520HK%2520gradient%2520flows%2520on%250Apositive%2520measures%2520--%2520where%2520the%2520typical%2520log-Sobolev%2520arguments%2520fail%2520--%2520we%2520develop%250Aa%2520specialized%2520shape-mass%2520decomposition%2520that%2520enables%2520new%2520analysis%2520results.%2520Our%250Aapproach%2520also%2520leverages%2520the%2520%2528Polyak-%2529%255CL%257B%257Dojasiewicz-type%2520functional%250Ainequalities%2520and%2520a%2520careful%2520extension%2520of%2520classical%2520dissipation%2520estimates.%2520These%250Afindings%2520provide%2520a%2520unified%2520and%2520complete%2520theoretical%2520framework%2520for%2520gradient%250Aflows%2520and%2520underpin%2520applications%2520in%2520computational%2520algorithms%2520for%2520statistical%250Ainference%252C%2520optimization%252C%2520and%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hellinger-Kantorovich%20Gradient%20Flows%3A%20Global%20Exponential%20Decay%20of%0A%20%20Entropy%20Functionals&entry.906535625=Alexander%20Mielke%20and%20Jia-Jie%20Zhu&entry.1292438233=%20%20We%20investigate%20a%20family%20of%20gradient%20flows%20of%20positive%20and%20probability%0Ameasures%2C%20focusing%20on%20the%20Hellinger-Kantorovich%20%28HK%29%20geometry%2C%20which%20unifies%0Atransport%20mechanism%20of%20Otto-Wasserstein%2C%20and%20the%20birth-death%20mechanism%20of%0AHellinger%20%28or%20Fisher-Rao%29.%20A%20central%20contribution%20is%20a%20complete%0Acharacterization%20of%20global%20exponential%20decay%20behaviors%20of%20entropy%20functionals%0A%28e.g.%20KL%2C%20%24%5Cchi%5E2%24%29%20under%20Otto-Wasserstein%20and%20Hellinger-type%20gradient%20flows.%0AIn%20particular%2C%20for%20the%20more%20challenging%20analysis%20of%20HK%20gradient%20flows%20on%0Apositive%20measures%20--%20where%20the%20typical%20log-Sobolev%20arguments%20fail%20--%20we%20develop%0Aa%20specialized%20shape-mass%20decomposition%20that%20enables%20new%20analysis%20results.%20Our%0Aapproach%20also%20leverages%20the%20%28Polyak-%29%5CL%7B%7Dojasiewicz-type%20functional%0Ainequalities%20and%20a%20careful%20extension%20of%20classical%20dissipation%20estimates.%20These%0Afindings%20provide%20a%20unified%20and%20complete%20theoretical%20framework%20for%20gradient%0Aflows%20and%20underpin%20applications%20in%20computational%20algorithms%20for%20statistical%0Ainference%2C%20optimization%2C%20and%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17049v1&entry.124074799=Read"},
{"title": "Data-Driven vs Traditional Approaches to Power Transformer's Top-Oil\n  Temperature Estimation", "author": "Francis Tembo and Federica Bragone and Tor Laneryd and Matthieu Barreau and Kateryna Morozovska", "abstract": "  Power transformers are subjected to electrical currents and temperature\nfluctuations that, if not properly controlled, can lead to major deterioration\nof their insulation system. Therefore, monitoring the temperature of a power\ntransformer is fundamental to ensure a long-term operational life. Models\npresented in the IEC 60076-7 and IEEE standards, for example, monitor the\ntemperature by calculating the top-oil and the hot-spot temperatures. However,\nthese models are not very accurate and rely on the power transformers'\nproperties. This paper focuses on finding an alternative method to predict the\ntop-oil temperatures given previous measurements. Given the large quantities of\ndata available, machine learning methods for time series forecasting are\nanalyzed and compared to the real measurements and the corresponding prediction\nof the IEC standard. The methods tested are Artificial Neural Networks (ANNs),\nTime-series Dense Encoder (TiDE), and Temporal Convolutional Networks (TCN)\nusing different combinations of historical measurements. Each of these methods\noutperformed the IEC 60076-7 model and they are extended to estimate the\ntemperature rise over ambient. To enhance prediction reliability, we explore\nthe application of quantile regression to construct prediction intervals for\nthe expected top-oil temperature ranges. The best-performing model successfully\nestimates conditional quantiles that provide sufficient coverage.\n", "link": "http://arxiv.org/abs/2501.16831v1", "date": "2025-01-28", "relevancy": 0.8545, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5004}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.393}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20vs%20Traditional%20Approaches%20to%20Power%20Transformer%27s%20Top-Oil%0A%20%20Temperature%20Estimation&body=Title%3A%20Data-Driven%20vs%20Traditional%20Approaches%20to%20Power%20Transformer%27s%20Top-Oil%0A%20%20Temperature%20Estimation%0AAuthor%3A%20Francis%20Tembo%20and%20Federica%20Bragone%20and%20Tor%20Laneryd%20and%20Matthieu%20Barreau%20and%20Kateryna%20Morozovska%0AAbstract%3A%20%20%20Power%20transformers%20are%20subjected%20to%20electrical%20currents%20and%20temperature%0Afluctuations%20that%2C%20if%20not%20properly%20controlled%2C%20can%20lead%20to%20major%20deterioration%0Aof%20their%20insulation%20system.%20Therefore%2C%20monitoring%20the%20temperature%20of%20a%20power%0Atransformer%20is%20fundamental%20to%20ensure%20a%20long-term%20operational%20life.%20Models%0Apresented%20in%20the%20IEC%2060076-7%20and%20IEEE%20standards%2C%20for%20example%2C%20monitor%20the%0Atemperature%20by%20calculating%20the%20top-oil%20and%20the%20hot-spot%20temperatures.%20However%2C%0Athese%20models%20are%20not%20very%20accurate%20and%20rely%20on%20the%20power%20transformers%27%0Aproperties.%20This%20paper%20focuses%20on%20finding%20an%20alternative%20method%20to%20predict%20the%0Atop-oil%20temperatures%20given%20previous%20measurements.%20Given%20the%20large%20quantities%20of%0Adata%20available%2C%20machine%20learning%20methods%20for%20time%20series%20forecasting%20are%0Aanalyzed%20and%20compared%20to%20the%20real%20measurements%20and%20the%20corresponding%20prediction%0Aof%20the%20IEC%20standard.%20The%20methods%20tested%20are%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%0ATime-series%20Dense%20Encoder%20%28TiDE%29%2C%20and%20Temporal%20Convolutional%20Networks%20%28TCN%29%0Ausing%20different%20combinations%20of%20historical%20measurements.%20Each%20of%20these%20methods%0Aoutperformed%20the%20IEC%2060076-7%20model%20and%20they%20are%20extended%20to%20estimate%20the%0Atemperature%20rise%20over%20ambient.%20To%20enhance%20prediction%20reliability%2C%20we%20explore%0Athe%20application%20of%20quantile%20regression%20to%20construct%20prediction%20intervals%20for%0Athe%20expected%20top-oil%20temperature%20ranges.%20The%20best-performing%20model%20successfully%0Aestimates%20conditional%20quantiles%20that%20provide%20sufficient%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520vs%2520Traditional%2520Approaches%2520to%2520Power%2520Transformer%2527s%2520Top-Oil%250A%2520%2520Temperature%2520Estimation%26entry.906535625%3DFrancis%2520Tembo%2520and%2520Federica%2520Bragone%2520and%2520Tor%2520Laneryd%2520and%2520Matthieu%2520Barreau%2520and%2520Kateryna%2520Morozovska%26entry.1292438233%3D%2520%2520Power%2520transformers%2520are%2520subjected%2520to%2520electrical%2520currents%2520and%2520temperature%250Afluctuations%2520that%252C%2520if%2520not%2520properly%2520controlled%252C%2520can%2520lead%2520to%2520major%2520deterioration%250Aof%2520their%2520insulation%2520system.%2520Therefore%252C%2520monitoring%2520the%2520temperature%2520of%2520a%2520power%250Atransformer%2520is%2520fundamental%2520to%2520ensure%2520a%2520long-term%2520operational%2520life.%2520Models%250Apresented%2520in%2520the%2520IEC%252060076-7%2520and%2520IEEE%2520standards%252C%2520for%2520example%252C%2520monitor%2520the%250Atemperature%2520by%2520calculating%2520the%2520top-oil%2520and%2520the%2520hot-spot%2520temperatures.%2520However%252C%250Athese%2520models%2520are%2520not%2520very%2520accurate%2520and%2520rely%2520on%2520the%2520power%2520transformers%2527%250Aproperties.%2520This%2520paper%2520focuses%2520on%2520finding%2520an%2520alternative%2520method%2520to%2520predict%2520the%250Atop-oil%2520temperatures%2520given%2520previous%2520measurements.%2520Given%2520the%2520large%2520quantities%2520of%250Adata%2520available%252C%2520machine%2520learning%2520methods%2520for%2520time%2520series%2520forecasting%2520are%250Aanalyzed%2520and%2520compared%2520to%2520the%2520real%2520measurements%2520and%2520the%2520corresponding%2520prediction%250Aof%2520the%2520IEC%2520standard.%2520The%2520methods%2520tested%2520are%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529%252C%250ATime-series%2520Dense%2520Encoder%2520%2528TiDE%2529%252C%2520and%2520Temporal%2520Convolutional%2520Networks%2520%2528TCN%2529%250Ausing%2520different%2520combinations%2520of%2520historical%2520measurements.%2520Each%2520of%2520these%2520methods%250Aoutperformed%2520the%2520IEC%252060076-7%2520model%2520and%2520they%2520are%2520extended%2520to%2520estimate%2520the%250Atemperature%2520rise%2520over%2520ambient.%2520To%2520enhance%2520prediction%2520reliability%252C%2520we%2520explore%250Athe%2520application%2520of%2520quantile%2520regression%2520to%2520construct%2520prediction%2520intervals%2520for%250Athe%2520expected%2520top-oil%2520temperature%2520ranges.%2520The%2520best-performing%2520model%2520successfully%250Aestimates%2520conditional%2520quantiles%2520that%2520provide%2520sufficient%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20vs%20Traditional%20Approaches%20to%20Power%20Transformer%27s%20Top-Oil%0A%20%20Temperature%20Estimation&entry.906535625=Francis%20Tembo%20and%20Federica%20Bragone%20and%20Tor%20Laneryd%20and%20Matthieu%20Barreau%20and%20Kateryna%20Morozovska&entry.1292438233=%20%20Power%20transformers%20are%20subjected%20to%20electrical%20currents%20and%20temperature%0Afluctuations%20that%2C%20if%20not%20properly%20controlled%2C%20can%20lead%20to%20major%20deterioration%0Aof%20their%20insulation%20system.%20Therefore%2C%20monitoring%20the%20temperature%20of%20a%20power%0Atransformer%20is%20fundamental%20to%20ensure%20a%20long-term%20operational%20life.%20Models%0Apresented%20in%20the%20IEC%2060076-7%20and%20IEEE%20standards%2C%20for%20example%2C%20monitor%20the%0Atemperature%20by%20calculating%20the%20top-oil%20and%20the%20hot-spot%20temperatures.%20However%2C%0Athese%20models%20are%20not%20very%20accurate%20and%20rely%20on%20the%20power%20transformers%27%0Aproperties.%20This%20paper%20focuses%20on%20finding%20an%20alternative%20method%20to%20predict%20the%0Atop-oil%20temperatures%20given%20previous%20measurements.%20Given%20the%20large%20quantities%20of%0Adata%20available%2C%20machine%20learning%20methods%20for%20time%20series%20forecasting%20are%0Aanalyzed%20and%20compared%20to%20the%20real%20measurements%20and%20the%20corresponding%20prediction%0Aof%20the%20IEC%20standard.%20The%20methods%20tested%20are%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%0ATime-series%20Dense%20Encoder%20%28TiDE%29%2C%20and%20Temporal%20Convolutional%20Networks%20%28TCN%29%0Ausing%20different%20combinations%20of%20historical%20measurements.%20Each%20of%20these%20methods%0Aoutperformed%20the%20IEC%2060076-7%20model%20and%20they%20are%20extended%20to%20estimate%20the%0Atemperature%20rise%20over%20ambient.%20To%20enhance%20prediction%20reliability%2C%20we%20explore%0Athe%20application%20of%20quantile%20regression%20to%20construct%20prediction%20intervals%20for%0Athe%20expected%20top-oil%20temperature%20ranges.%20The%20best-performing%20model%20successfully%0Aestimates%20conditional%20quantiles%20that%20provide%20sufficient%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16831v1&entry.124074799=Read"},
{"title": "Towards Open-Source and Modular Space Systems with ATMOS", "author": "Pedro Roque and Sujet Phodapol and Elias Krantz and Jaeyoung Lim and Joris Verhagen and Frank Jiang and David Dorner and Roland Siegwart and Ivan Stenius and Gunnar Tibert and Huina Mao and Jana Tumova and Christer Fuglesang and Dimos V. Dimarogonas", "abstract": "  In the near future, autonomous space systems will compose a large number of\nthe spacecraft being deployed. Their tasks will involve autonomous rendezvous\nand proximity operations with large structures, such as inspections or assembly\nof orbiting space stations and maintenance and human-assistance tasks over\nshared workspaces. To promote replicable and reliable scientific results for\nautonomous control of spacecraft, we present the design of a space systems\nlaboratory based on open-source and modular software and hardware. The\nsimulation software provides a software-in-the-loop (SITL) architecture that\nseamlessly transfers simulated results to the ATMOS platforms, developed for\ntesting of multi-agent autonomy schemes for microgravity. The manuscript\npresents the KTH space systems laboratory facilities and the ATMOS platform as\nopen-source hardware and software contributions. Preliminary results showcase\nSITL and real testing.\n", "link": "http://arxiv.org/abs/2501.16973v1", "date": "2025-01-28", "relevancy": 1.3632, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4927}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4655}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Open-Source%20and%20Modular%20Space%20Systems%20with%20ATMOS&body=Title%3A%20Towards%20Open-Source%20and%20Modular%20Space%20Systems%20with%20ATMOS%0AAuthor%3A%20Pedro%20Roque%20and%20Sujet%20Phodapol%20and%20Elias%20Krantz%20and%20Jaeyoung%20Lim%20and%20Joris%20Verhagen%20and%20Frank%20Jiang%20and%20David%20Dorner%20and%20Roland%20Siegwart%20and%20Ivan%20Stenius%20and%20Gunnar%20Tibert%20and%20Huina%20Mao%20and%20Jana%20Tumova%20and%20Christer%20Fuglesang%20and%20Dimos%20V.%20Dimarogonas%0AAbstract%3A%20%20%20In%20the%20near%20future%2C%20autonomous%20space%20systems%20will%20compose%20a%20large%20number%20of%0Athe%20spacecraft%20being%20deployed.%20Their%20tasks%20will%20involve%20autonomous%20rendezvous%0Aand%20proximity%20operations%20with%20large%20structures%2C%20such%20as%20inspections%20or%20assembly%0Aof%20orbiting%20space%20stations%20and%20maintenance%20and%20human-assistance%20tasks%20over%0Ashared%20workspaces.%20To%20promote%20replicable%20and%20reliable%20scientific%20results%20for%0Aautonomous%20control%20of%20spacecraft%2C%20we%20present%20the%20design%20of%20a%20space%20systems%0Alaboratory%20based%20on%20open-source%20and%20modular%20software%20and%20hardware.%20The%0Asimulation%20software%20provides%20a%20software-in-the-loop%20%28SITL%29%20architecture%20that%0Aseamlessly%20transfers%20simulated%20results%20to%20the%20ATMOS%20platforms%2C%20developed%20for%0Atesting%20of%20multi-agent%20autonomy%20schemes%20for%20microgravity.%20The%20manuscript%0Apresents%20the%20KTH%20space%20systems%20laboratory%20facilities%20and%20the%20ATMOS%20platform%20as%0Aopen-source%20hardware%20and%20software%20contributions.%20Preliminary%20results%20showcase%0ASITL%20and%20real%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Open-Source%2520and%2520Modular%2520Space%2520Systems%2520with%2520ATMOS%26entry.906535625%3DPedro%2520Roque%2520and%2520Sujet%2520Phodapol%2520and%2520Elias%2520Krantz%2520and%2520Jaeyoung%2520Lim%2520and%2520Joris%2520Verhagen%2520and%2520Frank%2520Jiang%2520and%2520David%2520Dorner%2520and%2520Roland%2520Siegwart%2520and%2520Ivan%2520Stenius%2520and%2520Gunnar%2520Tibert%2520and%2520Huina%2520Mao%2520and%2520Jana%2520Tumova%2520and%2520Christer%2520Fuglesang%2520and%2520Dimos%2520V.%2520Dimarogonas%26entry.1292438233%3D%2520%2520In%2520the%2520near%2520future%252C%2520autonomous%2520space%2520systems%2520will%2520compose%2520a%2520large%2520number%2520of%250Athe%2520spacecraft%2520being%2520deployed.%2520Their%2520tasks%2520will%2520involve%2520autonomous%2520rendezvous%250Aand%2520proximity%2520operations%2520with%2520large%2520structures%252C%2520such%2520as%2520inspections%2520or%2520assembly%250Aof%2520orbiting%2520space%2520stations%2520and%2520maintenance%2520and%2520human-assistance%2520tasks%2520over%250Ashared%2520workspaces.%2520To%2520promote%2520replicable%2520and%2520reliable%2520scientific%2520results%2520for%250Aautonomous%2520control%2520of%2520spacecraft%252C%2520we%2520present%2520the%2520design%2520of%2520a%2520space%2520systems%250Alaboratory%2520based%2520on%2520open-source%2520and%2520modular%2520software%2520and%2520hardware.%2520The%250Asimulation%2520software%2520provides%2520a%2520software-in-the-loop%2520%2528SITL%2529%2520architecture%2520that%250Aseamlessly%2520transfers%2520simulated%2520results%2520to%2520the%2520ATMOS%2520platforms%252C%2520developed%2520for%250Atesting%2520of%2520multi-agent%2520autonomy%2520schemes%2520for%2520microgravity.%2520The%2520manuscript%250Apresents%2520the%2520KTH%2520space%2520systems%2520laboratory%2520facilities%2520and%2520the%2520ATMOS%2520platform%2520as%250Aopen-source%2520hardware%2520and%2520software%2520contributions.%2520Preliminary%2520results%2520showcase%250ASITL%2520and%2520real%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Open-Source%20and%20Modular%20Space%20Systems%20with%20ATMOS&entry.906535625=Pedro%20Roque%20and%20Sujet%20Phodapol%20and%20Elias%20Krantz%20and%20Jaeyoung%20Lim%20and%20Joris%20Verhagen%20and%20Frank%20Jiang%20and%20David%20Dorner%20and%20Roland%20Siegwart%20and%20Ivan%20Stenius%20and%20Gunnar%20Tibert%20and%20Huina%20Mao%20and%20Jana%20Tumova%20and%20Christer%20Fuglesang%20and%20Dimos%20V.%20Dimarogonas&entry.1292438233=%20%20In%20the%20near%20future%2C%20autonomous%20space%20systems%20will%20compose%20a%20large%20number%20of%0Athe%20spacecraft%20being%20deployed.%20Their%20tasks%20will%20involve%20autonomous%20rendezvous%0Aand%20proximity%20operations%20with%20large%20structures%2C%20such%20as%20inspections%20or%20assembly%0Aof%20orbiting%20space%20stations%20and%20maintenance%20and%20human-assistance%20tasks%20over%0Ashared%20workspaces.%20To%20promote%20replicable%20and%20reliable%20scientific%20results%20for%0Aautonomous%20control%20of%20spacecraft%2C%20we%20present%20the%20design%20of%20a%20space%20systems%0Alaboratory%20based%20on%20open-source%20and%20modular%20software%20and%20hardware.%20The%0Asimulation%20software%20provides%20a%20software-in-the-loop%20%28SITL%29%20architecture%20that%0Aseamlessly%20transfers%20simulated%20results%20to%20the%20ATMOS%20platforms%2C%20developed%20for%0Atesting%20of%20multi-agent%20autonomy%20schemes%20for%20microgravity.%20The%20manuscript%0Apresents%20the%20KTH%20space%20systems%20laboratory%20facilities%20and%20the%20ATMOS%20platform%20as%0Aopen-source%20hardware%20and%20software%20contributions.%20Preliminary%20results%20showcase%0ASITL%20and%20real%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16973v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


