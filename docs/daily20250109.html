<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250108.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using\n  Gaussian Splatting with Depth-Feature Consistency", "author": "Han Huang and Yulun Wu and Chao Deng and Ge Gao and Ming Gu and Yu-Shen Liu", "abstract": "  Recently, Gaussian Splatting has sparked a new trend in the field of computer\nvision. Apart from novel view synthesis, it has also been extended to the area\nof multi-view reconstruction. The latest methods facilitate complete, detailed\nsurface reconstruction while ensuring fast training speed. However, these\nmethods still require dense input views, and their output quality significantly\ndegrades with sparse views. We observed that the Gaussian primitives tend to\noverfit the few training views, leading to noisy floaters and incomplete\nreconstruction surfaces. In this paper, we present an innovative sparse-view\nreconstruction framework that leverages intra-view depth and multi-view feature\nconsistency to achieve remarkably accurate surface reconstruction.\nSpecifically, we utilize monocular depth ranking information to supervise the\nconsistency of depth distribution within patches and employ a smoothness loss\nto enhance the continuity of the distribution. To achieve finer surface\nreconstruction, we optimize the absolute position of depth through multi-view\nprojection features. Extensive experiments on DTU and BlendedMVS demonstrate\nthat our method outperforms state-of-the-art methods with a speedup of 60x to\n200x, achieving swift and fine-grained mesh reconstruction without the need for\ncostly pre-training.\n", "link": "http://arxiv.org/abs/2501.04628v1", "date": "2025-01-08", "relevancy": 3.5091, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.726}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6926}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FatesGS%3A%20Fast%20and%20Accurate%20Sparse-View%20Surface%20Reconstruction%20using%0A%20%20Gaussian%20Splatting%20with%20Depth-Feature%20Consistency&body=Title%3A%20FatesGS%3A%20Fast%20and%20Accurate%20Sparse-View%20Surface%20Reconstruction%20using%0A%20%20Gaussian%20Splatting%20with%20Depth-Feature%20Consistency%0AAuthor%3A%20Han%20Huang%20and%20Yulun%20Wu%20and%20Chao%20Deng%20and%20Ge%20Gao%20and%20Ming%20Gu%20and%20Yu-Shen%20Liu%0AAbstract%3A%20%20%20Recently%2C%20Gaussian%20Splatting%20has%20sparked%20a%20new%20trend%20in%20the%20field%20of%20computer%0Avision.%20Apart%20from%20novel%20view%20synthesis%2C%20it%20has%20also%20been%20extended%20to%20the%20area%0Aof%20multi-view%20reconstruction.%20The%20latest%20methods%20facilitate%20complete%2C%20detailed%0Asurface%20reconstruction%20while%20ensuring%20fast%20training%20speed.%20However%2C%20these%0Amethods%20still%20require%20dense%20input%20views%2C%20and%20their%20output%20quality%20significantly%0Adegrades%20with%20sparse%20views.%20We%20observed%20that%20the%20Gaussian%20primitives%20tend%20to%0Aoverfit%20the%20few%20training%20views%2C%20leading%20to%20noisy%20floaters%20and%20incomplete%0Areconstruction%20surfaces.%20In%20this%20paper%2C%20we%20present%20an%20innovative%20sparse-view%0Areconstruction%20framework%20that%20leverages%20intra-view%20depth%20and%20multi-view%20feature%0Aconsistency%20to%20achieve%20remarkably%20accurate%20surface%20reconstruction.%0ASpecifically%2C%20we%20utilize%20monocular%20depth%20ranking%20information%20to%20supervise%20the%0Aconsistency%20of%20depth%20distribution%20within%20patches%20and%20employ%20a%20smoothness%20loss%0Ato%20enhance%20the%20continuity%20of%20the%20distribution.%20To%20achieve%20finer%20surface%0Areconstruction%2C%20we%20optimize%20the%20absolute%20position%20of%20depth%20through%20multi-view%0Aprojection%20features.%20Extensive%20experiments%20on%20DTU%20and%20BlendedMVS%20demonstrate%0Athat%20our%20method%20outperforms%20state-of-the-art%20methods%20with%20a%20speedup%20of%2060x%20to%0A200x%2C%20achieving%20swift%20and%20fine-grained%20mesh%20reconstruction%20without%20the%20need%20for%0Acostly%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFatesGS%253A%2520Fast%2520and%2520Accurate%2520Sparse-View%2520Surface%2520Reconstruction%2520using%250A%2520%2520Gaussian%2520Splatting%2520with%2520Depth-Feature%2520Consistency%26entry.906535625%3DHan%2520Huang%2520and%2520Yulun%2520Wu%2520and%2520Chao%2520Deng%2520and%2520Ge%2520Gao%2520and%2520Ming%2520Gu%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520Gaussian%2520Splatting%2520has%2520sparked%2520a%2520new%2520trend%2520in%2520the%2520field%2520of%2520computer%250Avision.%2520Apart%2520from%2520novel%2520view%2520synthesis%252C%2520it%2520has%2520also%2520been%2520extended%2520to%2520the%2520area%250Aof%2520multi-view%2520reconstruction.%2520The%2520latest%2520methods%2520facilitate%2520complete%252C%2520detailed%250Asurface%2520reconstruction%2520while%2520ensuring%2520fast%2520training%2520speed.%2520However%252C%2520these%250Amethods%2520still%2520require%2520dense%2520input%2520views%252C%2520and%2520their%2520output%2520quality%2520significantly%250Adegrades%2520with%2520sparse%2520views.%2520We%2520observed%2520that%2520the%2520Gaussian%2520primitives%2520tend%2520to%250Aoverfit%2520the%2520few%2520training%2520views%252C%2520leading%2520to%2520noisy%2520floaters%2520and%2520incomplete%250Areconstruction%2520surfaces.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520innovative%2520sparse-view%250Areconstruction%2520framework%2520that%2520leverages%2520intra-view%2520depth%2520and%2520multi-view%2520feature%250Aconsistency%2520to%2520achieve%2520remarkably%2520accurate%2520surface%2520reconstruction.%250ASpecifically%252C%2520we%2520utilize%2520monocular%2520depth%2520ranking%2520information%2520to%2520supervise%2520the%250Aconsistency%2520of%2520depth%2520distribution%2520within%2520patches%2520and%2520employ%2520a%2520smoothness%2520loss%250Ato%2520enhance%2520the%2520continuity%2520of%2520the%2520distribution.%2520To%2520achieve%2520finer%2520surface%250Areconstruction%252C%2520we%2520optimize%2520the%2520absolute%2520position%2520of%2520depth%2520through%2520multi-view%250Aprojection%2520features.%2520Extensive%2520experiments%2520on%2520DTU%2520and%2520BlendedMVS%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520state-of-the-art%2520methods%2520with%2520a%2520speedup%2520of%252060x%2520to%250A200x%252C%2520achieving%2520swift%2520and%2520fine-grained%2520mesh%2520reconstruction%2520without%2520the%2520need%2520for%250Acostly%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FatesGS%3A%20Fast%20and%20Accurate%20Sparse-View%20Surface%20Reconstruction%20using%0A%20%20Gaussian%20Splatting%20with%20Depth-Feature%20Consistency&entry.906535625=Han%20Huang%20and%20Yulun%20Wu%20and%20Chao%20Deng%20and%20Ge%20Gao%20and%20Ming%20Gu%20and%20Yu-Shen%20Liu&entry.1292438233=%20%20Recently%2C%20Gaussian%20Splatting%20has%20sparked%20a%20new%20trend%20in%20the%20field%20of%20computer%0Avision.%20Apart%20from%20novel%20view%20synthesis%2C%20it%20has%20also%20been%20extended%20to%20the%20area%0Aof%20multi-view%20reconstruction.%20The%20latest%20methods%20facilitate%20complete%2C%20detailed%0Asurface%20reconstruction%20while%20ensuring%20fast%20training%20speed.%20However%2C%20these%0Amethods%20still%20require%20dense%20input%20views%2C%20and%20their%20output%20quality%20significantly%0Adegrades%20with%20sparse%20views.%20We%20observed%20that%20the%20Gaussian%20primitives%20tend%20to%0Aoverfit%20the%20few%20training%20views%2C%20leading%20to%20noisy%20floaters%20and%20incomplete%0Areconstruction%20surfaces.%20In%20this%20paper%2C%20we%20present%20an%20innovative%20sparse-view%0Areconstruction%20framework%20that%20leverages%20intra-view%20depth%20and%20multi-view%20feature%0Aconsistency%20to%20achieve%20remarkably%20accurate%20surface%20reconstruction.%0ASpecifically%2C%20we%20utilize%20monocular%20depth%20ranking%20information%20to%20supervise%20the%0Aconsistency%20of%20depth%20distribution%20within%20patches%20and%20employ%20a%20smoothness%20loss%0Ato%20enhance%20the%20continuity%20of%20the%20distribution.%20To%20achieve%20finer%20surface%0Areconstruction%2C%20we%20optimize%20the%20absolute%20position%20of%20depth%20through%20multi-view%0Aprojection%20features.%20Extensive%20experiments%20on%20DTU%20and%20BlendedMVS%20demonstrate%0Athat%20our%20method%20outperforms%20state-of-the-art%20methods%20with%20a%20speedup%20of%2060x%20to%0A200x%2C%20achieving%20swift%20and%20fine-grained%20mesh%20reconstruction%20without%20the%20need%20for%0Acostly%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04628v1&entry.124074799=Read"},
{"title": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single\n  Images", "author": "Zixuan Huang and Mark Boss and Aaryaman Vasishta and James M. Rehg and Varun Jampani", "abstract": "  We study the problem of single-image 3D object reconstruction. Recent works\nhave diverged into two directions: regression-based modeling and generative\nmodeling. Regression methods efficiently infer visible surfaces, but struggle\nwith occluded regions. Generative methods handle uncertain regions better by\nmodeling distributions, but are computationally expensive and the generation is\noften misaligned with visible surfaces. In this paper, we present SPAR3D, a\nnovel two-stage approach aiming to take the best of both directions. The first\nstage of SPAR3D generates sparse 3D point clouds using a lightweight point\ndiffusion model, which has a fast sampling speed. The second stage uses both\nthe sampled point cloud and the input image to create highly detailed meshes.\nOur two-stage design enables probabilistic modeling of the ill-posed\nsingle-image 3D task while maintaining high computational efficiency and great\noutput fidelity. Using point clouds as an intermediate representation further\nallows for interactive user edits. Evaluated on diverse datasets, SPAR3D\ndemonstrates superior performance over previous state-of-the-art methods, at an\ninference speed of 0.7 seconds. Project page with code and model:\nhttps://spar3d.github.io\n", "link": "http://arxiv.org/abs/2501.04689v1", "date": "2025-01-08", "relevancy": 3.3437, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.678}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.678}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAR3D%3A%20Stable%20Point-Aware%20Reconstruction%20of%203D%20Objects%20from%20Single%0A%20%20Images&body=Title%3A%20SPAR3D%3A%20Stable%20Point-Aware%20Reconstruction%20of%203D%20Objects%20from%20Single%0A%20%20Images%0AAuthor%3A%20Zixuan%20Huang%20and%20Mark%20Boss%20and%20Aaryaman%20Vasishta%20and%20James%20M.%20Rehg%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20single-image%203D%20object%20reconstruction.%20Recent%20works%0Ahave%20diverged%20into%20two%20directions%3A%20regression-based%20modeling%20and%20generative%0Amodeling.%20Regression%20methods%20efficiently%20infer%20visible%20surfaces%2C%20but%20struggle%0Awith%20occluded%20regions.%20Generative%20methods%20handle%20uncertain%20regions%20better%20by%0Amodeling%20distributions%2C%20but%20are%20computationally%20expensive%20and%20the%20generation%20is%0Aoften%20misaligned%20with%20visible%20surfaces.%20In%20this%20paper%2C%20we%20present%20SPAR3D%2C%20a%0Anovel%20two-stage%20approach%20aiming%20to%20take%20the%20best%20of%20both%20directions.%20The%20first%0Astage%20of%20SPAR3D%20generates%20sparse%203D%20point%20clouds%20using%20a%20lightweight%20point%0Adiffusion%20model%2C%20which%20has%20a%20fast%20sampling%20speed.%20The%20second%20stage%20uses%20both%0Athe%20sampled%20point%20cloud%20and%20the%20input%20image%20to%20create%20highly%20detailed%20meshes.%0AOur%20two-stage%20design%20enables%20probabilistic%20modeling%20of%20the%20ill-posed%0Asingle-image%203D%20task%20while%20maintaining%20high%20computational%20efficiency%20and%20great%0Aoutput%20fidelity.%20Using%20point%20clouds%20as%20an%20intermediate%20representation%20further%0Aallows%20for%20interactive%20user%20edits.%20Evaluated%20on%20diverse%20datasets%2C%20SPAR3D%0Ademonstrates%20superior%20performance%20over%20previous%20state-of-the-art%20methods%2C%20at%20an%0Ainference%20speed%20of%200.7%20seconds.%20Project%20page%20with%20code%20and%20model%3A%0Ahttps%3A//spar3d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAR3D%253A%2520Stable%2520Point-Aware%2520Reconstruction%2520of%25203D%2520Objects%2520from%2520Single%250A%2520%2520Images%26entry.906535625%3DZixuan%2520Huang%2520and%2520Mark%2520Boss%2520and%2520Aaryaman%2520Vasishta%2520and%2520James%2520M.%2520Rehg%2520and%2520Varun%2520Jampani%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520single-image%25203D%2520object%2520reconstruction.%2520Recent%2520works%250Ahave%2520diverged%2520into%2520two%2520directions%253A%2520regression-based%2520modeling%2520and%2520generative%250Amodeling.%2520Regression%2520methods%2520efficiently%2520infer%2520visible%2520surfaces%252C%2520but%2520struggle%250Awith%2520occluded%2520regions.%2520Generative%2520methods%2520handle%2520uncertain%2520regions%2520better%2520by%250Amodeling%2520distributions%252C%2520but%2520are%2520computationally%2520expensive%2520and%2520the%2520generation%2520is%250Aoften%2520misaligned%2520with%2520visible%2520surfaces.%2520In%2520this%2520paper%252C%2520we%2520present%2520SPAR3D%252C%2520a%250Anovel%2520two-stage%2520approach%2520aiming%2520to%2520take%2520the%2520best%2520of%2520both%2520directions.%2520The%2520first%250Astage%2520of%2520SPAR3D%2520generates%2520sparse%25203D%2520point%2520clouds%2520using%2520a%2520lightweight%2520point%250Adiffusion%2520model%252C%2520which%2520has%2520a%2520fast%2520sampling%2520speed.%2520The%2520second%2520stage%2520uses%2520both%250Athe%2520sampled%2520point%2520cloud%2520and%2520the%2520input%2520image%2520to%2520create%2520highly%2520detailed%2520meshes.%250AOur%2520two-stage%2520design%2520enables%2520probabilistic%2520modeling%2520of%2520the%2520ill-posed%250Asingle-image%25203D%2520task%2520while%2520maintaining%2520high%2520computational%2520efficiency%2520and%2520great%250Aoutput%2520fidelity.%2520Using%2520point%2520clouds%2520as%2520an%2520intermediate%2520representation%2520further%250Aallows%2520for%2520interactive%2520user%2520edits.%2520Evaluated%2520on%2520diverse%2520datasets%252C%2520SPAR3D%250Ademonstrates%2520superior%2520performance%2520over%2520previous%2520state-of-the-art%2520methods%252C%2520at%2520an%250Ainference%2520speed%2520of%25200.7%2520seconds.%2520Project%2520page%2520with%2520code%2520and%2520model%253A%250Ahttps%253A//spar3d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAR3D%3A%20Stable%20Point-Aware%20Reconstruction%20of%203D%20Objects%20from%20Single%0A%20%20Images&entry.906535625=Zixuan%20Huang%20and%20Mark%20Boss%20and%20Aaryaman%20Vasishta%20and%20James%20M.%20Rehg%20and%20Varun%20Jampani&entry.1292438233=%20%20We%20study%20the%20problem%20of%20single-image%203D%20object%20reconstruction.%20Recent%20works%0Ahave%20diverged%20into%20two%20directions%3A%20regression-based%20modeling%20and%20generative%0Amodeling.%20Regression%20methods%20efficiently%20infer%20visible%20surfaces%2C%20but%20struggle%0Awith%20occluded%20regions.%20Generative%20methods%20handle%20uncertain%20regions%20better%20by%0Amodeling%20distributions%2C%20but%20are%20computationally%20expensive%20and%20the%20generation%20is%0Aoften%20misaligned%20with%20visible%20surfaces.%20In%20this%20paper%2C%20we%20present%20SPAR3D%2C%20a%0Anovel%20two-stage%20approach%20aiming%20to%20take%20the%20best%20of%20both%20directions.%20The%20first%0Astage%20of%20SPAR3D%20generates%20sparse%203D%20point%20clouds%20using%20a%20lightweight%20point%0Adiffusion%20model%2C%20which%20has%20a%20fast%20sampling%20speed.%20The%20second%20stage%20uses%20both%0Athe%20sampled%20point%20cloud%20and%20the%20input%20image%20to%20create%20highly%20detailed%20meshes.%0AOur%20two-stage%20design%20enables%20probabilistic%20modeling%20of%20the%20ill-posed%0Asingle-image%203D%20task%20while%20maintaining%20high%20computational%20efficiency%20and%20great%0Aoutput%20fidelity.%20Using%20point%20clouds%20as%20an%20intermediate%20representation%20further%0Aallows%20for%20interactive%20user%20edits.%20Evaluated%20on%20diverse%20datasets%2C%20SPAR3D%0Ademonstrates%20superior%20performance%20over%20previous%20state-of-the-art%20methods%2C%20at%20an%0Ainference%20speed%20of%200.7%20seconds.%20Project%20page%20with%20code%20and%20model%3A%0Ahttps%3A//spar3d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04689v1&entry.124074799=Read"},
{"title": "Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained\n  Tiling", "author": "Hao Gui and Lin Hu and Rui Chen and Mingxiao Huang and Yuxin Yin and Jin Yang and Yong Wu and Chen Liu and Zhongxu Sun and Xueyang Zhang and Kun Zhan", "abstract": "  3D Gaussian Splatting (3DGS) is increasingly attracting attention in both\nacademia and industry owing to its superior visual quality and rendering speed.\nHowever, training a 3DGS model remains a time-intensive task, especially in\nload imbalance scenarios where workload diversity among pixels and Gaussian\nspheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,\na Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS\ntraining process, perfectly solving load-imbalance issues. First, we\ninnovatively introduce the inter-block dynamic workload distribution technique\nto map workloads to Streaming Multiprocessor(SM) resources within a single GPU\ndynamically, which constitutes the foundation of load balancing. Second, we are\nthe first to propose the Gaussian-wise parallel rendering technique to\nsignificantly reduce workload divergence inside a warp, which serves as a\ncritical component in addressing load imbalance. Based on the above two\nmethods, we further creatively put forward the fine-grained combined load\nbalancing technique to uniformly distribute workload across all SMs, which\nboosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we\npresent a self-adaptive render kernel selection strategy during the 3DGS\ntraining process based on different load-balance situations, which effectively\nimproves training efficiency.\n", "link": "http://arxiv.org/abs/2412.17378v3", "date": "2025-01-08", "relevancy": 3.1482, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6413}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.633}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balanced%203DGS%3A%20Gaussian-wise%20Parallelism%20Rendering%20with%20Fine-Grained%0A%20%20Tiling&body=Title%3A%20Balanced%203DGS%3A%20Gaussian-wise%20Parallelism%20Rendering%20with%20Fine-Grained%0A%20%20Tiling%0AAuthor%3A%20Hao%20Gui%20and%20Lin%20Hu%20and%20Rui%20Chen%20and%20Mingxiao%20Huang%20and%20Yuxin%20Yin%20and%20Jin%20Yang%20and%20Yong%20Wu%20and%20Chen%20Liu%20and%20Zhongxu%20Sun%20and%20Xueyang%20Zhang%20and%20Kun%20Zhan%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20increasingly%20attracting%20attention%20in%20both%0Aacademia%20and%20industry%20owing%20to%20its%20superior%20visual%20quality%20and%20rendering%20speed.%0AHowever%2C%20training%20a%203DGS%20model%20remains%20a%20time-intensive%20task%2C%20especially%20in%0Aload%20imbalance%20scenarios%20where%20workload%20diversity%20among%20pixels%20and%20Gaussian%0Aspheres%20causes%20poor%20renderCUDA%20kernel%20performance.%20We%20introduce%20Balanced%203DGS%2C%0Aa%20Gaussian-wise%20parallelism%20rendering%20with%20fine-grained%20tiling%20approach%20in%203DGS%0Atraining%20process%2C%20perfectly%20solving%20load-imbalance%20issues.%20First%2C%20we%0Ainnovatively%20introduce%20the%20inter-block%20dynamic%20workload%20distribution%20technique%0Ato%20map%20workloads%20to%20Streaming%20Multiprocessor%28SM%29%20resources%20within%20a%20single%20GPU%0Adynamically%2C%20which%20constitutes%20the%20foundation%20of%20load%20balancing.%20Second%2C%20we%20are%0Athe%20first%20to%20propose%20the%20Gaussian-wise%20parallel%20rendering%20technique%20to%0Asignificantly%20reduce%20workload%20divergence%20inside%20a%20warp%2C%20which%20serves%20as%20a%0Acritical%20component%20in%20addressing%20load%20imbalance.%20Based%20on%20the%20above%20two%0Amethods%2C%20we%20further%20creatively%20put%20forward%20the%20fine-grained%20combined%20load%0Abalancing%20technique%20to%20uniformly%20distribute%20workload%20across%20all%20SMs%2C%20which%0Aboosts%20the%20forward%20renderCUDA%20kernel%20performance%20by%20up%20to%207.52x.%20Besides%2C%20we%0Apresent%20a%20self-adaptive%20render%20kernel%20selection%20strategy%20during%20the%203DGS%0Atraining%20process%20based%20on%20different%20load-balance%20situations%2C%20which%20effectively%0Aimproves%20training%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17378v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalanced%25203DGS%253A%2520Gaussian-wise%2520Parallelism%2520Rendering%2520with%2520Fine-Grained%250A%2520%2520Tiling%26entry.906535625%3DHao%2520Gui%2520and%2520Lin%2520Hu%2520and%2520Rui%2520Chen%2520and%2520Mingxiao%2520Huang%2520and%2520Yuxin%2520Yin%2520and%2520Jin%2520Yang%2520and%2520Yong%2520Wu%2520and%2520Chen%2520Liu%2520and%2520Zhongxu%2520Sun%2520and%2520Xueyang%2520Zhang%2520and%2520Kun%2520Zhan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520increasingly%2520attracting%2520attention%2520in%2520both%250Aacademia%2520and%2520industry%2520owing%2520to%2520its%2520superior%2520visual%2520quality%2520and%2520rendering%2520speed.%250AHowever%252C%2520training%2520a%25203DGS%2520model%2520remains%2520a%2520time-intensive%2520task%252C%2520especially%2520in%250Aload%2520imbalance%2520scenarios%2520where%2520workload%2520diversity%2520among%2520pixels%2520and%2520Gaussian%250Aspheres%2520causes%2520poor%2520renderCUDA%2520kernel%2520performance.%2520We%2520introduce%2520Balanced%25203DGS%252C%250Aa%2520Gaussian-wise%2520parallelism%2520rendering%2520with%2520fine-grained%2520tiling%2520approach%2520in%25203DGS%250Atraining%2520process%252C%2520perfectly%2520solving%2520load-imbalance%2520issues.%2520First%252C%2520we%250Ainnovatively%2520introduce%2520the%2520inter-block%2520dynamic%2520workload%2520distribution%2520technique%250Ato%2520map%2520workloads%2520to%2520Streaming%2520Multiprocessor%2528SM%2529%2520resources%2520within%2520a%2520single%2520GPU%250Adynamically%252C%2520which%2520constitutes%2520the%2520foundation%2520of%2520load%2520balancing.%2520Second%252C%2520we%2520are%250Athe%2520first%2520to%2520propose%2520the%2520Gaussian-wise%2520parallel%2520rendering%2520technique%2520to%250Asignificantly%2520reduce%2520workload%2520divergence%2520inside%2520a%2520warp%252C%2520which%2520serves%2520as%2520a%250Acritical%2520component%2520in%2520addressing%2520load%2520imbalance.%2520Based%2520on%2520the%2520above%2520two%250Amethods%252C%2520we%2520further%2520creatively%2520put%2520forward%2520the%2520fine-grained%2520combined%2520load%250Abalancing%2520technique%2520to%2520uniformly%2520distribute%2520workload%2520across%2520all%2520SMs%252C%2520which%250Aboosts%2520the%2520forward%2520renderCUDA%2520kernel%2520performance%2520by%2520up%2520to%25207.52x.%2520Besides%252C%2520we%250Apresent%2520a%2520self-adaptive%2520render%2520kernel%2520selection%2520strategy%2520during%2520the%25203DGS%250Atraining%2520process%2520based%2520on%2520different%2520load-balance%2520situations%252C%2520which%2520effectively%250Aimproves%2520training%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17378v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balanced%203DGS%3A%20Gaussian-wise%20Parallelism%20Rendering%20with%20Fine-Grained%0A%20%20Tiling&entry.906535625=Hao%20Gui%20and%20Lin%20Hu%20and%20Rui%20Chen%20and%20Mingxiao%20Huang%20and%20Yuxin%20Yin%20and%20Jin%20Yang%20and%20Yong%20Wu%20and%20Chen%20Liu%20and%20Zhongxu%20Sun%20and%20Xueyang%20Zhang%20and%20Kun%20Zhan&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20increasingly%20attracting%20attention%20in%20both%0Aacademia%20and%20industry%20owing%20to%20its%20superior%20visual%20quality%20and%20rendering%20speed.%0AHowever%2C%20training%20a%203DGS%20model%20remains%20a%20time-intensive%20task%2C%20especially%20in%0Aload%20imbalance%20scenarios%20where%20workload%20diversity%20among%20pixels%20and%20Gaussian%0Aspheres%20causes%20poor%20renderCUDA%20kernel%20performance.%20We%20introduce%20Balanced%203DGS%2C%0Aa%20Gaussian-wise%20parallelism%20rendering%20with%20fine-grained%20tiling%20approach%20in%203DGS%0Atraining%20process%2C%20perfectly%20solving%20load-imbalance%20issues.%20First%2C%20we%0Ainnovatively%20introduce%20the%20inter-block%20dynamic%20workload%20distribution%20technique%0Ato%20map%20workloads%20to%20Streaming%20Multiprocessor%28SM%29%20resources%20within%20a%20single%20GPU%0Adynamically%2C%20which%20constitutes%20the%20foundation%20of%20load%20balancing.%20Second%2C%20we%20are%0Athe%20first%20to%20propose%20the%20Gaussian-wise%20parallel%20rendering%20technique%20to%0Asignificantly%20reduce%20workload%20divergence%20inside%20a%20warp%2C%20which%20serves%20as%20a%0Acritical%20component%20in%20addressing%20load%20imbalance.%20Based%20on%20the%20above%20two%0Amethods%2C%20we%20further%20creatively%20put%20forward%20the%20fine-grained%20combined%20load%0Abalancing%20technique%20to%20uniformly%20distribute%20workload%20across%20all%20SMs%2C%20which%0Aboosts%20the%20forward%20renderCUDA%20kernel%20performance%20by%20up%20to%207.52x.%20Besides%2C%20we%0Apresent%20a%20self-adaptive%20render%20kernel%20selection%20strategy%20during%20the%203DGS%0Atraining%20process%20based%20on%20different%20load-balance%20situations%2C%20which%20effectively%0Aimproves%20training%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17378v3&entry.124074799=Read"},
{"title": "PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored\n  Point Cloud", "author": "Qiao Yu and Xianzhi Li and Yuan Tang and Xu Han and Jinfeng Xu and Long Hu and Min Chen", "abstract": "  Reconstructing textured meshes from colored point clouds is an important but\nchallenging task. Most existing methods yield blurry-looking textures or rely\non 3D training data that are hard to acquire. Regarding this, we propose\nPointDreamer, a novel framework for textured mesh reconstruction from colored\npoint cloud via diffusion-based 2D inpainting. Specifically, we first\nreconstruct an untextured mesh. Next, we project the input point cloud into 2D\nspace to generate sparse multi-view images, and then inpaint empty pixels\nutilizing a pre-trained 2D diffusion model. After that, we unproject the colors\nof the inpainted dense images onto the untextured mesh, thus obtaining the\nfinal textured mesh. This project-inpaint-unproject pipeline bridges the gap\nbetween 3D point clouds and 2D diffusion models for the first time. Thanks to\nthe powerful 2D diffusion model pre-trained on extensive 2D data, PointDreamer\nreconstructs clear, high-quality textures with high robustness to sparse or\nnoisy input. Also, it's zero-shot requiring no extra training. In addition, we\ndesign Non-Border-First unprojection strategy to address the border-area\ninconsistency issue, which is less explored but commonly-occurred in methods\nthat generate 3D textures from multiview images. Extensive qualitative and\nquantitative experiments on various synthetic and real-scanned datasets show\nthe SoTA performance of PointDreamer, by significantly outperforming baseline\nmethods with 30% improvement in LPIPS score (from 0.118 to 0.068). Code at:\nhttps://github.com/YuQiao0303/PointDreamer.\n", "link": "http://arxiv.org/abs/2406.15811v2", "date": "2025-01-08", "relevancy": 3.0976, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6233}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointDreamer%3A%20Zero-shot%203D%20Textured%20Mesh%20Reconstruction%20from%20Colored%0A%20%20Point%20Cloud&body=Title%3A%20PointDreamer%3A%20Zero-shot%203D%20Textured%20Mesh%20Reconstruction%20from%20Colored%0A%20%20Point%20Cloud%0AAuthor%3A%20Qiao%20Yu%20and%20Xianzhi%20Li%20and%20Yuan%20Tang%20and%20Xu%20Han%20and%20Jinfeng%20Xu%20and%20Long%20Hu%20and%20Min%20Chen%0AAbstract%3A%20%20%20Reconstructing%20textured%20meshes%20from%20colored%20point%20clouds%20is%20an%20important%20but%0Achallenging%20task.%20Most%20existing%20methods%20yield%20blurry-looking%20textures%20or%20rely%0Aon%203D%20training%20data%20that%20are%20hard%20to%20acquire.%20Regarding%20this%2C%20we%20propose%0APointDreamer%2C%20a%20novel%20framework%20for%20textured%20mesh%20reconstruction%20from%20colored%0Apoint%20cloud%20via%20diffusion-based%202D%20inpainting.%20Specifically%2C%20we%20first%0Areconstruct%20an%20untextured%20mesh.%20Next%2C%20we%20project%20the%20input%20point%20cloud%20into%202D%0Aspace%20to%20generate%20sparse%20multi-view%20images%2C%20and%20then%20inpaint%20empty%20pixels%0Autilizing%20a%20pre-trained%202D%20diffusion%20model.%20After%20that%2C%20we%20unproject%20the%20colors%0Aof%20the%20inpainted%20dense%20images%20onto%20the%20untextured%20mesh%2C%20thus%20obtaining%20the%0Afinal%20textured%20mesh.%20This%20project-inpaint-unproject%20pipeline%20bridges%20the%20gap%0Abetween%203D%20point%20clouds%20and%202D%20diffusion%20models%20for%20the%20first%20time.%20Thanks%20to%0Athe%20powerful%202D%20diffusion%20model%20pre-trained%20on%20extensive%202D%20data%2C%20PointDreamer%0Areconstructs%20clear%2C%20high-quality%20textures%20with%20high%20robustness%20to%20sparse%20or%0Anoisy%20input.%20Also%2C%20it%27s%20zero-shot%20requiring%20no%20extra%20training.%20In%20addition%2C%20we%0Adesign%20Non-Border-First%20unprojection%20strategy%20to%20address%20the%20border-area%0Ainconsistency%20issue%2C%20which%20is%20less%20explored%20but%20commonly-occurred%20in%20methods%0Athat%20generate%203D%20textures%20from%20multiview%20images.%20Extensive%20qualitative%20and%0Aquantitative%20experiments%20on%20various%20synthetic%20and%20real-scanned%20datasets%20show%0Athe%20SoTA%20performance%20of%20PointDreamer%2C%20by%20significantly%20outperforming%20baseline%0Amethods%20with%2030%25%20improvement%20in%20LPIPS%20score%20%28from%200.118%20to%200.068%29.%20Code%20at%3A%0Ahttps%3A//github.com/YuQiao0303/PointDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15811v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointDreamer%253A%2520Zero-shot%25203D%2520Textured%2520Mesh%2520Reconstruction%2520from%2520Colored%250A%2520%2520Point%2520Cloud%26entry.906535625%3DQiao%2520Yu%2520and%2520Xianzhi%2520Li%2520and%2520Yuan%2520Tang%2520and%2520Xu%2520Han%2520and%2520Jinfeng%2520Xu%2520and%2520Long%2520Hu%2520and%2520Min%2520Chen%26entry.1292438233%3D%2520%2520Reconstructing%2520textured%2520meshes%2520from%2520colored%2520point%2520clouds%2520is%2520an%2520important%2520but%250Achallenging%2520task.%2520Most%2520existing%2520methods%2520yield%2520blurry-looking%2520textures%2520or%2520rely%250Aon%25203D%2520training%2520data%2520that%2520are%2520hard%2520to%2520acquire.%2520Regarding%2520this%252C%2520we%2520propose%250APointDreamer%252C%2520a%2520novel%2520framework%2520for%2520textured%2520mesh%2520reconstruction%2520from%2520colored%250Apoint%2520cloud%2520via%2520diffusion-based%25202D%2520inpainting.%2520Specifically%252C%2520we%2520first%250Areconstruct%2520an%2520untextured%2520mesh.%2520Next%252C%2520we%2520project%2520the%2520input%2520point%2520cloud%2520into%25202D%250Aspace%2520to%2520generate%2520sparse%2520multi-view%2520images%252C%2520and%2520then%2520inpaint%2520empty%2520pixels%250Autilizing%2520a%2520pre-trained%25202D%2520diffusion%2520model.%2520After%2520that%252C%2520we%2520unproject%2520the%2520colors%250Aof%2520the%2520inpainted%2520dense%2520images%2520onto%2520the%2520untextured%2520mesh%252C%2520thus%2520obtaining%2520the%250Afinal%2520textured%2520mesh.%2520This%2520project-inpaint-unproject%2520pipeline%2520bridges%2520the%2520gap%250Abetween%25203D%2520point%2520clouds%2520and%25202D%2520diffusion%2520models%2520for%2520the%2520first%2520time.%2520Thanks%2520to%250Athe%2520powerful%25202D%2520diffusion%2520model%2520pre-trained%2520on%2520extensive%25202D%2520data%252C%2520PointDreamer%250Areconstructs%2520clear%252C%2520high-quality%2520textures%2520with%2520high%2520robustness%2520to%2520sparse%2520or%250Anoisy%2520input.%2520Also%252C%2520it%2527s%2520zero-shot%2520requiring%2520no%2520extra%2520training.%2520In%2520addition%252C%2520we%250Adesign%2520Non-Border-First%2520unprojection%2520strategy%2520to%2520address%2520the%2520border-area%250Ainconsistency%2520issue%252C%2520which%2520is%2520less%2520explored%2520but%2520commonly-occurred%2520in%2520methods%250Athat%2520generate%25203D%2520textures%2520from%2520multiview%2520images.%2520Extensive%2520qualitative%2520and%250Aquantitative%2520experiments%2520on%2520various%2520synthetic%2520and%2520real-scanned%2520datasets%2520show%250Athe%2520SoTA%2520performance%2520of%2520PointDreamer%252C%2520by%2520significantly%2520outperforming%2520baseline%250Amethods%2520with%252030%2525%2520improvement%2520in%2520LPIPS%2520score%2520%2528from%25200.118%2520to%25200.068%2529.%2520Code%2520at%253A%250Ahttps%253A//github.com/YuQiao0303/PointDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15811v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointDreamer%3A%20Zero-shot%203D%20Textured%20Mesh%20Reconstruction%20from%20Colored%0A%20%20Point%20Cloud&entry.906535625=Qiao%20Yu%20and%20Xianzhi%20Li%20and%20Yuan%20Tang%20and%20Xu%20Han%20and%20Jinfeng%20Xu%20and%20Long%20Hu%20and%20Min%20Chen&entry.1292438233=%20%20Reconstructing%20textured%20meshes%20from%20colored%20point%20clouds%20is%20an%20important%20but%0Achallenging%20task.%20Most%20existing%20methods%20yield%20blurry-looking%20textures%20or%20rely%0Aon%203D%20training%20data%20that%20are%20hard%20to%20acquire.%20Regarding%20this%2C%20we%20propose%0APointDreamer%2C%20a%20novel%20framework%20for%20textured%20mesh%20reconstruction%20from%20colored%0Apoint%20cloud%20via%20diffusion-based%202D%20inpainting.%20Specifically%2C%20we%20first%0Areconstruct%20an%20untextured%20mesh.%20Next%2C%20we%20project%20the%20input%20point%20cloud%20into%202D%0Aspace%20to%20generate%20sparse%20multi-view%20images%2C%20and%20then%20inpaint%20empty%20pixels%0Autilizing%20a%20pre-trained%202D%20diffusion%20model.%20After%20that%2C%20we%20unproject%20the%20colors%0Aof%20the%20inpainted%20dense%20images%20onto%20the%20untextured%20mesh%2C%20thus%20obtaining%20the%0Afinal%20textured%20mesh.%20This%20project-inpaint-unproject%20pipeline%20bridges%20the%20gap%0Abetween%203D%20point%20clouds%20and%202D%20diffusion%20models%20for%20the%20first%20time.%20Thanks%20to%0Athe%20powerful%202D%20diffusion%20model%20pre-trained%20on%20extensive%202D%20data%2C%20PointDreamer%0Areconstructs%20clear%2C%20high-quality%20textures%20with%20high%20robustness%20to%20sparse%20or%0Anoisy%20input.%20Also%2C%20it%27s%20zero-shot%20requiring%20no%20extra%20training.%20In%20addition%2C%20we%0Adesign%20Non-Border-First%20unprojection%20strategy%20to%20address%20the%20border-area%0Ainconsistency%20issue%2C%20which%20is%20less%20explored%20but%20commonly-occurred%20in%20methods%0Athat%20generate%203D%20textures%20from%20multiview%20images.%20Extensive%20qualitative%20and%0Aquantitative%20experiments%20on%20various%20synthetic%20and%20real-scanned%20datasets%20show%0Athe%20SoTA%20performance%20of%20PointDreamer%2C%20by%20significantly%20outperforming%20baseline%0Amethods%20with%2030%25%20improvement%20in%20LPIPS%20score%20%28from%200.118%20to%200.068%29.%20Code%20at%3A%0Ahttps%3A//github.com/YuQiao0303/PointDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15811v2&entry.124074799=Read"},
{"title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM", "author": "Yuqian Yuan and Hang Zhang and Wentong Li and Zesen Cheng and Boqiang Zhang and Long Li and Xin Li and Deli Zhao and Wenqiao Zhang and Yueting Zhuang and Jianke Zhu and Lidong Bing", "abstract": "  Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities.\n", "link": "http://arxiv.org/abs/2501.00599v2", "date": "2025-01-08", "relevancy": 3.0358, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6268}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoRefer%20Suite%3A%20Advancing%20Spatial-Temporal%20Object%20Understanding%20with%0A%20%20Video%20LLM&body=Title%3A%20VideoRefer%20Suite%3A%20Advancing%20Spatial-Temporal%20Object%20Understanding%20with%0A%20%20Video%20LLM%0AAuthor%3A%20Yuqian%20Yuan%20and%20Hang%20Zhang%20and%20Wentong%20Li%20and%20Zesen%20Cheng%20and%20Boqiang%20Zhang%20and%20Long%20Li%20and%20Xin%20Li%20and%20Deli%20Zhao%20and%20Wenqiao%20Zhang%20and%20Yueting%20Zhuang%20and%20Jianke%20Zhu%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20Video%20Large%20Language%20Models%20%28Video%20LLMs%29%20have%20recently%20exhibited%20remarkable%0Acapabilities%20in%20general%20video%20understanding.%20However%2C%20they%20mainly%20focus%20on%0Aholistic%20comprehension%20and%20struggle%20with%20capturing%20fine-grained%20spatial%20and%0Atemporal%20details.%20Besides%2C%20the%20lack%20of%20high-quality%20object-level%20video%0Ainstruction%20data%20and%20a%20comprehensive%20benchmark%20further%20hinders%20their%0Aadvancements.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20the%20VideoRefer%20Suite%20to%0Aempower%20Video%20LLM%20for%20finer-level%20spatial-temporal%20video%20understanding%2C%20i.e.%2C%0Aenabling%20perception%20and%20reasoning%20on%20any%20objects%20throughout%20the%20video.%0ASpecially%2C%20we%20thoroughly%20develop%20VideoRefer%20Suite%20across%20three%20essential%0Aaspects%3A%20dataset%2C%20model%2C%20and%20benchmark.%20Firstly%2C%20we%20introduce%20a%20multi-agent%0Adata%20engine%20to%20meticulously%20curate%20a%20large-scale%2C%20high-quality%20object-level%0Avideo%20instruction%20dataset%2C%20termed%20VideoRefer-700K.%20Next%2C%20we%20present%20the%0AVideoRefer%20model%2C%20which%20equips%20a%20versatile%20spatial-temporal%20object%20encoder%20to%0Acapture%20precise%20regional%20and%20sequential%20representations.%20Finally%2C%20we%0Ameticulously%20create%20a%20VideoRefer-Bench%20to%20comprehensively%20assess%20the%0Aspatial-temporal%20understanding%20capability%20of%20a%20Video%20LLM%2C%20evaluating%20it%20across%0Avarious%20aspects.%20Extensive%20experiments%20and%20analyses%20demonstrate%20that%20our%0AVideoRefer%20model%20not%20only%20achieves%20promising%20performance%20on%20video%20referring%0Abenchmarks%20but%20also%20facilitates%20general%20video%20understanding%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoRefer%2520Suite%253A%2520Advancing%2520Spatial-Temporal%2520Object%2520Understanding%2520with%250A%2520%2520Video%2520LLM%26entry.906535625%3DYuqian%2520Yuan%2520and%2520Hang%2520Zhang%2520and%2520Wentong%2520Li%2520and%2520Zesen%2520Cheng%2520and%2520Boqiang%2520Zhang%2520and%2520Long%2520Li%2520and%2520Xin%2520Li%2520and%2520Deli%2520Zhao%2520and%2520Wenqiao%2520Zhang%2520and%2520Yueting%2520Zhuang%2520and%2520Jianke%2520Zhu%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520Video%2520Large%2520Language%2520Models%2520%2528Video%2520LLMs%2529%2520have%2520recently%2520exhibited%2520remarkable%250Acapabilities%2520in%2520general%2520video%2520understanding.%2520However%252C%2520they%2520mainly%2520focus%2520on%250Aholistic%2520comprehension%2520and%2520struggle%2520with%2520capturing%2520fine-grained%2520spatial%2520and%250Atemporal%2520details.%2520Besides%252C%2520the%2520lack%2520of%2520high-quality%2520object-level%2520video%250Ainstruction%2520data%2520and%2520a%2520comprehensive%2520benchmark%2520further%2520hinders%2520their%250Aadvancements.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520VideoRefer%2520Suite%2520to%250Aempower%2520Video%2520LLM%2520for%2520finer-level%2520spatial-temporal%2520video%2520understanding%252C%2520i.e.%252C%250Aenabling%2520perception%2520and%2520reasoning%2520on%2520any%2520objects%2520throughout%2520the%2520video.%250ASpecially%252C%2520we%2520thoroughly%2520develop%2520VideoRefer%2520Suite%2520across%2520three%2520essential%250Aaspects%253A%2520dataset%252C%2520model%252C%2520and%2520benchmark.%2520Firstly%252C%2520we%2520introduce%2520a%2520multi-agent%250Adata%2520engine%2520to%2520meticulously%2520curate%2520a%2520large-scale%252C%2520high-quality%2520object-level%250Avideo%2520instruction%2520dataset%252C%2520termed%2520VideoRefer-700K.%2520Next%252C%2520we%2520present%2520the%250AVideoRefer%2520model%252C%2520which%2520equips%2520a%2520versatile%2520spatial-temporal%2520object%2520encoder%2520to%250Acapture%2520precise%2520regional%2520and%2520sequential%2520representations.%2520Finally%252C%2520we%250Ameticulously%2520create%2520a%2520VideoRefer-Bench%2520to%2520comprehensively%2520assess%2520the%250Aspatial-temporal%2520understanding%2520capability%2520of%2520a%2520Video%2520LLM%252C%2520evaluating%2520it%2520across%250Avarious%2520aspects.%2520Extensive%2520experiments%2520and%2520analyses%2520demonstrate%2520that%2520our%250AVideoRefer%2520model%2520not%2520only%2520achieves%2520promising%2520performance%2520on%2520video%2520referring%250Abenchmarks%2520but%2520also%2520facilitates%2520general%2520video%2520understanding%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoRefer%20Suite%3A%20Advancing%20Spatial-Temporal%20Object%20Understanding%20with%0A%20%20Video%20LLM&entry.906535625=Yuqian%20Yuan%20and%20Hang%20Zhang%20and%20Wentong%20Li%20and%20Zesen%20Cheng%20and%20Boqiang%20Zhang%20and%20Long%20Li%20and%20Xin%20Li%20and%20Deli%20Zhao%20and%20Wenqiao%20Zhang%20and%20Yueting%20Zhuang%20and%20Jianke%20Zhu%20and%20Lidong%20Bing&entry.1292438233=%20%20Video%20Large%20Language%20Models%20%28Video%20LLMs%29%20have%20recently%20exhibited%20remarkable%0Acapabilities%20in%20general%20video%20understanding.%20However%2C%20they%20mainly%20focus%20on%0Aholistic%20comprehension%20and%20struggle%20with%20capturing%20fine-grained%20spatial%20and%0Atemporal%20details.%20Besides%2C%20the%20lack%20of%20high-quality%20object-level%20video%0Ainstruction%20data%20and%20a%20comprehensive%20benchmark%20further%20hinders%20their%0Aadvancements.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20the%20VideoRefer%20Suite%20to%0Aempower%20Video%20LLM%20for%20finer-level%20spatial-temporal%20video%20understanding%2C%20i.e.%2C%0Aenabling%20perception%20and%20reasoning%20on%20any%20objects%20throughout%20the%20video.%0ASpecially%2C%20we%20thoroughly%20develop%20VideoRefer%20Suite%20across%20three%20essential%0Aaspects%3A%20dataset%2C%20model%2C%20and%20benchmark.%20Firstly%2C%20we%20introduce%20a%20multi-agent%0Adata%20engine%20to%20meticulously%20curate%20a%20large-scale%2C%20high-quality%20object-level%0Avideo%20instruction%20dataset%2C%20termed%20VideoRefer-700K.%20Next%2C%20we%20present%20the%0AVideoRefer%20model%2C%20which%20equips%20a%20versatile%20spatial-temporal%20object%20encoder%20to%0Acapture%20precise%20regional%20and%20sequential%20representations.%20Finally%2C%20we%0Ameticulously%20create%20a%20VideoRefer-Bench%20to%20comprehensively%20assess%20the%0Aspatial-temporal%20understanding%20capability%20of%20a%20Video%20LLM%2C%20evaluating%20it%20across%0Avarious%20aspects.%20Extensive%20experiments%20and%20analyses%20demonstrate%20that%20our%0AVideoRefer%20model%20not%20only%20achieves%20promising%20performance%20on%20video%20referring%0Abenchmarks%20but%20also%20facilitates%20general%20video%20understanding%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00599v2&entry.124074799=Read"},
{"title": "Disentangled Clothed Avatar Generation with Layered Representation", "author": "Weitian Zhang and Sijing Wu and Manwen Liao and Yichao Yan", "abstract": "  Clothed avatar generation has wide applications in virtual and augmented\nreality, filmmaking, and more. Previous methods have achieved success in\ngenerating diverse digital avatars, however, generating avatars with\ndisentangled components (\\eg, body, hair, and clothes) has long been a\nchallenge. In this paper, we propose LayerAvatar, the first feed-forward\ndiffusion-based method for generating component-disentangled clothed avatars.\nTo achieve this, we first propose a layered UV feature plane representation,\nwhere components are distributed in different layers of the Gaussian-based UV\nfeature plane with corresponding semantic labels. This representation supports\nhigh-resolution and real-time rendering, as well as expressive animation\nincluding controllable gestures and facial expressions. Based on the\nwell-designed representation, we train a single-stage diffusion model and\nintroduce constrain terms to address the severe occlusion problem of the\ninnermost human body layer. Extensive experiments demonstrate the impressive\nperformances of our method in generating disentangled clothed avatars, and we\nfurther explore its applications in component transfer. The project page is\navailable at: https://olivia23333.github.io/LayerAvatar/\n", "link": "http://arxiv.org/abs/2501.04631v1", "date": "2025-01-08", "relevancy": 3.0246, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6094}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6049}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Clothed%20Avatar%20Generation%20with%20Layered%20Representation&body=Title%3A%20Disentangled%20Clothed%20Avatar%20Generation%20with%20Layered%20Representation%0AAuthor%3A%20Weitian%20Zhang%20and%20Sijing%20Wu%20and%20Manwen%20Liao%20and%20Yichao%20Yan%0AAbstract%3A%20%20%20Clothed%20avatar%20generation%20has%20wide%20applications%20in%20virtual%20and%20augmented%0Areality%2C%20filmmaking%2C%20and%20more.%20Previous%20methods%20have%20achieved%20success%20in%0Agenerating%20diverse%20digital%20avatars%2C%20however%2C%20generating%20avatars%20with%0Adisentangled%20components%20%28%5Ceg%2C%20body%2C%20hair%2C%20and%20clothes%29%20has%20long%20been%20a%0Achallenge.%20In%20this%20paper%2C%20we%20propose%20LayerAvatar%2C%20the%20first%20feed-forward%0Adiffusion-based%20method%20for%20generating%20component-disentangled%20clothed%20avatars.%0ATo%20achieve%20this%2C%20we%20first%20propose%20a%20layered%20UV%20feature%20plane%20representation%2C%0Awhere%20components%20are%20distributed%20in%20different%20layers%20of%20the%20Gaussian-based%20UV%0Afeature%20plane%20with%20corresponding%20semantic%20labels.%20This%20representation%20supports%0Ahigh-resolution%20and%20real-time%20rendering%2C%20as%20well%20as%20expressive%20animation%0Aincluding%20controllable%20gestures%20and%20facial%20expressions.%20Based%20on%20the%0Awell-designed%20representation%2C%20we%20train%20a%20single-stage%20diffusion%20model%20and%0Aintroduce%20constrain%20terms%20to%20address%20the%20severe%20occlusion%20problem%20of%20the%0Ainnermost%20human%20body%20layer.%20Extensive%20experiments%20demonstrate%20the%20impressive%0Aperformances%20of%20our%20method%20in%20generating%20disentangled%20clothed%20avatars%2C%20and%20we%0Afurther%20explore%20its%20applications%20in%20component%20transfer.%20The%20project%20page%20is%0Aavailable%20at%3A%20https%3A//olivia23333.github.io/LayerAvatar/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Clothed%2520Avatar%2520Generation%2520with%2520Layered%2520Representation%26entry.906535625%3DWeitian%2520Zhang%2520and%2520Sijing%2520Wu%2520and%2520Manwen%2520Liao%2520and%2520Yichao%2520Yan%26entry.1292438233%3D%2520%2520Clothed%2520avatar%2520generation%2520has%2520wide%2520applications%2520in%2520virtual%2520and%2520augmented%250Areality%252C%2520filmmaking%252C%2520and%2520more.%2520Previous%2520methods%2520have%2520achieved%2520success%2520in%250Agenerating%2520diverse%2520digital%2520avatars%252C%2520however%252C%2520generating%2520avatars%2520with%250Adisentangled%2520components%2520%2528%255Ceg%252C%2520body%252C%2520hair%252C%2520and%2520clothes%2529%2520has%2520long%2520been%2520a%250Achallenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LayerAvatar%252C%2520the%2520first%2520feed-forward%250Adiffusion-based%2520method%2520for%2520generating%2520component-disentangled%2520clothed%2520avatars.%250ATo%2520achieve%2520this%252C%2520we%2520first%2520propose%2520a%2520layered%2520UV%2520feature%2520plane%2520representation%252C%250Awhere%2520components%2520are%2520distributed%2520in%2520different%2520layers%2520of%2520the%2520Gaussian-based%2520UV%250Afeature%2520plane%2520with%2520corresponding%2520semantic%2520labels.%2520This%2520representation%2520supports%250Ahigh-resolution%2520and%2520real-time%2520rendering%252C%2520as%2520well%2520as%2520expressive%2520animation%250Aincluding%2520controllable%2520gestures%2520and%2520facial%2520expressions.%2520Based%2520on%2520the%250Awell-designed%2520representation%252C%2520we%2520train%2520a%2520single-stage%2520diffusion%2520model%2520and%250Aintroduce%2520constrain%2520terms%2520to%2520address%2520the%2520severe%2520occlusion%2520problem%2520of%2520the%250Ainnermost%2520human%2520body%2520layer.%2520Extensive%2520experiments%2520demonstrate%2520the%2520impressive%250Aperformances%2520of%2520our%2520method%2520in%2520generating%2520disentangled%2520clothed%2520avatars%252C%2520and%2520we%250Afurther%2520explore%2520its%2520applications%2520in%2520component%2520transfer.%2520The%2520project%2520page%2520is%250Aavailable%2520at%253A%2520https%253A//olivia23333.github.io/LayerAvatar/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Clothed%20Avatar%20Generation%20with%20Layered%20Representation&entry.906535625=Weitian%20Zhang%20and%20Sijing%20Wu%20and%20Manwen%20Liao%20and%20Yichao%20Yan&entry.1292438233=%20%20Clothed%20avatar%20generation%20has%20wide%20applications%20in%20virtual%20and%20augmented%0Areality%2C%20filmmaking%2C%20and%20more.%20Previous%20methods%20have%20achieved%20success%20in%0Agenerating%20diverse%20digital%20avatars%2C%20however%2C%20generating%20avatars%20with%0Adisentangled%20components%20%28%5Ceg%2C%20body%2C%20hair%2C%20and%20clothes%29%20has%20long%20been%20a%0Achallenge.%20In%20this%20paper%2C%20we%20propose%20LayerAvatar%2C%20the%20first%20feed-forward%0Adiffusion-based%20method%20for%20generating%20component-disentangled%20clothed%20avatars.%0ATo%20achieve%20this%2C%20we%20first%20propose%20a%20layered%20UV%20feature%20plane%20representation%2C%0Awhere%20components%20are%20distributed%20in%20different%20layers%20of%20the%20Gaussian-based%20UV%0Afeature%20plane%20with%20corresponding%20semantic%20labels.%20This%20representation%20supports%0Ahigh-resolution%20and%20real-time%20rendering%2C%20as%20well%20as%20expressive%20animation%0Aincluding%20controllable%20gestures%20and%20facial%20expressions.%20Based%20on%20the%0Awell-designed%20representation%2C%20we%20train%20a%20single-stage%20diffusion%20model%20and%0Aintroduce%20constrain%20terms%20to%20address%20the%20severe%20occlusion%20problem%20of%20the%0Ainnermost%20human%20body%20layer.%20Extensive%20experiments%20demonstrate%20the%20impressive%0Aperformances%20of%20our%20method%20in%20generating%20disentangled%20clothed%20avatars%2C%20and%20we%0Afurther%20explore%20its%20applications%20in%20component%20transfer.%20The%20project%20page%20is%0Aavailable%20at%3A%20https%3A//olivia23333.github.io/LayerAvatar/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04631v1&entry.124074799=Read"},
{"title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs", "author": "Yikang Zhou and Tao Zhang and Shilin Xu and Shihao Chen and Qianyu Zhou and Yunhai Tong and Shunping Ji and Jiangning Zhang and Xiangtai Li and Lu Qi", "abstract": "  Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.\n", "link": "http://arxiv.org/abs/2501.04670v1", "date": "2025-01-08", "relevancy": 2.9491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20They%20the%20Same%3F%20Exploring%20Visual%20Correspondence%20Shortcomings%20of%0A%20%20Multimodal%20LLMs&body=Title%3A%20Are%20They%20the%20Same%3F%20Exploring%20Visual%20Correspondence%20Shortcomings%20of%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Yikang%20Zhou%20and%20Tao%20Zhang%20and%20Shilin%20Xu%20and%20Shihao%20Chen%20and%20Qianyu%20Zhou%20and%20Yunhai%20Tong%20and%20Shunping%20Ji%20and%20Jiangning%20Zhang%20and%20Xiangtai%20Li%20and%20Lu%20Qi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20models%20have%20shown%20a%20strong%20ability%20in%0Avisual%20perception%2C%20reasoning%20abilities%2C%20and%20vision-language%20understanding.%0AHowever%2C%20studies%20on%20visual%20matching%20ability%20are%20missing%2C%20where%20finding%20the%0Avisual%20correspondence%20of%20objects%20is%20essential%20in%20vision%20research.%20Our%20research%0Areveals%20that%20the%20matching%20capabilities%20in%20recent%20multimodal%20LLMs%20%28MLLMs%29%20still%0Aexhibit%20systematic%20shortcomings%2C%20even%20with%20current%20strong%20MLLMs%20models%2C%20GPT-4o.%0AIn%20particular%2C%20we%20construct%20a%20Multimodal%20Visual%20Matching%20%28MMVM%29%20benchmark%20to%0Afairly%20benchmark%20over%2030%20different%20MLLMs.%20The%20MMVM%20benchmark%20is%20built%20from%2015%0Aopen-source%20datasets%20and%20Internet%20videos%20with%20manual%20annotation.%20We%20categorize%0Athe%20data%20samples%20of%20MMVM%20benchmark%20into%20eight%20aspects%20based%20on%20the%20required%0Acues%20and%20capabilities%20to%20more%20comprehensively%20evaluate%20and%20analyze%20current%0AMLLMs.%20In%20addition%2C%20we%20have%20designed%20an%20automatic%20annotation%20pipeline%20to%0Agenerate%20the%20MMVM%20SFT%20dataset%2C%20including%20220K%20visual%20matching%20data%20with%0Areasoning%20annotation.%20Finally%2C%20we%20present%20CoLVA%2C%20a%20novel%20contrastive%20MLLM%20with%0Atwo%20novel%20technical%20designs%3A%20fine-grained%20vision%20expert%20with%20object-level%0Acontrastive%20learning%20and%20instruction%20augmentation%20strategy.%20CoLVA%20achieves%0A51.06%5C%25%20overall%20accuracy%20%28OA%29%20on%20the%20MMVM%20benchmark%2C%20surpassing%20GPT-4o%20and%0Abaseline%20by%208.41%5C%25%20and%2023.58%5C%25%20OA%2C%20respectively.%20The%20results%20show%20the%0Aeffectiveness%20of%20our%20MMVM%20SFT%20dataset%20and%20our%20novel%20technical%20designs.%20Code%2C%0Abenchmark%2C%20dataset%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zhouyiks/CoLVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520They%2520the%2520Same%253F%2520Exploring%2520Visual%2520Correspondence%2520Shortcomings%2520of%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DYikang%2520Zhou%2520and%2520Tao%2520Zhang%2520and%2520Shilin%2520Xu%2520and%2520Shihao%2520Chen%2520and%2520Qianyu%2520Zhou%2520and%2520Yunhai%2520Tong%2520and%2520Shunping%2520Ji%2520and%2520Jiangning%2520Zhang%2520and%2520Xiangtai%2520Li%2520and%2520Lu%2520Qi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520models%2520have%2520shown%2520a%2520strong%2520ability%2520in%250Avisual%2520perception%252C%2520reasoning%2520abilities%252C%2520and%2520vision-language%2520understanding.%250AHowever%252C%2520studies%2520on%2520visual%2520matching%2520ability%2520are%2520missing%252C%2520where%2520finding%2520the%250Avisual%2520correspondence%2520of%2520objects%2520is%2520essential%2520in%2520vision%2520research.%2520Our%2520research%250Areveals%2520that%2520the%2520matching%2520capabilities%2520in%2520recent%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520still%250Aexhibit%2520systematic%2520shortcomings%252C%2520even%2520with%2520current%2520strong%2520MLLMs%2520models%252C%2520GPT-4o.%250AIn%2520particular%252C%2520we%2520construct%2520a%2520Multimodal%2520Visual%2520Matching%2520%2528MMVM%2529%2520benchmark%2520to%250Afairly%2520benchmark%2520over%252030%2520different%2520MLLMs.%2520The%2520MMVM%2520benchmark%2520is%2520built%2520from%252015%250Aopen-source%2520datasets%2520and%2520Internet%2520videos%2520with%2520manual%2520annotation.%2520We%2520categorize%250Athe%2520data%2520samples%2520of%2520MMVM%2520benchmark%2520into%2520eight%2520aspects%2520based%2520on%2520the%2520required%250Acues%2520and%2520capabilities%2520to%2520more%2520comprehensively%2520evaluate%2520and%2520analyze%2520current%250AMLLMs.%2520In%2520addition%252C%2520we%2520have%2520designed%2520an%2520automatic%2520annotation%2520pipeline%2520to%250Agenerate%2520the%2520MMVM%2520SFT%2520dataset%252C%2520including%2520220K%2520visual%2520matching%2520data%2520with%250Areasoning%2520annotation.%2520Finally%252C%2520we%2520present%2520CoLVA%252C%2520a%2520novel%2520contrastive%2520MLLM%2520with%250Atwo%2520novel%2520technical%2520designs%253A%2520fine-grained%2520vision%2520expert%2520with%2520object-level%250Acontrastive%2520learning%2520and%2520instruction%2520augmentation%2520strategy.%2520CoLVA%2520achieves%250A51.06%255C%2525%2520overall%2520accuracy%2520%2528OA%2529%2520on%2520the%2520MMVM%2520benchmark%252C%2520surpassing%2520GPT-4o%2520and%250Abaseline%2520by%25208.41%255C%2525%2520and%252023.58%255C%2525%2520OA%252C%2520respectively.%2520The%2520results%2520show%2520the%250Aeffectiveness%2520of%2520our%2520MMVM%2520SFT%2520dataset%2520and%2520our%2520novel%2520technical%2520designs.%2520Code%252C%250Abenchmark%252C%2520dataset%252C%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/zhouyiks/CoLVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20They%20the%20Same%3F%20Exploring%20Visual%20Correspondence%20Shortcomings%20of%0A%20%20Multimodal%20LLMs&entry.906535625=Yikang%20Zhou%20and%20Tao%20Zhang%20and%20Shilin%20Xu%20and%20Shihao%20Chen%20and%20Qianyu%20Zhou%20and%20Yunhai%20Tong%20and%20Shunping%20Ji%20and%20Jiangning%20Zhang%20and%20Xiangtai%20Li%20and%20Lu%20Qi&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20models%20have%20shown%20a%20strong%20ability%20in%0Avisual%20perception%2C%20reasoning%20abilities%2C%20and%20vision-language%20understanding.%0AHowever%2C%20studies%20on%20visual%20matching%20ability%20are%20missing%2C%20where%20finding%20the%0Avisual%20correspondence%20of%20objects%20is%20essential%20in%20vision%20research.%20Our%20research%0Areveals%20that%20the%20matching%20capabilities%20in%20recent%20multimodal%20LLMs%20%28MLLMs%29%20still%0Aexhibit%20systematic%20shortcomings%2C%20even%20with%20current%20strong%20MLLMs%20models%2C%20GPT-4o.%0AIn%20particular%2C%20we%20construct%20a%20Multimodal%20Visual%20Matching%20%28MMVM%29%20benchmark%20to%0Afairly%20benchmark%20over%2030%20different%20MLLMs.%20The%20MMVM%20benchmark%20is%20built%20from%2015%0Aopen-source%20datasets%20and%20Internet%20videos%20with%20manual%20annotation.%20We%20categorize%0Athe%20data%20samples%20of%20MMVM%20benchmark%20into%20eight%20aspects%20based%20on%20the%20required%0Acues%20and%20capabilities%20to%20more%20comprehensively%20evaluate%20and%20analyze%20current%0AMLLMs.%20In%20addition%2C%20we%20have%20designed%20an%20automatic%20annotation%20pipeline%20to%0Agenerate%20the%20MMVM%20SFT%20dataset%2C%20including%20220K%20visual%20matching%20data%20with%0Areasoning%20annotation.%20Finally%2C%20we%20present%20CoLVA%2C%20a%20novel%20contrastive%20MLLM%20with%0Atwo%20novel%20technical%20designs%3A%20fine-grained%20vision%20expert%20with%20object-level%0Acontrastive%20learning%20and%20instruction%20augmentation%20strategy.%20CoLVA%20achieves%0A51.06%5C%25%20overall%20accuracy%20%28OA%29%20on%20the%20MMVM%20benchmark%2C%20surpassing%20GPT-4o%20and%0Abaseline%20by%208.41%5C%25%20and%2023.58%5C%25%20OA%2C%20respectively.%20The%20results%20show%20the%0Aeffectiveness%20of%20our%20MMVM%20SFT%20dataset%20and%20our%20novel%20technical%20designs.%20Code%2C%0Abenchmark%2C%20dataset%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zhouyiks/CoLVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04670v1&entry.124074799=Read"},
{"title": "Supervision-free Vision-Language Alignment", "author": "Giorgio Giannone and Ruoteng Li and Qianli Feng and Evgeny Perevodchikov and Rui Chen and Aleix Martinez", "abstract": "  Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Supervision-free Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on curated data or preference annotation. SVP leverages self-captioning\nand a pre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14%\naverage improvement in captioning tasks, up to 12% increase in object recall,\nand substantial reduction in hallucination rates. Notably, a small VLM using\nSVP achieves hallucination reductions comparable to a model five times larger,\nwhile a VLM with initially poor referring capabilities more than doubles its\nperformance, approaching parity with a model twice its size.\n", "link": "http://arxiv.org/abs/2501.04568v1", "date": "2025-01-08", "relevancy": 2.9308, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5966}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervision-free%20Vision-Language%20Alignment&body=Title%3A%20Supervision-free%20Vision-Language%20Alignment%0AAuthor%3A%20Giorgio%20Giannone%20and%20Ruoteng%20Li%20and%20Qianli%20Feng%20and%20Evgeny%20Perevodchikov%20and%20Rui%20Chen%20and%20Aleix%20Martinez%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Aintegrating%20visual%20and%20linguistic%20information%2C%20but%20their%20performance%20is%20often%0Aconstrained%20by%20the%20need%20for%20extensive%2C%20high-quality%20image-text%20training%20data.%0ACuration%20of%20these%20image-text%20pairs%20is%20both%20time-consuming%20and%20computationally%0Aexpensive.%20To%20address%20this%20challenge%2C%20we%20introduce%20SVP%20%28Supervision-free%20Visual%0AProjection%29%2C%20a%20novel%20framework%20that%20enhances%20vision-language%20alignment%20without%0Arelying%20on%20curated%20data%20or%20preference%20annotation.%20SVP%20leverages%20self-captioning%0Aand%20a%20pre-trained%20grounding%20model%20as%20a%20feedback%20mechanism%20to%20elicit%20latent%0Ainformation%20in%20VLMs.%20We%20evaluate%20our%20approach%20across%20six%20key%20areas%3A%20captioning%2C%0Areferring%2C%20visual%20question%20answering%2C%20multitasking%2C%20hallucination%20control%2C%20and%0Aobject%20recall.%20Results%20demonstrate%20significant%20improvements%2C%20including%20a%2014%25%0Aaverage%20improvement%20in%20captioning%20tasks%2C%20up%20to%2012%25%20increase%20in%20object%20recall%2C%0Aand%20substantial%20reduction%20in%20hallucination%20rates.%20Notably%2C%20a%20small%20VLM%20using%0ASVP%20achieves%20hallucination%20reductions%20comparable%20to%20a%20model%20five%20times%20larger%2C%0Awhile%20a%20VLM%20with%20initially%20poor%20referring%20capabilities%20more%20than%20doubles%20its%0Aperformance%2C%20approaching%20parity%20with%20a%20model%20twice%20its%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervision-free%2520Vision-Language%2520Alignment%26entry.906535625%3DGiorgio%2520Giannone%2520and%2520Ruoteng%2520Li%2520and%2520Qianli%2520Feng%2520and%2520Evgeny%2520Perevodchikov%2520and%2520Rui%2520Chen%2520and%2520Aleix%2520Martinez%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520potential%2520in%250Aintegrating%2520visual%2520and%2520linguistic%2520information%252C%2520but%2520their%2520performance%2520is%2520often%250Aconstrained%2520by%2520the%2520need%2520for%2520extensive%252C%2520high-quality%2520image-text%2520training%2520data.%250ACuration%2520of%2520these%2520image-text%2520pairs%2520is%2520both%2520time-consuming%2520and%2520computationally%250Aexpensive.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SVP%2520%2528Supervision-free%2520Visual%250AProjection%2529%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520vision-language%2520alignment%2520without%250Arelying%2520on%2520curated%2520data%2520or%2520preference%2520annotation.%2520SVP%2520leverages%2520self-captioning%250Aand%2520a%2520pre-trained%2520grounding%2520model%2520as%2520a%2520feedback%2520mechanism%2520to%2520elicit%2520latent%250Ainformation%2520in%2520VLMs.%2520We%2520evaluate%2520our%2520approach%2520across%2520six%2520key%2520areas%253A%2520captioning%252C%250Areferring%252C%2520visual%2520question%2520answering%252C%2520multitasking%252C%2520hallucination%2520control%252C%2520and%250Aobject%2520recall.%2520Results%2520demonstrate%2520significant%2520improvements%252C%2520including%2520a%252014%2525%250Aaverage%2520improvement%2520in%2520captioning%2520tasks%252C%2520up%2520to%252012%2525%2520increase%2520in%2520object%2520recall%252C%250Aand%2520substantial%2520reduction%2520in%2520hallucination%2520rates.%2520Notably%252C%2520a%2520small%2520VLM%2520using%250ASVP%2520achieves%2520hallucination%2520reductions%2520comparable%2520to%2520a%2520model%2520five%2520times%2520larger%252C%250Awhile%2520a%2520VLM%2520with%2520initially%2520poor%2520referring%2520capabilities%2520more%2520than%2520doubles%2520its%250Aperformance%252C%2520approaching%2520parity%2520with%2520a%2520model%2520twice%2520its%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervision-free%20Vision-Language%20Alignment&entry.906535625=Giorgio%20Giannone%20and%20Ruoteng%20Li%20and%20Qianli%20Feng%20and%20Evgeny%20Perevodchikov%20and%20Rui%20Chen%20and%20Aleix%20Martinez&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Aintegrating%20visual%20and%20linguistic%20information%2C%20but%20their%20performance%20is%20often%0Aconstrained%20by%20the%20need%20for%20extensive%2C%20high-quality%20image-text%20training%20data.%0ACuration%20of%20these%20image-text%20pairs%20is%20both%20time-consuming%20and%20computationally%0Aexpensive.%20To%20address%20this%20challenge%2C%20we%20introduce%20SVP%20%28Supervision-free%20Visual%0AProjection%29%2C%20a%20novel%20framework%20that%20enhances%20vision-language%20alignment%20without%0Arelying%20on%20curated%20data%20or%20preference%20annotation.%20SVP%20leverages%20self-captioning%0Aand%20a%20pre-trained%20grounding%20model%20as%20a%20feedback%20mechanism%20to%20elicit%20latent%0Ainformation%20in%20VLMs.%20We%20evaluate%20our%20approach%20across%20six%20key%20areas%3A%20captioning%2C%0Areferring%2C%20visual%20question%20answering%2C%20multitasking%2C%20hallucination%20control%2C%20and%0Aobject%20recall.%20Results%20demonstrate%20significant%20improvements%2C%20including%20a%2014%25%0Aaverage%20improvement%20in%20captioning%20tasks%2C%20up%20to%2012%25%20increase%20in%20object%20recall%2C%0Aand%20substantial%20reduction%20in%20hallucination%20rates.%20Notably%2C%20a%20small%20VLM%20using%0ASVP%20achieves%20hallucination%20reductions%20comparable%20to%20a%20model%20five%20times%20larger%2C%0Awhile%20a%20VLM%20with%20initially%20poor%20referring%20capabilities%20more%20than%20doubles%20its%0Aperformance%2C%20approaching%20parity%20with%20a%20model%20twice%20its%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04568v1&entry.124074799=Read"},
{"title": "One missing piece in Vision and Language: A Survey on Comics\n  Understanding", "author": "Emanuele Vivoli and Mohamed Ali Souibgui and Andrey Barsky and Artemis LLabr\u00e9s and Marco Bertini and Dimosthenis Karatzas", "abstract": "  Vision-language models have recently evolved into versatile systems capable\nof high performance across a range of tasks, such as document understanding,\nvisual question answering, and grounding, often in zero-shot settings. Comics\nUnderstanding, a complex and multifaceted field, stands to greatly benefit from\nthese advances. Comics, as a medium, combine rich visual and textual\nnarratives, challenging AI models with tasks that span image classification,\nobject detection, instance segmentation, and deeper narrative comprehension\nthrough sequential panels. However, the unique structure of comics --\ncharacterized by creative variations in style, reading order, and non-linear\nstorytelling -- presents a set of challenges distinct from those in other\nvisual-language domains. In this survey, we present a comprehensive review of\nComics Understanding from both dataset and task perspectives. Our contributions\nare fivefold: (1) We analyze the structure of the comics medium, detailing its\ndistinctive compositional elements; (2) We survey the widely used datasets and\ntasks in comics research, emphasizing their role in advancing the field; (3) We\nintroduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy\nthat redefines vision-language tasks within comics and lays the foundation for\nfuture work; (4) We provide a detailed review and categorization of existing\nmethods following the LoCU framework; (5) Finally, we highlight current\nresearch challenges and propose directions for future exploration, particularly\nin the context of vision-language models applied to comics. This survey is the\nfirst to propose a task-oriented framework for comics intelligence and aims to\nguide future research by addressing critical gaps in data availability and task\ndefinition. A project associated with this survey is available at\nhttps://github.com/emanuelevivoli/awesome-comics-understanding.\n", "link": "http://arxiv.org/abs/2409.09502v2", "date": "2025-01-08", "relevancy": 2.8926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6221}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20missing%20piece%20in%20Vision%20and%20Language%3A%20A%20Survey%20on%20Comics%0A%20%20Understanding&body=Title%3A%20One%20missing%20piece%20in%20Vision%20and%20Language%3A%20A%20Survey%20on%20Comics%0A%20%20Understanding%0AAuthor%3A%20Emanuele%20Vivoli%20and%20Mohamed%20Ali%20Souibgui%20and%20Andrey%20Barsky%20and%20Artemis%20LLabr%C3%A9s%20and%20Marco%20Bertini%20and%20Dimosthenis%20Karatzas%0AAbstract%3A%20%20%20Vision-language%20models%20have%20recently%20evolved%20into%20versatile%20systems%20capable%0Aof%20high%20performance%20across%20a%20range%20of%20tasks%2C%20such%20as%20document%20understanding%2C%0Avisual%20question%20answering%2C%20and%20grounding%2C%20often%20in%20zero-shot%20settings.%20Comics%0AUnderstanding%2C%20a%20complex%20and%20multifaceted%20field%2C%20stands%20to%20greatly%20benefit%20from%0Athese%20advances.%20Comics%2C%20as%20a%20medium%2C%20combine%20rich%20visual%20and%20textual%0Anarratives%2C%20challenging%20AI%20models%20with%20tasks%20that%20span%20image%20classification%2C%0Aobject%20detection%2C%20instance%20segmentation%2C%20and%20deeper%20narrative%20comprehension%0Athrough%20sequential%20panels.%20However%2C%20the%20unique%20structure%20of%20comics%20--%0Acharacterized%20by%20creative%20variations%20in%20style%2C%20reading%20order%2C%20and%20non-linear%0Astorytelling%20--%20presents%20a%20set%20of%20challenges%20distinct%20from%20those%20in%20other%0Avisual-language%20domains.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%0AComics%20Understanding%20from%20both%20dataset%20and%20task%20perspectives.%20Our%20contributions%0Aare%20fivefold%3A%20%281%29%20We%20analyze%20the%20structure%20of%20the%20comics%20medium%2C%20detailing%20its%0Adistinctive%20compositional%20elements%3B%20%282%29%20We%20survey%20the%20widely%20used%20datasets%20and%0Atasks%20in%20comics%20research%2C%20emphasizing%20their%20role%20in%20advancing%20the%20field%3B%20%283%29%20We%0Aintroduce%20the%20Layer%20of%20Comics%20Understanding%20%28LoCU%29%20framework%2C%20a%20novel%20taxonomy%0Athat%20redefines%20vision-language%20tasks%20within%20comics%20and%20lays%20the%20foundation%20for%0Afuture%20work%3B%20%284%29%20We%20provide%20a%20detailed%20review%20and%20categorization%20of%20existing%0Amethods%20following%20the%20LoCU%20framework%3B%20%285%29%20Finally%2C%20we%20highlight%20current%0Aresearch%20challenges%20and%20propose%20directions%20for%20future%20exploration%2C%20particularly%0Ain%20the%20context%20of%20vision-language%20models%20applied%20to%20comics.%20This%20survey%20is%20the%0Afirst%20to%20propose%20a%20task-oriented%20framework%20for%20comics%20intelligence%20and%20aims%20to%0Aguide%20future%20research%20by%20addressing%20critical%20gaps%20in%20data%20availability%20and%20task%0Adefinition.%20A%20project%20associated%20with%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/emanuelevivoli/awesome-comics-understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520missing%2520piece%2520in%2520Vision%2520and%2520Language%253A%2520A%2520Survey%2520on%2520Comics%250A%2520%2520Understanding%26entry.906535625%3DEmanuele%2520Vivoli%2520and%2520Mohamed%2520Ali%2520Souibgui%2520and%2520Andrey%2520Barsky%2520and%2520Artemis%2520LLabr%25C3%25A9s%2520and%2520Marco%2520Bertini%2520and%2520Dimosthenis%2520Karatzas%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520recently%2520evolved%2520into%2520versatile%2520systems%2520capable%250Aof%2520high%2520performance%2520across%2520a%2520range%2520of%2520tasks%252C%2520such%2520as%2520document%2520understanding%252C%250Avisual%2520question%2520answering%252C%2520and%2520grounding%252C%2520often%2520in%2520zero-shot%2520settings.%2520Comics%250AUnderstanding%252C%2520a%2520complex%2520and%2520multifaceted%2520field%252C%2520stands%2520to%2520greatly%2520benefit%2520from%250Athese%2520advances.%2520Comics%252C%2520as%2520a%2520medium%252C%2520combine%2520rich%2520visual%2520and%2520textual%250Anarratives%252C%2520challenging%2520AI%2520models%2520with%2520tasks%2520that%2520span%2520image%2520classification%252C%250Aobject%2520detection%252C%2520instance%2520segmentation%252C%2520and%2520deeper%2520narrative%2520comprehension%250Athrough%2520sequential%2520panels.%2520However%252C%2520the%2520unique%2520structure%2520of%2520comics%2520--%250Acharacterized%2520by%2520creative%2520variations%2520in%2520style%252C%2520reading%2520order%252C%2520and%2520non-linear%250Astorytelling%2520--%2520presents%2520a%2520set%2520of%2520challenges%2520distinct%2520from%2520those%2520in%2520other%250Avisual-language%2520domains.%2520In%2520this%2520survey%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%250AComics%2520Understanding%2520from%2520both%2520dataset%2520and%2520task%2520perspectives.%2520Our%2520contributions%250Aare%2520fivefold%253A%2520%25281%2529%2520We%2520analyze%2520the%2520structure%2520of%2520the%2520comics%2520medium%252C%2520detailing%2520its%250Adistinctive%2520compositional%2520elements%253B%2520%25282%2529%2520We%2520survey%2520the%2520widely%2520used%2520datasets%2520and%250Atasks%2520in%2520comics%2520research%252C%2520emphasizing%2520their%2520role%2520in%2520advancing%2520the%2520field%253B%2520%25283%2529%2520We%250Aintroduce%2520the%2520Layer%2520of%2520Comics%2520Understanding%2520%2528LoCU%2529%2520framework%252C%2520a%2520novel%2520taxonomy%250Athat%2520redefines%2520vision-language%2520tasks%2520within%2520comics%2520and%2520lays%2520the%2520foundation%2520for%250Afuture%2520work%253B%2520%25284%2529%2520We%2520provide%2520a%2520detailed%2520review%2520and%2520categorization%2520of%2520existing%250Amethods%2520following%2520the%2520LoCU%2520framework%253B%2520%25285%2529%2520Finally%252C%2520we%2520highlight%2520current%250Aresearch%2520challenges%2520and%2520propose%2520directions%2520for%2520future%2520exploration%252C%2520particularly%250Ain%2520the%2520context%2520of%2520vision-language%2520models%2520applied%2520to%2520comics.%2520This%2520survey%2520is%2520the%250Afirst%2520to%2520propose%2520a%2520task-oriented%2520framework%2520for%2520comics%2520intelligence%2520and%2520aims%2520to%250Aguide%2520future%2520research%2520by%2520addressing%2520critical%2520gaps%2520in%2520data%2520availability%2520and%2520task%250Adefinition.%2520A%2520project%2520associated%2520with%2520this%2520survey%2520is%2520available%2520at%250Ahttps%253A//github.com/emanuelevivoli/awesome-comics-understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20missing%20piece%20in%20Vision%20and%20Language%3A%20A%20Survey%20on%20Comics%0A%20%20Understanding&entry.906535625=Emanuele%20Vivoli%20and%20Mohamed%20Ali%20Souibgui%20and%20Andrey%20Barsky%20and%20Artemis%20LLabr%C3%A9s%20and%20Marco%20Bertini%20and%20Dimosthenis%20Karatzas&entry.1292438233=%20%20Vision-language%20models%20have%20recently%20evolved%20into%20versatile%20systems%20capable%0Aof%20high%20performance%20across%20a%20range%20of%20tasks%2C%20such%20as%20document%20understanding%2C%0Avisual%20question%20answering%2C%20and%20grounding%2C%20often%20in%20zero-shot%20settings.%20Comics%0AUnderstanding%2C%20a%20complex%20and%20multifaceted%20field%2C%20stands%20to%20greatly%20benefit%20from%0Athese%20advances.%20Comics%2C%20as%20a%20medium%2C%20combine%20rich%20visual%20and%20textual%0Anarratives%2C%20challenging%20AI%20models%20with%20tasks%20that%20span%20image%20classification%2C%0Aobject%20detection%2C%20instance%20segmentation%2C%20and%20deeper%20narrative%20comprehension%0Athrough%20sequential%20panels.%20However%2C%20the%20unique%20structure%20of%20comics%20--%0Acharacterized%20by%20creative%20variations%20in%20style%2C%20reading%20order%2C%20and%20non-linear%0Astorytelling%20--%20presents%20a%20set%20of%20challenges%20distinct%20from%20those%20in%20other%0Avisual-language%20domains.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%20review%20of%0AComics%20Understanding%20from%20both%20dataset%20and%20task%20perspectives.%20Our%20contributions%0Aare%20fivefold%3A%20%281%29%20We%20analyze%20the%20structure%20of%20the%20comics%20medium%2C%20detailing%20its%0Adistinctive%20compositional%20elements%3B%20%282%29%20We%20survey%20the%20widely%20used%20datasets%20and%0Atasks%20in%20comics%20research%2C%20emphasizing%20their%20role%20in%20advancing%20the%20field%3B%20%283%29%20We%0Aintroduce%20the%20Layer%20of%20Comics%20Understanding%20%28LoCU%29%20framework%2C%20a%20novel%20taxonomy%0Athat%20redefines%20vision-language%20tasks%20within%20comics%20and%20lays%20the%20foundation%20for%0Afuture%20work%3B%20%284%29%20We%20provide%20a%20detailed%20review%20and%20categorization%20of%20existing%0Amethods%20following%20the%20LoCU%20framework%3B%20%285%29%20Finally%2C%20we%20highlight%20current%0Aresearch%20challenges%20and%20propose%20directions%20for%20future%20exploration%2C%20particularly%0Ain%20the%20context%20of%20vision-language%20models%20applied%20to%20comics.%20This%20survey%20is%20the%0Afirst%20to%20propose%20a%20task-oriented%20framework%20for%20comics%20intelligence%20and%20aims%20to%0Aguide%20future%20research%20by%20addressing%20critical%20gaps%20in%20data%20availability%20and%20task%0Adefinition.%20A%20project%20associated%20with%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/emanuelevivoli/awesome-comics-understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09502v2&entry.124074799=Read"},
{"title": "Unified Coding for Both Human Perception and Generalized Machine\n  Analytics with CLIP Supervision", "author": "Kangsheng Yin and Quan Liu and Xuelin Shen and Yulin He and Wenhan Yang and Shiqi Wang", "abstract": "  The image compression model has long struggled with adaptability and\ngeneralization, as the decoded bitstream typically serves only human or machine\nneeds and fails to preserve information for unseen visual tasks. Therefore,\nthis paper innovatively introduces supervision obtained from multimodal\npre-training models and incorporates adaptive multi-objective optimization\ntailored to support both human visual perception and machine vision\nsimultaneously with a single bitstream, denoted as Unified and Generalized\nImage Coding for Machine (UG-ICM). Specifically, to get rid of the reliance\nbetween compression models with downstream task supervision, we introduce\nContrastive Language-Image Pre-training (CLIP) models into the training\nconstraint for improved generalization. Global-to-instance-wise CLIP\nsupervision is applied to help obtain hierarchical semantics that make models\nmore generalizable for the tasks relying on the information of different\ngranularity. Furthermore, for supporting both human and machine visions with\nonly a unifying bitstream, we incorporate a conditional decoding strategy that\ntakes as conditions human or machine preferences, enabling the bitstream to be\ndecoded into different versions for corresponding preferences. As such, our\nproposed UG-ICM is fully trained in a self-supervised manner, i.e., without\nawareness of any specific downstream models and tasks. The extensive\nexperiments have shown that the proposed UG-ICM is capable of achieving\nremarkable improvements in various unseen machine analytics tasks, while\nsimultaneously providing perceptually satisfying images.\n", "link": "http://arxiv.org/abs/2501.04579v1", "date": "2025-01-08", "relevancy": 2.8778, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Coding%20for%20Both%20Human%20Perception%20and%20Generalized%20Machine%0A%20%20Analytics%20with%20CLIP%20Supervision&body=Title%3A%20Unified%20Coding%20for%20Both%20Human%20Perception%20and%20Generalized%20Machine%0A%20%20Analytics%20with%20CLIP%20Supervision%0AAuthor%3A%20Kangsheng%20Yin%20and%20Quan%20Liu%20and%20Xuelin%20Shen%20and%20Yulin%20He%20and%20Wenhan%20Yang%20and%20Shiqi%20Wang%0AAbstract%3A%20%20%20The%20image%20compression%20model%20has%20long%20struggled%20with%20adaptability%20and%0Ageneralization%2C%20as%20the%20decoded%20bitstream%20typically%20serves%20only%20human%20or%20machine%0Aneeds%20and%20fails%20to%20preserve%20information%20for%20unseen%20visual%20tasks.%20Therefore%2C%0Athis%20paper%20innovatively%20introduces%20supervision%20obtained%20from%20multimodal%0Apre-training%20models%20and%20incorporates%20adaptive%20multi-objective%20optimization%0Atailored%20to%20support%20both%20human%20visual%20perception%20and%20machine%20vision%0Asimultaneously%20with%20a%20single%20bitstream%2C%20denoted%20as%20Unified%20and%20Generalized%0AImage%20Coding%20for%20Machine%20%28UG-ICM%29.%20Specifically%2C%20to%20get%20rid%20of%20the%20reliance%0Abetween%20compression%20models%20with%20downstream%20task%20supervision%2C%20we%20introduce%0AContrastive%20Language-Image%20Pre-training%20%28CLIP%29%20models%20into%20the%20training%0Aconstraint%20for%20improved%20generalization.%20Global-to-instance-wise%20CLIP%0Asupervision%20is%20applied%20to%20help%20obtain%20hierarchical%20semantics%20that%20make%20models%0Amore%20generalizable%20for%20the%20tasks%20relying%20on%20the%20information%20of%20different%0Agranularity.%20Furthermore%2C%20for%20supporting%20both%20human%20and%20machine%20visions%20with%0Aonly%20a%20unifying%20bitstream%2C%20we%20incorporate%20a%20conditional%20decoding%20strategy%20that%0Atakes%20as%20conditions%20human%20or%20machine%20preferences%2C%20enabling%20the%20bitstream%20to%20be%0Adecoded%20into%20different%20versions%20for%20corresponding%20preferences.%20As%20such%2C%20our%0Aproposed%20UG-ICM%20is%20fully%20trained%20in%20a%20self-supervised%20manner%2C%20i.e.%2C%20without%0Aawareness%20of%20any%20specific%20downstream%20models%20and%20tasks.%20The%20extensive%0Aexperiments%20have%20shown%20that%20the%20proposed%20UG-ICM%20is%20capable%20of%20achieving%0Aremarkable%20improvements%20in%20various%20unseen%20machine%20analytics%20tasks%2C%20while%0Asimultaneously%20providing%20perceptually%20satisfying%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Coding%2520for%2520Both%2520Human%2520Perception%2520and%2520Generalized%2520Machine%250A%2520%2520Analytics%2520with%2520CLIP%2520Supervision%26entry.906535625%3DKangsheng%2520Yin%2520and%2520Quan%2520Liu%2520and%2520Xuelin%2520Shen%2520and%2520Yulin%2520He%2520and%2520Wenhan%2520Yang%2520and%2520Shiqi%2520Wang%26entry.1292438233%3D%2520%2520The%2520image%2520compression%2520model%2520has%2520long%2520struggled%2520with%2520adaptability%2520and%250Ageneralization%252C%2520as%2520the%2520decoded%2520bitstream%2520typically%2520serves%2520only%2520human%2520or%2520machine%250Aneeds%2520and%2520fails%2520to%2520preserve%2520information%2520for%2520unseen%2520visual%2520tasks.%2520Therefore%252C%250Athis%2520paper%2520innovatively%2520introduces%2520supervision%2520obtained%2520from%2520multimodal%250Apre-training%2520models%2520and%2520incorporates%2520adaptive%2520multi-objective%2520optimization%250Atailored%2520to%2520support%2520both%2520human%2520visual%2520perception%2520and%2520machine%2520vision%250Asimultaneously%2520with%2520a%2520single%2520bitstream%252C%2520denoted%2520as%2520Unified%2520and%2520Generalized%250AImage%2520Coding%2520for%2520Machine%2520%2528UG-ICM%2529.%2520Specifically%252C%2520to%2520get%2520rid%2520of%2520the%2520reliance%250Abetween%2520compression%2520models%2520with%2520downstream%2520task%2520supervision%252C%2520we%2520introduce%250AContrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520models%2520into%2520the%2520training%250Aconstraint%2520for%2520improved%2520generalization.%2520Global-to-instance-wise%2520CLIP%250Asupervision%2520is%2520applied%2520to%2520help%2520obtain%2520hierarchical%2520semantics%2520that%2520make%2520models%250Amore%2520generalizable%2520for%2520the%2520tasks%2520relying%2520on%2520the%2520information%2520of%2520different%250Agranularity.%2520Furthermore%252C%2520for%2520supporting%2520both%2520human%2520and%2520machine%2520visions%2520with%250Aonly%2520a%2520unifying%2520bitstream%252C%2520we%2520incorporate%2520a%2520conditional%2520decoding%2520strategy%2520that%250Atakes%2520as%2520conditions%2520human%2520or%2520machine%2520preferences%252C%2520enabling%2520the%2520bitstream%2520to%2520be%250Adecoded%2520into%2520different%2520versions%2520for%2520corresponding%2520preferences.%2520As%2520such%252C%2520our%250Aproposed%2520UG-ICM%2520is%2520fully%2520trained%2520in%2520a%2520self-supervised%2520manner%252C%2520i.e.%252C%2520without%250Aawareness%2520of%2520any%2520specific%2520downstream%2520models%2520and%2520tasks.%2520The%2520extensive%250Aexperiments%2520have%2520shown%2520that%2520the%2520proposed%2520UG-ICM%2520is%2520capable%2520of%2520achieving%250Aremarkable%2520improvements%2520in%2520various%2520unseen%2520machine%2520analytics%2520tasks%252C%2520while%250Asimultaneously%2520providing%2520perceptually%2520satisfying%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Coding%20for%20Both%20Human%20Perception%20and%20Generalized%20Machine%0A%20%20Analytics%20with%20CLIP%20Supervision&entry.906535625=Kangsheng%20Yin%20and%20Quan%20Liu%20and%20Xuelin%20Shen%20and%20Yulin%20He%20and%20Wenhan%20Yang%20and%20Shiqi%20Wang&entry.1292438233=%20%20The%20image%20compression%20model%20has%20long%20struggled%20with%20adaptability%20and%0Ageneralization%2C%20as%20the%20decoded%20bitstream%20typically%20serves%20only%20human%20or%20machine%0Aneeds%20and%20fails%20to%20preserve%20information%20for%20unseen%20visual%20tasks.%20Therefore%2C%0Athis%20paper%20innovatively%20introduces%20supervision%20obtained%20from%20multimodal%0Apre-training%20models%20and%20incorporates%20adaptive%20multi-objective%20optimization%0Atailored%20to%20support%20both%20human%20visual%20perception%20and%20machine%20vision%0Asimultaneously%20with%20a%20single%20bitstream%2C%20denoted%20as%20Unified%20and%20Generalized%0AImage%20Coding%20for%20Machine%20%28UG-ICM%29.%20Specifically%2C%20to%20get%20rid%20of%20the%20reliance%0Abetween%20compression%20models%20with%20downstream%20task%20supervision%2C%20we%20introduce%0AContrastive%20Language-Image%20Pre-training%20%28CLIP%29%20models%20into%20the%20training%0Aconstraint%20for%20improved%20generalization.%20Global-to-instance-wise%20CLIP%0Asupervision%20is%20applied%20to%20help%20obtain%20hierarchical%20semantics%20that%20make%20models%0Amore%20generalizable%20for%20the%20tasks%20relying%20on%20the%20information%20of%20different%0Agranularity.%20Furthermore%2C%20for%20supporting%20both%20human%20and%20machine%20visions%20with%0Aonly%20a%20unifying%20bitstream%2C%20we%20incorporate%20a%20conditional%20decoding%20strategy%20that%0Atakes%20as%20conditions%20human%20or%20machine%20preferences%2C%20enabling%20the%20bitstream%20to%20be%0Adecoded%20into%20different%20versions%20for%20corresponding%20preferences.%20As%20such%2C%20our%0Aproposed%20UG-ICM%20is%20fully%20trained%20in%20a%20self-supervised%20manner%2C%20i.e.%2C%20without%0Aawareness%20of%20any%20specific%20downstream%20models%20and%20tasks.%20The%20extensive%0Aexperiments%20have%20shown%20that%20the%20proposed%20UG-ICM%20is%20capable%20of%20achieving%0Aremarkable%20improvements%20in%20various%20unseen%20machine%20analytics%20tasks%2C%20while%0Asimultaneously%20providing%20perceptually%20satisfying%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04579v1&entry.124074799=Read"},
{"title": "Towards Revisiting Visual Place Recognition for Joining Submaps in\n  Multimap SLAM", "author": "Markus Wei\u00dfflog and Stefan Schubert and Peter Protzel and Peer Neubert", "abstract": "  Visual SLAM is a key technology for many autonomous systems. However,\ntracking loss can lead to the creation of disjoint submaps in multimap SLAM\nsystems like ORB-SLAM3. Because of that, these systems employ submap merging\nstrategies. As we show, these strategies are not always successful. In this\npaper, we investigate the impact of using modern VPR approaches for submap\nmerging in visual SLAM. We argue that classical evaluation metrics are not\nsufficient to estimate the impact of a modern VPR component on the overall\nsystem. We show that naively replacing the VPR component does not leverage its\nfull potential without requiring substantial interference in the original\nsystem. Because of that, we present a post-processing pipeline along with a set\nof metrics that allow us to estimate the impact of modern VPR components. We\nevaluate our approach on the NCLT and Newer College datasets using ORB-SLAM3\nwith NetVLAD and HDC-DELF as VPR components. Additionally, we present a simple\napproach for combining VPR with temporal consistency for map merging. We show\nthat the map merging performance of ORB-SLAM3 can be improved. Building on\nthese results, researchers in VPR can assess the potential of their approaches\nfor SLAM systems.\n", "link": "http://arxiv.org/abs/2407.12408v2", "date": "2025-01-08", "relevancy": 2.8543, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6133}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5673}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Revisiting%20Visual%20Place%20Recognition%20for%20Joining%20Submaps%20in%0A%20%20Multimap%20SLAM&body=Title%3A%20Towards%20Revisiting%20Visual%20Place%20Recognition%20for%20Joining%20Submaps%20in%0A%20%20Multimap%20SLAM%0AAuthor%3A%20Markus%20Wei%C3%9Fflog%20and%20Stefan%20Schubert%20and%20Peter%20Protzel%20and%20Peer%20Neubert%0AAbstract%3A%20%20%20Visual%20SLAM%20is%20a%20key%20technology%20for%20many%20autonomous%20systems.%20However%2C%0Atracking%20loss%20can%20lead%20to%20the%20creation%20of%20disjoint%20submaps%20in%20multimap%20SLAM%0Asystems%20like%20ORB-SLAM3.%20Because%20of%20that%2C%20these%20systems%20employ%20submap%20merging%0Astrategies.%20As%20we%20show%2C%20these%20strategies%20are%20not%20always%20successful.%20In%20this%0Apaper%2C%20we%20investigate%20the%20impact%20of%20using%20modern%20VPR%20approaches%20for%20submap%0Amerging%20in%20visual%20SLAM.%20We%20argue%20that%20classical%20evaluation%20metrics%20are%20not%0Asufficient%20to%20estimate%20the%20impact%20of%20a%20modern%20VPR%20component%20on%20the%20overall%0Asystem.%20We%20show%20that%20naively%20replacing%20the%20VPR%20component%20does%20not%20leverage%20its%0Afull%20potential%20without%20requiring%20substantial%20interference%20in%20the%20original%0Asystem.%20Because%20of%20that%2C%20we%20present%20a%20post-processing%20pipeline%20along%20with%20a%20set%0Aof%20metrics%20that%20allow%20us%20to%20estimate%20the%20impact%20of%20modern%20VPR%20components.%20We%0Aevaluate%20our%20approach%20on%20the%20NCLT%20and%20Newer%20College%20datasets%20using%20ORB-SLAM3%0Awith%20NetVLAD%20and%20HDC-DELF%20as%20VPR%20components.%20Additionally%2C%20we%20present%20a%20simple%0Aapproach%20for%20combining%20VPR%20with%20temporal%20consistency%20for%20map%20merging.%20We%20show%0Athat%20the%20map%20merging%20performance%20of%20ORB-SLAM3%20can%20be%20improved.%20Building%20on%0Athese%20results%2C%20researchers%20in%20VPR%20can%20assess%20the%20potential%20of%20their%20approaches%0Afor%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Revisiting%2520Visual%2520Place%2520Recognition%2520for%2520Joining%2520Submaps%2520in%250A%2520%2520Multimap%2520SLAM%26entry.906535625%3DMarkus%2520Wei%25C3%259Fflog%2520and%2520Stefan%2520Schubert%2520and%2520Peter%2520Protzel%2520and%2520Peer%2520Neubert%26entry.1292438233%3D%2520%2520Visual%2520SLAM%2520is%2520a%2520key%2520technology%2520for%2520many%2520autonomous%2520systems.%2520However%252C%250Atracking%2520loss%2520can%2520lead%2520to%2520the%2520creation%2520of%2520disjoint%2520submaps%2520in%2520multimap%2520SLAM%250Asystems%2520like%2520ORB-SLAM3.%2520Because%2520of%2520that%252C%2520these%2520systems%2520employ%2520submap%2520merging%250Astrategies.%2520As%2520we%2520show%252C%2520these%2520strategies%2520are%2520not%2520always%2520successful.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520impact%2520of%2520using%2520modern%2520VPR%2520approaches%2520for%2520submap%250Amerging%2520in%2520visual%2520SLAM.%2520We%2520argue%2520that%2520classical%2520evaluation%2520metrics%2520are%2520not%250Asufficient%2520to%2520estimate%2520the%2520impact%2520of%2520a%2520modern%2520VPR%2520component%2520on%2520the%2520overall%250Asystem.%2520We%2520show%2520that%2520naively%2520replacing%2520the%2520VPR%2520component%2520does%2520not%2520leverage%2520its%250Afull%2520potential%2520without%2520requiring%2520substantial%2520interference%2520in%2520the%2520original%250Asystem.%2520Because%2520of%2520that%252C%2520we%2520present%2520a%2520post-processing%2520pipeline%2520along%2520with%2520a%2520set%250Aof%2520metrics%2520that%2520allow%2520us%2520to%2520estimate%2520the%2520impact%2520of%2520modern%2520VPR%2520components.%2520We%250Aevaluate%2520our%2520approach%2520on%2520the%2520NCLT%2520and%2520Newer%2520College%2520datasets%2520using%2520ORB-SLAM3%250Awith%2520NetVLAD%2520and%2520HDC-DELF%2520as%2520VPR%2520components.%2520Additionally%252C%2520we%2520present%2520a%2520simple%250Aapproach%2520for%2520combining%2520VPR%2520with%2520temporal%2520consistency%2520for%2520map%2520merging.%2520We%2520show%250Athat%2520the%2520map%2520merging%2520performance%2520of%2520ORB-SLAM3%2520can%2520be%2520improved.%2520Building%2520on%250Athese%2520results%252C%2520researchers%2520in%2520VPR%2520can%2520assess%2520the%2520potential%2520of%2520their%2520approaches%250Afor%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Revisiting%20Visual%20Place%20Recognition%20for%20Joining%20Submaps%20in%0A%20%20Multimap%20SLAM&entry.906535625=Markus%20Wei%C3%9Fflog%20and%20Stefan%20Schubert%20and%20Peter%20Protzel%20and%20Peer%20Neubert&entry.1292438233=%20%20Visual%20SLAM%20is%20a%20key%20technology%20for%20many%20autonomous%20systems.%20However%2C%0Atracking%20loss%20can%20lead%20to%20the%20creation%20of%20disjoint%20submaps%20in%20multimap%20SLAM%0Asystems%20like%20ORB-SLAM3.%20Because%20of%20that%2C%20these%20systems%20employ%20submap%20merging%0Astrategies.%20As%20we%20show%2C%20these%20strategies%20are%20not%20always%20successful.%20In%20this%0Apaper%2C%20we%20investigate%20the%20impact%20of%20using%20modern%20VPR%20approaches%20for%20submap%0Amerging%20in%20visual%20SLAM.%20We%20argue%20that%20classical%20evaluation%20metrics%20are%20not%0Asufficient%20to%20estimate%20the%20impact%20of%20a%20modern%20VPR%20component%20on%20the%20overall%0Asystem.%20We%20show%20that%20naively%20replacing%20the%20VPR%20component%20does%20not%20leverage%20its%0Afull%20potential%20without%20requiring%20substantial%20interference%20in%20the%20original%0Asystem.%20Because%20of%20that%2C%20we%20present%20a%20post-processing%20pipeline%20along%20with%20a%20set%0Aof%20metrics%20that%20allow%20us%20to%20estimate%20the%20impact%20of%20modern%20VPR%20components.%20We%0Aevaluate%20our%20approach%20on%20the%20NCLT%20and%20Newer%20College%20datasets%20using%20ORB-SLAM3%0Awith%20NetVLAD%20and%20HDC-DELF%20as%20VPR%20components.%20Additionally%2C%20we%20present%20a%20simple%0Aapproach%20for%20combining%20VPR%20with%20temporal%20consistency%20for%20map%20merging.%20We%20show%0Athat%20the%20map%20merging%20performance%20of%20ORB-SLAM3%20can%20be%20improved.%20Building%20on%0Athese%20results%2C%20researchers%20in%20VPR%20can%20assess%20the%20potential%20of%20their%20approaches%0Afor%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12408v2&entry.124074799=Read"},
{"title": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations", "author": "Archita Srivastava and Abhas Kumar and Rajesh Kumar and Prabhakar Srinivasan", "abstract": "  Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.\n", "link": "http://arxiv.org/abs/2501.04675v1", "date": "2025-01-08", "relevancy": 2.8454, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5968}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Financial%20VQA%20in%20Vision%20Language%20Models%20using%20Intermediate%0A%20%20Structured%20Representations&body=Title%3A%20Enhancing%20Financial%20VQA%20in%20Vision%20Language%20Models%20using%20Intermediate%0A%20%20Structured%20Representations%0AAuthor%3A%20Archita%20Srivastava%20and%20Abhas%20Kumar%20and%20Rajesh%20Kumar%20and%20Prabhakar%20Srinivasan%0AAbstract%3A%20%20%20Chart%20interpretation%20is%20crucial%20for%20visual%20data%20analysis%2C%20but%20accurately%0Aextracting%20information%20from%20charts%20poses%20significant%20challenges%20for%20automated%0Amodels.%20This%20study%20investigates%20the%20fine-tuning%20of%20DEPLOT%2C%20a%20modality%0Aconversion%20module%20that%20translates%20the%20image%20of%20a%20plot%20or%20chart%20to%20a%20linearized%0Atable%2C%20on%20a%20custom%20dataset%20of%2050%2C000%20bar%20charts.%20The%20dataset%20comprises%20simple%2C%0Astacked%2C%20and%20grouped%20bar%20charts%2C%20targeting%20the%20unique%20structural%20features%20of%0Athese%20visualizations.%20The%20finetuned%20DEPLOT%20model%20is%20evaluated%20against%20its%20base%0Aversion%20using%20a%20test%20set%20of%201%2C000%20images%20and%20two%20metrics%3A%20Relative%20Mapping%0ASimilarity%20%28RMS%29%2C%20which%20measures%20categorical%20mapping%20accuracy%2C%20and%20Relative%0ANumber%20Set%20Similarity%20%28RNSS%29%2C%20which%20evaluates%20numerical%20interpretation%0Aaccuracy.%20To%20further%20explore%20the%20reasoning%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20curate%20an%20additional%20set%20of%20100%20bar%20chart%20images%20paired%20with%0Aquestion%20answer%20sets.%20Our%20findings%20demonstrate%20that%20providing%20a%20structured%0Aintermediate%20table%20alongside%20the%20image%20significantly%20enhances%20LLM%20reasoning%0Aperformance%20compared%20to%20direct%20image%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Financial%2520VQA%2520in%2520Vision%2520Language%2520Models%2520using%2520Intermediate%250A%2520%2520Structured%2520Representations%26entry.906535625%3DArchita%2520Srivastava%2520and%2520Abhas%2520Kumar%2520and%2520Rajesh%2520Kumar%2520and%2520Prabhakar%2520Srinivasan%26entry.1292438233%3D%2520%2520Chart%2520interpretation%2520is%2520crucial%2520for%2520visual%2520data%2520analysis%252C%2520but%2520accurately%250Aextracting%2520information%2520from%2520charts%2520poses%2520significant%2520challenges%2520for%2520automated%250Amodels.%2520This%2520study%2520investigates%2520the%2520fine-tuning%2520of%2520DEPLOT%252C%2520a%2520modality%250Aconversion%2520module%2520that%2520translates%2520the%2520image%2520of%2520a%2520plot%2520or%2520chart%2520to%2520a%2520linearized%250Atable%252C%2520on%2520a%2520custom%2520dataset%2520of%252050%252C000%2520bar%2520charts.%2520The%2520dataset%2520comprises%2520simple%252C%250Astacked%252C%2520and%2520grouped%2520bar%2520charts%252C%2520targeting%2520the%2520unique%2520structural%2520features%2520of%250Athese%2520visualizations.%2520The%2520finetuned%2520DEPLOT%2520model%2520is%2520evaluated%2520against%2520its%2520base%250Aversion%2520using%2520a%2520test%2520set%2520of%25201%252C000%2520images%2520and%2520two%2520metrics%253A%2520Relative%2520Mapping%250ASimilarity%2520%2528RMS%2529%252C%2520which%2520measures%2520categorical%2520mapping%2520accuracy%252C%2520and%2520Relative%250ANumber%2520Set%2520Similarity%2520%2528RNSS%2529%252C%2520which%2520evaluates%2520numerical%2520interpretation%250Aaccuracy.%2520To%2520further%2520explore%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520we%2520curate%2520an%2520additional%2520set%2520of%2520100%2520bar%2520chart%2520images%2520paired%2520with%250Aquestion%2520answer%2520sets.%2520Our%2520findings%2520demonstrate%2520that%2520providing%2520a%2520structured%250Aintermediate%2520table%2520alongside%2520the%2520image%2520significantly%2520enhances%2520LLM%2520reasoning%250Aperformance%2520compared%2520to%2520direct%2520image%2520queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Financial%20VQA%20in%20Vision%20Language%20Models%20using%20Intermediate%0A%20%20Structured%20Representations&entry.906535625=Archita%20Srivastava%20and%20Abhas%20Kumar%20and%20Rajesh%20Kumar%20and%20Prabhakar%20Srinivasan&entry.1292438233=%20%20Chart%20interpretation%20is%20crucial%20for%20visual%20data%20analysis%2C%20but%20accurately%0Aextracting%20information%20from%20charts%20poses%20significant%20challenges%20for%20automated%0Amodels.%20This%20study%20investigates%20the%20fine-tuning%20of%20DEPLOT%2C%20a%20modality%0Aconversion%20module%20that%20translates%20the%20image%20of%20a%20plot%20or%20chart%20to%20a%20linearized%0Atable%2C%20on%20a%20custom%20dataset%20of%2050%2C000%20bar%20charts.%20The%20dataset%20comprises%20simple%2C%0Astacked%2C%20and%20grouped%20bar%20charts%2C%20targeting%20the%20unique%20structural%20features%20of%0Athese%20visualizations.%20The%20finetuned%20DEPLOT%20model%20is%20evaluated%20against%20its%20base%0Aversion%20using%20a%20test%20set%20of%201%2C000%20images%20and%20two%20metrics%3A%20Relative%20Mapping%0ASimilarity%20%28RMS%29%2C%20which%20measures%20categorical%20mapping%20accuracy%2C%20and%20Relative%0ANumber%20Set%20Similarity%20%28RNSS%29%2C%20which%20evaluates%20numerical%20interpretation%0Aaccuracy.%20To%20further%20explore%20the%20reasoning%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20curate%20an%20additional%20set%20of%20100%20bar%20chart%20images%20paired%20with%0Aquestion%20answer%20sets.%20Our%20findings%20demonstrate%20that%20providing%20a%20structured%0Aintermediate%20table%20alongside%20the%20image%20significantly%20enhances%20LLM%20reasoning%0Aperformance%20compared%20to%20direct%20image%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04675v1&entry.124074799=Read"},
{"title": "Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware\n  Point Cloud Registration in the Wild", "author": "Turcan Tuna and Julian Nubert and Patrick Pfreundschuh and Cesar Cadena and Shehryar Khattak and Marco Hutter", "abstract": "  The ICP registration algorithm has been a preferred method for LiDAR-based\nrobot localization for nearly a decade. However, even in modern SLAM solutions,\nICP can degrade and become unreliable in geometrically ill-conditioned\nenvironments. Current solutions primarily focus on utilizing additional sources\nof information, such as external odometry, to either replace the degenerate\ndirections of the optimization solution or add additional constraints in a\nsensor-fusion setup afterward.\n  In response, this work investigates and compares new and existing degeneracy\nmitigation methods for robust LiDAR-based localization and analyzes the\nefficacy of these approaches in degenerate environments for the first time in\nthe literature at this scale. Specifically, this work investigates i) the\neffect of using active or passive degeneracy mitigation methods for the problem\nof ill-conditioned ICP in LiDAR degenerate environments, ii) the evaluation of\nTSVD, inequality constraints, and linear/non-linear Tikhonov regularization for\nthe application of degenerate point cloud registration for the first time.\nFurthermore, a sensitivity analysis for least-squares minimization step of the\nICP problem is carried out to better understand how each method affects the\noptimization and what to expect from each method. The results of the analysis\nare validated through multiple real-world robotic field and simulated\nexperiments. The analysis demonstrates that active optimization degeneracy\nmitigation is necessary and advantageous in the absence of reliable external\nestimate assistance for LiDAR-SLAM, and soft-constrained methods can provide\nbetter results in complex ill-conditioned scenarios with heuristic fine-tuned\nparameters.\n", "link": "http://arxiv.org/abs/2408.11809v2", "date": "2025-01-08", "relevancy": 2.8031, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5883}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5487}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild&body=Title%3A%20Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild%0AAuthor%3A%20Turcan%20Tuna%20and%20Julian%20Nubert%20and%20Patrick%20Pfreundschuh%20and%20Cesar%20Cadena%20and%20Shehryar%20Khattak%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20The%20ICP%20registration%20algorithm%20has%20been%20a%20preferred%20method%20for%20LiDAR-based%0Arobot%20localization%20for%20nearly%20a%20decade.%20However%2C%20even%20in%20modern%20SLAM%20solutions%2C%0AICP%20can%20degrade%20and%20become%20unreliable%20in%20geometrically%20ill-conditioned%0Aenvironments.%20Current%20solutions%20primarily%20focus%20on%20utilizing%20additional%20sources%0Aof%20information%2C%20such%20as%20external%20odometry%2C%20to%20either%20replace%20the%20degenerate%0Adirections%20of%20the%20optimization%20solution%20or%20add%20additional%20constraints%20in%20a%0Asensor-fusion%20setup%20afterward.%0A%20%20In%20response%2C%20this%20work%20investigates%20and%20compares%20new%20and%20existing%20degeneracy%0Amitigation%20methods%20for%20robust%20LiDAR-based%20localization%20and%20analyzes%20the%0Aefficacy%20of%20these%20approaches%20in%20degenerate%20environments%20for%20the%20first%20time%20in%0Athe%20literature%20at%20this%20scale.%20Specifically%2C%20this%20work%20investigates%20i%29%20the%0Aeffect%20of%20using%20active%20or%20passive%20degeneracy%20mitigation%20methods%20for%20the%20problem%0Aof%20ill-conditioned%20ICP%20in%20LiDAR%20degenerate%20environments%2C%20ii%29%20the%20evaluation%20of%0ATSVD%2C%20inequality%20constraints%2C%20and%20linear/non-linear%20Tikhonov%20regularization%20for%0Athe%20application%20of%20degenerate%20point%20cloud%20registration%20for%20the%20first%20time.%0AFurthermore%2C%20a%20sensitivity%20analysis%20for%20least-squares%20minimization%20step%20of%20the%0AICP%20problem%20is%20carried%20out%20to%20better%20understand%20how%20each%20method%20affects%20the%0Aoptimization%20and%20what%20to%20expect%20from%20each%20method.%20The%20results%20of%20the%20analysis%0Aare%20validated%20through%20multiple%20real-world%20robotic%20field%20and%20simulated%0Aexperiments.%20The%20analysis%20demonstrates%20that%20active%20optimization%20degeneracy%0Amitigation%20is%20necessary%20and%20advantageous%20in%20the%20absence%20of%20reliable%20external%0Aestimate%20assistance%20for%20LiDAR-SLAM%2C%20and%20soft-constrained%20methods%20can%20provide%0Abetter%20results%20in%20complex%20ill-conditioned%20scenarios%20with%20heuristic%20fine-tuned%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformed%252C%2520Constrained%252C%2520Aligned%253A%2520A%2520Field%2520Analysis%2520on%2520Degeneracy-aware%250A%2520%2520Point%2520Cloud%2520Registration%2520in%2520the%2520Wild%26entry.906535625%3DTurcan%2520Tuna%2520and%2520Julian%2520Nubert%2520and%2520Patrick%2520Pfreundschuh%2520and%2520Cesar%2520Cadena%2520and%2520Shehryar%2520Khattak%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520The%2520ICP%2520registration%2520algorithm%2520has%2520been%2520a%2520preferred%2520method%2520for%2520LiDAR-based%250Arobot%2520localization%2520for%2520nearly%2520a%2520decade.%2520However%252C%2520even%2520in%2520modern%2520SLAM%2520solutions%252C%250AICP%2520can%2520degrade%2520and%2520become%2520unreliable%2520in%2520geometrically%2520ill-conditioned%250Aenvironments.%2520Current%2520solutions%2520primarily%2520focus%2520on%2520utilizing%2520additional%2520sources%250Aof%2520information%252C%2520such%2520as%2520external%2520odometry%252C%2520to%2520either%2520replace%2520the%2520degenerate%250Adirections%2520of%2520the%2520optimization%2520solution%2520or%2520add%2520additional%2520constraints%2520in%2520a%250Asensor-fusion%2520setup%2520afterward.%250A%2520%2520In%2520response%252C%2520this%2520work%2520investigates%2520and%2520compares%2520new%2520and%2520existing%2520degeneracy%250Amitigation%2520methods%2520for%2520robust%2520LiDAR-based%2520localization%2520and%2520analyzes%2520the%250Aefficacy%2520of%2520these%2520approaches%2520in%2520degenerate%2520environments%2520for%2520the%2520first%2520time%2520in%250Athe%2520literature%2520at%2520this%2520scale.%2520Specifically%252C%2520this%2520work%2520investigates%2520i%2529%2520the%250Aeffect%2520of%2520using%2520active%2520or%2520passive%2520degeneracy%2520mitigation%2520methods%2520for%2520the%2520problem%250Aof%2520ill-conditioned%2520ICP%2520in%2520LiDAR%2520degenerate%2520environments%252C%2520ii%2529%2520the%2520evaluation%2520of%250ATSVD%252C%2520inequality%2520constraints%252C%2520and%2520linear/non-linear%2520Tikhonov%2520regularization%2520for%250Athe%2520application%2520of%2520degenerate%2520point%2520cloud%2520registration%2520for%2520the%2520first%2520time.%250AFurthermore%252C%2520a%2520sensitivity%2520analysis%2520for%2520least-squares%2520minimization%2520step%2520of%2520the%250AICP%2520problem%2520is%2520carried%2520out%2520to%2520better%2520understand%2520how%2520each%2520method%2520affects%2520the%250Aoptimization%2520and%2520what%2520to%2520expect%2520from%2520each%2520method.%2520The%2520results%2520of%2520the%2520analysis%250Aare%2520validated%2520through%2520multiple%2520real-world%2520robotic%2520field%2520and%2520simulated%250Aexperiments.%2520The%2520analysis%2520demonstrates%2520that%2520active%2520optimization%2520degeneracy%250Amitigation%2520is%2520necessary%2520and%2520advantageous%2520in%2520the%2520absence%2520of%2520reliable%2520external%250Aestimate%2520assistance%2520for%2520LiDAR-SLAM%252C%2520and%2520soft-constrained%2520methods%2520can%2520provide%250Abetter%2520results%2520in%2520complex%2520ill-conditioned%2520scenarios%2520with%2520heuristic%2520fine-tuned%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild&entry.906535625=Turcan%20Tuna%20and%20Julian%20Nubert%20and%20Patrick%20Pfreundschuh%20and%20Cesar%20Cadena%20and%20Shehryar%20Khattak%20and%20Marco%20Hutter&entry.1292438233=%20%20The%20ICP%20registration%20algorithm%20has%20been%20a%20preferred%20method%20for%20LiDAR-based%0Arobot%20localization%20for%20nearly%20a%20decade.%20However%2C%20even%20in%20modern%20SLAM%20solutions%2C%0AICP%20can%20degrade%20and%20become%20unreliable%20in%20geometrically%20ill-conditioned%0Aenvironments.%20Current%20solutions%20primarily%20focus%20on%20utilizing%20additional%20sources%0Aof%20information%2C%20such%20as%20external%20odometry%2C%20to%20either%20replace%20the%20degenerate%0Adirections%20of%20the%20optimization%20solution%20or%20add%20additional%20constraints%20in%20a%0Asensor-fusion%20setup%20afterward.%0A%20%20In%20response%2C%20this%20work%20investigates%20and%20compares%20new%20and%20existing%20degeneracy%0Amitigation%20methods%20for%20robust%20LiDAR-based%20localization%20and%20analyzes%20the%0Aefficacy%20of%20these%20approaches%20in%20degenerate%20environments%20for%20the%20first%20time%20in%0Athe%20literature%20at%20this%20scale.%20Specifically%2C%20this%20work%20investigates%20i%29%20the%0Aeffect%20of%20using%20active%20or%20passive%20degeneracy%20mitigation%20methods%20for%20the%20problem%0Aof%20ill-conditioned%20ICP%20in%20LiDAR%20degenerate%20environments%2C%20ii%29%20the%20evaluation%20of%0ATSVD%2C%20inequality%20constraints%2C%20and%20linear/non-linear%20Tikhonov%20regularization%20for%0Athe%20application%20of%20degenerate%20point%20cloud%20registration%20for%20the%20first%20time.%0AFurthermore%2C%20a%20sensitivity%20analysis%20for%20least-squares%20minimization%20step%20of%20the%0AICP%20problem%20is%20carried%20out%20to%20better%20understand%20how%20each%20method%20affects%20the%0Aoptimization%20and%20what%20to%20expect%20from%20each%20method.%20The%20results%20of%20the%20analysis%0Aare%20validated%20through%20multiple%20real-world%20robotic%20field%20and%20simulated%0Aexperiments.%20The%20analysis%20demonstrates%20that%20active%20optimization%20degeneracy%0Amitigation%20is%20necessary%20and%20advantageous%20in%20the%20absence%20of%20reliable%20external%0Aestimate%20assistance%20for%20LiDAR-SLAM%2C%20and%20soft-constrained%20methods%20can%20provide%0Abetter%20results%20in%20complex%20ill-conditioned%20scenarios%20with%20heuristic%20fine-tuned%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11809v2&entry.124074799=Read"},
{"title": "Test-Time Optimization for Domain Adaptive Open Vocabulary Segmentation", "author": "Ulindu De Silva and Didula Samaraweera and Sasini Wanigathunga and Kavindu Kariyawasam and Kanchana Ranasinghe and Muzammal Naseer and Ranga Rodrigo", "abstract": "  We present Seg-TTO, a novel framework for zero-shot, open-vocabulary semantic\nsegmentation (OVSS), designed to excel in specialized domain tasks. While\ncurrent open vocabulary approaches show impressive performance on standard\nsegmentation benchmarks under zero-shot settings, they fall short of supervised\ncounterparts on highly domain-specific datasets. We focus on\nsegmentation-specific test-time optimization to address this gap. Segmentation\nrequires an understanding of multiple concepts within a single image while\nretaining the locality and spatial structure of representations. We propose a\nnovel self-supervised objective adhering to these requirements and use it to\nalign the model parameters with input images at test time. In the textual\nmodality, we learn multiple embeddings for each category to capture diverse\nconcepts within an image, while in the visual modality, we calculate\npixel-level losses followed by embedding aggregation operations specific to\npreserving spatial structure. Our resulting framework termed Seg-TTO is a\nplug-in-play module. We integrate Seg-TTO with three state-of-the-art OVSS\napproaches and evaluate across 22 challenging OVSS tasks covering a range of\nspecialized domains. Our Seg-TTO demonstrates clear performance improvements\nacross these establishing new state-of-the-art. Code:\nhttps://github.com/UlinduP/SegTTO.\n", "link": "http://arxiv.org/abs/2501.04696v1", "date": "2025-01-08", "relevancy": 2.7988, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Optimization%20for%20Domain%20Adaptive%20Open%20Vocabulary%20Segmentation&body=Title%3A%20Test-Time%20Optimization%20for%20Domain%20Adaptive%20Open%20Vocabulary%20Segmentation%0AAuthor%3A%20Ulindu%20De%20Silva%20and%20Didula%20Samaraweera%20and%20Sasini%20Wanigathunga%20and%20Kavindu%20Kariyawasam%20and%20Kanchana%20Ranasinghe%20and%20Muzammal%20Naseer%20and%20Ranga%20Rodrigo%0AAbstract%3A%20%20%20We%20present%20Seg-TTO%2C%20a%20novel%20framework%20for%20zero-shot%2C%20open-vocabulary%20semantic%0Asegmentation%20%28OVSS%29%2C%20designed%20to%20excel%20in%20specialized%20domain%20tasks.%20While%0Acurrent%20open%20vocabulary%20approaches%20show%20impressive%20performance%20on%20standard%0Asegmentation%20benchmarks%20under%20zero-shot%20settings%2C%20they%20fall%20short%20of%20supervised%0Acounterparts%20on%20highly%20domain-specific%20datasets.%20We%20focus%20on%0Asegmentation-specific%20test-time%20optimization%20to%20address%20this%20gap.%20Segmentation%0Arequires%20an%20understanding%20of%20multiple%20concepts%20within%20a%20single%20image%20while%0Aretaining%20the%20locality%20and%20spatial%20structure%20of%20representations.%20We%20propose%20a%0Anovel%20self-supervised%20objective%20adhering%20to%20these%20requirements%20and%20use%20it%20to%0Aalign%20the%20model%20parameters%20with%20input%20images%20at%20test%20time.%20In%20the%20textual%0Amodality%2C%20we%20learn%20multiple%20embeddings%20for%20each%20category%20to%20capture%20diverse%0Aconcepts%20within%20an%20image%2C%20while%20in%20the%20visual%20modality%2C%20we%20calculate%0Apixel-level%20losses%20followed%20by%20embedding%20aggregation%20operations%20specific%20to%0Apreserving%20spatial%20structure.%20Our%20resulting%20framework%20termed%20Seg-TTO%20is%20a%0Aplug-in-play%20module.%20We%20integrate%20Seg-TTO%20with%20three%20state-of-the-art%20OVSS%0Aapproaches%20and%20evaluate%20across%2022%20challenging%20OVSS%20tasks%20covering%20a%20range%20of%0Aspecialized%20domains.%20Our%20Seg-TTO%20demonstrates%20clear%20performance%20improvements%0Aacross%20these%20establishing%20new%20state-of-the-art.%20Code%3A%0Ahttps%3A//github.com/UlinduP/SegTTO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Optimization%2520for%2520Domain%2520Adaptive%2520Open%2520Vocabulary%2520Segmentation%26entry.906535625%3DUlindu%2520De%2520Silva%2520and%2520Didula%2520Samaraweera%2520and%2520Sasini%2520Wanigathunga%2520and%2520Kavindu%2520Kariyawasam%2520and%2520Kanchana%2520Ranasinghe%2520and%2520Muzammal%2520Naseer%2520and%2520Ranga%2520Rodrigo%26entry.1292438233%3D%2520%2520We%2520present%2520Seg-TTO%252C%2520a%2520novel%2520framework%2520for%2520zero-shot%252C%2520open-vocabulary%2520semantic%250Asegmentation%2520%2528OVSS%2529%252C%2520designed%2520to%2520excel%2520in%2520specialized%2520domain%2520tasks.%2520While%250Acurrent%2520open%2520vocabulary%2520approaches%2520show%2520impressive%2520performance%2520on%2520standard%250Asegmentation%2520benchmarks%2520under%2520zero-shot%2520settings%252C%2520they%2520fall%2520short%2520of%2520supervised%250Acounterparts%2520on%2520highly%2520domain-specific%2520datasets.%2520We%2520focus%2520on%250Asegmentation-specific%2520test-time%2520optimization%2520to%2520address%2520this%2520gap.%2520Segmentation%250Arequires%2520an%2520understanding%2520of%2520multiple%2520concepts%2520within%2520a%2520single%2520image%2520while%250Aretaining%2520the%2520locality%2520and%2520spatial%2520structure%2520of%2520representations.%2520We%2520propose%2520a%250Anovel%2520self-supervised%2520objective%2520adhering%2520to%2520these%2520requirements%2520and%2520use%2520it%2520to%250Aalign%2520the%2520model%2520parameters%2520with%2520input%2520images%2520at%2520test%2520time.%2520In%2520the%2520textual%250Amodality%252C%2520we%2520learn%2520multiple%2520embeddings%2520for%2520each%2520category%2520to%2520capture%2520diverse%250Aconcepts%2520within%2520an%2520image%252C%2520while%2520in%2520the%2520visual%2520modality%252C%2520we%2520calculate%250Apixel-level%2520losses%2520followed%2520by%2520embedding%2520aggregation%2520operations%2520specific%2520to%250Apreserving%2520spatial%2520structure.%2520Our%2520resulting%2520framework%2520termed%2520Seg-TTO%2520is%2520a%250Aplug-in-play%2520module.%2520We%2520integrate%2520Seg-TTO%2520with%2520three%2520state-of-the-art%2520OVSS%250Aapproaches%2520and%2520evaluate%2520across%252022%2520challenging%2520OVSS%2520tasks%2520covering%2520a%2520range%2520of%250Aspecialized%2520domains.%2520Our%2520Seg-TTO%2520demonstrates%2520clear%2520performance%2520improvements%250Aacross%2520these%2520establishing%2520new%2520state-of-the-art.%2520Code%253A%250Ahttps%253A//github.com/UlinduP/SegTTO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Optimization%20for%20Domain%20Adaptive%20Open%20Vocabulary%20Segmentation&entry.906535625=Ulindu%20De%20Silva%20and%20Didula%20Samaraweera%20and%20Sasini%20Wanigathunga%20and%20Kavindu%20Kariyawasam%20and%20Kanchana%20Ranasinghe%20and%20Muzammal%20Naseer%20and%20Ranga%20Rodrigo&entry.1292438233=%20%20We%20present%20Seg-TTO%2C%20a%20novel%20framework%20for%20zero-shot%2C%20open-vocabulary%20semantic%0Asegmentation%20%28OVSS%29%2C%20designed%20to%20excel%20in%20specialized%20domain%20tasks.%20While%0Acurrent%20open%20vocabulary%20approaches%20show%20impressive%20performance%20on%20standard%0Asegmentation%20benchmarks%20under%20zero-shot%20settings%2C%20they%20fall%20short%20of%20supervised%0Acounterparts%20on%20highly%20domain-specific%20datasets.%20We%20focus%20on%0Asegmentation-specific%20test-time%20optimization%20to%20address%20this%20gap.%20Segmentation%0Arequires%20an%20understanding%20of%20multiple%20concepts%20within%20a%20single%20image%20while%0Aretaining%20the%20locality%20and%20spatial%20structure%20of%20representations.%20We%20propose%20a%0Anovel%20self-supervised%20objective%20adhering%20to%20these%20requirements%20and%20use%20it%20to%0Aalign%20the%20model%20parameters%20with%20input%20images%20at%20test%20time.%20In%20the%20textual%0Amodality%2C%20we%20learn%20multiple%20embeddings%20for%20each%20category%20to%20capture%20diverse%0Aconcepts%20within%20an%20image%2C%20while%20in%20the%20visual%20modality%2C%20we%20calculate%0Apixel-level%20losses%20followed%20by%20embedding%20aggregation%20operations%20specific%20to%0Apreserving%20spatial%20structure.%20Our%20resulting%20framework%20termed%20Seg-TTO%20is%20a%0Aplug-in-play%20module.%20We%20integrate%20Seg-TTO%20with%20three%20state-of-the-art%20OVSS%0Aapproaches%20and%20evaluate%20across%2022%20challenging%20OVSS%20tasks%20covering%20a%20range%20of%0Aspecialized%20domains.%20Our%20Seg-TTO%20demonstrates%20clear%20performance%20improvements%0Aacross%20these%20establishing%20new%20state-of-the-art.%20Code%3A%0Ahttps%3A//github.com/UlinduP/SegTTO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04696v1&entry.124074799=Read"},
{"title": "LogicAD: Explainable Anomaly Detection via VLM-based Text Feature\n  Extraction", "author": "Er Jin and Qihui Feng and Yongli Mou and Stefan Decker and Gerhard Lakemeyer and Oliver Simons and Johannes Stegmaier", "abstract": "  Logical image understanding involves interpreting and reasoning about the\nrelationships and consistency within an image's visual content. This capability\nis essential in applications such as industrial inspection, where logical\nanomaly detection is critical for maintaining high-quality standards and\nminimizing costly recalls. Previous research in anomaly detection (AD) has\nrelied on prior knowledge for designing algorithms, which often requires\nextensive manual annotations, significant computing power, and large amounts of\ndata for training. Autoregressive, multimodal Vision Language Models (AVLMs)\noffer a promising alternative due to their exceptional performance in visual\nreasoning across various domains. Despite this, their application to logical AD\nremains unexplored. In this work, we investigate using AVLMs for logical AD and\ndemonstrate that they are well-suited to the task. Combining AVLMs with format\nembedding and a logic reasoner, we achieve SOTA performance on public\nbenchmarks, MVTec LOCO AD, with an AUROC of 86.0% and F1-max of 83.7%, along\nwith explanations of anomalies. This significantly outperforms the existing\nSOTA method by a large margin.\n", "link": "http://arxiv.org/abs/2501.01767v2", "date": "2025-01-08", "relevancy": 2.7111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogicAD%3A%20Explainable%20Anomaly%20Detection%20via%20VLM-based%20Text%20Feature%0A%20%20Extraction&body=Title%3A%20LogicAD%3A%20Explainable%20Anomaly%20Detection%20via%20VLM-based%20Text%20Feature%0A%20%20Extraction%0AAuthor%3A%20Er%20Jin%20and%20Qihui%20Feng%20and%20Yongli%20Mou%20and%20Stefan%20Decker%20and%20Gerhard%20Lakemeyer%20and%20Oliver%20Simons%20and%20Johannes%20Stegmaier%0AAbstract%3A%20%20%20Logical%20image%20understanding%20involves%20interpreting%20and%20reasoning%20about%20the%0Arelationships%20and%20consistency%20within%20an%20image%27s%20visual%20content.%20This%20capability%0Ais%20essential%20in%20applications%20such%20as%20industrial%20inspection%2C%20where%20logical%0Aanomaly%20detection%20is%20critical%20for%20maintaining%20high-quality%20standards%20and%0Aminimizing%20costly%20recalls.%20Previous%20research%20in%20anomaly%20detection%20%28AD%29%20has%0Arelied%20on%20prior%20knowledge%20for%20designing%20algorithms%2C%20which%20often%20requires%0Aextensive%20manual%20annotations%2C%20significant%20computing%20power%2C%20and%20large%20amounts%20of%0Adata%20for%20training.%20Autoregressive%2C%20multimodal%20Vision%20Language%20Models%20%28AVLMs%29%0Aoffer%20a%20promising%20alternative%20due%20to%20their%20exceptional%20performance%20in%20visual%0Areasoning%20across%20various%20domains.%20Despite%20this%2C%20their%20application%20to%20logical%20AD%0Aremains%20unexplored.%20In%20this%20work%2C%20we%20investigate%20using%20AVLMs%20for%20logical%20AD%20and%0Ademonstrate%20that%20they%20are%20well-suited%20to%20the%20task.%20Combining%20AVLMs%20with%20format%0Aembedding%20and%20a%20logic%20reasoner%2C%20we%20achieve%20SOTA%20performance%20on%20public%0Abenchmarks%2C%20MVTec%20LOCO%20AD%2C%20with%20an%20AUROC%20of%2086.0%25%20and%20F1-max%20of%2083.7%25%2C%20along%0Awith%20explanations%20of%20anomalies.%20This%20significantly%20outperforms%20the%20existing%0ASOTA%20method%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogicAD%253A%2520Explainable%2520Anomaly%2520Detection%2520via%2520VLM-based%2520Text%2520Feature%250A%2520%2520Extraction%26entry.906535625%3DEr%2520Jin%2520and%2520Qihui%2520Feng%2520and%2520Yongli%2520Mou%2520and%2520Stefan%2520Decker%2520and%2520Gerhard%2520Lakemeyer%2520and%2520Oliver%2520Simons%2520and%2520Johannes%2520Stegmaier%26entry.1292438233%3D%2520%2520Logical%2520image%2520understanding%2520involves%2520interpreting%2520and%2520reasoning%2520about%2520the%250Arelationships%2520and%2520consistency%2520within%2520an%2520image%2527s%2520visual%2520content.%2520This%2520capability%250Ais%2520essential%2520in%2520applications%2520such%2520as%2520industrial%2520inspection%252C%2520where%2520logical%250Aanomaly%2520detection%2520is%2520critical%2520for%2520maintaining%2520high-quality%2520standards%2520and%250Aminimizing%2520costly%2520recalls.%2520Previous%2520research%2520in%2520anomaly%2520detection%2520%2528AD%2529%2520has%250Arelied%2520on%2520prior%2520knowledge%2520for%2520designing%2520algorithms%252C%2520which%2520often%2520requires%250Aextensive%2520manual%2520annotations%252C%2520significant%2520computing%2520power%252C%2520and%2520large%2520amounts%2520of%250Adata%2520for%2520training.%2520Autoregressive%252C%2520multimodal%2520Vision%2520Language%2520Models%2520%2528AVLMs%2529%250Aoffer%2520a%2520promising%2520alternative%2520due%2520to%2520their%2520exceptional%2520performance%2520in%2520visual%250Areasoning%2520across%2520various%2520domains.%2520Despite%2520this%252C%2520their%2520application%2520to%2520logical%2520AD%250Aremains%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520investigate%2520using%2520AVLMs%2520for%2520logical%2520AD%2520and%250Ademonstrate%2520that%2520they%2520are%2520well-suited%2520to%2520the%2520task.%2520Combining%2520AVLMs%2520with%2520format%250Aembedding%2520and%2520a%2520logic%2520reasoner%252C%2520we%2520achieve%2520SOTA%2520performance%2520on%2520public%250Abenchmarks%252C%2520MVTec%2520LOCO%2520AD%252C%2520with%2520an%2520AUROC%2520of%252086.0%2525%2520and%2520F1-max%2520of%252083.7%2525%252C%2520along%250Awith%2520explanations%2520of%2520anomalies.%2520This%2520significantly%2520outperforms%2520the%2520existing%250ASOTA%2520method%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogicAD%3A%20Explainable%20Anomaly%20Detection%20via%20VLM-based%20Text%20Feature%0A%20%20Extraction&entry.906535625=Er%20Jin%20and%20Qihui%20Feng%20and%20Yongli%20Mou%20and%20Stefan%20Decker%20and%20Gerhard%20Lakemeyer%20and%20Oliver%20Simons%20and%20Johannes%20Stegmaier&entry.1292438233=%20%20Logical%20image%20understanding%20involves%20interpreting%20and%20reasoning%20about%20the%0Arelationships%20and%20consistency%20within%20an%20image%27s%20visual%20content.%20This%20capability%0Ais%20essential%20in%20applications%20such%20as%20industrial%20inspection%2C%20where%20logical%0Aanomaly%20detection%20is%20critical%20for%20maintaining%20high-quality%20standards%20and%0Aminimizing%20costly%20recalls.%20Previous%20research%20in%20anomaly%20detection%20%28AD%29%20has%0Arelied%20on%20prior%20knowledge%20for%20designing%20algorithms%2C%20which%20often%20requires%0Aextensive%20manual%20annotations%2C%20significant%20computing%20power%2C%20and%20large%20amounts%20of%0Adata%20for%20training.%20Autoregressive%2C%20multimodal%20Vision%20Language%20Models%20%28AVLMs%29%0Aoffer%20a%20promising%20alternative%20due%20to%20their%20exceptional%20performance%20in%20visual%0Areasoning%20across%20various%20domains.%20Despite%20this%2C%20their%20application%20to%20logical%20AD%0Aremains%20unexplored.%20In%20this%20work%2C%20we%20investigate%20using%20AVLMs%20for%20logical%20AD%20and%0Ademonstrate%20that%20they%20are%20well-suited%20to%20the%20task.%20Combining%20AVLMs%20with%20format%0Aembedding%20and%20a%20logic%20reasoner%2C%20we%20achieve%20SOTA%20performance%20on%20public%0Abenchmarks%2C%20MVTec%20LOCO%20AD%2C%20with%20an%20AUROC%20of%2086.0%25%20and%20F1-max%20of%2083.7%25%2C%20along%0Awith%20explanations%20of%20anomalies.%20This%20significantly%20outperforms%20the%20existing%0ASOTA%20method%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01767v2&entry.124074799=Read"},
{"title": "RadGPT: Constructing 3D Image-Text Tumor Datasets", "author": "Pedro R. A. S. Bassi and Mehmet Can Yavuz and Kang Wang and Xiaoxi Chen and Wenxuan Li and Sergio Decherchi and Andrea Cavalli and Yang Yang and Alan Yuille and Zongwei Zhou", "abstract": "  With over 85 million CT scans performed annually in the United States,\ncreating tumor-related reports is a challenging and time-consuming task for\nradiologists. To address this need, we present RadGPT, an Anatomy-Aware\nVision-Language AI Agent for generating detailed reports from CT scans. RadGPT\nfirst segments tumors, including benign cysts and malignant tumors, and their\nsurrounding anatomical structures, then transforms this information into both\nstructured reports and narrative reports. These reports provide tumor size,\nshape, location, attenuation, volume, and interactions with surrounding blood\nvessels and organs. Extensive evaluation on unseen hospitals shows that RadGPT\ncan produce accurate reports, with high sensitivity/specificity for small tumor\n(<2 cm) detection: 80/73% for liver tumors, 92/78% for kidney tumors, and\n77/77% for pancreatic tumors. For large tumors, sensitivity ranges from 89% to\n97%. The results significantly surpass the state-of-the-art in abdominal CT\nreport generation.\n  RadGPT generated reports for 17 public datasets. Through radiologist review\nand refinement, we have ensured the reports' accuracy, and created the first\npublicly available image-text 3D medical dataset, comprising over 1.8 million\ntext tokens and 2.7 million images from 9,262 CT scans, including 2,947 tumor\nscans/reports of 8,562 tumor instances. Our reports can: (1) localize tumors in\neight liver sub-segments and three pancreatic sub-segments annotated per-voxel;\n(2) determine pancreatic tumor stage (T1-T4) in 260 reports; and (3) present\nindividual analyses of multiple tumors--rare in human-made reports.\nImportantly, 948 of the reports are for early-stage tumors.\n", "link": "http://arxiv.org/abs/2501.04678v1", "date": "2025-01-08", "relevancy": 2.6968, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadGPT%3A%20Constructing%203D%20Image-Text%20Tumor%20Datasets&body=Title%3A%20RadGPT%3A%20Constructing%203D%20Image-Text%20Tumor%20Datasets%0AAuthor%3A%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Mehmet%20Can%20Yavuz%20and%20Kang%20Wang%20and%20Xiaoxi%20Chen%20and%20Wenxuan%20Li%20and%20Sergio%20Decherchi%20and%20Andrea%20Cavalli%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20With%20over%2085%20million%20CT%20scans%20performed%20annually%20in%20the%20United%20States%2C%0Acreating%20tumor-related%20reports%20is%20a%20challenging%20and%20time-consuming%20task%20for%0Aradiologists.%20To%20address%20this%20need%2C%20we%20present%20RadGPT%2C%20an%20Anatomy-Aware%0AVision-Language%20AI%20Agent%20for%20generating%20detailed%20reports%20from%20CT%20scans.%20RadGPT%0Afirst%20segments%20tumors%2C%20including%20benign%20cysts%20and%20malignant%20tumors%2C%20and%20their%0Asurrounding%20anatomical%20structures%2C%20then%20transforms%20this%20information%20into%20both%0Astructured%20reports%20and%20narrative%20reports.%20These%20reports%20provide%20tumor%20size%2C%0Ashape%2C%20location%2C%20attenuation%2C%20volume%2C%20and%20interactions%20with%20surrounding%20blood%0Avessels%20and%20organs.%20Extensive%20evaluation%20on%20unseen%20hospitals%20shows%20that%20RadGPT%0Acan%20produce%20accurate%20reports%2C%20with%20high%20sensitivity/specificity%20for%20small%20tumor%0A%28%3C2%20cm%29%20detection%3A%2080/73%25%20for%20liver%20tumors%2C%2092/78%25%20for%20kidney%20tumors%2C%20and%0A77/77%25%20for%20pancreatic%20tumors.%20For%20large%20tumors%2C%20sensitivity%20ranges%20from%2089%25%20to%0A97%25.%20The%20results%20significantly%20surpass%20the%20state-of-the-art%20in%20abdominal%20CT%0Areport%20generation.%0A%20%20RadGPT%20generated%20reports%20for%2017%20public%20datasets.%20Through%20radiologist%20review%0Aand%20refinement%2C%20we%20have%20ensured%20the%20reports%27%20accuracy%2C%20and%20created%20the%20first%0Apublicly%20available%20image-text%203D%20medical%20dataset%2C%20comprising%20over%201.8%20million%0Atext%20tokens%20and%202.7%20million%20images%20from%209%2C262%20CT%20scans%2C%20including%202%2C947%20tumor%0Ascans/reports%20of%208%2C562%20tumor%20instances.%20Our%20reports%20can%3A%20%281%29%20localize%20tumors%20in%0Aeight%20liver%20sub-segments%20and%20three%20pancreatic%20sub-segments%20annotated%20per-voxel%3B%0A%282%29%20determine%20pancreatic%20tumor%20stage%20%28T1-T4%29%20in%20260%20reports%3B%20and%20%283%29%20present%0Aindividual%20analyses%20of%20multiple%20tumors--rare%20in%20human-made%20reports.%0AImportantly%2C%20948%20of%20the%20reports%20are%20for%20early-stage%20tumors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadGPT%253A%2520Constructing%25203D%2520Image-Text%2520Tumor%2520Datasets%26entry.906535625%3DPedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Mehmet%2520Can%2520Yavuz%2520and%2520Kang%2520Wang%2520and%2520Xiaoxi%2520Chen%2520and%2520Wenxuan%2520Li%2520and%2520Sergio%2520Decherchi%2520and%2520Andrea%2520Cavalli%2520and%2520Yang%2520Yang%2520and%2520Alan%2520Yuille%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520With%2520over%252085%2520million%2520CT%2520scans%2520performed%2520annually%2520in%2520the%2520United%2520States%252C%250Acreating%2520tumor-related%2520reports%2520is%2520a%2520challenging%2520and%2520time-consuming%2520task%2520for%250Aradiologists.%2520To%2520address%2520this%2520need%252C%2520we%2520present%2520RadGPT%252C%2520an%2520Anatomy-Aware%250AVision-Language%2520AI%2520Agent%2520for%2520generating%2520detailed%2520reports%2520from%2520CT%2520scans.%2520RadGPT%250Afirst%2520segments%2520tumors%252C%2520including%2520benign%2520cysts%2520and%2520malignant%2520tumors%252C%2520and%2520their%250Asurrounding%2520anatomical%2520structures%252C%2520then%2520transforms%2520this%2520information%2520into%2520both%250Astructured%2520reports%2520and%2520narrative%2520reports.%2520These%2520reports%2520provide%2520tumor%2520size%252C%250Ashape%252C%2520location%252C%2520attenuation%252C%2520volume%252C%2520and%2520interactions%2520with%2520surrounding%2520blood%250Avessels%2520and%2520organs.%2520Extensive%2520evaluation%2520on%2520unseen%2520hospitals%2520shows%2520that%2520RadGPT%250Acan%2520produce%2520accurate%2520reports%252C%2520with%2520high%2520sensitivity/specificity%2520for%2520small%2520tumor%250A%2528%253C2%2520cm%2529%2520detection%253A%252080/73%2525%2520for%2520liver%2520tumors%252C%252092/78%2525%2520for%2520kidney%2520tumors%252C%2520and%250A77/77%2525%2520for%2520pancreatic%2520tumors.%2520For%2520large%2520tumors%252C%2520sensitivity%2520ranges%2520from%252089%2525%2520to%250A97%2525.%2520The%2520results%2520significantly%2520surpass%2520the%2520state-of-the-art%2520in%2520abdominal%2520CT%250Areport%2520generation.%250A%2520%2520RadGPT%2520generated%2520reports%2520for%252017%2520public%2520datasets.%2520Through%2520radiologist%2520review%250Aand%2520refinement%252C%2520we%2520have%2520ensured%2520the%2520reports%2527%2520accuracy%252C%2520and%2520created%2520the%2520first%250Apublicly%2520available%2520image-text%25203D%2520medical%2520dataset%252C%2520comprising%2520over%25201.8%2520million%250Atext%2520tokens%2520and%25202.7%2520million%2520images%2520from%25209%252C262%2520CT%2520scans%252C%2520including%25202%252C947%2520tumor%250Ascans/reports%2520of%25208%252C562%2520tumor%2520instances.%2520Our%2520reports%2520can%253A%2520%25281%2529%2520localize%2520tumors%2520in%250Aeight%2520liver%2520sub-segments%2520and%2520three%2520pancreatic%2520sub-segments%2520annotated%2520per-voxel%253B%250A%25282%2529%2520determine%2520pancreatic%2520tumor%2520stage%2520%2528T1-T4%2529%2520in%2520260%2520reports%253B%2520and%2520%25283%2529%2520present%250Aindividual%2520analyses%2520of%2520multiple%2520tumors--rare%2520in%2520human-made%2520reports.%250AImportantly%252C%2520948%2520of%2520the%2520reports%2520are%2520for%2520early-stage%2520tumors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadGPT%3A%20Constructing%203D%20Image-Text%20Tumor%20Datasets&entry.906535625=Pedro%20R.%20A.%20S.%20Bassi%20and%20Mehmet%20Can%20Yavuz%20and%20Kang%20Wang%20and%20Xiaoxi%20Chen%20and%20Wenxuan%20Li%20and%20Sergio%20Decherchi%20and%20Andrea%20Cavalli%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou&entry.1292438233=%20%20With%20over%2085%20million%20CT%20scans%20performed%20annually%20in%20the%20United%20States%2C%0Acreating%20tumor-related%20reports%20is%20a%20challenging%20and%20time-consuming%20task%20for%0Aradiologists.%20To%20address%20this%20need%2C%20we%20present%20RadGPT%2C%20an%20Anatomy-Aware%0AVision-Language%20AI%20Agent%20for%20generating%20detailed%20reports%20from%20CT%20scans.%20RadGPT%0Afirst%20segments%20tumors%2C%20including%20benign%20cysts%20and%20malignant%20tumors%2C%20and%20their%0Asurrounding%20anatomical%20structures%2C%20then%20transforms%20this%20information%20into%20both%0Astructured%20reports%20and%20narrative%20reports.%20These%20reports%20provide%20tumor%20size%2C%0Ashape%2C%20location%2C%20attenuation%2C%20volume%2C%20and%20interactions%20with%20surrounding%20blood%0Avessels%20and%20organs.%20Extensive%20evaluation%20on%20unseen%20hospitals%20shows%20that%20RadGPT%0Acan%20produce%20accurate%20reports%2C%20with%20high%20sensitivity/specificity%20for%20small%20tumor%0A%28%3C2%20cm%29%20detection%3A%2080/73%25%20for%20liver%20tumors%2C%2092/78%25%20for%20kidney%20tumors%2C%20and%0A77/77%25%20for%20pancreatic%20tumors.%20For%20large%20tumors%2C%20sensitivity%20ranges%20from%2089%25%20to%0A97%25.%20The%20results%20significantly%20surpass%20the%20state-of-the-art%20in%20abdominal%20CT%0Areport%20generation.%0A%20%20RadGPT%20generated%20reports%20for%2017%20public%20datasets.%20Through%20radiologist%20review%0Aand%20refinement%2C%20we%20have%20ensured%20the%20reports%27%20accuracy%2C%20and%20created%20the%20first%0Apublicly%20available%20image-text%203D%20medical%20dataset%2C%20comprising%20over%201.8%20million%0Atext%20tokens%20and%202.7%20million%20images%20from%209%2C262%20CT%20scans%2C%20including%202%2C947%20tumor%0Ascans/reports%20of%208%2C562%20tumor%20instances.%20Our%20reports%20can%3A%20%281%29%20localize%20tumors%20in%0Aeight%20liver%20sub-segments%20and%20three%20pancreatic%20sub-segments%20annotated%20per-voxel%3B%0A%282%29%20determine%20pancreatic%20tumor%20stage%20%28T1-T4%29%20in%20260%20reports%3B%20and%20%283%29%20present%0Aindividual%20analyses%20of%20multiple%20tumors--rare%20in%20human-made%20reports.%0AImportantly%2C%20948%20of%20the%20reports%20are%20for%20early-stage%20tumors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04678v1&entry.124074799=Read"},
{"title": "OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment\n  across Language with Real-time Self-Aware Emotional Speech Synthesis", "author": "Run Luo and Ting-En Lin and Haonan Zhang and Yuchuan Wu and Xiong Liu and Min Yang and Yongbin Li and Longze Chen and Jiaming Li and Lei Zhang and Yangyi Chen and Hamid Alinejad-Rokny and Fei Huang", "abstract": "  Recent advancements in omnimodal learning have been achieved in understanding\nand generation across images, text, and speech, though mainly within\nproprietary models. Limited omnimodal datasets and the inherent challenges\nassociated with real-time emotional speech generation have hindered open-source\nprogress. To address these issues, we propose openomni, a two-stage training\nmethod combining omnimodal alignment and speech generation to develop a\nstate-of-the-art omnimodal large language model. In the alignment phase, a\npre-trained speech model is further trained on text-image tasks to generalize\nfrom vision to speech in a (near) zero-shot manner, outperforming models\ntrained on tri-modal datasets. In the speech generation phase, a lightweight\ndecoder facilitates real-time emotional speech through training on speech tasks\nand preference learning. Experiments demonstrate that openomni consistently\nimproves across omnimodal, vision-language, and speech-language evaluations,\nenabling natural, emotion-rich dialogues and real-time emotional speech\ngeneration.\n", "link": "http://arxiv.org/abs/2501.04561v1", "date": "2025-01-08", "relevancy": 2.6928, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenOmni%3A%20Large%20Language%20Models%20Pivot%20Zero-shot%20Omnimodal%20Alignment%0A%20%20across%20Language%20with%20Real-time%20Self-Aware%20Emotional%20Speech%20Synthesis&body=Title%3A%20OpenOmni%3A%20Large%20Language%20Models%20Pivot%20Zero-shot%20Omnimodal%20Alignment%0A%20%20across%20Language%20with%20Real-time%20Self-Aware%20Emotional%20Speech%20Synthesis%0AAuthor%3A%20Run%20Luo%20and%20Ting-En%20Lin%20and%20Haonan%20Zhang%20and%20Yuchuan%20Wu%20and%20Xiong%20Liu%20and%20Min%20Yang%20and%20Yongbin%20Li%20and%20Longze%20Chen%20and%20Jiaming%20Li%20and%20Lei%20Zhang%20and%20Yangyi%20Chen%20and%20Hamid%20Alinejad-Rokny%20and%20Fei%20Huang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20omnimodal%20learning%20have%20been%20achieved%20in%20understanding%0Aand%20generation%20across%20images%2C%20text%2C%20and%20speech%2C%20though%20mainly%20within%0Aproprietary%20models.%20Limited%20omnimodal%20datasets%20and%20the%20inherent%20challenges%0Aassociated%20with%20real-time%20emotional%20speech%20generation%20have%20hindered%20open-source%0Aprogress.%20To%20address%20these%20issues%2C%20we%20propose%20openomni%2C%20a%20two-stage%20training%0Amethod%20combining%20omnimodal%20alignment%20and%20speech%20generation%20to%20develop%20a%0Astate-of-the-art%20omnimodal%20large%20language%20model.%20In%20the%20alignment%20phase%2C%20a%0Apre-trained%20speech%20model%20is%20further%20trained%20on%20text-image%20tasks%20to%20generalize%0Afrom%20vision%20to%20speech%20in%20a%20%28near%29%20zero-shot%20manner%2C%20outperforming%20models%0Atrained%20on%20tri-modal%20datasets.%20In%20the%20speech%20generation%20phase%2C%20a%20lightweight%0Adecoder%20facilitates%20real-time%20emotional%20speech%20through%20training%20on%20speech%20tasks%0Aand%20preference%20learning.%20Experiments%20demonstrate%20that%20openomni%20consistently%0Aimproves%20across%20omnimodal%2C%20vision-language%2C%20and%20speech-language%20evaluations%2C%0Aenabling%20natural%2C%20emotion-rich%20dialogues%20and%20real-time%20emotional%20speech%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenOmni%253A%2520Large%2520Language%2520Models%2520Pivot%2520Zero-shot%2520Omnimodal%2520Alignment%250A%2520%2520across%2520Language%2520with%2520Real-time%2520Self-Aware%2520Emotional%2520Speech%2520Synthesis%26entry.906535625%3DRun%2520Luo%2520and%2520Ting-En%2520Lin%2520and%2520Haonan%2520Zhang%2520and%2520Yuchuan%2520Wu%2520and%2520Xiong%2520Liu%2520and%2520Min%2520Yang%2520and%2520Yongbin%2520Li%2520and%2520Longze%2520Chen%2520and%2520Jiaming%2520Li%2520and%2520Lei%2520Zhang%2520and%2520Yangyi%2520Chen%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Fei%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520omnimodal%2520learning%2520have%2520been%2520achieved%2520in%2520understanding%250Aand%2520generation%2520across%2520images%252C%2520text%252C%2520and%2520speech%252C%2520though%2520mainly%2520within%250Aproprietary%2520models.%2520Limited%2520omnimodal%2520datasets%2520and%2520the%2520inherent%2520challenges%250Aassociated%2520with%2520real-time%2520emotional%2520speech%2520generation%2520have%2520hindered%2520open-source%250Aprogress.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520openomni%252C%2520a%2520two-stage%2520training%250Amethod%2520combining%2520omnimodal%2520alignment%2520and%2520speech%2520generation%2520to%2520develop%2520a%250Astate-of-the-art%2520omnimodal%2520large%2520language%2520model.%2520In%2520the%2520alignment%2520phase%252C%2520a%250Apre-trained%2520speech%2520model%2520is%2520further%2520trained%2520on%2520text-image%2520tasks%2520to%2520generalize%250Afrom%2520vision%2520to%2520speech%2520in%2520a%2520%2528near%2529%2520zero-shot%2520manner%252C%2520outperforming%2520models%250Atrained%2520on%2520tri-modal%2520datasets.%2520In%2520the%2520speech%2520generation%2520phase%252C%2520a%2520lightweight%250Adecoder%2520facilitates%2520real-time%2520emotional%2520speech%2520through%2520training%2520on%2520speech%2520tasks%250Aand%2520preference%2520learning.%2520Experiments%2520demonstrate%2520that%2520openomni%2520consistently%250Aimproves%2520across%2520omnimodal%252C%2520vision-language%252C%2520and%2520speech-language%2520evaluations%252C%250Aenabling%2520natural%252C%2520emotion-rich%2520dialogues%2520and%2520real-time%2520emotional%2520speech%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenOmni%3A%20Large%20Language%20Models%20Pivot%20Zero-shot%20Omnimodal%20Alignment%0A%20%20across%20Language%20with%20Real-time%20Self-Aware%20Emotional%20Speech%20Synthesis&entry.906535625=Run%20Luo%20and%20Ting-En%20Lin%20and%20Haonan%20Zhang%20and%20Yuchuan%20Wu%20and%20Xiong%20Liu%20and%20Min%20Yang%20and%20Yongbin%20Li%20and%20Longze%20Chen%20and%20Jiaming%20Li%20and%20Lei%20Zhang%20and%20Yangyi%20Chen%20and%20Hamid%20Alinejad-Rokny%20and%20Fei%20Huang&entry.1292438233=%20%20Recent%20advancements%20in%20omnimodal%20learning%20have%20been%20achieved%20in%20understanding%0Aand%20generation%20across%20images%2C%20text%2C%20and%20speech%2C%20though%20mainly%20within%0Aproprietary%20models.%20Limited%20omnimodal%20datasets%20and%20the%20inherent%20challenges%0Aassociated%20with%20real-time%20emotional%20speech%20generation%20have%20hindered%20open-source%0Aprogress.%20To%20address%20these%20issues%2C%20we%20propose%20openomni%2C%20a%20two-stage%20training%0Amethod%20combining%20omnimodal%20alignment%20and%20speech%20generation%20to%20develop%20a%0Astate-of-the-art%20omnimodal%20large%20language%20model.%20In%20the%20alignment%20phase%2C%20a%0Apre-trained%20speech%20model%20is%20further%20trained%20on%20text-image%20tasks%20to%20generalize%0Afrom%20vision%20to%20speech%20in%20a%20%28near%29%20zero-shot%20manner%2C%20outperforming%20models%0Atrained%20on%20tri-modal%20datasets.%20In%20the%20speech%20generation%20phase%2C%20a%20lightweight%0Adecoder%20facilitates%20real-time%20emotional%20speech%20through%20training%20on%20speech%20tasks%0Aand%20preference%20learning.%20Experiments%20demonstrate%20that%20openomni%20consistently%0Aimproves%20across%20omnimodal%2C%20vision-language%2C%20and%20speech-language%20evaluations%2C%0Aenabling%20natural%2C%20emotion-rich%20dialogues%20and%20real-time%20emotional%20speech%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04561v1&entry.124074799=Read"},
{"title": "Motion-Zero: Zero-Shot Moving Object Control Framework for\n  Diffusion-Based Video Generation", "author": "Changgu Chen and Junwei Shu and Gaoqi He and Changbo Wang and Yang Li", "abstract": "  Recent large-scale pre-trained diffusion models have demonstrated a powerful\ngenerative ability to produce high-quality videos from detailed text\ndescriptions. However, exerting control over the motion of objects in videos\ngenerated by any video diffusion model is a challenging problem. In this paper,\nwe propose a novel zero-shot moving object trajectory control framework,\nMotion-Zero, to enable a bounding-box-trajectories-controlled text-to-video\ndiffusion model. To this end, an initial noise prior module is designed to\nprovide a position-based prior to improve the stability of the appearance of\nthe moving object and the accuracy of position. In addition, based on the\nattention map of the U-net, spatial constraints are directly applied to the\ndenoising process of diffusion models, which further ensures the positional and\nspatial consistency of moving objects during the inference. Furthermore,\ntemporal consistency is guaranteed with a proposed shift temporal attention\nmechanism. Our method can be flexibly applied to various state-of-the-art video\ndiffusion models without any training process. Extensive experiments\ndemonstrate our proposed method can control the motion trajectories of objects\nand generate high-quality videos. Our project page is\nhttps://vpx-ecnu.github.io/MotionZero-website/\n", "link": "http://arxiv.org/abs/2401.10150v4", "date": "2025-01-08", "relevancy": 2.6512, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.709}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6663}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-Zero%3A%20Zero-Shot%20Moving%20Object%20Control%20Framework%20for%0A%20%20Diffusion-Based%20Video%20Generation&body=Title%3A%20Motion-Zero%3A%20Zero-Shot%20Moving%20Object%20Control%20Framework%20for%0A%20%20Diffusion-Based%20Video%20Generation%0AAuthor%3A%20Changgu%20Chen%20and%20Junwei%20Shu%20and%20Gaoqi%20He%20and%20Changbo%20Wang%20and%20Yang%20Li%0AAbstract%3A%20%20%20Recent%20large-scale%20pre-trained%20diffusion%20models%20have%20demonstrated%20a%20powerful%0Agenerative%20ability%20to%20produce%20high-quality%20videos%20from%20detailed%20text%0Adescriptions.%20However%2C%20exerting%20control%20over%20the%20motion%20of%20objects%20in%20videos%0Agenerated%20by%20any%20video%20diffusion%20model%20is%20a%20challenging%20problem.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20zero-shot%20moving%20object%20trajectory%20control%20framework%2C%0AMotion-Zero%2C%20to%20enable%20a%20bounding-box-trajectories-controlled%20text-to-video%0Adiffusion%20model.%20To%20this%20end%2C%20an%20initial%20noise%20prior%20module%20is%20designed%20to%0Aprovide%20a%20position-based%20prior%20to%20improve%20the%20stability%20of%20the%20appearance%20of%0Athe%20moving%20object%20and%20the%20accuracy%20of%20position.%20In%20addition%2C%20based%20on%20the%0Aattention%20map%20of%20the%20U-net%2C%20spatial%20constraints%20are%20directly%20applied%20to%20the%0Adenoising%20process%20of%20diffusion%20models%2C%20which%20further%20ensures%20the%20positional%20and%0Aspatial%20consistency%20of%20moving%20objects%20during%20the%20inference.%20Furthermore%2C%0Atemporal%20consistency%20is%20guaranteed%20with%20a%20proposed%20shift%20temporal%20attention%0Amechanism.%20Our%20method%20can%20be%20flexibly%20applied%20to%20various%20state-of-the-art%20video%0Adiffusion%20models%20without%20any%20training%20process.%20Extensive%20experiments%0Ademonstrate%20our%20proposed%20method%20can%20control%20the%20motion%20trajectories%20of%20objects%0Aand%20generate%20high-quality%20videos.%20Our%20project%20page%20is%0Ahttps%3A//vpx-ecnu.github.io/MotionZero-website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10150v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-Zero%253A%2520Zero-Shot%2520Moving%2520Object%2520Control%2520Framework%2520for%250A%2520%2520Diffusion-Based%2520Video%2520Generation%26entry.906535625%3DChanggu%2520Chen%2520and%2520Junwei%2520Shu%2520and%2520Gaoqi%2520He%2520and%2520Changbo%2520Wang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Recent%2520large-scale%2520pre-trained%2520diffusion%2520models%2520have%2520demonstrated%2520a%2520powerful%250Agenerative%2520ability%2520to%2520produce%2520high-quality%2520videos%2520from%2520detailed%2520text%250Adescriptions.%2520However%252C%2520exerting%2520control%2520over%2520the%2520motion%2520of%2520objects%2520in%2520videos%250Agenerated%2520by%2520any%2520video%2520diffusion%2520model%2520is%2520a%2520challenging%2520problem.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520zero-shot%2520moving%2520object%2520trajectory%2520control%2520framework%252C%250AMotion-Zero%252C%2520to%2520enable%2520a%2520bounding-box-trajectories-controlled%2520text-to-video%250Adiffusion%2520model.%2520To%2520this%2520end%252C%2520an%2520initial%2520noise%2520prior%2520module%2520is%2520designed%2520to%250Aprovide%2520a%2520position-based%2520prior%2520to%2520improve%2520the%2520stability%2520of%2520the%2520appearance%2520of%250Athe%2520moving%2520object%2520and%2520the%2520accuracy%2520of%2520position.%2520In%2520addition%252C%2520based%2520on%2520the%250Aattention%2520map%2520of%2520the%2520U-net%252C%2520spatial%2520constraints%2520are%2520directly%2520applied%2520to%2520the%250Adenoising%2520process%2520of%2520diffusion%2520models%252C%2520which%2520further%2520ensures%2520the%2520positional%2520and%250Aspatial%2520consistency%2520of%2520moving%2520objects%2520during%2520the%2520inference.%2520Furthermore%252C%250Atemporal%2520consistency%2520is%2520guaranteed%2520with%2520a%2520proposed%2520shift%2520temporal%2520attention%250Amechanism.%2520Our%2520method%2520can%2520be%2520flexibly%2520applied%2520to%2520various%2520state-of-the-art%2520video%250Adiffusion%2520models%2520without%2520any%2520training%2520process.%2520Extensive%2520experiments%250Ademonstrate%2520our%2520proposed%2520method%2520can%2520control%2520the%2520motion%2520trajectories%2520of%2520objects%250Aand%2520generate%2520high-quality%2520videos.%2520Our%2520project%2520page%2520is%250Ahttps%253A//vpx-ecnu.github.io/MotionZero-website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10150v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-Zero%3A%20Zero-Shot%20Moving%20Object%20Control%20Framework%20for%0A%20%20Diffusion-Based%20Video%20Generation&entry.906535625=Changgu%20Chen%20and%20Junwei%20Shu%20and%20Gaoqi%20He%20and%20Changbo%20Wang%20and%20Yang%20Li&entry.1292438233=%20%20Recent%20large-scale%20pre-trained%20diffusion%20models%20have%20demonstrated%20a%20powerful%0Agenerative%20ability%20to%20produce%20high-quality%20videos%20from%20detailed%20text%0Adescriptions.%20However%2C%20exerting%20control%20over%20the%20motion%20of%20objects%20in%20videos%0Agenerated%20by%20any%20video%20diffusion%20model%20is%20a%20challenging%20problem.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20zero-shot%20moving%20object%20trajectory%20control%20framework%2C%0AMotion-Zero%2C%20to%20enable%20a%20bounding-box-trajectories-controlled%20text-to-video%0Adiffusion%20model.%20To%20this%20end%2C%20an%20initial%20noise%20prior%20module%20is%20designed%20to%0Aprovide%20a%20position-based%20prior%20to%20improve%20the%20stability%20of%20the%20appearance%20of%0Athe%20moving%20object%20and%20the%20accuracy%20of%20position.%20In%20addition%2C%20based%20on%20the%0Aattention%20map%20of%20the%20U-net%2C%20spatial%20constraints%20are%20directly%20applied%20to%20the%0Adenoising%20process%20of%20diffusion%20models%2C%20which%20further%20ensures%20the%20positional%20and%0Aspatial%20consistency%20of%20moving%20objects%20during%20the%20inference.%20Furthermore%2C%0Atemporal%20consistency%20is%20guaranteed%20with%20a%20proposed%20shift%20temporal%20attention%0Amechanism.%20Our%20method%20can%20be%20flexibly%20applied%20to%20various%20state-of-the-art%20video%0Adiffusion%20models%20without%20any%20training%20process.%20Extensive%20experiments%0Ademonstrate%20our%20proposed%20method%20can%20control%20the%20motion%20trajectories%20of%20objects%0Aand%20generate%20high-quality%20videos.%20Our%20project%20page%20is%0Ahttps%3A//vpx-ecnu.github.io/MotionZero-website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10150v4&entry.124074799=Read"},
{"title": "User Simulation in the Era of Generative AI: User Modeling, Synthetic\n  Data Generation, and System Evaluation", "author": "Krisztian Balog and ChengXiang Zhai", "abstract": "  User simulation is an emerging interdisciplinary topic with multiple critical\napplications in the era of Generative AI. It involves creating an intelligent\nagent that mimics the actions of a human user interacting with an AI system,\nenabling researchers to model and analyze user behaviour, generate synthetic\ndata for training, and evaluate interactive AI systems in a controlled and\nreproducible manner. User simulation has profound implications for diverse\nfields and plays a vital role in the pursuit of Artificial General\nIntelligence. This paper provides an overview of user simulation, highlighting\nits key applications, connections to various disciplines, and outlining future\nresearch directions to advance this increasingly important technology.\n", "link": "http://arxiv.org/abs/2501.04410v1", "date": "2025-01-08", "relevancy": 2.6216, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5682}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.509}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User%20Simulation%20in%20the%20Era%20of%20Generative%20AI%3A%20User%20Modeling%2C%20Synthetic%0A%20%20Data%20Generation%2C%20and%20System%20Evaluation&body=Title%3A%20User%20Simulation%20in%20the%20Era%20of%20Generative%20AI%3A%20User%20Modeling%2C%20Synthetic%0A%20%20Data%20Generation%2C%20and%20System%20Evaluation%0AAuthor%3A%20Krisztian%20Balog%20and%20ChengXiang%20Zhai%0AAbstract%3A%20%20%20User%20simulation%20is%20an%20emerging%20interdisciplinary%20topic%20with%20multiple%20critical%0Aapplications%20in%20the%20era%20of%20Generative%20AI.%20It%20involves%20creating%20an%20intelligent%0Aagent%20that%20mimics%20the%20actions%20of%20a%20human%20user%20interacting%20with%20an%20AI%20system%2C%0Aenabling%20researchers%20to%20model%20and%20analyze%20user%20behaviour%2C%20generate%20synthetic%0Adata%20for%20training%2C%20and%20evaluate%20interactive%20AI%20systems%20in%20a%20controlled%20and%0Areproducible%20manner.%20User%20simulation%20has%20profound%20implications%20for%20diverse%0Afields%20and%20plays%20a%20vital%20role%20in%20the%20pursuit%20of%20Artificial%20General%0AIntelligence.%20This%20paper%20provides%20an%20overview%20of%20user%20simulation%2C%20highlighting%0Aits%20key%20applications%2C%20connections%20to%20various%20disciplines%2C%20and%20outlining%20future%0Aresearch%20directions%20to%20advance%20this%20increasingly%20important%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser%2520Simulation%2520in%2520the%2520Era%2520of%2520Generative%2520AI%253A%2520User%2520Modeling%252C%2520Synthetic%250A%2520%2520Data%2520Generation%252C%2520and%2520System%2520Evaluation%26entry.906535625%3DKrisztian%2520Balog%2520and%2520ChengXiang%2520Zhai%26entry.1292438233%3D%2520%2520User%2520simulation%2520is%2520an%2520emerging%2520interdisciplinary%2520topic%2520with%2520multiple%2520critical%250Aapplications%2520in%2520the%2520era%2520of%2520Generative%2520AI.%2520It%2520involves%2520creating%2520an%2520intelligent%250Aagent%2520that%2520mimics%2520the%2520actions%2520of%2520a%2520human%2520user%2520interacting%2520with%2520an%2520AI%2520system%252C%250Aenabling%2520researchers%2520to%2520model%2520and%2520analyze%2520user%2520behaviour%252C%2520generate%2520synthetic%250Adata%2520for%2520training%252C%2520and%2520evaluate%2520interactive%2520AI%2520systems%2520in%2520a%2520controlled%2520and%250Areproducible%2520manner.%2520User%2520simulation%2520has%2520profound%2520implications%2520for%2520diverse%250Afields%2520and%2520plays%2520a%2520vital%2520role%2520in%2520the%2520pursuit%2520of%2520Artificial%2520General%250AIntelligence.%2520This%2520paper%2520provides%2520an%2520overview%2520of%2520user%2520simulation%252C%2520highlighting%250Aits%2520key%2520applications%252C%2520connections%2520to%2520various%2520disciplines%252C%2520and%2520outlining%2520future%250Aresearch%2520directions%2520to%2520advance%2520this%2520increasingly%2520important%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User%20Simulation%20in%20the%20Era%20of%20Generative%20AI%3A%20User%20Modeling%2C%20Synthetic%0A%20%20Data%20Generation%2C%20and%20System%20Evaluation&entry.906535625=Krisztian%20Balog%20and%20ChengXiang%20Zhai&entry.1292438233=%20%20User%20simulation%20is%20an%20emerging%20interdisciplinary%20topic%20with%20multiple%20critical%0Aapplications%20in%20the%20era%20of%20Generative%20AI.%20It%20involves%20creating%20an%20intelligent%0Aagent%20that%20mimics%20the%20actions%20of%20a%20human%20user%20interacting%20with%20an%20AI%20system%2C%0Aenabling%20researchers%20to%20model%20and%20analyze%20user%20behaviour%2C%20generate%20synthetic%0Adata%20for%20training%2C%20and%20evaluate%20interactive%20AI%20systems%20in%20a%20controlled%20and%0Areproducible%20manner.%20User%20simulation%20has%20profound%20implications%20for%20diverse%0Afields%20and%20plays%20a%20vital%20role%20in%20the%20pursuit%20of%20Artificial%20General%0AIntelligence.%20This%20paper%20provides%20an%20overview%20of%20user%20simulation%2C%20highlighting%0Aits%20key%20applications%2C%20connections%20to%20various%20disciplines%2C%20and%20outlining%20future%0Aresearch%20directions%20to%20advance%20this%20increasingly%20important%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04410v1&entry.124074799=Read"},
{"title": "Embedding Similarity Guided License Plate Super Resolution", "author": "Abderrezzaq Sendjasni and Mohamed-Chaker Larabi", "abstract": "  Super-resolution (SR) techniques play a pivotal role in enhancing the quality\nof low-resolution images, particularly for applications such as security and\nsurveillance, where accurate license plate recognition is crucial. This study\nproposes a novel framework that combines pixel-based loss with embedding\nsimilarity learning to address the unique challenges of license plate\nsuper-resolution (LPSR). The introduced pixel and embedding consistency loss\n(PECL) integrates a Siamese network and applies contrastive loss to force\nembedding similarities to improve perceptual and structural fidelity. By\neffectively balancing pixel-wise accuracy with embedding-level consistency, the\nframework achieves superior alignment of fine-grained features between\nhigh-resolution (HR) and super-resolved (SR) license plates. Extensive\nexperiments on the CCPD dataset validate the efficacy of the proposed\nframework, demonstrating consistent improvements over state-of-the-art methods\nin terms of PSNR_RGB, PSNR_Y and optical character recognition (OCR) accuracy.\nThese results highlight the potential of embedding similarity learning to\nadvance both perceptual quality and task-specific performance in extreme\nsuper-resolution scenarios.\n", "link": "http://arxiv.org/abs/2501.01483v2", "date": "2025-01-08", "relevancy": 2.5807, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5385}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.506}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Similarity%20Guided%20License%20Plate%20Super%20Resolution&body=Title%3A%20Embedding%20Similarity%20Guided%20License%20Plate%20Super%20Resolution%0AAuthor%3A%20Abderrezzaq%20Sendjasni%20and%20Mohamed-Chaker%20Larabi%0AAbstract%3A%20%20%20Super-resolution%20%28SR%29%20techniques%20play%20a%20pivotal%20role%20in%20enhancing%20the%20quality%0Aof%20low-resolution%20images%2C%20particularly%20for%20applications%20such%20as%20security%20and%0Asurveillance%2C%20where%20accurate%20license%20plate%20recognition%20is%20crucial.%20This%20study%0Aproposes%20a%20novel%20framework%20that%20combines%20pixel-based%20loss%20with%20embedding%0Asimilarity%20learning%20to%20address%20the%20unique%20challenges%20of%20license%20plate%0Asuper-resolution%20%28LPSR%29.%20The%20introduced%20pixel%20and%20embedding%20consistency%20loss%0A%28PECL%29%20integrates%20a%20Siamese%20network%20and%20applies%20contrastive%20loss%20to%20force%0Aembedding%20similarities%20to%20improve%20perceptual%20and%20structural%20fidelity.%20By%0Aeffectively%20balancing%20pixel-wise%20accuracy%20with%20embedding-level%20consistency%2C%20the%0Aframework%20achieves%20superior%20alignment%20of%20fine-grained%20features%20between%0Ahigh-resolution%20%28HR%29%20and%20super-resolved%20%28SR%29%20license%20plates.%20Extensive%0Aexperiments%20on%20the%20CCPD%20dataset%20validate%20the%20efficacy%20of%20the%20proposed%0Aframework%2C%20demonstrating%20consistent%20improvements%20over%20state-of-the-art%20methods%0Ain%20terms%20of%20PSNR_RGB%2C%20PSNR_Y%20and%20optical%20character%20recognition%20%28OCR%29%20accuracy.%0AThese%20results%20highlight%20the%20potential%20of%20embedding%20similarity%20learning%20to%0Aadvance%20both%20perceptual%20quality%20and%20task-specific%20performance%20in%20extreme%0Asuper-resolution%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Similarity%2520Guided%2520License%2520Plate%2520Super%2520Resolution%26entry.906535625%3DAbderrezzaq%2520Sendjasni%2520and%2520Mohamed-Chaker%2520Larabi%26entry.1292438233%3D%2520%2520Super-resolution%2520%2528SR%2529%2520techniques%2520play%2520a%2520pivotal%2520role%2520in%2520enhancing%2520the%2520quality%250Aof%2520low-resolution%2520images%252C%2520particularly%2520for%2520applications%2520such%2520as%2520security%2520and%250Asurveillance%252C%2520where%2520accurate%2520license%2520plate%2520recognition%2520is%2520crucial.%2520This%2520study%250Aproposes%2520a%2520novel%2520framework%2520that%2520combines%2520pixel-based%2520loss%2520with%2520embedding%250Asimilarity%2520learning%2520to%2520address%2520the%2520unique%2520challenges%2520of%2520license%2520plate%250Asuper-resolution%2520%2528LPSR%2529.%2520The%2520introduced%2520pixel%2520and%2520embedding%2520consistency%2520loss%250A%2528PECL%2529%2520integrates%2520a%2520Siamese%2520network%2520and%2520applies%2520contrastive%2520loss%2520to%2520force%250Aembedding%2520similarities%2520to%2520improve%2520perceptual%2520and%2520structural%2520fidelity.%2520By%250Aeffectively%2520balancing%2520pixel-wise%2520accuracy%2520with%2520embedding-level%2520consistency%252C%2520the%250Aframework%2520achieves%2520superior%2520alignment%2520of%2520fine-grained%2520features%2520between%250Ahigh-resolution%2520%2528HR%2529%2520and%2520super-resolved%2520%2528SR%2529%2520license%2520plates.%2520Extensive%250Aexperiments%2520on%2520the%2520CCPD%2520dataset%2520validate%2520the%2520efficacy%2520of%2520the%2520proposed%250Aframework%252C%2520demonstrating%2520consistent%2520improvements%2520over%2520state-of-the-art%2520methods%250Ain%2520terms%2520of%2520PSNR_RGB%252C%2520PSNR_Y%2520and%2520optical%2520character%2520recognition%2520%2528OCR%2529%2520accuracy.%250AThese%2520results%2520highlight%2520the%2520potential%2520of%2520embedding%2520similarity%2520learning%2520to%250Aadvance%2520both%2520perceptual%2520quality%2520and%2520task-specific%2520performance%2520in%2520extreme%250Asuper-resolution%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Similarity%20Guided%20License%20Plate%20Super%20Resolution&entry.906535625=Abderrezzaq%20Sendjasni%20and%20Mohamed-Chaker%20Larabi&entry.1292438233=%20%20Super-resolution%20%28SR%29%20techniques%20play%20a%20pivotal%20role%20in%20enhancing%20the%20quality%0Aof%20low-resolution%20images%2C%20particularly%20for%20applications%20such%20as%20security%20and%0Asurveillance%2C%20where%20accurate%20license%20plate%20recognition%20is%20crucial.%20This%20study%0Aproposes%20a%20novel%20framework%20that%20combines%20pixel-based%20loss%20with%20embedding%0Asimilarity%20learning%20to%20address%20the%20unique%20challenges%20of%20license%20plate%0Asuper-resolution%20%28LPSR%29.%20The%20introduced%20pixel%20and%20embedding%20consistency%20loss%0A%28PECL%29%20integrates%20a%20Siamese%20network%20and%20applies%20contrastive%20loss%20to%20force%0Aembedding%20similarities%20to%20improve%20perceptual%20and%20structural%20fidelity.%20By%0Aeffectively%20balancing%20pixel-wise%20accuracy%20with%20embedding-level%20consistency%2C%20the%0Aframework%20achieves%20superior%20alignment%20of%20fine-grained%20features%20between%0Ahigh-resolution%20%28HR%29%20and%20super-resolved%20%28SR%29%20license%20plates.%20Extensive%0Aexperiments%20on%20the%20CCPD%20dataset%20validate%20the%20efficacy%20of%20the%20proposed%0Aframework%2C%20demonstrating%20consistent%20improvements%20over%20state-of-the-art%20methods%0Ain%20terms%20of%20PSNR_RGB%2C%20PSNR_Y%20and%20optical%20character%20recognition%20%28OCR%29%20accuracy.%0AThese%20results%20highlight%20the%20potential%20of%20embedding%20similarity%20learning%20to%0Aadvance%20both%20perceptual%20quality%20and%20task-specific%20performance%20in%20extreme%0Asuper-resolution%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01483v2&entry.124074799=Read"},
{"title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding", "author": "Abdulfattah Safa and G\u00f6zde G\u00fcl \u015eahin", "abstract": "  Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API.\n", "link": "http://arxiv.org/abs/2409.15861v2", "date": "2025-01-08", "relevancy": 2.568, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Zero-Shot%20Open-Vocabulary%20Pipeline%20for%20Dialogue%20Understanding&body=Title%3A%20A%20Zero-Shot%20Open-Vocabulary%20Pipeline%20for%20Dialogue%20Understanding%0AAuthor%3A%20Abdulfattah%20Safa%20and%20G%C3%B6zde%20G%C3%BCl%20%C5%9Eahin%0AAbstract%3A%20%20%20Dialogue%20State%20Tracking%20%28DST%29%20is%20crucial%20for%20understanding%20user%20needs%20and%0Aexecuting%20appropriate%20system%20actions%20in%20task-oriented%20dialogues.%20Majority%20of%0Aexisting%20DST%20methods%20are%20designed%20to%20work%20within%20predefined%20ontologies%20and%0Aassume%20the%20availability%20of%20gold%20domain%20labels%2C%20struggling%20with%20adapting%20to%20new%0Aslots%20values.%20While%20Large%20Language%20Models%20%28LLMs%29-based%20systems%20show%20promising%0Azero-shot%20DST%20performance%2C%20they%20either%20require%20extensive%20computational%0Aresources%20or%20they%20underperform%20existing%20fully-trained%20systems%2C%20limiting%20their%0Apracticality.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20zero-shot%2C%0Aopen-vocabulary%20system%20that%20integrates%20domain%20classification%20and%20DST%20in%20a%0Asingle%20pipeline.%20Our%20approach%20includes%20reformulating%20DST%20as%20a%0Aquestion-answering%20task%20for%20less%20capable%20models%20and%20employing%20self-refining%0Aprompts%20for%20more%20adaptable%20ones.%20Our%20system%20does%20not%20rely%20on%20fixed%20slot%20values%0Adefined%20in%20the%20ontology%20allowing%20the%20system%20to%20adapt%20dynamically.%20We%20compare%0Aour%20approach%20with%20existing%20SOTA%2C%20and%20show%20that%20it%20provides%20up%20to%2020%25%20better%0AJoint%20Goal%20Accuracy%20%28JGA%29%20over%20previous%20methods%20on%20datasets%20like%20Multi-WOZ%202.1%2C%0Awith%20up%20to%2090%25%20fewer%20requests%20to%20the%20LLM%20API.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Zero-Shot%2520Open-Vocabulary%2520Pipeline%2520for%2520Dialogue%2520Understanding%26entry.906535625%3DAbdulfattah%2520Safa%2520and%2520G%25C3%25B6zde%2520G%25C3%25BCl%2520%25C5%259Eahin%26entry.1292438233%3D%2520%2520Dialogue%2520State%2520Tracking%2520%2528DST%2529%2520is%2520crucial%2520for%2520understanding%2520user%2520needs%2520and%250Aexecuting%2520appropriate%2520system%2520actions%2520in%2520task-oriented%2520dialogues.%2520Majority%2520of%250Aexisting%2520DST%2520methods%2520are%2520designed%2520to%2520work%2520within%2520predefined%2520ontologies%2520and%250Aassume%2520the%2520availability%2520of%2520gold%2520domain%2520labels%252C%2520struggling%2520with%2520adapting%2520to%2520new%250Aslots%2520values.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529-based%2520systems%2520show%2520promising%250Azero-shot%2520DST%2520performance%252C%2520they%2520either%2520require%2520extensive%2520computational%250Aresources%2520or%2520they%2520underperform%2520existing%2520fully-trained%2520systems%252C%2520limiting%2520their%250Apracticality.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520zero-shot%252C%250Aopen-vocabulary%2520system%2520that%2520integrates%2520domain%2520classification%2520and%2520DST%2520in%2520a%250Asingle%2520pipeline.%2520Our%2520approach%2520includes%2520reformulating%2520DST%2520as%2520a%250Aquestion-answering%2520task%2520for%2520less%2520capable%2520models%2520and%2520employing%2520self-refining%250Aprompts%2520for%2520more%2520adaptable%2520ones.%2520Our%2520system%2520does%2520not%2520rely%2520on%2520fixed%2520slot%2520values%250Adefined%2520in%2520the%2520ontology%2520allowing%2520the%2520system%2520to%2520adapt%2520dynamically.%2520We%2520compare%250Aour%2520approach%2520with%2520existing%2520SOTA%252C%2520and%2520show%2520that%2520it%2520provides%2520up%2520to%252020%2525%2520better%250AJoint%2520Goal%2520Accuracy%2520%2528JGA%2529%2520over%2520previous%2520methods%2520on%2520datasets%2520like%2520Multi-WOZ%25202.1%252C%250Awith%2520up%2520to%252090%2525%2520fewer%2520requests%2520to%2520the%2520LLM%2520API.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Zero-Shot%20Open-Vocabulary%20Pipeline%20for%20Dialogue%20Understanding&entry.906535625=Abdulfattah%20Safa%20and%20G%C3%B6zde%20G%C3%BCl%20%C5%9Eahin&entry.1292438233=%20%20Dialogue%20State%20Tracking%20%28DST%29%20is%20crucial%20for%20understanding%20user%20needs%20and%0Aexecuting%20appropriate%20system%20actions%20in%20task-oriented%20dialogues.%20Majority%20of%0Aexisting%20DST%20methods%20are%20designed%20to%20work%20within%20predefined%20ontologies%20and%0Aassume%20the%20availability%20of%20gold%20domain%20labels%2C%20struggling%20with%20adapting%20to%20new%0Aslots%20values.%20While%20Large%20Language%20Models%20%28LLMs%29-based%20systems%20show%20promising%0Azero-shot%20DST%20performance%2C%20they%20either%20require%20extensive%20computational%0Aresources%20or%20they%20underperform%20existing%20fully-trained%20systems%2C%20limiting%20their%0Apracticality.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20zero-shot%2C%0Aopen-vocabulary%20system%20that%20integrates%20domain%20classification%20and%20DST%20in%20a%0Asingle%20pipeline.%20Our%20approach%20includes%20reformulating%20DST%20as%20a%0Aquestion-answering%20task%20for%20less%20capable%20models%20and%20employing%20self-refining%0Aprompts%20for%20more%20adaptable%20ones.%20Our%20system%20does%20not%20rely%20on%20fixed%20slot%20values%0Adefined%20in%20the%20ontology%20allowing%20the%20system%20to%20adapt%20dynamically.%20We%20compare%0Aour%20approach%20with%20existing%20SOTA%2C%20and%20show%20that%20it%20provides%20up%20to%2020%25%20better%0AJoint%20Goal%20Accuracy%20%28JGA%29%20over%20previous%20methods%20on%20datasets%20like%20Multi-WOZ%202.1%2C%0Awith%20up%20to%2090%25%20fewer%20requests%20to%20the%20LLM%20API.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15861v2&entry.124074799=Read"},
{"title": "Enhancing Low-Cost Video Editing with Lightweight Adaptors and\n  Temporal-Aware Inversion", "author": "Yangfan He and Sida Li and Kun Li and Jianhui Wang and Binxu Li and Tianyu Shi and Jun Yin and Miao Zhang and Xueqian Wang", "abstract": "  Recent advancements in text-to-image (T2I) generation using diffusion models\nhave enabled cost-effective video-editing applications by leveraging\npre-trained models, eliminating the need for resource-intensive training.\nHowever, the frame-independence of T2I generation often results in poor\ntemporal consistency. Existing methods address this issue through temporal\nlayer fine-tuning or inference-based temporal propagation, but these approaches\nsuffer from high training costs or limited temporal coherence. To address these\nchallenges, we propose a General and Efficient Adapter (GE-Adapter) that\nintegrates temporal-spatial and semantic consistency with Baliteral DDIM\ninversion. This framework introduces three key components: (1) Frame-based\nTemporal Consistency Blocks (FTC Blocks) to capture frame-specific features and\nenforce smooth inter-frame transitions via temporally-aware loss functions; (2)\nChannel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral\nfilters to enhance spatial coherence by reducing noise and artifacts; and (3)\nToken-based Semantic Consistency Module (TSC Module) to maintain semantic\nalignment using shared prompt tokens and frame-specific tokens. Our method\nsignificantly improves perceptual quality, text-image alignment, and temporal\ncoherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves\nenhanced fidelity and frame-to-frame coherence, offering a practical solution\nfor T2V editing.\n", "link": "http://arxiv.org/abs/2501.04606v1", "date": "2025-01-08", "relevancy": 2.5618, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6682}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.643}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Low-Cost%20Video%20Editing%20with%20Lightweight%20Adaptors%20and%0A%20%20Temporal-Aware%20Inversion&body=Title%3A%20Enhancing%20Low-Cost%20Video%20Editing%20with%20Lightweight%20Adaptors%20and%0A%20%20Temporal-Aware%20Inversion%0AAuthor%3A%20Yangfan%20He%20and%20Sida%20Li%20and%20Kun%20Li%20and%20Jianhui%20Wang%20and%20Binxu%20Li%20and%20Tianyu%20Shi%20and%20Jun%20Yin%20and%20Miao%20Zhang%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20%28T2I%29%20generation%20using%20diffusion%20models%0Ahave%20enabled%20cost-effective%20video-editing%20applications%20by%20leveraging%0Apre-trained%20models%2C%20eliminating%20the%20need%20for%20resource-intensive%20training.%0AHowever%2C%20the%20frame-independence%20of%20T2I%20generation%20often%20results%20in%20poor%0Atemporal%20consistency.%20Existing%20methods%20address%20this%20issue%20through%20temporal%0Alayer%20fine-tuning%20or%20inference-based%20temporal%20propagation%2C%20but%20these%20approaches%0Asuffer%20from%20high%20training%20costs%20or%20limited%20temporal%20coherence.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20General%20and%20Efficient%20Adapter%20%28GE-Adapter%29%20that%0Aintegrates%20temporal-spatial%20and%20semantic%20consistency%20with%20Baliteral%20DDIM%0Ainversion.%20This%20framework%20introduces%20three%20key%20components%3A%20%281%29%20Frame-based%0ATemporal%20Consistency%20Blocks%20%28FTC%20Blocks%29%20to%20capture%20frame-specific%20features%20and%0Aenforce%20smooth%20inter-frame%20transitions%20via%20temporally-aware%20loss%20functions%3B%20%282%29%0AChannel-dependent%20Spatial%20Consistency%20Blocks%20%28SCD%20Blocks%29%20employing%20bilateral%0Afilters%20to%20enhance%20spatial%20coherence%20by%20reducing%20noise%20and%20artifacts%3B%20and%20%283%29%0AToken-based%20Semantic%20Consistency%20Module%20%28TSC%20Module%29%20to%20maintain%20semantic%0Aalignment%20using%20shared%20prompt%20tokens%20and%20frame-specific%20tokens.%20Our%20method%0Asignificantly%20improves%20perceptual%20quality%2C%20text-image%20alignment%2C%20and%20temporal%0Acoherence%2C%20as%20demonstrated%20on%20the%20MSR-VTT%20dataset.%20Additionally%2C%20it%20achieves%0Aenhanced%20fidelity%20and%20frame-to-frame%20coherence%2C%20offering%20a%20practical%20solution%0Afor%20T2V%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Low-Cost%2520Video%2520Editing%2520with%2520Lightweight%2520Adaptors%2520and%250A%2520%2520Temporal-Aware%2520Inversion%26entry.906535625%3DYangfan%2520He%2520and%2520Sida%2520Li%2520and%2520Kun%2520Li%2520and%2520Jianhui%2520Wang%2520and%2520Binxu%2520Li%2520and%2520Tianyu%2520Shi%2520and%2520Jun%2520Yin%2520and%2520Miao%2520Zhang%2520and%2520Xueqian%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520%2528T2I%2529%2520generation%2520using%2520diffusion%2520models%250Ahave%2520enabled%2520cost-effective%2520video-editing%2520applications%2520by%2520leveraging%250Apre-trained%2520models%252C%2520eliminating%2520the%2520need%2520for%2520resource-intensive%2520training.%250AHowever%252C%2520the%2520frame-independence%2520of%2520T2I%2520generation%2520often%2520results%2520in%2520poor%250Atemporal%2520consistency.%2520Existing%2520methods%2520address%2520this%2520issue%2520through%2520temporal%250Alayer%2520fine-tuning%2520or%2520inference-based%2520temporal%2520propagation%252C%2520but%2520these%2520approaches%250Asuffer%2520from%2520high%2520training%2520costs%2520or%2520limited%2520temporal%2520coherence.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520General%2520and%2520Efficient%2520Adapter%2520%2528GE-Adapter%2529%2520that%250Aintegrates%2520temporal-spatial%2520and%2520semantic%2520consistency%2520with%2520Baliteral%2520DDIM%250Ainversion.%2520This%2520framework%2520introduces%2520three%2520key%2520components%253A%2520%25281%2529%2520Frame-based%250ATemporal%2520Consistency%2520Blocks%2520%2528FTC%2520Blocks%2529%2520to%2520capture%2520frame-specific%2520features%2520and%250Aenforce%2520smooth%2520inter-frame%2520transitions%2520via%2520temporally-aware%2520loss%2520functions%253B%2520%25282%2529%250AChannel-dependent%2520Spatial%2520Consistency%2520Blocks%2520%2528SCD%2520Blocks%2529%2520employing%2520bilateral%250Afilters%2520to%2520enhance%2520spatial%2520coherence%2520by%2520reducing%2520noise%2520and%2520artifacts%253B%2520and%2520%25283%2529%250AToken-based%2520Semantic%2520Consistency%2520Module%2520%2528TSC%2520Module%2529%2520to%2520maintain%2520semantic%250Aalignment%2520using%2520shared%2520prompt%2520tokens%2520and%2520frame-specific%2520tokens.%2520Our%2520method%250Asignificantly%2520improves%2520perceptual%2520quality%252C%2520text-image%2520alignment%252C%2520and%2520temporal%250Acoherence%252C%2520as%2520demonstrated%2520on%2520the%2520MSR-VTT%2520dataset.%2520Additionally%252C%2520it%2520achieves%250Aenhanced%2520fidelity%2520and%2520frame-to-frame%2520coherence%252C%2520offering%2520a%2520practical%2520solution%250Afor%2520T2V%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Low-Cost%20Video%20Editing%20with%20Lightweight%20Adaptors%20and%0A%20%20Temporal-Aware%20Inversion&entry.906535625=Yangfan%20He%20and%20Sida%20Li%20and%20Kun%20Li%20and%20Jianhui%20Wang%20and%20Binxu%20Li%20and%20Tianyu%20Shi%20and%20Jun%20Yin%20and%20Miao%20Zhang%20and%20Xueqian%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20%28T2I%29%20generation%20using%20diffusion%20models%0Ahave%20enabled%20cost-effective%20video-editing%20applications%20by%20leveraging%0Apre-trained%20models%2C%20eliminating%20the%20need%20for%20resource-intensive%20training.%0AHowever%2C%20the%20frame-independence%20of%20T2I%20generation%20often%20results%20in%20poor%0Atemporal%20consistency.%20Existing%20methods%20address%20this%20issue%20through%20temporal%0Alayer%20fine-tuning%20or%20inference-based%20temporal%20propagation%2C%20but%20these%20approaches%0Asuffer%20from%20high%20training%20costs%20or%20limited%20temporal%20coherence.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20General%20and%20Efficient%20Adapter%20%28GE-Adapter%29%20that%0Aintegrates%20temporal-spatial%20and%20semantic%20consistency%20with%20Baliteral%20DDIM%0Ainversion.%20This%20framework%20introduces%20three%20key%20components%3A%20%281%29%20Frame-based%0ATemporal%20Consistency%20Blocks%20%28FTC%20Blocks%29%20to%20capture%20frame-specific%20features%20and%0Aenforce%20smooth%20inter-frame%20transitions%20via%20temporally-aware%20loss%20functions%3B%20%282%29%0AChannel-dependent%20Spatial%20Consistency%20Blocks%20%28SCD%20Blocks%29%20employing%20bilateral%0Afilters%20to%20enhance%20spatial%20coherence%20by%20reducing%20noise%20and%20artifacts%3B%20and%20%283%29%0AToken-based%20Semantic%20Consistency%20Module%20%28TSC%20Module%29%20to%20maintain%20semantic%0Aalignment%20using%20shared%20prompt%20tokens%20and%20frame-specific%20tokens.%20Our%20method%0Asignificantly%20improves%20perceptual%20quality%2C%20text-image%20alignment%2C%20and%20temporal%0Acoherence%2C%20as%20demonstrated%20on%20the%20MSR-VTT%20dataset.%20Additionally%2C%20it%20achieves%0Aenhanced%20fidelity%20and%20frame-to-frame%20coherence%2C%20offering%20a%20practical%20solution%0Afor%20T2V%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04606v1&entry.124074799=Read"},
{"title": "Comprehensive Examination of Unrolled Networks for Linear Inverse\n  Problems", "author": "Eric Chen and Xi Chen and Arian Maleki and Shirin Jalali", "abstract": "  Unrolled networks have become prevalent in various computer vision and\nimaging tasks. Although they have demonstrated remarkable efficacy in solving\nspecific computer vision and computational imaging tasks, their adaptation to\nother applications presents considerable challenges. This is primarily due to\nthe multitude of design decisions that practitioners working on new\napplications must navigate, each potentially affecting the network's overall\nperformance. These decisions include selecting the optimization algorithm,\ndefining the loss function, and determining the number of convolutional layers,\namong others. Compounding the issue, evaluating each design choice requires\ntime-consuming simulations to train, fine-tune the neural network, and optimize\nfor its performance. As a result, the process of exploring multiple options and\nidentifying the optimal configuration becomes time-consuming and\ncomputationally demanding. The main objectives of this paper are (1) to unify\nsome ideas and methodologies used in unrolled networks to reduce the number of\ndesign choices a user has to make, and (2) to report a comprehensive ablation\nstudy to discuss the impact of each of the choices involved in designing\nunrolled networks and present practical recommendations based on our findings.\nWe anticipate that this study will help scientists and engineers design\nunrolled networks for their applications and diagnose problems within their\nnetworks efficiently.\n", "link": "http://arxiv.org/abs/2501.04608v1", "date": "2025-01-08", "relevancy": 2.5578, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Examination%20of%20Unrolled%20Networks%20for%20Linear%20Inverse%0A%20%20Problems&body=Title%3A%20Comprehensive%20Examination%20of%20Unrolled%20Networks%20for%20Linear%20Inverse%0A%20%20Problems%0AAuthor%3A%20Eric%20Chen%20and%20Xi%20Chen%20and%20Arian%20Maleki%20and%20Shirin%20Jalali%0AAbstract%3A%20%20%20Unrolled%20networks%20have%20become%20prevalent%20in%20various%20computer%20vision%20and%0Aimaging%20tasks.%20Although%20they%20have%20demonstrated%20remarkable%20efficacy%20in%20solving%0Aspecific%20computer%20vision%20and%20computational%20imaging%20tasks%2C%20their%20adaptation%20to%0Aother%20applications%20presents%20considerable%20challenges.%20This%20is%20primarily%20due%20to%0Athe%20multitude%20of%20design%20decisions%20that%20practitioners%20working%20on%20new%0Aapplications%20must%20navigate%2C%20each%20potentially%20affecting%20the%20network%27s%20overall%0Aperformance.%20These%20decisions%20include%20selecting%20the%20optimization%20algorithm%2C%0Adefining%20the%20loss%20function%2C%20and%20determining%20the%20number%20of%20convolutional%20layers%2C%0Aamong%20others.%20Compounding%20the%20issue%2C%20evaluating%20each%20design%20choice%20requires%0Atime-consuming%20simulations%20to%20train%2C%20fine-tune%20the%20neural%20network%2C%20and%20optimize%0Afor%20its%20performance.%20As%20a%20result%2C%20the%20process%20of%20exploring%20multiple%20options%20and%0Aidentifying%20the%20optimal%20configuration%20becomes%20time-consuming%20and%0Acomputationally%20demanding.%20The%20main%20objectives%20of%20this%20paper%20are%20%281%29%20to%20unify%0Asome%20ideas%20and%20methodologies%20used%20in%20unrolled%20networks%20to%20reduce%20the%20number%20of%0Adesign%20choices%20a%20user%20has%20to%20make%2C%20and%20%282%29%20to%20report%20a%20comprehensive%20ablation%0Astudy%20to%20discuss%20the%20impact%20of%20each%20of%20the%20choices%20involved%20in%20designing%0Aunrolled%20networks%20and%20present%20practical%20recommendations%20based%20on%20our%20findings.%0AWe%20anticipate%20that%20this%20study%20will%20help%20scientists%20and%20engineers%20design%0Aunrolled%20networks%20for%20their%20applications%20and%20diagnose%20problems%20within%20their%0Anetworks%20efficiently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Examination%2520of%2520Unrolled%2520Networks%2520for%2520Linear%2520Inverse%250A%2520%2520Problems%26entry.906535625%3DEric%2520Chen%2520and%2520Xi%2520Chen%2520and%2520Arian%2520Maleki%2520and%2520Shirin%2520Jalali%26entry.1292438233%3D%2520%2520Unrolled%2520networks%2520have%2520become%2520prevalent%2520in%2520various%2520computer%2520vision%2520and%250Aimaging%2520tasks.%2520Although%2520they%2520have%2520demonstrated%2520remarkable%2520efficacy%2520in%2520solving%250Aspecific%2520computer%2520vision%2520and%2520computational%2520imaging%2520tasks%252C%2520their%2520adaptation%2520to%250Aother%2520applications%2520presents%2520considerable%2520challenges.%2520This%2520is%2520primarily%2520due%2520to%250Athe%2520multitude%2520of%2520design%2520decisions%2520that%2520practitioners%2520working%2520on%2520new%250Aapplications%2520must%2520navigate%252C%2520each%2520potentially%2520affecting%2520the%2520network%2527s%2520overall%250Aperformance.%2520These%2520decisions%2520include%2520selecting%2520the%2520optimization%2520algorithm%252C%250Adefining%2520the%2520loss%2520function%252C%2520and%2520determining%2520the%2520number%2520of%2520convolutional%2520layers%252C%250Aamong%2520others.%2520Compounding%2520the%2520issue%252C%2520evaluating%2520each%2520design%2520choice%2520requires%250Atime-consuming%2520simulations%2520to%2520train%252C%2520fine-tune%2520the%2520neural%2520network%252C%2520and%2520optimize%250Afor%2520its%2520performance.%2520As%2520a%2520result%252C%2520the%2520process%2520of%2520exploring%2520multiple%2520options%2520and%250Aidentifying%2520the%2520optimal%2520configuration%2520becomes%2520time-consuming%2520and%250Acomputationally%2520demanding.%2520The%2520main%2520objectives%2520of%2520this%2520paper%2520are%2520%25281%2529%2520to%2520unify%250Asome%2520ideas%2520and%2520methodologies%2520used%2520in%2520unrolled%2520networks%2520to%2520reduce%2520the%2520number%2520of%250Adesign%2520choices%2520a%2520user%2520has%2520to%2520make%252C%2520and%2520%25282%2529%2520to%2520report%2520a%2520comprehensive%2520ablation%250Astudy%2520to%2520discuss%2520the%2520impact%2520of%2520each%2520of%2520the%2520choices%2520involved%2520in%2520designing%250Aunrolled%2520networks%2520and%2520present%2520practical%2520recommendations%2520based%2520on%2520our%2520findings.%250AWe%2520anticipate%2520that%2520this%2520study%2520will%2520help%2520scientists%2520and%2520engineers%2520design%250Aunrolled%2520networks%2520for%2520their%2520applications%2520and%2520diagnose%2520problems%2520within%2520their%250Anetworks%2520efficiently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Examination%20of%20Unrolled%20Networks%20for%20Linear%20Inverse%0A%20%20Problems&entry.906535625=Eric%20Chen%20and%20Xi%20Chen%20and%20Arian%20Maleki%20and%20Shirin%20Jalali&entry.1292438233=%20%20Unrolled%20networks%20have%20become%20prevalent%20in%20various%20computer%20vision%20and%0Aimaging%20tasks.%20Although%20they%20have%20demonstrated%20remarkable%20efficacy%20in%20solving%0Aspecific%20computer%20vision%20and%20computational%20imaging%20tasks%2C%20their%20adaptation%20to%0Aother%20applications%20presents%20considerable%20challenges.%20This%20is%20primarily%20due%20to%0Athe%20multitude%20of%20design%20decisions%20that%20practitioners%20working%20on%20new%0Aapplications%20must%20navigate%2C%20each%20potentially%20affecting%20the%20network%27s%20overall%0Aperformance.%20These%20decisions%20include%20selecting%20the%20optimization%20algorithm%2C%0Adefining%20the%20loss%20function%2C%20and%20determining%20the%20number%20of%20convolutional%20layers%2C%0Aamong%20others.%20Compounding%20the%20issue%2C%20evaluating%20each%20design%20choice%20requires%0Atime-consuming%20simulations%20to%20train%2C%20fine-tune%20the%20neural%20network%2C%20and%20optimize%0Afor%20its%20performance.%20As%20a%20result%2C%20the%20process%20of%20exploring%20multiple%20options%20and%0Aidentifying%20the%20optimal%20configuration%20becomes%20time-consuming%20and%0Acomputationally%20demanding.%20The%20main%20objectives%20of%20this%20paper%20are%20%281%29%20to%20unify%0Asome%20ideas%20and%20methodologies%20used%20in%20unrolled%20networks%20to%20reduce%20the%20number%20of%0Adesign%20choices%20a%20user%20has%20to%20make%2C%20and%20%282%29%20to%20report%20a%20comprehensive%20ablation%0Astudy%20to%20discuss%20the%20impact%20of%20each%20of%20the%20choices%20involved%20in%20designing%0Aunrolled%20networks%20and%20present%20practical%20recommendations%20based%20on%20our%20findings.%0AWe%20anticipate%20that%20this%20study%20will%20help%20scientists%20and%20engineers%20design%0Aunrolled%20networks%20for%20their%20applications%20and%20diagnose%20problems%20within%20their%0Anetworks%20efficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04608v1&entry.124074799=Read"},
{"title": "Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise\n  Scheduling", "author": "Nannan Li and Kevin J. Shih and Bryan A. Plummer", "abstract": "  Given an isolated garment image in a canonical product view and a separate\nimage of a person, the virtual try-on task aims to generate a new image of the\nperson wearing the target garment. Prior virtual try-on works face two major\nchallenges in achieving this goal: a) the paired (human, garment) training data\nhas limited availability; b) generating textures on the human that perfectly\nmatch that of the prompted garment is difficult, often resulting in distorted\ntext and faded textures. Our work explores ways to tackle these issues through\nboth synthetic data as well as model refinement. We introduce a garment\nextraction model that generates (human, synthetic garment) pairs from a single\nimage of a clothed individual. The synthetic pairs can then be used to augment\nthe training of virtual try-on. We also propose an Error-Aware Refinement-based\nSchr\\\"odinger Bridge (EARSB) that surgically targets localized generation\nerrors for correcting the output of a base virtual try-on model. To identify\nlikely errors, we propose a weakly-supervised error classifier that localizes\nregions for refinement, subsequently augmenting the Schr\\\"odinger Bridge's\nnoise schedule with its confidence heatmap. Experiments on VITON-HD and\nDressCode-Upper demonstrate that our synthetic data augmentation enhances the\nperformance of prior work, while EARSB improves the overall image quality. In\nuser studies, our model is preferred by the users in an average of 59% of\ncases.\n", "link": "http://arxiv.org/abs/2501.04666v1", "date": "2025-01-08", "relevancy": 2.5541, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6466}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6435}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Virtual%20Try-On%20with%20Synthetic%20Pairs%20and%20Error-Aware%20Noise%0A%20%20Scheduling&body=Title%3A%20Enhancing%20Virtual%20Try-On%20with%20Synthetic%20Pairs%20and%20Error-Aware%20Noise%0A%20%20Scheduling%0AAuthor%3A%20Nannan%20Li%20and%20Kevin%20J.%20Shih%20and%20Bryan%20A.%20Plummer%0AAbstract%3A%20%20%20Given%20an%20isolated%20garment%20image%20in%20a%20canonical%20product%20view%20and%20a%20separate%0Aimage%20of%20a%20person%2C%20the%20virtual%20try-on%20task%20aims%20to%20generate%20a%20new%20image%20of%20the%0Aperson%20wearing%20the%20target%20garment.%20Prior%20virtual%20try-on%20works%20face%20two%20major%0Achallenges%20in%20achieving%20this%20goal%3A%20a%29%20the%20paired%20%28human%2C%20garment%29%20training%20data%0Ahas%20limited%20availability%3B%20b%29%20generating%20textures%20on%20the%20human%20that%20perfectly%0Amatch%20that%20of%20the%20prompted%20garment%20is%20difficult%2C%20often%20resulting%20in%20distorted%0Atext%20and%20faded%20textures.%20Our%20work%20explores%20ways%20to%20tackle%20these%20issues%20through%0Aboth%20synthetic%20data%20as%20well%20as%20model%20refinement.%20We%20introduce%20a%20garment%0Aextraction%20model%20that%20generates%20%28human%2C%20synthetic%20garment%29%20pairs%20from%20a%20single%0Aimage%20of%20a%20clothed%20individual.%20The%20synthetic%20pairs%20can%20then%20be%20used%20to%20augment%0Athe%20training%20of%20virtual%20try-on.%20We%20also%20propose%20an%20Error-Aware%20Refinement-based%0ASchr%5C%22odinger%20Bridge%20%28EARSB%29%20that%20surgically%20targets%20localized%20generation%0Aerrors%20for%20correcting%20the%20output%20of%20a%20base%20virtual%20try-on%20model.%20To%20identify%0Alikely%20errors%2C%20we%20propose%20a%20weakly-supervised%20error%20classifier%20that%20localizes%0Aregions%20for%20refinement%2C%20subsequently%20augmenting%20the%20Schr%5C%22odinger%20Bridge%27s%0Anoise%20schedule%20with%20its%20confidence%20heatmap.%20Experiments%20on%20VITON-HD%20and%0ADressCode-Upper%20demonstrate%20that%20our%20synthetic%20data%20augmentation%20enhances%20the%0Aperformance%20of%20prior%20work%2C%20while%20EARSB%20improves%20the%20overall%20image%20quality.%20In%0Auser%20studies%2C%20our%20model%20is%20preferred%20by%20the%20users%20in%20an%20average%20of%2059%25%20of%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Virtual%2520Try-On%2520with%2520Synthetic%2520Pairs%2520and%2520Error-Aware%2520Noise%250A%2520%2520Scheduling%26entry.906535625%3DNannan%2520Li%2520and%2520Kevin%2520J.%2520Shih%2520and%2520Bryan%2520A.%2520Plummer%26entry.1292438233%3D%2520%2520Given%2520an%2520isolated%2520garment%2520image%2520in%2520a%2520canonical%2520product%2520view%2520and%2520a%2520separate%250Aimage%2520of%2520a%2520person%252C%2520the%2520virtual%2520try-on%2520task%2520aims%2520to%2520generate%2520a%2520new%2520image%2520of%2520the%250Aperson%2520wearing%2520the%2520target%2520garment.%2520Prior%2520virtual%2520try-on%2520works%2520face%2520two%2520major%250Achallenges%2520in%2520achieving%2520this%2520goal%253A%2520a%2529%2520the%2520paired%2520%2528human%252C%2520garment%2529%2520training%2520data%250Ahas%2520limited%2520availability%253B%2520b%2529%2520generating%2520textures%2520on%2520the%2520human%2520that%2520perfectly%250Amatch%2520that%2520of%2520the%2520prompted%2520garment%2520is%2520difficult%252C%2520often%2520resulting%2520in%2520distorted%250Atext%2520and%2520faded%2520textures.%2520Our%2520work%2520explores%2520ways%2520to%2520tackle%2520these%2520issues%2520through%250Aboth%2520synthetic%2520data%2520as%2520well%2520as%2520model%2520refinement.%2520We%2520introduce%2520a%2520garment%250Aextraction%2520model%2520that%2520generates%2520%2528human%252C%2520synthetic%2520garment%2529%2520pairs%2520from%2520a%2520single%250Aimage%2520of%2520a%2520clothed%2520individual.%2520The%2520synthetic%2520pairs%2520can%2520then%2520be%2520used%2520to%2520augment%250Athe%2520training%2520of%2520virtual%2520try-on.%2520We%2520also%2520propose%2520an%2520Error-Aware%2520Refinement-based%250ASchr%255C%2522odinger%2520Bridge%2520%2528EARSB%2529%2520that%2520surgically%2520targets%2520localized%2520generation%250Aerrors%2520for%2520correcting%2520the%2520output%2520of%2520a%2520base%2520virtual%2520try-on%2520model.%2520To%2520identify%250Alikely%2520errors%252C%2520we%2520propose%2520a%2520weakly-supervised%2520error%2520classifier%2520that%2520localizes%250Aregions%2520for%2520refinement%252C%2520subsequently%2520augmenting%2520the%2520Schr%255C%2522odinger%2520Bridge%2527s%250Anoise%2520schedule%2520with%2520its%2520confidence%2520heatmap.%2520Experiments%2520on%2520VITON-HD%2520and%250ADressCode-Upper%2520demonstrate%2520that%2520our%2520synthetic%2520data%2520augmentation%2520enhances%2520the%250Aperformance%2520of%2520prior%2520work%252C%2520while%2520EARSB%2520improves%2520the%2520overall%2520image%2520quality.%2520In%250Auser%2520studies%252C%2520our%2520model%2520is%2520preferred%2520by%2520the%2520users%2520in%2520an%2520average%2520of%252059%2525%2520of%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Virtual%20Try-On%20with%20Synthetic%20Pairs%20and%20Error-Aware%20Noise%0A%20%20Scheduling&entry.906535625=Nannan%20Li%20and%20Kevin%20J.%20Shih%20and%20Bryan%20A.%20Plummer&entry.1292438233=%20%20Given%20an%20isolated%20garment%20image%20in%20a%20canonical%20product%20view%20and%20a%20separate%0Aimage%20of%20a%20person%2C%20the%20virtual%20try-on%20task%20aims%20to%20generate%20a%20new%20image%20of%20the%0Aperson%20wearing%20the%20target%20garment.%20Prior%20virtual%20try-on%20works%20face%20two%20major%0Achallenges%20in%20achieving%20this%20goal%3A%20a%29%20the%20paired%20%28human%2C%20garment%29%20training%20data%0Ahas%20limited%20availability%3B%20b%29%20generating%20textures%20on%20the%20human%20that%20perfectly%0Amatch%20that%20of%20the%20prompted%20garment%20is%20difficult%2C%20often%20resulting%20in%20distorted%0Atext%20and%20faded%20textures.%20Our%20work%20explores%20ways%20to%20tackle%20these%20issues%20through%0Aboth%20synthetic%20data%20as%20well%20as%20model%20refinement.%20We%20introduce%20a%20garment%0Aextraction%20model%20that%20generates%20%28human%2C%20synthetic%20garment%29%20pairs%20from%20a%20single%0Aimage%20of%20a%20clothed%20individual.%20The%20synthetic%20pairs%20can%20then%20be%20used%20to%20augment%0Athe%20training%20of%20virtual%20try-on.%20We%20also%20propose%20an%20Error-Aware%20Refinement-based%0ASchr%5C%22odinger%20Bridge%20%28EARSB%29%20that%20surgically%20targets%20localized%20generation%0Aerrors%20for%20correcting%20the%20output%20of%20a%20base%20virtual%20try-on%20model.%20To%20identify%0Alikely%20errors%2C%20we%20propose%20a%20weakly-supervised%20error%20classifier%20that%20localizes%0Aregions%20for%20refinement%2C%20subsequently%20augmenting%20the%20Schr%5C%22odinger%20Bridge%27s%0Anoise%20schedule%20with%20its%20confidence%20heatmap.%20Experiments%20on%20VITON-HD%20and%0ADressCode-Upper%20demonstrate%20that%20our%20synthetic%20data%20augmentation%20enhances%20the%0Aperformance%20of%20prior%20work%2C%20while%20EARSB%20improves%20the%20overall%20image%20quality.%20In%0Auser%20studies%2C%20our%20model%20is%20preferred%20by%20the%20users%20in%20an%20average%20of%2059%25%20of%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04666v1&entry.124074799=Read"},
{"title": "Energy-based Hopfield Boosting for Out-of-Distribution Detection", "author": "Claus Hofmann and Simon Schmid and Bernhard Lehner and Daniel Klotz and Sepp Hochreiter", "abstract": "  Out-of-distribution (OOD) detection is critical when deploying machine\nlearning models in the real world. Outlier exposure methods, which incorporate\nauxiliary outlier data in the training process, can drastically improve OOD\ndetection performance compared to approaches without advanced training\nstrategies. We introduce Hopfield Boosting, a boosting approach, which\nleverages modern Hopfield energy (MHE) to sharpen the decision boundary between\nthe in-distribution and OOD data. Hopfield Boosting encourages the model to\nconcentrate on hard-to-distinguish auxiliary outlier examples that lie close to\nthe decision boundary between in-distribution and auxiliary outlier data. Our\nmethod achieves a new state-of-the-art in OOD detection with outlier exposure,\nimproving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94\non CIFAR-100.\n", "link": "http://arxiv.org/abs/2405.08766v2", "date": "2025-01-08", "relevancy": 2.5389, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5539}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4898}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-based%20Hopfield%20Boosting%20for%20Out-of-Distribution%20Detection&body=Title%3A%20Energy-based%20Hopfield%20Boosting%20for%20Out-of-Distribution%20Detection%0AAuthor%3A%20Claus%20Hofmann%20and%20Simon%20Schmid%20and%20Bernhard%20Lehner%20and%20Daniel%20Klotz%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20critical%20when%20deploying%20machine%0Alearning%20models%20in%20the%20real%20world.%20Outlier%20exposure%20methods%2C%20which%20incorporate%0Aauxiliary%20outlier%20data%20in%20the%20training%20process%2C%20can%20drastically%20improve%20OOD%0Adetection%20performance%20compared%20to%20approaches%20without%20advanced%20training%0Astrategies.%20We%20introduce%20Hopfield%20Boosting%2C%20a%20boosting%20approach%2C%20which%0Aleverages%20modern%20Hopfield%20energy%20%28MHE%29%20to%20sharpen%20the%20decision%20boundary%20between%0Athe%20in-distribution%20and%20OOD%20data.%20Hopfield%20Boosting%20encourages%20the%20model%20to%0Aconcentrate%20on%20hard-to-distinguish%20auxiliary%20outlier%20examples%20that%20lie%20close%20to%0Athe%20decision%20boundary%20between%20in-distribution%20and%20auxiliary%20outlier%20data.%20Our%0Amethod%20achieves%20a%20new%20state-of-the-art%20in%20OOD%20detection%20with%20outlier%20exposure%2C%0Aimproving%20the%20FPR95%20metric%20from%202.28%20to%200.92%20on%20CIFAR-10%20and%20from%2011.76%20to%207.94%0Aon%20CIFAR-100.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-based%2520Hopfield%2520Boosting%2520for%2520Out-of-Distribution%2520Detection%26entry.906535625%3DClaus%2520Hofmann%2520and%2520Simon%2520Schmid%2520and%2520Bernhard%2520Lehner%2520and%2520Daniel%2520Klotz%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520critical%2520when%2520deploying%2520machine%250Alearning%2520models%2520in%2520the%2520real%2520world.%2520Outlier%2520exposure%2520methods%252C%2520which%2520incorporate%250Aauxiliary%2520outlier%2520data%2520in%2520the%2520training%2520process%252C%2520can%2520drastically%2520improve%2520OOD%250Adetection%2520performance%2520compared%2520to%2520approaches%2520without%2520advanced%2520training%250Astrategies.%2520We%2520introduce%2520Hopfield%2520Boosting%252C%2520a%2520boosting%2520approach%252C%2520which%250Aleverages%2520modern%2520Hopfield%2520energy%2520%2528MHE%2529%2520to%2520sharpen%2520the%2520decision%2520boundary%2520between%250Athe%2520in-distribution%2520and%2520OOD%2520data.%2520Hopfield%2520Boosting%2520encourages%2520the%2520model%2520to%250Aconcentrate%2520on%2520hard-to-distinguish%2520auxiliary%2520outlier%2520examples%2520that%2520lie%2520close%2520to%250Athe%2520decision%2520boundary%2520between%2520in-distribution%2520and%2520auxiliary%2520outlier%2520data.%2520Our%250Amethod%2520achieves%2520a%2520new%2520state-of-the-art%2520in%2520OOD%2520detection%2520with%2520outlier%2520exposure%252C%250Aimproving%2520the%2520FPR95%2520metric%2520from%25202.28%2520to%25200.92%2520on%2520CIFAR-10%2520and%2520from%252011.76%2520to%25207.94%250Aon%2520CIFAR-100.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-based%20Hopfield%20Boosting%20for%20Out-of-Distribution%20Detection&entry.906535625=Claus%20Hofmann%20and%20Simon%20Schmid%20and%20Bernhard%20Lehner%20and%20Daniel%20Klotz%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20critical%20when%20deploying%20machine%0Alearning%20models%20in%20the%20real%20world.%20Outlier%20exposure%20methods%2C%20which%20incorporate%0Aauxiliary%20outlier%20data%20in%20the%20training%20process%2C%20can%20drastically%20improve%20OOD%0Adetection%20performance%20compared%20to%20approaches%20without%20advanced%20training%0Astrategies.%20We%20introduce%20Hopfield%20Boosting%2C%20a%20boosting%20approach%2C%20which%0Aleverages%20modern%20Hopfield%20energy%20%28MHE%29%20to%20sharpen%20the%20decision%20boundary%20between%0Athe%20in-distribution%20and%20OOD%20data.%20Hopfield%20Boosting%20encourages%20the%20model%20to%0Aconcentrate%20on%20hard-to-distinguish%20auxiliary%20outlier%20examples%20that%20lie%20close%20to%0Athe%20decision%20boundary%20between%20in-distribution%20and%20auxiliary%20outlier%20data.%20Our%0Amethod%20achieves%20a%20new%20state-of-the-art%20in%20OOD%20detection%20with%20outlier%20exposure%2C%0Aimproving%20the%20FPR95%20metric%20from%202.28%20to%200.92%20on%20CIFAR-10%20and%20from%2011.76%20to%207.94%0Aon%20CIFAR-100.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08766v2&entry.124074799=Read"},
{"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision\n  Language Models in Real-World Scenarios with Driving Theory Tests", "author": "Charles Corbi\u00e8re and Simon Roburin and Syrielle Montariol and Antoine Bosselut and Alexandre Alahi", "abstract": "  Large vision-language models (LVLMs) augment language models with visual\nunderstanding, enabling multimodal reasoning. However, due to the modality gap\nbetween textual and visual data, they often face significant challenges, such\nas over-reliance on text priors, hallucinations, and limited capacity for\ncomplex visual reasoning. Existing benchmarks to evaluate visual reasoning in\nLVLMs often rely on schematic or synthetic images and on imprecise\nmachine-generated explanations. To bridge the modality gap, we present\nDrivingVQA, a new benchmark derived from driving theory tests to evaluate\nvisual chain-of-thought reasoning in complex real-world scenarios. It offers\n3,931 expert-crafted multiple-choice problems and interleaved explanations\ngrounded with entities relevant to the reasoning process. We leverage this\ndataset to perform an extensive study of LVLMs' ability to reason about complex\nvisual scenarios. Our experiments reveal that open-source and proprietary LVLMs\nstruggle with visual chain-of-thought reasoning under zero-shot settings. We\ninvestigate training strategies that leverage relevant entities to improve\nvisual reasoning. Notably, we observe a performance boost of up to 7\\% when\nreasoning over image tokens of cropped regions tied to these entities.\n", "link": "http://arxiv.org/abs/2501.04671v1", "date": "2025-01-08", "relevancy": 2.5273, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRIVINGVQA%3A%20Analyzing%20Visual%20Chain-of-Thought%20Reasoning%20of%20Vision%0A%20%20Language%20Models%20in%20Real-World%20Scenarios%20with%20Driving%20Theory%20Tests&body=Title%3A%20DRIVINGVQA%3A%20Analyzing%20Visual%20Chain-of-Thought%20Reasoning%20of%20Vision%0A%20%20Language%20Models%20in%20Real-World%20Scenarios%20with%20Driving%20Theory%20Tests%0AAuthor%3A%20Charles%20Corbi%C3%A8re%20and%20Simon%20Roburin%20and%20Syrielle%20Montariol%20and%20Antoine%20Bosselut%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20augment%20language%20models%20with%20visual%0Aunderstanding%2C%20enabling%20multimodal%20reasoning.%20However%2C%20due%20to%20the%20modality%20gap%0Abetween%20textual%20and%20visual%20data%2C%20they%20often%20face%20significant%20challenges%2C%20such%0Aas%20over-reliance%20on%20text%20priors%2C%20hallucinations%2C%20and%20limited%20capacity%20for%0Acomplex%20visual%20reasoning.%20Existing%20benchmarks%20to%20evaluate%20visual%20reasoning%20in%0ALVLMs%20often%20rely%20on%20schematic%20or%20synthetic%20images%20and%20on%20imprecise%0Amachine-generated%20explanations.%20To%20bridge%20the%20modality%20gap%2C%20we%20present%0ADrivingVQA%2C%20a%20new%20benchmark%20derived%20from%20driving%20theory%20tests%20to%20evaluate%0Avisual%20chain-of-thought%20reasoning%20in%20complex%20real-world%20scenarios.%20It%20offers%0A3%2C931%20expert-crafted%20multiple-choice%20problems%20and%20interleaved%20explanations%0Agrounded%20with%20entities%20relevant%20to%20the%20reasoning%20process.%20We%20leverage%20this%0Adataset%20to%20perform%20an%20extensive%20study%20of%20LVLMs%27%20ability%20to%20reason%20about%20complex%0Avisual%20scenarios.%20Our%20experiments%20reveal%20that%20open-source%20and%20proprietary%20LVLMs%0Astruggle%20with%20visual%20chain-of-thought%20reasoning%20under%20zero-shot%20settings.%20We%0Ainvestigate%20training%20strategies%20that%20leverage%20relevant%20entities%20to%20improve%0Avisual%20reasoning.%20Notably%2C%20we%20observe%20a%20performance%20boost%20of%20up%20to%207%5C%25%20when%0Areasoning%20over%20image%20tokens%20of%20cropped%20regions%20tied%20to%20these%20entities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRIVINGVQA%253A%2520Analyzing%2520Visual%2520Chain-of-Thought%2520Reasoning%2520of%2520Vision%250A%2520%2520Language%2520Models%2520in%2520Real-World%2520Scenarios%2520with%2520Driving%2520Theory%2520Tests%26entry.906535625%3DCharles%2520Corbi%25C3%25A8re%2520and%2520Simon%2520Roburin%2520and%2520Syrielle%2520Montariol%2520and%2520Antoine%2520Bosselut%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520augment%2520language%2520models%2520with%2520visual%250Aunderstanding%252C%2520enabling%2520multimodal%2520reasoning.%2520However%252C%2520due%2520to%2520the%2520modality%2520gap%250Abetween%2520textual%2520and%2520visual%2520data%252C%2520they%2520often%2520face%2520significant%2520challenges%252C%2520such%250Aas%2520over-reliance%2520on%2520text%2520priors%252C%2520hallucinations%252C%2520and%2520limited%2520capacity%2520for%250Acomplex%2520visual%2520reasoning.%2520Existing%2520benchmarks%2520to%2520evaluate%2520visual%2520reasoning%2520in%250ALVLMs%2520often%2520rely%2520on%2520schematic%2520or%2520synthetic%2520images%2520and%2520on%2520imprecise%250Amachine-generated%2520explanations.%2520To%2520bridge%2520the%2520modality%2520gap%252C%2520we%2520present%250ADrivingVQA%252C%2520a%2520new%2520benchmark%2520derived%2520from%2520driving%2520theory%2520tests%2520to%2520evaluate%250Avisual%2520chain-of-thought%2520reasoning%2520in%2520complex%2520real-world%2520scenarios.%2520It%2520offers%250A3%252C931%2520expert-crafted%2520multiple-choice%2520problems%2520and%2520interleaved%2520explanations%250Agrounded%2520with%2520entities%2520relevant%2520to%2520the%2520reasoning%2520process.%2520We%2520leverage%2520this%250Adataset%2520to%2520perform%2520an%2520extensive%2520study%2520of%2520LVLMs%2527%2520ability%2520to%2520reason%2520about%2520complex%250Avisual%2520scenarios.%2520Our%2520experiments%2520reveal%2520that%2520open-source%2520and%2520proprietary%2520LVLMs%250Astruggle%2520with%2520visual%2520chain-of-thought%2520reasoning%2520under%2520zero-shot%2520settings.%2520We%250Ainvestigate%2520training%2520strategies%2520that%2520leverage%2520relevant%2520entities%2520to%2520improve%250Avisual%2520reasoning.%2520Notably%252C%2520we%2520observe%2520a%2520performance%2520boost%2520of%2520up%2520to%25207%255C%2525%2520when%250Areasoning%2520over%2520image%2520tokens%2520of%2520cropped%2520regions%2520tied%2520to%2520these%2520entities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRIVINGVQA%3A%20Analyzing%20Visual%20Chain-of-Thought%20Reasoning%20of%20Vision%0A%20%20Language%20Models%20in%20Real-World%20Scenarios%20with%20Driving%20Theory%20Tests&entry.906535625=Charles%20Corbi%C3%A8re%20and%20Simon%20Roburin%20and%20Syrielle%20Montariol%20and%20Antoine%20Bosselut%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20augment%20language%20models%20with%20visual%0Aunderstanding%2C%20enabling%20multimodal%20reasoning.%20However%2C%20due%20to%20the%20modality%20gap%0Abetween%20textual%20and%20visual%20data%2C%20they%20often%20face%20significant%20challenges%2C%20such%0Aas%20over-reliance%20on%20text%20priors%2C%20hallucinations%2C%20and%20limited%20capacity%20for%0Acomplex%20visual%20reasoning.%20Existing%20benchmarks%20to%20evaluate%20visual%20reasoning%20in%0ALVLMs%20often%20rely%20on%20schematic%20or%20synthetic%20images%20and%20on%20imprecise%0Amachine-generated%20explanations.%20To%20bridge%20the%20modality%20gap%2C%20we%20present%0ADrivingVQA%2C%20a%20new%20benchmark%20derived%20from%20driving%20theory%20tests%20to%20evaluate%0Avisual%20chain-of-thought%20reasoning%20in%20complex%20real-world%20scenarios.%20It%20offers%0A3%2C931%20expert-crafted%20multiple-choice%20problems%20and%20interleaved%20explanations%0Agrounded%20with%20entities%20relevant%20to%20the%20reasoning%20process.%20We%20leverage%20this%0Adataset%20to%20perform%20an%20extensive%20study%20of%20LVLMs%27%20ability%20to%20reason%20about%20complex%0Avisual%20scenarios.%20Our%20experiments%20reveal%20that%20open-source%20and%20proprietary%20LVLMs%0Astruggle%20with%20visual%20chain-of-thought%20reasoning%20under%20zero-shot%20settings.%20We%0Ainvestigate%20training%20strategies%20that%20leverage%20relevant%20entities%20to%20improve%0Avisual%20reasoning.%20Notably%2C%20we%20observe%20a%20performance%20boost%20of%20up%20to%207%5C%25%20when%0Areasoning%20over%20image%20tokens%20of%20cropped%20regions%20tied%20to%20these%20entities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04671v1&entry.124074799=Read"},
{"title": "Large-Scale Spectral Graph Neural Networks via Laplacian Sparsification:\n  Technical Report", "author": "Haipeng Ding and Zhewei Wei and Yuhang Ye", "abstract": "  Graph Neural Networks (GNNs) play a pivotal role in graph-based tasks for\ntheir proficiency in representation learning. Among the various GNN methods,\nspectral GNNs employing polynomial filters have shown promising performance on\ntasks involving both homophilous and heterophilous graph structures. However,\nThe scalability of spectral GNNs on large graphs is limited because they learn\nthe polynomial coefficients through multiple forward propagation executions\nduring forward propagation. Existing works have attempted to scale up spectral\nGNNs by eliminating the linear layers on the input node features, a change that\ncan disrupt end-to-end training, potentially impact performance, and become\nimpractical with high-dimensional input features. To address the above\nchallenges, we propose \"Spectral Graph Neural Networks with Laplacian\nSparsification (SGNN-LS)\", a novel graph spectral sparsification method to\napproximate the propagation patterns of spectral GNNs. We prove that our\nproposed method generates Laplacian sparsifiers that can approximate both fixed\nand learnable polynomial filters with theoretical guarantees. Our method allows\nthe application of linear layers on the input node features, enabling\nend-to-end training as well as the handling of raw text features. We conduct an\nextensive experimental analysis on datasets spanning various graph scales and\nproperties to demonstrate the superior efficiency and effectiveness of our\nmethod. The results show that our method yields superior results in comparison\nwith the corresponding approximated base models, especially on dataset\nOgbn-papers100M(111M nodes, 1.6B edges) and MAG-scholar-C (2.8M features).\n", "link": "http://arxiv.org/abs/2501.04570v1", "date": "2025-01-08", "relevancy": 2.5235, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5409}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.493}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Spectral%20Graph%20Neural%20Networks%20via%20Laplacian%20Sparsification%3A%0A%20%20Technical%20Report&body=Title%3A%20Large-Scale%20Spectral%20Graph%20Neural%20Networks%20via%20Laplacian%20Sparsification%3A%0A%20%20Technical%20Report%0AAuthor%3A%20Haipeng%20Ding%20and%20Zhewei%20Wei%20and%20Yuhang%20Ye%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20play%20a%20pivotal%20role%20in%20graph-based%20tasks%20for%0Atheir%20proficiency%20in%20representation%20learning.%20Among%20the%20various%20GNN%20methods%2C%0Aspectral%20GNNs%20employing%20polynomial%20filters%20have%20shown%20promising%20performance%20on%0Atasks%20involving%20both%20homophilous%20and%20heterophilous%20graph%20structures.%20However%2C%0AThe%20scalability%20of%20spectral%20GNNs%20on%20large%20graphs%20is%20limited%20because%20they%20learn%0Athe%20polynomial%20coefficients%20through%20multiple%20forward%20propagation%20executions%0Aduring%20forward%20propagation.%20Existing%20works%20have%20attempted%20to%20scale%20up%20spectral%0AGNNs%20by%20eliminating%20the%20linear%20layers%20on%20the%20input%20node%20features%2C%20a%20change%20that%0Acan%20disrupt%20end-to-end%20training%2C%20potentially%20impact%20performance%2C%20and%20become%0Aimpractical%20with%20high-dimensional%20input%20features.%20To%20address%20the%20above%0Achallenges%2C%20we%20propose%20%22Spectral%20Graph%20Neural%20Networks%20with%20Laplacian%0ASparsification%20%28SGNN-LS%29%22%2C%20a%20novel%20graph%20spectral%20sparsification%20method%20to%0Aapproximate%20the%20propagation%20patterns%20of%20spectral%20GNNs.%20We%20prove%20that%20our%0Aproposed%20method%20generates%20Laplacian%20sparsifiers%20that%20can%20approximate%20both%20fixed%0Aand%20learnable%20polynomial%20filters%20with%20theoretical%20guarantees.%20Our%20method%20allows%0Athe%20application%20of%20linear%20layers%20on%20the%20input%20node%20features%2C%20enabling%0Aend-to-end%20training%20as%20well%20as%20the%20handling%20of%20raw%20text%20features.%20We%20conduct%20an%0Aextensive%20experimental%20analysis%20on%20datasets%20spanning%20various%20graph%20scales%20and%0Aproperties%20to%20demonstrate%20the%20superior%20efficiency%20and%20effectiveness%20of%20our%0Amethod.%20The%20results%20show%20that%20our%20method%20yields%20superior%20results%20in%20comparison%0Awith%20the%20corresponding%20approximated%20base%20models%2C%20especially%20on%20dataset%0AOgbn-papers100M%28111M%20nodes%2C%201.6B%20edges%29%20and%20MAG-scholar-C%20%282.8M%20features%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Scale%2520Spectral%2520Graph%2520Neural%2520Networks%2520via%2520Laplacian%2520Sparsification%253A%250A%2520%2520Technical%2520Report%26entry.906535625%3DHaipeng%2520Ding%2520and%2520Zhewei%2520Wei%2520and%2520Yuhang%2520Ye%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520play%2520a%2520pivotal%2520role%2520in%2520graph-based%2520tasks%2520for%250Atheir%2520proficiency%2520in%2520representation%2520learning.%2520Among%2520the%2520various%2520GNN%2520methods%252C%250Aspectral%2520GNNs%2520employing%2520polynomial%2520filters%2520have%2520shown%2520promising%2520performance%2520on%250Atasks%2520involving%2520both%2520homophilous%2520and%2520heterophilous%2520graph%2520structures.%2520However%252C%250AThe%2520scalability%2520of%2520spectral%2520GNNs%2520on%2520large%2520graphs%2520is%2520limited%2520because%2520they%2520learn%250Athe%2520polynomial%2520coefficients%2520through%2520multiple%2520forward%2520propagation%2520executions%250Aduring%2520forward%2520propagation.%2520Existing%2520works%2520have%2520attempted%2520to%2520scale%2520up%2520spectral%250AGNNs%2520by%2520eliminating%2520the%2520linear%2520layers%2520on%2520the%2520input%2520node%2520features%252C%2520a%2520change%2520that%250Acan%2520disrupt%2520end-to-end%2520training%252C%2520potentially%2520impact%2520performance%252C%2520and%2520become%250Aimpractical%2520with%2520high-dimensional%2520input%2520features.%2520To%2520address%2520the%2520above%250Achallenges%252C%2520we%2520propose%2520%2522Spectral%2520Graph%2520Neural%2520Networks%2520with%2520Laplacian%250ASparsification%2520%2528SGNN-LS%2529%2522%252C%2520a%2520novel%2520graph%2520spectral%2520sparsification%2520method%2520to%250Aapproximate%2520the%2520propagation%2520patterns%2520of%2520spectral%2520GNNs.%2520We%2520prove%2520that%2520our%250Aproposed%2520method%2520generates%2520Laplacian%2520sparsifiers%2520that%2520can%2520approximate%2520both%2520fixed%250Aand%2520learnable%2520polynomial%2520filters%2520with%2520theoretical%2520guarantees.%2520Our%2520method%2520allows%250Athe%2520application%2520of%2520linear%2520layers%2520on%2520the%2520input%2520node%2520features%252C%2520enabling%250Aend-to-end%2520training%2520as%2520well%2520as%2520the%2520handling%2520of%2520raw%2520text%2520features.%2520We%2520conduct%2520an%250Aextensive%2520experimental%2520analysis%2520on%2520datasets%2520spanning%2520various%2520graph%2520scales%2520and%250Aproperties%2520to%2520demonstrate%2520the%2520superior%2520efficiency%2520and%2520effectiveness%2520of%2520our%250Amethod.%2520The%2520results%2520show%2520that%2520our%2520method%2520yields%2520superior%2520results%2520in%2520comparison%250Awith%2520the%2520corresponding%2520approximated%2520base%2520models%252C%2520especially%2520on%2520dataset%250AOgbn-papers100M%2528111M%2520nodes%252C%25201.6B%2520edges%2529%2520and%2520MAG-scholar-C%2520%25282.8M%2520features%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Spectral%20Graph%20Neural%20Networks%20via%20Laplacian%20Sparsification%3A%0A%20%20Technical%20Report&entry.906535625=Haipeng%20Ding%20and%20Zhewei%20Wei%20and%20Yuhang%20Ye&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20play%20a%20pivotal%20role%20in%20graph-based%20tasks%20for%0Atheir%20proficiency%20in%20representation%20learning.%20Among%20the%20various%20GNN%20methods%2C%0Aspectral%20GNNs%20employing%20polynomial%20filters%20have%20shown%20promising%20performance%20on%0Atasks%20involving%20both%20homophilous%20and%20heterophilous%20graph%20structures.%20However%2C%0AThe%20scalability%20of%20spectral%20GNNs%20on%20large%20graphs%20is%20limited%20because%20they%20learn%0Athe%20polynomial%20coefficients%20through%20multiple%20forward%20propagation%20executions%0Aduring%20forward%20propagation.%20Existing%20works%20have%20attempted%20to%20scale%20up%20spectral%0AGNNs%20by%20eliminating%20the%20linear%20layers%20on%20the%20input%20node%20features%2C%20a%20change%20that%0Acan%20disrupt%20end-to-end%20training%2C%20potentially%20impact%20performance%2C%20and%20become%0Aimpractical%20with%20high-dimensional%20input%20features.%20To%20address%20the%20above%0Achallenges%2C%20we%20propose%20%22Spectral%20Graph%20Neural%20Networks%20with%20Laplacian%0ASparsification%20%28SGNN-LS%29%22%2C%20a%20novel%20graph%20spectral%20sparsification%20method%20to%0Aapproximate%20the%20propagation%20patterns%20of%20spectral%20GNNs.%20We%20prove%20that%20our%0Aproposed%20method%20generates%20Laplacian%20sparsifiers%20that%20can%20approximate%20both%20fixed%0Aand%20learnable%20polynomial%20filters%20with%20theoretical%20guarantees.%20Our%20method%20allows%0Athe%20application%20of%20linear%20layers%20on%20the%20input%20node%20features%2C%20enabling%0Aend-to-end%20training%20as%20well%20as%20the%20handling%20of%20raw%20text%20features.%20We%20conduct%20an%0Aextensive%20experimental%20analysis%20on%20datasets%20spanning%20various%20graph%20scales%20and%0Aproperties%20to%20demonstrate%20the%20superior%20efficiency%20and%20effectiveness%20of%20our%0Amethod.%20The%20results%20show%20that%20our%20method%20yields%20superior%20results%20in%20comparison%0Awith%20the%20corresponding%20approximated%20base%20models%2C%20especially%20on%20dataset%0AOgbn-papers100M%28111M%20nodes%2C%201.6B%20edges%29%20and%20MAG-scholar-C%20%282.8M%20features%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04570v1&entry.124074799=Read"},
{"title": "Rapid Automated Mapping of Clouds on Titan With Instance Segmentation", "author": "Zachary Yahn and Douglas M Trent and Ethan Duncan and Beno\u00eet Seignovert and John Santerre and Conor Nixon", "abstract": "  Despite widespread adoption of deep learning models to address a variety of\ncomputer vision tasks, planetary science has yet to see extensive utilization\nof such tools to address its unique problems. On Titan, the largest moon of\nSaturn, tracking seasonal trends and weather patterns of clouds provides\ncrucial insights into one of the most complex climates in the Solar System, yet\nmuch of the available image data are still analyzed in a conventional way. In\nthis work, we apply a Mask R-CNN trained via transfer learning to perform\ninstance segmentation of clouds in Titan images acquired by the Cassini\nspacecraft - a previously unexplored approach to a big data problem in\nplanetary science. We demonstrate that an automated technique can provide\nquantitative measures for clouds, such as areas and centroids, that may\notherwise be prohibitively time-intensive to produce by human mapping.\nFurthermore, despite Titan specific challenges, our approach yields accuracy\ncomparable to contemporary cloud identification studies on Earth and other\nworlds. We compare the efficiencies of human-driven versus algorithmic\napproaches, showing that transfer learning provides speed-ups that may open new\nhorizons for data investigation for Titan. Moreover, we suggest that such\napproaches have broad potential for application to similar problems in\nplanetary science where they are currently under-utilized. Future planned\nmissions to the planets and remote sensing initiatives for the Earth promise to\nprovide a deluge of image data in the coming years that will benefit strongly\nfrom leveraging machine learning approaches to perform the analysis.\n", "link": "http://arxiv.org/abs/2501.04459v1", "date": "2025-01-08", "relevancy": 2.523, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Automated%20Mapping%20of%20Clouds%20on%20Titan%20With%20Instance%20Segmentation&body=Title%3A%20Rapid%20Automated%20Mapping%20of%20Clouds%20on%20Titan%20With%20Instance%20Segmentation%0AAuthor%3A%20Zachary%20Yahn%20and%20Douglas%20M%20Trent%20and%20Ethan%20Duncan%20and%20Beno%C3%AEt%20Seignovert%20and%20John%20Santerre%20and%20Conor%20Nixon%0AAbstract%3A%20%20%20Despite%20widespread%20adoption%20of%20deep%20learning%20models%20to%20address%20a%20variety%20of%0Acomputer%20vision%20tasks%2C%20planetary%20science%20has%20yet%20to%20see%20extensive%20utilization%0Aof%20such%20tools%20to%20address%20its%20unique%20problems.%20On%20Titan%2C%20the%20largest%20moon%20of%0ASaturn%2C%20tracking%20seasonal%20trends%20and%20weather%20patterns%20of%20clouds%20provides%0Acrucial%20insights%20into%20one%20of%20the%20most%20complex%20climates%20in%20the%20Solar%20System%2C%20yet%0Amuch%20of%20the%20available%20image%20data%20are%20still%20analyzed%20in%20a%20conventional%20way.%20In%0Athis%20work%2C%20we%20apply%20a%20Mask%20R-CNN%20trained%20via%20transfer%20learning%20to%20perform%0Ainstance%20segmentation%20of%20clouds%20in%20Titan%20images%20acquired%20by%20the%20Cassini%0Aspacecraft%20-%20a%20previously%20unexplored%20approach%20to%20a%20big%20data%20problem%20in%0Aplanetary%20science.%20We%20demonstrate%20that%20an%20automated%20technique%20can%20provide%0Aquantitative%20measures%20for%20clouds%2C%20such%20as%20areas%20and%20centroids%2C%20that%20may%0Aotherwise%20be%20prohibitively%20time-intensive%20to%20produce%20by%20human%20mapping.%0AFurthermore%2C%20despite%20Titan%20specific%20challenges%2C%20our%20approach%20yields%20accuracy%0Acomparable%20to%20contemporary%20cloud%20identification%20studies%20on%20Earth%20and%20other%0Aworlds.%20We%20compare%20the%20efficiencies%20of%20human-driven%20versus%20algorithmic%0Aapproaches%2C%20showing%20that%20transfer%20learning%20provides%20speed-ups%20that%20may%20open%20new%0Ahorizons%20for%20data%20investigation%20for%20Titan.%20Moreover%2C%20we%20suggest%20that%20such%0Aapproaches%20have%20broad%20potential%20for%20application%20to%20similar%20problems%20in%0Aplanetary%20science%20where%20they%20are%20currently%20under-utilized.%20Future%20planned%0Amissions%20to%20the%20planets%20and%20remote%20sensing%20initiatives%20for%20the%20Earth%20promise%20to%0Aprovide%20a%20deluge%20of%20image%20data%20in%20the%20coming%20years%20that%20will%20benefit%20strongly%0Afrom%20leveraging%20machine%20learning%20approaches%20to%20perform%20the%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Automated%2520Mapping%2520of%2520Clouds%2520on%2520Titan%2520With%2520Instance%2520Segmentation%26entry.906535625%3DZachary%2520Yahn%2520and%2520Douglas%2520M%2520Trent%2520and%2520Ethan%2520Duncan%2520and%2520Beno%25C3%25AEt%2520Seignovert%2520and%2520John%2520Santerre%2520and%2520Conor%2520Nixon%26entry.1292438233%3D%2520%2520Despite%2520widespread%2520adoption%2520of%2520deep%2520learning%2520models%2520to%2520address%2520a%2520variety%2520of%250Acomputer%2520vision%2520tasks%252C%2520planetary%2520science%2520has%2520yet%2520to%2520see%2520extensive%2520utilization%250Aof%2520such%2520tools%2520to%2520address%2520its%2520unique%2520problems.%2520On%2520Titan%252C%2520the%2520largest%2520moon%2520of%250ASaturn%252C%2520tracking%2520seasonal%2520trends%2520and%2520weather%2520patterns%2520of%2520clouds%2520provides%250Acrucial%2520insights%2520into%2520one%2520of%2520the%2520most%2520complex%2520climates%2520in%2520the%2520Solar%2520System%252C%2520yet%250Amuch%2520of%2520the%2520available%2520image%2520data%2520are%2520still%2520analyzed%2520in%2520a%2520conventional%2520way.%2520In%250Athis%2520work%252C%2520we%2520apply%2520a%2520Mask%2520R-CNN%2520trained%2520via%2520transfer%2520learning%2520to%2520perform%250Ainstance%2520segmentation%2520of%2520clouds%2520in%2520Titan%2520images%2520acquired%2520by%2520the%2520Cassini%250Aspacecraft%2520-%2520a%2520previously%2520unexplored%2520approach%2520to%2520a%2520big%2520data%2520problem%2520in%250Aplanetary%2520science.%2520We%2520demonstrate%2520that%2520an%2520automated%2520technique%2520can%2520provide%250Aquantitative%2520measures%2520for%2520clouds%252C%2520such%2520as%2520areas%2520and%2520centroids%252C%2520that%2520may%250Aotherwise%2520be%2520prohibitively%2520time-intensive%2520to%2520produce%2520by%2520human%2520mapping.%250AFurthermore%252C%2520despite%2520Titan%2520specific%2520challenges%252C%2520our%2520approach%2520yields%2520accuracy%250Acomparable%2520to%2520contemporary%2520cloud%2520identification%2520studies%2520on%2520Earth%2520and%2520other%250Aworlds.%2520We%2520compare%2520the%2520efficiencies%2520of%2520human-driven%2520versus%2520algorithmic%250Aapproaches%252C%2520showing%2520that%2520transfer%2520learning%2520provides%2520speed-ups%2520that%2520may%2520open%2520new%250Ahorizons%2520for%2520data%2520investigation%2520for%2520Titan.%2520Moreover%252C%2520we%2520suggest%2520that%2520such%250Aapproaches%2520have%2520broad%2520potential%2520for%2520application%2520to%2520similar%2520problems%2520in%250Aplanetary%2520science%2520where%2520they%2520are%2520currently%2520under-utilized.%2520Future%2520planned%250Amissions%2520to%2520the%2520planets%2520and%2520remote%2520sensing%2520initiatives%2520for%2520the%2520Earth%2520promise%2520to%250Aprovide%2520a%2520deluge%2520of%2520image%2520data%2520in%2520the%2520coming%2520years%2520that%2520will%2520benefit%2520strongly%250Afrom%2520leveraging%2520machine%2520learning%2520approaches%2520to%2520perform%2520the%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Automated%20Mapping%20of%20Clouds%20on%20Titan%20With%20Instance%20Segmentation&entry.906535625=Zachary%20Yahn%20and%20Douglas%20M%20Trent%20and%20Ethan%20Duncan%20and%20Beno%C3%AEt%20Seignovert%20and%20John%20Santerre%20and%20Conor%20Nixon&entry.1292438233=%20%20Despite%20widespread%20adoption%20of%20deep%20learning%20models%20to%20address%20a%20variety%20of%0Acomputer%20vision%20tasks%2C%20planetary%20science%20has%20yet%20to%20see%20extensive%20utilization%0Aof%20such%20tools%20to%20address%20its%20unique%20problems.%20On%20Titan%2C%20the%20largest%20moon%20of%0ASaturn%2C%20tracking%20seasonal%20trends%20and%20weather%20patterns%20of%20clouds%20provides%0Acrucial%20insights%20into%20one%20of%20the%20most%20complex%20climates%20in%20the%20Solar%20System%2C%20yet%0Amuch%20of%20the%20available%20image%20data%20are%20still%20analyzed%20in%20a%20conventional%20way.%20In%0Athis%20work%2C%20we%20apply%20a%20Mask%20R-CNN%20trained%20via%20transfer%20learning%20to%20perform%0Ainstance%20segmentation%20of%20clouds%20in%20Titan%20images%20acquired%20by%20the%20Cassini%0Aspacecraft%20-%20a%20previously%20unexplored%20approach%20to%20a%20big%20data%20problem%20in%0Aplanetary%20science.%20We%20demonstrate%20that%20an%20automated%20technique%20can%20provide%0Aquantitative%20measures%20for%20clouds%2C%20such%20as%20areas%20and%20centroids%2C%20that%20may%0Aotherwise%20be%20prohibitively%20time-intensive%20to%20produce%20by%20human%20mapping.%0AFurthermore%2C%20despite%20Titan%20specific%20challenges%2C%20our%20approach%20yields%20accuracy%0Acomparable%20to%20contemporary%20cloud%20identification%20studies%20on%20Earth%20and%20other%0Aworlds.%20We%20compare%20the%20efficiencies%20of%20human-driven%20versus%20algorithmic%0Aapproaches%2C%20showing%20that%20transfer%20learning%20provides%20speed-ups%20that%20may%20open%20new%0Ahorizons%20for%20data%20investigation%20for%20Titan.%20Moreover%2C%20we%20suggest%20that%20such%0Aapproaches%20have%20broad%20potential%20for%20application%20to%20similar%20problems%20in%0Aplanetary%20science%20where%20they%20are%20currently%20under-utilized.%20Future%20planned%0Amissions%20to%20the%20planets%20and%20remote%20sensing%20initiatives%20for%20the%20Earth%20promise%20to%0Aprovide%20a%20deluge%20of%20image%20data%20in%20the%20coming%20years%20that%20will%20benefit%20strongly%0Afrom%20leveraging%20machine%20learning%20approaches%20to%20perform%20the%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04459v1&entry.124074799=Read"},
{"title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation", "author": "Yaoxiang Wang and Haoling Li and Xin Zhang and Jie Wu and Xiao Liu and Wenxiang Hu and Zhongxin Guo and Yangyu Huang and Ying Xin and Yujiu Yang and Jinsong Su and Qi Chen and Scarlett Li", "abstract": "  Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.\n", "link": "http://arxiv.org/abs/2501.04694v1", "date": "2025-01-08", "relevancy": 2.4908, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EpiCoder%3A%20Encompassing%20Diversity%20and%20Complexity%20in%20Code%20Generation&body=Title%3A%20EpiCoder%3A%20Encompassing%20Diversity%20and%20Complexity%20in%20Code%20Generation%0AAuthor%3A%20Yaoxiang%20Wang%20and%20Haoling%20Li%20and%20Xin%20Zhang%20and%20Jie%20Wu%20and%20Xiao%20Liu%20and%20Wenxiang%20Hu%20and%20Zhongxin%20Guo%20and%20Yangyu%20Huang%20and%20Ying%20Xin%20and%20Yujiu%20Yang%20and%20Jinsong%20Su%20and%20Qi%20Chen%20and%20Scarlett%20Li%0AAbstract%3A%20%20%20Effective%20instruction%20tuning%20is%20indispensable%20for%20optimizing%20code%20LLMs%2C%0Aaligning%20model%20behavior%20with%20user%20expectations%20and%20enhancing%20model%20performance%0Ain%20real-world%20applications.%20However%2C%20most%20existing%20methods%20focus%20on%20code%0Asnippets%2C%20which%20are%20limited%20to%20specific%20functionalities%20and%20rigid%20structures%2C%0Arestricting%20the%20complexity%20and%20diversity%20of%20the%20synthesized%20data.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20a%20novel%20feature%20tree-based%20synthesis%20framework%0Ainspired%20by%20Abstract%20Syntax%20Trees%20%28AST%29.%20Unlike%20AST%2C%20which%20captures%20syntactic%0Astructure%20of%20code%2C%20our%20framework%20models%20semantic%20relationships%20between%20code%0Aelements%2C%20enabling%20the%20generation%20of%20more%20nuanced%20and%20diverse%20data.%20The%20feature%0Atree%20is%20constructed%20from%20raw%20data%20and%20refined%20iteratively%20to%20increase%20the%0Aquantity%20and%20diversity%20of%20the%20extracted%20features.%20This%20process%20enables%20the%0Aidentification%20of%20more%20complex%20patterns%20and%20relationships%20within%20the%20code.%20By%0Asampling%20subtrees%20with%20controlled%20depth%20and%20breadth%2C%20our%20framework%20allows%0Aprecise%20adjustments%20to%20the%20complexity%20of%20the%20generated%20code%2C%20supporting%20a%20wide%0Arange%20of%20tasks%20from%20simple%20function-level%20operations%20to%20intricate%20multi-file%0Ascenarios.%20We%20fine-tuned%20widely-used%20base%20models%20to%20create%20the%20EpiCoder%20series%2C%0Aachieving%20state-of-the-art%20performance%20at%20both%20the%20function%20and%20file%20levels%0Aacross%20multiple%20benchmarks.%20Notably%2C%20empirical%20evidence%20indicates%20that%20our%0Aapproach%20shows%20significant%20potential%20in%20synthesizing%20highly%20complex%0Arepository-level%20code%20data.%20Further%20analysis%20elucidates%20the%20merits%20of%20this%0Aapproach%20by%20rigorously%20assessing%20data%20complexity%20and%20diversity%20through%20software%0Aengineering%20principles%20and%20LLM-as-a-judge%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpiCoder%253A%2520Encompassing%2520Diversity%2520and%2520Complexity%2520in%2520Code%2520Generation%26entry.906535625%3DYaoxiang%2520Wang%2520and%2520Haoling%2520Li%2520and%2520Xin%2520Zhang%2520and%2520Jie%2520Wu%2520and%2520Xiao%2520Liu%2520and%2520Wenxiang%2520Hu%2520and%2520Zhongxin%2520Guo%2520and%2520Yangyu%2520Huang%2520and%2520Ying%2520Xin%2520and%2520Yujiu%2520Yang%2520and%2520Jinsong%2520Su%2520and%2520Qi%2520Chen%2520and%2520Scarlett%2520Li%26entry.1292438233%3D%2520%2520Effective%2520instruction%2520tuning%2520is%2520indispensable%2520for%2520optimizing%2520code%2520LLMs%252C%250Aaligning%2520model%2520behavior%2520with%2520user%2520expectations%2520and%2520enhancing%2520model%2520performance%250Ain%2520real-world%2520applications.%2520However%252C%2520most%2520existing%2520methods%2520focus%2520on%2520code%250Asnippets%252C%2520which%2520are%2520limited%2520to%2520specific%2520functionalities%2520and%2520rigid%2520structures%252C%250Arestricting%2520the%2520complexity%2520and%2520diversity%2520of%2520the%2520synthesized%2520data.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520introduce%2520a%2520novel%2520feature%2520tree-based%2520synthesis%2520framework%250Ainspired%2520by%2520Abstract%2520Syntax%2520Trees%2520%2528AST%2529.%2520Unlike%2520AST%252C%2520which%2520captures%2520syntactic%250Astructure%2520of%2520code%252C%2520our%2520framework%2520models%2520semantic%2520relationships%2520between%2520code%250Aelements%252C%2520enabling%2520the%2520generation%2520of%2520more%2520nuanced%2520and%2520diverse%2520data.%2520The%2520feature%250Atree%2520is%2520constructed%2520from%2520raw%2520data%2520and%2520refined%2520iteratively%2520to%2520increase%2520the%250Aquantity%2520and%2520diversity%2520of%2520the%2520extracted%2520features.%2520This%2520process%2520enables%2520the%250Aidentification%2520of%2520more%2520complex%2520patterns%2520and%2520relationships%2520within%2520the%2520code.%2520By%250Asampling%2520subtrees%2520with%2520controlled%2520depth%2520and%2520breadth%252C%2520our%2520framework%2520allows%250Aprecise%2520adjustments%2520to%2520the%2520complexity%2520of%2520the%2520generated%2520code%252C%2520supporting%2520a%2520wide%250Arange%2520of%2520tasks%2520from%2520simple%2520function-level%2520operations%2520to%2520intricate%2520multi-file%250Ascenarios.%2520We%2520fine-tuned%2520widely-used%2520base%2520models%2520to%2520create%2520the%2520EpiCoder%2520series%252C%250Aachieving%2520state-of-the-art%2520performance%2520at%2520both%2520the%2520function%2520and%2520file%2520levels%250Aacross%2520multiple%2520benchmarks.%2520Notably%252C%2520empirical%2520evidence%2520indicates%2520that%2520our%250Aapproach%2520shows%2520significant%2520potential%2520in%2520synthesizing%2520highly%2520complex%250Arepository-level%2520code%2520data.%2520Further%2520analysis%2520elucidates%2520the%2520merits%2520of%2520this%250Aapproach%2520by%2520rigorously%2520assessing%2520data%2520complexity%2520and%2520diversity%2520through%2520software%250Aengineering%2520principles%2520and%2520LLM-as-a-judge%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EpiCoder%3A%20Encompassing%20Diversity%20and%20Complexity%20in%20Code%20Generation&entry.906535625=Yaoxiang%20Wang%20and%20Haoling%20Li%20and%20Xin%20Zhang%20and%20Jie%20Wu%20and%20Xiao%20Liu%20and%20Wenxiang%20Hu%20and%20Zhongxin%20Guo%20and%20Yangyu%20Huang%20and%20Ying%20Xin%20and%20Yujiu%20Yang%20and%20Jinsong%20Su%20and%20Qi%20Chen%20and%20Scarlett%20Li&entry.1292438233=%20%20Effective%20instruction%20tuning%20is%20indispensable%20for%20optimizing%20code%20LLMs%2C%0Aaligning%20model%20behavior%20with%20user%20expectations%20and%20enhancing%20model%20performance%0Ain%20real-world%20applications.%20However%2C%20most%20existing%20methods%20focus%20on%20code%0Asnippets%2C%20which%20are%20limited%20to%20specific%20functionalities%20and%20rigid%20structures%2C%0Arestricting%20the%20complexity%20and%20diversity%20of%20the%20synthesized%20data.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20a%20novel%20feature%20tree-based%20synthesis%20framework%0Ainspired%20by%20Abstract%20Syntax%20Trees%20%28AST%29.%20Unlike%20AST%2C%20which%20captures%20syntactic%0Astructure%20of%20code%2C%20our%20framework%20models%20semantic%20relationships%20between%20code%0Aelements%2C%20enabling%20the%20generation%20of%20more%20nuanced%20and%20diverse%20data.%20The%20feature%0Atree%20is%20constructed%20from%20raw%20data%20and%20refined%20iteratively%20to%20increase%20the%0Aquantity%20and%20diversity%20of%20the%20extracted%20features.%20This%20process%20enables%20the%0Aidentification%20of%20more%20complex%20patterns%20and%20relationships%20within%20the%20code.%20By%0Asampling%20subtrees%20with%20controlled%20depth%20and%20breadth%2C%20our%20framework%20allows%0Aprecise%20adjustments%20to%20the%20complexity%20of%20the%20generated%20code%2C%20supporting%20a%20wide%0Arange%20of%20tasks%20from%20simple%20function-level%20operations%20to%20intricate%20multi-file%0Ascenarios.%20We%20fine-tuned%20widely-used%20base%20models%20to%20create%20the%20EpiCoder%20series%2C%0Aachieving%20state-of-the-art%20performance%20at%20both%20the%20function%20and%20file%20levels%0Aacross%20multiple%20benchmarks.%20Notably%2C%20empirical%20evidence%20indicates%20that%20our%0Aapproach%20shows%20significant%20potential%20in%20synthesizing%20highly%20complex%0Arepository-level%20code%20data.%20Further%20analysis%20elucidates%20the%20merits%20of%20this%0Aapproach%20by%20rigorously%20assessing%20data%20complexity%20and%20diversity%20through%20software%0Aengineering%20principles%20and%20LLM-as-a-judge%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04694v1&entry.124074799=Read"},
{"title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging", "author": "Wei Zhang and Hongcheng Guo and Anjie Le and Jian Yang and Jiaheng Liu and Zhoujun Li", "abstract": "  Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.\n", "link": "http://arxiv.org/abs/2402.18205v4", "date": "2025-01-08", "relevancy": 2.4662, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lemur%3A%20Log%20Parsing%20with%20Entropy%20Sampling%20and%20Chain-of-Thought%20Merging&body=Title%3A%20Lemur%3A%20Log%20Parsing%20with%20Entropy%20Sampling%20and%20Chain-of-Thought%20Merging%0AAuthor%3A%20Wei%20Zhang%20and%20Hongcheng%20Guo%20and%20Anjie%20Le%20and%20Jian%20Yang%20and%20Jiaheng%20Liu%20and%20Zhoujun%20Li%0AAbstract%3A%20%20%20Logs%20produced%20by%20extensive%20software%20systems%20are%20integral%20to%20monitoring%20system%0Abehaviors.%20Advanced%20log%20analysis%20facilitates%20the%20detection%2C%20alerting%2C%20and%0Adiagnosis%20of%20system%20faults.%20Log%20parsing%2C%20which%20entails%20transforming%20raw%20log%0Amessages%20into%20structured%20templates%2C%20constitutes%20a%20critical%20phase%20in%20the%0Aautomation%20of%20log%20analytics.%20Existing%20log%20parsers%20fail%20to%20identify%20the%20correct%0Atemplates%20due%20to%20reliance%20on%20human-made%20rules.%20Besides%2C%20These%20methods%20focus%20on%0Astatistical%20features%20while%20ignoring%20semantic%20information%20in%20log%20messages.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20cutting-edge%20%5Ctextbf%7BL%7Dog%20parsing%0Aframework%20with%20%5Ctextbf%7BE%7Dntropy%20sampling%20and%20Chain-of-Thought%20%5Ctextbf%7BM%7Derging%0A%28Lemur%29.%20Specifically%2C%20to%20discard%20the%20tedious%20manual%20rules.%20We%20propose%20a%20novel%0Asampling%20method%20inspired%20by%20information%20entropy%2C%20which%20efficiently%20clusters%0Atypical%20logs.%20Furthermore%2C%20to%20enhance%20the%20merging%20of%20log%20templates%2C%20we%20design%20a%0Achain-of-thought%20method%20for%20large%20language%20models%20%28LLMs%29.%20LLMs%20exhibit%0Aexceptional%20semantic%20comprehension%2C%20deftly%20distinguishing%20between%20parameters%0Aand%20invariant%20tokens.%20We%20have%20conducted%20experiments%20on%20large-scale%20public%0Adatasets.%20Extensive%20evaluation%20demonstrates%20that%20Lemur%20achieves%20the%0Astate-of-the-art%20performance%20and%20impressive%20efficiency.%20The%20Code%20is%20available%0Aat%20https%3A//github.com/zwpride/lemur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18205v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLemur%253A%2520Log%2520Parsing%2520with%2520Entropy%2520Sampling%2520and%2520Chain-of-Thought%2520Merging%26entry.906535625%3DWei%2520Zhang%2520and%2520Hongcheng%2520Guo%2520and%2520Anjie%2520Le%2520and%2520Jian%2520Yang%2520and%2520Jiaheng%2520Liu%2520and%2520Zhoujun%2520Li%26entry.1292438233%3D%2520%2520Logs%2520produced%2520by%2520extensive%2520software%2520systems%2520are%2520integral%2520to%2520monitoring%2520system%250Abehaviors.%2520Advanced%2520log%2520analysis%2520facilitates%2520the%2520detection%252C%2520alerting%252C%2520and%250Adiagnosis%2520of%2520system%2520faults.%2520Log%2520parsing%252C%2520which%2520entails%2520transforming%2520raw%2520log%250Amessages%2520into%2520structured%2520templates%252C%2520constitutes%2520a%2520critical%2520phase%2520in%2520the%250Aautomation%2520of%2520log%2520analytics.%2520Existing%2520log%2520parsers%2520fail%2520to%2520identify%2520the%2520correct%250Atemplates%2520due%2520to%2520reliance%2520on%2520human-made%2520rules.%2520Besides%252C%2520These%2520methods%2520focus%2520on%250Astatistical%2520features%2520while%2520ignoring%2520semantic%2520information%2520in%2520log%2520messages.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520cutting-edge%2520%255Ctextbf%257BL%257Dog%2520parsing%250Aframework%2520with%2520%255Ctextbf%257BE%257Dntropy%2520sampling%2520and%2520Chain-of-Thought%2520%255Ctextbf%257BM%257Derging%250A%2528Lemur%2529.%2520Specifically%252C%2520to%2520discard%2520the%2520tedious%2520manual%2520rules.%2520We%2520propose%2520a%2520novel%250Asampling%2520method%2520inspired%2520by%2520information%2520entropy%252C%2520which%2520efficiently%2520clusters%250Atypical%2520logs.%2520Furthermore%252C%2520to%2520enhance%2520the%2520merging%2520of%2520log%2520templates%252C%2520we%2520design%2520a%250Achain-of-thought%2520method%2520for%2520large%2520language%2520models%2520%2528LLMs%2529.%2520LLMs%2520exhibit%250Aexceptional%2520semantic%2520comprehension%252C%2520deftly%2520distinguishing%2520between%2520parameters%250Aand%2520invariant%2520tokens.%2520We%2520have%2520conducted%2520experiments%2520on%2520large-scale%2520public%250Adatasets.%2520Extensive%2520evaluation%2520demonstrates%2520that%2520Lemur%2520achieves%2520the%250Astate-of-the-art%2520performance%2520and%2520impressive%2520efficiency.%2520The%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/zwpride/lemur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18205v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lemur%3A%20Log%20Parsing%20with%20Entropy%20Sampling%20and%20Chain-of-Thought%20Merging&entry.906535625=Wei%20Zhang%20and%20Hongcheng%20Guo%20and%20Anjie%20Le%20and%20Jian%20Yang%20and%20Jiaheng%20Liu%20and%20Zhoujun%20Li&entry.1292438233=%20%20Logs%20produced%20by%20extensive%20software%20systems%20are%20integral%20to%20monitoring%20system%0Abehaviors.%20Advanced%20log%20analysis%20facilitates%20the%20detection%2C%20alerting%2C%20and%0Adiagnosis%20of%20system%20faults.%20Log%20parsing%2C%20which%20entails%20transforming%20raw%20log%0Amessages%20into%20structured%20templates%2C%20constitutes%20a%20critical%20phase%20in%20the%0Aautomation%20of%20log%20analytics.%20Existing%20log%20parsers%20fail%20to%20identify%20the%20correct%0Atemplates%20due%20to%20reliance%20on%20human-made%20rules.%20Besides%2C%20These%20methods%20focus%20on%0Astatistical%20features%20while%20ignoring%20semantic%20information%20in%20log%20messages.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20cutting-edge%20%5Ctextbf%7BL%7Dog%20parsing%0Aframework%20with%20%5Ctextbf%7BE%7Dntropy%20sampling%20and%20Chain-of-Thought%20%5Ctextbf%7BM%7Derging%0A%28Lemur%29.%20Specifically%2C%20to%20discard%20the%20tedious%20manual%20rules.%20We%20propose%20a%20novel%0Asampling%20method%20inspired%20by%20information%20entropy%2C%20which%20efficiently%20clusters%0Atypical%20logs.%20Furthermore%2C%20to%20enhance%20the%20merging%20of%20log%20templates%2C%20we%20design%20a%0Achain-of-thought%20method%20for%20large%20language%20models%20%28LLMs%29.%20LLMs%20exhibit%0Aexceptional%20semantic%20comprehension%2C%20deftly%20distinguishing%20between%20parameters%0Aand%20invariant%20tokens.%20We%20have%20conducted%20experiments%20on%20large-scale%20public%0Adatasets.%20Extensive%20evaluation%20demonstrates%20that%20Lemur%20achieves%20the%0Astate-of-the-art%20performance%20and%20impressive%20efficiency.%20The%20Code%20is%20available%0Aat%20https%3A//github.com/zwpride/lemur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18205v4&entry.124074799=Read"},
{"title": "FrontierNet: Learning Visual Cues to Explore", "author": "Boyang Sun and Hanzhi Chen and Stefan Leutenegger and Cesar Cadena and Marc Pollefeys and Hermann Blum", "abstract": "  Exploration of unknown environments is crucial for autonomous robots; it\nallows them to actively reason and decide on what new data to acquire for tasks\nsuch as mapping, object discovery, and environmental assessment. Existing\nmethods, such as frontier-based methods, rely heavily on 3D map operations,\nwhich are limited by map quality and often overlook valuable context from\nvisual cues. This work aims at leveraging 2D visual cues for efficient\nautonomous exploration, addressing the limitations of extracting goal poses\nfrom a 3D map. We propose a image-only frontier-based exploration system, with\nFrontierNet as a core component developed in this work. FrontierNet is a\nlearning-based model that (i) detects frontiers, and (ii) predicts their\ninformation gain, from posed RGB images enhanced by monocular depth priors. Our\napproach provides an alternative to existing 3D-dependent exploration systems,\nachieving a 16% improvement in early-stage exploration efficiency, as validated\nthrough extensive simulations and real-world experiments.\n", "link": "http://arxiv.org/abs/2501.04597v1", "date": "2025-01-08", "relevancy": 2.4533, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FrontierNet%3A%20Learning%20Visual%20Cues%20to%20Explore&body=Title%3A%20FrontierNet%3A%20Learning%20Visual%20Cues%20to%20Explore%0AAuthor%3A%20Boyang%20Sun%20and%20Hanzhi%20Chen%20and%20Stefan%20Leutenegger%20and%20Cesar%20Cadena%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%0AAbstract%3A%20%20%20Exploration%20of%20unknown%20environments%20is%20crucial%20for%20autonomous%20robots%3B%20it%0Aallows%20them%20to%20actively%20reason%20and%20decide%20on%20what%20new%20data%20to%20acquire%20for%20tasks%0Asuch%20as%20mapping%2C%20object%20discovery%2C%20and%20environmental%20assessment.%20Existing%0Amethods%2C%20such%20as%20frontier-based%20methods%2C%20rely%20heavily%20on%203D%20map%20operations%2C%0Awhich%20are%20limited%20by%20map%20quality%20and%20often%20overlook%20valuable%20context%20from%0Avisual%20cues.%20This%20work%20aims%20at%20leveraging%202D%20visual%20cues%20for%20efficient%0Aautonomous%20exploration%2C%20addressing%20the%20limitations%20of%20extracting%20goal%20poses%0Afrom%20a%203D%20map.%20We%20propose%20a%20image-only%20frontier-based%20exploration%20system%2C%20with%0AFrontierNet%20as%20a%20core%20component%20developed%20in%20this%20work.%20FrontierNet%20is%20a%0Alearning-based%20model%20that%20%28i%29%20detects%20frontiers%2C%20and%20%28ii%29%20predicts%20their%0Ainformation%20gain%2C%20from%20posed%20RGB%20images%20enhanced%20by%20monocular%20depth%20priors.%20Our%0Aapproach%20provides%20an%20alternative%20to%20existing%203D-dependent%20exploration%20systems%2C%0Aachieving%20a%2016%25%20improvement%20in%20early-stage%20exploration%20efficiency%2C%20as%20validated%0Athrough%20extensive%20simulations%20and%20real-world%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrontierNet%253A%2520Learning%2520Visual%2520Cues%2520to%2520Explore%26entry.906535625%3DBoyang%2520Sun%2520and%2520Hanzhi%2520Chen%2520and%2520Stefan%2520Leutenegger%2520and%2520Cesar%2520Cadena%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%26entry.1292438233%3D%2520%2520Exploration%2520of%2520unknown%2520environments%2520is%2520crucial%2520for%2520autonomous%2520robots%253B%2520it%250Aallows%2520them%2520to%2520actively%2520reason%2520and%2520decide%2520on%2520what%2520new%2520data%2520to%2520acquire%2520for%2520tasks%250Asuch%2520as%2520mapping%252C%2520object%2520discovery%252C%2520and%2520environmental%2520assessment.%2520Existing%250Amethods%252C%2520such%2520as%2520frontier-based%2520methods%252C%2520rely%2520heavily%2520on%25203D%2520map%2520operations%252C%250Awhich%2520are%2520limited%2520by%2520map%2520quality%2520and%2520often%2520overlook%2520valuable%2520context%2520from%250Avisual%2520cues.%2520This%2520work%2520aims%2520at%2520leveraging%25202D%2520visual%2520cues%2520for%2520efficient%250Aautonomous%2520exploration%252C%2520addressing%2520the%2520limitations%2520of%2520extracting%2520goal%2520poses%250Afrom%2520a%25203D%2520map.%2520We%2520propose%2520a%2520image-only%2520frontier-based%2520exploration%2520system%252C%2520with%250AFrontierNet%2520as%2520a%2520core%2520component%2520developed%2520in%2520this%2520work.%2520FrontierNet%2520is%2520a%250Alearning-based%2520model%2520that%2520%2528i%2529%2520detects%2520frontiers%252C%2520and%2520%2528ii%2529%2520predicts%2520their%250Ainformation%2520gain%252C%2520from%2520posed%2520RGB%2520images%2520enhanced%2520by%2520monocular%2520depth%2520priors.%2520Our%250Aapproach%2520provides%2520an%2520alternative%2520to%2520existing%25203D-dependent%2520exploration%2520systems%252C%250Aachieving%2520a%252016%2525%2520improvement%2520in%2520early-stage%2520exploration%2520efficiency%252C%2520as%2520validated%250Athrough%2520extensive%2520simulations%2520and%2520real-world%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FrontierNet%3A%20Learning%20Visual%20Cues%20to%20Explore&entry.906535625=Boyang%20Sun%20and%20Hanzhi%20Chen%20and%20Stefan%20Leutenegger%20and%20Cesar%20Cadena%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum&entry.1292438233=%20%20Exploration%20of%20unknown%20environments%20is%20crucial%20for%20autonomous%20robots%3B%20it%0Aallows%20them%20to%20actively%20reason%20and%20decide%20on%20what%20new%20data%20to%20acquire%20for%20tasks%0Asuch%20as%20mapping%2C%20object%20discovery%2C%20and%20environmental%20assessment.%20Existing%0Amethods%2C%20such%20as%20frontier-based%20methods%2C%20rely%20heavily%20on%203D%20map%20operations%2C%0Awhich%20are%20limited%20by%20map%20quality%20and%20often%20overlook%20valuable%20context%20from%0Avisual%20cues.%20This%20work%20aims%20at%20leveraging%202D%20visual%20cues%20for%20efficient%0Aautonomous%20exploration%2C%20addressing%20the%20limitations%20of%20extracting%20goal%20poses%0Afrom%20a%203D%20map.%20We%20propose%20a%20image-only%20frontier-based%20exploration%20system%2C%20with%0AFrontierNet%20as%20a%20core%20component%20developed%20in%20this%20work.%20FrontierNet%20is%20a%0Alearning-based%20model%20that%20%28i%29%20detects%20frontiers%2C%20and%20%28ii%29%20predicts%20their%0Ainformation%20gain%2C%20from%20posed%20RGB%20images%20enhanced%20by%20monocular%20depth%20priors.%20Our%0Aapproach%20provides%20an%20alternative%20to%20existing%203D-dependent%20exploration%20systems%2C%0Aachieving%20a%2016%25%20improvement%20in%20early-stage%20exploration%20efficiency%2C%20as%20validated%0Athrough%20extensive%20simulations%20and%20real-world%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04597v1&entry.124074799=Read"},
{"title": "Towards a Problem-Oriented Domain Adaptation Framework for Machine\n  Learning", "author": "Philipp Spitzer and Dominik Martin and Laurin Eichberger and Niklas K\u00fchl", "abstract": "  Domain adaptation is a sub-field of machine learning that involves\ntransferring knowledge from a source domain to perform the same task in the\ntarget domain. It is a typical challenge in machine learning that arises, e.g.,\nwhen data is obtained from various sources or when using a data basis that\nchanges over time. Recent advances in the field offer promising methods, but it\nis still challenging for researchers and practitioners to determine if domain\nadaptation is suitable for a given problem -- and, subsequently, to select the\nappropriate approach. This article employs design science research to develop a\nproblem-oriented framework for domain adaptation, which is matured in three\nevaluation episodes. We describe a framework that distinguishes between five\ndomain adaptation scenarios, provides recommendations for addressing each\nscenario, and offers guidelines for determining if a problem falls into one of\nthese scenarios. During the multiple evaluation episodes, the framework is\ntested on artificial and real-world datasets and an experimental study\ninvolving 100 participants. The evaluation demonstrates that the framework has\nthe explanatory power to capture any domain adaptation problem effectively. In\nsummary, we provide clear guidance for researchers and practitioners who want\nto employ domain adaptation but lack in-depth knowledge of the possibilities.\n", "link": "http://arxiv.org/abs/2501.04528v1", "date": "2025-01-08", "relevancy": 2.4214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Problem-Oriented%20Domain%20Adaptation%20Framework%20for%20Machine%0A%20%20Learning&body=Title%3A%20Towards%20a%20Problem-Oriented%20Domain%20Adaptation%20Framework%20for%20Machine%0A%20%20Learning%0AAuthor%3A%20Philipp%20Spitzer%20and%20Dominik%20Martin%20and%20Laurin%20Eichberger%20and%20Niklas%20K%C3%BChl%0AAbstract%3A%20%20%20Domain%20adaptation%20is%20a%20sub-field%20of%20machine%20learning%20that%20involves%0Atransferring%20knowledge%20from%20a%20source%20domain%20to%20perform%20the%20same%20task%20in%20the%0Atarget%20domain.%20It%20is%20a%20typical%20challenge%20in%20machine%20learning%20that%20arises%2C%20e.g.%2C%0Awhen%20data%20is%20obtained%20from%20various%20sources%20or%20when%20using%20a%20data%20basis%20that%0Achanges%20over%20time.%20Recent%20advances%20in%20the%20field%20offer%20promising%20methods%2C%20but%20it%0Ais%20still%20challenging%20for%20researchers%20and%20practitioners%20to%20determine%20if%20domain%0Aadaptation%20is%20suitable%20for%20a%20given%20problem%20--%20and%2C%20subsequently%2C%20to%20select%20the%0Aappropriate%20approach.%20This%20article%20employs%20design%20science%20research%20to%20develop%20a%0Aproblem-oriented%20framework%20for%20domain%20adaptation%2C%20which%20is%20matured%20in%20three%0Aevaluation%20episodes.%20We%20describe%20a%20framework%20that%20distinguishes%20between%20five%0Adomain%20adaptation%20scenarios%2C%20provides%20recommendations%20for%20addressing%20each%0Ascenario%2C%20and%20offers%20guidelines%20for%20determining%20if%20a%20problem%20falls%20into%20one%20of%0Athese%20scenarios.%20During%20the%20multiple%20evaluation%20episodes%2C%20the%20framework%20is%0Atested%20on%20artificial%20and%20real-world%20datasets%20and%20an%20experimental%20study%0Ainvolving%20100%20participants.%20The%20evaluation%20demonstrates%20that%20the%20framework%20has%0Athe%20explanatory%20power%20to%20capture%20any%20domain%20adaptation%20problem%20effectively.%20In%0Asummary%2C%20we%20provide%20clear%20guidance%20for%20researchers%20and%20practitioners%20who%20want%0Ato%20employ%20domain%20adaptation%20but%20lack%20in-depth%20knowledge%20of%20the%20possibilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Problem-Oriented%2520Domain%2520Adaptation%2520Framework%2520for%2520Machine%250A%2520%2520Learning%26entry.906535625%3DPhilipp%2520Spitzer%2520and%2520Dominik%2520Martin%2520and%2520Laurin%2520Eichberger%2520and%2520Niklas%2520K%25C3%25BChl%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520is%2520a%2520sub-field%2520of%2520machine%2520learning%2520that%2520involves%250Atransferring%2520knowledge%2520from%2520a%2520source%2520domain%2520to%2520perform%2520the%2520same%2520task%2520in%2520the%250Atarget%2520domain.%2520It%2520is%2520a%2520typical%2520challenge%2520in%2520machine%2520learning%2520that%2520arises%252C%2520e.g.%252C%250Awhen%2520data%2520is%2520obtained%2520from%2520various%2520sources%2520or%2520when%2520using%2520a%2520data%2520basis%2520that%250Achanges%2520over%2520time.%2520Recent%2520advances%2520in%2520the%2520field%2520offer%2520promising%2520methods%252C%2520but%2520it%250Ais%2520still%2520challenging%2520for%2520researchers%2520and%2520practitioners%2520to%2520determine%2520if%2520domain%250Aadaptation%2520is%2520suitable%2520for%2520a%2520given%2520problem%2520--%2520and%252C%2520subsequently%252C%2520to%2520select%2520the%250Aappropriate%2520approach.%2520This%2520article%2520employs%2520design%2520science%2520research%2520to%2520develop%2520a%250Aproblem-oriented%2520framework%2520for%2520domain%2520adaptation%252C%2520which%2520is%2520matured%2520in%2520three%250Aevaluation%2520episodes.%2520We%2520describe%2520a%2520framework%2520that%2520distinguishes%2520between%2520five%250Adomain%2520adaptation%2520scenarios%252C%2520provides%2520recommendations%2520for%2520addressing%2520each%250Ascenario%252C%2520and%2520offers%2520guidelines%2520for%2520determining%2520if%2520a%2520problem%2520falls%2520into%2520one%2520of%250Athese%2520scenarios.%2520During%2520the%2520multiple%2520evaluation%2520episodes%252C%2520the%2520framework%2520is%250Atested%2520on%2520artificial%2520and%2520real-world%2520datasets%2520and%2520an%2520experimental%2520study%250Ainvolving%2520100%2520participants.%2520The%2520evaluation%2520demonstrates%2520that%2520the%2520framework%2520has%250Athe%2520explanatory%2520power%2520to%2520capture%2520any%2520domain%2520adaptation%2520problem%2520effectively.%2520In%250Asummary%252C%2520we%2520provide%2520clear%2520guidance%2520for%2520researchers%2520and%2520practitioners%2520who%2520want%250Ato%2520employ%2520domain%2520adaptation%2520but%2520lack%2520in-depth%2520knowledge%2520of%2520the%2520possibilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Problem-Oriented%20Domain%20Adaptation%20Framework%20for%20Machine%0A%20%20Learning&entry.906535625=Philipp%20Spitzer%20and%20Dominik%20Martin%20and%20Laurin%20Eichberger%20and%20Niklas%20K%C3%BChl&entry.1292438233=%20%20Domain%20adaptation%20is%20a%20sub-field%20of%20machine%20learning%20that%20involves%0Atransferring%20knowledge%20from%20a%20source%20domain%20to%20perform%20the%20same%20task%20in%20the%0Atarget%20domain.%20It%20is%20a%20typical%20challenge%20in%20machine%20learning%20that%20arises%2C%20e.g.%2C%0Awhen%20data%20is%20obtained%20from%20various%20sources%20or%20when%20using%20a%20data%20basis%20that%0Achanges%20over%20time.%20Recent%20advances%20in%20the%20field%20offer%20promising%20methods%2C%20but%20it%0Ais%20still%20challenging%20for%20researchers%20and%20practitioners%20to%20determine%20if%20domain%0Aadaptation%20is%20suitable%20for%20a%20given%20problem%20--%20and%2C%20subsequently%2C%20to%20select%20the%0Aappropriate%20approach.%20This%20article%20employs%20design%20science%20research%20to%20develop%20a%0Aproblem-oriented%20framework%20for%20domain%20adaptation%2C%20which%20is%20matured%20in%20three%0Aevaluation%20episodes.%20We%20describe%20a%20framework%20that%20distinguishes%20between%20five%0Adomain%20adaptation%20scenarios%2C%20provides%20recommendations%20for%20addressing%20each%0Ascenario%2C%20and%20offers%20guidelines%20for%20determining%20if%20a%20problem%20falls%20into%20one%20of%0Athese%20scenarios.%20During%20the%20multiple%20evaluation%20episodes%2C%20the%20framework%20is%0Atested%20on%20artificial%20and%20real-world%20datasets%20and%20an%20experimental%20study%0Ainvolving%20100%20participants.%20The%20evaluation%20demonstrates%20that%20the%20framework%20has%0Athe%20explanatory%20power%20to%20capture%20any%20domain%20adaptation%20problem%20effectively.%20In%0Asummary%2C%20we%20provide%20clear%20guidance%20for%20researchers%20and%20practitioners%20who%20want%0Ato%20employ%20domain%20adaptation%20but%20lack%20in-depth%20knowledge%20of%20the%20possibilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04528v1&entry.124074799=Read"},
{"title": "Identity-Preserving Video Dubbing Using Motion Warping", "author": "Runzhen Liu and Qinjie Lin and Yunfei Liu and Lijian Lin and Ye Zhu and Yu Li and Chuhua Xian and Fa-Ting Hong", "abstract": "  Video dubbing aims to synthesize realistic, lip-synced videos from a\nreference video and a driving audio signal. Although existing methods can\naccurately generate mouth shapes driven by audio, they often fail to preserve\nidentity-specific features, largely because they do not effectively capture the\nnuanced interplay between audio cues and the visual attributes of reference\nidentity . As a result, the generated outputs frequently lack fidelity in\nreproducing the unique textural and structural details of the reference\nidentity. To address these limitations, we propose IPTalker, a novel and robust\nframework for video dubbing that achieves seamless alignment between driving\naudio and reference identity while ensuring both lip-sync accuracy and\nhigh-fidelity identity preservation. At the core of IPTalker is a\ntransformer-based alignment mechanism designed to dynamically capture and model\nthe correspondence between audio features and reference images, thereby\nenabling precise, identity-aware audio-visual integration. Building on this\nalignment, a motion warping strategy further refines the results by spatially\ndeforming reference images to match the target audio-driven configuration. A\ndedicated refinement process then mitigates occlusion artifacts and enhances\nthe preservation of fine-grained textures, such as mouth details and skin\nfeatures. Extensive qualitative and quantitative evaluations demonstrate that\nIPTalker consistently outperforms existing approaches in terms of realism, lip\nsynchronization, and identity retention, establishing a new state of the art\nfor high-quality, identity-consistent video dubbing.\n", "link": "http://arxiv.org/abs/2501.04586v1", "date": "2025-01-08", "relevancy": 2.4182, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6341}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity-Preserving%20Video%20Dubbing%20Using%20Motion%20Warping&body=Title%3A%20Identity-Preserving%20Video%20Dubbing%20Using%20Motion%20Warping%0AAuthor%3A%20Runzhen%20Liu%20and%20Qinjie%20Lin%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Ye%20Zhu%20and%20Yu%20Li%20and%20Chuhua%20Xian%20and%20Fa-Ting%20Hong%0AAbstract%3A%20%20%20Video%20dubbing%20aims%20to%20synthesize%20realistic%2C%20lip-synced%20videos%20from%20a%0Areference%20video%20and%20a%20driving%20audio%20signal.%20Although%20existing%20methods%20can%0Aaccurately%20generate%20mouth%20shapes%20driven%20by%20audio%2C%20they%20often%20fail%20to%20preserve%0Aidentity-specific%20features%2C%20largely%20because%20they%20do%20not%20effectively%20capture%20the%0Anuanced%20interplay%20between%20audio%20cues%20and%20the%20visual%20attributes%20of%20reference%0Aidentity%20.%20As%20a%20result%2C%20the%20generated%20outputs%20frequently%20lack%20fidelity%20in%0Areproducing%20the%20unique%20textural%20and%20structural%20details%20of%20the%20reference%0Aidentity.%20To%20address%20these%20limitations%2C%20we%20propose%20IPTalker%2C%20a%20novel%20and%20robust%0Aframework%20for%20video%20dubbing%20that%20achieves%20seamless%20alignment%20between%20driving%0Aaudio%20and%20reference%20identity%20while%20ensuring%20both%20lip-sync%20accuracy%20and%0Ahigh-fidelity%20identity%20preservation.%20At%20the%20core%20of%20IPTalker%20is%20a%0Atransformer-based%20alignment%20mechanism%20designed%20to%20dynamically%20capture%20and%20model%0Athe%20correspondence%20between%20audio%20features%20and%20reference%20images%2C%20thereby%0Aenabling%20precise%2C%20identity-aware%20audio-visual%20integration.%20Building%20on%20this%0Aalignment%2C%20a%20motion%20warping%20strategy%20further%20refines%20the%20results%20by%20spatially%0Adeforming%20reference%20images%20to%20match%20the%20target%20audio-driven%20configuration.%20A%0Adedicated%20refinement%20process%20then%20mitigates%20occlusion%20artifacts%20and%20enhances%0Athe%20preservation%20of%20fine-grained%20textures%2C%20such%20as%20mouth%20details%20and%20skin%0Afeatures.%20Extensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%0AIPTalker%20consistently%20outperforms%20existing%20approaches%20in%20terms%20of%20realism%2C%20lip%0Asynchronization%2C%20and%20identity%20retention%2C%20establishing%20a%20new%20state%20of%20the%20art%0Afor%20high-quality%2C%20identity-consistent%20video%20dubbing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity-Preserving%2520Video%2520Dubbing%2520Using%2520Motion%2520Warping%26entry.906535625%3DRunzhen%2520Liu%2520and%2520Qinjie%2520Lin%2520and%2520Yunfei%2520Liu%2520and%2520Lijian%2520Lin%2520and%2520Ye%2520Zhu%2520and%2520Yu%2520Li%2520and%2520Chuhua%2520Xian%2520and%2520Fa-Ting%2520Hong%26entry.1292438233%3D%2520%2520Video%2520dubbing%2520aims%2520to%2520synthesize%2520realistic%252C%2520lip-synced%2520videos%2520from%2520a%250Areference%2520video%2520and%2520a%2520driving%2520audio%2520signal.%2520Although%2520existing%2520methods%2520can%250Aaccurately%2520generate%2520mouth%2520shapes%2520driven%2520by%2520audio%252C%2520they%2520often%2520fail%2520to%2520preserve%250Aidentity-specific%2520features%252C%2520largely%2520because%2520they%2520do%2520not%2520effectively%2520capture%2520the%250Anuanced%2520interplay%2520between%2520audio%2520cues%2520and%2520the%2520visual%2520attributes%2520of%2520reference%250Aidentity%2520.%2520As%2520a%2520result%252C%2520the%2520generated%2520outputs%2520frequently%2520lack%2520fidelity%2520in%250Areproducing%2520the%2520unique%2520textural%2520and%2520structural%2520details%2520of%2520the%2520reference%250Aidentity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520IPTalker%252C%2520a%2520novel%2520and%2520robust%250Aframework%2520for%2520video%2520dubbing%2520that%2520achieves%2520seamless%2520alignment%2520between%2520driving%250Aaudio%2520and%2520reference%2520identity%2520while%2520ensuring%2520both%2520lip-sync%2520accuracy%2520and%250Ahigh-fidelity%2520identity%2520preservation.%2520At%2520the%2520core%2520of%2520IPTalker%2520is%2520a%250Atransformer-based%2520alignment%2520mechanism%2520designed%2520to%2520dynamically%2520capture%2520and%2520model%250Athe%2520correspondence%2520between%2520audio%2520features%2520and%2520reference%2520images%252C%2520thereby%250Aenabling%2520precise%252C%2520identity-aware%2520audio-visual%2520integration.%2520Building%2520on%2520this%250Aalignment%252C%2520a%2520motion%2520warping%2520strategy%2520further%2520refines%2520the%2520results%2520by%2520spatially%250Adeforming%2520reference%2520images%2520to%2520match%2520the%2520target%2520audio-driven%2520configuration.%2520A%250Adedicated%2520refinement%2520process%2520then%2520mitigates%2520occlusion%2520artifacts%2520and%2520enhances%250Athe%2520preservation%2520of%2520fine-grained%2520textures%252C%2520such%2520as%2520mouth%2520details%2520and%2520skin%250Afeatures.%2520Extensive%2520qualitative%2520and%2520quantitative%2520evaluations%2520demonstrate%2520that%250AIPTalker%2520consistently%2520outperforms%2520existing%2520approaches%2520in%2520terms%2520of%2520realism%252C%2520lip%250Asynchronization%252C%2520and%2520identity%2520retention%252C%2520establishing%2520a%2520new%2520state%2520of%2520the%2520art%250Afor%2520high-quality%252C%2520identity-consistent%2520video%2520dubbing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity-Preserving%20Video%20Dubbing%20Using%20Motion%20Warping&entry.906535625=Runzhen%20Liu%20and%20Qinjie%20Lin%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Ye%20Zhu%20and%20Yu%20Li%20and%20Chuhua%20Xian%20and%20Fa-Ting%20Hong&entry.1292438233=%20%20Video%20dubbing%20aims%20to%20synthesize%20realistic%2C%20lip-synced%20videos%20from%20a%0Areference%20video%20and%20a%20driving%20audio%20signal.%20Although%20existing%20methods%20can%0Aaccurately%20generate%20mouth%20shapes%20driven%20by%20audio%2C%20they%20often%20fail%20to%20preserve%0Aidentity-specific%20features%2C%20largely%20because%20they%20do%20not%20effectively%20capture%20the%0Anuanced%20interplay%20between%20audio%20cues%20and%20the%20visual%20attributes%20of%20reference%0Aidentity%20.%20As%20a%20result%2C%20the%20generated%20outputs%20frequently%20lack%20fidelity%20in%0Areproducing%20the%20unique%20textural%20and%20structural%20details%20of%20the%20reference%0Aidentity.%20To%20address%20these%20limitations%2C%20we%20propose%20IPTalker%2C%20a%20novel%20and%20robust%0Aframework%20for%20video%20dubbing%20that%20achieves%20seamless%20alignment%20between%20driving%0Aaudio%20and%20reference%20identity%20while%20ensuring%20both%20lip-sync%20accuracy%20and%0Ahigh-fidelity%20identity%20preservation.%20At%20the%20core%20of%20IPTalker%20is%20a%0Atransformer-based%20alignment%20mechanism%20designed%20to%20dynamically%20capture%20and%20model%0Athe%20correspondence%20between%20audio%20features%20and%20reference%20images%2C%20thereby%0Aenabling%20precise%2C%20identity-aware%20audio-visual%20integration.%20Building%20on%20this%0Aalignment%2C%20a%20motion%20warping%20strategy%20further%20refines%20the%20results%20by%20spatially%0Adeforming%20reference%20images%20to%20match%20the%20target%20audio-driven%20configuration.%20A%0Adedicated%20refinement%20process%20then%20mitigates%20occlusion%20artifacts%20and%20enhances%0Athe%20preservation%20of%20fine-grained%20textures%2C%20such%20as%20mouth%20details%20and%20skin%0Afeatures.%20Extensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%0AIPTalker%20consistently%20outperforms%20existing%20approaches%20in%20terms%20of%20realism%2C%20lip%0Asynchronization%2C%20and%20identity%20retention%2C%20establishing%20a%20new%20state%20of%20the%20art%0Afor%20high-quality%2C%20identity-consistent%20video%20dubbing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04586v1&entry.124074799=Read"},
{"title": "Resilient Peer-to-peer Learning based on Adaptive Aggregation", "author": "Chandreyee Bhowmick and Xenofon Koutsoukos", "abstract": "  Collaborative learning in peer-to-peer networks offers the benefits of\ndistributed learning while mitigating the risks associated with single points\nof failure inherent in centralized servers. However, adversarial workers pose\npotential threats by attempting to inject malicious information into the\nnetwork. Thus, ensuring the resilience of peer-to-peer learning emerges as a\npivotal research objective. The challenge is exacerbated in the presence of\nnon-convex loss functions and non-iid data distributions. This paper introduces\na resilient aggregation technique tailored for such scenarios, aimed at\nfostering similarity among peers' learning processes. The aggregation weights\nare determined through an optimization procedure, and use the loss function\ncomputed using the neighbor's models and individual private data, thereby\naddressing concerns regarding data privacy in distributed machine learning.\nTheoretical analysis demonstrates convergence of parameters with non-convex\nloss functions and non-iid data distributions. Empirical evaluations across\nthree distinct machine learning tasks support the claims. The empirical\nfindings, which encompass a range of diverse attack models, also demonstrate\nimproved accuracy when compared to existing methodologies.\n", "link": "http://arxiv.org/abs/2501.04610v1", "date": "2025-01-08", "relevancy": 2.4165, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4981}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resilient%20Peer-to-peer%20Learning%20based%20on%20Adaptive%20Aggregation&body=Title%3A%20Resilient%20Peer-to-peer%20Learning%20based%20on%20Adaptive%20Aggregation%0AAuthor%3A%20Chandreyee%20Bhowmick%20and%20Xenofon%20Koutsoukos%0AAbstract%3A%20%20%20Collaborative%20learning%20in%20peer-to-peer%20networks%20offers%20the%20benefits%20of%0Adistributed%20learning%20while%20mitigating%20the%20risks%20associated%20with%20single%20points%0Aof%20failure%20inherent%20in%20centralized%20servers.%20However%2C%20adversarial%20workers%20pose%0Apotential%20threats%20by%20attempting%20to%20inject%20malicious%20information%20into%20the%0Anetwork.%20Thus%2C%20ensuring%20the%20resilience%20of%20peer-to-peer%20learning%20emerges%20as%20a%0Apivotal%20research%20objective.%20The%20challenge%20is%20exacerbated%20in%20the%20presence%20of%0Anon-convex%20loss%20functions%20and%20non-iid%20data%20distributions.%20This%20paper%20introduces%0Aa%20resilient%20aggregation%20technique%20tailored%20for%20such%20scenarios%2C%20aimed%20at%0Afostering%20similarity%20among%20peers%27%20learning%20processes.%20The%20aggregation%20weights%0Aare%20determined%20through%20an%20optimization%20procedure%2C%20and%20use%20the%20loss%20function%0Acomputed%20using%20the%20neighbor%27s%20models%20and%20individual%20private%20data%2C%20thereby%0Aaddressing%20concerns%20regarding%20data%20privacy%20in%20distributed%20machine%20learning.%0ATheoretical%20analysis%20demonstrates%20convergence%20of%20parameters%20with%20non-convex%0Aloss%20functions%20and%20non-iid%20data%20distributions.%20Empirical%20evaluations%20across%0Athree%20distinct%20machine%20learning%20tasks%20support%20the%20claims.%20The%20empirical%0Afindings%2C%20which%20encompass%20a%20range%20of%20diverse%20attack%20models%2C%20also%20demonstrate%0Aimproved%20accuracy%20when%20compared%20to%20existing%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResilient%2520Peer-to-peer%2520Learning%2520based%2520on%2520Adaptive%2520Aggregation%26entry.906535625%3DChandreyee%2520Bhowmick%2520and%2520Xenofon%2520Koutsoukos%26entry.1292438233%3D%2520%2520Collaborative%2520learning%2520in%2520peer-to-peer%2520networks%2520offers%2520the%2520benefits%2520of%250Adistributed%2520learning%2520while%2520mitigating%2520the%2520risks%2520associated%2520with%2520single%2520points%250Aof%2520failure%2520inherent%2520in%2520centralized%2520servers.%2520However%252C%2520adversarial%2520workers%2520pose%250Apotential%2520threats%2520by%2520attempting%2520to%2520inject%2520malicious%2520information%2520into%2520the%250Anetwork.%2520Thus%252C%2520ensuring%2520the%2520resilience%2520of%2520peer-to-peer%2520learning%2520emerges%2520as%2520a%250Apivotal%2520research%2520objective.%2520The%2520challenge%2520is%2520exacerbated%2520in%2520the%2520presence%2520of%250Anon-convex%2520loss%2520functions%2520and%2520non-iid%2520data%2520distributions.%2520This%2520paper%2520introduces%250Aa%2520resilient%2520aggregation%2520technique%2520tailored%2520for%2520such%2520scenarios%252C%2520aimed%2520at%250Afostering%2520similarity%2520among%2520peers%2527%2520learning%2520processes.%2520The%2520aggregation%2520weights%250Aare%2520determined%2520through%2520an%2520optimization%2520procedure%252C%2520and%2520use%2520the%2520loss%2520function%250Acomputed%2520using%2520the%2520neighbor%2527s%2520models%2520and%2520individual%2520private%2520data%252C%2520thereby%250Aaddressing%2520concerns%2520regarding%2520data%2520privacy%2520in%2520distributed%2520machine%2520learning.%250ATheoretical%2520analysis%2520demonstrates%2520convergence%2520of%2520parameters%2520with%2520non-convex%250Aloss%2520functions%2520and%2520non-iid%2520data%2520distributions.%2520Empirical%2520evaluations%2520across%250Athree%2520distinct%2520machine%2520learning%2520tasks%2520support%2520the%2520claims.%2520The%2520empirical%250Afindings%252C%2520which%2520encompass%2520a%2520range%2520of%2520diverse%2520attack%2520models%252C%2520also%2520demonstrate%250Aimproved%2520accuracy%2520when%2520compared%2520to%2520existing%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resilient%20Peer-to-peer%20Learning%20based%20on%20Adaptive%20Aggregation&entry.906535625=Chandreyee%20Bhowmick%20and%20Xenofon%20Koutsoukos&entry.1292438233=%20%20Collaborative%20learning%20in%20peer-to-peer%20networks%20offers%20the%20benefits%20of%0Adistributed%20learning%20while%20mitigating%20the%20risks%20associated%20with%20single%20points%0Aof%20failure%20inherent%20in%20centralized%20servers.%20However%2C%20adversarial%20workers%20pose%0Apotential%20threats%20by%20attempting%20to%20inject%20malicious%20information%20into%20the%0Anetwork.%20Thus%2C%20ensuring%20the%20resilience%20of%20peer-to-peer%20learning%20emerges%20as%20a%0Apivotal%20research%20objective.%20The%20challenge%20is%20exacerbated%20in%20the%20presence%20of%0Anon-convex%20loss%20functions%20and%20non-iid%20data%20distributions.%20This%20paper%20introduces%0Aa%20resilient%20aggregation%20technique%20tailored%20for%20such%20scenarios%2C%20aimed%20at%0Afostering%20similarity%20among%20peers%27%20learning%20processes.%20The%20aggregation%20weights%0Aare%20determined%20through%20an%20optimization%20procedure%2C%20and%20use%20the%20loss%20function%0Acomputed%20using%20the%20neighbor%27s%20models%20and%20individual%20private%20data%2C%20thereby%0Aaddressing%20concerns%20regarding%20data%20privacy%20in%20distributed%20machine%20learning.%0ATheoretical%20analysis%20demonstrates%20convergence%20of%20parameters%20with%20non-convex%0Aloss%20functions%20and%20non-iid%20data%20distributions.%20Empirical%20evaluations%20across%0Athree%20distinct%20machine%20learning%20tasks%20support%20the%20claims.%20The%20empirical%0Afindings%2C%20which%20encompass%20a%20range%20of%20diverse%20attack%20models%2C%20also%20demonstrate%0Aimproved%20accuracy%20when%20compared%20to%20existing%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04610v1&entry.124074799=Read"},
{"title": "Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence\n  Benchmarks", "author": "Yang Wang and Chenghua Lin", "abstract": "  Recent advancements in natural language processing have highlighted the\nvulnerability of deep learning models to adversarial attacks. While various\ndefence mechanisms have been proposed, there is a lack of comprehensive\nbenchmarks that evaluate these defences across diverse datasets, models, and\ntasks. In this work, we address this gap by presenting an extensive benchmark\nfor textual adversarial defence that significantly expands upon previous work.\nOur benchmark incorporates a wide range of datasets, evaluates state-of-the-art\ndefence mechanisms, and extends the assessment to include critical tasks such\nas single-sentence classification, similarity and paraphrase identification,\nnatural language inference, and commonsense reasoning. This work not only\nserves as a valuable resource for researchers and practitioners in the field of\nadversarial robustness but also identifies key areas for future research in\ntextual adversarial defence. By establishing a new standard for benchmarking in\nthis domain, we aim to accelerate progress towards more robust and reliable\nnatural language processing systems.\n", "link": "http://arxiv.org/abs/2501.02654v2", "date": "2025-01-08", "relevancy": 2.4127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tougher%20Text%2C%20Smarter%20Models%3A%20Raising%20the%20Bar%20for%20Adversarial%20Defence%0A%20%20Benchmarks&body=Title%3A%20Tougher%20Text%2C%20Smarter%20Models%3A%20Raising%20the%20Bar%20for%20Adversarial%20Defence%0A%20%20Benchmarks%0AAuthor%3A%20Yang%20Wang%20and%20Chenghua%20Lin%0AAbstract%3A%20%20%20Recent%20advancements%20in%20natural%20language%20processing%20have%20highlighted%20the%0Avulnerability%20of%20deep%20learning%20models%20to%20adversarial%20attacks.%20While%20various%0Adefence%20mechanisms%20have%20been%20proposed%2C%20there%20is%20a%20lack%20of%20comprehensive%0Abenchmarks%20that%20evaluate%20these%20defences%20across%20diverse%20datasets%2C%20models%2C%20and%0Atasks.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20presenting%20an%20extensive%20benchmark%0Afor%20textual%20adversarial%20defence%20that%20significantly%20expands%20upon%20previous%20work.%0AOur%20benchmark%20incorporates%20a%20wide%20range%20of%20datasets%2C%20evaluates%20state-of-the-art%0Adefence%20mechanisms%2C%20and%20extends%20the%20assessment%20to%20include%20critical%20tasks%20such%0Aas%20single-sentence%20classification%2C%20similarity%20and%20paraphrase%20identification%2C%0Anatural%20language%20inference%2C%20and%20commonsense%20reasoning.%20This%20work%20not%20only%0Aserves%20as%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20in%20the%20field%20of%0Aadversarial%20robustness%20but%20also%20identifies%20key%20areas%20for%20future%20research%20in%0Atextual%20adversarial%20defence.%20By%20establishing%20a%20new%20standard%20for%20benchmarking%20in%0Athis%20domain%2C%20we%20aim%20to%20accelerate%20progress%20towards%20more%20robust%20and%20reliable%0Anatural%20language%20processing%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTougher%2520Text%252C%2520Smarter%2520Models%253A%2520Raising%2520the%2520Bar%2520for%2520Adversarial%2520Defence%250A%2520%2520Benchmarks%26entry.906535625%3DYang%2520Wang%2520and%2520Chenghua%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520natural%2520language%2520processing%2520have%2520highlighted%2520the%250Avulnerability%2520of%2520deep%2520learning%2520models%2520to%2520adversarial%2520attacks.%2520While%2520various%250Adefence%2520mechanisms%2520have%2520been%2520proposed%252C%2520there%2520is%2520a%2520lack%2520of%2520comprehensive%250Abenchmarks%2520that%2520evaluate%2520these%2520defences%2520across%2520diverse%2520datasets%252C%2520models%252C%2520and%250Atasks.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520presenting%2520an%2520extensive%2520benchmark%250Afor%2520textual%2520adversarial%2520defence%2520that%2520significantly%2520expands%2520upon%2520previous%2520work.%250AOur%2520benchmark%2520incorporates%2520a%2520wide%2520range%2520of%2520datasets%252C%2520evaluates%2520state-of-the-art%250Adefence%2520mechanisms%252C%2520and%2520extends%2520the%2520assessment%2520to%2520include%2520critical%2520tasks%2520such%250Aas%2520single-sentence%2520classification%252C%2520similarity%2520and%2520paraphrase%2520identification%252C%250Anatural%2520language%2520inference%252C%2520and%2520commonsense%2520reasoning.%2520This%2520work%2520not%2520only%250Aserves%2520as%2520a%2520valuable%2520resource%2520for%2520researchers%2520and%2520practitioners%2520in%2520the%2520field%2520of%250Aadversarial%2520robustness%2520but%2520also%2520identifies%2520key%2520areas%2520for%2520future%2520research%2520in%250Atextual%2520adversarial%2520defence.%2520By%2520establishing%2520a%2520new%2520standard%2520for%2520benchmarking%2520in%250Athis%2520domain%252C%2520we%2520aim%2520to%2520accelerate%2520progress%2520towards%2520more%2520robust%2520and%2520reliable%250Anatural%2520language%2520processing%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tougher%20Text%2C%20Smarter%20Models%3A%20Raising%20the%20Bar%20for%20Adversarial%20Defence%0A%20%20Benchmarks&entry.906535625=Yang%20Wang%20and%20Chenghua%20Lin&entry.1292438233=%20%20Recent%20advancements%20in%20natural%20language%20processing%20have%20highlighted%20the%0Avulnerability%20of%20deep%20learning%20models%20to%20adversarial%20attacks.%20While%20various%0Adefence%20mechanisms%20have%20been%20proposed%2C%20there%20is%20a%20lack%20of%20comprehensive%0Abenchmarks%20that%20evaluate%20these%20defences%20across%20diverse%20datasets%2C%20models%2C%20and%0Atasks.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20presenting%20an%20extensive%20benchmark%0Afor%20textual%20adversarial%20defence%20that%20significantly%20expands%20upon%20previous%20work.%0AOur%20benchmark%20incorporates%20a%20wide%20range%20of%20datasets%2C%20evaluates%20state-of-the-art%0Adefence%20mechanisms%2C%20and%20extends%20the%20assessment%20to%20include%20critical%20tasks%20such%0Aas%20single-sentence%20classification%2C%20similarity%20and%20paraphrase%20identification%2C%0Anatural%20language%20inference%2C%20and%20commonsense%20reasoning.%20This%20work%20not%20only%0Aserves%20as%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20in%20the%20field%20of%0Aadversarial%20robustness%20but%20also%20identifies%20key%20areas%20for%20future%20research%20in%0Atextual%20adversarial%20defence.%20By%20establishing%20a%20new%20standard%20for%20benchmarking%20in%0Athis%20domain%2C%20we%20aim%20to%20accelerate%20progress%20towards%20more%20robust%20and%20reliable%0Anatural%20language%20processing%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02654v2&entry.124074799=Read"},
{"title": "Edge-Wise Graph-Instructed Neural Networks", "author": "Francesco Della Santa and Antonio Mastropietro and Sandra Pieraccini and Francesco Vaccarino", "abstract": "  The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over some graph-structured input data, like the ones inferred from\nthe Barabasi-Albert graph, and improve the training regularization on graphs\nwith chaotic connectivity, like the ones inferred from the Erdos-Renyi graph.\n", "link": "http://arxiv.org/abs/2409.08023v2", "date": "2025-01-08", "relevancy": 2.4045, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5319}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4659}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Wise%20Graph-Instructed%20Neural%20Networks&body=Title%3A%20Edge-Wise%20Graph-Instructed%20Neural%20Networks%0AAuthor%3A%20Francesco%20Della%20Santa%20and%20Antonio%20Mastropietro%20and%20Sandra%20Pieraccini%20and%20Francesco%20Vaccarino%0AAbstract%3A%20%20%20The%20problem%20of%20multi-task%20regression%20over%20graph%20nodes%20has%20been%20recently%0Aapproached%20through%20Graph-Instructed%20Neural%20Network%20%28GINN%29%2C%20which%20is%20a%20promising%0Aarchitecture%20belonging%20to%20the%20subset%20of%20message-passing%20graph%20neural%20networks.%0AIn%20this%20work%2C%20we%20discuss%20the%20limitations%20of%20the%20Graph-Instructed%20%28GI%29%20layer%2C%0Aand%20we%20formalize%20a%20novel%20edge-wise%20GI%20%28EWGI%29%20layer.%20We%20discuss%20the%20advantages%0Aof%20the%20EWGI%20layer%20and%20we%20provide%20numerical%20evidence%20that%20EWGINNs%20perform%20better%0Athan%20GINNs%20over%20some%20graph-structured%20input%20data%2C%20like%20the%20ones%20inferred%20from%0Athe%20Barabasi-Albert%20graph%2C%20and%20improve%20the%20training%20regularization%20on%20graphs%0Awith%20chaotic%20connectivity%2C%20like%20the%20ones%20inferred%20from%20the%20Erdos-Renyi%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08023v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Wise%2520Graph-Instructed%2520Neural%2520Networks%26entry.906535625%3DFrancesco%2520Della%2520Santa%2520and%2520Antonio%2520Mastropietro%2520and%2520Sandra%2520Pieraccini%2520and%2520Francesco%2520Vaccarino%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520multi-task%2520regression%2520over%2520graph%2520nodes%2520has%2520been%2520recently%250Aapproached%2520through%2520Graph-Instructed%2520Neural%2520Network%2520%2528GINN%2529%252C%2520which%2520is%2520a%2520promising%250Aarchitecture%2520belonging%2520to%2520the%2520subset%2520of%2520message-passing%2520graph%2520neural%2520networks.%250AIn%2520this%2520work%252C%2520we%2520discuss%2520the%2520limitations%2520of%2520the%2520Graph-Instructed%2520%2528GI%2529%2520layer%252C%250Aand%2520we%2520formalize%2520a%2520novel%2520edge-wise%2520GI%2520%2528EWGI%2529%2520layer.%2520We%2520discuss%2520the%2520advantages%250Aof%2520the%2520EWGI%2520layer%2520and%2520we%2520provide%2520numerical%2520evidence%2520that%2520EWGINNs%2520perform%2520better%250Athan%2520GINNs%2520over%2520some%2520graph-structured%2520input%2520data%252C%2520like%2520the%2520ones%2520inferred%2520from%250Athe%2520Barabasi-Albert%2520graph%252C%2520and%2520improve%2520the%2520training%2520regularization%2520on%2520graphs%250Awith%2520chaotic%2520connectivity%252C%2520like%2520the%2520ones%2520inferred%2520from%2520the%2520Erdos-Renyi%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08023v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Wise%20Graph-Instructed%20Neural%20Networks&entry.906535625=Francesco%20Della%20Santa%20and%20Antonio%20Mastropietro%20and%20Sandra%20Pieraccini%20and%20Francesco%20Vaccarino&entry.1292438233=%20%20The%20problem%20of%20multi-task%20regression%20over%20graph%20nodes%20has%20been%20recently%0Aapproached%20through%20Graph-Instructed%20Neural%20Network%20%28GINN%29%2C%20which%20is%20a%20promising%0Aarchitecture%20belonging%20to%20the%20subset%20of%20message-passing%20graph%20neural%20networks.%0AIn%20this%20work%2C%20we%20discuss%20the%20limitations%20of%20the%20Graph-Instructed%20%28GI%29%20layer%2C%0Aand%20we%20formalize%20a%20novel%20edge-wise%20GI%20%28EWGI%29%20layer.%20We%20discuss%20the%20advantages%0Aof%20the%20EWGI%20layer%20and%20we%20provide%20numerical%20evidence%20that%20EWGINNs%20perform%20better%0Athan%20GINNs%20over%20some%20graph-structured%20input%20data%2C%20like%20the%20ones%20inferred%20from%0Athe%20Barabasi-Albert%20graph%2C%20and%20improve%20the%20training%20regularization%20on%20graphs%0Awith%20chaotic%20connectivity%2C%20like%20the%20ones%20inferred%20from%20the%20Erdos-Renyi%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08023v2&entry.124074799=Read"},
{"title": "Grokking at the Edge of Numerical Stability", "author": "Lucas Prieto and Melih Barsbey and Pedro A. M. Mediano and Tolga Birdal", "abstract": "  Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.\n", "link": "http://arxiv.org/abs/2501.04697v1", "date": "2025-01-08", "relevancy": 2.3512, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4932}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4615}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grokking%20at%20the%20Edge%20of%20Numerical%20Stability&body=Title%3A%20Grokking%20at%20the%20Edge%20of%20Numerical%20Stability%0AAuthor%3A%20Lucas%20Prieto%20and%20Melih%20Barsbey%20and%20Pedro%20A.%20M.%20Mediano%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Grokking%2C%20the%20sudden%20generalization%20that%20occurs%20after%20prolonged%20overfitting%2C%0Ais%20a%20surprising%20phenomenon%20challenging%20our%20understanding%20of%20deep%20learning.%0AAlthough%20significant%20progress%20has%20been%20made%20in%20understanding%20grokking%2C%20the%0Areasons%20behind%20the%20delayed%20generalization%20and%20its%20dependence%20on%20regularization%0Aremain%20unclear.%20In%20this%20work%2C%20we%20argue%20that%20without%20regularization%2C%20grokking%0Atasks%20push%20models%20to%20the%20edge%20of%20numerical%20stability%2C%20introducing%20floating%0Apoint%20errors%20in%20the%20Softmax%20function%2C%20which%20we%20refer%20to%20as%20Softmax%20Collapse%0A%28SC%29.%20We%20demonstrate%20that%20SC%20prevents%20grokking%20and%20that%20mitigating%20SC%20enables%0Agrokking%20without%20regularization.%20Investigating%20the%20root%20cause%20of%20SC%2C%20we%20find%0Athat%20beyond%20the%20point%20of%20overfitting%2C%20the%20gradients%20strongly%20align%20with%20what%20we%0Acall%20the%20na%5C%22ive%20loss%20minimization%20%28NLM%29%20direction.%20This%20component%20of%20the%0Agradient%20does%20not%20alter%20the%20model%27s%20predictions%20but%20decreases%20the%20loss%20by%0Ascaling%20the%20logits%2C%20typically%20by%20scaling%20the%20weights%20along%20their%20current%0Adirection.%20We%20show%20that%20this%20scaling%20of%20the%20logits%20explains%20the%20delay%20in%0Ageneralization%20characteristic%20of%20grokking%20and%20eventually%20leads%20to%20SC%2C%20halting%0Afurther%20learning.%20To%20validate%20our%20hypotheses%2C%20we%20introduce%20two%20key%0Acontributions%20that%20address%20the%20challenges%20in%20grokking%20tasks%3A%20StableMax%2C%20a%20new%0Aactivation%20function%20that%20prevents%20SC%20and%20enables%20grokking%20without%0Aregularization%2C%20and%20%24%5Cperp%24Grad%2C%20a%20training%20algorithm%20that%20promotes%20quick%0Ageneralization%20in%20grokking%20tasks%20by%20preventing%20NLM%20altogether.%20These%0Acontributions%20provide%20new%20insights%20into%20grokking%2C%20elucidating%20its%20delayed%0Ageneralization%2C%20reliance%20on%20regularization%2C%20and%20the%20effectiveness%20of%20existing%0Agrokking-inducing%20methods.%20Code%20for%20this%20paper%20is%20available%20at%0Ahttps%3A//github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrokking%2520at%2520the%2520Edge%2520of%2520Numerical%2520Stability%26entry.906535625%3DLucas%2520Prieto%2520and%2520Melih%2520Barsbey%2520and%2520Pedro%2520A.%2520M.%2520Mediano%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Grokking%252C%2520the%2520sudden%2520generalization%2520that%2520occurs%2520after%2520prolonged%2520overfitting%252C%250Ais%2520a%2520surprising%2520phenomenon%2520challenging%2520our%2520understanding%2520of%2520deep%2520learning.%250AAlthough%2520significant%2520progress%2520has%2520been%2520made%2520in%2520understanding%2520grokking%252C%2520the%250Areasons%2520behind%2520the%2520delayed%2520generalization%2520and%2520its%2520dependence%2520on%2520regularization%250Aremain%2520unclear.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520without%2520regularization%252C%2520grokking%250Atasks%2520push%2520models%2520to%2520the%2520edge%2520of%2520numerical%2520stability%252C%2520introducing%2520floating%250Apoint%2520errors%2520in%2520the%2520Softmax%2520function%252C%2520which%2520we%2520refer%2520to%2520as%2520Softmax%2520Collapse%250A%2528SC%2529.%2520We%2520demonstrate%2520that%2520SC%2520prevents%2520grokking%2520and%2520that%2520mitigating%2520SC%2520enables%250Agrokking%2520without%2520regularization.%2520Investigating%2520the%2520root%2520cause%2520of%2520SC%252C%2520we%2520find%250Athat%2520beyond%2520the%2520point%2520of%2520overfitting%252C%2520the%2520gradients%2520strongly%2520align%2520with%2520what%2520we%250Acall%2520the%2520na%255C%2522ive%2520loss%2520minimization%2520%2528NLM%2529%2520direction.%2520This%2520component%2520of%2520the%250Agradient%2520does%2520not%2520alter%2520the%2520model%2527s%2520predictions%2520but%2520decreases%2520the%2520loss%2520by%250Ascaling%2520the%2520logits%252C%2520typically%2520by%2520scaling%2520the%2520weights%2520along%2520their%2520current%250Adirection.%2520We%2520show%2520that%2520this%2520scaling%2520of%2520the%2520logits%2520explains%2520the%2520delay%2520in%250Ageneralization%2520characteristic%2520of%2520grokking%2520and%2520eventually%2520leads%2520to%2520SC%252C%2520halting%250Afurther%2520learning.%2520To%2520validate%2520our%2520hypotheses%252C%2520we%2520introduce%2520two%2520key%250Acontributions%2520that%2520address%2520the%2520challenges%2520in%2520grokking%2520tasks%253A%2520StableMax%252C%2520a%2520new%250Aactivation%2520function%2520that%2520prevents%2520SC%2520and%2520enables%2520grokking%2520without%250Aregularization%252C%2520and%2520%2524%255Cperp%2524Grad%252C%2520a%2520training%2520algorithm%2520that%2520promotes%2520quick%250Ageneralization%2520in%2520grokking%2520tasks%2520by%2520preventing%2520NLM%2520altogether.%2520These%250Acontributions%2520provide%2520new%2520insights%2520into%2520grokking%252C%2520elucidating%2520its%2520delayed%250Ageneralization%252C%2520reliance%2520on%2520regularization%252C%2520and%2520the%2520effectiveness%2520of%2520existing%250Agrokking-inducing%2520methods.%2520Code%2520for%2520this%2520paper%2520is%2520available%2520at%250Ahttps%253A//github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grokking%20at%20the%20Edge%20of%20Numerical%20Stability&entry.906535625=Lucas%20Prieto%20and%20Melih%20Barsbey%20and%20Pedro%20A.%20M.%20Mediano%20and%20Tolga%20Birdal&entry.1292438233=%20%20Grokking%2C%20the%20sudden%20generalization%20that%20occurs%20after%20prolonged%20overfitting%2C%0Ais%20a%20surprising%20phenomenon%20challenging%20our%20understanding%20of%20deep%20learning.%0AAlthough%20significant%20progress%20has%20been%20made%20in%20understanding%20grokking%2C%20the%0Areasons%20behind%20the%20delayed%20generalization%20and%20its%20dependence%20on%20regularization%0Aremain%20unclear.%20In%20this%20work%2C%20we%20argue%20that%20without%20regularization%2C%20grokking%0Atasks%20push%20models%20to%20the%20edge%20of%20numerical%20stability%2C%20introducing%20floating%0Apoint%20errors%20in%20the%20Softmax%20function%2C%20which%20we%20refer%20to%20as%20Softmax%20Collapse%0A%28SC%29.%20We%20demonstrate%20that%20SC%20prevents%20grokking%20and%20that%20mitigating%20SC%20enables%0Agrokking%20without%20regularization.%20Investigating%20the%20root%20cause%20of%20SC%2C%20we%20find%0Athat%20beyond%20the%20point%20of%20overfitting%2C%20the%20gradients%20strongly%20align%20with%20what%20we%0Acall%20the%20na%5C%22ive%20loss%20minimization%20%28NLM%29%20direction.%20This%20component%20of%20the%0Agradient%20does%20not%20alter%20the%20model%27s%20predictions%20but%20decreases%20the%20loss%20by%0Ascaling%20the%20logits%2C%20typically%20by%20scaling%20the%20weights%20along%20their%20current%0Adirection.%20We%20show%20that%20this%20scaling%20of%20the%20logits%20explains%20the%20delay%20in%0Ageneralization%20characteristic%20of%20grokking%20and%20eventually%20leads%20to%20SC%2C%20halting%0Afurther%20learning.%20To%20validate%20our%20hypotheses%2C%20we%20introduce%20two%20key%0Acontributions%20that%20address%20the%20challenges%20in%20grokking%20tasks%3A%20StableMax%2C%20a%20new%0Aactivation%20function%20that%20prevents%20SC%20and%20enables%20grokking%20without%0Aregularization%2C%20and%20%24%5Cperp%24Grad%2C%20a%20training%20algorithm%20that%20promotes%20quick%0Ageneralization%20in%20grokking%20tasks%20by%20preventing%20NLM%20altogether.%20These%0Acontributions%20provide%20new%20insights%20into%20grokking%2C%20elucidating%20its%20delayed%0Ageneralization%2C%20reliance%20on%20regularization%2C%20and%20the%20effectiveness%20of%20existing%0Agrokking-inducing%20methods.%20Code%20for%20this%20paper%20is%20available%20at%0Ahttps%3A//github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04697v1&entry.124074799=Read"},
{"title": "GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic\n  Features for Medical Image Segmentation", "author": "Niloufar Eghbali and Hassan Bagher-Ebadian and Tuka Alhanai and Mohammad M. Ghassemi", "abstract": "  Vision Transformers (ViTs) have shown promise in medical image semantic\nsegmentation (MISS) by capturing long-range correlations. However, ViTs often\nstruggle to model local spatial information effectively, which is essential for\naccurately segmenting fine anatomical details, particularly when applied to\nsmall datasets without extensive pre-training. We introduce Gabor and Laplacian\nof Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture\nenhancing Transformer-based models by incorporating learnable radiomic\nfeatures. This approach integrates dynamically adaptive Gabor and Laplacian of\nGaussian (LoG) filters to capture texture, edge, and boundary information,\nenhancing the feature representation processed by the Transformer model. Our\nmethod uniquely combines the long-range dependency modeling of Transformers\nwith the texture analysis capabilities of Gabor and LoG features. Evaluated on\nthe Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet\ndemonstrates significant improvements over state-of-the-art models, achieving a\n1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal\ncomputational overhead (only 15 and 30 additional parameters, respectively).\nGLoG-CSUnet's flexible design allows integration with various base models,\noffering a promising approach for incorporating radiomics-inspired feature\nextraction in Transformer architectures for medical image analysis. The code\nimplementation is available on GitHub at: https://github.com/HAAIL/GLoG-CSUnet.\n", "link": "http://arxiv.org/abs/2501.02788v2", "date": "2025-01-08", "relevancy": 2.3473, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6168}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLoG-CSUnet%3A%20Enhancing%20Vision%20Transformers%20with%20Adaptable%20Radiomic%0A%20%20Features%20for%20Medical%20Image%20Segmentation&body=Title%3A%20GLoG-CSUnet%3A%20Enhancing%20Vision%20Transformers%20with%20Adaptable%20Radiomic%0A%20%20Features%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Niloufar%20Eghbali%20and%20Hassan%20Bagher-Ebadian%20and%20Tuka%20Alhanai%20and%20Mohammad%20M.%20Ghassemi%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20shown%20promise%20in%20medical%20image%20semantic%0Asegmentation%20%28MISS%29%20by%20capturing%20long-range%20correlations.%20However%2C%20ViTs%20often%0Astruggle%20to%20model%20local%20spatial%20information%20effectively%2C%20which%20is%20essential%20for%0Aaccurately%20segmenting%20fine%20anatomical%20details%2C%20particularly%20when%20applied%20to%0Asmall%20datasets%20without%20extensive%20pre-training.%20We%20introduce%20Gabor%20and%20Laplacian%0Aof%20Gaussian%20Convolutional%20Swin%20Network%20%28GLoG-CSUnet%29%2C%20a%20novel%20architecture%0Aenhancing%20Transformer-based%20models%20by%20incorporating%20learnable%20radiomic%0Afeatures.%20This%20approach%20integrates%20dynamically%20adaptive%20Gabor%20and%20Laplacian%20of%0AGaussian%20%28LoG%29%20filters%20to%20capture%20texture%2C%20edge%2C%20and%20boundary%20information%2C%0Aenhancing%20the%20feature%20representation%20processed%20by%20the%20Transformer%20model.%20Our%0Amethod%20uniquely%20combines%20the%20long-range%20dependency%20modeling%20of%20Transformers%0Awith%20the%20texture%20analysis%20capabilities%20of%20Gabor%20and%20LoG%20features.%20Evaluated%20on%0Athe%20Synapse%20multi-organ%20and%20ACDC%20cardiac%20segmentation%20datasets%2C%20GLoG-CSUnet%0Ademonstrates%20significant%20improvements%20over%20state-of-the-art%20models%2C%20achieving%20a%0A1.14%25%20increase%20in%20Dice%20score%20for%20Synapse%20and%200.99%25%20for%20ACDC%2C%20with%20minimal%0Acomputational%20overhead%20%28only%2015%20and%2030%20additional%20parameters%2C%20respectively%29.%0AGLoG-CSUnet%27s%20flexible%20design%20allows%20integration%20with%20various%20base%20models%2C%0Aoffering%20a%20promising%20approach%20for%20incorporating%20radiomics-inspired%20feature%0Aextraction%20in%20Transformer%20architectures%20for%20medical%20image%20analysis.%20The%20code%0Aimplementation%20is%20available%20on%20GitHub%20at%3A%20https%3A//github.com/HAAIL/GLoG-CSUnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLoG-CSUnet%253A%2520Enhancing%2520Vision%2520Transformers%2520with%2520Adaptable%2520Radiomic%250A%2520%2520Features%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DNiloufar%2520Eghbali%2520and%2520Hassan%2520Bagher-Ebadian%2520and%2520Tuka%2520Alhanai%2520and%2520Mohammad%2520M.%2520Ghassemi%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520shown%2520promise%2520in%2520medical%2520image%2520semantic%250Asegmentation%2520%2528MISS%2529%2520by%2520capturing%2520long-range%2520correlations.%2520However%252C%2520ViTs%2520often%250Astruggle%2520to%2520model%2520local%2520spatial%2520information%2520effectively%252C%2520which%2520is%2520essential%2520for%250Aaccurately%2520segmenting%2520fine%2520anatomical%2520details%252C%2520particularly%2520when%2520applied%2520to%250Asmall%2520datasets%2520without%2520extensive%2520pre-training.%2520We%2520introduce%2520Gabor%2520and%2520Laplacian%250Aof%2520Gaussian%2520Convolutional%2520Swin%2520Network%2520%2528GLoG-CSUnet%2529%252C%2520a%2520novel%2520architecture%250Aenhancing%2520Transformer-based%2520models%2520by%2520incorporating%2520learnable%2520radiomic%250Afeatures.%2520This%2520approach%2520integrates%2520dynamically%2520adaptive%2520Gabor%2520and%2520Laplacian%2520of%250AGaussian%2520%2528LoG%2529%2520filters%2520to%2520capture%2520texture%252C%2520edge%252C%2520and%2520boundary%2520information%252C%250Aenhancing%2520the%2520feature%2520representation%2520processed%2520by%2520the%2520Transformer%2520model.%2520Our%250Amethod%2520uniquely%2520combines%2520the%2520long-range%2520dependency%2520modeling%2520of%2520Transformers%250Awith%2520the%2520texture%2520analysis%2520capabilities%2520of%2520Gabor%2520and%2520LoG%2520features.%2520Evaluated%2520on%250Athe%2520Synapse%2520multi-organ%2520and%2520ACDC%2520cardiac%2520segmentation%2520datasets%252C%2520GLoG-CSUnet%250Ademonstrates%2520significant%2520improvements%2520over%2520state-of-the-art%2520models%252C%2520achieving%2520a%250A1.14%2525%2520increase%2520in%2520Dice%2520score%2520for%2520Synapse%2520and%25200.99%2525%2520for%2520ACDC%252C%2520with%2520minimal%250Acomputational%2520overhead%2520%2528only%252015%2520and%252030%2520additional%2520parameters%252C%2520respectively%2529.%250AGLoG-CSUnet%2527s%2520flexible%2520design%2520allows%2520integration%2520with%2520various%2520base%2520models%252C%250Aoffering%2520a%2520promising%2520approach%2520for%2520incorporating%2520radiomics-inspired%2520feature%250Aextraction%2520in%2520Transformer%2520architectures%2520for%2520medical%2520image%2520analysis.%2520The%2520code%250Aimplementation%2520is%2520available%2520on%2520GitHub%2520at%253A%2520https%253A//github.com/HAAIL/GLoG-CSUnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLoG-CSUnet%3A%20Enhancing%20Vision%20Transformers%20with%20Adaptable%20Radiomic%0A%20%20Features%20for%20Medical%20Image%20Segmentation&entry.906535625=Niloufar%20Eghbali%20and%20Hassan%20Bagher-Ebadian%20and%20Tuka%20Alhanai%20and%20Mohammad%20M.%20Ghassemi&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20shown%20promise%20in%20medical%20image%20semantic%0Asegmentation%20%28MISS%29%20by%20capturing%20long-range%20correlations.%20However%2C%20ViTs%20often%0Astruggle%20to%20model%20local%20spatial%20information%20effectively%2C%20which%20is%20essential%20for%0Aaccurately%20segmenting%20fine%20anatomical%20details%2C%20particularly%20when%20applied%20to%0Asmall%20datasets%20without%20extensive%20pre-training.%20We%20introduce%20Gabor%20and%20Laplacian%0Aof%20Gaussian%20Convolutional%20Swin%20Network%20%28GLoG-CSUnet%29%2C%20a%20novel%20architecture%0Aenhancing%20Transformer-based%20models%20by%20incorporating%20learnable%20radiomic%0Afeatures.%20This%20approach%20integrates%20dynamically%20adaptive%20Gabor%20and%20Laplacian%20of%0AGaussian%20%28LoG%29%20filters%20to%20capture%20texture%2C%20edge%2C%20and%20boundary%20information%2C%0Aenhancing%20the%20feature%20representation%20processed%20by%20the%20Transformer%20model.%20Our%0Amethod%20uniquely%20combines%20the%20long-range%20dependency%20modeling%20of%20Transformers%0Awith%20the%20texture%20analysis%20capabilities%20of%20Gabor%20and%20LoG%20features.%20Evaluated%20on%0Athe%20Synapse%20multi-organ%20and%20ACDC%20cardiac%20segmentation%20datasets%2C%20GLoG-CSUnet%0Ademonstrates%20significant%20improvements%20over%20state-of-the-art%20models%2C%20achieving%20a%0A1.14%25%20increase%20in%20Dice%20score%20for%20Synapse%20and%200.99%25%20for%20ACDC%2C%20with%20minimal%0Acomputational%20overhead%20%28only%2015%20and%2030%20additional%20parameters%2C%20respectively%29.%0AGLoG-CSUnet%27s%20flexible%20design%20allows%20integration%20with%20various%20base%20models%2C%0Aoffering%20a%20promising%20approach%20for%20incorporating%20radiomics-inspired%20feature%0Aextraction%20in%20Transformer%20architectures%20for%20medical%20image%20analysis.%20The%20code%0Aimplementation%20is%20available%20on%20GitHub%20at%3A%20https%3A//github.com/HAAIL/GLoG-CSUnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02788v2&entry.124074799=Read"},
{"title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection", "author": "Ruijun Feng and Hammond Pearce and Pietro Liguori and Yulei Sui", "abstract": "  Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code.\n", "link": "http://arxiv.org/abs/2501.04510v1", "date": "2025-01-08", "relevancy": 2.3433, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGP-Tuning%3A%20Structure-Aware%20Soft%20Prompt%20Tuning%20for%20Code%20Vulnerability%0A%20%20Detection&body=Title%3A%20CGP-Tuning%3A%20Structure-Aware%20Soft%20Prompt%20Tuning%20for%20Code%20Vulnerability%0A%20%20Detection%0AAuthor%3A%20Ruijun%20Feng%20and%20Hammond%20Pearce%20and%20Pietro%20Liguori%20and%20Yulei%20Sui%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20proposed%20as%20powerful%20tools%20for%0Adetecting%20software%20vulnerabilities%2C%20where%20task-specific%20fine-tuning%20is%0Atypically%20employed%20to%20provide%20vulnerability-specific%20knowledge%20to%20the%20LLMs%20for%0Athis%20purpose.%20However%2C%20traditional%20full-parameter%20fine-tuning%20is%20inefficient%0Afor%20modern%2C%20complex%20LLMs%2C%20which%20contain%20billions%20of%20parameters.%0A%20%20Soft%20prompt%20tuning%20has%20been%20suggested%20as%20a%20more%20efficient%20alternative%20for%0Afine-tuning%20LLMs%20in%20general%20cases.%20However%2C%20pure%20soft%20prompt%20tuning%20treats%0Asource%20code%20as%20plain%20text%2C%20losing%20structural%20information%20inherent%20in%20source%0Acode.%20Meanwhile%2C%20graph-enhanced%20soft%20prompt%20tuning%20methods%2C%20which%20aim%20to%0Aaddress%20this%20issue%2C%20are%20unable%20to%20preserve%20the%20rich%20semantic%20information%20within%0Acode%20graphs%2C%20as%20they%20are%20primarily%20designed%20for%20general%20graph-related%20tasks%20and%0Afocus%20more%20on%20adjacency%20information.%20They%20also%20fail%20to%20ensure%20computational%0Aefficiency%20while%20accounting%20for%20graph-text%20interactions.%0A%20%20This%20paper%2C%20therefore%2C%20introduces%20a%20new%20code%20graph-enhanced%2C%20structure-aware%0Asoft%20prompt%20tuning%20method%20for%20vulnerability%20detection%2C%20referred%20to%20as%0ACGP-Tuning.%20It%20employs%20innovative%20type-aware%20embeddings%20to%20capture%20the%20rich%0Asemantic%20information%20within%20code%20graphs%2C%20along%20with%20a%20novel%20and%20efficient%0Across-modal%20alignment%20module%20that%20achieves%20linear%20computational%20cost%20while%0Aincorporating%20graph-text%20interactions.%20The%20proposed%20CGP-Tuning%20is%20evaluated%20on%0Athe%20latest%20DiverseVul%20dataset%20and%20the%20most%20recent%20open-source%20code%20LLMs%2C%0ACodeLlama%20and%20CodeGemma.%20Experimental%20results%20demonstrate%20that%20CGP-Tuning%0Aoutperforms%20the%20best%20state-of-the-art%20method%20by%20an%20average%20of%203.5%20percentage%0Apoints%20in%20accuracy%2C%20without%20compromising%20its%20vulnerability%20detection%0Acapabilities%20for%20long%20source%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGP-Tuning%253A%2520Structure-Aware%2520Soft%2520Prompt%2520Tuning%2520for%2520Code%2520Vulnerability%250A%2520%2520Detection%26entry.906535625%3DRuijun%2520Feng%2520and%2520Hammond%2520Pearce%2520and%2520Pietro%2520Liguori%2520and%2520Yulei%2520Sui%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520proposed%2520as%2520powerful%2520tools%2520for%250Adetecting%2520software%2520vulnerabilities%252C%2520where%2520task-specific%2520fine-tuning%2520is%250Atypically%2520employed%2520to%2520provide%2520vulnerability-specific%2520knowledge%2520to%2520the%2520LLMs%2520for%250Athis%2520purpose.%2520However%252C%2520traditional%2520full-parameter%2520fine-tuning%2520is%2520inefficient%250Afor%2520modern%252C%2520complex%2520LLMs%252C%2520which%2520contain%2520billions%2520of%2520parameters.%250A%2520%2520Soft%2520prompt%2520tuning%2520has%2520been%2520suggested%2520as%2520a%2520more%2520efficient%2520alternative%2520for%250Afine-tuning%2520LLMs%2520in%2520general%2520cases.%2520However%252C%2520pure%2520soft%2520prompt%2520tuning%2520treats%250Asource%2520code%2520as%2520plain%2520text%252C%2520losing%2520structural%2520information%2520inherent%2520in%2520source%250Acode.%2520Meanwhile%252C%2520graph-enhanced%2520soft%2520prompt%2520tuning%2520methods%252C%2520which%2520aim%2520to%250Aaddress%2520this%2520issue%252C%2520are%2520unable%2520to%2520preserve%2520the%2520rich%2520semantic%2520information%2520within%250Acode%2520graphs%252C%2520as%2520they%2520are%2520primarily%2520designed%2520for%2520general%2520graph-related%2520tasks%2520and%250Afocus%2520more%2520on%2520adjacency%2520information.%2520They%2520also%2520fail%2520to%2520ensure%2520computational%250Aefficiency%2520while%2520accounting%2520for%2520graph-text%2520interactions.%250A%2520%2520This%2520paper%252C%2520therefore%252C%2520introduces%2520a%2520new%2520code%2520graph-enhanced%252C%2520structure-aware%250Asoft%2520prompt%2520tuning%2520method%2520for%2520vulnerability%2520detection%252C%2520referred%2520to%2520as%250ACGP-Tuning.%2520It%2520employs%2520innovative%2520type-aware%2520embeddings%2520to%2520capture%2520the%2520rich%250Asemantic%2520information%2520within%2520code%2520graphs%252C%2520along%2520with%2520a%2520novel%2520and%2520efficient%250Across-modal%2520alignment%2520module%2520that%2520achieves%2520linear%2520computational%2520cost%2520while%250Aincorporating%2520graph-text%2520interactions.%2520The%2520proposed%2520CGP-Tuning%2520is%2520evaluated%2520on%250Athe%2520latest%2520DiverseVul%2520dataset%2520and%2520the%2520most%2520recent%2520open-source%2520code%2520LLMs%252C%250ACodeLlama%2520and%2520CodeGemma.%2520Experimental%2520results%2520demonstrate%2520that%2520CGP-Tuning%250Aoutperforms%2520the%2520best%2520state-of-the-art%2520method%2520by%2520an%2520average%2520of%25203.5%2520percentage%250Apoints%2520in%2520accuracy%252C%2520without%2520compromising%2520its%2520vulnerability%2520detection%250Acapabilities%2520for%2520long%2520source%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGP-Tuning%3A%20Structure-Aware%20Soft%20Prompt%20Tuning%20for%20Code%20Vulnerability%0A%20%20Detection&entry.906535625=Ruijun%20Feng%20and%20Hammond%20Pearce%20and%20Pietro%20Liguori%20and%20Yulei%20Sui&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20proposed%20as%20powerful%20tools%20for%0Adetecting%20software%20vulnerabilities%2C%20where%20task-specific%20fine-tuning%20is%0Atypically%20employed%20to%20provide%20vulnerability-specific%20knowledge%20to%20the%20LLMs%20for%0Athis%20purpose.%20However%2C%20traditional%20full-parameter%20fine-tuning%20is%20inefficient%0Afor%20modern%2C%20complex%20LLMs%2C%20which%20contain%20billions%20of%20parameters.%0A%20%20Soft%20prompt%20tuning%20has%20been%20suggested%20as%20a%20more%20efficient%20alternative%20for%0Afine-tuning%20LLMs%20in%20general%20cases.%20However%2C%20pure%20soft%20prompt%20tuning%20treats%0Asource%20code%20as%20plain%20text%2C%20losing%20structural%20information%20inherent%20in%20source%0Acode.%20Meanwhile%2C%20graph-enhanced%20soft%20prompt%20tuning%20methods%2C%20which%20aim%20to%0Aaddress%20this%20issue%2C%20are%20unable%20to%20preserve%20the%20rich%20semantic%20information%20within%0Acode%20graphs%2C%20as%20they%20are%20primarily%20designed%20for%20general%20graph-related%20tasks%20and%0Afocus%20more%20on%20adjacency%20information.%20They%20also%20fail%20to%20ensure%20computational%0Aefficiency%20while%20accounting%20for%20graph-text%20interactions.%0A%20%20This%20paper%2C%20therefore%2C%20introduces%20a%20new%20code%20graph-enhanced%2C%20structure-aware%0Asoft%20prompt%20tuning%20method%20for%20vulnerability%20detection%2C%20referred%20to%20as%0ACGP-Tuning.%20It%20employs%20innovative%20type-aware%20embeddings%20to%20capture%20the%20rich%0Asemantic%20information%20within%20code%20graphs%2C%20along%20with%20a%20novel%20and%20efficient%0Across-modal%20alignment%20module%20that%20achieves%20linear%20computational%20cost%20while%0Aincorporating%20graph-text%20interactions.%20The%20proposed%20CGP-Tuning%20is%20evaluated%20on%0Athe%20latest%20DiverseVul%20dataset%20and%20the%20most%20recent%20open-source%20code%20LLMs%2C%0ACodeLlama%20and%20CodeGemma.%20Experimental%20results%20demonstrate%20that%20CGP-Tuning%0Aoutperforms%20the%20best%20state-of-the-art%20method%20by%20an%20average%20of%203.5%20percentage%0Apoints%20in%20accuracy%2C%20without%20compromising%20its%20vulnerability%20detection%0Acapabilities%20for%20long%20source%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04510v1&entry.124074799=Read"},
{"title": "Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria\n  Shaping Modern Artificial Neural Network Architectures", "author": "Ziyuan Huang and Mark Newman and Maria Vaida and Srikar Bellur and Roozbeh Sadeghian and Andrew Siu and Hui Wang and Kevin Huggins", "abstract": "  This study examined the viability of enhancing the prediction accuracy of\nartificial neural networks (ANNs) in image classification tasks by developing\nANNs with evolution patterns similar to those of biological neural networks.\nResNet is a widely used family of neural networks with both deep and wide\nvariants; therefore, it was selected as the base model for our investigation.\nThe aim of this study is to improve the image classification performance of\nANNs via a novel approach inspired by the biological nervous system\narchitecture of planarians, which comprises a brain and two nerve cords. We\nbelieve that the unique neural architecture of planarians offers valuable\ninsights into the performance enhancement of ANNs. The proposed planarian\nneural architecture-based neural network was evaluated on the CIFAR-10 and\nCIFAR-100 datasets. Our results indicate that the proposed method exhibits\nhigher prediction accuracy than the baseline neural network models in image\nclassification tasks. These findings demonstrate the significant potential of\nbiologically inspired neural network architectures in improving the performance\nof ANNs in a wide range of applications.\n", "link": "http://arxiv.org/abs/2501.04700v1", "date": "2025-01-08", "relevancy": 2.3311, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.503}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planarian%20Neural%20Networks%3A%20Evolutionary%20Patterns%20from%20Basic%20Bilateria%0A%20%20Shaping%20Modern%20Artificial%20Neural%20Network%20Architectures&body=Title%3A%20Planarian%20Neural%20Networks%3A%20Evolutionary%20Patterns%20from%20Basic%20Bilateria%0A%20%20Shaping%20Modern%20Artificial%20Neural%20Network%20Architectures%0AAuthor%3A%20Ziyuan%20Huang%20and%20Mark%20Newman%20and%20Maria%20Vaida%20and%20Srikar%20Bellur%20and%20Roozbeh%20Sadeghian%20and%20Andrew%20Siu%20and%20Hui%20Wang%20and%20Kevin%20Huggins%0AAbstract%3A%20%20%20This%20study%20examined%20the%20viability%20of%20enhancing%20the%20prediction%20accuracy%20of%0Aartificial%20neural%20networks%20%28ANNs%29%20in%20image%20classification%20tasks%20by%20developing%0AANNs%20with%20evolution%20patterns%20similar%20to%20those%20of%20biological%20neural%20networks.%0AResNet%20is%20a%20widely%20used%20family%20of%20neural%20networks%20with%20both%20deep%20and%20wide%0Avariants%3B%20therefore%2C%20it%20was%20selected%20as%20the%20base%20model%20for%20our%20investigation.%0AThe%20aim%20of%20this%20study%20is%20to%20improve%20the%20image%20classification%20performance%20of%0AANNs%20via%20a%20novel%20approach%20inspired%20by%20the%20biological%20nervous%20system%0Aarchitecture%20of%20planarians%2C%20which%20comprises%20a%20brain%20and%20two%20nerve%20cords.%20We%0Abelieve%20that%20the%20unique%20neural%20architecture%20of%20planarians%20offers%20valuable%0Ainsights%20into%20the%20performance%20enhancement%20of%20ANNs.%20The%20proposed%20planarian%0Aneural%20architecture-based%20neural%20network%20was%20evaluated%20on%20the%20CIFAR-10%20and%0ACIFAR-100%20datasets.%20Our%20results%20indicate%20that%20the%20proposed%20method%20exhibits%0Ahigher%20prediction%20accuracy%20than%20the%20baseline%20neural%20network%20models%20in%20image%0Aclassification%20tasks.%20These%20findings%20demonstrate%20the%20significant%20potential%20of%0Abiologically%20inspired%20neural%20network%20architectures%20in%20improving%20the%20performance%0Aof%20ANNs%20in%20a%20wide%20range%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanarian%2520Neural%2520Networks%253A%2520Evolutionary%2520Patterns%2520from%2520Basic%2520Bilateria%250A%2520%2520Shaping%2520Modern%2520Artificial%2520Neural%2520Network%2520Architectures%26entry.906535625%3DZiyuan%2520Huang%2520and%2520Mark%2520Newman%2520and%2520Maria%2520Vaida%2520and%2520Srikar%2520Bellur%2520and%2520Roozbeh%2520Sadeghian%2520and%2520Andrew%2520Siu%2520and%2520Hui%2520Wang%2520and%2520Kevin%2520Huggins%26entry.1292438233%3D%2520%2520This%2520study%2520examined%2520the%2520viability%2520of%2520enhancing%2520the%2520prediction%2520accuracy%2520of%250Aartificial%2520neural%2520networks%2520%2528ANNs%2529%2520in%2520image%2520classification%2520tasks%2520by%2520developing%250AANNs%2520with%2520evolution%2520patterns%2520similar%2520to%2520those%2520of%2520biological%2520neural%2520networks.%250AResNet%2520is%2520a%2520widely%2520used%2520family%2520of%2520neural%2520networks%2520with%2520both%2520deep%2520and%2520wide%250Avariants%253B%2520therefore%252C%2520it%2520was%2520selected%2520as%2520the%2520base%2520model%2520for%2520our%2520investigation.%250AThe%2520aim%2520of%2520this%2520study%2520is%2520to%2520improve%2520the%2520image%2520classification%2520performance%2520of%250AANNs%2520via%2520a%2520novel%2520approach%2520inspired%2520by%2520the%2520biological%2520nervous%2520system%250Aarchitecture%2520of%2520planarians%252C%2520which%2520comprises%2520a%2520brain%2520and%2520two%2520nerve%2520cords.%2520We%250Abelieve%2520that%2520the%2520unique%2520neural%2520architecture%2520of%2520planarians%2520offers%2520valuable%250Ainsights%2520into%2520the%2520performance%2520enhancement%2520of%2520ANNs.%2520The%2520proposed%2520planarian%250Aneural%2520architecture-based%2520neural%2520network%2520was%2520evaluated%2520on%2520the%2520CIFAR-10%2520and%250ACIFAR-100%2520datasets.%2520Our%2520results%2520indicate%2520that%2520the%2520proposed%2520method%2520exhibits%250Ahigher%2520prediction%2520accuracy%2520than%2520the%2520baseline%2520neural%2520network%2520models%2520in%2520image%250Aclassification%2520tasks.%2520These%2520findings%2520demonstrate%2520the%2520significant%2520potential%2520of%250Abiologically%2520inspired%2520neural%2520network%2520architectures%2520in%2520improving%2520the%2520performance%250Aof%2520ANNs%2520in%2520a%2520wide%2520range%2520of%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planarian%20Neural%20Networks%3A%20Evolutionary%20Patterns%20from%20Basic%20Bilateria%0A%20%20Shaping%20Modern%20Artificial%20Neural%20Network%20Architectures&entry.906535625=Ziyuan%20Huang%20and%20Mark%20Newman%20and%20Maria%20Vaida%20and%20Srikar%20Bellur%20and%20Roozbeh%20Sadeghian%20and%20Andrew%20Siu%20and%20Hui%20Wang%20and%20Kevin%20Huggins&entry.1292438233=%20%20This%20study%20examined%20the%20viability%20of%20enhancing%20the%20prediction%20accuracy%20of%0Aartificial%20neural%20networks%20%28ANNs%29%20in%20image%20classification%20tasks%20by%20developing%0AANNs%20with%20evolution%20patterns%20similar%20to%20those%20of%20biological%20neural%20networks.%0AResNet%20is%20a%20widely%20used%20family%20of%20neural%20networks%20with%20both%20deep%20and%20wide%0Avariants%3B%20therefore%2C%20it%20was%20selected%20as%20the%20base%20model%20for%20our%20investigation.%0AThe%20aim%20of%20this%20study%20is%20to%20improve%20the%20image%20classification%20performance%20of%0AANNs%20via%20a%20novel%20approach%20inspired%20by%20the%20biological%20nervous%20system%0Aarchitecture%20of%20planarians%2C%20which%20comprises%20a%20brain%20and%20two%20nerve%20cords.%20We%0Abelieve%20that%20the%20unique%20neural%20architecture%20of%20planarians%20offers%20valuable%0Ainsights%20into%20the%20performance%20enhancement%20of%20ANNs.%20The%20proposed%20planarian%0Aneural%20architecture-based%20neural%20network%20was%20evaluated%20on%20the%20CIFAR-10%20and%0ACIFAR-100%20datasets.%20Our%20results%20indicate%20that%20the%20proposed%20method%20exhibits%0Ahigher%20prediction%20accuracy%20than%20the%20baseline%20neural%20network%20models%20in%20image%0Aclassification%20tasks.%20These%20findings%20demonstrate%20the%20significant%20potential%20of%0Abiologically%20inspired%20neural%20network%20architectures%20in%20improving%20the%20performance%0Aof%20ANNs%20in%20a%20wide%20range%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04700v1&entry.124074799=Read"},
{"title": "LiLMaps: Learnable Implicit Language Maps", "author": "Evgenii Kruzhkov and Sven Behnke", "abstract": "  One of the current trends in robotics is to employ large language models\n(LLMs) to provide non-predefined command execution and natural human-robot\ninteraction. It is useful to have an environment map together with its language\nrepresentation, which can be further utilized by LLMs. Such a comprehensive\nscene representation enables numerous ways of interaction with the map for\nautonomously operating robots. In this work, we present an approach that\nenhances incremental implicit mapping through the integration of\nvision-language features. Specifically, we (i) propose a decoder optimization\ntechnique for implicit language maps which can be used when new objects appear\non the scene, and (ii) address the problem of inconsistent vision-language\npredictions between different viewing positions. Our experiments demonstrate\nthe effectiveness of LiLMaps and solid improvements in performance.\n", "link": "http://arxiv.org/abs/2501.03304v2", "date": "2025-01-08", "relevancy": 2.322, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiLMaps%3A%20Learnable%20Implicit%20Language%20Maps&body=Title%3A%20LiLMaps%3A%20Learnable%20Implicit%20Language%20Maps%0AAuthor%3A%20Evgenii%20Kruzhkov%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20One%20of%20the%20current%20trends%20in%20robotics%20is%20to%20employ%20large%20language%20models%0A%28LLMs%29%20to%20provide%20non-predefined%20command%20execution%20and%20natural%20human-robot%0Ainteraction.%20It%20is%20useful%20to%20have%20an%20environment%20map%20together%20with%20its%20language%0Arepresentation%2C%20which%20can%20be%20further%20utilized%20by%20LLMs.%20Such%20a%20comprehensive%0Ascene%20representation%20enables%20numerous%20ways%20of%20interaction%20with%20the%20map%20for%0Aautonomously%20operating%20robots.%20In%20this%20work%2C%20we%20present%20an%20approach%20that%0Aenhances%20incremental%20implicit%20mapping%20through%20the%20integration%20of%0Avision-language%20features.%20Specifically%2C%20we%20%28i%29%20propose%20a%20decoder%20optimization%0Atechnique%20for%20implicit%20language%20maps%20which%20can%20be%20used%20when%20new%20objects%20appear%0Aon%20the%20scene%2C%20and%20%28ii%29%20address%20the%20problem%20of%20inconsistent%20vision-language%0Apredictions%20between%20different%20viewing%20positions.%20Our%20experiments%20demonstrate%0Athe%20effectiveness%20of%20LiLMaps%20and%20solid%20improvements%20in%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiLMaps%253A%2520Learnable%2520Implicit%2520Language%2520Maps%26entry.906535625%3DEvgenii%2520Kruzhkov%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520current%2520trends%2520in%2520robotics%2520is%2520to%2520employ%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520provide%2520non-predefined%2520command%2520execution%2520and%2520natural%2520human-robot%250Ainteraction.%2520It%2520is%2520useful%2520to%2520have%2520an%2520environment%2520map%2520together%2520with%2520its%2520language%250Arepresentation%252C%2520which%2520can%2520be%2520further%2520utilized%2520by%2520LLMs.%2520Such%2520a%2520comprehensive%250Ascene%2520representation%2520enables%2520numerous%2520ways%2520of%2520interaction%2520with%2520the%2520map%2520for%250Aautonomously%2520operating%2520robots.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520approach%2520that%250Aenhances%2520incremental%2520implicit%2520mapping%2520through%2520the%2520integration%2520of%250Avision-language%2520features.%2520Specifically%252C%2520we%2520%2528i%2529%2520propose%2520a%2520decoder%2520optimization%250Atechnique%2520for%2520implicit%2520language%2520maps%2520which%2520can%2520be%2520used%2520when%2520new%2520objects%2520appear%250Aon%2520the%2520scene%252C%2520and%2520%2528ii%2529%2520address%2520the%2520problem%2520of%2520inconsistent%2520vision-language%250Apredictions%2520between%2520different%2520viewing%2520positions.%2520Our%2520experiments%2520demonstrate%250Athe%2520effectiveness%2520of%2520LiLMaps%2520and%2520solid%2520improvements%2520in%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiLMaps%3A%20Learnable%20Implicit%20Language%20Maps&entry.906535625=Evgenii%20Kruzhkov%20and%20Sven%20Behnke&entry.1292438233=%20%20One%20of%20the%20current%20trends%20in%20robotics%20is%20to%20employ%20large%20language%20models%0A%28LLMs%29%20to%20provide%20non-predefined%20command%20execution%20and%20natural%20human-robot%0Ainteraction.%20It%20is%20useful%20to%20have%20an%20environment%20map%20together%20with%20its%20language%0Arepresentation%2C%20which%20can%20be%20further%20utilized%20by%20LLMs.%20Such%20a%20comprehensive%0Ascene%20representation%20enables%20numerous%20ways%20of%20interaction%20with%20the%20map%20for%0Aautonomously%20operating%20robots.%20In%20this%20work%2C%20we%20present%20an%20approach%20that%0Aenhances%20incremental%20implicit%20mapping%20through%20the%20integration%20of%0Avision-language%20features.%20Specifically%2C%20we%20%28i%29%20propose%20a%20decoder%20optimization%0Atechnique%20for%20implicit%20language%20maps%20which%20can%20be%20used%20when%20new%20objects%20appear%0Aon%20the%20scene%2C%20and%20%28ii%29%20address%20the%20problem%20of%20inconsistent%20vision-language%0Apredictions%20between%20different%20viewing%20positions.%20Our%20experiments%20demonstrate%0Athe%20effectiveness%20of%20LiLMaps%20and%20solid%20improvements%20in%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03304v2&entry.124074799=Read"},
{"title": "Boosting Column Generation with Graph Neural Networks for Joint Rider\n  Trip Planning and Crew Shift Scheduling", "author": "Jiawei Lu and Tinghan Ye and Wenbo Chen and Pascal Van Hentenryck", "abstract": "  Optimizing service schedules is pivotal to the reliable, efficient, and\ninclusive on-demand mobility. This pressing challenge is further exacerbated by\nthe increasing needs of an aging population, the oversubscription of existing\nservices, and the lack of effective solution methods. This study addresses the\nintricacies of service scheduling, by jointly optimizing rider trip planning\nand crew scheduling for a complex dynamic mobility service. The resulting\noptimization problems are extremely challenging computationally for\nstate-of-the-art methods. To address this fundamental gap, this paper\nintroduces the Joint Rider Trip Planning and Crew Shift Scheduling Problem\n(JRTPCSSP) and a novel solution method, called Attention and Gated GNN-Informed\nColumn Generation (AGGNNI-CG), that hybridizes column generation and machine\nlearning to obtain near-optimal solutions to the JRTPCSSP with real-life\nconstraints of the application. The key idea of the machine-learning component\nis to dramatically reduce the number of paths to explore in the pricing\nproblem, accelerating the most time-consuming component of the column\ngeneration. The machine learning component is a graph neural network with an\nattention mechanism and a gated architecture, which is particularly suited to\ncater for the different input sizes coming from daily operations. AGGNNI-CG has\nbeen applied to a challenging, real-world dataset from the Paratransit system\nof Chatham County in Georgia. It produces substantial improvements compared to\nthe baseline column generation approach, which typically cannot produce\nhigh-quality feasible solutions in reasonable time on large-scale complex\ninstances. AGGNNI-CG also produces significant improvements in service quality\ncompared to the existing system.\n", "link": "http://arxiv.org/abs/2401.03692v3", "date": "2025-01-08", "relevancy": 2.3156, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4725}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4597}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Column%20Generation%20with%20Graph%20Neural%20Networks%20for%20Joint%20Rider%0A%20%20Trip%20Planning%20and%20Crew%20Shift%20Scheduling&body=Title%3A%20Boosting%20Column%20Generation%20with%20Graph%20Neural%20Networks%20for%20Joint%20Rider%0A%20%20Trip%20Planning%20and%20Crew%20Shift%20Scheduling%0AAuthor%3A%20Jiawei%20Lu%20and%20Tinghan%20Ye%20and%20Wenbo%20Chen%20and%20Pascal%20Van%20Hentenryck%0AAbstract%3A%20%20%20Optimizing%20service%20schedules%20is%20pivotal%20to%20the%20reliable%2C%20efficient%2C%20and%0Ainclusive%20on-demand%20mobility.%20This%20pressing%20challenge%20is%20further%20exacerbated%20by%0Athe%20increasing%20needs%20of%20an%20aging%20population%2C%20the%20oversubscription%20of%20existing%0Aservices%2C%20and%20the%20lack%20of%20effective%20solution%20methods.%20This%20study%20addresses%20the%0Aintricacies%20of%20service%20scheduling%2C%20by%20jointly%20optimizing%20rider%20trip%20planning%0Aand%20crew%20scheduling%20for%20a%20complex%20dynamic%20mobility%20service.%20The%20resulting%0Aoptimization%20problems%20are%20extremely%20challenging%20computationally%20for%0Astate-of-the-art%20methods.%20To%20address%20this%20fundamental%20gap%2C%20this%20paper%0Aintroduces%20the%20Joint%20Rider%20Trip%20Planning%20and%20Crew%20Shift%20Scheduling%20Problem%0A%28JRTPCSSP%29%20and%20a%20novel%20solution%20method%2C%20called%20Attention%20and%20Gated%20GNN-Informed%0AColumn%20Generation%20%28AGGNNI-CG%29%2C%20that%20hybridizes%20column%20generation%20and%20machine%0Alearning%20to%20obtain%20near-optimal%20solutions%20to%20the%20JRTPCSSP%20with%20real-life%0Aconstraints%20of%20the%20application.%20The%20key%20idea%20of%20the%20machine-learning%20component%0Ais%20to%20dramatically%20reduce%20the%20number%20of%20paths%20to%20explore%20in%20the%20pricing%0Aproblem%2C%20accelerating%20the%20most%20time-consuming%20component%20of%20the%20column%0Ageneration.%20The%20machine%20learning%20component%20is%20a%20graph%20neural%20network%20with%20an%0Aattention%20mechanism%20and%20a%20gated%20architecture%2C%20which%20is%20particularly%20suited%20to%0Acater%20for%20the%20different%20input%20sizes%20coming%20from%20daily%20operations.%20AGGNNI-CG%20has%0Abeen%20applied%20to%20a%20challenging%2C%20real-world%20dataset%20from%20the%20Paratransit%20system%0Aof%20Chatham%20County%20in%20Georgia.%20It%20produces%20substantial%20improvements%20compared%20to%0Athe%20baseline%20column%20generation%20approach%2C%20which%20typically%20cannot%20produce%0Ahigh-quality%20feasible%20solutions%20in%20reasonable%20time%20on%20large-scale%20complex%0Ainstances.%20AGGNNI-CG%20also%20produces%20significant%20improvements%20in%20service%20quality%0Acompared%20to%20the%20existing%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03692v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Column%2520Generation%2520with%2520Graph%2520Neural%2520Networks%2520for%2520Joint%2520Rider%250A%2520%2520Trip%2520Planning%2520and%2520Crew%2520Shift%2520Scheduling%26entry.906535625%3DJiawei%2520Lu%2520and%2520Tinghan%2520Ye%2520and%2520Wenbo%2520Chen%2520and%2520Pascal%2520Van%2520Hentenryck%26entry.1292438233%3D%2520%2520Optimizing%2520service%2520schedules%2520is%2520pivotal%2520to%2520the%2520reliable%252C%2520efficient%252C%2520and%250Ainclusive%2520on-demand%2520mobility.%2520This%2520pressing%2520challenge%2520is%2520further%2520exacerbated%2520by%250Athe%2520increasing%2520needs%2520of%2520an%2520aging%2520population%252C%2520the%2520oversubscription%2520of%2520existing%250Aservices%252C%2520and%2520the%2520lack%2520of%2520effective%2520solution%2520methods.%2520This%2520study%2520addresses%2520the%250Aintricacies%2520of%2520service%2520scheduling%252C%2520by%2520jointly%2520optimizing%2520rider%2520trip%2520planning%250Aand%2520crew%2520scheduling%2520for%2520a%2520complex%2520dynamic%2520mobility%2520service.%2520The%2520resulting%250Aoptimization%2520problems%2520are%2520extremely%2520challenging%2520computationally%2520for%250Astate-of-the-art%2520methods.%2520To%2520address%2520this%2520fundamental%2520gap%252C%2520this%2520paper%250Aintroduces%2520the%2520Joint%2520Rider%2520Trip%2520Planning%2520and%2520Crew%2520Shift%2520Scheduling%2520Problem%250A%2528JRTPCSSP%2529%2520and%2520a%2520novel%2520solution%2520method%252C%2520called%2520Attention%2520and%2520Gated%2520GNN-Informed%250AColumn%2520Generation%2520%2528AGGNNI-CG%2529%252C%2520that%2520hybridizes%2520column%2520generation%2520and%2520machine%250Alearning%2520to%2520obtain%2520near-optimal%2520solutions%2520to%2520the%2520JRTPCSSP%2520with%2520real-life%250Aconstraints%2520of%2520the%2520application.%2520The%2520key%2520idea%2520of%2520the%2520machine-learning%2520component%250Ais%2520to%2520dramatically%2520reduce%2520the%2520number%2520of%2520paths%2520to%2520explore%2520in%2520the%2520pricing%250Aproblem%252C%2520accelerating%2520the%2520most%2520time-consuming%2520component%2520of%2520the%2520column%250Ageneration.%2520The%2520machine%2520learning%2520component%2520is%2520a%2520graph%2520neural%2520network%2520with%2520an%250Aattention%2520mechanism%2520and%2520a%2520gated%2520architecture%252C%2520which%2520is%2520particularly%2520suited%2520to%250Acater%2520for%2520the%2520different%2520input%2520sizes%2520coming%2520from%2520daily%2520operations.%2520AGGNNI-CG%2520has%250Abeen%2520applied%2520to%2520a%2520challenging%252C%2520real-world%2520dataset%2520from%2520the%2520Paratransit%2520system%250Aof%2520Chatham%2520County%2520in%2520Georgia.%2520It%2520produces%2520substantial%2520improvements%2520compared%2520to%250Athe%2520baseline%2520column%2520generation%2520approach%252C%2520which%2520typically%2520cannot%2520produce%250Ahigh-quality%2520feasible%2520solutions%2520in%2520reasonable%2520time%2520on%2520large-scale%2520complex%250Ainstances.%2520AGGNNI-CG%2520also%2520produces%2520significant%2520improvements%2520in%2520service%2520quality%250Acompared%2520to%2520the%2520existing%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03692v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Column%20Generation%20with%20Graph%20Neural%20Networks%20for%20Joint%20Rider%0A%20%20Trip%20Planning%20and%20Crew%20Shift%20Scheduling&entry.906535625=Jiawei%20Lu%20and%20Tinghan%20Ye%20and%20Wenbo%20Chen%20and%20Pascal%20Van%20Hentenryck&entry.1292438233=%20%20Optimizing%20service%20schedules%20is%20pivotal%20to%20the%20reliable%2C%20efficient%2C%20and%0Ainclusive%20on-demand%20mobility.%20This%20pressing%20challenge%20is%20further%20exacerbated%20by%0Athe%20increasing%20needs%20of%20an%20aging%20population%2C%20the%20oversubscription%20of%20existing%0Aservices%2C%20and%20the%20lack%20of%20effective%20solution%20methods.%20This%20study%20addresses%20the%0Aintricacies%20of%20service%20scheduling%2C%20by%20jointly%20optimizing%20rider%20trip%20planning%0Aand%20crew%20scheduling%20for%20a%20complex%20dynamic%20mobility%20service.%20The%20resulting%0Aoptimization%20problems%20are%20extremely%20challenging%20computationally%20for%0Astate-of-the-art%20methods.%20To%20address%20this%20fundamental%20gap%2C%20this%20paper%0Aintroduces%20the%20Joint%20Rider%20Trip%20Planning%20and%20Crew%20Shift%20Scheduling%20Problem%0A%28JRTPCSSP%29%20and%20a%20novel%20solution%20method%2C%20called%20Attention%20and%20Gated%20GNN-Informed%0AColumn%20Generation%20%28AGGNNI-CG%29%2C%20that%20hybridizes%20column%20generation%20and%20machine%0Alearning%20to%20obtain%20near-optimal%20solutions%20to%20the%20JRTPCSSP%20with%20real-life%0Aconstraints%20of%20the%20application.%20The%20key%20idea%20of%20the%20machine-learning%20component%0Ais%20to%20dramatically%20reduce%20the%20number%20of%20paths%20to%20explore%20in%20the%20pricing%0Aproblem%2C%20accelerating%20the%20most%20time-consuming%20component%20of%20the%20column%0Ageneration.%20The%20machine%20learning%20component%20is%20a%20graph%20neural%20network%20with%20an%0Aattention%20mechanism%20and%20a%20gated%20architecture%2C%20which%20is%20particularly%20suited%20to%0Acater%20for%20the%20different%20input%20sizes%20coming%20from%20daily%20operations.%20AGGNNI-CG%20has%0Abeen%20applied%20to%20a%20challenging%2C%20real-world%20dataset%20from%20the%20Paratransit%20system%0Aof%20Chatham%20County%20in%20Georgia.%20It%20produces%20substantial%20improvements%20compared%20to%0Athe%20baseline%20column%20generation%20approach%2C%20which%20typically%20cannot%20produce%0Ahigh-quality%20feasible%20solutions%20in%20reasonable%20time%20on%20large-scale%20complex%0Ainstances.%20AGGNNI-CG%20also%20produces%20significant%20improvements%20in%20service%20quality%0Acompared%20to%20the%20existing%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03692v3&entry.124074799=Read"},
{"title": "ENCODE: Encoding NetFlows for Network Anomaly Detection", "author": "Clinton Cao and Annibale Panichella and Sicco Verwer and Agathe Blaise and Filippo Rebecchi", "abstract": "  NetFlow data is a popular network log format used by many network analysts\nand researchers. The advantages of using NetFlow over deep packet inspection\nare that it is easier to collect and process, and it is less privacy intrusive.\nMany works have used machine learning to detect network attacks using NetFlow\ndata. The first step for these machine learning pipelines is to pre-process the\ndata before it is given to the machine learning algorithm. Many approaches\nexist to pre-process NetFlow data; however, these simply apply existing methods\nto the data, not considering the specific properties of network data. We argue\nthat for data originating from software systems, such as NetFlow or software\nlogs, similarities in frequency and contexts of feature values are more\nimportant than similarities in the value itself. In this work, we propose an\nencoding algorithm that directly takes the frequency and the context of the\nfeature values into account when the data is being processed. Different types\nof network behaviours can be clustered using this encoding, thus aiding the\nprocess of detecting anomalies within the network. We train several machine\nlearning models for anomaly detection using the data that has been encoded with\nour encoding algorithm. We evaluate the effectiveness of our encoding on a new\ndataset that we created for network attacks on Kubernetes clusters and two\nwell-known public NetFlow datasets. We empirically demonstrate that the machine\nlearning models benefit from using our encoding for anomaly detection.\n", "link": "http://arxiv.org/abs/2207.03890v3", "date": "2025-01-08", "relevancy": 2.3055, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENCODE%3A%20Encoding%20NetFlows%20for%20Network%20Anomaly%20Detection&body=Title%3A%20ENCODE%3A%20Encoding%20NetFlows%20for%20Network%20Anomaly%20Detection%0AAuthor%3A%20Clinton%20Cao%20and%20Annibale%20Panichella%20and%20Sicco%20Verwer%20and%20Agathe%20Blaise%20and%20Filippo%20Rebecchi%0AAbstract%3A%20%20%20NetFlow%20data%20is%20a%20popular%20network%20log%20format%20used%20by%20many%20network%20analysts%0Aand%20researchers.%20The%20advantages%20of%20using%20NetFlow%20over%20deep%20packet%20inspection%0Aare%20that%20it%20is%20easier%20to%20collect%20and%20process%2C%20and%20it%20is%20less%20privacy%20intrusive.%0AMany%20works%20have%20used%20machine%20learning%20to%20detect%20network%20attacks%20using%20NetFlow%0Adata.%20The%20first%20step%20for%20these%20machine%20learning%20pipelines%20is%20to%20pre-process%20the%0Adata%20before%20it%20is%20given%20to%20the%20machine%20learning%20algorithm.%20Many%20approaches%0Aexist%20to%20pre-process%20NetFlow%20data%3B%20however%2C%20these%20simply%20apply%20existing%20methods%0Ato%20the%20data%2C%20not%20considering%20the%20specific%20properties%20of%20network%20data.%20We%20argue%0Athat%20for%20data%20originating%20from%20software%20systems%2C%20such%20as%20NetFlow%20or%20software%0Alogs%2C%20similarities%20in%20frequency%20and%20contexts%20of%20feature%20values%20are%20more%0Aimportant%20than%20similarities%20in%20the%20value%20itself.%20In%20this%20work%2C%20we%20propose%20an%0Aencoding%20algorithm%20that%20directly%20takes%20the%20frequency%20and%20the%20context%20of%20the%0Afeature%20values%20into%20account%20when%20the%20data%20is%20being%20processed.%20Different%20types%0Aof%20network%20behaviours%20can%20be%20clustered%20using%20this%20encoding%2C%20thus%20aiding%20the%0Aprocess%20of%20detecting%20anomalies%20within%20the%20network.%20We%20train%20several%20machine%0Alearning%20models%20for%20anomaly%20detection%20using%20the%20data%20that%20has%20been%20encoded%20with%0Aour%20encoding%20algorithm.%20We%20evaluate%20the%20effectiveness%20of%20our%20encoding%20on%20a%20new%0Adataset%20that%20we%20created%20for%20network%20attacks%20on%20Kubernetes%20clusters%20and%20two%0Awell-known%20public%20NetFlow%20datasets.%20We%20empirically%20demonstrate%20that%20the%20machine%0Alearning%20models%20benefit%20from%20using%20our%20encoding%20for%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.03890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENCODE%253A%2520Encoding%2520NetFlows%2520for%2520Network%2520Anomaly%2520Detection%26entry.906535625%3DClinton%2520Cao%2520and%2520Annibale%2520Panichella%2520and%2520Sicco%2520Verwer%2520and%2520Agathe%2520Blaise%2520and%2520Filippo%2520Rebecchi%26entry.1292438233%3D%2520%2520NetFlow%2520data%2520is%2520a%2520popular%2520network%2520log%2520format%2520used%2520by%2520many%2520network%2520analysts%250Aand%2520researchers.%2520The%2520advantages%2520of%2520using%2520NetFlow%2520over%2520deep%2520packet%2520inspection%250Aare%2520that%2520it%2520is%2520easier%2520to%2520collect%2520and%2520process%252C%2520and%2520it%2520is%2520less%2520privacy%2520intrusive.%250AMany%2520works%2520have%2520used%2520machine%2520learning%2520to%2520detect%2520network%2520attacks%2520using%2520NetFlow%250Adata.%2520The%2520first%2520step%2520for%2520these%2520machine%2520learning%2520pipelines%2520is%2520to%2520pre-process%2520the%250Adata%2520before%2520it%2520is%2520given%2520to%2520the%2520machine%2520learning%2520algorithm.%2520Many%2520approaches%250Aexist%2520to%2520pre-process%2520NetFlow%2520data%253B%2520however%252C%2520these%2520simply%2520apply%2520existing%2520methods%250Ato%2520the%2520data%252C%2520not%2520considering%2520the%2520specific%2520properties%2520of%2520network%2520data.%2520We%2520argue%250Athat%2520for%2520data%2520originating%2520from%2520software%2520systems%252C%2520such%2520as%2520NetFlow%2520or%2520software%250Alogs%252C%2520similarities%2520in%2520frequency%2520and%2520contexts%2520of%2520feature%2520values%2520are%2520more%250Aimportant%2520than%2520similarities%2520in%2520the%2520value%2520itself.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%250Aencoding%2520algorithm%2520that%2520directly%2520takes%2520the%2520frequency%2520and%2520the%2520context%2520of%2520the%250Afeature%2520values%2520into%2520account%2520when%2520the%2520data%2520is%2520being%2520processed.%2520Different%2520types%250Aof%2520network%2520behaviours%2520can%2520be%2520clustered%2520using%2520this%2520encoding%252C%2520thus%2520aiding%2520the%250Aprocess%2520of%2520detecting%2520anomalies%2520within%2520the%2520network.%2520We%2520train%2520several%2520machine%250Alearning%2520models%2520for%2520anomaly%2520detection%2520using%2520the%2520data%2520that%2520has%2520been%2520encoded%2520with%250Aour%2520encoding%2520algorithm.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520encoding%2520on%2520a%2520new%250Adataset%2520that%2520we%2520created%2520for%2520network%2520attacks%2520on%2520Kubernetes%2520clusters%2520and%2520two%250Awell-known%2520public%2520NetFlow%2520datasets.%2520We%2520empirically%2520demonstrate%2520that%2520the%2520machine%250Alearning%2520models%2520benefit%2520from%2520using%2520our%2520encoding%2520for%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.03890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENCODE%3A%20Encoding%20NetFlows%20for%20Network%20Anomaly%20Detection&entry.906535625=Clinton%20Cao%20and%20Annibale%20Panichella%20and%20Sicco%20Verwer%20and%20Agathe%20Blaise%20and%20Filippo%20Rebecchi&entry.1292438233=%20%20NetFlow%20data%20is%20a%20popular%20network%20log%20format%20used%20by%20many%20network%20analysts%0Aand%20researchers.%20The%20advantages%20of%20using%20NetFlow%20over%20deep%20packet%20inspection%0Aare%20that%20it%20is%20easier%20to%20collect%20and%20process%2C%20and%20it%20is%20less%20privacy%20intrusive.%0AMany%20works%20have%20used%20machine%20learning%20to%20detect%20network%20attacks%20using%20NetFlow%0Adata.%20The%20first%20step%20for%20these%20machine%20learning%20pipelines%20is%20to%20pre-process%20the%0Adata%20before%20it%20is%20given%20to%20the%20machine%20learning%20algorithm.%20Many%20approaches%0Aexist%20to%20pre-process%20NetFlow%20data%3B%20however%2C%20these%20simply%20apply%20existing%20methods%0Ato%20the%20data%2C%20not%20considering%20the%20specific%20properties%20of%20network%20data.%20We%20argue%0Athat%20for%20data%20originating%20from%20software%20systems%2C%20such%20as%20NetFlow%20or%20software%0Alogs%2C%20similarities%20in%20frequency%20and%20contexts%20of%20feature%20values%20are%20more%0Aimportant%20than%20similarities%20in%20the%20value%20itself.%20In%20this%20work%2C%20we%20propose%20an%0Aencoding%20algorithm%20that%20directly%20takes%20the%20frequency%20and%20the%20context%20of%20the%0Afeature%20values%20into%20account%20when%20the%20data%20is%20being%20processed.%20Different%20types%0Aof%20network%20behaviours%20can%20be%20clustered%20using%20this%20encoding%2C%20thus%20aiding%20the%0Aprocess%20of%20detecting%20anomalies%20within%20the%20network.%20We%20train%20several%20machine%0Alearning%20models%20for%20anomaly%20detection%20using%20the%20data%20that%20has%20been%20encoded%20with%0Aour%20encoding%20algorithm.%20We%20evaluate%20the%20effectiveness%20of%20our%20encoding%20on%20a%20new%0Adataset%20that%20we%20created%20for%20network%20attacks%20on%20Kubernetes%20clusters%20and%20two%0Awell-known%20public%20NetFlow%20datasets.%20We%20empirically%20demonstrate%20that%20the%20machine%0Alearning%20models%20benefit%20from%20using%20our%20encoding%20for%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.03890v3&entry.124074799=Read"},
{"title": "iFADIT: Invertible Face Anonymization via Disentangled Identity\n  Transform", "author": "Lin Yuan and Kai Liang and Xiong Li and Tao Wu and Nannan Wang and Xinbo Gao", "abstract": "  Face anonymization aims to conceal the visual identity of a face to safeguard\nthe individual's privacy. Traditional methods like blurring and pixelation can\nlargely remove identifying features, but these techniques significantly degrade\nimage quality and are vulnerable to deep reconstruction attacks. Generative\nmodels have emerged as a promising solution for anonymizing faces while\npreserving a natural appearance.However, many still face limitations in visual\nquality and often overlook the potential to recover the original face from the\nanonymized version, which can be valuable in specific contexts such as image\nforensics. This paper proposes a novel framework named iFADIT, an acronym for\nInvertible Face Anonymization via Disentangled Identity Transform.The framework\nfeatures a disentanglement architecture coupled with a secure flow-based model:\nthe former decouples identity information from non-identifying attributes,\nwhile the latter transforms the decoupled identity into an anonymized version\nin an invertible manner controlled by a secret key. The anonymized face can\nthen be reconstructed based on a pre-trained StyleGAN that ensures high image\nquality and realistic facial details. Recovery of the original face (aka\nde-anonymization) is possible upon the availability of the matching secret, by\ninverting the anonymization process based on the same set of model parameters.\nFurthermore, a dedicated secret-key mechanism along with a dual-phase training\nstrategy is devised to ensure the desired properties of face anonymization.\nQualitative and quantitative experiments demonstrate the superiority of the\nproposed approach in anonymity, reversibility, security, diversity, and\ninterpretability over competing methods.\n", "link": "http://arxiv.org/abs/2501.04390v1", "date": "2025-01-08", "relevancy": 2.3044, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6086}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5579}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iFADIT%3A%20Invertible%20Face%20Anonymization%20via%20Disentangled%20Identity%0A%20%20Transform&body=Title%3A%20iFADIT%3A%20Invertible%20Face%20Anonymization%20via%20Disentangled%20Identity%0A%20%20Transform%0AAuthor%3A%20Lin%20Yuan%20and%20Kai%20Liang%20and%20Xiong%20Li%20and%20Tao%20Wu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Face%20anonymization%20aims%20to%20conceal%20the%20visual%20identity%20of%20a%20face%20to%20safeguard%0Athe%20individual%27s%20privacy.%20Traditional%20methods%20like%20blurring%20and%20pixelation%20can%0Alargely%20remove%20identifying%20features%2C%20but%20these%20techniques%20significantly%20degrade%0Aimage%20quality%20and%20are%20vulnerable%20to%20deep%20reconstruction%20attacks.%20Generative%0Amodels%20have%20emerged%20as%20a%20promising%20solution%20for%20anonymizing%20faces%20while%0Apreserving%20a%20natural%20appearance.However%2C%20many%20still%20face%20limitations%20in%20visual%0Aquality%20and%20often%20overlook%20the%20potential%20to%20recover%20the%20original%20face%20from%20the%0Aanonymized%20version%2C%20which%20can%20be%20valuable%20in%20specific%20contexts%20such%20as%20image%0Aforensics.%20This%20paper%20proposes%20a%20novel%20framework%20named%20iFADIT%2C%20an%20acronym%20for%0AInvertible%20Face%20Anonymization%20via%20Disentangled%20Identity%20Transform.The%20framework%0Afeatures%20a%20disentanglement%20architecture%20coupled%20with%20a%20secure%20flow-based%20model%3A%0Athe%20former%20decouples%20identity%20information%20from%20non-identifying%20attributes%2C%0Awhile%20the%20latter%20transforms%20the%20decoupled%20identity%20into%20an%20anonymized%20version%0Ain%20an%20invertible%20manner%20controlled%20by%20a%20secret%20key.%20The%20anonymized%20face%20can%0Athen%20be%20reconstructed%20based%20on%20a%20pre-trained%20StyleGAN%20that%20ensures%20high%20image%0Aquality%20and%20realistic%20facial%20details.%20Recovery%20of%20the%20original%20face%20%28aka%0Ade-anonymization%29%20is%20possible%20upon%20the%20availability%20of%20the%20matching%20secret%2C%20by%0Ainverting%20the%20anonymization%20process%20based%20on%20the%20same%20set%20of%20model%20parameters.%0AFurthermore%2C%20a%20dedicated%20secret-key%20mechanism%20along%20with%20a%20dual-phase%20training%0Astrategy%20is%20devised%20to%20ensure%20the%20desired%20properties%20of%20face%20anonymization.%0AQualitative%20and%20quantitative%20experiments%20demonstrate%20the%20superiority%20of%20the%0Aproposed%20approach%20in%20anonymity%2C%20reversibility%2C%20security%2C%20diversity%2C%20and%0Ainterpretability%20over%20competing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiFADIT%253A%2520Invertible%2520Face%2520Anonymization%2520via%2520Disentangled%2520Identity%250A%2520%2520Transform%26entry.906535625%3DLin%2520Yuan%2520and%2520Kai%2520Liang%2520and%2520Xiong%2520Li%2520and%2520Tao%2520Wu%2520and%2520Nannan%2520Wang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Face%2520anonymization%2520aims%2520to%2520conceal%2520the%2520visual%2520identity%2520of%2520a%2520face%2520to%2520safeguard%250Athe%2520individual%2527s%2520privacy.%2520Traditional%2520methods%2520like%2520blurring%2520and%2520pixelation%2520can%250Alargely%2520remove%2520identifying%2520features%252C%2520but%2520these%2520techniques%2520significantly%2520degrade%250Aimage%2520quality%2520and%2520are%2520vulnerable%2520to%2520deep%2520reconstruction%2520attacks.%2520Generative%250Amodels%2520have%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520anonymizing%2520faces%2520while%250Apreserving%2520a%2520natural%2520appearance.However%252C%2520many%2520still%2520face%2520limitations%2520in%2520visual%250Aquality%2520and%2520often%2520overlook%2520the%2520potential%2520to%2520recover%2520the%2520original%2520face%2520from%2520the%250Aanonymized%2520version%252C%2520which%2520can%2520be%2520valuable%2520in%2520specific%2520contexts%2520such%2520as%2520image%250Aforensics.%2520This%2520paper%2520proposes%2520a%2520novel%2520framework%2520named%2520iFADIT%252C%2520an%2520acronym%2520for%250AInvertible%2520Face%2520Anonymization%2520via%2520Disentangled%2520Identity%2520Transform.The%2520framework%250Afeatures%2520a%2520disentanglement%2520architecture%2520coupled%2520with%2520a%2520secure%2520flow-based%2520model%253A%250Athe%2520former%2520decouples%2520identity%2520information%2520from%2520non-identifying%2520attributes%252C%250Awhile%2520the%2520latter%2520transforms%2520the%2520decoupled%2520identity%2520into%2520an%2520anonymized%2520version%250Ain%2520an%2520invertible%2520manner%2520controlled%2520by%2520a%2520secret%2520key.%2520The%2520anonymized%2520face%2520can%250Athen%2520be%2520reconstructed%2520based%2520on%2520a%2520pre-trained%2520StyleGAN%2520that%2520ensures%2520high%2520image%250Aquality%2520and%2520realistic%2520facial%2520details.%2520Recovery%2520of%2520the%2520original%2520face%2520%2528aka%250Ade-anonymization%2529%2520is%2520possible%2520upon%2520the%2520availability%2520of%2520the%2520matching%2520secret%252C%2520by%250Ainverting%2520the%2520anonymization%2520process%2520based%2520on%2520the%2520same%2520set%2520of%2520model%2520parameters.%250AFurthermore%252C%2520a%2520dedicated%2520secret-key%2520mechanism%2520along%2520with%2520a%2520dual-phase%2520training%250Astrategy%2520is%2520devised%2520to%2520ensure%2520the%2520desired%2520properties%2520of%2520face%2520anonymization.%250AQualitative%2520and%2520quantitative%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520the%250Aproposed%2520approach%2520in%2520anonymity%252C%2520reversibility%252C%2520security%252C%2520diversity%252C%2520and%250Ainterpretability%2520over%2520competing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iFADIT%3A%20Invertible%20Face%20Anonymization%20via%20Disentangled%20Identity%0A%20%20Transform&entry.906535625=Lin%20Yuan%20and%20Kai%20Liang%20and%20Xiong%20Li%20and%20Tao%20Wu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Face%20anonymization%20aims%20to%20conceal%20the%20visual%20identity%20of%20a%20face%20to%20safeguard%0Athe%20individual%27s%20privacy.%20Traditional%20methods%20like%20blurring%20and%20pixelation%20can%0Alargely%20remove%20identifying%20features%2C%20but%20these%20techniques%20significantly%20degrade%0Aimage%20quality%20and%20are%20vulnerable%20to%20deep%20reconstruction%20attacks.%20Generative%0Amodels%20have%20emerged%20as%20a%20promising%20solution%20for%20anonymizing%20faces%20while%0Apreserving%20a%20natural%20appearance.However%2C%20many%20still%20face%20limitations%20in%20visual%0Aquality%20and%20often%20overlook%20the%20potential%20to%20recover%20the%20original%20face%20from%20the%0Aanonymized%20version%2C%20which%20can%20be%20valuable%20in%20specific%20contexts%20such%20as%20image%0Aforensics.%20This%20paper%20proposes%20a%20novel%20framework%20named%20iFADIT%2C%20an%20acronym%20for%0AInvertible%20Face%20Anonymization%20via%20Disentangled%20Identity%20Transform.The%20framework%0Afeatures%20a%20disentanglement%20architecture%20coupled%20with%20a%20secure%20flow-based%20model%3A%0Athe%20former%20decouples%20identity%20information%20from%20non-identifying%20attributes%2C%0Awhile%20the%20latter%20transforms%20the%20decoupled%20identity%20into%20an%20anonymized%20version%0Ain%20an%20invertible%20manner%20controlled%20by%20a%20secret%20key.%20The%20anonymized%20face%20can%0Athen%20be%20reconstructed%20based%20on%20a%20pre-trained%20StyleGAN%20that%20ensures%20high%20image%0Aquality%20and%20realistic%20facial%20details.%20Recovery%20of%20the%20original%20face%20%28aka%0Ade-anonymization%29%20is%20possible%20upon%20the%20availability%20of%20the%20matching%20secret%2C%20by%0Ainverting%20the%20anonymization%20process%20based%20on%20the%20same%20set%20of%20model%20parameters.%0AFurthermore%2C%20a%20dedicated%20secret-key%20mechanism%20along%20with%20a%20dual-phase%20training%0Astrategy%20is%20devised%20to%20ensure%20the%20desired%20properties%20of%20face%20anonymization.%0AQualitative%20and%20quantitative%20experiments%20demonstrate%20the%20superiority%20of%20the%0Aproposed%20approach%20in%20anonymity%2C%20reversibility%2C%20security%2C%20diversity%2C%20and%0Ainterpretability%20over%20competing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04390v1&entry.124074799=Read"},
{"title": "Rethinking High-speed Image Reconstruction Framework with Spike Camera", "author": "Kang Chen and Yajing Zheng and Tiejun Huang and Zhaofei Yu", "abstract": "  Spike cameras, as innovative neuromorphic devices, generate continuous spike\nstreams to capture high-speed scenes with lower bandwidth and higher dynamic\nrange than traditional RGB cameras. However, reconstructing high-quality images\nfrom the spike input under low-light conditions remains challenging.\nConventional learning-based methods often rely on the synthetic dataset as the\nsupervision for training. Still, these approaches falter when dealing with\nnoisy spikes fired under the low-light environment, leading to further\nperformance degradation in the real-world dataset. This phenomenon is primarily\ndue to inadequate noise modelling and the domain gap between synthetic and real\ndatasets, resulting in recovered images with unclear textures, excessive noise,\nand diminished brightness. To address these challenges, we introduce a novel\nspike-to-image reconstruction framework SpikeCLIP that goes beyond traditional\ntraining paradigms. Leveraging the CLIP model's powerful capability to align\ntext and images, we incorporate the textual description of the captured scene\nand unpaired high-quality datasets as the supervision. Our experiments on\nreal-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP\nsignificantly enhances texture details and the luminance balance of recovered\nimages. Furthermore, the reconstructed images are well-aligned with the broader\nvisual features needed for downstream tasks, ensuring more robust and versatile\nperformance in challenging environments.\n", "link": "http://arxiv.org/abs/2501.04477v1", "date": "2025-01-08", "relevancy": 2.2963, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6174}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.544}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20High-speed%20Image%20Reconstruction%20Framework%20with%20Spike%20Camera&body=Title%3A%20Rethinking%20High-speed%20Image%20Reconstruction%20Framework%20with%20Spike%20Camera%0AAuthor%3A%20Kang%20Chen%20and%20Yajing%20Zheng%20and%20Tiejun%20Huang%20and%20Zhaofei%20Yu%0AAbstract%3A%20%20%20Spike%20cameras%2C%20as%20innovative%20neuromorphic%20devices%2C%20generate%20continuous%20spike%0Astreams%20to%20capture%20high-speed%20scenes%20with%20lower%20bandwidth%20and%20higher%20dynamic%0Arange%20than%20traditional%20RGB%20cameras.%20However%2C%20reconstructing%20high-quality%20images%0Afrom%20the%20spike%20input%20under%20low-light%20conditions%20remains%20challenging.%0AConventional%20learning-based%20methods%20often%20rely%20on%20the%20synthetic%20dataset%20as%20the%0Asupervision%20for%20training.%20Still%2C%20these%20approaches%20falter%20when%20dealing%20with%0Anoisy%20spikes%20fired%20under%20the%20low-light%20environment%2C%20leading%20to%20further%0Aperformance%20degradation%20in%20the%20real-world%20dataset.%20This%20phenomenon%20is%20primarily%0Adue%20to%20inadequate%20noise%20modelling%20and%20the%20domain%20gap%20between%20synthetic%20and%20real%0Adatasets%2C%20resulting%20in%20recovered%20images%20with%20unclear%20textures%2C%20excessive%20noise%2C%0Aand%20diminished%20brightness.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%0Aspike-to-image%20reconstruction%20framework%20SpikeCLIP%20that%20goes%20beyond%20traditional%0Atraining%20paradigms.%20Leveraging%20the%20CLIP%20model%27s%20powerful%20capability%20to%20align%0Atext%20and%20images%2C%20we%20incorporate%20the%20textual%20description%20of%20the%20captured%20scene%0Aand%20unpaired%20high-quality%20datasets%20as%20the%20supervision.%20Our%20experiments%20on%0Areal-world%20low-light%20datasets%20U-CALTECH%20and%20U-CIFAR%20demonstrate%20that%20SpikeCLIP%0Asignificantly%20enhances%20texture%20details%20and%20the%20luminance%20balance%20of%20recovered%0Aimages.%20Furthermore%2C%20the%20reconstructed%20images%20are%20well-aligned%20with%20the%20broader%0Avisual%20features%20needed%20for%20downstream%20tasks%2C%20ensuring%20more%20robust%20and%20versatile%0Aperformance%20in%20challenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520High-speed%2520Image%2520Reconstruction%2520Framework%2520with%2520Spike%2520Camera%26entry.906535625%3DKang%2520Chen%2520and%2520Yajing%2520Zheng%2520and%2520Tiejun%2520Huang%2520and%2520Zhaofei%2520Yu%26entry.1292438233%3D%2520%2520Spike%2520cameras%252C%2520as%2520innovative%2520neuromorphic%2520devices%252C%2520generate%2520continuous%2520spike%250Astreams%2520to%2520capture%2520high-speed%2520scenes%2520with%2520lower%2520bandwidth%2520and%2520higher%2520dynamic%250Arange%2520than%2520traditional%2520RGB%2520cameras.%2520However%252C%2520reconstructing%2520high-quality%2520images%250Afrom%2520the%2520spike%2520input%2520under%2520low-light%2520conditions%2520remains%2520challenging.%250AConventional%2520learning-based%2520methods%2520often%2520rely%2520on%2520the%2520synthetic%2520dataset%2520as%2520the%250Asupervision%2520for%2520training.%2520Still%252C%2520these%2520approaches%2520falter%2520when%2520dealing%2520with%250Anoisy%2520spikes%2520fired%2520under%2520the%2520low-light%2520environment%252C%2520leading%2520to%2520further%250Aperformance%2520degradation%2520in%2520the%2520real-world%2520dataset.%2520This%2520phenomenon%2520is%2520primarily%250Adue%2520to%2520inadequate%2520noise%2520modelling%2520and%2520the%2520domain%2520gap%2520between%2520synthetic%2520and%2520real%250Adatasets%252C%2520resulting%2520in%2520recovered%2520images%2520with%2520unclear%2520textures%252C%2520excessive%2520noise%252C%250Aand%2520diminished%2520brightness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%250Aspike-to-image%2520reconstruction%2520framework%2520SpikeCLIP%2520that%2520goes%2520beyond%2520traditional%250Atraining%2520paradigms.%2520Leveraging%2520the%2520CLIP%2520model%2527s%2520powerful%2520capability%2520to%2520align%250Atext%2520and%2520images%252C%2520we%2520incorporate%2520the%2520textual%2520description%2520of%2520the%2520captured%2520scene%250Aand%2520unpaired%2520high-quality%2520datasets%2520as%2520the%2520supervision.%2520Our%2520experiments%2520on%250Areal-world%2520low-light%2520datasets%2520U-CALTECH%2520and%2520U-CIFAR%2520demonstrate%2520that%2520SpikeCLIP%250Asignificantly%2520enhances%2520texture%2520details%2520and%2520the%2520luminance%2520balance%2520of%2520recovered%250Aimages.%2520Furthermore%252C%2520the%2520reconstructed%2520images%2520are%2520well-aligned%2520with%2520the%2520broader%250Avisual%2520features%2520needed%2520for%2520downstream%2520tasks%252C%2520ensuring%2520more%2520robust%2520and%2520versatile%250Aperformance%2520in%2520challenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20High-speed%20Image%20Reconstruction%20Framework%20with%20Spike%20Camera&entry.906535625=Kang%20Chen%20and%20Yajing%20Zheng%20and%20Tiejun%20Huang%20and%20Zhaofei%20Yu&entry.1292438233=%20%20Spike%20cameras%2C%20as%20innovative%20neuromorphic%20devices%2C%20generate%20continuous%20spike%0Astreams%20to%20capture%20high-speed%20scenes%20with%20lower%20bandwidth%20and%20higher%20dynamic%0Arange%20than%20traditional%20RGB%20cameras.%20However%2C%20reconstructing%20high-quality%20images%0Afrom%20the%20spike%20input%20under%20low-light%20conditions%20remains%20challenging.%0AConventional%20learning-based%20methods%20often%20rely%20on%20the%20synthetic%20dataset%20as%20the%0Asupervision%20for%20training.%20Still%2C%20these%20approaches%20falter%20when%20dealing%20with%0Anoisy%20spikes%20fired%20under%20the%20low-light%20environment%2C%20leading%20to%20further%0Aperformance%20degradation%20in%20the%20real-world%20dataset.%20This%20phenomenon%20is%20primarily%0Adue%20to%20inadequate%20noise%20modelling%20and%20the%20domain%20gap%20between%20synthetic%20and%20real%0Adatasets%2C%20resulting%20in%20recovered%20images%20with%20unclear%20textures%2C%20excessive%20noise%2C%0Aand%20diminished%20brightness.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%0Aspike-to-image%20reconstruction%20framework%20SpikeCLIP%20that%20goes%20beyond%20traditional%0Atraining%20paradigms.%20Leveraging%20the%20CLIP%20model%27s%20powerful%20capability%20to%20align%0Atext%20and%20images%2C%20we%20incorporate%20the%20textual%20description%20of%20the%20captured%20scene%0Aand%20unpaired%20high-quality%20datasets%20as%20the%20supervision.%20Our%20experiments%20on%0Areal-world%20low-light%20datasets%20U-CALTECH%20and%20U-CIFAR%20demonstrate%20that%20SpikeCLIP%0Asignificantly%20enhances%20texture%20details%20and%20the%20luminance%20balance%20of%20recovered%0Aimages.%20Furthermore%2C%20the%20reconstructed%20images%20are%20well-aligned%20with%20the%20broader%0Avisual%20features%20needed%20for%20downstream%20tasks%2C%20ensuring%20more%20robust%20and%20versatile%0Aperformance%20in%20challenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04477v1&entry.124074799=Read"},
{"title": "Boosting Salient Object Detection with Knowledge Distillated from Large\n  Foundation Models", "author": "Miaoyang He and Shuyong Gao and Tsui Qin Mok and Weifeng Ge and Wengqiang Zhang", "abstract": "  Salient Object Detection (SOD) aims to identify and segment prominent regions\nwithin a scene. Traditional models rely on manually annotated pseudo labels\nwith precise pixel-level accuracy, which is time-consuming. We developed a\nlow-cost, high-precision annotation method by leveraging large foundation\nmodels to address the challenges. Specifically, we use a weakly supervised\napproach to guide large models in generating pseudo-labels through textual\nprompts. Since large models do not effectively focus on the salient regions of\nimages, we manually annotate a subset of text to fine-tune the model. Based on\nthis approach, which enables precise and rapid generation of pseudo-labels, we\nintroduce a new dataset, BDS-TR. Compared to the previous DUTS-TR dataset,\nBDS-TR is more prominent in scale and encompasses a wider variety of categories\nand scenes. This expansion will enhance our model's applicability across a\nbroader range of scenarios and provide a more comprehensive foundational\ndataset for future SOD research. Additionally, we present an edge decoder based\non dynamic upsampling, which focuses on object edges while gradually recovering\nimage feature resolution. Comprehensive experiments on five benchmark datasets\ndemonstrate that our method significantly outperforms state-of-the-art\napproaches and also surpasses several existing fully-supervised SOD methods.\nThe code and results will be made available.\n", "link": "http://arxiv.org/abs/2501.04582v1", "date": "2025-01-08", "relevancy": 2.2936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Salient%20Object%20Detection%20with%20Knowledge%20Distillated%20from%20Large%0A%20%20Foundation%20Models&body=Title%3A%20Boosting%20Salient%20Object%20Detection%20with%20Knowledge%20Distillated%20from%20Large%0A%20%20Foundation%20Models%0AAuthor%3A%20Miaoyang%20He%20and%20Shuyong%20Gao%20and%20Tsui%20Qin%20Mok%20and%20Weifeng%20Ge%20and%20Wengqiang%20Zhang%0AAbstract%3A%20%20%20Salient%20Object%20Detection%20%28SOD%29%20aims%20to%20identify%20and%20segment%20prominent%20regions%0Awithin%20a%20scene.%20Traditional%20models%20rely%20on%20manually%20annotated%20pseudo%20labels%0Awith%20precise%20pixel-level%20accuracy%2C%20which%20is%20time-consuming.%20We%20developed%20a%0Alow-cost%2C%20high-precision%20annotation%20method%20by%20leveraging%20large%20foundation%0Amodels%20to%20address%20the%20challenges.%20Specifically%2C%20we%20use%20a%20weakly%20supervised%0Aapproach%20to%20guide%20large%20models%20in%20generating%20pseudo-labels%20through%20textual%0Aprompts.%20Since%20large%20models%20do%20not%20effectively%20focus%20on%20the%20salient%20regions%20of%0Aimages%2C%20we%20manually%20annotate%20a%20subset%20of%20text%20to%20fine-tune%20the%20model.%20Based%20on%0Athis%20approach%2C%20which%20enables%20precise%20and%20rapid%20generation%20of%20pseudo-labels%2C%20we%0Aintroduce%20a%20new%20dataset%2C%20BDS-TR.%20Compared%20to%20the%20previous%20DUTS-TR%20dataset%2C%0ABDS-TR%20is%20more%20prominent%20in%20scale%20and%20encompasses%20a%20wider%20variety%20of%20categories%0Aand%20scenes.%20This%20expansion%20will%20enhance%20our%20model%27s%20applicability%20across%20a%0Abroader%20range%20of%20scenarios%20and%20provide%20a%20more%20comprehensive%20foundational%0Adataset%20for%20future%20SOD%20research.%20Additionally%2C%20we%20present%20an%20edge%20decoder%20based%0Aon%20dynamic%20upsampling%2C%20which%20focuses%20on%20object%20edges%20while%20gradually%20recovering%0Aimage%20feature%20resolution.%20Comprehensive%20experiments%20on%20five%20benchmark%20datasets%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aapproaches%20and%20also%20surpasses%20several%20existing%20fully-supervised%20SOD%20methods.%0AThe%20code%20and%20results%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Salient%2520Object%2520Detection%2520with%2520Knowledge%2520Distillated%2520from%2520Large%250A%2520%2520Foundation%2520Models%26entry.906535625%3DMiaoyang%2520He%2520and%2520Shuyong%2520Gao%2520and%2520Tsui%2520Qin%2520Mok%2520and%2520Weifeng%2520Ge%2520and%2520Wengqiang%2520Zhang%26entry.1292438233%3D%2520%2520Salient%2520Object%2520Detection%2520%2528SOD%2529%2520aims%2520to%2520identify%2520and%2520segment%2520prominent%2520regions%250Awithin%2520a%2520scene.%2520Traditional%2520models%2520rely%2520on%2520manually%2520annotated%2520pseudo%2520labels%250Awith%2520precise%2520pixel-level%2520accuracy%252C%2520which%2520is%2520time-consuming.%2520We%2520developed%2520a%250Alow-cost%252C%2520high-precision%2520annotation%2520method%2520by%2520leveraging%2520large%2520foundation%250Amodels%2520to%2520address%2520the%2520challenges.%2520Specifically%252C%2520we%2520use%2520a%2520weakly%2520supervised%250Aapproach%2520to%2520guide%2520large%2520models%2520in%2520generating%2520pseudo-labels%2520through%2520textual%250Aprompts.%2520Since%2520large%2520models%2520do%2520not%2520effectively%2520focus%2520on%2520the%2520salient%2520regions%2520of%250Aimages%252C%2520we%2520manually%2520annotate%2520a%2520subset%2520of%2520text%2520to%2520fine-tune%2520the%2520model.%2520Based%2520on%250Athis%2520approach%252C%2520which%2520enables%2520precise%2520and%2520rapid%2520generation%2520of%2520pseudo-labels%252C%2520we%250Aintroduce%2520a%2520new%2520dataset%252C%2520BDS-TR.%2520Compared%2520to%2520the%2520previous%2520DUTS-TR%2520dataset%252C%250ABDS-TR%2520is%2520more%2520prominent%2520in%2520scale%2520and%2520encompasses%2520a%2520wider%2520variety%2520of%2520categories%250Aand%2520scenes.%2520This%2520expansion%2520will%2520enhance%2520our%2520model%2527s%2520applicability%2520across%2520a%250Abroader%2520range%2520of%2520scenarios%2520and%2520provide%2520a%2520more%2520comprehensive%2520foundational%250Adataset%2520for%2520future%2520SOD%2520research.%2520Additionally%252C%2520we%2520present%2520an%2520edge%2520decoder%2520based%250Aon%2520dynamic%2520upsampling%252C%2520which%2520focuses%2520on%2520object%2520edges%2520while%2520gradually%2520recovering%250Aimage%2520feature%2520resolution.%2520Comprehensive%2520experiments%2520on%2520five%2520benchmark%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%250Aapproaches%2520and%2520also%2520surpasses%2520several%2520existing%2520fully-supervised%2520SOD%2520methods.%250AThe%2520code%2520and%2520results%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Salient%20Object%20Detection%20with%20Knowledge%20Distillated%20from%20Large%0A%20%20Foundation%20Models&entry.906535625=Miaoyang%20He%20and%20Shuyong%20Gao%20and%20Tsui%20Qin%20Mok%20and%20Weifeng%20Ge%20and%20Wengqiang%20Zhang&entry.1292438233=%20%20Salient%20Object%20Detection%20%28SOD%29%20aims%20to%20identify%20and%20segment%20prominent%20regions%0Awithin%20a%20scene.%20Traditional%20models%20rely%20on%20manually%20annotated%20pseudo%20labels%0Awith%20precise%20pixel-level%20accuracy%2C%20which%20is%20time-consuming.%20We%20developed%20a%0Alow-cost%2C%20high-precision%20annotation%20method%20by%20leveraging%20large%20foundation%0Amodels%20to%20address%20the%20challenges.%20Specifically%2C%20we%20use%20a%20weakly%20supervised%0Aapproach%20to%20guide%20large%20models%20in%20generating%20pseudo-labels%20through%20textual%0Aprompts.%20Since%20large%20models%20do%20not%20effectively%20focus%20on%20the%20salient%20regions%20of%0Aimages%2C%20we%20manually%20annotate%20a%20subset%20of%20text%20to%20fine-tune%20the%20model.%20Based%20on%0Athis%20approach%2C%20which%20enables%20precise%20and%20rapid%20generation%20of%20pseudo-labels%2C%20we%0Aintroduce%20a%20new%20dataset%2C%20BDS-TR.%20Compared%20to%20the%20previous%20DUTS-TR%20dataset%2C%0ABDS-TR%20is%20more%20prominent%20in%20scale%20and%20encompasses%20a%20wider%20variety%20of%20categories%0Aand%20scenes.%20This%20expansion%20will%20enhance%20our%20model%27s%20applicability%20across%20a%0Abroader%20range%20of%20scenarios%20and%20provide%20a%20more%20comprehensive%20foundational%0Adataset%20for%20future%20SOD%20research.%20Additionally%2C%20we%20present%20an%20edge%20decoder%20based%0Aon%20dynamic%20upsampling%2C%20which%20focuses%20on%20object%20edges%20while%20gradually%20recovering%0Aimage%20feature%20resolution.%20Comprehensive%20experiments%20on%20five%20benchmark%20datasets%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aapproaches%20and%20also%20surpasses%20several%20existing%20fully-supervised%20SOD%20methods.%0AThe%20code%20and%20results%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04582v1&entry.124074799=Read"},
{"title": "ViG-Bias: Visually Grounded Bias Discovery and Mitigation", "author": "Badr-Eddine Marani and Mohamed Hanini and Nihitha Malayarukil and Stergios Christodoulidis and Maria Vakalopoulou and Enzo Ferrante", "abstract": "  The proliferation of machine learning models in critical decision making\nprocesses has underscored the need for bias discovery and mitigation\nstrategies. Identifying the reasons behind a biased system is not\nstraightforward, since in many occasions they are associated with hidden\nspurious correlations which are not easy to spot. Standard approaches rely on\nbias audits performed by analyzing model performance in pre-defined subgroups\nof data samples, usually characterized by common attributes like gender or\nethnicity when it comes to people, or other specific attributes defining\nsemantically coherent groups of images. However, it is not always possible to\nknow a-priori the specific attributes defining the failure modes of visual\nrecognition systems. Recent approaches propose to discover these groups by\nleveraging large vision language models, which enable the extraction of\ncross-modal embeddings and the generation of textual descriptions to\ncharacterize the subgroups where a certain model is underperforming. In this\nwork, we argue that incorporating visual explanations (e.g. heatmaps generated\nvia GradCAM or other approaches) can boost the performance of such bias\ndiscovery and mitigation frameworks. To this end, we introduce Visually\nGrounded Bias Discovery and Mitigation (ViG-Bias), a simple yet effective\ntechnique which can be integrated to a variety of existing frameworks to\nimprove both, discovery and mitigation performance. Our comprehensive\nevaluation shows that incorporating visual explanations enhances existing\ntechniques like DOMINO, FACTS and Bias-to-Text, across several challenging\ndatasets, including CelebA, Waterbirds, and NICO++.\n", "link": "http://arxiv.org/abs/2407.01996v4", "date": "2025-01-08", "relevancy": 2.2714, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5792}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViG-Bias%3A%20Visually%20Grounded%20Bias%20Discovery%20and%20Mitigation&body=Title%3A%20ViG-Bias%3A%20Visually%20Grounded%20Bias%20Discovery%20and%20Mitigation%0AAuthor%3A%20Badr-Eddine%20Marani%20and%20Mohamed%20Hanini%20and%20Nihitha%20Malayarukil%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%20and%20Enzo%20Ferrante%0AAbstract%3A%20%20%20The%20proliferation%20of%20machine%20learning%20models%20in%20critical%20decision%20making%0Aprocesses%20has%20underscored%20the%20need%20for%20bias%20discovery%20and%20mitigation%0Astrategies.%20Identifying%20the%20reasons%20behind%20a%20biased%20system%20is%20not%0Astraightforward%2C%20since%20in%20many%20occasions%20they%20are%20associated%20with%20hidden%0Aspurious%20correlations%20which%20are%20not%20easy%20to%20spot.%20Standard%20approaches%20rely%20on%0Abias%20audits%20performed%20by%20analyzing%20model%20performance%20in%20pre-defined%20subgroups%0Aof%20data%20samples%2C%20usually%20characterized%20by%20common%20attributes%20like%20gender%20or%0Aethnicity%20when%20it%20comes%20to%20people%2C%20or%20other%20specific%20attributes%20defining%0Asemantically%20coherent%20groups%20of%20images.%20However%2C%20it%20is%20not%20always%20possible%20to%0Aknow%20a-priori%20the%20specific%20attributes%20defining%20the%20failure%20modes%20of%20visual%0Arecognition%20systems.%20Recent%20approaches%20propose%20to%20discover%20these%20groups%20by%0Aleveraging%20large%20vision%20language%20models%2C%20which%20enable%20the%20extraction%20of%0Across-modal%20embeddings%20and%20the%20generation%20of%20textual%20descriptions%20to%0Acharacterize%20the%20subgroups%20where%20a%20certain%20model%20is%20underperforming.%20In%20this%0Awork%2C%20we%20argue%20that%20incorporating%20visual%20explanations%20%28e.g.%20heatmaps%20generated%0Avia%20GradCAM%20or%20other%20approaches%29%20can%20boost%20the%20performance%20of%20such%20bias%0Adiscovery%20and%20mitigation%20frameworks.%20To%20this%20end%2C%20we%20introduce%20Visually%0AGrounded%20Bias%20Discovery%20and%20Mitigation%20%28ViG-Bias%29%2C%20a%20simple%20yet%20effective%0Atechnique%20which%20can%20be%20integrated%20to%20a%20variety%20of%20existing%20frameworks%20to%0Aimprove%20both%2C%20discovery%20and%20mitigation%20performance.%20Our%20comprehensive%0Aevaluation%20shows%20that%20incorporating%20visual%20explanations%20enhances%20existing%0Atechniques%20like%20DOMINO%2C%20FACTS%20and%20Bias-to-Text%2C%20across%20several%20challenging%0Adatasets%2C%20including%20CelebA%2C%20Waterbirds%2C%20and%20NICO%2B%2B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01996v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViG-Bias%253A%2520Visually%2520Grounded%2520Bias%2520Discovery%2520and%2520Mitigation%26entry.906535625%3DBadr-Eddine%2520Marani%2520and%2520Mohamed%2520Hanini%2520and%2520Nihitha%2520Malayarukil%2520and%2520Stergios%2520Christodoulidis%2520and%2520Maria%2520Vakalopoulou%2520and%2520Enzo%2520Ferrante%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520machine%2520learning%2520models%2520in%2520critical%2520decision%2520making%250Aprocesses%2520has%2520underscored%2520the%2520need%2520for%2520bias%2520discovery%2520and%2520mitigation%250Astrategies.%2520Identifying%2520the%2520reasons%2520behind%2520a%2520biased%2520system%2520is%2520not%250Astraightforward%252C%2520since%2520in%2520many%2520occasions%2520they%2520are%2520associated%2520with%2520hidden%250Aspurious%2520correlations%2520which%2520are%2520not%2520easy%2520to%2520spot.%2520Standard%2520approaches%2520rely%2520on%250Abias%2520audits%2520performed%2520by%2520analyzing%2520model%2520performance%2520in%2520pre-defined%2520subgroups%250Aof%2520data%2520samples%252C%2520usually%2520characterized%2520by%2520common%2520attributes%2520like%2520gender%2520or%250Aethnicity%2520when%2520it%2520comes%2520to%2520people%252C%2520or%2520other%2520specific%2520attributes%2520defining%250Asemantically%2520coherent%2520groups%2520of%2520images.%2520However%252C%2520it%2520is%2520not%2520always%2520possible%2520to%250Aknow%2520a-priori%2520the%2520specific%2520attributes%2520defining%2520the%2520failure%2520modes%2520of%2520visual%250Arecognition%2520systems.%2520Recent%2520approaches%2520propose%2520to%2520discover%2520these%2520groups%2520by%250Aleveraging%2520large%2520vision%2520language%2520models%252C%2520which%2520enable%2520the%2520extraction%2520of%250Across-modal%2520embeddings%2520and%2520the%2520generation%2520of%2520textual%2520descriptions%2520to%250Acharacterize%2520the%2520subgroups%2520where%2520a%2520certain%2520model%2520is%2520underperforming.%2520In%2520this%250Awork%252C%2520we%2520argue%2520that%2520incorporating%2520visual%2520explanations%2520%2528e.g.%2520heatmaps%2520generated%250Avia%2520GradCAM%2520or%2520other%2520approaches%2529%2520can%2520boost%2520the%2520performance%2520of%2520such%2520bias%250Adiscovery%2520and%2520mitigation%2520frameworks.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Visually%250AGrounded%2520Bias%2520Discovery%2520and%2520Mitigation%2520%2528ViG-Bias%2529%252C%2520a%2520simple%2520yet%2520effective%250Atechnique%2520which%2520can%2520be%2520integrated%2520to%2520a%2520variety%2520of%2520existing%2520frameworks%2520to%250Aimprove%2520both%252C%2520discovery%2520and%2520mitigation%2520performance.%2520Our%2520comprehensive%250Aevaluation%2520shows%2520that%2520incorporating%2520visual%2520explanations%2520enhances%2520existing%250Atechniques%2520like%2520DOMINO%252C%2520FACTS%2520and%2520Bias-to-Text%252C%2520across%2520several%2520challenging%250Adatasets%252C%2520including%2520CelebA%252C%2520Waterbirds%252C%2520and%2520NICO%252B%252B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01996v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViG-Bias%3A%20Visually%20Grounded%20Bias%20Discovery%20and%20Mitigation&entry.906535625=Badr-Eddine%20Marani%20and%20Mohamed%20Hanini%20and%20Nihitha%20Malayarukil%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%20and%20Enzo%20Ferrante&entry.1292438233=%20%20The%20proliferation%20of%20machine%20learning%20models%20in%20critical%20decision%20making%0Aprocesses%20has%20underscored%20the%20need%20for%20bias%20discovery%20and%20mitigation%0Astrategies.%20Identifying%20the%20reasons%20behind%20a%20biased%20system%20is%20not%0Astraightforward%2C%20since%20in%20many%20occasions%20they%20are%20associated%20with%20hidden%0Aspurious%20correlations%20which%20are%20not%20easy%20to%20spot.%20Standard%20approaches%20rely%20on%0Abias%20audits%20performed%20by%20analyzing%20model%20performance%20in%20pre-defined%20subgroups%0Aof%20data%20samples%2C%20usually%20characterized%20by%20common%20attributes%20like%20gender%20or%0Aethnicity%20when%20it%20comes%20to%20people%2C%20or%20other%20specific%20attributes%20defining%0Asemantically%20coherent%20groups%20of%20images.%20However%2C%20it%20is%20not%20always%20possible%20to%0Aknow%20a-priori%20the%20specific%20attributes%20defining%20the%20failure%20modes%20of%20visual%0Arecognition%20systems.%20Recent%20approaches%20propose%20to%20discover%20these%20groups%20by%0Aleveraging%20large%20vision%20language%20models%2C%20which%20enable%20the%20extraction%20of%0Across-modal%20embeddings%20and%20the%20generation%20of%20textual%20descriptions%20to%0Acharacterize%20the%20subgroups%20where%20a%20certain%20model%20is%20underperforming.%20In%20this%0Awork%2C%20we%20argue%20that%20incorporating%20visual%20explanations%20%28e.g.%20heatmaps%20generated%0Avia%20GradCAM%20or%20other%20approaches%29%20can%20boost%20the%20performance%20of%20such%20bias%0Adiscovery%20and%20mitigation%20frameworks.%20To%20this%20end%2C%20we%20introduce%20Visually%0AGrounded%20Bias%20Discovery%20and%20Mitigation%20%28ViG-Bias%29%2C%20a%20simple%20yet%20effective%0Atechnique%20which%20can%20be%20integrated%20to%20a%20variety%20of%20existing%20frameworks%20to%0Aimprove%20both%2C%20discovery%20and%20mitigation%20performance.%20Our%20comprehensive%0Aevaluation%20shows%20that%20incorporating%20visual%20explanations%20enhances%20existing%0Atechniques%20like%20DOMINO%2C%20FACTS%20and%20Bias-to-Text%2C%20across%20several%20challenging%0Adatasets%2C%20including%20CelebA%2C%20Waterbirds%2C%20and%20NICO%2B%2B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01996v4&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters", "author": "Byungjun Kim and Minju Kim and Dayeon Seo and Bugeun Kim", "abstract": "  We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions have been a focus, negotiations between merchant NPCs\nand players on item prices have not received sufficient attention. First, we\ndefine passive pricing as the limited ability of merchants to modify predefined\nitem prices. Second, passive communication means that merchants can only\ninteract with players in a scripted manner. To tackle these issues and create\nan active merchant NPC, we propose a merchant framework based on large language\nmodels (LLMs), called MART, which consists of an appraiser module and a\nnegotiator module. We conducted two experiments to guide game developers in\nselecting appropriate implementations by comparing different training methods\nand LLM sizes. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs. We expect our\nfindings to guide developers in using LLMs for developing active merchant NPCs.\n", "link": "http://arxiv.org/abs/2412.11189v2", "date": "2025-01-08", "relevancy": 2.2691, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Active%20Merchant%20Non-player%0A%20%20Characters&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Active%20Merchant%20Non-player%0A%20%20Characters%0AAuthor%3A%20Byungjun%20Kim%20and%20Minju%20Kim%20and%20Dayeon%20Seo%20and%20Bugeun%20Kim%0AAbstract%3A%20%20%20We%20highlight%20two%20significant%20issues%20leading%20to%20the%20passivity%20of%20current%0Amerchant%20non-player%20characters%20%28NPCs%29%3A%20pricing%20and%20communication.%20While%0Aimmersive%20interactions%20have%20been%20a%20focus%2C%20negotiations%20between%20merchant%20NPCs%0Aand%20players%20on%20item%20prices%20have%20not%20received%20sufficient%20attention.%20First%2C%20we%0Adefine%20passive%20pricing%20as%20the%20limited%20ability%20of%20merchants%20to%20modify%20predefined%0Aitem%20prices.%20Second%2C%20passive%20communication%20means%20that%20merchants%20can%20only%0Ainteract%20with%20players%20in%20a%20scripted%20manner.%20To%20tackle%20these%20issues%20and%20create%0Aan%20active%20merchant%20NPC%2C%20we%20propose%20a%20merchant%20framework%20based%20on%20large%20language%0Amodels%20%28LLMs%29%2C%20called%20MART%2C%20which%20consists%20of%20an%20appraiser%20module%20and%20a%0Anegotiator%20module.%20We%20conducted%20two%20experiments%20to%20guide%20game%20developers%20in%0Aselecting%20appropriate%20implementations%20by%20comparing%20different%20training%20methods%0Aand%20LLM%20sizes.%20Our%20findings%20indicate%20that%20finetuning%20methods%2C%20such%20as%0Asupervised%20finetuning%20%28SFT%29%20and%20knowledge%20distillation%20%28KD%29%2C%20are%20effective%20in%0Ausing%20smaller%20LLMs%20to%20implement%20active%20merchant%20NPCs.%20Additionally%2C%20we%20found%0Athree%20irregular%20cases%20arising%20from%20the%20responses%20of%20LLMs.%20We%20expect%20our%0Afindings%20to%20guide%20developers%20in%20using%20LLMs%20for%20developing%20active%20merchant%20NPCs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Active%2520Merchant%2520Non-player%250A%2520%2520Characters%26entry.906535625%3DByungjun%2520Kim%2520and%2520Minju%2520Kim%2520and%2520Dayeon%2520Seo%2520and%2520Bugeun%2520Kim%26entry.1292438233%3D%2520%2520We%2520highlight%2520two%2520significant%2520issues%2520leading%2520to%2520the%2520passivity%2520of%2520current%250Amerchant%2520non-player%2520characters%2520%2528NPCs%2529%253A%2520pricing%2520and%2520communication.%2520While%250Aimmersive%2520interactions%2520have%2520been%2520a%2520focus%252C%2520negotiations%2520between%2520merchant%2520NPCs%250Aand%2520players%2520on%2520item%2520prices%2520have%2520not%2520received%2520sufficient%2520attention.%2520First%252C%2520we%250Adefine%2520passive%2520pricing%2520as%2520the%2520limited%2520ability%2520of%2520merchants%2520to%2520modify%2520predefined%250Aitem%2520prices.%2520Second%252C%2520passive%2520communication%2520means%2520that%2520merchants%2520can%2520only%250Ainteract%2520with%2520players%2520in%2520a%2520scripted%2520manner.%2520To%2520tackle%2520these%2520issues%2520and%2520create%250Aan%2520active%2520merchant%2520NPC%252C%2520we%2520propose%2520a%2520merchant%2520framework%2520based%2520on%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520called%2520MART%252C%2520which%2520consists%2520of%2520an%2520appraiser%2520module%2520and%2520a%250Anegotiator%2520module.%2520We%2520conducted%2520two%2520experiments%2520to%2520guide%2520game%2520developers%2520in%250Aselecting%2520appropriate%2520implementations%2520by%2520comparing%2520different%2520training%2520methods%250Aand%2520LLM%2520sizes.%2520Our%2520findings%2520indicate%2520that%2520finetuning%2520methods%252C%2520such%2520as%250Asupervised%2520finetuning%2520%2528SFT%2529%2520and%2520knowledge%2520distillation%2520%2528KD%2529%252C%2520are%2520effective%2520in%250Ausing%2520smaller%2520LLMs%2520to%2520implement%2520active%2520merchant%2520NPCs.%2520Additionally%252C%2520we%2520found%250Athree%2520irregular%2520cases%2520arising%2520from%2520the%2520responses%2520of%2520LLMs.%2520We%2520expect%2520our%250Afindings%2520to%2520guide%2520developers%2520in%2520using%2520LLMs%2520for%2520developing%2520active%2520merchant%2520NPCs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Active%20Merchant%20Non-player%0A%20%20Characters&entry.906535625=Byungjun%20Kim%20and%20Minju%20Kim%20and%20Dayeon%20Seo%20and%20Bugeun%20Kim&entry.1292438233=%20%20We%20highlight%20two%20significant%20issues%20leading%20to%20the%20passivity%20of%20current%0Amerchant%20non-player%20characters%20%28NPCs%29%3A%20pricing%20and%20communication.%20While%0Aimmersive%20interactions%20have%20been%20a%20focus%2C%20negotiations%20between%20merchant%20NPCs%0Aand%20players%20on%20item%20prices%20have%20not%20received%20sufficient%20attention.%20First%2C%20we%0Adefine%20passive%20pricing%20as%20the%20limited%20ability%20of%20merchants%20to%20modify%20predefined%0Aitem%20prices.%20Second%2C%20passive%20communication%20means%20that%20merchants%20can%20only%0Ainteract%20with%20players%20in%20a%20scripted%20manner.%20To%20tackle%20these%20issues%20and%20create%0Aan%20active%20merchant%20NPC%2C%20we%20propose%20a%20merchant%20framework%20based%20on%20large%20language%0Amodels%20%28LLMs%29%2C%20called%20MART%2C%20which%20consists%20of%20an%20appraiser%20module%20and%20a%0Anegotiator%20module.%20We%20conducted%20two%20experiments%20to%20guide%20game%20developers%20in%0Aselecting%20appropriate%20implementations%20by%20comparing%20different%20training%20methods%0Aand%20LLM%20sizes.%20Our%20findings%20indicate%20that%20finetuning%20methods%2C%20such%20as%0Asupervised%20finetuning%20%28SFT%29%20and%20knowledge%20distillation%20%28KD%29%2C%20are%20effective%20in%0Ausing%20smaller%20LLMs%20to%20implement%20active%20merchant%20NPCs.%20Additionally%2C%20we%20found%0Athree%20irregular%20cases%20arising%20from%20the%20responses%20of%20LLMs.%20We%20expect%20our%0Afindings%20to%20guide%20developers%20in%20using%20LLMs%20for%20developing%20active%20merchant%20NPCs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11189v2&entry.124074799=Read"},
{"title": "TSCM: A Teacher-Student Model for Vision Place Recognition Using\n  Cross-Metric Knowledge Distillation", "author": "Yehui Shen and Mingmin Liu and Huimin Lu and Xieyuanli Chen", "abstract": "  Visual place recognition (VPR) plays a pivotal role in autonomous exploration\nand navigation of mobile robots within complex outdoor environments. While\ncost-effective and easily deployed, camera sensors are sensitive to lighting\nand weather changes, and even slight image alterations can greatly affect VPR\nefficiency and precision. Existing methods overcome this by exploiting powerful\nyet large networks, leading to significant consumption of computational\nresources. In this paper, we propose a high-performance teacher and lightweight\nstudent distillation framework called TSCM. It exploits our devised\ncross-metric knowledge distillation to narrow the performance gap between the\nteacher and student models, maintaining superior performance while enabling\nminimal computational load during deployment. We conduct comprehensive\nevaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k.\nExperimental results demonstrate the superiority of our method over baseline\nmodels in terms of recognition accuracy and model parameter efficiency.\nMoreover, our ablation studies show that the proposed knowledge distillation\ntechnique surpasses other counterparts. The code of our method has been\nreleased at https://github.com/nubot-nudt/TSCM.\n", "link": "http://arxiv.org/abs/2404.01587v2", "date": "2025-01-08", "relevancy": 2.2437, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSCM%3A%20A%20Teacher-Student%20Model%20for%20Vision%20Place%20Recognition%20Using%0A%20%20Cross-Metric%20Knowledge%20Distillation&body=Title%3A%20TSCM%3A%20A%20Teacher-Student%20Model%20for%20Vision%20Place%20Recognition%20Using%0A%20%20Cross-Metric%20Knowledge%20Distillation%0AAuthor%3A%20Yehui%20Shen%20and%20Mingmin%20Liu%20and%20Huimin%20Lu%20and%20Xieyuanli%20Chen%0AAbstract%3A%20%20%20Visual%20place%20recognition%20%28VPR%29%20plays%20a%20pivotal%20role%20in%20autonomous%20exploration%0Aand%20navigation%20of%20mobile%20robots%20within%20complex%20outdoor%20environments.%20While%0Acost-effective%20and%20easily%20deployed%2C%20camera%20sensors%20are%20sensitive%20to%20lighting%0Aand%20weather%20changes%2C%20and%20even%20slight%20image%20alterations%20can%20greatly%20affect%20VPR%0Aefficiency%20and%20precision.%20Existing%20methods%20overcome%20this%20by%20exploiting%20powerful%0Ayet%20large%20networks%2C%20leading%20to%20significant%20consumption%20of%20computational%0Aresources.%20In%20this%20paper%2C%20we%20propose%20a%20high-performance%20teacher%20and%20lightweight%0Astudent%20distillation%20framework%20called%20TSCM.%20It%20exploits%20our%20devised%0Across-metric%20knowledge%20distillation%20to%20narrow%20the%20performance%20gap%20between%20the%0Ateacher%20and%20student%20models%2C%20maintaining%20superior%20performance%20while%20enabling%0Aminimal%20computational%20load%20during%20deployment.%20We%20conduct%20comprehensive%0Aevaluations%20on%20large-scale%20datasets%2C%20namely%20Pittsburgh30k%20and%20Pittsburgh250k.%0AExperimental%20results%20demonstrate%20the%20superiority%20of%20our%20method%20over%20baseline%0Amodels%20in%20terms%20of%20recognition%20accuracy%20and%20model%20parameter%20efficiency.%0AMoreover%2C%20our%20ablation%20studies%20show%20that%20the%20proposed%20knowledge%20distillation%0Atechnique%20surpasses%20other%20counterparts.%20The%20code%20of%20our%20method%20has%20been%0Areleased%20at%20https%3A//github.com/nubot-nudt/TSCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSCM%253A%2520A%2520Teacher-Student%2520Model%2520for%2520Vision%2520Place%2520Recognition%2520Using%250A%2520%2520Cross-Metric%2520Knowledge%2520Distillation%26entry.906535625%3DYehui%2520Shen%2520and%2520Mingmin%2520Liu%2520and%2520Huimin%2520Lu%2520and%2520Xieyuanli%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520place%2520recognition%2520%2528VPR%2529%2520plays%2520a%2520pivotal%2520role%2520in%2520autonomous%2520exploration%250Aand%2520navigation%2520of%2520mobile%2520robots%2520within%2520complex%2520outdoor%2520environments.%2520While%250Acost-effective%2520and%2520easily%2520deployed%252C%2520camera%2520sensors%2520are%2520sensitive%2520to%2520lighting%250Aand%2520weather%2520changes%252C%2520and%2520even%2520slight%2520image%2520alterations%2520can%2520greatly%2520affect%2520VPR%250Aefficiency%2520and%2520precision.%2520Existing%2520methods%2520overcome%2520this%2520by%2520exploiting%2520powerful%250Ayet%2520large%2520networks%252C%2520leading%2520to%2520significant%2520consumption%2520of%2520computational%250Aresources.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520high-performance%2520teacher%2520and%2520lightweight%250Astudent%2520distillation%2520framework%2520called%2520TSCM.%2520It%2520exploits%2520our%2520devised%250Across-metric%2520knowledge%2520distillation%2520to%2520narrow%2520the%2520performance%2520gap%2520between%2520the%250Ateacher%2520and%2520student%2520models%252C%2520maintaining%2520superior%2520performance%2520while%2520enabling%250Aminimal%2520computational%2520load%2520during%2520deployment.%2520We%2520conduct%2520comprehensive%250Aevaluations%2520on%2520large-scale%2520datasets%252C%2520namely%2520Pittsburgh30k%2520and%2520Pittsburgh250k.%250AExperimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520baseline%250Amodels%2520in%2520terms%2520of%2520recognition%2520accuracy%2520and%2520model%2520parameter%2520efficiency.%250AMoreover%252C%2520our%2520ablation%2520studies%2520show%2520that%2520the%2520proposed%2520knowledge%2520distillation%250Atechnique%2520surpasses%2520other%2520counterparts.%2520The%2520code%2520of%2520our%2520method%2520has%2520been%250Areleased%2520at%2520https%253A//github.com/nubot-nudt/TSCM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSCM%3A%20A%20Teacher-Student%20Model%20for%20Vision%20Place%20Recognition%20Using%0A%20%20Cross-Metric%20Knowledge%20Distillation&entry.906535625=Yehui%20Shen%20and%20Mingmin%20Liu%20and%20Huimin%20Lu%20and%20Xieyuanli%20Chen&entry.1292438233=%20%20Visual%20place%20recognition%20%28VPR%29%20plays%20a%20pivotal%20role%20in%20autonomous%20exploration%0Aand%20navigation%20of%20mobile%20robots%20within%20complex%20outdoor%20environments.%20While%0Acost-effective%20and%20easily%20deployed%2C%20camera%20sensors%20are%20sensitive%20to%20lighting%0Aand%20weather%20changes%2C%20and%20even%20slight%20image%20alterations%20can%20greatly%20affect%20VPR%0Aefficiency%20and%20precision.%20Existing%20methods%20overcome%20this%20by%20exploiting%20powerful%0Ayet%20large%20networks%2C%20leading%20to%20significant%20consumption%20of%20computational%0Aresources.%20In%20this%20paper%2C%20we%20propose%20a%20high-performance%20teacher%20and%20lightweight%0Astudent%20distillation%20framework%20called%20TSCM.%20It%20exploits%20our%20devised%0Across-metric%20knowledge%20distillation%20to%20narrow%20the%20performance%20gap%20between%20the%0Ateacher%20and%20student%20models%2C%20maintaining%20superior%20performance%20while%20enabling%0Aminimal%20computational%20load%20during%20deployment.%20We%20conduct%20comprehensive%0Aevaluations%20on%20large-scale%20datasets%2C%20namely%20Pittsburgh30k%20and%20Pittsburgh250k.%0AExperimental%20results%20demonstrate%20the%20superiority%20of%20our%20method%20over%20baseline%0Amodels%20in%20terms%20of%20recognition%20accuracy%20and%20model%20parameter%20efficiency.%0AMoreover%2C%20our%20ablation%20studies%20show%20that%20the%20proposed%20knowledge%20distillation%0Atechnique%20surpasses%20other%20counterparts.%20The%20code%20of%20our%20method%20has%20been%0Areleased%20at%20https%3A//github.com/nubot-nudt/TSCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01587v2&entry.124074799=Read"},
{"title": "MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration", "author": "Zhi Jin and Yuwei Qiu and Kaihao Zhang and Hongdong Li and Wenhan Luo", "abstract": "  Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2.\n", "link": "http://arxiv.org/abs/2501.04486v1", "date": "2025-01-08", "relevancy": 2.2405, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6185}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MB-TaylorFormer%20V2%3A%20Improved%20Multi-branch%20Linear%20Transformer%20Expanded%20by%0A%20%20Taylor%20Formula%20for%20Image%20Restoration&body=Title%3A%20MB-TaylorFormer%20V2%3A%20Improved%20Multi-branch%20Linear%20Transformer%20Expanded%20by%0A%20%20Taylor%20Formula%20for%20Image%20Restoration%0AAuthor%3A%20Zhi%20Jin%20and%20Yuwei%20Qiu%20and%20Kaihao%20Zhang%20and%20Hongdong%20Li%20and%20Wenhan%20Luo%0AAbstract%3A%20%20%20Recently%2C%20Transformer%20networks%20have%20demonstrated%20outstanding%20performance%20in%0Athe%20field%20of%20image%20restoration%20due%20to%20the%20global%20receptive%20field%20and%0Aadaptability%20to%20input.%20However%2C%20the%20quadratic%20computational%20complexity%20of%0ASoftmax-attention%20poses%20a%20significant%20limitation%20on%20its%20extensive%20application%0Ain%20image%20restoration%20tasks%2C%20particularly%20for%20high-resolution%20images.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20a%20novel%20variant%20of%20the%20Transformer.%20This%20variant%0Aleverages%20the%20Taylor%20expansion%20to%20approximate%20the%20Softmax-attention%20and%0Autilizes%20the%20concept%20of%20norm-preserving%20mapping%20to%20approximate%20the%20remainder%20of%0Athe%20first-order%20Taylor%20expansion%2C%20resulting%20in%20a%20linear%20computational%0Acomplexity.%20Moreover%2C%20we%20introduce%20a%20multi-branch%20architecture%20featuring%0Amulti-scale%20patch%20embedding%20into%20the%20proposed%20Transformer%2C%20which%20has%20four%0Adistinct%20advantages%3A%201%29%20various%20sizes%20of%20the%20receptive%20field%3B%202%29%20multi-level%0Asemantic%20information%3B%203%29%20flexible%20shapes%20of%20the%20receptive%20field%3B%204%29%20accelerated%0Atraining%20and%20inference%20speed.%20Hence%2C%20the%20proposed%20model%2C%20named%20the%20second%0Aversion%20of%20Taylor%20formula%20expansion-based%20Transformer%20%28for%20short%0AMB-TaylorFormer%20V2%29%20has%20the%20capability%20to%20concurrently%20process%20coarse-to-fine%0Afeatures%2C%20capture%20long-distance%20pixel%20interactions%20with%20limited%20computational%0Acost%2C%20and%20improve%20the%20approximation%20of%20the%20Taylor%20expansion%20remainder.%0AExperimental%20results%20across%20diverse%20image%20restoration%20benchmarks%20demonstrate%0Athat%20MB-TaylorFormer%20V2%20achieves%20state-of-the-art%20performance%20in%20multiple%20image%0Arestoration%20tasks%2C%20such%20as%20image%20dehazing%2C%20deraining%2C%20desnowing%2C%20motion%0Adeblurring%2C%20and%20denoising%2C%20with%20very%20little%20computational%20overhead.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/FVL2020/MB-TaylorFormerV2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMB-TaylorFormer%2520V2%253A%2520Improved%2520Multi-branch%2520Linear%2520Transformer%2520Expanded%2520by%250A%2520%2520Taylor%2520Formula%2520for%2520Image%2520Restoration%26entry.906535625%3DZhi%2520Jin%2520and%2520Yuwei%2520Qiu%2520and%2520Kaihao%2520Zhang%2520and%2520Hongdong%2520Li%2520and%2520Wenhan%2520Luo%26entry.1292438233%3D%2520%2520Recently%252C%2520Transformer%2520networks%2520have%2520demonstrated%2520outstanding%2520performance%2520in%250Athe%2520field%2520of%2520image%2520restoration%2520due%2520to%2520the%2520global%2520receptive%2520field%2520and%250Aadaptability%2520to%2520input.%2520However%252C%2520the%2520quadratic%2520computational%2520complexity%2520of%250ASoftmax-attention%2520poses%2520a%2520significant%2520limitation%2520on%2520its%2520extensive%2520application%250Ain%2520image%2520restoration%2520tasks%252C%2520particularly%2520for%2520high-resolution%2520images.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520variant%2520of%2520the%2520Transformer.%2520This%2520variant%250Aleverages%2520the%2520Taylor%2520expansion%2520to%2520approximate%2520the%2520Softmax-attention%2520and%250Autilizes%2520the%2520concept%2520of%2520norm-preserving%2520mapping%2520to%2520approximate%2520the%2520remainder%2520of%250Athe%2520first-order%2520Taylor%2520expansion%252C%2520resulting%2520in%2520a%2520linear%2520computational%250Acomplexity.%2520Moreover%252C%2520we%2520introduce%2520a%2520multi-branch%2520architecture%2520featuring%250Amulti-scale%2520patch%2520embedding%2520into%2520the%2520proposed%2520Transformer%252C%2520which%2520has%2520four%250Adistinct%2520advantages%253A%25201%2529%2520various%2520sizes%2520of%2520the%2520receptive%2520field%253B%25202%2529%2520multi-level%250Asemantic%2520information%253B%25203%2529%2520flexible%2520shapes%2520of%2520the%2520receptive%2520field%253B%25204%2529%2520accelerated%250Atraining%2520and%2520inference%2520speed.%2520Hence%252C%2520the%2520proposed%2520model%252C%2520named%2520the%2520second%250Aversion%2520of%2520Taylor%2520formula%2520expansion-based%2520Transformer%2520%2528for%2520short%250AMB-TaylorFormer%2520V2%2529%2520has%2520the%2520capability%2520to%2520concurrently%2520process%2520coarse-to-fine%250Afeatures%252C%2520capture%2520long-distance%2520pixel%2520interactions%2520with%2520limited%2520computational%250Acost%252C%2520and%2520improve%2520the%2520approximation%2520of%2520the%2520Taylor%2520expansion%2520remainder.%250AExperimental%2520results%2520across%2520diverse%2520image%2520restoration%2520benchmarks%2520demonstrate%250Athat%2520MB-TaylorFormer%2520V2%2520achieves%2520state-of-the-art%2520performance%2520in%2520multiple%2520image%250Arestoration%2520tasks%252C%2520such%2520as%2520image%2520dehazing%252C%2520deraining%252C%2520desnowing%252C%2520motion%250Adeblurring%252C%2520and%2520denoising%252C%2520with%2520very%2520little%2520computational%2520overhead.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/FVL2020/MB-TaylorFormerV2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MB-TaylorFormer%20V2%3A%20Improved%20Multi-branch%20Linear%20Transformer%20Expanded%20by%0A%20%20Taylor%20Formula%20for%20Image%20Restoration&entry.906535625=Zhi%20Jin%20and%20Yuwei%20Qiu%20and%20Kaihao%20Zhang%20and%20Hongdong%20Li%20and%20Wenhan%20Luo&entry.1292438233=%20%20Recently%2C%20Transformer%20networks%20have%20demonstrated%20outstanding%20performance%20in%0Athe%20field%20of%20image%20restoration%20due%20to%20the%20global%20receptive%20field%20and%0Aadaptability%20to%20input.%20However%2C%20the%20quadratic%20computational%20complexity%20of%0ASoftmax-attention%20poses%20a%20significant%20limitation%20on%20its%20extensive%20application%0Ain%20image%20restoration%20tasks%2C%20particularly%20for%20high-resolution%20images.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20a%20novel%20variant%20of%20the%20Transformer.%20This%20variant%0Aleverages%20the%20Taylor%20expansion%20to%20approximate%20the%20Softmax-attention%20and%0Autilizes%20the%20concept%20of%20norm-preserving%20mapping%20to%20approximate%20the%20remainder%20of%0Athe%20first-order%20Taylor%20expansion%2C%20resulting%20in%20a%20linear%20computational%0Acomplexity.%20Moreover%2C%20we%20introduce%20a%20multi-branch%20architecture%20featuring%0Amulti-scale%20patch%20embedding%20into%20the%20proposed%20Transformer%2C%20which%20has%20four%0Adistinct%20advantages%3A%201%29%20various%20sizes%20of%20the%20receptive%20field%3B%202%29%20multi-level%0Asemantic%20information%3B%203%29%20flexible%20shapes%20of%20the%20receptive%20field%3B%204%29%20accelerated%0Atraining%20and%20inference%20speed.%20Hence%2C%20the%20proposed%20model%2C%20named%20the%20second%0Aversion%20of%20Taylor%20formula%20expansion-based%20Transformer%20%28for%20short%0AMB-TaylorFormer%20V2%29%20has%20the%20capability%20to%20concurrently%20process%20coarse-to-fine%0Afeatures%2C%20capture%20long-distance%20pixel%20interactions%20with%20limited%20computational%0Acost%2C%20and%20improve%20the%20approximation%20of%20the%20Taylor%20expansion%20remainder.%0AExperimental%20results%20across%20diverse%20image%20restoration%20benchmarks%20demonstrate%0Athat%20MB-TaylorFormer%20V2%20achieves%20state-of-the-art%20performance%20in%20multiple%20image%0Arestoration%20tasks%2C%20such%20as%20image%20dehazing%2C%20deraining%2C%20desnowing%2C%20motion%0Adeblurring%2C%20and%20denoising%2C%20with%20very%20little%20computational%20overhead.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/FVL2020/MB-TaylorFormerV2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04486v1&entry.124074799=Read"},
{"title": "$O(k)$-Equivariant Dimensionality Reduction on Stiefel Manifolds", "author": "Andrew Lee and Harlin Lee and Jose A. Perea and Nikolas Schonsheck and Madeleine Weinstein", "abstract": "  Many real-world datasets live on high-dimensional Stiefel and Grassmannian\nmanifolds, $V_k(\\mathbb{R}^N)$ and $Gr(k, \\mathbb{R}^N)$ respectively, and\nbenefit from projection onto lower-dimensional Stiefel and Grassmannian\nmanifolds. In this work, we propose an algorithm called \\textit{Principal\nStiefel Coordinates (PSC)} to reduce data dimensionality from $\nV_k(\\mathbb{R}^N)$ to $V_k(\\mathbb{R}^n)$ in an \\textit{$O(k)$-equivariant}\nmanner ($k \\leq n \\ll N$). We begin by observing that each element $\\alpha \\in\nV_n(\\mathbb{R}^N)$ defines an isometric embedding of $V_k(\\mathbb{R}^n)$ into\n$V_k(\\mathbb{R}^N)$. Next, we describe two ways of finding a suitable embedding\nmap $\\alpha$: one via an extension of principal component analysis\n($\\alpha_{PCA}$), and one that further minimizes data fit error using gradient\ndescent ($\\alpha_{GD}$). Then, we define a continuous and $O(k)$-equivariant\nmap $\\pi_\\alpha$ that acts as a \"closest point operator\" to project the data\nonto the image of $V_k(\\mathbb{R}^n)$ in $V_k(\\mathbb{R}^N)$ under the\nembedding determined by $\\alpha$, while minimizing distortion. Because this\ndimensionality reduction is $O(k)$-equivariant, these results extend to\nGrassmannian manifolds as well. Lastly, we show that $\\pi_{\\alpha_{PCA}}$\nglobally minimizes projection error in a noiseless setting, while\n$\\pi_{\\alpha_{GD}}$ achieves a meaningfully different and improved outcome when\nthe data does not lie exactly on the image of a linearly embedded\nlower-dimensional Stiefel manifold as above. Multiple numerical experiments\nusing synthetic and real-world data are performed.\n", "link": "http://arxiv.org/abs/2309.10775v2", "date": "2025-01-08", "relevancy": 2.2358, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4522}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4452}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24O%28k%29%24-Equivariant%20Dimensionality%20Reduction%20on%20Stiefel%20Manifolds&body=Title%3A%20%24O%28k%29%24-Equivariant%20Dimensionality%20Reduction%20on%20Stiefel%20Manifolds%0AAuthor%3A%20Andrew%20Lee%20and%20Harlin%20Lee%20and%20Jose%20A.%20Perea%20and%20Nikolas%20Schonsheck%20and%20Madeleine%20Weinstein%0AAbstract%3A%20%20%20Many%20real-world%20datasets%20live%20on%20high-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds%2C%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20and%20%24Gr%28k%2C%20%5Cmathbb%7BR%7D%5EN%29%24%20respectively%2C%20and%0Abenefit%20from%20projection%20onto%20lower-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds.%20In%20this%20work%2C%20we%20propose%20an%20algorithm%20called%20%5Ctextit%7BPrincipal%0AStiefel%20Coordinates%20%28PSC%29%7D%20to%20reduce%20data%20dimensionality%20from%20%24%0AV_k%28%5Cmathbb%7BR%7D%5EN%29%24%20to%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20an%20%5Ctextit%7B%24O%28k%29%24-equivariant%7D%0Amanner%20%28%24k%20%5Cleq%20n%20%5Cll%20N%24%29.%20We%20begin%20by%20observing%20that%20each%20element%20%24%5Calpha%20%5Cin%0AV_n%28%5Cmathbb%7BR%7D%5EN%29%24%20defines%20an%20isometric%20embedding%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20into%0A%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24.%20Next%2C%20we%20describe%20two%20ways%20of%20finding%20a%20suitable%20embedding%0Amap%20%24%5Calpha%24%3A%20one%20via%20an%20extension%20of%20principal%20component%20analysis%0A%28%24%5Calpha_%7BPCA%7D%24%29%2C%20and%20one%20that%20further%20minimizes%20data%20fit%20error%20using%20gradient%0Adescent%20%28%24%5Calpha_%7BGD%7D%24%29.%20Then%2C%20we%20define%20a%20continuous%20and%20%24O%28k%29%24-equivariant%0Amap%20%24%5Cpi_%5Calpha%24%20that%20acts%20as%20a%20%22closest%20point%20operator%22%20to%20project%20the%20data%0Aonto%20the%20image%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20under%20the%0Aembedding%20determined%20by%20%24%5Calpha%24%2C%20while%20minimizing%20distortion.%20Because%20this%0Adimensionality%20reduction%20is%20%24O%28k%29%24-equivariant%2C%20these%20results%20extend%20to%0AGrassmannian%20manifolds%20as%20well.%20Lastly%2C%20we%20show%20that%20%24%5Cpi_%7B%5Calpha_%7BPCA%7D%7D%24%0Aglobally%20minimizes%20projection%20error%20in%20a%20noiseless%20setting%2C%20while%0A%24%5Cpi_%7B%5Calpha_%7BGD%7D%7D%24%20achieves%20a%20meaningfully%20different%20and%20improved%20outcome%20when%0Athe%20data%20does%20not%20lie%20exactly%20on%20the%20image%20of%20a%20linearly%20embedded%0Alower-dimensional%20Stiefel%20manifold%20as%20above.%20Multiple%20numerical%20experiments%0Ausing%20synthetic%20and%20real-world%20data%20are%20performed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524O%2528k%2529%2524-Equivariant%2520Dimensionality%2520Reduction%2520on%2520Stiefel%2520Manifolds%26entry.906535625%3DAndrew%2520Lee%2520and%2520Harlin%2520Lee%2520and%2520Jose%2520A.%2520Perea%2520and%2520Nikolas%2520Schonsheck%2520and%2520Madeleine%2520Weinstein%26entry.1292438233%3D%2520%2520Many%2520real-world%2520datasets%2520live%2520on%2520high-dimensional%2520Stiefel%2520and%2520Grassmannian%250Amanifolds%252C%2520%2524V_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520and%2520%2524Gr%2528k%252C%2520%255Cmathbb%257BR%257D%255EN%2529%2524%2520respectively%252C%2520and%250Abenefit%2520from%2520projection%2520onto%2520lower-dimensional%2520Stiefel%2520and%2520Grassmannian%250Amanifolds.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520algorithm%2520called%2520%255Ctextit%257BPrincipal%250AStiefel%2520Coordinates%2520%2528PSC%2529%257D%2520to%2520reduce%2520data%2520dimensionality%2520from%2520%2524%250AV_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520to%2520%2524V_k%2528%255Cmathbb%257BR%257D%255En%2529%2524%2520in%2520an%2520%255Ctextit%257B%2524O%2528k%2529%2524-equivariant%257D%250Amanner%2520%2528%2524k%2520%255Cleq%2520n%2520%255Cll%2520N%2524%2529.%2520We%2520begin%2520by%2520observing%2520that%2520each%2520element%2520%2524%255Calpha%2520%255Cin%250AV_n%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520defines%2520an%2520isometric%2520embedding%2520of%2520%2524V_k%2528%255Cmathbb%257BR%257D%255En%2529%2524%2520into%250A%2524V_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524.%2520Next%252C%2520we%2520describe%2520two%2520ways%2520of%2520finding%2520a%2520suitable%2520embedding%250Amap%2520%2524%255Calpha%2524%253A%2520one%2520via%2520an%2520extension%2520of%2520principal%2520component%2520analysis%250A%2528%2524%255Calpha_%257BPCA%257D%2524%2529%252C%2520and%2520one%2520that%2520further%2520minimizes%2520data%2520fit%2520error%2520using%2520gradient%250Adescent%2520%2528%2524%255Calpha_%257BGD%257D%2524%2529.%2520Then%252C%2520we%2520define%2520a%2520continuous%2520and%2520%2524O%2528k%2529%2524-equivariant%250Amap%2520%2524%255Cpi_%255Calpha%2524%2520that%2520acts%2520as%2520a%2520%2522closest%2520point%2520operator%2522%2520to%2520project%2520the%2520data%250Aonto%2520the%2520image%2520of%2520%2524V_k%2528%255Cmathbb%257BR%257D%255En%2529%2524%2520in%2520%2524V_k%2528%255Cmathbb%257BR%257D%255EN%2529%2524%2520under%2520the%250Aembedding%2520determined%2520by%2520%2524%255Calpha%2524%252C%2520while%2520minimizing%2520distortion.%2520Because%2520this%250Adimensionality%2520reduction%2520is%2520%2524O%2528k%2529%2524-equivariant%252C%2520these%2520results%2520extend%2520to%250AGrassmannian%2520manifolds%2520as%2520well.%2520Lastly%252C%2520we%2520show%2520that%2520%2524%255Cpi_%257B%255Calpha_%257BPCA%257D%257D%2524%250Aglobally%2520minimizes%2520projection%2520error%2520in%2520a%2520noiseless%2520setting%252C%2520while%250A%2524%255Cpi_%257B%255Calpha_%257BGD%257D%257D%2524%2520achieves%2520a%2520meaningfully%2520different%2520and%2520improved%2520outcome%2520when%250Athe%2520data%2520does%2520not%2520lie%2520exactly%2520on%2520the%2520image%2520of%2520a%2520linearly%2520embedded%250Alower-dimensional%2520Stiefel%2520manifold%2520as%2520above.%2520Multiple%2520numerical%2520experiments%250Ausing%2520synthetic%2520and%2520real-world%2520data%2520are%2520performed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24O%28k%29%24-Equivariant%20Dimensionality%20Reduction%20on%20Stiefel%20Manifolds&entry.906535625=Andrew%20Lee%20and%20Harlin%20Lee%20and%20Jose%20A.%20Perea%20and%20Nikolas%20Schonsheck%20and%20Madeleine%20Weinstein&entry.1292438233=%20%20Many%20real-world%20datasets%20live%20on%20high-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds%2C%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20and%20%24Gr%28k%2C%20%5Cmathbb%7BR%7D%5EN%29%24%20respectively%2C%20and%0Abenefit%20from%20projection%20onto%20lower-dimensional%20Stiefel%20and%20Grassmannian%0Amanifolds.%20In%20this%20work%2C%20we%20propose%20an%20algorithm%20called%20%5Ctextit%7BPrincipal%0AStiefel%20Coordinates%20%28PSC%29%7D%20to%20reduce%20data%20dimensionality%20from%20%24%0AV_k%28%5Cmathbb%7BR%7D%5EN%29%24%20to%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20an%20%5Ctextit%7B%24O%28k%29%24-equivariant%7D%0Amanner%20%28%24k%20%5Cleq%20n%20%5Cll%20N%24%29.%20We%20begin%20by%20observing%20that%20each%20element%20%24%5Calpha%20%5Cin%0AV_n%28%5Cmathbb%7BR%7D%5EN%29%24%20defines%20an%20isometric%20embedding%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20into%0A%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24.%20Next%2C%20we%20describe%20two%20ways%20of%20finding%20a%20suitable%20embedding%0Amap%20%24%5Calpha%24%3A%20one%20via%20an%20extension%20of%20principal%20component%20analysis%0A%28%24%5Calpha_%7BPCA%7D%24%29%2C%20and%20one%20that%20further%20minimizes%20data%20fit%20error%20using%20gradient%0Adescent%20%28%24%5Calpha_%7BGD%7D%24%29.%20Then%2C%20we%20define%20a%20continuous%20and%20%24O%28k%29%24-equivariant%0Amap%20%24%5Cpi_%5Calpha%24%20that%20acts%20as%20a%20%22closest%20point%20operator%22%20to%20project%20the%20data%0Aonto%20the%20image%20of%20%24V_k%28%5Cmathbb%7BR%7D%5En%29%24%20in%20%24V_k%28%5Cmathbb%7BR%7D%5EN%29%24%20under%20the%0Aembedding%20determined%20by%20%24%5Calpha%24%2C%20while%20minimizing%20distortion.%20Because%20this%0Adimensionality%20reduction%20is%20%24O%28k%29%24-equivariant%2C%20these%20results%20extend%20to%0AGrassmannian%20manifolds%20as%20well.%20Lastly%2C%20we%20show%20that%20%24%5Cpi_%7B%5Calpha_%7BPCA%7D%7D%24%0Aglobally%20minimizes%20projection%20error%20in%20a%20noiseless%20setting%2C%20while%0A%24%5Cpi_%7B%5Calpha_%7BGD%7D%7D%24%20achieves%20a%20meaningfully%20different%20and%20improved%20outcome%20when%0Athe%20data%20does%20not%20lie%20exactly%20on%20the%20image%20of%20a%20linearly%20embedded%0Alower-dimensional%20Stiefel%20manifold%20as%20above.%20Multiple%20numerical%20experiments%0Ausing%20synthetic%20and%20real-world%20data%20are%20performed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10775v2&entry.124074799=Read"},
{"title": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model", "author": "Han Shu and Wenshuo Li and Yehui Tang and Yiman Zhang and Yihao Chen and Houqiang Li and Yunhe Wang and Xinghao Chen", "abstract": "  Recently segment anything model (SAM) has shown powerful segmentation\ncapability and has drawn great attention in computer vision fields. Massive\nfollowing works have developed various applications based on the pre-trained\nSAM and achieved impressive performance on downstream vision tasks. However,\nSAM consists of heavy architectures and requires massive computational\ncapacity, which hinders the further application of SAM on computation\nconstrained edge devices. To this end, in this paper we propose a framework to\nobtain a tiny segment anything model (TinySAM) while maintaining the strong\nzero-shot performance. We first propose a full-stage knowledge distillation\nmethod with hard prompt sampling and hard mask weighting strategy to distill a\nlightweight student model. We also adapt the post-training quantization to the\nprompt-based segmentation task and further reduce the computational cost.\nMoreover, a hierarchical segmenting everything strategy is proposed to\naccelerate the everything inference by $2\\times$ with almost no performance\ndegradation. With all these proposed methods, our TinySAM leads to orders of\nmagnitude computational reduction and pushes the envelope for efficient segment\nanything task. Extensive experiments on various zero-shot transfer tasks\ndemonstrate the significantly advantageous performance of our TinySAM against\ncounterpart methods. Codes are available at\nhttps://github.com/xinghaochen/TinySAM and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/TinySAM.\n", "link": "http://arxiv.org/abs/2312.13789v3", "date": "2025-01-08", "relevancy": 2.2109, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5499}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinySAM%3A%20Pushing%20the%20Envelope%20for%20Efficient%20Segment%20Anything%20Model&body=Title%3A%20TinySAM%3A%20Pushing%20the%20Envelope%20for%20Efficient%20Segment%20Anything%20Model%0AAuthor%3A%20Han%20Shu%20and%20Wenshuo%20Li%20and%20Yehui%20Tang%20and%20Yiman%20Zhang%20and%20Yihao%20Chen%20and%20Houqiang%20Li%20and%20Yunhe%20Wang%20and%20Xinghao%20Chen%0AAbstract%3A%20%20%20Recently%20segment%20anything%20model%20%28SAM%29%20has%20shown%20powerful%20segmentation%0Acapability%20and%20has%20drawn%20great%20attention%20in%20computer%20vision%20fields.%20Massive%0Afollowing%20works%20have%20developed%20various%20applications%20based%20on%20the%20pre-trained%0ASAM%20and%20achieved%20impressive%20performance%20on%20downstream%20vision%20tasks.%20However%2C%0ASAM%20consists%20of%20heavy%20architectures%20and%20requires%20massive%20computational%0Acapacity%2C%20which%20hinders%20the%20further%20application%20of%20SAM%20on%20computation%0Aconstrained%20edge%20devices.%20To%20this%20end%2C%20in%20this%20paper%20we%20propose%20a%20framework%20to%0Aobtain%20a%20tiny%20segment%20anything%20model%20%28TinySAM%29%20while%20maintaining%20the%20strong%0Azero-shot%20performance.%20We%20first%20propose%20a%20full-stage%20knowledge%20distillation%0Amethod%20with%20hard%20prompt%20sampling%20and%20hard%20mask%20weighting%20strategy%20to%20distill%20a%0Alightweight%20student%20model.%20We%20also%20adapt%20the%20post-training%20quantization%20to%20the%0Aprompt-based%20segmentation%20task%20and%20further%20reduce%20the%20computational%20cost.%0AMoreover%2C%20a%20hierarchical%20segmenting%20everything%20strategy%20is%20proposed%20to%0Aaccelerate%20the%20everything%20inference%20by%20%242%5Ctimes%24%20with%20almost%20no%20performance%0Adegradation.%20With%20all%20these%20proposed%20methods%2C%20our%20TinySAM%20leads%20to%20orders%20of%0Amagnitude%20computational%20reduction%20and%20pushes%20the%20envelope%20for%20efficient%20segment%0Aanything%20task.%20Extensive%20experiments%20on%20various%20zero-shot%20transfer%20tasks%0Ademonstrate%20the%20significantly%20advantageous%20performance%20of%20our%20TinySAM%20against%0Acounterpart%20methods.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/xinghaochen/TinySAM%20and%0Ahttps%3A//gitee.com/mindspore/models/tree/master/research/cv/TinySAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13789v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinySAM%253A%2520Pushing%2520the%2520Envelope%2520for%2520Efficient%2520Segment%2520Anything%2520Model%26entry.906535625%3DHan%2520Shu%2520and%2520Wenshuo%2520Li%2520and%2520Yehui%2520Tang%2520and%2520Yiman%2520Zhang%2520and%2520Yihao%2520Chen%2520and%2520Houqiang%2520Li%2520and%2520Yunhe%2520Wang%2520and%2520Xinghao%2520Chen%26entry.1292438233%3D%2520%2520Recently%2520segment%2520anything%2520model%2520%2528SAM%2529%2520has%2520shown%2520powerful%2520segmentation%250Acapability%2520and%2520has%2520drawn%2520great%2520attention%2520in%2520computer%2520vision%2520fields.%2520Massive%250Afollowing%2520works%2520have%2520developed%2520various%2520applications%2520based%2520on%2520the%2520pre-trained%250ASAM%2520and%2520achieved%2520impressive%2520performance%2520on%2520downstream%2520vision%2520tasks.%2520However%252C%250ASAM%2520consists%2520of%2520heavy%2520architectures%2520and%2520requires%2520massive%2520computational%250Acapacity%252C%2520which%2520hinders%2520the%2520further%2520application%2520of%2520SAM%2520on%2520computation%250Aconstrained%2520edge%2520devices.%2520To%2520this%2520end%252C%2520in%2520this%2520paper%2520we%2520propose%2520a%2520framework%2520to%250Aobtain%2520a%2520tiny%2520segment%2520anything%2520model%2520%2528TinySAM%2529%2520while%2520maintaining%2520the%2520strong%250Azero-shot%2520performance.%2520We%2520first%2520propose%2520a%2520full-stage%2520knowledge%2520distillation%250Amethod%2520with%2520hard%2520prompt%2520sampling%2520and%2520hard%2520mask%2520weighting%2520strategy%2520to%2520distill%2520a%250Alightweight%2520student%2520model.%2520We%2520also%2520adapt%2520the%2520post-training%2520quantization%2520to%2520the%250Aprompt-based%2520segmentation%2520task%2520and%2520further%2520reduce%2520the%2520computational%2520cost.%250AMoreover%252C%2520a%2520hierarchical%2520segmenting%2520everything%2520strategy%2520is%2520proposed%2520to%250Aaccelerate%2520the%2520everything%2520inference%2520by%2520%25242%255Ctimes%2524%2520with%2520almost%2520no%2520performance%250Adegradation.%2520With%2520all%2520these%2520proposed%2520methods%252C%2520our%2520TinySAM%2520leads%2520to%2520orders%2520of%250Amagnitude%2520computational%2520reduction%2520and%2520pushes%2520the%2520envelope%2520for%2520efficient%2520segment%250Aanything%2520task.%2520Extensive%2520experiments%2520on%2520various%2520zero-shot%2520transfer%2520tasks%250Ademonstrate%2520the%2520significantly%2520advantageous%2520performance%2520of%2520our%2520TinySAM%2520against%250Acounterpart%2520methods.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/xinghaochen/TinySAM%2520and%250Ahttps%253A//gitee.com/mindspore/models/tree/master/research/cv/TinySAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13789v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinySAM%3A%20Pushing%20the%20Envelope%20for%20Efficient%20Segment%20Anything%20Model&entry.906535625=Han%20Shu%20and%20Wenshuo%20Li%20and%20Yehui%20Tang%20and%20Yiman%20Zhang%20and%20Yihao%20Chen%20and%20Houqiang%20Li%20and%20Yunhe%20Wang%20and%20Xinghao%20Chen&entry.1292438233=%20%20Recently%20segment%20anything%20model%20%28SAM%29%20has%20shown%20powerful%20segmentation%0Acapability%20and%20has%20drawn%20great%20attention%20in%20computer%20vision%20fields.%20Massive%0Afollowing%20works%20have%20developed%20various%20applications%20based%20on%20the%20pre-trained%0ASAM%20and%20achieved%20impressive%20performance%20on%20downstream%20vision%20tasks.%20However%2C%0ASAM%20consists%20of%20heavy%20architectures%20and%20requires%20massive%20computational%0Acapacity%2C%20which%20hinders%20the%20further%20application%20of%20SAM%20on%20computation%0Aconstrained%20edge%20devices.%20To%20this%20end%2C%20in%20this%20paper%20we%20propose%20a%20framework%20to%0Aobtain%20a%20tiny%20segment%20anything%20model%20%28TinySAM%29%20while%20maintaining%20the%20strong%0Azero-shot%20performance.%20We%20first%20propose%20a%20full-stage%20knowledge%20distillation%0Amethod%20with%20hard%20prompt%20sampling%20and%20hard%20mask%20weighting%20strategy%20to%20distill%20a%0Alightweight%20student%20model.%20We%20also%20adapt%20the%20post-training%20quantization%20to%20the%0Aprompt-based%20segmentation%20task%20and%20further%20reduce%20the%20computational%20cost.%0AMoreover%2C%20a%20hierarchical%20segmenting%20everything%20strategy%20is%20proposed%20to%0Aaccelerate%20the%20everything%20inference%20by%20%242%5Ctimes%24%20with%20almost%20no%20performance%0Adegradation.%20With%20all%20these%20proposed%20methods%2C%20our%20TinySAM%20leads%20to%20orders%20of%0Amagnitude%20computational%20reduction%20and%20pushes%20the%20envelope%20for%20efficient%20segment%0Aanything%20task.%20Extensive%20experiments%20on%20various%20zero-shot%20transfer%20tasks%0Ademonstrate%20the%20significantly%20advantageous%20performance%20of%20our%20TinySAM%20against%0Acounterpart%20methods.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/xinghaochen/TinySAM%20and%0Ahttps%3A//gitee.com/mindspore/models/tree/master/research/cv/TinySAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13789v3&entry.124074799=Read"},
{"title": "MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced\n  AI Applications with Retrieval Augmented Generation and Knowledge Graphs", "author": "Irene Siragusa and Salvatore Contino and Massimo La Ciura and Rosario Alicata and Roberto Pirrone", "abstract": "  The increasing interest in developing Artificial Intelligence applications in\nthe medical domain, suffers from the lack of high-quality data set, mainly due\nto privacy-related issues. In addition, the recent increase in large multimodal\nmodels (LMM) leads to the need for multimodal medical data sets, where clinical\nreports and findings are attached to the corresponding CT or MRI scans. This\npaper illustrates the entire workflow for building the MedPix 2.0 data set.\nStarting with the well-known multimodal data set\nMedPix\\textsuperscript{\\textregistered}, mainly used by physicians, nurses, and\nhealthcare students for Continuing Medical Education purposes, a semi-automatic\npipeline was developed to extract visual and textual data followed by a manual\ncuring procedure in which noisy samples were removed, thus creating a MongoDB\ndatabase. Along with the data set, we developed a GUI aimed at navigating\nefficiently the MongoDB instance and obtaining the raw data that can be easily\nused for training and/or fine-tuning LMMs. To enforce this point, in this work,\nwe first recall DR-Minerva, a RAG-based LMM trained using MedPix 2.0.\nDR-Minerva predicts the body part and the modality used to scan its input\nimage. We also propose the extension of DR-Minerva with a Knowledge Graph that\nuses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting\narchitecture can be queried in a end-to-end manner, as a medical decision\nsupport system. MedPix 2.0 is available on GitHub.\n\\url{https://github.com/CHILab1/MedPix-2.0}\n", "link": "http://arxiv.org/abs/2407.02994v2", "date": "2025-01-08", "relevancy": 2.2094, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5823}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedPix%202.0%3A%20A%20Comprehensive%20Multimodal%20Biomedical%20Data%20set%20for%20Advanced%0A%20%20AI%20Applications%20with%20Retrieval%20Augmented%20Generation%20and%20Knowledge%20Graphs&body=Title%3A%20MedPix%202.0%3A%20A%20Comprehensive%20Multimodal%20Biomedical%20Data%20set%20for%20Advanced%0A%20%20AI%20Applications%20with%20Retrieval%20Augmented%20Generation%20and%20Knowledge%20Graphs%0AAuthor%3A%20Irene%20Siragusa%20and%20Salvatore%20Contino%20and%20Massimo%20La%20Ciura%20and%20Rosario%20Alicata%20and%20Roberto%20Pirrone%0AAbstract%3A%20%20%20The%20increasing%20interest%20in%20developing%20Artificial%20Intelligence%20applications%20in%0Athe%20medical%20domain%2C%20suffers%20from%20the%20lack%20of%20high-quality%20data%20set%2C%20mainly%20due%0Ato%20privacy-related%20issues.%20In%20addition%2C%20the%20recent%20increase%20in%20large%20multimodal%0Amodels%20%28LMM%29%20leads%20to%20the%20need%20for%20multimodal%20medical%20data%20sets%2C%20where%20clinical%0Areports%20and%20findings%20are%20attached%20to%20the%20corresponding%20CT%20or%20MRI%20scans.%20This%0Apaper%20illustrates%20the%20entire%20workflow%20for%20building%20the%20MedPix%202.0%20data%20set.%0AStarting%20with%20the%20well-known%20multimodal%20data%20set%0AMedPix%5Ctextsuperscript%7B%5Ctextregistered%7D%2C%20mainly%20used%20by%20physicians%2C%20nurses%2C%20and%0Ahealthcare%20students%20for%20Continuing%20Medical%20Education%20purposes%2C%20a%20semi-automatic%0Apipeline%20was%20developed%20to%20extract%20visual%20and%20textual%20data%20followed%20by%20a%20manual%0Acuring%20procedure%20in%20which%20noisy%20samples%20were%20removed%2C%20thus%20creating%20a%20MongoDB%0Adatabase.%20Along%20with%20the%20data%20set%2C%20we%20developed%20a%20GUI%20aimed%20at%20navigating%0Aefficiently%20the%20MongoDB%20instance%20and%20obtaining%20the%20raw%20data%20that%20can%20be%20easily%0Aused%20for%20training%20and/or%20fine-tuning%20LMMs.%20To%20enforce%20this%20point%2C%20in%20this%20work%2C%0Awe%20first%20recall%20DR-Minerva%2C%20a%20RAG-based%20LMM%20trained%20using%20MedPix%202.0.%0ADR-Minerva%20predicts%20the%20body%20part%20and%20the%20modality%20used%20to%20scan%20its%20input%0Aimage.%20We%20also%20propose%20the%20extension%20of%20DR-Minerva%20with%20a%20Knowledge%20Graph%20that%0Auses%20Llama%203.1%20Instruct%208B%2C%20and%20leverages%20MedPix%202.0.%20The%20resulting%0Aarchitecture%20can%20be%20queried%20in%20a%20end-to-end%20manner%2C%20as%20a%20medical%20decision%0Asupport%20system.%20MedPix%202.0%20is%20available%20on%20GitHub.%0A%5Curl%7Bhttps%3A//github.com/CHILab1/MedPix-2.0%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedPix%25202.0%253A%2520A%2520Comprehensive%2520Multimodal%2520Biomedical%2520Data%2520set%2520for%2520Advanced%250A%2520%2520AI%2520Applications%2520with%2520Retrieval%2520Augmented%2520Generation%2520and%2520Knowledge%2520Graphs%26entry.906535625%3DIrene%2520Siragusa%2520and%2520Salvatore%2520Contino%2520and%2520Massimo%2520La%2520Ciura%2520and%2520Rosario%2520Alicata%2520and%2520Roberto%2520Pirrone%26entry.1292438233%3D%2520%2520The%2520increasing%2520interest%2520in%2520developing%2520Artificial%2520Intelligence%2520applications%2520in%250Athe%2520medical%2520domain%252C%2520suffers%2520from%2520the%2520lack%2520of%2520high-quality%2520data%2520set%252C%2520mainly%2520due%250Ato%2520privacy-related%2520issues.%2520In%2520addition%252C%2520the%2520recent%2520increase%2520in%2520large%2520multimodal%250Amodels%2520%2528LMM%2529%2520leads%2520to%2520the%2520need%2520for%2520multimodal%2520medical%2520data%2520sets%252C%2520where%2520clinical%250Areports%2520and%2520findings%2520are%2520attached%2520to%2520the%2520corresponding%2520CT%2520or%2520MRI%2520scans.%2520This%250Apaper%2520illustrates%2520the%2520entire%2520workflow%2520for%2520building%2520the%2520MedPix%25202.0%2520data%2520set.%250AStarting%2520with%2520the%2520well-known%2520multimodal%2520data%2520set%250AMedPix%255Ctextsuperscript%257B%255Ctextregistered%257D%252C%2520mainly%2520used%2520by%2520physicians%252C%2520nurses%252C%2520and%250Ahealthcare%2520students%2520for%2520Continuing%2520Medical%2520Education%2520purposes%252C%2520a%2520semi-automatic%250Apipeline%2520was%2520developed%2520to%2520extract%2520visual%2520and%2520textual%2520data%2520followed%2520by%2520a%2520manual%250Acuring%2520procedure%2520in%2520which%2520noisy%2520samples%2520were%2520removed%252C%2520thus%2520creating%2520a%2520MongoDB%250Adatabase.%2520Along%2520with%2520the%2520data%2520set%252C%2520we%2520developed%2520a%2520GUI%2520aimed%2520at%2520navigating%250Aefficiently%2520the%2520MongoDB%2520instance%2520and%2520obtaining%2520the%2520raw%2520data%2520that%2520can%2520be%2520easily%250Aused%2520for%2520training%2520and/or%2520fine-tuning%2520LMMs.%2520To%2520enforce%2520this%2520point%252C%2520in%2520this%2520work%252C%250Awe%2520first%2520recall%2520DR-Minerva%252C%2520a%2520RAG-based%2520LMM%2520trained%2520using%2520MedPix%25202.0.%250ADR-Minerva%2520predicts%2520the%2520body%2520part%2520and%2520the%2520modality%2520used%2520to%2520scan%2520its%2520input%250Aimage.%2520We%2520also%2520propose%2520the%2520extension%2520of%2520DR-Minerva%2520with%2520a%2520Knowledge%2520Graph%2520that%250Auses%2520Llama%25203.1%2520Instruct%25208B%252C%2520and%2520leverages%2520MedPix%25202.0.%2520The%2520resulting%250Aarchitecture%2520can%2520be%2520queried%2520in%2520a%2520end-to-end%2520manner%252C%2520as%2520a%2520medical%2520decision%250Asupport%2520system.%2520MedPix%25202.0%2520is%2520available%2520on%2520GitHub.%250A%255Curl%257Bhttps%253A//github.com/CHILab1/MedPix-2.0%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedPix%202.0%3A%20A%20Comprehensive%20Multimodal%20Biomedical%20Data%20set%20for%20Advanced%0A%20%20AI%20Applications%20with%20Retrieval%20Augmented%20Generation%20and%20Knowledge%20Graphs&entry.906535625=Irene%20Siragusa%20and%20Salvatore%20Contino%20and%20Massimo%20La%20Ciura%20and%20Rosario%20Alicata%20and%20Roberto%20Pirrone&entry.1292438233=%20%20The%20increasing%20interest%20in%20developing%20Artificial%20Intelligence%20applications%20in%0Athe%20medical%20domain%2C%20suffers%20from%20the%20lack%20of%20high-quality%20data%20set%2C%20mainly%20due%0Ato%20privacy-related%20issues.%20In%20addition%2C%20the%20recent%20increase%20in%20large%20multimodal%0Amodels%20%28LMM%29%20leads%20to%20the%20need%20for%20multimodal%20medical%20data%20sets%2C%20where%20clinical%0Areports%20and%20findings%20are%20attached%20to%20the%20corresponding%20CT%20or%20MRI%20scans.%20This%0Apaper%20illustrates%20the%20entire%20workflow%20for%20building%20the%20MedPix%202.0%20data%20set.%0AStarting%20with%20the%20well-known%20multimodal%20data%20set%0AMedPix%5Ctextsuperscript%7B%5Ctextregistered%7D%2C%20mainly%20used%20by%20physicians%2C%20nurses%2C%20and%0Ahealthcare%20students%20for%20Continuing%20Medical%20Education%20purposes%2C%20a%20semi-automatic%0Apipeline%20was%20developed%20to%20extract%20visual%20and%20textual%20data%20followed%20by%20a%20manual%0Acuring%20procedure%20in%20which%20noisy%20samples%20were%20removed%2C%20thus%20creating%20a%20MongoDB%0Adatabase.%20Along%20with%20the%20data%20set%2C%20we%20developed%20a%20GUI%20aimed%20at%20navigating%0Aefficiently%20the%20MongoDB%20instance%20and%20obtaining%20the%20raw%20data%20that%20can%20be%20easily%0Aused%20for%20training%20and/or%20fine-tuning%20LMMs.%20To%20enforce%20this%20point%2C%20in%20this%20work%2C%0Awe%20first%20recall%20DR-Minerva%2C%20a%20RAG-based%20LMM%20trained%20using%20MedPix%202.0.%0ADR-Minerva%20predicts%20the%20body%20part%20and%20the%20modality%20used%20to%20scan%20its%20input%0Aimage.%20We%20also%20propose%20the%20extension%20of%20DR-Minerva%20with%20a%20Knowledge%20Graph%20that%0Auses%20Llama%203.1%20Instruct%208B%2C%20and%20leverages%20MedPix%202.0.%20The%20resulting%0Aarchitecture%20can%20be%20queried%20in%20a%20end-to-end%20manner%2C%20as%20a%20medical%20decision%0Asupport%20system.%20MedPix%202.0%20is%20available%20on%20GitHub.%0A%5Curl%7Bhttps%3A//github.com/CHILab1/MedPix-2.0%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02994v2&entry.124074799=Read"},
{"title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and\n  Future Directions", "author": "Doaa Mahmud and Hadeel Hajmohamed and Shamma Almentheri and Shamma Alqaydi and Lameya Aldhaheri and Ruhul Amin Khalil and Nasir Saeed", "abstract": "  Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems.\n", "link": "http://arxiv.org/abs/2501.04437v1", "date": "2025-01-08", "relevancy": 2.2016, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20LLMs%20with%20ITS%3A%20Recent%20Advances%2C%20Potentials%2C%20Challenges%2C%20and%0A%20%20Future%20Directions&body=Title%3A%20Integrating%20LLMs%20with%20ITS%3A%20Recent%20Advances%2C%20Potentials%2C%20Challenges%2C%20and%0A%20%20Future%20Directions%0AAuthor%3A%20Doaa%20Mahmud%20and%20Hadeel%20Hajmohamed%20and%20Shamma%20Almentheri%20and%20Shamma%20Alqaydi%20and%20Lameya%20Aldhaheri%20and%20Ruhul%20Amin%20Khalil%20and%20Nasir%20Saeed%0AAbstract%3A%20%20%20Intelligent%20Transportation%20Systems%20%28ITS%29%20are%20crucial%20for%20the%20development%20and%0Aoperation%20of%20smart%20cities%2C%20addressing%20key%20challenges%20in%20efficiency%2C%0Aproductivity%2C%20and%20environmental%20sustainability.%20This%20paper%20comprehensively%0Areviews%20the%20transformative%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%0Aoptimizing%20ITS.%20Initially%2C%20we%20provide%20an%20extensive%20overview%20of%20ITS%2C%0Ahighlighting%20its%20components%2C%20operational%20principles%2C%20and%20overall%20effectiveness.%0AWe%20then%20delve%20into%20the%20theoretical%20background%20of%20various%20LLM%20techniques%2C%20such%0Aas%20GPT%2C%20T5%2C%20CTRL%2C%20and%20BERT%2C%20elucidating%20their%20relevance%20to%20ITS%20applications.%0AFollowing%20this%2C%20we%20examine%20the%20wide-ranging%20applications%20of%20LLMs%20within%20ITS%2C%0Aincluding%20traffic%20flow%20prediction%2C%20vehicle%20detection%20and%20classification%2C%0Aautonomous%20driving%2C%20traffic%20sign%20recognition%2C%20and%20pedestrian%20detection.%20Our%0Aanalysis%20reveals%20how%20these%20advanced%20models%20can%20significantly%20enhance%20traffic%0Amanagement%20and%20safety.%20Finally%2C%20we%20explore%20the%20challenges%20and%20limitations%20LLMs%0Aface%20in%20ITS%2C%20such%20as%20data%20availability%2C%20computational%20constraints%2C%20and%20ethical%0Aconsiderations.%20We%20also%20present%20several%20future%20research%20directions%20and%0Apotential%20innovations%20to%20address%20these%20challenges.%20This%20paper%20aims%20to%20guide%0Aresearchers%20and%20practitioners%20through%20the%20complexities%20and%20opportunities%20of%0Aintegrating%20LLMs%20in%20ITS%2C%20offering%20a%20roadmap%20to%20create%20more%20efficient%2C%0Asustainable%2C%20and%20responsive%20next-generation%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520LLMs%2520with%2520ITS%253A%2520Recent%2520Advances%252C%2520Potentials%252C%2520Challenges%252C%2520and%250A%2520%2520Future%2520Directions%26entry.906535625%3DDoaa%2520Mahmud%2520and%2520Hadeel%2520Hajmohamed%2520and%2520Shamma%2520Almentheri%2520and%2520Shamma%2520Alqaydi%2520and%2520Lameya%2520Aldhaheri%2520and%2520Ruhul%2520Amin%2520Khalil%2520and%2520Nasir%2520Saeed%26entry.1292438233%3D%2520%2520Intelligent%2520Transportation%2520Systems%2520%2528ITS%2529%2520are%2520crucial%2520for%2520the%2520development%2520and%250Aoperation%2520of%2520smart%2520cities%252C%2520addressing%2520key%2520challenges%2520in%2520efficiency%252C%250Aproductivity%252C%2520and%2520environmental%2520sustainability.%2520This%2520paper%2520comprehensively%250Areviews%2520the%2520transformative%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%250Aoptimizing%2520ITS.%2520Initially%252C%2520we%2520provide%2520an%2520extensive%2520overview%2520of%2520ITS%252C%250Ahighlighting%2520its%2520components%252C%2520operational%2520principles%252C%2520and%2520overall%2520effectiveness.%250AWe%2520then%2520delve%2520into%2520the%2520theoretical%2520background%2520of%2520various%2520LLM%2520techniques%252C%2520such%250Aas%2520GPT%252C%2520T5%252C%2520CTRL%252C%2520and%2520BERT%252C%2520elucidating%2520their%2520relevance%2520to%2520ITS%2520applications.%250AFollowing%2520this%252C%2520we%2520examine%2520the%2520wide-ranging%2520applications%2520of%2520LLMs%2520within%2520ITS%252C%250Aincluding%2520traffic%2520flow%2520prediction%252C%2520vehicle%2520detection%2520and%2520classification%252C%250Aautonomous%2520driving%252C%2520traffic%2520sign%2520recognition%252C%2520and%2520pedestrian%2520detection.%2520Our%250Aanalysis%2520reveals%2520how%2520these%2520advanced%2520models%2520can%2520significantly%2520enhance%2520traffic%250Amanagement%2520and%2520safety.%2520Finally%252C%2520we%2520explore%2520the%2520challenges%2520and%2520limitations%2520LLMs%250Aface%2520in%2520ITS%252C%2520such%2520as%2520data%2520availability%252C%2520computational%2520constraints%252C%2520and%2520ethical%250Aconsiderations.%2520We%2520also%2520present%2520several%2520future%2520research%2520directions%2520and%250Apotential%2520innovations%2520to%2520address%2520these%2520challenges.%2520This%2520paper%2520aims%2520to%2520guide%250Aresearchers%2520and%2520practitioners%2520through%2520the%2520complexities%2520and%2520opportunities%2520of%250Aintegrating%2520LLMs%2520in%2520ITS%252C%2520offering%2520a%2520roadmap%2520to%2520create%2520more%2520efficient%252C%250Asustainable%252C%2520and%2520responsive%2520next-generation%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20LLMs%20with%20ITS%3A%20Recent%20Advances%2C%20Potentials%2C%20Challenges%2C%20and%0A%20%20Future%20Directions&entry.906535625=Doaa%20Mahmud%20and%20Hadeel%20Hajmohamed%20and%20Shamma%20Almentheri%20and%20Shamma%20Alqaydi%20and%20Lameya%20Aldhaheri%20and%20Ruhul%20Amin%20Khalil%20and%20Nasir%20Saeed&entry.1292438233=%20%20Intelligent%20Transportation%20Systems%20%28ITS%29%20are%20crucial%20for%20the%20development%20and%0Aoperation%20of%20smart%20cities%2C%20addressing%20key%20challenges%20in%20efficiency%2C%0Aproductivity%2C%20and%20environmental%20sustainability.%20This%20paper%20comprehensively%0Areviews%20the%20transformative%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%0Aoptimizing%20ITS.%20Initially%2C%20we%20provide%20an%20extensive%20overview%20of%20ITS%2C%0Ahighlighting%20its%20components%2C%20operational%20principles%2C%20and%20overall%20effectiveness.%0AWe%20then%20delve%20into%20the%20theoretical%20background%20of%20various%20LLM%20techniques%2C%20such%0Aas%20GPT%2C%20T5%2C%20CTRL%2C%20and%20BERT%2C%20elucidating%20their%20relevance%20to%20ITS%20applications.%0AFollowing%20this%2C%20we%20examine%20the%20wide-ranging%20applications%20of%20LLMs%20within%20ITS%2C%0Aincluding%20traffic%20flow%20prediction%2C%20vehicle%20detection%20and%20classification%2C%0Aautonomous%20driving%2C%20traffic%20sign%20recognition%2C%20and%20pedestrian%20detection.%20Our%0Aanalysis%20reveals%20how%20these%20advanced%20models%20can%20significantly%20enhance%20traffic%0Amanagement%20and%20safety.%20Finally%2C%20we%20explore%20the%20challenges%20and%20limitations%20LLMs%0Aface%20in%20ITS%2C%20such%20as%20data%20availability%2C%20computational%20constraints%2C%20and%20ethical%0Aconsiderations.%20We%20also%20present%20several%20future%20research%20directions%20and%0Apotential%20innovations%20to%20address%20these%20challenges.%20This%20paper%20aims%20to%20guide%0Aresearchers%20and%20practitioners%20through%20the%20complexities%20and%20opportunities%20of%0Aintegrating%20LLMs%20in%20ITS%2C%20offering%20a%20roadmap%20to%20create%20more%20efficient%2C%0Asustainable%2C%20and%20responsive%20next-generation%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04437v1&entry.124074799=Read"},
{"title": "ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic\n  Segmentation", "author": "Jingyun Wang and Guoliang Kang", "abstract": "  Recent works utilize CLIP to perform the challenging unsupervised semantic\nsegmentation task where only images without annotations are available. However,\nwe observe that when adopting CLIP to such a pixel-level understanding task,\nunexpected bias (including class-preference bias and space-preference bias)\noccurs. Previous works don't explicitly model the bias, which largely\nconstrains the segmentation performance. In this paper, we propose to\nexplicitly model and rectify the bias existing in CLIP to facilitate the\nunsupervised semantic segmentation task. Specifically, we design a learnable\n\"Reference\" prompt to encode class-preference bias and a projection of the\npositional embedding in the vision transformer to encode space-preference bias\nrespectively. To avoid interference, two kinds of biases are firstly\nindependently encoded into different features, i.e., the Reference feature and\nthe positional feature. Via a matrix multiplication between the Reference\nfeature and the positional feature, a bias logit map is generated to explicitly\nrepresent two kinds of biases. Then we rectify the logits of CLIP via a simple\nelement-wise subtraction. To make the rectified results smoother and more\ncontextual, we design a mask decoder which takes the feature of CLIP and the\nrectified logits as input and outputs a rectified segmentation mask with the\nhelp of Gumbel-Softmax operation. A contrastive loss based on the masked visual\nfeatures and the text features of different classes is imposed, which makes the\nbias modeling and rectification process meaningful and effective. Extensive\nexperiments on various benchmarks including PASCAL VOC, PASCAL Context, ADE20K,\nCityscapes, and COCO Stuff demonstrate that our method performs favorably\nagainst previous state-of-the-arts. The implementation is available at:\nhttps://github.com/dogehhh/ReCLIP.\n", "link": "http://arxiv.org/abs/2408.06747v2", "date": "2025-01-08", "relevancy": 2.1798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5745}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCLIP%2B%2B%3A%20Learn%20to%20Rectify%20the%20Bias%20of%20CLIP%20for%20Unsupervised%20Semantic%0A%20%20Segmentation&body=Title%3A%20ReCLIP%2B%2B%3A%20Learn%20to%20Rectify%20the%20Bias%20of%20CLIP%20for%20Unsupervised%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jingyun%20Wang%20and%20Guoliang%20Kang%0AAbstract%3A%20%20%20Recent%20works%20utilize%20CLIP%20to%20perform%20the%20challenging%20unsupervised%20semantic%0Asegmentation%20task%20where%20only%20images%20without%20annotations%20are%20available.%20However%2C%0Awe%20observe%20that%20when%20adopting%20CLIP%20to%20such%20a%20pixel-level%20understanding%20task%2C%0Aunexpected%20bias%20%28including%20class-preference%20bias%20and%20space-preference%20bias%29%0Aoccurs.%20Previous%20works%20don%27t%20explicitly%20model%20the%20bias%2C%20which%20largely%0Aconstrains%20the%20segmentation%20performance.%20In%20this%20paper%2C%20we%20propose%20to%0Aexplicitly%20model%20and%20rectify%20the%20bias%20existing%20in%20CLIP%20to%20facilitate%20the%0Aunsupervised%20semantic%20segmentation%20task.%20Specifically%2C%20we%20design%20a%20learnable%0A%22Reference%22%20prompt%20to%20encode%20class-preference%20bias%20and%20a%20projection%20of%20the%0Apositional%20embedding%20in%20the%20vision%20transformer%20to%20encode%20space-preference%20bias%0Arespectively.%20To%20avoid%20interference%2C%20two%20kinds%20of%20biases%20are%20firstly%0Aindependently%20encoded%20into%20different%20features%2C%20i.e.%2C%20the%20Reference%20feature%20and%0Athe%20positional%20feature.%20Via%20a%20matrix%20multiplication%20between%20the%20Reference%0Afeature%20and%20the%20positional%20feature%2C%20a%20bias%20logit%20map%20is%20generated%20to%20explicitly%0Arepresent%20two%20kinds%20of%20biases.%20Then%20we%20rectify%20the%20logits%20of%20CLIP%20via%20a%20simple%0Aelement-wise%20subtraction.%20To%20make%20the%20rectified%20results%20smoother%20and%20more%0Acontextual%2C%20we%20design%20a%20mask%20decoder%20which%20takes%20the%20feature%20of%20CLIP%20and%20the%0Arectified%20logits%20as%20input%20and%20outputs%20a%20rectified%20segmentation%20mask%20with%20the%0Ahelp%20of%20Gumbel-Softmax%20operation.%20A%20contrastive%20loss%20based%20on%20the%20masked%20visual%0Afeatures%20and%20the%20text%20features%20of%20different%20classes%20is%20imposed%2C%20which%20makes%20the%0Abias%20modeling%20and%20rectification%20process%20meaningful%20and%20effective.%20Extensive%0Aexperiments%20on%20various%20benchmarks%20including%20PASCAL%20VOC%2C%20PASCAL%20Context%2C%20ADE20K%2C%0ACityscapes%2C%20and%20COCO%20Stuff%20demonstrate%20that%20our%20method%20performs%20favorably%0Aagainst%20previous%20state-of-the-arts.%20The%20implementation%20is%20available%20at%3A%0Ahttps%3A//github.com/dogehhh/ReCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCLIP%252B%252B%253A%2520Learn%2520to%2520Rectify%2520the%2520Bias%2520of%2520CLIP%2520for%2520Unsupervised%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DJingyun%2520Wang%2520and%2520Guoliang%2520Kang%26entry.1292438233%3D%2520%2520Recent%2520works%2520utilize%2520CLIP%2520to%2520perform%2520the%2520challenging%2520unsupervised%2520semantic%250Asegmentation%2520task%2520where%2520only%2520images%2520without%2520annotations%2520are%2520available.%2520However%252C%250Awe%2520observe%2520that%2520when%2520adopting%2520CLIP%2520to%2520such%2520a%2520pixel-level%2520understanding%2520task%252C%250Aunexpected%2520bias%2520%2528including%2520class-preference%2520bias%2520and%2520space-preference%2520bias%2529%250Aoccurs.%2520Previous%2520works%2520don%2527t%2520explicitly%2520model%2520the%2520bias%252C%2520which%2520largely%250Aconstrains%2520the%2520segmentation%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%250Aexplicitly%2520model%2520and%2520rectify%2520the%2520bias%2520existing%2520in%2520CLIP%2520to%2520facilitate%2520the%250Aunsupervised%2520semantic%2520segmentation%2520task.%2520Specifically%252C%2520we%2520design%2520a%2520learnable%250A%2522Reference%2522%2520prompt%2520to%2520encode%2520class-preference%2520bias%2520and%2520a%2520projection%2520of%2520the%250Apositional%2520embedding%2520in%2520the%2520vision%2520transformer%2520to%2520encode%2520space-preference%2520bias%250Arespectively.%2520To%2520avoid%2520interference%252C%2520two%2520kinds%2520of%2520biases%2520are%2520firstly%250Aindependently%2520encoded%2520into%2520different%2520features%252C%2520i.e.%252C%2520the%2520Reference%2520feature%2520and%250Athe%2520positional%2520feature.%2520Via%2520a%2520matrix%2520multiplication%2520between%2520the%2520Reference%250Afeature%2520and%2520the%2520positional%2520feature%252C%2520a%2520bias%2520logit%2520map%2520is%2520generated%2520to%2520explicitly%250Arepresent%2520two%2520kinds%2520of%2520biases.%2520Then%2520we%2520rectify%2520the%2520logits%2520of%2520CLIP%2520via%2520a%2520simple%250Aelement-wise%2520subtraction.%2520To%2520make%2520the%2520rectified%2520results%2520smoother%2520and%2520more%250Acontextual%252C%2520we%2520design%2520a%2520mask%2520decoder%2520which%2520takes%2520the%2520feature%2520of%2520CLIP%2520and%2520the%250Arectified%2520logits%2520as%2520input%2520and%2520outputs%2520a%2520rectified%2520segmentation%2520mask%2520with%2520the%250Ahelp%2520of%2520Gumbel-Softmax%2520operation.%2520A%2520contrastive%2520loss%2520based%2520on%2520the%2520masked%2520visual%250Afeatures%2520and%2520the%2520text%2520features%2520of%2520different%2520classes%2520is%2520imposed%252C%2520which%2520makes%2520the%250Abias%2520modeling%2520and%2520rectification%2520process%2520meaningful%2520and%2520effective.%2520Extensive%250Aexperiments%2520on%2520various%2520benchmarks%2520including%2520PASCAL%2520VOC%252C%2520PASCAL%2520Context%252C%2520ADE20K%252C%250ACityscapes%252C%2520and%2520COCO%2520Stuff%2520demonstrate%2520that%2520our%2520method%2520performs%2520favorably%250Aagainst%2520previous%2520state-of-the-arts.%2520The%2520implementation%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/dogehhh/ReCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCLIP%2B%2B%3A%20Learn%20to%20Rectify%20the%20Bias%20of%20CLIP%20for%20Unsupervised%20Semantic%0A%20%20Segmentation&entry.906535625=Jingyun%20Wang%20and%20Guoliang%20Kang&entry.1292438233=%20%20Recent%20works%20utilize%20CLIP%20to%20perform%20the%20challenging%20unsupervised%20semantic%0Asegmentation%20task%20where%20only%20images%20without%20annotations%20are%20available.%20However%2C%0Awe%20observe%20that%20when%20adopting%20CLIP%20to%20such%20a%20pixel-level%20understanding%20task%2C%0Aunexpected%20bias%20%28including%20class-preference%20bias%20and%20space-preference%20bias%29%0Aoccurs.%20Previous%20works%20don%27t%20explicitly%20model%20the%20bias%2C%20which%20largely%0Aconstrains%20the%20segmentation%20performance.%20In%20this%20paper%2C%20we%20propose%20to%0Aexplicitly%20model%20and%20rectify%20the%20bias%20existing%20in%20CLIP%20to%20facilitate%20the%0Aunsupervised%20semantic%20segmentation%20task.%20Specifically%2C%20we%20design%20a%20learnable%0A%22Reference%22%20prompt%20to%20encode%20class-preference%20bias%20and%20a%20projection%20of%20the%0Apositional%20embedding%20in%20the%20vision%20transformer%20to%20encode%20space-preference%20bias%0Arespectively.%20To%20avoid%20interference%2C%20two%20kinds%20of%20biases%20are%20firstly%0Aindependently%20encoded%20into%20different%20features%2C%20i.e.%2C%20the%20Reference%20feature%20and%0Athe%20positional%20feature.%20Via%20a%20matrix%20multiplication%20between%20the%20Reference%0Afeature%20and%20the%20positional%20feature%2C%20a%20bias%20logit%20map%20is%20generated%20to%20explicitly%0Arepresent%20two%20kinds%20of%20biases.%20Then%20we%20rectify%20the%20logits%20of%20CLIP%20via%20a%20simple%0Aelement-wise%20subtraction.%20To%20make%20the%20rectified%20results%20smoother%20and%20more%0Acontextual%2C%20we%20design%20a%20mask%20decoder%20which%20takes%20the%20feature%20of%20CLIP%20and%20the%0Arectified%20logits%20as%20input%20and%20outputs%20a%20rectified%20segmentation%20mask%20with%20the%0Ahelp%20of%20Gumbel-Softmax%20operation.%20A%20contrastive%20loss%20based%20on%20the%20masked%20visual%0Afeatures%20and%20the%20text%20features%20of%20different%20classes%20is%20imposed%2C%20which%20makes%20the%0Abias%20modeling%20and%20rectification%20process%20meaningful%20and%20effective.%20Extensive%0Aexperiments%20on%20various%20benchmarks%20including%20PASCAL%20VOC%2C%20PASCAL%20Context%2C%20ADE20K%2C%0ACityscapes%2C%20and%20COCO%20Stuff%20demonstrate%20that%20our%20method%20performs%20favorably%0Aagainst%20previous%20state-of-the-arts.%20The%20implementation%20is%20available%20at%3A%0Ahttps%3A//github.com/dogehhh/ReCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06747v2&entry.124074799=Read"},
{"title": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar", "author": "Wesley Scivetti and Melissa Torgbi and Austin Blodgett and Mollie Shichman and Taylor Hudson and Claire Bonial and Harish Tayyar Madabushi", "abstract": "  Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.\n", "link": "http://arxiv.org/abs/2501.04661v1", "date": "2025-01-08", "relevancy": 2.1705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Language%20Comprehension%20in%20Large%20Language%20Models%20Using%0A%20%20Construction%20Grammar&body=Title%3A%20Assessing%20Language%20Comprehension%20in%20Large%20Language%20Models%20Using%0A%20%20Construction%20Grammar%0AAuthor%3A%20Wesley%20Scivetti%20and%20Melissa%20Torgbi%20and%20Austin%20Blodgett%20and%20Mollie%20Shichman%20and%20Taylor%20Hudson%20and%20Claire%20Bonial%20and%20Harish%20Tayyar%20Madabushi%0AAbstract%3A%20%20%20Large%20Language%20Models%2C%20despite%20their%20significant%20capabilities%2C%20are%20known%20to%0Afail%20in%20surprising%20and%20unpredictable%20ways.%20Evaluating%20their%20true%0A%60understanding%27%20of%20language%20is%20particularly%20challenging%20due%20to%20the%20extensive%0Aweb-scale%20data%20they%20are%20trained%20on.%20Therefore%2C%20we%20construct%20an%20evaluation%20to%0Asystematically%20assess%20natural%20language%20understanding%20%28NLU%29%20in%20LLMs%20by%0Aleveraging%20Construction%20Grammar%20%28CxG%29%2C%20which%20provides%20insights%20into%20the%20meaning%0Acaptured%20by%20linguistic%20elements%20known%20as%20constructions%20%28Cxns%29.%20CxG%20is%0Awell-suited%20for%20this%20purpose%20because%20provides%20a%20theoretical%20basis%20to%20construct%0Atargeted%20evaluation%20sets.%20These%20datasets%20are%20carefully%20constructed%20to%20include%0Aexamples%20which%20are%20unlikely%20to%20appear%20in%20pre-training%20data%2C%20yet%20intuitive%20and%0Aeasy%20for%20humans%20to%20understand%2C%20enabling%20a%20more%20targeted%20and%20reliable%0Aassessment.%20Our%20experiments%20focus%20on%20downstream%20natural%20language%20inference%20and%0Areasoning%20tasks%20by%20comparing%20LLMs%27%20understanding%20of%20the%20underlying%20meanings%0Acommunicated%20through%208%20unique%20Cxns%20with%20that%20of%20humans.%20The%20results%20show%20that%0Awhile%20LLMs%20demonstrate%20some%20knowledge%20of%20constructional%20information%2C%20even%20the%0Alatest%20models%20including%20GPT-o1%20struggle%20with%20abstract%20meanings%20conveyed%20by%0Athese%20Cxns%2C%20as%20demonstrated%20in%20cases%20where%20test%20sentences%20are%20dissimilar%20to%0Atheir%20pre-training%20data.%20We%20argue%20that%20such%20cases%20provide%20a%20more%20accurate%20test%0Aof%20true%20language%20understanding%2C%20highlighting%20key%20limitations%20in%20LLMs%27%20semantic%0Acapabilities.%20We%20make%20our%20novel%20dataset%20and%20associated%20experimental%20data%0Aincluding%20prompts%20and%20model%20responses%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Language%2520Comprehension%2520in%2520Large%2520Language%2520Models%2520Using%250A%2520%2520Construction%2520Grammar%26entry.906535625%3DWesley%2520Scivetti%2520and%2520Melissa%2520Torgbi%2520and%2520Austin%2520Blodgett%2520and%2520Mollie%2520Shichman%2520and%2520Taylor%2520Hudson%2520and%2520Claire%2520Bonial%2520and%2520Harish%2520Tayyar%2520Madabushi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%252C%2520despite%2520their%2520significant%2520capabilities%252C%2520are%2520known%2520to%250Afail%2520in%2520surprising%2520and%2520unpredictable%2520ways.%2520Evaluating%2520their%2520true%250A%2560understanding%2527%2520of%2520language%2520is%2520particularly%2520challenging%2520due%2520to%2520the%2520extensive%250Aweb-scale%2520data%2520they%2520are%2520trained%2520on.%2520Therefore%252C%2520we%2520construct%2520an%2520evaluation%2520to%250Asystematically%2520assess%2520natural%2520language%2520understanding%2520%2528NLU%2529%2520in%2520LLMs%2520by%250Aleveraging%2520Construction%2520Grammar%2520%2528CxG%2529%252C%2520which%2520provides%2520insights%2520into%2520the%2520meaning%250Acaptured%2520by%2520linguistic%2520elements%2520known%2520as%2520constructions%2520%2528Cxns%2529.%2520CxG%2520is%250Awell-suited%2520for%2520this%2520purpose%2520because%2520provides%2520a%2520theoretical%2520basis%2520to%2520construct%250Atargeted%2520evaluation%2520sets.%2520These%2520datasets%2520are%2520carefully%2520constructed%2520to%2520include%250Aexamples%2520which%2520are%2520unlikely%2520to%2520appear%2520in%2520pre-training%2520data%252C%2520yet%2520intuitive%2520and%250Aeasy%2520for%2520humans%2520to%2520understand%252C%2520enabling%2520a%2520more%2520targeted%2520and%2520reliable%250Aassessment.%2520Our%2520experiments%2520focus%2520on%2520downstream%2520natural%2520language%2520inference%2520and%250Areasoning%2520tasks%2520by%2520comparing%2520LLMs%2527%2520understanding%2520of%2520the%2520underlying%2520meanings%250Acommunicated%2520through%25208%2520unique%2520Cxns%2520with%2520that%2520of%2520humans.%2520The%2520results%2520show%2520that%250Awhile%2520LLMs%2520demonstrate%2520some%2520knowledge%2520of%2520constructional%2520information%252C%2520even%2520the%250Alatest%2520models%2520including%2520GPT-o1%2520struggle%2520with%2520abstract%2520meanings%2520conveyed%2520by%250Athese%2520Cxns%252C%2520as%2520demonstrated%2520in%2520cases%2520where%2520test%2520sentences%2520are%2520dissimilar%2520to%250Atheir%2520pre-training%2520data.%2520We%2520argue%2520that%2520such%2520cases%2520provide%2520a%2520more%2520accurate%2520test%250Aof%2520true%2520language%2520understanding%252C%2520highlighting%2520key%2520limitations%2520in%2520LLMs%2527%2520semantic%250Acapabilities.%2520We%2520make%2520our%2520novel%2520dataset%2520and%2520associated%2520experimental%2520data%250Aincluding%2520prompts%2520and%2520model%2520responses%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Language%20Comprehension%20in%20Large%20Language%20Models%20Using%0A%20%20Construction%20Grammar&entry.906535625=Wesley%20Scivetti%20and%20Melissa%20Torgbi%20and%20Austin%20Blodgett%20and%20Mollie%20Shichman%20and%20Taylor%20Hudson%20and%20Claire%20Bonial%20and%20Harish%20Tayyar%20Madabushi&entry.1292438233=%20%20Large%20Language%20Models%2C%20despite%20their%20significant%20capabilities%2C%20are%20known%20to%0Afail%20in%20surprising%20and%20unpredictable%20ways.%20Evaluating%20their%20true%0A%60understanding%27%20of%20language%20is%20particularly%20challenging%20due%20to%20the%20extensive%0Aweb-scale%20data%20they%20are%20trained%20on.%20Therefore%2C%20we%20construct%20an%20evaluation%20to%0Asystematically%20assess%20natural%20language%20understanding%20%28NLU%29%20in%20LLMs%20by%0Aleveraging%20Construction%20Grammar%20%28CxG%29%2C%20which%20provides%20insights%20into%20the%20meaning%0Acaptured%20by%20linguistic%20elements%20known%20as%20constructions%20%28Cxns%29.%20CxG%20is%0Awell-suited%20for%20this%20purpose%20because%20provides%20a%20theoretical%20basis%20to%20construct%0Atargeted%20evaluation%20sets.%20These%20datasets%20are%20carefully%20constructed%20to%20include%0Aexamples%20which%20are%20unlikely%20to%20appear%20in%20pre-training%20data%2C%20yet%20intuitive%20and%0Aeasy%20for%20humans%20to%20understand%2C%20enabling%20a%20more%20targeted%20and%20reliable%0Aassessment.%20Our%20experiments%20focus%20on%20downstream%20natural%20language%20inference%20and%0Areasoning%20tasks%20by%20comparing%20LLMs%27%20understanding%20of%20the%20underlying%20meanings%0Acommunicated%20through%208%20unique%20Cxns%20with%20that%20of%20humans.%20The%20results%20show%20that%0Awhile%20LLMs%20demonstrate%20some%20knowledge%20of%20constructional%20information%2C%20even%20the%0Alatest%20models%20including%20GPT-o1%20struggle%20with%20abstract%20meanings%20conveyed%20by%0Athese%20Cxns%2C%20as%20demonstrated%20in%20cases%20where%20test%20sentences%20are%20dissimilar%20to%0Atheir%20pre-training%20data.%20We%20argue%20that%20such%20cases%20provide%20a%20more%20accurate%20test%0Aof%20true%20language%20understanding%2C%20highlighting%20key%20limitations%20in%20LLMs%27%20semantic%0Acapabilities.%20We%20make%20our%20novel%20dataset%20and%20associated%20experimental%20data%0Aincluding%20prompts%20and%20model%20responses%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04661v1&entry.124074799=Read"},
{"title": "LeGrad: An Explainability Method for Vision Transformers via Feature\n  Formation Sensitivity", "author": "Walid Bousselham and Angie Boggust and Sofian Chaybouti and Hendrik Strobelt and Hilde Kuehne", "abstract": "  Vision Transformers (ViTs), with their ability to model long-range\ndependencies through self-attention mechanisms, have become a standard\narchitecture in computer vision. However, the interpretability of these models\nremains a challenge. To address this, we propose LeGrad, an explainability\nmethod specifically designed for ViTs. LeGrad computes the gradient with\nrespect to the attention maps of ViT layers, considering the gradient itself as\nthe explainability signal. We aggregate the signal over all layers, combining\nthe activations of the last as well as intermediate tokens to produce the\nmerged explainability map. This makes LeGrad a conceptually simple and an\neasy-to-implement tool for enhancing the transparency of ViTs. We evaluate\nLeGrad in challenging segmentation, perturbation, and open-vocabulary settings,\nshowcasing its versatility compared to other SotA explainability methods\ndemonstrating its superior spatial fidelity and robustness to perturbations. A\ndemo and the code is available at https://github.com/WalBouss/LeGrad.\n", "link": "http://arxiv.org/abs/2404.03214v2", "date": "2025-01-08", "relevancy": 2.1667, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeGrad%3A%20An%20Explainability%20Method%20for%20Vision%20Transformers%20via%20Feature%0A%20%20Formation%20Sensitivity&body=Title%3A%20LeGrad%3A%20An%20Explainability%20Method%20for%20Vision%20Transformers%20via%20Feature%0A%20%20Formation%20Sensitivity%0AAuthor%3A%20Walid%20Bousselham%20and%20Angie%20Boggust%20and%20Sofian%20Chaybouti%20and%20Hendrik%20Strobelt%20and%20Hilde%20Kuehne%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%2C%20with%20their%20ability%20to%20model%20long-range%0Adependencies%20through%20self-attention%20mechanisms%2C%20have%20become%20a%20standard%0Aarchitecture%20in%20computer%20vision.%20However%2C%20the%20interpretability%20of%20these%20models%0Aremains%20a%20challenge.%20To%20address%20this%2C%20we%20propose%20LeGrad%2C%20an%20explainability%0Amethod%20specifically%20designed%20for%20ViTs.%20LeGrad%20computes%20the%20gradient%20with%0Arespect%20to%20the%20attention%20maps%20of%20ViT%20layers%2C%20considering%20the%20gradient%20itself%20as%0Athe%20explainability%20signal.%20We%20aggregate%20the%20signal%20over%20all%20layers%2C%20combining%0Athe%20activations%20of%20the%20last%20as%20well%20as%20intermediate%20tokens%20to%20produce%20the%0Amerged%20explainability%20map.%20This%20makes%20LeGrad%20a%20conceptually%20simple%20and%20an%0Aeasy-to-implement%20tool%20for%20enhancing%20the%20transparency%20of%20ViTs.%20We%20evaluate%0ALeGrad%20in%20challenging%20segmentation%2C%20perturbation%2C%20and%20open-vocabulary%20settings%2C%0Ashowcasing%20its%20versatility%20compared%20to%20other%20SotA%20explainability%20methods%0Ademonstrating%20its%20superior%20spatial%20fidelity%20and%20robustness%20to%20perturbations.%20A%0Ademo%20and%20the%20code%20is%20available%20at%20https%3A//github.com/WalBouss/LeGrad.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeGrad%253A%2520An%2520Explainability%2520Method%2520for%2520Vision%2520Transformers%2520via%2520Feature%250A%2520%2520Formation%2520Sensitivity%26entry.906535625%3DWalid%2520Bousselham%2520and%2520Angie%2520Boggust%2520and%2520Sofian%2520Chaybouti%2520and%2520Hendrik%2520Strobelt%2520and%2520Hilde%2520Kuehne%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520with%2520their%2520ability%2520to%2520model%2520long-range%250Adependencies%2520through%2520self-attention%2520mechanisms%252C%2520have%2520become%2520a%2520standard%250Aarchitecture%2520in%2520computer%2520vision.%2520However%252C%2520the%2520interpretability%2520of%2520these%2520models%250Aremains%2520a%2520challenge.%2520To%2520address%2520this%252C%2520we%2520propose%2520LeGrad%252C%2520an%2520explainability%250Amethod%2520specifically%2520designed%2520for%2520ViTs.%2520LeGrad%2520computes%2520the%2520gradient%2520with%250Arespect%2520to%2520the%2520attention%2520maps%2520of%2520ViT%2520layers%252C%2520considering%2520the%2520gradient%2520itself%2520as%250Athe%2520explainability%2520signal.%2520We%2520aggregate%2520the%2520signal%2520over%2520all%2520layers%252C%2520combining%250Athe%2520activations%2520of%2520the%2520last%2520as%2520well%2520as%2520intermediate%2520tokens%2520to%2520produce%2520the%250Amerged%2520explainability%2520map.%2520This%2520makes%2520LeGrad%2520a%2520conceptually%2520simple%2520and%2520an%250Aeasy-to-implement%2520tool%2520for%2520enhancing%2520the%2520transparency%2520of%2520ViTs.%2520We%2520evaluate%250ALeGrad%2520in%2520challenging%2520segmentation%252C%2520perturbation%252C%2520and%2520open-vocabulary%2520settings%252C%250Ashowcasing%2520its%2520versatility%2520compared%2520to%2520other%2520SotA%2520explainability%2520methods%250Ademonstrating%2520its%2520superior%2520spatial%2520fidelity%2520and%2520robustness%2520to%2520perturbations.%2520A%250Ademo%2520and%2520the%2520code%2520is%2520available%2520at%2520https%253A//github.com/WalBouss/LeGrad.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeGrad%3A%20An%20Explainability%20Method%20for%20Vision%20Transformers%20via%20Feature%0A%20%20Formation%20Sensitivity&entry.906535625=Walid%20Bousselham%20and%20Angie%20Boggust%20and%20Sofian%20Chaybouti%20and%20Hendrik%20Strobelt%20and%20Hilde%20Kuehne&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%2C%20with%20their%20ability%20to%20model%20long-range%0Adependencies%20through%20self-attention%20mechanisms%2C%20have%20become%20a%20standard%0Aarchitecture%20in%20computer%20vision.%20However%2C%20the%20interpretability%20of%20these%20models%0Aremains%20a%20challenge.%20To%20address%20this%2C%20we%20propose%20LeGrad%2C%20an%20explainability%0Amethod%20specifically%20designed%20for%20ViTs.%20LeGrad%20computes%20the%20gradient%20with%0Arespect%20to%20the%20attention%20maps%20of%20ViT%20layers%2C%20considering%20the%20gradient%20itself%20as%0Athe%20explainability%20signal.%20We%20aggregate%20the%20signal%20over%20all%20layers%2C%20combining%0Athe%20activations%20of%20the%20last%20as%20well%20as%20intermediate%20tokens%20to%20produce%20the%0Amerged%20explainability%20map.%20This%20makes%20LeGrad%20a%20conceptually%20simple%20and%20an%0Aeasy-to-implement%20tool%20for%20enhancing%20the%20transparency%20of%20ViTs.%20We%20evaluate%0ALeGrad%20in%20challenging%20segmentation%2C%20perturbation%2C%20and%20open-vocabulary%20settings%2C%0Ashowcasing%20its%20versatility%20compared%20to%20other%20SotA%20explainability%20methods%0Ademonstrating%20its%20superior%20spatial%20fidelity%20and%20robustness%20to%20perturbations.%20A%0Ademo%20and%20the%20code%20is%20available%20at%20https%3A//github.com/WalBouss/LeGrad.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03214v2&entry.124074799=Read"},
{"title": "Evaluating Image Caption via Cycle-consistent Text-to-Image Generation", "author": "Tianyu Cui and Jinbin Bai and Guo-Hua Wang and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and Ye Shi", "abstract": "  Evaluating image captions typically relies on reference captions, which are\ncostly to obtain and exhibit significant diversity and subjectivity. While\nreference-free evaluation metrics have been proposed, most focus on cross-modal\nevaluation between captions and images. Recent research has revealed that the\nmodality gap generally exists in the representation of contrastive\nlearning-based multi-modal systems, undermining the reliability of\ncross-modality metrics like CLIPScore. In this paper, we propose CAMScore, a\ncyclic reference-free automatic evaluation metric for image captioning models.\nTo circumvent the aforementioned modality gap, CAMScore utilizes a\ntext-to-image model to generate images from captions and subsequently evaluates\nthese generated images against the original images. Furthermore, to provide\nfine-grained information for a more comprehensive evaluation, we design a\nthree-level evaluation framework for CAMScore that encompasses pixel-level,\nsemantic-level, and objective-level perspectives. Extensive experiment results\nacross multiple benchmark datasets show that CAMScore achieves a superior\ncorrelation with human judgments compared to existing reference-based and\nreference-free metrics, demonstrating the effectiveness of the framework.\n", "link": "http://arxiv.org/abs/2501.03567v2", "date": "2025-01-08", "relevancy": 2.1607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5639}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Image%20Caption%20via%20Cycle-consistent%20Text-to-Image%20Generation&body=Title%3A%20Evaluating%20Image%20Caption%20via%20Cycle-consistent%20Text-to-Image%20Generation%0AAuthor%3A%20Tianyu%20Cui%20and%20Jinbin%20Bai%20and%20Guo-Hua%20Wang%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Ye%20Shi%0AAbstract%3A%20%20%20Evaluating%20image%20captions%20typically%20relies%20on%20reference%20captions%2C%20which%20are%0Acostly%20to%20obtain%20and%20exhibit%20significant%20diversity%20and%20subjectivity.%20While%0Areference-free%20evaluation%20metrics%20have%20been%20proposed%2C%20most%20focus%20on%20cross-modal%0Aevaluation%20between%20captions%20and%20images.%20Recent%20research%20has%20revealed%20that%20the%0Amodality%20gap%20generally%20exists%20in%20the%20representation%20of%20contrastive%0Alearning-based%20multi-modal%20systems%2C%20undermining%20the%20reliability%20of%0Across-modality%20metrics%20like%20CLIPScore.%20In%20this%20paper%2C%20we%20propose%20CAMScore%2C%20a%0Acyclic%20reference-free%20automatic%20evaluation%20metric%20for%20image%20captioning%20models.%0ATo%20circumvent%20the%20aforementioned%20modality%20gap%2C%20CAMScore%20utilizes%20a%0Atext-to-image%20model%20to%20generate%20images%20from%20captions%20and%20subsequently%20evaluates%0Athese%20generated%20images%20against%20the%20original%20images.%20Furthermore%2C%20to%20provide%0Afine-grained%20information%20for%20a%20more%20comprehensive%20evaluation%2C%20we%20design%20a%0Athree-level%20evaluation%20framework%20for%20CAMScore%20that%20encompasses%20pixel-level%2C%0Asemantic-level%2C%20and%20objective-level%20perspectives.%20Extensive%20experiment%20results%0Aacross%20multiple%20benchmark%20datasets%20show%20that%20CAMScore%20achieves%20a%20superior%0Acorrelation%20with%20human%20judgments%20compared%20to%20existing%20reference-based%20and%0Areference-free%20metrics%2C%20demonstrating%20the%20effectiveness%20of%20the%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Image%2520Caption%2520via%2520Cycle-consistent%2520Text-to-Image%2520Generation%26entry.906535625%3DTianyu%2520Cui%2520and%2520Jinbin%2520Bai%2520and%2520Guo-Hua%2520Wang%2520and%2520Qing-Guo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520Ye%2520Shi%26entry.1292438233%3D%2520%2520Evaluating%2520image%2520captions%2520typically%2520relies%2520on%2520reference%2520captions%252C%2520which%2520are%250Acostly%2520to%2520obtain%2520and%2520exhibit%2520significant%2520diversity%2520and%2520subjectivity.%2520While%250Areference-free%2520evaluation%2520metrics%2520have%2520been%2520proposed%252C%2520most%2520focus%2520on%2520cross-modal%250Aevaluation%2520between%2520captions%2520and%2520images.%2520Recent%2520research%2520has%2520revealed%2520that%2520the%250Amodality%2520gap%2520generally%2520exists%2520in%2520the%2520representation%2520of%2520contrastive%250Alearning-based%2520multi-modal%2520systems%252C%2520undermining%2520the%2520reliability%2520of%250Across-modality%2520metrics%2520like%2520CLIPScore.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CAMScore%252C%2520a%250Acyclic%2520reference-free%2520automatic%2520evaluation%2520metric%2520for%2520image%2520captioning%2520models.%250ATo%2520circumvent%2520the%2520aforementioned%2520modality%2520gap%252C%2520CAMScore%2520utilizes%2520a%250Atext-to-image%2520model%2520to%2520generate%2520images%2520from%2520captions%2520and%2520subsequently%2520evaluates%250Athese%2520generated%2520images%2520against%2520the%2520original%2520images.%2520Furthermore%252C%2520to%2520provide%250Afine-grained%2520information%2520for%2520a%2520more%2520comprehensive%2520evaluation%252C%2520we%2520design%2520a%250Athree-level%2520evaluation%2520framework%2520for%2520CAMScore%2520that%2520encompasses%2520pixel-level%252C%250Asemantic-level%252C%2520and%2520objective-level%2520perspectives.%2520Extensive%2520experiment%2520results%250Aacross%2520multiple%2520benchmark%2520datasets%2520show%2520that%2520CAMScore%2520achieves%2520a%2520superior%250Acorrelation%2520with%2520human%2520judgments%2520compared%2520to%2520existing%2520reference-based%2520and%250Areference-free%2520metrics%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Image%20Caption%20via%20Cycle-consistent%20Text-to-Image%20Generation&entry.906535625=Tianyu%20Cui%20and%20Jinbin%20Bai%20and%20Guo-Hua%20Wang%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Ye%20Shi&entry.1292438233=%20%20Evaluating%20image%20captions%20typically%20relies%20on%20reference%20captions%2C%20which%20are%0Acostly%20to%20obtain%20and%20exhibit%20significant%20diversity%20and%20subjectivity.%20While%0Areference-free%20evaluation%20metrics%20have%20been%20proposed%2C%20most%20focus%20on%20cross-modal%0Aevaluation%20between%20captions%20and%20images.%20Recent%20research%20has%20revealed%20that%20the%0Amodality%20gap%20generally%20exists%20in%20the%20representation%20of%20contrastive%0Alearning-based%20multi-modal%20systems%2C%20undermining%20the%20reliability%20of%0Across-modality%20metrics%20like%20CLIPScore.%20In%20this%20paper%2C%20we%20propose%20CAMScore%2C%20a%0Acyclic%20reference-free%20automatic%20evaluation%20metric%20for%20image%20captioning%20models.%0ATo%20circumvent%20the%20aforementioned%20modality%20gap%2C%20CAMScore%20utilizes%20a%0Atext-to-image%20model%20to%20generate%20images%20from%20captions%20and%20subsequently%20evaluates%0Athese%20generated%20images%20against%20the%20original%20images.%20Furthermore%2C%20to%20provide%0Afine-grained%20information%20for%20a%20more%20comprehensive%20evaluation%2C%20we%20design%20a%0Athree-level%20evaluation%20framework%20for%20CAMScore%20that%20encompasses%20pixel-level%2C%0Asemantic-level%2C%20and%20objective-level%20perspectives.%20Extensive%20experiment%20results%0Aacross%20multiple%20benchmark%20datasets%20show%20that%20CAMScore%20achieves%20a%20superior%0Acorrelation%20with%20human%20judgments%20compared%20to%20existing%20reference-based%20and%0Areference-free%20metrics%2C%20demonstrating%20the%20effectiveness%20of%20the%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03567v2&entry.124074799=Read"},
{"title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference", "author": "Han Zhao and Min Zhang and Wei Zhao and Pengxiang Ding and Siteng Huang and Donglin Wang", "abstract": "  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n", "link": "http://arxiv.org/abs/2403.14520v4", "date": "2025-01-08", "relevancy": 2.1584, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&body=Title%3A%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference%0AAuthor%3A%20Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20e.g.%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%20speed%20due%0Ato%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%20of%0Aclosed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%20in%0Aovercoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14520v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCobra%253A%2520Extending%2520Mamba%2520to%2520Multi-Modal%2520Large%2520Language%2520Model%2520for%2520Efficient%250A%2520%2520Inference%26entry.906535625%3DHan%2520Zhao%2520and%2520Min%2520Zhang%2520and%2520Wei%2520Zhao%2520and%2520Pengxiang%2520Ding%2520and%2520Siteng%2520Huang%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520application%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLM%2529%250Ain%2520various%2520fields%2520has%2520achieved%2520remarkable%2520success.%2520However%252C%2520as%2520the%2520foundation%250Amodel%2520for%2520many%2520downstream%2520tasks%252C%2520current%2520MLLMs%2520are%2520composed%2520of%2520the%2520well-known%250ATransformer%2520network%252C%2520which%2520has%2520a%2520less%2520efficient%2520quadratic%2520computation%250Acomplexity.%2520To%2520improve%2520the%2520efficiency%2520of%2520such%2520basic%2520models%252C%2520we%2520propose%2520Cobra%252C%2520a%250Alinear%2520computational%2520complexity%2520MLLM.%2520Specifically%252C%2520Cobra%2520integrates%2520the%250Aefficient%2520Mamba%2520language%2520model%2520into%2520the%2520visual%2520modality.%2520Moreover%252C%2520we%2520explore%250Aand%2520study%2520various%2520modal%2520fusion%2520schemes%2520to%2520create%2520an%2520effective%2520multi-modal%250AMamba.%2520Extensive%2520experiments%2520demonstrate%2520that%2520%25281%2529%2520Cobra%2520achieves%2520extremely%250Acompetitive%2520performance%2520with%2520current%2520computationally%2520efficient%2520state-of-the-art%250Amethods%252C%2520e.g.%252C%2520LLaVA-Phi%252C%2520TinyLLaVA%252C%2520and%2520MobileVLM%2520v2%252C%2520and%2520has%2520faster%2520speed%2520due%250Ato%2520Cobra%2527s%2520linear%2520sequential%2520modeling.%2520%25282%2529%2520Interestingly%252C%2520the%2520results%2520of%250Aclosed-set%2520challenging%2520prediction%2520benchmarks%2520show%2520that%2520Cobra%2520performs%2520well%2520in%250Aovercoming%2520visual%2520illusions%2520and%2520spatial%2520relationship%2520judgments.%2520%25283%2529%2520Notably%252C%250ACobra%2520even%2520achieves%2520comparable%2520performance%2520to%2520LLaVA%2520with%2520about%252043%2525%2520of%2520the%250Anumber%2520of%2520parameters.%2520We%2520will%2520make%2520all%2520codes%2520of%2520Cobra%2520open-source%2520and%2520hope%2520that%250Athe%2520proposed%2520method%2520can%2520facilitate%2520future%2520research%2520on%2520complexity%2520problems%2520in%250AMLLM.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%2520https%253A//sites.google.com/view/cobravlm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14520v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&entry.906535625=Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20e.g.%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%20speed%20due%0Ato%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%20of%0Aclosed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%20in%0Aovercoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14520v4&entry.124074799=Read"},
{"title": "RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark", "author": "Xin Zhang and Xue Yang and Yuxuan Li and Jian Yang and Ming-Ming Cheng and Xiang Li", "abstract": "  Rotated object detection has made significant progress in the optical remote\nsensing. However, advancements in the Synthetic Aperture Radar (SAR) field are\nlaggard behind, primarily due to the absence of a large-scale dataset.\nAnnotating such a dataset is inefficient and costly. A promising solution is to\nemploy a weakly supervised model (e.g., trained with available horizontal boxes\nonly) to generate pseudo-rotated boxes for reference before manual calibration.\nUnfortunately, the existing weakly supervised models exhibit limited accuracy\nin predicting the object's angle. Previous works attempt to enhance angle\nprediction by using angle resolvers that decouple angles into cosine and sine\nencodings. In this work, we first reevaluate these resolvers from a unified\nperspective of dimension mapping and expose that they share the same\nshortcomings: these methods overlook the unit cycle constraint inherent in\nthese encodings, easily leading to prediction biases. To address this issue, we\npropose the Unit Cycle Resolver, which incorporates a unit circle constraint\nloss to improve angle prediction accuracy. Our approach can effectively improve\nthe performance of existing state-of-the-art weakly supervised methods and even\nsurpasses fully supervised models on existing optical benchmarks (i.e.,\nDOTA-v1.0 dataset). With the aid of UCR, we further annotate and introduce\nRSAR, the largest multi-class rotated SAR object detection dataset to date.\nExtensive experiments on both RSAR and optical datasets demonstrate that our\nUCR enhances angle prediction accuracy. Our dataset and code can be found at:\nhttps://github.com/zhasion/RSAR.\n", "link": "http://arxiv.org/abs/2501.04440v1", "date": "2025-01-08", "relevancy": 2.1509, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.549}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5345}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSAR%3A%20Restricted%20State%20Angle%20Resolver%20and%20Rotated%20SAR%20Benchmark&body=Title%3A%20RSAR%3A%20Restricted%20State%20Angle%20Resolver%20and%20Rotated%20SAR%20Benchmark%0AAuthor%3A%20Xin%20Zhang%20and%20Xue%20Yang%20and%20Yuxuan%20Li%20and%20Jian%20Yang%20and%20Ming-Ming%20Cheng%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Rotated%20object%20detection%20has%20made%20significant%20progress%20in%20the%20optical%20remote%0Asensing.%20However%2C%20advancements%20in%20the%20Synthetic%20Aperture%20Radar%20%28SAR%29%20field%20are%0Alaggard%20behind%2C%20primarily%20due%20to%20the%20absence%20of%20a%20large-scale%20dataset.%0AAnnotating%20such%20a%20dataset%20is%20inefficient%20and%20costly.%20A%20promising%20solution%20is%20to%0Aemploy%20a%20weakly%20supervised%20model%20%28e.g.%2C%20trained%20with%20available%20horizontal%20boxes%0Aonly%29%20to%20generate%20pseudo-rotated%20boxes%20for%20reference%20before%20manual%20calibration.%0AUnfortunately%2C%20the%20existing%20weakly%20supervised%20models%20exhibit%20limited%20accuracy%0Ain%20predicting%20the%20object%27s%20angle.%20Previous%20works%20attempt%20to%20enhance%20angle%0Aprediction%20by%20using%20angle%20resolvers%20that%20decouple%20angles%20into%20cosine%20and%20sine%0Aencodings.%20In%20this%20work%2C%20we%20first%20reevaluate%20these%20resolvers%20from%20a%20unified%0Aperspective%20of%20dimension%20mapping%20and%20expose%20that%20they%20share%20the%20same%0Ashortcomings%3A%20these%20methods%20overlook%20the%20unit%20cycle%20constraint%20inherent%20in%0Athese%20encodings%2C%20easily%20leading%20to%20prediction%20biases.%20To%20address%20this%20issue%2C%20we%0Apropose%20the%20Unit%20Cycle%20Resolver%2C%20which%20incorporates%20a%20unit%20circle%20constraint%0Aloss%20to%20improve%20angle%20prediction%20accuracy.%20Our%20approach%20can%20effectively%20improve%0Athe%20performance%20of%20existing%20state-of-the-art%20weakly%20supervised%20methods%20and%20even%0Asurpasses%20fully%20supervised%20models%20on%20existing%20optical%20benchmarks%20%28i.e.%2C%0ADOTA-v1.0%20dataset%29.%20With%20the%20aid%20of%20UCR%2C%20we%20further%20annotate%20and%20introduce%0ARSAR%2C%20the%20largest%20multi-class%20rotated%20SAR%20object%20detection%20dataset%20to%20date.%0AExtensive%20experiments%20on%20both%20RSAR%20and%20optical%20datasets%20demonstrate%20that%20our%0AUCR%20enhances%20angle%20prediction%20accuracy.%20Our%20dataset%20and%20code%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/zhasion/RSAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSAR%253A%2520Restricted%2520State%2520Angle%2520Resolver%2520and%2520Rotated%2520SAR%2520Benchmark%26entry.906535625%3DXin%2520Zhang%2520and%2520Xue%2520Yang%2520and%2520Yuxuan%2520Li%2520and%2520Jian%2520Yang%2520and%2520Ming-Ming%2520Cheng%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Rotated%2520object%2520detection%2520has%2520made%2520significant%2520progress%2520in%2520the%2520optical%2520remote%250Asensing.%2520However%252C%2520advancements%2520in%2520the%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520field%2520are%250Alaggard%2520behind%252C%2520primarily%2520due%2520to%2520the%2520absence%2520of%2520a%2520large-scale%2520dataset.%250AAnnotating%2520such%2520a%2520dataset%2520is%2520inefficient%2520and%2520costly.%2520A%2520promising%2520solution%2520is%2520to%250Aemploy%2520a%2520weakly%2520supervised%2520model%2520%2528e.g.%252C%2520trained%2520with%2520available%2520horizontal%2520boxes%250Aonly%2529%2520to%2520generate%2520pseudo-rotated%2520boxes%2520for%2520reference%2520before%2520manual%2520calibration.%250AUnfortunately%252C%2520the%2520existing%2520weakly%2520supervised%2520models%2520exhibit%2520limited%2520accuracy%250Ain%2520predicting%2520the%2520object%2527s%2520angle.%2520Previous%2520works%2520attempt%2520to%2520enhance%2520angle%250Aprediction%2520by%2520using%2520angle%2520resolvers%2520that%2520decouple%2520angles%2520into%2520cosine%2520and%2520sine%250Aencodings.%2520In%2520this%2520work%252C%2520we%2520first%2520reevaluate%2520these%2520resolvers%2520from%2520a%2520unified%250Aperspective%2520of%2520dimension%2520mapping%2520and%2520expose%2520that%2520they%2520share%2520the%2520same%250Ashortcomings%253A%2520these%2520methods%2520overlook%2520the%2520unit%2520cycle%2520constraint%2520inherent%2520in%250Athese%2520encodings%252C%2520easily%2520leading%2520to%2520prediction%2520biases.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520the%2520Unit%2520Cycle%2520Resolver%252C%2520which%2520incorporates%2520a%2520unit%2520circle%2520constraint%250Aloss%2520to%2520improve%2520angle%2520prediction%2520accuracy.%2520Our%2520approach%2520can%2520effectively%2520improve%250Athe%2520performance%2520of%2520existing%2520state-of-the-art%2520weakly%2520supervised%2520methods%2520and%2520even%250Asurpasses%2520fully%2520supervised%2520models%2520on%2520existing%2520optical%2520benchmarks%2520%2528i.e.%252C%250ADOTA-v1.0%2520dataset%2529.%2520With%2520the%2520aid%2520of%2520UCR%252C%2520we%2520further%2520annotate%2520and%2520introduce%250ARSAR%252C%2520the%2520largest%2520multi-class%2520rotated%2520SAR%2520object%2520detection%2520dataset%2520to%2520date.%250AExtensive%2520experiments%2520on%2520both%2520RSAR%2520and%2520optical%2520datasets%2520demonstrate%2520that%2520our%250AUCR%2520enhances%2520angle%2520prediction%2520accuracy.%2520Our%2520dataset%2520and%2520code%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/zhasion/RSAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSAR%3A%20Restricted%20State%20Angle%20Resolver%20and%20Rotated%20SAR%20Benchmark&entry.906535625=Xin%20Zhang%20and%20Xue%20Yang%20and%20Yuxuan%20Li%20and%20Jian%20Yang%20and%20Ming-Ming%20Cheng%20and%20Xiang%20Li&entry.1292438233=%20%20Rotated%20object%20detection%20has%20made%20significant%20progress%20in%20the%20optical%20remote%0Asensing.%20However%2C%20advancements%20in%20the%20Synthetic%20Aperture%20Radar%20%28SAR%29%20field%20are%0Alaggard%20behind%2C%20primarily%20due%20to%20the%20absence%20of%20a%20large-scale%20dataset.%0AAnnotating%20such%20a%20dataset%20is%20inefficient%20and%20costly.%20A%20promising%20solution%20is%20to%0Aemploy%20a%20weakly%20supervised%20model%20%28e.g.%2C%20trained%20with%20available%20horizontal%20boxes%0Aonly%29%20to%20generate%20pseudo-rotated%20boxes%20for%20reference%20before%20manual%20calibration.%0AUnfortunately%2C%20the%20existing%20weakly%20supervised%20models%20exhibit%20limited%20accuracy%0Ain%20predicting%20the%20object%27s%20angle.%20Previous%20works%20attempt%20to%20enhance%20angle%0Aprediction%20by%20using%20angle%20resolvers%20that%20decouple%20angles%20into%20cosine%20and%20sine%0Aencodings.%20In%20this%20work%2C%20we%20first%20reevaluate%20these%20resolvers%20from%20a%20unified%0Aperspective%20of%20dimension%20mapping%20and%20expose%20that%20they%20share%20the%20same%0Ashortcomings%3A%20these%20methods%20overlook%20the%20unit%20cycle%20constraint%20inherent%20in%0Athese%20encodings%2C%20easily%20leading%20to%20prediction%20biases.%20To%20address%20this%20issue%2C%20we%0Apropose%20the%20Unit%20Cycle%20Resolver%2C%20which%20incorporates%20a%20unit%20circle%20constraint%0Aloss%20to%20improve%20angle%20prediction%20accuracy.%20Our%20approach%20can%20effectively%20improve%0Athe%20performance%20of%20existing%20state-of-the-art%20weakly%20supervised%20methods%20and%20even%0Asurpasses%20fully%20supervised%20models%20on%20existing%20optical%20benchmarks%20%28i.e.%2C%0ADOTA-v1.0%20dataset%29.%20With%20the%20aid%20of%20UCR%2C%20we%20further%20annotate%20and%20introduce%0ARSAR%2C%20the%20largest%20multi-class%20rotated%20SAR%20object%20detection%20dataset%20to%20date.%0AExtensive%20experiments%20on%20both%20RSAR%20and%20optical%20datasets%20demonstrate%20that%20our%0AUCR%20enhances%20angle%20prediction%20accuracy.%20Our%20dataset%20and%20code%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/zhasion/RSAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04440v1&entry.124074799=Read"},
{"title": "Learning from Ambiguous Data with Hard Labels", "author": "Zeke Xie and Zheng He and Nan Lu and Lichen Bai and Bao Li and Shuo Yang and Mingming Sun and Ping Li", "abstract": "  Real-world data often contains intrinsic ambiguity that the common\nsingle-hard-label annotation paradigm ignores. Standard training using\nambiguous data with these hard labels may produce overly confident models and\nthus leading to poor generalization. In this paper, we propose a novel\nframework called Quantized Label Learning (QLL) to alleviate this issue. First,\nwe formulate QLL as learning from (very) ambiguous data with hard labels:\nideally, each ambiguous instance should be associated with a ground-truth\nsoft-label distribution describing its corresponding probabilistic weight in\neach class, however, this is usually not accessible; in practice, we can only\nobserve a quantized label, i.e., a hard label sampled (quantized) from the\ncorresponding ground-truth soft-label distribution, of each instance, which can\nbe seen as a biased approximation of the ground-truth soft-label. Second, we\npropose a Class-wise Positive-Unlabeled (CPU) risk estimator that allows us to\ntrain accurate classifiers from only ambiguous data with quantized labels.\nThird, to simulate ambiguous datasets with quantized labels in the real world,\nwe design a mixing-based ambiguous data generation procedure for empirical\nevaluation. Experiments demonstrate that our CPU method can significantly\nimprove model generalization performance and outperform the baselines.\n", "link": "http://arxiv.org/abs/2501.01844v2", "date": "2025-01-08", "relevancy": 2.1005, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5839}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5165}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Ambiguous%20Data%20with%20Hard%20Labels&body=Title%3A%20Learning%20from%20Ambiguous%20Data%20with%20Hard%20Labels%0AAuthor%3A%20Zeke%20Xie%20and%20Zheng%20He%20and%20Nan%20Lu%20and%20Lichen%20Bai%20and%20Bao%20Li%20and%20Shuo%20Yang%20and%20Mingming%20Sun%20and%20Ping%20Li%0AAbstract%3A%20%20%20Real-world%20data%20often%20contains%20intrinsic%20ambiguity%20that%20the%20common%0Asingle-hard-label%20annotation%20paradigm%20ignores.%20Standard%20training%20using%0Aambiguous%20data%20with%20these%20hard%20labels%20may%20produce%20overly%20confident%20models%20and%0Athus%20leading%20to%20poor%20generalization.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%20called%20Quantized%20Label%20Learning%20%28QLL%29%20to%20alleviate%20this%20issue.%20First%2C%0Awe%20formulate%20QLL%20as%20learning%20from%20%28very%29%20ambiguous%20data%20with%20hard%20labels%3A%0Aideally%2C%20each%20ambiguous%20instance%20should%20be%20associated%20with%20a%20ground-truth%0Asoft-label%20distribution%20describing%20its%20corresponding%20probabilistic%20weight%20in%0Aeach%20class%2C%20however%2C%20this%20is%20usually%20not%20accessible%3B%20in%20practice%2C%20we%20can%20only%0Aobserve%20a%20quantized%20label%2C%20i.e.%2C%20a%20hard%20label%20sampled%20%28quantized%29%20from%20the%0Acorresponding%20ground-truth%20soft-label%20distribution%2C%20of%20each%20instance%2C%20which%20can%0Abe%20seen%20as%20a%20biased%20approximation%20of%20the%20ground-truth%20soft-label.%20Second%2C%20we%0Apropose%20a%20Class-wise%20Positive-Unlabeled%20%28CPU%29%20risk%20estimator%20that%20allows%20us%20to%0Atrain%20accurate%20classifiers%20from%20only%20ambiguous%20data%20with%20quantized%20labels.%0AThird%2C%20to%20simulate%20ambiguous%20datasets%20with%20quantized%20labels%20in%20the%20real%20world%2C%0Awe%20design%20a%20mixing-based%20ambiguous%20data%20generation%20procedure%20for%20empirical%0Aevaluation.%20Experiments%20demonstrate%20that%20our%20CPU%20method%20can%20significantly%0Aimprove%20model%20generalization%20performance%20and%20outperform%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Ambiguous%2520Data%2520with%2520Hard%2520Labels%26entry.906535625%3DZeke%2520Xie%2520and%2520Zheng%2520He%2520and%2520Nan%2520Lu%2520and%2520Lichen%2520Bai%2520and%2520Bao%2520Li%2520and%2520Shuo%2520Yang%2520and%2520Mingming%2520Sun%2520and%2520Ping%2520Li%26entry.1292438233%3D%2520%2520Real-world%2520data%2520often%2520contains%2520intrinsic%2520ambiguity%2520that%2520the%2520common%250Asingle-hard-label%2520annotation%2520paradigm%2520ignores.%2520Standard%2520training%2520using%250Aambiguous%2520data%2520with%2520these%2520hard%2520labels%2520may%2520produce%2520overly%2520confident%2520models%2520and%250Athus%2520leading%2520to%2520poor%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520called%2520Quantized%2520Label%2520Learning%2520%2528QLL%2529%2520to%2520alleviate%2520this%2520issue.%2520First%252C%250Awe%2520formulate%2520QLL%2520as%2520learning%2520from%2520%2528very%2529%2520ambiguous%2520data%2520with%2520hard%2520labels%253A%250Aideally%252C%2520each%2520ambiguous%2520instance%2520should%2520be%2520associated%2520with%2520a%2520ground-truth%250Asoft-label%2520distribution%2520describing%2520its%2520corresponding%2520probabilistic%2520weight%2520in%250Aeach%2520class%252C%2520however%252C%2520this%2520is%2520usually%2520not%2520accessible%253B%2520in%2520practice%252C%2520we%2520can%2520only%250Aobserve%2520a%2520quantized%2520label%252C%2520i.e.%252C%2520a%2520hard%2520label%2520sampled%2520%2528quantized%2529%2520from%2520the%250Acorresponding%2520ground-truth%2520soft-label%2520distribution%252C%2520of%2520each%2520instance%252C%2520which%2520can%250Abe%2520seen%2520as%2520a%2520biased%2520approximation%2520of%2520the%2520ground-truth%2520soft-label.%2520Second%252C%2520we%250Apropose%2520a%2520Class-wise%2520Positive-Unlabeled%2520%2528CPU%2529%2520risk%2520estimator%2520that%2520allows%2520us%2520to%250Atrain%2520accurate%2520classifiers%2520from%2520only%2520ambiguous%2520data%2520with%2520quantized%2520labels.%250AThird%252C%2520to%2520simulate%2520ambiguous%2520datasets%2520with%2520quantized%2520labels%2520in%2520the%2520real%2520world%252C%250Awe%2520design%2520a%2520mixing-based%2520ambiguous%2520data%2520generation%2520procedure%2520for%2520empirical%250Aevaluation.%2520Experiments%2520demonstrate%2520that%2520our%2520CPU%2520method%2520can%2520significantly%250Aimprove%2520model%2520generalization%2520performance%2520and%2520outperform%2520the%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Ambiguous%20Data%20with%20Hard%20Labels&entry.906535625=Zeke%20Xie%20and%20Zheng%20He%20and%20Nan%20Lu%20and%20Lichen%20Bai%20and%20Bao%20Li%20and%20Shuo%20Yang%20and%20Mingming%20Sun%20and%20Ping%20Li&entry.1292438233=%20%20Real-world%20data%20often%20contains%20intrinsic%20ambiguity%20that%20the%20common%0Asingle-hard-label%20annotation%20paradigm%20ignores.%20Standard%20training%20using%0Aambiguous%20data%20with%20these%20hard%20labels%20may%20produce%20overly%20confident%20models%20and%0Athus%20leading%20to%20poor%20generalization.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%20called%20Quantized%20Label%20Learning%20%28QLL%29%20to%20alleviate%20this%20issue.%20First%2C%0Awe%20formulate%20QLL%20as%20learning%20from%20%28very%29%20ambiguous%20data%20with%20hard%20labels%3A%0Aideally%2C%20each%20ambiguous%20instance%20should%20be%20associated%20with%20a%20ground-truth%0Asoft-label%20distribution%20describing%20its%20corresponding%20probabilistic%20weight%20in%0Aeach%20class%2C%20however%2C%20this%20is%20usually%20not%20accessible%3B%20in%20practice%2C%20we%20can%20only%0Aobserve%20a%20quantized%20label%2C%20i.e.%2C%20a%20hard%20label%20sampled%20%28quantized%29%20from%20the%0Acorresponding%20ground-truth%20soft-label%20distribution%2C%20of%20each%20instance%2C%20which%20can%0Abe%20seen%20as%20a%20biased%20approximation%20of%20the%20ground-truth%20soft-label.%20Second%2C%20we%0Apropose%20a%20Class-wise%20Positive-Unlabeled%20%28CPU%29%20risk%20estimator%20that%20allows%20us%20to%0Atrain%20accurate%20classifiers%20from%20only%20ambiguous%20data%20with%20quantized%20labels.%0AThird%2C%20to%20simulate%20ambiguous%20datasets%20with%20quantized%20labels%20in%20the%20real%20world%2C%0Awe%20design%20a%20mixing-based%20ambiguous%20data%20generation%20procedure%20for%20empirical%0Aevaluation.%20Experiments%20demonstrate%20that%20our%20CPU%20method%20can%20significantly%0Aimprove%20model%20generalization%20performance%20and%20outperform%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01844v2&entry.124074799=Read"},
{"title": "Knowledge Retrieval Based on Generative AI", "author": "Te-Lun Yang and Jyi-Shane Liu and Yuen-Hsien Tseng and Jyh-Shing Roger Jang", "abstract": "  This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI.\n  The system's effectiveness is assessed through a two-stage evaluation:\nautomatic and assisted performance evaluations. The automatic evaluation\ncalculates accuracy by comparing the model's auto-generated labels with ground\ntruth answers, measuring performance under standardized conditions without\nhuman intervention. The assisted performance evaluation involves 20\nfinance-related multiple-choice questions answered by 20 participants without\nfinancial backgrounds. Initially, participants answer independently. Later,\nthey receive system-generated reference information to assist in answering,\nexamining whether the system improves accuracy when assistance is provided.\n  The main contributions of this research are: (1) Enhanced LLM Capability: By\nintegrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly\nrelevant results, reduces hallucinations, and dynamically accesses authorized\nor public knowledge sources. (2) Improved Data Privacy: A customized RAG\narchitecture enables local operation of the LLM, eliminating the need to send\nprivate data to external servers. This approach enhances data security, reduces\nreliance on commercial services, lowers operational costs, and mitigates\nprivacy risks.\n", "link": "http://arxiv.org/abs/2501.04635v1", "date": "2025-01-08", "relevancy": 2.0774, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5344}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5138}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Retrieval%20Based%20on%20Generative%20AI&body=Title%3A%20Knowledge%20Retrieval%20Based%20on%20Generative%20AI%0AAuthor%3A%20Te-Lun%20Yang%20and%20Jyi-Shane%20Liu%20and%20Yuen-Hsien%20Tseng%20and%20Jyh-Shing%20Roger%20Jang%0AAbstract%3A%20%20%20This%20study%20develops%20a%20question-answering%20system%20based%20on%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20using%20Chinese%20Wikipedia%20and%20Lawbank%20as%20retrieval%20sources.%0AUsing%20TTQA%20and%20TMMLU%2B%20as%20evaluation%20datasets%2C%20the%20system%20employs%20BGE-M3%20for%0Adense%20vector%20retrieval%20to%20obtain%20highly%20relevant%20search%20results%20and%0ABGE-reranker%20to%20reorder%20these%20results%20based%20on%20query%20relevance.%20The%20most%0Apertinent%20retrieval%20outcomes%20serve%20as%20reference%20knowledge%20for%20a%20Large%20Language%0AModel%20%28LLM%29%2C%20enhancing%20its%20ability%20to%20answer%20questions%20and%20establishing%20a%0Aknowledge%20retrieval%20system%20grounded%20in%20generative%20AI.%0A%20%20The%20system%27s%20effectiveness%20is%20assessed%20through%20a%20two-stage%20evaluation%3A%0Aautomatic%20and%20assisted%20performance%20evaluations.%20The%20automatic%20evaluation%0Acalculates%20accuracy%20by%20comparing%20the%20model%27s%20auto-generated%20labels%20with%20ground%0Atruth%20answers%2C%20measuring%20performance%20under%20standardized%20conditions%20without%0Ahuman%20intervention.%20The%20assisted%20performance%20evaluation%20involves%2020%0Afinance-related%20multiple-choice%20questions%20answered%20by%2020%20participants%20without%0Afinancial%20backgrounds.%20Initially%2C%20participants%20answer%20independently.%20Later%2C%0Athey%20receive%20system-generated%20reference%20information%20to%20assist%20in%20answering%2C%0Aexamining%20whether%20the%20system%20improves%20accuracy%20when%20assistance%20is%20provided.%0A%20%20The%20main%20contributions%20of%20this%20research%20are%3A%20%281%29%20Enhanced%20LLM%20Capability%3A%20By%0Aintegrating%20BGE-M3%20and%20BGE-reranker%2C%20the%20system%20retrieves%20and%20reorders%20highly%0Arelevant%20results%2C%20reduces%20hallucinations%2C%20and%20dynamically%20accesses%20authorized%0Aor%20public%20knowledge%20sources.%20%282%29%20Improved%20Data%20Privacy%3A%20A%20customized%20RAG%0Aarchitecture%20enables%20local%20operation%20of%20the%20LLM%2C%20eliminating%20the%20need%20to%20send%0Aprivate%20data%20to%20external%20servers.%20This%20approach%20enhances%20data%20security%2C%20reduces%0Areliance%20on%20commercial%20services%2C%20lowers%20operational%20costs%2C%20and%20mitigates%0Aprivacy%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Retrieval%2520Based%2520on%2520Generative%2520AI%26entry.906535625%3DTe-Lun%2520Yang%2520and%2520Jyi-Shane%2520Liu%2520and%2520Yuen-Hsien%2520Tseng%2520and%2520Jyh-Shing%2520Roger%2520Jang%26entry.1292438233%3D%2520%2520This%2520study%2520develops%2520a%2520question-answering%2520system%2520based%2520on%2520Retrieval-Augmented%250AGeneration%2520%2528RAG%2529%2520using%2520Chinese%2520Wikipedia%2520and%2520Lawbank%2520as%2520retrieval%2520sources.%250AUsing%2520TTQA%2520and%2520TMMLU%252B%2520as%2520evaluation%2520datasets%252C%2520the%2520system%2520employs%2520BGE-M3%2520for%250Adense%2520vector%2520retrieval%2520to%2520obtain%2520highly%2520relevant%2520search%2520results%2520and%250ABGE-reranker%2520to%2520reorder%2520these%2520results%2520based%2520on%2520query%2520relevance.%2520The%2520most%250Apertinent%2520retrieval%2520outcomes%2520serve%2520as%2520reference%2520knowledge%2520for%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529%252C%2520enhancing%2520its%2520ability%2520to%2520answer%2520questions%2520and%2520establishing%2520a%250Aknowledge%2520retrieval%2520system%2520grounded%2520in%2520generative%2520AI.%250A%2520%2520The%2520system%2527s%2520effectiveness%2520is%2520assessed%2520through%2520a%2520two-stage%2520evaluation%253A%250Aautomatic%2520and%2520assisted%2520performance%2520evaluations.%2520The%2520automatic%2520evaluation%250Acalculates%2520accuracy%2520by%2520comparing%2520the%2520model%2527s%2520auto-generated%2520labels%2520with%2520ground%250Atruth%2520answers%252C%2520measuring%2520performance%2520under%2520standardized%2520conditions%2520without%250Ahuman%2520intervention.%2520The%2520assisted%2520performance%2520evaluation%2520involves%252020%250Afinance-related%2520multiple-choice%2520questions%2520answered%2520by%252020%2520participants%2520without%250Afinancial%2520backgrounds.%2520Initially%252C%2520participants%2520answer%2520independently.%2520Later%252C%250Athey%2520receive%2520system-generated%2520reference%2520information%2520to%2520assist%2520in%2520answering%252C%250Aexamining%2520whether%2520the%2520system%2520improves%2520accuracy%2520when%2520assistance%2520is%2520provided.%250A%2520%2520The%2520main%2520contributions%2520of%2520this%2520research%2520are%253A%2520%25281%2529%2520Enhanced%2520LLM%2520Capability%253A%2520By%250Aintegrating%2520BGE-M3%2520and%2520BGE-reranker%252C%2520the%2520system%2520retrieves%2520and%2520reorders%2520highly%250Arelevant%2520results%252C%2520reduces%2520hallucinations%252C%2520and%2520dynamically%2520accesses%2520authorized%250Aor%2520public%2520knowledge%2520sources.%2520%25282%2529%2520Improved%2520Data%2520Privacy%253A%2520A%2520customized%2520RAG%250Aarchitecture%2520enables%2520local%2520operation%2520of%2520the%2520LLM%252C%2520eliminating%2520the%2520need%2520to%2520send%250Aprivate%2520data%2520to%2520external%2520servers.%2520This%2520approach%2520enhances%2520data%2520security%252C%2520reduces%250Areliance%2520on%2520commercial%2520services%252C%2520lowers%2520operational%2520costs%252C%2520and%2520mitigates%250Aprivacy%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Retrieval%20Based%20on%20Generative%20AI&entry.906535625=Te-Lun%20Yang%20and%20Jyi-Shane%20Liu%20and%20Yuen-Hsien%20Tseng%20and%20Jyh-Shing%20Roger%20Jang&entry.1292438233=%20%20This%20study%20develops%20a%20question-answering%20system%20based%20on%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20using%20Chinese%20Wikipedia%20and%20Lawbank%20as%20retrieval%20sources.%0AUsing%20TTQA%20and%20TMMLU%2B%20as%20evaluation%20datasets%2C%20the%20system%20employs%20BGE-M3%20for%0Adense%20vector%20retrieval%20to%20obtain%20highly%20relevant%20search%20results%20and%0ABGE-reranker%20to%20reorder%20these%20results%20based%20on%20query%20relevance.%20The%20most%0Apertinent%20retrieval%20outcomes%20serve%20as%20reference%20knowledge%20for%20a%20Large%20Language%0AModel%20%28LLM%29%2C%20enhancing%20its%20ability%20to%20answer%20questions%20and%20establishing%20a%0Aknowledge%20retrieval%20system%20grounded%20in%20generative%20AI.%0A%20%20The%20system%27s%20effectiveness%20is%20assessed%20through%20a%20two-stage%20evaluation%3A%0Aautomatic%20and%20assisted%20performance%20evaluations.%20The%20automatic%20evaluation%0Acalculates%20accuracy%20by%20comparing%20the%20model%27s%20auto-generated%20labels%20with%20ground%0Atruth%20answers%2C%20measuring%20performance%20under%20standardized%20conditions%20without%0Ahuman%20intervention.%20The%20assisted%20performance%20evaluation%20involves%2020%0Afinance-related%20multiple-choice%20questions%20answered%20by%2020%20participants%20without%0Afinancial%20backgrounds.%20Initially%2C%20participants%20answer%20independently.%20Later%2C%0Athey%20receive%20system-generated%20reference%20information%20to%20assist%20in%20answering%2C%0Aexamining%20whether%20the%20system%20improves%20accuracy%20when%20assistance%20is%20provided.%0A%20%20The%20main%20contributions%20of%20this%20research%20are%3A%20%281%29%20Enhanced%20LLM%20Capability%3A%20By%0Aintegrating%20BGE-M3%20and%20BGE-reranker%2C%20the%20system%20retrieves%20and%20reorders%20highly%0Arelevant%20results%2C%20reduces%20hallucinations%2C%20and%20dynamically%20accesses%20authorized%0Aor%20public%20knowledge%20sources.%20%282%29%20Improved%20Data%20Privacy%3A%20A%20customized%20RAG%0Aarchitecture%20enables%20local%20operation%20of%20the%20LLM%2C%20eliminating%20the%20need%20to%20send%0Aprivate%20data%20to%20external%20servers.%20This%20approach%20enhances%20data%20security%2C%20reduces%0Areliance%20on%20commercial%20services%2C%20lowers%20operational%20costs%2C%20and%20mitigates%0Aprivacy%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04635v1&entry.124074799=Read"},
{"title": "Hybrid Artificial Intelligence Strategies for Drone Navigation", "author": "Rub\u00e9n San-Segundo and Luc\u00eda Angulo and Manuel Gil-Mart\u00edn and David Carrami\u00f1ana and Ana M. Bernardos", "abstract": "  Objective: This paper describes the development of hybrid artificial\nintelligence strategies for drone navigation. Methods: The navigation module\ncombines a deep learning model with a rule-based engine depending on the agent\nstate. The deep learning model has been trained using reinforcement learning.\nThe rule-based engine uses expert knowledge to deal with specific situations.\nThe navigation module incorporates several strategies to explain the drone\ndecision based on its observation space, and different mechanisms for including\nhuman decisions in the navigation process. Finally, this paper proposes an\nevaluation methodology based on defining several scenarios and analyzing the\nperformance of the different strategies according to metrics adapted to each\nscenario. Results: Two main navigation problems have been studied. For the\nfirst scenario (reaching known targets), it has been possible to obtain a 90%\ntask completion rate, reducing significantly the number of collisions thanks to\nthe rule-based engine. For the second scenario, it has been possible to reduce\n20% of the time required to locate all the targets using the reinforcement\nlearning model. Conclusions: Reinforcement learning is a very good strategy to\nlearn policies for drone navigation, but in critical situations, it is\nnecessary to complement it with a rule-based module to increase task success\nrate.\n", "link": "http://arxiv.org/abs/2501.04472v1", "date": "2025-01-08", "relevancy": 2.0715, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Artificial%20Intelligence%20Strategies%20for%20Drone%20Navigation&body=Title%3A%20Hybrid%20Artificial%20Intelligence%20Strategies%20for%20Drone%20Navigation%0AAuthor%3A%20Rub%C3%A9n%20San-Segundo%20and%20Luc%C3%ADa%20Angulo%20and%20Manuel%20Gil-Mart%C3%ADn%20and%20David%20Carrami%C3%B1ana%20and%20Ana%20M.%20Bernardos%0AAbstract%3A%20%20%20Objective%3A%20This%20paper%20describes%20the%20development%20of%20hybrid%20artificial%0Aintelligence%20strategies%20for%20drone%20navigation.%20Methods%3A%20The%20navigation%20module%0Acombines%20a%20deep%20learning%20model%20with%20a%20rule-based%20engine%20depending%20on%20the%20agent%0Astate.%20The%20deep%20learning%20model%20has%20been%20trained%20using%20reinforcement%20learning.%0AThe%20rule-based%20engine%20uses%20expert%20knowledge%20to%20deal%20with%20specific%20situations.%0AThe%20navigation%20module%20incorporates%20several%20strategies%20to%20explain%20the%20drone%0Adecision%20based%20on%20its%20observation%20space%2C%20and%20different%20mechanisms%20for%20including%0Ahuman%20decisions%20in%20the%20navigation%20process.%20Finally%2C%20this%20paper%20proposes%20an%0Aevaluation%20methodology%20based%20on%20defining%20several%20scenarios%20and%20analyzing%20the%0Aperformance%20of%20the%20different%20strategies%20according%20to%20metrics%20adapted%20to%20each%0Ascenario.%20Results%3A%20Two%20main%20navigation%20problems%20have%20been%20studied.%20For%20the%0Afirst%20scenario%20%28reaching%20known%20targets%29%2C%20it%20has%20been%20possible%20to%20obtain%20a%2090%25%0Atask%20completion%20rate%2C%20reducing%20significantly%20the%20number%20of%20collisions%20thanks%20to%0Athe%20rule-based%20engine.%20For%20the%20second%20scenario%2C%20it%20has%20been%20possible%20to%20reduce%0A20%25%20of%20the%20time%20required%20to%20locate%20all%20the%20targets%20using%20the%20reinforcement%0Alearning%20model.%20Conclusions%3A%20Reinforcement%20learning%20is%20a%20very%20good%20strategy%20to%0Alearn%20policies%20for%20drone%20navigation%2C%20but%20in%20critical%20situations%2C%20it%20is%0Anecessary%20to%20complement%20it%20with%20a%20rule-based%20module%20to%20increase%20task%20success%0Arate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Artificial%2520Intelligence%2520Strategies%2520for%2520Drone%2520Navigation%26entry.906535625%3DRub%25C3%25A9n%2520San-Segundo%2520and%2520Luc%25C3%25ADa%2520Angulo%2520and%2520Manuel%2520Gil-Mart%25C3%25ADn%2520and%2520David%2520Carrami%25C3%25B1ana%2520and%2520Ana%2520M.%2520Bernardos%26entry.1292438233%3D%2520%2520Objective%253A%2520This%2520paper%2520describes%2520the%2520development%2520of%2520hybrid%2520artificial%250Aintelligence%2520strategies%2520for%2520drone%2520navigation.%2520Methods%253A%2520The%2520navigation%2520module%250Acombines%2520a%2520deep%2520learning%2520model%2520with%2520a%2520rule-based%2520engine%2520depending%2520on%2520the%2520agent%250Astate.%2520The%2520deep%2520learning%2520model%2520has%2520been%2520trained%2520using%2520reinforcement%2520learning.%250AThe%2520rule-based%2520engine%2520uses%2520expert%2520knowledge%2520to%2520deal%2520with%2520specific%2520situations.%250AThe%2520navigation%2520module%2520incorporates%2520several%2520strategies%2520to%2520explain%2520the%2520drone%250Adecision%2520based%2520on%2520its%2520observation%2520space%252C%2520and%2520different%2520mechanisms%2520for%2520including%250Ahuman%2520decisions%2520in%2520the%2520navigation%2520process.%2520Finally%252C%2520this%2520paper%2520proposes%2520an%250Aevaluation%2520methodology%2520based%2520on%2520defining%2520several%2520scenarios%2520and%2520analyzing%2520the%250Aperformance%2520of%2520the%2520different%2520strategies%2520according%2520to%2520metrics%2520adapted%2520to%2520each%250Ascenario.%2520Results%253A%2520Two%2520main%2520navigation%2520problems%2520have%2520been%2520studied.%2520For%2520the%250Afirst%2520scenario%2520%2528reaching%2520known%2520targets%2529%252C%2520it%2520has%2520been%2520possible%2520to%2520obtain%2520a%252090%2525%250Atask%2520completion%2520rate%252C%2520reducing%2520significantly%2520the%2520number%2520of%2520collisions%2520thanks%2520to%250Athe%2520rule-based%2520engine.%2520For%2520the%2520second%2520scenario%252C%2520it%2520has%2520been%2520possible%2520to%2520reduce%250A20%2525%2520of%2520the%2520time%2520required%2520to%2520locate%2520all%2520the%2520targets%2520using%2520the%2520reinforcement%250Alearning%2520model.%2520Conclusions%253A%2520Reinforcement%2520learning%2520is%2520a%2520very%2520good%2520strategy%2520to%250Alearn%2520policies%2520for%2520drone%2520navigation%252C%2520but%2520in%2520critical%2520situations%252C%2520it%2520is%250Anecessary%2520to%2520complement%2520it%2520with%2520a%2520rule-based%2520module%2520to%2520increase%2520task%2520success%250Arate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Artificial%20Intelligence%20Strategies%20for%20Drone%20Navigation&entry.906535625=Rub%C3%A9n%20San-Segundo%20and%20Luc%C3%ADa%20Angulo%20and%20Manuel%20Gil-Mart%C3%ADn%20and%20David%20Carrami%C3%B1ana%20and%20Ana%20M.%20Bernardos&entry.1292438233=%20%20Objective%3A%20This%20paper%20describes%20the%20development%20of%20hybrid%20artificial%0Aintelligence%20strategies%20for%20drone%20navigation.%20Methods%3A%20The%20navigation%20module%0Acombines%20a%20deep%20learning%20model%20with%20a%20rule-based%20engine%20depending%20on%20the%20agent%0Astate.%20The%20deep%20learning%20model%20has%20been%20trained%20using%20reinforcement%20learning.%0AThe%20rule-based%20engine%20uses%20expert%20knowledge%20to%20deal%20with%20specific%20situations.%0AThe%20navigation%20module%20incorporates%20several%20strategies%20to%20explain%20the%20drone%0Adecision%20based%20on%20its%20observation%20space%2C%20and%20different%20mechanisms%20for%20including%0Ahuman%20decisions%20in%20the%20navigation%20process.%20Finally%2C%20this%20paper%20proposes%20an%0Aevaluation%20methodology%20based%20on%20defining%20several%20scenarios%20and%20analyzing%20the%0Aperformance%20of%20the%20different%20strategies%20according%20to%20metrics%20adapted%20to%20each%0Ascenario.%20Results%3A%20Two%20main%20navigation%20problems%20have%20been%20studied.%20For%20the%0Afirst%20scenario%20%28reaching%20known%20targets%29%2C%20it%20has%20been%20possible%20to%20obtain%20a%2090%25%0Atask%20completion%20rate%2C%20reducing%20significantly%20the%20number%20of%20collisions%20thanks%20to%0Athe%20rule-based%20engine.%20For%20the%20second%20scenario%2C%20it%20has%20been%20possible%20to%20reduce%0A20%25%20of%20the%20time%20required%20to%20locate%20all%20the%20targets%20using%20the%20reinforcement%0Alearning%20model.%20Conclusions%3A%20Reinforcement%20learning%20is%20a%20very%20good%20strategy%20to%0Alearn%20policies%20for%20drone%20navigation%2C%20but%20in%20critical%20situations%2C%20it%20is%0Anecessary%20to%20complement%20it%20with%20a%20rule-based%20module%20to%20increase%20task%20success%0Arate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04472v1&entry.124074799=Read"},
{"title": "Towards Fair Class-wise Robustness: Class Optimal Distribution\n  Adversarial Training", "author": "Hongxin Zhi and Hongtao Yu and Shaome Li and Xiuming Zhao and Yiteng Wu", "abstract": "  Adversarial training has proven to be a highly effective method for improving\nthe robustness of deep neural networks against adversarial attacks.\nNonetheless, it has been observed to exhibit a limitation in terms of robust\nfairness, characterized by a significant disparity in robustness across\ndifferent classes. Recent efforts to mitigate this problem have turned to\nclass-wise reweighted methods. However, these methods suffer from a lack of\nrigorous theoretical analysis and are limited in their exploration of the\nweight space, as they mainly rely on existing heuristic algorithms or intuition\nto compute weights. In addition, these methods fail to guarantee the\nconsistency of the optimization direction due to the decoupled optimization of\nweights and the model parameters. They potentially lead to suboptimal weight\nassignments and consequently, a suboptimal model. To address these problems,\nthis paper proposes a novel min-max training framework, Class Optimal\nDistribution Adversarial Training (CODAT), which employs distributionally\nrobust optimization to fully explore the class-wise weight space, thus enabling\nthe identification of the optimal weight with theoretical guarantees.\nFurthermore, we derive a closed-form optimal solution to the internal\nmaximization and then get a deterministic equivalent objective function, which\nprovides a theoretical basis for the joint optimization of weights and model\nparameters. Meanwhile, we propose a fairness elasticity coefficient for the\nevaluation of the algorithm with regard to both robustness and robust fairness.\nExperimental results on various datasets show that the proposed method can\neffectively improve the robust fairness of the model and outperform the\nstate-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2501.04527v1", "date": "2025-01-08", "relevancy": 2.068, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5253}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fair%20Class-wise%20Robustness%3A%20Class%20Optimal%20Distribution%0A%20%20Adversarial%20Training&body=Title%3A%20Towards%20Fair%20Class-wise%20Robustness%3A%20Class%20Optimal%20Distribution%0A%20%20Adversarial%20Training%0AAuthor%3A%20Hongxin%20Zhi%20and%20Hongtao%20Yu%20and%20Shaome%20Li%20and%20Xiuming%20Zhao%20and%20Yiteng%20Wu%0AAbstract%3A%20%20%20Adversarial%20training%20has%20proven%20to%20be%20a%20highly%20effective%20method%20for%20improving%0Athe%20robustness%20of%20deep%20neural%20networks%20against%20adversarial%20attacks.%0ANonetheless%2C%20it%20has%20been%20observed%20to%20exhibit%20a%20limitation%20in%20terms%20of%20robust%0Afairness%2C%20characterized%20by%20a%20significant%20disparity%20in%20robustness%20across%0Adifferent%20classes.%20Recent%20efforts%20to%20mitigate%20this%20problem%20have%20turned%20to%0Aclass-wise%20reweighted%20methods.%20However%2C%20these%20methods%20suffer%20from%20a%20lack%20of%0Arigorous%20theoretical%20analysis%20and%20are%20limited%20in%20their%20exploration%20of%20the%0Aweight%20space%2C%20as%20they%20mainly%20rely%20on%20existing%20heuristic%20algorithms%20or%20intuition%0Ato%20compute%20weights.%20In%20addition%2C%20these%20methods%20fail%20to%20guarantee%20the%0Aconsistency%20of%20the%20optimization%20direction%20due%20to%20the%20decoupled%20optimization%20of%0Aweights%20and%20the%20model%20parameters.%20They%20potentially%20lead%20to%20suboptimal%20weight%0Aassignments%20and%20consequently%2C%20a%20suboptimal%20model.%20To%20address%20these%20problems%2C%0Athis%20paper%20proposes%20a%20novel%20min-max%20training%20framework%2C%20Class%20Optimal%0ADistribution%20Adversarial%20Training%20%28CODAT%29%2C%20which%20employs%20distributionally%0Arobust%20optimization%20to%20fully%20explore%20the%20class-wise%20weight%20space%2C%20thus%20enabling%0Athe%20identification%20of%20the%20optimal%20weight%20with%20theoretical%20guarantees.%0AFurthermore%2C%20we%20derive%20a%20closed-form%20optimal%20solution%20to%20the%20internal%0Amaximization%20and%20then%20get%20a%20deterministic%20equivalent%20objective%20function%2C%20which%0Aprovides%20a%20theoretical%20basis%20for%20the%20joint%20optimization%20of%20weights%20and%20model%0Aparameters.%20Meanwhile%2C%20we%20propose%20a%20fairness%20elasticity%20coefficient%20for%20the%0Aevaluation%20of%20the%20algorithm%20with%20regard%20to%20both%20robustness%20and%20robust%20fairness.%0AExperimental%20results%20on%20various%20datasets%20show%20that%20the%20proposed%20method%20can%0Aeffectively%20improve%20the%20robust%20fairness%20of%20the%20model%20and%20outperform%20the%0Astate-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fair%2520Class-wise%2520Robustness%253A%2520Class%2520Optimal%2520Distribution%250A%2520%2520Adversarial%2520Training%26entry.906535625%3DHongxin%2520Zhi%2520and%2520Hongtao%2520Yu%2520and%2520Shaome%2520Li%2520and%2520Xiuming%2520Zhao%2520and%2520Yiteng%2520Wu%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520has%2520proven%2520to%2520be%2520a%2520highly%2520effective%2520method%2520for%2520improving%250Athe%2520robustness%2520of%2520deep%2520neural%2520networks%2520against%2520adversarial%2520attacks.%250ANonetheless%252C%2520it%2520has%2520been%2520observed%2520to%2520exhibit%2520a%2520limitation%2520in%2520terms%2520of%2520robust%250Afairness%252C%2520characterized%2520by%2520a%2520significant%2520disparity%2520in%2520robustness%2520across%250Adifferent%2520classes.%2520Recent%2520efforts%2520to%2520mitigate%2520this%2520problem%2520have%2520turned%2520to%250Aclass-wise%2520reweighted%2520methods.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520a%2520lack%2520of%250Arigorous%2520theoretical%2520analysis%2520and%2520are%2520limited%2520in%2520their%2520exploration%2520of%2520the%250Aweight%2520space%252C%2520as%2520they%2520mainly%2520rely%2520on%2520existing%2520heuristic%2520algorithms%2520or%2520intuition%250Ato%2520compute%2520weights.%2520In%2520addition%252C%2520these%2520methods%2520fail%2520to%2520guarantee%2520the%250Aconsistency%2520of%2520the%2520optimization%2520direction%2520due%2520to%2520the%2520decoupled%2520optimization%2520of%250Aweights%2520and%2520the%2520model%2520parameters.%2520They%2520potentially%2520lead%2520to%2520suboptimal%2520weight%250Aassignments%2520and%2520consequently%252C%2520a%2520suboptimal%2520model.%2520To%2520address%2520these%2520problems%252C%250Athis%2520paper%2520proposes%2520a%2520novel%2520min-max%2520training%2520framework%252C%2520Class%2520Optimal%250ADistribution%2520Adversarial%2520Training%2520%2528CODAT%2529%252C%2520which%2520employs%2520distributionally%250Arobust%2520optimization%2520to%2520fully%2520explore%2520the%2520class-wise%2520weight%2520space%252C%2520thus%2520enabling%250Athe%2520identification%2520of%2520the%2520optimal%2520weight%2520with%2520theoretical%2520guarantees.%250AFurthermore%252C%2520we%2520derive%2520a%2520closed-form%2520optimal%2520solution%2520to%2520the%2520internal%250Amaximization%2520and%2520then%2520get%2520a%2520deterministic%2520equivalent%2520objective%2520function%252C%2520which%250Aprovides%2520a%2520theoretical%2520basis%2520for%2520the%2520joint%2520optimization%2520of%2520weights%2520and%2520model%250Aparameters.%2520Meanwhile%252C%2520we%2520propose%2520a%2520fairness%2520elasticity%2520coefficient%2520for%2520the%250Aevaluation%2520of%2520the%2520algorithm%2520with%2520regard%2520to%2520both%2520robustness%2520and%2520robust%2520fairness.%250AExperimental%2520results%2520on%2520various%2520datasets%2520show%2520that%2520the%2520proposed%2520method%2520can%250Aeffectively%2520improve%2520the%2520robust%2520fairness%2520of%2520the%2520model%2520and%2520outperform%2520the%250Astate-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fair%20Class-wise%20Robustness%3A%20Class%20Optimal%20Distribution%0A%20%20Adversarial%20Training&entry.906535625=Hongxin%20Zhi%20and%20Hongtao%20Yu%20and%20Shaome%20Li%20and%20Xiuming%20Zhao%20and%20Yiteng%20Wu&entry.1292438233=%20%20Adversarial%20training%20has%20proven%20to%20be%20a%20highly%20effective%20method%20for%20improving%0Athe%20robustness%20of%20deep%20neural%20networks%20against%20adversarial%20attacks.%0ANonetheless%2C%20it%20has%20been%20observed%20to%20exhibit%20a%20limitation%20in%20terms%20of%20robust%0Afairness%2C%20characterized%20by%20a%20significant%20disparity%20in%20robustness%20across%0Adifferent%20classes.%20Recent%20efforts%20to%20mitigate%20this%20problem%20have%20turned%20to%0Aclass-wise%20reweighted%20methods.%20However%2C%20these%20methods%20suffer%20from%20a%20lack%20of%0Arigorous%20theoretical%20analysis%20and%20are%20limited%20in%20their%20exploration%20of%20the%0Aweight%20space%2C%20as%20they%20mainly%20rely%20on%20existing%20heuristic%20algorithms%20or%20intuition%0Ato%20compute%20weights.%20In%20addition%2C%20these%20methods%20fail%20to%20guarantee%20the%0Aconsistency%20of%20the%20optimization%20direction%20due%20to%20the%20decoupled%20optimization%20of%0Aweights%20and%20the%20model%20parameters.%20They%20potentially%20lead%20to%20suboptimal%20weight%0Aassignments%20and%20consequently%2C%20a%20suboptimal%20model.%20To%20address%20these%20problems%2C%0Athis%20paper%20proposes%20a%20novel%20min-max%20training%20framework%2C%20Class%20Optimal%0ADistribution%20Adversarial%20Training%20%28CODAT%29%2C%20which%20employs%20distributionally%0Arobust%20optimization%20to%20fully%20explore%20the%20class-wise%20weight%20space%2C%20thus%20enabling%0Athe%20identification%20of%20the%20optimal%20weight%20with%20theoretical%20guarantees.%0AFurthermore%2C%20we%20derive%20a%20closed-form%20optimal%20solution%20to%20the%20internal%0Amaximization%20and%20then%20get%20a%20deterministic%20equivalent%20objective%20function%2C%20which%0Aprovides%20a%20theoretical%20basis%20for%20the%20joint%20optimization%20of%20weights%20and%20model%0Aparameters.%20Meanwhile%2C%20we%20propose%20a%20fairness%20elasticity%20coefficient%20for%20the%0Aevaluation%20of%20the%20algorithm%20with%20regard%20to%20both%20robustness%20and%20robust%20fairness.%0AExperimental%20results%20on%20various%20datasets%20show%20that%20the%20proposed%20method%20can%0Aeffectively%20improve%20the%20robust%20fairness%20of%20the%20model%20and%20outperform%20the%0Astate-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04527v1&entry.124074799=Read"},
{"title": "A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in\n  Temporal Point Processes", "author": "Qingmei Wang and Yuxin Wu and Yujie Long and Jing Huang and Fengyuan Ran and Bing Su and Hongteng Xu", "abstract": "  An event sequence generated by a temporal point process is often associated\nwith a hidden and structured event branching process that captures the\ntriggering relations between its historical and current events. In this study,\nwe design a new plug-and-play module based on the Bregman ADMM (BADMM)\nalgorithm, which infers event branches associated with event sequences in the\nmaximum likelihood estimation framework of temporal point processes (TPPs).\nSpecifically, we formulate the inference of event branches as an optimization\nproblem for the event transition matrix under sparse and low-rank constraints,\nwhich is embedded in existing TPP models or their learning paradigms. We can\nimplement this optimization problem based on subspace clustering and sparse\ngroup-lasso, respectively, and solve it using the Bregman ADMM algorithm, whose\nunrolling leads to the proposed BADMM module. When learning a classic TPP\n(e.g., Hawkes process) by the expectation-maximization algorithm, the BADMM\nmodule helps derive structured responsibility matrices in the E-step.\nSimilarly, the BADMM module helps derive low-rank and sparse attention maps for\nthe neural TPPs with self-attention layers. The structured responsibility\nmatrices and attention maps, which work as learned event transition matrices,\nindicate event branches, e.g., inferring isolated events and those key events\ntriggering many subsequent events. Experiments on both synthetic and real-world\ndata show that plugging our BADMM module into existing TPP models and learning\nparadigms can improve model performance and provide us with interpretable\nstructured event branches. The code is available at\n\\url{https://github.com/qingmeiwangdaily/BADMM_TPP}.\n", "link": "http://arxiv.org/abs/2501.04529v1", "date": "2025-01-08", "relevancy": 2.0611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5359}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5211}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Plug-and-Play%20Bregman%20ADMM%20Module%20for%20Inferring%20Event%20Branches%20in%0A%20%20Temporal%20Point%20Processes&body=Title%3A%20A%20Plug-and-Play%20Bregman%20ADMM%20Module%20for%20Inferring%20Event%20Branches%20in%0A%20%20Temporal%20Point%20Processes%0AAuthor%3A%20Qingmei%20Wang%20and%20Yuxin%20Wu%20and%20Yujie%20Long%20and%20Jing%20Huang%20and%20Fengyuan%20Ran%20and%20Bing%20Su%20and%20Hongteng%20Xu%0AAbstract%3A%20%20%20An%20event%20sequence%20generated%20by%20a%20temporal%20point%20process%20is%20often%20associated%0Awith%20a%20hidden%20and%20structured%20event%20branching%20process%20that%20captures%20the%0Atriggering%20relations%20between%20its%20historical%20and%20current%20events.%20In%20this%20study%2C%0Awe%20design%20a%20new%20plug-and-play%20module%20based%20on%20the%20Bregman%20ADMM%20%28BADMM%29%0Aalgorithm%2C%20which%20infers%20event%20branches%20associated%20with%20event%20sequences%20in%20the%0Amaximum%20likelihood%20estimation%20framework%20of%20temporal%20point%20processes%20%28TPPs%29.%0ASpecifically%2C%20we%20formulate%20the%20inference%20of%20event%20branches%20as%20an%20optimization%0Aproblem%20for%20the%20event%20transition%20matrix%20under%20sparse%20and%20low-rank%20constraints%2C%0Awhich%20is%20embedded%20in%20existing%20TPP%20models%20or%20their%20learning%20paradigms.%20We%20can%0Aimplement%20this%20optimization%20problem%20based%20on%20subspace%20clustering%20and%20sparse%0Agroup-lasso%2C%20respectively%2C%20and%20solve%20it%20using%20the%20Bregman%20ADMM%20algorithm%2C%20whose%0Aunrolling%20leads%20to%20the%20proposed%20BADMM%20module.%20When%20learning%20a%20classic%20TPP%0A%28e.g.%2C%20Hawkes%20process%29%20by%20the%20expectation-maximization%20algorithm%2C%20the%20BADMM%0Amodule%20helps%20derive%20structured%20responsibility%20matrices%20in%20the%20E-step.%0ASimilarly%2C%20the%20BADMM%20module%20helps%20derive%20low-rank%20and%20sparse%20attention%20maps%20for%0Athe%20neural%20TPPs%20with%20self-attention%20layers.%20The%20structured%20responsibility%0Amatrices%20and%20attention%20maps%2C%20which%20work%20as%20learned%20event%20transition%20matrices%2C%0Aindicate%20event%20branches%2C%20e.g.%2C%20inferring%20isolated%20events%20and%20those%20key%20events%0Atriggering%20many%20subsequent%20events.%20Experiments%20on%20both%20synthetic%20and%20real-world%0Adata%20show%20that%20plugging%20our%20BADMM%20module%20into%20existing%20TPP%20models%20and%20learning%0Aparadigms%20can%20improve%20model%20performance%20and%20provide%20us%20with%20interpretable%0Astructured%20event%20branches.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/qingmeiwangdaily/BADMM_TPP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Plug-and-Play%2520Bregman%2520ADMM%2520Module%2520for%2520Inferring%2520Event%2520Branches%2520in%250A%2520%2520Temporal%2520Point%2520Processes%26entry.906535625%3DQingmei%2520Wang%2520and%2520Yuxin%2520Wu%2520and%2520Yujie%2520Long%2520and%2520Jing%2520Huang%2520and%2520Fengyuan%2520Ran%2520and%2520Bing%2520Su%2520and%2520Hongteng%2520Xu%26entry.1292438233%3D%2520%2520An%2520event%2520sequence%2520generated%2520by%2520a%2520temporal%2520point%2520process%2520is%2520often%2520associated%250Awith%2520a%2520hidden%2520and%2520structured%2520event%2520branching%2520process%2520that%2520captures%2520the%250Atriggering%2520relations%2520between%2520its%2520historical%2520and%2520current%2520events.%2520In%2520this%2520study%252C%250Awe%2520design%2520a%2520new%2520plug-and-play%2520module%2520based%2520on%2520the%2520Bregman%2520ADMM%2520%2528BADMM%2529%250Aalgorithm%252C%2520which%2520infers%2520event%2520branches%2520associated%2520with%2520event%2520sequences%2520in%2520the%250Amaximum%2520likelihood%2520estimation%2520framework%2520of%2520temporal%2520point%2520processes%2520%2528TPPs%2529.%250ASpecifically%252C%2520we%2520formulate%2520the%2520inference%2520of%2520event%2520branches%2520as%2520an%2520optimization%250Aproblem%2520for%2520the%2520event%2520transition%2520matrix%2520under%2520sparse%2520and%2520low-rank%2520constraints%252C%250Awhich%2520is%2520embedded%2520in%2520existing%2520TPP%2520models%2520or%2520their%2520learning%2520paradigms.%2520We%2520can%250Aimplement%2520this%2520optimization%2520problem%2520based%2520on%2520subspace%2520clustering%2520and%2520sparse%250Agroup-lasso%252C%2520respectively%252C%2520and%2520solve%2520it%2520using%2520the%2520Bregman%2520ADMM%2520algorithm%252C%2520whose%250Aunrolling%2520leads%2520to%2520the%2520proposed%2520BADMM%2520module.%2520When%2520learning%2520a%2520classic%2520TPP%250A%2528e.g.%252C%2520Hawkes%2520process%2529%2520by%2520the%2520expectation-maximization%2520algorithm%252C%2520the%2520BADMM%250Amodule%2520helps%2520derive%2520structured%2520responsibility%2520matrices%2520in%2520the%2520E-step.%250ASimilarly%252C%2520the%2520BADMM%2520module%2520helps%2520derive%2520low-rank%2520and%2520sparse%2520attention%2520maps%2520for%250Athe%2520neural%2520TPPs%2520with%2520self-attention%2520layers.%2520The%2520structured%2520responsibility%250Amatrices%2520and%2520attention%2520maps%252C%2520which%2520work%2520as%2520learned%2520event%2520transition%2520matrices%252C%250Aindicate%2520event%2520branches%252C%2520e.g.%252C%2520inferring%2520isolated%2520events%2520and%2520those%2520key%2520events%250Atriggering%2520many%2520subsequent%2520events.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real-world%250Adata%2520show%2520that%2520plugging%2520our%2520BADMM%2520module%2520into%2520existing%2520TPP%2520models%2520and%2520learning%250Aparadigms%2520can%2520improve%2520model%2520performance%2520and%2520provide%2520us%2520with%2520interpretable%250Astructured%2520event%2520branches.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/qingmeiwangdaily/BADMM_TPP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Plug-and-Play%20Bregman%20ADMM%20Module%20for%20Inferring%20Event%20Branches%20in%0A%20%20Temporal%20Point%20Processes&entry.906535625=Qingmei%20Wang%20and%20Yuxin%20Wu%20and%20Yujie%20Long%20and%20Jing%20Huang%20and%20Fengyuan%20Ran%20and%20Bing%20Su%20and%20Hongteng%20Xu&entry.1292438233=%20%20An%20event%20sequence%20generated%20by%20a%20temporal%20point%20process%20is%20often%20associated%0Awith%20a%20hidden%20and%20structured%20event%20branching%20process%20that%20captures%20the%0Atriggering%20relations%20between%20its%20historical%20and%20current%20events.%20In%20this%20study%2C%0Awe%20design%20a%20new%20plug-and-play%20module%20based%20on%20the%20Bregman%20ADMM%20%28BADMM%29%0Aalgorithm%2C%20which%20infers%20event%20branches%20associated%20with%20event%20sequences%20in%20the%0Amaximum%20likelihood%20estimation%20framework%20of%20temporal%20point%20processes%20%28TPPs%29.%0ASpecifically%2C%20we%20formulate%20the%20inference%20of%20event%20branches%20as%20an%20optimization%0Aproblem%20for%20the%20event%20transition%20matrix%20under%20sparse%20and%20low-rank%20constraints%2C%0Awhich%20is%20embedded%20in%20existing%20TPP%20models%20or%20their%20learning%20paradigms.%20We%20can%0Aimplement%20this%20optimization%20problem%20based%20on%20subspace%20clustering%20and%20sparse%0Agroup-lasso%2C%20respectively%2C%20and%20solve%20it%20using%20the%20Bregman%20ADMM%20algorithm%2C%20whose%0Aunrolling%20leads%20to%20the%20proposed%20BADMM%20module.%20When%20learning%20a%20classic%20TPP%0A%28e.g.%2C%20Hawkes%20process%29%20by%20the%20expectation-maximization%20algorithm%2C%20the%20BADMM%0Amodule%20helps%20derive%20structured%20responsibility%20matrices%20in%20the%20E-step.%0ASimilarly%2C%20the%20BADMM%20module%20helps%20derive%20low-rank%20and%20sparse%20attention%20maps%20for%0Athe%20neural%20TPPs%20with%20self-attention%20layers.%20The%20structured%20responsibility%0Amatrices%20and%20attention%20maps%2C%20which%20work%20as%20learned%20event%20transition%20matrices%2C%0Aindicate%20event%20branches%2C%20e.g.%2C%20inferring%20isolated%20events%20and%20those%20key%20events%0Atriggering%20many%20subsequent%20events.%20Experiments%20on%20both%20synthetic%20and%20real-world%0Adata%20show%20that%20plugging%20our%20BADMM%20module%20into%20existing%20TPP%20models%20and%20learning%0Aparadigms%20can%20improve%20model%20performance%20and%20provide%20us%20with%20interpretable%0Astructured%20event%20branches.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/qingmeiwangdaily/BADMM_TPP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04529v1&entry.124074799=Read"},
{"title": "Re-ranking the Context for Multimodal Retrieval Augmented Generation", "author": "Matin Mortaheb and Mohammad A. Amir Khojastepour and Srimat T. Chakradhar and Sennur Ulukus", "abstract": "  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response.\n", "link": "http://arxiv.org/abs/2501.04695v1", "date": "2025-01-08", "relevancy": 2.06, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-ranking%20the%20Context%20for%20Multimodal%20Retrieval%20Augmented%20Generation&body=Title%3A%20Re-ranking%20the%20Context%20for%20Multimodal%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Matin%20Mortaheb%20and%20Mohammad%20A.%20Amir%20Khojastepour%20and%20Srimat%20T.%20Chakradhar%20and%20Sennur%20Ulukus%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20enhances%20large%20language%20models%20%28LLMs%29%20by%0Aincorporating%20external%20knowledge%20to%20generate%20a%20response%20within%20a%20context%20with%0Aimproved%20accuracy%20and%20reduced%20hallucinations.%20However%2C%20multi-modal%20RAG%20systems%0Aface%20unique%20challenges%3A%20%28i%29%20the%20retrieval%20process%20may%20select%20irrelevant%20entries%0Ato%20user%20query%20%28e.g.%2C%20images%2C%20documents%29%2C%20and%20%28ii%29%20vision-language%20models%20or%0Amulti-modal%20language%20models%20like%20GPT-4o%20may%20hallucinate%20when%20processing%20these%0Aentries%20to%20generate%20RAG%20output.%20In%20this%20paper%2C%20we%20aim%20to%20address%20the%20first%0Achallenge%2C%20i.e%2C%20improving%20the%20selection%20of%20relevant%20context%20from%20the%0Aknowledge-base%20in%20retrieval%20phase%20of%20the%20multi-modal%20RAG.%20Specifically%2C%20we%0Aleverage%20the%20relevancy%20score%20%28RS%29%20measure%20designed%20in%20our%20previous%20work%20for%0Aevaluating%20the%20RAG%20performance%20to%20select%20more%20relevant%20entries%20in%20retrieval%0Aprocess.%20The%20retrieval%20based%20on%20embeddings%2C%20say%20CLIP-based%20embedding%2C%20and%0Acosine%20similarity%20usually%20perform%20poorly%20particularly%20for%20multi-modal%20data.%20We%0Ashow%20that%20by%20using%20a%20more%20advanced%20relevancy%20measure%2C%20one%20can%20enhance%20the%0Aretrieval%20process%20by%20selecting%20more%20relevant%20pieces%20from%20the%20knowledge-base%20and%0Aeliminate%20the%20irrelevant%20pieces%20from%20the%20context%20by%20adaptively%20selecting%0Aup-to-%24k%24%20entries%20instead%20of%20fixed%20number%20of%20entries.%20Our%20evaluation%20using%20COCO%0Adataset%20demonstrates%20significant%20enhancement%20in%20selecting%20relevant%20context%20and%0Aaccuracy%20of%20the%20generated%20response.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-ranking%2520the%2520Context%2520for%2520Multimodal%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DMatin%2520Mortaheb%2520and%2520Mohammad%2520A.%2520Amir%2520Khojastepour%2520and%2520Srimat%2520T.%2520Chakradhar%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520enhances%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%250Aincorporating%2520external%2520knowledge%2520to%2520generate%2520a%2520response%2520within%2520a%2520context%2520with%250Aimproved%2520accuracy%2520and%2520reduced%2520hallucinations.%2520However%252C%2520multi-modal%2520RAG%2520systems%250Aface%2520unique%2520challenges%253A%2520%2528i%2529%2520the%2520retrieval%2520process%2520may%2520select%2520irrelevant%2520entries%250Ato%2520user%2520query%2520%2528e.g.%252C%2520images%252C%2520documents%2529%252C%2520and%2520%2528ii%2529%2520vision-language%2520models%2520or%250Amulti-modal%2520language%2520models%2520like%2520GPT-4o%2520may%2520hallucinate%2520when%2520processing%2520these%250Aentries%2520to%2520generate%2520RAG%2520output.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520address%2520the%2520first%250Achallenge%252C%2520i.e%252C%2520improving%2520the%2520selection%2520of%2520relevant%2520context%2520from%2520the%250Aknowledge-base%2520in%2520retrieval%2520phase%2520of%2520the%2520multi-modal%2520RAG.%2520Specifically%252C%2520we%250Aleverage%2520the%2520relevancy%2520score%2520%2528RS%2529%2520measure%2520designed%2520in%2520our%2520previous%2520work%2520for%250Aevaluating%2520the%2520RAG%2520performance%2520to%2520select%2520more%2520relevant%2520entries%2520in%2520retrieval%250Aprocess.%2520The%2520retrieval%2520based%2520on%2520embeddings%252C%2520say%2520CLIP-based%2520embedding%252C%2520and%250Acosine%2520similarity%2520usually%2520perform%2520poorly%2520particularly%2520for%2520multi-modal%2520data.%2520We%250Ashow%2520that%2520by%2520using%2520a%2520more%2520advanced%2520relevancy%2520measure%252C%2520one%2520can%2520enhance%2520the%250Aretrieval%2520process%2520by%2520selecting%2520more%2520relevant%2520pieces%2520from%2520the%2520knowledge-base%2520and%250Aeliminate%2520the%2520irrelevant%2520pieces%2520from%2520the%2520context%2520by%2520adaptively%2520selecting%250Aup-to-%2524k%2524%2520entries%2520instead%2520of%2520fixed%2520number%2520of%2520entries.%2520Our%2520evaluation%2520using%2520COCO%250Adataset%2520demonstrates%2520significant%2520enhancement%2520in%2520selecting%2520relevant%2520context%2520and%250Aaccuracy%2520of%2520the%2520generated%2520response.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-ranking%20the%20Context%20for%20Multimodal%20Retrieval%20Augmented%20Generation&entry.906535625=Matin%20Mortaheb%20and%20Mohammad%20A.%20Amir%20Khojastepour%20and%20Srimat%20T.%20Chakradhar%20and%20Sennur%20Ulukus&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20enhances%20large%20language%20models%20%28LLMs%29%20by%0Aincorporating%20external%20knowledge%20to%20generate%20a%20response%20within%20a%20context%20with%0Aimproved%20accuracy%20and%20reduced%20hallucinations.%20However%2C%20multi-modal%20RAG%20systems%0Aface%20unique%20challenges%3A%20%28i%29%20the%20retrieval%20process%20may%20select%20irrelevant%20entries%0Ato%20user%20query%20%28e.g.%2C%20images%2C%20documents%29%2C%20and%20%28ii%29%20vision-language%20models%20or%0Amulti-modal%20language%20models%20like%20GPT-4o%20may%20hallucinate%20when%20processing%20these%0Aentries%20to%20generate%20RAG%20output.%20In%20this%20paper%2C%20we%20aim%20to%20address%20the%20first%0Achallenge%2C%20i.e%2C%20improving%20the%20selection%20of%20relevant%20context%20from%20the%0Aknowledge-base%20in%20retrieval%20phase%20of%20the%20multi-modal%20RAG.%20Specifically%2C%20we%0Aleverage%20the%20relevancy%20score%20%28RS%29%20measure%20designed%20in%20our%20previous%20work%20for%0Aevaluating%20the%20RAG%20performance%20to%20select%20more%20relevant%20entries%20in%20retrieval%0Aprocess.%20The%20retrieval%20based%20on%20embeddings%2C%20say%20CLIP-based%20embedding%2C%20and%0Acosine%20similarity%20usually%20perform%20poorly%20particularly%20for%20multi-modal%20data.%20We%0Ashow%20that%20by%20using%20a%20more%20advanced%20relevancy%20measure%2C%20one%20can%20enhance%20the%0Aretrieval%20process%20by%20selecting%20more%20relevant%20pieces%20from%20the%20knowledge-base%20and%0Aeliminate%20the%20irrelevant%20pieces%20from%20the%20context%20by%20adaptively%20selecting%0Aup-to-%24k%24%20entries%20instead%20of%20fixed%20number%20of%20entries.%20Our%20evaluation%20using%20COCO%0Adataset%20demonstrates%20significant%20enhancement%20in%20selecting%20relevant%20context%20and%0Aaccuracy%20of%20the%20generated%20response.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04695v1&entry.124074799=Read"},
{"title": "Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA", "author": "Lanlan Feng and Ce Zhu and Yipeng Liu and Saiprasad Ravishankar and Longxiu Huang", "abstract": "  Robust tensor principal component analysis (RTPCA) aims to separate the\nlow-rank and sparse components from multi-dimensional data, making it an\nessential technique in the signal processing and computer vision fields.\nRecently emerging tensor singular value decomposition (t-SVD) has gained\nconsiderable attention for its ability to better capture the low-rank structure\nof tensors compared to traditional matrix SVD. However, existing methods often\nrely on the computationally expensive tensor nuclear norm (TNN), which limits\ntheir scalability for real-world tensors. To address this issue, we explore an\nefficient scaled gradient descent (SGD) approach within the t-SVD framework for\nthe first time, and propose the RTPCA-SGD method. Theoretically, we rigorously\nestablish the recovery guarantees of RTPCA-SGD under mild assumptions,\ndemonstrating that with appropriate parameter selection, it achieves linear\nconvergence to the true low-rank tensor at a constant rate, independent of the\ncondition number. To enhance its practical applicability, we further propose a\nlearnable self-supervised deep unfolding model, which enables effective\nparameter learning. Numerical experiments on both synthetic and real-world\ndatasets demonstrate the superior performance of the proposed methods while\nmaintaining competitive computational efficiency, especially consuming less\ntime than RTPCA-TNN.\n", "link": "http://arxiv.org/abs/2501.04565v1", "date": "2025-01-08", "relevancy": 2.0442, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5454}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4881}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Scaled%20Gradient%20Descent%20for%20Guaranteed%20Robust%20Tensor%20PCA&body=Title%3A%20Learnable%20Scaled%20Gradient%20Descent%20for%20Guaranteed%20Robust%20Tensor%20PCA%0AAuthor%3A%20Lanlan%20Feng%20and%20Ce%20Zhu%20and%20Yipeng%20Liu%20and%20Saiprasad%20Ravishankar%20and%20Longxiu%20Huang%0AAbstract%3A%20%20%20Robust%20tensor%20principal%20component%20analysis%20%28RTPCA%29%20aims%20to%20separate%20the%0Alow-rank%20and%20sparse%20components%20from%20multi-dimensional%20data%2C%20making%20it%20an%0Aessential%20technique%20in%20the%20signal%20processing%20and%20computer%20vision%20fields.%0ARecently%20emerging%20tensor%20singular%20value%20decomposition%20%28t-SVD%29%20has%20gained%0Aconsiderable%20attention%20for%20its%20ability%20to%20better%20capture%20the%20low-rank%20structure%0Aof%20tensors%20compared%20to%20traditional%20matrix%20SVD.%20However%2C%20existing%20methods%20often%0Arely%20on%20the%20computationally%20expensive%20tensor%20nuclear%20norm%20%28TNN%29%2C%20which%20limits%0Atheir%20scalability%20for%20real-world%20tensors.%20To%20address%20this%20issue%2C%20we%20explore%20an%0Aefficient%20scaled%20gradient%20descent%20%28SGD%29%20approach%20within%20the%20t-SVD%20framework%20for%0Athe%20first%20time%2C%20and%20propose%20the%20RTPCA-SGD%20method.%20Theoretically%2C%20we%20rigorously%0Aestablish%20the%20recovery%20guarantees%20of%20RTPCA-SGD%20under%20mild%20assumptions%2C%0Ademonstrating%20that%20with%20appropriate%20parameter%20selection%2C%20it%20achieves%20linear%0Aconvergence%20to%20the%20true%20low-rank%20tensor%20at%20a%20constant%20rate%2C%20independent%20of%20the%0Acondition%20number.%20To%20enhance%20its%20practical%20applicability%2C%20we%20further%20propose%20a%0Alearnable%20self-supervised%20deep%20unfolding%20model%2C%20which%20enables%20effective%0Aparameter%20learning.%20Numerical%20experiments%20on%20both%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20methods%20while%0Amaintaining%20competitive%20computational%20efficiency%2C%20especially%20consuming%20less%0Atime%20than%20RTPCA-TNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Scaled%2520Gradient%2520Descent%2520for%2520Guaranteed%2520Robust%2520Tensor%2520PCA%26entry.906535625%3DLanlan%2520Feng%2520and%2520Ce%2520Zhu%2520and%2520Yipeng%2520Liu%2520and%2520Saiprasad%2520Ravishankar%2520and%2520Longxiu%2520Huang%26entry.1292438233%3D%2520%2520Robust%2520tensor%2520principal%2520component%2520analysis%2520%2528RTPCA%2529%2520aims%2520to%2520separate%2520the%250Alow-rank%2520and%2520sparse%2520components%2520from%2520multi-dimensional%2520data%252C%2520making%2520it%2520an%250Aessential%2520technique%2520in%2520the%2520signal%2520processing%2520and%2520computer%2520vision%2520fields.%250ARecently%2520emerging%2520tensor%2520singular%2520value%2520decomposition%2520%2528t-SVD%2529%2520has%2520gained%250Aconsiderable%2520attention%2520for%2520its%2520ability%2520to%2520better%2520capture%2520the%2520low-rank%2520structure%250Aof%2520tensors%2520compared%2520to%2520traditional%2520matrix%2520SVD.%2520However%252C%2520existing%2520methods%2520often%250Arely%2520on%2520the%2520computationally%2520expensive%2520tensor%2520nuclear%2520norm%2520%2528TNN%2529%252C%2520which%2520limits%250Atheir%2520scalability%2520for%2520real-world%2520tensors.%2520To%2520address%2520this%2520issue%252C%2520we%2520explore%2520an%250Aefficient%2520scaled%2520gradient%2520descent%2520%2528SGD%2529%2520approach%2520within%2520the%2520t-SVD%2520framework%2520for%250Athe%2520first%2520time%252C%2520and%2520propose%2520the%2520RTPCA-SGD%2520method.%2520Theoretically%252C%2520we%2520rigorously%250Aestablish%2520the%2520recovery%2520guarantees%2520of%2520RTPCA-SGD%2520under%2520mild%2520assumptions%252C%250Ademonstrating%2520that%2520with%2520appropriate%2520parameter%2520selection%252C%2520it%2520achieves%2520linear%250Aconvergence%2520to%2520the%2520true%2520low-rank%2520tensor%2520at%2520a%2520constant%2520rate%252C%2520independent%2520of%2520the%250Acondition%2520number.%2520To%2520enhance%2520its%2520practical%2520applicability%252C%2520we%2520further%2520propose%2520a%250Alearnable%2520self-supervised%2520deep%2520unfolding%2520model%252C%2520which%2520enables%2520effective%250Aparameter%2520learning.%2520Numerical%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%250Adatasets%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520methods%2520while%250Amaintaining%2520competitive%2520computational%2520efficiency%252C%2520especially%2520consuming%2520less%250Atime%2520than%2520RTPCA-TNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Scaled%20Gradient%20Descent%20for%20Guaranteed%20Robust%20Tensor%20PCA&entry.906535625=Lanlan%20Feng%20and%20Ce%20Zhu%20and%20Yipeng%20Liu%20and%20Saiprasad%20Ravishankar%20and%20Longxiu%20Huang&entry.1292438233=%20%20Robust%20tensor%20principal%20component%20analysis%20%28RTPCA%29%20aims%20to%20separate%20the%0Alow-rank%20and%20sparse%20components%20from%20multi-dimensional%20data%2C%20making%20it%20an%0Aessential%20technique%20in%20the%20signal%20processing%20and%20computer%20vision%20fields.%0ARecently%20emerging%20tensor%20singular%20value%20decomposition%20%28t-SVD%29%20has%20gained%0Aconsiderable%20attention%20for%20its%20ability%20to%20better%20capture%20the%20low-rank%20structure%0Aof%20tensors%20compared%20to%20traditional%20matrix%20SVD.%20However%2C%20existing%20methods%20often%0Arely%20on%20the%20computationally%20expensive%20tensor%20nuclear%20norm%20%28TNN%29%2C%20which%20limits%0Atheir%20scalability%20for%20real-world%20tensors.%20To%20address%20this%20issue%2C%20we%20explore%20an%0Aefficient%20scaled%20gradient%20descent%20%28SGD%29%20approach%20within%20the%20t-SVD%20framework%20for%0Athe%20first%20time%2C%20and%20propose%20the%20RTPCA-SGD%20method.%20Theoretically%2C%20we%20rigorously%0Aestablish%20the%20recovery%20guarantees%20of%20RTPCA-SGD%20under%20mild%20assumptions%2C%0Ademonstrating%20that%20with%20appropriate%20parameter%20selection%2C%20it%20achieves%20linear%0Aconvergence%20to%20the%20true%20low-rank%20tensor%20at%20a%20constant%20rate%2C%20independent%20of%20the%0Acondition%20number.%20To%20enhance%20its%20practical%20applicability%2C%20we%20further%20propose%20a%0Alearnable%20self-supervised%20deep%20unfolding%20model%2C%20which%20enables%20effective%0Aparameter%20learning.%20Numerical%20experiments%20on%20both%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20methods%20while%0Amaintaining%20competitive%20computational%20efficiency%2C%20especially%20consuming%20less%0Atime%20than%20RTPCA-TNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04565v1&entry.124074799=Read"},
{"title": "A novel Facial Recognition technique with Focusing on Masked Faces", "author": "Dana A Abdullah and Dana Rasul Hamad and Hakem Beitollahi and Ismail Y Maolood and Abdulhady Abas Abdullah and Aso Khaleel Ameen", "abstract": "  Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.\n", "link": "http://arxiv.org/abs/2501.04444v1", "date": "2025-01-08", "relevancy": 2.033, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5207}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5059}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20novel%20Facial%20Recognition%20technique%20with%20Focusing%20on%20Masked%20Faces&body=Title%3A%20A%20novel%20Facial%20Recognition%20technique%20with%20Focusing%20on%20Masked%20Faces%0AAuthor%3A%20Dana%20A%20Abdullah%20and%20Dana%20Rasul%20Hamad%20and%20Hakem%20Beitollahi%20and%20Ismail%20Y%20Maolood%20and%20Abdulhady%20Abas%20Abdullah%20and%20Aso%20Khaleel%20Ameen%0AAbstract%3A%20%20%20Recognizing%20the%20same%20faces%20with%20and%20without%20masks%20is%20important%20for%20ensuring%0Aconsistent%20identification%20in%20security%2C%20access%20control%2C%20and%20public%20safety.%20This%0Acapability%20is%20crucial%20in%20scenarios%20like%20law%20enforcement%2C%20healthcare%2C%20and%0Asurveillance%2C%20where%20accurate%20recognition%20must%20be%20maintained%20despite%20facial%0Aocclusion.%20This%20research%20focuses%20on%20the%20challenge%20of%20recognizing%20the%20same%20faces%0Awith%20and%20without%20masks%20by%20employing%20cosine%20similarity%20as%20the%20primary%20technique.%0AWith%20the%20increased%20use%20of%20masks%2C%20traditional%20facial%20recognition%20systems%20face%0Asignificant%20accuracy%20issues%2C%20making%20it%20crucial%20to%20develop%20methods%20that%20can%0Areliably%20identify%20individuals%20in%20masked%20conditions.%20For%20that%20reason%2C%20this%20study%0Aproposed%20Masked-Unmasked%20Face%20Matching%20Model%20%28MUFM%29.%20This%20model%20employs%0Atransfer%20learning%20using%20the%20Visual%20Geometry%20Group%20%28VGG16%29%20model%20to%20extract%0Asignificant%20facial%20features%2C%20which%20are%20subsequently%20classified%20utilizing%20the%0AK-Nearest%20Neighbors%20%28K-NN%29%20algorithm.%20The%20cosine%20similarity%20metric%20is%20employed%0Ato%20compare%20masked%20and%20unmasked%20faces%20of%20the%20same%20individuals.%20This%20approach%0Arepresents%20a%20novel%20contribution%2C%20as%20the%20task%20of%20recognizing%20the%20same%20individual%0Awith%20and%20without%20a%20mask%20using%20cosine%20similarity%20has%20not%20been%20previously%0Aaddressed.%20By%20integrating%20these%20advanced%20methodologies%2C%20the%20research%0Ademonstrates%20effective%20identification%20of%20individuals%20despite%20the%20presence%20of%0Amasks%2C%20addressing%20a%20significant%20limitation%20in%20traditional%20systems.%20Using%20data%0Ais%20another%20essential%20part%20of%20this%20work%2C%20by%20collecting%20and%20preparing%20an%20image%0Adataset%20from%20three%20different%20sources%20especially%20some%20of%20those%20data%20are%20real%0Aprovided%20a%20comprehensive%20power%20of%20this%20research.%20The%20image%20dataset%20used%20were%0Aalready%20collected%20in%20three%20different%20datasets%20of%20masked%20and%20unmasked%20for%20the%0Asame%20faces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520novel%2520Facial%2520Recognition%2520technique%2520with%2520Focusing%2520on%2520Masked%2520Faces%26entry.906535625%3DDana%2520A%2520Abdullah%2520and%2520Dana%2520Rasul%2520Hamad%2520and%2520Hakem%2520Beitollahi%2520and%2520Ismail%2520Y%2520Maolood%2520and%2520Abdulhady%2520Abas%2520Abdullah%2520and%2520Aso%2520Khaleel%2520Ameen%26entry.1292438233%3D%2520%2520Recognizing%2520the%2520same%2520faces%2520with%2520and%2520without%2520masks%2520is%2520important%2520for%2520ensuring%250Aconsistent%2520identification%2520in%2520security%252C%2520access%2520control%252C%2520and%2520public%2520safety.%2520This%250Acapability%2520is%2520crucial%2520in%2520scenarios%2520like%2520law%2520enforcement%252C%2520healthcare%252C%2520and%250Asurveillance%252C%2520where%2520accurate%2520recognition%2520must%2520be%2520maintained%2520despite%2520facial%250Aocclusion.%2520This%2520research%2520focuses%2520on%2520the%2520challenge%2520of%2520recognizing%2520the%2520same%2520faces%250Awith%2520and%2520without%2520masks%2520by%2520employing%2520cosine%2520similarity%2520as%2520the%2520primary%2520technique.%250AWith%2520the%2520increased%2520use%2520of%2520masks%252C%2520traditional%2520facial%2520recognition%2520systems%2520face%250Asignificant%2520accuracy%2520issues%252C%2520making%2520it%2520crucial%2520to%2520develop%2520methods%2520that%2520can%250Areliably%2520identify%2520individuals%2520in%2520masked%2520conditions.%2520For%2520that%2520reason%252C%2520this%2520study%250Aproposed%2520Masked-Unmasked%2520Face%2520Matching%2520Model%2520%2528MUFM%2529.%2520This%2520model%2520employs%250Atransfer%2520learning%2520using%2520the%2520Visual%2520Geometry%2520Group%2520%2528VGG16%2529%2520model%2520to%2520extract%250Asignificant%2520facial%2520features%252C%2520which%2520are%2520subsequently%2520classified%2520utilizing%2520the%250AK-Nearest%2520Neighbors%2520%2528K-NN%2529%2520algorithm.%2520The%2520cosine%2520similarity%2520metric%2520is%2520employed%250Ato%2520compare%2520masked%2520and%2520unmasked%2520faces%2520of%2520the%2520same%2520individuals.%2520This%2520approach%250Arepresents%2520a%2520novel%2520contribution%252C%2520as%2520the%2520task%2520of%2520recognizing%2520the%2520same%2520individual%250Awith%2520and%2520without%2520a%2520mask%2520using%2520cosine%2520similarity%2520has%2520not%2520been%2520previously%250Aaddressed.%2520By%2520integrating%2520these%2520advanced%2520methodologies%252C%2520the%2520research%250Ademonstrates%2520effective%2520identification%2520of%2520individuals%2520despite%2520the%2520presence%2520of%250Amasks%252C%2520addressing%2520a%2520significant%2520limitation%2520in%2520traditional%2520systems.%2520Using%2520data%250Ais%2520another%2520essential%2520part%2520of%2520this%2520work%252C%2520by%2520collecting%2520and%2520preparing%2520an%2520image%250Adataset%2520from%2520three%2520different%2520sources%2520especially%2520some%2520of%2520those%2520data%2520are%2520real%250Aprovided%2520a%2520comprehensive%2520power%2520of%2520this%2520research.%2520The%2520image%2520dataset%2520used%2520were%250Aalready%2520collected%2520in%2520three%2520different%2520datasets%2520of%2520masked%2520and%2520unmasked%2520for%2520the%250Asame%2520faces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20novel%20Facial%20Recognition%20technique%20with%20Focusing%20on%20Masked%20Faces&entry.906535625=Dana%20A%20Abdullah%20and%20Dana%20Rasul%20Hamad%20and%20Hakem%20Beitollahi%20and%20Ismail%20Y%20Maolood%20and%20Abdulhady%20Abas%20Abdullah%20and%20Aso%20Khaleel%20Ameen&entry.1292438233=%20%20Recognizing%20the%20same%20faces%20with%20and%20without%20masks%20is%20important%20for%20ensuring%0Aconsistent%20identification%20in%20security%2C%20access%20control%2C%20and%20public%20safety.%20This%0Acapability%20is%20crucial%20in%20scenarios%20like%20law%20enforcement%2C%20healthcare%2C%20and%0Asurveillance%2C%20where%20accurate%20recognition%20must%20be%20maintained%20despite%20facial%0Aocclusion.%20This%20research%20focuses%20on%20the%20challenge%20of%20recognizing%20the%20same%20faces%0Awith%20and%20without%20masks%20by%20employing%20cosine%20similarity%20as%20the%20primary%20technique.%0AWith%20the%20increased%20use%20of%20masks%2C%20traditional%20facial%20recognition%20systems%20face%0Asignificant%20accuracy%20issues%2C%20making%20it%20crucial%20to%20develop%20methods%20that%20can%0Areliably%20identify%20individuals%20in%20masked%20conditions.%20For%20that%20reason%2C%20this%20study%0Aproposed%20Masked-Unmasked%20Face%20Matching%20Model%20%28MUFM%29.%20This%20model%20employs%0Atransfer%20learning%20using%20the%20Visual%20Geometry%20Group%20%28VGG16%29%20model%20to%20extract%0Asignificant%20facial%20features%2C%20which%20are%20subsequently%20classified%20utilizing%20the%0AK-Nearest%20Neighbors%20%28K-NN%29%20algorithm.%20The%20cosine%20similarity%20metric%20is%20employed%0Ato%20compare%20masked%20and%20unmasked%20faces%20of%20the%20same%20individuals.%20This%20approach%0Arepresents%20a%20novel%20contribution%2C%20as%20the%20task%20of%20recognizing%20the%20same%20individual%0Awith%20and%20without%20a%20mask%20using%20cosine%20similarity%20has%20not%20been%20previously%0Aaddressed.%20By%20integrating%20these%20advanced%20methodologies%2C%20the%20research%0Ademonstrates%20effective%20identification%20of%20individuals%20despite%20the%20presence%20of%0Amasks%2C%20addressing%20a%20significant%20limitation%20in%20traditional%20systems.%20Using%20data%0Ais%20another%20essential%20part%20of%20this%20work%2C%20by%20collecting%20and%20preparing%20an%20image%0Adataset%20from%20three%20different%20sources%20especially%20some%20of%20those%20data%20are%20real%0Aprovided%20a%20comprehensive%20power%20of%20this%20research.%20The%20image%20dataset%20used%20were%0Aalready%20collected%20in%20three%20different%20datasets%20of%20masked%20and%20unmasked%20for%20the%0Asame%20faces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04444v1&entry.124074799=Read"},
{"title": "The Indoor-Training Effect: unexpected gains from distribution shifts in\n  the transition function", "author": "Serena Bono and Spandan Madan and Ishaan Grover and Mao Yasueda and Cynthia Breazeal and Hanspeter Pfister and Gabriel Kreiman", "abstract": "  Is it better to perform tennis training in a pristine indoor environment or a\nnoisy outdoor one? To model this problem, here we investigate whether shifts in\nthe transition probabilities between the training and testing environments in\nreinforcement learning problems can lead to better performance under certain\nconditions. We generate new Markov Decision Processes (MDPs) starting from a\ngiven MDP, by adding quantifiable, parametric noise into the transition\nfunction. We refer to this process as Noise Injection and the resulting\nenvironments as {\\delta}-environments. This process allows us to create\nvariations of the same environment with quantitative control over noise serving\nas a metric of distance between environments. Conventional wisdom suggests that\ntraining and testing on the same MDP should yield the best results. In stark\ncontrast, we observe that agents can perform better when trained on the\nnoise-free environment and tested on the noisy {\\delta}-environments, compared\nto training and testing on the same {\\delta}-environments. We confirm that this\nfinding extends beyond noise variations: it is possible to showcase the same\nphenomenon in ATARI game variations including varying Ghost behaviour in\nPacMan, and Paddle behaviour in Pong. We demonstrate this intriguing behaviour\nacross 60 different variations of ATARI games, including PacMan, Pong, and\nBreakout. We refer to this phenomenon as the Indoor-Training Effect. Code to\nreproduce our experiments and to implement Noise Injection can be found at\nhttps://bit.ly/3X6CTYk.\n", "link": "http://arxiv.org/abs/2401.15856v2", "date": "2025-01-08", "relevancy": 2.0259, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5214}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4998}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Indoor-Training%20Effect%3A%20unexpected%20gains%20from%20distribution%20shifts%20in%0A%20%20the%20transition%20function&body=Title%3A%20The%20Indoor-Training%20Effect%3A%20unexpected%20gains%20from%20distribution%20shifts%20in%0A%20%20the%20transition%20function%0AAuthor%3A%20Serena%20Bono%20and%20Spandan%20Madan%20and%20Ishaan%20Grover%20and%20Mao%20Yasueda%20and%20Cynthia%20Breazeal%20and%20Hanspeter%20Pfister%20and%20Gabriel%20Kreiman%0AAbstract%3A%20%20%20Is%20it%20better%20to%20perform%20tennis%20training%20in%20a%20pristine%20indoor%20environment%20or%20a%0Anoisy%20outdoor%20one%3F%20To%20model%20this%20problem%2C%20here%20we%20investigate%20whether%20shifts%20in%0Athe%20transition%20probabilities%20between%20the%20training%20and%20testing%20environments%20in%0Areinforcement%20learning%20problems%20can%20lead%20to%20better%20performance%20under%20certain%0Aconditions.%20We%20generate%20new%20Markov%20Decision%20Processes%20%28MDPs%29%20starting%20from%20a%0Agiven%20MDP%2C%20by%20adding%20quantifiable%2C%20parametric%20noise%20into%20the%20transition%0Afunction.%20We%20refer%20to%20this%20process%20as%20Noise%20Injection%20and%20the%20resulting%0Aenvironments%20as%20%7B%5Cdelta%7D-environments.%20This%20process%20allows%20us%20to%20create%0Avariations%20of%20the%20same%20environment%20with%20quantitative%20control%20over%20noise%20serving%0Aas%20a%20metric%20of%20distance%20between%20environments.%20Conventional%20wisdom%20suggests%20that%0Atraining%20and%20testing%20on%20the%20same%20MDP%20should%20yield%20the%20best%20results.%20In%20stark%0Acontrast%2C%20we%20observe%20that%20agents%20can%20perform%20better%20when%20trained%20on%20the%0Anoise-free%20environment%20and%20tested%20on%20the%20noisy%20%7B%5Cdelta%7D-environments%2C%20compared%0Ato%20training%20and%20testing%20on%20the%20same%20%7B%5Cdelta%7D-environments.%20We%20confirm%20that%20this%0Afinding%20extends%20beyond%20noise%20variations%3A%20it%20is%20possible%20to%20showcase%20the%20same%0Aphenomenon%20in%20ATARI%20game%20variations%20including%20varying%20Ghost%20behaviour%20in%0APacMan%2C%20and%20Paddle%20behaviour%20in%20Pong.%20We%20demonstrate%20this%20intriguing%20behaviour%0Aacross%2060%20different%20variations%20of%20ATARI%20games%2C%20including%20PacMan%2C%20Pong%2C%20and%0ABreakout.%20We%20refer%20to%20this%20phenomenon%20as%20the%20Indoor-Training%20Effect.%20Code%20to%0Areproduce%20our%20experiments%20and%20to%20implement%20Noise%20Injection%20can%20be%20found%20at%0Ahttps%3A//bit.ly/3X6CTYk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Indoor-Training%2520Effect%253A%2520unexpected%2520gains%2520from%2520distribution%2520shifts%2520in%250A%2520%2520the%2520transition%2520function%26entry.906535625%3DSerena%2520Bono%2520and%2520Spandan%2520Madan%2520and%2520Ishaan%2520Grover%2520and%2520Mao%2520Yasueda%2520and%2520Cynthia%2520Breazeal%2520and%2520Hanspeter%2520Pfister%2520and%2520Gabriel%2520Kreiman%26entry.1292438233%3D%2520%2520Is%2520it%2520better%2520to%2520perform%2520tennis%2520training%2520in%2520a%2520pristine%2520indoor%2520environment%2520or%2520a%250Anoisy%2520outdoor%2520one%253F%2520To%2520model%2520this%2520problem%252C%2520here%2520we%2520investigate%2520whether%2520shifts%2520in%250Athe%2520transition%2520probabilities%2520between%2520the%2520training%2520and%2520testing%2520environments%2520in%250Areinforcement%2520learning%2520problems%2520can%2520lead%2520to%2520better%2520performance%2520under%2520certain%250Aconditions.%2520We%2520generate%2520new%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529%2520starting%2520from%2520a%250Agiven%2520MDP%252C%2520by%2520adding%2520quantifiable%252C%2520parametric%2520noise%2520into%2520the%2520transition%250Afunction.%2520We%2520refer%2520to%2520this%2520process%2520as%2520Noise%2520Injection%2520and%2520the%2520resulting%250Aenvironments%2520as%2520%257B%255Cdelta%257D-environments.%2520This%2520process%2520allows%2520us%2520to%2520create%250Avariations%2520of%2520the%2520same%2520environment%2520with%2520quantitative%2520control%2520over%2520noise%2520serving%250Aas%2520a%2520metric%2520of%2520distance%2520between%2520environments.%2520Conventional%2520wisdom%2520suggests%2520that%250Atraining%2520and%2520testing%2520on%2520the%2520same%2520MDP%2520should%2520yield%2520the%2520best%2520results.%2520In%2520stark%250Acontrast%252C%2520we%2520observe%2520that%2520agents%2520can%2520perform%2520better%2520when%2520trained%2520on%2520the%250Anoise-free%2520environment%2520and%2520tested%2520on%2520the%2520noisy%2520%257B%255Cdelta%257D-environments%252C%2520compared%250Ato%2520training%2520and%2520testing%2520on%2520the%2520same%2520%257B%255Cdelta%257D-environments.%2520We%2520confirm%2520that%2520this%250Afinding%2520extends%2520beyond%2520noise%2520variations%253A%2520it%2520is%2520possible%2520to%2520showcase%2520the%2520same%250Aphenomenon%2520in%2520ATARI%2520game%2520variations%2520including%2520varying%2520Ghost%2520behaviour%2520in%250APacMan%252C%2520and%2520Paddle%2520behaviour%2520in%2520Pong.%2520We%2520demonstrate%2520this%2520intriguing%2520behaviour%250Aacross%252060%2520different%2520variations%2520of%2520ATARI%2520games%252C%2520including%2520PacMan%252C%2520Pong%252C%2520and%250ABreakout.%2520We%2520refer%2520to%2520this%2520phenomenon%2520as%2520the%2520Indoor-Training%2520Effect.%2520Code%2520to%250Areproduce%2520our%2520experiments%2520and%2520to%2520implement%2520Noise%2520Injection%2520can%2520be%2520found%2520at%250Ahttps%253A//bit.ly/3X6CTYk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Indoor-Training%20Effect%3A%20unexpected%20gains%20from%20distribution%20shifts%20in%0A%20%20the%20transition%20function&entry.906535625=Serena%20Bono%20and%20Spandan%20Madan%20and%20Ishaan%20Grover%20and%20Mao%20Yasueda%20and%20Cynthia%20Breazeal%20and%20Hanspeter%20Pfister%20and%20Gabriel%20Kreiman&entry.1292438233=%20%20Is%20it%20better%20to%20perform%20tennis%20training%20in%20a%20pristine%20indoor%20environment%20or%20a%0Anoisy%20outdoor%20one%3F%20To%20model%20this%20problem%2C%20here%20we%20investigate%20whether%20shifts%20in%0Athe%20transition%20probabilities%20between%20the%20training%20and%20testing%20environments%20in%0Areinforcement%20learning%20problems%20can%20lead%20to%20better%20performance%20under%20certain%0Aconditions.%20We%20generate%20new%20Markov%20Decision%20Processes%20%28MDPs%29%20starting%20from%20a%0Agiven%20MDP%2C%20by%20adding%20quantifiable%2C%20parametric%20noise%20into%20the%20transition%0Afunction.%20We%20refer%20to%20this%20process%20as%20Noise%20Injection%20and%20the%20resulting%0Aenvironments%20as%20%7B%5Cdelta%7D-environments.%20This%20process%20allows%20us%20to%20create%0Avariations%20of%20the%20same%20environment%20with%20quantitative%20control%20over%20noise%20serving%0Aas%20a%20metric%20of%20distance%20between%20environments.%20Conventional%20wisdom%20suggests%20that%0Atraining%20and%20testing%20on%20the%20same%20MDP%20should%20yield%20the%20best%20results.%20In%20stark%0Acontrast%2C%20we%20observe%20that%20agents%20can%20perform%20better%20when%20trained%20on%20the%0Anoise-free%20environment%20and%20tested%20on%20the%20noisy%20%7B%5Cdelta%7D-environments%2C%20compared%0Ato%20training%20and%20testing%20on%20the%20same%20%7B%5Cdelta%7D-environments.%20We%20confirm%20that%20this%0Afinding%20extends%20beyond%20noise%20variations%3A%20it%20is%20possible%20to%20showcase%20the%20same%0Aphenomenon%20in%20ATARI%20game%20variations%20including%20varying%20Ghost%20behaviour%20in%0APacMan%2C%20and%20Paddle%20behaviour%20in%20Pong.%20We%20demonstrate%20this%20intriguing%20behaviour%0Aacross%2060%20different%20variations%20of%20ATARI%20games%2C%20including%20PacMan%2C%20Pong%2C%20and%0ABreakout.%20We%20refer%20to%20this%20phenomenon%20as%20the%20Indoor-Training%20Effect.%20Code%20to%0Areproduce%20our%20experiments%20and%20to%20implement%20Noise%20Injection%20can%20be%20found%20at%0Ahttps%3A//bit.ly/3X6CTYk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15856v2&entry.124074799=Read"},
{"title": "The Harmonic Exponential Filter for Nonparametric Estimation on Motion\n  Groups", "author": "Miguel Saavedra-Ruiz and Steven A. Parkison and Ria Arora and James Richard Forbes and Liam Paull", "abstract": "  Bayesian estimation is a vital tool in robotics as it allows systems to\nupdate the robot state belief using incomplete information from noisy sensors.\nTo render the state estimation problem tractable, many systems assume that the\nmotion and measurement noise, as well as the state distribution, are unimodal\nand Gaussian. However, there are numerous scenarios and systems that do not\ncomply with these assumptions. Existing nonparametric filters that are used to\nmodel multimodal distributions have drawbacks that limit their ability to\nrepresent a diverse set of distributions. This paper introduces a novel\napproach to nonparametric Bayesian filtering on motion groups, designed to\nhandle multimodal distributions using harmonic exponential distributions. This\napproach leverages two key insights of harmonic exponential distributions: a)\nthe product of two distributions can be expressed as the element-wise addition\nof their log-likelihood Fourier coefficients, and b) the convolution of two\ndistributions can be efficiently computed as the tensor product of their\nFourier coefficients. These observations enable the development of an efficient\nand asymptotically exact solution to the Bayes filter up to the band limit of a\nFourier transform. We demonstrate our filter's performance compared with\nestablished nonparametric filtering methods across simulated and real-world\nlocalization tasks.\n", "link": "http://arxiv.org/abs/2408.00907v2", "date": "2025-01-08", "relevancy": 2.021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5026}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Harmonic%20Exponential%20Filter%20for%20Nonparametric%20Estimation%20on%20Motion%0A%20%20Groups&body=Title%3A%20The%20Harmonic%20Exponential%20Filter%20for%20Nonparametric%20Estimation%20on%20Motion%0A%20%20Groups%0AAuthor%3A%20Miguel%20Saavedra-Ruiz%20and%20Steven%20A.%20Parkison%20and%20Ria%20Arora%20and%20James%20Richard%20Forbes%20and%20Liam%20Paull%0AAbstract%3A%20%20%20Bayesian%20estimation%20is%20a%20vital%20tool%20in%20robotics%20as%20it%20allows%20systems%20to%0Aupdate%20the%20robot%20state%20belief%20using%20incomplete%20information%20from%20noisy%20sensors.%0ATo%20render%20the%20state%20estimation%20problem%20tractable%2C%20many%20systems%20assume%20that%20the%0Amotion%20and%20measurement%20noise%2C%20as%20well%20as%20the%20state%20distribution%2C%20are%20unimodal%0Aand%20Gaussian.%20However%2C%20there%20are%20numerous%20scenarios%20and%20systems%20that%20do%20not%0Acomply%20with%20these%20assumptions.%20Existing%20nonparametric%20filters%20that%20are%20used%20to%0Amodel%20multimodal%20distributions%20have%20drawbacks%20that%20limit%20their%20ability%20to%0Arepresent%20a%20diverse%20set%20of%20distributions.%20This%20paper%20introduces%20a%20novel%0Aapproach%20to%20nonparametric%20Bayesian%20filtering%20on%20motion%20groups%2C%20designed%20to%0Ahandle%20multimodal%20distributions%20using%20harmonic%20exponential%20distributions.%20This%0Aapproach%20leverages%20two%20key%20insights%20of%20harmonic%20exponential%20distributions%3A%20a%29%0Athe%20product%20of%20two%20distributions%20can%20be%20expressed%20as%20the%20element-wise%20addition%0Aof%20their%20log-likelihood%20Fourier%20coefficients%2C%20and%20b%29%20the%20convolution%20of%20two%0Adistributions%20can%20be%20efficiently%20computed%20as%20the%20tensor%20product%20of%20their%0AFourier%20coefficients.%20These%20observations%20enable%20the%20development%20of%20an%20efficient%0Aand%20asymptotically%20exact%20solution%20to%20the%20Bayes%20filter%20up%20to%20the%20band%20limit%20of%20a%0AFourier%20transform.%20We%20demonstrate%20our%20filter%27s%20performance%20compared%20with%0Aestablished%20nonparametric%20filtering%20methods%20across%20simulated%20and%20real-world%0Alocalization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00907v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Harmonic%2520Exponential%2520Filter%2520for%2520Nonparametric%2520Estimation%2520on%2520Motion%250A%2520%2520Groups%26entry.906535625%3DMiguel%2520Saavedra-Ruiz%2520and%2520Steven%2520A.%2520Parkison%2520and%2520Ria%2520Arora%2520and%2520James%2520Richard%2520Forbes%2520and%2520Liam%2520Paull%26entry.1292438233%3D%2520%2520Bayesian%2520estimation%2520is%2520a%2520vital%2520tool%2520in%2520robotics%2520as%2520it%2520allows%2520systems%2520to%250Aupdate%2520the%2520robot%2520state%2520belief%2520using%2520incomplete%2520information%2520from%2520noisy%2520sensors.%250ATo%2520render%2520the%2520state%2520estimation%2520problem%2520tractable%252C%2520many%2520systems%2520assume%2520that%2520the%250Amotion%2520and%2520measurement%2520noise%252C%2520as%2520well%2520as%2520the%2520state%2520distribution%252C%2520are%2520unimodal%250Aand%2520Gaussian.%2520However%252C%2520there%2520are%2520numerous%2520scenarios%2520and%2520systems%2520that%2520do%2520not%250Acomply%2520with%2520these%2520assumptions.%2520Existing%2520nonparametric%2520filters%2520that%2520are%2520used%2520to%250Amodel%2520multimodal%2520distributions%2520have%2520drawbacks%2520that%2520limit%2520their%2520ability%2520to%250Arepresent%2520a%2520diverse%2520set%2520of%2520distributions.%2520This%2520paper%2520introduces%2520a%2520novel%250Aapproach%2520to%2520nonparametric%2520Bayesian%2520filtering%2520on%2520motion%2520groups%252C%2520designed%2520to%250Ahandle%2520multimodal%2520distributions%2520using%2520harmonic%2520exponential%2520distributions.%2520This%250Aapproach%2520leverages%2520two%2520key%2520insights%2520of%2520harmonic%2520exponential%2520distributions%253A%2520a%2529%250Athe%2520product%2520of%2520two%2520distributions%2520can%2520be%2520expressed%2520as%2520the%2520element-wise%2520addition%250Aof%2520their%2520log-likelihood%2520Fourier%2520coefficients%252C%2520and%2520b%2529%2520the%2520convolution%2520of%2520two%250Adistributions%2520can%2520be%2520efficiently%2520computed%2520as%2520the%2520tensor%2520product%2520of%2520their%250AFourier%2520coefficients.%2520These%2520observations%2520enable%2520the%2520development%2520of%2520an%2520efficient%250Aand%2520asymptotically%2520exact%2520solution%2520to%2520the%2520Bayes%2520filter%2520up%2520to%2520the%2520band%2520limit%2520of%2520a%250AFourier%2520transform.%2520We%2520demonstrate%2520our%2520filter%2527s%2520performance%2520compared%2520with%250Aestablished%2520nonparametric%2520filtering%2520methods%2520across%2520simulated%2520and%2520real-world%250Alocalization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00907v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Harmonic%20Exponential%20Filter%20for%20Nonparametric%20Estimation%20on%20Motion%0A%20%20Groups&entry.906535625=Miguel%20Saavedra-Ruiz%20and%20Steven%20A.%20Parkison%20and%20Ria%20Arora%20and%20James%20Richard%20Forbes%20and%20Liam%20Paull&entry.1292438233=%20%20Bayesian%20estimation%20is%20a%20vital%20tool%20in%20robotics%20as%20it%20allows%20systems%20to%0Aupdate%20the%20robot%20state%20belief%20using%20incomplete%20information%20from%20noisy%20sensors.%0ATo%20render%20the%20state%20estimation%20problem%20tractable%2C%20many%20systems%20assume%20that%20the%0Amotion%20and%20measurement%20noise%2C%20as%20well%20as%20the%20state%20distribution%2C%20are%20unimodal%0Aand%20Gaussian.%20However%2C%20there%20are%20numerous%20scenarios%20and%20systems%20that%20do%20not%0Acomply%20with%20these%20assumptions.%20Existing%20nonparametric%20filters%20that%20are%20used%20to%0Amodel%20multimodal%20distributions%20have%20drawbacks%20that%20limit%20their%20ability%20to%0Arepresent%20a%20diverse%20set%20of%20distributions.%20This%20paper%20introduces%20a%20novel%0Aapproach%20to%20nonparametric%20Bayesian%20filtering%20on%20motion%20groups%2C%20designed%20to%0Ahandle%20multimodal%20distributions%20using%20harmonic%20exponential%20distributions.%20This%0Aapproach%20leverages%20two%20key%20insights%20of%20harmonic%20exponential%20distributions%3A%20a%29%0Athe%20product%20of%20two%20distributions%20can%20be%20expressed%20as%20the%20element-wise%20addition%0Aof%20their%20log-likelihood%20Fourier%20coefficients%2C%20and%20b%29%20the%20convolution%20of%20two%0Adistributions%20can%20be%20efficiently%20computed%20as%20the%20tensor%20product%20of%20their%0AFourier%20coefficients.%20These%20observations%20enable%20the%20development%20of%20an%20efficient%0Aand%20asymptotically%20exact%20solution%20to%20the%20Bayes%20filter%20up%20to%20the%20band%20limit%20of%20a%0AFourier%20transform.%20We%20demonstrate%20our%20filter%27s%20performance%20compared%20with%0Aestablished%20nonparametric%20filtering%20methods%20across%20simulated%20and%20real-world%0Alocalization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00907v2&entry.124074799=Read"},
{"title": "A Semantic Partitioning Method for Large-Scale Training of Knowledge\n  Graph Embeddings", "author": "Yuhe Bai", "abstract": "  In recent years, knowledge graph embeddings have achieved great success. Many\nmethods have been proposed and achieved state-of-the-art results in various\ntasks. However, most of the current methods present one or more of the\nfollowing problems: (i) They only consider fact triplets, while ignoring the\nontology information of knowledge graphs. (ii) The obtained embeddings do not\ncontain much semantic information. Therefore, using these embeddings for\nsemantic tasks is problematic. (iii) They do not enable large-scale training.\nIn this paper, we propose a new algorithm that incorporates the ontology of\nknowledge graphs and partitions the knowledge graph based on classes to include\nmore semantic information for parallel training of large-scale knowledge graph\nembeddings. Our preliminary results show that our algorithm performs well on\nseveral popular benchmarks.\n", "link": "http://arxiv.org/abs/2501.04613v1", "date": "2025-01-08", "relevancy": 2.0125, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Semantic%20Partitioning%20Method%20for%20Large-Scale%20Training%20of%20Knowledge%0A%20%20Graph%20Embeddings&body=Title%3A%20A%20Semantic%20Partitioning%20Method%20for%20Large-Scale%20Training%20of%20Knowledge%0A%20%20Graph%20Embeddings%0AAuthor%3A%20Yuhe%20Bai%0AAbstract%3A%20%20%20In%20recent%20years%2C%20knowledge%20graph%20embeddings%20have%20achieved%20great%20success.%20Many%0Amethods%20have%20been%20proposed%20and%20achieved%20state-of-the-art%20results%20in%20various%0Atasks.%20However%2C%20most%20of%20the%20current%20methods%20present%20one%20or%20more%20of%20the%0Afollowing%20problems%3A%20%28i%29%20They%20only%20consider%20fact%20triplets%2C%20while%20ignoring%20the%0Aontology%20information%20of%20knowledge%20graphs.%20%28ii%29%20The%20obtained%20embeddings%20do%20not%0Acontain%20much%20semantic%20information.%20Therefore%2C%20using%20these%20embeddings%20for%0Asemantic%20tasks%20is%20problematic.%20%28iii%29%20They%20do%20not%20enable%20large-scale%20training.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20algorithm%20that%20incorporates%20the%20ontology%20of%0Aknowledge%20graphs%20and%20partitions%20the%20knowledge%20graph%20based%20on%20classes%20to%20include%0Amore%20semantic%20information%20for%20parallel%20training%20of%20large-scale%20knowledge%20graph%0Aembeddings.%20Our%20preliminary%20results%20show%20that%20our%20algorithm%20performs%20well%20on%0Aseveral%20popular%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Semantic%2520Partitioning%2520Method%2520for%2520Large-Scale%2520Training%2520of%2520Knowledge%250A%2520%2520Graph%2520Embeddings%26entry.906535625%3DYuhe%2520Bai%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520knowledge%2520graph%2520embeddings%2520have%2520achieved%2520great%2520success.%2520Many%250Amethods%2520have%2520been%2520proposed%2520and%2520achieved%2520state-of-the-art%2520results%2520in%2520various%250Atasks.%2520However%252C%2520most%2520of%2520the%2520current%2520methods%2520present%2520one%2520or%2520more%2520of%2520the%250Afollowing%2520problems%253A%2520%2528i%2529%2520They%2520only%2520consider%2520fact%2520triplets%252C%2520while%2520ignoring%2520the%250Aontology%2520information%2520of%2520knowledge%2520graphs.%2520%2528ii%2529%2520The%2520obtained%2520embeddings%2520do%2520not%250Acontain%2520much%2520semantic%2520information.%2520Therefore%252C%2520using%2520these%2520embeddings%2520for%250Asemantic%2520tasks%2520is%2520problematic.%2520%2528iii%2529%2520They%2520do%2520not%2520enable%2520large-scale%2520training.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520algorithm%2520that%2520incorporates%2520the%2520ontology%2520of%250Aknowledge%2520graphs%2520and%2520partitions%2520the%2520knowledge%2520graph%2520based%2520on%2520classes%2520to%2520include%250Amore%2520semantic%2520information%2520for%2520parallel%2520training%2520of%2520large-scale%2520knowledge%2520graph%250Aembeddings.%2520Our%2520preliminary%2520results%2520show%2520that%2520our%2520algorithm%2520performs%2520well%2520on%250Aseveral%2520popular%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semantic%20Partitioning%20Method%20for%20Large-Scale%20Training%20of%20Knowledge%0A%20%20Graph%20Embeddings&entry.906535625=Yuhe%20Bai&entry.1292438233=%20%20In%20recent%20years%2C%20knowledge%20graph%20embeddings%20have%20achieved%20great%20success.%20Many%0Amethods%20have%20been%20proposed%20and%20achieved%20state-of-the-art%20results%20in%20various%0Atasks.%20However%2C%20most%20of%20the%20current%20methods%20present%20one%20or%20more%20of%20the%0Afollowing%20problems%3A%20%28i%29%20They%20only%20consider%20fact%20triplets%2C%20while%20ignoring%20the%0Aontology%20information%20of%20knowledge%20graphs.%20%28ii%29%20The%20obtained%20embeddings%20do%20not%0Acontain%20much%20semantic%20information.%20Therefore%2C%20using%20these%20embeddings%20for%0Asemantic%20tasks%20is%20problematic.%20%28iii%29%20They%20do%20not%20enable%20large-scale%20training.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20algorithm%20that%20incorporates%20the%20ontology%20of%0Aknowledge%20graphs%20and%20partitions%20the%20knowledge%20graph%20based%20on%20classes%20to%20include%0Amore%20semantic%20information%20for%20parallel%20training%20of%20large-scale%20knowledge%20graph%0Aembeddings.%20Our%20preliminary%20results%20show%20that%20our%20algorithm%20performs%20well%20on%0Aseveral%20popular%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04613v1&entry.124074799=Read"},
{"title": "ConceptMaster: Multi-Concept Video Customization on Diffusion\n  Transformer Models Without Test-Time Tuning", "author": "Yuzhou Huang and Ziyang Yuan and Quande Liu and Qiulin Wang and Xintao Wang and Ruimao Zhang and Pengfei Wan and Di Zhang and Kun Gai", "abstract": "  Text-to-video generation has made remarkable advancements through diffusion\nmodels. However, Multi-Concept Video Customization (MCVC) remains a significant\nchallenge. We identify two key challenges in this task: 1) the identity\ndecoupling problem, where directly adopting existing customization methods\ninevitably mix attributes when handling multiple concepts simultaneously, and\n2) the scarcity of high-quality video-entity pairs, which is crucial for\ntraining such a model that represents and decouples various concepts well. To\naddress these challenges, we introduce ConceptMaster, an innovative framework\nthat effectively tackles the critical issues of identity decoupling while\nmaintaining concept fidelity in customized videos. Specifically, we introduce a\nnovel strategy of learning decoupled multi-concept embeddings that are injected\ninto the diffusion models in a standalone manner, which effectively guarantees\nthe quality of customized videos with multiple identities, even for highly\nsimilar visual concepts. To further overcome the scarcity of high-quality MCVC\ndata, we carefully establish a data construction pipeline, which enables\nsystematic collection of precise multi-concept video-entity data across diverse\nconcepts. A comprehensive benchmark is designed to validate the effectiveness\nof our model from three critical dimensions: concept fidelity, identity\ndecoupling ability, and video generation quality across six different concept\ncomposition scenarios. Extensive experiments demonstrate that our ConceptMaster\nsignificantly outperforms previous approaches for this task, paving the way for\ngenerating personalized and semantically accurate videos across multiple\nconcepts.\n", "link": "http://arxiv.org/abs/2501.04698v1", "date": "2025-01-08", "relevancy": 1.9994, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7025}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6883}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConceptMaster%3A%20Multi-Concept%20Video%20Customization%20on%20Diffusion%0A%20%20Transformer%20Models%20Without%20Test-Time%20Tuning&body=Title%3A%20ConceptMaster%3A%20Multi-Concept%20Video%20Customization%20on%20Diffusion%0A%20%20Transformer%20Models%20Without%20Test-Time%20Tuning%0AAuthor%3A%20Yuzhou%20Huang%20and%20Ziyang%20Yuan%20and%20Quande%20Liu%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Ruimao%20Zhang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Text-to-video%20generation%20has%20made%20remarkable%20advancements%20through%20diffusion%0Amodels.%20However%2C%20Multi-Concept%20Video%20Customization%20%28MCVC%29%20remains%20a%20significant%0Achallenge.%20We%20identify%20two%20key%20challenges%20in%20this%20task%3A%201%29%20the%20identity%0Adecoupling%20problem%2C%20where%20directly%20adopting%20existing%20customization%20methods%0Ainevitably%20mix%20attributes%20when%20handling%20multiple%20concepts%20simultaneously%2C%20and%0A2%29%20the%20scarcity%20of%20high-quality%20video-entity%20pairs%2C%20which%20is%20crucial%20for%0Atraining%20such%20a%20model%20that%20represents%20and%20decouples%20various%20concepts%20well.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20ConceptMaster%2C%20an%20innovative%20framework%0Athat%20effectively%20tackles%20the%20critical%20issues%20of%20identity%20decoupling%20while%0Amaintaining%20concept%20fidelity%20in%20customized%20videos.%20Specifically%2C%20we%20introduce%20a%0Anovel%20strategy%20of%20learning%20decoupled%20multi-concept%20embeddings%20that%20are%20injected%0Ainto%20the%20diffusion%20models%20in%20a%20standalone%20manner%2C%20which%20effectively%20guarantees%0Athe%20quality%20of%20customized%20videos%20with%20multiple%20identities%2C%20even%20for%20highly%0Asimilar%20visual%20concepts.%20To%20further%20overcome%20the%20scarcity%20of%20high-quality%20MCVC%0Adata%2C%20we%20carefully%20establish%20a%20data%20construction%20pipeline%2C%20which%20enables%0Asystematic%20collection%20of%20precise%20multi-concept%20video-entity%20data%20across%20diverse%0Aconcepts.%20A%20comprehensive%20benchmark%20is%20designed%20to%20validate%20the%20effectiveness%0Aof%20our%20model%20from%20three%20critical%20dimensions%3A%20concept%20fidelity%2C%20identity%0Adecoupling%20ability%2C%20and%20video%20generation%20quality%20across%20six%20different%20concept%0Acomposition%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20our%20ConceptMaster%0Asignificantly%20outperforms%20previous%20approaches%20for%20this%20task%2C%20paving%20the%20way%20for%0Agenerating%20personalized%20and%20semantically%20accurate%20videos%20across%20multiple%0Aconcepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptMaster%253A%2520Multi-Concept%2520Video%2520Customization%2520on%2520Diffusion%250A%2520%2520Transformer%2520Models%2520Without%2520Test-Time%2520Tuning%26entry.906535625%3DYuzhou%2520Huang%2520and%2520Ziyang%2520Yuan%2520and%2520Quande%2520Liu%2520and%2520Qiulin%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Ruimao%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520Text-to-video%2520generation%2520has%2520made%2520remarkable%2520advancements%2520through%2520diffusion%250Amodels.%2520However%252C%2520Multi-Concept%2520Video%2520Customization%2520%2528MCVC%2529%2520remains%2520a%2520significant%250Achallenge.%2520We%2520identify%2520two%2520key%2520challenges%2520in%2520this%2520task%253A%25201%2529%2520the%2520identity%250Adecoupling%2520problem%252C%2520where%2520directly%2520adopting%2520existing%2520customization%2520methods%250Ainevitably%2520mix%2520attributes%2520when%2520handling%2520multiple%2520concepts%2520simultaneously%252C%2520and%250A2%2529%2520the%2520scarcity%2520of%2520high-quality%2520video-entity%2520pairs%252C%2520which%2520is%2520crucial%2520for%250Atraining%2520such%2520a%2520model%2520that%2520represents%2520and%2520decouples%2520various%2520concepts%2520well.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520ConceptMaster%252C%2520an%2520innovative%2520framework%250Athat%2520effectively%2520tackles%2520the%2520critical%2520issues%2520of%2520identity%2520decoupling%2520while%250Amaintaining%2520concept%2520fidelity%2520in%2520customized%2520videos.%2520Specifically%252C%2520we%2520introduce%2520a%250Anovel%2520strategy%2520of%2520learning%2520decoupled%2520multi-concept%2520embeddings%2520that%2520are%2520injected%250Ainto%2520the%2520diffusion%2520models%2520in%2520a%2520standalone%2520manner%252C%2520which%2520effectively%2520guarantees%250Athe%2520quality%2520of%2520customized%2520videos%2520with%2520multiple%2520identities%252C%2520even%2520for%2520highly%250Asimilar%2520visual%2520concepts.%2520To%2520further%2520overcome%2520the%2520scarcity%2520of%2520high-quality%2520MCVC%250Adata%252C%2520we%2520carefully%2520establish%2520a%2520data%2520construction%2520pipeline%252C%2520which%2520enables%250Asystematic%2520collection%2520of%2520precise%2520multi-concept%2520video-entity%2520data%2520across%2520diverse%250Aconcepts.%2520A%2520comprehensive%2520benchmark%2520is%2520designed%2520to%2520validate%2520the%2520effectiveness%250Aof%2520our%2520model%2520from%2520three%2520critical%2520dimensions%253A%2520concept%2520fidelity%252C%2520identity%250Adecoupling%2520ability%252C%2520and%2520video%2520generation%2520quality%2520across%2520six%2520different%2520concept%250Acomposition%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520ConceptMaster%250Asignificantly%2520outperforms%2520previous%2520approaches%2520for%2520this%2520task%252C%2520paving%2520the%2520way%2520for%250Agenerating%2520personalized%2520and%2520semantically%2520accurate%2520videos%2520across%2520multiple%250Aconcepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConceptMaster%3A%20Multi-Concept%20Video%20Customization%20on%20Diffusion%0A%20%20Transformer%20Models%20Without%20Test-Time%20Tuning&entry.906535625=Yuzhou%20Huang%20and%20Ziyang%20Yuan%20and%20Quande%20Liu%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Ruimao%20Zhang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Kun%20Gai&entry.1292438233=%20%20Text-to-video%20generation%20has%20made%20remarkable%20advancements%20through%20diffusion%0Amodels.%20However%2C%20Multi-Concept%20Video%20Customization%20%28MCVC%29%20remains%20a%20significant%0Achallenge.%20We%20identify%20two%20key%20challenges%20in%20this%20task%3A%201%29%20the%20identity%0Adecoupling%20problem%2C%20where%20directly%20adopting%20existing%20customization%20methods%0Ainevitably%20mix%20attributes%20when%20handling%20multiple%20concepts%20simultaneously%2C%20and%0A2%29%20the%20scarcity%20of%20high-quality%20video-entity%20pairs%2C%20which%20is%20crucial%20for%0Atraining%20such%20a%20model%20that%20represents%20and%20decouples%20various%20concepts%20well.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20ConceptMaster%2C%20an%20innovative%20framework%0Athat%20effectively%20tackles%20the%20critical%20issues%20of%20identity%20decoupling%20while%0Amaintaining%20concept%20fidelity%20in%20customized%20videos.%20Specifically%2C%20we%20introduce%20a%0Anovel%20strategy%20of%20learning%20decoupled%20multi-concept%20embeddings%20that%20are%20injected%0Ainto%20the%20diffusion%20models%20in%20a%20standalone%20manner%2C%20which%20effectively%20guarantees%0Athe%20quality%20of%20customized%20videos%20with%20multiple%20identities%2C%20even%20for%20highly%0Asimilar%20visual%20concepts.%20To%20further%20overcome%20the%20scarcity%20of%20high-quality%20MCVC%0Adata%2C%20we%20carefully%20establish%20a%20data%20construction%20pipeline%2C%20which%20enables%0Asystematic%20collection%20of%20precise%20multi-concept%20video-entity%20data%20across%20diverse%0Aconcepts.%20A%20comprehensive%20benchmark%20is%20designed%20to%20validate%20the%20effectiveness%0Aof%20our%20model%20from%20three%20critical%20dimensions%3A%20concept%20fidelity%2C%20identity%0Adecoupling%20ability%2C%20and%20video%20generation%20quality%20across%20six%20different%20concept%0Acomposition%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20our%20ConceptMaster%0Asignificantly%20outperforms%20previous%20approaches%20for%20this%20task%2C%20paving%20the%20way%20for%0Agenerating%20personalized%20and%20semantically%20accurate%20videos%20across%20multiple%0Aconcepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04698v1&entry.124074799=Read"},
{"title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research\n  Directions", "author": "Na Yan and Yang Su and Yansha Deng and Robert Schober", "abstract": "  Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings.\n", "link": "http://arxiv.org/abs/2501.04436v1", "date": "2025-01-08", "relevancy": 1.9894, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Fine-Tuning%20of%20LLMs%3A%20Framework%20Comparison%20and%20Research%0A%20%20Directions&body=Title%3A%20Federated%20Fine-Tuning%20of%20LLMs%3A%20Framework%20Comparison%20and%20Research%0A%20%20Directions%0AAuthor%3A%20Na%20Yan%20and%20Yang%20Su%20and%20Yansha%20Deng%20and%20Robert%20Schober%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20provides%20a%20privacy-preserving%20solution%20for%0Afine-tuning%20pre-trained%20large%20language%20models%20%28LLMs%29%20using%20distributed%20private%0Adatasets%2C%20enabling%20task-specific%20adaptation%20while%20preserving%20data%20privacy.%0AHowever%2C%20fine-tuning%20the%20extensive%20parameters%20in%20LLMs%20is%20particularly%0Achallenging%20in%20resource-constrained%20federated%20scenarios%20due%20to%20the%20significant%0Acommunication%20and%20computational%20costs.%20To%20gain%20a%20deeper%20understanding%20of%20how%0Athese%20challenges%20can%20be%20addressed%2C%20this%20article%20conducts%20a%20comparative%20analysis%0Athree%20advanced%20federated%20LLM%20%28FedLLM%29%20frameworks%20that%20integrate%20knowledge%0Adistillation%20%28KD%29%20and%20split%20learning%20%28SL%29%20to%20mitigate%20these%20issues%3A%201%29%20FedLLMs%2C%0Awhere%20clients%20upload%20model%20parameters%20or%20gradients%20to%20enable%20straightforward%0Aand%20effective%20fine-tuning%3B%202%29%20KD-FedLLMs%2C%20which%20leverage%20KD%20for%20efficient%0Aknowledge%20sharing%20via%20logits%3B%20and%203%29%20Split-FedLLMs%2C%20which%20split%20the%20LLMs%20into%0Atwo%20parts%2C%20with%20one%20part%20executed%20on%20the%20client%20and%20the%20other%20one%20on%20the%0Aserver%2C%20to%20balance%20the%20computational%20load.%20Each%20framework%20is%20evaluated%20based%20on%0Akey%20performance%20metrics%2C%20including%20model%20accuracy%2C%20communication%20overhead%2C%20and%0Aclient-side%20computational%20load%2C%20offering%20insights%20into%20their%20effectiveness%20for%0Avarious%20federated%20fine-tuning%20scenarios.%20Through%20this%20analysis%2C%20we%20identify%0Aframework-specific%20optimization%20opportunities%20to%20enhance%20the%20efficiency%20of%0AFedLLMs%20and%20discuss%20broader%20research%20directions%2C%20highlighting%20open%0Aopportunities%20to%20better%20adapt%20FedLLMs%20for%20real-world%20applications.%20A%20use%20case%0Ais%20presented%20to%20demonstrate%20the%20performance%20comparison%20of%20these%20three%0Aframeworks%20under%20varying%20configurations%20and%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Fine-Tuning%2520of%2520LLMs%253A%2520Framework%2520Comparison%2520and%2520Research%250A%2520%2520Directions%26entry.906535625%3DNa%2520Yan%2520and%2520Yang%2520Su%2520and%2520Yansha%2520Deng%2520and%2520Robert%2520Schober%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520provides%2520a%2520privacy-preserving%2520solution%2520for%250Afine-tuning%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520using%2520distributed%2520private%250Adatasets%252C%2520enabling%2520task-specific%2520adaptation%2520while%2520preserving%2520data%2520privacy.%250AHowever%252C%2520fine-tuning%2520the%2520extensive%2520parameters%2520in%2520LLMs%2520is%2520particularly%250Achallenging%2520in%2520resource-constrained%2520federated%2520scenarios%2520due%2520to%2520the%2520significant%250Acommunication%2520and%2520computational%2520costs.%2520To%2520gain%2520a%2520deeper%2520understanding%2520of%2520how%250Athese%2520challenges%2520can%2520be%2520addressed%252C%2520this%2520article%2520conducts%2520a%2520comparative%2520analysis%250Athree%2520advanced%2520federated%2520LLM%2520%2528FedLLM%2529%2520frameworks%2520that%2520integrate%2520knowledge%250Adistillation%2520%2528KD%2529%2520and%2520split%2520learning%2520%2528SL%2529%2520to%2520mitigate%2520these%2520issues%253A%25201%2529%2520FedLLMs%252C%250Awhere%2520clients%2520upload%2520model%2520parameters%2520or%2520gradients%2520to%2520enable%2520straightforward%250Aand%2520effective%2520fine-tuning%253B%25202%2529%2520KD-FedLLMs%252C%2520which%2520leverage%2520KD%2520for%2520efficient%250Aknowledge%2520sharing%2520via%2520logits%253B%2520and%25203%2529%2520Split-FedLLMs%252C%2520which%2520split%2520the%2520LLMs%2520into%250Atwo%2520parts%252C%2520with%2520one%2520part%2520executed%2520on%2520the%2520client%2520and%2520the%2520other%2520one%2520on%2520the%250Aserver%252C%2520to%2520balance%2520the%2520computational%2520load.%2520Each%2520framework%2520is%2520evaluated%2520based%2520on%250Akey%2520performance%2520metrics%252C%2520including%2520model%2520accuracy%252C%2520communication%2520overhead%252C%2520and%250Aclient-side%2520computational%2520load%252C%2520offering%2520insights%2520into%2520their%2520effectiveness%2520for%250Avarious%2520federated%2520fine-tuning%2520scenarios.%2520Through%2520this%2520analysis%252C%2520we%2520identify%250Aframework-specific%2520optimization%2520opportunities%2520to%2520enhance%2520the%2520efficiency%2520of%250AFedLLMs%2520and%2520discuss%2520broader%2520research%2520directions%252C%2520highlighting%2520open%250Aopportunities%2520to%2520better%2520adapt%2520FedLLMs%2520for%2520real-world%2520applications.%2520A%2520use%2520case%250Ais%2520presented%2520to%2520demonstrate%2520the%2520performance%2520comparison%2520of%2520these%2520three%250Aframeworks%2520under%2520varying%2520configurations%2520and%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Fine-Tuning%20of%20LLMs%3A%20Framework%20Comparison%20and%20Research%0A%20%20Directions&entry.906535625=Na%20Yan%20and%20Yang%20Su%20and%20Yansha%20Deng%20and%20Robert%20Schober&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20provides%20a%20privacy-preserving%20solution%20for%0Afine-tuning%20pre-trained%20large%20language%20models%20%28LLMs%29%20using%20distributed%20private%0Adatasets%2C%20enabling%20task-specific%20adaptation%20while%20preserving%20data%20privacy.%0AHowever%2C%20fine-tuning%20the%20extensive%20parameters%20in%20LLMs%20is%20particularly%0Achallenging%20in%20resource-constrained%20federated%20scenarios%20due%20to%20the%20significant%0Acommunication%20and%20computational%20costs.%20To%20gain%20a%20deeper%20understanding%20of%20how%0Athese%20challenges%20can%20be%20addressed%2C%20this%20article%20conducts%20a%20comparative%20analysis%0Athree%20advanced%20federated%20LLM%20%28FedLLM%29%20frameworks%20that%20integrate%20knowledge%0Adistillation%20%28KD%29%20and%20split%20learning%20%28SL%29%20to%20mitigate%20these%20issues%3A%201%29%20FedLLMs%2C%0Awhere%20clients%20upload%20model%20parameters%20or%20gradients%20to%20enable%20straightforward%0Aand%20effective%20fine-tuning%3B%202%29%20KD-FedLLMs%2C%20which%20leverage%20KD%20for%20efficient%0Aknowledge%20sharing%20via%20logits%3B%20and%203%29%20Split-FedLLMs%2C%20which%20split%20the%20LLMs%20into%0Atwo%20parts%2C%20with%20one%20part%20executed%20on%20the%20client%20and%20the%20other%20one%20on%20the%0Aserver%2C%20to%20balance%20the%20computational%20load.%20Each%20framework%20is%20evaluated%20based%20on%0Akey%20performance%20metrics%2C%20including%20model%20accuracy%2C%20communication%20overhead%2C%20and%0Aclient-side%20computational%20load%2C%20offering%20insights%20into%20their%20effectiveness%20for%0Avarious%20federated%20fine-tuning%20scenarios.%20Through%20this%20analysis%2C%20we%20identify%0Aframework-specific%20optimization%20opportunities%20to%20enhance%20the%20efficiency%20of%0AFedLLMs%20and%20discuss%20broader%20research%20directions%2C%20highlighting%20open%0Aopportunities%20to%20better%20adapt%20FedLLMs%20for%20real-world%20applications.%20A%20use%20case%0Ais%20presented%20to%20demonstrate%20the%20performance%20comparison%20of%20these%20three%0Aframeworks%20under%20varying%20configurations%20and%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04436v1&entry.124074799=Read"},
{"title": "Efficient Video-Based ALPR System Using YOLO and Visual Rhythm", "author": "Victor Nascimento Ribeiro and Nina S. T. Hirata", "abstract": "  Automatic License Plate Recognition (ALPR) involves extracting vehicle\nlicense plate information from image or a video capture. These systems have\ngained popularity due to the wide availability of low-cost surveillance cameras\nand advances in Deep Learning. Typically, video-based ALPR systems rely on\nmultiple frames to detect the vehicle and recognize the license plates.\nTherefore, we propose a system capable of extracting exactly one frame per\nvehicle and recognizing its license plate characters from this singular image\nusing an Optical Character Recognition (OCR) model. Early experiments show that\nthis methodology is viable.\n", "link": "http://arxiv.org/abs/2501.02270v2", "date": "2025-01-08", "relevancy": 1.987, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5091}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4955}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Video-Based%20ALPR%20System%20Using%20YOLO%20and%20Visual%20Rhythm&body=Title%3A%20Efficient%20Video-Based%20ALPR%20System%20Using%20YOLO%20and%20Visual%20Rhythm%0AAuthor%3A%20Victor%20Nascimento%20Ribeiro%20and%20Nina%20S.%20T.%20Hirata%0AAbstract%3A%20%20%20Automatic%20License%20Plate%20Recognition%20%28ALPR%29%20involves%20extracting%20vehicle%0Alicense%20plate%20information%20from%20image%20or%20a%20video%20capture.%20These%20systems%20have%0Agained%20popularity%20due%20to%20the%20wide%20availability%20of%20low-cost%20surveillance%20cameras%0Aand%20advances%20in%20Deep%20Learning.%20Typically%2C%20video-based%20ALPR%20systems%20rely%20on%0Amultiple%20frames%20to%20detect%20the%20vehicle%20and%20recognize%20the%20license%20plates.%0ATherefore%2C%20we%20propose%20a%20system%20capable%20of%20extracting%20exactly%20one%20frame%20per%0Avehicle%20and%20recognizing%20its%20license%20plate%20characters%20from%20this%20singular%20image%0Ausing%20an%20Optical%20Character%20Recognition%20%28OCR%29%20model.%20Early%20experiments%20show%20that%0Athis%20methodology%20is%20viable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Video-Based%2520ALPR%2520System%2520Using%2520YOLO%2520and%2520Visual%2520Rhythm%26entry.906535625%3DVictor%2520Nascimento%2520Ribeiro%2520and%2520Nina%2520S.%2520T.%2520Hirata%26entry.1292438233%3D%2520%2520Automatic%2520License%2520Plate%2520Recognition%2520%2528ALPR%2529%2520involves%2520extracting%2520vehicle%250Alicense%2520plate%2520information%2520from%2520image%2520or%2520a%2520video%2520capture.%2520These%2520systems%2520have%250Agained%2520popularity%2520due%2520to%2520the%2520wide%2520availability%2520of%2520low-cost%2520surveillance%2520cameras%250Aand%2520advances%2520in%2520Deep%2520Learning.%2520Typically%252C%2520video-based%2520ALPR%2520systems%2520rely%2520on%250Amultiple%2520frames%2520to%2520detect%2520the%2520vehicle%2520and%2520recognize%2520the%2520license%2520plates.%250ATherefore%252C%2520we%2520propose%2520a%2520system%2520capable%2520of%2520extracting%2520exactly%2520one%2520frame%2520per%250Avehicle%2520and%2520recognizing%2520its%2520license%2520plate%2520characters%2520from%2520this%2520singular%2520image%250Ausing%2520an%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520model.%2520Early%2520experiments%2520show%2520that%250Athis%2520methodology%2520is%2520viable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Video-Based%20ALPR%20System%20Using%20YOLO%20and%20Visual%20Rhythm&entry.906535625=Victor%20Nascimento%20Ribeiro%20and%20Nina%20S.%20T.%20Hirata&entry.1292438233=%20%20Automatic%20License%20Plate%20Recognition%20%28ALPR%29%20involves%20extracting%20vehicle%0Alicense%20plate%20information%20from%20image%20or%20a%20video%20capture.%20These%20systems%20have%0Agained%20popularity%20due%20to%20the%20wide%20availability%20of%20low-cost%20surveillance%20cameras%0Aand%20advances%20in%20Deep%20Learning.%20Typically%2C%20video-based%20ALPR%20systems%20rely%20on%0Amultiple%20frames%20to%20detect%20the%20vehicle%20and%20recognize%20the%20license%20plates.%0ATherefore%2C%20we%20propose%20a%20system%20capable%20of%20extracting%20exactly%20one%20frame%20per%0Avehicle%20and%20recognizing%20its%20license%20plate%20characters%20from%20this%20singular%20image%0Ausing%20an%20Optical%20Character%20Recognition%20%28OCR%29%20model.%20Early%20experiments%20show%20that%0Athis%20methodology%20is%20viable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02270v2&entry.124074799=Read"},
{"title": "Forget Vectors at Play: Universal Input Perturbations Driving Machine\n  Unlearning in Image Classification", "author": "Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and Jiancheng Liu and Gaowen Liu and Sijia Liu and Yan Yan", "abstract": "  Machine unlearning (MU), which seeks to erase the influence of specific\nunwanted data from already-trained models, is becoming increasingly vital in\nmodel editing, particularly to comply with evolving data regulations like the\n``right to be forgotten''. Conventional approaches are predominantly\nmodel-based, typically requiring retraining or fine-tuning the model's weights\nto meet unlearning requirements. In this work, we approach the MU problem from\na novel input perturbation-based perspective, where the model weights remain\nintact throughout the unlearning process. We demonstrate the existence of a\nproactive input-based unlearning strategy, referred to forget vector, which can\nbe generated as an input-agnostic data perturbation and remains as effective as\nmodel-based approximate unlearning approaches. We also explore forget vector\narithmetic, whereby multiple class-specific forget vectors are combined through\nsimple operations (e.g., linear combinations) to generate new forget vectors\nfor unseen unlearning tasks, such as forgetting arbitrary subsets across\nclasses. Extensive experiments validate the effectiveness and adaptability of\nthe forget vector, showcasing its competitive performance relative to\nstate-of-the-art model-based methods. Codes are available at\nhttps://github.com/Changchangsun/Forget-Vector.\n", "link": "http://arxiv.org/abs/2412.16780v2", "date": "2025-01-08", "relevancy": 1.9853, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4979}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20Vectors%20at%20Play%3A%20Universal%20Input%20Perturbations%20Driving%20Machine%0A%20%20Unlearning%20in%20Image%20Classification&body=Title%3A%20Forget%20Vectors%20at%20Play%3A%20Universal%20Input%20Perturbations%20Driving%20Machine%0A%20%20Unlearning%20in%20Image%20Classification%0AAuthor%3A%20Changchang%20Sun%20and%20Ren%20Wang%20and%20Yihua%20Zhang%20and%20Jinghan%20Jia%20and%20Jiancheng%20Liu%20and%20Gaowen%20Liu%20and%20Sijia%20Liu%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Machine%20unlearning%20%28MU%29%2C%20which%20seeks%20to%20erase%20the%20influence%20of%20specific%0Aunwanted%20data%20from%20already-trained%20models%2C%20is%20becoming%20increasingly%20vital%20in%0Amodel%20editing%2C%20particularly%20to%20comply%20with%20evolving%20data%20regulations%20like%20the%0A%60%60right%20to%20be%20forgotten%27%27.%20Conventional%20approaches%20are%20predominantly%0Amodel-based%2C%20typically%20requiring%20retraining%20or%20fine-tuning%20the%20model%27s%20weights%0Ato%20meet%20unlearning%20requirements.%20In%20this%20work%2C%20we%20approach%20the%20MU%20problem%20from%0Aa%20novel%20input%20perturbation-based%20perspective%2C%20where%20the%20model%20weights%20remain%0Aintact%20throughout%20the%20unlearning%20process.%20We%20demonstrate%20the%20existence%20of%20a%0Aproactive%20input-based%20unlearning%20strategy%2C%20referred%20to%20forget%20vector%2C%20which%20can%0Abe%20generated%20as%20an%20input-agnostic%20data%20perturbation%20and%20remains%20as%20effective%20as%0Amodel-based%20approximate%20unlearning%20approaches.%20We%20also%20explore%20forget%20vector%0Aarithmetic%2C%20whereby%20multiple%20class-specific%20forget%20vectors%20are%20combined%20through%0Asimple%20operations%20%28e.g.%2C%20linear%20combinations%29%20to%20generate%20new%20forget%20vectors%0Afor%20unseen%20unlearning%20tasks%2C%20such%20as%20forgetting%20arbitrary%20subsets%20across%0Aclasses.%20Extensive%20experiments%20validate%20the%20effectiveness%20and%20adaptability%20of%0Athe%20forget%20vector%2C%20showcasing%20its%20competitive%20performance%20relative%20to%0Astate-of-the-art%20model-based%20methods.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Changchangsun/Forget-Vector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520Vectors%2520at%2520Play%253A%2520Universal%2520Input%2520Perturbations%2520Driving%2520Machine%250A%2520%2520Unlearning%2520in%2520Image%2520Classification%26entry.906535625%3DChangchang%2520Sun%2520and%2520Ren%2520Wang%2520and%2520Yihua%2520Zhang%2520and%2520Jinghan%2520Jia%2520and%2520Jiancheng%2520Liu%2520and%2520Gaowen%2520Liu%2520and%2520Sijia%2520Liu%2520and%2520Yan%2520Yan%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520%2528MU%2529%252C%2520which%2520seeks%2520to%2520erase%2520the%2520influence%2520of%2520specific%250Aunwanted%2520data%2520from%2520already-trained%2520models%252C%2520is%2520becoming%2520increasingly%2520vital%2520in%250Amodel%2520editing%252C%2520particularly%2520to%2520comply%2520with%2520evolving%2520data%2520regulations%2520like%2520the%250A%2560%2560right%2520to%2520be%2520forgotten%2527%2527.%2520Conventional%2520approaches%2520are%2520predominantly%250Amodel-based%252C%2520typically%2520requiring%2520retraining%2520or%2520fine-tuning%2520the%2520model%2527s%2520weights%250Ato%2520meet%2520unlearning%2520requirements.%2520In%2520this%2520work%252C%2520we%2520approach%2520the%2520MU%2520problem%2520from%250Aa%2520novel%2520input%2520perturbation-based%2520perspective%252C%2520where%2520the%2520model%2520weights%2520remain%250Aintact%2520throughout%2520the%2520unlearning%2520process.%2520We%2520demonstrate%2520the%2520existence%2520of%2520a%250Aproactive%2520input-based%2520unlearning%2520strategy%252C%2520referred%2520to%2520forget%2520vector%252C%2520which%2520can%250Abe%2520generated%2520as%2520an%2520input-agnostic%2520data%2520perturbation%2520and%2520remains%2520as%2520effective%2520as%250Amodel-based%2520approximate%2520unlearning%2520approaches.%2520We%2520also%2520explore%2520forget%2520vector%250Aarithmetic%252C%2520whereby%2520multiple%2520class-specific%2520forget%2520vectors%2520are%2520combined%2520through%250Asimple%2520operations%2520%2528e.g.%252C%2520linear%2520combinations%2529%2520to%2520generate%2520new%2520forget%2520vectors%250Afor%2520unseen%2520unlearning%2520tasks%252C%2520such%2520as%2520forgetting%2520arbitrary%2520subsets%2520across%250Aclasses.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520and%2520adaptability%2520of%250Athe%2520forget%2520vector%252C%2520showcasing%2520its%2520competitive%2520performance%2520relative%2520to%250Astate-of-the-art%2520model-based%2520methods.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/Changchangsun/Forget-Vector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20Vectors%20at%20Play%3A%20Universal%20Input%20Perturbations%20Driving%20Machine%0A%20%20Unlearning%20in%20Image%20Classification&entry.906535625=Changchang%20Sun%20and%20Ren%20Wang%20and%20Yihua%20Zhang%20and%20Jinghan%20Jia%20and%20Jiancheng%20Liu%20and%20Gaowen%20Liu%20and%20Sijia%20Liu%20and%20Yan%20Yan&entry.1292438233=%20%20Machine%20unlearning%20%28MU%29%2C%20which%20seeks%20to%20erase%20the%20influence%20of%20specific%0Aunwanted%20data%20from%20already-trained%20models%2C%20is%20becoming%20increasingly%20vital%20in%0Amodel%20editing%2C%20particularly%20to%20comply%20with%20evolving%20data%20regulations%20like%20the%0A%60%60right%20to%20be%20forgotten%27%27.%20Conventional%20approaches%20are%20predominantly%0Amodel-based%2C%20typically%20requiring%20retraining%20or%20fine-tuning%20the%20model%27s%20weights%0Ato%20meet%20unlearning%20requirements.%20In%20this%20work%2C%20we%20approach%20the%20MU%20problem%20from%0Aa%20novel%20input%20perturbation-based%20perspective%2C%20where%20the%20model%20weights%20remain%0Aintact%20throughout%20the%20unlearning%20process.%20We%20demonstrate%20the%20existence%20of%20a%0Aproactive%20input-based%20unlearning%20strategy%2C%20referred%20to%20forget%20vector%2C%20which%20can%0Abe%20generated%20as%20an%20input-agnostic%20data%20perturbation%20and%20remains%20as%20effective%20as%0Amodel-based%20approximate%20unlearning%20approaches.%20We%20also%20explore%20forget%20vector%0Aarithmetic%2C%20whereby%20multiple%20class-specific%20forget%20vectors%20are%20combined%20through%0Asimple%20operations%20%28e.g.%2C%20linear%20combinations%29%20to%20generate%20new%20forget%20vectors%0Afor%20unseen%20unlearning%20tasks%2C%20such%20as%20forgetting%20arbitrary%20subsets%20across%0Aclasses.%20Extensive%20experiments%20validate%20the%20effectiveness%20and%20adaptability%20of%0Athe%20forget%20vector%2C%20showcasing%20its%20competitive%20performance%20relative%20to%0Astate-of-the-art%20model-based%20methods.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Changchangsun/Forget-Vector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16780v2&entry.124074799=Read"},
{"title": "HyFusion: Enhanced Reception Field Transformer for Hyperspectral Image\n  Fusion", "author": "Chia-Ming Lee and Yu-Fan Lin and Yu-Hao Ho and Li-Wei Kang and Chih-Chung Hsu", "abstract": "  Hyperspectral image (HSI) fusion addresses the challenge of reconstructing\nHigh-Resolution HSIs (HR-HSIs) from High-Resolution Multispectral images\n(HR-MSIs) and Low-Resolution HSIs (LR-HSIs), a critical task given the high\ncosts and hardware limitations associated with acquiring high-quality HSIs.\nWhile existing methods leverage spatial and spectral relationships, they often\nsuffer from limited receptive fields and insufficient feature utilization,\nleading to suboptimal performance. Furthermore, the scarcity of high-quality\nHSI data highlights the importance of efficient data utilization to maximize\nreconstruction quality. To address these issues, we propose HyFusion, a novel\nframework designed to enhance the receptive field and enable effective feature\nmap reusing, thereby maximizing data utilization. First, HR-MSI and LR-HSI\ninputs are concatenated to form a quasi-fused draft, preserving complementary\nspatial and spectral details. Next, the Enhanced Reception Field Block (ERFB)\nis introduced, combining shifting-window attention and dense connections to\nexpand the receptive field, effectively capturing long-range dependencies and\nreusing features to reduce information loss, thereby boosting data efficiency.\nFinally, the Dual-Coupled Network (DCN) dynamically extracts high-frequency\nspectral and spatial features from LR-HSI and HR-MSI, ensuring efficient\ncross-domain fusion. Extensive experiments demonstrate that HyFusion achieves\nstate-of-the-art performance in HR-MSI/LR-HSI fusion, significantly improving\nreconstruction quality while maintaining a compact model size and computational\nefficiency. By integrating enhanced receptive fields and feature map reusing,\nHyFusion provides a practical and effective solution for HSI fusion in\nresource-constrained scenarios, setting a new benchmark in hyperspectral\nimaging. Our code will be publicly available.\n", "link": "http://arxiv.org/abs/2501.04665v1", "date": "2025-01-08", "relevancy": 1.9797, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4981}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4966}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyFusion%3A%20Enhanced%20Reception%20Field%20Transformer%20for%20Hyperspectral%20Image%0A%20%20Fusion&body=Title%3A%20HyFusion%3A%20Enhanced%20Reception%20Field%20Transformer%20for%20Hyperspectral%20Image%0A%20%20Fusion%0AAuthor%3A%20Chia-Ming%20Lee%20and%20Yu-Fan%20Lin%20and%20Yu-Hao%20Ho%20and%20Li-Wei%20Kang%20and%20Chih-Chung%20Hsu%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20fusion%20addresses%20the%20challenge%20of%20reconstructing%0AHigh-Resolution%20HSIs%20%28HR-HSIs%29%20from%20High-Resolution%20Multispectral%20images%0A%28HR-MSIs%29%20and%20Low-Resolution%20HSIs%20%28LR-HSIs%29%2C%20a%20critical%20task%20given%20the%20high%0Acosts%20and%20hardware%20limitations%20associated%20with%20acquiring%20high-quality%20HSIs.%0AWhile%20existing%20methods%20leverage%20spatial%20and%20spectral%20relationships%2C%20they%20often%0Asuffer%20from%20limited%20receptive%20fields%20and%20insufficient%20feature%20utilization%2C%0Aleading%20to%20suboptimal%20performance.%20Furthermore%2C%20the%20scarcity%20of%20high-quality%0AHSI%20data%20highlights%20the%20importance%20of%20efficient%20data%20utilization%20to%20maximize%0Areconstruction%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20HyFusion%2C%20a%20novel%0Aframework%20designed%20to%20enhance%20the%20receptive%20field%20and%20enable%20effective%20feature%0Amap%20reusing%2C%20thereby%20maximizing%20data%20utilization.%20First%2C%20HR-MSI%20and%20LR-HSI%0Ainputs%20are%20concatenated%20to%20form%20a%20quasi-fused%20draft%2C%20preserving%20complementary%0Aspatial%20and%20spectral%20details.%20Next%2C%20the%20Enhanced%20Reception%20Field%20Block%20%28ERFB%29%0Ais%20introduced%2C%20combining%20shifting-window%20attention%20and%20dense%20connections%20to%0Aexpand%20the%20receptive%20field%2C%20effectively%20capturing%20long-range%20dependencies%20and%0Areusing%20features%20to%20reduce%20information%20loss%2C%20thereby%20boosting%20data%20efficiency.%0AFinally%2C%20the%20Dual-Coupled%20Network%20%28DCN%29%20dynamically%20extracts%20high-frequency%0Aspectral%20and%20spatial%20features%20from%20LR-HSI%20and%20HR-MSI%2C%20ensuring%20efficient%0Across-domain%20fusion.%20Extensive%20experiments%20demonstrate%20that%20HyFusion%20achieves%0Astate-of-the-art%20performance%20in%20HR-MSI/LR-HSI%20fusion%2C%20significantly%20improving%0Areconstruction%20quality%20while%20maintaining%20a%20compact%20model%20size%20and%20computational%0Aefficiency.%20By%20integrating%20enhanced%20receptive%20fields%20and%20feature%20map%20reusing%2C%0AHyFusion%20provides%20a%20practical%20and%20effective%20solution%20for%20HSI%20fusion%20in%0Aresource-constrained%20scenarios%2C%20setting%20a%20new%20benchmark%20in%20hyperspectral%0Aimaging.%20Our%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyFusion%253A%2520Enhanced%2520Reception%2520Field%2520Transformer%2520for%2520Hyperspectral%2520Image%250A%2520%2520Fusion%26entry.906535625%3DChia-Ming%2520Lee%2520and%2520Yu-Fan%2520Lin%2520and%2520Yu-Hao%2520Ho%2520and%2520Li-Wei%2520Kang%2520and%2520Chih-Chung%2520Hsu%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520fusion%2520addresses%2520the%2520challenge%2520of%2520reconstructing%250AHigh-Resolution%2520HSIs%2520%2528HR-HSIs%2529%2520from%2520High-Resolution%2520Multispectral%2520images%250A%2528HR-MSIs%2529%2520and%2520Low-Resolution%2520HSIs%2520%2528LR-HSIs%2529%252C%2520a%2520critical%2520task%2520given%2520the%2520high%250Acosts%2520and%2520hardware%2520limitations%2520associated%2520with%2520acquiring%2520high-quality%2520HSIs.%250AWhile%2520existing%2520methods%2520leverage%2520spatial%2520and%2520spectral%2520relationships%252C%2520they%2520often%250Asuffer%2520from%2520limited%2520receptive%2520fields%2520and%2520insufficient%2520feature%2520utilization%252C%250Aleading%2520to%2520suboptimal%2520performance.%2520Furthermore%252C%2520the%2520scarcity%2520of%2520high-quality%250AHSI%2520data%2520highlights%2520the%2520importance%2520of%2520efficient%2520data%2520utilization%2520to%2520maximize%250Areconstruction%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520HyFusion%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520enhance%2520the%2520receptive%2520field%2520and%2520enable%2520effective%2520feature%250Amap%2520reusing%252C%2520thereby%2520maximizing%2520data%2520utilization.%2520First%252C%2520HR-MSI%2520and%2520LR-HSI%250Ainputs%2520are%2520concatenated%2520to%2520form%2520a%2520quasi-fused%2520draft%252C%2520preserving%2520complementary%250Aspatial%2520and%2520spectral%2520details.%2520Next%252C%2520the%2520Enhanced%2520Reception%2520Field%2520Block%2520%2528ERFB%2529%250Ais%2520introduced%252C%2520combining%2520shifting-window%2520attention%2520and%2520dense%2520connections%2520to%250Aexpand%2520the%2520receptive%2520field%252C%2520effectively%2520capturing%2520long-range%2520dependencies%2520and%250Areusing%2520features%2520to%2520reduce%2520information%2520loss%252C%2520thereby%2520boosting%2520data%2520efficiency.%250AFinally%252C%2520the%2520Dual-Coupled%2520Network%2520%2528DCN%2529%2520dynamically%2520extracts%2520high-frequency%250Aspectral%2520and%2520spatial%2520features%2520from%2520LR-HSI%2520and%2520HR-MSI%252C%2520ensuring%2520efficient%250Across-domain%2520fusion.%2520Extensive%2520experiments%2520demonstrate%2520that%2520HyFusion%2520achieves%250Astate-of-the-art%2520performance%2520in%2520HR-MSI/LR-HSI%2520fusion%252C%2520significantly%2520improving%250Areconstruction%2520quality%2520while%2520maintaining%2520a%2520compact%2520model%2520size%2520and%2520computational%250Aefficiency.%2520By%2520integrating%2520enhanced%2520receptive%2520fields%2520and%2520feature%2520map%2520reusing%252C%250AHyFusion%2520provides%2520a%2520practical%2520and%2520effective%2520solution%2520for%2520HSI%2520fusion%2520in%250Aresource-constrained%2520scenarios%252C%2520setting%2520a%2520new%2520benchmark%2520in%2520hyperspectral%250Aimaging.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyFusion%3A%20Enhanced%20Reception%20Field%20Transformer%20for%20Hyperspectral%20Image%0A%20%20Fusion&entry.906535625=Chia-Ming%20Lee%20and%20Yu-Fan%20Lin%20and%20Yu-Hao%20Ho%20and%20Li-Wei%20Kang%20and%20Chih-Chung%20Hsu&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20fusion%20addresses%20the%20challenge%20of%20reconstructing%0AHigh-Resolution%20HSIs%20%28HR-HSIs%29%20from%20High-Resolution%20Multispectral%20images%0A%28HR-MSIs%29%20and%20Low-Resolution%20HSIs%20%28LR-HSIs%29%2C%20a%20critical%20task%20given%20the%20high%0Acosts%20and%20hardware%20limitations%20associated%20with%20acquiring%20high-quality%20HSIs.%0AWhile%20existing%20methods%20leverage%20spatial%20and%20spectral%20relationships%2C%20they%20often%0Asuffer%20from%20limited%20receptive%20fields%20and%20insufficient%20feature%20utilization%2C%0Aleading%20to%20suboptimal%20performance.%20Furthermore%2C%20the%20scarcity%20of%20high-quality%0AHSI%20data%20highlights%20the%20importance%20of%20efficient%20data%20utilization%20to%20maximize%0Areconstruction%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20HyFusion%2C%20a%20novel%0Aframework%20designed%20to%20enhance%20the%20receptive%20field%20and%20enable%20effective%20feature%0Amap%20reusing%2C%20thereby%20maximizing%20data%20utilization.%20First%2C%20HR-MSI%20and%20LR-HSI%0Ainputs%20are%20concatenated%20to%20form%20a%20quasi-fused%20draft%2C%20preserving%20complementary%0Aspatial%20and%20spectral%20details.%20Next%2C%20the%20Enhanced%20Reception%20Field%20Block%20%28ERFB%29%0Ais%20introduced%2C%20combining%20shifting-window%20attention%20and%20dense%20connections%20to%0Aexpand%20the%20receptive%20field%2C%20effectively%20capturing%20long-range%20dependencies%20and%0Areusing%20features%20to%20reduce%20information%20loss%2C%20thereby%20boosting%20data%20efficiency.%0AFinally%2C%20the%20Dual-Coupled%20Network%20%28DCN%29%20dynamically%20extracts%20high-frequency%0Aspectral%20and%20spatial%20features%20from%20LR-HSI%20and%20HR-MSI%2C%20ensuring%20efficient%0Across-domain%20fusion.%20Extensive%20experiments%20demonstrate%20that%20HyFusion%20achieves%0Astate-of-the-art%20performance%20in%20HR-MSI/LR-HSI%20fusion%2C%20significantly%20improving%0Areconstruction%20quality%20while%20maintaining%20a%20compact%20model%20size%20and%20computational%0Aefficiency.%20By%20integrating%20enhanced%20receptive%20fields%20and%20feature%20map%20reusing%2C%0AHyFusion%20provides%20a%20practical%20and%20effective%20solution%20for%20HSI%20fusion%20in%0Aresource-constrained%20scenarios%2C%20setting%20a%20new%20benchmark%20in%20hyperspectral%0Aimaging.%20Our%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04665v1&entry.124074799=Read"},
{"title": "Histogram-Equalized Quantization for logic-gated Residual Neural\n  Networks", "author": "Van Thien Nguyen and William Guicquero and Gilles Sicard", "abstract": "  Adjusting the quantization according to the data or to the model loss seems\nmandatory to enable a high accuracy in the context of quantized neural\nnetworks. This work presents Histogram-Equalized Quantization (HEQ), an\nadaptive framework for linear symmetric quantization. HEQ automatically adapts\nthe quantization thresholds using a unique step size optimization. We\nempirically show that HEQ achieves state-of-the-art performances on CIFAR-10.\nExperiments on the STL-10 dataset even show that HEQ enables a proper training\nof our proposed logic-gated (OR, MUX) residual networks with a higher accuracy\nat a lower hardware complexity than previous work.\n", "link": "http://arxiv.org/abs/2501.04517v1", "date": "2025-01-08", "relevancy": 1.9761, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5007}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4908}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histogram-Equalized%20Quantization%20for%20logic-gated%20Residual%20Neural%0A%20%20Networks&body=Title%3A%20Histogram-Equalized%20Quantization%20for%20logic-gated%20Residual%20Neural%0A%20%20Networks%0AAuthor%3A%20Van%20Thien%20Nguyen%20and%20William%20Guicquero%20and%20Gilles%20Sicard%0AAbstract%3A%20%20%20Adjusting%20the%20quantization%20according%20to%20the%20data%20or%20to%20the%20model%20loss%20seems%0Amandatory%20to%20enable%20a%20high%20accuracy%20in%20the%20context%20of%20quantized%20neural%0Anetworks.%20This%20work%20presents%20Histogram-Equalized%20Quantization%20%28HEQ%29%2C%20an%0Aadaptive%20framework%20for%20linear%20symmetric%20quantization.%20HEQ%20automatically%20adapts%0Athe%20quantization%20thresholds%20using%20a%20unique%20step%20size%20optimization.%20We%0Aempirically%20show%20that%20HEQ%20achieves%20state-of-the-art%20performances%20on%20CIFAR-10.%0AExperiments%20on%20the%20STL-10%20dataset%20even%20show%20that%20HEQ%20enables%20a%20proper%20training%0Aof%20our%20proposed%20logic-gated%20%28OR%2C%20MUX%29%20residual%20networks%20with%20a%20higher%20accuracy%0Aat%20a%20lower%20hardware%20complexity%20than%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistogram-Equalized%2520Quantization%2520for%2520logic-gated%2520Residual%2520Neural%250A%2520%2520Networks%26entry.906535625%3DVan%2520Thien%2520Nguyen%2520and%2520William%2520Guicquero%2520and%2520Gilles%2520Sicard%26entry.1292438233%3D%2520%2520Adjusting%2520the%2520quantization%2520according%2520to%2520the%2520data%2520or%2520to%2520the%2520model%2520loss%2520seems%250Amandatory%2520to%2520enable%2520a%2520high%2520accuracy%2520in%2520the%2520context%2520of%2520quantized%2520neural%250Anetworks.%2520This%2520work%2520presents%2520Histogram-Equalized%2520Quantization%2520%2528HEQ%2529%252C%2520an%250Aadaptive%2520framework%2520for%2520linear%2520symmetric%2520quantization.%2520HEQ%2520automatically%2520adapts%250Athe%2520quantization%2520thresholds%2520using%2520a%2520unique%2520step%2520size%2520optimization.%2520We%250Aempirically%2520show%2520that%2520HEQ%2520achieves%2520state-of-the-art%2520performances%2520on%2520CIFAR-10.%250AExperiments%2520on%2520the%2520STL-10%2520dataset%2520even%2520show%2520that%2520HEQ%2520enables%2520a%2520proper%2520training%250Aof%2520our%2520proposed%2520logic-gated%2520%2528OR%252C%2520MUX%2529%2520residual%2520networks%2520with%2520a%2520higher%2520accuracy%250Aat%2520a%2520lower%2520hardware%2520complexity%2520than%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histogram-Equalized%20Quantization%20for%20logic-gated%20Residual%20Neural%0A%20%20Networks&entry.906535625=Van%20Thien%20Nguyen%20and%20William%20Guicquero%20and%20Gilles%20Sicard&entry.1292438233=%20%20Adjusting%20the%20quantization%20according%20to%20the%20data%20or%20to%20the%20model%20loss%20seems%0Amandatory%20to%20enable%20a%20high%20accuracy%20in%20the%20context%20of%20quantized%20neural%0Anetworks.%20This%20work%20presents%20Histogram-Equalized%20Quantization%20%28HEQ%29%2C%20an%0Aadaptive%20framework%20for%20linear%20symmetric%20quantization.%20HEQ%20automatically%20adapts%0Athe%20quantization%20thresholds%20using%20a%20unique%20step%20size%20optimization.%20We%0Aempirically%20show%20that%20HEQ%20achieves%20state-of-the-art%20performances%20on%20CIFAR-10.%0AExperiments%20on%20the%20STL-10%20dataset%20even%20show%20that%20HEQ%20enables%20a%20proper%20training%0Aof%20our%20proposed%20logic-gated%20%28OR%2C%20MUX%29%20residual%20networks%20with%20a%20higher%20accuracy%0Aat%20a%20lower%20hardware%20complexity%20than%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04517v1&entry.124074799=Read"},
{"title": "Histogram-Equalized Quantization for logic-gated Residual Neural\n  Networks", "author": "Van Thien Nguyen and William Guicquero and Gilles Sicard", "abstract": "  Adjusting the quantization according to the data or to the model loss seems\nmandatory to enable a high accuracy in the context of quantized neural\nnetworks. This work presents Histogram-Equalized Quantization (HEQ), an\nadaptive framework for linear symmetric quantization. HEQ automatically adapts\nthe quantization thresholds using a unique step size optimization. We\nempirically show that HEQ achieves state-of-the-art performances on CIFAR-10.\nExperiments on the STL-10 dataset even show that HEQ enables a proper training\nof our proposed logic-gated (OR, MUX) residual networks with a higher accuracy\nat a lower hardware complexity than previous work.\n", "link": "http://arxiv.org/abs/2501.04517v1", "date": "2025-01-08", "relevancy": 1.9761, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5007}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4908}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histogram-Equalized%20Quantization%20for%20logic-gated%20Residual%20Neural%0A%20%20Networks&body=Title%3A%20Histogram-Equalized%20Quantization%20for%20logic-gated%20Residual%20Neural%0A%20%20Networks%0AAuthor%3A%20Van%20Thien%20Nguyen%20and%20William%20Guicquero%20and%20Gilles%20Sicard%0AAbstract%3A%20%20%20Adjusting%20the%20quantization%20according%20to%20the%20data%20or%20to%20the%20model%20loss%20seems%0Amandatory%20to%20enable%20a%20high%20accuracy%20in%20the%20context%20of%20quantized%20neural%0Anetworks.%20This%20work%20presents%20Histogram-Equalized%20Quantization%20%28HEQ%29%2C%20an%0Aadaptive%20framework%20for%20linear%20symmetric%20quantization.%20HEQ%20automatically%20adapts%0Athe%20quantization%20thresholds%20using%20a%20unique%20step%20size%20optimization.%20We%0Aempirically%20show%20that%20HEQ%20achieves%20state-of-the-art%20performances%20on%20CIFAR-10.%0AExperiments%20on%20the%20STL-10%20dataset%20even%20show%20that%20HEQ%20enables%20a%20proper%20training%0Aof%20our%20proposed%20logic-gated%20%28OR%2C%20MUX%29%20residual%20networks%20with%20a%20higher%20accuracy%0Aat%20a%20lower%20hardware%20complexity%20than%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistogram-Equalized%2520Quantization%2520for%2520logic-gated%2520Residual%2520Neural%250A%2520%2520Networks%26entry.906535625%3DVan%2520Thien%2520Nguyen%2520and%2520William%2520Guicquero%2520and%2520Gilles%2520Sicard%26entry.1292438233%3D%2520%2520Adjusting%2520the%2520quantization%2520according%2520to%2520the%2520data%2520or%2520to%2520the%2520model%2520loss%2520seems%250Amandatory%2520to%2520enable%2520a%2520high%2520accuracy%2520in%2520the%2520context%2520of%2520quantized%2520neural%250Anetworks.%2520This%2520work%2520presents%2520Histogram-Equalized%2520Quantization%2520%2528HEQ%2529%252C%2520an%250Aadaptive%2520framework%2520for%2520linear%2520symmetric%2520quantization.%2520HEQ%2520automatically%2520adapts%250Athe%2520quantization%2520thresholds%2520using%2520a%2520unique%2520step%2520size%2520optimization.%2520We%250Aempirically%2520show%2520that%2520HEQ%2520achieves%2520state-of-the-art%2520performances%2520on%2520CIFAR-10.%250AExperiments%2520on%2520the%2520STL-10%2520dataset%2520even%2520show%2520that%2520HEQ%2520enables%2520a%2520proper%2520training%250Aof%2520our%2520proposed%2520logic-gated%2520%2528OR%252C%2520MUX%2529%2520residual%2520networks%2520with%2520a%2520higher%2520accuracy%250Aat%2520a%2520lower%2520hardware%2520complexity%2520than%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histogram-Equalized%20Quantization%20for%20logic-gated%20Residual%20Neural%0A%20%20Networks&entry.906535625=Van%20Thien%20Nguyen%20and%20William%20Guicquero%20and%20Gilles%20Sicard&entry.1292438233=%20%20Adjusting%20the%20quantization%20according%20to%20the%20data%20or%20to%20the%20model%20loss%20seems%0Amandatory%20to%20enable%20a%20high%20accuracy%20in%20the%20context%20of%20quantized%20neural%0Anetworks.%20This%20work%20presents%20Histogram-Equalized%20Quantization%20%28HEQ%29%2C%20an%0Aadaptive%20framework%20for%20linear%20symmetric%20quantization.%20HEQ%20automatically%20adapts%0Athe%20quantization%20thresholds%20using%20a%20unique%20step%20size%20optimization.%20We%0Aempirically%20show%20that%20HEQ%20achieves%20state-of-the-art%20performances%20on%20CIFAR-10.%0AExperiments%20on%20the%20STL-10%20dataset%20even%20show%20that%20HEQ%20enables%20a%20proper%20training%0Aof%20our%20proposed%20logic-gated%20%28OR%2C%20MUX%29%20residual%20networks%20with%20a%20higher%20accuracy%0Aat%20a%20lower%20hardware%20complexity%20than%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04517v1&entry.124074799=Read"},
{"title": "The Race to Efficiency: A New Perspective on AI Scaling Laws", "author": "Chien-Ping Lu", "abstract": "  As large-scale AI models expand, training becomes costlier and sustaining\nprogress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020),\nHoffmann et al. (2022)) predict training loss from a static compute budget yet\nneglect time and efficiency, prompting the question: how can we balance\nballooning GPU fleets with rapidly improving hardware and algorithms? We\nintroduce the relative-loss equation, a time- and efficiency-aware framework\nthat extends classical AI scaling laws. Our model shows that, without ongoing\nefficiency gains, advanced performance could demand millennia of training or\nunrealistically large GPU fleets. However, near-exponential progress remains\nachievable if the \"efficiency-doubling rate\" parallels Moore's Law. By\nformalizing this race to efficiency, we offer a quantitative roadmap for\nbalancing front-loaded GPU investments with incremental improvements across the\nAI stack. Empirical trends suggest that sustained efficiency gains can push AI\nscaling well into the coming decade, providing a new perspective on the\ndiminishing returns inherent in classical scaling.\n", "link": "http://arxiv.org/abs/2501.02156v3", "date": "2025-01-08", "relevancy": 1.9752, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5339}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Race%20to%20Efficiency%3A%20A%20New%20Perspective%20on%20AI%20Scaling%20Laws&body=Title%3A%20The%20Race%20to%20Efficiency%3A%20A%20New%20Perspective%20on%20AI%20Scaling%20Laws%0AAuthor%3A%20Chien-Ping%20Lu%0AAbstract%3A%20%20%20As%20large-scale%20AI%20models%20expand%2C%20training%20becomes%20costlier%20and%20sustaining%0Aprogress%20grows%20harder.%20Classical%20scaling%20laws%20%28e.g.%2C%20Kaplan%20et%20al.%20%282020%29%2C%0AHoffmann%20et%20al.%20%282022%29%29%20predict%20training%20loss%20from%20a%20static%20compute%20budget%20yet%0Aneglect%20time%20and%20efficiency%2C%20prompting%20the%20question%3A%20how%20can%20we%20balance%0Aballooning%20GPU%20fleets%20with%20rapidly%20improving%20hardware%20and%20algorithms%3F%20We%0Aintroduce%20the%20relative-loss%20equation%2C%20a%20time-%20and%20efficiency-aware%20framework%0Athat%20extends%20classical%20AI%20scaling%20laws.%20Our%20model%20shows%20that%2C%20without%20ongoing%0Aefficiency%20gains%2C%20advanced%20performance%20could%20demand%20millennia%20of%20training%20or%0Aunrealistically%20large%20GPU%20fleets.%20However%2C%20near-exponential%20progress%20remains%0Aachievable%20if%20the%20%22efficiency-doubling%20rate%22%20parallels%20Moore%27s%20Law.%20By%0Aformalizing%20this%20race%20to%20efficiency%2C%20we%20offer%20a%20quantitative%20roadmap%20for%0Abalancing%20front-loaded%20GPU%20investments%20with%20incremental%20improvements%20across%20the%0AAI%20stack.%20Empirical%20trends%20suggest%20that%20sustained%20efficiency%20gains%20can%20push%20AI%0Ascaling%20well%20into%20the%20coming%20decade%2C%20providing%20a%20new%20perspective%20on%20the%0Adiminishing%20returns%20inherent%20in%20classical%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02156v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Race%2520to%2520Efficiency%253A%2520A%2520New%2520Perspective%2520on%2520AI%2520Scaling%2520Laws%26entry.906535625%3DChien-Ping%2520Lu%26entry.1292438233%3D%2520%2520As%2520large-scale%2520AI%2520models%2520expand%252C%2520training%2520becomes%2520costlier%2520and%2520sustaining%250Aprogress%2520grows%2520harder.%2520Classical%2520scaling%2520laws%2520%2528e.g.%252C%2520Kaplan%2520et%2520al.%2520%25282020%2529%252C%250AHoffmann%2520et%2520al.%2520%25282022%2529%2529%2520predict%2520training%2520loss%2520from%2520a%2520static%2520compute%2520budget%2520yet%250Aneglect%2520time%2520and%2520efficiency%252C%2520prompting%2520the%2520question%253A%2520how%2520can%2520we%2520balance%250Aballooning%2520GPU%2520fleets%2520with%2520rapidly%2520improving%2520hardware%2520and%2520algorithms%253F%2520We%250Aintroduce%2520the%2520relative-loss%2520equation%252C%2520a%2520time-%2520and%2520efficiency-aware%2520framework%250Athat%2520extends%2520classical%2520AI%2520scaling%2520laws.%2520Our%2520model%2520shows%2520that%252C%2520without%2520ongoing%250Aefficiency%2520gains%252C%2520advanced%2520performance%2520could%2520demand%2520millennia%2520of%2520training%2520or%250Aunrealistically%2520large%2520GPU%2520fleets.%2520However%252C%2520near-exponential%2520progress%2520remains%250Aachievable%2520if%2520the%2520%2522efficiency-doubling%2520rate%2522%2520parallels%2520Moore%2527s%2520Law.%2520By%250Aformalizing%2520this%2520race%2520to%2520efficiency%252C%2520we%2520offer%2520a%2520quantitative%2520roadmap%2520for%250Abalancing%2520front-loaded%2520GPU%2520investments%2520with%2520incremental%2520improvements%2520across%2520the%250AAI%2520stack.%2520Empirical%2520trends%2520suggest%2520that%2520sustained%2520efficiency%2520gains%2520can%2520push%2520AI%250Ascaling%2520well%2520into%2520the%2520coming%2520decade%252C%2520providing%2520a%2520new%2520perspective%2520on%2520the%250Adiminishing%2520returns%2520inherent%2520in%2520classical%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02156v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Race%20to%20Efficiency%3A%20A%20New%20Perspective%20on%20AI%20Scaling%20Laws&entry.906535625=Chien-Ping%20Lu&entry.1292438233=%20%20As%20large-scale%20AI%20models%20expand%2C%20training%20becomes%20costlier%20and%20sustaining%0Aprogress%20grows%20harder.%20Classical%20scaling%20laws%20%28e.g.%2C%20Kaplan%20et%20al.%20%282020%29%2C%0AHoffmann%20et%20al.%20%282022%29%29%20predict%20training%20loss%20from%20a%20static%20compute%20budget%20yet%0Aneglect%20time%20and%20efficiency%2C%20prompting%20the%20question%3A%20how%20can%20we%20balance%0Aballooning%20GPU%20fleets%20with%20rapidly%20improving%20hardware%20and%20algorithms%3F%20We%0Aintroduce%20the%20relative-loss%20equation%2C%20a%20time-%20and%20efficiency-aware%20framework%0Athat%20extends%20classical%20AI%20scaling%20laws.%20Our%20model%20shows%20that%2C%20without%20ongoing%0Aefficiency%20gains%2C%20advanced%20performance%20could%20demand%20millennia%20of%20training%20or%0Aunrealistically%20large%20GPU%20fleets.%20However%2C%20near-exponential%20progress%20remains%0Aachievable%20if%20the%20%22efficiency-doubling%20rate%22%20parallels%20Moore%27s%20Law.%20By%0Aformalizing%20this%20race%20to%20efficiency%2C%20we%20offer%20a%20quantitative%20roadmap%20for%0Abalancing%20front-loaded%20GPU%20investments%20with%20incremental%20improvements%20across%20the%0AAI%20stack.%20Empirical%20trends%20suggest%20that%20sustained%20efficiency%20gains%20can%20push%20AI%0Ascaling%20well%20into%20the%20coming%20decade%2C%20providing%20a%20new%20perspective%20on%20the%0Adiminishing%20returns%20inherent%20in%20classical%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02156v3&entry.124074799=Read"},
{"title": "Research on environment perception and behavior prediction of\n  intelligent UAV based on semantic communication", "author": "Kechong Ren and Li Gao and Qi Guan", "abstract": "  The convergence of drone delivery systems, virtual worlds, and blockchain has\ntransformed logistics and supply chain management, providing a fast, and\nenvironmentally friendly alternative to traditional ground transportation\nmethods;Provide users with a real-world experience, virtual service providers\nneed to collect up-to-the-minute delivery information from edge devices. To\naddress this challenge, 1) a reinforcement learning approach is introduced to\nenable drones with fast training capabilities and the ability to autonomously\nadapt to new virtual scenarios for effective resource allocation.2) A semantic\ncommunication framework for meta-universes is proposed, which utilizes the\nextraction of semantic information to reduce the communication cost and\nincentivize the transmission of information for meta-universe services.3) In\norder to ensure that user information security, a lightweight authentication\nand key agreement scheme is designed between the drone and the user by\nintroducing blockchain technology. In our experiments, the drone adaptation\nperformance is improved by about 35\\%, and the local offloading rate can reach\n90\\% with the increase of the number of base stations. The semantic\ncommunication system proposed in this paper is compared with the Cross Entropy\nbaseline model. Introducing blockchain technology the throughput of the\ntransaction is maintained at a stable value with different number of drones.\n", "link": "http://arxiv.org/abs/2501.04480v1", "date": "2025-01-08", "relevancy": 1.9732, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5277}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20environment%20perception%20and%20behavior%20prediction%20of%0A%20%20intelligent%20UAV%20based%20on%20semantic%20communication&body=Title%3A%20Research%20on%20environment%20perception%20and%20behavior%20prediction%20of%0A%20%20intelligent%20UAV%20based%20on%20semantic%20communication%0AAuthor%3A%20Kechong%20Ren%20and%20Li%20Gao%20and%20Qi%20Guan%0AAbstract%3A%20%20%20The%20convergence%20of%20drone%20delivery%20systems%2C%20virtual%20worlds%2C%20and%20blockchain%20has%0Atransformed%20logistics%20and%20supply%20chain%20management%2C%20providing%20a%20fast%2C%20and%0Aenvironmentally%20friendly%20alternative%20to%20traditional%20ground%20transportation%0Amethods%3BProvide%20users%20with%20a%20real-world%20experience%2C%20virtual%20service%20providers%0Aneed%20to%20collect%20up-to-the-minute%20delivery%20information%20from%20edge%20devices.%20To%0Aaddress%20this%20challenge%2C%201%29%20a%20reinforcement%20learning%20approach%20is%20introduced%20to%0Aenable%20drones%20with%20fast%20training%20capabilities%20and%20the%20ability%20to%20autonomously%0Aadapt%20to%20new%20virtual%20scenarios%20for%20effective%20resource%20allocation.2%29%20A%20semantic%0Acommunication%20framework%20for%20meta-universes%20is%20proposed%2C%20which%20utilizes%20the%0Aextraction%20of%20semantic%20information%20to%20reduce%20the%20communication%20cost%20and%0Aincentivize%20the%20transmission%20of%20information%20for%20meta-universe%20services.3%29%20In%0Aorder%20to%20ensure%20that%20user%20information%20security%2C%20a%20lightweight%20authentication%0Aand%20key%20agreement%20scheme%20is%20designed%20between%20the%20drone%20and%20the%20user%20by%0Aintroducing%20blockchain%20technology.%20In%20our%20experiments%2C%20the%20drone%20adaptation%0Aperformance%20is%20improved%20by%20about%2035%5C%25%2C%20and%20the%20local%20offloading%20rate%20can%20reach%0A90%5C%25%20with%20the%20increase%20of%20the%20number%20of%20base%20stations.%20The%20semantic%0Acommunication%20system%20proposed%20in%20this%20paper%20is%20compared%20with%20the%20Cross%20Entropy%0Abaseline%20model.%20Introducing%20blockchain%20technology%20the%20throughput%20of%20the%0Atransaction%20is%20maintained%20at%20a%20stable%20value%20with%20different%20number%20of%20drones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520environment%2520perception%2520and%2520behavior%2520prediction%2520of%250A%2520%2520intelligent%2520UAV%2520based%2520on%2520semantic%2520communication%26entry.906535625%3DKechong%2520Ren%2520and%2520Li%2520Gao%2520and%2520Qi%2520Guan%26entry.1292438233%3D%2520%2520The%2520convergence%2520of%2520drone%2520delivery%2520systems%252C%2520virtual%2520worlds%252C%2520and%2520blockchain%2520has%250Atransformed%2520logistics%2520and%2520supply%2520chain%2520management%252C%2520providing%2520a%2520fast%252C%2520and%250Aenvironmentally%2520friendly%2520alternative%2520to%2520traditional%2520ground%2520transportation%250Amethods%253BProvide%2520users%2520with%2520a%2520real-world%2520experience%252C%2520virtual%2520service%2520providers%250Aneed%2520to%2520collect%2520up-to-the-minute%2520delivery%2520information%2520from%2520edge%2520devices.%2520To%250Aaddress%2520this%2520challenge%252C%25201%2529%2520a%2520reinforcement%2520learning%2520approach%2520is%2520introduced%2520to%250Aenable%2520drones%2520with%2520fast%2520training%2520capabilities%2520and%2520the%2520ability%2520to%2520autonomously%250Aadapt%2520to%2520new%2520virtual%2520scenarios%2520for%2520effective%2520resource%2520allocation.2%2529%2520A%2520semantic%250Acommunication%2520framework%2520for%2520meta-universes%2520is%2520proposed%252C%2520which%2520utilizes%2520the%250Aextraction%2520of%2520semantic%2520information%2520to%2520reduce%2520the%2520communication%2520cost%2520and%250Aincentivize%2520the%2520transmission%2520of%2520information%2520for%2520meta-universe%2520services.3%2529%2520In%250Aorder%2520to%2520ensure%2520that%2520user%2520information%2520security%252C%2520a%2520lightweight%2520authentication%250Aand%2520key%2520agreement%2520scheme%2520is%2520designed%2520between%2520the%2520drone%2520and%2520the%2520user%2520by%250Aintroducing%2520blockchain%2520technology.%2520In%2520our%2520experiments%252C%2520the%2520drone%2520adaptation%250Aperformance%2520is%2520improved%2520by%2520about%252035%255C%2525%252C%2520and%2520the%2520local%2520offloading%2520rate%2520can%2520reach%250A90%255C%2525%2520with%2520the%2520increase%2520of%2520the%2520number%2520of%2520base%2520stations.%2520The%2520semantic%250Acommunication%2520system%2520proposed%2520in%2520this%2520paper%2520is%2520compared%2520with%2520the%2520Cross%2520Entropy%250Abaseline%2520model.%2520Introducing%2520blockchain%2520technology%2520the%2520throughput%2520of%2520the%250Atransaction%2520is%2520maintained%2520at%2520a%2520stable%2520value%2520with%2520different%2520number%2520of%2520drones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20environment%20perception%20and%20behavior%20prediction%20of%0A%20%20intelligent%20UAV%20based%20on%20semantic%20communication&entry.906535625=Kechong%20Ren%20and%20Li%20Gao%20and%20Qi%20Guan&entry.1292438233=%20%20The%20convergence%20of%20drone%20delivery%20systems%2C%20virtual%20worlds%2C%20and%20blockchain%20has%0Atransformed%20logistics%20and%20supply%20chain%20management%2C%20providing%20a%20fast%2C%20and%0Aenvironmentally%20friendly%20alternative%20to%20traditional%20ground%20transportation%0Amethods%3BProvide%20users%20with%20a%20real-world%20experience%2C%20virtual%20service%20providers%0Aneed%20to%20collect%20up-to-the-minute%20delivery%20information%20from%20edge%20devices.%20To%0Aaddress%20this%20challenge%2C%201%29%20a%20reinforcement%20learning%20approach%20is%20introduced%20to%0Aenable%20drones%20with%20fast%20training%20capabilities%20and%20the%20ability%20to%20autonomously%0Aadapt%20to%20new%20virtual%20scenarios%20for%20effective%20resource%20allocation.2%29%20A%20semantic%0Acommunication%20framework%20for%20meta-universes%20is%20proposed%2C%20which%20utilizes%20the%0Aextraction%20of%20semantic%20information%20to%20reduce%20the%20communication%20cost%20and%0Aincentivize%20the%20transmission%20of%20information%20for%20meta-universe%20services.3%29%20In%0Aorder%20to%20ensure%20that%20user%20information%20security%2C%20a%20lightweight%20authentication%0Aand%20key%20agreement%20scheme%20is%20designed%20between%20the%20drone%20and%20the%20user%20by%0Aintroducing%20blockchain%20technology.%20In%20our%20experiments%2C%20the%20drone%20adaptation%0Aperformance%20is%20improved%20by%20about%2035%5C%25%2C%20and%20the%20local%20offloading%20rate%20can%20reach%0A90%5C%25%20with%20the%20increase%20of%20the%20number%20of%20base%20stations.%20The%20semantic%0Acommunication%20system%20proposed%20in%20this%20paper%20is%20compared%20with%20the%20Cross%20Entropy%0Abaseline%20model.%20Introducing%20blockchain%20technology%20the%20throughput%20of%20the%0Atransaction%20is%20maintained%20at%20a%20stable%20value%20with%20different%20number%20of%20drones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04480v1&entry.124074799=Read"},
{"title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought", "author": "Violet Xiang and Charlie Snell and Kanishk Gandhi and Alon Albalak and Anikait Singh and Chase Blagden and Duy Phung and Rafael Rafailov and Nathan Lile and Dakota Mahan and Louis Castricato and Jan-Philipp Franken and Nick Haber and Chelsea Finn", "abstract": "  We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.\n", "link": "http://arxiv.org/abs/2501.04682v1", "date": "2025-01-08", "relevancy": 1.969, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20System%202%20Reasoning%20in%20LLMs%3A%20Learning%20How%20to%20Think%20With%20Meta%0A%20%20Chain-of-Thought&body=Title%3A%20Towards%20System%202%20Reasoning%20in%20LLMs%3A%20Learning%20How%20to%20Think%20With%20Meta%0A%20%20Chain-of-Thought%0AAuthor%3A%20Violet%20Xiang%20and%20Charlie%20Snell%20and%20Kanishk%20Gandhi%20and%20Alon%20Albalak%20and%20Anikait%20Singh%20and%20Chase%20Blagden%20and%20Duy%20Phung%20and%20Rafael%20Rafailov%20and%20Nathan%20Lile%20and%20Dakota%20Mahan%20and%20Louis%20Castricato%20and%20Jan-Philipp%20Franken%20and%20Nick%20Haber%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%2C%20Meta%20Chain-of-Thought%20%28Meta-CoT%29%2C%20which%20extends%0Atraditional%20Chain-of-Thought%20%28CoT%29%20by%20explicitly%20modeling%20the%20underlying%0Areasoning%20required%20to%20arrive%20at%20a%20particular%20CoT.%20We%20present%20empirical%20evidence%0Afrom%20state-of-the-art%20models%20exhibiting%20behaviors%20consistent%20with%20in-context%0Asearch%2C%20and%20explore%20methods%20for%20producing%20Meta-CoT%20via%20process%20supervision%2C%0Asynthetic%20data%20generation%2C%20and%20search%20algorithms.%20Finally%2C%20we%20outline%20a%0Aconcrete%20pipeline%20for%20training%20a%20model%20to%20produce%20Meta-CoTs%2C%20incorporating%0Ainstruction%20tuning%20with%20linearized%20search%20traces%20and%20reinforcement%20learning%0Apost-training.%20Finally%2C%20we%20discuss%20open%20research%20questions%2C%20including%20scaling%0Alaws%2C%20verifier%20roles%2C%20and%20the%20potential%20for%20discovering%20novel%20reasoning%0Aalgorithms.%20This%20work%20provides%20a%20theoretical%20and%20practical%20roadmap%20to%20enable%0AMeta-CoT%20in%20LLMs%2C%20paving%20the%20way%20for%20more%20powerful%20and%20human-like%20reasoning%20in%0Aartificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520System%25202%2520Reasoning%2520in%2520LLMs%253A%2520Learning%2520How%2520to%2520Think%2520With%2520Meta%250A%2520%2520Chain-of-Thought%26entry.906535625%3DViolet%2520Xiang%2520and%2520Charlie%2520Snell%2520and%2520Kanishk%2520Gandhi%2520and%2520Alon%2520Albalak%2520and%2520Anikait%2520Singh%2520and%2520Chase%2520Blagden%2520and%2520Duy%2520Phung%2520and%2520Rafael%2520Rafailov%2520and%2520Nathan%2520Lile%2520and%2520Dakota%2520Mahan%2520and%2520Louis%2520Castricato%2520and%2520Jan-Philipp%2520Franken%2520and%2520Nick%2520Haber%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%252C%2520Meta%2520Chain-of-Thought%2520%2528Meta-CoT%2529%252C%2520which%2520extends%250Atraditional%2520Chain-of-Thought%2520%2528CoT%2529%2520by%2520explicitly%2520modeling%2520the%2520underlying%250Areasoning%2520required%2520to%2520arrive%2520at%2520a%2520particular%2520CoT.%2520We%2520present%2520empirical%2520evidence%250Afrom%2520state-of-the-art%2520models%2520exhibiting%2520behaviors%2520consistent%2520with%2520in-context%250Asearch%252C%2520and%2520explore%2520methods%2520for%2520producing%2520Meta-CoT%2520via%2520process%2520supervision%252C%250Asynthetic%2520data%2520generation%252C%2520and%2520search%2520algorithms.%2520Finally%252C%2520we%2520outline%2520a%250Aconcrete%2520pipeline%2520for%2520training%2520a%2520model%2520to%2520produce%2520Meta-CoTs%252C%2520incorporating%250Ainstruction%2520tuning%2520with%2520linearized%2520search%2520traces%2520and%2520reinforcement%2520learning%250Apost-training.%2520Finally%252C%2520we%2520discuss%2520open%2520research%2520questions%252C%2520including%2520scaling%250Alaws%252C%2520verifier%2520roles%252C%2520and%2520the%2520potential%2520for%2520discovering%2520novel%2520reasoning%250Aalgorithms.%2520This%2520work%2520provides%2520a%2520theoretical%2520and%2520practical%2520roadmap%2520to%2520enable%250AMeta-CoT%2520in%2520LLMs%252C%2520paving%2520the%2520way%2520for%2520more%2520powerful%2520and%2520human-like%2520reasoning%2520in%250Aartificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20System%202%20Reasoning%20in%20LLMs%3A%20Learning%20How%20to%20Think%20With%20Meta%0A%20%20Chain-of-Thought&entry.906535625=Violet%20Xiang%20and%20Charlie%20Snell%20and%20Kanishk%20Gandhi%20and%20Alon%20Albalak%20and%20Anikait%20Singh%20and%20Chase%20Blagden%20and%20Duy%20Phung%20and%20Rafael%20Rafailov%20and%20Nathan%20Lile%20and%20Dakota%20Mahan%20and%20Louis%20Castricato%20and%20Jan-Philipp%20Franken%20and%20Nick%20Haber%20and%20Chelsea%20Finn&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%2C%20Meta%20Chain-of-Thought%20%28Meta-CoT%29%2C%20which%20extends%0Atraditional%20Chain-of-Thought%20%28CoT%29%20by%20explicitly%20modeling%20the%20underlying%0Areasoning%20required%20to%20arrive%20at%20a%20particular%20CoT.%20We%20present%20empirical%20evidence%0Afrom%20state-of-the-art%20models%20exhibiting%20behaviors%20consistent%20with%20in-context%0Asearch%2C%20and%20explore%20methods%20for%20producing%20Meta-CoT%20via%20process%20supervision%2C%0Asynthetic%20data%20generation%2C%20and%20search%20algorithms.%20Finally%2C%20we%20outline%20a%0Aconcrete%20pipeline%20for%20training%20a%20model%20to%20produce%20Meta-CoTs%2C%20incorporating%0Ainstruction%20tuning%20with%20linearized%20search%20traces%20and%20reinforcement%20learning%0Apost-training.%20Finally%2C%20we%20discuss%20open%20research%20questions%2C%20including%20scaling%0Alaws%2C%20verifier%20roles%2C%20and%20the%20potential%20for%20discovering%20novel%20reasoning%0Aalgorithms.%20This%20work%20provides%20a%20theoretical%20and%20practical%20roadmap%20to%20enable%0AMeta-CoT%20in%20LLMs%2C%20paving%20the%20way%20for%20more%20powerful%20and%20human-like%20reasoning%20in%0Aartificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04682v1&entry.124074799=Read"},
{"title": "Bridging Simplicity and Sophistication using GLinear: A Novel\n  Architecture for Enhanced Time Series Prediction", "author": "Syed Tahir Hussain Rizvi and Neel Kanwal and Muddasar Naeem and Alfredo Cuzzocrea and Antonio Coronato", "abstract": "  Time Series Forecasting (TSF) is an important application across many fields.\nThere is a debate about whether Transformers, despite being good at\nunderstanding long sequences, struggle with preserving temporal relationships\nin time series data. Recent research suggests that simpler linear models might\noutperform or at least provide competitive performance compared to complex\nTransformer-based models for TSF tasks. In this paper, we propose a novel\ndata-efficient architecture, GLinear, for multivariate TSF that exploits\nperiodic patterns to provide better accuracy. It also provides better\nprediction accuracy by using a smaller amount of historical data compared to\nother state-of-the-art linear predictors. Four different datasets (ETTh1,\nElectricity, Traffic, and Weather) are used to evaluate the performance of the\nproposed predictor. A performance comparison with state-of-the-art linear\narchitectures (such as NLinear, DLinear, and RLinear) and transformer-based\ntime series predictor (Autoformer) shows that the GLinear, despite being\nparametrically efficient, significantly outperforms the existing architectures\nin most cases of multivariate TSF. We hope that the proposed GLinear opens new\nfronts of research and development of simpler and more sophisticated\narchitectures for data and computationally efficient time-series analysis.\n", "link": "http://arxiv.org/abs/2501.01087v3", "date": "2025-01-08", "relevancy": 1.9589, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5075}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Simplicity%20and%20Sophistication%20using%20GLinear%3A%20A%20Novel%0A%20%20Architecture%20for%20Enhanced%20Time%20Series%20Prediction&body=Title%3A%20Bridging%20Simplicity%20and%20Sophistication%20using%20GLinear%3A%20A%20Novel%0A%20%20Architecture%20for%20Enhanced%20Time%20Series%20Prediction%0AAuthor%3A%20Syed%20Tahir%20Hussain%20Rizvi%20and%20Neel%20Kanwal%20and%20Muddasar%20Naeem%20and%20Alfredo%20Cuzzocrea%20and%20Antonio%20Coronato%0AAbstract%3A%20%20%20Time%20Series%20Forecasting%20%28TSF%29%20is%20an%20important%20application%20across%20many%20fields.%0AThere%20is%20a%20debate%20about%20whether%20Transformers%2C%20despite%20being%20good%20at%0Aunderstanding%20long%20sequences%2C%20struggle%20with%20preserving%20temporal%20relationships%0Ain%20time%20series%20data.%20Recent%20research%20suggests%20that%20simpler%20linear%20models%20might%0Aoutperform%20or%20at%20least%20provide%20competitive%20performance%20compared%20to%20complex%0ATransformer-based%20models%20for%20TSF%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Adata-efficient%20architecture%2C%20GLinear%2C%20for%20multivariate%20TSF%20that%20exploits%0Aperiodic%20patterns%20to%20provide%20better%20accuracy.%20It%20also%20provides%20better%0Aprediction%20accuracy%20by%20using%20a%20smaller%20amount%20of%20historical%20data%20compared%20to%0Aother%20state-of-the-art%20linear%20predictors.%20Four%20different%20datasets%20%28ETTh1%2C%0AElectricity%2C%20Traffic%2C%20and%20Weather%29%20are%20used%20to%20evaluate%20the%20performance%20of%20the%0Aproposed%20predictor.%20A%20performance%20comparison%20with%20state-of-the-art%20linear%0Aarchitectures%20%28such%20as%20NLinear%2C%20DLinear%2C%20and%20RLinear%29%20and%20transformer-based%0Atime%20series%20predictor%20%28Autoformer%29%20shows%20that%20the%20GLinear%2C%20despite%20being%0Aparametrically%20efficient%2C%20significantly%20outperforms%20the%20existing%20architectures%0Ain%20most%20cases%20of%20multivariate%20TSF.%20We%20hope%20that%20the%20proposed%20GLinear%20opens%20new%0Afronts%20of%20research%20and%20development%20of%20simpler%20and%20more%20sophisticated%0Aarchitectures%20for%20data%20and%20computationally%20efficient%20time-series%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01087v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Simplicity%2520and%2520Sophistication%2520using%2520GLinear%253A%2520A%2520Novel%250A%2520%2520Architecture%2520for%2520Enhanced%2520Time%2520Series%2520Prediction%26entry.906535625%3DSyed%2520Tahir%2520Hussain%2520Rizvi%2520and%2520Neel%2520Kanwal%2520and%2520Muddasar%2520Naeem%2520and%2520Alfredo%2520Cuzzocrea%2520and%2520Antonio%2520Coronato%26entry.1292438233%3D%2520%2520Time%2520Series%2520Forecasting%2520%2528TSF%2529%2520is%2520an%2520important%2520application%2520across%2520many%2520fields.%250AThere%2520is%2520a%2520debate%2520about%2520whether%2520Transformers%252C%2520despite%2520being%2520good%2520at%250Aunderstanding%2520long%2520sequences%252C%2520struggle%2520with%2520preserving%2520temporal%2520relationships%250Ain%2520time%2520series%2520data.%2520Recent%2520research%2520suggests%2520that%2520simpler%2520linear%2520models%2520might%250Aoutperform%2520or%2520at%2520least%2520provide%2520competitive%2520performance%2520compared%2520to%2520complex%250ATransformer-based%2520models%2520for%2520TSF%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Adata-efficient%2520architecture%252C%2520GLinear%252C%2520for%2520multivariate%2520TSF%2520that%2520exploits%250Aperiodic%2520patterns%2520to%2520provide%2520better%2520accuracy.%2520It%2520also%2520provides%2520better%250Aprediction%2520accuracy%2520by%2520using%2520a%2520smaller%2520amount%2520of%2520historical%2520data%2520compared%2520to%250Aother%2520state-of-the-art%2520linear%2520predictors.%2520Four%2520different%2520datasets%2520%2528ETTh1%252C%250AElectricity%252C%2520Traffic%252C%2520and%2520Weather%2529%2520are%2520used%2520to%2520evaluate%2520the%2520performance%2520of%2520the%250Aproposed%2520predictor.%2520A%2520performance%2520comparison%2520with%2520state-of-the-art%2520linear%250Aarchitectures%2520%2528such%2520as%2520NLinear%252C%2520DLinear%252C%2520and%2520RLinear%2529%2520and%2520transformer-based%250Atime%2520series%2520predictor%2520%2528Autoformer%2529%2520shows%2520that%2520the%2520GLinear%252C%2520despite%2520being%250Aparametrically%2520efficient%252C%2520significantly%2520outperforms%2520the%2520existing%2520architectures%250Ain%2520most%2520cases%2520of%2520multivariate%2520TSF.%2520We%2520hope%2520that%2520the%2520proposed%2520GLinear%2520opens%2520new%250Afronts%2520of%2520research%2520and%2520development%2520of%2520simpler%2520and%2520more%2520sophisticated%250Aarchitectures%2520for%2520data%2520and%2520computationally%2520efficient%2520time-series%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01087v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Simplicity%20and%20Sophistication%20using%20GLinear%3A%20A%20Novel%0A%20%20Architecture%20for%20Enhanced%20Time%20Series%20Prediction&entry.906535625=Syed%20Tahir%20Hussain%20Rizvi%20and%20Neel%20Kanwal%20and%20Muddasar%20Naeem%20and%20Alfredo%20Cuzzocrea%20and%20Antonio%20Coronato&entry.1292438233=%20%20Time%20Series%20Forecasting%20%28TSF%29%20is%20an%20important%20application%20across%20many%20fields.%0AThere%20is%20a%20debate%20about%20whether%20Transformers%2C%20despite%20being%20good%20at%0Aunderstanding%20long%20sequences%2C%20struggle%20with%20preserving%20temporal%20relationships%0Ain%20time%20series%20data.%20Recent%20research%20suggests%20that%20simpler%20linear%20models%20might%0Aoutperform%20or%20at%20least%20provide%20competitive%20performance%20compared%20to%20complex%0ATransformer-based%20models%20for%20TSF%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Adata-efficient%20architecture%2C%20GLinear%2C%20for%20multivariate%20TSF%20that%20exploits%0Aperiodic%20patterns%20to%20provide%20better%20accuracy.%20It%20also%20provides%20better%0Aprediction%20accuracy%20by%20using%20a%20smaller%20amount%20of%20historical%20data%20compared%20to%0Aother%20state-of-the-art%20linear%20predictors.%20Four%20different%20datasets%20%28ETTh1%2C%0AElectricity%2C%20Traffic%2C%20and%20Weather%29%20are%20used%20to%20evaluate%20the%20performance%20of%20the%0Aproposed%20predictor.%20A%20performance%20comparison%20with%20state-of-the-art%20linear%0Aarchitectures%20%28such%20as%20NLinear%2C%20DLinear%2C%20and%20RLinear%29%20and%20transformer-based%0Atime%20series%20predictor%20%28Autoformer%29%20shows%20that%20the%20GLinear%2C%20despite%20being%0Aparametrically%20efficient%2C%20significantly%20outperforms%20the%20existing%20architectures%0Ain%20most%20cases%20of%20multivariate%20TSF.%20We%20hope%20that%20the%20proposed%20GLinear%20opens%20new%0Afronts%20of%20research%20and%20development%20of%20simpler%20and%20more%20sophisticated%0Aarchitectures%20for%20data%20and%20computationally%20efficient%20time-series%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01087v3&entry.124074799=Read"},
{"title": "A Digital Shadow for Modeling, Studying and Preventing Urban Crime", "author": "Juan Palma-Borda and Eduardo Guzm\u00e1n and Mar\u00eda-Victoria Belmonte", "abstract": "  Crime is one of the greatest threats to urban security. Around 80 percent of\nthe world's population lives in countries with high levels of criminality. Most\nof the crimes committed in the cities take place in their urban environments.\nThis paper presents the development and validation of a digital shadow platform\nfor modeling and simulating urban crime. This digital shadow has been\nconstructed using data-driven agent-based modeling and simulation techniques,\nwhich are suitable for capturing dynamic interactions among individuals and\nwith their environment. Our approach transforms and integrates well-known\ncriminological theories and the expert knowledge of law enforcement agencies\n(LEA), policy makers, and other stakeholders under a theoretical model, which\nis in turn combined with real crime, spatial (cartographic) and socio-economic\ndata into an urban model characterizing the daily behavior of citizens. The\ndigital shadow has also been instantiated for the city of Malaga, for which we\nhad over 300,000 complaints available. This instance has been calibrated with\nthose complaints and other geographic and socio-economic information of the\ncity. To the best of our knowledge, our digital shadow is the first for large\nurban areas that has been calibrated with a large dataset of real crime reports\nand with an accurate representation of the urban environment. The performance\nindicators of the model after being calibrated, in terms of the metrics widely\nused in predictive policing, suggest that our simulated crime generation\nmatches the general pattern of crime in the city according to historical data.\nOur digital shadow platform could be an interesting tool for modeling and\npredicting criminal behavior in an urban environment on a daily basis and,\nthus, a useful tool for policy makers, criminologists, sociologists, LEAs, etc.\nto study and prevent urban crime.\n", "link": "http://arxiv.org/abs/2501.04435v1", "date": "2025-01-08", "relevancy": 1.9537, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4989}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Digital%20Shadow%20for%20Modeling%2C%20Studying%20and%20Preventing%20Urban%20Crime&body=Title%3A%20A%20Digital%20Shadow%20for%20Modeling%2C%20Studying%20and%20Preventing%20Urban%20Crime%0AAuthor%3A%20Juan%20Palma-Borda%20and%20Eduardo%20Guzm%C3%A1n%20and%20Mar%C3%ADa-Victoria%20Belmonte%0AAbstract%3A%20%20%20Crime%20is%20one%20of%20the%20greatest%20threats%20to%20urban%20security.%20Around%2080%20percent%20of%0Athe%20world%27s%20population%20lives%20in%20countries%20with%20high%20levels%20of%20criminality.%20Most%0Aof%20the%20crimes%20committed%20in%20the%20cities%20take%20place%20in%20their%20urban%20environments.%0AThis%20paper%20presents%20the%20development%20and%20validation%20of%20a%20digital%20shadow%20platform%0Afor%20modeling%20and%20simulating%20urban%20crime.%20This%20digital%20shadow%20has%20been%0Aconstructed%20using%20data-driven%20agent-based%20modeling%20and%20simulation%20techniques%2C%0Awhich%20are%20suitable%20for%20capturing%20dynamic%20interactions%20among%20individuals%20and%0Awith%20their%20environment.%20Our%20approach%20transforms%20and%20integrates%20well-known%0Acriminological%20theories%20and%20the%20expert%20knowledge%20of%20law%20enforcement%20agencies%0A%28LEA%29%2C%20policy%20makers%2C%20and%20other%20stakeholders%20under%20a%20theoretical%20model%2C%20which%0Ais%20in%20turn%20combined%20with%20real%20crime%2C%20spatial%20%28cartographic%29%20and%20socio-economic%0Adata%20into%20an%20urban%20model%20characterizing%20the%20daily%20behavior%20of%20citizens.%20The%0Adigital%20shadow%20has%20also%20been%20instantiated%20for%20the%20city%20of%20Malaga%2C%20for%20which%20we%0Ahad%20over%20300%2C000%20complaints%20available.%20This%20instance%20has%20been%20calibrated%20with%0Athose%20complaints%20and%20other%20geographic%20and%20socio-economic%20information%20of%20the%0Acity.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20digital%20shadow%20is%20the%20first%20for%20large%0Aurban%20areas%20that%20has%20been%20calibrated%20with%20a%20large%20dataset%20of%20real%20crime%20reports%0Aand%20with%20an%20accurate%20representation%20of%20the%20urban%20environment.%20The%20performance%0Aindicators%20of%20the%20model%20after%20being%20calibrated%2C%20in%20terms%20of%20the%20metrics%20widely%0Aused%20in%20predictive%20policing%2C%20suggest%20that%20our%20simulated%20crime%20generation%0Amatches%20the%20general%20pattern%20of%20crime%20in%20the%20city%20according%20to%20historical%20data.%0AOur%20digital%20shadow%20platform%20could%20be%20an%20interesting%20tool%20for%20modeling%20and%0Apredicting%20criminal%20behavior%20in%20an%20urban%20environment%20on%20a%20daily%20basis%20and%2C%0Athus%2C%20a%20useful%20tool%20for%20policy%20makers%2C%20criminologists%2C%20sociologists%2C%20LEAs%2C%20etc.%0Ato%20study%20and%20prevent%20urban%20crime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Digital%2520Shadow%2520for%2520Modeling%252C%2520Studying%2520and%2520Preventing%2520Urban%2520Crime%26entry.906535625%3DJuan%2520Palma-Borda%2520and%2520Eduardo%2520Guzm%25C3%25A1n%2520and%2520Mar%25C3%25ADa-Victoria%2520Belmonte%26entry.1292438233%3D%2520%2520Crime%2520is%2520one%2520of%2520the%2520greatest%2520threats%2520to%2520urban%2520security.%2520Around%252080%2520percent%2520of%250Athe%2520world%2527s%2520population%2520lives%2520in%2520countries%2520with%2520high%2520levels%2520of%2520criminality.%2520Most%250Aof%2520the%2520crimes%2520committed%2520in%2520the%2520cities%2520take%2520place%2520in%2520their%2520urban%2520environments.%250AThis%2520paper%2520presents%2520the%2520development%2520and%2520validation%2520of%2520a%2520digital%2520shadow%2520platform%250Afor%2520modeling%2520and%2520simulating%2520urban%2520crime.%2520This%2520digital%2520shadow%2520has%2520been%250Aconstructed%2520using%2520data-driven%2520agent-based%2520modeling%2520and%2520simulation%2520techniques%252C%250Awhich%2520are%2520suitable%2520for%2520capturing%2520dynamic%2520interactions%2520among%2520individuals%2520and%250Awith%2520their%2520environment.%2520Our%2520approach%2520transforms%2520and%2520integrates%2520well-known%250Acriminological%2520theories%2520and%2520the%2520expert%2520knowledge%2520of%2520law%2520enforcement%2520agencies%250A%2528LEA%2529%252C%2520policy%2520makers%252C%2520and%2520other%2520stakeholders%2520under%2520a%2520theoretical%2520model%252C%2520which%250Ais%2520in%2520turn%2520combined%2520with%2520real%2520crime%252C%2520spatial%2520%2528cartographic%2529%2520and%2520socio-economic%250Adata%2520into%2520an%2520urban%2520model%2520characterizing%2520the%2520daily%2520behavior%2520of%2520citizens.%2520The%250Adigital%2520shadow%2520has%2520also%2520been%2520instantiated%2520for%2520the%2520city%2520of%2520Malaga%252C%2520for%2520which%2520we%250Ahad%2520over%2520300%252C000%2520complaints%2520available.%2520This%2520instance%2520has%2520been%2520calibrated%2520with%250Athose%2520complaints%2520and%2520other%2520geographic%2520and%2520socio-economic%2520information%2520of%2520the%250Acity.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520digital%2520shadow%2520is%2520the%2520first%2520for%2520large%250Aurban%2520areas%2520that%2520has%2520been%2520calibrated%2520with%2520a%2520large%2520dataset%2520of%2520real%2520crime%2520reports%250Aand%2520with%2520an%2520accurate%2520representation%2520of%2520the%2520urban%2520environment.%2520The%2520performance%250Aindicators%2520of%2520the%2520model%2520after%2520being%2520calibrated%252C%2520in%2520terms%2520of%2520the%2520metrics%2520widely%250Aused%2520in%2520predictive%2520policing%252C%2520suggest%2520that%2520our%2520simulated%2520crime%2520generation%250Amatches%2520the%2520general%2520pattern%2520of%2520crime%2520in%2520the%2520city%2520according%2520to%2520historical%2520data.%250AOur%2520digital%2520shadow%2520platform%2520could%2520be%2520an%2520interesting%2520tool%2520for%2520modeling%2520and%250Apredicting%2520criminal%2520behavior%2520in%2520an%2520urban%2520environment%2520on%2520a%2520daily%2520basis%2520and%252C%250Athus%252C%2520a%2520useful%2520tool%2520for%2520policy%2520makers%252C%2520criminologists%252C%2520sociologists%252C%2520LEAs%252C%2520etc.%250Ato%2520study%2520and%2520prevent%2520urban%2520crime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Digital%20Shadow%20for%20Modeling%2C%20Studying%20and%20Preventing%20Urban%20Crime&entry.906535625=Juan%20Palma-Borda%20and%20Eduardo%20Guzm%C3%A1n%20and%20Mar%C3%ADa-Victoria%20Belmonte&entry.1292438233=%20%20Crime%20is%20one%20of%20the%20greatest%20threats%20to%20urban%20security.%20Around%2080%20percent%20of%0Athe%20world%27s%20population%20lives%20in%20countries%20with%20high%20levels%20of%20criminality.%20Most%0Aof%20the%20crimes%20committed%20in%20the%20cities%20take%20place%20in%20their%20urban%20environments.%0AThis%20paper%20presents%20the%20development%20and%20validation%20of%20a%20digital%20shadow%20platform%0Afor%20modeling%20and%20simulating%20urban%20crime.%20This%20digital%20shadow%20has%20been%0Aconstructed%20using%20data-driven%20agent-based%20modeling%20and%20simulation%20techniques%2C%0Awhich%20are%20suitable%20for%20capturing%20dynamic%20interactions%20among%20individuals%20and%0Awith%20their%20environment.%20Our%20approach%20transforms%20and%20integrates%20well-known%0Acriminological%20theories%20and%20the%20expert%20knowledge%20of%20law%20enforcement%20agencies%0A%28LEA%29%2C%20policy%20makers%2C%20and%20other%20stakeholders%20under%20a%20theoretical%20model%2C%20which%0Ais%20in%20turn%20combined%20with%20real%20crime%2C%20spatial%20%28cartographic%29%20and%20socio-economic%0Adata%20into%20an%20urban%20model%20characterizing%20the%20daily%20behavior%20of%20citizens.%20The%0Adigital%20shadow%20has%20also%20been%20instantiated%20for%20the%20city%20of%20Malaga%2C%20for%20which%20we%0Ahad%20over%20300%2C000%20complaints%20available.%20This%20instance%20has%20been%20calibrated%20with%0Athose%20complaints%20and%20other%20geographic%20and%20socio-economic%20information%20of%20the%0Acity.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20digital%20shadow%20is%20the%20first%20for%20large%0Aurban%20areas%20that%20has%20been%20calibrated%20with%20a%20large%20dataset%20of%20real%20crime%20reports%0Aand%20with%20an%20accurate%20representation%20of%20the%20urban%20environment.%20The%20performance%0Aindicators%20of%20the%20model%20after%20being%20calibrated%2C%20in%20terms%20of%20the%20metrics%20widely%0Aused%20in%20predictive%20policing%2C%20suggest%20that%20our%20simulated%20crime%20generation%0Amatches%20the%20general%20pattern%20of%20crime%20in%20the%20city%20according%20to%20historical%20data.%0AOur%20digital%20shadow%20platform%20could%20be%20an%20interesting%20tool%20for%20modeling%20and%0Apredicting%20criminal%20behavior%20in%20an%20urban%20environment%20on%20a%20daily%20basis%20and%2C%0Athus%2C%20a%20useful%20tool%20for%20policy%20makers%2C%20criminologists%2C%20sociologists%2C%20LEAs%2C%20etc.%0Ato%20study%20and%20prevent%20urban%20crime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04435v1&entry.124074799=Read"},
{"title": "HypeRL: Parameter-Informed Reinforcement Learning for Parametric PDEs", "author": "Nicol\u00f2 Botteghi and Stefania Fresca and Mengwu Guo and Andrea Manzoni", "abstract": "  In this work, we devise a new, general-purpose reinforcement learning\nstrategy for the optimal control of parametric partial differential equations\n(PDEs). Such problems frequently arise in applied sciences and engineering and\nentail a significant complexity when control and/or state variables are\ndistributed in high-dimensional space or depend on varying parameters.\nTraditional numerical methods, relying on either iterative minimization\nalgorithms or dynamic programming, while reliable, often become computationally\ninfeasible. Indeed, in either way, the optimal control problem must be solved\nfor each instance of the parameters, and this is out of reach when dealing with\nhigh-dimensional time-dependent and parametric PDEs. In this paper, we propose\nHypeRL, a deep reinforcement learning (DRL) framework to overcome the\nlimitations shown by traditional methods. HypeRL aims at approximating the\noptimal control policy directly. Specifically, we employ an actor-critic DRL\napproach to learn an optimal feedback control strategy that can generalize\nacross the range of variation of the parameters. To effectively learn such\noptimal control laws, encoding the parameter information into the DRL policy\nand value function neural networks (NNs) is essential. To do so, HypeRL uses\ntwo additional NNs, often called hypernetworks, to learn the weights and biases\nof the value function and the policy NNs. We validate the proposed approach on\ntwo PDE-constrained optimal control benchmarks, namely a 1D\nKuramoto-Sivashinsky equation and a 2D Navier-Stokes equations, by showing that\nthe knowledge of the PDE parameters and how this information is encoded, i.e.,\nvia a hypernetwork, is an essential ingredient for learning parameter-dependent\ncontrol policies that can generalize effectively to unseen scenarios and for\nimproving the sample efficiency of such policies.\n", "link": "http://arxiv.org/abs/2501.04538v1", "date": "2025-01-08", "relevancy": 1.9533, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4815}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HypeRL%3A%20Parameter-Informed%20Reinforcement%20Learning%20for%20Parametric%20PDEs&body=Title%3A%20HypeRL%3A%20Parameter-Informed%20Reinforcement%20Learning%20for%20Parametric%20PDEs%0AAuthor%3A%20Nicol%C3%B2%20Botteghi%20and%20Stefania%20Fresca%20and%20Mengwu%20Guo%20and%20Andrea%20Manzoni%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20devise%20a%20new%2C%20general-purpose%20reinforcement%20learning%0Astrategy%20for%20the%20optimal%20control%20of%20parametric%20partial%20differential%20equations%0A%28PDEs%29.%20Such%20problems%20frequently%20arise%20in%20applied%20sciences%20and%20engineering%20and%0Aentail%20a%20significant%20complexity%20when%20control%20and/or%20state%20variables%20are%0Adistributed%20in%20high-dimensional%20space%20or%20depend%20on%20varying%20parameters.%0ATraditional%20numerical%20methods%2C%20relying%20on%20either%20iterative%20minimization%0Aalgorithms%20or%20dynamic%20programming%2C%20while%20reliable%2C%20often%20become%20computationally%0Ainfeasible.%20Indeed%2C%20in%20either%20way%2C%20the%20optimal%20control%20problem%20must%20be%20solved%0Afor%20each%20instance%20of%20the%20parameters%2C%20and%20this%20is%20out%20of%20reach%20when%20dealing%20with%0Ahigh-dimensional%20time-dependent%20and%20parametric%20PDEs.%20In%20this%20paper%2C%20we%20propose%0AHypeRL%2C%20a%20deep%20reinforcement%20learning%20%28DRL%29%20framework%20to%20overcome%20the%0Alimitations%20shown%20by%20traditional%20methods.%20HypeRL%20aims%20at%20approximating%20the%0Aoptimal%20control%20policy%20directly.%20Specifically%2C%20we%20employ%20an%20actor-critic%20DRL%0Aapproach%20to%20learn%20an%20optimal%20feedback%20control%20strategy%20that%20can%20generalize%0Aacross%20the%20range%20of%20variation%20of%20the%20parameters.%20To%20effectively%20learn%20such%0Aoptimal%20control%20laws%2C%20encoding%20the%20parameter%20information%20into%20the%20DRL%20policy%0Aand%20value%20function%20neural%20networks%20%28NNs%29%20is%20essential.%20To%20do%20so%2C%20HypeRL%20uses%0Atwo%20additional%20NNs%2C%20often%20called%20hypernetworks%2C%20to%20learn%20the%20weights%20and%20biases%0Aof%20the%20value%20function%20and%20the%20policy%20NNs.%20We%20validate%20the%20proposed%20approach%20on%0Atwo%20PDE-constrained%20optimal%20control%20benchmarks%2C%20namely%20a%201D%0AKuramoto-Sivashinsky%20equation%20and%20a%202D%20Navier-Stokes%20equations%2C%20by%20showing%20that%0Athe%20knowledge%20of%20the%20PDE%20parameters%20and%20how%20this%20information%20is%20encoded%2C%20i.e.%2C%0Avia%20a%20hypernetwork%2C%20is%20an%20essential%20ingredient%20for%20learning%20parameter-dependent%0Acontrol%20policies%20that%20can%20generalize%20effectively%20to%20unseen%20scenarios%20and%20for%0Aimproving%20the%20sample%20efficiency%20of%20such%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypeRL%253A%2520Parameter-Informed%2520Reinforcement%2520Learning%2520for%2520Parametric%2520PDEs%26entry.906535625%3DNicol%25C3%25B2%2520Botteghi%2520and%2520Stefania%2520Fresca%2520and%2520Mengwu%2520Guo%2520and%2520Andrea%2520Manzoni%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520devise%2520a%2520new%252C%2520general-purpose%2520reinforcement%2520learning%250Astrategy%2520for%2520the%2520optimal%2520control%2520of%2520parametric%2520partial%2520differential%2520equations%250A%2528PDEs%2529.%2520Such%2520problems%2520frequently%2520arise%2520in%2520applied%2520sciences%2520and%2520engineering%2520and%250Aentail%2520a%2520significant%2520complexity%2520when%2520control%2520and/or%2520state%2520variables%2520are%250Adistributed%2520in%2520high-dimensional%2520space%2520or%2520depend%2520on%2520varying%2520parameters.%250ATraditional%2520numerical%2520methods%252C%2520relying%2520on%2520either%2520iterative%2520minimization%250Aalgorithms%2520or%2520dynamic%2520programming%252C%2520while%2520reliable%252C%2520often%2520become%2520computationally%250Ainfeasible.%2520Indeed%252C%2520in%2520either%2520way%252C%2520the%2520optimal%2520control%2520problem%2520must%2520be%2520solved%250Afor%2520each%2520instance%2520of%2520the%2520parameters%252C%2520and%2520this%2520is%2520out%2520of%2520reach%2520when%2520dealing%2520with%250Ahigh-dimensional%2520time-dependent%2520and%2520parametric%2520PDEs.%2520In%2520this%2520paper%252C%2520we%2520propose%250AHypeRL%252C%2520a%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520framework%2520to%2520overcome%2520the%250Alimitations%2520shown%2520by%2520traditional%2520methods.%2520HypeRL%2520aims%2520at%2520approximating%2520the%250Aoptimal%2520control%2520policy%2520directly.%2520Specifically%252C%2520we%2520employ%2520an%2520actor-critic%2520DRL%250Aapproach%2520to%2520learn%2520an%2520optimal%2520feedback%2520control%2520strategy%2520that%2520can%2520generalize%250Aacross%2520the%2520range%2520of%2520variation%2520of%2520the%2520parameters.%2520To%2520effectively%2520learn%2520such%250Aoptimal%2520control%2520laws%252C%2520encoding%2520the%2520parameter%2520information%2520into%2520the%2520DRL%2520policy%250Aand%2520value%2520function%2520neural%2520networks%2520%2528NNs%2529%2520is%2520essential.%2520To%2520do%2520so%252C%2520HypeRL%2520uses%250Atwo%2520additional%2520NNs%252C%2520often%2520called%2520hypernetworks%252C%2520to%2520learn%2520the%2520weights%2520and%2520biases%250Aof%2520the%2520value%2520function%2520and%2520the%2520policy%2520NNs.%2520We%2520validate%2520the%2520proposed%2520approach%2520on%250Atwo%2520PDE-constrained%2520optimal%2520control%2520benchmarks%252C%2520namely%2520a%25201D%250AKuramoto-Sivashinsky%2520equation%2520and%2520a%25202D%2520Navier-Stokes%2520equations%252C%2520by%2520showing%2520that%250Athe%2520knowledge%2520of%2520the%2520PDE%2520parameters%2520and%2520how%2520this%2520information%2520is%2520encoded%252C%2520i.e.%252C%250Avia%2520a%2520hypernetwork%252C%2520is%2520an%2520essential%2520ingredient%2520for%2520learning%2520parameter-dependent%250Acontrol%2520policies%2520that%2520can%2520generalize%2520effectively%2520to%2520unseen%2520scenarios%2520and%2520for%250Aimproving%2520the%2520sample%2520efficiency%2520of%2520such%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HypeRL%3A%20Parameter-Informed%20Reinforcement%20Learning%20for%20Parametric%20PDEs&entry.906535625=Nicol%C3%B2%20Botteghi%20and%20Stefania%20Fresca%20and%20Mengwu%20Guo%20and%20Andrea%20Manzoni&entry.1292438233=%20%20In%20this%20work%2C%20we%20devise%20a%20new%2C%20general-purpose%20reinforcement%20learning%0Astrategy%20for%20the%20optimal%20control%20of%20parametric%20partial%20differential%20equations%0A%28PDEs%29.%20Such%20problems%20frequently%20arise%20in%20applied%20sciences%20and%20engineering%20and%0Aentail%20a%20significant%20complexity%20when%20control%20and/or%20state%20variables%20are%0Adistributed%20in%20high-dimensional%20space%20or%20depend%20on%20varying%20parameters.%0ATraditional%20numerical%20methods%2C%20relying%20on%20either%20iterative%20minimization%0Aalgorithms%20or%20dynamic%20programming%2C%20while%20reliable%2C%20often%20become%20computationally%0Ainfeasible.%20Indeed%2C%20in%20either%20way%2C%20the%20optimal%20control%20problem%20must%20be%20solved%0Afor%20each%20instance%20of%20the%20parameters%2C%20and%20this%20is%20out%20of%20reach%20when%20dealing%20with%0Ahigh-dimensional%20time-dependent%20and%20parametric%20PDEs.%20In%20this%20paper%2C%20we%20propose%0AHypeRL%2C%20a%20deep%20reinforcement%20learning%20%28DRL%29%20framework%20to%20overcome%20the%0Alimitations%20shown%20by%20traditional%20methods.%20HypeRL%20aims%20at%20approximating%20the%0Aoptimal%20control%20policy%20directly.%20Specifically%2C%20we%20employ%20an%20actor-critic%20DRL%0Aapproach%20to%20learn%20an%20optimal%20feedback%20control%20strategy%20that%20can%20generalize%0Aacross%20the%20range%20of%20variation%20of%20the%20parameters.%20To%20effectively%20learn%20such%0Aoptimal%20control%20laws%2C%20encoding%20the%20parameter%20information%20into%20the%20DRL%20policy%0Aand%20value%20function%20neural%20networks%20%28NNs%29%20is%20essential.%20To%20do%20so%2C%20HypeRL%20uses%0Atwo%20additional%20NNs%2C%20often%20called%20hypernetworks%2C%20to%20learn%20the%20weights%20and%20biases%0Aof%20the%20value%20function%20and%20the%20policy%20NNs.%20We%20validate%20the%20proposed%20approach%20on%0Atwo%20PDE-constrained%20optimal%20control%20benchmarks%2C%20namely%20a%201D%0AKuramoto-Sivashinsky%20equation%20and%20a%202D%20Navier-Stokes%20equations%2C%20by%20showing%20that%0Athe%20knowledge%20of%20the%20PDE%20parameters%20and%20how%20this%20information%20is%20encoded%2C%20i.e.%2C%0Avia%20a%20hypernetwork%2C%20is%20an%20essential%20ingredient%20for%20learning%20parameter-dependent%0Acontrol%20policies%20that%20can%20generalize%20effectively%20to%20unseen%20scenarios%20and%20for%0Aimproving%20the%20sample%20efficiency%20of%20such%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04538v1&entry.124074799=Read"},
{"title": "Combining YOLO and Visual Rhythm for Vehicle Counting", "author": "Victor Nascimento Ribeiro and Nina S. T. Hirata", "abstract": "  Video-based vehicle detection and counting play a critical role in managing\ntransport infrastructure. Traditional image-based counting methods usually\ninvolve two main steps: initial detection and subsequent tracking, which are\napplied to all video frames, leading to a significant increase in computational\ncomplexity. To address this issue, this work presents an alternative and more\nefficient method for vehicle detection and counting. The proposed approach\neliminates the need for a tracking step and focuses solely on detecting\nvehicles in key video frames, thereby increasing its efficiency. To achieve\nthis, we developed a system that combines YOLO, for vehicle detection, with\nVisual Rhythm, a way to create time-spatial images that allows us to focus on\nframes that contain useful information. Additionally, this method can be used\nfor counting in any application involving unidirectional moving targets to be\ndetected and identified. Experimental analysis using real videos shows that the\nproposed method achieves mean counting accuracy around 99.15% over a set of\nvideos, with a processing speed three times faster than tracking based\napproaches.\n", "link": "http://arxiv.org/abs/2501.04534v1", "date": "2025-01-08", "relevancy": 1.948, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4945}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4917}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20YOLO%20and%20Visual%20Rhythm%20for%20Vehicle%20Counting&body=Title%3A%20Combining%20YOLO%20and%20Visual%20Rhythm%20for%20Vehicle%20Counting%0AAuthor%3A%20Victor%20Nascimento%20Ribeiro%20and%20Nina%20S.%20T.%20Hirata%0AAbstract%3A%20%20%20Video-based%20vehicle%20detection%20and%20counting%20play%20a%20critical%20role%20in%20managing%0Atransport%20infrastructure.%20Traditional%20image-based%20counting%20methods%20usually%0Ainvolve%20two%20main%20steps%3A%20initial%20detection%20and%20subsequent%20tracking%2C%20which%20are%0Aapplied%20to%20all%20video%20frames%2C%20leading%20to%20a%20significant%20increase%20in%20computational%0Acomplexity.%20To%20address%20this%20issue%2C%20this%20work%20presents%20an%20alternative%20and%20more%0Aefficient%20method%20for%20vehicle%20detection%20and%20counting.%20The%20proposed%20approach%0Aeliminates%20the%20need%20for%20a%20tracking%20step%20and%20focuses%20solely%20on%20detecting%0Avehicles%20in%20key%20video%20frames%2C%20thereby%20increasing%20its%20efficiency.%20To%20achieve%0Athis%2C%20we%20developed%20a%20system%20that%20combines%20YOLO%2C%20for%20vehicle%20detection%2C%20with%0AVisual%20Rhythm%2C%20a%20way%20to%20create%20time-spatial%20images%20that%20allows%20us%20to%20focus%20on%0Aframes%20that%20contain%20useful%20information.%20Additionally%2C%20this%20method%20can%20be%20used%0Afor%20counting%20in%20any%20application%20involving%20unidirectional%20moving%20targets%20to%20be%0Adetected%20and%20identified.%20Experimental%20analysis%20using%20real%20videos%20shows%20that%20the%0Aproposed%20method%20achieves%20mean%20counting%20accuracy%20around%2099.15%25%20over%20a%20set%20of%0Avideos%2C%20with%20a%20processing%20speed%20three%20times%20faster%20than%20tracking%20based%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520YOLO%2520and%2520Visual%2520Rhythm%2520for%2520Vehicle%2520Counting%26entry.906535625%3DVictor%2520Nascimento%2520Ribeiro%2520and%2520Nina%2520S.%2520T.%2520Hirata%26entry.1292438233%3D%2520%2520Video-based%2520vehicle%2520detection%2520and%2520counting%2520play%2520a%2520critical%2520role%2520in%2520managing%250Atransport%2520infrastructure.%2520Traditional%2520image-based%2520counting%2520methods%2520usually%250Ainvolve%2520two%2520main%2520steps%253A%2520initial%2520detection%2520and%2520subsequent%2520tracking%252C%2520which%2520are%250Aapplied%2520to%2520all%2520video%2520frames%252C%2520leading%2520to%2520a%2520significant%2520increase%2520in%2520computational%250Acomplexity.%2520To%2520address%2520this%2520issue%252C%2520this%2520work%2520presents%2520an%2520alternative%2520and%2520more%250Aefficient%2520method%2520for%2520vehicle%2520detection%2520and%2520counting.%2520The%2520proposed%2520approach%250Aeliminates%2520the%2520need%2520for%2520a%2520tracking%2520step%2520and%2520focuses%2520solely%2520on%2520detecting%250Avehicles%2520in%2520key%2520video%2520frames%252C%2520thereby%2520increasing%2520its%2520efficiency.%2520To%2520achieve%250Athis%252C%2520we%2520developed%2520a%2520system%2520that%2520combines%2520YOLO%252C%2520for%2520vehicle%2520detection%252C%2520with%250AVisual%2520Rhythm%252C%2520a%2520way%2520to%2520create%2520time-spatial%2520images%2520that%2520allows%2520us%2520to%2520focus%2520on%250Aframes%2520that%2520contain%2520useful%2520information.%2520Additionally%252C%2520this%2520method%2520can%2520be%2520used%250Afor%2520counting%2520in%2520any%2520application%2520involving%2520unidirectional%2520moving%2520targets%2520to%2520be%250Adetected%2520and%2520identified.%2520Experimental%2520analysis%2520using%2520real%2520videos%2520shows%2520that%2520the%250Aproposed%2520method%2520achieves%2520mean%2520counting%2520accuracy%2520around%252099.15%2525%2520over%2520a%2520set%2520of%250Avideos%252C%2520with%2520a%2520processing%2520speed%2520three%2520times%2520faster%2520than%2520tracking%2520based%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20YOLO%20and%20Visual%20Rhythm%20for%20Vehicle%20Counting&entry.906535625=Victor%20Nascimento%20Ribeiro%20and%20Nina%20S.%20T.%20Hirata&entry.1292438233=%20%20Video-based%20vehicle%20detection%20and%20counting%20play%20a%20critical%20role%20in%20managing%0Atransport%20infrastructure.%20Traditional%20image-based%20counting%20methods%20usually%0Ainvolve%20two%20main%20steps%3A%20initial%20detection%20and%20subsequent%20tracking%2C%20which%20are%0Aapplied%20to%20all%20video%20frames%2C%20leading%20to%20a%20significant%20increase%20in%20computational%0Acomplexity.%20To%20address%20this%20issue%2C%20this%20work%20presents%20an%20alternative%20and%20more%0Aefficient%20method%20for%20vehicle%20detection%20and%20counting.%20The%20proposed%20approach%0Aeliminates%20the%20need%20for%20a%20tracking%20step%20and%20focuses%20solely%20on%20detecting%0Avehicles%20in%20key%20video%20frames%2C%20thereby%20increasing%20its%20efficiency.%20To%20achieve%0Athis%2C%20we%20developed%20a%20system%20that%20combines%20YOLO%2C%20for%20vehicle%20detection%2C%20with%0AVisual%20Rhythm%2C%20a%20way%20to%20create%20time-spatial%20images%20that%20allows%20us%20to%20focus%20on%0Aframes%20that%20contain%20useful%20information.%20Additionally%2C%20this%20method%20can%20be%20used%0Afor%20counting%20in%20any%20application%20involving%20unidirectional%20moving%20targets%20to%20be%0Adetected%20and%20identified.%20Experimental%20analysis%20using%20real%20videos%20shows%20that%20the%0Aproposed%20method%20achieves%20mean%20counting%20accuracy%20around%2099.15%25%20over%20a%20set%20of%0Avideos%2C%20with%20a%20processing%20speed%20three%20times%20faster%20than%20tracking%20based%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04534v1&entry.124074799=Read"},
{"title": "Multi-Fidelity Bayesian Optimization With Across-Task Transferable\n  Max-Value Entropy Search", "author": "Yunchuan Zhang and Sangwoo Park and Osvaldo Simeone", "abstract": "  In many applications, ranging from logistics to engineering, a designer is\nfaced with a sequence of optimization tasks for which the objectives are in the\nform of black-box functions that are costly to evaluate. Furthermore,\nhigher-fidelity evaluations of the optimization objectives often entail a\nlarger cost. Existing multi-fidelity black-box optimization strategies select\ncandidate solutions and fidelity levels with the goal of maximizing the\ninformation about the optimal value or the optimal solution for the current\ntask. Assuming that successive optimization tasks are related, this paper\nintroduces a novel information-theoretic acquisition function that balances the\nneed to acquire information about the current task with the goal of collecting\ninformation transferable to future tasks. The proposed method transfers across\ntasks distributions over parameters of a Gaussian process surrogate model by\nimplementing particle-based variational Bayesian updates. Theoretical insights\nbased on the analysis of the expected regret substantiate the benefits of\nacquiring transferable knowledge across tasks. Furthermore, experimental\nresults across synthetic and real-world examples reveal that the proposed\nacquisition strategy that caters to future tasks can significantly improve the\noptimization efficiency as soon as a sufficient number of tasks is processed.\n", "link": "http://arxiv.org/abs/2403.09570v4", "date": "2025-01-08", "relevancy": 1.9466, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Fidelity%20Bayesian%20Optimization%20With%20Across-Task%20Transferable%0A%20%20Max-Value%20Entropy%20Search&body=Title%3A%20Multi-Fidelity%20Bayesian%20Optimization%20With%20Across-Task%20Transferable%0A%20%20Max-Value%20Entropy%20Search%0AAuthor%3A%20Yunchuan%20Zhang%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20In%20many%20applications%2C%20ranging%20from%20logistics%20to%20engineering%2C%20a%20designer%20is%0Afaced%20with%20a%20sequence%20of%20optimization%20tasks%20for%20which%20the%20objectives%20are%20in%20the%0Aform%20of%20black-box%20functions%20that%20are%20costly%20to%20evaluate.%20Furthermore%2C%0Ahigher-fidelity%20evaluations%20of%20the%20optimization%20objectives%20often%20entail%20a%0Alarger%20cost.%20Existing%20multi-fidelity%20black-box%20optimization%20strategies%20select%0Acandidate%20solutions%20and%20fidelity%20levels%20with%20the%20goal%20of%20maximizing%20the%0Ainformation%20about%20the%20optimal%20value%20or%20the%20optimal%20solution%20for%20the%20current%0Atask.%20Assuming%20that%20successive%20optimization%20tasks%20are%20related%2C%20this%20paper%0Aintroduces%20a%20novel%20information-theoretic%20acquisition%20function%20that%20balances%20the%0Aneed%20to%20acquire%20information%20about%20the%20current%20task%20with%20the%20goal%20of%20collecting%0Ainformation%20transferable%20to%20future%20tasks.%20The%20proposed%20method%20transfers%20across%0Atasks%20distributions%20over%20parameters%20of%20a%20Gaussian%20process%20surrogate%20model%20by%0Aimplementing%20particle-based%20variational%20Bayesian%20updates.%20Theoretical%20insights%0Abased%20on%20the%20analysis%20of%20the%20expected%20regret%20substantiate%20the%20benefits%20of%0Aacquiring%20transferable%20knowledge%20across%20tasks.%20Furthermore%2C%20experimental%0Aresults%20across%20synthetic%20and%20real-world%20examples%20reveal%20that%20the%20proposed%0Aacquisition%20strategy%20that%20caters%20to%20future%20tasks%20can%20significantly%20improve%20the%0Aoptimization%20efficiency%20as%20soon%20as%20a%20sufficient%20number%20of%20tasks%20is%20processed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09570v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Fidelity%2520Bayesian%2520Optimization%2520With%2520Across-Task%2520Transferable%250A%2520%2520Max-Value%2520Entropy%2520Search%26entry.906535625%3DYunchuan%2520Zhang%2520and%2520Sangwoo%2520Park%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520In%2520many%2520applications%252C%2520ranging%2520from%2520logistics%2520to%2520engineering%252C%2520a%2520designer%2520is%250Afaced%2520with%2520a%2520sequence%2520of%2520optimization%2520tasks%2520for%2520which%2520the%2520objectives%2520are%2520in%2520the%250Aform%2520of%2520black-box%2520functions%2520that%2520are%2520costly%2520to%2520evaluate.%2520Furthermore%252C%250Ahigher-fidelity%2520evaluations%2520of%2520the%2520optimization%2520objectives%2520often%2520entail%2520a%250Alarger%2520cost.%2520Existing%2520multi-fidelity%2520black-box%2520optimization%2520strategies%2520select%250Acandidate%2520solutions%2520and%2520fidelity%2520levels%2520with%2520the%2520goal%2520of%2520maximizing%2520the%250Ainformation%2520about%2520the%2520optimal%2520value%2520or%2520the%2520optimal%2520solution%2520for%2520the%2520current%250Atask.%2520Assuming%2520that%2520successive%2520optimization%2520tasks%2520are%2520related%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520information-theoretic%2520acquisition%2520function%2520that%2520balances%2520the%250Aneed%2520to%2520acquire%2520information%2520about%2520the%2520current%2520task%2520with%2520the%2520goal%2520of%2520collecting%250Ainformation%2520transferable%2520to%2520future%2520tasks.%2520The%2520proposed%2520method%2520transfers%2520across%250Atasks%2520distributions%2520over%2520parameters%2520of%2520a%2520Gaussian%2520process%2520surrogate%2520model%2520by%250Aimplementing%2520particle-based%2520variational%2520Bayesian%2520updates.%2520Theoretical%2520insights%250Abased%2520on%2520the%2520analysis%2520of%2520the%2520expected%2520regret%2520substantiate%2520the%2520benefits%2520of%250Aacquiring%2520transferable%2520knowledge%2520across%2520tasks.%2520Furthermore%252C%2520experimental%250Aresults%2520across%2520synthetic%2520and%2520real-world%2520examples%2520reveal%2520that%2520the%2520proposed%250Aacquisition%2520strategy%2520that%2520caters%2520to%2520future%2520tasks%2520can%2520significantly%2520improve%2520the%250Aoptimization%2520efficiency%2520as%2520soon%2520as%2520a%2520sufficient%2520number%2520of%2520tasks%2520is%2520processed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09570v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Fidelity%20Bayesian%20Optimization%20With%20Across-Task%20Transferable%0A%20%20Max-Value%20Entropy%20Search&entry.906535625=Yunchuan%20Zhang%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20In%20many%20applications%2C%20ranging%20from%20logistics%20to%20engineering%2C%20a%20designer%20is%0Afaced%20with%20a%20sequence%20of%20optimization%20tasks%20for%20which%20the%20objectives%20are%20in%20the%0Aform%20of%20black-box%20functions%20that%20are%20costly%20to%20evaluate.%20Furthermore%2C%0Ahigher-fidelity%20evaluations%20of%20the%20optimization%20objectives%20often%20entail%20a%0Alarger%20cost.%20Existing%20multi-fidelity%20black-box%20optimization%20strategies%20select%0Acandidate%20solutions%20and%20fidelity%20levels%20with%20the%20goal%20of%20maximizing%20the%0Ainformation%20about%20the%20optimal%20value%20or%20the%20optimal%20solution%20for%20the%20current%0Atask.%20Assuming%20that%20successive%20optimization%20tasks%20are%20related%2C%20this%20paper%0Aintroduces%20a%20novel%20information-theoretic%20acquisition%20function%20that%20balances%20the%0Aneed%20to%20acquire%20information%20about%20the%20current%20task%20with%20the%20goal%20of%20collecting%0Ainformation%20transferable%20to%20future%20tasks.%20The%20proposed%20method%20transfers%20across%0Atasks%20distributions%20over%20parameters%20of%20a%20Gaussian%20process%20surrogate%20model%20by%0Aimplementing%20particle-based%20variational%20Bayesian%20updates.%20Theoretical%20insights%0Abased%20on%20the%20analysis%20of%20the%20expected%20regret%20substantiate%20the%20benefits%20of%0Aacquiring%20transferable%20knowledge%20across%20tasks.%20Furthermore%2C%20experimental%0Aresults%20across%20synthetic%20and%20real-world%20examples%20reveal%20that%20the%20proposed%0Aacquisition%20strategy%20that%20caters%20to%20future%20tasks%20can%20significantly%20improve%20the%0Aoptimization%20efficiency%20as%20soon%20as%20a%20sufficient%20number%20of%20tasks%20is%20processed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09570v4&entry.124074799=Read"},
{"title": "Towards Realistic Evaluation of Commit Message Generation by Matching\n  Online and Offline Settings", "author": "Petr Tsvetkov and Aleksandra Eliseeva and Danny Dig and Alexander Bezzubov and Yaroslav Golubev and Timofey Bryksin and Yaroslav Zharov", "abstract": "  When a Commit Message Generation (CMG) system is integrated into the IDEs and\nother products at JetBrains, we perform online evaluation based on user\nacceptance of the generated messages. However, performing online experiments\nwith every change to a CMG system is troublesome, as each iteration affects\nusers and requires time to collect enough statistics. On the other hand,\noffline evaluation, a prevalent approach in the research literature,\nfacilitates fast experiments but employs automatic metrics that are not\nguaranteed to represent the preferences of real users. In this work, we\ndescribe a novel way we employed to deal with this problem at JetBrains, by\nleveraging an online metric - the number of edits users introduce before\ncommitting the generated messages to the VCS - to select metrics for offline\nexperiments.\n  To support this new type of evaluation, we develop a novel markup collection\ntool mimicking the real workflow with a CMG system, collect a dataset with 57\npairs consisting of commit messages generated by GPT-4 and their counterparts\nedited by human experts, and design and verify a way to synthetically extend\nsuch a dataset. Then, we use the final dataset of 656 pairs to study how the\nwidely used similarity metrics correlate with the online metric reflecting the\nreal users' experience.\n  Our results indicate that edit distance exhibits the highest correlation with\nthe online metric, whereas commonly used similarity metrics such as BLEU and\nMETEOR demonstrate low correlation. This contradicts the previous studies on\nsimilarity metrics for CMG, suggesting that user interactions with a CMG system\nin real-world settings differ significantly from the responses by human\nlabelers within controlled environments. We release all the code and the\ndataset to support future research in the field: https://jb.gg/cmg-evaluation.\n", "link": "http://arxiv.org/abs/2410.12046v2", "date": "2025-01-08", "relevancy": 1.9197, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4751}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Realistic%20Evaluation%20of%20Commit%20Message%20Generation%20by%20Matching%0A%20%20Online%20and%20Offline%20Settings&body=Title%3A%20Towards%20Realistic%20Evaluation%20of%20Commit%20Message%20Generation%20by%20Matching%0A%20%20Online%20and%20Offline%20Settings%0AAuthor%3A%20Petr%20Tsvetkov%20and%20Aleksandra%20Eliseeva%20and%20Danny%20Dig%20and%20Alexander%20Bezzubov%20and%20Yaroslav%20Golubev%20and%20Timofey%20Bryksin%20and%20Yaroslav%20Zharov%0AAbstract%3A%20%20%20When%20a%20Commit%20Message%20Generation%20%28CMG%29%20system%20is%20integrated%20into%20the%20IDEs%20and%0Aother%20products%20at%20JetBrains%2C%20we%20perform%20online%20evaluation%20based%20on%20user%0Aacceptance%20of%20the%20generated%20messages.%20However%2C%20performing%20online%20experiments%0Awith%20every%20change%20to%20a%20CMG%20system%20is%20troublesome%2C%20as%20each%20iteration%20affects%0Ausers%20and%20requires%20time%20to%20collect%20enough%20statistics.%20On%20the%20other%20hand%2C%0Aoffline%20evaluation%2C%20a%20prevalent%20approach%20in%20the%20research%20literature%2C%0Afacilitates%20fast%20experiments%20but%20employs%20automatic%20metrics%20that%20are%20not%0Aguaranteed%20to%20represent%20the%20preferences%20of%20real%20users.%20In%20this%20work%2C%20we%0Adescribe%20a%20novel%20way%20we%20employed%20to%20deal%20with%20this%20problem%20at%20JetBrains%2C%20by%0Aleveraging%20an%20online%20metric%20-%20the%20number%20of%20edits%20users%20introduce%20before%0Acommitting%20the%20generated%20messages%20to%20the%20VCS%20-%20to%20select%20metrics%20for%20offline%0Aexperiments.%0A%20%20To%20support%20this%20new%20type%20of%20evaluation%2C%20we%20develop%20a%20novel%20markup%20collection%0Atool%20mimicking%20the%20real%20workflow%20with%20a%20CMG%20system%2C%20collect%20a%20dataset%20with%2057%0Apairs%20consisting%20of%20commit%20messages%20generated%20by%20GPT-4%20and%20their%20counterparts%0Aedited%20by%20human%20experts%2C%20and%20design%20and%20verify%20a%20way%20to%20synthetically%20extend%0Asuch%20a%20dataset.%20Then%2C%20we%20use%20the%20final%20dataset%20of%20656%20pairs%20to%20study%20how%20the%0Awidely%20used%20similarity%20metrics%20correlate%20with%20the%20online%20metric%20reflecting%20the%0Areal%20users%27%20experience.%0A%20%20Our%20results%20indicate%20that%20edit%20distance%20exhibits%20the%20highest%20correlation%20with%0Athe%20online%20metric%2C%20whereas%20commonly%20used%20similarity%20metrics%20such%20as%20BLEU%20and%0AMETEOR%20demonstrate%20low%20correlation.%20This%20contradicts%20the%20previous%20studies%20on%0Asimilarity%20metrics%20for%20CMG%2C%20suggesting%20that%20user%20interactions%20with%20a%20CMG%20system%0Ain%20real-world%20settings%20differ%20significantly%20from%20the%20responses%20by%20human%0Alabelers%20within%20controlled%20environments.%20We%20release%20all%20the%20code%20and%20the%0Adataset%20to%20support%20future%20research%20in%20the%20field%3A%20https%3A//jb.gg/cmg-evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Realistic%2520Evaluation%2520of%2520Commit%2520Message%2520Generation%2520by%2520Matching%250A%2520%2520Online%2520and%2520Offline%2520Settings%26entry.906535625%3DPetr%2520Tsvetkov%2520and%2520Aleksandra%2520Eliseeva%2520and%2520Danny%2520Dig%2520and%2520Alexander%2520Bezzubov%2520and%2520Yaroslav%2520Golubev%2520and%2520Timofey%2520Bryksin%2520and%2520Yaroslav%2520Zharov%26entry.1292438233%3D%2520%2520When%2520a%2520Commit%2520Message%2520Generation%2520%2528CMG%2529%2520system%2520is%2520integrated%2520into%2520the%2520IDEs%2520and%250Aother%2520products%2520at%2520JetBrains%252C%2520we%2520perform%2520online%2520evaluation%2520based%2520on%2520user%250Aacceptance%2520of%2520the%2520generated%2520messages.%2520However%252C%2520performing%2520online%2520experiments%250Awith%2520every%2520change%2520to%2520a%2520CMG%2520system%2520is%2520troublesome%252C%2520as%2520each%2520iteration%2520affects%250Ausers%2520and%2520requires%2520time%2520to%2520collect%2520enough%2520statistics.%2520On%2520the%2520other%2520hand%252C%250Aoffline%2520evaluation%252C%2520a%2520prevalent%2520approach%2520in%2520the%2520research%2520literature%252C%250Afacilitates%2520fast%2520experiments%2520but%2520employs%2520automatic%2520metrics%2520that%2520are%2520not%250Aguaranteed%2520to%2520represent%2520the%2520preferences%2520of%2520real%2520users.%2520In%2520this%2520work%252C%2520we%250Adescribe%2520a%2520novel%2520way%2520we%2520employed%2520to%2520deal%2520with%2520this%2520problem%2520at%2520JetBrains%252C%2520by%250Aleveraging%2520an%2520online%2520metric%2520-%2520the%2520number%2520of%2520edits%2520users%2520introduce%2520before%250Acommitting%2520the%2520generated%2520messages%2520to%2520the%2520VCS%2520-%2520to%2520select%2520metrics%2520for%2520offline%250Aexperiments.%250A%2520%2520To%2520support%2520this%2520new%2520type%2520of%2520evaluation%252C%2520we%2520develop%2520a%2520novel%2520markup%2520collection%250Atool%2520mimicking%2520the%2520real%2520workflow%2520with%2520a%2520CMG%2520system%252C%2520collect%2520a%2520dataset%2520with%252057%250Apairs%2520consisting%2520of%2520commit%2520messages%2520generated%2520by%2520GPT-4%2520and%2520their%2520counterparts%250Aedited%2520by%2520human%2520experts%252C%2520and%2520design%2520and%2520verify%2520a%2520way%2520to%2520synthetically%2520extend%250Asuch%2520a%2520dataset.%2520Then%252C%2520we%2520use%2520the%2520final%2520dataset%2520of%2520656%2520pairs%2520to%2520study%2520how%2520the%250Awidely%2520used%2520similarity%2520metrics%2520correlate%2520with%2520the%2520online%2520metric%2520reflecting%2520the%250Areal%2520users%2527%2520experience.%250A%2520%2520Our%2520results%2520indicate%2520that%2520edit%2520distance%2520exhibits%2520the%2520highest%2520correlation%2520with%250Athe%2520online%2520metric%252C%2520whereas%2520commonly%2520used%2520similarity%2520metrics%2520such%2520as%2520BLEU%2520and%250AMETEOR%2520demonstrate%2520low%2520correlation.%2520This%2520contradicts%2520the%2520previous%2520studies%2520on%250Asimilarity%2520metrics%2520for%2520CMG%252C%2520suggesting%2520that%2520user%2520interactions%2520with%2520a%2520CMG%2520system%250Ain%2520real-world%2520settings%2520differ%2520significantly%2520from%2520the%2520responses%2520by%2520human%250Alabelers%2520within%2520controlled%2520environments.%2520We%2520release%2520all%2520the%2520code%2520and%2520the%250Adataset%2520to%2520support%2520future%2520research%2520in%2520the%2520field%253A%2520https%253A//jb.gg/cmg-evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Realistic%20Evaluation%20of%20Commit%20Message%20Generation%20by%20Matching%0A%20%20Online%20and%20Offline%20Settings&entry.906535625=Petr%20Tsvetkov%20and%20Aleksandra%20Eliseeva%20and%20Danny%20Dig%20and%20Alexander%20Bezzubov%20and%20Yaroslav%20Golubev%20and%20Timofey%20Bryksin%20and%20Yaroslav%20Zharov&entry.1292438233=%20%20When%20a%20Commit%20Message%20Generation%20%28CMG%29%20system%20is%20integrated%20into%20the%20IDEs%20and%0Aother%20products%20at%20JetBrains%2C%20we%20perform%20online%20evaluation%20based%20on%20user%0Aacceptance%20of%20the%20generated%20messages.%20However%2C%20performing%20online%20experiments%0Awith%20every%20change%20to%20a%20CMG%20system%20is%20troublesome%2C%20as%20each%20iteration%20affects%0Ausers%20and%20requires%20time%20to%20collect%20enough%20statistics.%20On%20the%20other%20hand%2C%0Aoffline%20evaluation%2C%20a%20prevalent%20approach%20in%20the%20research%20literature%2C%0Afacilitates%20fast%20experiments%20but%20employs%20automatic%20metrics%20that%20are%20not%0Aguaranteed%20to%20represent%20the%20preferences%20of%20real%20users.%20In%20this%20work%2C%20we%0Adescribe%20a%20novel%20way%20we%20employed%20to%20deal%20with%20this%20problem%20at%20JetBrains%2C%20by%0Aleveraging%20an%20online%20metric%20-%20the%20number%20of%20edits%20users%20introduce%20before%0Acommitting%20the%20generated%20messages%20to%20the%20VCS%20-%20to%20select%20metrics%20for%20offline%0Aexperiments.%0A%20%20To%20support%20this%20new%20type%20of%20evaluation%2C%20we%20develop%20a%20novel%20markup%20collection%0Atool%20mimicking%20the%20real%20workflow%20with%20a%20CMG%20system%2C%20collect%20a%20dataset%20with%2057%0Apairs%20consisting%20of%20commit%20messages%20generated%20by%20GPT-4%20and%20their%20counterparts%0Aedited%20by%20human%20experts%2C%20and%20design%20and%20verify%20a%20way%20to%20synthetically%20extend%0Asuch%20a%20dataset.%20Then%2C%20we%20use%20the%20final%20dataset%20of%20656%20pairs%20to%20study%20how%20the%0Awidely%20used%20similarity%20metrics%20correlate%20with%20the%20online%20metric%20reflecting%20the%0Areal%20users%27%20experience.%0A%20%20Our%20results%20indicate%20that%20edit%20distance%20exhibits%20the%20highest%20correlation%20with%0Athe%20online%20metric%2C%20whereas%20commonly%20used%20similarity%20metrics%20such%20as%20BLEU%20and%0AMETEOR%20demonstrate%20low%20correlation.%20This%20contradicts%20the%20previous%20studies%20on%0Asimilarity%20metrics%20for%20CMG%2C%20suggesting%20that%20user%20interactions%20with%20a%20CMG%20system%0Ain%20real-world%20settings%20differ%20significantly%20from%20the%20responses%20by%20human%0Alabelers%20within%20controlled%20environments.%20We%20release%20all%20the%20code%20and%20the%0Adataset%20to%20support%20future%20research%20in%20the%20field%3A%20https%3A//jb.gg/cmg-evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12046v2&entry.124074799=Read"},
{"title": "NSA: Neuro-symbolic ARC Challenge", "author": "Pawe\u0142 Batorski and Jannik Brinkmann and Paul Swoboda", "abstract": "  The Abstraction and Reasoning Corpus (ARC) evaluates general reasoning\ncapabilities that are difficult for both machine learning models and\ncombinatorial search methods. We propose a neuro-symbolic approach that\ncombines a transformer for proposal generation with combinatorial search using\na domain-specific language. The transformer narrows the search space by\nproposing promising search directions, which allows the combinatorial search to\nfind the actual solution in short time. We pre-train the trainsformer with\nsynthetically generated data. During test-time we generate additional\ntask-specific training tasks and fine-tune our model. Our results surpass\ncomparable state of the art on the ARC evaluation set by 27% and compare\nfavourably on the ARC train set. We make our code and dataset publicly\navailable at https://github.com/Batorskq/NSA.\n", "link": "http://arxiv.org/abs/2501.04424v1", "date": "2025-01-08", "relevancy": 1.9196, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NSA%3A%20Neuro-symbolic%20ARC%20Challenge&body=Title%3A%20NSA%3A%20Neuro-symbolic%20ARC%20Challenge%0AAuthor%3A%20Pawe%C5%82%20Batorski%20and%20Jannik%20Brinkmann%20and%20Paul%20Swoboda%0AAbstract%3A%20%20%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20evaluates%20general%20reasoning%0Acapabilities%20that%20are%20difficult%20for%20both%20machine%20learning%20models%20and%0Acombinatorial%20search%20methods.%20We%20propose%20a%20neuro-symbolic%20approach%20that%0Acombines%20a%20transformer%20for%20proposal%20generation%20with%20combinatorial%20search%20using%0Aa%20domain-specific%20language.%20The%20transformer%20narrows%20the%20search%20space%20by%0Aproposing%20promising%20search%20directions%2C%20which%20allows%20the%20combinatorial%20search%20to%0Afind%20the%20actual%20solution%20in%20short%20time.%20We%20pre-train%20the%20trainsformer%20with%0Asynthetically%20generated%20data.%20During%20test-time%20we%20generate%20additional%0Atask-specific%20training%20tasks%20and%20fine-tune%20our%20model.%20Our%20results%20surpass%0Acomparable%20state%20of%20the%20art%20on%20the%20ARC%20evaluation%20set%20by%2027%25%20and%20compare%0Afavourably%20on%20the%20ARC%20train%20set.%20We%20make%20our%20code%20and%20dataset%20publicly%0Aavailable%20at%20https%3A//github.com/Batorskq/NSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNSA%253A%2520Neuro-symbolic%2520ARC%2520Challenge%26entry.906535625%3DPawe%25C5%2582%2520Batorski%2520and%2520Jannik%2520Brinkmann%2520and%2520Paul%2520Swoboda%26entry.1292438233%3D%2520%2520The%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520%2528ARC%2529%2520evaluates%2520general%2520reasoning%250Acapabilities%2520that%2520are%2520difficult%2520for%2520both%2520machine%2520learning%2520models%2520and%250Acombinatorial%2520search%2520methods.%2520We%2520propose%2520a%2520neuro-symbolic%2520approach%2520that%250Acombines%2520a%2520transformer%2520for%2520proposal%2520generation%2520with%2520combinatorial%2520search%2520using%250Aa%2520domain-specific%2520language.%2520The%2520transformer%2520narrows%2520the%2520search%2520space%2520by%250Aproposing%2520promising%2520search%2520directions%252C%2520which%2520allows%2520the%2520combinatorial%2520search%2520to%250Afind%2520the%2520actual%2520solution%2520in%2520short%2520time.%2520We%2520pre-train%2520the%2520trainsformer%2520with%250Asynthetically%2520generated%2520data.%2520During%2520test-time%2520we%2520generate%2520additional%250Atask-specific%2520training%2520tasks%2520and%2520fine-tune%2520our%2520model.%2520Our%2520results%2520surpass%250Acomparable%2520state%2520of%2520the%2520art%2520on%2520the%2520ARC%2520evaluation%2520set%2520by%252027%2525%2520and%2520compare%250Afavourably%2520on%2520the%2520ARC%2520train%2520set.%2520We%2520make%2520our%2520code%2520and%2520dataset%2520publicly%250Aavailable%2520at%2520https%253A//github.com/Batorskq/NSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NSA%3A%20Neuro-symbolic%20ARC%20Challenge&entry.906535625=Pawe%C5%82%20Batorski%20and%20Jannik%20Brinkmann%20and%20Paul%20Swoboda&entry.1292438233=%20%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20evaluates%20general%20reasoning%0Acapabilities%20that%20are%20difficult%20for%20both%20machine%20learning%20models%20and%0Acombinatorial%20search%20methods.%20We%20propose%20a%20neuro-symbolic%20approach%20that%0Acombines%20a%20transformer%20for%20proposal%20generation%20with%20combinatorial%20search%20using%0Aa%20domain-specific%20language.%20The%20transformer%20narrows%20the%20search%20space%20by%0Aproposing%20promising%20search%20directions%2C%20which%20allows%20the%20combinatorial%20search%20to%0Afind%20the%20actual%20solution%20in%20short%20time.%20We%20pre-train%20the%20trainsformer%20with%0Asynthetically%20generated%20data.%20During%20test-time%20we%20generate%20additional%0Atask-specific%20training%20tasks%20and%20fine-tune%20our%20model.%20Our%20results%20surpass%0Acomparable%20state%20of%20the%20art%20on%20the%20ARC%20evaluation%20set%20by%2027%25%20and%20compare%0Afavourably%20on%20the%20ARC%20train%20set.%20We%20make%20our%20code%20and%20dataset%20publicly%0Aavailable%20at%20https%3A//github.com/Batorskq/NSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04424v1&entry.124074799=Read"},
{"title": "Rad4XCNN: a new agnostic method for post-hoc global explanation of\n  CNN-derived features by means of radiomics", "author": "Francesco Prinzi and Carmelo Militello and Calogero Zarcaro and Tommaso Vincenzo Bartolotta and Salvatore Gaglio and Salvatore Vitabile", "abstract": "  In recent years, machine learning-based clinical decision support systems\n(CDSS) have played a key role in the analysis of several medical conditions.\nDespite their promising capabilities, the lack of transparency in AI models\nposes significant challenges, particularly in medical contexts where\nreliability is a mandatory aspect. However, it appears that explainability is\ninversely proportional to accuracy. For this reason, achieving transparency\nwithout compromising predictive accuracy remains a key challenge. This paper\npresents a novel method, namely Rad4XCNN, to enhance the predictive power of\nCNN-derived features with the inherent interpretability of radiomic features.\nRad4XCNN diverges from conventional methods based on saliency maps, by\nassociating intelligible meaning to CNN-derived features by means of Radiomics,\noffering new perspectives on explanation methods beyond visualization maps.\nUsing a breast cancer classification task as a case study, we evaluated\nRad4XCNN on ultrasound imaging datasets, including an online dataset and two\nin-house datasets for internal and external validation. Some key results are:\ni) CNN-derived features guarantee more robust accuracy when compared against\nViT-derived and radiomic features; ii) conventional visualization map methods\nfor explanation present several pitfalls; iii) Rad4XCNN does not sacrifice\nmodel accuracy for their explainability; iv) Rad4XCNN provides a global\nexplanation enabling the physician to extract global insights and findings. Our\nmethod can mitigate some concerns related to the explainability-accuracy\ntrade-off. This study highlighted the importance of proposing new methods for\nmodel explanation without affecting their accuracy.\n", "link": "http://arxiv.org/abs/2405.02334v2", "date": "2025-01-08", "relevancy": 1.9158, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rad4XCNN%3A%20a%20new%20agnostic%20method%20for%20post-hoc%20global%20explanation%20of%0A%20%20CNN-derived%20features%20by%20means%20of%20radiomics&body=Title%3A%20Rad4XCNN%3A%20a%20new%20agnostic%20method%20for%20post-hoc%20global%20explanation%20of%0A%20%20CNN-derived%20features%20by%20means%20of%20radiomics%0AAuthor%3A%20Francesco%20Prinzi%20and%20Carmelo%20Militello%20and%20Calogero%20Zarcaro%20and%20Tommaso%20Vincenzo%20Bartolotta%20and%20Salvatore%20Gaglio%20and%20Salvatore%20Vitabile%0AAbstract%3A%20%20%20In%20recent%20years%2C%20machine%20learning-based%20clinical%20decision%20support%20systems%0A%28CDSS%29%20have%20played%20a%20key%20role%20in%20the%20analysis%20of%20several%20medical%20conditions.%0ADespite%20their%20promising%20capabilities%2C%20the%20lack%20of%20transparency%20in%20AI%20models%0Aposes%20significant%20challenges%2C%20particularly%20in%20medical%20contexts%20where%0Areliability%20is%20a%20mandatory%20aspect.%20However%2C%20it%20appears%20that%20explainability%20is%0Ainversely%20proportional%20to%20accuracy.%20For%20this%20reason%2C%20achieving%20transparency%0Awithout%20compromising%20predictive%20accuracy%20remains%20a%20key%20challenge.%20This%20paper%0Apresents%20a%20novel%20method%2C%20namely%20Rad4XCNN%2C%20to%20enhance%20the%20predictive%20power%20of%0ACNN-derived%20features%20with%20the%20inherent%20interpretability%20of%20radiomic%20features.%0ARad4XCNN%20diverges%20from%20conventional%20methods%20based%20on%20saliency%20maps%2C%20by%0Aassociating%20intelligible%20meaning%20to%20CNN-derived%20features%20by%20means%20of%20Radiomics%2C%0Aoffering%20new%20perspectives%20on%20explanation%20methods%20beyond%20visualization%20maps.%0AUsing%20a%20breast%20cancer%20classification%20task%20as%20a%20case%20study%2C%20we%20evaluated%0ARad4XCNN%20on%20ultrasound%20imaging%20datasets%2C%20including%20an%20online%20dataset%20and%20two%0Ain-house%20datasets%20for%20internal%20and%20external%20validation.%20Some%20key%20results%20are%3A%0Ai%29%20CNN-derived%20features%20guarantee%20more%20robust%20accuracy%20when%20compared%20against%0AViT-derived%20and%20radiomic%20features%3B%20ii%29%20conventional%20visualization%20map%20methods%0Afor%20explanation%20present%20several%20pitfalls%3B%20iii%29%20Rad4XCNN%20does%20not%20sacrifice%0Amodel%20accuracy%20for%20their%20explainability%3B%20iv%29%20Rad4XCNN%20provides%20a%20global%0Aexplanation%20enabling%20the%20physician%20to%20extract%20global%20insights%20and%20findings.%20Our%0Amethod%20can%20mitigate%20some%20concerns%20related%20to%20the%20explainability-accuracy%0Atrade-off.%20This%20study%20highlighted%20the%20importance%20of%20proposing%20new%20methods%20for%0Amodel%20explanation%20without%20affecting%20their%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRad4XCNN%253A%2520a%2520new%2520agnostic%2520method%2520for%2520post-hoc%2520global%2520explanation%2520of%250A%2520%2520CNN-derived%2520features%2520by%2520means%2520of%2520radiomics%26entry.906535625%3DFrancesco%2520Prinzi%2520and%2520Carmelo%2520Militello%2520and%2520Calogero%2520Zarcaro%2520and%2520Tommaso%2520Vincenzo%2520Bartolotta%2520and%2520Salvatore%2520Gaglio%2520and%2520Salvatore%2520Vitabile%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520machine%2520learning-based%2520clinical%2520decision%2520support%2520systems%250A%2528CDSS%2529%2520have%2520played%2520a%2520key%2520role%2520in%2520the%2520analysis%2520of%2520several%2520medical%2520conditions.%250ADespite%2520their%2520promising%2520capabilities%252C%2520the%2520lack%2520of%2520transparency%2520in%2520AI%2520models%250Aposes%2520significant%2520challenges%252C%2520particularly%2520in%2520medical%2520contexts%2520where%250Areliability%2520is%2520a%2520mandatory%2520aspect.%2520However%252C%2520it%2520appears%2520that%2520explainability%2520is%250Ainversely%2520proportional%2520to%2520accuracy.%2520For%2520this%2520reason%252C%2520achieving%2520transparency%250Awithout%2520compromising%2520predictive%2520accuracy%2520remains%2520a%2520key%2520challenge.%2520This%2520paper%250Apresents%2520a%2520novel%2520method%252C%2520namely%2520Rad4XCNN%252C%2520to%2520enhance%2520the%2520predictive%2520power%2520of%250ACNN-derived%2520features%2520with%2520the%2520inherent%2520interpretability%2520of%2520radiomic%2520features.%250ARad4XCNN%2520diverges%2520from%2520conventional%2520methods%2520based%2520on%2520saliency%2520maps%252C%2520by%250Aassociating%2520intelligible%2520meaning%2520to%2520CNN-derived%2520features%2520by%2520means%2520of%2520Radiomics%252C%250Aoffering%2520new%2520perspectives%2520on%2520explanation%2520methods%2520beyond%2520visualization%2520maps.%250AUsing%2520a%2520breast%2520cancer%2520classification%2520task%2520as%2520a%2520case%2520study%252C%2520we%2520evaluated%250ARad4XCNN%2520on%2520ultrasound%2520imaging%2520datasets%252C%2520including%2520an%2520online%2520dataset%2520and%2520two%250Ain-house%2520datasets%2520for%2520internal%2520and%2520external%2520validation.%2520Some%2520key%2520results%2520are%253A%250Ai%2529%2520CNN-derived%2520features%2520guarantee%2520more%2520robust%2520accuracy%2520when%2520compared%2520against%250AViT-derived%2520and%2520radiomic%2520features%253B%2520ii%2529%2520conventional%2520visualization%2520map%2520methods%250Afor%2520explanation%2520present%2520several%2520pitfalls%253B%2520iii%2529%2520Rad4XCNN%2520does%2520not%2520sacrifice%250Amodel%2520accuracy%2520for%2520their%2520explainability%253B%2520iv%2529%2520Rad4XCNN%2520provides%2520a%2520global%250Aexplanation%2520enabling%2520the%2520physician%2520to%2520extract%2520global%2520insights%2520and%2520findings.%2520Our%250Amethod%2520can%2520mitigate%2520some%2520concerns%2520related%2520to%2520the%2520explainability-accuracy%250Atrade-off.%2520This%2520study%2520highlighted%2520the%2520importance%2520of%2520proposing%2520new%2520methods%2520for%250Amodel%2520explanation%2520without%2520affecting%2520their%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rad4XCNN%3A%20a%20new%20agnostic%20method%20for%20post-hoc%20global%20explanation%20of%0A%20%20CNN-derived%20features%20by%20means%20of%20radiomics&entry.906535625=Francesco%20Prinzi%20and%20Carmelo%20Militello%20and%20Calogero%20Zarcaro%20and%20Tommaso%20Vincenzo%20Bartolotta%20and%20Salvatore%20Gaglio%20and%20Salvatore%20Vitabile&entry.1292438233=%20%20In%20recent%20years%2C%20machine%20learning-based%20clinical%20decision%20support%20systems%0A%28CDSS%29%20have%20played%20a%20key%20role%20in%20the%20analysis%20of%20several%20medical%20conditions.%0ADespite%20their%20promising%20capabilities%2C%20the%20lack%20of%20transparency%20in%20AI%20models%0Aposes%20significant%20challenges%2C%20particularly%20in%20medical%20contexts%20where%0Areliability%20is%20a%20mandatory%20aspect.%20However%2C%20it%20appears%20that%20explainability%20is%0Ainversely%20proportional%20to%20accuracy.%20For%20this%20reason%2C%20achieving%20transparency%0Awithout%20compromising%20predictive%20accuracy%20remains%20a%20key%20challenge.%20This%20paper%0Apresents%20a%20novel%20method%2C%20namely%20Rad4XCNN%2C%20to%20enhance%20the%20predictive%20power%20of%0ACNN-derived%20features%20with%20the%20inherent%20interpretability%20of%20radiomic%20features.%0ARad4XCNN%20diverges%20from%20conventional%20methods%20based%20on%20saliency%20maps%2C%20by%0Aassociating%20intelligible%20meaning%20to%20CNN-derived%20features%20by%20means%20of%20Radiomics%2C%0Aoffering%20new%20perspectives%20on%20explanation%20methods%20beyond%20visualization%20maps.%0AUsing%20a%20breast%20cancer%20classification%20task%20as%20a%20case%20study%2C%20we%20evaluated%0ARad4XCNN%20on%20ultrasound%20imaging%20datasets%2C%20including%20an%20online%20dataset%20and%20two%0Ain-house%20datasets%20for%20internal%20and%20external%20validation.%20Some%20key%20results%20are%3A%0Ai%29%20CNN-derived%20features%20guarantee%20more%20robust%20accuracy%20when%20compared%20against%0AViT-derived%20and%20radiomic%20features%3B%20ii%29%20conventional%20visualization%20map%20methods%0Afor%20explanation%20present%20several%20pitfalls%3B%20iii%29%20Rad4XCNN%20does%20not%20sacrifice%0Amodel%20accuracy%20for%20their%20explainability%3B%20iv%29%20Rad4XCNN%20provides%20a%20global%0Aexplanation%20enabling%20the%20physician%20to%20extract%20global%20insights%20and%20findings.%20Our%0Amethod%20can%20mitigate%20some%20concerns%20related%20to%20the%20explainability-accuracy%0Atrade-off.%20This%20study%20highlighted%20the%20importance%20of%20proposing%20new%20methods%20for%0Amodel%20explanation%20without%20affecting%20their%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02334v2&entry.124074799=Read"},
{"title": "Implementation Of Wildlife Observation System", "author": "Neethu K N and Rakshitha Y Nayak and  Rashmi and Meghana S", "abstract": "  By entering the habitats of wild animals, wildlife watchers can engage\nclosely with them. There are some wild animals that are not always safe to\napproach. Therefore, we suggest this system for observing wildlife. Android\nphones can be used by users to see live events. Wildlife observers can thus get\na close-up view of wild animals by employing this robotic vehicle. The commands\nare delivered to the system via a Wi-Fi module. As we developed the technology\nto enable our robot to deal with the challenges of maintaining continuous\nsurveillance of a target, we found that our robot needed to be able to move\nsilently and purposefully when monitoring a natural target without being\nnoticed. After processing the data, the computer sends commands to the motors\nto turn on. The driver motors, which deliver the essential signal outputs to\ndrive the vehicle movement, are now in charge of driving the motors.\n", "link": "http://arxiv.org/abs/2501.04398v1", "date": "2025-01-08", "relevancy": 1.8965, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.497}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4635}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implementation%20Of%20Wildlife%20Observation%20System&body=Title%3A%20Implementation%20Of%20Wildlife%20Observation%20System%0AAuthor%3A%20Neethu%20K%20N%20and%20Rakshitha%20Y%20Nayak%20and%20%20Rashmi%20and%20Meghana%20S%0AAbstract%3A%20%20%20By%20entering%20the%20habitats%20of%20wild%20animals%2C%20wildlife%20watchers%20can%20engage%0Aclosely%20with%20them.%20There%20are%20some%20wild%20animals%20that%20are%20not%20always%20safe%20to%0Aapproach.%20Therefore%2C%20we%20suggest%20this%20system%20for%20observing%20wildlife.%20Android%0Aphones%20can%20be%20used%20by%20users%20to%20see%20live%20events.%20Wildlife%20observers%20can%20thus%20get%0Aa%20close-up%20view%20of%20wild%20animals%20by%20employing%20this%20robotic%20vehicle.%20The%20commands%0Aare%20delivered%20to%20the%20system%20via%20a%20Wi-Fi%20module.%20As%20we%20developed%20the%20technology%0Ato%20enable%20our%20robot%20to%20deal%20with%20the%20challenges%20of%20maintaining%20continuous%0Asurveillance%20of%20a%20target%2C%20we%20found%20that%20our%20robot%20needed%20to%20be%20able%20to%20move%0Asilently%20and%20purposefully%20when%20monitoring%20a%20natural%20target%20without%20being%0Anoticed.%20After%20processing%20the%20data%2C%20the%20computer%20sends%20commands%20to%20the%20motors%0Ato%20turn%20on.%20The%20driver%20motors%2C%20which%20deliver%20the%20essential%20signal%20outputs%20to%0Adrive%20the%20vehicle%20movement%2C%20are%20now%20in%20charge%20of%20driving%20the%20motors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplementation%2520Of%2520Wildlife%2520Observation%2520System%26entry.906535625%3DNeethu%2520K%2520N%2520and%2520Rakshitha%2520Y%2520Nayak%2520and%2520%2520Rashmi%2520and%2520Meghana%2520S%26entry.1292438233%3D%2520%2520By%2520entering%2520the%2520habitats%2520of%2520wild%2520animals%252C%2520wildlife%2520watchers%2520can%2520engage%250Aclosely%2520with%2520them.%2520There%2520are%2520some%2520wild%2520animals%2520that%2520are%2520not%2520always%2520safe%2520to%250Aapproach.%2520Therefore%252C%2520we%2520suggest%2520this%2520system%2520for%2520observing%2520wildlife.%2520Android%250Aphones%2520can%2520be%2520used%2520by%2520users%2520to%2520see%2520live%2520events.%2520Wildlife%2520observers%2520can%2520thus%2520get%250Aa%2520close-up%2520view%2520of%2520wild%2520animals%2520by%2520employing%2520this%2520robotic%2520vehicle.%2520The%2520commands%250Aare%2520delivered%2520to%2520the%2520system%2520via%2520a%2520Wi-Fi%2520module.%2520As%2520we%2520developed%2520the%2520technology%250Ato%2520enable%2520our%2520robot%2520to%2520deal%2520with%2520the%2520challenges%2520of%2520maintaining%2520continuous%250Asurveillance%2520of%2520a%2520target%252C%2520we%2520found%2520that%2520our%2520robot%2520needed%2520to%2520be%2520able%2520to%2520move%250Asilently%2520and%2520purposefully%2520when%2520monitoring%2520a%2520natural%2520target%2520without%2520being%250Anoticed.%2520After%2520processing%2520the%2520data%252C%2520the%2520computer%2520sends%2520commands%2520to%2520the%2520motors%250Ato%2520turn%2520on.%2520The%2520driver%2520motors%252C%2520which%2520deliver%2520the%2520essential%2520signal%2520outputs%2520to%250Adrive%2520the%2520vehicle%2520movement%252C%2520are%2520now%2520in%2520charge%2520of%2520driving%2520the%2520motors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implementation%20Of%20Wildlife%20Observation%20System&entry.906535625=Neethu%20K%20N%20and%20Rakshitha%20Y%20Nayak%20and%20%20Rashmi%20and%20Meghana%20S&entry.1292438233=%20%20By%20entering%20the%20habitats%20of%20wild%20animals%2C%20wildlife%20watchers%20can%20engage%0Aclosely%20with%20them.%20There%20are%20some%20wild%20animals%20that%20are%20not%20always%20safe%20to%0Aapproach.%20Therefore%2C%20we%20suggest%20this%20system%20for%20observing%20wildlife.%20Android%0Aphones%20can%20be%20used%20by%20users%20to%20see%20live%20events.%20Wildlife%20observers%20can%20thus%20get%0Aa%20close-up%20view%20of%20wild%20animals%20by%20employing%20this%20robotic%20vehicle.%20The%20commands%0Aare%20delivered%20to%20the%20system%20via%20a%20Wi-Fi%20module.%20As%20we%20developed%20the%20technology%0Ato%20enable%20our%20robot%20to%20deal%20with%20the%20challenges%20of%20maintaining%20continuous%0Asurveillance%20of%20a%20target%2C%20we%20found%20that%20our%20robot%20needed%20to%20be%20able%20to%20move%0Asilently%20and%20purposefully%20when%20monitoring%20a%20natural%20target%20without%20being%0Anoticed.%20After%20processing%20the%20data%2C%20the%20computer%20sends%20commands%20to%20the%20motors%0Ato%20turn%20on.%20The%20driver%20motors%2C%20which%20deliver%20the%20essential%20signal%20outputs%20to%0Adrive%20the%20vehicle%20movement%2C%20are%20now%20in%20charge%20of%20driving%20the%20motors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04398v1&entry.124074799=Read"},
{"title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving", "author": "Xuewen Luo and Fan Ding and Fengze Yang and Yang Zhou and Junnyong Loo and Hwa Hui Tew and Chenxi Liu", "abstract": "  This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.\n", "link": "http://arxiv.org/abs/2501.03535v2", "date": "2025-01-08", "relevancy": 1.7157, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SenseRAG%3A%20Constructing%20Environmental%20Knowledge%20Bases%20with%20Proactive%0A%20%20Querying%20for%20LLM-Based%20Autonomous%20Driving&body=Title%3A%20SenseRAG%3A%20Constructing%20Environmental%20Knowledge%20Bases%20with%20Proactive%0A%20%20Querying%20for%20LLM-Based%20Autonomous%20Driving%0AAuthor%3A%20Xuewen%20Luo%20and%20Fan%20Ding%20and%20Fengze%20Yang%20and%20Yang%20Zhou%20and%20Junnyong%20Loo%20and%20Hwa%20Hui%20Tew%20and%20Chenxi%20Liu%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20critical%20need%20for%20enhanced%20situational%20awareness%20in%0Aautonomous%20driving%20%28AD%29%20by%20leveraging%20the%20contextual%20reasoning%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29.%20Unlike%20traditional%20perception%20systems%20that%20rely%0Aon%20rigid%2C%20label-based%20annotations%2C%20it%20integrates%20real-time%2C%20multimodal%20sensor%0Adata%20into%20a%20unified%2C%20LLMs-readable%20knowledge%20base%2C%20enabling%20LLMs%20to%20dynamically%0Aunderstand%20and%20respond%20to%20complex%20driving%20environments.%20To%20overcome%20the%0Ainherent%20latency%20and%20modality%20limitations%20of%20LLMs%2C%20a%20proactive%0ARetrieval-Augmented%20Generation%20%28RAG%29%20is%20designed%20for%20AD%2C%20combined%20with%20a%0Achain-of-thought%20prompting%20mechanism%2C%20ensuring%20rapid%20and%20context-rich%0Aunderstanding.%20Experimental%20results%20using%20real-world%20Vehicle-to-everything%0A%28V2X%29%20datasets%20demonstrate%20significant%20improvements%20in%20perception%20and%0Aprediction%20performance%2C%20highlighting%20the%20potential%20of%20this%20framework%20to%20enhance%0Asafety%2C%20adaptability%2C%20and%20decision-making%20in%20next-generation%20AD%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSenseRAG%253A%2520Constructing%2520Environmental%2520Knowledge%2520Bases%2520with%2520Proactive%250A%2520%2520Querying%2520for%2520LLM-Based%2520Autonomous%2520Driving%26entry.906535625%3DXuewen%2520Luo%2520and%2520Fan%2520Ding%2520and%2520Fengze%2520Yang%2520and%2520Yang%2520Zhou%2520and%2520Junnyong%2520Loo%2520and%2520Hwa%2520Hui%2520Tew%2520and%2520Chenxi%2520Liu%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520critical%2520need%2520for%2520enhanced%2520situational%2520awareness%2520in%250Aautonomous%2520driving%2520%2528AD%2529%2520by%2520leveraging%2520the%2520contextual%2520reasoning%2520capabilities%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520Unlike%2520traditional%2520perception%2520systems%2520that%2520rely%250Aon%2520rigid%252C%2520label-based%2520annotations%252C%2520it%2520integrates%2520real-time%252C%2520multimodal%2520sensor%250Adata%2520into%2520a%2520unified%252C%2520LLMs-readable%2520knowledge%2520base%252C%2520enabling%2520LLMs%2520to%2520dynamically%250Aunderstand%2520and%2520respond%2520to%2520complex%2520driving%2520environments.%2520To%2520overcome%2520the%250Ainherent%2520latency%2520and%2520modality%2520limitations%2520of%2520LLMs%252C%2520a%2520proactive%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520is%2520designed%2520for%2520AD%252C%2520combined%2520with%2520a%250Achain-of-thought%2520prompting%2520mechanism%252C%2520ensuring%2520rapid%2520and%2520context-rich%250Aunderstanding.%2520Experimental%2520results%2520using%2520real-world%2520Vehicle-to-everything%250A%2528V2X%2529%2520datasets%2520demonstrate%2520significant%2520improvements%2520in%2520perception%2520and%250Aprediction%2520performance%252C%2520highlighting%2520the%2520potential%2520of%2520this%2520framework%2520to%2520enhance%250Asafety%252C%2520adaptability%252C%2520and%2520decision-making%2520in%2520next-generation%2520AD%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SenseRAG%3A%20Constructing%20Environmental%20Knowledge%20Bases%20with%20Proactive%0A%20%20Querying%20for%20LLM-Based%20Autonomous%20Driving&entry.906535625=Xuewen%20Luo%20and%20Fan%20Ding%20and%20Fengze%20Yang%20and%20Yang%20Zhou%20and%20Junnyong%20Loo%20and%20Hwa%20Hui%20Tew%20and%20Chenxi%20Liu&entry.1292438233=%20%20This%20study%20addresses%20the%20critical%20need%20for%20enhanced%20situational%20awareness%20in%0Aautonomous%20driving%20%28AD%29%20by%20leveraging%20the%20contextual%20reasoning%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29.%20Unlike%20traditional%20perception%20systems%20that%20rely%0Aon%20rigid%2C%20label-based%20annotations%2C%20it%20integrates%20real-time%2C%20multimodal%20sensor%0Adata%20into%20a%20unified%2C%20LLMs-readable%20knowledge%20base%2C%20enabling%20LLMs%20to%20dynamically%0Aunderstand%20and%20respond%20to%20complex%20driving%20environments.%20To%20overcome%20the%0Ainherent%20latency%20and%20modality%20limitations%20of%20LLMs%2C%20a%20proactive%0ARetrieval-Augmented%20Generation%20%28RAG%29%20is%20designed%20for%20AD%2C%20combined%20with%20a%0Achain-of-thought%20prompting%20mechanism%2C%20ensuring%20rapid%20and%20context-rich%0Aunderstanding.%20Experimental%20results%20using%20real-world%20Vehicle-to-everything%0A%28V2X%29%20datasets%20demonstrate%20significant%20improvements%20in%20perception%20and%0Aprediction%20performance%2C%20highlighting%20the%20potential%20of%20this%20framework%20to%20enhance%0Asafety%2C%20adaptability%2C%20and%20decision-making%20in%20next-generation%20AD%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03535v2&entry.124074799=Read"},
{"title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG", "author": "Patrice B\u00e9chard and Orlando Marquez Ayala", "abstract": "  Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.\n", "link": "http://arxiv.org/abs/2501.04652v1", "date": "2025-01-08", "relevancy": 1.4915, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5085}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4843}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20retriever%20fine-tuning%20for%20domain-specific%20and%20efficient%20RAG&body=Title%3A%20Multi-task%20retriever%20fine-tuning%20for%20domain-specific%20and%20efficient%20RAG%0AAuthor%3A%20Patrice%20B%C3%A9chard%20and%20Orlando%20Marquez%20Ayala%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20become%20ubiquitous%20when%20deploying%0ALarge%20Language%20Models%20%28LLMs%29%2C%20as%20it%20can%20address%20typical%20limitations%20such%20as%0Agenerating%20hallucinated%20or%20outdated%20information.%20However%2C%20when%20building%0Areal-world%20RAG%20applications%2C%20practical%20issues%20arise.%20First%2C%20the%20retrieved%0Ainformation%20is%20generally%20domain-specific.%20Since%20it%20is%20computationally%20expensive%0Ato%20fine-tune%20LLMs%2C%20it%20is%20more%20feasible%20to%20fine-tune%20the%20retriever%20to%20improve%0Athe%20quality%20of%20the%20data%20included%20in%20the%20LLM%20input.%20Second%2C%20as%20more%20applications%0Aare%20deployed%20in%20the%20same%20real-world%20system%2C%20one%20cannot%20afford%20to%20deploy%0Aseparate%20retrievers.%20Moreover%2C%20these%20RAG%20applications%20normally%20retrieve%0Adifferent%20kinds%20of%20data.%20Our%20solution%20is%20to%20instruction%20fine-tune%20a%20small%0Aretriever%20encoder%20on%20a%20variety%20of%20domain-specific%20tasks%20to%20allow%20us%20to%20deploy%0Aone%20encoder%20that%20can%20serve%20many%20use%20cases%2C%20thereby%20achieving%20low-cost%2C%0Ascalability%2C%20and%20speed.%20We%20show%20how%20this%20encoder%20generalizes%20to%20out-of-domain%0Asettings%20as%20well%20as%20to%20an%20unseen%20retrieval%20task%20on%20real-world%20enterprise%20use%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520retriever%2520fine-tuning%2520for%2520domain-specific%2520and%2520efficient%2520RAG%26entry.906535625%3DPatrice%2520B%25C3%25A9chard%2520and%2520Orlando%2520Marquez%2520Ayala%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520become%2520ubiquitous%2520when%2520deploying%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520as%2520it%2520can%2520address%2520typical%2520limitations%2520such%2520as%250Agenerating%2520hallucinated%2520or%2520outdated%2520information.%2520However%252C%2520when%2520building%250Areal-world%2520RAG%2520applications%252C%2520practical%2520issues%2520arise.%2520First%252C%2520the%2520retrieved%250Ainformation%2520is%2520generally%2520domain-specific.%2520Since%2520it%2520is%2520computationally%2520expensive%250Ato%2520fine-tune%2520LLMs%252C%2520it%2520is%2520more%2520feasible%2520to%2520fine-tune%2520the%2520retriever%2520to%2520improve%250Athe%2520quality%2520of%2520the%2520data%2520included%2520in%2520the%2520LLM%2520input.%2520Second%252C%2520as%2520more%2520applications%250Aare%2520deployed%2520in%2520the%2520same%2520real-world%2520system%252C%2520one%2520cannot%2520afford%2520to%2520deploy%250Aseparate%2520retrievers.%2520Moreover%252C%2520these%2520RAG%2520applications%2520normally%2520retrieve%250Adifferent%2520kinds%2520of%2520data.%2520Our%2520solution%2520is%2520to%2520instruction%2520fine-tune%2520a%2520small%250Aretriever%2520encoder%2520on%2520a%2520variety%2520of%2520domain-specific%2520tasks%2520to%2520allow%2520us%2520to%2520deploy%250Aone%2520encoder%2520that%2520can%2520serve%2520many%2520use%2520cases%252C%2520thereby%2520achieving%2520low-cost%252C%250Ascalability%252C%2520and%2520speed.%2520We%2520show%2520how%2520this%2520encoder%2520generalizes%2520to%2520out-of-domain%250Asettings%2520as%2520well%2520as%2520to%2520an%2520unseen%2520retrieval%2520task%2520on%2520real-world%2520enterprise%2520use%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20retriever%20fine-tuning%20for%20domain-specific%20and%20efficient%20RAG&entry.906535625=Patrice%20B%C3%A9chard%20and%20Orlando%20Marquez%20Ayala&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20become%20ubiquitous%20when%20deploying%0ALarge%20Language%20Models%20%28LLMs%29%2C%20as%20it%20can%20address%20typical%20limitations%20such%20as%0Agenerating%20hallucinated%20or%20outdated%20information.%20However%2C%20when%20building%0Areal-world%20RAG%20applications%2C%20practical%20issues%20arise.%20First%2C%20the%20retrieved%0Ainformation%20is%20generally%20domain-specific.%20Since%20it%20is%20computationally%20expensive%0Ato%20fine-tune%20LLMs%2C%20it%20is%20more%20feasible%20to%20fine-tune%20the%20retriever%20to%20improve%0Athe%20quality%20of%20the%20data%20included%20in%20the%20LLM%20input.%20Second%2C%20as%20more%20applications%0Aare%20deployed%20in%20the%20same%20real-world%20system%2C%20one%20cannot%20afford%20to%20deploy%0Aseparate%20retrievers.%20Moreover%2C%20these%20RAG%20applications%20normally%20retrieve%0Adifferent%20kinds%20of%20data.%20Our%20solution%20is%20to%20instruction%20fine-tune%20a%20small%0Aretriever%20encoder%20on%20a%20variety%20of%20domain-specific%20tasks%20to%20allow%20us%20to%20deploy%0Aone%20encoder%20that%20can%20serve%20many%20use%20cases%2C%20thereby%20achieving%20low-cost%2C%0Ascalability%2C%20and%20speed.%20We%20show%20how%20this%20encoder%20generalizes%20to%20out-of-domain%0Asettings%20as%20well%20as%20to%20an%20unseen%20retrieval%20task%20on%20real-world%20enterprise%20use%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04652v1&entry.124074799=Read"},
{"title": "Deep Multi-Objective Reinforcement Learning for Utility-Based\n  Infrastructural Maintenance Optimization", "author": "Jesse van Remmerden and Maurice Kenter and Diederik M. Roijers and Charalampos Andriotis and Yingqian Zhang and Zaharah Bukhsh", "abstract": "  In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent\nActor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL)\nmethod for infrastructural maintenance optimization, an area traditionally\ndominated by single-objective reinforcement learning (RL) approaches. Previous\nsingle-objective RL methods combine multiple objectives, such as probability of\ncollapse and cost, into a singular reward signal through reward-shaping. In\ncontrast, MO-DCMAC can optimize a policy for multiple objectives directly, even\nwhen the utility function is non-linear. We evaluated MO-DCMAC using two\nutility functions, which use probability of collapse and cost as input. The\nfirst utility function is the Threshold utility, in which MO-DCMAC should\nminimize cost so that the probability of collapse is never above the threshold.\nThe second is based on the Failure Mode, Effects, and Criticality Analysis\n(FMECA) methodology used by asset managers to asses maintenance plans. We\nevaluated MO-DCMAC, with both utility functions, in multiple maintenance\nenvironments, including ones based on a case study of the historical quay walls\nof Amsterdam. The performance of MO-DCMAC was compared against multiple\nrule-based policies based on heuristics currently used for constructing\nmaintenance plans. Our results demonstrate that MO-DCMAC outperforms\ntraditional rule-based policies across various environments and utility\nfunctions.\n", "link": "http://arxiv.org/abs/2406.06184v2", "date": "2025-01-08", "relevancy": 1.0031, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5064}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Multi-Objective%20Reinforcement%20Learning%20for%20Utility-Based%0A%20%20Infrastructural%20Maintenance%20Optimization&body=Title%3A%20Deep%20Multi-Objective%20Reinforcement%20Learning%20for%20Utility-Based%0A%20%20Infrastructural%20Maintenance%20Optimization%0AAuthor%3A%20Jesse%20van%20Remmerden%20and%20Maurice%20Kenter%20and%20Diederik%20M.%20Roijers%20and%20Charalampos%20Andriotis%20and%20Yingqian%20Zhang%20and%20Zaharah%20Bukhsh%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Multi-Objective%20Deep%20Centralized%20Multi-Agent%0AActor-Critic%20%28MO-%20DCMAC%29%2C%20a%20multi-objective%20reinforcement%20learning%20%28MORL%29%0Amethod%20for%20infrastructural%20maintenance%20optimization%2C%20an%20area%20traditionally%0Adominated%20by%20single-objective%20reinforcement%20learning%20%28RL%29%20approaches.%20Previous%0Asingle-objective%20RL%20methods%20combine%20multiple%20objectives%2C%20such%20as%20probability%20of%0Acollapse%20and%20cost%2C%20into%20a%20singular%20reward%20signal%20through%20reward-shaping.%20In%0Acontrast%2C%20MO-DCMAC%20can%20optimize%20a%20policy%20for%20multiple%20objectives%20directly%2C%20even%0Awhen%20the%20utility%20function%20is%20non-linear.%20We%20evaluated%20MO-DCMAC%20using%20two%0Autility%20functions%2C%20which%20use%20probability%20of%20collapse%20and%20cost%20as%20input.%20The%0Afirst%20utility%20function%20is%20the%20Threshold%20utility%2C%20in%20which%20MO-DCMAC%20should%0Aminimize%20cost%20so%20that%20the%20probability%20of%20collapse%20is%20never%20above%20the%20threshold.%0AThe%20second%20is%20based%20on%20the%20Failure%20Mode%2C%20Effects%2C%20and%20Criticality%20Analysis%0A%28FMECA%29%20methodology%20used%20by%20asset%20managers%20to%20asses%20maintenance%20plans.%20We%0Aevaluated%20MO-DCMAC%2C%20with%20both%20utility%20functions%2C%20in%20multiple%20maintenance%0Aenvironments%2C%20including%20ones%20based%20on%20a%20case%20study%20of%20the%20historical%20quay%20walls%0Aof%20Amsterdam.%20The%20performance%20of%20MO-DCMAC%20was%20compared%20against%20multiple%0Arule-based%20policies%20based%20on%20heuristics%20currently%20used%20for%20constructing%0Amaintenance%20plans.%20Our%20results%20demonstrate%20that%20MO-DCMAC%20outperforms%0Atraditional%20rule-based%20policies%20across%20various%20environments%20and%20utility%0Afunctions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Multi-Objective%2520Reinforcement%2520Learning%2520for%2520Utility-Based%250A%2520%2520Infrastructural%2520Maintenance%2520Optimization%26entry.906535625%3DJesse%2520van%2520Remmerden%2520and%2520Maurice%2520Kenter%2520and%2520Diederik%2520M.%2520Roijers%2520and%2520Charalampos%2520Andriotis%2520and%2520Yingqian%2520Zhang%2520and%2520Zaharah%2520Bukhsh%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Multi-Objective%2520Deep%2520Centralized%2520Multi-Agent%250AActor-Critic%2520%2528MO-%2520DCMAC%2529%252C%2520a%2520multi-objective%2520reinforcement%2520learning%2520%2528MORL%2529%250Amethod%2520for%2520infrastructural%2520maintenance%2520optimization%252C%2520an%2520area%2520traditionally%250Adominated%2520by%2520single-objective%2520reinforcement%2520learning%2520%2528RL%2529%2520approaches.%2520Previous%250Asingle-objective%2520RL%2520methods%2520combine%2520multiple%2520objectives%252C%2520such%2520as%2520probability%2520of%250Acollapse%2520and%2520cost%252C%2520into%2520a%2520singular%2520reward%2520signal%2520through%2520reward-shaping.%2520In%250Acontrast%252C%2520MO-DCMAC%2520can%2520optimize%2520a%2520policy%2520for%2520multiple%2520objectives%2520directly%252C%2520even%250Awhen%2520the%2520utility%2520function%2520is%2520non-linear.%2520We%2520evaluated%2520MO-DCMAC%2520using%2520two%250Autility%2520functions%252C%2520which%2520use%2520probability%2520of%2520collapse%2520and%2520cost%2520as%2520input.%2520The%250Afirst%2520utility%2520function%2520is%2520the%2520Threshold%2520utility%252C%2520in%2520which%2520MO-DCMAC%2520should%250Aminimize%2520cost%2520so%2520that%2520the%2520probability%2520of%2520collapse%2520is%2520never%2520above%2520the%2520threshold.%250AThe%2520second%2520is%2520based%2520on%2520the%2520Failure%2520Mode%252C%2520Effects%252C%2520and%2520Criticality%2520Analysis%250A%2528FMECA%2529%2520methodology%2520used%2520by%2520asset%2520managers%2520to%2520asses%2520maintenance%2520plans.%2520We%250Aevaluated%2520MO-DCMAC%252C%2520with%2520both%2520utility%2520functions%252C%2520in%2520multiple%2520maintenance%250Aenvironments%252C%2520including%2520ones%2520based%2520on%2520a%2520case%2520study%2520of%2520the%2520historical%2520quay%2520walls%250Aof%2520Amsterdam.%2520The%2520performance%2520of%2520MO-DCMAC%2520was%2520compared%2520against%2520multiple%250Arule-based%2520policies%2520based%2520on%2520heuristics%2520currently%2520used%2520for%2520constructing%250Amaintenance%2520plans.%2520Our%2520results%2520demonstrate%2520that%2520MO-DCMAC%2520outperforms%250Atraditional%2520rule-based%2520policies%2520across%2520various%2520environments%2520and%2520utility%250Afunctions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Multi-Objective%20Reinforcement%20Learning%20for%20Utility-Based%0A%20%20Infrastructural%20Maintenance%20Optimization&entry.906535625=Jesse%20van%20Remmerden%20and%20Maurice%20Kenter%20and%20Diederik%20M.%20Roijers%20and%20Charalampos%20Andriotis%20and%20Yingqian%20Zhang%20and%20Zaharah%20Bukhsh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Multi-Objective%20Deep%20Centralized%20Multi-Agent%0AActor-Critic%20%28MO-%20DCMAC%29%2C%20a%20multi-objective%20reinforcement%20learning%20%28MORL%29%0Amethod%20for%20infrastructural%20maintenance%20optimization%2C%20an%20area%20traditionally%0Adominated%20by%20single-objective%20reinforcement%20learning%20%28RL%29%20approaches.%20Previous%0Asingle-objective%20RL%20methods%20combine%20multiple%20objectives%2C%20such%20as%20probability%20of%0Acollapse%20and%20cost%2C%20into%20a%20singular%20reward%20signal%20through%20reward-shaping.%20In%0Acontrast%2C%20MO-DCMAC%20can%20optimize%20a%20policy%20for%20multiple%20objectives%20directly%2C%20even%0Awhen%20the%20utility%20function%20is%20non-linear.%20We%20evaluated%20MO-DCMAC%20using%20two%0Autility%20functions%2C%20which%20use%20probability%20of%20collapse%20and%20cost%20as%20input.%20The%0Afirst%20utility%20function%20is%20the%20Threshold%20utility%2C%20in%20which%20MO-DCMAC%20should%0Aminimize%20cost%20so%20that%20the%20probability%20of%20collapse%20is%20never%20above%20the%20threshold.%0AThe%20second%20is%20based%20on%20the%20Failure%20Mode%2C%20Effects%2C%20and%20Criticality%20Analysis%0A%28FMECA%29%20methodology%20used%20by%20asset%20managers%20to%20asses%20maintenance%20plans.%20We%0Aevaluated%20MO-DCMAC%2C%20with%20both%20utility%20functions%2C%20in%20multiple%20maintenance%0Aenvironments%2C%20including%20ones%20based%20on%20a%20case%20study%20of%20the%20historical%20quay%20walls%0Aof%20Amsterdam.%20The%20performance%20of%20MO-DCMAC%20was%20compared%20against%20multiple%0Arule-based%20policies%20based%20on%20heuristics%20currently%20used%20for%20constructing%0Amaintenance%20plans.%20Our%20results%20demonstrate%20that%20MO-DCMAC%20outperforms%0Atraditional%20rule-based%20policies%20across%20various%20environments%20and%20utility%0Afunctions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06184v2&entry.124074799=Read"},
{"title": "A Survey on Path Planning Problem of Rolling Contacts: Approaches,\n  Applications and Future Challenges", "author": "Seyed Amir Tafrishi and Mikhail Svinin and Kenji Tahara", "abstract": "  This paper explores an eclectic range of path-planning methodologies\nengineered for rolling surfaces. Our focus is on the kinematic intricacies of\nrolling contact systems, which are investigated through a motion planning lens.\nBeyond summarizing the approaches to single-contact rotational surfaces, we\nexplore the challenging domain of spin-rolling multi-contact systems. Our work\nproposes solutions for the higher-dimensional problem of multiple rotating\nobjects in contact. Venturing beyond kinematics, these methodologies find\napplication across a spectrum of domains, including rolling robots,\nreconfigurable swarm robotics, micro/nano manipulation, and nonprehensile\nmanipulations. Through meticulously examining established planning strategies,\nwe unveil their practical implementations in various real-world scenarios, from\nintricate dexterous manipulation tasks to the nimble manoeuvring of rolling\nrobots and even shape planning of multi-contact swarms of particles. This study\nintroduces the persistent challenges and unexplored frontiers of robotics,\nintricately linked to both path planning and mechanism design. As we illuminate\nexisting solutions, we also set the stage for future breakthroughs in this\ndynamic and rapidly evolving field by highlighting the critical importance of\naddressing rolling contact problems.\n", "link": "http://arxiv.org/abs/2501.04442v1", "date": "2025-01-08", "relevancy": 1.4951, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5057}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4965}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Path%20Planning%20Problem%20of%20Rolling%20Contacts%3A%20Approaches%2C%0A%20%20Applications%20and%20Future%20Challenges&body=Title%3A%20A%20Survey%20on%20Path%20Planning%20Problem%20of%20Rolling%20Contacts%3A%20Approaches%2C%0A%20%20Applications%20and%20Future%20Challenges%0AAuthor%3A%20Seyed%20Amir%20Tafrishi%20and%20Mikhail%20Svinin%20and%20Kenji%20Tahara%0AAbstract%3A%20%20%20This%20paper%20explores%20an%20eclectic%20range%20of%20path-planning%20methodologies%0Aengineered%20for%20rolling%20surfaces.%20Our%20focus%20is%20on%20the%20kinematic%20intricacies%20of%0Arolling%20contact%20systems%2C%20which%20are%20investigated%20through%20a%20motion%20planning%20lens.%0ABeyond%20summarizing%20the%20approaches%20to%20single-contact%20rotational%20surfaces%2C%20we%0Aexplore%20the%20challenging%20domain%20of%20spin-rolling%20multi-contact%20systems.%20Our%20work%0Aproposes%20solutions%20for%20the%20higher-dimensional%20problem%20of%20multiple%20rotating%0Aobjects%20in%20contact.%20Venturing%20beyond%20kinematics%2C%20these%20methodologies%20find%0Aapplication%20across%20a%20spectrum%20of%20domains%2C%20including%20rolling%20robots%2C%0Areconfigurable%20swarm%20robotics%2C%20micro/nano%20manipulation%2C%20and%20nonprehensile%0Amanipulations.%20Through%20meticulously%20examining%20established%20planning%20strategies%2C%0Awe%20unveil%20their%20practical%20implementations%20in%20various%20real-world%20scenarios%2C%20from%0Aintricate%20dexterous%20manipulation%20tasks%20to%20the%20nimble%20manoeuvring%20of%20rolling%0Arobots%20and%20even%20shape%20planning%20of%20multi-contact%20swarms%20of%20particles.%20This%20study%0Aintroduces%20the%20persistent%20challenges%20and%20unexplored%20frontiers%20of%20robotics%2C%0Aintricately%20linked%20to%20both%20path%20planning%20and%20mechanism%20design.%20As%20we%20illuminate%0Aexisting%20solutions%2C%20we%20also%20set%20the%20stage%20for%20future%20breakthroughs%20in%20this%0Adynamic%20and%20rapidly%20evolving%20field%20by%20highlighting%20the%20critical%20importance%20of%0Aaddressing%20rolling%20contact%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Path%2520Planning%2520Problem%2520of%2520Rolling%2520Contacts%253A%2520Approaches%252C%250A%2520%2520Applications%2520and%2520Future%2520Challenges%26entry.906535625%3DSeyed%2520Amir%2520Tafrishi%2520and%2520Mikhail%2520Svinin%2520and%2520Kenji%2520Tahara%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520an%2520eclectic%2520range%2520of%2520path-planning%2520methodologies%250Aengineered%2520for%2520rolling%2520surfaces.%2520Our%2520focus%2520is%2520on%2520the%2520kinematic%2520intricacies%2520of%250Arolling%2520contact%2520systems%252C%2520which%2520are%2520investigated%2520through%2520a%2520motion%2520planning%2520lens.%250ABeyond%2520summarizing%2520the%2520approaches%2520to%2520single-contact%2520rotational%2520surfaces%252C%2520we%250Aexplore%2520the%2520challenging%2520domain%2520of%2520spin-rolling%2520multi-contact%2520systems.%2520Our%2520work%250Aproposes%2520solutions%2520for%2520the%2520higher-dimensional%2520problem%2520of%2520multiple%2520rotating%250Aobjects%2520in%2520contact.%2520Venturing%2520beyond%2520kinematics%252C%2520these%2520methodologies%2520find%250Aapplication%2520across%2520a%2520spectrum%2520of%2520domains%252C%2520including%2520rolling%2520robots%252C%250Areconfigurable%2520swarm%2520robotics%252C%2520micro/nano%2520manipulation%252C%2520and%2520nonprehensile%250Amanipulations.%2520Through%2520meticulously%2520examining%2520established%2520planning%2520strategies%252C%250Awe%2520unveil%2520their%2520practical%2520implementations%2520in%2520various%2520real-world%2520scenarios%252C%2520from%250Aintricate%2520dexterous%2520manipulation%2520tasks%2520to%2520the%2520nimble%2520manoeuvring%2520of%2520rolling%250Arobots%2520and%2520even%2520shape%2520planning%2520of%2520multi-contact%2520swarms%2520of%2520particles.%2520This%2520study%250Aintroduces%2520the%2520persistent%2520challenges%2520and%2520unexplored%2520frontiers%2520of%2520robotics%252C%250Aintricately%2520linked%2520to%2520both%2520path%2520planning%2520and%2520mechanism%2520design.%2520As%2520we%2520illuminate%250Aexisting%2520solutions%252C%2520we%2520also%2520set%2520the%2520stage%2520for%2520future%2520breakthroughs%2520in%2520this%250Adynamic%2520and%2520rapidly%2520evolving%2520field%2520by%2520highlighting%2520the%2520critical%2520importance%2520of%250Aaddressing%2520rolling%2520contact%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Path%20Planning%20Problem%20of%20Rolling%20Contacts%3A%20Approaches%2C%0A%20%20Applications%20and%20Future%20Challenges&entry.906535625=Seyed%20Amir%20Tafrishi%20and%20Mikhail%20Svinin%20and%20Kenji%20Tahara&entry.1292438233=%20%20This%20paper%20explores%20an%20eclectic%20range%20of%20path-planning%20methodologies%0Aengineered%20for%20rolling%20surfaces.%20Our%20focus%20is%20on%20the%20kinematic%20intricacies%20of%0Arolling%20contact%20systems%2C%20which%20are%20investigated%20through%20a%20motion%20planning%20lens.%0ABeyond%20summarizing%20the%20approaches%20to%20single-contact%20rotational%20surfaces%2C%20we%0Aexplore%20the%20challenging%20domain%20of%20spin-rolling%20multi-contact%20systems.%20Our%20work%0Aproposes%20solutions%20for%20the%20higher-dimensional%20problem%20of%20multiple%20rotating%0Aobjects%20in%20contact.%20Venturing%20beyond%20kinematics%2C%20these%20methodologies%20find%0Aapplication%20across%20a%20spectrum%20of%20domains%2C%20including%20rolling%20robots%2C%0Areconfigurable%20swarm%20robotics%2C%20micro/nano%20manipulation%2C%20and%20nonprehensile%0Amanipulations.%20Through%20meticulously%20examining%20established%20planning%20strategies%2C%0Awe%20unveil%20their%20practical%20implementations%20in%20various%20real-world%20scenarios%2C%20from%0Aintricate%20dexterous%20manipulation%20tasks%20to%20the%20nimble%20manoeuvring%20of%20rolling%0Arobots%20and%20even%20shape%20planning%20of%20multi-contact%20swarms%20of%20particles.%20This%20study%0Aintroduces%20the%20persistent%20challenges%20and%20unexplored%20frontiers%20of%20robotics%2C%0Aintricately%20linked%20to%20both%20path%20planning%20and%20mechanism%20design.%20As%20we%20illuminate%0Aexisting%20solutions%2C%20we%20also%20set%20the%20stage%20for%20future%20breakthroughs%20in%20this%0Adynamic%20and%20rapidly%20evolving%20field%20by%20highlighting%20the%20critical%20importance%20of%0Aaddressing%20rolling%20contact%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04442v1&entry.124074799=Read"},
{"title": "Parallelized Midpoint Randomization for Langevin Monte Carlo", "author": "Lu Yu and Arnak Dalalyan", "abstract": "  We study the problem of sampling from a target probability density function\nin frameworks where parallel evaluations of the log-density gradient are\nfeasible. Focusing on smooth and strongly log-concave densities, we revisit the\nparallelized randomized midpoint method and investigate its properties using\nrecently developed techniques for analyzing its sequential version. Through\nthese techniques, we derive upper bounds on the Wasserstein distance between\nsampling and target densities. These bounds quantify the substantial runtime\nimprovements achieved through parallel processing.\n", "link": "http://arxiv.org/abs/2402.14434v4", "date": "2025-01-08", "relevancy": 1.8599, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4704}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.466}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallelized%20Midpoint%20Randomization%20for%20Langevin%20Monte%20Carlo&body=Title%3A%20Parallelized%20Midpoint%20Randomization%20for%20Langevin%20Monte%20Carlo%0AAuthor%3A%20Lu%20Yu%20and%20Arnak%20Dalalyan%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20sampling%20from%20a%20target%20probability%20density%20function%0Ain%20frameworks%20where%20parallel%20evaluations%20of%20the%20log-density%20gradient%20are%0Afeasible.%20Focusing%20on%20smooth%20and%20strongly%20log-concave%20densities%2C%20we%20revisit%20the%0Aparallelized%20randomized%20midpoint%20method%20and%20investigate%20its%20properties%20using%0Arecently%20developed%20techniques%20for%20analyzing%20its%20sequential%20version.%20Through%0Athese%20techniques%2C%20we%20derive%20upper%20bounds%20on%20the%20Wasserstein%20distance%20between%0Asampling%20and%20target%20densities.%20These%20bounds%20quantify%20the%20substantial%20runtime%0Aimprovements%20achieved%20through%20parallel%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14434v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallelized%2520Midpoint%2520Randomization%2520for%2520Langevin%2520Monte%2520Carlo%26entry.906535625%3DLu%2520Yu%2520and%2520Arnak%2520Dalalyan%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520sampling%2520from%2520a%2520target%2520probability%2520density%2520function%250Ain%2520frameworks%2520where%2520parallel%2520evaluations%2520of%2520the%2520log-density%2520gradient%2520are%250Afeasible.%2520Focusing%2520on%2520smooth%2520and%2520strongly%2520log-concave%2520densities%252C%2520we%2520revisit%2520the%250Aparallelized%2520randomized%2520midpoint%2520method%2520and%2520investigate%2520its%2520properties%2520using%250Arecently%2520developed%2520techniques%2520for%2520analyzing%2520its%2520sequential%2520version.%2520Through%250Athese%2520techniques%252C%2520we%2520derive%2520upper%2520bounds%2520on%2520the%2520Wasserstein%2520distance%2520between%250Asampling%2520and%2520target%2520densities.%2520These%2520bounds%2520quantify%2520the%2520substantial%2520runtime%250Aimprovements%2520achieved%2520through%2520parallel%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14434v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallelized%20Midpoint%20Randomization%20for%20Langevin%20Monte%20Carlo&entry.906535625=Lu%20Yu%20and%20Arnak%20Dalalyan&entry.1292438233=%20%20We%20study%20the%20problem%20of%20sampling%20from%20a%20target%20probability%20density%20function%0Ain%20frameworks%20where%20parallel%20evaluations%20of%20the%20log-density%20gradient%20are%0Afeasible.%20Focusing%20on%20smooth%20and%20strongly%20log-concave%20densities%2C%20we%20revisit%20the%0Aparallelized%20randomized%20midpoint%20method%20and%20investigate%20its%20properties%20using%0Arecently%20developed%20techniques%20for%20analyzing%20its%20sequential%20version.%20Through%0Athese%20techniques%2C%20we%20derive%20upper%20bounds%20on%20the%20Wasserstein%20distance%20between%0Asampling%20and%20target%20densities.%20These%20bounds%20quantify%20the%20substantial%20runtime%0Aimprovements%20achieved%20through%20parallel%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14434v4&entry.124074799=Read"},
{"title": "A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word\n  GRNG for AI Uncertainty Estimation", "author": "Zephan M. Enciso and Boyang Cheng and Likai Pei and Jianbo Liu and Steven Davis and Ningyuan Cao and Michael Niemier", "abstract": "  Uncertainty estimation is an indispensable capability for AI-enabled,\nsafety-critical applications, e.g. autonomous vehicles or medical diagnosis.\nBayesian neural networks (BNNs) use Bayesian statistics to provide both\nclassification predictions and uncertainty estimation, but they suffer from\nhigh computational overhead associated with random number generation and\nrepeated sample iterations. Furthermore, BNNs are not immediately amenable to\nacceleration through compute-in-memory architectures due to the frequent memory\nwrites necessary after each RNG operation. To address these challenges, we\npresent an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the\nSRAM memory words. This integration reduces RNG overhead and enables\nfully-parallel compute-in-memory operations for BNNs. The prototype chip\nachieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput\nwhile occupying 0.45 mm2, bringing AI uncertainty estimation to edge\ncomputation.\n", "link": "http://arxiv.org/abs/2501.04577v1", "date": "2025-01-08", "relevancy": 1.3774, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4662}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%2065%20nm%20Bayesian%20Neural%20Network%20Accelerator%20with%20360%20fJ/Sample%20In-Word%0A%20%20GRNG%20for%20AI%20Uncertainty%20Estimation&body=Title%3A%20A%2065%20nm%20Bayesian%20Neural%20Network%20Accelerator%20with%20360%20fJ/Sample%20In-Word%0A%20%20GRNG%20for%20AI%20Uncertainty%20Estimation%0AAuthor%3A%20Zephan%20M.%20Enciso%20and%20Boyang%20Cheng%20and%20Likai%20Pei%20and%20Jianbo%20Liu%20and%20Steven%20Davis%20and%20Ningyuan%20Cao%20and%20Michael%20Niemier%0AAbstract%3A%20%20%20Uncertainty%20estimation%20is%20an%20indispensable%20capability%20for%20AI-enabled%2C%0Asafety-critical%20applications%2C%20e.g.%20autonomous%20vehicles%20or%20medical%20diagnosis.%0ABayesian%20neural%20networks%20%28BNNs%29%20use%20Bayesian%20statistics%20to%20provide%20both%0Aclassification%20predictions%20and%20uncertainty%20estimation%2C%20but%20they%20suffer%20from%0Ahigh%20computational%20overhead%20associated%20with%20random%20number%20generation%20and%0Arepeated%20sample%20iterations.%20Furthermore%2C%20BNNs%20are%20not%20immediately%20amenable%20to%0Aacceleration%20through%20compute-in-memory%20architectures%20due%20to%20the%20frequent%20memory%0Awrites%20necessary%20after%20each%20RNG%20operation.%20To%20address%20these%20challenges%2C%20we%0Apresent%20an%20ASIC%20that%20integrates%20360%20fJ/Sample%20Gaussian%20RNG%20directly%20into%20the%0ASRAM%20memory%20words.%20This%20integration%20reduces%20RNG%20overhead%20and%20enables%0Afully-parallel%20compute-in-memory%20operations%20for%20BNNs.%20The%20prototype%20chip%0Aachieves%205.12%20GSa/s%20RNG%20throughput%20and%20102%20GOp/s%20neural%20network%20throughput%0Awhile%20occupying%200.45%20mm2%2C%20bringing%20AI%20uncertainty%20estimation%20to%20edge%0Acomputation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%252065%2520nm%2520Bayesian%2520Neural%2520Network%2520Accelerator%2520with%2520360%2520fJ/Sample%2520In-Word%250A%2520%2520GRNG%2520for%2520AI%2520Uncertainty%2520Estimation%26entry.906535625%3DZephan%2520M.%2520Enciso%2520and%2520Boyang%2520Cheng%2520and%2520Likai%2520Pei%2520and%2520Jianbo%2520Liu%2520and%2520Steven%2520Davis%2520and%2520Ningyuan%2520Cao%2520and%2520Michael%2520Niemier%26entry.1292438233%3D%2520%2520Uncertainty%2520estimation%2520is%2520an%2520indispensable%2520capability%2520for%2520AI-enabled%252C%250Asafety-critical%2520applications%252C%2520e.g.%2520autonomous%2520vehicles%2520or%2520medical%2520diagnosis.%250ABayesian%2520neural%2520networks%2520%2528BNNs%2529%2520use%2520Bayesian%2520statistics%2520to%2520provide%2520both%250Aclassification%2520predictions%2520and%2520uncertainty%2520estimation%252C%2520but%2520they%2520suffer%2520from%250Ahigh%2520computational%2520overhead%2520associated%2520with%2520random%2520number%2520generation%2520and%250Arepeated%2520sample%2520iterations.%2520Furthermore%252C%2520BNNs%2520are%2520not%2520immediately%2520amenable%2520to%250Aacceleration%2520through%2520compute-in-memory%2520architectures%2520due%2520to%2520the%2520frequent%2520memory%250Awrites%2520necessary%2520after%2520each%2520RNG%2520operation.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apresent%2520an%2520ASIC%2520that%2520integrates%2520360%2520fJ/Sample%2520Gaussian%2520RNG%2520directly%2520into%2520the%250ASRAM%2520memory%2520words.%2520This%2520integration%2520reduces%2520RNG%2520overhead%2520and%2520enables%250Afully-parallel%2520compute-in-memory%2520operations%2520for%2520BNNs.%2520The%2520prototype%2520chip%250Aachieves%25205.12%2520GSa/s%2520RNG%2520throughput%2520and%2520102%2520GOp/s%2520neural%2520network%2520throughput%250Awhile%2520occupying%25200.45%2520mm2%252C%2520bringing%2520AI%2520uncertainty%2520estimation%2520to%2520edge%250Acomputation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%2065%20nm%20Bayesian%20Neural%20Network%20Accelerator%20with%20360%20fJ/Sample%20In-Word%0A%20%20GRNG%20for%20AI%20Uncertainty%20Estimation&entry.906535625=Zephan%20M.%20Enciso%20and%20Boyang%20Cheng%20and%20Likai%20Pei%20and%20Jianbo%20Liu%20and%20Steven%20Davis%20and%20Ningyuan%20Cao%20and%20Michael%20Niemier&entry.1292438233=%20%20Uncertainty%20estimation%20is%20an%20indispensable%20capability%20for%20AI-enabled%2C%0Asafety-critical%20applications%2C%20e.g.%20autonomous%20vehicles%20or%20medical%20diagnosis.%0ABayesian%20neural%20networks%20%28BNNs%29%20use%20Bayesian%20statistics%20to%20provide%20both%0Aclassification%20predictions%20and%20uncertainty%20estimation%2C%20but%20they%20suffer%20from%0Ahigh%20computational%20overhead%20associated%20with%20random%20number%20generation%20and%0Arepeated%20sample%20iterations.%20Furthermore%2C%20BNNs%20are%20not%20immediately%20amenable%20to%0Aacceleration%20through%20compute-in-memory%20architectures%20due%20to%20the%20frequent%20memory%0Awrites%20necessary%20after%20each%20RNG%20operation.%20To%20address%20these%20challenges%2C%20we%0Apresent%20an%20ASIC%20that%20integrates%20360%20fJ/Sample%20Gaussian%20RNG%20directly%20into%20the%0ASRAM%20memory%20words.%20This%20integration%20reduces%20RNG%20overhead%20and%20enables%0Afully-parallel%20compute-in-memory%20operations%20for%20BNNs.%20The%20prototype%20chip%0Aachieves%205.12%20GSa/s%20RNG%20throughput%20and%20102%20GOp/s%20neural%20network%20throughput%0Awhile%20occupying%200.45%20mm2%2C%20bringing%20AI%20uncertainty%20estimation%20to%20edge%0Acomputation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04577v1&entry.124074799=Read"},
{"title": "Lossless Privacy-Preserving Aggregation for Decentralized Federated\n  Learning", "author": "Xiaoye Miao and Bin Li and Yangyang Wu and Meng Xi and Xinkui Zhao and Jianwei Yin", "abstract": "  Privacy concerns arise as sensitive data proliferate. Despite decentralized\nfederated learning (DFL) aggregating gradients from neighbors to avoid direct\ndata transmission, it still poses indirect data leaks from the transmitted\ngradients. Existing privacy-preserving methods for DFL add noise to gradients.\nThey either diminish the model predictive accuracy or suffer from ineffective\ngradient protection. In this paper, we propose a novel lossless\nprivacy-preserving aggregation rule named LPPA to enhance gradient protection\nas much as possible but without loss of DFL model predictive accuracy. LPPA\nsubtly injects the noise difference between the sent and received noise into\ntransmitted gradients for gradient protection. The noise difference\nincorporates neighbors' randomness for each client, effectively safeguarding\nagainst data leaks. LPPA employs the noise flow conservation theory to ensure\nthat the noise impact can be globally eliminated. The global sum of all noise\ndifferences remains zero, ensuring that accurate gradient aggregation is\nunaffected and the model accuracy remains intact. We theoretically prove that\nthe privacy-preserving capacity of LPPA is \\sqrt{2} times greater than that of\nnoise addition, while maintaining comparable model accuracy to the standard DFL\naggregation without noise injection. Experimental results verify the\ntheoretical findings and show that LPPA achieves a 13% mean improvement in\naccuracy over noise addition. We also demonstrate the effectiveness of LPPA in\nprotecting raw data and guaranteeing lossless model accuracy.\n", "link": "http://arxiv.org/abs/2501.04409v1", "date": "2025-01-08", "relevancy": 1.3802, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4752}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4605}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lossless%20Privacy-Preserving%20Aggregation%20for%20Decentralized%20Federated%0A%20%20Learning&body=Title%3A%20Lossless%20Privacy-Preserving%20Aggregation%20for%20Decentralized%20Federated%0A%20%20Learning%0AAuthor%3A%20Xiaoye%20Miao%20and%20Bin%20Li%20and%20Yangyang%20Wu%20and%20Meng%20Xi%20and%20Xinkui%20Zhao%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20Privacy%20concerns%20arise%20as%20sensitive%20data%20proliferate.%20Despite%20decentralized%0Afederated%20learning%20%28DFL%29%20aggregating%20gradients%20from%20neighbors%20to%20avoid%20direct%0Adata%20transmission%2C%20it%20still%20poses%20indirect%20data%20leaks%20from%20the%20transmitted%0Agradients.%20Existing%20privacy-preserving%20methods%20for%20DFL%20add%20noise%20to%20gradients.%0AThey%20either%20diminish%20the%20model%20predictive%20accuracy%20or%20suffer%20from%20ineffective%0Agradient%20protection.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20lossless%0Aprivacy-preserving%20aggregation%20rule%20named%20LPPA%20to%20enhance%20gradient%20protection%0Aas%20much%20as%20possible%20but%20without%20loss%20of%20DFL%20model%20predictive%20accuracy.%20LPPA%0Asubtly%20injects%20the%20noise%20difference%20between%20the%20sent%20and%20received%20noise%20into%0Atransmitted%20gradients%20for%20gradient%20protection.%20The%20noise%20difference%0Aincorporates%20neighbors%27%20randomness%20for%20each%20client%2C%20effectively%20safeguarding%0Aagainst%20data%20leaks.%20LPPA%20employs%20the%20noise%20flow%20conservation%20theory%20to%20ensure%0Athat%20the%20noise%20impact%20can%20be%20globally%20eliminated.%20The%20global%20sum%20of%20all%20noise%0Adifferences%20remains%20zero%2C%20ensuring%20that%20accurate%20gradient%20aggregation%20is%0Aunaffected%20and%20the%20model%20accuracy%20remains%20intact.%20We%20theoretically%20prove%20that%0Athe%20privacy-preserving%20capacity%20of%20LPPA%20is%20%5Csqrt%7B2%7D%20times%20greater%20than%20that%20of%0Anoise%20addition%2C%20while%20maintaining%20comparable%20model%20accuracy%20to%20the%20standard%20DFL%0Aaggregation%20without%20noise%20injection.%20Experimental%20results%20verify%20the%0Atheoretical%20findings%20and%20show%20that%20LPPA%20achieves%20a%2013%25%20mean%20improvement%20in%0Aaccuracy%20over%20noise%20addition.%20We%20also%20demonstrate%20the%20effectiveness%20of%20LPPA%20in%0Aprotecting%20raw%20data%20and%20guaranteeing%20lossless%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLossless%2520Privacy-Preserving%2520Aggregation%2520for%2520Decentralized%2520Federated%250A%2520%2520Learning%26entry.906535625%3DXiaoye%2520Miao%2520and%2520Bin%2520Li%2520and%2520Yangyang%2520Wu%2520and%2520Meng%2520Xi%2520and%2520Xinkui%2520Zhao%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520Privacy%2520concerns%2520arise%2520as%2520sensitive%2520data%2520proliferate.%2520Despite%2520decentralized%250Afederated%2520learning%2520%2528DFL%2529%2520aggregating%2520gradients%2520from%2520neighbors%2520to%2520avoid%2520direct%250Adata%2520transmission%252C%2520it%2520still%2520poses%2520indirect%2520data%2520leaks%2520from%2520the%2520transmitted%250Agradients.%2520Existing%2520privacy-preserving%2520methods%2520for%2520DFL%2520add%2520noise%2520to%2520gradients.%250AThey%2520either%2520diminish%2520the%2520model%2520predictive%2520accuracy%2520or%2520suffer%2520from%2520ineffective%250Agradient%2520protection.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520lossless%250Aprivacy-preserving%2520aggregation%2520rule%2520named%2520LPPA%2520to%2520enhance%2520gradient%2520protection%250Aas%2520much%2520as%2520possible%2520but%2520without%2520loss%2520of%2520DFL%2520model%2520predictive%2520accuracy.%2520LPPA%250Asubtly%2520injects%2520the%2520noise%2520difference%2520between%2520the%2520sent%2520and%2520received%2520noise%2520into%250Atransmitted%2520gradients%2520for%2520gradient%2520protection.%2520The%2520noise%2520difference%250Aincorporates%2520neighbors%2527%2520randomness%2520for%2520each%2520client%252C%2520effectively%2520safeguarding%250Aagainst%2520data%2520leaks.%2520LPPA%2520employs%2520the%2520noise%2520flow%2520conservation%2520theory%2520to%2520ensure%250Athat%2520the%2520noise%2520impact%2520can%2520be%2520globally%2520eliminated.%2520The%2520global%2520sum%2520of%2520all%2520noise%250Adifferences%2520remains%2520zero%252C%2520ensuring%2520that%2520accurate%2520gradient%2520aggregation%2520is%250Aunaffected%2520and%2520the%2520model%2520accuracy%2520remains%2520intact.%2520We%2520theoretically%2520prove%2520that%250Athe%2520privacy-preserving%2520capacity%2520of%2520LPPA%2520is%2520%255Csqrt%257B2%257D%2520times%2520greater%2520than%2520that%2520of%250Anoise%2520addition%252C%2520while%2520maintaining%2520comparable%2520model%2520accuracy%2520to%2520the%2520standard%2520DFL%250Aaggregation%2520without%2520noise%2520injection.%2520Experimental%2520results%2520verify%2520the%250Atheoretical%2520findings%2520and%2520show%2520that%2520LPPA%2520achieves%2520a%252013%2525%2520mean%2520improvement%2520in%250Aaccuracy%2520over%2520noise%2520addition.%2520We%2520also%2520demonstrate%2520the%2520effectiveness%2520of%2520LPPA%2520in%250Aprotecting%2520raw%2520data%2520and%2520guaranteeing%2520lossless%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lossless%20Privacy-Preserving%20Aggregation%20for%20Decentralized%20Federated%0A%20%20Learning&entry.906535625=Xiaoye%20Miao%20and%20Bin%20Li%20and%20Yangyang%20Wu%20and%20Meng%20Xi%20and%20Xinkui%20Zhao%20and%20Jianwei%20Yin&entry.1292438233=%20%20Privacy%20concerns%20arise%20as%20sensitive%20data%20proliferate.%20Despite%20decentralized%0Afederated%20learning%20%28DFL%29%20aggregating%20gradients%20from%20neighbors%20to%20avoid%20direct%0Adata%20transmission%2C%20it%20still%20poses%20indirect%20data%20leaks%20from%20the%20transmitted%0Agradients.%20Existing%20privacy-preserving%20methods%20for%20DFL%20add%20noise%20to%20gradients.%0AThey%20either%20diminish%20the%20model%20predictive%20accuracy%20or%20suffer%20from%20ineffective%0Agradient%20protection.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20lossless%0Aprivacy-preserving%20aggregation%20rule%20named%20LPPA%20to%20enhance%20gradient%20protection%0Aas%20much%20as%20possible%20but%20without%20loss%20of%20DFL%20model%20predictive%20accuracy.%20LPPA%0Asubtly%20injects%20the%20noise%20difference%20between%20the%20sent%20and%20received%20noise%20into%0Atransmitted%20gradients%20for%20gradient%20protection.%20The%20noise%20difference%0Aincorporates%20neighbors%27%20randomness%20for%20each%20client%2C%20effectively%20safeguarding%0Aagainst%20data%20leaks.%20LPPA%20employs%20the%20noise%20flow%20conservation%20theory%20to%20ensure%0Athat%20the%20noise%20impact%20can%20be%20globally%20eliminated.%20The%20global%20sum%20of%20all%20noise%0Adifferences%20remains%20zero%2C%20ensuring%20that%20accurate%20gradient%20aggregation%20is%0Aunaffected%20and%20the%20model%20accuracy%20remains%20intact.%20We%20theoretically%20prove%20that%0Athe%20privacy-preserving%20capacity%20of%20LPPA%20is%20%5Csqrt%7B2%7D%20times%20greater%20than%20that%20of%0Anoise%20addition%2C%20while%20maintaining%20comparable%20model%20accuracy%20to%20the%20standard%20DFL%0Aaggregation%20without%20noise%20injection.%20Experimental%20results%20verify%20the%0Atheoretical%20findings%20and%20show%20that%20LPPA%20achieves%20a%2013%25%20mean%20improvement%20in%0Aaccuracy%20over%20noise%20addition.%20We%20also%20demonstrate%20the%20effectiveness%20of%20LPPA%20in%0Aprotecting%20raw%20data%20and%20guaranteeing%20lossless%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04409v1&entry.124074799=Read"},
{"title": "AutoSTF: Decoupled Neural Architecture Search for Cost-Effective\n  Automated Spatio-Temporal Forecasting", "author": "Tengfei Lyu and Weijia Zhang and Jinliang Deng and Hao Liu", "abstract": "  Spatio-temporal forecasting is a critical component of various smart city\napplications, such as transportation optimization, energy management, and\nsocio-economic analysis. Recently, several automated spatio-temporal\nforecasting methods have been proposed to automatically search the optimal\nneural network architecture for capturing complex spatio-temporal dependencies.\nHowever, the existing automated approaches suffer from expensive neural\narchitecture search overhead, which hinders their practical use and the further\nexploration of diverse spatio-temporal operators in a finer granularity. In\nthis paper, we propose AutoSTF, a decoupled automatic neural architecture\nsearch framework for cost-effective automated spatio-temporal forecasting. From\nthe efficiency perspective, we first decouple the mixed search space into\ntemporal space and spatial space and respectively devise representation\ncompression and parameter-sharing schemes to mitigate the parameter explosion.\nThe decoupled spatio-temporal search not only expedites the model optimization\nprocess but also leaves new room for more effective spatio-temporal dependency\nmodeling. From the effectiveness perspective, we propose a multi-patch transfer\nmodule to jointly capture multi-granularity temporal dependencies and extend\nthe spatial search space to enable finer-grained layer-wise spatial dependency\nsearch. Extensive experiments on eight datasets demonstrate the superiority of\nAutoSTF in terms of both accuracy and efficiency. Specifically, our proposed\nmethod achieves up to 13.48x speed-up compared to state-of-the-art automatic\nspatio-temporal forecasting methods while maintaining the best forecasting\naccuracy.\n", "link": "http://arxiv.org/abs/2409.16586v2", "date": "2025-01-08", "relevancy": 1.4868, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4978}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4965}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoSTF%3A%20Decoupled%20Neural%20Architecture%20Search%20for%20Cost-Effective%0A%20%20Automated%20Spatio-Temporal%20Forecasting&body=Title%3A%20AutoSTF%3A%20Decoupled%20Neural%20Architecture%20Search%20for%20Cost-Effective%0A%20%20Automated%20Spatio-Temporal%20Forecasting%0AAuthor%3A%20Tengfei%20Lyu%20and%20Weijia%20Zhang%20and%20Jinliang%20Deng%20and%20Hao%20Liu%0AAbstract%3A%20%20%20Spatio-temporal%20forecasting%20is%20a%20critical%20component%20of%20various%20smart%20city%0Aapplications%2C%20such%20as%20transportation%20optimization%2C%20energy%20management%2C%20and%0Asocio-economic%20analysis.%20Recently%2C%20several%20automated%20spatio-temporal%0Aforecasting%20methods%20have%20been%20proposed%20to%20automatically%20search%20the%20optimal%0Aneural%20network%20architecture%20for%20capturing%20complex%20spatio-temporal%20dependencies.%0AHowever%2C%20the%20existing%20automated%20approaches%20suffer%20from%20expensive%20neural%0Aarchitecture%20search%20overhead%2C%20which%20hinders%20their%20practical%20use%20and%20the%20further%0Aexploration%20of%20diverse%20spatio-temporal%20operators%20in%20a%20finer%20granularity.%20In%0Athis%20paper%2C%20we%20propose%20AutoSTF%2C%20a%20decoupled%20automatic%20neural%20architecture%0Asearch%20framework%20for%20cost-effective%20automated%20spatio-temporal%20forecasting.%20From%0Athe%20efficiency%20perspective%2C%20we%20first%20decouple%20the%20mixed%20search%20space%20into%0Atemporal%20space%20and%20spatial%20space%20and%20respectively%20devise%20representation%0Acompression%20and%20parameter-sharing%20schemes%20to%20mitigate%20the%20parameter%20explosion.%0AThe%20decoupled%20spatio-temporal%20search%20not%20only%20expedites%20the%20model%20optimization%0Aprocess%20but%20also%20leaves%20new%20room%20for%20more%20effective%20spatio-temporal%20dependency%0Amodeling.%20From%20the%20effectiveness%20perspective%2C%20we%20propose%20a%20multi-patch%20transfer%0Amodule%20to%20jointly%20capture%20multi-granularity%20temporal%20dependencies%20and%20extend%0Athe%20spatial%20search%20space%20to%20enable%20finer-grained%20layer-wise%20spatial%20dependency%0Asearch.%20Extensive%20experiments%20on%20eight%20datasets%20demonstrate%20the%20superiority%20of%0AAutoSTF%20in%20terms%20of%20both%20accuracy%20and%20efficiency.%20Specifically%2C%20our%20proposed%0Amethod%20achieves%20up%20to%2013.48x%20speed-up%20compared%20to%20state-of-the-art%20automatic%0Aspatio-temporal%20forecasting%20methods%20while%20maintaining%20the%20best%20forecasting%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoSTF%253A%2520Decoupled%2520Neural%2520Architecture%2520Search%2520for%2520Cost-Effective%250A%2520%2520Automated%2520Spatio-Temporal%2520Forecasting%26entry.906535625%3DTengfei%2520Lyu%2520and%2520Weijia%2520Zhang%2520and%2520Jinliang%2520Deng%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520Spatio-temporal%2520forecasting%2520is%2520a%2520critical%2520component%2520of%2520various%2520smart%2520city%250Aapplications%252C%2520such%2520as%2520transportation%2520optimization%252C%2520energy%2520management%252C%2520and%250Asocio-economic%2520analysis.%2520Recently%252C%2520several%2520automated%2520spatio-temporal%250Aforecasting%2520methods%2520have%2520been%2520proposed%2520to%2520automatically%2520search%2520the%2520optimal%250Aneural%2520network%2520architecture%2520for%2520capturing%2520complex%2520spatio-temporal%2520dependencies.%250AHowever%252C%2520the%2520existing%2520automated%2520approaches%2520suffer%2520from%2520expensive%2520neural%250Aarchitecture%2520search%2520overhead%252C%2520which%2520hinders%2520their%2520practical%2520use%2520and%2520the%2520further%250Aexploration%2520of%2520diverse%2520spatio-temporal%2520operators%2520in%2520a%2520finer%2520granularity.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520AutoSTF%252C%2520a%2520decoupled%2520automatic%2520neural%2520architecture%250Asearch%2520framework%2520for%2520cost-effective%2520automated%2520spatio-temporal%2520forecasting.%2520From%250Athe%2520efficiency%2520perspective%252C%2520we%2520first%2520decouple%2520the%2520mixed%2520search%2520space%2520into%250Atemporal%2520space%2520and%2520spatial%2520space%2520and%2520respectively%2520devise%2520representation%250Acompression%2520and%2520parameter-sharing%2520schemes%2520to%2520mitigate%2520the%2520parameter%2520explosion.%250AThe%2520decoupled%2520spatio-temporal%2520search%2520not%2520only%2520expedites%2520the%2520model%2520optimization%250Aprocess%2520but%2520also%2520leaves%2520new%2520room%2520for%2520more%2520effective%2520spatio-temporal%2520dependency%250Amodeling.%2520From%2520the%2520effectiveness%2520perspective%252C%2520we%2520propose%2520a%2520multi-patch%2520transfer%250Amodule%2520to%2520jointly%2520capture%2520multi-granularity%2520temporal%2520dependencies%2520and%2520extend%250Athe%2520spatial%2520search%2520space%2520to%2520enable%2520finer-grained%2520layer-wise%2520spatial%2520dependency%250Asearch.%2520Extensive%2520experiments%2520on%2520eight%2520datasets%2520demonstrate%2520the%2520superiority%2520of%250AAutoSTF%2520in%2520terms%2520of%2520both%2520accuracy%2520and%2520efficiency.%2520Specifically%252C%2520our%2520proposed%250Amethod%2520achieves%2520up%2520to%252013.48x%2520speed-up%2520compared%2520to%2520state-of-the-art%2520automatic%250Aspatio-temporal%2520forecasting%2520methods%2520while%2520maintaining%2520the%2520best%2520forecasting%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoSTF%3A%20Decoupled%20Neural%20Architecture%20Search%20for%20Cost-Effective%0A%20%20Automated%20Spatio-Temporal%20Forecasting&entry.906535625=Tengfei%20Lyu%20and%20Weijia%20Zhang%20and%20Jinliang%20Deng%20and%20Hao%20Liu&entry.1292438233=%20%20Spatio-temporal%20forecasting%20is%20a%20critical%20component%20of%20various%20smart%20city%0Aapplications%2C%20such%20as%20transportation%20optimization%2C%20energy%20management%2C%20and%0Asocio-economic%20analysis.%20Recently%2C%20several%20automated%20spatio-temporal%0Aforecasting%20methods%20have%20been%20proposed%20to%20automatically%20search%20the%20optimal%0Aneural%20network%20architecture%20for%20capturing%20complex%20spatio-temporal%20dependencies.%0AHowever%2C%20the%20existing%20automated%20approaches%20suffer%20from%20expensive%20neural%0Aarchitecture%20search%20overhead%2C%20which%20hinders%20their%20practical%20use%20and%20the%20further%0Aexploration%20of%20diverse%20spatio-temporal%20operators%20in%20a%20finer%20granularity.%20In%0Athis%20paper%2C%20we%20propose%20AutoSTF%2C%20a%20decoupled%20automatic%20neural%20architecture%0Asearch%20framework%20for%20cost-effective%20automated%20spatio-temporal%20forecasting.%20From%0Athe%20efficiency%20perspective%2C%20we%20first%20decouple%20the%20mixed%20search%20space%20into%0Atemporal%20space%20and%20spatial%20space%20and%20respectively%20devise%20representation%0Acompression%20and%20parameter-sharing%20schemes%20to%20mitigate%20the%20parameter%20explosion.%0AThe%20decoupled%20spatio-temporal%20search%20not%20only%20expedites%20the%20model%20optimization%0Aprocess%20but%20also%20leaves%20new%20room%20for%20more%20effective%20spatio-temporal%20dependency%0Amodeling.%20From%20the%20effectiveness%20perspective%2C%20we%20propose%20a%20multi-patch%20transfer%0Amodule%20to%20jointly%20capture%20multi-granularity%20temporal%20dependencies%20and%20extend%0Athe%20spatial%20search%20space%20to%20enable%20finer-grained%20layer-wise%20spatial%20dependency%0Asearch.%20Extensive%20experiments%20on%20eight%20datasets%20demonstrate%20the%20superiority%20of%0AAutoSTF%20in%20terms%20of%20both%20accuracy%20and%20efficiency.%20Specifically%2C%20our%20proposed%0Amethod%20achieves%20up%20to%2013.48x%20speed-up%20compared%20to%20state-of-the-art%20automatic%0Aspatio-temporal%20forecasting%20methods%20while%20maintaining%20the%20best%20forecasting%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16586v2&entry.124074799=Read"},
{"title": "Rethinking Byzantine Robustness in Federated Recommendation from Sparse\n  Aggregation Perspective", "author": "Zhongjian Zhang and Mengmei Zhang and Xiao Wang and Lingjuan Lyu and Bo Yan and Junping Du and Chuan Shi", "abstract": "  To preserve user privacy in recommender systems, federated recommendation\n(FR) based on federated learning (FL) emerges, keeping the personal data on the\nlocal client and updating a model collaboratively. Unlike FL, FR has a unique\nsparse aggregation mechanism, where the embedding of each item is updated by\nonly partial clients, instead of full clients in a dense aggregation of general\nFL. Recently, as an essential principle of FL, model security has received\nincreasing attention, especially for Byzantine attacks, where malicious clients\ncan send arbitrary updates. The problem of exploring the Byzantine robustness\nof FR is particularly critical since in the domains applying FR, e.g.,\ne-commerce, malicious clients can be injected easily by registering new\naccounts. However, existing Byzantine works neglect the unique sparse\naggregation of FR, making them unsuitable for our problem. Thus, we make the\nfirst effort to investigate Byzantine attacks on FR from the perspective of\nsparse aggregation, which is non-trivial: it is not clear how to define\nByzantine robustness under sparse aggregations and design Byzantine attacks\nunder limited knowledge/capability. In this paper, we reformulate the Byzantine\nrobustness under sparse aggregation by defining the aggregation for a single\nitem as the smallest execution unit. Then we propose a family of effective\nattack strategies, named Spattack, which exploit the vulnerability in sparse\naggregation and are categorized along the adversary's knowledge and capability.\nExtensive experimental results demonstrate that Spattack can effectively\nprevent convergence and even break down defenses under a few malicious clients,\nraising alarms for securing FR systems.\n", "link": "http://arxiv.org/abs/2501.03301v2", "date": "2025-01-08", "relevancy": 1.7112, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4296}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.428}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Byzantine%20Robustness%20in%20Federated%20Recommendation%20from%20Sparse%0A%20%20Aggregation%20Perspective&body=Title%3A%20Rethinking%20Byzantine%20Robustness%20in%20Federated%20Recommendation%20from%20Sparse%0A%20%20Aggregation%20Perspective%0AAuthor%3A%20Zhongjian%20Zhang%20and%20Mengmei%20Zhang%20and%20Xiao%20Wang%20and%20Lingjuan%20Lyu%20and%20Bo%20Yan%20and%20Junping%20Du%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20To%20preserve%20user%20privacy%20in%20recommender%20systems%2C%20federated%20recommendation%0A%28FR%29%20based%20on%20federated%20learning%20%28FL%29%20emerges%2C%20keeping%20the%20personal%20data%20on%20the%0Alocal%20client%20and%20updating%20a%20model%20collaboratively.%20Unlike%20FL%2C%20FR%20has%20a%20unique%0Asparse%20aggregation%20mechanism%2C%20where%20the%20embedding%20of%20each%20item%20is%20updated%20by%0Aonly%20partial%20clients%2C%20instead%20of%20full%20clients%20in%20a%20dense%20aggregation%20of%20general%0AFL.%20Recently%2C%20as%20an%20essential%20principle%20of%20FL%2C%20model%20security%20has%20received%0Aincreasing%20attention%2C%20especially%20for%20Byzantine%20attacks%2C%20where%20malicious%20clients%0Acan%20send%20arbitrary%20updates.%20The%20problem%20of%20exploring%20the%20Byzantine%20robustness%0Aof%20FR%20is%20particularly%20critical%20since%20in%20the%20domains%20applying%20FR%2C%20e.g.%2C%0Ae-commerce%2C%20malicious%20clients%20can%20be%20injected%20easily%20by%20registering%20new%0Aaccounts.%20However%2C%20existing%20Byzantine%20works%20neglect%20the%20unique%20sparse%0Aaggregation%20of%20FR%2C%20making%20them%20unsuitable%20for%20our%20problem.%20Thus%2C%20we%20make%20the%0Afirst%20effort%20to%20investigate%20Byzantine%20attacks%20on%20FR%20from%20the%20perspective%20of%0Asparse%20aggregation%2C%20which%20is%20non-trivial%3A%20it%20is%20not%20clear%20how%20to%20define%0AByzantine%20robustness%20under%20sparse%20aggregations%20and%20design%20Byzantine%20attacks%0Aunder%20limited%20knowledge/capability.%20In%20this%20paper%2C%20we%20reformulate%20the%20Byzantine%0Arobustness%20under%20sparse%20aggregation%20by%20defining%20the%20aggregation%20for%20a%20single%0Aitem%20as%20the%20smallest%20execution%20unit.%20Then%20we%20propose%20a%20family%20of%20effective%0Aattack%20strategies%2C%20named%20Spattack%2C%20which%20exploit%20the%20vulnerability%20in%20sparse%0Aaggregation%20and%20are%20categorized%20along%20the%20adversary%27s%20knowledge%20and%20capability.%0AExtensive%20experimental%20results%20demonstrate%20that%20Spattack%20can%20effectively%0Aprevent%20convergence%20and%20even%20break%20down%20defenses%20under%20a%20few%20malicious%20clients%2C%0Araising%20alarms%20for%20securing%20FR%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Byzantine%2520Robustness%2520in%2520Federated%2520Recommendation%2520from%2520Sparse%250A%2520%2520Aggregation%2520Perspective%26entry.906535625%3DZhongjian%2520Zhang%2520and%2520Mengmei%2520Zhang%2520and%2520Xiao%2520Wang%2520and%2520Lingjuan%2520Lyu%2520and%2520Bo%2520Yan%2520and%2520Junping%2520Du%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520To%2520preserve%2520user%2520privacy%2520in%2520recommender%2520systems%252C%2520federated%2520recommendation%250A%2528FR%2529%2520based%2520on%2520federated%2520learning%2520%2528FL%2529%2520emerges%252C%2520keeping%2520the%2520personal%2520data%2520on%2520the%250Alocal%2520client%2520and%2520updating%2520a%2520model%2520collaboratively.%2520Unlike%2520FL%252C%2520FR%2520has%2520a%2520unique%250Asparse%2520aggregation%2520mechanism%252C%2520where%2520the%2520embedding%2520of%2520each%2520item%2520is%2520updated%2520by%250Aonly%2520partial%2520clients%252C%2520instead%2520of%2520full%2520clients%2520in%2520a%2520dense%2520aggregation%2520of%2520general%250AFL.%2520Recently%252C%2520as%2520an%2520essential%2520principle%2520of%2520FL%252C%2520model%2520security%2520has%2520received%250Aincreasing%2520attention%252C%2520especially%2520for%2520Byzantine%2520attacks%252C%2520where%2520malicious%2520clients%250Acan%2520send%2520arbitrary%2520updates.%2520The%2520problem%2520of%2520exploring%2520the%2520Byzantine%2520robustness%250Aof%2520FR%2520is%2520particularly%2520critical%2520since%2520in%2520the%2520domains%2520applying%2520FR%252C%2520e.g.%252C%250Ae-commerce%252C%2520malicious%2520clients%2520can%2520be%2520injected%2520easily%2520by%2520registering%2520new%250Aaccounts.%2520However%252C%2520existing%2520Byzantine%2520works%2520neglect%2520the%2520unique%2520sparse%250Aaggregation%2520of%2520FR%252C%2520making%2520them%2520unsuitable%2520for%2520our%2520problem.%2520Thus%252C%2520we%2520make%2520the%250Afirst%2520effort%2520to%2520investigate%2520Byzantine%2520attacks%2520on%2520FR%2520from%2520the%2520perspective%2520of%250Asparse%2520aggregation%252C%2520which%2520is%2520non-trivial%253A%2520it%2520is%2520not%2520clear%2520how%2520to%2520define%250AByzantine%2520robustness%2520under%2520sparse%2520aggregations%2520and%2520design%2520Byzantine%2520attacks%250Aunder%2520limited%2520knowledge/capability.%2520In%2520this%2520paper%252C%2520we%2520reformulate%2520the%2520Byzantine%250Arobustness%2520under%2520sparse%2520aggregation%2520by%2520defining%2520the%2520aggregation%2520for%2520a%2520single%250Aitem%2520as%2520the%2520smallest%2520execution%2520unit.%2520Then%2520we%2520propose%2520a%2520family%2520of%2520effective%250Aattack%2520strategies%252C%2520named%2520Spattack%252C%2520which%2520exploit%2520the%2520vulnerability%2520in%2520sparse%250Aaggregation%2520and%2520are%2520categorized%2520along%2520the%2520adversary%2527s%2520knowledge%2520and%2520capability.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520Spattack%2520can%2520effectively%250Aprevent%2520convergence%2520and%2520even%2520break%2520down%2520defenses%2520under%2520a%2520few%2520malicious%2520clients%252C%250Araising%2520alarms%2520for%2520securing%2520FR%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Byzantine%20Robustness%20in%20Federated%20Recommendation%20from%20Sparse%0A%20%20Aggregation%20Perspective&entry.906535625=Zhongjian%20Zhang%20and%20Mengmei%20Zhang%20and%20Xiao%20Wang%20and%20Lingjuan%20Lyu%20and%20Bo%20Yan%20and%20Junping%20Du%20and%20Chuan%20Shi&entry.1292438233=%20%20To%20preserve%20user%20privacy%20in%20recommender%20systems%2C%20federated%20recommendation%0A%28FR%29%20based%20on%20federated%20learning%20%28FL%29%20emerges%2C%20keeping%20the%20personal%20data%20on%20the%0Alocal%20client%20and%20updating%20a%20model%20collaboratively.%20Unlike%20FL%2C%20FR%20has%20a%20unique%0Asparse%20aggregation%20mechanism%2C%20where%20the%20embedding%20of%20each%20item%20is%20updated%20by%0Aonly%20partial%20clients%2C%20instead%20of%20full%20clients%20in%20a%20dense%20aggregation%20of%20general%0AFL.%20Recently%2C%20as%20an%20essential%20principle%20of%20FL%2C%20model%20security%20has%20received%0Aincreasing%20attention%2C%20especially%20for%20Byzantine%20attacks%2C%20where%20malicious%20clients%0Acan%20send%20arbitrary%20updates.%20The%20problem%20of%20exploring%20the%20Byzantine%20robustness%0Aof%20FR%20is%20particularly%20critical%20since%20in%20the%20domains%20applying%20FR%2C%20e.g.%2C%0Ae-commerce%2C%20malicious%20clients%20can%20be%20injected%20easily%20by%20registering%20new%0Aaccounts.%20However%2C%20existing%20Byzantine%20works%20neglect%20the%20unique%20sparse%0Aaggregation%20of%20FR%2C%20making%20them%20unsuitable%20for%20our%20problem.%20Thus%2C%20we%20make%20the%0Afirst%20effort%20to%20investigate%20Byzantine%20attacks%20on%20FR%20from%20the%20perspective%20of%0Asparse%20aggregation%2C%20which%20is%20non-trivial%3A%20it%20is%20not%20clear%20how%20to%20define%0AByzantine%20robustness%20under%20sparse%20aggregations%20and%20design%20Byzantine%20attacks%0Aunder%20limited%20knowledge/capability.%20In%20this%20paper%2C%20we%20reformulate%20the%20Byzantine%0Arobustness%20under%20sparse%20aggregation%20by%20defining%20the%20aggregation%20for%20a%20single%0Aitem%20as%20the%20smallest%20execution%20unit.%20Then%20we%20propose%20a%20family%20of%20effective%0Aattack%20strategies%2C%20named%20Spattack%2C%20which%20exploit%20the%20vulnerability%20in%20sparse%0Aaggregation%20and%20are%20categorized%20along%20the%20adversary%27s%20knowledge%20and%20capability.%0AExtensive%20experimental%20results%20demonstrate%20that%20Spattack%20can%20effectively%0Aprevent%20convergence%20and%20even%20break%20down%20defenses%20under%20a%20few%20malicious%20clients%2C%0Araising%20alarms%20for%20securing%20FR%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03301v2&entry.124074799=Read"},
{"title": "MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data\n  Generation", "author": "Daniele Molino and Francesco Di Feola and Eliodoro Faiella and Deborah Fazzini and Domiziana Santucci and Linlin Shen and Valerio Guarrasi and Paolo Soda", "abstract": "  Artificial Intelligence is revolutionizing medical practice, enhancing\ndiagnostic accuracy and healthcare delivery. However, its adaptation in medical\nsettings still faces significant challenges, related to data availability and\nprivacy constraints. Synthetic data has emerged as a promising solution to\nmitigate these issues, addressing data scarcity while preserving privacy.\nRecently, Latent Diffusion Models have emerged as a powerful tool for\ngenerating high-quality synthetic data. Meanwhile, the integration of different\nmodalities has gained interest, emphasizing the need of models capable of\nhandle multimodal medical data.Existing approaches struggle to integrate\ncomplementary information and lack the ability to generate modalities\nsimultaneously. To address this challenge, we present MedCoDi-M, a\n6.77-billion-parameter model, designed for multimodal medical data generation,\nthat, following Foundation Model paradigm, exploits contrastive learning and\nlarge quantity of data to build a shared latent space which capture the\nrelationships between different data modalities. Further, we introduce the\nMulti-Prompt training technique, which significantly boosts MedCoDi-M's\ngeneration under different settings. We extensively validate MedCoDi-M: first\nwe benchmark it against five competitors on the MIMIC-CXR dataset, a\nstate-of-the-art dataset for Chest X-ray and radiological report generation.\nSecondly, we perform a Visual Turing Test with expert radiologists to assess\nthe realism and clinical relevance of the generated data, ensuring alignment\nwith real-world scenarios. Finally, we assess the utility of MedCoDi-M in\naddressing key challenges in the medical field, such as anonymization, data\nscarcity and imbalance learning. The results are promising, demonstrating the\napplicability of MedCoDi-M in medical contexts. Project page is at\nhttps://cosbidev.github.io/MedCoDi-M/.\n", "link": "http://arxiv.org/abs/2501.04614v1", "date": "2025-01-08", "relevancy": 1.1381, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5907}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5614}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedCoDi-M%3A%20A%20Multi-Prompt%20Foundation%20Model%20for%20Multimodal%20Medical%20Data%0A%20%20Generation&body=Title%3A%20MedCoDi-M%3A%20A%20Multi-Prompt%20Foundation%20Model%20for%20Multimodal%20Medical%20Data%0A%20%20Generation%0AAuthor%3A%20Daniele%20Molino%20and%20Francesco%20Di%20Feola%20and%20Eliodoro%20Faiella%20and%20Deborah%20Fazzini%20and%20Domiziana%20Santucci%20and%20Linlin%20Shen%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20Artificial%20Intelligence%20is%20revolutionizing%20medical%20practice%2C%20enhancing%0Adiagnostic%20accuracy%20and%20healthcare%20delivery.%20However%2C%20its%20adaptation%20in%20medical%0Asettings%20still%20faces%20significant%20challenges%2C%20related%20to%20data%20availability%20and%0Aprivacy%20constraints.%20Synthetic%20data%20has%20emerged%20as%20a%20promising%20solution%20to%0Amitigate%20these%20issues%2C%20addressing%20data%20scarcity%20while%20preserving%20privacy.%0ARecently%2C%20Latent%20Diffusion%20Models%20have%20emerged%20as%20a%20powerful%20tool%20for%0Agenerating%20high-quality%20synthetic%20data.%20Meanwhile%2C%20the%20integration%20of%20different%0Amodalities%20has%20gained%20interest%2C%20emphasizing%20the%20need%20of%20models%20capable%20of%0Ahandle%20multimodal%20medical%20data.Existing%20approaches%20struggle%20to%20integrate%0Acomplementary%20information%20and%20lack%20the%20ability%20to%20generate%20modalities%0Asimultaneously.%20To%20address%20this%20challenge%2C%20we%20present%20MedCoDi-M%2C%20a%0A6.77-billion-parameter%20model%2C%20designed%20for%20multimodal%20medical%20data%20generation%2C%0Athat%2C%20following%20Foundation%20Model%20paradigm%2C%20exploits%20contrastive%20learning%20and%0Alarge%20quantity%20of%20data%20to%20build%20a%20shared%20latent%20space%20which%20capture%20the%0Arelationships%20between%20different%20data%20modalities.%20Further%2C%20we%20introduce%20the%0AMulti-Prompt%20training%20technique%2C%20which%20significantly%20boosts%20MedCoDi-M%27s%0Ageneration%20under%20different%20settings.%20We%20extensively%20validate%20MedCoDi-M%3A%20first%0Awe%20benchmark%20it%20against%20five%20competitors%20on%20the%20MIMIC-CXR%20dataset%2C%20a%0Astate-of-the-art%20dataset%20for%20Chest%20X-ray%20and%20radiological%20report%20generation.%0ASecondly%2C%20we%20perform%20a%20Visual%20Turing%20Test%20with%20expert%20radiologists%20to%20assess%0Athe%20realism%20and%20clinical%20relevance%20of%20the%20generated%20data%2C%20ensuring%20alignment%0Awith%20real-world%20scenarios.%20Finally%2C%20we%20assess%20the%20utility%20of%20MedCoDi-M%20in%0Aaddressing%20key%20challenges%20in%20the%20medical%20field%2C%20such%20as%20anonymization%2C%20data%0Ascarcity%20and%20imbalance%20learning.%20The%20results%20are%20promising%2C%20demonstrating%20the%0Aapplicability%20of%20MedCoDi-M%20in%20medical%20contexts.%20Project%20page%20is%20at%0Ahttps%3A//cosbidev.github.io/MedCoDi-M/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedCoDi-M%253A%2520A%2520Multi-Prompt%2520Foundation%2520Model%2520for%2520Multimodal%2520Medical%2520Data%250A%2520%2520Generation%26entry.906535625%3DDaniele%2520Molino%2520and%2520Francesco%2520Di%2520Feola%2520and%2520Eliodoro%2520Faiella%2520and%2520Deborah%2520Fazzini%2520and%2520Domiziana%2520Santucci%2520and%2520Linlin%2520Shen%2520and%2520Valerio%2520Guarrasi%2520and%2520Paolo%2520Soda%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520is%2520revolutionizing%2520medical%2520practice%252C%2520enhancing%250Adiagnostic%2520accuracy%2520and%2520healthcare%2520delivery.%2520However%252C%2520its%2520adaptation%2520in%2520medical%250Asettings%2520still%2520faces%2520significant%2520challenges%252C%2520related%2520to%2520data%2520availability%2520and%250Aprivacy%2520constraints.%2520Synthetic%2520data%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%250Amitigate%2520these%2520issues%252C%2520addressing%2520data%2520scarcity%2520while%2520preserving%2520privacy.%250ARecently%252C%2520Latent%2520Diffusion%2520Models%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%250Agenerating%2520high-quality%2520synthetic%2520data.%2520Meanwhile%252C%2520the%2520integration%2520of%2520different%250Amodalities%2520has%2520gained%2520interest%252C%2520emphasizing%2520the%2520need%2520of%2520models%2520capable%2520of%250Ahandle%2520multimodal%2520medical%2520data.Existing%2520approaches%2520struggle%2520to%2520integrate%250Acomplementary%2520information%2520and%2520lack%2520the%2520ability%2520to%2520generate%2520modalities%250Asimultaneously.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520MedCoDi-M%252C%2520a%250A6.77-billion-parameter%2520model%252C%2520designed%2520for%2520multimodal%2520medical%2520data%2520generation%252C%250Athat%252C%2520following%2520Foundation%2520Model%2520paradigm%252C%2520exploits%2520contrastive%2520learning%2520and%250Alarge%2520quantity%2520of%2520data%2520to%2520build%2520a%2520shared%2520latent%2520space%2520which%2520capture%2520the%250Arelationships%2520between%2520different%2520data%2520modalities.%2520Further%252C%2520we%2520introduce%2520the%250AMulti-Prompt%2520training%2520technique%252C%2520which%2520significantly%2520boosts%2520MedCoDi-M%2527s%250Ageneration%2520under%2520different%2520settings.%2520We%2520extensively%2520validate%2520MedCoDi-M%253A%2520first%250Awe%2520benchmark%2520it%2520against%2520five%2520competitors%2520on%2520the%2520MIMIC-CXR%2520dataset%252C%2520a%250Astate-of-the-art%2520dataset%2520for%2520Chest%2520X-ray%2520and%2520radiological%2520report%2520generation.%250ASecondly%252C%2520we%2520perform%2520a%2520Visual%2520Turing%2520Test%2520with%2520expert%2520radiologists%2520to%2520assess%250Athe%2520realism%2520and%2520clinical%2520relevance%2520of%2520the%2520generated%2520data%252C%2520ensuring%2520alignment%250Awith%2520real-world%2520scenarios.%2520Finally%252C%2520we%2520assess%2520the%2520utility%2520of%2520MedCoDi-M%2520in%250Aaddressing%2520key%2520challenges%2520in%2520the%2520medical%2520field%252C%2520such%2520as%2520anonymization%252C%2520data%250Ascarcity%2520and%2520imbalance%2520learning.%2520The%2520results%2520are%2520promising%252C%2520demonstrating%2520the%250Aapplicability%2520of%2520MedCoDi-M%2520in%2520medical%2520contexts.%2520Project%2520page%2520is%2520at%250Ahttps%253A//cosbidev.github.io/MedCoDi-M/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedCoDi-M%3A%20A%20Multi-Prompt%20Foundation%20Model%20for%20Multimodal%20Medical%20Data%0A%20%20Generation&entry.906535625=Daniele%20Molino%20and%20Francesco%20Di%20Feola%20and%20Eliodoro%20Faiella%20and%20Deborah%20Fazzini%20and%20Domiziana%20Santucci%20and%20Linlin%20Shen%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda&entry.1292438233=%20%20Artificial%20Intelligence%20is%20revolutionizing%20medical%20practice%2C%20enhancing%0Adiagnostic%20accuracy%20and%20healthcare%20delivery.%20However%2C%20its%20adaptation%20in%20medical%0Asettings%20still%20faces%20significant%20challenges%2C%20related%20to%20data%20availability%20and%0Aprivacy%20constraints.%20Synthetic%20data%20has%20emerged%20as%20a%20promising%20solution%20to%0Amitigate%20these%20issues%2C%20addressing%20data%20scarcity%20while%20preserving%20privacy.%0ARecently%2C%20Latent%20Diffusion%20Models%20have%20emerged%20as%20a%20powerful%20tool%20for%0Agenerating%20high-quality%20synthetic%20data.%20Meanwhile%2C%20the%20integration%20of%20different%0Amodalities%20has%20gained%20interest%2C%20emphasizing%20the%20need%20of%20models%20capable%20of%0Ahandle%20multimodal%20medical%20data.Existing%20approaches%20struggle%20to%20integrate%0Acomplementary%20information%20and%20lack%20the%20ability%20to%20generate%20modalities%0Asimultaneously.%20To%20address%20this%20challenge%2C%20we%20present%20MedCoDi-M%2C%20a%0A6.77-billion-parameter%20model%2C%20designed%20for%20multimodal%20medical%20data%20generation%2C%0Athat%2C%20following%20Foundation%20Model%20paradigm%2C%20exploits%20contrastive%20learning%20and%0Alarge%20quantity%20of%20data%20to%20build%20a%20shared%20latent%20space%20which%20capture%20the%0Arelationships%20between%20different%20data%20modalities.%20Further%2C%20we%20introduce%20the%0AMulti-Prompt%20training%20technique%2C%20which%20significantly%20boosts%20MedCoDi-M%27s%0Ageneration%20under%20different%20settings.%20We%20extensively%20validate%20MedCoDi-M%3A%20first%0Awe%20benchmark%20it%20against%20five%20competitors%20on%20the%20MIMIC-CXR%20dataset%2C%20a%0Astate-of-the-art%20dataset%20for%20Chest%20X-ray%20and%20radiological%20report%20generation.%0ASecondly%2C%20we%20perform%20a%20Visual%20Turing%20Test%20with%20expert%20radiologists%20to%20assess%0Athe%20realism%20and%20clinical%20relevance%20of%20the%20generated%20data%2C%20ensuring%20alignment%0Awith%20real-world%20scenarios.%20Finally%2C%20we%20assess%20the%20utility%20of%20MedCoDi-M%20in%0Aaddressing%20key%20challenges%20in%20the%20medical%20field%2C%20such%20as%20anonymization%2C%20data%0Ascarcity%20and%20imbalance%20learning.%20The%20results%20are%20promising%2C%20demonstrating%20the%0Aapplicability%20of%20MedCoDi-M%20in%20medical%20contexts.%20Project%20page%20is%20at%0Ahttps%3A//cosbidev.github.io/MedCoDi-M/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04614v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


