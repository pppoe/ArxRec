<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251016.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian\n  Splatting Training on GPU", "author": "Junyi Wu and Jiaming Xu and Jinhao Li and Yongkang Zhou and Jiayi Pan and Xingyang Li and Guohao Dai", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction\ntechnique. The traditional 3DGS training pipeline follows three sequential\nsteps: Gaussian densification, Gaussian projection, and color splatting.\nDespite its promising reconstruction quality, this conventional approach\nsuffers from three critical inefficiencies: (1) Skewed density allocation\nduring Gaussian densification, (2) Imbalanced computation workload during\nGaussian projection and (3) Fragmented memory access during color splatting.\n  To tackle the above challenges, we introduce BalanceGS, the algorithm-system\nco-design for efficient training in 3DGS. (1) At the algorithm level, we\npropose heuristic workload-sensitive Gaussian density control to automatically\nbalance point distributions - removing 80% redundant Gaussians in dense regions\nwhile filling gaps in sparse areas. (2) At the system level, we propose\nSimilarity-based Gaussian sampling and merging, which replaces the static\none-to-one thread-pixel mapping with adaptive workload distribution - threads\nnow dynamically process variable numbers of Gaussians based on local cluster\ndensity. (3) At the mapping level, we propose reordering-based memory access\nmapping strategy that restructures RGB storage and enables batch loading in\nshared memory.\n  Extensive experiments demonstrate that compared with 3DGS, our approach\nachieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible\nquality degradation.\n", "link": "http://arxiv.org/abs/2510.14564v1", "date": "2025-10-16", "relevancy": 3.4703, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7433}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6758}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BalanceGS%3A%20Algorithm-System%20Co-design%20for%20Efficient%203D%20Gaussian%0A%20%20Splatting%20Training%20on%20GPU&body=Title%3A%20BalanceGS%3A%20Algorithm-System%20Co-design%20for%20Efficient%203D%20Gaussian%0A%20%20Splatting%20Training%20on%20GPU%0AAuthor%3A%20Junyi%20Wu%20and%20Jiaming%20Xu%20and%20Jinhao%20Li%20and%20Yongkang%20Zhou%20and%20Jiayi%20Pan%20and%20Xingyang%20Li%20and%20Guohao%20Dai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%203D%20reconstruction%0Atechnique.%20The%20traditional%203DGS%20training%20pipeline%20follows%20three%20sequential%0Asteps%3A%20Gaussian%20densification%2C%20Gaussian%20projection%2C%20and%20color%20splatting.%0ADespite%20its%20promising%20reconstruction%20quality%2C%20this%20conventional%20approach%0Asuffers%20from%20three%20critical%20inefficiencies%3A%20%281%29%20Skewed%20density%20allocation%0Aduring%20Gaussian%20densification%2C%20%282%29%20Imbalanced%20computation%20workload%20during%0AGaussian%20projection%20and%20%283%29%20Fragmented%20memory%20access%20during%20color%20splatting.%0A%20%20To%20tackle%20the%20above%20challenges%2C%20we%20introduce%20BalanceGS%2C%20the%20algorithm-system%0Aco-design%20for%20efficient%20training%20in%203DGS.%20%281%29%20At%20the%20algorithm%20level%2C%20we%0Apropose%20heuristic%20workload-sensitive%20Gaussian%20density%20control%20to%20automatically%0Abalance%20point%20distributions%20-%20removing%2080%25%20redundant%20Gaussians%20in%20dense%20regions%0Awhile%20filling%20gaps%20in%20sparse%20areas.%20%282%29%20At%20the%20system%20level%2C%20we%20propose%0ASimilarity-based%20Gaussian%20sampling%20and%20merging%2C%20which%20replaces%20the%20static%0Aone-to-one%20thread-pixel%20mapping%20with%20adaptive%20workload%20distribution%20-%20threads%0Anow%20dynamically%20process%20variable%20numbers%20of%20Gaussians%20based%20on%20local%20cluster%0Adensity.%20%283%29%20At%20the%20mapping%20level%2C%20we%20propose%20reordering-based%20memory%20access%0Amapping%20strategy%20that%20restructures%20RGB%20storage%20and%20enables%20batch%20loading%20in%0Ashared%20memory.%0A%20%20Extensive%20experiments%20demonstrate%20that%20compared%20with%203DGS%2C%20our%20approach%0Aachieves%20a%201.44%24%5Ctimes%24%20training%20speedup%20on%20a%20NVIDIA%20A100%20GPU%20with%20negligible%0Aquality%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalanceGS%253A%2520Algorithm-System%2520Co-design%2520for%2520Efficient%25203D%2520Gaussian%250A%2520%2520Splatting%2520Training%2520on%2520GPU%26entry.906535625%3DJunyi%2520Wu%2520and%2520Jiaming%2520Xu%2520and%2520Jinhao%2520Li%2520and%2520Yongkang%2520Zhou%2520and%2520Jiayi%2520Pan%2520and%2520Xingyang%2520Li%2520and%2520Guohao%2520Dai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%25203D%2520reconstruction%250Atechnique.%2520The%2520traditional%25203DGS%2520training%2520pipeline%2520follows%2520three%2520sequential%250Asteps%253A%2520Gaussian%2520densification%252C%2520Gaussian%2520projection%252C%2520and%2520color%2520splatting.%250ADespite%2520its%2520promising%2520reconstruction%2520quality%252C%2520this%2520conventional%2520approach%250Asuffers%2520from%2520three%2520critical%2520inefficiencies%253A%2520%25281%2529%2520Skewed%2520density%2520allocation%250Aduring%2520Gaussian%2520densification%252C%2520%25282%2529%2520Imbalanced%2520computation%2520workload%2520during%250AGaussian%2520projection%2520and%2520%25283%2529%2520Fragmented%2520memory%2520access%2520during%2520color%2520splatting.%250A%2520%2520To%2520tackle%2520the%2520above%2520challenges%252C%2520we%2520introduce%2520BalanceGS%252C%2520the%2520algorithm-system%250Aco-design%2520for%2520efficient%2520training%2520in%25203DGS.%2520%25281%2529%2520At%2520the%2520algorithm%2520level%252C%2520we%250Apropose%2520heuristic%2520workload-sensitive%2520Gaussian%2520density%2520control%2520to%2520automatically%250Abalance%2520point%2520distributions%2520-%2520removing%252080%2525%2520redundant%2520Gaussians%2520in%2520dense%2520regions%250Awhile%2520filling%2520gaps%2520in%2520sparse%2520areas.%2520%25282%2529%2520At%2520the%2520system%2520level%252C%2520we%2520propose%250ASimilarity-based%2520Gaussian%2520sampling%2520and%2520merging%252C%2520which%2520replaces%2520the%2520static%250Aone-to-one%2520thread-pixel%2520mapping%2520with%2520adaptive%2520workload%2520distribution%2520-%2520threads%250Anow%2520dynamically%2520process%2520variable%2520numbers%2520of%2520Gaussians%2520based%2520on%2520local%2520cluster%250Adensity.%2520%25283%2529%2520At%2520the%2520mapping%2520level%252C%2520we%2520propose%2520reordering-based%2520memory%2520access%250Amapping%2520strategy%2520that%2520restructures%2520RGB%2520storage%2520and%2520enables%2520batch%2520loading%2520in%250Ashared%2520memory.%250A%2520%2520Extensive%2520experiments%2520demonstrate%2520that%2520compared%2520with%25203DGS%252C%2520our%2520approach%250Aachieves%2520a%25201.44%2524%255Ctimes%2524%2520training%2520speedup%2520on%2520a%2520NVIDIA%2520A100%2520GPU%2520with%2520negligible%250Aquality%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BalanceGS%3A%20Algorithm-System%20Co-design%20for%20Efficient%203D%20Gaussian%0A%20%20Splatting%20Training%20on%20GPU&entry.906535625=Junyi%20Wu%20and%20Jiaming%20Xu%20and%20Jinhao%20Li%20and%20Yongkang%20Zhou%20and%20Jiayi%20Pan%20and%20Xingyang%20Li%20and%20Guohao%20Dai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%203D%20reconstruction%0Atechnique.%20The%20traditional%203DGS%20training%20pipeline%20follows%20three%20sequential%0Asteps%3A%20Gaussian%20densification%2C%20Gaussian%20projection%2C%20and%20color%20splatting.%0ADespite%20its%20promising%20reconstruction%20quality%2C%20this%20conventional%20approach%0Asuffers%20from%20three%20critical%20inefficiencies%3A%20%281%29%20Skewed%20density%20allocation%0Aduring%20Gaussian%20densification%2C%20%282%29%20Imbalanced%20computation%20workload%20during%0AGaussian%20projection%20and%20%283%29%20Fragmented%20memory%20access%20during%20color%20splatting.%0A%20%20To%20tackle%20the%20above%20challenges%2C%20we%20introduce%20BalanceGS%2C%20the%20algorithm-system%0Aco-design%20for%20efficient%20training%20in%203DGS.%20%281%29%20At%20the%20algorithm%20level%2C%20we%0Apropose%20heuristic%20workload-sensitive%20Gaussian%20density%20control%20to%20automatically%0Abalance%20point%20distributions%20-%20removing%2080%25%20redundant%20Gaussians%20in%20dense%20regions%0Awhile%20filling%20gaps%20in%20sparse%20areas.%20%282%29%20At%20the%20system%20level%2C%20we%20propose%0ASimilarity-based%20Gaussian%20sampling%20and%20merging%2C%20which%20replaces%20the%20static%0Aone-to-one%20thread-pixel%20mapping%20with%20adaptive%20workload%20distribution%20-%20threads%0Anow%20dynamically%20process%20variable%20numbers%20of%20Gaussians%20based%20on%20local%20cluster%0Adensity.%20%283%29%20At%20the%20mapping%20level%2C%20we%20propose%20reordering-based%20memory%20access%0Amapping%20strategy%20that%20restructures%20RGB%20storage%20and%20enables%20batch%20loading%20in%0Ashared%20memory.%0A%20%20Extensive%20experiments%20demonstrate%20that%20compared%20with%203DGS%2C%20our%20approach%0Aachieves%20a%201.44%24%5Ctimes%24%20training%20speedup%20on%20a%20NVIDIA%20A100%20GPU%20with%20negligible%0Aquality%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14564v1&entry.124074799=Read"},
{"title": "Leveraging Learned Image Prior for 3D Gaussian Compression", "author": "Seungjoo Shin and Jaesik Park and Sunghyun Cho", "abstract": "  Compression techniques for 3D Gaussian Splatting (3DGS) have recently\nachieved considerable success in minimizing storage overhead for 3D Gaussians\nwhile preserving high rendering quality. Despite the impressive storage\nreduction, the lack of learned priors restricts further advances in the\nrate-distortion trade-off for 3DGS compression tasks. To address this, we\nintroduce a novel 3DGS compression framework that leverages the powerful\nrepresentational capacity of learned image priors to recover\ncompression-induced quality degradation. Built upon initially compressed\nGaussians, our restoration network effectively models the compression artifacts\nin the image space between degraded and original Gaussians. To enhance the\nrate-distortion performance, we provide coarse rendering residuals into the\nrestoration network as side information. By leveraging the supervision of\nrestored images, the compressed Gaussians are refined, resulting in a highly\ncompact representation with enhanced rendering performance. Our framework is\ndesigned to be compatible with existing Gaussian compression methods, making it\nbroadly applicable across different baselines. Extensive experiments validate\nthe effectiveness of our framework, demonstrating superior rate-distortion\nperformance and outperforming the rendering quality of state-of-the-art 3DGS\ncompression methods while requiring substantially less storage.\n", "link": "http://arxiv.org/abs/2510.14705v1", "date": "2025-10-16", "relevancy": 3.1238, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6362}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6203}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Learned%20Image%20Prior%20for%203D%20Gaussian%20Compression&body=Title%3A%20Leveraging%20Learned%20Image%20Prior%20for%203D%20Gaussian%20Compression%0AAuthor%3A%20Seungjoo%20Shin%20and%20Jaesik%20Park%20and%20Sunghyun%20Cho%0AAbstract%3A%20%20%20Compression%20techniques%20for%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20recently%0Aachieved%20considerable%20success%20in%20minimizing%20storage%20overhead%20for%203D%20Gaussians%0Awhile%20preserving%20high%20rendering%20quality.%20Despite%20the%20impressive%20storage%0Areduction%2C%20the%20lack%20of%20learned%20priors%20restricts%20further%20advances%20in%20the%0Arate-distortion%20trade-off%20for%203DGS%20compression%20tasks.%20To%20address%20this%2C%20we%0Aintroduce%20a%20novel%203DGS%20compression%20framework%20that%20leverages%20the%20powerful%0Arepresentational%20capacity%20of%20learned%20image%20priors%20to%20recover%0Acompression-induced%20quality%20degradation.%20Built%20upon%20initially%20compressed%0AGaussians%2C%20our%20restoration%20network%20effectively%20models%20the%20compression%20artifacts%0Ain%20the%20image%20space%20between%20degraded%20and%20original%20Gaussians.%20To%20enhance%20the%0Arate-distortion%20performance%2C%20we%20provide%20coarse%20rendering%20residuals%20into%20the%0Arestoration%20network%20as%20side%20information.%20By%20leveraging%20the%20supervision%20of%0Arestored%20images%2C%20the%20compressed%20Gaussians%20are%20refined%2C%20resulting%20in%20a%20highly%0Acompact%20representation%20with%20enhanced%20rendering%20performance.%20Our%20framework%20is%0Adesigned%20to%20be%20compatible%20with%20existing%20Gaussian%20compression%20methods%2C%20making%20it%0Abroadly%20applicable%20across%20different%20baselines.%20Extensive%20experiments%20validate%0Athe%20effectiveness%20of%20our%20framework%2C%20demonstrating%20superior%20rate-distortion%0Aperformance%20and%20outperforming%20the%20rendering%20quality%20of%20state-of-the-art%203DGS%0Acompression%20methods%20while%20requiring%20substantially%20less%20storage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Learned%2520Image%2520Prior%2520for%25203D%2520Gaussian%2520Compression%26entry.906535625%3DSeungjoo%2520Shin%2520and%2520Jaesik%2520Park%2520and%2520Sunghyun%2520Cho%26entry.1292438233%3D%2520%2520Compression%2520techniques%2520for%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520recently%250Aachieved%2520considerable%2520success%2520in%2520minimizing%2520storage%2520overhead%2520for%25203D%2520Gaussians%250Awhile%2520preserving%2520high%2520rendering%2520quality.%2520Despite%2520the%2520impressive%2520storage%250Areduction%252C%2520the%2520lack%2520of%2520learned%2520priors%2520restricts%2520further%2520advances%2520in%2520the%250Arate-distortion%2520trade-off%2520for%25203DGS%2520compression%2520tasks.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520novel%25203DGS%2520compression%2520framework%2520that%2520leverages%2520the%2520powerful%250Arepresentational%2520capacity%2520of%2520learned%2520image%2520priors%2520to%2520recover%250Acompression-induced%2520quality%2520degradation.%2520Built%2520upon%2520initially%2520compressed%250AGaussians%252C%2520our%2520restoration%2520network%2520effectively%2520models%2520the%2520compression%2520artifacts%250Ain%2520the%2520image%2520space%2520between%2520degraded%2520and%2520original%2520Gaussians.%2520To%2520enhance%2520the%250Arate-distortion%2520performance%252C%2520we%2520provide%2520coarse%2520rendering%2520residuals%2520into%2520the%250Arestoration%2520network%2520as%2520side%2520information.%2520By%2520leveraging%2520the%2520supervision%2520of%250Arestored%2520images%252C%2520the%2520compressed%2520Gaussians%2520are%2520refined%252C%2520resulting%2520in%2520a%2520highly%250Acompact%2520representation%2520with%2520enhanced%2520rendering%2520performance.%2520Our%2520framework%2520is%250Adesigned%2520to%2520be%2520compatible%2520with%2520existing%2520Gaussian%2520compression%2520methods%252C%2520making%2520it%250Abroadly%2520applicable%2520across%2520different%2520baselines.%2520Extensive%2520experiments%2520validate%250Athe%2520effectiveness%2520of%2520our%2520framework%252C%2520demonstrating%2520superior%2520rate-distortion%250Aperformance%2520and%2520outperforming%2520the%2520rendering%2520quality%2520of%2520state-of-the-art%25203DGS%250Acompression%2520methods%2520while%2520requiring%2520substantially%2520less%2520storage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Learned%20Image%20Prior%20for%203D%20Gaussian%20Compression&entry.906535625=Seungjoo%20Shin%20and%20Jaesik%20Park%20and%20Sunghyun%20Cho&entry.1292438233=%20%20Compression%20techniques%20for%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20recently%0Aachieved%20considerable%20success%20in%20minimizing%20storage%20overhead%20for%203D%20Gaussians%0Awhile%20preserving%20high%20rendering%20quality.%20Despite%20the%20impressive%20storage%0Areduction%2C%20the%20lack%20of%20learned%20priors%20restricts%20further%20advances%20in%20the%0Arate-distortion%20trade-off%20for%203DGS%20compression%20tasks.%20To%20address%20this%2C%20we%0Aintroduce%20a%20novel%203DGS%20compression%20framework%20that%20leverages%20the%20powerful%0Arepresentational%20capacity%20of%20learned%20image%20priors%20to%20recover%0Acompression-induced%20quality%20degradation.%20Built%20upon%20initially%20compressed%0AGaussians%2C%20our%20restoration%20network%20effectively%20models%20the%20compression%20artifacts%0Ain%20the%20image%20space%20between%20degraded%20and%20original%20Gaussians.%20To%20enhance%20the%0Arate-distortion%20performance%2C%20we%20provide%20coarse%20rendering%20residuals%20into%20the%0Arestoration%20network%20as%20side%20information.%20By%20leveraging%20the%20supervision%20of%0Arestored%20images%2C%20the%20compressed%20Gaussians%20are%20refined%2C%20resulting%20in%20a%20highly%0Acompact%20representation%20with%20enhanced%20rendering%20performance.%20Our%20framework%20is%0Adesigned%20to%20be%20compatible%20with%20existing%20Gaussian%20compression%20methods%2C%20making%20it%0Abroadly%20applicable%20across%20different%20baselines.%20Extensive%20experiments%20validate%0Athe%20effectiveness%20of%20our%20framework%2C%20demonstrating%20superior%20rate-distortion%0Aperformance%20and%20outperforming%20the%20rendering%20quality%20of%20state-of-the-art%203DGS%0Acompression%20methods%20while%20requiring%20substantially%20less%20storage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14705v1&entry.124074799=Read"},
{"title": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided\n  Autoregressive Perspective", "author": "Yuchen Deng and Xiuyang Wu and Hai-Tao Zheng and Suiyang Zhang and Yi He and Yuxing Han", "abstract": "  Talking-head animation focuses on generating realistic facial videos from\naudio input. Following Generative Adversarial Networks (GANs), diffusion models\nhave become the mainstream, owing to their robust generative capacities.\nHowever, inherent limitations of the diffusion process often lead to\ninter-frame flicker and slow inference, restricting their practical deployment.\nTo address this, we introduce AvatarSync, an autoregressive framework on\nphoneme representations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly by text or audio\ninput. To mitigate flicker and ensure continuity, AvatarSync leverages an\nautoregressive pipeline that enhances temporal modeling. In addition, to ensure\ncontrollability, we introduce phonemes, which are the basic units of speech\nsounds, and construct a many-to-one mapping from text/audio to phonemes,\nenabling precise phoneme-to-visual alignment. Additionally, to further\naccelerate inference, we adopt a two-stage generation strategy that decouples\nsemantic modeling from visual dynamics, and incorporate a customized\nPhoneme-Frame Causal Attention Mask to support multi-step parallel\nacceleration. Extensive experiments conducted on both Chinese (CMLR) and\nEnglish (HDTF) datasets demonstrate that AvatarSync outperforms existing\ntalking-head animation methods in visual fidelity, temporal consistency, and\ncomputational efficiency, providing a scalable and controllable solution.\n", "link": "http://arxiv.org/abs/2509.12052v2", "date": "2025-10-16", "relevancy": 3.1071, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.631}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.631}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvatarSync%3A%20Rethinking%20Talking-Head%20Animation%20through%20Phoneme-Guided%0A%20%20Autoregressive%20Perspective&body=Title%3A%20AvatarSync%3A%20Rethinking%20Talking-Head%20Animation%20through%20Phoneme-Guided%0A%20%20Autoregressive%20Perspective%0AAuthor%3A%20Yuchen%20Deng%20and%20Xiuyang%20Wu%20and%20Hai-Tao%20Zheng%20and%20Suiyang%20Zhang%20and%20Yi%20He%20and%20Yuxing%20Han%0AAbstract%3A%20%20%20Talking-head%20animation%20focuses%20on%20generating%20realistic%20facial%20videos%20from%0Aaudio%20input.%20Following%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20diffusion%20models%0Ahave%20become%20the%20mainstream%2C%20owing%20to%20their%20robust%20generative%20capacities.%0AHowever%2C%20inherent%20limitations%20of%20the%20diffusion%20process%20often%20lead%20to%0Ainter-frame%20flicker%20and%20slow%20inference%2C%20restricting%20their%20practical%20deployment.%0ATo%20address%20this%2C%20we%20introduce%20AvatarSync%2C%20an%20autoregressive%20framework%20on%0Aphoneme%20representations%20that%20generates%20realistic%20and%20controllable%20talking-head%0Aanimations%20from%20a%20single%20reference%20image%2C%20driven%20directly%20by%20text%20or%20audio%0Ainput.%20To%20mitigate%20flicker%20and%20ensure%20continuity%2C%20AvatarSync%20leverages%20an%0Aautoregressive%20pipeline%20that%20enhances%20temporal%20modeling.%20In%20addition%2C%20to%20ensure%0Acontrollability%2C%20we%20introduce%20phonemes%2C%20which%20are%20the%20basic%20units%20of%20speech%0Asounds%2C%20and%20construct%20a%20many-to-one%20mapping%20from%20text/audio%20to%20phonemes%2C%0Aenabling%20precise%20phoneme-to-visual%20alignment.%20Additionally%2C%20to%20further%0Aaccelerate%20inference%2C%20we%20adopt%20a%20two-stage%20generation%20strategy%20that%20decouples%0Asemantic%20modeling%20from%20visual%20dynamics%2C%20and%20incorporate%20a%20customized%0APhoneme-Frame%20Causal%20Attention%20Mask%20to%20support%20multi-step%20parallel%0Aacceleration.%20Extensive%20experiments%20conducted%20on%20both%20Chinese%20%28CMLR%29%20and%0AEnglish%20%28HDTF%29%20datasets%20demonstrate%20that%20AvatarSync%20outperforms%20existing%0Atalking-head%20animation%20methods%20in%20visual%20fidelity%2C%20temporal%20consistency%2C%20and%0Acomputational%20efficiency%2C%20providing%20a%20scalable%20and%20controllable%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatarSync%253A%2520Rethinking%2520Talking-Head%2520Animation%2520through%2520Phoneme-Guided%250A%2520%2520Autoregressive%2520Perspective%26entry.906535625%3DYuchen%2520Deng%2520and%2520Xiuyang%2520Wu%2520and%2520Hai-Tao%2520Zheng%2520and%2520Suiyang%2520Zhang%2520and%2520Yi%2520He%2520and%2520Yuxing%2520Han%26entry.1292438233%3D%2520%2520Talking-head%2520animation%2520focuses%2520on%2520generating%2520realistic%2520facial%2520videos%2520from%250Aaudio%2520input.%2520Following%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%252C%2520diffusion%2520models%250Ahave%2520become%2520the%2520mainstream%252C%2520owing%2520to%2520their%2520robust%2520generative%2520capacities.%250AHowever%252C%2520inherent%2520limitations%2520of%2520the%2520diffusion%2520process%2520often%2520lead%2520to%250Ainter-frame%2520flicker%2520and%2520slow%2520inference%252C%2520restricting%2520their%2520practical%2520deployment.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520AvatarSync%252C%2520an%2520autoregressive%2520framework%2520on%250Aphoneme%2520representations%2520that%2520generates%2520realistic%2520and%2520controllable%2520talking-head%250Aanimations%2520from%2520a%2520single%2520reference%2520image%252C%2520driven%2520directly%2520by%2520text%2520or%2520audio%250Ainput.%2520To%2520mitigate%2520flicker%2520and%2520ensure%2520continuity%252C%2520AvatarSync%2520leverages%2520an%250Aautoregressive%2520pipeline%2520that%2520enhances%2520temporal%2520modeling.%2520In%2520addition%252C%2520to%2520ensure%250Acontrollability%252C%2520we%2520introduce%2520phonemes%252C%2520which%2520are%2520the%2520basic%2520units%2520of%2520speech%250Asounds%252C%2520and%2520construct%2520a%2520many-to-one%2520mapping%2520from%2520text/audio%2520to%2520phonemes%252C%250Aenabling%2520precise%2520phoneme-to-visual%2520alignment.%2520Additionally%252C%2520to%2520further%250Aaccelerate%2520inference%252C%2520we%2520adopt%2520a%2520two-stage%2520generation%2520strategy%2520that%2520decouples%250Asemantic%2520modeling%2520from%2520visual%2520dynamics%252C%2520and%2520incorporate%2520a%2520customized%250APhoneme-Frame%2520Causal%2520Attention%2520Mask%2520to%2520support%2520multi-step%2520parallel%250Aacceleration.%2520Extensive%2520experiments%2520conducted%2520on%2520both%2520Chinese%2520%2528CMLR%2529%2520and%250AEnglish%2520%2528HDTF%2529%2520datasets%2520demonstrate%2520that%2520AvatarSync%2520outperforms%2520existing%250Atalking-head%2520animation%2520methods%2520in%2520visual%2520fidelity%252C%2520temporal%2520consistency%252C%2520and%250Acomputational%2520efficiency%252C%2520providing%2520a%2520scalable%2520and%2520controllable%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvatarSync%3A%20Rethinking%20Talking-Head%20Animation%20through%20Phoneme-Guided%0A%20%20Autoregressive%20Perspective&entry.906535625=Yuchen%20Deng%20and%20Xiuyang%20Wu%20and%20Hai-Tao%20Zheng%20and%20Suiyang%20Zhang%20and%20Yi%20He%20and%20Yuxing%20Han&entry.1292438233=%20%20Talking-head%20animation%20focuses%20on%20generating%20realistic%20facial%20videos%20from%0Aaudio%20input.%20Following%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20diffusion%20models%0Ahave%20become%20the%20mainstream%2C%20owing%20to%20their%20robust%20generative%20capacities.%0AHowever%2C%20inherent%20limitations%20of%20the%20diffusion%20process%20often%20lead%20to%0Ainter-frame%20flicker%20and%20slow%20inference%2C%20restricting%20their%20practical%20deployment.%0ATo%20address%20this%2C%20we%20introduce%20AvatarSync%2C%20an%20autoregressive%20framework%20on%0Aphoneme%20representations%20that%20generates%20realistic%20and%20controllable%20talking-head%0Aanimations%20from%20a%20single%20reference%20image%2C%20driven%20directly%20by%20text%20or%20audio%0Ainput.%20To%20mitigate%20flicker%20and%20ensure%20continuity%2C%20AvatarSync%20leverages%20an%0Aautoregressive%20pipeline%20that%20enhances%20temporal%20modeling.%20In%20addition%2C%20to%20ensure%0Acontrollability%2C%20we%20introduce%20phonemes%2C%20which%20are%20the%20basic%20units%20of%20speech%0Asounds%2C%20and%20construct%20a%20many-to-one%20mapping%20from%20text/audio%20to%20phonemes%2C%0Aenabling%20precise%20phoneme-to-visual%20alignment.%20Additionally%2C%20to%20further%0Aaccelerate%20inference%2C%20we%20adopt%20a%20two-stage%20generation%20strategy%20that%20decouples%0Asemantic%20modeling%20from%20visual%20dynamics%2C%20and%20incorporate%20a%20customized%0APhoneme-Frame%20Causal%20Attention%20Mask%20to%20support%20multi-step%20parallel%0Aacceleration.%20Extensive%20experiments%20conducted%20on%20both%20Chinese%20%28CMLR%29%20and%0AEnglish%20%28HDTF%29%20datasets%20demonstrate%20that%20AvatarSync%20outperforms%20existing%0Atalking-head%20animation%20methods%20in%20visual%20fidelity%2C%20temporal%20consistency%2C%20and%0Acomputational%20efficiency%2C%20providing%20a%20scalable%20and%20controllable%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12052v2&entry.124074799=Read"},
{"title": "TTT3R: 3D Reconstruction as Test-Time Training", "author": "Xingyu Chen and Yue Chen and Yuliang Xiu and Andreas Geiger and Anpei Chen", "abstract": "  Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R\n", "link": "http://arxiv.org/abs/2509.26645v3", "date": "2025-10-16", "relevancy": 3.0765, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6095}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training&body=Title%3A%20TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training%0AAuthor%3A%20Xingyu%20Chen%20and%20Yue%20Chen%20and%20Yuliang%20Xiu%20and%20Andreas%20Geiger%20and%20Anpei%20Chen%0AAbstract%3A%20%20%20Modern%20Recurrent%20Neural%20Networks%20have%20become%20a%20competitive%20architecture%20for%0A3D%20reconstruction%20due%20to%20their%20linear-time%20complexity.%20However%2C%20their%0Aperformance%20degrades%20significantly%20when%20applied%20beyond%20the%20training%20context%0Alength%2C%20revealing%20limited%20length%20generalization.%20In%20this%20work%2C%20we%20revisit%20the%0A3D%20reconstruction%20foundation%20models%20from%20a%20Test-Time%20Training%20perspective%2C%0Aframing%20their%20designs%20as%20an%20online%20learning%20problem.%20Building%20on%20this%0Aperspective%2C%20we%20leverage%20the%20alignment%20confidence%20between%20the%20memory%20state%20and%0Aincoming%20observations%20to%20derive%20a%20closed-form%20learning%20rate%20for%20memory%20updates%2C%0Ato%20balance%20between%20retaining%20historical%20information%20and%20adapting%20to%20new%0Aobservations.%20This%20training-free%20intervention%2C%20termed%20TTT3R%2C%20substantially%0Aimproves%20length%20generalization%2C%20achieving%20a%20%242%5Ctimes%24%20improvement%20in%20global%0Apose%20estimation%20over%20baselines%2C%20while%20operating%20at%2020%20FPS%20with%20just%206%20GB%20of%20GPU%0Amemory%20to%20process%20thousands%20of%20images.%20Code%20available%20in%0Ahttps%3A//rover-xingyu.github.io/TTT3R%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26645v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTT3R%253A%25203D%2520Reconstruction%2520as%2520Test-Time%2520Training%26entry.906535625%3DXingyu%2520Chen%2520and%2520Yue%2520Chen%2520and%2520Yuliang%2520Xiu%2520and%2520Andreas%2520Geiger%2520and%2520Anpei%2520Chen%26entry.1292438233%3D%2520%2520Modern%2520Recurrent%2520Neural%2520Networks%2520have%2520become%2520a%2520competitive%2520architecture%2520for%250A3D%2520reconstruction%2520due%2520to%2520their%2520linear-time%2520complexity.%2520However%252C%2520their%250Aperformance%2520degrades%2520significantly%2520when%2520applied%2520beyond%2520the%2520training%2520context%250Alength%252C%2520revealing%2520limited%2520length%2520generalization.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%250A3D%2520reconstruction%2520foundation%2520models%2520from%2520a%2520Test-Time%2520Training%2520perspective%252C%250Aframing%2520their%2520designs%2520as%2520an%2520online%2520learning%2520problem.%2520Building%2520on%2520this%250Aperspective%252C%2520we%2520leverage%2520the%2520alignment%2520confidence%2520between%2520the%2520memory%2520state%2520and%250Aincoming%2520observations%2520to%2520derive%2520a%2520closed-form%2520learning%2520rate%2520for%2520memory%2520updates%252C%250Ato%2520balance%2520between%2520retaining%2520historical%2520information%2520and%2520adapting%2520to%2520new%250Aobservations.%2520This%2520training-free%2520intervention%252C%2520termed%2520TTT3R%252C%2520substantially%250Aimproves%2520length%2520generalization%252C%2520achieving%2520a%2520%25242%255Ctimes%2524%2520improvement%2520in%2520global%250Apose%2520estimation%2520over%2520baselines%252C%2520while%2520operating%2520at%252020%2520FPS%2520with%2520just%25206%2520GB%2520of%2520GPU%250Amemory%2520to%2520process%2520thousands%2520of%2520images.%2520Code%2520available%2520in%250Ahttps%253A//rover-xingyu.github.io/TTT3R%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26645v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training&entry.906535625=Xingyu%20Chen%20and%20Yue%20Chen%20and%20Yuliang%20Xiu%20and%20Andreas%20Geiger%20and%20Anpei%20Chen&entry.1292438233=%20%20Modern%20Recurrent%20Neural%20Networks%20have%20become%20a%20competitive%20architecture%20for%0A3D%20reconstruction%20due%20to%20their%20linear-time%20complexity.%20However%2C%20their%0Aperformance%20degrades%20significantly%20when%20applied%20beyond%20the%20training%20context%0Alength%2C%20revealing%20limited%20length%20generalization.%20In%20this%20work%2C%20we%20revisit%20the%0A3D%20reconstruction%20foundation%20models%20from%20a%20Test-Time%20Training%20perspective%2C%0Aframing%20their%20designs%20as%20an%20online%20learning%20problem.%20Building%20on%20this%0Aperspective%2C%20we%20leverage%20the%20alignment%20confidence%20between%20the%20memory%20state%20and%0Aincoming%20observations%20to%20derive%20a%20closed-form%20learning%20rate%20for%20memory%20updates%2C%0Ato%20balance%20between%20retaining%20historical%20information%20and%20adapting%20to%20new%0Aobservations.%20This%20training-free%20intervention%2C%20termed%20TTT3R%2C%20substantially%0Aimproves%20length%20generalization%2C%20achieving%20a%20%242%5Ctimes%24%20improvement%20in%20global%0Apose%20estimation%20over%20baselines%2C%20while%20operating%20at%2020%20FPS%20with%20just%206%20GB%20of%20GPU%0Amemory%20to%20process%20thousands%20of%20images.%20Code%20available%20in%0Ahttps%3A//rover-xingyu.github.io/TTT3R%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26645v3&entry.124074799=Read"},
{"title": "Talking Points: Describing and Localizing Pixels", "author": "Matan Rusanovsky and Shimon Malnick and Shai Avidan", "abstract": "  Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points.\n", "link": "http://arxiv.org/abs/2510.14583v1", "date": "2025-10-16", "relevancy": 3.0364, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talking%20Points%3A%20Describing%20and%20Localizing%20Pixels&body=Title%3A%20Talking%20Points%3A%20Describing%20and%20Localizing%20Pixels%0AAuthor%3A%20Matan%20Rusanovsky%20and%20Shimon%20Malnick%20and%20Shai%20Avidan%0AAbstract%3A%20%20%20Vision-language%20models%20have%20achieved%20remarkable%20success%20in%20cross-modal%0Aunderstanding.%20Yet%2C%20these%20models%20remain%20limited%20to%20object-level%20or%20region-level%0Agrounding%2C%20lacking%20the%20capability%20for%20pixel-precise%20keypoint%20comprehension%0Athrough%20natural%20language.%20We%20introduce%20a%20novel%20framework%20for%20pixel%20level%0Agrounding.%20The%20framework%20consists%20of%20two%20complementary%20components%3A%20a%20Point%0ADescriptor%20that%20generates%20rich%2C%20contextual%20descriptions%20of%20individual%0Akeypoints%2C%20and%20a%20Point%20Localizer%20that%20regresses%20precise%20pixel%20coordinates%20from%0Athese%20descriptions.%20Unlike%20prior%20work%20that%20relies%20on%20templated%20prompts%20or%0Akeypoint%20names%2C%20our%20approach%20produces%20free-form%2C%20coarse-to-fine%20descriptions%0Athat%20situate%20keypoints%20within%20their%20visual%20context.%20Since%20there%20is%20no%20available%0Adataset%20to%20train%20such%20a%20system%2C%20we%20introduce%20LlamaPointInPart%2C%20a%20carefully%0Acurated%20dataset%20of%2020K%2B%20image-keypoint-description%20triplets%20synthesized%20from%0Amultiple%20vision-language%20models%2C%20capturing%20multi-scale%20information%20from%0Ascene-level%20context%20to%20visual%20features%20around%20the%20keypoint.%20For%20cross-category%0Ageneralization%2C%20we%20optimize%20the%20Point%20Descriptor%20on%20AP-10K%20via%20GRPO%2C%20using%20the%0Afrozen%20Point%20Localizer%20as%20a%20reward%20model%20to%20produce%20descriptions%20that%20maximize%0Alocalization%20accuracy.%20To%20evaluate%20our%20results%20we%20establish%20a%20new%20evaluation%0Aprotocol.%20Instead%20of%20comparing%20the%20text%20description%20produced%20by%20our%20method%20to%0Athe%20ground%20truth%2C%20we%20use%20the%20localizer%20to%20determine%20how%20close%20is%20the%20predicted%0Apoint%20generated%20to%20the%20ground%20truth%20point.%20Experiments%20demonstrate%20superior%0Aperformance%20compared%20to%20baseline%20models%20on%20LlamaPointInPart.The%20bidirectional%0Anature%20of%20our%20framework%20should%20enable%20future%20applications%20in%20both%0Akeypoint-guided%20image%20understanding%20and%20language-guided%20precise%20localization.%0AOur%20code%20and%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/matanr/Talking_Points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalking%2520Points%253A%2520Describing%2520and%2520Localizing%2520Pixels%26entry.906535625%3DMatan%2520Rusanovsky%2520and%2520Shimon%2520Malnick%2520and%2520Shai%2520Avidan%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520cross-modal%250Aunderstanding.%2520Yet%252C%2520these%2520models%2520remain%2520limited%2520to%2520object-level%2520or%2520region-level%250Agrounding%252C%2520lacking%2520the%2520capability%2520for%2520pixel-precise%2520keypoint%2520comprehension%250Athrough%2520natural%2520language.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520pixel%2520level%250Agrounding.%2520The%2520framework%2520consists%2520of%2520two%2520complementary%2520components%253A%2520a%2520Point%250ADescriptor%2520that%2520generates%2520rich%252C%2520contextual%2520descriptions%2520of%2520individual%250Akeypoints%252C%2520and%2520a%2520Point%2520Localizer%2520that%2520regresses%2520precise%2520pixel%2520coordinates%2520from%250Athese%2520descriptions.%2520Unlike%2520prior%2520work%2520that%2520relies%2520on%2520templated%2520prompts%2520or%250Akeypoint%2520names%252C%2520our%2520approach%2520produces%2520free-form%252C%2520coarse-to-fine%2520descriptions%250Athat%2520situate%2520keypoints%2520within%2520their%2520visual%2520context.%2520Since%2520there%2520is%2520no%2520available%250Adataset%2520to%2520train%2520such%2520a%2520system%252C%2520we%2520introduce%2520LlamaPointInPart%252C%2520a%2520carefully%250Acurated%2520dataset%2520of%252020K%252B%2520image-keypoint-description%2520triplets%2520synthesized%2520from%250Amultiple%2520vision-language%2520models%252C%2520capturing%2520multi-scale%2520information%2520from%250Ascene-level%2520context%2520to%2520visual%2520features%2520around%2520the%2520keypoint.%2520For%2520cross-category%250Ageneralization%252C%2520we%2520optimize%2520the%2520Point%2520Descriptor%2520on%2520AP-10K%2520via%2520GRPO%252C%2520using%2520the%250Afrozen%2520Point%2520Localizer%2520as%2520a%2520reward%2520model%2520to%2520produce%2520descriptions%2520that%2520maximize%250Alocalization%2520accuracy.%2520To%2520evaluate%2520our%2520results%2520we%2520establish%2520a%2520new%2520evaluation%250Aprotocol.%2520Instead%2520of%2520comparing%2520the%2520text%2520description%2520produced%2520by%2520our%2520method%2520to%250Athe%2520ground%2520truth%252C%2520we%2520use%2520the%2520localizer%2520to%2520determine%2520how%2520close%2520is%2520the%2520predicted%250Apoint%2520generated%2520to%2520the%2520ground%2520truth%2520point.%2520Experiments%2520demonstrate%2520superior%250Aperformance%2520compared%2520to%2520baseline%2520models%2520on%2520LlamaPointInPart.The%2520bidirectional%250Anature%2520of%2520our%2520framework%2520should%2520enable%2520future%2520applications%2520in%2520both%250Akeypoint-guided%2520image%2520understanding%2520and%2520language-guided%2520precise%2520localization.%250AOur%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/matanr/Talking_Points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talking%20Points%3A%20Describing%20and%20Localizing%20Pixels&entry.906535625=Matan%20Rusanovsky%20and%20Shimon%20Malnick%20and%20Shai%20Avidan&entry.1292438233=%20%20Vision-language%20models%20have%20achieved%20remarkable%20success%20in%20cross-modal%0Aunderstanding.%20Yet%2C%20these%20models%20remain%20limited%20to%20object-level%20or%20region-level%0Agrounding%2C%20lacking%20the%20capability%20for%20pixel-precise%20keypoint%20comprehension%0Athrough%20natural%20language.%20We%20introduce%20a%20novel%20framework%20for%20pixel%20level%0Agrounding.%20The%20framework%20consists%20of%20two%20complementary%20components%3A%20a%20Point%0ADescriptor%20that%20generates%20rich%2C%20contextual%20descriptions%20of%20individual%0Akeypoints%2C%20and%20a%20Point%20Localizer%20that%20regresses%20precise%20pixel%20coordinates%20from%0Athese%20descriptions.%20Unlike%20prior%20work%20that%20relies%20on%20templated%20prompts%20or%0Akeypoint%20names%2C%20our%20approach%20produces%20free-form%2C%20coarse-to-fine%20descriptions%0Athat%20situate%20keypoints%20within%20their%20visual%20context.%20Since%20there%20is%20no%20available%0Adataset%20to%20train%20such%20a%20system%2C%20we%20introduce%20LlamaPointInPart%2C%20a%20carefully%0Acurated%20dataset%20of%2020K%2B%20image-keypoint-description%20triplets%20synthesized%20from%0Amultiple%20vision-language%20models%2C%20capturing%20multi-scale%20information%20from%0Ascene-level%20context%20to%20visual%20features%20around%20the%20keypoint.%20For%20cross-category%0Ageneralization%2C%20we%20optimize%20the%20Point%20Descriptor%20on%20AP-10K%20via%20GRPO%2C%20using%20the%0Afrozen%20Point%20Localizer%20as%20a%20reward%20model%20to%20produce%20descriptions%20that%20maximize%0Alocalization%20accuracy.%20To%20evaluate%20our%20results%20we%20establish%20a%20new%20evaluation%0Aprotocol.%20Instead%20of%20comparing%20the%20text%20description%20produced%20by%20our%20method%20to%0Athe%20ground%20truth%2C%20we%20use%20the%20localizer%20to%20determine%20how%20close%20is%20the%20predicted%0Apoint%20generated%20to%20the%20ground%20truth%20point.%20Experiments%20demonstrate%20superior%0Aperformance%20compared%20to%20baseline%20models%20on%20LlamaPointInPart.The%20bidirectional%0Anature%20of%20our%20framework%20should%20enable%20future%20applications%20in%20both%0Akeypoint-guided%20image%20understanding%20and%20language-guided%20precise%20localization.%0AOur%20code%20and%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/matanr/Talking_Points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14583v1&entry.124074799=Read"},
{"title": "Exploring Image Representation with Decoupled Classical Visual\n  Descriptors", "author": "Chenyuan Qu and Hao Chen and Jianbo Jiao", "abstract": "  Exploring and understanding efficient image representations is a\nlong-standing challenge in computer vision. While deep learning has achieved\nremarkable progress across image understanding tasks, its internal\nrepresentations are often opaque, making it difficult to interpret how visual\ninformation is processed. In contrast, classical visual descriptors (e.g. edge,\ncolour, and intensity distribution) have long been fundamental to image\nanalysis and remain intuitively understandable to humans. Motivated by this\ngap, we ask a central question: Can modern learning benefit from these\nclassical cues? In this paper, we answer it with VisualSplit, a framework that\nexplicitly decomposes images into decoupled classical descriptors, treating\neach as an independent but complementary component of visual knowledge. Through\na reconstruction-driven pre-training scheme, VisualSplit learns to capture the\nessence of each visual descriptor while preserving their interpretability. By\nexplicitly decomposing visual attributes, our method inherently facilitates\neffective attribute control in various advanced visual tasks, including image\ngeneration and editing, extending beyond conventional classification and\nsegmentation, suggesting the effectiveness of this new learning approach for\nvisual understanding. Project page: https://chenyuanqu.com/VisualSplit/.\n", "link": "http://arxiv.org/abs/2510.14536v1", "date": "2025-10-16", "relevancy": 3.0254, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Image%20Representation%20with%20Decoupled%20Classical%20Visual%0A%20%20Descriptors&body=Title%3A%20Exploring%20Image%20Representation%20with%20Decoupled%20Classical%20Visual%0A%20%20Descriptors%0AAuthor%3A%20Chenyuan%20Qu%20and%20Hao%20Chen%20and%20Jianbo%20Jiao%0AAbstract%3A%20%20%20Exploring%20and%20understanding%20efficient%20image%20representations%20is%20a%0Along-standing%20challenge%20in%20computer%20vision.%20While%20deep%20learning%20has%20achieved%0Aremarkable%20progress%20across%20image%20understanding%20tasks%2C%20its%20internal%0Arepresentations%20are%20often%20opaque%2C%20making%20it%20difficult%20to%20interpret%20how%20visual%0Ainformation%20is%20processed.%20In%20contrast%2C%20classical%20visual%20descriptors%20%28e.g.%20edge%2C%0Acolour%2C%20and%20intensity%20distribution%29%20have%20long%20been%20fundamental%20to%20image%0Aanalysis%20and%20remain%20intuitively%20understandable%20to%20humans.%20Motivated%20by%20this%0Agap%2C%20we%20ask%20a%20central%20question%3A%20Can%20modern%20learning%20benefit%20from%20these%0Aclassical%20cues%3F%20In%20this%20paper%2C%20we%20answer%20it%20with%20VisualSplit%2C%20a%20framework%20that%0Aexplicitly%20decomposes%20images%20into%20decoupled%20classical%20descriptors%2C%20treating%0Aeach%20as%20an%20independent%20but%20complementary%20component%20of%20visual%20knowledge.%20Through%0Aa%20reconstruction-driven%20pre-training%20scheme%2C%20VisualSplit%20learns%20to%20capture%20the%0Aessence%20of%20each%20visual%20descriptor%20while%20preserving%20their%20interpretability.%20By%0Aexplicitly%20decomposing%20visual%20attributes%2C%20our%20method%20inherently%20facilitates%0Aeffective%20attribute%20control%20in%20various%20advanced%20visual%20tasks%2C%20including%20image%0Ageneration%20and%20editing%2C%20extending%20beyond%20conventional%20classification%20and%0Asegmentation%2C%20suggesting%20the%20effectiveness%20of%20this%20new%20learning%20approach%20for%0Avisual%20understanding.%20Project%20page%3A%20https%3A//chenyuanqu.com/VisualSplit/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Image%2520Representation%2520with%2520Decoupled%2520Classical%2520Visual%250A%2520%2520Descriptors%26entry.906535625%3DChenyuan%2520Qu%2520and%2520Hao%2520Chen%2520and%2520Jianbo%2520Jiao%26entry.1292438233%3D%2520%2520Exploring%2520and%2520understanding%2520efficient%2520image%2520representations%2520is%2520a%250Along-standing%2520challenge%2520in%2520computer%2520vision.%2520While%2520deep%2520learning%2520has%2520achieved%250Aremarkable%2520progress%2520across%2520image%2520understanding%2520tasks%252C%2520its%2520internal%250Arepresentations%2520are%2520often%2520opaque%252C%2520making%2520it%2520difficult%2520to%2520interpret%2520how%2520visual%250Ainformation%2520is%2520processed.%2520In%2520contrast%252C%2520classical%2520visual%2520descriptors%2520%2528e.g.%2520edge%252C%250Acolour%252C%2520and%2520intensity%2520distribution%2529%2520have%2520long%2520been%2520fundamental%2520to%2520image%250Aanalysis%2520and%2520remain%2520intuitively%2520understandable%2520to%2520humans.%2520Motivated%2520by%2520this%250Agap%252C%2520we%2520ask%2520a%2520central%2520question%253A%2520Can%2520modern%2520learning%2520benefit%2520from%2520these%250Aclassical%2520cues%253F%2520In%2520this%2520paper%252C%2520we%2520answer%2520it%2520with%2520VisualSplit%252C%2520a%2520framework%2520that%250Aexplicitly%2520decomposes%2520images%2520into%2520decoupled%2520classical%2520descriptors%252C%2520treating%250Aeach%2520as%2520an%2520independent%2520but%2520complementary%2520component%2520of%2520visual%2520knowledge.%2520Through%250Aa%2520reconstruction-driven%2520pre-training%2520scheme%252C%2520VisualSplit%2520learns%2520to%2520capture%2520the%250Aessence%2520of%2520each%2520visual%2520descriptor%2520while%2520preserving%2520their%2520interpretability.%2520By%250Aexplicitly%2520decomposing%2520visual%2520attributes%252C%2520our%2520method%2520inherently%2520facilitates%250Aeffective%2520attribute%2520control%2520in%2520various%2520advanced%2520visual%2520tasks%252C%2520including%2520image%250Ageneration%2520and%2520editing%252C%2520extending%2520beyond%2520conventional%2520classification%2520and%250Asegmentation%252C%2520suggesting%2520the%2520effectiveness%2520of%2520this%2520new%2520learning%2520approach%2520for%250Avisual%2520understanding.%2520Project%2520page%253A%2520https%253A//chenyuanqu.com/VisualSplit/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Image%20Representation%20with%20Decoupled%20Classical%20Visual%0A%20%20Descriptors&entry.906535625=Chenyuan%20Qu%20and%20Hao%20Chen%20and%20Jianbo%20Jiao&entry.1292438233=%20%20Exploring%20and%20understanding%20efficient%20image%20representations%20is%20a%0Along-standing%20challenge%20in%20computer%20vision.%20While%20deep%20learning%20has%20achieved%0Aremarkable%20progress%20across%20image%20understanding%20tasks%2C%20its%20internal%0Arepresentations%20are%20often%20opaque%2C%20making%20it%20difficult%20to%20interpret%20how%20visual%0Ainformation%20is%20processed.%20In%20contrast%2C%20classical%20visual%20descriptors%20%28e.g.%20edge%2C%0Acolour%2C%20and%20intensity%20distribution%29%20have%20long%20been%20fundamental%20to%20image%0Aanalysis%20and%20remain%20intuitively%20understandable%20to%20humans.%20Motivated%20by%20this%0Agap%2C%20we%20ask%20a%20central%20question%3A%20Can%20modern%20learning%20benefit%20from%20these%0Aclassical%20cues%3F%20In%20this%20paper%2C%20we%20answer%20it%20with%20VisualSplit%2C%20a%20framework%20that%0Aexplicitly%20decomposes%20images%20into%20decoupled%20classical%20descriptors%2C%20treating%0Aeach%20as%20an%20independent%20but%20complementary%20component%20of%20visual%20knowledge.%20Through%0Aa%20reconstruction-driven%20pre-training%20scheme%2C%20VisualSplit%20learns%20to%20capture%20the%0Aessence%20of%20each%20visual%20descriptor%20while%20preserving%20their%20interpretability.%20By%0Aexplicitly%20decomposing%20visual%20attributes%2C%20our%20method%20inherently%20facilitates%0Aeffective%20attribute%20control%20in%20various%20advanced%20visual%20tasks%2C%20including%20image%0Ageneration%20and%20editing%2C%20extending%20beyond%20conventional%20classification%20and%0Asegmentation%2C%20suggesting%20the%20effectiveness%20of%20this%20new%20learning%20approach%20for%0Avisual%20understanding.%20Project%20page%3A%20https%3A//chenyuanqu.com/VisualSplit/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14536v1&entry.124074799=Read"},
{"title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object\n  Interactions", "author": "Guangyi Han and Wei Zhai and Yuhang Yang and Yang Cao and Zheng-Jun Zha", "abstract": "  Hand-object interaction (HOI) is fundamental for humans to express intent.\nExisting HOI generation research is predominantly confined to fixed grasping\npatterns, where control is tied to physical priors such as force closure or\ngeneric intent instructions, even when expressed through elaborate language.\nSuch an overly general conditioning imposes a strong inductive bias for stable\ngrasps, thus failing to capture the diversity of daily HOI. To address these\nlimitations, we introduce Free-Form HOI Generation, which aims to generate\ncontrollable, diverse, and physically plausible HOI conditioned on fine-grained\nintent, extending HOI from grasping to free-form interactions, like pushing,\npoking, and rotating. To support this task, we construct WildO2, an in-the-wild\ndiverse 3D HOI dataset, which includes diverse HOI derived from internet\nvideos. Specifically, it contains 4.4k unique interactions across 92 intents\nand 610 object categories, each with detailed semantic annotations. Building on\nthis dataset, we propose TOUCH, a three-stage framework centered on a\nmulti-level diffusion model that facilitates fine-grained semantic control to\ngenerate versatile hand poses beyond grasping priors. This process leverages\nexplicit contact modeling for conditioning and is subsequently refined with\ncontact consistency and physical constraints to ensure realism. Comprehensive\nexperiments demonstrate our method's ability to generate controllable, diverse,\nand physically plausible hand interactions representative of daily activities.\nThe project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.\n", "link": "http://arxiv.org/abs/2510.14874v1", "date": "2025-10-16", "relevancy": 3.0176, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6309}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6003}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TOUCH%3A%20Text-guided%20Controllable%20Generation%20of%20Free-Form%20Hand-Object%0A%20%20Interactions&body=Title%3A%20TOUCH%3A%20Text-guided%20Controllable%20Generation%20of%20Free-Form%20Hand-Object%0A%20%20Interactions%0AAuthor%3A%20Guangyi%20Han%20and%20Wei%20Zhai%20and%20Yuhang%20Yang%20and%20Yang%20Cao%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Hand-object%20interaction%20%28HOI%29%20is%20fundamental%20for%20humans%20to%20express%20intent.%0AExisting%20HOI%20generation%20research%20is%20predominantly%20confined%20to%20fixed%20grasping%0Apatterns%2C%20where%20control%20is%20tied%20to%20physical%20priors%20such%20as%20force%20closure%20or%0Ageneric%20intent%20instructions%2C%20even%20when%20expressed%20through%20elaborate%20language.%0ASuch%20an%20overly%20general%20conditioning%20imposes%20a%20strong%20inductive%20bias%20for%20stable%0Agrasps%2C%20thus%20failing%20to%20capture%20the%20diversity%20of%20daily%20HOI.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20Free-Form%20HOI%20Generation%2C%20which%20aims%20to%20generate%0Acontrollable%2C%20diverse%2C%20and%20physically%20plausible%20HOI%20conditioned%20on%20fine-grained%0Aintent%2C%20extending%20HOI%20from%20grasping%20to%20free-form%20interactions%2C%20like%20pushing%2C%0Apoking%2C%20and%20rotating.%20To%20support%20this%20task%2C%20we%20construct%20WildO2%2C%20an%20in-the-wild%0Adiverse%203D%20HOI%20dataset%2C%20which%20includes%20diverse%20HOI%20derived%20from%20internet%0Avideos.%20Specifically%2C%20it%20contains%204.4k%20unique%20interactions%20across%2092%20intents%0Aand%20610%20object%20categories%2C%20each%20with%20detailed%20semantic%20annotations.%20Building%20on%0Athis%20dataset%2C%20we%20propose%20TOUCH%2C%20a%20three-stage%20framework%20centered%20on%20a%0Amulti-level%20diffusion%20model%20that%20facilitates%20fine-grained%20semantic%20control%20to%0Agenerate%20versatile%20hand%20poses%20beyond%20grasping%20priors.%20This%20process%20leverages%0Aexplicit%20contact%20modeling%20for%20conditioning%20and%20is%20subsequently%20refined%20with%0Acontact%20consistency%20and%20physical%20constraints%20to%20ensure%20realism.%20Comprehensive%0Aexperiments%20demonstrate%20our%20method%27s%20ability%20to%20generate%20controllable%2C%20diverse%2C%0Aand%20physically%20plausible%20hand%20interactions%20representative%20of%20daily%20activities.%0AThe%20project%20page%20is%20%24%5Chref%7Bhttps%3A//guangyid.github.io/hoi123touch%7D%7Bhere%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTOUCH%253A%2520Text-guided%2520Controllable%2520Generation%2520of%2520Free-Form%2520Hand-Object%250A%2520%2520Interactions%26entry.906535625%3DGuangyi%2520Han%2520and%2520Wei%2520Zhai%2520and%2520Yuhang%2520Yang%2520and%2520Yang%2520Cao%2520and%2520Zheng-Jun%2520Zha%26entry.1292438233%3D%2520%2520Hand-object%2520interaction%2520%2528HOI%2529%2520is%2520fundamental%2520for%2520humans%2520to%2520express%2520intent.%250AExisting%2520HOI%2520generation%2520research%2520is%2520predominantly%2520confined%2520to%2520fixed%2520grasping%250Apatterns%252C%2520where%2520control%2520is%2520tied%2520to%2520physical%2520priors%2520such%2520as%2520force%2520closure%2520or%250Ageneric%2520intent%2520instructions%252C%2520even%2520when%2520expressed%2520through%2520elaborate%2520language.%250ASuch%2520an%2520overly%2520general%2520conditioning%2520imposes%2520a%2520strong%2520inductive%2520bias%2520for%2520stable%250Agrasps%252C%2520thus%2520failing%2520to%2520capture%2520the%2520diversity%2520of%2520daily%2520HOI.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520Free-Form%2520HOI%2520Generation%252C%2520which%2520aims%2520to%2520generate%250Acontrollable%252C%2520diverse%252C%2520and%2520physically%2520plausible%2520HOI%2520conditioned%2520on%2520fine-grained%250Aintent%252C%2520extending%2520HOI%2520from%2520grasping%2520to%2520free-form%2520interactions%252C%2520like%2520pushing%252C%250Apoking%252C%2520and%2520rotating.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520WildO2%252C%2520an%2520in-the-wild%250Adiverse%25203D%2520HOI%2520dataset%252C%2520which%2520includes%2520diverse%2520HOI%2520derived%2520from%2520internet%250Avideos.%2520Specifically%252C%2520it%2520contains%25204.4k%2520unique%2520interactions%2520across%252092%2520intents%250Aand%2520610%2520object%2520categories%252C%2520each%2520with%2520detailed%2520semantic%2520annotations.%2520Building%2520on%250Athis%2520dataset%252C%2520we%2520propose%2520TOUCH%252C%2520a%2520three-stage%2520framework%2520centered%2520on%2520a%250Amulti-level%2520diffusion%2520model%2520that%2520facilitates%2520fine-grained%2520semantic%2520control%2520to%250Agenerate%2520versatile%2520hand%2520poses%2520beyond%2520grasping%2520priors.%2520This%2520process%2520leverages%250Aexplicit%2520contact%2520modeling%2520for%2520conditioning%2520and%2520is%2520subsequently%2520refined%2520with%250Acontact%2520consistency%2520and%2520physical%2520constraints%2520to%2520ensure%2520realism.%2520Comprehensive%250Aexperiments%2520demonstrate%2520our%2520method%2527s%2520ability%2520to%2520generate%2520controllable%252C%2520diverse%252C%250Aand%2520physically%2520plausible%2520hand%2520interactions%2520representative%2520of%2520daily%2520activities.%250AThe%2520project%2520page%2520is%2520%2524%255Chref%257Bhttps%253A//guangyid.github.io/hoi123touch%257D%257Bhere%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOUCH%3A%20Text-guided%20Controllable%20Generation%20of%20Free-Form%20Hand-Object%0A%20%20Interactions&entry.906535625=Guangyi%20Han%20and%20Wei%20Zhai%20and%20Yuhang%20Yang%20and%20Yang%20Cao%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Hand-object%20interaction%20%28HOI%29%20is%20fundamental%20for%20humans%20to%20express%20intent.%0AExisting%20HOI%20generation%20research%20is%20predominantly%20confined%20to%20fixed%20grasping%0Apatterns%2C%20where%20control%20is%20tied%20to%20physical%20priors%20such%20as%20force%20closure%20or%0Ageneric%20intent%20instructions%2C%20even%20when%20expressed%20through%20elaborate%20language.%0ASuch%20an%20overly%20general%20conditioning%20imposes%20a%20strong%20inductive%20bias%20for%20stable%0Agrasps%2C%20thus%20failing%20to%20capture%20the%20diversity%20of%20daily%20HOI.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20Free-Form%20HOI%20Generation%2C%20which%20aims%20to%20generate%0Acontrollable%2C%20diverse%2C%20and%20physically%20plausible%20HOI%20conditioned%20on%20fine-grained%0Aintent%2C%20extending%20HOI%20from%20grasping%20to%20free-form%20interactions%2C%20like%20pushing%2C%0Apoking%2C%20and%20rotating.%20To%20support%20this%20task%2C%20we%20construct%20WildO2%2C%20an%20in-the-wild%0Adiverse%203D%20HOI%20dataset%2C%20which%20includes%20diverse%20HOI%20derived%20from%20internet%0Avideos.%20Specifically%2C%20it%20contains%204.4k%20unique%20interactions%20across%2092%20intents%0Aand%20610%20object%20categories%2C%20each%20with%20detailed%20semantic%20annotations.%20Building%20on%0Athis%20dataset%2C%20we%20propose%20TOUCH%2C%20a%20three-stage%20framework%20centered%20on%20a%0Amulti-level%20diffusion%20model%20that%20facilitates%20fine-grained%20semantic%20control%20to%0Agenerate%20versatile%20hand%20poses%20beyond%20grasping%20priors.%20This%20process%20leverages%0Aexplicit%20contact%20modeling%20for%20conditioning%20and%20is%20subsequently%20refined%20with%0Acontact%20consistency%20and%20physical%20constraints%20to%20ensure%20realism.%20Comprehensive%0Aexperiments%20demonstrate%20our%20method%27s%20ability%20to%20generate%20controllable%2C%20diverse%2C%0Aand%20physically%20plausible%20hand%20interactions%20representative%20of%20daily%20activities.%0AThe%20project%20page%20is%20%24%5Chref%7Bhttps%3A//guangyid.github.io/hoi123touch%7D%7Bhere%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14874v1&entry.124074799=Read"},
{"title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment", "author": "Binod Singh and Sayan Deb Sarkar and Iro Armeni", "abstract": "  Aligning 3D scene graphs is a crucial initial step for several applications\nin robot navigation and embodied perception. Current methods in 3D scene graph\nalignment often rely on single-modality point cloud data and struggle with\nincomplete or noisy input. We introduce SGAligner++, a cross-modal,\nlanguage-aided framework for 3D scene graph alignment. Our method addresses the\nchallenge of aligning partially overlapping scene observations across\nheterogeneous modalities by learning a unified joint embedding space, enabling\naccurate alignment even under low-overlap conditions and sensor noise. By\nemploying lightweight unimodal encoders and attention-based fusion, SGAligner++\nenhances scene understanding for tasks such as visual localization, 3D\nreconstruction, and navigation, while ensuring scalability and minimal\ncomputational overhead. Extensive evaluations on real-world datasets\ndemonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%\non noisy real-world reconstructions, while enabling cross-modal generalization.\n", "link": "http://arxiv.org/abs/2509.20401v2", "date": "2025-10-16", "relevancy": 3.0145, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGAligner%2B%2B%3A%20Cross-Modal%20Language-Aided%203D%20Scene%20Graph%20Alignment&body=Title%3A%20SGAligner%2B%2B%3A%20Cross-Modal%20Language-Aided%203D%20Scene%20Graph%20Alignment%0AAuthor%3A%20Binod%20Singh%20and%20Sayan%20Deb%20Sarkar%20and%20Iro%20Armeni%0AAbstract%3A%20%20%20Aligning%203D%20scene%20graphs%20is%20a%20crucial%20initial%20step%20for%20several%20applications%0Ain%20robot%20navigation%20and%20embodied%20perception.%20Current%20methods%20in%203D%20scene%20graph%0Aalignment%20often%20rely%20on%20single-modality%20point%20cloud%20data%20and%20struggle%20with%0Aincomplete%20or%20noisy%20input.%20We%20introduce%20SGAligner%2B%2B%2C%20a%20cross-modal%2C%0Alanguage-aided%20framework%20for%203D%20scene%20graph%20alignment.%20Our%20method%20addresses%20the%0Achallenge%20of%20aligning%20partially%20overlapping%20scene%20observations%20across%0Aheterogeneous%20modalities%20by%20learning%20a%20unified%20joint%20embedding%20space%2C%20enabling%0Aaccurate%20alignment%20even%20under%20low-overlap%20conditions%20and%20sensor%20noise.%20By%0Aemploying%20lightweight%20unimodal%20encoders%20and%20attention-based%20fusion%2C%20SGAligner%2B%2B%0Aenhances%20scene%20understanding%20for%20tasks%20such%20as%20visual%20localization%2C%203D%0Areconstruction%2C%20and%20navigation%2C%20while%20ensuring%20scalability%20and%20minimal%0Acomputational%20overhead.%20Extensive%20evaluations%20on%20real-world%20datasets%0Ademonstrate%20that%20SGAligner%2B%2B%20outperforms%20state-of-the-art%20methods%20by%20up%20to%2040%25%0Aon%20noisy%20real-world%20reconstructions%2C%20while%20enabling%20cross-modal%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20401v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGAligner%252B%252B%253A%2520Cross-Modal%2520Language-Aided%25203D%2520Scene%2520Graph%2520Alignment%26entry.906535625%3DBinod%2520Singh%2520and%2520Sayan%2520Deb%2520Sarkar%2520and%2520Iro%2520Armeni%26entry.1292438233%3D%2520%2520Aligning%25203D%2520scene%2520graphs%2520is%2520a%2520crucial%2520initial%2520step%2520for%2520several%2520applications%250Ain%2520robot%2520navigation%2520and%2520embodied%2520perception.%2520Current%2520methods%2520in%25203D%2520scene%2520graph%250Aalignment%2520often%2520rely%2520on%2520single-modality%2520point%2520cloud%2520data%2520and%2520struggle%2520with%250Aincomplete%2520or%2520noisy%2520input.%2520We%2520introduce%2520SGAligner%252B%252B%252C%2520a%2520cross-modal%252C%250Alanguage-aided%2520framework%2520for%25203D%2520scene%2520graph%2520alignment.%2520Our%2520method%2520addresses%2520the%250Achallenge%2520of%2520aligning%2520partially%2520overlapping%2520scene%2520observations%2520across%250Aheterogeneous%2520modalities%2520by%2520learning%2520a%2520unified%2520joint%2520embedding%2520space%252C%2520enabling%250Aaccurate%2520alignment%2520even%2520under%2520low-overlap%2520conditions%2520and%2520sensor%2520noise.%2520By%250Aemploying%2520lightweight%2520unimodal%2520encoders%2520and%2520attention-based%2520fusion%252C%2520SGAligner%252B%252B%250Aenhances%2520scene%2520understanding%2520for%2520tasks%2520such%2520as%2520visual%2520localization%252C%25203D%250Areconstruction%252C%2520and%2520navigation%252C%2520while%2520ensuring%2520scalability%2520and%2520minimal%250Acomputational%2520overhead.%2520Extensive%2520evaluations%2520on%2520real-world%2520datasets%250Ademonstrate%2520that%2520SGAligner%252B%252B%2520outperforms%2520state-of-the-art%2520methods%2520by%2520up%2520to%252040%2525%250Aon%2520noisy%2520real-world%2520reconstructions%252C%2520while%2520enabling%2520cross-modal%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20401v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGAligner%2B%2B%3A%20Cross-Modal%20Language-Aided%203D%20Scene%20Graph%20Alignment&entry.906535625=Binod%20Singh%20and%20Sayan%20Deb%20Sarkar%20and%20Iro%20Armeni&entry.1292438233=%20%20Aligning%203D%20scene%20graphs%20is%20a%20crucial%20initial%20step%20for%20several%20applications%0Ain%20robot%20navigation%20and%20embodied%20perception.%20Current%20methods%20in%203D%20scene%20graph%0Aalignment%20often%20rely%20on%20single-modality%20point%20cloud%20data%20and%20struggle%20with%0Aincomplete%20or%20noisy%20input.%20We%20introduce%20SGAligner%2B%2B%2C%20a%20cross-modal%2C%0Alanguage-aided%20framework%20for%203D%20scene%20graph%20alignment.%20Our%20method%20addresses%20the%0Achallenge%20of%20aligning%20partially%20overlapping%20scene%20observations%20across%0Aheterogeneous%20modalities%20by%20learning%20a%20unified%20joint%20embedding%20space%2C%20enabling%0Aaccurate%20alignment%20even%20under%20low-overlap%20conditions%20and%20sensor%20noise.%20By%0Aemploying%20lightweight%20unimodal%20encoders%20and%20attention-based%20fusion%2C%20SGAligner%2B%2B%0Aenhances%20scene%20understanding%20for%20tasks%20such%20as%20visual%20localization%2C%203D%0Areconstruction%2C%20and%20navigation%2C%20while%20ensuring%20scalability%20and%20minimal%0Acomputational%20overhead.%20Extensive%20evaluations%20on%20real-world%20datasets%0Ademonstrate%20that%20SGAligner%2B%2B%20outperforms%20state-of-the-art%20methods%20by%20up%20to%2040%25%0Aon%20noisy%20real-world%20reconstructions%2C%20while%20enabling%20cross-modal%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20401v2&entry.124074799=Read"},
{"title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints", "author": "Meiqi Wu and Jiashu Zhu and Xiaokun Feng and Chubin Chen and Chen Zhu and Bingze Song and Fangyuan Mao and Jiahong Wu and Xiangxiang Chu and Kaiqi Huang", "abstract": "  Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.\n", "link": "http://arxiv.org/abs/2510.14847v1", "date": "2025-10-16", "relevancy": 2.9973, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6067}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5959}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagerySearch%3A%20Adaptive%20Test-Time%20Search%20for%20Video%20Generation%20Beyond%0A%20%20Semantic%20Dependency%20Constraints&body=Title%3A%20ImagerySearch%3A%20Adaptive%20Test-Time%20Search%20for%20Video%20Generation%20Beyond%0A%20%20Semantic%20Dependency%20Constraints%0AAuthor%3A%20Meiqi%20Wu%20and%20Jiashu%20Zhu%20and%20Xiaokun%20Feng%20and%20Chubin%20Chen%20and%20Chen%20Zhu%20and%20Bingze%20Song%20and%20Fangyuan%20Mao%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Video%20generation%20models%20have%20achieved%20remarkable%20progress%2C%20particularly%0Aexcelling%20in%20realistic%20scenarios%3B%20however%2C%20their%20performance%20degrades%20notably%0Ain%20imaginative%20scenarios.%20These%20prompts%20often%20involve%20rarely%20co-occurring%0Aconcepts%20with%20long-distance%20semantic%20relationships%2C%20falling%20outside%20training%0Adistributions.%20Existing%20methods%20typically%20apply%20test-time%20scaling%20for%20improving%0Avideo%20quality%2C%20but%20their%20fixed%20search%20spaces%20and%20static%20reward%20designs%20limit%0Aadaptability%20to%20imaginative%20scenarios.%20To%20fill%20this%20gap%2C%20we%20propose%0AImagerySearch%2C%20a%20prompt-guided%20adaptive%20test-time%20search%20strategy%20that%0Adynamically%20adjusts%20both%20the%20inference%20search%20space%20and%20reward%20function%0Aaccording%20to%20semantic%20relationships%20in%20the%20prompt.%20This%20enables%20more%20coherent%0Aand%20visually%20plausible%20videos%20in%20challenging%20imaginative%20settings.%20To%20evaluate%0Aprogress%20in%20this%20direction%2C%20we%20introduce%20LDT-Bench%2C%20the%20first%20dedicated%0Abenchmark%20for%20long-distance%20semantic%20prompts%2C%20consisting%20of%202%2C839%20diverse%0Aconcept%20pairs%20and%20an%20automated%20protocol%20for%20assessing%20creative%20generation%0Acapabilities.%20Extensive%20experiments%20show%20that%20ImagerySearch%20consistently%0Aoutperforms%20strong%20video%20generation%20baselines%20and%20existing%20test-time%20scaling%0Aapproaches%20on%20LDT-Bench%2C%20and%20achieves%20competitive%20improvements%20on%20VBench%2C%0Ademonstrating%20its%20effectiveness%20across%20diverse%20prompt%20types.%20We%20will%20release%0ALDT-Bench%20and%20code%20to%20facilitate%20future%20research%20on%20imaginative%20video%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagerySearch%253A%2520Adaptive%2520Test-Time%2520Search%2520for%2520Video%2520Generation%2520Beyond%250A%2520%2520Semantic%2520Dependency%2520Constraints%26entry.906535625%3DMeiqi%2520Wu%2520and%2520Jiashu%2520Zhu%2520and%2520Xiaokun%2520Feng%2520and%2520Chubin%2520Chen%2520and%2520Chen%2520Zhu%2520and%2520Bingze%2520Song%2520and%2520Fangyuan%2520Mao%2520and%2520Jiahong%2520Wu%2520and%2520Xiangxiang%2520Chu%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520have%2520achieved%2520remarkable%2520progress%252C%2520particularly%250Aexcelling%2520in%2520realistic%2520scenarios%253B%2520however%252C%2520their%2520performance%2520degrades%2520notably%250Ain%2520imaginative%2520scenarios.%2520These%2520prompts%2520often%2520involve%2520rarely%2520co-occurring%250Aconcepts%2520with%2520long-distance%2520semantic%2520relationships%252C%2520falling%2520outside%2520training%250Adistributions.%2520Existing%2520methods%2520typically%2520apply%2520test-time%2520scaling%2520for%2520improving%250Avideo%2520quality%252C%2520but%2520their%2520fixed%2520search%2520spaces%2520and%2520static%2520reward%2520designs%2520limit%250Aadaptability%2520to%2520imaginative%2520scenarios.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%250AImagerySearch%252C%2520a%2520prompt-guided%2520adaptive%2520test-time%2520search%2520strategy%2520that%250Adynamically%2520adjusts%2520both%2520the%2520inference%2520search%2520space%2520and%2520reward%2520function%250Aaccording%2520to%2520semantic%2520relationships%2520in%2520the%2520prompt.%2520This%2520enables%2520more%2520coherent%250Aand%2520visually%2520plausible%2520videos%2520in%2520challenging%2520imaginative%2520settings.%2520To%2520evaluate%250Aprogress%2520in%2520this%2520direction%252C%2520we%2520introduce%2520LDT-Bench%252C%2520the%2520first%2520dedicated%250Abenchmark%2520for%2520long-distance%2520semantic%2520prompts%252C%2520consisting%2520of%25202%252C839%2520diverse%250Aconcept%2520pairs%2520and%2520an%2520automated%2520protocol%2520for%2520assessing%2520creative%2520generation%250Acapabilities.%2520Extensive%2520experiments%2520show%2520that%2520ImagerySearch%2520consistently%250Aoutperforms%2520strong%2520video%2520generation%2520baselines%2520and%2520existing%2520test-time%2520scaling%250Aapproaches%2520on%2520LDT-Bench%252C%2520and%2520achieves%2520competitive%2520improvements%2520on%2520VBench%252C%250Ademonstrating%2520its%2520effectiveness%2520across%2520diverse%2520prompt%2520types.%2520We%2520will%2520release%250ALDT-Bench%2520and%2520code%2520to%2520facilitate%2520future%2520research%2520on%2520imaginative%2520video%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagerySearch%3A%20Adaptive%20Test-Time%20Search%20for%20Video%20Generation%20Beyond%0A%20%20Semantic%20Dependency%20Constraints&entry.906535625=Meiqi%20Wu%20and%20Jiashu%20Zhu%20and%20Xiaokun%20Feng%20and%20Chubin%20Chen%20and%20Chen%20Zhu%20and%20Bingze%20Song%20and%20Fangyuan%20Mao%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Video%20generation%20models%20have%20achieved%20remarkable%20progress%2C%20particularly%0Aexcelling%20in%20realistic%20scenarios%3B%20however%2C%20their%20performance%20degrades%20notably%0Ain%20imaginative%20scenarios.%20These%20prompts%20often%20involve%20rarely%20co-occurring%0Aconcepts%20with%20long-distance%20semantic%20relationships%2C%20falling%20outside%20training%0Adistributions.%20Existing%20methods%20typically%20apply%20test-time%20scaling%20for%20improving%0Avideo%20quality%2C%20but%20their%20fixed%20search%20spaces%20and%20static%20reward%20designs%20limit%0Aadaptability%20to%20imaginative%20scenarios.%20To%20fill%20this%20gap%2C%20we%20propose%0AImagerySearch%2C%20a%20prompt-guided%20adaptive%20test-time%20search%20strategy%20that%0Adynamically%20adjusts%20both%20the%20inference%20search%20space%20and%20reward%20function%0Aaccording%20to%20semantic%20relationships%20in%20the%20prompt.%20This%20enables%20more%20coherent%0Aand%20visually%20plausible%20videos%20in%20challenging%20imaginative%20settings.%20To%20evaluate%0Aprogress%20in%20this%20direction%2C%20we%20introduce%20LDT-Bench%2C%20the%20first%20dedicated%0Abenchmark%20for%20long-distance%20semantic%20prompts%2C%20consisting%20of%202%2C839%20diverse%0Aconcept%20pairs%20and%20an%20automated%20protocol%20for%20assessing%20creative%20generation%0Acapabilities.%20Extensive%20experiments%20show%20that%20ImagerySearch%20consistently%0Aoutperforms%20strong%20video%20generation%20baselines%20and%20existing%20test-time%20scaling%0Aapproaches%20on%20LDT-Bench%2C%20and%20achieves%20competitive%20improvements%20on%20VBench%2C%0Ademonstrating%20its%20effectiveness%20across%20diverse%20prompt%20types.%20We%20will%20release%0ALDT-Bench%20and%20code%20to%20facilitate%20future%20research%20on%20imaginative%20video%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14847v1&entry.124074799=Read"},
{"title": "3DOT: Texture Transfer for 3DGS Objects from a Single Reference Image", "author": "Xiao Cao and Beibei Lin and Bo Wang and Zhiyong Huang and Robby T. Tan", "abstract": "  3D texture swapping allows for the customization of 3D object textures,\nenabling efficient and versatile visual transformations in 3D editing. While no\ndedicated method exists, adapted 2D editing and text-driven 3D editing\napproaches can serve this purpose. However, 2D editing requires frame-by-frame\nmanipulation, causing inconsistencies across views, while text-driven 3D\nediting struggles to preserve texture characteristics from reference images. To\ntackle these challenges, we introduce 3DSwapping, a 3D texture swapping method\nthat integrates: 1) progressive generation, 2) view-consistency gradient\nguidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,\nour progressive generation process starts by editing a single reference image\nand gradually propagates the edits to adjacent views. Our view-consistency\ngradient guidance further reinforces consistency by conditioning the generation\nmodel on feature differences between consistent and inconsistent outputs. To\npreserve texture characteristics, we introduce prompt-tuning-based gradient\nguidance, which learns a token that precisely captures the difference between\nthe reference image and the 3D object. This token then guides the editing\nprocess, ensuring more consistent texture preservation across views. Overall,\n3DSwapping integrates these novel strategies to achieve higher-fidelity texture\ntransfer while preserving structural coherence across multiple viewpoints.\nExtensive qualitative and quantitative evaluations confirm that our three novel\ncomponents enable convincing and effective 2D texture swapping for 3D objects.\nCode will be available upon acceptance.\n", "link": "http://arxiv.org/abs/2503.18853v2", "date": "2025-10-16", "relevancy": 2.9359, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6224}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5699}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DOT%3A%20Texture%20Transfer%20for%203DGS%20Objects%20from%20a%20Single%20Reference%20Image&body=Title%3A%203DOT%3A%20Texture%20Transfer%20for%203DGS%20Objects%20from%20a%20Single%20Reference%20Image%0AAuthor%3A%20Xiao%20Cao%20and%20Beibei%20Lin%20and%20Bo%20Wang%20and%20Zhiyong%20Huang%20and%20Robby%20T.%20Tan%0AAbstract%3A%20%20%203D%20texture%20swapping%20allows%20for%20the%20customization%20of%203D%20object%20textures%2C%0Aenabling%20efficient%20and%20versatile%20visual%20transformations%20in%203D%20editing.%20While%20no%0Adedicated%20method%20exists%2C%20adapted%202D%20editing%20and%20text-driven%203D%20editing%0Aapproaches%20can%20serve%20this%20purpose.%20However%2C%202D%20editing%20requires%20frame-by-frame%0Amanipulation%2C%20causing%20inconsistencies%20across%20views%2C%20while%20text-driven%203D%0Aediting%20struggles%20to%20preserve%20texture%20characteristics%20from%20reference%20images.%20To%0Atackle%20these%20challenges%2C%20we%20introduce%203DSwapping%2C%20a%203D%20texture%20swapping%20method%0Athat%20integrates%3A%201%29%20progressive%20generation%2C%202%29%20view-consistency%20gradient%0Aguidance%2C%20and%203%29%20prompt-tuned%20gradient%20guidance.%20To%20ensure%20view%20consistency%2C%0Aour%20progressive%20generation%20process%20starts%20by%20editing%20a%20single%20reference%20image%0Aand%20gradually%20propagates%20the%20edits%20to%20adjacent%20views.%20Our%20view-consistency%0Agradient%20guidance%20further%20reinforces%20consistency%20by%20conditioning%20the%20generation%0Amodel%20on%20feature%20differences%20between%20consistent%20and%20inconsistent%20outputs.%20To%0Apreserve%20texture%20characteristics%2C%20we%20introduce%20prompt-tuning-based%20gradient%0Aguidance%2C%20which%20learns%20a%20token%20that%20precisely%20captures%20the%20difference%20between%0Athe%20reference%20image%20and%20the%203D%20object.%20This%20token%20then%20guides%20the%20editing%0Aprocess%2C%20ensuring%20more%20consistent%20texture%20preservation%20across%20views.%20Overall%2C%0A3DSwapping%20integrates%20these%20novel%20strategies%20to%20achieve%20higher-fidelity%20texture%0Atransfer%20while%20preserving%20structural%20coherence%20across%20multiple%20viewpoints.%0AExtensive%20qualitative%20and%20quantitative%20evaluations%20confirm%20that%20our%20three%20novel%0Acomponents%20enable%20convincing%20and%20effective%202D%20texture%20swapping%20for%203D%20objects.%0ACode%20will%20be%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DOT%253A%2520Texture%2520Transfer%2520for%25203DGS%2520Objects%2520from%2520a%2520Single%2520Reference%2520Image%26entry.906535625%3DXiao%2520Cao%2520and%2520Beibei%2520Lin%2520and%2520Bo%2520Wang%2520and%2520Zhiyong%2520Huang%2520and%2520Robby%2520T.%2520Tan%26entry.1292438233%3D%2520%25203D%2520texture%2520swapping%2520allows%2520for%2520the%2520customization%2520of%25203D%2520object%2520textures%252C%250Aenabling%2520efficient%2520and%2520versatile%2520visual%2520transformations%2520in%25203D%2520editing.%2520While%2520no%250Adedicated%2520method%2520exists%252C%2520adapted%25202D%2520editing%2520and%2520text-driven%25203D%2520editing%250Aapproaches%2520can%2520serve%2520this%2520purpose.%2520However%252C%25202D%2520editing%2520requires%2520frame-by-frame%250Amanipulation%252C%2520causing%2520inconsistencies%2520across%2520views%252C%2520while%2520text-driven%25203D%250Aediting%2520struggles%2520to%2520preserve%2520texture%2520characteristics%2520from%2520reference%2520images.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520introduce%25203DSwapping%252C%2520a%25203D%2520texture%2520swapping%2520method%250Athat%2520integrates%253A%25201%2529%2520progressive%2520generation%252C%25202%2529%2520view-consistency%2520gradient%250Aguidance%252C%2520and%25203%2529%2520prompt-tuned%2520gradient%2520guidance.%2520To%2520ensure%2520view%2520consistency%252C%250Aour%2520progressive%2520generation%2520process%2520starts%2520by%2520editing%2520a%2520single%2520reference%2520image%250Aand%2520gradually%2520propagates%2520the%2520edits%2520to%2520adjacent%2520views.%2520Our%2520view-consistency%250Agradient%2520guidance%2520further%2520reinforces%2520consistency%2520by%2520conditioning%2520the%2520generation%250Amodel%2520on%2520feature%2520differences%2520between%2520consistent%2520and%2520inconsistent%2520outputs.%2520To%250Apreserve%2520texture%2520characteristics%252C%2520we%2520introduce%2520prompt-tuning-based%2520gradient%250Aguidance%252C%2520which%2520learns%2520a%2520token%2520that%2520precisely%2520captures%2520the%2520difference%2520between%250Athe%2520reference%2520image%2520and%2520the%25203D%2520object.%2520This%2520token%2520then%2520guides%2520the%2520editing%250Aprocess%252C%2520ensuring%2520more%2520consistent%2520texture%2520preservation%2520across%2520views.%2520Overall%252C%250A3DSwapping%2520integrates%2520these%2520novel%2520strategies%2520to%2520achieve%2520higher-fidelity%2520texture%250Atransfer%2520while%2520preserving%2520structural%2520coherence%2520across%2520multiple%2520viewpoints.%250AExtensive%2520qualitative%2520and%2520quantitative%2520evaluations%2520confirm%2520that%2520our%2520three%2520novel%250Acomponents%2520enable%2520convincing%2520and%2520effective%25202D%2520texture%2520swapping%2520for%25203D%2520objects.%250ACode%2520will%2520be%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DOT%3A%20Texture%20Transfer%20for%203DGS%20Objects%20from%20a%20Single%20Reference%20Image&entry.906535625=Xiao%20Cao%20and%20Beibei%20Lin%20and%20Bo%20Wang%20and%20Zhiyong%20Huang%20and%20Robby%20T.%20Tan&entry.1292438233=%20%203D%20texture%20swapping%20allows%20for%20the%20customization%20of%203D%20object%20textures%2C%0Aenabling%20efficient%20and%20versatile%20visual%20transformations%20in%203D%20editing.%20While%20no%0Adedicated%20method%20exists%2C%20adapted%202D%20editing%20and%20text-driven%203D%20editing%0Aapproaches%20can%20serve%20this%20purpose.%20However%2C%202D%20editing%20requires%20frame-by-frame%0Amanipulation%2C%20causing%20inconsistencies%20across%20views%2C%20while%20text-driven%203D%0Aediting%20struggles%20to%20preserve%20texture%20characteristics%20from%20reference%20images.%20To%0Atackle%20these%20challenges%2C%20we%20introduce%203DSwapping%2C%20a%203D%20texture%20swapping%20method%0Athat%20integrates%3A%201%29%20progressive%20generation%2C%202%29%20view-consistency%20gradient%0Aguidance%2C%20and%203%29%20prompt-tuned%20gradient%20guidance.%20To%20ensure%20view%20consistency%2C%0Aour%20progressive%20generation%20process%20starts%20by%20editing%20a%20single%20reference%20image%0Aand%20gradually%20propagates%20the%20edits%20to%20adjacent%20views.%20Our%20view-consistency%0Agradient%20guidance%20further%20reinforces%20consistency%20by%20conditioning%20the%20generation%0Amodel%20on%20feature%20differences%20between%20consistent%20and%20inconsistent%20outputs.%20To%0Apreserve%20texture%20characteristics%2C%20we%20introduce%20prompt-tuning-based%20gradient%0Aguidance%2C%20which%20learns%20a%20token%20that%20precisely%20captures%20the%20difference%20between%0Athe%20reference%20image%20and%20the%203D%20object.%20This%20token%20then%20guides%20the%20editing%0Aprocess%2C%20ensuring%20more%20consistent%20texture%20preservation%20across%20views.%20Overall%2C%0A3DSwapping%20integrates%20these%20novel%20strategies%20to%20achieve%20higher-fidelity%20texture%0Atransfer%20while%20preserving%20structural%20coherence%20across%20multiple%20viewpoints.%0AExtensive%20qualitative%20and%20quantitative%20evaluations%20confirm%20that%20our%20three%20novel%0Acomponents%20enable%20convincing%20and%20effective%202D%20texture%20swapping%20for%203D%20objects.%0ACode%20will%20be%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18853v2&entry.124074799=Read"},
{"title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for\n  Open-Vocabulary Object Detection", "author": "Hojun Choi and Youngsun Lim and Jaeyo Shin and Hyunjung Shim", "abstract": "  Open-vocabulary object detection (OVD) seeks to recognize and localize object\ncategories beyond those seen during training. Recent approaches typically\nleverage vision-language models (VLMs) to generate pseudo-labels using\nimage-text alignment, allowing detectors to generalize to unseen classes\nwithout explicit supervision. However, these methods depend heavily on direct\nimage-text matching, neglecting the intermediate reasoning steps essential for\ninterpreting semantically complex scenes. This results in limited robustness\nwhen confronted with crowded or occluded visual contexts. In this paper, we\nintroduce CoT-PL, a new framework that employs structured visual\nchain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL\ndecomposes object understanding into three interpretable steps: (1) region\nperception even for unseen objects, (2) category recognition via zero-shot\nreasoning, and (3) background grounding to separate semantically complex\nobjects. Crucially, the third step naturally motivates our contrastive\nbackground learning (CBL) that uses the pre-computed background cues as\nnegatives to promote feature disentanglement between objects and background. In\nthis way, CoT reasoning and CBL form an integrated pipeline tailored to robust\npseudo-labeling in crowded or occluded scenes. Notably, in these two settings,\nour novel-class pseudo-label quality achieves relative improvements of 103.4%\nand 168.4% over the best prior, respectively. Our extensive experiments\ndemonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9\nmask AP on LVIS for novel classes, setting a new state of the art.\n", "link": "http://arxiv.org/abs/2510.14792v1", "date": "2025-10-16", "relevancy": 2.9024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-PL%3A%20Visual%20Chain-of-Thought%20Reasoning%20Meets%20Pseudo-Labeling%20for%0A%20%20Open-Vocabulary%20Object%20Detection&body=Title%3A%20CoT-PL%3A%20Visual%20Chain-of-Thought%20Reasoning%20Meets%20Pseudo-Labeling%20for%0A%20%20Open-Vocabulary%20Object%20Detection%0AAuthor%3A%20Hojun%20Choi%20and%20Youngsun%20Lim%20and%20Jaeyo%20Shin%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Open-vocabulary%20object%20detection%20%28OVD%29%20seeks%20to%20recognize%20and%20localize%20object%0Acategories%20beyond%20those%20seen%20during%20training.%20Recent%20approaches%20typically%0Aleverage%20vision-language%20models%20%28VLMs%29%20to%20generate%20pseudo-labels%20using%0Aimage-text%20alignment%2C%20allowing%20detectors%20to%20generalize%20to%20unseen%20classes%0Awithout%20explicit%20supervision.%20However%2C%20these%20methods%20depend%20heavily%20on%20direct%0Aimage-text%20matching%2C%20neglecting%20the%20intermediate%20reasoning%20steps%20essential%20for%0Ainterpreting%20semantically%20complex%20scenes.%20This%20results%20in%20limited%20robustness%0Awhen%20confronted%20with%20crowded%20or%20occluded%20visual%20contexts.%20In%20this%20paper%2C%20we%0Aintroduce%20CoT-PL%2C%20a%20new%20framework%20that%20employs%20structured%20visual%0Achain-of-thought%20%28CoT%29%20reasoning%20into%20the%20pseudo-labeling%20process.%20CoT-PL%0Adecomposes%20object%20understanding%20into%20three%20interpretable%20steps%3A%20%281%29%20region%0Aperception%20even%20for%20unseen%20objects%2C%20%282%29%20category%20recognition%20via%20zero-shot%0Areasoning%2C%20and%20%283%29%20background%20grounding%20to%20separate%20semantically%20complex%0Aobjects.%20Crucially%2C%20the%20third%20step%20naturally%20motivates%20our%20contrastive%0Abackground%20learning%20%28CBL%29%20that%20uses%20the%20pre-computed%20background%20cues%20as%0Anegatives%20to%20promote%20feature%20disentanglement%20between%20objects%20and%20background.%20In%0Athis%20way%2C%20CoT%20reasoning%20and%20CBL%20form%20an%20integrated%20pipeline%20tailored%20to%20robust%0Apseudo-labeling%20in%20crowded%20or%20occluded%20scenes.%20Notably%2C%20in%20these%20two%20settings%2C%0Aour%20novel-class%20pseudo-label%20quality%20achieves%20relative%20improvements%20of%20103.4%25%0Aand%20168.4%25%20over%20the%20best%20prior%2C%20respectively.%20Our%20extensive%20experiments%0Ademonstrate%20that%20CoT-PL%20achieves%20%2B7.7%20AP50%20on%20open-vocabulary%20COCO%20and%20%2B2.9%0Amask%20AP%20on%20LVIS%20for%20novel%20classes%2C%20setting%20a%20new%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-PL%253A%2520Visual%2520Chain-of-Thought%2520Reasoning%2520Meets%2520Pseudo-Labeling%2520for%250A%2520%2520Open-Vocabulary%2520Object%2520Detection%26entry.906535625%3DHojun%2520Choi%2520and%2520Youngsun%2520Lim%2520and%2520Jaeyo%2520Shin%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Open-vocabulary%2520object%2520detection%2520%2528OVD%2529%2520seeks%2520to%2520recognize%2520and%2520localize%2520object%250Acategories%2520beyond%2520those%2520seen%2520during%2520training.%2520Recent%2520approaches%2520typically%250Aleverage%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520generate%2520pseudo-labels%2520using%250Aimage-text%2520alignment%252C%2520allowing%2520detectors%2520to%2520generalize%2520to%2520unseen%2520classes%250Awithout%2520explicit%2520supervision.%2520However%252C%2520these%2520methods%2520depend%2520heavily%2520on%2520direct%250Aimage-text%2520matching%252C%2520neglecting%2520the%2520intermediate%2520reasoning%2520steps%2520essential%2520for%250Ainterpreting%2520semantically%2520complex%2520scenes.%2520This%2520results%2520in%2520limited%2520robustness%250Awhen%2520confronted%2520with%2520crowded%2520or%2520occluded%2520visual%2520contexts.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520CoT-PL%252C%2520a%2520new%2520framework%2520that%2520employs%2520structured%2520visual%250Achain-of-thought%2520%2528CoT%2529%2520reasoning%2520into%2520the%2520pseudo-labeling%2520process.%2520CoT-PL%250Adecomposes%2520object%2520understanding%2520into%2520three%2520interpretable%2520steps%253A%2520%25281%2529%2520region%250Aperception%2520even%2520for%2520unseen%2520objects%252C%2520%25282%2529%2520category%2520recognition%2520via%2520zero-shot%250Areasoning%252C%2520and%2520%25283%2529%2520background%2520grounding%2520to%2520separate%2520semantically%2520complex%250Aobjects.%2520Crucially%252C%2520the%2520third%2520step%2520naturally%2520motivates%2520our%2520contrastive%250Abackground%2520learning%2520%2528CBL%2529%2520that%2520uses%2520the%2520pre-computed%2520background%2520cues%2520as%250Anegatives%2520to%2520promote%2520feature%2520disentanglement%2520between%2520objects%2520and%2520background.%2520In%250Athis%2520way%252C%2520CoT%2520reasoning%2520and%2520CBL%2520form%2520an%2520integrated%2520pipeline%2520tailored%2520to%2520robust%250Apseudo-labeling%2520in%2520crowded%2520or%2520occluded%2520scenes.%2520Notably%252C%2520in%2520these%2520two%2520settings%252C%250Aour%2520novel-class%2520pseudo-label%2520quality%2520achieves%2520relative%2520improvements%2520of%2520103.4%2525%250Aand%2520168.4%2525%2520over%2520the%2520best%2520prior%252C%2520respectively.%2520Our%2520extensive%2520experiments%250Ademonstrate%2520that%2520CoT-PL%2520achieves%2520%252B7.7%2520AP50%2520on%2520open-vocabulary%2520COCO%2520and%2520%252B2.9%250Amask%2520AP%2520on%2520LVIS%2520for%2520novel%2520classes%252C%2520setting%2520a%2520new%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-PL%3A%20Visual%20Chain-of-Thought%20Reasoning%20Meets%20Pseudo-Labeling%20for%0A%20%20Open-Vocabulary%20Object%20Detection&entry.906535625=Hojun%20Choi%20and%20Youngsun%20Lim%20and%20Jaeyo%20Shin%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Open-vocabulary%20object%20detection%20%28OVD%29%20seeks%20to%20recognize%20and%20localize%20object%0Acategories%20beyond%20those%20seen%20during%20training.%20Recent%20approaches%20typically%0Aleverage%20vision-language%20models%20%28VLMs%29%20to%20generate%20pseudo-labels%20using%0Aimage-text%20alignment%2C%20allowing%20detectors%20to%20generalize%20to%20unseen%20classes%0Awithout%20explicit%20supervision.%20However%2C%20these%20methods%20depend%20heavily%20on%20direct%0Aimage-text%20matching%2C%20neglecting%20the%20intermediate%20reasoning%20steps%20essential%20for%0Ainterpreting%20semantically%20complex%20scenes.%20This%20results%20in%20limited%20robustness%0Awhen%20confronted%20with%20crowded%20or%20occluded%20visual%20contexts.%20In%20this%20paper%2C%20we%0Aintroduce%20CoT-PL%2C%20a%20new%20framework%20that%20employs%20structured%20visual%0Achain-of-thought%20%28CoT%29%20reasoning%20into%20the%20pseudo-labeling%20process.%20CoT-PL%0Adecomposes%20object%20understanding%20into%20three%20interpretable%20steps%3A%20%281%29%20region%0Aperception%20even%20for%20unseen%20objects%2C%20%282%29%20category%20recognition%20via%20zero-shot%0Areasoning%2C%20and%20%283%29%20background%20grounding%20to%20separate%20semantically%20complex%0Aobjects.%20Crucially%2C%20the%20third%20step%20naturally%20motivates%20our%20contrastive%0Abackground%20learning%20%28CBL%29%20that%20uses%20the%20pre-computed%20background%20cues%20as%0Anegatives%20to%20promote%20feature%20disentanglement%20between%20objects%20and%20background.%20In%0Athis%20way%2C%20CoT%20reasoning%20and%20CBL%20form%20an%20integrated%20pipeline%20tailored%20to%20robust%0Apseudo-labeling%20in%20crowded%20or%20occluded%20scenes.%20Notably%2C%20in%20these%20two%20settings%2C%0Aour%20novel-class%20pseudo-label%20quality%20achieves%20relative%20improvements%20of%20103.4%25%0Aand%20168.4%25%20over%20the%20best%20prior%2C%20respectively.%20Our%20extensive%20experiments%0Ademonstrate%20that%20CoT-PL%20achieves%20%2B7.7%20AP50%20on%20open-vocabulary%20COCO%20and%20%2B2.9%0Amask%20AP%20on%20LVIS%20for%20novel%20classes%2C%20setting%20a%20new%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14792v1&entry.124074799=Read"},
{"title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale", "author": "Haiwen Diao and Mingxuan Li and Silei Wu and Linjun Dai and Xiaohua Wang and Hanming Deng and Lewei Lu and Dahua Lin and Ziwei Liu", "abstract": "  The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.\n", "link": "http://arxiv.org/abs/2510.14979v1", "date": "2025-10-16", "relevancy": 2.8983, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6132}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Words%20--%20Towards%20Native%20Vision-Language%20Primitives%20at%0A%20%20Scale&body=Title%3A%20From%20Pixels%20to%20Words%20--%20Towards%20Native%20Vision-Language%20Primitives%20at%0A%20%20Scale%0AAuthor%3A%20Haiwen%20Diao%20and%20Mingxuan%20Li%20and%20Silei%20Wu%20and%20Linjun%20Dai%20and%20Xiaohua%20Wang%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Dahua%20Lin%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20The%20edifice%20of%20native%20Vision-Language%20Models%20%28VLMs%29%20has%20emerged%20as%20a%20rising%0Acontender%20to%20typical%20modular%20VLMs%2C%20shaped%20by%20evolving%20model%20architectures%20and%0Atraining%20paradigms.%20Yet%2C%20two%20lingering%20clouds%20cast%20shadows%20over%20its%20widespread%0Aexploration%20and%20promotion%3A%20%28-%29%20What%20fundamental%20constraints%20set%20native%20VLMs%0Aapart%20from%20modular%20ones%2C%20and%20to%20what%20extent%20can%20these%20barriers%20be%20overcome%3F%20%28-%29%0AHow%20to%20make%20research%20in%20native%20VLMs%20more%20accessible%20and%20democratized%2C%20thereby%0Aaccelerating%20progress%20in%20the%20field.%20In%20this%20paper%2C%20we%20clarify%20these%20challenges%0Aand%20outline%20guiding%20principles%20for%20constructing%20native%20VLMs.%20Specifically%2C%20one%0Anative%20VLM%20primitive%20should%3A%20%28i%29%20effectively%20align%20pixel%20and%20word%0Arepresentations%20within%20a%20shared%20semantic%20space%3B%20%28ii%29%20seamlessly%20integrate%20the%0Astrengths%20of%20formerly%20separate%20vision%20and%20language%20modules%3B%20%28iii%29%20inherently%0Aembody%20various%20cross-modal%20properties%20that%20support%20unified%20vision-language%0Aencoding%2C%20aligning%2C%20and%20reasoning.%20Hence%2C%20we%20launch%20NEO%2C%20a%20novel%20family%20of%0Anative%20VLMs%20built%20from%20first%20principles%2C%20capable%20of%20rivaling%20top-tier%20modular%0Acounterparts%20across%20diverse%20real-world%20scenarios.%20With%20only%20390M%20image-text%0Aexamples%2C%20NEO%20efficiently%20develops%20visual%20perception%20from%20scratch%20while%0Amitigating%20vision-language%20conflicts%20inside%20a%20dense%20and%20monolithic%20model%0Acrafted%20from%20our%20elaborate%20primitives.%20We%20position%20NEO%20as%20a%20cornerstone%20for%0Ascalable%20and%20powerful%20native%20VLMs%2C%20paired%20with%20a%20rich%20set%20of%20reusable%0Acomponents%20that%20foster%20a%20cost-effective%20and%20extensible%20ecosystem.%20Our%20code%20and%0Amodels%20are%20publicly%20available%20at%3A%20https%3A//github.com/EvolvingLMMs-Lab/NEO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Words%2520--%2520Towards%2520Native%2520Vision-Language%2520Primitives%2520at%250A%2520%2520Scale%26entry.906535625%3DHaiwen%2520Diao%2520and%2520Mingxuan%2520Li%2520and%2520Silei%2520Wu%2520and%2520Linjun%2520Dai%2520and%2520Xiaohua%2520Wang%2520and%2520Hanming%2520Deng%2520and%2520Lewei%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520The%2520edifice%2520of%2520native%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520emerged%2520as%2520a%2520rising%250Acontender%2520to%2520typical%2520modular%2520VLMs%252C%2520shaped%2520by%2520evolving%2520model%2520architectures%2520and%250Atraining%2520paradigms.%2520Yet%252C%2520two%2520lingering%2520clouds%2520cast%2520shadows%2520over%2520its%2520widespread%250Aexploration%2520and%2520promotion%253A%2520%2528-%2529%2520What%2520fundamental%2520constraints%2520set%2520native%2520VLMs%250Aapart%2520from%2520modular%2520ones%252C%2520and%2520to%2520what%2520extent%2520can%2520these%2520barriers%2520be%2520overcome%253F%2520%2528-%2529%250AHow%2520to%2520make%2520research%2520in%2520native%2520VLMs%2520more%2520accessible%2520and%2520democratized%252C%2520thereby%250Aaccelerating%2520progress%2520in%2520the%2520field.%2520In%2520this%2520paper%252C%2520we%2520clarify%2520these%2520challenges%250Aand%2520outline%2520guiding%2520principles%2520for%2520constructing%2520native%2520VLMs.%2520Specifically%252C%2520one%250Anative%2520VLM%2520primitive%2520should%253A%2520%2528i%2529%2520effectively%2520align%2520pixel%2520and%2520word%250Arepresentations%2520within%2520a%2520shared%2520semantic%2520space%253B%2520%2528ii%2529%2520seamlessly%2520integrate%2520the%250Astrengths%2520of%2520formerly%2520separate%2520vision%2520and%2520language%2520modules%253B%2520%2528iii%2529%2520inherently%250Aembody%2520various%2520cross-modal%2520properties%2520that%2520support%2520unified%2520vision-language%250Aencoding%252C%2520aligning%252C%2520and%2520reasoning.%2520Hence%252C%2520we%2520launch%2520NEO%252C%2520a%2520novel%2520family%2520of%250Anative%2520VLMs%2520built%2520from%2520first%2520principles%252C%2520capable%2520of%2520rivaling%2520top-tier%2520modular%250Acounterparts%2520across%2520diverse%2520real-world%2520scenarios.%2520With%2520only%2520390M%2520image-text%250Aexamples%252C%2520NEO%2520efficiently%2520develops%2520visual%2520perception%2520from%2520scratch%2520while%250Amitigating%2520vision-language%2520conflicts%2520inside%2520a%2520dense%2520and%2520monolithic%2520model%250Acrafted%2520from%2520our%2520elaborate%2520primitives.%2520We%2520position%2520NEO%2520as%2520a%2520cornerstone%2520for%250Ascalable%2520and%2520powerful%2520native%2520VLMs%252C%2520paired%2520with%2520a%2520rich%2520set%2520of%2520reusable%250Acomponents%2520that%2520foster%2520a%2520cost-effective%2520and%2520extensible%2520ecosystem.%2520Our%2520code%2520and%250Amodels%2520are%2520publicly%2520available%2520at%253A%2520https%253A//github.com/EvolvingLMMs-Lab/NEO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Words%20--%20Towards%20Native%20Vision-Language%20Primitives%20at%0A%20%20Scale&entry.906535625=Haiwen%20Diao%20and%20Mingxuan%20Li%20and%20Silei%20Wu%20and%20Linjun%20Dai%20and%20Xiaohua%20Wang%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Dahua%20Lin%20and%20Ziwei%20Liu&entry.1292438233=%20%20The%20edifice%20of%20native%20Vision-Language%20Models%20%28VLMs%29%20has%20emerged%20as%20a%20rising%0Acontender%20to%20typical%20modular%20VLMs%2C%20shaped%20by%20evolving%20model%20architectures%20and%0Atraining%20paradigms.%20Yet%2C%20two%20lingering%20clouds%20cast%20shadows%20over%20its%20widespread%0Aexploration%20and%20promotion%3A%20%28-%29%20What%20fundamental%20constraints%20set%20native%20VLMs%0Aapart%20from%20modular%20ones%2C%20and%20to%20what%20extent%20can%20these%20barriers%20be%20overcome%3F%20%28-%29%0AHow%20to%20make%20research%20in%20native%20VLMs%20more%20accessible%20and%20democratized%2C%20thereby%0Aaccelerating%20progress%20in%20the%20field.%20In%20this%20paper%2C%20we%20clarify%20these%20challenges%0Aand%20outline%20guiding%20principles%20for%20constructing%20native%20VLMs.%20Specifically%2C%20one%0Anative%20VLM%20primitive%20should%3A%20%28i%29%20effectively%20align%20pixel%20and%20word%0Arepresentations%20within%20a%20shared%20semantic%20space%3B%20%28ii%29%20seamlessly%20integrate%20the%0Astrengths%20of%20formerly%20separate%20vision%20and%20language%20modules%3B%20%28iii%29%20inherently%0Aembody%20various%20cross-modal%20properties%20that%20support%20unified%20vision-language%0Aencoding%2C%20aligning%2C%20and%20reasoning.%20Hence%2C%20we%20launch%20NEO%2C%20a%20novel%20family%20of%0Anative%20VLMs%20built%20from%20first%20principles%2C%20capable%20of%20rivaling%20top-tier%20modular%0Acounterparts%20across%20diverse%20real-world%20scenarios.%20With%20only%20390M%20image-text%0Aexamples%2C%20NEO%20efficiently%20develops%20visual%20perception%20from%20scratch%20while%0Amitigating%20vision-language%20conflicts%20inside%20a%20dense%20and%20monolithic%20model%0Acrafted%20from%20our%20elaborate%20primitives.%20We%20position%20NEO%20as%20a%20cornerstone%20for%0Ascalable%20and%20powerful%20native%20VLMs%2C%20paired%20with%20a%20rich%20set%20of%20reusable%0Acomponents%20that%20foster%20a%20cost-effective%20and%20extensible%20ecosystem.%20Our%20code%20and%0Amodels%20are%20publicly%20available%20at%3A%20https%3A//github.com/EvolvingLMMs-Lab/NEO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14979v1&entry.124074799=Read"},
{"title": "On Large Multimodal Models as Open-World Image Classifiers", "author": "Alessandro Conti and Massimiliano Mancini and Enrico Fini and Yiming Wang and Paolo Rota and Elisa Ricci", "abstract": "  Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.\n", "link": "http://arxiv.org/abs/2503.21851v2", "date": "2025-10-16", "relevancy": 2.8699, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Large%20Multimodal%20Models%20as%20Open-World%20Image%20Classifiers&body=Title%3A%20On%20Large%20Multimodal%20Models%20as%20Open-World%20Image%20Classifiers%0AAuthor%3A%20Alessandro%20Conti%20and%20Massimiliano%20Mancini%20and%20Enrico%20Fini%20and%20Yiming%20Wang%20and%20Paolo%20Rota%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Traditional%20image%20classification%20requires%20a%20predefined%20list%20of%20semantic%0Acategories.%20In%20contrast%2C%20Large%20Multimodal%20Models%20%28LMMs%29%20can%20sidestep%20this%0Arequirement%20by%20classifying%20images%20directly%20using%20natural%20language%20%28e.g.%2C%0Aanswering%20the%20prompt%20%22What%20is%20the%20main%20object%20in%20the%20image%3F%22%29.%20Despite%20this%0Aremarkable%20capability%2C%20most%20existing%20studies%20on%20LMM%20classification%20performance%0Aare%20surprisingly%20limited%20in%20scope%2C%20often%20assuming%20a%20closed-world%20setting%20with%20a%0Apredefined%20set%20of%20categories.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20thoroughly%0Aevaluating%20LMM%20classification%20performance%20in%20a%20truly%20open-world%20setting.%20We%0Afirst%20formalize%20the%20task%20and%20introduce%20an%20evaluation%20protocol%2C%20defining%20various%0Ametrics%20to%20assess%20the%20alignment%20between%20predicted%20and%20ground%20truth%20classes.%20We%0Athen%20evaluate%2013%20models%20across%2010%20benchmarks%2C%20encompassing%20prototypical%2C%0Anon-prototypical%2C%20fine-grained%2C%20and%20very%20fine-grained%20classes%2C%20demonstrating%0Athe%20challenges%20LMMs%20face%20in%20this%20task.%20Further%20analyses%20based%20on%20the%20proposed%0Ametrics%20reveal%20the%20types%20of%20errors%20LMMs%20make%2C%20highlighting%20challenges%20related%0Ato%20granularity%20and%20fine-grained%20capabilities%2C%20showing%20how%20tailored%20prompting%0Aand%20reasoning%20can%20alleviate%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Large%2520Multimodal%2520Models%2520as%2520Open-World%2520Image%2520Classifiers%26entry.906535625%3DAlessandro%2520Conti%2520and%2520Massimiliano%2520Mancini%2520and%2520Enrico%2520Fini%2520and%2520Yiming%2520Wang%2520and%2520Paolo%2520Rota%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Traditional%2520image%2520classification%2520requires%2520a%2520predefined%2520list%2520of%2520semantic%250Acategories.%2520In%2520contrast%252C%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520can%2520sidestep%2520this%250Arequirement%2520by%2520classifying%2520images%2520directly%2520using%2520natural%2520language%2520%2528e.g.%252C%250Aanswering%2520the%2520prompt%2520%2522What%2520is%2520the%2520main%2520object%2520in%2520the%2520image%253F%2522%2529.%2520Despite%2520this%250Aremarkable%2520capability%252C%2520most%2520existing%2520studies%2520on%2520LMM%2520classification%2520performance%250Aare%2520surprisingly%2520limited%2520in%2520scope%252C%2520often%2520assuming%2520a%2520closed-world%2520setting%2520with%2520a%250Apredefined%2520set%2520of%2520categories.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520thoroughly%250Aevaluating%2520LMM%2520classification%2520performance%2520in%2520a%2520truly%2520open-world%2520setting.%2520We%250Afirst%2520formalize%2520the%2520task%2520and%2520introduce%2520an%2520evaluation%2520protocol%252C%2520defining%2520various%250Ametrics%2520to%2520assess%2520the%2520alignment%2520between%2520predicted%2520and%2520ground%2520truth%2520classes.%2520We%250Athen%2520evaluate%252013%2520models%2520across%252010%2520benchmarks%252C%2520encompassing%2520prototypical%252C%250Anon-prototypical%252C%2520fine-grained%252C%2520and%2520very%2520fine-grained%2520classes%252C%2520demonstrating%250Athe%2520challenges%2520LMMs%2520face%2520in%2520this%2520task.%2520Further%2520analyses%2520based%2520on%2520the%2520proposed%250Ametrics%2520reveal%2520the%2520types%2520of%2520errors%2520LMMs%2520make%252C%2520highlighting%2520challenges%2520related%250Ato%2520granularity%2520and%2520fine-grained%2520capabilities%252C%2520showing%2520how%2520tailored%2520prompting%250Aand%2520reasoning%2520can%2520alleviate%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Large%20Multimodal%20Models%20as%20Open-World%20Image%20Classifiers&entry.906535625=Alessandro%20Conti%20and%20Massimiliano%20Mancini%20and%20Enrico%20Fini%20and%20Yiming%20Wang%20and%20Paolo%20Rota%20and%20Elisa%20Ricci&entry.1292438233=%20%20Traditional%20image%20classification%20requires%20a%20predefined%20list%20of%20semantic%0Acategories.%20In%20contrast%2C%20Large%20Multimodal%20Models%20%28LMMs%29%20can%20sidestep%20this%0Arequirement%20by%20classifying%20images%20directly%20using%20natural%20language%20%28e.g.%2C%0Aanswering%20the%20prompt%20%22What%20is%20the%20main%20object%20in%20the%20image%3F%22%29.%20Despite%20this%0Aremarkable%20capability%2C%20most%20existing%20studies%20on%20LMM%20classification%20performance%0Aare%20surprisingly%20limited%20in%20scope%2C%20often%20assuming%20a%20closed-world%20setting%20with%20a%0Apredefined%20set%20of%20categories.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20thoroughly%0Aevaluating%20LMM%20classification%20performance%20in%20a%20truly%20open-world%20setting.%20We%0Afirst%20formalize%20the%20task%20and%20introduce%20an%20evaluation%20protocol%2C%20defining%20various%0Ametrics%20to%20assess%20the%20alignment%20between%20predicted%20and%20ground%20truth%20classes.%20We%0Athen%20evaluate%2013%20models%20across%2010%20benchmarks%2C%20encompassing%20prototypical%2C%0Anon-prototypical%2C%20fine-grained%2C%20and%20very%20fine-grained%20classes%2C%20demonstrating%0Athe%20challenges%20LMMs%20face%20in%20this%20task.%20Further%20analyses%20based%20on%20the%20proposed%0Ametrics%20reveal%20the%20types%20of%20errors%20LMMs%20make%2C%20highlighting%20challenges%20related%0Ato%20granularity%20and%20fine-grained%20capabilities%2C%20showing%20how%20tailored%20prompting%0Aand%20reasoning%20can%20alleviate%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21851v2&entry.124074799=Read"},
{"title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster\n  VLM Inference", "author": "Natan Bagrov and Eugene Khvedchenia and Borys Tymchenko and Shay Aharon and Lior Kadoch and Tomer Keren and Ofri Masad and Yonatan Geifman and Ran Zilberstein and Tuomas Rintamaki and Matthieu Le and Andrew Tao", "abstract": "  Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.\n", "link": "http://arxiv.org/abs/2510.14624v1", "date": "2025-10-16", "relevancy": 2.8467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Video%20Sampling%3A%20Pruning%20Temporally%20Redundant%20Tokens%20for%20Faster%0A%20%20VLM%20Inference&body=Title%3A%20Efficient%20Video%20Sampling%3A%20Pruning%20Temporally%20Redundant%20Tokens%20for%20Faster%0A%20%20VLM%20Inference%0AAuthor%3A%20Natan%20Bagrov%20and%20Eugene%20Khvedchenia%20and%20Borys%20Tymchenko%20and%20Shay%20Aharon%20and%20Lior%20Kadoch%20and%20Tomer%20Keren%20and%20Ofri%20Masad%20and%20Yonatan%20Geifman%20and%20Ran%20Zilberstein%20and%20Tuomas%20Rintamaki%20and%20Matthieu%20Le%20and%20Andrew%20Tao%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20expanded%20from%20static%20image%0Aunderstanding%20to%20video%20reasoning%2C%20but%20their%20scalability%20is%20fundamentally%0Alimited%20by%20the%20quadratic%20cost%20of%20processing%20dense%20frame%20sequences.%20Long%20videos%0Aoften%20exceed%20the%20token%20budget%20of%20modern%20language%20models%2C%20leading%20to%20severe%0Acontext%20limitations%20and%20latency%20issues.%20We%20introduce%20Efficient%20Video%20Sampling%0A%28EVS%29%2C%20a%20simple%2C%20plug-and-play%20method%20for%20reducing%20token%20redundancy%20in%20videos%0Aby%20identifying%20and%20pruning%20temporally%20static%20patches%20--%20spatial%20regions%20that%0Aremain%20unchanged%20across%20consecutive%20frames.%20EVS%20preserves%20positional%20identity%2C%0Arequires%20no%20architectural%20changes%20or%20retraining.%20We%20show%20that%20EVS%20substantially%0Areduces%20token%20count%20while%20maintaining%20semantic%20fidelity%2C%20enabling%20faster%0Ainference%20and%20longer%20input%20sequences.%20Applied%20at%20inference%20time%2C%20EVS%20reduces%0Alarge%20language%20model%20%28LLM%29%20time-to-first-token%20%28TTFT%29%20by%20up%20to%204x%20with%20minimal%0Aaccuracy%20loss.%20When%20combined%20with%20an%20uptraining%20phase%20using%20stochastic%20pruning%0Arates%2C%20EVS%20yields%20models%20that%20are%20robust%20to%20varying%20compression%20levels%20and%0Aretain%20full%20performance%20under%20aggressive%20pruning.%20Extensive%20experiments%0Ademonstrate%20that%20EVS%20consistently%20improves%20efficiency-accuracy%20trade-offs%2C%0Aunlocking%20scalable%20video-language%20understanding%20without%20sacrificing%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Video%2520Sampling%253A%2520Pruning%2520Temporally%2520Redundant%2520Tokens%2520for%2520Faster%250A%2520%2520VLM%2520Inference%26entry.906535625%3DNatan%2520Bagrov%2520and%2520Eugene%2520Khvedchenia%2520and%2520Borys%2520Tymchenko%2520and%2520Shay%2520Aharon%2520and%2520Lior%2520Kadoch%2520and%2520Tomer%2520Keren%2520and%2520Ofri%2520Masad%2520and%2520Yonatan%2520Geifman%2520and%2520Ran%2520Zilberstein%2520and%2520Tuomas%2520Rintamaki%2520and%2520Matthieu%2520Le%2520and%2520Andrew%2520Tao%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520recently%2520expanded%2520from%2520static%2520image%250Aunderstanding%2520to%2520video%2520reasoning%252C%2520but%2520their%2520scalability%2520is%2520fundamentally%250Alimited%2520by%2520the%2520quadratic%2520cost%2520of%2520processing%2520dense%2520frame%2520sequences.%2520Long%2520videos%250Aoften%2520exceed%2520the%2520token%2520budget%2520of%2520modern%2520language%2520models%252C%2520leading%2520to%2520severe%250Acontext%2520limitations%2520and%2520latency%2520issues.%2520We%2520introduce%2520Efficient%2520Video%2520Sampling%250A%2528EVS%2529%252C%2520a%2520simple%252C%2520plug-and-play%2520method%2520for%2520reducing%2520token%2520redundancy%2520in%2520videos%250Aby%2520identifying%2520and%2520pruning%2520temporally%2520static%2520patches%2520--%2520spatial%2520regions%2520that%250Aremain%2520unchanged%2520across%2520consecutive%2520frames.%2520EVS%2520preserves%2520positional%2520identity%252C%250Arequires%2520no%2520architectural%2520changes%2520or%2520retraining.%2520We%2520show%2520that%2520EVS%2520substantially%250Areduces%2520token%2520count%2520while%2520maintaining%2520semantic%2520fidelity%252C%2520enabling%2520faster%250Ainference%2520and%2520longer%2520input%2520sequences.%2520Applied%2520at%2520inference%2520time%252C%2520EVS%2520reduces%250Alarge%2520language%2520model%2520%2528LLM%2529%2520time-to-first-token%2520%2528TTFT%2529%2520by%2520up%2520to%25204x%2520with%2520minimal%250Aaccuracy%2520loss.%2520When%2520combined%2520with%2520an%2520uptraining%2520phase%2520using%2520stochastic%2520pruning%250Arates%252C%2520EVS%2520yields%2520models%2520that%2520are%2520robust%2520to%2520varying%2520compression%2520levels%2520and%250Aretain%2520full%2520performance%2520under%2520aggressive%2520pruning.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520EVS%2520consistently%2520improves%2520efficiency-accuracy%2520trade-offs%252C%250Aunlocking%2520scalable%2520video-language%2520understanding%2520without%2520sacrificing%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Video%20Sampling%3A%20Pruning%20Temporally%20Redundant%20Tokens%20for%20Faster%0A%20%20VLM%20Inference&entry.906535625=Natan%20Bagrov%20and%20Eugene%20Khvedchenia%20and%20Borys%20Tymchenko%20and%20Shay%20Aharon%20and%20Lior%20Kadoch%20and%20Tomer%20Keren%20and%20Ofri%20Masad%20and%20Yonatan%20Geifman%20and%20Ran%20Zilberstein%20and%20Tuomas%20Rintamaki%20and%20Matthieu%20Le%20and%20Andrew%20Tao&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20expanded%20from%20static%20image%0Aunderstanding%20to%20video%20reasoning%2C%20but%20their%20scalability%20is%20fundamentally%0Alimited%20by%20the%20quadratic%20cost%20of%20processing%20dense%20frame%20sequences.%20Long%20videos%0Aoften%20exceed%20the%20token%20budget%20of%20modern%20language%20models%2C%20leading%20to%20severe%0Acontext%20limitations%20and%20latency%20issues.%20We%20introduce%20Efficient%20Video%20Sampling%0A%28EVS%29%2C%20a%20simple%2C%20plug-and-play%20method%20for%20reducing%20token%20redundancy%20in%20videos%0Aby%20identifying%20and%20pruning%20temporally%20static%20patches%20--%20spatial%20regions%20that%0Aremain%20unchanged%20across%20consecutive%20frames.%20EVS%20preserves%20positional%20identity%2C%0Arequires%20no%20architectural%20changes%20or%20retraining.%20We%20show%20that%20EVS%20substantially%0Areduces%20token%20count%20while%20maintaining%20semantic%20fidelity%2C%20enabling%20faster%0Ainference%20and%20longer%20input%20sequences.%20Applied%20at%20inference%20time%2C%20EVS%20reduces%0Alarge%20language%20model%20%28LLM%29%20time-to-first-token%20%28TTFT%29%20by%20up%20to%204x%20with%20minimal%0Aaccuracy%20loss.%20When%20combined%20with%20an%20uptraining%20phase%20using%20stochastic%20pruning%0Arates%2C%20EVS%20yields%20models%20that%20are%20robust%20to%20varying%20compression%20levels%20and%0Aretain%20full%20performance%20under%20aggressive%20pruning.%20Extensive%20experiments%0Ademonstrate%20that%20EVS%20consistently%20improves%20efficiency-accuracy%20trade-offs%2C%0Aunlocking%20scalable%20video-language%20understanding%20without%20sacrificing%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14624v1&entry.124074799=Read"},
{"title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition\n  Capabilities of Multimodal Large Language Models with Answer Extraction", "author": "Logan Lawrence and Oindrila Saha and Megan Wei and Chen Sun and Subhransu Maji and Grant Van Horn", "abstract": "  Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.\n", "link": "http://arxiv.org/abs/2510.14885v1", "date": "2025-10-16", "relevancy": 2.8415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5727}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20May%20Speak%20Freely%3A%20Improving%20the%20Fine-Grained%20Visual%20Recognition%0A%20%20Capabilities%20of%20Multimodal%20Large%20Language%20Models%20with%20Answer%20Extraction&body=Title%3A%20You%20May%20Speak%20Freely%3A%20Improving%20the%20Fine-Grained%20Visual%20Recognition%0A%20%20Capabilities%20of%20Multimodal%20Large%20Language%20Models%20with%20Answer%20Extraction%0AAuthor%3A%20Logan%20Lawrence%20and%20Oindrila%20Saha%20and%20Megan%20Wei%20and%20Chen%20Sun%20and%20Subhransu%20Maji%20and%20Grant%20Van%20Horn%0AAbstract%3A%20%20%20Despite%20the%20renewed%20interest%20in%20zero-shot%20visual%20classification%20due%20to%20the%0Arise%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20the%20problem%20of%20evaluating%0Afree-form%20responses%20of%20auto-regressive%20models%20remains%20a%20persistent%20challenge.%0AMost%20existing%20works%20focus%20on%20language-only%20tasks%20or%20don%27t%20consider%20Multiple%0AChoice%20Questions%20%28MCQs%29%20beyond%205-way%20options%2C%20both%20of%20which%20are%20critical%0Acapabilities%20to%20solve%20tasks%20in%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20where%0Achoice%20counts%20are%20in%20the%20hundreds%20to%20thousands%20and%20the%20choices%20are%20highly%0Arelated.%20Furthermore%2C%20in%20this%20highly%20multi-way%20MCQ%20setting%20it%20is%20not%20clear%20how%0Ato%20extend%20LLM%20choice%20extraction%20to%20retrieval-based%20problems%2C%20where%20computing%0Aprobabilities%20over%20the%20choice%20set%20is%20computationally%20costly.%20In%20this%20work%20we%0Ainvestigate%20nlg2choice%2C%20a%20simple%20two-stage%20method%20which%20first%20asks%20the%20MLLM%20an%0Aopen-ended%20question%20for%20the%20task%20with%20minimal%20constraints%2C%20then%20uses%20text-only%0Aconstrained%20decoding%20to%20predict%20the%20most%20likely%20choice.%20In%20retrieval%20settings%2C%0Awe%20compute%20the%20probability%20of%20the%20constrained%20response%20taking%20that%20choice%20with%0Aan%20early%20stopping%20method%20to%20significantly%20improve%20throughput.%20Our%20results%20show%0Aimprovement%20over%20a%20suite%20of%20seven%20fine-grained%20visual%20datasets%20when%20evaluating%0Ain%20terms%20of%20classification%20and%20retrieval%2C%20and%20show%20that%20this%20performance%20holds%0Aover%20the%20various%20ways%20that%20users%20of%20LLMs%20can%20implement%20tasks%20in%20natural%0Alanguage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520May%2520Speak%2520Freely%253A%2520Improving%2520the%2520Fine-Grained%2520Visual%2520Recognition%250A%2520%2520Capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Answer%2520Extraction%26entry.906535625%3DLogan%2520Lawrence%2520and%2520Oindrila%2520Saha%2520and%2520Megan%2520Wei%2520and%2520Chen%2520Sun%2520and%2520Subhransu%2520Maji%2520and%2520Grant%2520Van%2520Horn%26entry.1292438233%3D%2520%2520Despite%2520the%2520renewed%2520interest%2520in%2520zero-shot%2520visual%2520classification%2520due%2520to%2520the%250Arise%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520the%2520problem%2520of%2520evaluating%250Afree-form%2520responses%2520of%2520auto-regressive%2520models%2520remains%2520a%2520persistent%2520challenge.%250AMost%2520existing%2520works%2520focus%2520on%2520language-only%2520tasks%2520or%2520don%2527t%2520consider%2520Multiple%250AChoice%2520Questions%2520%2528MCQs%2529%2520beyond%25205-way%2520options%252C%2520both%2520of%2520which%2520are%2520critical%250Acapabilities%2520to%2520solve%2520tasks%2520in%2520Fine-Grained%2520Visual%2520Classification%2520%2528FGVC%2529%2520where%250Achoice%2520counts%2520are%2520in%2520the%2520hundreds%2520to%2520thousands%2520and%2520the%2520choices%2520are%2520highly%250Arelated.%2520Furthermore%252C%2520in%2520this%2520highly%2520multi-way%2520MCQ%2520setting%2520it%2520is%2520not%2520clear%2520how%250Ato%2520extend%2520LLM%2520choice%2520extraction%2520to%2520retrieval-based%2520problems%252C%2520where%2520computing%250Aprobabilities%2520over%2520the%2520choice%2520set%2520is%2520computationally%2520costly.%2520In%2520this%2520work%2520we%250Ainvestigate%2520nlg2choice%252C%2520a%2520simple%2520two-stage%2520method%2520which%2520first%2520asks%2520the%2520MLLM%2520an%250Aopen-ended%2520question%2520for%2520the%2520task%2520with%2520minimal%2520constraints%252C%2520then%2520uses%2520text-only%250Aconstrained%2520decoding%2520to%2520predict%2520the%2520most%2520likely%2520choice.%2520In%2520retrieval%2520settings%252C%250Awe%2520compute%2520the%2520probability%2520of%2520the%2520constrained%2520response%2520taking%2520that%2520choice%2520with%250Aan%2520early%2520stopping%2520method%2520to%2520significantly%2520improve%2520throughput.%2520Our%2520results%2520show%250Aimprovement%2520over%2520a%2520suite%2520of%2520seven%2520fine-grained%2520visual%2520datasets%2520when%2520evaluating%250Ain%2520terms%2520of%2520classification%2520and%2520retrieval%252C%2520and%2520show%2520that%2520this%2520performance%2520holds%250Aover%2520the%2520various%2520ways%2520that%2520users%2520of%2520LLMs%2520can%2520implement%2520tasks%2520in%2520natural%250Alanguage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20May%20Speak%20Freely%3A%20Improving%20the%20Fine-Grained%20Visual%20Recognition%0A%20%20Capabilities%20of%20Multimodal%20Large%20Language%20Models%20with%20Answer%20Extraction&entry.906535625=Logan%20Lawrence%20and%20Oindrila%20Saha%20and%20Megan%20Wei%20and%20Chen%20Sun%20and%20Subhransu%20Maji%20and%20Grant%20Van%20Horn&entry.1292438233=%20%20Despite%20the%20renewed%20interest%20in%20zero-shot%20visual%20classification%20due%20to%20the%0Arise%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20the%20problem%20of%20evaluating%0Afree-form%20responses%20of%20auto-regressive%20models%20remains%20a%20persistent%20challenge.%0AMost%20existing%20works%20focus%20on%20language-only%20tasks%20or%20don%27t%20consider%20Multiple%0AChoice%20Questions%20%28MCQs%29%20beyond%205-way%20options%2C%20both%20of%20which%20are%20critical%0Acapabilities%20to%20solve%20tasks%20in%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20where%0Achoice%20counts%20are%20in%20the%20hundreds%20to%20thousands%20and%20the%20choices%20are%20highly%0Arelated.%20Furthermore%2C%20in%20this%20highly%20multi-way%20MCQ%20setting%20it%20is%20not%20clear%20how%0Ato%20extend%20LLM%20choice%20extraction%20to%20retrieval-based%20problems%2C%20where%20computing%0Aprobabilities%20over%20the%20choice%20set%20is%20computationally%20costly.%20In%20this%20work%20we%0Ainvestigate%20nlg2choice%2C%20a%20simple%20two-stage%20method%20which%20first%20asks%20the%20MLLM%20an%0Aopen-ended%20question%20for%20the%20task%20with%20minimal%20constraints%2C%20then%20uses%20text-only%0Aconstrained%20decoding%20to%20predict%20the%20most%20likely%20choice.%20In%20retrieval%20settings%2C%0Awe%20compute%20the%20probability%20of%20the%20constrained%20response%20taking%20that%20choice%20with%0Aan%20early%20stopping%20method%20to%20significantly%20improve%20throughput.%20Our%20results%20show%0Aimprovement%20over%20a%20suite%20of%20seven%20fine-grained%20visual%20datasets%20when%20evaluating%0Ain%20terms%20of%20classification%20and%20retrieval%2C%20and%20show%20that%20this%20performance%20holds%0Aover%20the%20various%20ways%20that%20users%20of%20LLMs%20can%20implement%20tasks%20in%20natural%0Alanguage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14885v1&entry.124074799=Read"},
{"title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning", "author": "Weikang Shi and Aldrich Yu and Rongyao Fang and Houxing Ren and Ke Wang and Aojun Zhou and Changyao Tian and Xinyu Fu and Yuxuan Hu and Zimu Lu and Linjiang Huang and Si Liu and Rui Liu and Hongsheng Li", "abstract": "  While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/\n", "link": "http://arxiv.org/abs/2510.14958v1", "date": "2025-10-16", "relevancy": 2.8315, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathCanvas%3A%20Intrinsic%20Visual%20Chain-of-Thought%20for%20Multimodal%0A%20%20Mathematical%20Reasoning&body=Title%3A%20MathCanvas%3A%20Intrinsic%20Visual%20Chain-of-Thought%20for%20Multimodal%0A%20%20Mathematical%20Reasoning%0AAuthor%3A%20Weikang%20Shi%20and%20Aldrich%20Yu%20and%20Rongyao%20Fang%20and%20Houxing%20Ren%20and%20Ke%20Wang%20and%20Aojun%20Zhou%20and%20Changyao%20Tian%20and%20Xinyu%20Fu%20and%20Yuxuan%20Hu%20and%20Zimu%20Lu%20and%20Linjiang%20Huang%20and%20Si%20Liu%20and%20Rui%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20excelled%20in%20textual%20reasoning%2C%20they%0Astruggle%20with%20mathematical%20domains%20like%20geometry%20that%20intrinsically%20rely%20on%0Avisual%20aids.%20Existing%20approaches%20to%20Visual%20Chain-of-Thought%20%28VCoT%29%20are%20often%0Alimited%20by%20rigid%20external%20tools%20or%20fail%20to%20generate%20the%20high-fidelity%2C%0Astrategically-timed%20diagrams%20necessary%20for%20complex%20problem-solving.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20MathCanvas%2C%20a%20comprehensive%20framework%20designed%20to%20endow%0Aunified%20Large%20Multimodal%20Models%20%28LMMs%29%20with%20intrinsic%20VCoT%20capabilities%20for%0Amathematics.%20Our%20approach%20consists%20of%20two%20phases.%20First%2C%20a%20Visual%20Manipulation%0Astage%20pre-trains%20the%20model%20on%20a%20novel%2015.2M-pair%20corpus%2C%20comprising%2010M%0Acaption-to-diagram%20pairs%20%28MathCanvas-Imagen%29%20and%205.2M%20step-by-step%20editing%0Atrajectories%20%28MathCanvas-Edit%29%2C%20to%20master%20diagram%20generation%20and%20editing.%0ASecond%2C%20a%20Strategic%20Visual-Aided%20Reasoning%20stage%20fine-tunes%20the%20model%20on%0AMathCanvas-Instruct%2C%20a%20new%20219K-example%20dataset%20of%20interleaved%20visual-textual%0Areasoning%20paths%2C%20teaching%20it%20when%20and%20how%20to%20leverage%20visual%20aids.%20To%0Afacilitate%20rigorous%20evaluation%2C%20we%20introduce%20MathCanvas-Bench%2C%20a%20challenging%0Abenchmark%20with%203K%20problems%20that%20require%20models%20to%20produce%20interleaved%0Avisual-textual%20solutions.%20Our%20model%2C%20BAGEL-Canvas%2C%20trained%20under%20this%0Aframework%2C%20achieves%20an%2086%25%20relative%20improvement%20over%20strong%20LMM%20baselines%20on%0AMathCanvas-Bench%2C%20demonstrating%20excellent%20generalization%20to%20other%20public%20math%0Abenchmarks.%20Our%20work%20provides%20a%20complete%20toolkit-framework%2C%20datasets%2C%20and%0Abenchmark-to%20unlock%20complex%2C%20human-like%20visual-aided%20reasoning%20in%20LMMs.%20Project%0APage%3A%20https%3A//mathcanvas.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathCanvas%253A%2520Intrinsic%2520Visual%2520Chain-of-Thought%2520for%2520Multimodal%250A%2520%2520Mathematical%2520Reasoning%26entry.906535625%3DWeikang%2520Shi%2520and%2520Aldrich%2520Yu%2520and%2520Rongyao%2520Fang%2520and%2520Houxing%2520Ren%2520and%2520Ke%2520Wang%2520and%2520Aojun%2520Zhou%2520and%2520Changyao%2520Tian%2520and%2520Xinyu%2520Fu%2520and%2520Yuxuan%2520Hu%2520and%2520Zimu%2520Lu%2520and%2520Linjiang%2520Huang%2520and%2520Si%2520Liu%2520and%2520Rui%2520Liu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520excelled%2520in%2520textual%2520reasoning%252C%2520they%250Astruggle%2520with%2520mathematical%2520domains%2520like%2520geometry%2520that%2520intrinsically%2520rely%2520on%250Avisual%2520aids.%2520Existing%2520approaches%2520to%2520Visual%2520Chain-of-Thought%2520%2528VCoT%2529%2520are%2520often%250Alimited%2520by%2520rigid%2520external%2520tools%2520or%2520fail%2520to%2520generate%2520the%2520high-fidelity%252C%250Astrategically-timed%2520diagrams%2520necessary%2520for%2520complex%2520problem-solving.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520MathCanvas%252C%2520a%2520comprehensive%2520framework%2520designed%2520to%2520endow%250Aunified%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520with%2520intrinsic%2520VCoT%2520capabilities%2520for%250Amathematics.%2520Our%2520approach%2520consists%2520of%2520two%2520phases.%2520First%252C%2520a%2520Visual%2520Manipulation%250Astage%2520pre-trains%2520the%2520model%2520on%2520a%2520novel%252015.2M-pair%2520corpus%252C%2520comprising%252010M%250Acaption-to-diagram%2520pairs%2520%2528MathCanvas-Imagen%2529%2520and%25205.2M%2520step-by-step%2520editing%250Atrajectories%2520%2528MathCanvas-Edit%2529%252C%2520to%2520master%2520diagram%2520generation%2520and%2520editing.%250ASecond%252C%2520a%2520Strategic%2520Visual-Aided%2520Reasoning%2520stage%2520fine-tunes%2520the%2520model%2520on%250AMathCanvas-Instruct%252C%2520a%2520new%2520219K-example%2520dataset%2520of%2520interleaved%2520visual-textual%250Areasoning%2520paths%252C%2520teaching%2520it%2520when%2520and%2520how%2520to%2520leverage%2520visual%2520aids.%2520To%250Afacilitate%2520rigorous%2520evaluation%252C%2520we%2520introduce%2520MathCanvas-Bench%252C%2520a%2520challenging%250Abenchmark%2520with%25203K%2520problems%2520that%2520require%2520models%2520to%2520produce%2520interleaved%250Avisual-textual%2520solutions.%2520Our%2520model%252C%2520BAGEL-Canvas%252C%2520trained%2520under%2520this%250Aframework%252C%2520achieves%2520an%252086%2525%2520relative%2520improvement%2520over%2520strong%2520LMM%2520baselines%2520on%250AMathCanvas-Bench%252C%2520demonstrating%2520excellent%2520generalization%2520to%2520other%2520public%2520math%250Abenchmarks.%2520Our%2520work%2520provides%2520a%2520complete%2520toolkit-framework%252C%2520datasets%252C%2520and%250Abenchmark-to%2520unlock%2520complex%252C%2520human-like%2520visual-aided%2520reasoning%2520in%2520LMMs.%2520Project%250APage%253A%2520https%253A//mathcanvas.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathCanvas%3A%20Intrinsic%20Visual%20Chain-of-Thought%20for%20Multimodal%0A%20%20Mathematical%20Reasoning&entry.906535625=Weikang%20Shi%20and%20Aldrich%20Yu%20and%20Rongyao%20Fang%20and%20Houxing%20Ren%20and%20Ke%20Wang%20and%20Aojun%20Zhou%20and%20Changyao%20Tian%20and%20Xinyu%20Fu%20and%20Yuxuan%20Hu%20and%20Zimu%20Lu%20and%20Linjiang%20Huang%20and%20Si%20Liu%20and%20Rui%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20excelled%20in%20textual%20reasoning%2C%20they%0Astruggle%20with%20mathematical%20domains%20like%20geometry%20that%20intrinsically%20rely%20on%0Avisual%20aids.%20Existing%20approaches%20to%20Visual%20Chain-of-Thought%20%28VCoT%29%20are%20often%0Alimited%20by%20rigid%20external%20tools%20or%20fail%20to%20generate%20the%20high-fidelity%2C%0Astrategically-timed%20diagrams%20necessary%20for%20complex%20problem-solving.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20MathCanvas%2C%20a%20comprehensive%20framework%20designed%20to%20endow%0Aunified%20Large%20Multimodal%20Models%20%28LMMs%29%20with%20intrinsic%20VCoT%20capabilities%20for%0Amathematics.%20Our%20approach%20consists%20of%20two%20phases.%20First%2C%20a%20Visual%20Manipulation%0Astage%20pre-trains%20the%20model%20on%20a%20novel%2015.2M-pair%20corpus%2C%20comprising%2010M%0Acaption-to-diagram%20pairs%20%28MathCanvas-Imagen%29%20and%205.2M%20step-by-step%20editing%0Atrajectories%20%28MathCanvas-Edit%29%2C%20to%20master%20diagram%20generation%20and%20editing.%0ASecond%2C%20a%20Strategic%20Visual-Aided%20Reasoning%20stage%20fine-tunes%20the%20model%20on%0AMathCanvas-Instruct%2C%20a%20new%20219K-example%20dataset%20of%20interleaved%20visual-textual%0Areasoning%20paths%2C%20teaching%20it%20when%20and%20how%20to%20leverage%20visual%20aids.%20To%0Afacilitate%20rigorous%20evaluation%2C%20we%20introduce%20MathCanvas-Bench%2C%20a%20challenging%0Abenchmark%20with%203K%20problems%20that%20require%20models%20to%20produce%20interleaved%0Avisual-textual%20solutions.%20Our%20model%2C%20BAGEL-Canvas%2C%20trained%20under%20this%0Aframework%2C%20achieves%20an%2086%25%20relative%20improvement%20over%20strong%20LMM%20baselines%20on%0AMathCanvas-Bench%2C%20demonstrating%20excellent%20generalization%20to%20other%20public%20math%0Abenchmarks.%20Our%20work%20provides%20a%20complete%20toolkit-framework%2C%20datasets%2C%20and%0Abenchmark-to%20unlock%20complex%2C%20human-like%20visual-aided%20reasoning%20in%20LMMs.%20Project%0APage%3A%20https%3A//mathcanvas.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14958v1&entry.124074799=Read"},
{"title": "Reasoning in Space via Grounding in the World", "author": "Yiming Chen and Zekun Qi and Wenyao Zhang and Xin Jin and Li Zhang and Peidong Liu", "abstract": "  In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2510.13800v2", "date": "2025-10-16", "relevancy": 2.787, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20in%20Space%20via%20Grounding%20in%20the%20World&body=Title%3A%20Reasoning%20in%20Space%20via%20Grounding%20in%20the%20World%0AAuthor%3A%20Yiming%20Chen%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Xin%20Jin%20and%20Li%20Zhang%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20claim%20that%203D%20visual%20grounding%20is%20the%20cornerstone%20of%0Aspatial%20reasoning%20and%20introduce%20the%20Grounded-Spatial%20Reasoner%20%28GS-Reasoner%29%20to%0Aexplore%20the%20effective%20spatial%20representations%20that%20bridge%20the%20gap%20between%20them.%0AExisting%203D%20LLMs%20suffer%20from%20the%20absence%20of%20a%20unified%203D%20representation%20capable%0Aof%20jointly%20capturing%20semantic%20and%20geometric%20information.%20This%20deficiency%20is%0Amanifested%20either%20in%20poor%20performance%20on%20grounding%20or%20in%20an%20excessive%20reliance%0Aon%20external%20modules%2C%20ultimately%20hindering%20the%20seamless%20integration%20of%20grounding%0Aand%20spatial%20reasoning.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%0Adual-path%20pooling%20mechanism%20that%20tightly%20aligns%20geometric%20features%20with%20both%0Asemantic%20and%20positional%20cues%2C%20constructing%20a%20unified%20image%20patch-based%203D%0Arepresentation%20that%20encapsulates%20all%20essential%20information%20without%20increasing%0Athe%20number%20of%20input%20tokens.%20Leveraging%20this%20holistic%20representation%2C%0AGS-Reasoner%20is%20the%20first%203D%20LLM%20that%20achieves%20autoregressive%20grounding%20entirely%0Awithout%20external%20modules%20while%20delivering%20performance%20comparable%20to%0Astate-of-the-art%20models%2C%20establishing%20a%20unified%20and%20self-contained%20framework%0Afor%203D%20spatial%20reasoning.%20To%20further%20bridge%20grounding%20and%20spatial%20reasoning%2C%20we%0Aintroduce%20the%20Grounded%20Chain-of-Thought%20%28GCoT%29%20dataset.%20This%20dataset%20is%0Ameticulously%20curated%20to%20include%20both%203D%20bounding%20box%20annotations%20for%20objects%0Areferenced%20in%20reasoning%20questions%20and%20step-by-step%20reasoning%20paths%20that%0Aintegrate%20grounding%20as%20a%20core%20component%20of%20the%20problem-solving%20process.%0AExtensive%20experiments%20demonstrate%20that%20GS-Reasoner%20achieves%20impressive%20results%0Aon%203D%20visual%20grounding%2C%20which%20in%20turn%20significantly%20enhances%20its%20spatial%0Areasoning%20capabilities%2C%20leading%20to%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520in%2520Space%2520via%2520Grounding%2520in%2520the%2520World%26entry.906535625%3DYiming%2520Chen%2520and%2520Zekun%2520Qi%2520and%2520Wenyao%2520Zhang%2520and%2520Xin%2520Jin%2520and%2520Li%2520Zhang%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520claim%2520that%25203D%2520visual%2520grounding%2520is%2520the%2520cornerstone%2520of%250Aspatial%2520reasoning%2520and%2520introduce%2520the%2520Grounded-Spatial%2520Reasoner%2520%2528GS-Reasoner%2529%2520to%250Aexplore%2520the%2520effective%2520spatial%2520representations%2520that%2520bridge%2520the%2520gap%2520between%2520them.%250AExisting%25203D%2520LLMs%2520suffer%2520from%2520the%2520absence%2520of%2520a%2520unified%25203D%2520representation%2520capable%250Aof%2520jointly%2520capturing%2520semantic%2520and%2520geometric%2520information.%2520This%2520deficiency%2520is%250Amanifested%2520either%2520in%2520poor%2520performance%2520on%2520grounding%2520or%2520in%2520an%2520excessive%2520reliance%250Aon%2520external%2520modules%252C%2520ultimately%2520hindering%2520the%2520seamless%2520integration%2520of%2520grounding%250Aand%2520spatial%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Adual-path%2520pooling%2520mechanism%2520that%2520tightly%2520aligns%2520geometric%2520features%2520with%2520both%250Asemantic%2520and%2520positional%2520cues%252C%2520constructing%2520a%2520unified%2520image%2520patch-based%25203D%250Arepresentation%2520that%2520encapsulates%2520all%2520essential%2520information%2520without%2520increasing%250Athe%2520number%2520of%2520input%2520tokens.%2520Leveraging%2520this%2520holistic%2520representation%252C%250AGS-Reasoner%2520is%2520the%2520first%25203D%2520LLM%2520that%2520achieves%2520autoregressive%2520grounding%2520entirely%250Awithout%2520external%2520modules%2520while%2520delivering%2520performance%2520comparable%2520to%250Astate-of-the-art%2520models%252C%2520establishing%2520a%2520unified%2520and%2520self-contained%2520framework%250Afor%25203D%2520spatial%2520reasoning.%2520To%2520further%2520bridge%2520grounding%2520and%2520spatial%2520reasoning%252C%2520we%250Aintroduce%2520the%2520Grounded%2520Chain-of-Thought%2520%2528GCoT%2529%2520dataset.%2520This%2520dataset%2520is%250Ameticulously%2520curated%2520to%2520include%2520both%25203D%2520bounding%2520box%2520annotations%2520for%2520objects%250Areferenced%2520in%2520reasoning%2520questions%2520and%2520step-by-step%2520reasoning%2520paths%2520that%250Aintegrate%2520grounding%2520as%2520a%2520core%2520component%2520of%2520the%2520problem-solving%2520process.%250AExtensive%2520experiments%2520demonstrate%2520that%2520GS-Reasoner%2520achieves%2520impressive%2520results%250Aon%25203D%2520visual%2520grounding%252C%2520which%2520in%2520turn%2520significantly%2520enhances%2520its%2520spatial%250Areasoning%2520capabilities%252C%2520leading%2520to%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20in%20Space%20via%20Grounding%20in%20the%20World&entry.906535625=Yiming%20Chen%20and%20Zekun%20Qi%20and%20Wenyao%20Zhang%20and%20Xin%20Jin%20and%20Li%20Zhang%20and%20Peidong%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20claim%20that%203D%20visual%20grounding%20is%20the%20cornerstone%20of%0Aspatial%20reasoning%20and%20introduce%20the%20Grounded-Spatial%20Reasoner%20%28GS-Reasoner%29%20to%0Aexplore%20the%20effective%20spatial%20representations%20that%20bridge%20the%20gap%20between%20them.%0AExisting%203D%20LLMs%20suffer%20from%20the%20absence%20of%20a%20unified%203D%20representation%20capable%0Aof%20jointly%20capturing%20semantic%20and%20geometric%20information.%20This%20deficiency%20is%0Amanifested%20either%20in%20poor%20performance%20on%20grounding%20or%20in%20an%20excessive%20reliance%0Aon%20external%20modules%2C%20ultimately%20hindering%20the%20seamless%20integration%20of%20grounding%0Aand%20spatial%20reasoning.%20To%20address%20this%2C%20we%20propose%20a%20simple%20yet%20effective%0Adual-path%20pooling%20mechanism%20that%20tightly%20aligns%20geometric%20features%20with%20both%0Asemantic%20and%20positional%20cues%2C%20constructing%20a%20unified%20image%20patch-based%203D%0Arepresentation%20that%20encapsulates%20all%20essential%20information%20without%20increasing%0Athe%20number%20of%20input%20tokens.%20Leveraging%20this%20holistic%20representation%2C%0AGS-Reasoner%20is%20the%20first%203D%20LLM%20that%20achieves%20autoregressive%20grounding%20entirely%0Awithout%20external%20modules%20while%20delivering%20performance%20comparable%20to%0Astate-of-the-art%20models%2C%20establishing%20a%20unified%20and%20self-contained%20framework%0Afor%203D%20spatial%20reasoning.%20To%20further%20bridge%20grounding%20and%20spatial%20reasoning%2C%20we%0Aintroduce%20the%20Grounded%20Chain-of-Thought%20%28GCoT%29%20dataset.%20This%20dataset%20is%0Ameticulously%20curated%20to%20include%20both%203D%20bounding%20box%20annotations%20for%20objects%0Areferenced%20in%20reasoning%20questions%20and%20step-by-step%20reasoning%20paths%20that%0Aintegrate%20grounding%20as%20a%20core%20component%20of%20the%20problem-solving%20process.%0AExtensive%20experiments%20demonstrate%20that%20GS-Reasoner%20achieves%20impressive%20results%0Aon%203D%20visual%20grounding%2C%20which%20in%20turn%20significantly%20enhances%20its%20spatial%0Areasoning%20capabilities%2C%20leading%20to%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13800v2&entry.124074799=Read"},
{"title": "Acquisition of interpretable domain information during brain MR image\n  harmonization for content-based image retrieval", "author": "Keima Abe and Hayato Muraki and Shuhei Tomoshige and Kenichi Oishi and Hitoshi Iyatomi", "abstract": "  Medical images like MR scans often show domain shifts across imaging sites\ndue to scanner and protocol differences, which degrade machine learning\nperformance in tasks such as disease classification. Domain harmonization is\nthus a critical research focus. Recent approaches encode brain images\n$\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then\ndisentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and\n$\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these\nmethods often lack interpretability$-$an essential requirement in medical\napplications$-$leaving practical issues unresolved. We propose\nPseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a\ngeneral framework for domain harmonization and interpretable representation\nlearning that preserves disease-relevant information in brain MR images.\nPL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract\n$\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image\n$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the\nencoder and domain predictor, the model learns to reconstruct the input image\n$\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and\n$\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared\nto prior methods, PL-SE-ADA achieves equal or better performance in image\nreconstruction, disease classification, and domain recognition. It also enables\nvisualization of both domain-independent brain features and domain-specific\ncomponents, offering high interpretability across the entire framework.\n", "link": "http://arxiv.org/abs/2510.14535v1", "date": "2025-10-16", "relevancy": 2.7661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Acquisition%20of%20interpretable%20domain%20information%20during%20brain%20MR%20image%0A%20%20harmonization%20for%20content-based%20image%20retrieval&body=Title%3A%20Acquisition%20of%20interpretable%20domain%20information%20during%20brain%20MR%20image%0A%20%20harmonization%20for%20content-based%20image%20retrieval%0AAuthor%3A%20Keima%20Abe%20and%20Hayato%20Muraki%20and%20Shuhei%20Tomoshige%20and%20Kenichi%20Oishi%20and%20Hitoshi%20Iyatomi%0AAbstract%3A%20%20%20Medical%20images%20like%20MR%20scans%20often%20show%20domain%20shifts%20across%20imaging%20sites%0Adue%20to%20scanner%20and%20protocol%20differences%2C%20which%20degrade%20machine%20learning%0Aperformance%20in%20tasks%20such%20as%20disease%20classification.%20Domain%20harmonization%20is%0Athus%20a%20critical%20research%20focus.%20Recent%20approaches%20encode%20brain%20images%0A%24%5Cboldsymbol%7Bx%7D%24%20into%20a%20low-dimensional%20latent%20space%20%24%5Cboldsymbol%7Bz%7D%24%2C%20then%0Adisentangle%20it%20into%20%24%5Cboldsymbol%7Bz_u%7D%24%20%28domain-invariant%29%20and%0A%24%5Cboldsymbol%7Bz_d%7D%24%20%28domain-specific%29%2C%20achieving%20strong%20results.%20However%2C%20these%0Amethods%20often%20lack%20interpretability%24-%24an%20essential%20requirement%20in%20medical%0Aapplications%24-%24leaving%20practical%20issues%20unresolved.%20We%20propose%0APseudo-Linear-Style%20Encoder%20Adversarial%20Domain%20Adaptation%20%28PL-SE-ADA%29%2C%20a%0Ageneral%20framework%20for%20domain%20harmonization%20and%20interpretable%20representation%0Alearning%20that%20preserves%20disease-relevant%20information%20in%20brain%20MR%20images.%0APL-SE-ADA%20includes%20two%20encoders%20%24f_E%24%20and%20%24f_%7BSE%7D%24%20to%20extract%0A%24%5Cboldsymbol%7Bz_u%7D%24%20and%20%24%5Cboldsymbol%7Bz_d%7D%24%2C%20a%20decoder%20to%20reconstruct%20the%20image%0A%24f_D%24%2C%20and%20a%20domain%20predictor%20%24g_D%24.%20Beyond%20adversarial%20training%20between%20the%0Aencoder%20and%20domain%20predictor%2C%20the%20model%20learns%20to%20reconstruct%20the%20input%20image%0A%24%5Cboldsymbol%7Bx%7D%24%20by%20summing%20reconstructions%20from%20%24%5Cboldsymbol%7Bz_u%7D%24%20and%0A%24%5Cboldsymbol%7Bz_d%7D%24%2C%20ensuring%20both%20harmonization%20and%20informativeness.%20Compared%0Ato%20prior%20methods%2C%20PL-SE-ADA%20achieves%20equal%20or%20better%20performance%20in%20image%0Areconstruction%2C%20disease%20classification%2C%20and%20domain%20recognition.%20It%20also%20enables%0Avisualization%20of%20both%20domain-independent%20brain%20features%20and%20domain-specific%0Acomponents%2C%20offering%20high%20interpretability%20across%20the%20entire%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAcquisition%2520of%2520interpretable%2520domain%2520information%2520during%2520brain%2520MR%2520image%250A%2520%2520harmonization%2520for%2520content-based%2520image%2520retrieval%26entry.906535625%3DKeima%2520Abe%2520and%2520Hayato%2520Muraki%2520and%2520Shuhei%2520Tomoshige%2520and%2520Kenichi%2520Oishi%2520and%2520Hitoshi%2520Iyatomi%26entry.1292438233%3D%2520%2520Medical%2520images%2520like%2520MR%2520scans%2520often%2520show%2520domain%2520shifts%2520across%2520imaging%2520sites%250Adue%2520to%2520scanner%2520and%2520protocol%2520differences%252C%2520which%2520degrade%2520machine%2520learning%250Aperformance%2520in%2520tasks%2520such%2520as%2520disease%2520classification.%2520Domain%2520harmonization%2520is%250Athus%2520a%2520critical%2520research%2520focus.%2520Recent%2520approaches%2520encode%2520brain%2520images%250A%2524%255Cboldsymbol%257Bx%257D%2524%2520into%2520a%2520low-dimensional%2520latent%2520space%2520%2524%255Cboldsymbol%257Bz%257D%2524%252C%2520then%250Adisentangle%2520it%2520into%2520%2524%255Cboldsymbol%257Bz_u%257D%2524%2520%2528domain-invariant%2529%2520and%250A%2524%255Cboldsymbol%257Bz_d%257D%2524%2520%2528domain-specific%2529%252C%2520achieving%2520strong%2520results.%2520However%252C%2520these%250Amethods%2520often%2520lack%2520interpretability%2524-%2524an%2520essential%2520requirement%2520in%2520medical%250Aapplications%2524-%2524leaving%2520practical%2520issues%2520unresolved.%2520We%2520propose%250APseudo-Linear-Style%2520Encoder%2520Adversarial%2520Domain%2520Adaptation%2520%2528PL-SE-ADA%2529%252C%2520a%250Ageneral%2520framework%2520for%2520domain%2520harmonization%2520and%2520interpretable%2520representation%250Alearning%2520that%2520preserves%2520disease-relevant%2520information%2520in%2520brain%2520MR%2520images.%250APL-SE-ADA%2520includes%2520two%2520encoders%2520%2524f_E%2524%2520and%2520%2524f_%257BSE%257D%2524%2520to%2520extract%250A%2524%255Cboldsymbol%257Bz_u%257D%2524%2520and%2520%2524%255Cboldsymbol%257Bz_d%257D%2524%252C%2520a%2520decoder%2520to%2520reconstruct%2520the%2520image%250A%2524f_D%2524%252C%2520and%2520a%2520domain%2520predictor%2520%2524g_D%2524.%2520Beyond%2520adversarial%2520training%2520between%2520the%250Aencoder%2520and%2520domain%2520predictor%252C%2520the%2520model%2520learns%2520to%2520reconstruct%2520the%2520input%2520image%250A%2524%255Cboldsymbol%257Bx%257D%2524%2520by%2520summing%2520reconstructions%2520from%2520%2524%255Cboldsymbol%257Bz_u%257D%2524%2520and%250A%2524%255Cboldsymbol%257Bz_d%257D%2524%252C%2520ensuring%2520both%2520harmonization%2520and%2520informativeness.%2520Compared%250Ato%2520prior%2520methods%252C%2520PL-SE-ADA%2520achieves%2520equal%2520or%2520better%2520performance%2520in%2520image%250Areconstruction%252C%2520disease%2520classification%252C%2520and%2520domain%2520recognition.%2520It%2520also%2520enables%250Avisualization%2520of%2520both%2520domain-independent%2520brain%2520features%2520and%2520domain-specific%250Acomponents%252C%2520offering%2520high%2520interpretability%2520across%2520the%2520entire%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Acquisition%20of%20interpretable%20domain%20information%20during%20brain%20MR%20image%0A%20%20harmonization%20for%20content-based%20image%20retrieval&entry.906535625=Keima%20Abe%20and%20Hayato%20Muraki%20and%20Shuhei%20Tomoshige%20and%20Kenichi%20Oishi%20and%20Hitoshi%20Iyatomi&entry.1292438233=%20%20Medical%20images%20like%20MR%20scans%20often%20show%20domain%20shifts%20across%20imaging%20sites%0Adue%20to%20scanner%20and%20protocol%20differences%2C%20which%20degrade%20machine%20learning%0Aperformance%20in%20tasks%20such%20as%20disease%20classification.%20Domain%20harmonization%20is%0Athus%20a%20critical%20research%20focus.%20Recent%20approaches%20encode%20brain%20images%0A%24%5Cboldsymbol%7Bx%7D%24%20into%20a%20low-dimensional%20latent%20space%20%24%5Cboldsymbol%7Bz%7D%24%2C%20then%0Adisentangle%20it%20into%20%24%5Cboldsymbol%7Bz_u%7D%24%20%28domain-invariant%29%20and%0A%24%5Cboldsymbol%7Bz_d%7D%24%20%28domain-specific%29%2C%20achieving%20strong%20results.%20However%2C%20these%0Amethods%20often%20lack%20interpretability%24-%24an%20essential%20requirement%20in%20medical%0Aapplications%24-%24leaving%20practical%20issues%20unresolved.%20We%20propose%0APseudo-Linear-Style%20Encoder%20Adversarial%20Domain%20Adaptation%20%28PL-SE-ADA%29%2C%20a%0Ageneral%20framework%20for%20domain%20harmonization%20and%20interpretable%20representation%0Alearning%20that%20preserves%20disease-relevant%20information%20in%20brain%20MR%20images.%0APL-SE-ADA%20includes%20two%20encoders%20%24f_E%24%20and%20%24f_%7BSE%7D%24%20to%20extract%0A%24%5Cboldsymbol%7Bz_u%7D%24%20and%20%24%5Cboldsymbol%7Bz_d%7D%24%2C%20a%20decoder%20to%20reconstruct%20the%20image%0A%24f_D%24%2C%20and%20a%20domain%20predictor%20%24g_D%24.%20Beyond%20adversarial%20training%20between%20the%0Aencoder%20and%20domain%20predictor%2C%20the%20model%20learns%20to%20reconstruct%20the%20input%20image%0A%24%5Cboldsymbol%7Bx%7D%24%20by%20summing%20reconstructions%20from%20%24%5Cboldsymbol%7Bz_u%7D%24%20and%0A%24%5Cboldsymbol%7Bz_d%7D%24%2C%20ensuring%20both%20harmonization%20and%20informativeness.%20Compared%0Ato%20prior%20methods%2C%20PL-SE-ADA%20achieves%20equal%20or%20better%20performance%20in%20image%0Areconstruction%2C%20disease%20classification%2C%20and%20domain%20recognition.%20It%20also%20enables%0Avisualization%20of%20both%20domain-independent%20brain%20features%20and%20domain-specific%0Acomponents%2C%20offering%20high%20interpretability%20across%20the%20entire%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14535v1&entry.124074799=Read"},
{"title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video", "author": "Yulin Zhang and Cheng Shi and Yang Wang and Sibei Yang", "abstract": "  Envision an AI capable of functioning in human-like settings, moving beyond\nmere observation to actively understand, anticipate, and proactively respond to\nunfolding events. Towards this vision, we focus on the innovative task where,\ngiven ego-streaming video input, an assistant proactively answers diverse,\nevolving questions at the opportune moment, while maintaining synchronized\nperception and reasoning. This task embodies three key properties: (1)\nProactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized\nEfficiency. To evaluate and address these properties, we first introduce\nESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a\nnovel framework designed for their rigorous assessment. Secondly, we propose a\ncomprehensive technical pipeline to enable models to tackle this challenging\ntask. This pipeline comprises: (1) a data engine, (2) a multi-stage training\nstrategy, and (3) a proactive dynamic compression technique. Our proposed model\neffectively addresses these critical properties while outperforming multiple\nbaselines across diverse online and offline benchmarks. Project\nPage:https://zhangyl4.github.io/publications/eyes-wide-open/\n", "link": "http://arxiv.org/abs/2510.14560v1", "date": "2025-10-16", "relevancy": 2.7482, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyes%20Wide%20Open%3A%20Ego%20Proactive%20Video-LLM%20for%20Streaming%20Video&body=Title%3A%20Eyes%20Wide%20Open%3A%20Ego%20Proactive%20Video-LLM%20for%20Streaming%20Video%0AAuthor%3A%20Yulin%20Zhang%20and%20Cheng%20Shi%20and%20Yang%20Wang%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20Envision%20an%20AI%20capable%20of%20functioning%20in%20human-like%20settings%2C%20moving%20beyond%0Amere%20observation%20to%20actively%20understand%2C%20anticipate%2C%20and%20proactively%20respond%20to%0Aunfolding%20events.%20Towards%20this%20vision%2C%20we%20focus%20on%20the%20innovative%20task%20where%2C%0Agiven%20ego-streaming%20video%20input%2C%20an%20assistant%20proactively%20answers%20diverse%2C%0Aevolving%20questions%20at%20the%20opportune%20moment%2C%20while%20maintaining%20synchronized%0Aperception%20and%20reasoning.%20This%20task%20embodies%20three%20key%20properties%3A%20%281%29%0AProactive%20Coherence%2C%20%282%29%20Just-in-Time%20Responsiveness%2C%20and%20%283%29%20Synchronized%0AEfficiency.%20To%20evaluate%20and%20address%20these%20properties%2C%20we%20first%20introduce%0AESTP-Bench%20%28Ego%20Streaming%20Proactive%20Benchmark%29%20alongside%20the%20ESTP-F1%20metric-a%0Anovel%20framework%20designed%20for%20their%20rigorous%20assessment.%20Secondly%2C%20we%20propose%20a%0Acomprehensive%20technical%20pipeline%20to%20enable%20models%20to%20tackle%20this%20challenging%0Atask.%20This%20pipeline%20comprises%3A%20%281%29%20a%20data%20engine%2C%20%282%29%20a%20multi-stage%20training%0Astrategy%2C%20and%20%283%29%20a%20proactive%20dynamic%20compression%20technique.%20Our%20proposed%20model%0Aeffectively%20addresses%20these%20critical%20properties%20while%20outperforming%20multiple%0Abaselines%20across%20diverse%20online%20and%20offline%20benchmarks.%20Project%0APage%3Ahttps%3A//zhangyl4.github.io/publications/eyes-wide-open/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyes%2520Wide%2520Open%253A%2520Ego%2520Proactive%2520Video-LLM%2520for%2520Streaming%2520Video%26entry.906535625%3DYulin%2520Zhang%2520and%2520Cheng%2520Shi%2520and%2520Yang%2520Wang%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520Envision%2520an%2520AI%2520capable%2520of%2520functioning%2520in%2520human-like%2520settings%252C%2520moving%2520beyond%250Amere%2520observation%2520to%2520actively%2520understand%252C%2520anticipate%252C%2520and%2520proactively%2520respond%2520to%250Aunfolding%2520events.%2520Towards%2520this%2520vision%252C%2520we%2520focus%2520on%2520the%2520innovative%2520task%2520where%252C%250Agiven%2520ego-streaming%2520video%2520input%252C%2520an%2520assistant%2520proactively%2520answers%2520diverse%252C%250Aevolving%2520questions%2520at%2520the%2520opportune%2520moment%252C%2520while%2520maintaining%2520synchronized%250Aperception%2520and%2520reasoning.%2520This%2520task%2520embodies%2520three%2520key%2520properties%253A%2520%25281%2529%250AProactive%2520Coherence%252C%2520%25282%2529%2520Just-in-Time%2520Responsiveness%252C%2520and%2520%25283%2529%2520Synchronized%250AEfficiency.%2520To%2520evaluate%2520and%2520address%2520these%2520properties%252C%2520we%2520first%2520introduce%250AESTP-Bench%2520%2528Ego%2520Streaming%2520Proactive%2520Benchmark%2529%2520alongside%2520the%2520ESTP-F1%2520metric-a%250Anovel%2520framework%2520designed%2520for%2520their%2520rigorous%2520assessment.%2520Secondly%252C%2520we%2520propose%2520a%250Acomprehensive%2520technical%2520pipeline%2520to%2520enable%2520models%2520to%2520tackle%2520this%2520challenging%250Atask.%2520This%2520pipeline%2520comprises%253A%2520%25281%2529%2520a%2520data%2520engine%252C%2520%25282%2529%2520a%2520multi-stage%2520training%250Astrategy%252C%2520and%2520%25283%2529%2520a%2520proactive%2520dynamic%2520compression%2520technique.%2520Our%2520proposed%2520model%250Aeffectively%2520addresses%2520these%2520critical%2520properties%2520while%2520outperforming%2520multiple%250Abaselines%2520across%2520diverse%2520online%2520and%2520offline%2520benchmarks.%2520Project%250APage%253Ahttps%253A//zhangyl4.github.io/publications/eyes-wide-open/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyes%20Wide%20Open%3A%20Ego%20Proactive%20Video-LLM%20for%20Streaming%20Video&entry.906535625=Yulin%20Zhang%20and%20Cheng%20Shi%20and%20Yang%20Wang%20and%20Sibei%20Yang&entry.1292438233=%20%20Envision%20an%20AI%20capable%20of%20functioning%20in%20human-like%20settings%2C%20moving%20beyond%0Amere%20observation%20to%20actively%20understand%2C%20anticipate%2C%20and%20proactively%20respond%20to%0Aunfolding%20events.%20Towards%20this%20vision%2C%20we%20focus%20on%20the%20innovative%20task%20where%2C%0Agiven%20ego-streaming%20video%20input%2C%20an%20assistant%20proactively%20answers%20diverse%2C%0Aevolving%20questions%20at%20the%20opportune%20moment%2C%20while%20maintaining%20synchronized%0Aperception%20and%20reasoning.%20This%20task%20embodies%20three%20key%20properties%3A%20%281%29%0AProactive%20Coherence%2C%20%282%29%20Just-in-Time%20Responsiveness%2C%20and%20%283%29%20Synchronized%0AEfficiency.%20To%20evaluate%20and%20address%20these%20properties%2C%20we%20first%20introduce%0AESTP-Bench%20%28Ego%20Streaming%20Proactive%20Benchmark%29%20alongside%20the%20ESTP-F1%20metric-a%0Anovel%20framework%20designed%20for%20their%20rigorous%20assessment.%20Secondly%2C%20we%20propose%20a%0Acomprehensive%20technical%20pipeline%20to%20enable%20models%20to%20tackle%20this%20challenging%0Atask.%20This%20pipeline%20comprises%3A%20%281%29%20a%20data%20engine%2C%20%282%29%20a%20multi-stage%20training%0Astrategy%2C%20and%20%283%29%20a%20proactive%20dynamic%20compression%20technique.%20Our%20proposed%20model%0Aeffectively%20addresses%20these%20critical%20properties%20while%20outperforming%20multiple%0Abaselines%20across%20diverse%20online%20and%20offline%20benchmarks.%20Project%0APage%3Ahttps%3A//zhangyl4.github.io/publications/eyes-wide-open/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14560v1&entry.124074799=Read"},
{"title": "Beyond Hallucinations: The Illusion of Understanding in Large Language\n  Models", "author": "Rikard Rosenbacke and Carl Rosenbacke and Victor Rosenbacke and Martin McKee", "abstract": "  Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding.\n", "link": "http://arxiv.org/abs/2510.14665v1", "date": "2025-10-16", "relevancy": 2.732, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Hallucinations%3A%20The%20Illusion%20of%20Understanding%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20Beyond%20Hallucinations%3A%20The%20Illusion%20of%20Understanding%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Rikard%20Rosenbacke%20and%20Carl%20Rosenbacke%20and%20Victor%20Rosenbacke%20and%20Martin%20McKee%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20becoming%20deeply%20embedded%20in%20human%0Acommunication%20and%20decision-making%2C%20yet%20they%20inherit%20the%20ambiguity%2C%20bias%2C%20and%0Alack%20of%20direct%20access%20to%20truth%20inherent%20in%20language%20itself.%20While%20their%20outputs%0Aare%20fluent%2C%20emotionally%20resonant%2C%20and%20coherent%2C%20they%20are%20generated%20through%0Astatistical%20prediction%20rather%20than%20grounded%20reasoning.%20This%20creates%20the%20risk%20of%0Ahallucination%2C%20responses%20that%20sound%20convincing%20but%20lack%20factual%20validity.%0ABuilding%20on%20Geoffrey%20Hinton%27s%20observation%20that%20AI%20mirrors%20human%20intuition%0Arather%20than%20reasoning%2C%20this%20paper%20argues%20that%20LLMs%20operationalize%20System%201%0Acognition%20at%20scale%3A%20fast%2C%20associative%2C%20and%20persuasive%2C%20but%20without%20reflection%0Aor%20falsification.%20To%20address%20this%2C%20we%20introduce%20the%20Rose-Frame%2C%20a%0Athree-dimensional%20framework%20for%20diagnosing%20cognitive%20and%20epistemic%20drift%20in%0Ahuman-AI%20interaction.%20The%20three%20axes%20are%3A%20%28i%29%20Map%20vs.%20Territory%2C%20which%0Adistinguishes%20representations%20of%20reality%20%28epistemology%29%20from%20reality%20itself%0A%28ontology%29%3B%20%28ii%29%20Intuition%20vs.%20Reason%2C%20drawing%20on%20dual-process%20theory%20to%0Aseparate%20fast%2C%20emotional%20judgments%20from%20slow%2C%20reflective%20thinking%3B%20and%20%28iii%29%0AConflict%20vs.%20Confirmation%2C%20which%20examines%20whether%20ideas%20are%20critically%20tested%0Athrough%20disagreement%20or%20simply%20reinforced%20through%20mutual%20validation.%20Each%0Adimension%20captures%20a%20distinct%20failure%20mode%2C%20and%20their%20combination%20amplifies%0Amisalignment.%20Rose-Frame%20does%20not%20attempt%20to%20fix%20LLMs%20with%20more%20data%20or%20rules.%0AInstead%2C%20it%20offers%20a%20reflective%20tool%20that%20makes%20both%20the%20model%27s%20limitations%0Aand%20the%20user%27s%20assumptions%20visible%2C%20enabling%20more%20transparent%20and%20critically%0Aaware%20AI%20deployment.%20It%20reframes%20alignment%20as%20cognitive%20governance%3A%20intuition%2C%0Awhether%20human%20or%20artificial%2C%20must%20remain%20governed%20by%20human%20reason.%20Only%20by%0Aembedding%20reflective%2C%20falsifiable%20oversight%20can%20we%20align%20machine%20fluency%20with%0Ahuman%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Hallucinations%253A%2520The%2520Illusion%2520of%2520Understanding%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DRikard%2520Rosenbacke%2520and%2520Carl%2520Rosenbacke%2520and%2520Victor%2520Rosenbacke%2520and%2520Martin%2520McKee%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520becoming%2520deeply%2520embedded%2520in%2520human%250Acommunication%2520and%2520decision-making%252C%2520yet%2520they%2520inherit%2520the%2520ambiguity%252C%2520bias%252C%2520and%250Alack%2520of%2520direct%2520access%2520to%2520truth%2520inherent%2520in%2520language%2520itself.%2520While%2520their%2520outputs%250Aare%2520fluent%252C%2520emotionally%2520resonant%252C%2520and%2520coherent%252C%2520they%2520are%2520generated%2520through%250Astatistical%2520prediction%2520rather%2520than%2520grounded%2520reasoning.%2520This%2520creates%2520the%2520risk%2520of%250Ahallucination%252C%2520responses%2520that%2520sound%2520convincing%2520but%2520lack%2520factual%2520validity.%250ABuilding%2520on%2520Geoffrey%2520Hinton%2527s%2520observation%2520that%2520AI%2520mirrors%2520human%2520intuition%250Arather%2520than%2520reasoning%252C%2520this%2520paper%2520argues%2520that%2520LLMs%2520operationalize%2520System%25201%250Acognition%2520at%2520scale%253A%2520fast%252C%2520associative%252C%2520and%2520persuasive%252C%2520but%2520without%2520reflection%250Aor%2520falsification.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Rose-Frame%252C%2520a%250Athree-dimensional%2520framework%2520for%2520diagnosing%2520cognitive%2520and%2520epistemic%2520drift%2520in%250Ahuman-AI%2520interaction.%2520The%2520three%2520axes%2520are%253A%2520%2528i%2529%2520Map%2520vs.%2520Territory%252C%2520which%250Adistinguishes%2520representations%2520of%2520reality%2520%2528epistemology%2529%2520from%2520reality%2520itself%250A%2528ontology%2529%253B%2520%2528ii%2529%2520Intuition%2520vs.%2520Reason%252C%2520drawing%2520on%2520dual-process%2520theory%2520to%250Aseparate%2520fast%252C%2520emotional%2520judgments%2520from%2520slow%252C%2520reflective%2520thinking%253B%2520and%2520%2528iii%2529%250AConflict%2520vs.%2520Confirmation%252C%2520which%2520examines%2520whether%2520ideas%2520are%2520critically%2520tested%250Athrough%2520disagreement%2520or%2520simply%2520reinforced%2520through%2520mutual%2520validation.%2520Each%250Adimension%2520captures%2520a%2520distinct%2520failure%2520mode%252C%2520and%2520their%2520combination%2520amplifies%250Amisalignment.%2520Rose-Frame%2520does%2520not%2520attempt%2520to%2520fix%2520LLMs%2520with%2520more%2520data%2520or%2520rules.%250AInstead%252C%2520it%2520offers%2520a%2520reflective%2520tool%2520that%2520makes%2520both%2520the%2520model%2527s%2520limitations%250Aand%2520the%2520user%2527s%2520assumptions%2520visible%252C%2520enabling%2520more%2520transparent%2520and%2520critically%250Aaware%2520AI%2520deployment.%2520It%2520reframes%2520alignment%2520as%2520cognitive%2520governance%253A%2520intuition%252C%250Awhether%2520human%2520or%2520artificial%252C%2520must%2520remain%2520governed%2520by%2520human%2520reason.%2520Only%2520by%250Aembedding%2520reflective%252C%2520falsifiable%2520oversight%2520can%2520we%2520align%2520machine%2520fluency%2520with%250Ahuman%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Hallucinations%3A%20The%20Illusion%20of%20Understanding%20in%20Large%20Language%0A%20%20Models&entry.906535625=Rikard%20Rosenbacke%20and%20Carl%20Rosenbacke%20and%20Victor%20Rosenbacke%20and%20Martin%20McKee&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20becoming%20deeply%20embedded%20in%20human%0Acommunication%20and%20decision-making%2C%20yet%20they%20inherit%20the%20ambiguity%2C%20bias%2C%20and%0Alack%20of%20direct%20access%20to%20truth%20inherent%20in%20language%20itself.%20While%20their%20outputs%0Aare%20fluent%2C%20emotionally%20resonant%2C%20and%20coherent%2C%20they%20are%20generated%20through%0Astatistical%20prediction%20rather%20than%20grounded%20reasoning.%20This%20creates%20the%20risk%20of%0Ahallucination%2C%20responses%20that%20sound%20convincing%20but%20lack%20factual%20validity.%0ABuilding%20on%20Geoffrey%20Hinton%27s%20observation%20that%20AI%20mirrors%20human%20intuition%0Arather%20than%20reasoning%2C%20this%20paper%20argues%20that%20LLMs%20operationalize%20System%201%0Acognition%20at%20scale%3A%20fast%2C%20associative%2C%20and%20persuasive%2C%20but%20without%20reflection%0Aor%20falsification.%20To%20address%20this%2C%20we%20introduce%20the%20Rose-Frame%2C%20a%0Athree-dimensional%20framework%20for%20diagnosing%20cognitive%20and%20epistemic%20drift%20in%0Ahuman-AI%20interaction.%20The%20three%20axes%20are%3A%20%28i%29%20Map%20vs.%20Territory%2C%20which%0Adistinguishes%20representations%20of%20reality%20%28epistemology%29%20from%20reality%20itself%0A%28ontology%29%3B%20%28ii%29%20Intuition%20vs.%20Reason%2C%20drawing%20on%20dual-process%20theory%20to%0Aseparate%20fast%2C%20emotional%20judgments%20from%20slow%2C%20reflective%20thinking%3B%20and%20%28iii%29%0AConflict%20vs.%20Confirmation%2C%20which%20examines%20whether%20ideas%20are%20critically%20tested%0Athrough%20disagreement%20or%20simply%20reinforced%20through%20mutual%20validation.%20Each%0Adimension%20captures%20a%20distinct%20failure%20mode%2C%20and%20their%20combination%20amplifies%0Amisalignment.%20Rose-Frame%20does%20not%20attempt%20to%20fix%20LLMs%20with%20more%20data%20or%20rules.%0AInstead%2C%20it%20offers%20a%20reflective%20tool%20that%20makes%20both%20the%20model%27s%20limitations%0Aand%20the%20user%27s%20assumptions%20visible%2C%20enabling%20more%20transparent%20and%20critically%0Aaware%20AI%20deployment.%20It%20reframes%20alignment%20as%20cognitive%20governance%3A%20intuition%2C%0Awhether%20human%20or%20artificial%2C%20must%20remain%20governed%20by%20human%20reason.%20Only%20by%0Aembedding%20reflective%2C%20falsifiable%20oversight%20can%20we%20align%20machine%20fluency%20with%0Ahuman%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14665v1&entry.124074799=Read"},
{"title": "Comparing Human and Language Models Sentence Processing Difficulties on\n  Complex Structures", "author": "Samuel Joseph Amouyal and Aya Meltzer-Asscher and Jonathan Berant", "abstract": "  Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.\n", "link": "http://arxiv.org/abs/2510.07141v2", "date": "2025-10-16", "relevancy": 2.731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Human%20and%20Language%20Models%20Sentence%20Processing%20Difficulties%20on%0A%20%20Complex%20Structures&body=Title%3A%20Comparing%20Human%20and%20Language%20Models%20Sentence%20Processing%20Difficulties%20on%0A%20%20Complex%20Structures%0AAuthor%3A%20Samuel%20Joseph%20Amouyal%20and%20Aya%20Meltzer-Asscher%20and%20Jonathan%20Berant%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20that%20fluently%20converse%20with%20humans%20are%20a%20reality%0A-%20but%20do%20LLMs%20experience%20human-like%20processing%20difficulties%3F%20We%20systematically%0Acompare%20human%20and%20LLM%20sentence%20comprehension%20across%20seven%20challenging%0Alinguistic%20structures.%20We%20collect%20sentence%20comprehension%20data%20from%20humans%20and%0Afive%20families%20of%20state-of-the-art%20LLMs%2C%20varying%20in%20size%20and%20training%20procedure%0Ain%20a%20unified%20experimental%20framework.%20Our%20results%20show%20LLMs%20overall%20struggle%20on%0Athe%20target%20structures%2C%20but%20especially%20on%20garden%20path%20%28GP%29%20sentences.%20Indeed%2C%0Awhile%20the%20strongest%20models%20achieve%20near%20perfect%20accuracy%20on%20non-GP%20structures%0A%2893.7%25%20for%20GPT-5%29%2C%20they%20struggle%20on%20GP%20structures%20%2846.8%25%20for%20GPT-5%29.%0AAdditionally%2C%20when%20ranking%20structures%20based%20on%20average%20performance%2C%20rank%0Acorrelation%20between%20humans%20and%20models%20increases%20with%20parameter%20count.%20For%20each%0Atarget%20structure%2C%20we%20also%20collect%20data%20for%20their%20matched%20baseline%20without%20the%0Adifficult%20structure.%20Comparing%20performance%20on%20the%20target%20vs.%20baseline%0Asentences%2C%20the%20performance%20gap%20observed%20in%20humans%20holds%20for%20LLMs%2C%20with%20two%0Aexceptions%3A%20for%20models%20that%20are%20too%20weak%20performance%20is%20uniformly%20low%20across%0Aboth%20sentence%20types%2C%20and%20for%20models%20that%20are%20too%20strong%20the%20performance%20is%0Auniformly%20high.%20Together%2C%20these%20reveal%20convergence%20and%20divergence%20in%20human%20and%0ALLM%20sentence%20comprehension%2C%20offering%20new%20insights%20into%20the%20similarity%20of%20humans%0Aand%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Human%2520and%2520Language%2520Models%2520Sentence%2520Processing%2520Difficulties%2520on%250A%2520%2520Complex%2520Structures%26entry.906535625%3DSamuel%2520Joseph%2520Amouyal%2520and%2520Aya%2520Meltzer-Asscher%2520and%2520Jonathan%2520Berant%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520that%2520fluently%2520converse%2520with%2520humans%2520are%2520a%2520reality%250A-%2520but%2520do%2520LLMs%2520experience%2520human-like%2520processing%2520difficulties%253F%2520We%2520systematically%250Acompare%2520human%2520and%2520LLM%2520sentence%2520comprehension%2520across%2520seven%2520challenging%250Alinguistic%2520structures.%2520We%2520collect%2520sentence%2520comprehension%2520data%2520from%2520humans%2520and%250Afive%2520families%2520of%2520state-of-the-art%2520LLMs%252C%2520varying%2520in%2520size%2520and%2520training%2520procedure%250Ain%2520a%2520unified%2520experimental%2520framework.%2520Our%2520results%2520show%2520LLMs%2520overall%2520struggle%2520on%250Athe%2520target%2520structures%252C%2520but%2520especially%2520on%2520garden%2520path%2520%2528GP%2529%2520sentences.%2520Indeed%252C%250Awhile%2520the%2520strongest%2520models%2520achieve%2520near%2520perfect%2520accuracy%2520on%2520non-GP%2520structures%250A%252893.7%2525%2520for%2520GPT-5%2529%252C%2520they%2520struggle%2520on%2520GP%2520structures%2520%252846.8%2525%2520for%2520GPT-5%2529.%250AAdditionally%252C%2520when%2520ranking%2520structures%2520based%2520on%2520average%2520performance%252C%2520rank%250Acorrelation%2520between%2520humans%2520and%2520models%2520increases%2520with%2520parameter%2520count.%2520For%2520each%250Atarget%2520structure%252C%2520we%2520also%2520collect%2520data%2520for%2520their%2520matched%2520baseline%2520without%2520the%250Adifficult%2520structure.%2520Comparing%2520performance%2520on%2520the%2520target%2520vs.%2520baseline%250Asentences%252C%2520the%2520performance%2520gap%2520observed%2520in%2520humans%2520holds%2520for%2520LLMs%252C%2520with%2520two%250Aexceptions%253A%2520for%2520models%2520that%2520are%2520too%2520weak%2520performance%2520is%2520uniformly%2520low%2520across%250Aboth%2520sentence%2520types%252C%2520and%2520for%2520models%2520that%2520are%2520too%2520strong%2520the%2520performance%2520is%250Auniformly%2520high.%2520Together%252C%2520these%2520reveal%2520convergence%2520and%2520divergence%2520in%2520human%2520and%250ALLM%2520sentence%2520comprehension%252C%2520offering%2520new%2520insights%2520into%2520the%2520similarity%2520of%2520humans%250Aand%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Human%20and%20Language%20Models%20Sentence%20Processing%20Difficulties%20on%0A%20%20Complex%20Structures&entry.906535625=Samuel%20Joseph%20Amouyal%20and%20Aya%20Meltzer-Asscher%20and%20Jonathan%20Berant&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20that%20fluently%20converse%20with%20humans%20are%20a%20reality%0A-%20but%20do%20LLMs%20experience%20human-like%20processing%20difficulties%3F%20We%20systematically%0Acompare%20human%20and%20LLM%20sentence%20comprehension%20across%20seven%20challenging%0Alinguistic%20structures.%20We%20collect%20sentence%20comprehension%20data%20from%20humans%20and%0Afive%20families%20of%20state-of-the-art%20LLMs%2C%20varying%20in%20size%20and%20training%20procedure%0Ain%20a%20unified%20experimental%20framework.%20Our%20results%20show%20LLMs%20overall%20struggle%20on%0Athe%20target%20structures%2C%20but%20especially%20on%20garden%20path%20%28GP%29%20sentences.%20Indeed%2C%0Awhile%20the%20strongest%20models%20achieve%20near%20perfect%20accuracy%20on%20non-GP%20structures%0A%2893.7%25%20for%20GPT-5%29%2C%20they%20struggle%20on%20GP%20structures%20%2846.8%25%20for%20GPT-5%29.%0AAdditionally%2C%20when%20ranking%20structures%20based%20on%20average%20performance%2C%20rank%0Acorrelation%20between%20humans%20and%20models%20increases%20with%20parameter%20count.%20For%20each%0Atarget%20structure%2C%20we%20also%20collect%20data%20for%20their%20matched%20baseline%20without%20the%0Adifficult%20structure.%20Comparing%20performance%20on%20the%20target%20vs.%20baseline%0Asentences%2C%20the%20performance%20gap%20observed%20in%20humans%20holds%20for%20LLMs%2C%20with%20two%0Aexceptions%3A%20for%20models%20that%20are%20too%20weak%20performance%20is%20uniformly%20low%20across%0Aboth%20sentence%20types%2C%20and%20for%20models%20that%20are%20too%20strong%20the%20performance%20is%0Auniformly%20high.%20Together%2C%20these%20reveal%20convergence%20and%20divergence%20in%20human%20and%0ALLM%20sentence%20comprehension%2C%20offering%20new%20insights%20into%20the%20similarity%20of%20humans%0Aand%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07141v2&entry.124074799=Read"},
{"title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering", "author": "Yuyang Hong and Jiaqi Gu and Qi Yang and Lubin Fan and Yue Wu and Ying Wang and Kun Ding and Shiming Xiang and Jieping Ye", "abstract": "  Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF\n", "link": "http://arxiv.org/abs/2510.14605v1", "date": "2025-10-16", "relevancy": 2.7048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-based%20Visual%20Question%20Answer%20with%20Multimodal%20Processing%2C%0A%20%20Retrieval%20and%20Filtering&body=Title%3A%20Knowledge-based%20Visual%20Question%20Answer%20with%20Multimodal%20Processing%2C%0A%20%20Retrieval%20and%20Filtering%0AAuthor%3A%20Yuyang%20Hong%20and%20Jiaqi%20Gu%20and%20Qi%20Yang%20and%20Lubin%20Fan%20and%20Yue%20Wu%20and%20Ying%20Wang%20and%20Kun%20Ding%20and%20Shiming%20Xiang%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20Knowledge-based%20visual%20question%20answering%20%28KB-VQA%29%20requires%20visual%20language%0Amodels%20%28VLMs%29%20to%20integrate%20visual%20understanding%20with%20external%20knowledge%0Aretrieval.%20Although%20retrieval-augmented%20generation%20%28RAG%29%20achieves%20significant%0Aadvances%20in%20this%20task%20by%20combining%20knowledge-base%20querying%2C%20it%20still%20struggles%0Awith%20the%20quality%20of%20multimodal%20queries%20and%20the%20relevance%20of%20retrieved%20results.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20three-stage%20method%2C%20termed%0AWiki-PRF%2C%20including%20Processing%2C%20Retrieval%20and%20Filtering%20stages.%20The%20processing%0Astage%20dynamically%20invokes%20visual%20tools%20to%20extract%20precise%20multimodal%0Ainformation%20for%20retrieval.%20The%20retrieval%20stage%20integrates%20visual%20and%20text%0Afeatures%20to%20achieve%20multimodal%20knowledge%20retrieval.%20The%20filtering%20stage%0Aperforms%20relevance%20filtering%20and%20concentration%20on%20retrieval%20results.%20To%20this%0Aend%2C%20we%20introduce%20a%20visual%20language%20model%20trained%20with%20answer%20accuracy%20and%0Aformat%20consistency%20as%20reward%20signals%20via%20a%20reinforcement%20learning%20manner.%20This%0Aenhances%20the%20model%27s%20reasoning%2C%20tool%20invocation%20for%20accurate%20queries%2C%20and%0Afiltering%20of%20irrelevant%20content.%20Experiments%20on%20benchmark%20datasets%20%28E-VQA%20and%0AInfoSeek%29%20show%20significant%20improvements~%2836.0%20and%2042.8%29%20in%20answer%20quality%2C%0Aachieving%20state-of-the-art%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/cqu-student/Wiki-PRF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-based%2520Visual%2520Question%2520Answer%2520with%2520Multimodal%2520Processing%252C%250A%2520%2520Retrieval%2520and%2520Filtering%26entry.906535625%3DYuyang%2520Hong%2520and%2520Jiaqi%2520Gu%2520and%2520Qi%2520Yang%2520and%2520Lubin%2520Fan%2520and%2520Yue%2520Wu%2520and%2520Ying%2520Wang%2520and%2520Kun%2520Ding%2520and%2520Shiming%2520Xiang%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520Knowledge-based%2520visual%2520question%2520answering%2520%2528KB-VQA%2529%2520requires%2520visual%2520language%250Amodels%2520%2528VLMs%2529%2520to%2520integrate%2520visual%2520understanding%2520with%2520external%2520knowledge%250Aretrieval.%2520Although%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520achieves%2520significant%250Aadvances%2520in%2520this%2520task%2520by%2520combining%2520knowledge-base%2520querying%252C%2520it%2520still%2520struggles%250Awith%2520the%2520quality%2520of%2520multimodal%2520queries%2520and%2520the%2520relevance%2520of%2520retrieved%2520results.%250ATo%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520three-stage%2520method%252C%2520termed%250AWiki-PRF%252C%2520including%2520Processing%252C%2520Retrieval%2520and%2520Filtering%2520stages.%2520The%2520processing%250Astage%2520dynamically%2520invokes%2520visual%2520tools%2520to%2520extract%2520precise%2520multimodal%250Ainformation%2520for%2520retrieval.%2520The%2520retrieval%2520stage%2520integrates%2520visual%2520and%2520text%250Afeatures%2520to%2520achieve%2520multimodal%2520knowledge%2520retrieval.%2520The%2520filtering%2520stage%250Aperforms%2520relevance%2520filtering%2520and%2520concentration%2520on%2520retrieval%2520results.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520a%2520visual%2520language%2520model%2520trained%2520with%2520answer%2520accuracy%2520and%250Aformat%2520consistency%2520as%2520reward%2520signals%2520via%2520a%2520reinforcement%2520learning%2520manner.%2520This%250Aenhances%2520the%2520model%2527s%2520reasoning%252C%2520tool%2520invocation%2520for%2520accurate%2520queries%252C%2520and%250Afiltering%2520of%2520irrelevant%2520content.%2520Experiments%2520on%2520benchmark%2520datasets%2520%2528E-VQA%2520and%250AInfoSeek%2529%2520show%2520significant%2520improvements~%252836.0%2520and%252042.8%2529%2520in%2520answer%2520quality%252C%250Aachieving%2520state-of-the-art%2520performance.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/cqu-student/Wiki-PRF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-based%20Visual%20Question%20Answer%20with%20Multimodal%20Processing%2C%0A%20%20Retrieval%20and%20Filtering&entry.906535625=Yuyang%20Hong%20and%20Jiaqi%20Gu%20and%20Qi%20Yang%20and%20Lubin%20Fan%20and%20Yue%20Wu%20and%20Ying%20Wang%20and%20Kun%20Ding%20and%20Shiming%20Xiang%20and%20Jieping%20Ye&entry.1292438233=%20%20Knowledge-based%20visual%20question%20answering%20%28KB-VQA%29%20requires%20visual%20language%0Amodels%20%28VLMs%29%20to%20integrate%20visual%20understanding%20with%20external%20knowledge%0Aretrieval.%20Although%20retrieval-augmented%20generation%20%28RAG%29%20achieves%20significant%0Aadvances%20in%20this%20task%20by%20combining%20knowledge-base%20querying%2C%20it%20still%20struggles%0Awith%20the%20quality%20of%20multimodal%20queries%20and%20the%20relevance%20of%20retrieved%20results.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20three-stage%20method%2C%20termed%0AWiki-PRF%2C%20including%20Processing%2C%20Retrieval%20and%20Filtering%20stages.%20The%20processing%0Astage%20dynamically%20invokes%20visual%20tools%20to%20extract%20precise%20multimodal%0Ainformation%20for%20retrieval.%20The%20retrieval%20stage%20integrates%20visual%20and%20text%0Afeatures%20to%20achieve%20multimodal%20knowledge%20retrieval.%20The%20filtering%20stage%0Aperforms%20relevance%20filtering%20and%20concentration%20on%20retrieval%20results.%20To%20this%0Aend%2C%20we%20introduce%20a%20visual%20language%20model%20trained%20with%20answer%20accuracy%20and%0Aformat%20consistency%20as%20reward%20signals%20via%20a%20reinforcement%20learning%20manner.%20This%0Aenhances%20the%20model%27s%20reasoning%2C%20tool%20invocation%20for%20accurate%20queries%2C%20and%0Afiltering%20of%20irrelevant%20content.%20Experiments%20on%20benchmark%20datasets%20%28E-VQA%20and%0AInfoSeek%29%20show%20significant%20improvements~%2836.0%20and%2042.8%29%20in%20answer%20quality%2C%0Aachieving%20state-of-the-art%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/cqu-student/Wiki-PRF%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14605v1&entry.124074799=Read"},
{"title": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection", "author": "Mohammad Mahdi Hemmatyar and Mahdi Jafari and Mohammad Amin Yousefi and Mohammad Reza Nemati and Mobin Azadani and Hamid Reza Rastad and Amirmohammad Akbari", "abstract": "  Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios.\n", "link": "http://arxiv.org/abs/2509.22544v2", "date": "2025-10-16", "relevancy": 2.7048, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyCoVAD%3A%20A%20Hybrid%20SSL-LLM%20Model%20for%20Complex%20Video%20Anomaly%20Detection&body=Title%3A%20HyCoVAD%3A%20A%20Hybrid%20SSL-LLM%20Model%20for%20Complex%20Video%20Anomaly%20Detection%0AAuthor%3A%20Mohammad%20Mahdi%20Hemmatyar%20and%20Mahdi%20Jafari%20and%20Mohammad%20Amin%20Yousefi%20and%20Mohammad%20Reza%20Nemati%20and%20Mobin%20Azadani%20and%20Hamid%20Reza%20Rastad%20and%20Amirmohammad%20Akbari%0AAbstract%3A%20%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20crucial%20for%20intelligent%20surveillance%2C%20but%20a%0Asignificant%20challenge%20lies%20in%20identifying%20complex%20anomalies%2C%20which%20are%20events%0Adefined%20by%20intricate%20relationships%20and%20temporal%20dependencies%20among%20multiple%0Aentities%20rather%20than%20by%20isolated%20actions.%20While%20self-supervised%20learning%20%28SSL%29%0Amethods%20effectively%20model%20low-level%20spatiotemporal%20patterns%2C%20they%20often%0Astruggle%20to%20grasp%20the%20semantic%20meaning%20of%20these%20interactions.%20Conversely%2C%20large%0Alanguage%20models%20%28LLMs%29%20offer%20powerful%20contextual%20reasoning%20but%20are%0Acomputationally%20expensive%20for%20frame-by-frame%20analysis%20and%20lack%20fine-grained%0Aspatial%20localization.%20We%20introduce%20HyCoVAD%2C%20Hybrid%20Complex%20Video%20Anomaly%0ADetection%2C%20a%20hybrid%20SSL-LLM%20model%20that%20combines%20a%20multi-task%20SSL%20temporal%0Aanalyzer%20with%20LLM%20validator.%20The%20SSL%20module%20is%20built%20upon%20an%20nnFormer%20backbone%0Awhich%20is%20a%20transformer-based%20model%20for%20image%20segmentation.%20It%20is%20trained%20with%0Amultiple%20proxy%20tasks%2C%20learns%20from%20video%20frames%20to%20identify%20those%20suspected%20of%0Aanomaly.%20The%20selected%20frames%20are%20then%20forwarded%20to%20the%20LLM%2C%20which%20enriches%20the%0Aanalysis%20with%20semantic%20context%20by%20applying%20structured%2C%20rule-based%20reasoning%20to%0Avalidate%20the%20presence%20of%20anomalies.%20Experiments%20on%20the%20challenging%20ComplexVAD%0Adataset%20show%20that%20HyCoVAD%20achieves%20a%2072.5%25%20frame-level%20AUC%2C%20outperforming%0Aexisting%20baselines%20by%2012.5%25%20while%20reducing%20LLM%20computation.%20We%20release%20our%0Ainteraction%20anomaly%20taxonomy%2C%20adaptive%20thresholding%20protocol%2C%20and%20code%20to%0Afacilitate%20future%20research%20in%20complex%20VAD%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyCoVAD%253A%2520A%2520Hybrid%2520SSL-LLM%2520Model%2520for%2520Complex%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DMohammad%2520Mahdi%2520Hemmatyar%2520and%2520Mahdi%2520Jafari%2520and%2520Mohammad%2520Amin%2520Yousefi%2520and%2520Mohammad%2520Reza%2520Nemati%2520and%2520Mobin%2520Azadani%2520and%2520Hamid%2520Reza%2520Rastad%2520and%2520Amirmohammad%2520Akbari%26entry.1292438233%3D%2520%2520Video%2520anomaly%2520detection%2520%2528VAD%2529%2520is%2520crucial%2520for%2520intelligent%2520surveillance%252C%2520but%2520a%250Asignificant%2520challenge%2520lies%2520in%2520identifying%2520complex%2520anomalies%252C%2520which%2520are%2520events%250Adefined%2520by%2520intricate%2520relationships%2520and%2520temporal%2520dependencies%2520among%2520multiple%250Aentities%2520rather%2520than%2520by%2520isolated%2520actions.%2520While%2520self-supervised%2520learning%2520%2528SSL%2529%250Amethods%2520effectively%2520model%2520low-level%2520spatiotemporal%2520patterns%252C%2520they%2520often%250Astruggle%2520to%2520grasp%2520the%2520semantic%2520meaning%2520of%2520these%2520interactions.%2520Conversely%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520offer%2520powerful%2520contextual%2520reasoning%2520but%2520are%250Acomputationally%2520expensive%2520for%2520frame-by-frame%2520analysis%2520and%2520lack%2520fine-grained%250Aspatial%2520localization.%2520We%2520introduce%2520HyCoVAD%252C%2520Hybrid%2520Complex%2520Video%2520Anomaly%250ADetection%252C%2520a%2520hybrid%2520SSL-LLM%2520model%2520that%2520combines%2520a%2520multi-task%2520SSL%2520temporal%250Aanalyzer%2520with%2520LLM%2520validator.%2520The%2520SSL%2520module%2520is%2520built%2520upon%2520an%2520nnFormer%2520backbone%250Awhich%2520is%2520a%2520transformer-based%2520model%2520for%2520image%2520segmentation.%2520It%2520is%2520trained%2520with%250Amultiple%2520proxy%2520tasks%252C%2520learns%2520from%2520video%2520frames%2520to%2520identify%2520those%2520suspected%2520of%250Aanomaly.%2520The%2520selected%2520frames%2520are%2520then%2520forwarded%2520to%2520the%2520LLM%252C%2520which%2520enriches%2520the%250Aanalysis%2520with%2520semantic%2520context%2520by%2520applying%2520structured%252C%2520rule-based%2520reasoning%2520to%250Avalidate%2520the%2520presence%2520of%2520anomalies.%2520Experiments%2520on%2520the%2520challenging%2520ComplexVAD%250Adataset%2520show%2520that%2520HyCoVAD%2520achieves%2520a%252072.5%2525%2520frame-level%2520AUC%252C%2520outperforming%250Aexisting%2520baselines%2520by%252012.5%2525%2520while%2520reducing%2520LLM%2520computation.%2520We%2520release%2520our%250Ainteraction%2520anomaly%2520taxonomy%252C%2520adaptive%2520thresholding%2520protocol%252C%2520and%2520code%2520to%250Afacilitate%2520future%2520research%2520in%2520complex%2520VAD%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyCoVAD%3A%20A%20Hybrid%20SSL-LLM%20Model%20for%20Complex%20Video%20Anomaly%20Detection&entry.906535625=Mohammad%20Mahdi%20Hemmatyar%20and%20Mahdi%20Jafari%20and%20Mohammad%20Amin%20Yousefi%20and%20Mohammad%20Reza%20Nemati%20and%20Mobin%20Azadani%20and%20Hamid%20Reza%20Rastad%20and%20Amirmohammad%20Akbari&entry.1292438233=%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20crucial%20for%20intelligent%20surveillance%2C%20but%20a%0Asignificant%20challenge%20lies%20in%20identifying%20complex%20anomalies%2C%20which%20are%20events%0Adefined%20by%20intricate%20relationships%20and%20temporal%20dependencies%20among%20multiple%0Aentities%20rather%20than%20by%20isolated%20actions.%20While%20self-supervised%20learning%20%28SSL%29%0Amethods%20effectively%20model%20low-level%20spatiotemporal%20patterns%2C%20they%20often%0Astruggle%20to%20grasp%20the%20semantic%20meaning%20of%20these%20interactions.%20Conversely%2C%20large%0Alanguage%20models%20%28LLMs%29%20offer%20powerful%20contextual%20reasoning%20but%20are%0Acomputationally%20expensive%20for%20frame-by-frame%20analysis%20and%20lack%20fine-grained%0Aspatial%20localization.%20We%20introduce%20HyCoVAD%2C%20Hybrid%20Complex%20Video%20Anomaly%0ADetection%2C%20a%20hybrid%20SSL-LLM%20model%20that%20combines%20a%20multi-task%20SSL%20temporal%0Aanalyzer%20with%20LLM%20validator.%20The%20SSL%20module%20is%20built%20upon%20an%20nnFormer%20backbone%0Awhich%20is%20a%20transformer-based%20model%20for%20image%20segmentation.%20It%20is%20trained%20with%0Amultiple%20proxy%20tasks%2C%20learns%20from%20video%20frames%20to%20identify%20those%20suspected%20of%0Aanomaly.%20The%20selected%20frames%20are%20then%20forwarded%20to%20the%20LLM%2C%20which%20enriches%20the%0Aanalysis%20with%20semantic%20context%20by%20applying%20structured%2C%20rule-based%20reasoning%20to%0Avalidate%20the%20presence%20of%20anomalies.%20Experiments%20on%20the%20challenging%20ComplexVAD%0Adataset%20show%20that%20HyCoVAD%20achieves%20a%2072.5%25%20frame-level%20AUC%2C%20outperforming%0Aexisting%20baselines%20by%2012.5%25%20while%20reducing%20LLM%20computation.%20We%20release%20our%0Ainteraction%20anomaly%20taxonomy%2C%20adaptive%20thresholding%20protocol%2C%20and%20code%20to%0Afacilitate%20future%20research%20in%20complex%20VAD%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22544v2&entry.124074799=Read"},
{"title": "Free-Grained Hierarchical Recognition", "author": "Seulki Park and Zilin Wang and Stella X. Yu", "abstract": "  Hierarchical image classification predicts labels across a semantic taxonomy,\nbut existing methods typically assume complete, fine-grained annotations, an\nassumption rarely met in practice. Real-world supervision varies in\ngranularity, influenced by image quality, annotator expertise, and task\ndemands; a distant bird may be labeled Bird, while a close-up reveals Bald\neagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet\nand structured into cognitively inspired basic, subordinate, and fine-grained\nlevels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,\nmixed-granularity labels reflecting human annotation behavior. We propose\nfree-grain learning, with heterogeneous supervision across instances. We\ndevelop methods that enhance semantic guidance via pseudo-attributes from\nvision-language models and visual guidance via semi-supervised learning. These,\nalong with strong baselines, substantially improve performance under mixed\nsupervision. Together, our benchmark and methods advance hierarchical\nclassification under real-world constraints.\n", "link": "http://arxiv.org/abs/2510.14737v1", "date": "2025-10-16", "relevancy": 2.6882, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.559}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-Grained%20Hierarchical%20Recognition&body=Title%3A%20Free-Grained%20Hierarchical%20Recognition%0AAuthor%3A%20Seulki%20Park%20and%20Zilin%20Wang%20and%20Stella%20X.%20Yu%0AAbstract%3A%20%20%20Hierarchical%20image%20classification%20predicts%20labels%20across%20a%20semantic%20taxonomy%2C%0Abut%20existing%20methods%20typically%20assume%20complete%2C%20fine-grained%20annotations%2C%20an%0Aassumption%20rarely%20met%20in%20practice.%20Real-world%20supervision%20varies%20in%0Agranularity%2C%20influenced%20by%20image%20quality%2C%20annotator%20expertise%2C%20and%20task%0Ademands%3B%20a%20distant%20bird%20may%20be%20labeled%20Bird%2C%20while%20a%20close-up%20reveals%20Bald%0Aeagle.%20We%20introduce%20ImageNet-F%2C%20a%20large-scale%20benchmark%20curated%20from%20ImageNet%0Aand%20structured%20into%20cognitively%20inspired%20basic%2C%20subordinate%2C%20and%20fine-grained%0Alevels.%20Using%20CLIP%20as%20a%20proxy%20for%20semantic%20ambiguity%2C%20we%20simulate%20realistic%2C%0Amixed-granularity%20labels%20reflecting%20human%20annotation%20behavior.%20We%20propose%0Afree-grain%20learning%2C%20with%20heterogeneous%20supervision%20across%20instances.%20We%0Adevelop%20methods%20that%20enhance%20semantic%20guidance%20via%20pseudo-attributes%20from%0Avision-language%20models%20and%20visual%20guidance%20via%20semi-supervised%20learning.%20These%2C%0Aalong%20with%20strong%20baselines%2C%20substantially%20improve%20performance%20under%20mixed%0Asupervision.%20Together%2C%20our%20benchmark%20and%20methods%20advance%20hierarchical%0Aclassification%20under%20real-world%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-Grained%2520Hierarchical%2520Recognition%26entry.906535625%3DSeulki%2520Park%2520and%2520Zilin%2520Wang%2520and%2520Stella%2520X.%2520Yu%26entry.1292438233%3D%2520%2520Hierarchical%2520image%2520classification%2520predicts%2520labels%2520across%2520a%2520semantic%2520taxonomy%252C%250Abut%2520existing%2520methods%2520typically%2520assume%2520complete%252C%2520fine-grained%2520annotations%252C%2520an%250Aassumption%2520rarely%2520met%2520in%2520practice.%2520Real-world%2520supervision%2520varies%2520in%250Agranularity%252C%2520influenced%2520by%2520image%2520quality%252C%2520annotator%2520expertise%252C%2520and%2520task%250Ademands%253B%2520a%2520distant%2520bird%2520may%2520be%2520labeled%2520Bird%252C%2520while%2520a%2520close-up%2520reveals%2520Bald%250Aeagle.%2520We%2520introduce%2520ImageNet-F%252C%2520a%2520large-scale%2520benchmark%2520curated%2520from%2520ImageNet%250Aand%2520structured%2520into%2520cognitively%2520inspired%2520basic%252C%2520subordinate%252C%2520and%2520fine-grained%250Alevels.%2520Using%2520CLIP%2520as%2520a%2520proxy%2520for%2520semantic%2520ambiguity%252C%2520we%2520simulate%2520realistic%252C%250Amixed-granularity%2520labels%2520reflecting%2520human%2520annotation%2520behavior.%2520We%2520propose%250Afree-grain%2520learning%252C%2520with%2520heterogeneous%2520supervision%2520across%2520instances.%2520We%250Adevelop%2520methods%2520that%2520enhance%2520semantic%2520guidance%2520via%2520pseudo-attributes%2520from%250Avision-language%2520models%2520and%2520visual%2520guidance%2520via%2520semi-supervised%2520learning.%2520These%252C%250Aalong%2520with%2520strong%2520baselines%252C%2520substantially%2520improve%2520performance%2520under%2520mixed%250Asupervision.%2520Together%252C%2520our%2520benchmark%2520and%2520methods%2520advance%2520hierarchical%250Aclassification%2520under%2520real-world%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-Grained%20Hierarchical%20Recognition&entry.906535625=Seulki%20Park%20and%20Zilin%20Wang%20and%20Stella%20X.%20Yu&entry.1292438233=%20%20Hierarchical%20image%20classification%20predicts%20labels%20across%20a%20semantic%20taxonomy%2C%0Abut%20existing%20methods%20typically%20assume%20complete%2C%20fine-grained%20annotations%2C%20an%0Aassumption%20rarely%20met%20in%20practice.%20Real-world%20supervision%20varies%20in%0Agranularity%2C%20influenced%20by%20image%20quality%2C%20annotator%20expertise%2C%20and%20task%0Ademands%3B%20a%20distant%20bird%20may%20be%20labeled%20Bird%2C%20while%20a%20close-up%20reveals%20Bald%0Aeagle.%20We%20introduce%20ImageNet-F%2C%20a%20large-scale%20benchmark%20curated%20from%20ImageNet%0Aand%20structured%20into%20cognitively%20inspired%20basic%2C%20subordinate%2C%20and%20fine-grained%0Alevels.%20Using%20CLIP%20as%20a%20proxy%20for%20semantic%20ambiguity%2C%20we%20simulate%20realistic%2C%0Amixed-granularity%20labels%20reflecting%20human%20annotation%20behavior.%20We%20propose%0Afree-grain%20learning%2C%20with%20heterogeneous%20supervision%20across%20instances.%20We%0Adevelop%20methods%20that%20enhance%20semantic%20guidance%20via%20pseudo-attributes%20from%0Avision-language%20models%20and%20visual%20guidance%20via%20semi-supervised%20learning.%20These%2C%0Aalong%20with%20strong%20baselines%2C%20substantially%20improve%20performance%20under%20mixed%0Asupervision.%20Together%2C%20our%20benchmark%20and%20methods%20advance%20hierarchical%0Aclassification%20under%20real-world%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14737v1&entry.124074799=Read"},
{"title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for\n  Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging", "author": "Arefin Ittesafun Abian and Ripon Kumar Debnath and Md. Abdur Rahman and Mohaimenul Azam Khan Raiaan and Md Rafiqul Islam and Asif Karim and Reem E. Mohamed and Sami Azam", "abstract": "  Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the AMOS 2022\ndataset obtains an average Dice of 85.09%, IoU of 76.66%, ASSD of 19.49 mm, and\nVOE of 23.34%, indicating strong generalization across different datasets.\nThese results confirm the effectiveness and robustness of HANS-Net in providing\nanatomically consistent, accurate, and confident liver and tumor segmentation.\n", "link": "http://arxiv.org/abs/2507.11325v2", "date": "2025-10-16", "relevancy": 2.6876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5395}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HANS-Net%3A%20Hyperbolic%20Convolution%20and%20Adaptive%20Temporal%20Attention%20for%0A%20%20Accurate%20and%20Generalizable%20Liver%20and%20Tumor%20Segmentation%20in%20CT%20Imaging&body=Title%3A%20HANS-Net%3A%20Hyperbolic%20Convolution%20and%20Adaptive%20Temporal%20Attention%20for%0A%20%20Accurate%20and%20Generalizable%20Liver%20and%20Tumor%20Segmentation%20in%20CT%20Imaging%0AAuthor%3A%20Arefin%20Ittesafun%20Abian%20and%20Ripon%20Kumar%20Debnath%20and%20Md.%20Abdur%20Rahman%20and%20Mohaimenul%20Azam%20Khan%20Raiaan%20and%20Md%20Rafiqul%20Islam%20and%20Asif%20Karim%20and%20Reem%20E.%20Mohamed%20and%20Sami%20Azam%0AAbstract%3A%20%20%20Accurate%20liver%20and%20tumor%20segmentation%20on%20abdominal%20CT%20images%20is%20critical%20for%0Areliable%20diagnosis%20and%20treatment%20planning%2C%20but%20remains%20challenging%20due%20to%0Acomplex%20anatomical%20structures%2C%20variability%20in%20tumor%20appearance%2C%20and%20limited%0Aannotated%20data.%20To%20address%20these%20issues%2C%20we%20introduce%20Hyperbolic-convolutions%0AAdaptive-temporal-attention%20with%20Neural-representation%20and%20Synaptic-plasticity%0ANetwork%20%28HANS-Net%29%2C%20a%20novel%20segmentation%20framework%20that%20synergistically%0Acombines%20hyperbolic%20convolutions%20for%20hierarchical%20geometric%20representation%2C%20a%0Awavelet-inspired%20decomposition%20module%20for%20multi-scale%20texture%20learning%2C%20a%0Abiologically%20motivated%20synaptic%20plasticity%20mechanism%20for%20adaptive%20feature%0Aenhancement%2C%20and%20an%20implicit%20neural%20representation%20branch%20to%20model%20fine-grained%0Aand%20continuous%20anatomical%20boundaries.%20Additionally%2C%20we%20incorporate%0Auncertainty-aware%20Monte%20Carlo%20dropout%20to%20quantify%20prediction%20confidence%20and%0Alightweight%20temporal%20attention%20to%20improve%20inter-slice%20consistency%20without%0Asacrificing%20efficiency.%20Extensive%20evaluations%20of%20the%20LiTS%20dataset%20demonstrate%0Athat%20HANS-Net%20achieves%20a%20mean%20Dice%20score%20of%2093.26%25%2C%20an%20IoU%20of%2088.09%25%2C%20an%0Aaverage%20symmetric%20surface%20distance%20%28ASSD%29%20of%200.72%20mm%2C%20and%20a%20volume%20overlap%0Aerror%20%28VOE%29%20of%2011.91%25.%20Furthermore%2C%20cross-dataset%20validation%20on%20the%20AMOS%202022%0Adataset%20obtains%20an%20average%20Dice%20of%2085.09%25%2C%20IoU%20of%2076.66%25%2C%20ASSD%20of%2019.49%20mm%2C%20and%0AVOE%20of%2023.34%25%2C%20indicating%20strong%20generalization%20across%20different%20datasets.%0AThese%20results%20confirm%20the%20effectiveness%20and%20robustness%20of%20HANS-Net%20in%20providing%0Aanatomically%20consistent%2C%20accurate%2C%20and%20confident%20liver%20and%20tumor%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHANS-Net%253A%2520Hyperbolic%2520Convolution%2520and%2520Adaptive%2520Temporal%2520Attention%2520for%250A%2520%2520Accurate%2520and%2520Generalizable%2520Liver%2520and%2520Tumor%2520Segmentation%2520in%2520CT%2520Imaging%26entry.906535625%3DArefin%2520Ittesafun%2520Abian%2520and%2520Ripon%2520Kumar%2520Debnath%2520and%2520Md.%2520Abdur%2520Rahman%2520and%2520Mohaimenul%2520Azam%2520Khan%2520Raiaan%2520and%2520Md%2520Rafiqul%2520Islam%2520and%2520Asif%2520Karim%2520and%2520Reem%2520E.%2520Mohamed%2520and%2520Sami%2520Azam%26entry.1292438233%3D%2520%2520Accurate%2520liver%2520and%2520tumor%2520segmentation%2520on%2520abdominal%2520CT%2520images%2520is%2520critical%2520for%250Areliable%2520diagnosis%2520and%2520treatment%2520planning%252C%2520but%2520remains%2520challenging%2520due%2520to%250Acomplex%2520anatomical%2520structures%252C%2520variability%2520in%2520tumor%2520appearance%252C%2520and%2520limited%250Aannotated%2520data.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Hyperbolic-convolutions%250AAdaptive-temporal-attention%2520with%2520Neural-representation%2520and%2520Synaptic-plasticity%250ANetwork%2520%2528HANS-Net%2529%252C%2520a%2520novel%2520segmentation%2520framework%2520that%2520synergistically%250Acombines%2520hyperbolic%2520convolutions%2520for%2520hierarchical%2520geometric%2520representation%252C%2520a%250Awavelet-inspired%2520decomposition%2520module%2520for%2520multi-scale%2520texture%2520learning%252C%2520a%250Abiologically%2520motivated%2520synaptic%2520plasticity%2520mechanism%2520for%2520adaptive%2520feature%250Aenhancement%252C%2520and%2520an%2520implicit%2520neural%2520representation%2520branch%2520to%2520model%2520fine-grained%250Aand%2520continuous%2520anatomical%2520boundaries.%2520Additionally%252C%2520we%2520incorporate%250Auncertainty-aware%2520Monte%2520Carlo%2520dropout%2520to%2520quantify%2520prediction%2520confidence%2520and%250Alightweight%2520temporal%2520attention%2520to%2520improve%2520inter-slice%2520consistency%2520without%250Asacrificing%2520efficiency.%2520Extensive%2520evaluations%2520of%2520the%2520LiTS%2520dataset%2520demonstrate%250Athat%2520HANS-Net%2520achieves%2520a%2520mean%2520Dice%2520score%2520of%252093.26%2525%252C%2520an%2520IoU%2520of%252088.09%2525%252C%2520an%250Aaverage%2520symmetric%2520surface%2520distance%2520%2528ASSD%2529%2520of%25200.72%2520mm%252C%2520and%2520a%2520volume%2520overlap%250Aerror%2520%2528VOE%2529%2520of%252011.91%2525.%2520Furthermore%252C%2520cross-dataset%2520validation%2520on%2520the%2520AMOS%25202022%250Adataset%2520obtains%2520an%2520average%2520Dice%2520of%252085.09%2525%252C%2520IoU%2520of%252076.66%2525%252C%2520ASSD%2520of%252019.49%2520mm%252C%2520and%250AVOE%2520of%252023.34%2525%252C%2520indicating%2520strong%2520generalization%2520across%2520different%2520datasets.%250AThese%2520results%2520confirm%2520the%2520effectiveness%2520and%2520robustness%2520of%2520HANS-Net%2520in%2520providing%250Aanatomically%2520consistent%252C%2520accurate%252C%2520and%2520confident%2520liver%2520and%2520tumor%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HANS-Net%3A%20Hyperbolic%20Convolution%20and%20Adaptive%20Temporal%20Attention%20for%0A%20%20Accurate%20and%20Generalizable%20Liver%20and%20Tumor%20Segmentation%20in%20CT%20Imaging&entry.906535625=Arefin%20Ittesafun%20Abian%20and%20Ripon%20Kumar%20Debnath%20and%20Md.%20Abdur%20Rahman%20and%20Mohaimenul%20Azam%20Khan%20Raiaan%20and%20Md%20Rafiqul%20Islam%20and%20Asif%20Karim%20and%20Reem%20E.%20Mohamed%20and%20Sami%20Azam&entry.1292438233=%20%20Accurate%20liver%20and%20tumor%20segmentation%20on%20abdominal%20CT%20images%20is%20critical%20for%0Areliable%20diagnosis%20and%20treatment%20planning%2C%20but%20remains%20challenging%20due%20to%0Acomplex%20anatomical%20structures%2C%20variability%20in%20tumor%20appearance%2C%20and%20limited%0Aannotated%20data.%20To%20address%20these%20issues%2C%20we%20introduce%20Hyperbolic-convolutions%0AAdaptive-temporal-attention%20with%20Neural-representation%20and%20Synaptic-plasticity%0ANetwork%20%28HANS-Net%29%2C%20a%20novel%20segmentation%20framework%20that%20synergistically%0Acombines%20hyperbolic%20convolutions%20for%20hierarchical%20geometric%20representation%2C%20a%0Awavelet-inspired%20decomposition%20module%20for%20multi-scale%20texture%20learning%2C%20a%0Abiologically%20motivated%20synaptic%20plasticity%20mechanism%20for%20adaptive%20feature%0Aenhancement%2C%20and%20an%20implicit%20neural%20representation%20branch%20to%20model%20fine-grained%0Aand%20continuous%20anatomical%20boundaries.%20Additionally%2C%20we%20incorporate%0Auncertainty-aware%20Monte%20Carlo%20dropout%20to%20quantify%20prediction%20confidence%20and%0Alightweight%20temporal%20attention%20to%20improve%20inter-slice%20consistency%20without%0Asacrificing%20efficiency.%20Extensive%20evaluations%20of%20the%20LiTS%20dataset%20demonstrate%0Athat%20HANS-Net%20achieves%20a%20mean%20Dice%20score%20of%2093.26%25%2C%20an%20IoU%20of%2088.09%25%2C%20an%0Aaverage%20symmetric%20surface%20distance%20%28ASSD%29%20of%200.72%20mm%2C%20and%20a%20volume%20overlap%0Aerror%20%28VOE%29%20of%2011.91%25.%20Furthermore%2C%20cross-dataset%20validation%20on%20the%20AMOS%202022%0Adataset%20obtains%20an%20average%20Dice%20of%2085.09%25%2C%20IoU%20of%2076.66%25%2C%20ASSD%20of%2019.49%20mm%2C%20and%0AVOE%20of%2023.34%25%2C%20indicating%20strong%20generalization%20across%20different%20datasets.%0AThese%20results%20confirm%20the%20effectiveness%20and%20robustness%20of%20HANS-Net%20in%20providing%0Aanatomically%20consistent%2C%20accurate%2C%20and%20confident%20liver%20and%20tumor%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11325v2&entry.124074799=Read"},
{"title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object\n  Detection", "author": "Dingzhou Xie and Rushi Lan and Cheng Pang and Enhao Ning and Jiahao Zeng and Wei Zheng", "abstract": "  Recent object detection methods have made remarkable progress by leveraging\nattention mechanisms to improve feature discriminability. However, most\nexisting approaches are confined to refining single-layer or fusing dual-layer\nfeatures, overlooking the rich inter-layer dependencies across multi-scale\nrepresentations. This limits their ability to capture comprehensive contextual\ninformation essential for detecting objects with large scale variations. In\nthis paper, we propose a novel Cross-Layer Feature Self-Attention Module\n(CFSAM), which holistically models both local and global dependencies within\nmulti-scale feature maps. CFSAM consists of three key components: a\nconvolutional local feature extractor, a Transformer-based global modeling unit\nthat efficiently captures cross-layer interactions, and a feature fusion\nmechanism to restore and enhance the original representations. When integrated\ninto the SSD300 framework, CFSAM significantly boosts detection performance,\nachieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO\n(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the\nmodule accelerates convergence during training without introducing substantial\ncomputational overhead. Our work highlights the importance of explicit\ncross-layer attention modeling in advancing multi-scale object detection.\n", "link": "http://arxiv.org/abs/2510.14726v1", "date": "2025-10-16", "relevancy": 2.6854, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Layer%20Feature%20Self-Attention%20Module%20for%20Multi-Scale%20Object%0A%20%20Detection&body=Title%3A%20Cross-Layer%20Feature%20Self-Attention%20Module%20for%20Multi-Scale%20Object%0A%20%20Detection%0AAuthor%3A%20Dingzhou%20Xie%20and%20Rushi%20Lan%20and%20Cheng%20Pang%20and%20Enhao%20Ning%20and%20Jiahao%20Zeng%20and%20Wei%20Zheng%0AAbstract%3A%20%20%20Recent%20object%20detection%20methods%20have%20made%20remarkable%20progress%20by%20leveraging%0Aattention%20mechanisms%20to%20improve%20feature%20discriminability.%20However%2C%20most%0Aexisting%20approaches%20are%20confined%20to%20refining%20single-layer%20or%20fusing%20dual-layer%0Afeatures%2C%20overlooking%20the%20rich%20inter-layer%20dependencies%20across%20multi-scale%0Arepresentations.%20This%20limits%20their%20ability%20to%20capture%20comprehensive%20contextual%0Ainformation%20essential%20for%20detecting%20objects%20with%20large%20scale%20variations.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Cross-Layer%20Feature%20Self-Attention%20Module%0A%28CFSAM%29%2C%20which%20holistically%20models%20both%20local%20and%20global%20dependencies%20within%0Amulti-scale%20feature%20maps.%20CFSAM%20consists%20of%20three%20key%20components%3A%20a%0Aconvolutional%20local%20feature%20extractor%2C%20a%20Transformer-based%20global%20modeling%20unit%0Athat%20efficiently%20captures%20cross-layer%20interactions%2C%20and%20a%20feature%20fusion%0Amechanism%20to%20restore%20and%20enhance%20the%20original%20representations.%20When%20integrated%0Ainto%20the%20SSD300%20framework%2C%20CFSAM%20significantly%20boosts%20detection%20performance%2C%0Aachieving%2078.6%25%20mAP%20on%20PASCAL%20VOC%20%28vs.%2075.5%25%20baseline%29%20and%2052.1%25%20mAP%20on%20COCO%0A%28vs.%2043.1%25%20baseline%29%2C%20outperforming%20existing%20attention%20modules.%20Moreover%2C%20the%0Amodule%20accelerates%20convergence%20during%20training%20without%20introducing%20substantial%0Acomputational%20overhead.%20Our%20work%20highlights%20the%20importance%20of%20explicit%0Across-layer%20attention%20modeling%20in%20advancing%20multi-scale%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Layer%2520Feature%2520Self-Attention%2520Module%2520for%2520Multi-Scale%2520Object%250A%2520%2520Detection%26entry.906535625%3DDingzhou%2520Xie%2520and%2520Rushi%2520Lan%2520and%2520Cheng%2520Pang%2520and%2520Enhao%2520Ning%2520and%2520Jiahao%2520Zeng%2520and%2520Wei%2520Zheng%26entry.1292438233%3D%2520%2520Recent%2520object%2520detection%2520methods%2520have%2520made%2520remarkable%2520progress%2520by%2520leveraging%250Aattention%2520mechanisms%2520to%2520improve%2520feature%2520discriminability.%2520However%252C%2520most%250Aexisting%2520approaches%2520are%2520confined%2520to%2520refining%2520single-layer%2520or%2520fusing%2520dual-layer%250Afeatures%252C%2520overlooking%2520the%2520rich%2520inter-layer%2520dependencies%2520across%2520multi-scale%250Arepresentations.%2520This%2520limits%2520their%2520ability%2520to%2520capture%2520comprehensive%2520contextual%250Ainformation%2520essential%2520for%2520detecting%2520objects%2520with%2520large%2520scale%2520variations.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Cross-Layer%2520Feature%2520Self-Attention%2520Module%250A%2528CFSAM%2529%252C%2520which%2520holistically%2520models%2520both%2520local%2520and%2520global%2520dependencies%2520within%250Amulti-scale%2520feature%2520maps.%2520CFSAM%2520consists%2520of%2520three%2520key%2520components%253A%2520a%250Aconvolutional%2520local%2520feature%2520extractor%252C%2520a%2520Transformer-based%2520global%2520modeling%2520unit%250Athat%2520efficiently%2520captures%2520cross-layer%2520interactions%252C%2520and%2520a%2520feature%2520fusion%250Amechanism%2520to%2520restore%2520and%2520enhance%2520the%2520original%2520representations.%2520When%2520integrated%250Ainto%2520the%2520SSD300%2520framework%252C%2520CFSAM%2520significantly%2520boosts%2520detection%2520performance%252C%250Aachieving%252078.6%2525%2520mAP%2520on%2520PASCAL%2520VOC%2520%2528vs.%252075.5%2525%2520baseline%2529%2520and%252052.1%2525%2520mAP%2520on%2520COCO%250A%2528vs.%252043.1%2525%2520baseline%2529%252C%2520outperforming%2520existing%2520attention%2520modules.%2520Moreover%252C%2520the%250Amodule%2520accelerates%2520convergence%2520during%2520training%2520without%2520introducing%2520substantial%250Acomputational%2520overhead.%2520Our%2520work%2520highlights%2520the%2520importance%2520of%2520explicit%250Across-layer%2520attention%2520modeling%2520in%2520advancing%2520multi-scale%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Layer%20Feature%20Self-Attention%20Module%20for%20Multi-Scale%20Object%0A%20%20Detection&entry.906535625=Dingzhou%20Xie%20and%20Rushi%20Lan%20and%20Cheng%20Pang%20and%20Enhao%20Ning%20and%20Jiahao%20Zeng%20and%20Wei%20Zheng&entry.1292438233=%20%20Recent%20object%20detection%20methods%20have%20made%20remarkable%20progress%20by%20leveraging%0Aattention%20mechanisms%20to%20improve%20feature%20discriminability.%20However%2C%20most%0Aexisting%20approaches%20are%20confined%20to%20refining%20single-layer%20or%20fusing%20dual-layer%0Afeatures%2C%20overlooking%20the%20rich%20inter-layer%20dependencies%20across%20multi-scale%0Arepresentations.%20This%20limits%20their%20ability%20to%20capture%20comprehensive%20contextual%0Ainformation%20essential%20for%20detecting%20objects%20with%20large%20scale%20variations.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Cross-Layer%20Feature%20Self-Attention%20Module%0A%28CFSAM%29%2C%20which%20holistically%20models%20both%20local%20and%20global%20dependencies%20within%0Amulti-scale%20feature%20maps.%20CFSAM%20consists%20of%20three%20key%20components%3A%20a%0Aconvolutional%20local%20feature%20extractor%2C%20a%20Transformer-based%20global%20modeling%20unit%0Athat%20efficiently%20captures%20cross-layer%20interactions%2C%20and%20a%20feature%20fusion%0Amechanism%20to%20restore%20and%20enhance%20the%20original%20representations.%20When%20integrated%0Ainto%20the%20SSD300%20framework%2C%20CFSAM%20significantly%20boosts%20detection%20performance%2C%0Aachieving%2078.6%25%20mAP%20on%20PASCAL%20VOC%20%28vs.%2075.5%25%20baseline%29%20and%2052.1%25%20mAP%20on%20COCO%0A%28vs.%2043.1%25%20baseline%29%2C%20outperforming%20existing%20attention%20modules.%20Moreover%2C%20the%0Amodule%20accelerates%20convergence%20during%20training%20without%20introducing%20substantial%0Acomputational%20overhead.%20Our%20work%20highlights%20the%20importance%20of%20explicit%0Across-layer%20attention%20modeling%20in%20advancing%20multi-scale%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14726v1&entry.124074799=Read"},
{"title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models\n  for Oral and Maxillofacial Radiology", "author": "Xinrui Huang and Fan Xiao and Dongming He and Anqi Gao and Dandan Li and Xiaofan Zhang and Shaoting Zhang and Xudong Wang", "abstract": "  Oral and maxillofacial radiology plays a vital role in dental healthcare, but\nradiographic image interpretation is limited by a shortage of trained\nprofessionals. While AI approaches have shown promise, existing dental AI\nsystems are restricted by their single-modality focus, task-specific design,\nand reliance on costly labeled data, hindering their generalization across\ndiverse clinical scenarios. To address these challenges, we introduce DentVFM,\nthe first family of vision foundation models (VFMs) designed for dentistry.\nDentVFM generates task-agnostic visual representations for a wide range of\ndental applications and uses self-supervised learning on DentVista, a large\ncurated dental imaging dataset with approximately 1.6 million multi-modal\nradiographic images from various medical centers. DentVFM includes 2D and 3D\nvariants based on the Vision Transformer (ViT) architecture. To address gaps in\ndental intelligence assessment and benchmarks, we introduce DentBench, a\ncomprehensive benchmark covering eight dental subspecialties, more diseases,\nimaging modalities, and a wide geographical distribution. DentVFM shows\nimpressive generalist intelligence, demonstrating robust generalization to\ndiverse dental tasks, such as disease diagnosis, treatment analysis, biomarker\nidentification, and anatomical landmark detection and segmentation.\nExperimental results indicate DentVFM significantly outperforms supervised,\nself-supervised, and weakly supervised baselines, offering superior\ngeneralization, label efficiency, and scalability. Additionally, DentVFM\nenables cross-modality diagnostics, providing more reliable results than\nexperienced dentists in situations where conventional imaging is unavailable.\nDentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and\nlabel-efficient model to improve intelligent dental healthcare and address\ncritical gaps in global oral healthcare.\n", "link": "http://arxiv.org/abs/2510.14532v1", "date": "2025-10-16", "relevancy": 2.6771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Intelligence%20in%20Dentistry%3A%20Vision%20Foundation%20Models%0A%20%20for%20Oral%20and%20Maxillofacial%20Radiology&body=Title%3A%20Towards%20Generalist%20Intelligence%20in%20Dentistry%3A%20Vision%20Foundation%20Models%0A%20%20for%20Oral%20and%20Maxillofacial%20Radiology%0AAuthor%3A%20Xinrui%20Huang%20and%20Fan%20Xiao%20and%20Dongming%20He%20and%20Anqi%20Gao%20and%20Dandan%20Li%20and%20Xiaofan%20Zhang%20and%20Shaoting%20Zhang%20and%20Xudong%20Wang%0AAbstract%3A%20%20%20Oral%20and%20maxillofacial%20radiology%20plays%20a%20vital%20role%20in%20dental%20healthcare%2C%20but%0Aradiographic%20image%20interpretation%20is%20limited%20by%20a%20shortage%20of%20trained%0Aprofessionals.%20While%20AI%20approaches%20have%20shown%20promise%2C%20existing%20dental%20AI%0Asystems%20are%20restricted%20by%20their%20single-modality%20focus%2C%20task-specific%20design%2C%0Aand%20reliance%20on%20costly%20labeled%20data%2C%20hindering%20their%20generalization%20across%0Adiverse%20clinical%20scenarios.%20To%20address%20these%20challenges%2C%20we%20introduce%20DentVFM%2C%0Athe%20first%20family%20of%20vision%20foundation%20models%20%28VFMs%29%20designed%20for%20dentistry.%0ADentVFM%20generates%20task-agnostic%20visual%20representations%20for%20a%20wide%20range%20of%0Adental%20applications%20and%20uses%20self-supervised%20learning%20on%20DentVista%2C%20a%20large%0Acurated%20dental%20imaging%20dataset%20with%20approximately%201.6%20million%20multi-modal%0Aradiographic%20images%20from%20various%20medical%20centers.%20DentVFM%20includes%202D%20and%203D%0Avariants%20based%20on%20the%20Vision%20Transformer%20%28ViT%29%20architecture.%20To%20address%20gaps%20in%0Adental%20intelligence%20assessment%20and%20benchmarks%2C%20we%20introduce%20DentBench%2C%20a%0Acomprehensive%20benchmark%20covering%20eight%20dental%20subspecialties%2C%20more%20diseases%2C%0Aimaging%20modalities%2C%20and%20a%20wide%20geographical%20distribution.%20DentVFM%20shows%0Aimpressive%20generalist%20intelligence%2C%20demonstrating%20robust%20generalization%20to%0Adiverse%20dental%20tasks%2C%20such%20as%20disease%20diagnosis%2C%20treatment%20analysis%2C%20biomarker%0Aidentification%2C%20and%20anatomical%20landmark%20detection%20and%20segmentation.%0AExperimental%20results%20indicate%20DentVFM%20significantly%20outperforms%20supervised%2C%0Aself-supervised%2C%20and%20weakly%20supervised%20baselines%2C%20offering%20superior%0Ageneralization%2C%20label%20efficiency%2C%20and%20scalability.%20Additionally%2C%20DentVFM%0Aenables%20cross-modality%20diagnostics%2C%20providing%20more%20reliable%20results%20than%0Aexperienced%20dentists%20in%20situations%20where%20conventional%20imaging%20is%20unavailable.%0ADentVFM%20sets%20a%20new%20paradigm%20for%20dental%20AI%2C%20offering%20a%20scalable%2C%20adaptable%2C%20and%0Alabel-efficient%20model%20to%20improve%20intelligent%20dental%20healthcare%20and%20address%0Acritical%20gaps%20in%20global%20oral%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalist%2520Intelligence%2520in%2520Dentistry%253A%2520Vision%2520Foundation%2520Models%250A%2520%2520for%2520Oral%2520and%2520Maxillofacial%2520Radiology%26entry.906535625%3DXinrui%2520Huang%2520and%2520Fan%2520Xiao%2520and%2520Dongming%2520He%2520and%2520Anqi%2520Gao%2520and%2520Dandan%2520Li%2520and%2520Xiaofan%2520Zhang%2520and%2520Shaoting%2520Zhang%2520and%2520Xudong%2520Wang%26entry.1292438233%3D%2520%2520Oral%2520and%2520maxillofacial%2520radiology%2520plays%2520a%2520vital%2520role%2520in%2520dental%2520healthcare%252C%2520but%250Aradiographic%2520image%2520interpretation%2520is%2520limited%2520by%2520a%2520shortage%2520of%2520trained%250Aprofessionals.%2520While%2520AI%2520approaches%2520have%2520shown%2520promise%252C%2520existing%2520dental%2520AI%250Asystems%2520are%2520restricted%2520by%2520their%2520single-modality%2520focus%252C%2520task-specific%2520design%252C%250Aand%2520reliance%2520on%2520costly%2520labeled%2520data%252C%2520hindering%2520their%2520generalization%2520across%250Adiverse%2520clinical%2520scenarios.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520DentVFM%252C%250Athe%2520first%2520family%2520of%2520vision%2520foundation%2520models%2520%2528VFMs%2529%2520designed%2520for%2520dentistry.%250ADentVFM%2520generates%2520task-agnostic%2520visual%2520representations%2520for%2520a%2520wide%2520range%2520of%250Adental%2520applications%2520and%2520uses%2520self-supervised%2520learning%2520on%2520DentVista%252C%2520a%2520large%250Acurated%2520dental%2520imaging%2520dataset%2520with%2520approximately%25201.6%2520million%2520multi-modal%250Aradiographic%2520images%2520from%2520various%2520medical%2520centers.%2520DentVFM%2520includes%25202D%2520and%25203D%250Avariants%2520based%2520on%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520architecture.%2520To%2520address%2520gaps%2520in%250Adental%2520intelligence%2520assessment%2520and%2520benchmarks%252C%2520we%2520introduce%2520DentBench%252C%2520a%250Acomprehensive%2520benchmark%2520covering%2520eight%2520dental%2520subspecialties%252C%2520more%2520diseases%252C%250Aimaging%2520modalities%252C%2520and%2520a%2520wide%2520geographical%2520distribution.%2520DentVFM%2520shows%250Aimpressive%2520generalist%2520intelligence%252C%2520demonstrating%2520robust%2520generalization%2520to%250Adiverse%2520dental%2520tasks%252C%2520such%2520as%2520disease%2520diagnosis%252C%2520treatment%2520analysis%252C%2520biomarker%250Aidentification%252C%2520and%2520anatomical%2520landmark%2520detection%2520and%2520segmentation.%250AExperimental%2520results%2520indicate%2520DentVFM%2520significantly%2520outperforms%2520supervised%252C%250Aself-supervised%252C%2520and%2520weakly%2520supervised%2520baselines%252C%2520offering%2520superior%250Ageneralization%252C%2520label%2520efficiency%252C%2520and%2520scalability.%2520Additionally%252C%2520DentVFM%250Aenables%2520cross-modality%2520diagnostics%252C%2520providing%2520more%2520reliable%2520results%2520than%250Aexperienced%2520dentists%2520in%2520situations%2520where%2520conventional%2520imaging%2520is%2520unavailable.%250ADentVFM%2520sets%2520a%2520new%2520paradigm%2520for%2520dental%2520AI%252C%2520offering%2520a%2520scalable%252C%2520adaptable%252C%2520and%250Alabel-efficient%2520model%2520to%2520improve%2520intelligent%2520dental%2520healthcare%2520and%2520address%250Acritical%2520gaps%2520in%2520global%2520oral%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Intelligence%20in%20Dentistry%3A%20Vision%20Foundation%20Models%0A%20%20for%20Oral%20and%20Maxillofacial%20Radiology&entry.906535625=Xinrui%20Huang%20and%20Fan%20Xiao%20and%20Dongming%20He%20and%20Anqi%20Gao%20and%20Dandan%20Li%20and%20Xiaofan%20Zhang%20and%20Shaoting%20Zhang%20and%20Xudong%20Wang&entry.1292438233=%20%20Oral%20and%20maxillofacial%20radiology%20plays%20a%20vital%20role%20in%20dental%20healthcare%2C%20but%0Aradiographic%20image%20interpretation%20is%20limited%20by%20a%20shortage%20of%20trained%0Aprofessionals.%20While%20AI%20approaches%20have%20shown%20promise%2C%20existing%20dental%20AI%0Asystems%20are%20restricted%20by%20their%20single-modality%20focus%2C%20task-specific%20design%2C%0Aand%20reliance%20on%20costly%20labeled%20data%2C%20hindering%20their%20generalization%20across%0Adiverse%20clinical%20scenarios.%20To%20address%20these%20challenges%2C%20we%20introduce%20DentVFM%2C%0Athe%20first%20family%20of%20vision%20foundation%20models%20%28VFMs%29%20designed%20for%20dentistry.%0ADentVFM%20generates%20task-agnostic%20visual%20representations%20for%20a%20wide%20range%20of%0Adental%20applications%20and%20uses%20self-supervised%20learning%20on%20DentVista%2C%20a%20large%0Acurated%20dental%20imaging%20dataset%20with%20approximately%201.6%20million%20multi-modal%0Aradiographic%20images%20from%20various%20medical%20centers.%20DentVFM%20includes%202D%20and%203D%0Avariants%20based%20on%20the%20Vision%20Transformer%20%28ViT%29%20architecture.%20To%20address%20gaps%20in%0Adental%20intelligence%20assessment%20and%20benchmarks%2C%20we%20introduce%20DentBench%2C%20a%0Acomprehensive%20benchmark%20covering%20eight%20dental%20subspecialties%2C%20more%20diseases%2C%0Aimaging%20modalities%2C%20and%20a%20wide%20geographical%20distribution.%20DentVFM%20shows%0Aimpressive%20generalist%20intelligence%2C%20demonstrating%20robust%20generalization%20to%0Adiverse%20dental%20tasks%2C%20such%20as%20disease%20diagnosis%2C%20treatment%20analysis%2C%20biomarker%0Aidentification%2C%20and%20anatomical%20landmark%20detection%20and%20segmentation.%0AExperimental%20results%20indicate%20DentVFM%20significantly%20outperforms%20supervised%2C%0Aself-supervised%2C%20and%20weakly%20supervised%20baselines%2C%20offering%20superior%0Ageneralization%2C%20label%20efficiency%2C%20and%20scalability.%20Additionally%2C%20DentVFM%0Aenables%20cross-modality%20diagnostics%2C%20providing%20more%20reliable%20results%20than%0Aexperienced%20dentists%20in%20situations%20where%20conventional%20imaging%20is%20unavailable.%0ADentVFM%20sets%20a%20new%20paradigm%20for%20dental%20AI%2C%20offering%20a%20scalable%2C%20adaptable%2C%20and%0Alabel-efficient%20model%20to%20improve%20intelligent%20dental%20healthcare%20and%20address%0Acritical%20gaps%20in%20global%20oral%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14532v1&entry.124074799=Read"},
{"title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for\n  Autonomous Driving", "author": "Ziyue Zhu and Zhanqian Wu and Zhenxin Zhu and Lijun Zhou and Haiyang Sun and Bing Wan and Kun Ma and Guang Chen and Hangjun Ye and Jin Xie and jian Yang", "abstract": "  Recent advances in driving-scene generation and reconstruction have\ndemonstrated significant potential for enhancing autonomous driving systems by\nproducing scalable and controllable training data. Existing generation methods\nprimarily focus on synthesizing diverse and high-fidelity driving videos;\nhowever, due to limited 3D consistency and sparse viewpoint coverage, they\nstruggle to support convenient and high-quality novel-view synthesis (NVS).\nConversely, recent 3D/4D reconstruction approaches have significantly improved\nNVS for real-world driving scenes, yet inherently lack generative capabilities.\nTo overcome this dilemma between scene generation and reconstruction, we\npropose WorldSplat, a novel feed-forward framework for 4D driving-scene\ngeneration. Our approach effectively generates consistent multi-track videos\nthrough two key steps: (i) We introduce a 4D-aware latent diffusion model\nintegrating multi-modal information to produce pixel-aligned 4D Gaussians in a\nfeed-forward manner. (ii) Subsequently, we refine the novel view videos\nrendered from these Gaussians using a enhanced video diffusion model. Extensive\nexperiments conducted on benchmark datasets demonstrate that WorldSplat\neffectively generates high-fidelity, temporally and spatially consistent\nmulti-track novel view driving videos. Project:\nhttps://wm-research.github.io/worldsplat/\n", "link": "http://arxiv.org/abs/2509.23402v2", "date": "2025-10-16", "relevancy": 2.6751, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.7186}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6841}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldSplat%3A%20Gaussian-Centric%20Feed-Forward%204D%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20WorldSplat%3A%20Gaussian-Centric%20Feed-Forward%204D%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Ziyue%20Zhu%20and%20Zhanqian%20Wu%20and%20Zhenxin%20Zhu%20and%20Lijun%20Zhou%20and%20Haiyang%20Sun%20and%20Bing%20Wan%20and%20Kun%20Ma%20and%20Guang%20Chen%20and%20Hangjun%20Ye%20and%20Jin%20Xie%20and%20jian%20Yang%0AAbstract%3A%20%20%20Recent%20advances%20in%20driving-scene%20generation%20and%20reconstruction%20have%0Ademonstrated%20significant%20potential%20for%20enhancing%20autonomous%20driving%20systems%20by%0Aproducing%20scalable%20and%20controllable%20training%20data.%20Existing%20generation%20methods%0Aprimarily%20focus%20on%20synthesizing%20diverse%20and%20high-fidelity%20driving%20videos%3B%0Ahowever%2C%20due%20to%20limited%203D%20consistency%20and%20sparse%20viewpoint%20coverage%2C%20they%0Astruggle%20to%20support%20convenient%20and%20high-quality%20novel-view%20synthesis%20%28NVS%29.%0AConversely%2C%20recent%203D/4D%20reconstruction%20approaches%20have%20significantly%20improved%0ANVS%20for%20real-world%20driving%20scenes%2C%20yet%20inherently%20lack%20generative%20capabilities.%0ATo%20overcome%20this%20dilemma%20between%20scene%20generation%20and%20reconstruction%2C%20we%0Apropose%20WorldSplat%2C%20a%20novel%20feed-forward%20framework%20for%204D%20driving-scene%0Ageneration.%20Our%20approach%20effectively%20generates%20consistent%20multi-track%20videos%0Athrough%20two%20key%20steps%3A%20%28i%29%20We%20introduce%20a%204D-aware%20latent%20diffusion%20model%0Aintegrating%20multi-modal%20information%20to%20produce%20pixel-aligned%204D%20Gaussians%20in%20a%0Afeed-forward%20manner.%20%28ii%29%20Subsequently%2C%20we%20refine%20the%20novel%20view%20videos%0Arendered%20from%20these%20Gaussians%20using%20a%20enhanced%20video%20diffusion%20model.%20Extensive%0Aexperiments%20conducted%20on%20benchmark%20datasets%20demonstrate%20that%20WorldSplat%0Aeffectively%20generates%20high-fidelity%2C%20temporally%20and%20spatially%20consistent%0Amulti-track%20novel%20view%20driving%20videos.%20Project%3A%0Ahttps%3A//wm-research.github.io/worldsplat/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldSplat%253A%2520Gaussian-Centric%2520Feed-Forward%25204D%2520Scene%2520Generation%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DZiyue%2520Zhu%2520and%2520Zhanqian%2520Wu%2520and%2520Zhenxin%2520Zhu%2520and%2520Lijun%2520Zhou%2520and%2520Haiyang%2520Sun%2520and%2520Bing%2520Wan%2520and%2520Kun%2520Ma%2520and%2520Guang%2520Chen%2520and%2520Hangjun%2520Ye%2520and%2520Jin%2520Xie%2520and%2520jian%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520driving-scene%2520generation%2520and%2520reconstruction%2520have%250Ademonstrated%2520significant%2520potential%2520for%2520enhancing%2520autonomous%2520driving%2520systems%2520by%250Aproducing%2520scalable%2520and%2520controllable%2520training%2520data.%2520Existing%2520generation%2520methods%250Aprimarily%2520focus%2520on%2520synthesizing%2520diverse%2520and%2520high-fidelity%2520driving%2520videos%253B%250Ahowever%252C%2520due%2520to%2520limited%25203D%2520consistency%2520and%2520sparse%2520viewpoint%2520coverage%252C%2520they%250Astruggle%2520to%2520support%2520convenient%2520and%2520high-quality%2520novel-view%2520synthesis%2520%2528NVS%2529.%250AConversely%252C%2520recent%25203D/4D%2520reconstruction%2520approaches%2520have%2520significantly%2520improved%250ANVS%2520for%2520real-world%2520driving%2520scenes%252C%2520yet%2520inherently%2520lack%2520generative%2520capabilities.%250ATo%2520overcome%2520this%2520dilemma%2520between%2520scene%2520generation%2520and%2520reconstruction%252C%2520we%250Apropose%2520WorldSplat%252C%2520a%2520novel%2520feed-forward%2520framework%2520for%25204D%2520driving-scene%250Ageneration.%2520Our%2520approach%2520effectively%2520generates%2520consistent%2520multi-track%2520videos%250Athrough%2520two%2520key%2520steps%253A%2520%2528i%2529%2520We%2520introduce%2520a%25204D-aware%2520latent%2520diffusion%2520model%250Aintegrating%2520multi-modal%2520information%2520to%2520produce%2520pixel-aligned%25204D%2520Gaussians%2520in%2520a%250Afeed-forward%2520manner.%2520%2528ii%2529%2520Subsequently%252C%2520we%2520refine%2520the%2520novel%2520view%2520videos%250Arendered%2520from%2520these%2520Gaussians%2520using%2520a%2520enhanced%2520video%2520diffusion%2520model.%2520Extensive%250Aexperiments%2520conducted%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520WorldSplat%250Aeffectively%2520generates%2520high-fidelity%252C%2520temporally%2520and%2520spatially%2520consistent%250Amulti-track%2520novel%2520view%2520driving%2520videos.%2520Project%253A%250Ahttps%253A//wm-research.github.io/worldsplat/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldSplat%3A%20Gaussian-Centric%20Feed-Forward%204D%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving&entry.906535625=Ziyue%20Zhu%20and%20Zhanqian%20Wu%20and%20Zhenxin%20Zhu%20and%20Lijun%20Zhou%20and%20Haiyang%20Sun%20and%20Bing%20Wan%20and%20Kun%20Ma%20and%20Guang%20Chen%20and%20Hangjun%20Ye%20and%20Jin%20Xie%20and%20jian%20Yang&entry.1292438233=%20%20Recent%20advances%20in%20driving-scene%20generation%20and%20reconstruction%20have%0Ademonstrated%20significant%20potential%20for%20enhancing%20autonomous%20driving%20systems%20by%0Aproducing%20scalable%20and%20controllable%20training%20data.%20Existing%20generation%20methods%0Aprimarily%20focus%20on%20synthesizing%20diverse%20and%20high-fidelity%20driving%20videos%3B%0Ahowever%2C%20due%20to%20limited%203D%20consistency%20and%20sparse%20viewpoint%20coverage%2C%20they%0Astruggle%20to%20support%20convenient%20and%20high-quality%20novel-view%20synthesis%20%28NVS%29.%0AConversely%2C%20recent%203D/4D%20reconstruction%20approaches%20have%20significantly%20improved%0ANVS%20for%20real-world%20driving%20scenes%2C%20yet%20inherently%20lack%20generative%20capabilities.%0ATo%20overcome%20this%20dilemma%20between%20scene%20generation%20and%20reconstruction%2C%20we%0Apropose%20WorldSplat%2C%20a%20novel%20feed-forward%20framework%20for%204D%20driving-scene%0Ageneration.%20Our%20approach%20effectively%20generates%20consistent%20multi-track%20videos%0Athrough%20two%20key%20steps%3A%20%28i%29%20We%20introduce%20a%204D-aware%20latent%20diffusion%20model%0Aintegrating%20multi-modal%20information%20to%20produce%20pixel-aligned%204D%20Gaussians%20in%20a%0Afeed-forward%20manner.%20%28ii%29%20Subsequently%2C%20we%20refine%20the%20novel%20view%20videos%0Arendered%20from%20these%20Gaussians%20using%20a%20enhanced%20video%20diffusion%20model.%20Extensive%0Aexperiments%20conducted%20on%20benchmark%20datasets%20demonstrate%20that%20WorldSplat%0Aeffectively%20generates%20high-fidelity%2C%20temporally%20and%20spatially%20consistent%0Amulti-track%20novel%20view%20driving%20videos.%20Project%3A%0Ahttps%3A//wm-research.github.io/worldsplat/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23402v2&entry.124074799=Read"},
{"title": "SCENEFORGE: Enhancing 3D-text alignment with Structured Scene\n  Compositions", "author": "Cristian Sbrolli and Matteo Matteucci", "abstract": "  The whole is greater than the sum of its parts-even in 3D-text contrastive\nlearning. We introduce SceneForge, a novel framework that enhances contrastive\nalignment between 3D point clouds and text through structured multi-object\nscene compositions. SceneForge leverages individual 3D shapes to construct\nmulti-object scenes with explicit spatial relations, pairing them with coherent\nmulti-object descriptions refined by a large language model. By augmenting\ncontrastive training with these structured, compositional samples, SceneForge\neffectively addresses the scarcity of large-scale 3D-text datasets,\nsignificantly enriching data complexity and diversity. We systematically\ninvestigate critical design elements, such as the optimal number of objects per\nscene, the proportion of compositional samples in training batches, and scene\nconstruction strategies. Extensive experiments demonstrate that SceneForge\ndelivers substantial performance gains across multiple tasks, including\nzero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,\nas well as few-shot part segmentation on ShapeNetPart. SceneForge's\ncompositional augmentations are model-agnostic, consistently improving\nperformance across multiple encoder architectures. Moreover, SceneForge\nimproves 3D visual question answering on ScanQA, generalizes robustly to\nretrieval scenarios with increasing scene complexity, and showcases spatial\nreasoning capabilities by adapting spatial configurations to align precisely\nwith textual instructions.\n", "link": "http://arxiv.org/abs/2509.15693v2", "date": "2025-10-16", "relevancy": 2.6744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6809}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCENEFORGE%3A%20Enhancing%203D-text%20alignment%20with%20Structured%20Scene%0A%20%20Compositions&body=Title%3A%20SCENEFORGE%3A%20Enhancing%203D-text%20alignment%20with%20Structured%20Scene%0A%20%20Compositions%0AAuthor%3A%20Cristian%20Sbrolli%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20The%20whole%20is%20greater%20than%20the%20sum%20of%20its%20parts-even%20in%203D-text%20contrastive%0Alearning.%20We%20introduce%20SceneForge%2C%20a%20novel%20framework%20that%20enhances%20contrastive%0Aalignment%20between%203D%20point%20clouds%20and%20text%20through%20structured%20multi-object%0Ascene%20compositions.%20SceneForge%20leverages%20individual%203D%20shapes%20to%20construct%0Amulti-object%20scenes%20with%20explicit%20spatial%20relations%2C%20pairing%20them%20with%20coherent%0Amulti-object%20descriptions%20refined%20by%20a%20large%20language%20model.%20By%20augmenting%0Acontrastive%20training%20with%20these%20structured%2C%20compositional%20samples%2C%20SceneForge%0Aeffectively%20addresses%20the%20scarcity%20of%20large-scale%203D-text%20datasets%2C%0Asignificantly%20enriching%20data%20complexity%20and%20diversity.%20We%20systematically%0Ainvestigate%20critical%20design%20elements%2C%20such%20as%20the%20optimal%20number%20of%20objects%20per%0Ascene%2C%20the%20proportion%20of%20compositional%20samples%20in%20training%20batches%2C%20and%20scene%0Aconstruction%20strategies.%20Extensive%20experiments%20demonstrate%20that%20SceneForge%0Adelivers%20substantial%20performance%20gains%20across%20multiple%20tasks%2C%20including%0Azero-shot%20classification%20on%20ModelNet%2C%20ScanObjNN%2C%20Objaverse-LVIS%2C%20and%20ScanNet%2C%0Aas%20well%20as%20few-shot%20part%20segmentation%20on%20ShapeNetPart.%20SceneForge%27s%0Acompositional%20augmentations%20are%20model-agnostic%2C%20consistently%20improving%0Aperformance%20across%20multiple%20encoder%20architectures.%20Moreover%2C%20SceneForge%0Aimproves%203D%20visual%20question%20answering%20on%20ScanQA%2C%20generalizes%20robustly%20to%0Aretrieval%20scenarios%20with%20increasing%20scene%20complexity%2C%20and%20showcases%20spatial%0Areasoning%20capabilities%20by%20adapting%20spatial%20configurations%20to%20align%20precisely%0Awith%20textual%20instructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCENEFORGE%253A%2520Enhancing%25203D-text%2520alignment%2520with%2520Structured%2520Scene%250A%2520%2520Compositions%26entry.906535625%3DCristian%2520Sbrolli%2520and%2520Matteo%2520Matteucci%26entry.1292438233%3D%2520%2520The%2520whole%2520is%2520greater%2520than%2520the%2520sum%2520of%2520its%2520parts-even%2520in%25203D-text%2520contrastive%250Alearning.%2520We%2520introduce%2520SceneForge%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520contrastive%250Aalignment%2520between%25203D%2520point%2520clouds%2520and%2520text%2520through%2520structured%2520multi-object%250Ascene%2520compositions.%2520SceneForge%2520leverages%2520individual%25203D%2520shapes%2520to%2520construct%250Amulti-object%2520scenes%2520with%2520explicit%2520spatial%2520relations%252C%2520pairing%2520them%2520with%2520coherent%250Amulti-object%2520descriptions%2520refined%2520by%2520a%2520large%2520language%2520model.%2520By%2520augmenting%250Acontrastive%2520training%2520with%2520these%2520structured%252C%2520compositional%2520samples%252C%2520SceneForge%250Aeffectively%2520addresses%2520the%2520scarcity%2520of%2520large-scale%25203D-text%2520datasets%252C%250Asignificantly%2520enriching%2520data%2520complexity%2520and%2520diversity.%2520We%2520systematically%250Ainvestigate%2520critical%2520design%2520elements%252C%2520such%2520as%2520the%2520optimal%2520number%2520of%2520objects%2520per%250Ascene%252C%2520the%2520proportion%2520of%2520compositional%2520samples%2520in%2520training%2520batches%252C%2520and%2520scene%250Aconstruction%2520strategies.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SceneForge%250Adelivers%2520substantial%2520performance%2520gains%2520across%2520multiple%2520tasks%252C%2520including%250Azero-shot%2520classification%2520on%2520ModelNet%252C%2520ScanObjNN%252C%2520Objaverse-LVIS%252C%2520and%2520ScanNet%252C%250Aas%2520well%2520as%2520few-shot%2520part%2520segmentation%2520on%2520ShapeNetPart.%2520SceneForge%2527s%250Acompositional%2520augmentations%2520are%2520model-agnostic%252C%2520consistently%2520improving%250Aperformance%2520across%2520multiple%2520encoder%2520architectures.%2520Moreover%252C%2520SceneForge%250Aimproves%25203D%2520visual%2520question%2520answering%2520on%2520ScanQA%252C%2520generalizes%2520robustly%2520to%250Aretrieval%2520scenarios%2520with%2520increasing%2520scene%2520complexity%252C%2520and%2520showcases%2520spatial%250Areasoning%2520capabilities%2520by%2520adapting%2520spatial%2520configurations%2520to%2520align%2520precisely%250Awith%2520textual%2520instructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCENEFORGE%3A%20Enhancing%203D-text%20alignment%20with%20Structured%20Scene%0A%20%20Compositions&entry.906535625=Cristian%20Sbrolli%20and%20Matteo%20Matteucci&entry.1292438233=%20%20The%20whole%20is%20greater%20than%20the%20sum%20of%20its%20parts-even%20in%203D-text%20contrastive%0Alearning.%20We%20introduce%20SceneForge%2C%20a%20novel%20framework%20that%20enhances%20contrastive%0Aalignment%20between%203D%20point%20clouds%20and%20text%20through%20structured%20multi-object%0Ascene%20compositions.%20SceneForge%20leverages%20individual%203D%20shapes%20to%20construct%0Amulti-object%20scenes%20with%20explicit%20spatial%20relations%2C%20pairing%20them%20with%20coherent%0Amulti-object%20descriptions%20refined%20by%20a%20large%20language%20model.%20By%20augmenting%0Acontrastive%20training%20with%20these%20structured%2C%20compositional%20samples%2C%20SceneForge%0Aeffectively%20addresses%20the%20scarcity%20of%20large-scale%203D-text%20datasets%2C%0Asignificantly%20enriching%20data%20complexity%20and%20diversity.%20We%20systematically%0Ainvestigate%20critical%20design%20elements%2C%20such%20as%20the%20optimal%20number%20of%20objects%20per%0Ascene%2C%20the%20proportion%20of%20compositional%20samples%20in%20training%20batches%2C%20and%20scene%0Aconstruction%20strategies.%20Extensive%20experiments%20demonstrate%20that%20SceneForge%0Adelivers%20substantial%20performance%20gains%20across%20multiple%20tasks%2C%20including%0Azero-shot%20classification%20on%20ModelNet%2C%20ScanObjNN%2C%20Objaverse-LVIS%2C%20and%20ScanNet%2C%0Aas%20well%20as%20few-shot%20part%20segmentation%20on%20ShapeNetPart.%20SceneForge%27s%0Acompositional%20augmentations%20are%20model-agnostic%2C%20consistently%20improving%0Aperformance%20across%20multiple%20encoder%20architectures.%20Moreover%2C%20SceneForge%0Aimproves%203D%20visual%20question%20answering%20on%20ScanQA%2C%20generalizes%20robustly%20to%0Aretrieval%20scenarios%20with%20increasing%20scene%20complexity%2C%20and%20showcases%20spatial%0Areasoning%20capabilities%20by%20adapting%20spatial%20configurations%20to%20align%20precisely%0Awith%20textual%20instructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15693v2&entry.124074799=Read"},
{"title": "TABSurfer: a Hybrid Deep Learning Architecture for Subcortical\n  Segmentation", "author": "Aaron Cao and Vishwanatha M. Rao and Kejia Liu and Xinrui Liu and Andrew F. Laine and Jia Guo", "abstract": "  Subcortical segmentation remains challenging despite its important\napplications in quantitative structural analysis of brain MRI scans. The most\naccurate method, manual segmentation, is highly labor intensive, so automated\ntools like FreeSurfer have been adopted to handle this task. However, these\ntraditional pipelines are slow and inefficient for processing large datasets.\nIn this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformer\nhybrid deep learning model designed for superior subcortical segmentation\ncompared to existing state-of-the-art tools. To evaluate, we first demonstrate\nTABSurfer's consistent performance across various T1w MRI datasets with\nsignificantly shorter processing times compared to FreeSurfer. Then, we\nvalidate against manual segmentations, where TABSurfer outperforms FreeSurfer\nbased on the manual ground truth. In each test, we also establish TABSurfer's\nadvantage over a leading deep learning benchmark, FastSurferVINN. Together,\nthese studies highlight TABSurfer's utility as a powerful tool for fully\nautomated subcortical segmentation with high fidelity.\n", "link": "http://arxiv.org/abs/2312.08267v2", "date": "2025-10-16", "relevancy": 2.6673, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5632}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5197}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TABSurfer%3A%20a%20Hybrid%20Deep%20Learning%20Architecture%20for%20Subcortical%0A%20%20Segmentation&body=Title%3A%20TABSurfer%3A%20a%20Hybrid%20Deep%20Learning%20Architecture%20for%20Subcortical%0A%20%20Segmentation%0AAuthor%3A%20Aaron%20Cao%20and%20Vishwanatha%20M.%20Rao%20and%20Kejia%20Liu%20and%20Xinrui%20Liu%20and%20Andrew%20F.%20Laine%20and%20Jia%20Guo%0AAbstract%3A%20%20%20Subcortical%20segmentation%20remains%20challenging%20despite%20its%20important%0Aapplications%20in%20quantitative%20structural%20analysis%20of%20brain%20MRI%20scans.%20The%20most%0Aaccurate%20method%2C%20manual%20segmentation%2C%20is%20highly%20labor%20intensive%2C%20so%20automated%0Atools%20like%20FreeSurfer%20have%20been%20adopted%20to%20handle%20this%20task.%20However%2C%20these%0Atraditional%20pipelines%20are%20slow%20and%20inefficient%20for%20processing%20large%20datasets.%0AIn%20this%20study%2C%20we%20propose%20TABSurfer%2C%20a%20novel%203D%20patch-based%20CNN-Transformer%0Ahybrid%20deep%20learning%20model%20designed%20for%20superior%20subcortical%20segmentation%0Acompared%20to%20existing%20state-of-the-art%20tools.%20To%20evaluate%2C%20we%20first%20demonstrate%0ATABSurfer%27s%20consistent%20performance%20across%20various%20T1w%20MRI%20datasets%20with%0Asignificantly%20shorter%20processing%20times%20compared%20to%20FreeSurfer.%20Then%2C%20we%0Avalidate%20against%20manual%20segmentations%2C%20where%20TABSurfer%20outperforms%20FreeSurfer%0Abased%20on%20the%20manual%20ground%20truth.%20In%20each%20test%2C%20we%20also%20establish%20TABSurfer%27s%0Aadvantage%20over%20a%20leading%20deep%20learning%20benchmark%2C%20FastSurferVINN.%20Together%2C%0Athese%20studies%20highlight%20TABSurfer%27s%20utility%20as%20a%20powerful%20tool%20for%20fully%0Aautomated%20subcortical%20segmentation%20with%20high%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTABSurfer%253A%2520a%2520Hybrid%2520Deep%2520Learning%2520Architecture%2520for%2520Subcortical%250A%2520%2520Segmentation%26entry.906535625%3DAaron%2520Cao%2520and%2520Vishwanatha%2520M.%2520Rao%2520and%2520Kejia%2520Liu%2520and%2520Xinrui%2520Liu%2520and%2520Andrew%2520F.%2520Laine%2520and%2520Jia%2520Guo%26entry.1292438233%3D%2520%2520Subcortical%2520segmentation%2520remains%2520challenging%2520despite%2520its%2520important%250Aapplications%2520in%2520quantitative%2520structural%2520analysis%2520of%2520brain%2520MRI%2520scans.%2520The%2520most%250Aaccurate%2520method%252C%2520manual%2520segmentation%252C%2520is%2520highly%2520labor%2520intensive%252C%2520so%2520automated%250Atools%2520like%2520FreeSurfer%2520have%2520been%2520adopted%2520to%2520handle%2520this%2520task.%2520However%252C%2520these%250Atraditional%2520pipelines%2520are%2520slow%2520and%2520inefficient%2520for%2520processing%2520large%2520datasets.%250AIn%2520this%2520study%252C%2520we%2520propose%2520TABSurfer%252C%2520a%2520novel%25203D%2520patch-based%2520CNN-Transformer%250Ahybrid%2520deep%2520learning%2520model%2520designed%2520for%2520superior%2520subcortical%2520segmentation%250Acompared%2520to%2520existing%2520state-of-the-art%2520tools.%2520To%2520evaluate%252C%2520we%2520first%2520demonstrate%250ATABSurfer%2527s%2520consistent%2520performance%2520across%2520various%2520T1w%2520MRI%2520datasets%2520with%250Asignificantly%2520shorter%2520processing%2520times%2520compared%2520to%2520FreeSurfer.%2520Then%252C%2520we%250Avalidate%2520against%2520manual%2520segmentations%252C%2520where%2520TABSurfer%2520outperforms%2520FreeSurfer%250Abased%2520on%2520the%2520manual%2520ground%2520truth.%2520In%2520each%2520test%252C%2520we%2520also%2520establish%2520TABSurfer%2527s%250Aadvantage%2520over%2520a%2520leading%2520deep%2520learning%2520benchmark%252C%2520FastSurferVINN.%2520Together%252C%250Athese%2520studies%2520highlight%2520TABSurfer%2527s%2520utility%2520as%2520a%2520powerful%2520tool%2520for%2520fully%250Aautomated%2520subcortical%2520segmentation%2520with%2520high%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TABSurfer%3A%20a%20Hybrid%20Deep%20Learning%20Architecture%20for%20Subcortical%0A%20%20Segmentation&entry.906535625=Aaron%20Cao%20and%20Vishwanatha%20M.%20Rao%20and%20Kejia%20Liu%20and%20Xinrui%20Liu%20and%20Andrew%20F.%20Laine%20and%20Jia%20Guo&entry.1292438233=%20%20Subcortical%20segmentation%20remains%20challenging%20despite%20its%20important%0Aapplications%20in%20quantitative%20structural%20analysis%20of%20brain%20MRI%20scans.%20The%20most%0Aaccurate%20method%2C%20manual%20segmentation%2C%20is%20highly%20labor%20intensive%2C%20so%20automated%0Atools%20like%20FreeSurfer%20have%20been%20adopted%20to%20handle%20this%20task.%20However%2C%20these%0Atraditional%20pipelines%20are%20slow%20and%20inefficient%20for%20processing%20large%20datasets.%0AIn%20this%20study%2C%20we%20propose%20TABSurfer%2C%20a%20novel%203D%20patch-based%20CNN-Transformer%0Ahybrid%20deep%20learning%20model%20designed%20for%20superior%20subcortical%20segmentation%0Acompared%20to%20existing%20state-of-the-art%20tools.%20To%20evaluate%2C%20we%20first%20demonstrate%0ATABSurfer%27s%20consistent%20performance%20across%20various%20T1w%20MRI%20datasets%20with%0Asignificantly%20shorter%20processing%20times%20compared%20to%20FreeSurfer.%20Then%2C%20we%0Avalidate%20against%20manual%20segmentations%2C%20where%20TABSurfer%20outperforms%20FreeSurfer%0Abased%20on%20the%20manual%20ground%20truth.%20In%20each%20test%2C%20we%20also%20establish%20TABSurfer%27s%0Aadvantage%20over%20a%20leading%20deep%20learning%20benchmark%2C%20FastSurferVINN.%20Together%2C%0Athese%20studies%20highlight%20TABSurfer%27s%20utility%20as%20a%20powerful%20tool%20for%20fully%0Aautomated%20subcortical%20segmentation%20with%20high%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08267v2&entry.124074799=Read"},
{"title": "Preservation of Language Understanding Capabilities in Speech-aware\n  Large Language Models", "author": "Marek Kubis and Pawe\u0142 Sk\u00f3rzewski and Iwona Christop and Mateusz Czy\u017cnikiewicz and Jakub Kubiak and \u0141ukasz Bondaruk and Marcin Lewandowski", "abstract": "  The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.\n", "link": "http://arxiv.org/abs/2509.12171v2", "date": "2025-10-16", "relevancy": 2.6563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preservation%20of%20Language%20Understanding%20Capabilities%20in%20Speech-aware%0A%20%20Large%20Language%20Models&body=Title%3A%20Preservation%20of%20Language%20Understanding%20Capabilities%20in%20Speech-aware%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Marek%20Kubis%20and%20Pawe%C5%82%20Sk%C3%B3rzewski%20and%20Iwona%20Christop%20and%20Mateusz%20Czy%C5%BCnikiewicz%20and%20Jakub%20Kubiak%20and%20%C5%81ukasz%20Bondaruk%20and%20Marcin%20Lewandowski%0AAbstract%3A%20%20%20The%20paper%20presents%20C3T%20%28Cross-modal%20Capabilities%20Conservation%20Test%29%2C%20a%20new%0Abenchmark%20for%20assessing%20the%20performance%20of%20speech-aware%20large%20language%20models.%0AThe%20benchmark%20utilizes%20textual%20tasks%20and%20a%20voice%20cloning%20text-to-speech%20model%0Ato%20quantify%20the%20extent%20to%20which%20language%20understanding%20capabilities%20are%0Apreserved%20when%20the%20model%20is%20accessed%20via%20speech%20input.%20C3T%20quantifies%20the%0Afairness%20of%20the%20model%20for%20different%20categories%20of%20speakers%20and%20its%20robustness%0Aacross%20text%20and%20speech%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreservation%2520of%2520Language%2520Understanding%2520Capabilities%2520in%2520Speech-aware%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DMarek%2520Kubis%2520and%2520Pawe%25C5%2582%2520Sk%25C3%25B3rzewski%2520and%2520Iwona%2520Christop%2520and%2520Mateusz%2520Czy%25C5%25BCnikiewicz%2520and%2520Jakub%2520Kubiak%2520and%2520%25C5%2581ukasz%2520Bondaruk%2520and%2520Marcin%2520Lewandowski%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520C3T%2520%2528Cross-modal%2520Capabilities%2520Conservation%2520Test%2529%252C%2520a%2520new%250Abenchmark%2520for%2520assessing%2520the%2520performance%2520of%2520speech-aware%2520large%2520language%2520models.%250AThe%2520benchmark%2520utilizes%2520textual%2520tasks%2520and%2520a%2520voice%2520cloning%2520text-to-speech%2520model%250Ato%2520quantify%2520the%2520extent%2520to%2520which%2520language%2520understanding%2520capabilities%2520are%250Apreserved%2520when%2520the%2520model%2520is%2520accessed%2520via%2520speech%2520input.%2520C3T%2520quantifies%2520the%250Afairness%2520of%2520the%2520model%2520for%2520different%2520categories%2520of%2520speakers%2520and%2520its%2520robustness%250Aacross%2520text%2520and%2520speech%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preservation%20of%20Language%20Understanding%20Capabilities%20in%20Speech-aware%0A%20%20Large%20Language%20Models&entry.906535625=Marek%20Kubis%20and%20Pawe%C5%82%20Sk%C3%B3rzewski%20and%20Iwona%20Christop%20and%20Mateusz%20Czy%C5%BCnikiewicz%20and%20Jakub%20Kubiak%20and%20%C5%81ukasz%20Bondaruk%20and%20Marcin%20Lewandowski&entry.1292438233=%20%20The%20paper%20presents%20C3T%20%28Cross-modal%20Capabilities%20Conservation%20Test%29%2C%20a%20new%0Abenchmark%20for%20assessing%20the%20performance%20of%20speech-aware%20large%20language%20models.%0AThe%20benchmark%20utilizes%20textual%20tasks%20and%20a%20voice%20cloning%20text-to-speech%20model%0Ato%20quantify%20the%20extent%20to%20which%20language%20understanding%20capabilities%20are%0Apreserved%20when%20the%20model%20is%20accessed%20via%20speech%20input.%20C3T%20quantifies%20the%0Afairness%20of%20the%20model%20for%20different%20categories%20of%20speakers%20and%20its%20robustness%0Aacross%20text%20and%20speech%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12171v2&entry.124074799=Read"},
{"title": "Shape of Motion: 4D Reconstruction from a Single Video", "author": "Qianqian Wang and Vickie Ye and Hang Gao and Weijia Zeng and Jake Austin and Zhengqi Li and Angjoo Kanazawa", "abstract": "  Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches\ndepend on templates, are effective only in quasi-static scenes, or fail to\nmodel 3D motion explicitly. We introduce a method for reconstructing generic\ndynamic scenes, featuring explicit, persistent 3D motion trajectories in the\nworld coordinate frame, from casually captured monocular videos. We tackle the\nproblem with two key insights: First, we exploit the low-dimensional structure\nof 3D motion by representing scene motion with a compact set of SE(3) motion\nbases. Each point's motion is expressed as a linear combination of these bases,\nfacilitating soft decomposition of the scene into multiple rigidly-moving\ngroups. Second, we take advantage of off-the-shelf data-driven priors such as\nmonocular depth maps and long-range 2D tracks, and devise a method to\neffectively consolidate these noisy supervisory signals, resulting in a\nglobally consistent representation of the dynamic scene. Experiments show that\nour method achieves state-of-the-art performance for both long-range 3D/2D\nmotion estimation and novel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/\n", "link": "http://arxiv.org/abs/2407.13764v2", "date": "2025-10-16", "relevancy": 2.6461, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6653}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6599}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shape%20of%20Motion%3A%204D%20Reconstruction%20from%20a%20Single%20Video&body=Title%3A%20Shape%20of%20Motion%3A%204D%20Reconstruction%20from%20a%20Single%20Video%0AAuthor%3A%20Qianqian%20Wang%20and%20Vickie%20Ye%20and%20Hang%20Gao%20and%20Weijia%20Zeng%20and%20Jake%20Austin%20and%20Zhengqi%20Li%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20Monocular%20dynamic%20reconstruction%20is%20a%20challenging%20and%20long-standing%20vision%0Aproblem%20due%20to%20the%20highly%20ill-posed%20nature%20of%20the%20task.%20Existing%20approaches%0Adepend%20on%20templates%2C%20are%20effective%20only%20in%20quasi-static%20scenes%2C%20or%20fail%20to%0Amodel%203D%20motion%20explicitly.%20We%20introduce%20a%20method%20for%20reconstructing%20generic%0Adynamic%20scenes%2C%20featuring%20explicit%2C%20persistent%203D%20motion%20trajectories%20in%20the%0Aworld%20coordinate%20frame%2C%20from%20casually%20captured%20monocular%20videos.%20We%20tackle%20the%0Aproblem%20with%20two%20key%20insights%3A%20First%2C%20we%20exploit%20the%20low-dimensional%20structure%0Aof%203D%20motion%20by%20representing%20scene%20motion%20with%20a%20compact%20set%20of%20SE%283%29%20motion%0Abases.%20Each%20point%27s%20motion%20is%20expressed%20as%20a%20linear%20combination%20of%20these%20bases%2C%0Afacilitating%20soft%20decomposition%20of%20the%20scene%20into%20multiple%20rigidly-moving%0Agroups.%20Second%2C%20we%20take%20advantage%20of%20off-the-shelf%20data-driven%20priors%20such%20as%0Amonocular%20depth%20maps%20and%20long-range%202D%20tracks%2C%20and%20devise%20a%20method%20to%0Aeffectively%20consolidate%20these%20noisy%20supervisory%20signals%2C%20resulting%20in%20a%0Aglobally%20consistent%20representation%20of%20the%20dynamic%20scene.%20Experiments%20show%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20for%20both%20long-range%203D/2D%0Amotion%20estimation%20and%20novel%20view%20synthesis%20on%20dynamic%20scenes.%20Project%20Page%3A%0Ahttps%3A//shape-of-motion.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13764v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShape%2520of%2520Motion%253A%25204D%2520Reconstruction%2520from%2520a%2520Single%2520Video%26entry.906535625%3DQianqian%2520Wang%2520and%2520Vickie%2520Ye%2520and%2520Hang%2520Gao%2520and%2520Weijia%2520Zeng%2520and%2520Jake%2520Austin%2520and%2520Zhengqi%2520Li%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520Monocular%2520dynamic%2520reconstruction%2520is%2520a%2520challenging%2520and%2520long-standing%2520vision%250Aproblem%2520due%2520to%2520the%2520highly%2520ill-posed%2520nature%2520of%2520the%2520task.%2520Existing%2520approaches%250Adepend%2520on%2520templates%252C%2520are%2520effective%2520only%2520in%2520quasi-static%2520scenes%252C%2520or%2520fail%2520to%250Amodel%25203D%2520motion%2520explicitly.%2520We%2520introduce%2520a%2520method%2520for%2520reconstructing%2520generic%250Adynamic%2520scenes%252C%2520featuring%2520explicit%252C%2520persistent%25203D%2520motion%2520trajectories%2520in%2520the%250Aworld%2520coordinate%2520frame%252C%2520from%2520casually%2520captured%2520monocular%2520videos.%2520We%2520tackle%2520the%250Aproblem%2520with%2520two%2520key%2520insights%253A%2520First%252C%2520we%2520exploit%2520the%2520low-dimensional%2520structure%250Aof%25203D%2520motion%2520by%2520representing%2520scene%2520motion%2520with%2520a%2520compact%2520set%2520of%2520SE%25283%2529%2520motion%250Abases.%2520Each%2520point%2527s%2520motion%2520is%2520expressed%2520as%2520a%2520linear%2520combination%2520of%2520these%2520bases%252C%250Afacilitating%2520soft%2520decomposition%2520of%2520the%2520scene%2520into%2520multiple%2520rigidly-moving%250Agroups.%2520Second%252C%2520we%2520take%2520advantage%2520of%2520off-the-shelf%2520data-driven%2520priors%2520such%2520as%250Amonocular%2520depth%2520maps%2520and%2520long-range%25202D%2520tracks%252C%2520and%2520devise%2520a%2520method%2520to%250Aeffectively%2520consolidate%2520these%2520noisy%2520supervisory%2520signals%252C%2520resulting%2520in%2520a%250Aglobally%2520consistent%2520representation%2520of%2520the%2520dynamic%2520scene.%2520Experiments%2520show%2520that%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520for%2520both%2520long-range%25203D/2D%250Amotion%2520estimation%2520and%2520novel%2520view%2520synthesis%2520on%2520dynamic%2520scenes.%2520Project%2520Page%253A%250Ahttps%253A//shape-of-motion.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13764v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape%20of%20Motion%3A%204D%20Reconstruction%20from%20a%20Single%20Video&entry.906535625=Qianqian%20Wang%20and%20Vickie%20Ye%20and%20Hang%20Gao%20and%20Weijia%20Zeng%20and%20Jake%20Austin%20and%20Zhengqi%20Li%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20Monocular%20dynamic%20reconstruction%20is%20a%20challenging%20and%20long-standing%20vision%0Aproblem%20due%20to%20the%20highly%20ill-posed%20nature%20of%20the%20task.%20Existing%20approaches%0Adepend%20on%20templates%2C%20are%20effective%20only%20in%20quasi-static%20scenes%2C%20or%20fail%20to%0Amodel%203D%20motion%20explicitly.%20We%20introduce%20a%20method%20for%20reconstructing%20generic%0Adynamic%20scenes%2C%20featuring%20explicit%2C%20persistent%203D%20motion%20trajectories%20in%20the%0Aworld%20coordinate%20frame%2C%20from%20casually%20captured%20monocular%20videos.%20We%20tackle%20the%0Aproblem%20with%20two%20key%20insights%3A%20First%2C%20we%20exploit%20the%20low-dimensional%20structure%0Aof%203D%20motion%20by%20representing%20scene%20motion%20with%20a%20compact%20set%20of%20SE%283%29%20motion%0Abases.%20Each%20point%27s%20motion%20is%20expressed%20as%20a%20linear%20combination%20of%20these%20bases%2C%0Afacilitating%20soft%20decomposition%20of%20the%20scene%20into%20multiple%20rigidly-moving%0Agroups.%20Second%2C%20we%20take%20advantage%20of%20off-the-shelf%20data-driven%20priors%20such%20as%0Amonocular%20depth%20maps%20and%20long-range%202D%20tracks%2C%20and%20devise%20a%20method%20to%0Aeffectively%20consolidate%20these%20noisy%20supervisory%20signals%2C%20resulting%20in%20a%0Aglobally%20consistent%20representation%20of%20the%20dynamic%20scene.%20Experiments%20show%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20for%20both%20long-range%203D/2D%0Amotion%20estimation%20and%20novel%20view%20synthesis%20on%20dynamic%20scenes.%20Project%20Page%3A%0Ahttps%3A//shape-of-motion.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13764v2&entry.124074799=Read"},
{"title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering", "author": "Nicolas von L\u00fctzow and Matthias Nie\u00dfner", "abstract": "  Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives - octahedra and tetrahedra - both of which define\nhomogeneous volumes bounded by triangular faces. To optimize these primitives,\nwe present a differentiable rasterizer that runs efficiently on GPUs, allowing\nend-to-end gradient-based optimization while maintaining real-time rendering\ncapabilities. Through experiments on real-world datasets, we demonstrate\ncomparable performance to state-of-the-art volumetric methods while requiring\nfewer primitives to achieve similar reconstruction fidelity. Our findings\ndeepen the understanding of 3D representations by providing insights into the\nfidelity and performance characteristics of transparent polyhedra and suggest\nthat adopting novel primitives can expand the available design space.\n", "link": "http://arxiv.org/abs/2501.16312v4", "date": "2025-10-16", "relevancy": 2.6425, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5468}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5232}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering&body=Title%3A%20LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering%0AAuthor%3A%20Nicolas%20von%20L%C3%BCtzow%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20Volumetric%20rendering%20has%20become%20central%20to%20modern%20novel%20view%20synthesis%0Amethods%2C%20which%20use%20differentiable%20rendering%20to%20optimize%203D%20scene%0Arepresentations%20directly%20from%20observed%20views.%20While%20many%20recent%20works%20build%20on%0ANeRF%20or%203D%20Gaussians%2C%20we%20explore%20an%20alternative%20volumetric%20scene%0Arepresentation.%20More%20specifically%2C%20we%20introduce%20two%20new%20scene%20representations%0Abased%20on%20linear%20primitives%20-%20octahedra%20and%20tetrahedra%20-%20both%20of%20which%20define%0Ahomogeneous%20volumes%20bounded%20by%20triangular%20faces.%20To%20optimize%20these%20primitives%2C%0Awe%20present%20a%20differentiable%20rasterizer%20that%20runs%20efficiently%20on%20GPUs%2C%20allowing%0Aend-to-end%20gradient-based%20optimization%20while%20maintaining%20real-time%20rendering%0Acapabilities.%20Through%20experiments%20on%20real-world%20datasets%2C%20we%20demonstrate%0Acomparable%20performance%20to%20state-of-the-art%20volumetric%20methods%20while%20requiring%0Afewer%20primitives%20to%20achieve%20similar%20reconstruction%20fidelity.%20Our%20findings%0Adeepen%20the%20understanding%20of%203D%20representations%20by%20providing%20insights%20into%20the%0Afidelity%20and%20performance%20characteristics%20of%20transparent%20polyhedra%20and%20suggest%0Athat%20adopting%20novel%20primitives%20can%20expand%20the%20available%20design%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16312v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinPrim%253A%2520Linear%2520Primitives%2520for%2520Differentiable%2520Volumetric%2520Rendering%26entry.906535625%3DNicolas%2520von%2520L%25C3%25BCtzow%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520Volumetric%2520rendering%2520has%2520become%2520central%2520to%2520modern%2520novel%2520view%2520synthesis%250Amethods%252C%2520which%2520use%2520differentiable%2520rendering%2520to%2520optimize%25203D%2520scene%250Arepresentations%2520directly%2520from%2520observed%2520views.%2520While%2520many%2520recent%2520works%2520build%2520on%250ANeRF%2520or%25203D%2520Gaussians%252C%2520we%2520explore%2520an%2520alternative%2520volumetric%2520scene%250Arepresentation.%2520More%2520specifically%252C%2520we%2520introduce%2520two%2520new%2520scene%2520representations%250Abased%2520on%2520linear%2520primitives%2520-%2520octahedra%2520and%2520tetrahedra%2520-%2520both%2520of%2520which%2520define%250Ahomogeneous%2520volumes%2520bounded%2520by%2520triangular%2520faces.%2520To%2520optimize%2520these%2520primitives%252C%250Awe%2520present%2520a%2520differentiable%2520rasterizer%2520that%2520runs%2520efficiently%2520on%2520GPUs%252C%2520allowing%250Aend-to-end%2520gradient-based%2520optimization%2520while%2520maintaining%2520real-time%2520rendering%250Acapabilities.%2520Through%2520experiments%2520on%2520real-world%2520datasets%252C%2520we%2520demonstrate%250Acomparable%2520performance%2520to%2520state-of-the-art%2520volumetric%2520methods%2520while%2520requiring%250Afewer%2520primitives%2520to%2520achieve%2520similar%2520reconstruction%2520fidelity.%2520Our%2520findings%250Adeepen%2520the%2520understanding%2520of%25203D%2520representations%2520by%2520providing%2520insights%2520into%2520the%250Afidelity%2520and%2520performance%2520characteristics%2520of%2520transparent%2520polyhedra%2520and%2520suggest%250Athat%2520adopting%2520novel%2520primitives%2520can%2520expand%2520the%2520available%2520design%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16312v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinPrim%3A%20Linear%20Primitives%20for%20Differentiable%20Volumetric%20Rendering&entry.906535625=Nicolas%20von%20L%C3%BCtzow%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20Volumetric%20rendering%20has%20become%20central%20to%20modern%20novel%20view%20synthesis%0Amethods%2C%20which%20use%20differentiable%20rendering%20to%20optimize%203D%20scene%0Arepresentations%20directly%20from%20observed%20views.%20While%20many%20recent%20works%20build%20on%0ANeRF%20or%203D%20Gaussians%2C%20we%20explore%20an%20alternative%20volumetric%20scene%0Arepresentation.%20More%20specifically%2C%20we%20introduce%20two%20new%20scene%20representations%0Abased%20on%20linear%20primitives%20-%20octahedra%20and%20tetrahedra%20-%20both%20of%20which%20define%0Ahomogeneous%20volumes%20bounded%20by%20triangular%20faces.%20To%20optimize%20these%20primitives%2C%0Awe%20present%20a%20differentiable%20rasterizer%20that%20runs%20efficiently%20on%20GPUs%2C%20allowing%0Aend-to-end%20gradient-based%20optimization%20while%20maintaining%20real-time%20rendering%0Acapabilities.%20Through%20experiments%20on%20real-world%20datasets%2C%20we%20demonstrate%0Acomparable%20performance%20to%20state-of-the-art%20volumetric%20methods%20while%20requiring%0Afewer%20primitives%20to%20achieve%20similar%20reconstruction%20fidelity.%20Our%20findings%0Adeepen%20the%20understanding%20of%203D%20representations%20by%20providing%20insights%20into%20the%0Afidelity%20and%20performance%20characteristics%20of%20transparent%20polyhedra%20and%20suggest%0Athat%20adopting%20novel%20primitives%20can%20expand%20the%20available%20design%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16312v4&entry.124074799=Read"},
{"title": "Programmatic Representation Learning with Language Models", "author": "Gabriel Poesia and Georgia Gabriela Sampaio", "abstract": "  Classical models for supervised machine learning, such as decision trees, are\nefficient and interpretable predictors, but their quality is highly dependent\non the particular choice of input features. Although neural networks can learn\nuseful representations directly from raw data (e.g., images or text), this\ncomes at the expense of interpretability and the need for specialized hardware\nto run them efficiently. In this paper, we explore a hypothesis class we call\nLearned Programmatic Representations (LeaPR) models, which stack arbitrary\nfeatures represented as code (functions from data points to scalars) and\ndecision tree predictors. We synthesize feature functions using Large Language\nModels (LLMs), which have rich prior knowledge in a wide range of domains and a\nremarkable ability to write code using existing domain-specific libraries. We\npropose two algorithms to learn LeaPR models from supervised data. First, we\ndesign an adaptation of FunSearch to learn features rather than directly\ngenerate predictors. Then, we develop a novel variant of the classical ID3\nalgorithm for decision tree learning, where new features are generated on\ndemand when splitting leaf nodes. In experiments from chess position evaluation\nto image and text classification, our methods learn high-quality, neural\nnetwork-free predictors often competitive with neural networks. Our work\nsuggests a flexible paradigm for learning interpretable representations\nend-to-end where features and predictions can be readily inspected and\nunderstood.\n", "link": "http://arxiv.org/abs/2510.14825v1", "date": "2025-10-16", "relevancy": 2.638, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programmatic%20Representation%20Learning%20with%20Language%20Models&body=Title%3A%20Programmatic%20Representation%20Learning%20with%20Language%20Models%0AAuthor%3A%20Gabriel%20Poesia%20and%20Georgia%20Gabriela%20Sampaio%0AAbstract%3A%20%20%20Classical%20models%20for%20supervised%20machine%20learning%2C%20such%20as%20decision%20trees%2C%20are%0Aefficient%20and%20interpretable%20predictors%2C%20but%20their%20quality%20is%20highly%20dependent%0Aon%20the%20particular%20choice%20of%20input%20features.%20Although%20neural%20networks%20can%20learn%0Auseful%20representations%20directly%20from%20raw%20data%20%28e.g.%2C%20images%20or%20text%29%2C%20this%0Acomes%20at%20the%20expense%20of%20interpretability%20and%20the%20need%20for%20specialized%20hardware%0Ato%20run%20them%20efficiently.%20In%20this%20paper%2C%20we%20explore%20a%20hypothesis%20class%20we%20call%0ALearned%20Programmatic%20Representations%20%28LeaPR%29%20models%2C%20which%20stack%20arbitrary%0Afeatures%20represented%20as%20code%20%28functions%20from%20data%20points%20to%20scalars%29%20and%0Adecision%20tree%20predictors.%20We%20synthesize%20feature%20functions%20using%20Large%20Language%0AModels%20%28LLMs%29%2C%20which%20have%20rich%20prior%20knowledge%20in%20a%20wide%20range%20of%20domains%20and%20a%0Aremarkable%20ability%20to%20write%20code%20using%20existing%20domain-specific%20libraries.%20We%0Apropose%20two%20algorithms%20to%20learn%20LeaPR%20models%20from%20supervised%20data.%20First%2C%20we%0Adesign%20an%20adaptation%20of%20FunSearch%20to%20learn%20features%20rather%20than%20directly%0Agenerate%20predictors.%20Then%2C%20we%20develop%20a%20novel%20variant%20of%20the%20classical%20ID3%0Aalgorithm%20for%20decision%20tree%20learning%2C%20where%20new%20features%20are%20generated%20on%0Ademand%20when%20splitting%20leaf%20nodes.%20In%20experiments%20from%20chess%20position%20evaluation%0Ato%20image%20and%20text%20classification%2C%20our%20methods%20learn%20high-quality%2C%20neural%0Anetwork-free%20predictors%20often%20competitive%20with%20neural%20networks.%20Our%20work%0Asuggests%20a%20flexible%20paradigm%20for%20learning%20interpretable%20representations%0Aend-to-end%20where%20features%20and%20predictions%20can%20be%20readily%20inspected%20and%0Aunderstood.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgrammatic%2520Representation%2520Learning%2520with%2520Language%2520Models%26entry.906535625%3DGabriel%2520Poesia%2520and%2520Georgia%2520Gabriela%2520Sampaio%26entry.1292438233%3D%2520%2520Classical%2520models%2520for%2520supervised%2520machine%2520learning%252C%2520such%2520as%2520decision%2520trees%252C%2520are%250Aefficient%2520and%2520interpretable%2520predictors%252C%2520but%2520their%2520quality%2520is%2520highly%2520dependent%250Aon%2520the%2520particular%2520choice%2520of%2520input%2520features.%2520Although%2520neural%2520networks%2520can%2520learn%250Auseful%2520representations%2520directly%2520from%2520raw%2520data%2520%2528e.g.%252C%2520images%2520or%2520text%2529%252C%2520this%250Acomes%2520at%2520the%2520expense%2520of%2520interpretability%2520and%2520the%2520need%2520for%2520specialized%2520hardware%250Ato%2520run%2520them%2520efficiently.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520hypothesis%2520class%2520we%2520call%250ALearned%2520Programmatic%2520Representations%2520%2528LeaPR%2529%2520models%252C%2520which%2520stack%2520arbitrary%250Afeatures%2520represented%2520as%2520code%2520%2528functions%2520from%2520data%2520points%2520to%2520scalars%2529%2520and%250Adecision%2520tree%2520predictors.%2520We%2520synthesize%2520feature%2520functions%2520using%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520which%2520have%2520rich%2520prior%2520knowledge%2520in%2520a%2520wide%2520range%2520of%2520domains%2520and%2520a%250Aremarkable%2520ability%2520to%2520write%2520code%2520using%2520existing%2520domain-specific%2520libraries.%2520We%250Apropose%2520two%2520algorithms%2520to%2520learn%2520LeaPR%2520models%2520from%2520supervised%2520data.%2520First%252C%2520we%250Adesign%2520an%2520adaptation%2520of%2520FunSearch%2520to%2520learn%2520features%2520rather%2520than%2520directly%250Agenerate%2520predictors.%2520Then%252C%2520we%2520develop%2520a%2520novel%2520variant%2520of%2520the%2520classical%2520ID3%250Aalgorithm%2520for%2520decision%2520tree%2520learning%252C%2520where%2520new%2520features%2520are%2520generated%2520on%250Ademand%2520when%2520splitting%2520leaf%2520nodes.%2520In%2520experiments%2520from%2520chess%2520position%2520evaluation%250Ato%2520image%2520and%2520text%2520classification%252C%2520our%2520methods%2520learn%2520high-quality%252C%2520neural%250Anetwork-free%2520predictors%2520often%2520competitive%2520with%2520neural%2520networks.%2520Our%2520work%250Asuggests%2520a%2520flexible%2520paradigm%2520for%2520learning%2520interpretable%2520representations%250Aend-to-end%2520where%2520features%2520and%2520predictions%2520can%2520be%2520readily%2520inspected%2520and%250Aunderstood.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programmatic%20Representation%20Learning%20with%20Language%20Models&entry.906535625=Gabriel%20Poesia%20and%20Georgia%20Gabriela%20Sampaio&entry.1292438233=%20%20Classical%20models%20for%20supervised%20machine%20learning%2C%20such%20as%20decision%20trees%2C%20are%0Aefficient%20and%20interpretable%20predictors%2C%20but%20their%20quality%20is%20highly%20dependent%0Aon%20the%20particular%20choice%20of%20input%20features.%20Although%20neural%20networks%20can%20learn%0Auseful%20representations%20directly%20from%20raw%20data%20%28e.g.%2C%20images%20or%20text%29%2C%20this%0Acomes%20at%20the%20expense%20of%20interpretability%20and%20the%20need%20for%20specialized%20hardware%0Ato%20run%20them%20efficiently.%20In%20this%20paper%2C%20we%20explore%20a%20hypothesis%20class%20we%20call%0ALearned%20Programmatic%20Representations%20%28LeaPR%29%20models%2C%20which%20stack%20arbitrary%0Afeatures%20represented%20as%20code%20%28functions%20from%20data%20points%20to%20scalars%29%20and%0Adecision%20tree%20predictors.%20We%20synthesize%20feature%20functions%20using%20Large%20Language%0AModels%20%28LLMs%29%2C%20which%20have%20rich%20prior%20knowledge%20in%20a%20wide%20range%20of%20domains%20and%20a%0Aremarkable%20ability%20to%20write%20code%20using%20existing%20domain-specific%20libraries.%20We%0Apropose%20two%20algorithms%20to%20learn%20LeaPR%20models%20from%20supervised%20data.%20First%2C%20we%0Adesign%20an%20adaptation%20of%20FunSearch%20to%20learn%20features%20rather%20than%20directly%0Agenerate%20predictors.%20Then%2C%20we%20develop%20a%20novel%20variant%20of%20the%20classical%20ID3%0Aalgorithm%20for%20decision%20tree%20learning%2C%20where%20new%20features%20are%20generated%20on%0Ademand%20when%20splitting%20leaf%20nodes.%20In%20experiments%20from%20chess%20position%20evaluation%0Ato%20image%20and%20text%20classification%2C%20our%20methods%20learn%20high-quality%2C%20neural%0Anetwork-free%20predictors%20often%20competitive%20with%20neural%20networks.%20Our%20work%0Asuggests%20a%20flexible%20paradigm%20for%20learning%20interpretable%20representations%0Aend-to-end%20where%20features%20and%20predictions%20can%20be%20readily%20inspected%20and%0Aunderstood.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14825v1&entry.124074799=Read"},
{"title": "Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings", "author": "Shayan Gharib and Marcelo Hartmann and Arto Klami", "abstract": "  We address the problem of distribution shift in unsupervised domain\nadaptation with a moment-matching approach. Existing methods typically align\nlow-order statistical moments of the source and target distributions in an\nembedding space using ad-hoc similarity measures. We propose a principled\nalternative that instead leverages the intrinsic geometry of these\ndistributions by adopting a Riemannian distance for this alignment. Our key\nnovelty lies in expressing the first- and second-order moments as a single\nsymmetric positive definite (SPD) matrix through Siegel embeddings. This\nenables simultaneous adaptation of both moments using the natural geometric\ndistance on the shared manifold of SPD matrices, preserving the mean and\ncovariance structure of the source and target distributions and yielding a more\nfaithful metric for cross-domain comparison. We connect the Riemannian manifold\ndistance to the target-domain error bound, and validate the method on image\ndenoising and image classification benchmarks. Our code is publicly available\nat https://github.com/shayangharib/GeoAdapt.\n", "link": "http://arxiv.org/abs/2510.14666v1", "date": "2025-10-16", "relevancy": 2.6236, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5236}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Moment%20Alignment%20for%20Domain%20Adaptation%20via%20Siegel%20Embeddings&body=Title%3A%20Geometric%20Moment%20Alignment%20for%20Domain%20Adaptation%20via%20Siegel%20Embeddings%0AAuthor%3A%20Shayan%20Gharib%20and%20Marcelo%20Hartmann%20and%20Arto%20Klami%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20distribution%20shift%20in%20unsupervised%20domain%0Aadaptation%20with%20a%20moment-matching%20approach.%20Existing%20methods%20typically%20align%0Alow-order%20statistical%20moments%20of%20the%20source%20and%20target%20distributions%20in%20an%0Aembedding%20space%20using%20ad-hoc%20similarity%20measures.%20We%20propose%20a%20principled%0Aalternative%20that%20instead%20leverages%20the%20intrinsic%20geometry%20of%20these%0Adistributions%20by%20adopting%20a%20Riemannian%20distance%20for%20this%20alignment.%20Our%20key%0Anovelty%20lies%20in%20expressing%20the%20first-%20and%20second-order%20moments%20as%20a%20single%0Asymmetric%20positive%20definite%20%28SPD%29%20matrix%20through%20Siegel%20embeddings.%20This%0Aenables%20simultaneous%20adaptation%20of%20both%20moments%20using%20the%20natural%20geometric%0Adistance%20on%20the%20shared%20manifold%20of%20SPD%20matrices%2C%20preserving%20the%20mean%20and%0Acovariance%20structure%20of%20the%20source%20and%20target%20distributions%20and%20yielding%20a%20more%0Afaithful%20metric%20for%20cross-domain%20comparison.%20We%20connect%20the%20Riemannian%20manifold%0Adistance%20to%20the%20target-domain%20error%20bound%2C%20and%20validate%20the%20method%20on%20image%0Adenoising%20and%20image%20classification%20benchmarks.%20Our%20code%20is%20publicly%20available%0Aat%20https%3A//github.com/shayangharib/GeoAdapt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Moment%2520Alignment%2520for%2520Domain%2520Adaptation%2520via%2520Siegel%2520Embeddings%26entry.906535625%3DShayan%2520Gharib%2520and%2520Marcelo%2520Hartmann%2520and%2520Arto%2520Klami%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520distribution%2520shift%2520in%2520unsupervised%2520domain%250Aadaptation%2520with%2520a%2520moment-matching%2520approach.%2520Existing%2520methods%2520typically%2520align%250Alow-order%2520statistical%2520moments%2520of%2520the%2520source%2520and%2520target%2520distributions%2520in%2520an%250Aembedding%2520space%2520using%2520ad-hoc%2520similarity%2520measures.%2520We%2520propose%2520a%2520principled%250Aalternative%2520that%2520instead%2520leverages%2520the%2520intrinsic%2520geometry%2520of%2520these%250Adistributions%2520by%2520adopting%2520a%2520Riemannian%2520distance%2520for%2520this%2520alignment.%2520Our%2520key%250Anovelty%2520lies%2520in%2520expressing%2520the%2520first-%2520and%2520second-order%2520moments%2520as%2520a%2520single%250Asymmetric%2520positive%2520definite%2520%2528SPD%2529%2520matrix%2520through%2520Siegel%2520embeddings.%2520This%250Aenables%2520simultaneous%2520adaptation%2520of%2520both%2520moments%2520using%2520the%2520natural%2520geometric%250Adistance%2520on%2520the%2520shared%2520manifold%2520of%2520SPD%2520matrices%252C%2520preserving%2520the%2520mean%2520and%250Acovariance%2520structure%2520of%2520the%2520source%2520and%2520target%2520distributions%2520and%2520yielding%2520a%2520more%250Afaithful%2520metric%2520for%2520cross-domain%2520comparison.%2520We%2520connect%2520the%2520Riemannian%2520manifold%250Adistance%2520to%2520the%2520target-domain%2520error%2520bound%252C%2520and%2520validate%2520the%2520method%2520on%2520image%250Adenoising%2520and%2520image%2520classification%2520benchmarks.%2520Our%2520code%2520is%2520publicly%2520available%250Aat%2520https%253A//github.com/shayangharib/GeoAdapt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Moment%20Alignment%20for%20Domain%20Adaptation%20via%20Siegel%20Embeddings&entry.906535625=Shayan%20Gharib%20and%20Marcelo%20Hartmann%20and%20Arto%20Klami&entry.1292438233=%20%20We%20address%20the%20problem%20of%20distribution%20shift%20in%20unsupervised%20domain%0Aadaptation%20with%20a%20moment-matching%20approach.%20Existing%20methods%20typically%20align%0Alow-order%20statistical%20moments%20of%20the%20source%20and%20target%20distributions%20in%20an%0Aembedding%20space%20using%20ad-hoc%20similarity%20measures.%20We%20propose%20a%20principled%0Aalternative%20that%20instead%20leverages%20the%20intrinsic%20geometry%20of%20these%0Adistributions%20by%20adopting%20a%20Riemannian%20distance%20for%20this%20alignment.%20Our%20key%0Anovelty%20lies%20in%20expressing%20the%20first-%20and%20second-order%20moments%20as%20a%20single%0Asymmetric%20positive%20definite%20%28SPD%29%20matrix%20through%20Siegel%20embeddings.%20This%0Aenables%20simultaneous%20adaptation%20of%20both%20moments%20using%20the%20natural%20geometric%0Adistance%20on%20the%20shared%20manifold%20of%20SPD%20matrices%2C%20preserving%20the%20mean%20and%0Acovariance%20structure%20of%20the%20source%20and%20target%20distributions%20and%20yielding%20a%20more%0Afaithful%20metric%20for%20cross-domain%20comparison.%20We%20connect%20the%20Riemannian%20manifold%0Adistance%20to%20the%20target-domain%20error%20bound%2C%20and%20validate%20the%20method%20on%20image%0Adenoising%20and%20image%20classification%20benchmarks.%20Our%20code%20is%20publicly%20available%0Aat%20https%3A//github.com/shayangharib/GeoAdapt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14666v1&entry.124074799=Read"},
{"title": "The Mechanistic Emergence of Symbol Grounding in Language Models", "author": "Shuyu Wu and Ziqiao Ma and Xiaoxi Luo and Yidong Huang and Josue Torres-Fonseca and Freda Shi and Joyce Chai", "abstract": "  Symbol grounding (Harnad, 1990) describes how symbols such as words acquire\ntheir meanings by connecting to real-world sensorimotor experiences. Recent\nwork has shown preliminary evidence that grounding may emerge in\n(vision-)language models trained at scale without using explicit grounding\nobjectives. Yet, the specific loci of this emergence and the mechanisms that\ndrive it remain largely unexplored. To address this problem, we introduce a\ncontrolled evaluation framework that systematically traces how symbol grounding\narises within the internal computations through mechanistic and causal\nanalysis. Our findings show that grounding concentrates in middle-layer\ncomputations and is implemented through the aggregate mechanism, where\nattention heads aggregate the environmental ground to support the prediction of\nlinguistic forms. This phenomenon replicates in multimodal dialogue and across\narchitectures (Transformers and state-space models), but not in unidirectional\nLSTMs. Our results provide behavioral and mechanistic evidence that symbol\ngrounding can emerge in language models, with practical implications for\npredicting and potentially controlling the reliability of generation.\n", "link": "http://arxiv.org/abs/2510.13796v2", "date": "2025-10-16", "relevancy": 2.6133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Mechanistic%20Emergence%20of%20Symbol%20Grounding%20in%20Language%20Models&body=Title%3A%20The%20Mechanistic%20Emergence%20of%20Symbol%20Grounding%20in%20Language%20Models%0AAuthor%3A%20Shuyu%20Wu%20and%20Ziqiao%20Ma%20and%20Xiaoxi%20Luo%20and%20Yidong%20Huang%20and%20Josue%20Torres-Fonseca%20and%20Freda%20Shi%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Symbol%20grounding%20%28Harnad%2C%201990%29%20describes%20how%20symbols%20such%20as%20words%20acquire%0Atheir%20meanings%20by%20connecting%20to%20real-world%20sensorimotor%20experiences.%20Recent%0Awork%20has%20shown%20preliminary%20evidence%20that%20grounding%20may%20emerge%20in%0A%28vision-%29language%20models%20trained%20at%20scale%20without%20using%20explicit%20grounding%0Aobjectives.%20Yet%2C%20the%20specific%20loci%20of%20this%20emergence%20and%20the%20mechanisms%20that%0Adrive%20it%20remain%20largely%20unexplored.%20To%20address%20this%20problem%2C%20we%20introduce%20a%0Acontrolled%20evaluation%20framework%20that%20systematically%20traces%20how%20symbol%20grounding%0Aarises%20within%20the%20internal%20computations%20through%20mechanistic%20and%20causal%0Aanalysis.%20Our%20findings%20show%20that%20grounding%20concentrates%20in%20middle-layer%0Acomputations%20and%20is%20implemented%20through%20the%20aggregate%20mechanism%2C%20where%0Aattention%20heads%20aggregate%20the%20environmental%20ground%20to%20support%20the%20prediction%20of%0Alinguistic%20forms.%20This%20phenomenon%20replicates%20in%20multimodal%20dialogue%20and%20across%0Aarchitectures%20%28Transformers%20and%20state-space%20models%29%2C%20but%20not%20in%20unidirectional%0ALSTMs.%20Our%20results%20provide%20behavioral%20and%20mechanistic%20evidence%20that%20symbol%0Agrounding%20can%20emerge%20in%20language%20models%2C%20with%20practical%20implications%20for%0Apredicting%20and%20potentially%20controlling%20the%20reliability%20of%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Mechanistic%2520Emergence%2520of%2520Symbol%2520Grounding%2520in%2520Language%2520Models%26entry.906535625%3DShuyu%2520Wu%2520and%2520Ziqiao%2520Ma%2520and%2520Xiaoxi%2520Luo%2520and%2520Yidong%2520Huang%2520and%2520Josue%2520Torres-Fonseca%2520and%2520Freda%2520Shi%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Symbol%2520grounding%2520%2528Harnad%252C%25201990%2529%2520describes%2520how%2520symbols%2520such%2520as%2520words%2520acquire%250Atheir%2520meanings%2520by%2520connecting%2520to%2520real-world%2520sensorimotor%2520experiences.%2520Recent%250Awork%2520has%2520shown%2520preliminary%2520evidence%2520that%2520grounding%2520may%2520emerge%2520in%250A%2528vision-%2529language%2520models%2520trained%2520at%2520scale%2520without%2520using%2520explicit%2520grounding%250Aobjectives.%2520Yet%252C%2520the%2520specific%2520loci%2520of%2520this%2520emergence%2520and%2520the%2520mechanisms%2520that%250Adrive%2520it%2520remain%2520largely%2520unexplored.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%250Acontrolled%2520evaluation%2520framework%2520that%2520systematically%2520traces%2520how%2520symbol%2520grounding%250Aarises%2520within%2520the%2520internal%2520computations%2520through%2520mechanistic%2520and%2520causal%250Aanalysis.%2520Our%2520findings%2520show%2520that%2520grounding%2520concentrates%2520in%2520middle-layer%250Acomputations%2520and%2520is%2520implemented%2520through%2520the%2520aggregate%2520mechanism%252C%2520where%250Aattention%2520heads%2520aggregate%2520the%2520environmental%2520ground%2520to%2520support%2520the%2520prediction%2520of%250Alinguistic%2520forms.%2520This%2520phenomenon%2520replicates%2520in%2520multimodal%2520dialogue%2520and%2520across%250Aarchitectures%2520%2528Transformers%2520and%2520state-space%2520models%2529%252C%2520but%2520not%2520in%2520unidirectional%250ALSTMs.%2520Our%2520results%2520provide%2520behavioral%2520and%2520mechanistic%2520evidence%2520that%2520symbol%250Agrounding%2520can%2520emerge%2520in%2520language%2520models%252C%2520with%2520practical%2520implications%2520for%250Apredicting%2520and%2520potentially%2520controlling%2520the%2520reliability%2520of%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Mechanistic%20Emergence%20of%20Symbol%20Grounding%20in%20Language%20Models&entry.906535625=Shuyu%20Wu%20and%20Ziqiao%20Ma%20and%20Xiaoxi%20Luo%20and%20Yidong%20Huang%20and%20Josue%20Torres-Fonseca%20and%20Freda%20Shi%20and%20Joyce%20Chai&entry.1292438233=%20%20Symbol%20grounding%20%28Harnad%2C%201990%29%20describes%20how%20symbols%20such%20as%20words%20acquire%0Atheir%20meanings%20by%20connecting%20to%20real-world%20sensorimotor%20experiences.%20Recent%0Awork%20has%20shown%20preliminary%20evidence%20that%20grounding%20may%20emerge%20in%0A%28vision-%29language%20models%20trained%20at%20scale%20without%20using%20explicit%20grounding%0Aobjectives.%20Yet%2C%20the%20specific%20loci%20of%20this%20emergence%20and%20the%20mechanisms%20that%0Adrive%20it%20remain%20largely%20unexplored.%20To%20address%20this%20problem%2C%20we%20introduce%20a%0Acontrolled%20evaluation%20framework%20that%20systematically%20traces%20how%20symbol%20grounding%0Aarises%20within%20the%20internal%20computations%20through%20mechanistic%20and%20causal%0Aanalysis.%20Our%20findings%20show%20that%20grounding%20concentrates%20in%20middle-layer%0Acomputations%20and%20is%20implemented%20through%20the%20aggregate%20mechanism%2C%20where%0Aattention%20heads%20aggregate%20the%20environmental%20ground%20to%20support%20the%20prediction%20of%0Alinguistic%20forms.%20This%20phenomenon%20replicates%20in%20multimodal%20dialogue%20and%20across%0Aarchitectures%20%28Transformers%20and%20state-space%20models%29%2C%20but%20not%20in%20unidirectional%0ALSTMs.%20Our%20results%20provide%20behavioral%20and%20mechanistic%20evidence%20that%20symbol%0Agrounding%20can%20emerge%20in%20language%20models%2C%20with%20practical%20implications%20for%0Apredicting%20and%20potentially%20controlling%20the%20reliability%20of%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13796v2&entry.124074799=Read"},
{"title": "ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual\n  Try-On", "author": "Junseo Park and Hyeryung Jang", "abstract": "  Virtual try-on (VITON) aims to generate realistic images of a person wearing\na target garment, requiring precise garment alignment in try-on regions and\nfaithful preservation of identity and background in non-try-on regions. While\nlatent diffusion models (LDMs) have advanced alignment and detail synthesis,\npreserving non-try-on regions remains challenging. A common post-hoc strategy\ndirectly replaces these regions with original content, but abrupt transitions\noften produce boundary artifacts. To overcome this, we reformulate VITON as a\nlinear inverse problem and adopt trajectory-aligned solvers that progressively\nenforce measurement consistency, reducing abrupt changes in non-try-on regions.\nHowever, existing solvers still suffer from semantic drift during generation,\nleading to artifacts. We propose ART-VITON, a measurement-guided diffusion\nframework that ensures measurement adherence while maintaining artifact-free\nsynthesis. Our method integrates residual prior-based initialization to\nmitigate training-inference mismatch and artifact-free measurement-guided\nsampling that combines data consistency, frequency-level correction, and\nperiodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0\ndemonstrate that ART-VITON effectively preserves identity and background,\neliminates boundary artifacts, and consistently improves visual fidelity and\nrobustness over state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2509.25749v2", "date": "2025-10-16", "relevancy": 2.6041, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.661}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6559}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ART-VITON%3A%20Measurement-Guided%20Latent%20Diffusion%20for%20Artifact-Free%20Virtual%0A%20%20Try-On&body=Title%3A%20ART-VITON%3A%20Measurement-Guided%20Latent%20Diffusion%20for%20Artifact-Free%20Virtual%0A%20%20Try-On%0AAuthor%3A%20Junseo%20Park%20and%20Hyeryung%20Jang%0AAbstract%3A%20%20%20Virtual%20try-on%20%28VITON%29%20aims%20to%20generate%20realistic%20images%20of%20a%20person%20wearing%0Aa%20target%20garment%2C%20requiring%20precise%20garment%20alignment%20in%20try-on%20regions%20and%0Afaithful%20preservation%20of%20identity%20and%20background%20in%20non-try-on%20regions.%20While%0Alatent%20diffusion%20models%20%28LDMs%29%20have%20advanced%20alignment%20and%20detail%20synthesis%2C%0Apreserving%20non-try-on%20regions%20remains%20challenging.%20A%20common%20post-hoc%20strategy%0Adirectly%20replaces%20these%20regions%20with%20original%20content%2C%20but%20abrupt%20transitions%0Aoften%20produce%20boundary%20artifacts.%20To%20overcome%20this%2C%20we%20reformulate%20VITON%20as%20a%0Alinear%20inverse%20problem%20and%20adopt%20trajectory-aligned%20solvers%20that%20progressively%0Aenforce%20measurement%20consistency%2C%20reducing%20abrupt%20changes%20in%20non-try-on%20regions.%0AHowever%2C%20existing%20solvers%20still%20suffer%20from%20semantic%20drift%20during%20generation%2C%0Aleading%20to%20artifacts.%20We%20propose%20ART-VITON%2C%20a%20measurement-guided%20diffusion%0Aframework%20that%20ensures%20measurement%20adherence%20while%20maintaining%20artifact-free%0Asynthesis.%20Our%20method%20integrates%20residual%20prior-based%20initialization%20to%0Amitigate%20training-inference%20mismatch%20and%20artifact-free%20measurement-guided%0Asampling%20that%20combines%20data%20consistency%2C%20frequency-level%20correction%2C%20and%0Aperiodic%20standard%20denoising.%20Experiments%20on%20VITON-HD%2C%20DressCode%2C%20and%20SHHQ-1.0%0Ademonstrate%20that%20ART-VITON%20effectively%20preserves%20identity%20and%20background%2C%0Aeliminates%20boundary%20artifacts%2C%20and%20consistently%20improves%20visual%20fidelity%20and%0Arobustness%20over%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DART-VITON%253A%2520Measurement-Guided%2520Latent%2520Diffusion%2520for%2520Artifact-Free%2520Virtual%250A%2520%2520Try-On%26entry.906535625%3DJunseo%2520Park%2520and%2520Hyeryung%2520Jang%26entry.1292438233%3D%2520%2520Virtual%2520try-on%2520%2528VITON%2529%2520aims%2520to%2520generate%2520realistic%2520images%2520of%2520a%2520person%2520wearing%250Aa%2520target%2520garment%252C%2520requiring%2520precise%2520garment%2520alignment%2520in%2520try-on%2520regions%2520and%250Afaithful%2520preservation%2520of%2520identity%2520and%2520background%2520in%2520non-try-on%2520regions.%2520While%250Alatent%2520diffusion%2520models%2520%2528LDMs%2529%2520have%2520advanced%2520alignment%2520and%2520detail%2520synthesis%252C%250Apreserving%2520non-try-on%2520regions%2520remains%2520challenging.%2520A%2520common%2520post-hoc%2520strategy%250Adirectly%2520replaces%2520these%2520regions%2520with%2520original%2520content%252C%2520but%2520abrupt%2520transitions%250Aoften%2520produce%2520boundary%2520artifacts.%2520To%2520overcome%2520this%252C%2520we%2520reformulate%2520VITON%2520as%2520a%250Alinear%2520inverse%2520problem%2520and%2520adopt%2520trajectory-aligned%2520solvers%2520that%2520progressively%250Aenforce%2520measurement%2520consistency%252C%2520reducing%2520abrupt%2520changes%2520in%2520non-try-on%2520regions.%250AHowever%252C%2520existing%2520solvers%2520still%2520suffer%2520from%2520semantic%2520drift%2520during%2520generation%252C%250Aleading%2520to%2520artifacts.%2520We%2520propose%2520ART-VITON%252C%2520a%2520measurement-guided%2520diffusion%250Aframework%2520that%2520ensures%2520measurement%2520adherence%2520while%2520maintaining%2520artifact-free%250Asynthesis.%2520Our%2520method%2520integrates%2520residual%2520prior-based%2520initialization%2520to%250Amitigate%2520training-inference%2520mismatch%2520and%2520artifact-free%2520measurement-guided%250Asampling%2520that%2520combines%2520data%2520consistency%252C%2520frequency-level%2520correction%252C%2520and%250Aperiodic%2520standard%2520denoising.%2520Experiments%2520on%2520VITON-HD%252C%2520DressCode%252C%2520and%2520SHHQ-1.0%250Ademonstrate%2520that%2520ART-VITON%2520effectively%2520preserves%2520identity%2520and%2520background%252C%250Aeliminates%2520boundary%2520artifacts%252C%2520and%2520consistently%2520improves%2520visual%2520fidelity%2520and%250Arobustness%2520over%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ART-VITON%3A%20Measurement-Guided%20Latent%20Diffusion%20for%20Artifact-Free%20Virtual%0A%20%20Try-On&entry.906535625=Junseo%20Park%20and%20Hyeryung%20Jang&entry.1292438233=%20%20Virtual%20try-on%20%28VITON%29%20aims%20to%20generate%20realistic%20images%20of%20a%20person%20wearing%0Aa%20target%20garment%2C%20requiring%20precise%20garment%20alignment%20in%20try-on%20regions%20and%0Afaithful%20preservation%20of%20identity%20and%20background%20in%20non-try-on%20regions.%20While%0Alatent%20diffusion%20models%20%28LDMs%29%20have%20advanced%20alignment%20and%20detail%20synthesis%2C%0Apreserving%20non-try-on%20regions%20remains%20challenging.%20A%20common%20post-hoc%20strategy%0Adirectly%20replaces%20these%20regions%20with%20original%20content%2C%20but%20abrupt%20transitions%0Aoften%20produce%20boundary%20artifacts.%20To%20overcome%20this%2C%20we%20reformulate%20VITON%20as%20a%0Alinear%20inverse%20problem%20and%20adopt%20trajectory-aligned%20solvers%20that%20progressively%0Aenforce%20measurement%20consistency%2C%20reducing%20abrupt%20changes%20in%20non-try-on%20regions.%0AHowever%2C%20existing%20solvers%20still%20suffer%20from%20semantic%20drift%20during%20generation%2C%0Aleading%20to%20artifacts.%20We%20propose%20ART-VITON%2C%20a%20measurement-guided%20diffusion%0Aframework%20that%20ensures%20measurement%20adherence%20while%20maintaining%20artifact-free%0Asynthesis.%20Our%20method%20integrates%20residual%20prior-based%20initialization%20to%0Amitigate%20training-inference%20mismatch%20and%20artifact-free%20measurement-guided%0Asampling%20that%20combines%20data%20consistency%2C%20frequency-level%20correction%2C%20and%0Aperiodic%20standard%20denoising.%20Experiments%20on%20VITON-HD%2C%20DressCode%2C%20and%20SHHQ-1.0%0Ademonstrate%20that%20ART-VITON%20effectively%20preserves%20identity%20and%20background%2C%0Aeliminates%20boundary%20artifacts%2C%20and%20consistently%20improves%20visual%20fidelity%20and%0Arobustness%20over%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25749v2&entry.124074799=Read"},
{"title": "WithAnyone: Towards Controllable and ID Consistent Image Generation", "author": "Hengyuan Xu and Wei Cheng and Peng Xing and Yixiao Fang and Shuhan Wu and Rui Wang and Xianfang Zeng and Daxin Jiang and Gang Yu and Xingjun Ma and Yu-Gang Jiang", "abstract": "  Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.\n", "link": "http://arxiv.org/abs/2510.14975v1", "date": "2025-10-16", "relevancy": 2.5964, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.696}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6206}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WithAnyone%3A%20Towards%20Controllable%20and%20ID%20Consistent%20Image%20Generation&body=Title%3A%20WithAnyone%3A%20Towards%20Controllable%20and%20ID%20Consistent%20Image%20Generation%0AAuthor%3A%20Hengyuan%20Xu%20and%20Wei%20Cheng%20and%20Peng%20Xing%20and%20Yixiao%20Fang%20and%20Shuhan%20Wu%20and%20Rui%20Wang%20and%20Xianfang%20Zeng%20and%20Daxin%20Jiang%20and%20Gang%20Yu%20and%20Xingjun%20Ma%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Identity-consistent%20generation%20has%20become%20an%20important%20focus%20in%20text-to-image%0Aresearch%2C%20with%20recent%20models%20achieving%20notable%20success%20in%20producing%20images%0Aaligned%20with%20a%20reference%20identity.%20Yet%2C%20the%20scarcity%20of%20large-scale%20paired%0Adatasets%20containing%20multiple%20images%20of%20the%20same%20individual%20forces%20most%0Aapproaches%20to%20adopt%20reconstruction-based%20training.%20This%20reliance%20often%20leads%20to%0Aa%20failure%20mode%20we%20term%20copy-paste%2C%20where%20the%20model%20directly%20replicates%20the%0Areference%20face%20rather%20than%20preserving%20identity%20across%20natural%20variations%20in%0Apose%2C%20expression%2C%20or%20lighting.%20Such%20over-similarity%20undermines%20controllability%0Aand%20limits%20the%20expressive%20power%20of%20generation.%20To%20address%20these%20limitations%2C%20we%0A%281%29%20construct%20a%20large-scale%20paired%20dataset%20MultiID-2M%2C%20tailored%20for%0Amulti-person%20scenarios%2C%20providing%20diverse%20references%20for%20each%20identity%3B%20%282%29%0Aintroduce%20a%20benchmark%20that%20quantifies%20both%20copy-paste%20artifacts%20and%20the%0Atrade-off%20between%20identity%20fidelity%20and%20variation%3B%20and%20%283%29%20propose%20a%20novel%0Atraining%20paradigm%20with%20a%20contrastive%20identity%20loss%20that%20leverages%20paired%20data%0Ato%20balance%20fidelity%20with%20diversity.%20These%20contributions%20culminate%20in%0AWithAnyone%2C%20a%20diffusion-based%20model%20that%20effectively%20mitigates%20copy-paste%20while%0Apreserving%20high%20identity%20similarity.%20Extensive%20qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20that%20WithAnyone%20significantly%20reduces%20copy-paste%0Aartifacts%2C%20improves%20controllability%20over%20pose%20and%20expression%2C%20and%20maintains%0Astrong%20perceptual%20quality.%20User%20studies%20further%20validate%20that%20our%20method%0Aachieves%20high%20identity%20fidelity%20while%20enabling%20expressive%20controllable%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWithAnyone%253A%2520Towards%2520Controllable%2520and%2520ID%2520Consistent%2520Image%2520Generation%26entry.906535625%3DHengyuan%2520Xu%2520and%2520Wei%2520Cheng%2520and%2520Peng%2520Xing%2520and%2520Yixiao%2520Fang%2520and%2520Shuhan%2520Wu%2520and%2520Rui%2520Wang%2520and%2520Xianfang%2520Zeng%2520and%2520Daxin%2520Jiang%2520and%2520Gang%2520Yu%2520and%2520Xingjun%2520Ma%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Identity-consistent%2520generation%2520has%2520become%2520an%2520important%2520focus%2520in%2520text-to-image%250Aresearch%252C%2520with%2520recent%2520models%2520achieving%2520notable%2520success%2520in%2520producing%2520images%250Aaligned%2520with%2520a%2520reference%2520identity.%2520Yet%252C%2520the%2520scarcity%2520of%2520large-scale%2520paired%250Adatasets%2520containing%2520multiple%2520images%2520of%2520the%2520same%2520individual%2520forces%2520most%250Aapproaches%2520to%2520adopt%2520reconstruction-based%2520training.%2520This%2520reliance%2520often%2520leads%2520to%250Aa%2520failure%2520mode%2520we%2520term%2520copy-paste%252C%2520where%2520the%2520model%2520directly%2520replicates%2520the%250Areference%2520face%2520rather%2520than%2520preserving%2520identity%2520across%2520natural%2520variations%2520in%250Apose%252C%2520expression%252C%2520or%2520lighting.%2520Such%2520over-similarity%2520undermines%2520controllability%250Aand%2520limits%2520the%2520expressive%2520power%2520of%2520generation.%2520To%2520address%2520these%2520limitations%252C%2520we%250A%25281%2529%2520construct%2520a%2520large-scale%2520paired%2520dataset%2520MultiID-2M%252C%2520tailored%2520for%250Amulti-person%2520scenarios%252C%2520providing%2520diverse%2520references%2520for%2520each%2520identity%253B%2520%25282%2529%250Aintroduce%2520a%2520benchmark%2520that%2520quantifies%2520both%2520copy-paste%2520artifacts%2520and%2520the%250Atrade-off%2520between%2520identity%2520fidelity%2520and%2520variation%253B%2520and%2520%25283%2529%2520propose%2520a%2520novel%250Atraining%2520paradigm%2520with%2520a%2520contrastive%2520identity%2520loss%2520that%2520leverages%2520paired%2520data%250Ato%2520balance%2520fidelity%2520with%2520diversity.%2520These%2520contributions%2520culminate%2520in%250AWithAnyone%252C%2520a%2520diffusion-based%2520model%2520that%2520effectively%2520mitigates%2520copy-paste%2520while%250Apreserving%2520high%2520identity%2520similarity.%2520Extensive%2520qualitative%2520and%2520quantitative%250Aexperiments%2520demonstrate%2520that%2520WithAnyone%2520significantly%2520reduces%2520copy-paste%250Aartifacts%252C%2520improves%2520controllability%2520over%2520pose%2520and%2520expression%252C%2520and%2520maintains%250Astrong%2520perceptual%2520quality.%2520User%2520studies%2520further%2520validate%2520that%2520our%2520method%250Aachieves%2520high%2520identity%2520fidelity%2520while%2520enabling%2520expressive%2520controllable%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WithAnyone%3A%20Towards%20Controllable%20and%20ID%20Consistent%20Image%20Generation&entry.906535625=Hengyuan%20Xu%20and%20Wei%20Cheng%20and%20Peng%20Xing%20and%20Yixiao%20Fang%20and%20Shuhan%20Wu%20and%20Rui%20Wang%20and%20Xianfang%20Zeng%20and%20Daxin%20Jiang%20and%20Gang%20Yu%20and%20Xingjun%20Ma%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Identity-consistent%20generation%20has%20become%20an%20important%20focus%20in%20text-to-image%0Aresearch%2C%20with%20recent%20models%20achieving%20notable%20success%20in%20producing%20images%0Aaligned%20with%20a%20reference%20identity.%20Yet%2C%20the%20scarcity%20of%20large-scale%20paired%0Adatasets%20containing%20multiple%20images%20of%20the%20same%20individual%20forces%20most%0Aapproaches%20to%20adopt%20reconstruction-based%20training.%20This%20reliance%20often%20leads%20to%0Aa%20failure%20mode%20we%20term%20copy-paste%2C%20where%20the%20model%20directly%20replicates%20the%0Areference%20face%20rather%20than%20preserving%20identity%20across%20natural%20variations%20in%0Apose%2C%20expression%2C%20or%20lighting.%20Such%20over-similarity%20undermines%20controllability%0Aand%20limits%20the%20expressive%20power%20of%20generation.%20To%20address%20these%20limitations%2C%20we%0A%281%29%20construct%20a%20large-scale%20paired%20dataset%20MultiID-2M%2C%20tailored%20for%0Amulti-person%20scenarios%2C%20providing%20diverse%20references%20for%20each%20identity%3B%20%282%29%0Aintroduce%20a%20benchmark%20that%20quantifies%20both%20copy-paste%20artifacts%20and%20the%0Atrade-off%20between%20identity%20fidelity%20and%20variation%3B%20and%20%283%29%20propose%20a%20novel%0Atraining%20paradigm%20with%20a%20contrastive%20identity%20loss%20that%20leverages%20paired%20data%0Ato%20balance%20fidelity%20with%20diversity.%20These%20contributions%20culminate%20in%0AWithAnyone%2C%20a%20diffusion-based%20model%20that%20effectively%20mitigates%20copy-paste%20while%0Apreserving%20high%20identity%20similarity.%20Extensive%20qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20that%20WithAnyone%20significantly%20reduces%20copy-paste%0Aartifacts%2C%20improves%20controllability%20over%20pose%20and%20expression%2C%20and%20maintains%0Astrong%20perceptual%20quality.%20User%20studies%20further%20validate%20that%20our%20method%0Aachieves%20high%20identity%20fidelity%20while%20enabling%20expressive%20controllable%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14975v1&entry.124074799=Read"},
{"title": "C4D: 4D Made from 3D through Dual Correspondences", "author": "Shizun Wang and Zhenxiang Jiang and Xingyi Yang and Xinchao Wang", "abstract": "  Recovering 4D from monocular video, which jointly estimates dynamic geometry\nand camera poses, is an inevitably challenging problem. While recent\npointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great\nprogress in reconstructing static scenes, directly applying them to dynamic\nscenes leads to inaccurate results. This discrepancy arises because moving\nobjects violate multi-view geometric constraints, disrupting the\nreconstruction. To address this, we introduce C4D, a framework that leverages\ntemporal Correspondences to extend existing 3D reconstruction formulation to\n4D. Specifically, apart from predicting pointmaps, C4D captures two types of\ncorrespondences: short-term optical flow and long-term point tracking. We train\na dynamic-aware point tracker that provides additional mobility information,\nfacilitating the estimation of motion masks to separate moving elements from\nthe static background, thus offering more reliable guidance for dynamic scenes.\nFurthermore, we introduce a set of dynamic scene optimization objectives to\nrecover per-frame 3D geometry and camera parameters. Simultaneously, the\ncorrespondences lift 2D trajectories into smooth 3D trajectories, enabling\nfully integrated 4D reconstruction. Experiments show that our framework\nachieves complete 4D recovery and demonstrates strong performance across\nmultiple downstream tasks, including depth estimation, camera pose estimation,\nand point tracking. Project Page: https://littlepure2333.github.io/C4D\n", "link": "http://arxiv.org/abs/2510.14960v1", "date": "2025-10-16", "relevancy": 2.5958, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6513}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6513}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C4D%3A%204D%20Made%20from%203D%20through%20Dual%20Correspondences&body=Title%3A%20C4D%3A%204D%20Made%20from%203D%20through%20Dual%20Correspondences%0AAuthor%3A%20Shizun%20Wang%20and%20Zhenxiang%20Jiang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Recovering%204D%20from%20monocular%20video%2C%20which%20jointly%20estimates%20dynamic%20geometry%0Aand%20camera%20poses%2C%20is%20an%20inevitably%20challenging%20problem.%20While%20recent%0Apointmap-based%203D%20reconstruction%20methods%20%28e.g.%2C%20DUSt3R%29%20have%20made%20great%0Aprogress%20in%20reconstructing%20static%20scenes%2C%20directly%20applying%20them%20to%20dynamic%0Ascenes%20leads%20to%20inaccurate%20results.%20This%20discrepancy%20arises%20because%20moving%0Aobjects%20violate%20multi-view%20geometric%20constraints%2C%20disrupting%20the%0Areconstruction.%20To%20address%20this%2C%20we%20introduce%20C4D%2C%20a%20framework%20that%20leverages%0Atemporal%20Correspondences%20to%20extend%20existing%203D%20reconstruction%20formulation%20to%0A4D.%20Specifically%2C%20apart%20from%20predicting%20pointmaps%2C%20C4D%20captures%20two%20types%20of%0Acorrespondences%3A%20short-term%20optical%20flow%20and%20long-term%20point%20tracking.%20We%20train%0Aa%20dynamic-aware%20point%20tracker%20that%20provides%20additional%20mobility%20information%2C%0Afacilitating%20the%20estimation%20of%20motion%20masks%20to%20separate%20moving%20elements%20from%0Athe%20static%20background%2C%20thus%20offering%20more%20reliable%20guidance%20for%20dynamic%20scenes.%0AFurthermore%2C%20we%20introduce%20a%20set%20of%20dynamic%20scene%20optimization%20objectives%20to%0Arecover%20per-frame%203D%20geometry%20and%20camera%20parameters.%20Simultaneously%2C%20the%0Acorrespondences%20lift%202D%20trajectories%20into%20smooth%203D%20trajectories%2C%20enabling%0Afully%20integrated%204D%20reconstruction.%20Experiments%20show%20that%20our%20framework%0Aachieves%20complete%204D%20recovery%20and%20demonstrates%20strong%20performance%20across%0Amultiple%20downstream%20tasks%2C%20including%20depth%20estimation%2C%20camera%20pose%20estimation%2C%0Aand%20point%20tracking.%20Project%20Page%3A%20https%3A//littlepure2333.github.io/C4D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC4D%253A%25204D%2520Made%2520from%25203D%2520through%2520Dual%2520Correspondences%26entry.906535625%3DShizun%2520Wang%2520and%2520Zhenxiang%2520Jiang%2520and%2520Xingyi%2520Yang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Recovering%25204D%2520from%2520monocular%2520video%252C%2520which%2520jointly%2520estimates%2520dynamic%2520geometry%250Aand%2520camera%2520poses%252C%2520is%2520an%2520inevitably%2520challenging%2520problem.%2520While%2520recent%250Apointmap-based%25203D%2520reconstruction%2520methods%2520%2528e.g.%252C%2520DUSt3R%2529%2520have%2520made%2520great%250Aprogress%2520in%2520reconstructing%2520static%2520scenes%252C%2520directly%2520applying%2520them%2520to%2520dynamic%250Ascenes%2520leads%2520to%2520inaccurate%2520results.%2520This%2520discrepancy%2520arises%2520because%2520moving%250Aobjects%2520violate%2520multi-view%2520geometric%2520constraints%252C%2520disrupting%2520the%250Areconstruction.%2520To%2520address%2520this%252C%2520we%2520introduce%2520C4D%252C%2520a%2520framework%2520that%2520leverages%250Atemporal%2520Correspondences%2520to%2520extend%2520existing%25203D%2520reconstruction%2520formulation%2520to%250A4D.%2520Specifically%252C%2520apart%2520from%2520predicting%2520pointmaps%252C%2520C4D%2520captures%2520two%2520types%2520of%250Acorrespondences%253A%2520short-term%2520optical%2520flow%2520and%2520long-term%2520point%2520tracking.%2520We%2520train%250Aa%2520dynamic-aware%2520point%2520tracker%2520that%2520provides%2520additional%2520mobility%2520information%252C%250Afacilitating%2520the%2520estimation%2520of%2520motion%2520masks%2520to%2520separate%2520moving%2520elements%2520from%250Athe%2520static%2520background%252C%2520thus%2520offering%2520more%2520reliable%2520guidance%2520for%2520dynamic%2520scenes.%250AFurthermore%252C%2520we%2520introduce%2520a%2520set%2520of%2520dynamic%2520scene%2520optimization%2520objectives%2520to%250Arecover%2520per-frame%25203D%2520geometry%2520and%2520camera%2520parameters.%2520Simultaneously%252C%2520the%250Acorrespondences%2520lift%25202D%2520trajectories%2520into%2520smooth%25203D%2520trajectories%252C%2520enabling%250Afully%2520integrated%25204D%2520reconstruction.%2520Experiments%2520show%2520that%2520our%2520framework%250Aachieves%2520complete%25204D%2520recovery%2520and%2520demonstrates%2520strong%2520performance%2520across%250Amultiple%2520downstream%2520tasks%252C%2520including%2520depth%2520estimation%252C%2520camera%2520pose%2520estimation%252C%250Aand%2520point%2520tracking.%2520Project%2520Page%253A%2520https%253A//littlepure2333.github.io/C4D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C4D%3A%204D%20Made%20from%203D%20through%20Dual%20Correspondences&entry.906535625=Shizun%20Wang%20and%20Zhenxiang%20Jiang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Recovering%204D%20from%20monocular%20video%2C%20which%20jointly%20estimates%20dynamic%20geometry%0Aand%20camera%20poses%2C%20is%20an%20inevitably%20challenging%20problem.%20While%20recent%0Apointmap-based%203D%20reconstruction%20methods%20%28e.g.%2C%20DUSt3R%29%20have%20made%20great%0Aprogress%20in%20reconstructing%20static%20scenes%2C%20directly%20applying%20them%20to%20dynamic%0Ascenes%20leads%20to%20inaccurate%20results.%20This%20discrepancy%20arises%20because%20moving%0Aobjects%20violate%20multi-view%20geometric%20constraints%2C%20disrupting%20the%0Areconstruction.%20To%20address%20this%2C%20we%20introduce%20C4D%2C%20a%20framework%20that%20leverages%0Atemporal%20Correspondences%20to%20extend%20existing%203D%20reconstruction%20formulation%20to%0A4D.%20Specifically%2C%20apart%20from%20predicting%20pointmaps%2C%20C4D%20captures%20two%20types%20of%0Acorrespondences%3A%20short-term%20optical%20flow%20and%20long-term%20point%20tracking.%20We%20train%0Aa%20dynamic-aware%20point%20tracker%20that%20provides%20additional%20mobility%20information%2C%0Afacilitating%20the%20estimation%20of%20motion%20masks%20to%20separate%20moving%20elements%20from%0Athe%20static%20background%2C%20thus%20offering%20more%20reliable%20guidance%20for%20dynamic%20scenes.%0AFurthermore%2C%20we%20introduce%20a%20set%20of%20dynamic%20scene%20optimization%20objectives%20to%0Arecover%20per-frame%203D%20geometry%20and%20camera%20parameters.%20Simultaneously%2C%20the%0Acorrespondences%20lift%202D%20trajectories%20into%20smooth%203D%20trajectories%2C%20enabling%0Afully%20integrated%204D%20reconstruction.%20Experiments%20show%20that%20our%20framework%0Aachieves%20complete%204D%20recovery%20and%20demonstrates%20strong%20performance%20across%0Amultiple%20downstream%20tasks%2C%20including%20depth%20estimation%2C%20camera%20pose%20estimation%2C%0Aand%20point%20tracking.%20Project%20Page%3A%20https%3A//littlepure2333.github.io/C4D%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14960v1&entry.124074799=Read"},
{"title": "Disentangled and Self-Explainable Node Representation Learning", "author": "Simone Piaggesi and Andr\u00e9 Panisson and Megha Khosla", "abstract": "  Node representations, or embeddings, are low-dimensional vectors that capture\nnode properties, typically learned through unsupervised structural similarity\nobjectives or supervised tasks. While recent efforts have focused on explaining\ngraph model decisions, the interpretability of unsupervised node embeddings\nremains underexplored. To bridge this gap, we introduce DiSeNE (Disentangled\nand Self-Explainable Node Embedding), a framework that generates\nself-explainable embeddings in an unsupervised manner. Our method employs\ndisentangled representation learning to produce dimension-wise interpretable\nembeddings, where each dimension is aligned with distinct topological structure\nof the graph. We formalize novel desiderata for disentangled and interpretable\nembeddings, which drive our new objective functions, optimizing simultaneously\nfor both interpretability and disentanglement. Additionally, we propose several\nnew metrics to evaluate representation quality and human interpretability.\nExtensive experiments across multiple benchmark datasets demonstrate the\neffectiveness of our approach.\n", "link": "http://arxiv.org/abs/2410.21043v2", "date": "2025-10-16", "relevancy": 2.5776, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20and%20Self-Explainable%20Node%20Representation%20Learning&body=Title%3A%20Disentangled%20and%20Self-Explainable%20Node%20Representation%20Learning%0AAuthor%3A%20Simone%20Piaggesi%20and%20Andr%C3%A9%20Panisson%20and%20Megha%20Khosla%0AAbstract%3A%20%20%20Node%20representations%2C%20or%20embeddings%2C%20are%20low-dimensional%20vectors%20that%20capture%0Anode%20properties%2C%20typically%20learned%20through%20unsupervised%20structural%20similarity%0Aobjectives%20or%20supervised%20tasks.%20While%20recent%20efforts%20have%20focused%20on%20explaining%0Agraph%20model%20decisions%2C%20the%20interpretability%20of%20unsupervised%20node%20embeddings%0Aremains%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DiSeNE%20%28Disentangled%0Aand%20Self-Explainable%20Node%20Embedding%29%2C%20a%20framework%20that%20generates%0Aself-explainable%20embeddings%20in%20an%20unsupervised%20manner.%20Our%20method%20employs%0Adisentangled%20representation%20learning%20to%20produce%20dimension-wise%20interpretable%0Aembeddings%2C%20where%20each%20dimension%20is%20aligned%20with%20distinct%20topological%20structure%0Aof%20the%20graph.%20We%20formalize%20novel%20desiderata%20for%20disentangled%20and%20interpretable%0Aembeddings%2C%20which%20drive%20our%20new%20objective%20functions%2C%20optimizing%20simultaneously%0Afor%20both%20interpretability%20and%20disentanglement.%20Additionally%2C%20we%20propose%20several%0Anew%20metrics%20to%20evaluate%20representation%20quality%20and%20human%20interpretability.%0AExtensive%20experiments%20across%20multiple%20benchmark%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520and%2520Self-Explainable%2520Node%2520Representation%2520Learning%26entry.906535625%3DSimone%2520Piaggesi%2520and%2520Andr%25C3%25A9%2520Panisson%2520and%2520Megha%2520Khosla%26entry.1292438233%3D%2520%2520Node%2520representations%252C%2520or%2520embeddings%252C%2520are%2520low-dimensional%2520vectors%2520that%2520capture%250Anode%2520properties%252C%2520typically%2520learned%2520through%2520unsupervised%2520structural%2520similarity%250Aobjectives%2520or%2520supervised%2520tasks.%2520While%2520recent%2520efforts%2520have%2520focused%2520on%2520explaining%250Agraph%2520model%2520decisions%252C%2520the%2520interpretability%2520of%2520unsupervised%2520node%2520embeddings%250Aremains%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520DiSeNE%2520%2528Disentangled%250Aand%2520Self-Explainable%2520Node%2520Embedding%2529%252C%2520a%2520framework%2520that%2520generates%250Aself-explainable%2520embeddings%2520in%2520an%2520unsupervised%2520manner.%2520Our%2520method%2520employs%250Adisentangled%2520representation%2520learning%2520to%2520produce%2520dimension-wise%2520interpretable%250Aembeddings%252C%2520where%2520each%2520dimension%2520is%2520aligned%2520with%2520distinct%2520topological%2520structure%250Aof%2520the%2520graph.%2520We%2520formalize%2520novel%2520desiderata%2520for%2520disentangled%2520and%2520interpretable%250Aembeddings%252C%2520which%2520drive%2520our%2520new%2520objective%2520functions%252C%2520optimizing%2520simultaneously%250Afor%2520both%2520interpretability%2520and%2520disentanglement.%2520Additionally%252C%2520we%2520propose%2520several%250Anew%2520metrics%2520to%2520evaluate%2520representation%2520quality%2520and%2520human%2520interpretability.%250AExtensive%2520experiments%2520across%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20and%20Self-Explainable%20Node%20Representation%20Learning&entry.906535625=Simone%20Piaggesi%20and%20Andr%C3%A9%20Panisson%20and%20Megha%20Khosla&entry.1292438233=%20%20Node%20representations%2C%20or%20embeddings%2C%20are%20low-dimensional%20vectors%20that%20capture%0Anode%20properties%2C%20typically%20learned%20through%20unsupervised%20structural%20similarity%0Aobjectives%20or%20supervised%20tasks.%20While%20recent%20efforts%20have%20focused%20on%20explaining%0Agraph%20model%20decisions%2C%20the%20interpretability%20of%20unsupervised%20node%20embeddings%0Aremains%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DiSeNE%20%28Disentangled%0Aand%20Self-Explainable%20Node%20Embedding%29%2C%20a%20framework%20that%20generates%0Aself-explainable%20embeddings%20in%20an%20unsupervised%20manner.%20Our%20method%20employs%0Adisentangled%20representation%20learning%20to%20produce%20dimension-wise%20interpretable%0Aembeddings%2C%20where%20each%20dimension%20is%20aligned%20with%20distinct%20topological%20structure%0Aof%20the%20graph.%20We%20formalize%20novel%20desiderata%20for%20disentangled%20and%20interpretable%0Aembeddings%2C%20which%20drive%20our%20new%20objective%20functions%2C%20optimizing%20simultaneously%0Afor%20both%20interpretability%20and%20disentanglement.%20Additionally%2C%20we%20propose%20several%0Anew%20metrics%20to%20evaluate%20representation%20quality%20and%20human%20interpretability.%0AExtensive%20experiments%20across%20multiple%20benchmark%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21043v2&entry.124074799=Read"},
{"title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation", "author": "Shaowei Liu and Chuan Guo and Bing Zhou and Jian Wang", "abstract": "  Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.\n", "link": "http://arxiv.org/abs/2510.14976v1", "date": "2025-10-16", "relevancy": 2.5753, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6634}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6608}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ponimator%3A%20Unfolding%20Interactive%20Pose%20for%20Versatile%20Human-human%0A%20%20Interaction%20Animation&body=Title%3A%20Ponimator%3A%20Unfolding%20Interactive%20Pose%20for%20Versatile%20Human-human%0A%20%20Interaction%20Animation%0AAuthor%3A%20Shaowei%20Liu%20and%20Chuan%20Guo%20and%20Bing%20Zhou%20and%20Jian%20Wang%0AAbstract%3A%20%20%20Close-proximity%20human-human%20interactive%20poses%20convey%20rich%20contextual%0Ainformation%20about%20interaction%20dynamics.%20Given%20such%20poses%2C%20humans%20can%0Aintuitively%20infer%20the%20context%20and%20anticipate%20possible%20past%20and%20future%20dynamics%2C%0Adrawing%20on%20strong%20priors%20of%20human%20behavior.%20Inspired%20by%20this%20observation%2C%20we%0Apropose%20Ponimator%2C%20a%20simple%20framework%20anchored%20on%20proximal%20interactive%20poses%0Afor%20versatile%20interaction%20animation.%20Our%20training%20data%20consists%20of%0Aclose-contact%20two-person%20poses%20and%20their%20surrounding%20temporal%20context%20from%0Amotion-capture%20interaction%20datasets.%20Leveraging%20interactive%20pose%20priors%2C%0APonimator%20employs%20two%20conditional%20diffusion%20models%3A%20%281%29%20a%20pose%20animator%20that%0Auses%20the%20temporal%20prior%20to%20generate%20dynamic%20motion%20sequences%20from%20interactive%0Aposes%2C%20and%20%282%29%20a%20pose%20generator%20that%20applies%20the%20spatial%20prior%20to%20synthesize%0Ainteractive%20poses%20from%20a%20single%20pose%2C%20text%2C%20or%20both%20when%20interactive%20poses%20are%0Aunavailable.%20Collectively%2C%20Ponimator%20supports%20diverse%20tasks%2C%20including%0Aimage-based%20interaction%20animation%2C%20reaction%20animation%2C%20and%20text-to-interaction%0Asynthesis%2C%20facilitating%20the%20transfer%20of%20interaction%20knowledge%20from%20high-quality%0Amocap%20data%20to%20open-world%20scenarios.%20Empirical%20experiments%20across%20diverse%0Adatasets%20and%20applications%20demonstrate%20the%20universality%20of%20the%20pose%20prior%20and%0Athe%20effectiveness%20and%20robustness%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPonimator%253A%2520Unfolding%2520Interactive%2520Pose%2520for%2520Versatile%2520Human-human%250A%2520%2520Interaction%2520Animation%26entry.906535625%3DShaowei%2520Liu%2520and%2520Chuan%2520Guo%2520and%2520Bing%2520Zhou%2520and%2520Jian%2520Wang%26entry.1292438233%3D%2520%2520Close-proximity%2520human-human%2520interactive%2520poses%2520convey%2520rich%2520contextual%250Ainformation%2520about%2520interaction%2520dynamics.%2520Given%2520such%2520poses%252C%2520humans%2520can%250Aintuitively%2520infer%2520the%2520context%2520and%2520anticipate%2520possible%2520past%2520and%2520future%2520dynamics%252C%250Adrawing%2520on%2520strong%2520priors%2520of%2520human%2520behavior.%2520Inspired%2520by%2520this%2520observation%252C%2520we%250Apropose%2520Ponimator%252C%2520a%2520simple%2520framework%2520anchored%2520on%2520proximal%2520interactive%2520poses%250Afor%2520versatile%2520interaction%2520animation.%2520Our%2520training%2520data%2520consists%2520of%250Aclose-contact%2520two-person%2520poses%2520and%2520their%2520surrounding%2520temporal%2520context%2520from%250Amotion-capture%2520interaction%2520datasets.%2520Leveraging%2520interactive%2520pose%2520priors%252C%250APonimator%2520employs%2520two%2520conditional%2520diffusion%2520models%253A%2520%25281%2529%2520a%2520pose%2520animator%2520that%250Auses%2520the%2520temporal%2520prior%2520to%2520generate%2520dynamic%2520motion%2520sequences%2520from%2520interactive%250Aposes%252C%2520and%2520%25282%2529%2520a%2520pose%2520generator%2520that%2520applies%2520the%2520spatial%2520prior%2520to%2520synthesize%250Ainteractive%2520poses%2520from%2520a%2520single%2520pose%252C%2520text%252C%2520or%2520both%2520when%2520interactive%2520poses%2520are%250Aunavailable.%2520Collectively%252C%2520Ponimator%2520supports%2520diverse%2520tasks%252C%2520including%250Aimage-based%2520interaction%2520animation%252C%2520reaction%2520animation%252C%2520and%2520text-to-interaction%250Asynthesis%252C%2520facilitating%2520the%2520transfer%2520of%2520interaction%2520knowledge%2520from%2520high-quality%250Amocap%2520data%2520to%2520open-world%2520scenarios.%2520Empirical%2520experiments%2520across%2520diverse%250Adatasets%2520and%2520applications%2520demonstrate%2520the%2520universality%2520of%2520the%2520pose%2520prior%2520and%250Athe%2520effectiveness%2520and%2520robustness%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ponimator%3A%20Unfolding%20Interactive%20Pose%20for%20Versatile%20Human-human%0A%20%20Interaction%20Animation&entry.906535625=Shaowei%20Liu%20and%20Chuan%20Guo%20and%20Bing%20Zhou%20and%20Jian%20Wang&entry.1292438233=%20%20Close-proximity%20human-human%20interactive%20poses%20convey%20rich%20contextual%0Ainformation%20about%20interaction%20dynamics.%20Given%20such%20poses%2C%20humans%20can%0Aintuitively%20infer%20the%20context%20and%20anticipate%20possible%20past%20and%20future%20dynamics%2C%0Adrawing%20on%20strong%20priors%20of%20human%20behavior.%20Inspired%20by%20this%20observation%2C%20we%0Apropose%20Ponimator%2C%20a%20simple%20framework%20anchored%20on%20proximal%20interactive%20poses%0Afor%20versatile%20interaction%20animation.%20Our%20training%20data%20consists%20of%0Aclose-contact%20two-person%20poses%20and%20their%20surrounding%20temporal%20context%20from%0Amotion-capture%20interaction%20datasets.%20Leveraging%20interactive%20pose%20priors%2C%0APonimator%20employs%20two%20conditional%20diffusion%20models%3A%20%281%29%20a%20pose%20animator%20that%0Auses%20the%20temporal%20prior%20to%20generate%20dynamic%20motion%20sequences%20from%20interactive%0Aposes%2C%20and%20%282%29%20a%20pose%20generator%20that%20applies%20the%20spatial%20prior%20to%20synthesize%0Ainteractive%20poses%20from%20a%20single%20pose%2C%20text%2C%20or%20both%20when%20interactive%20poses%20are%0Aunavailable.%20Collectively%2C%20Ponimator%20supports%20diverse%20tasks%2C%20including%0Aimage-based%20interaction%20animation%2C%20reaction%20animation%2C%20and%20text-to-interaction%0Asynthesis%2C%20facilitating%20the%20transfer%20of%20interaction%20knowledge%20from%20high-quality%0Amocap%20data%20to%20open-world%20scenarios.%20Empirical%20experiments%20across%20diverse%0Adatasets%20and%20applications%20demonstrate%20the%20universality%20of%20the%20pose%20prior%20and%0Athe%20effectiveness%20and%20robustness%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14976v1&entry.124074799=Read"},
{"title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing", "author": "Hadi Alzayer and Yunzhi Zhang and Chen Geng and Jia-Bin Huang and Jiajun Wu", "abstract": "  We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.\n", "link": "http://arxiv.org/abs/2510.14981v1", "date": "2025-10-16", "relevancy": 2.5656, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6429}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6429}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coupled%20Diffusion%20Sampling%20for%20Training-Free%20Multi-View%20Image%20Editing&body=Title%3A%20Coupled%20Diffusion%20Sampling%20for%20Training-Free%20Multi-View%20Image%20Editing%0AAuthor%3A%20Hadi%20Alzayer%20and%20Yunzhi%20Zhang%20and%20Chen%20Geng%20and%20Jia-Bin%20Huang%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20We%20present%20an%20inference-time%20diffusion%20sampling%20method%20to%20perform%20multi-view%0Aconsistent%20image%20editing%20using%20pre-trained%202D%20image%20editing%20models.%20These%0Amodels%20can%20independently%20produce%20high-quality%20edits%20for%20each%20image%20in%20a%20set%20of%0Amulti-view%20images%20of%20a%203D%20scene%20or%20object%2C%20but%20they%20do%20not%20maintain%20consistency%0Aacross%20views.%20Existing%20approaches%20typically%20address%20this%20by%20optimizing%20over%0Aexplicit%203D%20representations%2C%20but%20they%20suffer%20from%20a%20lengthy%20optimization%0Aprocess%20and%20instability%20under%20sparse%20view%20settings.%20We%20propose%20an%20implicit%203D%0Aregularization%20approach%20by%20constraining%20the%20generated%202D%20image%20sequences%20to%0Aadhere%20to%20a%20pre-trained%20multi-view%20image%20distribution.%20This%20is%20achieved%20through%0Acoupled%20diffusion%20sampling%2C%20a%20simple%20diffusion%20sampling%20technique%20that%0Aconcurrently%20samples%20two%20trajectories%20from%20both%20a%20multi-view%20image%20distribution%0Aand%20a%202D%20edited%20image%20distribution%2C%20using%20a%20coupling%20term%20to%20enforce%20the%0Amulti-view%20consistency%20among%20the%20generated%20images.%20We%20validate%20the%0Aeffectiveness%20and%20generality%20of%20this%20framework%20on%20three%20distinct%20multi-view%0Aimage%20editing%20tasks%2C%20demonstrating%20its%20applicability%20across%20various%20model%0Aarchitectures%20and%20highlighting%20its%20potential%20as%20a%20general%20solution%20for%0Amulti-view%20consistent%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoupled%2520Diffusion%2520Sampling%2520for%2520Training-Free%2520Multi-View%2520Image%2520Editing%26entry.906535625%3DHadi%2520Alzayer%2520and%2520Yunzhi%2520Zhang%2520and%2520Chen%2520Geng%2520and%2520Jia-Bin%2520Huang%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520inference-time%2520diffusion%2520sampling%2520method%2520to%2520perform%2520multi-view%250Aconsistent%2520image%2520editing%2520using%2520pre-trained%25202D%2520image%2520editing%2520models.%2520These%250Amodels%2520can%2520independently%2520produce%2520high-quality%2520edits%2520for%2520each%2520image%2520in%2520a%2520set%2520of%250Amulti-view%2520images%2520of%2520a%25203D%2520scene%2520or%2520object%252C%2520but%2520they%2520do%2520not%2520maintain%2520consistency%250Aacross%2520views.%2520Existing%2520approaches%2520typically%2520address%2520this%2520by%2520optimizing%2520over%250Aexplicit%25203D%2520representations%252C%2520but%2520they%2520suffer%2520from%2520a%2520lengthy%2520optimization%250Aprocess%2520and%2520instability%2520under%2520sparse%2520view%2520settings.%2520We%2520propose%2520an%2520implicit%25203D%250Aregularization%2520approach%2520by%2520constraining%2520the%2520generated%25202D%2520image%2520sequences%2520to%250Aadhere%2520to%2520a%2520pre-trained%2520multi-view%2520image%2520distribution.%2520This%2520is%2520achieved%2520through%250Acoupled%2520diffusion%2520sampling%252C%2520a%2520simple%2520diffusion%2520sampling%2520technique%2520that%250Aconcurrently%2520samples%2520two%2520trajectories%2520from%2520both%2520a%2520multi-view%2520image%2520distribution%250Aand%2520a%25202D%2520edited%2520image%2520distribution%252C%2520using%2520a%2520coupling%2520term%2520to%2520enforce%2520the%250Amulti-view%2520consistency%2520among%2520the%2520generated%2520images.%2520We%2520validate%2520the%250Aeffectiveness%2520and%2520generality%2520of%2520this%2520framework%2520on%2520three%2520distinct%2520multi-view%250Aimage%2520editing%2520tasks%252C%2520demonstrating%2520its%2520applicability%2520across%2520various%2520model%250Aarchitectures%2520and%2520highlighting%2520its%2520potential%2520as%2520a%2520general%2520solution%2520for%250Amulti-view%2520consistent%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coupled%20Diffusion%20Sampling%20for%20Training-Free%20Multi-View%20Image%20Editing&entry.906535625=Hadi%20Alzayer%20and%20Yunzhi%20Zhang%20and%20Chen%20Geng%20and%20Jia-Bin%20Huang%20and%20Jiajun%20Wu&entry.1292438233=%20%20We%20present%20an%20inference-time%20diffusion%20sampling%20method%20to%20perform%20multi-view%0Aconsistent%20image%20editing%20using%20pre-trained%202D%20image%20editing%20models.%20These%0Amodels%20can%20independently%20produce%20high-quality%20edits%20for%20each%20image%20in%20a%20set%20of%0Amulti-view%20images%20of%20a%203D%20scene%20or%20object%2C%20but%20they%20do%20not%20maintain%20consistency%0Aacross%20views.%20Existing%20approaches%20typically%20address%20this%20by%20optimizing%20over%0Aexplicit%203D%20representations%2C%20but%20they%20suffer%20from%20a%20lengthy%20optimization%0Aprocess%20and%20instability%20under%20sparse%20view%20settings.%20We%20propose%20an%20implicit%203D%0Aregularization%20approach%20by%20constraining%20the%20generated%202D%20image%20sequences%20to%0Aadhere%20to%20a%20pre-trained%20multi-view%20image%20distribution.%20This%20is%20achieved%20through%0Acoupled%20diffusion%20sampling%2C%20a%20simple%20diffusion%20sampling%20technique%20that%0Aconcurrently%20samples%20two%20trajectories%20from%20both%20a%20multi-view%20image%20distribution%0Aand%20a%202D%20edited%20image%20distribution%2C%20using%20a%20coupling%20term%20to%20enforce%20the%0Amulti-view%20consistency%20among%20the%20generated%20images.%20We%20validate%20the%0Aeffectiveness%20and%20generality%20of%20this%20framework%20on%20three%20distinct%20multi-view%0Aimage%20editing%20tasks%2C%20demonstrating%20its%20applicability%20across%20various%20model%0Aarchitectures%20and%20highlighting%20its%20potential%20as%20a%20general%20solution%20for%0Amulti-view%20consistent%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14981v1&entry.124074799=Read"},
{"title": "Higher-order interactions of multi-layer prompt", "author": "Ziyu Zheng and Yaming Yang and Ziyu Guan and Wei Zhao and Xinyan Huang and Weigang Lu", "abstract": "  The \"pre-train, prompt\" paradigm has successfully evolved in representation\nlearning. While current prompt-tuning methods often introduce learnable\nprompts, they predominantly treat prompts as isolated, independent components\nacross different network layers. This overlooks the complex and synergistic\nhigher-order interactions that exist between prompts at various hierarchical\ndepths, consequently limiting the expressive power and semantic richness of the\nprompted model. To address this fundamental gap, we propose a novel framework\nthat explicitly models the Higher-order Interactions of Multi-layer Prompt. Our\napproach conceptualizes prompts from different layers not as separate entities,\nbut as a cohesive system where their inter-relationships are critical. We\ndesign an innovative interaction module that captures these sophisticated,\nnon-linear correlations among multi-layer prompts, effectively modeling their\ncooperative effects. This allows the model to dynamically aggregate and refine\nprompt information across the network's depth, leading to a more integrated and\npowerful prompting strategy. Extensive experiments on eight benchmark datasets\ndemonstrate that our method, by leveraging these higher-order interactions,\nconsistently surpasses state-of-the-art prompt-tuning baselines. The\nperformance advantage is particularly pronounced in few-shot scenarios,\nvalidating that capturing the intricate interplay between multi-layer prompts\nis key to unlocking more robust and generalizable representation learning.\n", "link": "http://arxiv.org/abs/2510.09394v2", "date": "2025-10-16", "relevancy": 2.5571, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-order%20interactions%20of%20multi-layer%20prompt&body=Title%3A%20Higher-order%20interactions%20of%20multi-layer%20prompt%0AAuthor%3A%20Ziyu%20Zheng%20and%20Yaming%20Yang%20and%20Ziyu%20Guan%20and%20Wei%20Zhao%20and%20Xinyan%20Huang%20and%20Weigang%20Lu%0AAbstract%3A%20%20%20The%20%22pre-train%2C%20prompt%22%20paradigm%20has%20successfully%20evolved%20in%20representation%0Alearning.%20While%20current%20prompt-tuning%20methods%20often%20introduce%20learnable%0Aprompts%2C%20they%20predominantly%20treat%20prompts%20as%20isolated%2C%20independent%20components%0Aacross%20different%20network%20layers.%20This%20overlooks%20the%20complex%20and%20synergistic%0Ahigher-order%20interactions%20that%20exist%20between%20prompts%20at%20various%20hierarchical%0Adepths%2C%20consequently%20limiting%20the%20expressive%20power%20and%20semantic%20richness%20of%20the%0Aprompted%20model.%20To%20address%20this%20fundamental%20gap%2C%20we%20propose%20a%20novel%20framework%0Athat%20explicitly%20models%20the%20Higher-order%20Interactions%20of%20Multi-layer%20Prompt.%20Our%0Aapproach%20conceptualizes%20prompts%20from%20different%20layers%20not%20as%20separate%20entities%2C%0Abut%20as%20a%20cohesive%20system%20where%20their%20inter-relationships%20are%20critical.%20We%0Adesign%20an%20innovative%20interaction%20module%20that%20captures%20these%20sophisticated%2C%0Anon-linear%20correlations%20among%20multi-layer%20prompts%2C%20effectively%20modeling%20their%0Acooperative%20effects.%20This%20allows%20the%20model%20to%20dynamically%20aggregate%20and%20refine%0Aprompt%20information%20across%20the%20network%27s%20depth%2C%20leading%20to%20a%20more%20integrated%20and%0Apowerful%20prompting%20strategy.%20Extensive%20experiments%20on%20eight%20benchmark%20datasets%0Ademonstrate%20that%20our%20method%2C%20by%20leveraging%20these%20higher-order%20interactions%2C%0Aconsistently%20surpasses%20state-of-the-art%20prompt-tuning%20baselines.%20The%0Aperformance%20advantage%20is%20particularly%20pronounced%20in%20few-shot%20scenarios%2C%0Avalidating%20that%20capturing%20the%20intricate%20interplay%20between%20multi-layer%20prompts%0Ais%20key%20to%20unlocking%20more%20robust%20and%20generalizable%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-order%2520interactions%2520of%2520multi-layer%2520prompt%26entry.906535625%3DZiyu%2520Zheng%2520and%2520Yaming%2520Yang%2520and%2520Ziyu%2520Guan%2520and%2520Wei%2520Zhao%2520and%2520Xinyan%2520Huang%2520and%2520Weigang%2520Lu%26entry.1292438233%3D%2520%2520The%2520%2522pre-train%252C%2520prompt%2522%2520paradigm%2520has%2520successfully%2520evolved%2520in%2520representation%250Alearning.%2520While%2520current%2520prompt-tuning%2520methods%2520often%2520introduce%2520learnable%250Aprompts%252C%2520they%2520predominantly%2520treat%2520prompts%2520as%2520isolated%252C%2520independent%2520components%250Aacross%2520different%2520network%2520layers.%2520This%2520overlooks%2520the%2520complex%2520and%2520synergistic%250Ahigher-order%2520interactions%2520that%2520exist%2520between%2520prompts%2520at%2520various%2520hierarchical%250Adepths%252C%2520consequently%2520limiting%2520the%2520expressive%2520power%2520and%2520semantic%2520richness%2520of%2520the%250Aprompted%2520model.%2520To%2520address%2520this%2520fundamental%2520gap%252C%2520we%2520propose%2520a%2520novel%2520framework%250Athat%2520explicitly%2520models%2520the%2520Higher-order%2520Interactions%2520of%2520Multi-layer%2520Prompt.%2520Our%250Aapproach%2520conceptualizes%2520prompts%2520from%2520different%2520layers%2520not%2520as%2520separate%2520entities%252C%250Abut%2520as%2520a%2520cohesive%2520system%2520where%2520their%2520inter-relationships%2520are%2520critical.%2520We%250Adesign%2520an%2520innovative%2520interaction%2520module%2520that%2520captures%2520these%2520sophisticated%252C%250Anon-linear%2520correlations%2520among%2520multi-layer%2520prompts%252C%2520effectively%2520modeling%2520their%250Acooperative%2520effects.%2520This%2520allows%2520the%2520model%2520to%2520dynamically%2520aggregate%2520and%2520refine%250Aprompt%2520information%2520across%2520the%2520network%2527s%2520depth%252C%2520leading%2520to%2520a%2520more%2520integrated%2520and%250Apowerful%2520prompting%2520strategy.%2520Extensive%2520experiments%2520on%2520eight%2520benchmark%2520datasets%250Ademonstrate%2520that%2520our%2520method%252C%2520by%2520leveraging%2520these%2520higher-order%2520interactions%252C%250Aconsistently%2520surpasses%2520state-of-the-art%2520prompt-tuning%2520baselines.%2520The%250Aperformance%2520advantage%2520is%2520particularly%2520pronounced%2520in%2520few-shot%2520scenarios%252C%250Avalidating%2520that%2520capturing%2520the%2520intricate%2520interplay%2520between%2520multi-layer%2520prompts%250Ais%2520key%2520to%2520unlocking%2520more%2520robust%2520and%2520generalizable%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-order%20interactions%20of%20multi-layer%20prompt&entry.906535625=Ziyu%20Zheng%20and%20Yaming%20Yang%20and%20Ziyu%20Guan%20and%20Wei%20Zhao%20and%20Xinyan%20Huang%20and%20Weigang%20Lu&entry.1292438233=%20%20The%20%22pre-train%2C%20prompt%22%20paradigm%20has%20successfully%20evolved%20in%20representation%0Alearning.%20While%20current%20prompt-tuning%20methods%20often%20introduce%20learnable%0Aprompts%2C%20they%20predominantly%20treat%20prompts%20as%20isolated%2C%20independent%20components%0Aacross%20different%20network%20layers.%20This%20overlooks%20the%20complex%20and%20synergistic%0Ahigher-order%20interactions%20that%20exist%20between%20prompts%20at%20various%20hierarchical%0Adepths%2C%20consequently%20limiting%20the%20expressive%20power%20and%20semantic%20richness%20of%20the%0Aprompted%20model.%20To%20address%20this%20fundamental%20gap%2C%20we%20propose%20a%20novel%20framework%0Athat%20explicitly%20models%20the%20Higher-order%20Interactions%20of%20Multi-layer%20Prompt.%20Our%0Aapproach%20conceptualizes%20prompts%20from%20different%20layers%20not%20as%20separate%20entities%2C%0Abut%20as%20a%20cohesive%20system%20where%20their%20inter-relationships%20are%20critical.%20We%0Adesign%20an%20innovative%20interaction%20module%20that%20captures%20these%20sophisticated%2C%0Anon-linear%20correlations%20among%20multi-layer%20prompts%2C%20effectively%20modeling%20their%0Acooperative%20effects.%20This%20allows%20the%20model%20to%20dynamically%20aggregate%20and%20refine%0Aprompt%20information%20across%20the%20network%27s%20depth%2C%20leading%20to%20a%20more%20integrated%20and%0Apowerful%20prompting%20strategy.%20Extensive%20experiments%20on%20eight%20benchmark%20datasets%0Ademonstrate%20that%20our%20method%2C%20by%20leveraging%20these%20higher-order%20interactions%2C%0Aconsistently%20surpasses%20state-of-the-art%20prompt-tuning%20baselines.%20The%0Aperformance%20advantage%20is%20particularly%20pronounced%20in%20few-shot%20scenarios%2C%0Avalidating%20that%20capturing%20the%20intricate%20interplay%20between%20multi-layer%20prompts%0Ais%20key%20to%20unlocking%20more%20robust%20and%20generalizable%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09394v2&entry.124074799=Read"},
{"title": "Hopfield-Fenchel-Young Networks: A Unified Framework for Associative\n  Memory Retrieval", "author": "Saul Santos and Vlad Niculae and Daniel McNamee and Andr\u00e9 F. T. Martins", "abstract": "  Associative memory models, such as Hopfield networks and their modern\nvariants, have garnered renewed interest due to advancements in memory capacity\nand connections with self-attention in transformers. In this work, we introduce\na unified framework-Hopfield-Fenchel-Young networks-which generalizes these\nmodels to a broader family of energy functions. Our energies are formulated as\nthe difference between two Fenchel-Young losses: one, parameterized by a\ngeneralized entropy, defines the Hopfield scoring mechanism, while the other\napplies a post-transformation to the Hopfield output. By utilizing Tsallis and\nnorm entropies, we derive end-to-end differentiable update rules that enable\nsparse transformations, uncovering new connections between loss margins,\nsparsity, and exact retrieval of single memory patterns. We further extend this\nframework to structured Hopfield networks using the SparseMAP transformation,\nallowing the retrieval of pattern associations rather than a single pattern.\nOur framework unifies and extends traditional and modern Hopfield networks and\nprovides an energy minimization perspective for widely used\npost-transformations like $\\ell_2$-normalization and layer normalization-all\nthrough suitable choices of Fenchel-Young losses and by using convex analysis\nas a building block. Finally, we validate our Hopfield-Fenchel-Young networks\non diverse memory recall tasks, including free and sequential recall.\nExperiments on simulated data, image retrieval, multiple instance learning, and\ntext rationalization demonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2411.08590v3", "date": "2025-10-16", "relevancy": 2.5566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5199}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5071}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hopfield-Fenchel-Young%20Networks%3A%20A%20Unified%20Framework%20for%20Associative%0A%20%20Memory%20Retrieval&body=Title%3A%20Hopfield-Fenchel-Young%20Networks%3A%20A%20Unified%20Framework%20for%20Associative%0A%20%20Memory%20Retrieval%0AAuthor%3A%20Saul%20Santos%20and%20Vlad%20Niculae%20and%20Daniel%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Associative%20memory%20models%2C%20such%20as%20Hopfield%20networks%20and%20their%20modern%0Avariants%2C%20have%20garnered%20renewed%20interest%20due%20to%20advancements%20in%20memory%20capacity%0Aand%20connections%20with%20self-attention%20in%20transformers.%20In%20this%20work%2C%20we%20introduce%0Aa%20unified%20framework-Hopfield-Fenchel-Young%20networks-which%20generalizes%20these%0Amodels%20to%20a%20broader%20family%20of%20energy%20functions.%20Our%20energies%20are%20formulated%20as%0Athe%20difference%20between%20two%20Fenchel-Young%20losses%3A%20one%2C%20parameterized%20by%20a%0Ageneralized%20entropy%2C%20defines%20the%20Hopfield%20scoring%20mechanism%2C%20while%20the%20other%0Aapplies%20a%20post-transformation%20to%20the%20Hopfield%20output.%20By%20utilizing%20Tsallis%20and%0Anorm%20entropies%2C%20we%20derive%20end-to-end%20differentiable%20update%20rules%20that%20enable%0Asparse%20transformations%2C%20uncovering%20new%20connections%20between%20loss%20margins%2C%0Asparsity%2C%20and%20exact%20retrieval%20of%20single%20memory%20patterns.%20We%20further%20extend%20this%0Aframework%20to%20structured%20Hopfield%20networks%20using%20the%20SparseMAP%20transformation%2C%0Aallowing%20the%20retrieval%20of%20pattern%20associations%20rather%20than%20a%20single%20pattern.%0AOur%20framework%20unifies%20and%20extends%20traditional%20and%20modern%20Hopfield%20networks%20and%0Aprovides%20an%20energy%20minimization%20perspective%20for%20widely%20used%0Apost-transformations%20like%20%24%5Cell_2%24-normalization%20and%20layer%20normalization-all%0Athrough%20suitable%20choices%20of%20Fenchel-Young%20losses%20and%20by%20using%20convex%20analysis%0Aas%20a%20building%20block.%20Finally%2C%20we%20validate%20our%20Hopfield-Fenchel-Young%20networks%0Aon%20diverse%20memory%20recall%20tasks%2C%20including%20free%20and%20sequential%20recall.%0AExperiments%20on%20simulated%20data%2C%20image%20retrieval%2C%20multiple%20instance%20learning%2C%20and%0Atext%20rationalization%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08590v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHopfield-Fenchel-Young%2520Networks%253A%2520A%2520Unified%2520Framework%2520for%2520Associative%250A%2520%2520Memory%2520Retrieval%26entry.906535625%3DSaul%2520Santos%2520and%2520Vlad%2520Niculae%2520and%2520Daniel%2520McNamee%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Associative%2520memory%2520models%252C%2520such%2520as%2520Hopfield%2520networks%2520and%2520their%2520modern%250Avariants%252C%2520have%2520garnered%2520renewed%2520interest%2520due%2520to%2520advancements%2520in%2520memory%2520capacity%250Aand%2520connections%2520with%2520self-attention%2520in%2520transformers.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aa%2520unified%2520framework-Hopfield-Fenchel-Young%2520networks-which%2520generalizes%2520these%250Amodels%2520to%2520a%2520broader%2520family%2520of%2520energy%2520functions.%2520Our%2520energies%2520are%2520formulated%2520as%250Athe%2520difference%2520between%2520two%2520Fenchel-Young%2520losses%253A%2520one%252C%2520parameterized%2520by%2520a%250Ageneralized%2520entropy%252C%2520defines%2520the%2520Hopfield%2520scoring%2520mechanism%252C%2520while%2520the%2520other%250Aapplies%2520a%2520post-transformation%2520to%2520the%2520Hopfield%2520output.%2520By%2520utilizing%2520Tsallis%2520and%250Anorm%2520entropies%252C%2520we%2520derive%2520end-to-end%2520differentiable%2520update%2520rules%2520that%2520enable%250Asparse%2520transformations%252C%2520uncovering%2520new%2520connections%2520between%2520loss%2520margins%252C%250Asparsity%252C%2520and%2520exact%2520retrieval%2520of%2520single%2520memory%2520patterns.%2520We%2520further%2520extend%2520this%250Aframework%2520to%2520structured%2520Hopfield%2520networks%2520using%2520the%2520SparseMAP%2520transformation%252C%250Aallowing%2520the%2520retrieval%2520of%2520pattern%2520associations%2520rather%2520than%2520a%2520single%2520pattern.%250AOur%2520framework%2520unifies%2520and%2520extends%2520traditional%2520and%2520modern%2520Hopfield%2520networks%2520and%250Aprovides%2520an%2520energy%2520minimization%2520perspective%2520for%2520widely%2520used%250Apost-transformations%2520like%2520%2524%255Cell_2%2524-normalization%2520and%2520layer%2520normalization-all%250Athrough%2520suitable%2520choices%2520of%2520Fenchel-Young%2520losses%2520and%2520by%2520using%2520convex%2520analysis%250Aas%2520a%2520building%2520block.%2520Finally%252C%2520we%2520validate%2520our%2520Hopfield-Fenchel-Young%2520networks%250Aon%2520diverse%2520memory%2520recall%2520tasks%252C%2520including%2520free%2520and%2520sequential%2520recall.%250AExperiments%2520on%2520simulated%2520data%252C%2520image%2520retrieval%252C%2520multiple%2520instance%2520learning%252C%2520and%250Atext%2520rationalization%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08590v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hopfield-Fenchel-Young%20Networks%3A%20A%20Unified%20Framework%20for%20Associative%0A%20%20Memory%20Retrieval&entry.906535625=Saul%20Santos%20and%20Vlad%20Niculae%20and%20Daniel%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Associative%20memory%20models%2C%20such%20as%20Hopfield%20networks%20and%20their%20modern%0Avariants%2C%20have%20garnered%20renewed%20interest%20due%20to%20advancements%20in%20memory%20capacity%0Aand%20connections%20with%20self-attention%20in%20transformers.%20In%20this%20work%2C%20we%20introduce%0Aa%20unified%20framework-Hopfield-Fenchel-Young%20networks-which%20generalizes%20these%0Amodels%20to%20a%20broader%20family%20of%20energy%20functions.%20Our%20energies%20are%20formulated%20as%0Athe%20difference%20between%20two%20Fenchel-Young%20losses%3A%20one%2C%20parameterized%20by%20a%0Ageneralized%20entropy%2C%20defines%20the%20Hopfield%20scoring%20mechanism%2C%20while%20the%20other%0Aapplies%20a%20post-transformation%20to%20the%20Hopfield%20output.%20By%20utilizing%20Tsallis%20and%0Anorm%20entropies%2C%20we%20derive%20end-to-end%20differentiable%20update%20rules%20that%20enable%0Asparse%20transformations%2C%20uncovering%20new%20connections%20between%20loss%20margins%2C%0Asparsity%2C%20and%20exact%20retrieval%20of%20single%20memory%20patterns.%20We%20further%20extend%20this%0Aframework%20to%20structured%20Hopfield%20networks%20using%20the%20SparseMAP%20transformation%2C%0Aallowing%20the%20retrieval%20of%20pattern%20associations%20rather%20than%20a%20single%20pattern.%0AOur%20framework%20unifies%20and%20extends%20traditional%20and%20modern%20Hopfield%20networks%20and%0Aprovides%20an%20energy%20minimization%20perspective%20for%20widely%20used%0Apost-transformations%20like%20%24%5Cell_2%24-normalization%20and%20layer%20normalization-all%0Athrough%20suitable%20choices%20of%20Fenchel-Young%20losses%20and%20by%20using%20convex%20analysis%0Aas%20a%20building%20block.%20Finally%2C%20we%20validate%20our%20Hopfield-Fenchel-Young%20networks%0Aon%20diverse%20memory%20recall%20tasks%2C%20including%20free%20and%20sequential%20recall.%0AExperiments%20on%20simulated%20data%2C%20image%20retrieval%2C%20multiple%20instance%20learning%2C%20and%0Atext%20rationalization%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08590v3&entry.124074799=Read"},
{"title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking", "author": "Ziqi Dai and Xin Zhang and Mingxin Li and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang and Wenjie Li and Min Zhang", "abstract": "  In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.\n", "link": "http://arxiv.org/abs/2510.14824v1", "date": "2025-10-16", "relevancy": 2.546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Fine-Tuning%20or%20Contrastive%20Learning%3F%20Towards%20Better%0A%20%20Multimodal%20LLM%20Reranking&body=Title%3A%20Supervised%20Fine-Tuning%20or%20Contrastive%20Learning%3F%20Towards%20Better%0A%20%20Multimodal%20LLM%20Reranking%0AAuthor%3A%20Ziqi%20Dai%20and%20Xin%20Zhang%20and%20Mingxin%20Li%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Meishan%20Zhang%20and%20Wenjie%20Li%20and%20Min%20Zhang%0AAbstract%3A%20%20%20In%20information%20retrieval%2C%20training%20reranking%20models%20mainly%20focuses%20on%20two%0Atypes%20of%20objectives%3A%20metric%20learning%20%28e.g.%20contrastive%20loss%20to%20increase%20the%0Apredicted%20scores%20on%20relevant%20query-document%20pairs%29%20and%20classification%20%28binary%0Alabel%20prediction%20of%20relevance%20vs.%20irrelevance%29.%20For%20BERT-style%20encoders%2C%0Avarious%20studies%20have%20shown%20that%20contrastive%20learning%20%28CL%29%20can%20be%20more%20effective%0Athan%20discriminative%20%28classification%29%20learning.%20However%2C%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20classification%20via%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20predicts%0A%27%27yes%27%27%20%28resp.%20%27%27no%27%27%29%20token%20for%20relevant%20%28resp.%20irrelevant%29%20pairs%2C%20appears%0Amore%20promising%20as%20it%20aligns%20well%20with%20the%20generative%20nature%20of%20LLMs.%20This%0Adivergence%20raises%20a%20central%20question%3A%20which%20objective%20is%20intrinsically%20better%0Asuited%20to%20LLM-based%20reranking%2C%20and%20what%20mechanism%20underlies%20the%20difference%3F%20In%0Athis%20work%2C%20we%20conduct%20a%20comprehensive%20comparison%20and%20analysis%20between%20CL%20and%0ASFT%20for%20reranking%2C%20taking%20the%20universal%20multimodal%20retrieval%20%28UMR%29%20as%20the%0Aexperimental%20playground.%20We%20first%20decompose%20the%20objectives%20into%20two%20components%3A%0Aweight%2C%20which%20controls%20the%20magnitude%20of%20those%20updates%2C%20and%20direction%2C%20which%0Aguides%20the%20model%20updates%2C%20then%20present%20a%20unified%20framework%20for%20understanding%0Atheir%20interactions.%20Through%20probing%20experiments%2C%20we%20find%20that%20SFT%20provides%20a%0Asubstantially%20stronger%20weighting%20scheme%20than%20CL%2C%20whereas%20the%20preferred%20scoring%0Adirection%20shows%20no%20clear%20winner.%20Taken%20together%2C%20these%20results%20point%20to%20a%0Aconsistent%20advantage%20of%20SFT%20over%20CL%20for%20LLM%20reranking.%20To%20further%20validate%20our%0Afindings%2C%20we%20conduct%20large-scale%20training%20with%20SFT%20and%20present%20new%0Astate-of-the-art%20rerankers%20on%20the%20MRB%20benchmark.%20We%20also%20provide%20ablations%20on%0ASFT%20settings%20and%20expect%20our%20findings%20to%20benefit%20future%20research%20and%0Aapplications%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Fine-Tuning%2520or%2520Contrastive%2520Learning%253F%2520Towards%2520Better%250A%2520%2520Multimodal%2520LLM%2520Reranking%26entry.906535625%3DZiqi%2520Dai%2520and%2520Xin%2520Zhang%2520and%2520Mingxin%2520Li%2520and%2520Yanzhao%2520Zhang%2520and%2520Dingkun%2520Long%2520and%2520Pengjun%2520Xie%2520and%2520Meishan%2520Zhang%2520and%2520Wenjie%2520Li%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520In%2520information%2520retrieval%252C%2520training%2520reranking%2520models%2520mainly%2520focuses%2520on%2520two%250Atypes%2520of%2520objectives%253A%2520metric%2520learning%2520%2528e.g.%2520contrastive%2520loss%2520to%2520increase%2520the%250Apredicted%2520scores%2520on%2520relevant%2520query-document%2520pairs%2529%2520and%2520classification%2520%2528binary%250Alabel%2520prediction%2520of%2520relevance%2520vs.%2520irrelevance%2529.%2520For%2520BERT-style%2520encoders%252C%250Avarious%2520studies%2520have%2520shown%2520that%2520contrastive%2520learning%2520%2528CL%2529%2520can%2520be%2520more%2520effective%250Athan%2520discriminative%2520%2528classification%2529%2520learning.%2520However%252C%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520classification%2520via%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520which%2520predicts%250A%2527%2527yes%2527%2527%2520%2528resp.%2520%2527%2527no%2527%2527%2529%2520token%2520for%2520relevant%2520%2528resp.%2520irrelevant%2529%2520pairs%252C%2520appears%250Amore%2520promising%2520as%2520it%2520aligns%2520well%2520with%2520the%2520generative%2520nature%2520of%2520LLMs.%2520This%250Adivergence%2520raises%2520a%2520central%2520question%253A%2520which%2520objective%2520is%2520intrinsically%2520better%250Asuited%2520to%2520LLM-based%2520reranking%252C%2520and%2520what%2520mechanism%2520underlies%2520the%2520difference%253F%2520In%250Athis%2520work%252C%2520we%2520conduct%2520a%2520comprehensive%2520comparison%2520and%2520analysis%2520between%2520CL%2520and%250ASFT%2520for%2520reranking%252C%2520taking%2520the%2520universal%2520multimodal%2520retrieval%2520%2528UMR%2529%2520as%2520the%250Aexperimental%2520playground.%2520We%2520first%2520decompose%2520the%2520objectives%2520into%2520two%2520components%253A%250Aweight%252C%2520which%2520controls%2520the%2520magnitude%2520of%2520those%2520updates%252C%2520and%2520direction%252C%2520which%250Aguides%2520the%2520model%2520updates%252C%2520then%2520present%2520a%2520unified%2520framework%2520for%2520understanding%250Atheir%2520interactions.%2520Through%2520probing%2520experiments%252C%2520we%2520find%2520that%2520SFT%2520provides%2520a%250Asubstantially%2520stronger%2520weighting%2520scheme%2520than%2520CL%252C%2520whereas%2520the%2520preferred%2520scoring%250Adirection%2520shows%2520no%2520clear%2520winner.%2520Taken%2520together%252C%2520these%2520results%2520point%2520to%2520a%250Aconsistent%2520advantage%2520of%2520SFT%2520over%2520CL%2520for%2520LLM%2520reranking.%2520To%2520further%2520validate%2520our%250Afindings%252C%2520we%2520conduct%2520large-scale%2520training%2520with%2520SFT%2520and%2520present%2520new%250Astate-of-the-art%2520rerankers%2520on%2520the%2520MRB%2520benchmark.%2520We%2520also%2520provide%2520ablations%2520on%250ASFT%2520settings%2520and%2520expect%2520our%2520findings%2520to%2520benefit%2520future%2520research%2520and%250Aapplications%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Fine-Tuning%20or%20Contrastive%20Learning%3F%20Towards%20Better%0A%20%20Multimodal%20LLM%20Reranking&entry.906535625=Ziqi%20Dai%20and%20Xin%20Zhang%20and%20Mingxin%20Li%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Meishan%20Zhang%20and%20Wenjie%20Li%20and%20Min%20Zhang&entry.1292438233=%20%20In%20information%20retrieval%2C%20training%20reranking%20models%20mainly%20focuses%20on%20two%0Atypes%20of%20objectives%3A%20metric%20learning%20%28e.g.%20contrastive%20loss%20to%20increase%20the%0Apredicted%20scores%20on%20relevant%20query-document%20pairs%29%20and%20classification%20%28binary%0Alabel%20prediction%20of%20relevance%20vs.%20irrelevance%29.%20For%20BERT-style%20encoders%2C%0Avarious%20studies%20have%20shown%20that%20contrastive%20learning%20%28CL%29%20can%20be%20more%20effective%0Athan%20discriminative%20%28classification%29%20learning.%20However%2C%20for%20large%20language%0Amodels%20%28LLMs%29%2C%20classification%20via%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20predicts%0A%27%27yes%27%27%20%28resp.%20%27%27no%27%27%29%20token%20for%20relevant%20%28resp.%20irrelevant%29%20pairs%2C%20appears%0Amore%20promising%20as%20it%20aligns%20well%20with%20the%20generative%20nature%20of%20LLMs.%20This%0Adivergence%20raises%20a%20central%20question%3A%20which%20objective%20is%20intrinsically%20better%0Asuited%20to%20LLM-based%20reranking%2C%20and%20what%20mechanism%20underlies%20the%20difference%3F%20In%0Athis%20work%2C%20we%20conduct%20a%20comprehensive%20comparison%20and%20analysis%20between%20CL%20and%0ASFT%20for%20reranking%2C%20taking%20the%20universal%20multimodal%20retrieval%20%28UMR%29%20as%20the%0Aexperimental%20playground.%20We%20first%20decompose%20the%20objectives%20into%20two%20components%3A%0Aweight%2C%20which%20controls%20the%20magnitude%20of%20those%20updates%2C%20and%20direction%2C%20which%0Aguides%20the%20model%20updates%2C%20then%20present%20a%20unified%20framework%20for%20understanding%0Atheir%20interactions.%20Through%20probing%20experiments%2C%20we%20find%20that%20SFT%20provides%20a%0Asubstantially%20stronger%20weighting%20scheme%20than%20CL%2C%20whereas%20the%20preferred%20scoring%0Adirection%20shows%20no%20clear%20winner.%20Taken%20together%2C%20these%20results%20point%20to%20a%0Aconsistent%20advantage%20of%20SFT%20over%20CL%20for%20LLM%20reranking.%20To%20further%20validate%20our%0Afindings%2C%20we%20conduct%20large-scale%20training%20with%20SFT%20and%20present%20new%0Astate-of-the-art%20rerankers%20on%20the%20MRB%20benchmark.%20We%20also%20provide%20ablations%20on%0ASFT%20settings%20and%20expect%20our%20findings%20to%20benefit%20future%20research%20and%0Aapplications%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14824v1&entry.124074799=Read"},
{"title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored\n  Encoding", "author": "Zhifei Chen and Tianshuo Xu and Leyi Wu and Luozhou Wang and Dongyu Yan and Zihan You and Wenting Luo and Guo Zhang and Yingcong Chen", "abstract": "  Video generation has recently made striking visual progress, but maintaining\ncoherent object motion and interactions remains difficult. We trace two\npractical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)\noften collapse to too few effective tokens after encoding, weakening guidance;\nand (ii) optimizing for appearance and motion in a single head can favor\ntexture over temporal consistency. We present STANCE, an image-to-video\nframework that addresses both issues with two simple components. First, we\nintroduce Instance Cues -- a pixel-aligned control signal that turns sparse,\nuser-editable hints into a dense 2.5D (camera-relative) motion field by\naveraging per-instance flow and augmenting with monocular depth over the\ninstance mask. This reduces depth ambiguity compared to 2D arrow inputs while\nremaining easy to use. Second, we preserve the salience of these cues in token\nspace with Dense RoPE, which tags a small set of motion tokens (anchored on the\nfirst frame) with spatial-addressable rotary embeddings. Paired with joint RGB\n\\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors\nstructure while RGB handles appearance, stabilizing optimization and improving\ntemporal coherence without requiring per-frame trajectory scripts.\n", "link": "http://arxiv.org/abs/2510.14588v1", "date": "2025-10-16", "relevancy": 2.5358, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6438}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6301}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STANCE%3A%20Motion%20Coherent%20Video%20Generation%20Via%20Sparse-to-Dense%20Anchored%0A%20%20Encoding&body=Title%3A%20STANCE%3A%20Motion%20Coherent%20Video%20Generation%20Via%20Sparse-to-Dense%20Anchored%0A%20%20Encoding%0AAuthor%3A%20Zhifei%20Chen%20and%20Tianshuo%20Xu%20and%20Leyi%20Wu%20and%20Luozhou%20Wang%20and%20Dongyu%20Yan%20and%20Zihan%20You%20and%20Wenting%20Luo%20and%20Guo%20Zhang%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20Video%20generation%20has%20recently%20made%20striking%20visual%20progress%2C%20but%20maintaining%0Acoherent%20object%20motion%20and%20interactions%20remains%20difficult.%20We%20trace%20two%0Apractical%20bottlenecks%3A%20%28i%29%20human-provided%20motion%20hints%20%28e.g.%2C%20small%202D%20maps%29%0Aoften%20collapse%20to%20too%20few%20effective%20tokens%20after%20encoding%2C%20weakening%20guidance%3B%0Aand%20%28ii%29%20optimizing%20for%20appearance%20and%20motion%20in%20a%20single%20head%20can%20favor%0Atexture%20over%20temporal%20consistency.%20We%20present%20STANCE%2C%20an%20image-to-video%0Aframework%20that%20addresses%20both%20issues%20with%20two%20simple%20components.%20First%2C%20we%0Aintroduce%20Instance%20Cues%20--%20a%20pixel-aligned%20control%20signal%20that%20turns%20sparse%2C%0Auser-editable%20hints%20into%20a%20dense%202.5D%20%28camera-relative%29%20motion%20field%20by%0Aaveraging%20per-instance%20flow%20and%20augmenting%20with%20monocular%20depth%20over%20the%0Ainstance%20mask.%20This%20reduces%20depth%20ambiguity%20compared%20to%202D%20arrow%20inputs%20while%0Aremaining%20easy%20to%20use.%20Second%2C%20we%20preserve%20the%20salience%20of%20these%20cues%20in%20token%0Aspace%20with%20Dense%20RoPE%2C%20which%20tags%20a%20small%20set%20of%20motion%20tokens%20%28anchored%20on%20the%0Afirst%20frame%29%20with%20spatial-addressable%20rotary%20embeddings.%20Paired%20with%20joint%20RGB%0A%5C%28%2B%5C%29%20auxiliary-map%20prediction%20%28segmentation%20or%20depth%29%2C%20our%20model%20anchors%0Astructure%20while%20RGB%20handles%20appearance%2C%20stabilizing%20optimization%20and%20improving%0Atemporal%20coherence%20without%20requiring%20per-frame%20trajectory%20scripts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTANCE%253A%2520Motion%2520Coherent%2520Video%2520Generation%2520Via%2520Sparse-to-Dense%2520Anchored%250A%2520%2520Encoding%26entry.906535625%3DZhifei%2520Chen%2520and%2520Tianshuo%2520Xu%2520and%2520Leyi%2520Wu%2520and%2520Luozhou%2520Wang%2520and%2520Dongyu%2520Yan%2520and%2520Zihan%2520You%2520and%2520Wenting%2520Luo%2520and%2520Guo%2520Zhang%2520and%2520Yingcong%2520Chen%26entry.1292438233%3D%2520%2520Video%2520generation%2520has%2520recently%2520made%2520striking%2520visual%2520progress%252C%2520but%2520maintaining%250Acoherent%2520object%2520motion%2520and%2520interactions%2520remains%2520difficult.%2520We%2520trace%2520two%250Apractical%2520bottlenecks%253A%2520%2528i%2529%2520human-provided%2520motion%2520hints%2520%2528e.g.%252C%2520small%25202D%2520maps%2529%250Aoften%2520collapse%2520to%2520too%2520few%2520effective%2520tokens%2520after%2520encoding%252C%2520weakening%2520guidance%253B%250Aand%2520%2528ii%2529%2520optimizing%2520for%2520appearance%2520and%2520motion%2520in%2520a%2520single%2520head%2520can%2520favor%250Atexture%2520over%2520temporal%2520consistency.%2520We%2520present%2520STANCE%252C%2520an%2520image-to-video%250Aframework%2520that%2520addresses%2520both%2520issues%2520with%2520two%2520simple%2520components.%2520First%252C%2520we%250Aintroduce%2520Instance%2520Cues%2520--%2520a%2520pixel-aligned%2520control%2520signal%2520that%2520turns%2520sparse%252C%250Auser-editable%2520hints%2520into%2520a%2520dense%25202.5D%2520%2528camera-relative%2529%2520motion%2520field%2520by%250Aaveraging%2520per-instance%2520flow%2520and%2520augmenting%2520with%2520monocular%2520depth%2520over%2520the%250Ainstance%2520mask.%2520This%2520reduces%2520depth%2520ambiguity%2520compared%2520to%25202D%2520arrow%2520inputs%2520while%250Aremaining%2520easy%2520to%2520use.%2520Second%252C%2520we%2520preserve%2520the%2520salience%2520of%2520these%2520cues%2520in%2520token%250Aspace%2520with%2520Dense%2520RoPE%252C%2520which%2520tags%2520a%2520small%2520set%2520of%2520motion%2520tokens%2520%2528anchored%2520on%2520the%250Afirst%2520frame%2529%2520with%2520spatial-addressable%2520rotary%2520embeddings.%2520Paired%2520with%2520joint%2520RGB%250A%255C%2528%252B%255C%2529%2520auxiliary-map%2520prediction%2520%2528segmentation%2520or%2520depth%2529%252C%2520our%2520model%2520anchors%250Astructure%2520while%2520RGB%2520handles%2520appearance%252C%2520stabilizing%2520optimization%2520and%2520improving%250Atemporal%2520coherence%2520without%2520requiring%2520per-frame%2520trajectory%2520scripts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STANCE%3A%20Motion%20Coherent%20Video%20Generation%20Via%20Sparse-to-Dense%20Anchored%0A%20%20Encoding&entry.906535625=Zhifei%20Chen%20and%20Tianshuo%20Xu%20and%20Leyi%20Wu%20and%20Luozhou%20Wang%20and%20Dongyu%20Yan%20and%20Zihan%20You%20and%20Wenting%20Luo%20and%20Guo%20Zhang%20and%20Yingcong%20Chen&entry.1292438233=%20%20Video%20generation%20has%20recently%20made%20striking%20visual%20progress%2C%20but%20maintaining%0Acoherent%20object%20motion%20and%20interactions%20remains%20difficult.%20We%20trace%20two%0Apractical%20bottlenecks%3A%20%28i%29%20human-provided%20motion%20hints%20%28e.g.%2C%20small%202D%20maps%29%0Aoften%20collapse%20to%20too%20few%20effective%20tokens%20after%20encoding%2C%20weakening%20guidance%3B%0Aand%20%28ii%29%20optimizing%20for%20appearance%20and%20motion%20in%20a%20single%20head%20can%20favor%0Atexture%20over%20temporal%20consistency.%20We%20present%20STANCE%2C%20an%20image-to-video%0Aframework%20that%20addresses%20both%20issues%20with%20two%20simple%20components.%20First%2C%20we%0Aintroduce%20Instance%20Cues%20--%20a%20pixel-aligned%20control%20signal%20that%20turns%20sparse%2C%0Auser-editable%20hints%20into%20a%20dense%202.5D%20%28camera-relative%29%20motion%20field%20by%0Aaveraging%20per-instance%20flow%20and%20augmenting%20with%20monocular%20depth%20over%20the%0Ainstance%20mask.%20This%20reduces%20depth%20ambiguity%20compared%20to%202D%20arrow%20inputs%20while%0Aremaining%20easy%20to%20use.%20Second%2C%20we%20preserve%20the%20salience%20of%20these%20cues%20in%20token%0Aspace%20with%20Dense%20RoPE%2C%20which%20tags%20a%20small%20set%20of%20motion%20tokens%20%28anchored%20on%20the%0Afirst%20frame%29%20with%20spatial-addressable%20rotary%20embeddings.%20Paired%20with%20joint%20RGB%0A%5C%28%2B%5C%29%20auxiliary-map%20prediction%20%28segmentation%20or%20depth%29%2C%20our%20model%20anchors%0Astructure%20while%20RGB%20handles%20appearance%2C%20stabilizing%20optimization%20and%20improving%0Atemporal%20coherence%20without%20requiring%20per-frame%20trajectory%20scripts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14588v1&entry.124074799=Read"},
{"title": "Subspace-Boosted Model Merging", "author": "Ronald Skorobogat and Karsten Roth and Mariana-Iuliana Georgescu", "abstract": "  Model merging enables the combination of multiple specialized expert models\ninto a single model capable of performing multiple tasks. However, the benefits\nof merging an increasing amount of specialized experts generally lead to\ndiminishing returns and reduced overall performance gains. In this work, we\noffer an explanation and analysis from a task arithmetic perspective; revealing\nthat as the merging process (across numerous existing merging methods)\ncontinues for more and more experts, the associated task vector space\nexperiences rank collapse. To mitigate this issue, we introduce Subspace\nBoosting, which operates on the singular value decomposed task vector space and\nmaintains task vector ranks. Subspace Boosting raises merging efficacy for up\nto 20 expert models by large margins of more than 10% when evaluated on both\nvision and language benchmarks. Moreover, we propose employing Higher-Order\nGeneralized Singular Value Decomposition to quantify task similarity, offering\na new interpretable perspective on model merging.\n", "link": "http://arxiv.org/abs/2506.16506v2", "date": "2025-10-16", "relevancy": 2.5333, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subspace-Boosted%20Model%20Merging&body=Title%3A%20Subspace-Boosted%20Model%20Merging%0AAuthor%3A%20Ronald%20Skorobogat%20and%20Karsten%20Roth%20and%20Mariana-Iuliana%20Georgescu%0AAbstract%3A%20%20%20Model%20merging%20enables%20the%20combination%20of%20multiple%20specialized%20expert%20models%0Ainto%20a%20single%20model%20capable%20of%20performing%20multiple%20tasks.%20However%2C%20the%20benefits%0Aof%20merging%20an%20increasing%20amount%20of%20specialized%20experts%20generally%20lead%20to%0Adiminishing%20returns%20and%20reduced%20overall%20performance%20gains.%20In%20this%20work%2C%20we%0Aoffer%20an%20explanation%20and%20analysis%20from%20a%20task%20arithmetic%20perspective%3B%20revealing%0Athat%20as%20the%20merging%20process%20%28across%20numerous%20existing%20merging%20methods%29%0Acontinues%20for%20more%20and%20more%20experts%2C%20the%20associated%20task%20vector%20space%0Aexperiences%20rank%20collapse.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20Subspace%0ABoosting%2C%20which%20operates%20on%20the%20singular%20value%20decomposed%20task%20vector%20space%20and%0Amaintains%20task%20vector%20ranks.%20Subspace%20Boosting%20raises%20merging%20efficacy%20for%20up%0Ato%2020%20expert%20models%20by%20large%20margins%20of%20more%20than%2010%25%20when%20evaluated%20on%20both%0Avision%20and%20language%20benchmarks.%20Moreover%2C%20we%20propose%20employing%20Higher-Order%0AGeneralized%20Singular%20Value%20Decomposition%20to%20quantify%20task%20similarity%2C%20offering%0Aa%20new%20interpretable%20perspective%20on%20model%20merging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubspace-Boosted%2520Model%2520Merging%26entry.906535625%3DRonald%2520Skorobogat%2520and%2520Karsten%2520Roth%2520and%2520Mariana-Iuliana%2520Georgescu%26entry.1292438233%3D%2520%2520Model%2520merging%2520enables%2520the%2520combination%2520of%2520multiple%2520specialized%2520expert%2520models%250Ainto%2520a%2520single%2520model%2520capable%2520of%2520performing%2520multiple%2520tasks.%2520However%252C%2520the%2520benefits%250Aof%2520merging%2520an%2520increasing%2520amount%2520of%2520specialized%2520experts%2520generally%2520lead%2520to%250Adiminishing%2520returns%2520and%2520reduced%2520overall%2520performance%2520gains.%2520In%2520this%2520work%252C%2520we%250Aoffer%2520an%2520explanation%2520and%2520analysis%2520from%2520a%2520task%2520arithmetic%2520perspective%253B%2520revealing%250Athat%2520as%2520the%2520merging%2520process%2520%2528across%2520numerous%2520existing%2520merging%2520methods%2529%250Acontinues%2520for%2520more%2520and%2520more%2520experts%252C%2520the%2520associated%2520task%2520vector%2520space%250Aexperiences%2520rank%2520collapse.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520introduce%2520Subspace%250ABoosting%252C%2520which%2520operates%2520on%2520the%2520singular%2520value%2520decomposed%2520task%2520vector%2520space%2520and%250Amaintains%2520task%2520vector%2520ranks.%2520Subspace%2520Boosting%2520raises%2520merging%2520efficacy%2520for%2520up%250Ato%252020%2520expert%2520models%2520by%2520large%2520margins%2520of%2520more%2520than%252010%2525%2520when%2520evaluated%2520on%2520both%250Avision%2520and%2520language%2520benchmarks.%2520Moreover%252C%2520we%2520propose%2520employing%2520Higher-Order%250AGeneralized%2520Singular%2520Value%2520Decomposition%2520to%2520quantify%2520task%2520similarity%252C%2520offering%250Aa%2520new%2520interpretable%2520perspective%2520on%2520model%2520merging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subspace-Boosted%20Model%20Merging&entry.906535625=Ronald%20Skorobogat%20and%20Karsten%20Roth%20and%20Mariana-Iuliana%20Georgescu&entry.1292438233=%20%20Model%20merging%20enables%20the%20combination%20of%20multiple%20specialized%20expert%20models%0Ainto%20a%20single%20model%20capable%20of%20performing%20multiple%20tasks.%20However%2C%20the%20benefits%0Aof%20merging%20an%20increasing%20amount%20of%20specialized%20experts%20generally%20lead%20to%0Adiminishing%20returns%20and%20reduced%20overall%20performance%20gains.%20In%20this%20work%2C%20we%0Aoffer%20an%20explanation%20and%20analysis%20from%20a%20task%20arithmetic%20perspective%3B%20revealing%0Athat%20as%20the%20merging%20process%20%28across%20numerous%20existing%20merging%20methods%29%0Acontinues%20for%20more%20and%20more%20experts%2C%20the%20associated%20task%20vector%20space%0Aexperiences%20rank%20collapse.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20Subspace%0ABoosting%2C%20which%20operates%20on%20the%20singular%20value%20decomposed%20task%20vector%20space%20and%0Amaintains%20task%20vector%20ranks.%20Subspace%20Boosting%20raises%20merging%20efficacy%20for%20up%0Ato%2020%20expert%20models%20by%20large%20margins%20of%20more%20than%2010%25%20when%20evaluated%20on%20both%0Avision%20and%20language%20benchmarks.%20Moreover%2C%20we%20propose%20employing%20Higher-Order%0AGeneralized%20Singular%20Value%20Decomposition%20to%20quantify%20task%20similarity%2C%20offering%0Aa%20new%20interpretable%20perspective%20on%20model%20merging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16506v2&entry.124074799=Read"},
{"title": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in\n  LLMs Continual Learning", "author": "Zhiyi Wan and Wanrou Du and Liang Li and Miao Pan and Xiaoqi Qin", "abstract": "  Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in an end-to-end\ntraining stage. Specifically, OA-Adapter introduces a dynamic bottleneck\ndimension adaptation mechanism that simultaneously allocates an efficient\nparameter budget and optimizes task objectives without misalignment.To\neffectively preserve previously acquired knowledge while coordinating with the\ndynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter\nachieves higher average accuracy while using 58.5% fewer parameters on the\nstandard CL benchmark, and maintains its advantages on two larger benchmarks\ncomprising 15 tasks.\n", "link": "http://arxiv.org/abs/2505.22358v2", "date": "2025-10-16", "relevancy": 2.5229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5338}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4926}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Budget%20Allocation%20for%20Orthogonal-Subspace%20Adapter%20Tuning%20in%0A%20%20LLMs%20Continual%20Learning&body=Title%3A%20Adaptive%20Budget%20Allocation%20for%20Orthogonal-Subspace%20Adapter%20Tuning%20in%0A%20%20LLMs%20Continual%20Learning%0AAuthor%3A%20Zhiyi%20Wan%20and%20Wanrou%20Du%20and%20Liang%20Li%20and%20Miao%20Pan%20and%20Xiaoqi%20Qin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20suffer%20from%20catastrophic%20forgetting%20in%0Acontinual%20learning%20%28CL%29%20scenarios%2C%20where%20performance%20on%20previously%20learned%0Atasks%20degrades%20severely%20while%20training%20on%20sequentially%20arriving%20tasks.%20Although%0Apioneering%20CL%20approaches%20using%20orthogonal%20subspaces%20can%20mitigate%20task%0Ainterference%2C%20they%20typically%20employ%20fixed%20budget%20allocation%2C%20neglecting%20the%0Avarying%20complexity%20across%20tasks%20and%20layers.%20Besides%2C%20recent%20budget-adaptive%0Atuning%20methods%20for%20LLMs%20often%20adopt%20multi-stage%20paradigms%20that%20decouple%0Aoptimization%20and%20budget%20allocation.%20Such%20decoupling%20results%20in%20potential%0Amisalignment%2C%20which%20hinders%20those%20approaches%27%20practical%20application%20in%20CL%0Ascenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20OA-Adapter%2C%20a%20novel%0Aparameter-efficient%20approach%20for%20continual%20learning%20in%20LLMs%20that%20unifies%0Adynamic%20budget%20adaptation%20with%20orthogonal%20subspace%20learning%20in%20an%20end-to-end%0Atraining%20stage.%20Specifically%2C%20OA-Adapter%20introduces%20a%20dynamic%20bottleneck%0Adimension%20adaptation%20mechanism%20that%20simultaneously%20allocates%20an%20efficient%0Aparameter%20budget%20and%20optimizes%20task%20objectives%20without%20misalignment.To%0Aeffectively%20preserve%20previously%20acquired%20knowledge%20while%20coordinating%20with%20the%0Adynamic%20budget%20allocation%2C%20orthogonal%20constraints%20are%20applied%20specifically%0Abetween%20the%20parameter%20subspace%20of%20the%20current%20task%20and%20the%20dynamically%0Aallocated%20parameter%20subspaces%20of%20historical%20tasks.%20Experimental%20results%20on%0Acontinual%20learning%20benchmarks%20demonstrate%20that%20OA-Adapter%20outperforms%0Astate-of-the-art%20methods%20in%20both%20accuracy%20and%20parameter%20efficiency.%20OA-Adapter%0Aachieves%20higher%20average%20accuracy%20while%20using%2058.5%25%20fewer%20parameters%20on%20the%0Astandard%20CL%20benchmark%2C%20and%20maintains%20its%20advantages%20on%20two%20larger%20benchmarks%0Acomprising%2015%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Budget%2520Allocation%2520for%2520Orthogonal-Subspace%2520Adapter%2520Tuning%2520in%250A%2520%2520LLMs%2520Continual%2520Learning%26entry.906535625%3DZhiyi%2520Wan%2520and%2520Wanrou%2520Du%2520and%2520Liang%2520Li%2520and%2520Miao%2520Pan%2520and%2520Xiaoqi%2520Qin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520suffer%2520from%2520catastrophic%2520forgetting%2520in%250Acontinual%2520learning%2520%2528CL%2529%2520scenarios%252C%2520where%2520performance%2520on%2520previously%2520learned%250Atasks%2520degrades%2520severely%2520while%2520training%2520on%2520sequentially%2520arriving%2520tasks.%2520Although%250Apioneering%2520CL%2520approaches%2520using%2520orthogonal%2520subspaces%2520can%2520mitigate%2520task%250Ainterference%252C%2520they%2520typically%2520employ%2520fixed%2520budget%2520allocation%252C%2520neglecting%2520the%250Avarying%2520complexity%2520across%2520tasks%2520and%2520layers.%2520Besides%252C%2520recent%2520budget-adaptive%250Atuning%2520methods%2520for%2520LLMs%2520often%2520adopt%2520multi-stage%2520paradigms%2520that%2520decouple%250Aoptimization%2520and%2520budget%2520allocation.%2520Such%2520decoupling%2520results%2520in%2520potential%250Amisalignment%252C%2520which%2520hinders%2520those%2520approaches%2527%2520practical%2520application%2520in%2520CL%250Ascenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520OA-Adapter%252C%2520a%2520novel%250Aparameter-efficient%2520approach%2520for%2520continual%2520learning%2520in%2520LLMs%2520that%2520unifies%250Adynamic%2520budget%2520adaptation%2520with%2520orthogonal%2520subspace%2520learning%2520in%2520an%2520end-to-end%250Atraining%2520stage.%2520Specifically%252C%2520OA-Adapter%2520introduces%2520a%2520dynamic%2520bottleneck%250Adimension%2520adaptation%2520mechanism%2520that%2520simultaneously%2520allocates%2520an%2520efficient%250Aparameter%2520budget%2520and%2520optimizes%2520task%2520objectives%2520without%2520misalignment.To%250Aeffectively%2520preserve%2520previously%2520acquired%2520knowledge%2520while%2520coordinating%2520with%2520the%250Adynamic%2520budget%2520allocation%252C%2520orthogonal%2520constraints%2520are%2520applied%2520specifically%250Abetween%2520the%2520parameter%2520subspace%2520of%2520the%2520current%2520task%2520and%2520the%2520dynamically%250Aallocated%2520parameter%2520subspaces%2520of%2520historical%2520tasks.%2520Experimental%2520results%2520on%250Acontinual%2520learning%2520benchmarks%2520demonstrate%2520that%2520OA-Adapter%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520both%2520accuracy%2520and%2520parameter%2520efficiency.%2520OA-Adapter%250Aachieves%2520higher%2520average%2520accuracy%2520while%2520using%252058.5%2525%2520fewer%2520parameters%2520on%2520the%250Astandard%2520CL%2520benchmark%252C%2520and%2520maintains%2520its%2520advantages%2520on%2520two%2520larger%2520benchmarks%250Acomprising%252015%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Budget%20Allocation%20for%20Orthogonal-Subspace%20Adapter%20Tuning%20in%0A%20%20LLMs%20Continual%20Learning&entry.906535625=Zhiyi%20Wan%20and%20Wanrou%20Du%20and%20Liang%20Li%20and%20Miao%20Pan%20and%20Xiaoqi%20Qin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20suffer%20from%20catastrophic%20forgetting%20in%0Acontinual%20learning%20%28CL%29%20scenarios%2C%20where%20performance%20on%20previously%20learned%0Atasks%20degrades%20severely%20while%20training%20on%20sequentially%20arriving%20tasks.%20Although%0Apioneering%20CL%20approaches%20using%20orthogonal%20subspaces%20can%20mitigate%20task%0Ainterference%2C%20they%20typically%20employ%20fixed%20budget%20allocation%2C%20neglecting%20the%0Avarying%20complexity%20across%20tasks%20and%20layers.%20Besides%2C%20recent%20budget-adaptive%0Atuning%20methods%20for%20LLMs%20often%20adopt%20multi-stage%20paradigms%20that%20decouple%0Aoptimization%20and%20budget%20allocation.%20Such%20decoupling%20results%20in%20potential%0Amisalignment%2C%20which%20hinders%20those%20approaches%27%20practical%20application%20in%20CL%0Ascenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20OA-Adapter%2C%20a%20novel%0Aparameter-efficient%20approach%20for%20continual%20learning%20in%20LLMs%20that%20unifies%0Adynamic%20budget%20adaptation%20with%20orthogonal%20subspace%20learning%20in%20an%20end-to-end%0Atraining%20stage.%20Specifically%2C%20OA-Adapter%20introduces%20a%20dynamic%20bottleneck%0Adimension%20adaptation%20mechanism%20that%20simultaneously%20allocates%20an%20efficient%0Aparameter%20budget%20and%20optimizes%20task%20objectives%20without%20misalignment.To%0Aeffectively%20preserve%20previously%20acquired%20knowledge%20while%20coordinating%20with%20the%0Adynamic%20budget%20allocation%2C%20orthogonal%20constraints%20are%20applied%20specifically%0Abetween%20the%20parameter%20subspace%20of%20the%20current%20task%20and%20the%20dynamically%0Aallocated%20parameter%20subspaces%20of%20historical%20tasks.%20Experimental%20results%20on%0Acontinual%20learning%20benchmarks%20demonstrate%20that%20OA-Adapter%20outperforms%0Astate-of-the-art%20methods%20in%20both%20accuracy%20and%20parameter%20efficiency.%20OA-Adapter%0Aachieves%20higher%20average%20accuracy%20while%20using%2058.5%25%20fewer%20parameters%20on%20the%0Astandard%20CL%20benchmark%2C%20and%20maintains%20its%20advantages%20on%20two%20larger%20benchmarks%0Acomprising%2015%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22358v2&entry.124074799=Read"},
{"title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion\n  Transformer", "author": "Mohsen Ghafoorian and Denis Korzhenkov and Amirhossein Habibian", "abstract": "  Transformer-based video diffusion models (VDMs) deliver state-of-the-art\nvideo generation quality but are constrained by the quadratic cost of\nself-attention, making long sequences and high resolutions computationally\nexpensive. While linear attention offers sub-quadratic complexity, prior\nattempts fail to match the expressiveness of softmax attention without costly\nretraining. We introduce Attention Surgery, an efficient framework for\nlinearizing or hybridizing attention in pretrained VDMs without training from\nscratch. Inspired by recent advances in language models, our method combines a\nnovel hybrid attention mechanism-mixing softmax and linear tokens-with a\nlightweight distillation and fine-tuning pipeline requiring only a few\nGPU-days. Additionally, we incorporate a cost-aware block-rate strategy to\nbalance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a\nstate-of-the-art DiT-based VDM, Attention Surgery achieves the first\ncompetitive sub-quadratic attention video diffusion models, reducing attention\ncost by up to 40\\% in terms of FLOPs, while maintaining generation quality as\nmeasured on the standard VBench and VBench-2.0 benchmarks. Project page is\navailable at: https://qualcomm-ai-research.github.io/attention-surgery.\n", "link": "http://arxiv.org/abs/2509.24899v2", "date": "2025-10-16", "relevancy": 2.5166, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6417}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6389}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Surgery%3A%20An%20Efficient%20Recipe%20to%20Linearize%20Your%20Video%20Diffusion%0A%20%20Transformer&body=Title%3A%20Attention%20Surgery%3A%20An%20Efficient%20Recipe%20to%20Linearize%20Your%20Video%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Mohsen%20Ghafoorian%20and%20Denis%20Korzhenkov%20and%20Amirhossein%20Habibian%0AAbstract%3A%20%20%20Transformer-based%20video%20diffusion%20models%20%28VDMs%29%20deliver%20state-of-the-art%0Avideo%20generation%20quality%20but%20are%20constrained%20by%20the%20quadratic%20cost%20of%0Aself-attention%2C%20making%20long%20sequences%20and%20high%20resolutions%20computationally%0Aexpensive.%20While%20linear%20attention%20offers%20sub-quadratic%20complexity%2C%20prior%0Aattempts%20fail%20to%20match%20the%20expressiveness%20of%20softmax%20attention%20without%20costly%0Aretraining.%20We%20introduce%20Attention%20Surgery%2C%20an%20efficient%20framework%20for%0Alinearizing%20or%20hybridizing%20attention%20in%20pretrained%20VDMs%20without%20training%20from%0Ascratch.%20Inspired%20by%20recent%20advances%20in%20language%20models%2C%20our%20method%20combines%20a%0Anovel%20hybrid%20attention%20mechanism-mixing%20softmax%20and%20linear%20tokens-with%20a%0Alightweight%20distillation%20and%20fine-tuning%20pipeline%20requiring%20only%20a%20few%0AGPU-days.%20Additionally%2C%20we%20incorporate%20a%20cost-aware%20block-rate%20strategy%20to%0Abalance%20expressiveness%20and%20efficiency%20across%20layers.%20Applied%20to%20Wan2.1%201.3B%2C%20a%0Astate-of-the-art%20DiT-based%20VDM%2C%20Attention%20Surgery%20achieves%20the%20first%0Acompetitive%20sub-quadratic%20attention%20video%20diffusion%20models%2C%20reducing%20attention%0Acost%20by%20up%20to%2040%5C%25%20in%20terms%20of%20FLOPs%2C%20while%20maintaining%20generation%20quality%20as%0Ameasured%20on%20the%20standard%20VBench%20and%20VBench-2.0%20benchmarks.%20Project%20page%20is%0Aavailable%20at%3A%20https%3A//qualcomm-ai-research.github.io/attention-surgery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Surgery%253A%2520An%2520Efficient%2520Recipe%2520to%2520Linearize%2520Your%2520Video%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DMohsen%2520Ghafoorian%2520and%2520Denis%2520Korzhenkov%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3D%2520%2520Transformer-based%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520deliver%2520state-of-the-art%250Avideo%2520generation%2520quality%2520but%2520are%2520constrained%2520by%2520the%2520quadratic%2520cost%2520of%250Aself-attention%252C%2520making%2520long%2520sequences%2520and%2520high%2520resolutions%2520computationally%250Aexpensive.%2520While%2520linear%2520attention%2520offers%2520sub-quadratic%2520complexity%252C%2520prior%250Aattempts%2520fail%2520to%2520match%2520the%2520expressiveness%2520of%2520softmax%2520attention%2520without%2520costly%250Aretraining.%2520We%2520introduce%2520Attention%2520Surgery%252C%2520an%2520efficient%2520framework%2520for%250Alinearizing%2520or%2520hybridizing%2520attention%2520in%2520pretrained%2520VDMs%2520without%2520training%2520from%250Ascratch.%2520Inspired%2520by%2520recent%2520advances%2520in%2520language%2520models%252C%2520our%2520method%2520combines%2520a%250Anovel%2520hybrid%2520attention%2520mechanism-mixing%2520softmax%2520and%2520linear%2520tokens-with%2520a%250Alightweight%2520distillation%2520and%2520fine-tuning%2520pipeline%2520requiring%2520only%2520a%2520few%250AGPU-days.%2520Additionally%252C%2520we%2520incorporate%2520a%2520cost-aware%2520block-rate%2520strategy%2520to%250Abalance%2520expressiveness%2520and%2520efficiency%2520across%2520layers.%2520Applied%2520to%2520Wan2.1%25201.3B%252C%2520a%250Astate-of-the-art%2520DiT-based%2520VDM%252C%2520Attention%2520Surgery%2520achieves%2520the%2520first%250Acompetitive%2520sub-quadratic%2520attention%2520video%2520diffusion%2520models%252C%2520reducing%2520attention%250Acost%2520by%2520up%2520to%252040%255C%2525%2520in%2520terms%2520of%2520FLOPs%252C%2520while%2520maintaining%2520generation%2520quality%2520as%250Ameasured%2520on%2520the%2520standard%2520VBench%2520and%2520VBench-2.0%2520benchmarks.%2520Project%2520page%2520is%250Aavailable%2520at%253A%2520https%253A//qualcomm-ai-research.github.io/attention-surgery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Surgery%3A%20An%20Efficient%20Recipe%20to%20Linearize%20Your%20Video%20Diffusion%0A%20%20Transformer&entry.906535625=Mohsen%20Ghafoorian%20and%20Denis%20Korzhenkov%20and%20Amirhossein%20Habibian&entry.1292438233=%20%20Transformer-based%20video%20diffusion%20models%20%28VDMs%29%20deliver%20state-of-the-art%0Avideo%20generation%20quality%20but%20are%20constrained%20by%20the%20quadratic%20cost%20of%0Aself-attention%2C%20making%20long%20sequences%20and%20high%20resolutions%20computationally%0Aexpensive.%20While%20linear%20attention%20offers%20sub-quadratic%20complexity%2C%20prior%0Aattempts%20fail%20to%20match%20the%20expressiveness%20of%20softmax%20attention%20without%20costly%0Aretraining.%20We%20introduce%20Attention%20Surgery%2C%20an%20efficient%20framework%20for%0Alinearizing%20or%20hybridizing%20attention%20in%20pretrained%20VDMs%20without%20training%20from%0Ascratch.%20Inspired%20by%20recent%20advances%20in%20language%20models%2C%20our%20method%20combines%20a%0Anovel%20hybrid%20attention%20mechanism-mixing%20softmax%20and%20linear%20tokens-with%20a%0Alightweight%20distillation%20and%20fine-tuning%20pipeline%20requiring%20only%20a%20few%0AGPU-days.%20Additionally%2C%20we%20incorporate%20a%20cost-aware%20block-rate%20strategy%20to%0Abalance%20expressiveness%20and%20efficiency%20across%20layers.%20Applied%20to%20Wan2.1%201.3B%2C%20a%0Astate-of-the-art%20DiT-based%20VDM%2C%20Attention%20Surgery%20achieves%20the%20first%0Acompetitive%20sub-quadratic%20attention%20video%20diffusion%20models%2C%20reducing%20attention%0Acost%20by%20up%20to%2040%5C%25%20in%20terms%20of%20FLOPs%2C%20while%20maintaining%20generation%20quality%20as%0Ameasured%20on%20the%20standard%20VBench%20and%20VBench-2.0%20benchmarks.%20Project%20page%20is%0Aavailable%20at%3A%20https%3A//qualcomm-ai-research.github.io/attention-surgery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24899v2&entry.124074799=Read"},
{"title": "FedPPA: Progressive Parameter Alignment for Personalized Federated\n  Learning", "author": "Maulidi Adi Prasetia and Muhamad Risqi U. Saputra and Guntur Dharma Putra", "abstract": "  Federated Learning (FL) is designed as a decentralized, privacy-preserving\nmachine learning paradigm that enables multiple clients to collaboratively\ntrain a model without sharing their data. In real-world scenarios, however,\nclients often have heterogeneous computational resources and hold\nnon-independent and identically distributed data (non-IID), which poses\nsignificant challenges during training. Personalized Federated Learning (PFL)\nhas emerged to address these issues by customizing models for each client based\non their unique data distribution. Despite its potential, existing PFL\napproaches typically overlook the coexistence of model and data heterogeneity\narising from clients with diverse computational capabilities. To overcome this\nlimitation, we propose a novel method, called Progressive Parameter Alignment\n(FedPPA), which progressively aligns the weights of common layers across\nclients with the global model's weights. Our approach not only mitigates\ninconsistencies between global and local models during client updates, but also\npreserves client's local knowledge, thereby enhancing personalization\nrobustness in non-IID settings. To further enhance the global model performance\nwhile retaining strong personalization, we also integrate entropy-based\nweighted averaging into the FedPPA framework. Experiments on three image\nclassification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate\nthat FedPPA consistently outperforms existing FL algorithms, achieving superior\nperformance in personalized adaptation.\n", "link": "http://arxiv.org/abs/2510.14698v1", "date": "2025-10-16", "relevancy": 2.5156, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedPPA%3A%20Progressive%20Parameter%20Alignment%20for%20Personalized%20Federated%0A%20%20Learning&body=Title%3A%20FedPPA%3A%20Progressive%20Parameter%20Alignment%20for%20Personalized%20Federated%0A%20%20Learning%0AAuthor%3A%20Maulidi%20Adi%20Prasetia%20and%20Muhamad%20Risqi%20U.%20Saputra%20and%20Guntur%20Dharma%20Putra%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20designed%20as%20a%20decentralized%2C%20privacy-preserving%0Amachine%20learning%20paradigm%20that%20enables%20multiple%20clients%20to%20collaboratively%0Atrain%20a%20model%20without%20sharing%20their%20data.%20In%20real-world%20scenarios%2C%20however%2C%0Aclients%20often%20have%20heterogeneous%20computational%20resources%20and%20hold%0Anon-independent%20and%20identically%20distributed%20data%20%28non-IID%29%2C%20which%20poses%0Asignificant%20challenges%20during%20training.%20Personalized%20Federated%20Learning%20%28PFL%29%0Ahas%20emerged%20to%20address%20these%20issues%20by%20customizing%20models%20for%20each%20client%20based%0Aon%20their%20unique%20data%20distribution.%20Despite%20its%20potential%2C%20existing%20PFL%0Aapproaches%20typically%20overlook%20the%20coexistence%20of%20model%20and%20data%20heterogeneity%0Aarising%20from%20clients%20with%20diverse%20computational%20capabilities.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20a%20novel%20method%2C%20called%20Progressive%20Parameter%20Alignment%0A%28FedPPA%29%2C%20which%20progressively%20aligns%20the%20weights%20of%20common%20layers%20across%0Aclients%20with%20the%20global%20model%27s%20weights.%20Our%20approach%20not%20only%20mitigates%0Ainconsistencies%20between%20global%20and%20local%20models%20during%20client%20updates%2C%20but%20also%0Apreserves%20client%27s%20local%20knowledge%2C%20thereby%20enhancing%20personalization%0Arobustness%20in%20non-IID%20settings.%20To%20further%20enhance%20the%20global%20model%20performance%0Awhile%20retaining%20strong%20personalization%2C%20we%20also%20integrate%20entropy-based%0Aweighted%20averaging%20into%20the%20FedPPA%20framework.%20Experiments%20on%20three%20image%0Aclassification%20datasets%2C%20including%20MNIST%2C%20FMNIST%2C%20and%20CIFAR-10%2C%20demonstrate%0Athat%20FedPPA%20consistently%20outperforms%20existing%20FL%20algorithms%2C%20achieving%20superior%0Aperformance%20in%20personalized%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedPPA%253A%2520Progressive%2520Parameter%2520Alignment%2520for%2520Personalized%2520Federated%250A%2520%2520Learning%26entry.906535625%3DMaulidi%2520Adi%2520Prasetia%2520and%2520Muhamad%2520Risqi%2520U.%2520Saputra%2520and%2520Guntur%2520Dharma%2520Putra%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520designed%2520as%2520a%2520decentralized%252C%2520privacy-preserving%250Amachine%2520learning%2520paradigm%2520that%2520enables%2520multiple%2520clients%2520to%2520collaboratively%250Atrain%2520a%2520model%2520without%2520sharing%2520their%2520data.%2520In%2520real-world%2520scenarios%252C%2520however%252C%250Aclients%2520often%2520have%2520heterogeneous%2520computational%2520resources%2520and%2520hold%250Anon-independent%2520and%2520identically%2520distributed%2520data%2520%2528non-IID%2529%252C%2520which%2520poses%250Asignificant%2520challenges%2520during%2520training.%2520Personalized%2520Federated%2520Learning%2520%2528PFL%2529%250Ahas%2520emerged%2520to%2520address%2520these%2520issues%2520by%2520customizing%2520models%2520for%2520each%2520client%2520based%250Aon%2520their%2520unique%2520data%2520distribution.%2520Despite%2520its%2520potential%252C%2520existing%2520PFL%250Aapproaches%2520typically%2520overlook%2520the%2520coexistence%2520of%2520model%2520and%2520data%2520heterogeneity%250Aarising%2520from%2520clients%2520with%2520diverse%2520computational%2520capabilities.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520called%2520Progressive%2520Parameter%2520Alignment%250A%2528FedPPA%2529%252C%2520which%2520progressively%2520aligns%2520the%2520weights%2520of%2520common%2520layers%2520across%250Aclients%2520with%2520the%2520global%2520model%2527s%2520weights.%2520Our%2520approach%2520not%2520only%2520mitigates%250Ainconsistencies%2520between%2520global%2520and%2520local%2520models%2520during%2520client%2520updates%252C%2520but%2520also%250Apreserves%2520client%2527s%2520local%2520knowledge%252C%2520thereby%2520enhancing%2520personalization%250Arobustness%2520in%2520non-IID%2520settings.%2520To%2520further%2520enhance%2520the%2520global%2520model%2520performance%250Awhile%2520retaining%2520strong%2520personalization%252C%2520we%2520also%2520integrate%2520entropy-based%250Aweighted%2520averaging%2520into%2520the%2520FedPPA%2520framework.%2520Experiments%2520on%2520three%2520image%250Aclassification%2520datasets%252C%2520including%2520MNIST%252C%2520FMNIST%252C%2520and%2520CIFAR-10%252C%2520demonstrate%250Athat%2520FedPPA%2520consistently%2520outperforms%2520existing%2520FL%2520algorithms%252C%2520achieving%2520superior%250Aperformance%2520in%2520personalized%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedPPA%3A%20Progressive%20Parameter%20Alignment%20for%20Personalized%20Federated%0A%20%20Learning&entry.906535625=Maulidi%20Adi%20Prasetia%20and%20Muhamad%20Risqi%20U.%20Saputra%20and%20Guntur%20Dharma%20Putra&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20designed%20as%20a%20decentralized%2C%20privacy-preserving%0Amachine%20learning%20paradigm%20that%20enables%20multiple%20clients%20to%20collaboratively%0Atrain%20a%20model%20without%20sharing%20their%20data.%20In%20real-world%20scenarios%2C%20however%2C%0Aclients%20often%20have%20heterogeneous%20computational%20resources%20and%20hold%0Anon-independent%20and%20identically%20distributed%20data%20%28non-IID%29%2C%20which%20poses%0Asignificant%20challenges%20during%20training.%20Personalized%20Federated%20Learning%20%28PFL%29%0Ahas%20emerged%20to%20address%20these%20issues%20by%20customizing%20models%20for%20each%20client%20based%0Aon%20their%20unique%20data%20distribution.%20Despite%20its%20potential%2C%20existing%20PFL%0Aapproaches%20typically%20overlook%20the%20coexistence%20of%20model%20and%20data%20heterogeneity%0Aarising%20from%20clients%20with%20diverse%20computational%20capabilities.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20a%20novel%20method%2C%20called%20Progressive%20Parameter%20Alignment%0A%28FedPPA%29%2C%20which%20progressively%20aligns%20the%20weights%20of%20common%20layers%20across%0Aclients%20with%20the%20global%20model%27s%20weights.%20Our%20approach%20not%20only%20mitigates%0Ainconsistencies%20between%20global%20and%20local%20models%20during%20client%20updates%2C%20but%20also%0Apreserves%20client%27s%20local%20knowledge%2C%20thereby%20enhancing%20personalization%0Arobustness%20in%20non-IID%20settings.%20To%20further%20enhance%20the%20global%20model%20performance%0Awhile%20retaining%20strong%20personalization%2C%20we%20also%20integrate%20entropy-based%0Aweighted%20averaging%20into%20the%20FedPPA%20framework.%20Experiments%20on%20three%20image%0Aclassification%20datasets%2C%20including%20MNIST%2C%20FMNIST%2C%20and%20CIFAR-10%2C%20demonstrate%0Athat%20FedPPA%20consistently%20outperforms%20existing%20FL%20algorithms%2C%20achieving%20superior%0Aperformance%20in%20personalized%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14698v1&entry.124074799=Read"},
{"title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models", "author": "Jonathan Roberts and Kai Han and Samuel Albanie", "abstract": "  Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is predominantly\nsynthetic, ensuring high-quality, noise-free questions. GRAB is comprised of\n3284 questions, covering five tasks and 23 graph properties. We evaluate 20\nLMMs on GRAB, finding it to be a challenging benchmark, with the highest\nperforming model attaining a score of just 21.0%. Finally, we conduct various\nablations to investigate where the models succeed and struggle. We release GRAB\nand a lightweight GRAB-Lite to encourage progress in this important, growing\ndomain.\n", "link": "http://arxiv.org/abs/2408.11817v3", "date": "2025-10-16", "relevancy": 2.5079, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models&body=Title%3A%20GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models%0AAuthor%3A%20Jonathan%20Roberts%20and%20Kai%20Han%20and%20Samuel%20Albanie%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20proficiencies%20across%20many%0Avisual%20tasks.%20Although%20numerous%20well-known%20benchmarks%20exist%20to%20evaluate%20model%0Aperformance%2C%20they%20increasingly%20have%20insufficient%20headroom.%20As%20such%2C%20there%20is%20a%0Apressing%20need%20for%20a%20new%20generation%20of%20benchmarks%20challenging%20enough%20for%20the%0Anext%20generation%20of%20LMMs.%20One%20area%20that%20LMMs%20show%20potential%20is%20graph%20analysis%2C%0Aspecifically%2C%20the%20tasks%20an%20analyst%20might%20typically%20perform%20when%20interpreting%0Afigures%20such%20as%20estimating%20the%20mean%2C%20intercepts%20or%20correlations%20of%20functions%0Aand%20data%20series.%20In%20this%20work%2C%20we%20introduce%20GRAB%2C%20a%20graph%20analysis%20benchmark%2C%0Afit%20for%20current%20and%20future%20frontier%20LMMs.%20Our%20benchmark%20is%20predominantly%0Asynthetic%2C%20ensuring%20high-quality%2C%20noise-free%20questions.%20GRAB%20is%20comprised%20of%0A3284%20questions%2C%20covering%20five%20tasks%20and%2023%20graph%20properties.%20We%20evaluate%2020%0ALMMs%20on%20GRAB%2C%20finding%20it%20to%20be%20a%20challenging%20benchmark%2C%20with%20the%20highest%0Aperforming%20model%20attaining%20a%20score%20of%20just%2021.0%25.%20Finally%2C%20we%20conduct%20various%0Aablations%20to%20investigate%20where%20the%20models%20succeed%20and%20struggle.%20We%20release%20GRAB%0Aand%20a%20lightweight%20GRAB-Lite%20to%20encourage%20progress%20in%20this%20important%2C%20growing%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11817v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAB%253A%2520A%2520Challenging%2520GRaph%2520Analysis%2520Benchmark%2520for%2520Large%2520Multimodal%2520Models%26entry.906535625%3DJonathan%2520Roberts%2520and%2520Kai%2520Han%2520and%2520Samuel%2520Albanie%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520exhibited%2520proficiencies%2520across%2520many%250Avisual%2520tasks.%2520Although%2520numerous%2520well-known%2520benchmarks%2520exist%2520to%2520evaluate%2520model%250Aperformance%252C%2520they%2520increasingly%2520have%2520insufficient%2520headroom.%2520As%2520such%252C%2520there%2520is%2520a%250Apressing%2520need%2520for%2520a%2520new%2520generation%2520of%2520benchmarks%2520challenging%2520enough%2520for%2520the%250Anext%2520generation%2520of%2520LMMs.%2520One%2520area%2520that%2520LMMs%2520show%2520potential%2520is%2520graph%2520analysis%252C%250Aspecifically%252C%2520the%2520tasks%2520an%2520analyst%2520might%2520typically%2520perform%2520when%2520interpreting%250Afigures%2520such%2520as%2520estimating%2520the%2520mean%252C%2520intercepts%2520or%2520correlations%2520of%2520functions%250Aand%2520data%2520series.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GRAB%252C%2520a%2520graph%2520analysis%2520benchmark%252C%250Afit%2520for%2520current%2520and%2520future%2520frontier%2520LMMs.%2520Our%2520benchmark%2520is%2520predominantly%250Asynthetic%252C%2520ensuring%2520high-quality%252C%2520noise-free%2520questions.%2520GRAB%2520is%2520comprised%2520of%250A3284%2520questions%252C%2520covering%2520five%2520tasks%2520and%252023%2520graph%2520properties.%2520We%2520evaluate%252020%250ALMMs%2520on%2520GRAB%252C%2520finding%2520it%2520to%2520be%2520a%2520challenging%2520benchmark%252C%2520with%2520the%2520highest%250Aperforming%2520model%2520attaining%2520a%2520score%2520of%2520just%252021.0%2525.%2520Finally%252C%2520we%2520conduct%2520various%250Aablations%2520to%2520investigate%2520where%2520the%2520models%2520succeed%2520and%2520struggle.%2520We%2520release%2520GRAB%250Aand%2520a%2520lightweight%2520GRAB-Lite%2520to%2520encourage%2520progress%2520in%2520this%2520important%252C%2520growing%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11817v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models&entry.906535625=Jonathan%20Roberts%20and%20Kai%20Han%20and%20Samuel%20Albanie&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20proficiencies%20across%20many%0Avisual%20tasks.%20Although%20numerous%20well-known%20benchmarks%20exist%20to%20evaluate%20model%0Aperformance%2C%20they%20increasingly%20have%20insufficient%20headroom.%20As%20such%2C%20there%20is%20a%0Apressing%20need%20for%20a%20new%20generation%20of%20benchmarks%20challenging%20enough%20for%20the%0Anext%20generation%20of%20LMMs.%20One%20area%20that%20LMMs%20show%20potential%20is%20graph%20analysis%2C%0Aspecifically%2C%20the%20tasks%20an%20analyst%20might%20typically%20perform%20when%20interpreting%0Afigures%20such%20as%20estimating%20the%20mean%2C%20intercepts%20or%20correlations%20of%20functions%0Aand%20data%20series.%20In%20this%20work%2C%20we%20introduce%20GRAB%2C%20a%20graph%20analysis%20benchmark%2C%0Afit%20for%20current%20and%20future%20frontier%20LMMs.%20Our%20benchmark%20is%20predominantly%0Asynthetic%2C%20ensuring%20high-quality%2C%20noise-free%20questions.%20GRAB%20is%20comprised%20of%0A3284%20questions%2C%20covering%20five%20tasks%20and%2023%20graph%20properties.%20We%20evaluate%2020%0ALMMs%20on%20GRAB%2C%20finding%20it%20to%20be%20a%20challenging%20benchmark%2C%20with%20the%20highest%0Aperforming%20model%20attaining%20a%20score%20of%20just%2021.0%25.%20Finally%2C%20we%20conduct%20various%0Aablations%20to%20investigate%20where%20the%20models%20succeed%20and%20struggle.%20We%20release%20GRAB%0Aand%20a%20lightweight%20GRAB-Lite%20to%20encourage%20progress%20in%20this%20important%2C%20growing%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11817v3&entry.124074799=Read"},
{"title": "Backdoor Unlearning by Linear Task Decomposition", "author": "Amel Abdelraheem and Alessandro Favero and Gerome Bovet and Pascal Frossard", "abstract": "  Foundation models have revolutionized computer vision by enabling broad\ngeneralization across diverse tasks. Yet, they remain highly susceptible to\nadversarial perturbations and targeted backdoor attacks. Mitigating such\nvulnerabilities remains an open challenge, especially given that the\nlarge-scale nature of the models prohibits retraining to ensure safety.\nExisting backdoor removal approaches rely on costly fine-tuning to override the\nharmful behavior, and can often degrade performance on other unrelated tasks.\nThis raises the question of whether backdoors can be removed without\ncompromising the general capabilities of the models. In this work, we address\nthis question and study how backdoors are encoded in the model weight space,\nfinding that they are disentangled from other benign tasks. Specifically, this\nseparation enables the isolation and erasure of the backdoor's influence on the\nmodel with minimal impact on clean performance. Building on this insight, we\nintroduce a simple unlearning method that leverages such disentanglement.\nThrough extensive experiments with CLIP-based models and common adversarial\ntriggers, we show that, given the knowledge of the attack, our method achieves\napproximately perfect unlearning, while retaining, on average, 96% of clean\naccuracy. Additionally, we demonstrate that even when the attack and its\npresence are unknown, our method successfully unlearns backdoors by proper\nestimation using reverse-engineered triggers. Overall, our method consistently\nyields better unlearning and clean accuracy tradeoffs when compared to present\nstate-of-the-art defenses.\n", "link": "http://arxiv.org/abs/2510.14845v1", "date": "2025-10-16", "relevancy": 2.4937, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4932}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdoor%20Unlearning%20by%20Linear%20Task%20Decomposition&body=Title%3A%20Backdoor%20Unlearning%20by%20Linear%20Task%20Decomposition%0AAuthor%3A%20Amel%20Abdelraheem%20and%20Alessandro%20Favero%20and%20Gerome%20Bovet%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20Foundation%20models%20have%20revolutionized%20computer%20vision%20by%20enabling%20broad%0Ageneralization%20across%20diverse%20tasks.%20Yet%2C%20they%20remain%20highly%20susceptible%20to%0Aadversarial%20perturbations%20and%20targeted%20backdoor%20attacks.%20Mitigating%20such%0Avulnerabilities%20remains%20an%20open%20challenge%2C%20especially%20given%20that%20the%0Alarge-scale%20nature%20of%20the%20models%20prohibits%20retraining%20to%20ensure%20safety.%0AExisting%20backdoor%20removal%20approaches%20rely%20on%20costly%20fine-tuning%20to%20override%20the%0Aharmful%20behavior%2C%20and%20can%20often%20degrade%20performance%20on%20other%20unrelated%20tasks.%0AThis%20raises%20the%20question%20of%20whether%20backdoors%20can%20be%20removed%20without%0Acompromising%20the%20general%20capabilities%20of%20the%20models.%20In%20this%20work%2C%20we%20address%0Athis%20question%20and%20study%20how%20backdoors%20are%20encoded%20in%20the%20model%20weight%20space%2C%0Afinding%20that%20they%20are%20disentangled%20from%20other%20benign%20tasks.%20Specifically%2C%20this%0Aseparation%20enables%20the%20isolation%20and%20erasure%20of%20the%20backdoor%27s%20influence%20on%20the%0Amodel%20with%20minimal%20impact%20on%20clean%20performance.%20Building%20on%20this%20insight%2C%20we%0Aintroduce%20a%20simple%20unlearning%20method%20that%20leverages%20such%20disentanglement.%0AThrough%20extensive%20experiments%20with%20CLIP-based%20models%20and%20common%20adversarial%0Atriggers%2C%20we%20show%20that%2C%20given%20the%20knowledge%20of%20the%20attack%2C%20our%20method%20achieves%0Aapproximately%20perfect%20unlearning%2C%20while%20retaining%2C%20on%20average%2C%2096%25%20of%20clean%0Aaccuracy.%20Additionally%2C%20we%20demonstrate%20that%20even%20when%20the%20attack%20and%20its%0Apresence%20are%20unknown%2C%20our%20method%20successfully%20unlearns%20backdoors%20by%20proper%0Aestimation%20using%20reverse-engineered%20triggers.%20Overall%2C%20our%20method%20consistently%0Ayields%20better%20unlearning%20and%20clean%20accuracy%20tradeoffs%20when%20compared%20to%20present%0Astate-of-the-art%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoor%2520Unlearning%2520by%2520Linear%2520Task%2520Decomposition%26entry.906535625%3DAmel%2520Abdelraheem%2520and%2520Alessandro%2520Favero%2520and%2520Gerome%2520Bovet%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520revolutionized%2520computer%2520vision%2520by%2520enabling%2520broad%250Ageneralization%2520across%2520diverse%2520tasks.%2520Yet%252C%2520they%2520remain%2520highly%2520susceptible%2520to%250Aadversarial%2520perturbations%2520and%2520targeted%2520backdoor%2520attacks.%2520Mitigating%2520such%250Avulnerabilities%2520remains%2520an%2520open%2520challenge%252C%2520especially%2520given%2520that%2520the%250Alarge-scale%2520nature%2520of%2520the%2520models%2520prohibits%2520retraining%2520to%2520ensure%2520safety.%250AExisting%2520backdoor%2520removal%2520approaches%2520rely%2520on%2520costly%2520fine-tuning%2520to%2520override%2520the%250Aharmful%2520behavior%252C%2520and%2520can%2520often%2520degrade%2520performance%2520on%2520other%2520unrelated%2520tasks.%250AThis%2520raises%2520the%2520question%2520of%2520whether%2520backdoors%2520can%2520be%2520removed%2520without%250Acompromising%2520the%2520general%2520capabilities%2520of%2520the%2520models.%2520In%2520this%2520work%252C%2520we%2520address%250Athis%2520question%2520and%2520study%2520how%2520backdoors%2520are%2520encoded%2520in%2520the%2520model%2520weight%2520space%252C%250Afinding%2520that%2520they%2520are%2520disentangled%2520from%2520other%2520benign%2520tasks.%2520Specifically%252C%2520this%250Aseparation%2520enables%2520the%2520isolation%2520and%2520erasure%2520of%2520the%2520backdoor%2527s%2520influence%2520on%2520the%250Amodel%2520with%2520minimal%2520impact%2520on%2520clean%2520performance.%2520Building%2520on%2520this%2520insight%252C%2520we%250Aintroduce%2520a%2520simple%2520unlearning%2520method%2520that%2520leverages%2520such%2520disentanglement.%250AThrough%2520extensive%2520experiments%2520with%2520CLIP-based%2520models%2520and%2520common%2520adversarial%250Atriggers%252C%2520we%2520show%2520that%252C%2520given%2520the%2520knowledge%2520of%2520the%2520attack%252C%2520our%2520method%2520achieves%250Aapproximately%2520perfect%2520unlearning%252C%2520while%2520retaining%252C%2520on%2520average%252C%252096%2525%2520of%2520clean%250Aaccuracy.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520even%2520when%2520the%2520attack%2520and%2520its%250Apresence%2520are%2520unknown%252C%2520our%2520method%2520successfully%2520unlearns%2520backdoors%2520by%2520proper%250Aestimation%2520using%2520reverse-engineered%2520triggers.%2520Overall%252C%2520our%2520method%2520consistently%250Ayields%2520better%2520unlearning%2520and%2520clean%2520accuracy%2520tradeoffs%2520when%2520compared%2520to%2520present%250Astate-of-the-art%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdoor%20Unlearning%20by%20Linear%20Task%20Decomposition&entry.906535625=Amel%20Abdelraheem%20and%20Alessandro%20Favero%20and%20Gerome%20Bovet%20and%20Pascal%20Frossard&entry.1292438233=%20%20Foundation%20models%20have%20revolutionized%20computer%20vision%20by%20enabling%20broad%0Ageneralization%20across%20diverse%20tasks.%20Yet%2C%20they%20remain%20highly%20susceptible%20to%0Aadversarial%20perturbations%20and%20targeted%20backdoor%20attacks.%20Mitigating%20such%0Avulnerabilities%20remains%20an%20open%20challenge%2C%20especially%20given%20that%20the%0Alarge-scale%20nature%20of%20the%20models%20prohibits%20retraining%20to%20ensure%20safety.%0AExisting%20backdoor%20removal%20approaches%20rely%20on%20costly%20fine-tuning%20to%20override%20the%0Aharmful%20behavior%2C%20and%20can%20often%20degrade%20performance%20on%20other%20unrelated%20tasks.%0AThis%20raises%20the%20question%20of%20whether%20backdoors%20can%20be%20removed%20without%0Acompromising%20the%20general%20capabilities%20of%20the%20models.%20In%20this%20work%2C%20we%20address%0Athis%20question%20and%20study%20how%20backdoors%20are%20encoded%20in%20the%20model%20weight%20space%2C%0Afinding%20that%20they%20are%20disentangled%20from%20other%20benign%20tasks.%20Specifically%2C%20this%0Aseparation%20enables%20the%20isolation%20and%20erasure%20of%20the%20backdoor%27s%20influence%20on%20the%0Amodel%20with%20minimal%20impact%20on%20clean%20performance.%20Building%20on%20this%20insight%2C%20we%0Aintroduce%20a%20simple%20unlearning%20method%20that%20leverages%20such%20disentanglement.%0AThrough%20extensive%20experiments%20with%20CLIP-based%20models%20and%20common%20adversarial%0Atriggers%2C%20we%20show%20that%2C%20given%20the%20knowledge%20of%20the%20attack%2C%20our%20method%20achieves%0Aapproximately%20perfect%20unlearning%2C%20while%20retaining%2C%20on%20average%2C%2096%25%20of%20clean%0Aaccuracy.%20Additionally%2C%20we%20demonstrate%20that%20even%20when%20the%20attack%20and%20its%0Apresence%20are%20unknown%2C%20our%20method%20successfully%20unlearns%20backdoors%20by%20proper%0Aestimation%20using%20reverse-engineered%20triggers.%20Overall%2C%20our%20method%20consistently%0Ayields%20better%20unlearning%20and%20clean%20accuracy%20tradeoffs%20when%20compared%20to%20present%0Astate-of-the-art%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14845v1&entry.124074799=Read"},
{"title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using\n  Speech, Text, and EEG", "author": "Annisaa Fitri Nurfidausi and Eleonora Mancini and Paolo Torroni", "abstract": "  Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection.\n", "link": "http://arxiv.org/abs/2510.14922v1", "date": "2025-10-16", "relevancy": 2.485, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRI-DEP%3A%20A%20Trimodal%20Comparative%20Study%20for%20Depression%20Detection%20Using%0A%20%20Speech%2C%20Text%2C%20and%20EEG&body=Title%3A%20TRI-DEP%3A%20A%20Trimodal%20Comparative%20Study%20for%20Depression%20Detection%20Using%0A%20%20Speech%2C%20Text%2C%20and%20EEG%0AAuthor%3A%20Annisaa%20Fitri%20Nurfidausi%20and%20Eleonora%20Mancini%20and%20Paolo%20Torroni%0AAbstract%3A%20%20%20Depression%20is%20a%20widespread%20mental%20health%20disorder%2C%20yet%20its%20automatic%0Adetection%20remains%20challenging.%20Prior%20work%20has%20explored%20unimodal%20and%20multimodal%0Aapproaches%2C%20with%20multimodal%20systems%20showing%20promise%20by%20leveraging%20complementary%0Asignals.%20However%2C%20existing%20studies%20are%20limited%20in%20scope%2C%20lack%20systematic%0Acomparisons%20of%20features%2C%20and%20suffer%20from%20inconsistent%20evaluation%20protocols.%20We%0Aaddress%20these%20gaps%20by%20systematically%20exploring%20feature%20representations%20and%0Amodelling%20strategies%20across%20EEG%2C%20together%20with%20speech%20and%20text.%20We%20evaluate%0Ahandcrafted%20features%20versus%20pre-trained%20embeddings%2C%20assess%20the%20effectiveness%20of%0Adifferent%20neural%20encoders%2C%20compare%20unimodal%2C%20bimodal%2C%20and%20trimodal%0Aconfigurations%2C%20and%20analyse%20fusion%20strategies%20with%20attention%20to%20the%20role%20of%0AEEG.%20Consistent%20subject-independent%20splits%20are%20applied%20to%20ensure%20robust%2C%0Areproducible%20benchmarking.%20Our%20results%20show%20that%20%28i%29%20the%20combination%20of%20EEG%2C%0Aspeech%20and%20text%20modalities%20enhances%20multimodal%20detection%2C%20%28ii%29%20pretrained%0Aembeddings%20outperform%20handcrafted%20features%2C%20and%20%28iii%29%20carefully%20designed%0Atrimodal%20models%20achieve%20state-of-the-art%20performance.%20Our%20work%20lays%20the%0Agroundwork%20for%20future%20research%20in%20multimodal%20depression%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRI-DEP%253A%2520A%2520Trimodal%2520Comparative%2520Study%2520for%2520Depression%2520Detection%2520Using%250A%2520%2520Speech%252C%2520Text%252C%2520and%2520EEG%26entry.906535625%3DAnnisaa%2520Fitri%2520Nurfidausi%2520and%2520Eleonora%2520Mancini%2520and%2520Paolo%2520Torroni%26entry.1292438233%3D%2520%2520Depression%2520is%2520a%2520widespread%2520mental%2520health%2520disorder%252C%2520yet%2520its%2520automatic%250Adetection%2520remains%2520challenging.%2520Prior%2520work%2520has%2520explored%2520unimodal%2520and%2520multimodal%250Aapproaches%252C%2520with%2520multimodal%2520systems%2520showing%2520promise%2520by%2520leveraging%2520complementary%250Asignals.%2520However%252C%2520existing%2520studies%2520are%2520limited%2520in%2520scope%252C%2520lack%2520systematic%250Acomparisons%2520of%2520features%252C%2520and%2520suffer%2520from%2520inconsistent%2520evaluation%2520protocols.%2520We%250Aaddress%2520these%2520gaps%2520by%2520systematically%2520exploring%2520feature%2520representations%2520and%250Amodelling%2520strategies%2520across%2520EEG%252C%2520together%2520with%2520speech%2520and%2520text.%2520We%2520evaluate%250Ahandcrafted%2520features%2520versus%2520pre-trained%2520embeddings%252C%2520assess%2520the%2520effectiveness%2520of%250Adifferent%2520neural%2520encoders%252C%2520compare%2520unimodal%252C%2520bimodal%252C%2520and%2520trimodal%250Aconfigurations%252C%2520and%2520analyse%2520fusion%2520strategies%2520with%2520attention%2520to%2520the%2520role%2520of%250AEEG.%2520Consistent%2520subject-independent%2520splits%2520are%2520applied%2520to%2520ensure%2520robust%252C%250Areproducible%2520benchmarking.%2520Our%2520results%2520show%2520that%2520%2528i%2529%2520the%2520combination%2520of%2520EEG%252C%250Aspeech%2520and%2520text%2520modalities%2520enhances%2520multimodal%2520detection%252C%2520%2528ii%2529%2520pretrained%250Aembeddings%2520outperform%2520handcrafted%2520features%252C%2520and%2520%2528iii%2529%2520carefully%2520designed%250Atrimodal%2520models%2520achieve%2520state-of-the-art%2520performance.%2520Our%2520work%2520lays%2520the%250Agroundwork%2520for%2520future%2520research%2520in%2520multimodal%2520depression%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRI-DEP%3A%20A%20Trimodal%20Comparative%20Study%20for%20Depression%20Detection%20Using%0A%20%20Speech%2C%20Text%2C%20and%20EEG&entry.906535625=Annisaa%20Fitri%20Nurfidausi%20and%20Eleonora%20Mancini%20and%20Paolo%20Torroni&entry.1292438233=%20%20Depression%20is%20a%20widespread%20mental%20health%20disorder%2C%20yet%20its%20automatic%0Adetection%20remains%20challenging.%20Prior%20work%20has%20explored%20unimodal%20and%20multimodal%0Aapproaches%2C%20with%20multimodal%20systems%20showing%20promise%20by%20leveraging%20complementary%0Asignals.%20However%2C%20existing%20studies%20are%20limited%20in%20scope%2C%20lack%20systematic%0Acomparisons%20of%20features%2C%20and%20suffer%20from%20inconsistent%20evaluation%20protocols.%20We%0Aaddress%20these%20gaps%20by%20systematically%20exploring%20feature%20representations%20and%0Amodelling%20strategies%20across%20EEG%2C%20together%20with%20speech%20and%20text.%20We%20evaluate%0Ahandcrafted%20features%20versus%20pre-trained%20embeddings%2C%20assess%20the%20effectiveness%20of%0Adifferent%20neural%20encoders%2C%20compare%20unimodal%2C%20bimodal%2C%20and%20trimodal%0Aconfigurations%2C%20and%20analyse%20fusion%20strategies%20with%20attention%20to%20the%20role%20of%0AEEG.%20Consistent%20subject-independent%20splits%20are%20applied%20to%20ensure%20robust%2C%0Areproducible%20benchmarking.%20Our%20results%20show%20that%20%28i%29%20the%20combination%20of%20EEG%2C%0Aspeech%20and%20text%20modalities%20enhances%20multimodal%20detection%2C%20%28ii%29%20pretrained%0Aembeddings%20outperform%20handcrafted%20features%2C%20and%20%28iii%29%20carefully%20designed%0Atrimodal%20models%20achieve%20state-of-the-art%20performance.%20Our%20work%20lays%20the%0Agroundwork%20for%20future%20research%20in%20multimodal%20depression%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14922v1&entry.124074799=Read"},
{"title": "GraphLand: Evaluating Graph Machine Learning Models on Diverse\n  Industrial Data", "author": "Gleb Bazhenov and Oleg Platonov and Liudmila Prokhorenkova", "abstract": "  Although data that can be naturally represented as graphs is widespread in\nreal-world applications across diverse industries, popular graph ML benchmarks\nfor node property prediction only cover a surprisingly narrow set of data\ndomains, and graph neural networks (GNNs) are often evaluated on just a few\nacademic citation networks. This issue is particularly pressing in light of the\nrecent growing interest in designing graph foundation models. These models are\nsupposed to be able to transfer to diverse graph datasets from different\ndomains, and yet the proposed graph foundation models are often evaluated on a\nvery limited set of datasets from narrow applications. To alleviate this issue,\nwe introduce GraphLand: a benchmark of 14 diverse graph datasets for node\nproperty prediction from a range of different industrial applications.\nGraphLand allows evaluating graph ML models on a wide range of graphs with\ndiverse sizes, structural characteristics, and feature sets, all in a unified\nsetting. Further, GraphLand allows investigating such previously underexplored\nresearch questions as how realistic temporal distributional shifts under\ntransductive and inductive settings influence graph ML model performance. To\nmimic realistic industrial settings, we use GraphLand to compare GNNs with\ngradient-boosted decision trees (GBDT) models that are popular in industrial\napplications and show that GBDTs provided with additional graph-based input\nfeatures can sometimes be very strong baselines. Further, we evaluate currently\navailable general-purpose graph foundation models and find that they fail to\nproduce competitive results on our proposed datasets.\n", "link": "http://arxiv.org/abs/2409.14500v4", "date": "2025-10-16", "relevancy": 2.4734, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphLand%3A%20Evaluating%20Graph%20Machine%20Learning%20Models%20on%20Diverse%0A%20%20Industrial%20Data&body=Title%3A%20GraphLand%3A%20Evaluating%20Graph%20Machine%20Learning%20Models%20on%20Diverse%0A%20%20Industrial%20Data%0AAuthor%3A%20Gleb%20Bazhenov%20and%20Oleg%20Platonov%20and%20Liudmila%20Prokhorenkova%0AAbstract%3A%20%20%20Although%20data%20that%20can%20be%20naturally%20represented%20as%20graphs%20is%20widespread%20in%0Areal-world%20applications%20across%20diverse%20industries%2C%20popular%20graph%20ML%20benchmarks%0Afor%20node%20property%20prediction%20only%20cover%20a%20surprisingly%20narrow%20set%20of%20data%0Adomains%2C%20and%20graph%20neural%20networks%20%28GNNs%29%20are%20often%20evaluated%20on%20just%20a%20few%0Aacademic%20citation%20networks.%20This%20issue%20is%20particularly%20pressing%20in%20light%20of%20the%0Arecent%20growing%20interest%20in%20designing%20graph%20foundation%20models.%20These%20models%20are%0Asupposed%20to%20be%20able%20to%20transfer%20to%20diverse%20graph%20datasets%20from%20different%0Adomains%2C%20and%20yet%20the%20proposed%20graph%20foundation%20models%20are%20often%20evaluated%20on%20a%0Avery%20limited%20set%20of%20datasets%20from%20narrow%20applications.%20To%20alleviate%20this%20issue%2C%0Awe%20introduce%20GraphLand%3A%20a%20benchmark%20of%2014%20diverse%20graph%20datasets%20for%20node%0Aproperty%20prediction%20from%20a%20range%20of%20different%20industrial%20applications.%0AGraphLand%20allows%20evaluating%20graph%20ML%20models%20on%20a%20wide%20range%20of%20graphs%20with%0Adiverse%20sizes%2C%20structural%20characteristics%2C%20and%20feature%20sets%2C%20all%20in%20a%20unified%0Asetting.%20Further%2C%20GraphLand%20allows%20investigating%20such%20previously%20underexplored%0Aresearch%20questions%20as%20how%20realistic%20temporal%20distributional%20shifts%20under%0Atransductive%20and%20inductive%20settings%20influence%20graph%20ML%20model%20performance.%20To%0Amimic%20realistic%20industrial%20settings%2C%20we%20use%20GraphLand%20to%20compare%20GNNs%20with%0Agradient-boosted%20decision%20trees%20%28GBDT%29%20models%20that%20are%20popular%20in%20industrial%0Aapplications%20and%20show%20that%20GBDTs%20provided%20with%20additional%20graph-based%20input%0Afeatures%20can%20sometimes%20be%20very%20strong%20baselines.%20Further%2C%20we%20evaluate%20currently%0Aavailable%20general-purpose%20graph%20foundation%20models%20and%20find%20that%20they%20fail%20to%0Aproduce%20competitive%20results%20on%20our%20proposed%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14500v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphLand%253A%2520Evaluating%2520Graph%2520Machine%2520Learning%2520Models%2520on%2520Diverse%250A%2520%2520Industrial%2520Data%26entry.906535625%3DGleb%2520Bazhenov%2520and%2520Oleg%2520Platonov%2520and%2520Liudmila%2520Prokhorenkova%26entry.1292438233%3D%2520%2520Although%2520data%2520that%2520can%2520be%2520naturally%2520represented%2520as%2520graphs%2520is%2520widespread%2520in%250Areal-world%2520applications%2520across%2520diverse%2520industries%252C%2520popular%2520graph%2520ML%2520benchmarks%250Afor%2520node%2520property%2520prediction%2520only%2520cover%2520a%2520surprisingly%2520narrow%2520set%2520of%2520data%250Adomains%252C%2520and%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520often%2520evaluated%2520on%2520just%2520a%2520few%250Aacademic%2520citation%2520networks.%2520This%2520issue%2520is%2520particularly%2520pressing%2520in%2520light%2520of%2520the%250Arecent%2520growing%2520interest%2520in%2520designing%2520graph%2520foundation%2520models.%2520These%2520models%2520are%250Asupposed%2520to%2520be%2520able%2520to%2520transfer%2520to%2520diverse%2520graph%2520datasets%2520from%2520different%250Adomains%252C%2520and%2520yet%2520the%2520proposed%2520graph%2520foundation%2520models%2520are%2520often%2520evaluated%2520on%2520a%250Avery%2520limited%2520set%2520of%2520datasets%2520from%2520narrow%2520applications.%2520To%2520alleviate%2520this%2520issue%252C%250Awe%2520introduce%2520GraphLand%253A%2520a%2520benchmark%2520of%252014%2520diverse%2520graph%2520datasets%2520for%2520node%250Aproperty%2520prediction%2520from%2520a%2520range%2520of%2520different%2520industrial%2520applications.%250AGraphLand%2520allows%2520evaluating%2520graph%2520ML%2520models%2520on%2520a%2520wide%2520range%2520of%2520graphs%2520with%250Adiverse%2520sizes%252C%2520structural%2520characteristics%252C%2520and%2520feature%2520sets%252C%2520all%2520in%2520a%2520unified%250Asetting.%2520Further%252C%2520GraphLand%2520allows%2520investigating%2520such%2520previously%2520underexplored%250Aresearch%2520questions%2520as%2520how%2520realistic%2520temporal%2520distributional%2520shifts%2520under%250Atransductive%2520and%2520inductive%2520settings%2520influence%2520graph%2520ML%2520model%2520performance.%2520To%250Amimic%2520realistic%2520industrial%2520settings%252C%2520we%2520use%2520GraphLand%2520to%2520compare%2520GNNs%2520with%250Agradient-boosted%2520decision%2520trees%2520%2528GBDT%2529%2520models%2520that%2520are%2520popular%2520in%2520industrial%250Aapplications%2520and%2520show%2520that%2520GBDTs%2520provided%2520with%2520additional%2520graph-based%2520input%250Afeatures%2520can%2520sometimes%2520be%2520very%2520strong%2520baselines.%2520Further%252C%2520we%2520evaluate%2520currently%250Aavailable%2520general-purpose%2520graph%2520foundation%2520models%2520and%2520find%2520that%2520they%2520fail%2520to%250Aproduce%2520competitive%2520results%2520on%2520our%2520proposed%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14500v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphLand%3A%20Evaluating%20Graph%20Machine%20Learning%20Models%20on%20Diverse%0A%20%20Industrial%20Data&entry.906535625=Gleb%20Bazhenov%20and%20Oleg%20Platonov%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20Although%20data%20that%20can%20be%20naturally%20represented%20as%20graphs%20is%20widespread%20in%0Areal-world%20applications%20across%20diverse%20industries%2C%20popular%20graph%20ML%20benchmarks%0Afor%20node%20property%20prediction%20only%20cover%20a%20surprisingly%20narrow%20set%20of%20data%0Adomains%2C%20and%20graph%20neural%20networks%20%28GNNs%29%20are%20often%20evaluated%20on%20just%20a%20few%0Aacademic%20citation%20networks.%20This%20issue%20is%20particularly%20pressing%20in%20light%20of%20the%0Arecent%20growing%20interest%20in%20designing%20graph%20foundation%20models.%20These%20models%20are%0Asupposed%20to%20be%20able%20to%20transfer%20to%20diverse%20graph%20datasets%20from%20different%0Adomains%2C%20and%20yet%20the%20proposed%20graph%20foundation%20models%20are%20often%20evaluated%20on%20a%0Avery%20limited%20set%20of%20datasets%20from%20narrow%20applications.%20To%20alleviate%20this%20issue%2C%0Awe%20introduce%20GraphLand%3A%20a%20benchmark%20of%2014%20diverse%20graph%20datasets%20for%20node%0Aproperty%20prediction%20from%20a%20range%20of%20different%20industrial%20applications.%0AGraphLand%20allows%20evaluating%20graph%20ML%20models%20on%20a%20wide%20range%20of%20graphs%20with%0Adiverse%20sizes%2C%20structural%20characteristics%2C%20and%20feature%20sets%2C%20all%20in%20a%20unified%0Asetting.%20Further%2C%20GraphLand%20allows%20investigating%20such%20previously%20underexplored%0Aresearch%20questions%20as%20how%20realistic%20temporal%20distributional%20shifts%20under%0Atransductive%20and%20inductive%20settings%20influence%20graph%20ML%20model%20performance.%20To%0Amimic%20realistic%20industrial%20settings%2C%20we%20use%20GraphLand%20to%20compare%20GNNs%20with%0Agradient-boosted%20decision%20trees%20%28GBDT%29%20models%20that%20are%20popular%20in%20industrial%0Aapplications%20and%20show%20that%20GBDTs%20provided%20with%20additional%20graph-based%20input%0Afeatures%20can%20sometimes%20be%20very%20strong%20baselines.%20Further%2C%20we%20evaluate%20currently%0Aavailable%20general-purpose%20graph%20foundation%20models%20and%20find%20that%20they%20fail%20to%0Aproduce%20competitive%20results%20on%20our%20proposed%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14500v4&entry.124074799=Read"},
{"title": "Where are the Whales: A Human-in-the-loop Detection Method for\n  Identifying Whales in High-resolution Satellite Imagery", "author": "Caleb Robinson and Kimberly T. Goetz and Christin B. Khan and Meredith Sackett and Kathleen Leonard and Rahul Dodhia and Juan M. Lavista Ferres", "abstract": "  Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.\n", "link": "http://arxiv.org/abs/2510.14709v1", "date": "2025-10-16", "relevancy": 2.459, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4961}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4957}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20are%20the%20Whales%3A%20A%20Human-in-the-loop%20Detection%20Method%20for%0A%20%20Identifying%20Whales%20in%20High-resolution%20Satellite%20Imagery&body=Title%3A%20Where%20are%20the%20Whales%3A%20A%20Human-in-the-loop%20Detection%20Method%20for%0A%20%20Identifying%20Whales%20in%20High-resolution%20Satellite%20Imagery%0AAuthor%3A%20Caleb%20Robinson%20and%20Kimberly%20T.%20Goetz%20and%20Christin%20B.%20Khan%20and%20Meredith%20Sackett%20and%20Kathleen%20Leonard%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%0AAbstract%3A%20%20%20Effective%20monitoring%20of%20whale%20populations%20is%20critical%20for%20conservation%2C%20but%0Atraditional%20survey%20methods%20are%20expensive%20and%20difficult%20to%20scale.%20While%20prior%0Awork%20has%20shown%20that%20whales%20can%20be%20identified%20in%20very%20high-resolution%20%28VHR%29%0Asatellite%20imagery%2C%20large-scale%20automated%20detection%20remains%20challenging%20due%20to%20a%0Alack%20of%20annotated%20imagery%2C%20variability%20in%20image%20quality%20and%20environmental%0Aconditions%2C%20and%20the%20cost%20of%20building%20robust%20machine%20learning%20pipelines%20over%0Amassive%20remote%20sensing%20archives.%20We%20present%20a%20semi-automated%20approach%20for%0Asurfacing%20possible%20whale%20detections%20in%20VHR%20imagery%20using%20a%20statistical%20anomaly%0Adetection%20method%20that%20flags%20spatial%20outliers%2C%20i.e.%20%22interesting%20points%22.%20We%0Apair%20this%20detector%20with%20a%20web-based%20labeling%20interface%20designed%20to%20enable%0Aexperts%20to%20quickly%20annotate%20the%20interesting%20points.%20We%20evaluate%20our%20system%20on%0Athree%20benchmark%20scenes%20with%20known%20whale%20annotations%20and%20achieve%20recalls%20of%0A90.3%25%20to%2096.4%25%2C%20while%20reducing%20the%20area%20requiring%20expert%20inspection%20by%20up%20to%0A99.8%25%20--%20from%20over%201%2C000%20sq%20km%20to%20less%20than%202%20sq%20km%20in%20some%20cases.%20Our%20method%0Adoes%20not%20rely%20on%20labeled%20training%20data%20and%20offers%20a%20scalable%20first%20step%20toward%0Afuture%20machine-assisted%20marine%20mammal%20monitoring%20from%20space.%20We%20have%20open%0Asourced%20this%20pipeline%20at%20https%3A//github.com/microsoft/whales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520are%2520the%2520Whales%253A%2520A%2520Human-in-the-loop%2520Detection%2520Method%2520for%250A%2520%2520Identifying%2520Whales%2520in%2520High-resolution%2520Satellite%2520Imagery%26entry.906535625%3DCaleb%2520Robinson%2520and%2520Kimberly%2520T.%2520Goetz%2520and%2520Christin%2520B.%2520Khan%2520and%2520Meredith%2520Sackett%2520and%2520Kathleen%2520Leonard%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520M.%2520Lavista%2520Ferres%26entry.1292438233%3D%2520%2520Effective%2520monitoring%2520of%2520whale%2520populations%2520is%2520critical%2520for%2520conservation%252C%2520but%250Atraditional%2520survey%2520methods%2520are%2520expensive%2520and%2520difficult%2520to%2520scale.%2520While%2520prior%250Awork%2520has%2520shown%2520that%2520whales%2520can%2520be%2520identified%2520in%2520very%2520high-resolution%2520%2528VHR%2529%250Asatellite%2520imagery%252C%2520large-scale%2520automated%2520detection%2520remains%2520challenging%2520due%2520to%2520a%250Alack%2520of%2520annotated%2520imagery%252C%2520variability%2520in%2520image%2520quality%2520and%2520environmental%250Aconditions%252C%2520and%2520the%2520cost%2520of%2520building%2520robust%2520machine%2520learning%2520pipelines%2520over%250Amassive%2520remote%2520sensing%2520archives.%2520We%2520present%2520a%2520semi-automated%2520approach%2520for%250Asurfacing%2520possible%2520whale%2520detections%2520in%2520VHR%2520imagery%2520using%2520a%2520statistical%2520anomaly%250Adetection%2520method%2520that%2520flags%2520spatial%2520outliers%252C%2520i.e.%2520%2522interesting%2520points%2522.%2520We%250Apair%2520this%2520detector%2520with%2520a%2520web-based%2520labeling%2520interface%2520designed%2520to%2520enable%250Aexperts%2520to%2520quickly%2520annotate%2520the%2520interesting%2520points.%2520We%2520evaluate%2520our%2520system%2520on%250Athree%2520benchmark%2520scenes%2520with%2520known%2520whale%2520annotations%2520and%2520achieve%2520recalls%2520of%250A90.3%2525%2520to%252096.4%2525%252C%2520while%2520reducing%2520the%2520area%2520requiring%2520expert%2520inspection%2520by%2520up%2520to%250A99.8%2525%2520--%2520from%2520over%25201%252C000%2520sq%2520km%2520to%2520less%2520than%25202%2520sq%2520km%2520in%2520some%2520cases.%2520Our%2520method%250Adoes%2520not%2520rely%2520on%2520labeled%2520training%2520data%2520and%2520offers%2520a%2520scalable%2520first%2520step%2520toward%250Afuture%2520machine-assisted%2520marine%2520mammal%2520monitoring%2520from%2520space.%2520We%2520have%2520open%250Asourced%2520this%2520pipeline%2520at%2520https%253A//github.com/microsoft/whales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20are%20the%20Whales%3A%20A%20Human-in-the-loop%20Detection%20Method%20for%0A%20%20Identifying%20Whales%20in%20High-resolution%20Satellite%20Imagery&entry.906535625=Caleb%20Robinson%20and%20Kimberly%20T.%20Goetz%20and%20Christin%20B.%20Khan%20and%20Meredith%20Sackett%20and%20Kathleen%20Leonard%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres&entry.1292438233=%20%20Effective%20monitoring%20of%20whale%20populations%20is%20critical%20for%20conservation%2C%20but%0Atraditional%20survey%20methods%20are%20expensive%20and%20difficult%20to%20scale.%20While%20prior%0Awork%20has%20shown%20that%20whales%20can%20be%20identified%20in%20very%20high-resolution%20%28VHR%29%0Asatellite%20imagery%2C%20large-scale%20automated%20detection%20remains%20challenging%20due%20to%20a%0Alack%20of%20annotated%20imagery%2C%20variability%20in%20image%20quality%20and%20environmental%0Aconditions%2C%20and%20the%20cost%20of%20building%20robust%20machine%20learning%20pipelines%20over%0Amassive%20remote%20sensing%20archives.%20We%20present%20a%20semi-automated%20approach%20for%0Asurfacing%20possible%20whale%20detections%20in%20VHR%20imagery%20using%20a%20statistical%20anomaly%0Adetection%20method%20that%20flags%20spatial%20outliers%2C%20i.e.%20%22interesting%20points%22.%20We%0Apair%20this%20detector%20with%20a%20web-based%20labeling%20interface%20designed%20to%20enable%0Aexperts%20to%20quickly%20annotate%20the%20interesting%20points.%20We%20evaluate%20our%20system%20on%0Athree%20benchmark%20scenes%20with%20known%20whale%20annotations%20and%20achieve%20recalls%20of%0A90.3%25%20to%2096.4%25%2C%20while%20reducing%20the%20area%20requiring%20expert%20inspection%20by%20up%20to%0A99.8%25%20--%20from%20over%201%2C000%20sq%20km%20to%20less%20than%202%20sq%20km%20in%20some%20cases.%20Our%20method%0Adoes%20not%20rely%20on%20labeled%20training%20data%20and%20offers%20a%20scalable%20first%20step%20toward%0Afuture%20machine-assisted%20marine%20mammal%20monitoring%20from%20space.%20We%20have%20open%0Asourced%20this%20pipeline%20at%20https%3A//github.com/microsoft/whales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14709v1&entry.124074799=Read"},
{"title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance", "author": "Zhe Li and Cheng Chi and Yangyang Wei and Boan Zhu and Yibo Peng and Tao Huang and Pengwei Wang and Zhongyuan Wang and Shanghang Zhang and Chang Xu", "abstract": "  Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.\n", "link": "http://arxiv.org/abs/2510.14952v1", "date": "2025-10-16", "relevancy": 2.4556, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6382}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6105}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Language%20to%20Locomotion%3A%20Retargeting-free%20Humanoid%20Control%20via%0A%20%20Motion%20Latent%20Guidance&body=Title%3A%20From%20Language%20to%20Locomotion%3A%20Retargeting-free%20Humanoid%20Control%20via%0A%20%20Motion%20Latent%20Guidance%0AAuthor%3A%20Zhe%20Li%20and%20Cheng%20Chi%20and%20Yangyang%20Wei%20and%20Boan%20Zhu%20and%20Yibo%20Peng%20and%20Tao%20Huang%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Shanghang%20Zhang%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Natural%20language%20offers%20a%20natural%20interface%20for%20humanoid%20robots%2C%20but%20existing%0Alanguage-guided%20humanoid%20locomotion%20pipelines%20remain%20cumbersome%20and%20unreliable.%0AThey%20typically%20decode%20human%20motion%2C%20retarget%20it%20to%20robot%20morphology%2C%20and%20then%0Atrack%20it%20with%20a%20physics-based%20controller.%20However%2C%20this%20multi-stage%20process%20is%0Aprone%20to%20cumulative%20errors%2C%20introduces%20high%20latency%2C%20and%20yields%20weak%20coupling%0Abetween%20semantics%20and%20control.%20These%20limitations%20call%20for%20a%20more%20direct%20pathway%0Afrom%20language%20to%20action%2C%20one%20that%20eliminates%20fragile%20intermediate%20stages.%0ATherefore%2C%20we%20present%20RoboGhost%2C%20a%20retargeting-free%20framework%20that%20directly%0Aconditions%20humanoid%20policies%20on%20language-grounded%20motion%20latents.%20By%20bypassing%0Aexplicit%20motion%20decoding%20and%20retargeting%2C%20RoboGhost%20enables%20a%20diffusion-based%0Apolicy%20to%20denoise%20executable%20actions%20directly%20from%20noise%2C%20preserving%20semantic%0Aintent%20and%20supporting%20fast%2C%20reactive%20control.%20A%20hybrid%20causal%0Atransformer-diffusion%20motion%20generator%20further%20ensures%20long-horizon%20consistency%0Awhile%20maintaining%20stability%20and%20diversity%2C%20yielding%20rich%20latent%20representations%0Afor%20precise%20humanoid%20behavior.%20Extensive%20experiments%20demonstrate%20that%20RoboGhost%0Asubstantially%20reduces%20deployment%20latency%2C%20improves%20success%20rates%20and%20tracking%0Aaccuracy%2C%20and%20produces%20smooth%2C%20semantically%20aligned%20locomotion%20on%20real%0Ahumanoids.%20Beyond%20text%2C%20the%20framework%20naturally%20extends%20to%20other%20modalities%0Asuch%20as%20images%2C%20audio%2C%20and%20music%2C%20providing%20a%20general%20foundation%20for%0Avision-language-action%20humanoid%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Language%2520to%2520Locomotion%253A%2520Retargeting-free%2520Humanoid%2520Control%2520via%250A%2520%2520Motion%2520Latent%2520Guidance%26entry.906535625%3DZhe%2520Li%2520and%2520Cheng%2520Chi%2520and%2520Yangyang%2520Wei%2520and%2520Boan%2520Zhu%2520and%2520Yibo%2520Peng%2520and%2520Tao%2520Huang%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Shanghang%2520Zhang%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Natural%2520language%2520offers%2520a%2520natural%2520interface%2520for%2520humanoid%2520robots%252C%2520but%2520existing%250Alanguage-guided%2520humanoid%2520locomotion%2520pipelines%2520remain%2520cumbersome%2520and%2520unreliable.%250AThey%2520typically%2520decode%2520human%2520motion%252C%2520retarget%2520it%2520to%2520robot%2520morphology%252C%2520and%2520then%250Atrack%2520it%2520with%2520a%2520physics-based%2520controller.%2520However%252C%2520this%2520multi-stage%2520process%2520is%250Aprone%2520to%2520cumulative%2520errors%252C%2520introduces%2520high%2520latency%252C%2520and%2520yields%2520weak%2520coupling%250Abetween%2520semantics%2520and%2520control.%2520These%2520limitations%2520call%2520for%2520a%2520more%2520direct%2520pathway%250Afrom%2520language%2520to%2520action%252C%2520one%2520that%2520eliminates%2520fragile%2520intermediate%2520stages.%250ATherefore%252C%2520we%2520present%2520RoboGhost%252C%2520a%2520retargeting-free%2520framework%2520that%2520directly%250Aconditions%2520humanoid%2520policies%2520on%2520language-grounded%2520motion%2520latents.%2520By%2520bypassing%250Aexplicit%2520motion%2520decoding%2520and%2520retargeting%252C%2520RoboGhost%2520enables%2520a%2520diffusion-based%250Apolicy%2520to%2520denoise%2520executable%2520actions%2520directly%2520from%2520noise%252C%2520preserving%2520semantic%250Aintent%2520and%2520supporting%2520fast%252C%2520reactive%2520control.%2520A%2520hybrid%2520causal%250Atransformer-diffusion%2520motion%2520generator%2520further%2520ensures%2520long-horizon%2520consistency%250Awhile%2520maintaining%2520stability%2520and%2520diversity%252C%2520yielding%2520rich%2520latent%2520representations%250Afor%2520precise%2520humanoid%2520behavior.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RoboGhost%250Asubstantially%2520reduces%2520deployment%2520latency%252C%2520improves%2520success%2520rates%2520and%2520tracking%250Aaccuracy%252C%2520and%2520produces%2520smooth%252C%2520semantically%2520aligned%2520locomotion%2520on%2520real%250Ahumanoids.%2520Beyond%2520text%252C%2520the%2520framework%2520naturally%2520extends%2520to%2520other%2520modalities%250Asuch%2520as%2520images%252C%2520audio%252C%2520and%2520music%252C%2520providing%2520a%2520general%2520foundation%2520for%250Avision-language-action%2520humanoid%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Language%20to%20Locomotion%3A%20Retargeting-free%20Humanoid%20Control%20via%0A%20%20Motion%20Latent%20Guidance&entry.906535625=Zhe%20Li%20and%20Cheng%20Chi%20and%20Yangyang%20Wei%20and%20Boan%20Zhu%20and%20Yibo%20Peng%20and%20Tao%20Huang%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Shanghang%20Zhang%20and%20Chang%20Xu&entry.1292438233=%20%20Natural%20language%20offers%20a%20natural%20interface%20for%20humanoid%20robots%2C%20but%20existing%0Alanguage-guided%20humanoid%20locomotion%20pipelines%20remain%20cumbersome%20and%20unreliable.%0AThey%20typically%20decode%20human%20motion%2C%20retarget%20it%20to%20robot%20morphology%2C%20and%20then%0Atrack%20it%20with%20a%20physics-based%20controller.%20However%2C%20this%20multi-stage%20process%20is%0Aprone%20to%20cumulative%20errors%2C%20introduces%20high%20latency%2C%20and%20yields%20weak%20coupling%0Abetween%20semantics%20and%20control.%20These%20limitations%20call%20for%20a%20more%20direct%20pathway%0Afrom%20language%20to%20action%2C%20one%20that%20eliminates%20fragile%20intermediate%20stages.%0ATherefore%2C%20we%20present%20RoboGhost%2C%20a%20retargeting-free%20framework%20that%20directly%0Aconditions%20humanoid%20policies%20on%20language-grounded%20motion%20latents.%20By%20bypassing%0Aexplicit%20motion%20decoding%20and%20retargeting%2C%20RoboGhost%20enables%20a%20diffusion-based%0Apolicy%20to%20denoise%20executable%20actions%20directly%20from%20noise%2C%20preserving%20semantic%0Aintent%20and%20supporting%20fast%2C%20reactive%20control.%20A%20hybrid%20causal%0Atransformer-diffusion%20motion%20generator%20further%20ensures%20long-horizon%20consistency%0Awhile%20maintaining%20stability%20and%20diversity%2C%20yielding%20rich%20latent%20representations%0Afor%20precise%20humanoid%20behavior.%20Extensive%20experiments%20demonstrate%20that%20RoboGhost%0Asubstantially%20reduces%20deployment%20latency%2C%20improves%20success%20rates%20and%20tracking%0Aaccuracy%2C%20and%20produces%20smooth%2C%20semantically%20aligned%20locomotion%20on%20real%0Ahumanoids.%20Beyond%20text%2C%20the%20framework%20naturally%20extends%20to%20other%20modalities%0Asuch%20as%20images%2C%20audio%2C%20and%20music%2C%20providing%20a%20general%20foundation%20for%0Avision-language-action%20humanoid%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14952v1&entry.124074799=Read"},
{"title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision\n  Models", "author": "Simone Carnemolla and Matteo Pennisi and Sarinda Samarasinghe and Giovanni Bellitto and Simone Palazzo and Daniela Giordano and Mubarak Shah and Concetto Spampinato", "abstract": "  Understanding and explaining the behavior of machine learning models is\nessential for building transparent and trustworthy AI systems. We introduce\nDEXTER, a data-free framework that employs diffusion models and large language\nmodels to generate global, textual explanations of visual classifiers. DEXTER\noperates by optimizing text prompts to synthesize class-conditional images that\nstrongly activate a target classifier. These synthetic samples are then used to\nelicit detailed natural language reports that describe class-specific decision\npatterns and biases. Unlike prior work, DEXTER enables natural language\nexplanation about a classifier's decision process without access to training\ndata or ground-truth labels. We demonstrate DEXTER's flexibility across three\ntasks-activation maximization, slice discovery and debiasing, and bias\nexplanation-each illustrating its ability to uncover the internal mechanisms of\nvisual classifiers. Quantitative and qualitative evaluations, including a user\nstudy, show that DEXTER produces accurate, interpretable outputs. Experiments\non ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms\nexisting approaches in global model explanation and class-level bias reporting.\nCode is available at https://github.com/perceivelab/dexter.\n", "link": "http://arxiv.org/abs/2510.14741v1", "date": "2025-10-16", "relevancy": 2.4372, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6184}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEXTER%3A%20Diffusion-Guided%20EXplanations%20with%20TExtual%20Reasoning%20for%20Vision%0A%20%20Models&body=Title%3A%20DEXTER%3A%20Diffusion-Guided%20EXplanations%20with%20TExtual%20Reasoning%20for%20Vision%0A%20%20Models%0AAuthor%3A%20Simone%20Carnemolla%20and%20Matteo%20Pennisi%20and%20Sarinda%20Samarasinghe%20and%20Giovanni%20Bellitto%20and%20Simone%20Palazzo%20and%20Daniela%20Giordano%20and%20Mubarak%20Shah%20and%20Concetto%20Spampinato%0AAbstract%3A%20%20%20Understanding%20and%20explaining%20the%20behavior%20of%20machine%20learning%20models%20is%0Aessential%20for%20building%20transparent%20and%20trustworthy%20AI%20systems.%20We%20introduce%0ADEXTER%2C%20a%20data-free%20framework%20that%20employs%20diffusion%20models%20and%20large%20language%0Amodels%20to%20generate%20global%2C%20textual%20explanations%20of%20visual%20classifiers.%20DEXTER%0Aoperates%20by%20optimizing%20text%20prompts%20to%20synthesize%20class-conditional%20images%20that%0Astrongly%20activate%20a%20target%20classifier.%20These%20synthetic%20samples%20are%20then%20used%20to%0Aelicit%20detailed%20natural%20language%20reports%20that%20describe%20class-specific%20decision%0Apatterns%20and%20biases.%20Unlike%20prior%20work%2C%20DEXTER%20enables%20natural%20language%0Aexplanation%20about%20a%20classifier%27s%20decision%20process%20without%20access%20to%20training%0Adata%20or%20ground-truth%20labels.%20We%20demonstrate%20DEXTER%27s%20flexibility%20across%20three%0Atasks-activation%20maximization%2C%20slice%20discovery%20and%20debiasing%2C%20and%20bias%0Aexplanation-each%20illustrating%20its%20ability%20to%20uncover%20the%20internal%20mechanisms%20of%0Avisual%20classifiers.%20Quantitative%20and%20qualitative%20evaluations%2C%20including%20a%20user%0Astudy%2C%20show%20that%20DEXTER%20produces%20accurate%2C%20interpretable%20outputs.%20Experiments%0Aon%20ImageNet%2C%20Waterbirds%2C%20CelebA%2C%20and%20FairFaces%20confirm%20that%20DEXTER%20outperforms%0Aexisting%20approaches%20in%20global%20model%20explanation%20and%20class-level%20bias%20reporting.%0ACode%20is%20available%20at%20https%3A//github.com/perceivelab/dexter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEXTER%253A%2520Diffusion-Guided%2520EXplanations%2520with%2520TExtual%2520Reasoning%2520for%2520Vision%250A%2520%2520Models%26entry.906535625%3DSimone%2520Carnemolla%2520and%2520Matteo%2520Pennisi%2520and%2520Sarinda%2520Samarasinghe%2520and%2520Giovanni%2520Bellitto%2520and%2520Simone%2520Palazzo%2520and%2520Daniela%2520Giordano%2520and%2520Mubarak%2520Shah%2520and%2520Concetto%2520Spampinato%26entry.1292438233%3D%2520%2520Understanding%2520and%2520explaining%2520the%2520behavior%2520of%2520machine%2520learning%2520models%2520is%250Aessential%2520for%2520building%2520transparent%2520and%2520trustworthy%2520AI%2520systems.%2520We%2520introduce%250ADEXTER%252C%2520a%2520data-free%2520framework%2520that%2520employs%2520diffusion%2520models%2520and%2520large%2520language%250Amodels%2520to%2520generate%2520global%252C%2520textual%2520explanations%2520of%2520visual%2520classifiers.%2520DEXTER%250Aoperates%2520by%2520optimizing%2520text%2520prompts%2520to%2520synthesize%2520class-conditional%2520images%2520that%250Astrongly%2520activate%2520a%2520target%2520classifier.%2520These%2520synthetic%2520samples%2520are%2520then%2520used%2520to%250Aelicit%2520detailed%2520natural%2520language%2520reports%2520that%2520describe%2520class-specific%2520decision%250Apatterns%2520and%2520biases.%2520Unlike%2520prior%2520work%252C%2520DEXTER%2520enables%2520natural%2520language%250Aexplanation%2520about%2520a%2520classifier%2527s%2520decision%2520process%2520without%2520access%2520to%2520training%250Adata%2520or%2520ground-truth%2520labels.%2520We%2520demonstrate%2520DEXTER%2527s%2520flexibility%2520across%2520three%250Atasks-activation%2520maximization%252C%2520slice%2520discovery%2520and%2520debiasing%252C%2520and%2520bias%250Aexplanation-each%2520illustrating%2520its%2520ability%2520to%2520uncover%2520the%2520internal%2520mechanisms%2520of%250Avisual%2520classifiers.%2520Quantitative%2520and%2520qualitative%2520evaluations%252C%2520including%2520a%2520user%250Astudy%252C%2520show%2520that%2520DEXTER%2520produces%2520accurate%252C%2520interpretable%2520outputs.%2520Experiments%250Aon%2520ImageNet%252C%2520Waterbirds%252C%2520CelebA%252C%2520and%2520FairFaces%2520confirm%2520that%2520DEXTER%2520outperforms%250Aexisting%2520approaches%2520in%2520global%2520model%2520explanation%2520and%2520class-level%2520bias%2520reporting.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/perceivelab/dexter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEXTER%3A%20Diffusion-Guided%20EXplanations%20with%20TExtual%20Reasoning%20for%20Vision%0A%20%20Models&entry.906535625=Simone%20Carnemolla%20and%20Matteo%20Pennisi%20and%20Sarinda%20Samarasinghe%20and%20Giovanni%20Bellitto%20and%20Simone%20Palazzo%20and%20Daniela%20Giordano%20and%20Mubarak%20Shah%20and%20Concetto%20Spampinato&entry.1292438233=%20%20Understanding%20and%20explaining%20the%20behavior%20of%20machine%20learning%20models%20is%0Aessential%20for%20building%20transparent%20and%20trustworthy%20AI%20systems.%20We%20introduce%0ADEXTER%2C%20a%20data-free%20framework%20that%20employs%20diffusion%20models%20and%20large%20language%0Amodels%20to%20generate%20global%2C%20textual%20explanations%20of%20visual%20classifiers.%20DEXTER%0Aoperates%20by%20optimizing%20text%20prompts%20to%20synthesize%20class-conditional%20images%20that%0Astrongly%20activate%20a%20target%20classifier.%20These%20synthetic%20samples%20are%20then%20used%20to%0Aelicit%20detailed%20natural%20language%20reports%20that%20describe%20class-specific%20decision%0Apatterns%20and%20biases.%20Unlike%20prior%20work%2C%20DEXTER%20enables%20natural%20language%0Aexplanation%20about%20a%20classifier%27s%20decision%20process%20without%20access%20to%20training%0Adata%20or%20ground-truth%20labels.%20We%20demonstrate%20DEXTER%27s%20flexibility%20across%20three%0Atasks-activation%20maximization%2C%20slice%20discovery%20and%20debiasing%2C%20and%20bias%0Aexplanation-each%20illustrating%20its%20ability%20to%20uncover%20the%20internal%20mechanisms%20of%0Avisual%20classifiers.%20Quantitative%20and%20qualitative%20evaluations%2C%20including%20a%20user%0Astudy%2C%20show%20that%20DEXTER%20produces%20accurate%2C%20interpretable%20outputs.%20Experiments%0Aon%20ImageNet%2C%20Waterbirds%2C%20CelebA%2C%20and%20FairFaces%20confirm%20that%20DEXTER%20outperforms%0Aexisting%20approaches%20in%20global%20model%20explanation%20and%20class-level%20bias%20reporting.%0ACode%20is%20available%20at%20https%3A//github.com/perceivelab/dexter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14741v1&entry.124074799=Read"},
{"title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with\n  More Reports, Fewer Masks", "author": "Pedro R. A. S. Bassi and Xinze Zhou and Wenxuan Li and Szymon P\u0142otka and Jieneng Chen and Qi Chen and Zheren Zhu and Jakub Prz\u0105do and Ibrahim E. Hamac\u0131 and Sezgin Er and Yuhan Wang and Ashwin Kumar and Bjoern Menze and Jaros\u0142aw B. \u0106wik\u0142a and Yuyin Zhou and Akshay S. Chaudhari and Curtis P. Langlotz and Sergio Decherchi and Andrea Cavalli and Kang Wang and Yang Yang and Alan L. Yuille and Zongwei Zhou", "abstract": "  Early tumor detection save lives. Each year, more than 300 million computed\ntomography (CT) scans are performed worldwide, offering a vast opportunity for\neffective cancer screening. However, detecting small or early-stage tumors on\nthese CT scans remains challenging, even for experts. Artificial intelligence\n(AI) models can assist by highlighting suspicious regions, but training such\nmodels typically requires extensive tumor masks--detailed, voxel-wise outlines\nof tumors manually drawn by radiologists. Drawing these masks is costly,\nrequiring years of effort and millions of dollars. In contrast, nearly every CT\nscan in clinical practice is already accompanied by medical reports describing\nthe tumor's size, number, appearance, and sometimes, pathology\nresults--information that is rich, abundant, and often underutilized for AI\ntraining. We introduce R-Super, which trains AI to segment tumors that match\ntheir descriptions in medical reports. This approach scales AI training with\nlarge collections of readily available medical reports, substantially reducing\nthe need for manually drawn tumor masks. When trained on 101,654 reports, AI\nmodels achieved performance comparable to those trained on 723 masks. Combining\nreports and masks further improved sensitivity by +13% and specificity by +8%,\nsurpassing radiologists in detecting five of the seven tumor types. Notably,\nR-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,\nbladder, uterus, and esophagus, for which no public masks or AI models\npreviously existed. This study challenges the long-held belief that\nlarge-scale, labor-intensive tumor mask creation is indispensable, establishing\na scalable and accessible path toward early detection across diverse tumor\ntypes.\n  We plan to release our trained models, code, and dataset at\nhttps://github.com/MrGiovanni/R-Super\n", "link": "http://arxiv.org/abs/2510.14803v1", "date": "2025-10-16", "relevancy": 2.4368, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4997}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4844}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Artificial%20Intelligence%20for%20Multi-Tumor%20Early%20Detection%20with%0A%20%20More%20Reports%2C%20Fewer%20Masks&body=Title%3A%20Scaling%20Artificial%20Intelligence%20for%20Multi-Tumor%20Early%20Detection%20with%0A%20%20More%20Reports%2C%20Fewer%20Masks%0AAuthor%3A%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Xinze%20Zhou%20and%20Wenxuan%20Li%20and%20Szymon%20P%C5%82otka%20and%20Jieneng%20Chen%20and%20Qi%20Chen%20and%20Zheren%20Zhu%20and%20Jakub%20Prz%C4%85do%20and%20Ibrahim%20E.%20Hamac%C4%B1%20and%20Sezgin%20Er%20and%20Yuhan%20Wang%20and%20Ashwin%20Kumar%20and%20Bjoern%20Menze%20and%20Jaros%C5%82aw%20B.%20%C4%86wik%C5%82a%20and%20Yuyin%20Zhou%20and%20Akshay%20S.%20Chaudhari%20and%20Curtis%20P.%20Langlotz%20and%20Sergio%20Decherchi%20and%20Andrea%20Cavalli%20and%20Kang%20Wang%20and%20Yang%20Yang%20and%20Alan%20L.%20Yuille%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20Early%20tumor%20detection%20save%20lives.%20Each%20year%2C%20more%20than%20300%20million%20computed%0Atomography%20%28CT%29%20scans%20are%20performed%20worldwide%2C%20offering%20a%20vast%20opportunity%20for%0Aeffective%20cancer%20screening.%20However%2C%20detecting%20small%20or%20early-stage%20tumors%20on%0Athese%20CT%20scans%20remains%20challenging%2C%20even%20for%20experts.%20Artificial%20intelligence%0A%28AI%29%20models%20can%20assist%20by%20highlighting%20suspicious%20regions%2C%20but%20training%20such%0Amodels%20typically%20requires%20extensive%20tumor%20masks--detailed%2C%20voxel-wise%20outlines%0Aof%20tumors%20manually%20drawn%20by%20radiologists.%20Drawing%20these%20masks%20is%20costly%2C%0Arequiring%20years%20of%20effort%20and%20millions%20of%20dollars.%20In%20contrast%2C%20nearly%20every%20CT%0Ascan%20in%20clinical%20practice%20is%20already%20accompanied%20by%20medical%20reports%20describing%0Athe%20tumor%27s%20size%2C%20number%2C%20appearance%2C%20and%20sometimes%2C%20pathology%0Aresults--information%20that%20is%20rich%2C%20abundant%2C%20and%20often%20underutilized%20for%20AI%0Atraining.%20We%20introduce%20R-Super%2C%20which%20trains%20AI%20to%20segment%20tumors%20that%20match%0Atheir%20descriptions%20in%20medical%20reports.%20This%20approach%20scales%20AI%20training%20with%0Alarge%20collections%20of%20readily%20available%20medical%20reports%2C%20substantially%20reducing%0Athe%20need%20for%20manually%20drawn%20tumor%20masks.%20When%20trained%20on%20101%2C654%20reports%2C%20AI%0Amodels%20achieved%20performance%20comparable%20to%20those%20trained%20on%20723%20masks.%20Combining%0Areports%20and%20masks%20further%20improved%20sensitivity%20by%20%2B13%25%20and%20specificity%20by%20%2B8%25%2C%0Asurpassing%20radiologists%20in%20detecting%20five%20of%20the%20seven%20tumor%20types.%20Notably%2C%0AR-Super%20enabled%20segmentation%20of%20tumors%20in%20the%20spleen%2C%20gallbladder%2C%20prostate%2C%0Abladder%2C%20uterus%2C%20and%20esophagus%2C%20for%20which%20no%20public%20masks%20or%20AI%20models%0Apreviously%20existed.%20This%20study%20challenges%20the%20long-held%20belief%20that%0Alarge-scale%2C%20labor-intensive%20tumor%20mask%20creation%20is%20indispensable%2C%20establishing%0Aa%20scalable%20and%20accessible%20path%20toward%20early%20detection%20across%20diverse%20tumor%0Atypes.%0A%20%20We%20plan%20to%20release%20our%20trained%20models%2C%20code%2C%20and%20dataset%20at%0Ahttps%3A//github.com/MrGiovanni/R-Super%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Artificial%2520Intelligence%2520for%2520Multi-Tumor%2520Early%2520Detection%2520with%250A%2520%2520More%2520Reports%252C%2520Fewer%2520Masks%26entry.906535625%3DPedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Xinze%2520Zhou%2520and%2520Wenxuan%2520Li%2520and%2520Szymon%2520P%25C5%2582otka%2520and%2520Jieneng%2520Chen%2520and%2520Qi%2520Chen%2520and%2520Zheren%2520Zhu%2520and%2520Jakub%2520Prz%25C4%2585do%2520and%2520Ibrahim%2520E.%2520Hamac%25C4%25B1%2520and%2520Sezgin%2520Er%2520and%2520Yuhan%2520Wang%2520and%2520Ashwin%2520Kumar%2520and%2520Bjoern%2520Menze%2520and%2520Jaros%25C5%2582aw%2520B.%2520%25C4%2586wik%25C5%2582a%2520and%2520Yuyin%2520Zhou%2520and%2520Akshay%2520S.%2520Chaudhari%2520and%2520Curtis%2520P.%2520Langlotz%2520and%2520Sergio%2520Decherchi%2520and%2520Andrea%2520Cavalli%2520and%2520Kang%2520Wang%2520and%2520Yang%2520Yang%2520and%2520Alan%2520L.%2520Yuille%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520Early%2520tumor%2520detection%2520save%2520lives.%2520Each%2520year%252C%2520more%2520than%2520300%2520million%2520computed%250Atomography%2520%2528CT%2529%2520scans%2520are%2520performed%2520worldwide%252C%2520offering%2520a%2520vast%2520opportunity%2520for%250Aeffective%2520cancer%2520screening.%2520However%252C%2520detecting%2520small%2520or%2520early-stage%2520tumors%2520on%250Athese%2520CT%2520scans%2520remains%2520challenging%252C%2520even%2520for%2520experts.%2520Artificial%2520intelligence%250A%2528AI%2529%2520models%2520can%2520assist%2520by%2520highlighting%2520suspicious%2520regions%252C%2520but%2520training%2520such%250Amodels%2520typically%2520requires%2520extensive%2520tumor%2520masks--detailed%252C%2520voxel-wise%2520outlines%250Aof%2520tumors%2520manually%2520drawn%2520by%2520radiologists.%2520Drawing%2520these%2520masks%2520is%2520costly%252C%250Arequiring%2520years%2520of%2520effort%2520and%2520millions%2520of%2520dollars.%2520In%2520contrast%252C%2520nearly%2520every%2520CT%250Ascan%2520in%2520clinical%2520practice%2520is%2520already%2520accompanied%2520by%2520medical%2520reports%2520describing%250Athe%2520tumor%2527s%2520size%252C%2520number%252C%2520appearance%252C%2520and%2520sometimes%252C%2520pathology%250Aresults--information%2520that%2520is%2520rich%252C%2520abundant%252C%2520and%2520often%2520underutilized%2520for%2520AI%250Atraining.%2520We%2520introduce%2520R-Super%252C%2520which%2520trains%2520AI%2520to%2520segment%2520tumors%2520that%2520match%250Atheir%2520descriptions%2520in%2520medical%2520reports.%2520This%2520approach%2520scales%2520AI%2520training%2520with%250Alarge%2520collections%2520of%2520readily%2520available%2520medical%2520reports%252C%2520substantially%2520reducing%250Athe%2520need%2520for%2520manually%2520drawn%2520tumor%2520masks.%2520When%2520trained%2520on%2520101%252C654%2520reports%252C%2520AI%250Amodels%2520achieved%2520performance%2520comparable%2520to%2520those%2520trained%2520on%2520723%2520masks.%2520Combining%250Areports%2520and%2520masks%2520further%2520improved%2520sensitivity%2520by%2520%252B13%2525%2520and%2520specificity%2520by%2520%252B8%2525%252C%250Asurpassing%2520radiologists%2520in%2520detecting%2520five%2520of%2520the%2520seven%2520tumor%2520types.%2520Notably%252C%250AR-Super%2520enabled%2520segmentation%2520of%2520tumors%2520in%2520the%2520spleen%252C%2520gallbladder%252C%2520prostate%252C%250Abladder%252C%2520uterus%252C%2520and%2520esophagus%252C%2520for%2520which%2520no%2520public%2520masks%2520or%2520AI%2520models%250Apreviously%2520existed.%2520This%2520study%2520challenges%2520the%2520long-held%2520belief%2520that%250Alarge-scale%252C%2520labor-intensive%2520tumor%2520mask%2520creation%2520is%2520indispensable%252C%2520establishing%250Aa%2520scalable%2520and%2520accessible%2520path%2520toward%2520early%2520detection%2520across%2520diverse%2520tumor%250Atypes.%250A%2520%2520We%2520plan%2520to%2520release%2520our%2520trained%2520models%252C%2520code%252C%2520and%2520dataset%2520at%250Ahttps%253A//github.com/MrGiovanni/R-Super%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Artificial%20Intelligence%20for%20Multi-Tumor%20Early%20Detection%20with%0A%20%20More%20Reports%2C%20Fewer%20Masks&entry.906535625=Pedro%20R.%20A.%20S.%20Bassi%20and%20Xinze%20Zhou%20and%20Wenxuan%20Li%20and%20Szymon%20P%C5%82otka%20and%20Jieneng%20Chen%20and%20Qi%20Chen%20and%20Zheren%20Zhu%20and%20Jakub%20Prz%C4%85do%20and%20Ibrahim%20E.%20Hamac%C4%B1%20and%20Sezgin%20Er%20and%20Yuhan%20Wang%20and%20Ashwin%20Kumar%20and%20Bjoern%20Menze%20and%20Jaros%C5%82aw%20B.%20%C4%86wik%C5%82a%20and%20Yuyin%20Zhou%20and%20Akshay%20S.%20Chaudhari%20and%20Curtis%20P.%20Langlotz%20and%20Sergio%20Decherchi%20and%20Andrea%20Cavalli%20and%20Kang%20Wang%20and%20Yang%20Yang%20and%20Alan%20L.%20Yuille%20and%20Zongwei%20Zhou&entry.1292438233=%20%20Early%20tumor%20detection%20save%20lives.%20Each%20year%2C%20more%20than%20300%20million%20computed%0Atomography%20%28CT%29%20scans%20are%20performed%20worldwide%2C%20offering%20a%20vast%20opportunity%20for%0Aeffective%20cancer%20screening.%20However%2C%20detecting%20small%20or%20early-stage%20tumors%20on%0Athese%20CT%20scans%20remains%20challenging%2C%20even%20for%20experts.%20Artificial%20intelligence%0A%28AI%29%20models%20can%20assist%20by%20highlighting%20suspicious%20regions%2C%20but%20training%20such%0Amodels%20typically%20requires%20extensive%20tumor%20masks--detailed%2C%20voxel-wise%20outlines%0Aof%20tumors%20manually%20drawn%20by%20radiologists.%20Drawing%20these%20masks%20is%20costly%2C%0Arequiring%20years%20of%20effort%20and%20millions%20of%20dollars.%20In%20contrast%2C%20nearly%20every%20CT%0Ascan%20in%20clinical%20practice%20is%20already%20accompanied%20by%20medical%20reports%20describing%0Athe%20tumor%27s%20size%2C%20number%2C%20appearance%2C%20and%20sometimes%2C%20pathology%0Aresults--information%20that%20is%20rich%2C%20abundant%2C%20and%20often%20underutilized%20for%20AI%0Atraining.%20We%20introduce%20R-Super%2C%20which%20trains%20AI%20to%20segment%20tumors%20that%20match%0Atheir%20descriptions%20in%20medical%20reports.%20This%20approach%20scales%20AI%20training%20with%0Alarge%20collections%20of%20readily%20available%20medical%20reports%2C%20substantially%20reducing%0Athe%20need%20for%20manually%20drawn%20tumor%20masks.%20When%20trained%20on%20101%2C654%20reports%2C%20AI%0Amodels%20achieved%20performance%20comparable%20to%20those%20trained%20on%20723%20masks.%20Combining%0Areports%20and%20masks%20further%20improved%20sensitivity%20by%20%2B13%25%20and%20specificity%20by%20%2B8%25%2C%0Asurpassing%20radiologists%20in%20detecting%20five%20of%20the%20seven%20tumor%20types.%20Notably%2C%0AR-Super%20enabled%20segmentation%20of%20tumors%20in%20the%20spleen%2C%20gallbladder%2C%20prostate%2C%0Abladder%2C%20uterus%2C%20and%20esophagus%2C%20for%20which%20no%20public%20masks%20or%20AI%20models%0Apreviously%20existed.%20This%20study%20challenges%20the%20long-held%20belief%20that%0Alarge-scale%2C%20labor-intensive%20tumor%20mask%20creation%20is%20indispensable%2C%20establishing%0Aa%20scalable%20and%20accessible%20path%20toward%20early%20detection%20across%20diverse%20tumor%0Atypes.%0A%20%20We%20plan%20to%20release%20our%20trained%20models%2C%20code%2C%20and%20dataset%20at%0Ahttps%3A//github.com/MrGiovanni/R-Super%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14803v1&entry.124074799=Read"},
{"title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar", "author": "Yinxi Li and Yuntian Deng and Pengyu Nie", "abstract": "  Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.\n", "link": "http://arxiv.org/abs/2510.14972v1", "date": "2025-10-16", "relevancy": 2.402, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokDrift%3A%20When%20LLM%20Speaks%20in%20Subwords%20but%20Code%20Speaks%20in%20Grammar&body=Title%3A%20TokDrift%3A%20When%20LLM%20Speaks%20in%20Subwords%20but%20Code%20Speaks%20in%20Grammar%0AAuthor%3A%20Yinxi%20Li%20and%20Yuntian%20Deng%20and%20Pengyu%20Nie%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20for%20code%20rely%20on%20subword%20tokenizers%2C%20such%20as%0Abyte-pair%20encoding%20%28BPE%29%2C%20learned%20from%20mixed%20natural%20language%20text%20and%0Aprogramming%20language%20code%20but%20driven%20by%20statistics%20rather%20than%20grammar.%20As%20a%0Aresult%2C%20semantically%20identical%20code%20snippets%20can%20be%20tokenized%20differently%0Adepending%20on%20superficial%20factors%20such%20as%20whitespace%20or%20identifier%20naming.%20To%0Ameasure%20the%20impact%20of%20this%20misalignment%2C%20we%20introduce%20TokDrift%2C%20a%20framework%0Athat%20applies%20semantic-preserving%20rewrite%20rules%20to%20create%20code%20variants%0Adiffering%20only%20in%20tokenization.%20Across%20nine%20code%20LLMs%2C%20including%20large%20ones%0Awith%20over%2030B%20parameters%2C%20even%20minor%20formatting%20changes%20can%20cause%20substantial%0Ashifts%20in%20model%20behavior.%20Layer-wise%20analysis%20shows%20that%20the%20issue%20originates%0Ain%20early%20embeddings%2C%20where%20subword%20segmentation%20fails%20to%20capture%20grammar%20token%0Aboundaries.%20Our%20findings%20identify%20misaligned%20tokenization%20as%20a%20hidden%20obstacle%0Ato%20reliable%20code%20understanding%20and%20generation%2C%20highlighting%20the%20need%20for%0Agrammar-aware%20tokenization%20for%20future%20code%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokDrift%253A%2520When%2520LLM%2520Speaks%2520in%2520Subwords%2520but%2520Code%2520Speaks%2520in%2520Grammar%26entry.906535625%3DYinxi%2520Li%2520and%2520Yuntian%2520Deng%2520and%2520Pengyu%2520Nie%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520for%2520code%2520rely%2520on%2520subword%2520tokenizers%252C%2520such%2520as%250Abyte-pair%2520encoding%2520%2528BPE%2529%252C%2520learned%2520from%2520mixed%2520natural%2520language%2520text%2520and%250Aprogramming%2520language%2520code%2520but%2520driven%2520by%2520statistics%2520rather%2520than%2520grammar.%2520As%2520a%250Aresult%252C%2520semantically%2520identical%2520code%2520snippets%2520can%2520be%2520tokenized%2520differently%250Adepending%2520on%2520superficial%2520factors%2520such%2520as%2520whitespace%2520or%2520identifier%2520naming.%2520To%250Ameasure%2520the%2520impact%2520of%2520this%2520misalignment%252C%2520we%2520introduce%2520TokDrift%252C%2520a%2520framework%250Athat%2520applies%2520semantic-preserving%2520rewrite%2520rules%2520to%2520create%2520code%2520variants%250Adiffering%2520only%2520in%2520tokenization.%2520Across%2520nine%2520code%2520LLMs%252C%2520including%2520large%2520ones%250Awith%2520over%252030B%2520parameters%252C%2520even%2520minor%2520formatting%2520changes%2520can%2520cause%2520substantial%250Ashifts%2520in%2520model%2520behavior.%2520Layer-wise%2520analysis%2520shows%2520that%2520the%2520issue%2520originates%250Ain%2520early%2520embeddings%252C%2520where%2520subword%2520segmentation%2520fails%2520to%2520capture%2520grammar%2520token%250Aboundaries.%2520Our%2520findings%2520identify%2520misaligned%2520tokenization%2520as%2520a%2520hidden%2520obstacle%250Ato%2520reliable%2520code%2520understanding%2520and%2520generation%252C%2520highlighting%2520the%2520need%2520for%250Agrammar-aware%2520tokenization%2520for%2520future%2520code%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokDrift%3A%20When%20LLM%20Speaks%20in%20Subwords%20but%20Code%20Speaks%20in%20Grammar&entry.906535625=Yinxi%20Li%20and%20Yuntian%20Deng%20and%20Pengyu%20Nie&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20for%20code%20rely%20on%20subword%20tokenizers%2C%20such%20as%0Abyte-pair%20encoding%20%28BPE%29%2C%20learned%20from%20mixed%20natural%20language%20text%20and%0Aprogramming%20language%20code%20but%20driven%20by%20statistics%20rather%20than%20grammar.%20As%20a%0Aresult%2C%20semantically%20identical%20code%20snippets%20can%20be%20tokenized%20differently%0Adepending%20on%20superficial%20factors%20such%20as%20whitespace%20or%20identifier%20naming.%20To%0Ameasure%20the%20impact%20of%20this%20misalignment%2C%20we%20introduce%20TokDrift%2C%20a%20framework%0Athat%20applies%20semantic-preserving%20rewrite%20rules%20to%20create%20code%20variants%0Adiffering%20only%20in%20tokenization.%20Across%20nine%20code%20LLMs%2C%20including%20large%20ones%0Awith%20over%2030B%20parameters%2C%20even%20minor%20formatting%20changes%20can%20cause%20substantial%0Ashifts%20in%20model%20behavior.%20Layer-wise%20analysis%20shows%20that%20the%20issue%20originates%0Ain%20early%20embeddings%2C%20where%20subword%20segmentation%20fails%20to%20capture%20grammar%20token%0Aboundaries.%20Our%20findings%20identify%20misaligned%20tokenization%20as%20a%20hidden%20obstacle%0Ato%20reliable%20code%20understanding%20and%20generation%2C%20highlighting%20the%20need%20for%0Agrammar-aware%20tokenization%20for%20future%20code%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14972v1&entry.124074799=Read"},
{"title": "Adapting Self-Supervised Representations as a Latent Space for Efficient\n  Generation", "author": "Ming Gui and Johannes Schusterbauer and Timy Phan and Felix Krause and Josh Susskind and Miguel Angel Bautista and Bj\u00f6rn Ommer", "abstract": "  We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.\n", "link": "http://arxiv.org/abs/2510.14630v1", "date": "2025-10-16", "relevancy": 2.3912, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6265}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.578}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Self-Supervised%20Representations%20as%20a%20Latent%20Space%20for%20Efficient%0A%20%20Generation&body=Title%3A%20Adapting%20Self-Supervised%20Representations%20as%20a%20Latent%20Space%20for%20Efficient%0A%20%20Generation%0AAuthor%3A%20Ming%20Gui%20and%20Johannes%20Schusterbauer%20and%20Timy%20Phan%20and%20Felix%20Krause%20and%20Josh%20Susskind%20and%20Miguel%20Angel%20Bautista%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20We%20introduce%20Representation%20Tokenizer%20%28RepTok%29%2C%20a%20generative%20modeling%0Aframework%20that%20represents%20an%20image%20using%20a%20single%20continuous%20latent%20token%0Aobtained%20from%20self-supervised%20vision%20transformers.%20Building%20on%20a%20pre-trained%0ASSL%20encoder%2C%20we%20fine-tune%20only%20the%20semantic%20token%20embedding%20and%20pair%20it%20with%20a%0Agenerative%20decoder%20trained%20jointly%20using%20a%20standard%20flow%20matching%20objective.%0AThis%20adaptation%20enriches%20the%20token%20with%20low-level%2C%20reconstruction-relevant%0Adetails%2C%20enabling%20faithful%20image%20reconstruction.%20To%20preserve%20the%20favorable%0Ageometry%20of%20the%20original%20SSL%20space%2C%20we%20add%20a%20cosine-similarity%20loss%20that%0Aregularizes%20the%20adapted%20token%2C%20ensuring%20the%20latent%20space%20remains%20smooth%20and%0Asuitable%20for%20generation.%20Our%20single-token%20formulation%20resolves%20spatial%0Aredundancies%20of%202D%20latent%20spaces%20and%20significantly%20reduces%20training%20costs.%0ADespite%20its%20simplicity%20and%20efficiency%2C%20RepTok%20achieves%20competitive%20results%20on%0Aclass-conditional%20ImageNet%20generation%20and%20naturally%20extends%20to%20text-to-image%0Asynthesis%2C%20reaching%20competitive%20zero-shot%20performance%20on%20MS-COCO%20under%0Aextremely%20limited%20training%20budgets.%20Our%20findings%20highlight%20the%20potential%20of%0Afine-tuned%20SSL%20representations%20as%20compact%20and%20effective%20latent%20spaces%20for%0Aefficient%20generative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Self-Supervised%2520Representations%2520as%2520a%2520Latent%2520Space%2520for%2520Efficient%250A%2520%2520Generation%26entry.906535625%3DMing%2520Gui%2520and%2520Johannes%2520Schusterbauer%2520and%2520Timy%2520Phan%2520and%2520Felix%2520Krause%2520and%2520Josh%2520Susskind%2520and%2520Miguel%2520Angel%2520Bautista%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520We%2520introduce%2520Representation%2520Tokenizer%2520%2528RepTok%2529%252C%2520a%2520generative%2520modeling%250Aframework%2520that%2520represents%2520an%2520image%2520using%2520a%2520single%2520continuous%2520latent%2520token%250Aobtained%2520from%2520self-supervised%2520vision%2520transformers.%2520Building%2520on%2520a%2520pre-trained%250ASSL%2520encoder%252C%2520we%2520fine-tune%2520only%2520the%2520semantic%2520token%2520embedding%2520and%2520pair%2520it%2520with%2520a%250Agenerative%2520decoder%2520trained%2520jointly%2520using%2520a%2520standard%2520flow%2520matching%2520objective.%250AThis%2520adaptation%2520enriches%2520the%2520token%2520with%2520low-level%252C%2520reconstruction-relevant%250Adetails%252C%2520enabling%2520faithful%2520image%2520reconstruction.%2520To%2520preserve%2520the%2520favorable%250Ageometry%2520of%2520the%2520original%2520SSL%2520space%252C%2520we%2520add%2520a%2520cosine-similarity%2520loss%2520that%250Aregularizes%2520the%2520adapted%2520token%252C%2520ensuring%2520the%2520latent%2520space%2520remains%2520smooth%2520and%250Asuitable%2520for%2520generation.%2520Our%2520single-token%2520formulation%2520resolves%2520spatial%250Aredundancies%2520of%25202D%2520latent%2520spaces%2520and%2520significantly%2520reduces%2520training%2520costs.%250ADespite%2520its%2520simplicity%2520and%2520efficiency%252C%2520RepTok%2520achieves%2520competitive%2520results%2520on%250Aclass-conditional%2520ImageNet%2520generation%2520and%2520naturally%2520extends%2520to%2520text-to-image%250Asynthesis%252C%2520reaching%2520competitive%2520zero-shot%2520performance%2520on%2520MS-COCO%2520under%250Aextremely%2520limited%2520training%2520budgets.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%250Afine-tuned%2520SSL%2520representations%2520as%2520compact%2520and%2520effective%2520latent%2520spaces%2520for%250Aefficient%2520generative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Self-Supervised%20Representations%20as%20a%20Latent%20Space%20for%20Efficient%0A%20%20Generation&entry.906535625=Ming%20Gui%20and%20Johannes%20Schusterbauer%20and%20Timy%20Phan%20and%20Felix%20Krause%20and%20Josh%20Susskind%20and%20Miguel%20Angel%20Bautista%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20We%20introduce%20Representation%20Tokenizer%20%28RepTok%29%2C%20a%20generative%20modeling%0Aframework%20that%20represents%20an%20image%20using%20a%20single%20continuous%20latent%20token%0Aobtained%20from%20self-supervised%20vision%20transformers.%20Building%20on%20a%20pre-trained%0ASSL%20encoder%2C%20we%20fine-tune%20only%20the%20semantic%20token%20embedding%20and%20pair%20it%20with%20a%0Agenerative%20decoder%20trained%20jointly%20using%20a%20standard%20flow%20matching%20objective.%0AThis%20adaptation%20enriches%20the%20token%20with%20low-level%2C%20reconstruction-relevant%0Adetails%2C%20enabling%20faithful%20image%20reconstruction.%20To%20preserve%20the%20favorable%0Ageometry%20of%20the%20original%20SSL%20space%2C%20we%20add%20a%20cosine-similarity%20loss%20that%0Aregularizes%20the%20adapted%20token%2C%20ensuring%20the%20latent%20space%20remains%20smooth%20and%0Asuitable%20for%20generation.%20Our%20single-token%20formulation%20resolves%20spatial%0Aredundancies%20of%202D%20latent%20spaces%20and%20significantly%20reduces%20training%20costs.%0ADespite%20its%20simplicity%20and%20efficiency%2C%20RepTok%20achieves%20competitive%20results%20on%0Aclass-conditional%20ImageNet%20generation%20and%20naturally%20extends%20to%20text-to-image%0Asynthesis%2C%20reaching%20competitive%20zero-shot%20performance%20on%20MS-COCO%20under%0Aextremely%20limited%20training%20budgets.%20Our%20findings%20highlight%20the%20potential%20of%0Afine-tuned%20SSL%20representations%20as%20compact%20and%20effective%20latent%20spaces%20for%0Aefficient%20generative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14630v1&entry.124074799=Read"},
{"title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with\n  Multi-Scale Reference Attention", "author": "Keli Liu and Zhendong Wang and Wengang Zhou and Shaodong Xu and Ruixiao Dong and Houqiang Li", "abstract": "  Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.\n", "link": "http://arxiv.org/abs/2510.14882v1", "date": "2025-10-16", "relevancy": 2.3911, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.649}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaleWeaver%3A%20Weaving%20Efficient%20Controllable%20T2I%20Generation%20with%0A%20%20Multi-Scale%20Reference%20Attention&body=Title%3A%20ScaleWeaver%3A%20Weaving%20Efficient%20Controllable%20T2I%20Generation%20with%0A%20%20Multi-Scale%20Reference%20Attention%0AAuthor%3A%20Keli%20Liu%20and%20Zhendong%20Wang%20and%20Wengang%20Zhou%20and%20Shaodong%20Xu%20and%20Ruixiao%20Dong%20and%20Houqiang%20Li%0AAbstract%3A%20%20%20Text-to-image%20generation%20with%20visual%20autoregressive~%28VAR%29%20models%20has%20recently%0Aachieved%20impressive%20advances%20in%20generation%20fidelity%20and%20inference%20efficiency.%0AWhile%20control%20mechanisms%20have%20been%20explored%20for%20diffusion%20models%2C%20enabling%0Aprecise%20and%20flexible%20control%20within%20VAR%20paradigm%20remains%20underexplored.%20To%0Abridge%20this%20critical%20gap%2C%20in%20this%20paper%2C%20we%20introduce%20ScaleWeaver%2C%20a%20novel%0Aframework%20designed%20to%20achieve%20high-fidelity%2C%20controllable%20generation%20upon%0Aadvanced%20VAR%20models%20through%20parameter-efficient%20fine-tuning.%20The%20core%20module%20in%0AScaleWeaver%20is%20the%20improved%20MMDiT%20block%20with%20the%20proposed%20Reference%20Attention%0Amodule%2C%20which%20efficiently%20and%20effectively%20incorporates%20conditional%20information.%0ADifferent%20from%20MM%20Attention%2C%20the%20proposed%20Reference%20Attention%20module%20discards%0Athe%20unnecessary%20attention%20from%20image%24%5Crightarrow%24condition%2C%20reducing%0Acomputational%20cost%20while%20stabilizing%20control%20injection.%20Besides%2C%20it%0Astrategically%20emphasizes%20parameter%20reuse%2C%20leveraging%20the%20capability%20of%20the%20VAR%0Abackbone%20itself%20with%20a%20few%20introduced%20parameters%20to%20process%20control%0Ainformation%2C%20and%20equipping%20a%20zero-initialized%20linear%20projection%20to%20ensure%20that%0Acontrol%20signals%20are%20incorporated%20effectively%20without%20disrupting%20the%20generative%0Acapability%20of%20the%20base%20model.%20Extensive%20experiments%20show%20that%20ScaleWeaver%0Adelivers%20high-quality%20generation%20and%20precise%20control%20while%20attaining%20superior%0Aefficiency%20over%20diffusion-based%20methods%2C%20making%20ScaleWeaver%20a%20practical%20and%0Aeffective%20solution%20for%20controllable%20text-to-image%20generation%20within%20the%20visual%0Aautoregressive%20paradigm.%20Code%20and%20models%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaleWeaver%253A%2520Weaving%2520Efficient%2520Controllable%2520T2I%2520Generation%2520with%250A%2520%2520Multi-Scale%2520Reference%2520Attention%26entry.906535625%3DKeli%2520Liu%2520and%2520Zhendong%2520Wang%2520and%2520Wengang%2520Zhou%2520and%2520Shaodong%2520Xu%2520and%2520Ruixiao%2520Dong%2520and%2520Houqiang%2520Li%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520with%2520visual%2520autoregressive~%2528VAR%2529%2520models%2520has%2520recently%250Aachieved%2520impressive%2520advances%2520in%2520generation%2520fidelity%2520and%2520inference%2520efficiency.%250AWhile%2520control%2520mechanisms%2520have%2520been%2520explored%2520for%2520diffusion%2520models%252C%2520enabling%250Aprecise%2520and%2520flexible%2520control%2520within%2520VAR%2520paradigm%2520remains%2520underexplored.%2520To%250Abridge%2520this%2520critical%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520ScaleWeaver%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520achieve%2520high-fidelity%252C%2520controllable%2520generation%2520upon%250Aadvanced%2520VAR%2520models%2520through%2520parameter-efficient%2520fine-tuning.%2520The%2520core%2520module%2520in%250AScaleWeaver%2520is%2520the%2520improved%2520MMDiT%2520block%2520with%2520the%2520proposed%2520Reference%2520Attention%250Amodule%252C%2520which%2520efficiently%2520and%2520effectively%2520incorporates%2520conditional%2520information.%250ADifferent%2520from%2520MM%2520Attention%252C%2520the%2520proposed%2520Reference%2520Attention%2520module%2520discards%250Athe%2520unnecessary%2520attention%2520from%2520image%2524%255Crightarrow%2524condition%252C%2520reducing%250Acomputational%2520cost%2520while%2520stabilizing%2520control%2520injection.%2520Besides%252C%2520it%250Astrategically%2520emphasizes%2520parameter%2520reuse%252C%2520leveraging%2520the%2520capability%2520of%2520the%2520VAR%250Abackbone%2520itself%2520with%2520a%2520few%2520introduced%2520parameters%2520to%2520process%2520control%250Ainformation%252C%2520and%2520equipping%2520a%2520zero-initialized%2520linear%2520projection%2520to%2520ensure%2520that%250Acontrol%2520signals%2520are%2520incorporated%2520effectively%2520without%2520disrupting%2520the%2520generative%250Acapability%2520of%2520the%2520base%2520model.%2520Extensive%2520experiments%2520show%2520that%2520ScaleWeaver%250Adelivers%2520high-quality%2520generation%2520and%2520precise%2520control%2520while%2520attaining%2520superior%250Aefficiency%2520over%2520diffusion-based%2520methods%252C%2520making%2520ScaleWeaver%2520a%2520practical%2520and%250Aeffective%2520solution%2520for%2520controllable%2520text-to-image%2520generation%2520within%2520the%2520visual%250Aautoregressive%2520paradigm.%2520Code%2520and%2520models%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaleWeaver%3A%20Weaving%20Efficient%20Controllable%20T2I%20Generation%20with%0A%20%20Multi-Scale%20Reference%20Attention&entry.906535625=Keli%20Liu%20and%20Zhendong%20Wang%20and%20Wengang%20Zhou%20and%20Shaodong%20Xu%20and%20Ruixiao%20Dong%20and%20Houqiang%20Li&entry.1292438233=%20%20Text-to-image%20generation%20with%20visual%20autoregressive~%28VAR%29%20models%20has%20recently%0Aachieved%20impressive%20advances%20in%20generation%20fidelity%20and%20inference%20efficiency.%0AWhile%20control%20mechanisms%20have%20been%20explored%20for%20diffusion%20models%2C%20enabling%0Aprecise%20and%20flexible%20control%20within%20VAR%20paradigm%20remains%20underexplored.%20To%0Abridge%20this%20critical%20gap%2C%20in%20this%20paper%2C%20we%20introduce%20ScaleWeaver%2C%20a%20novel%0Aframework%20designed%20to%20achieve%20high-fidelity%2C%20controllable%20generation%20upon%0Aadvanced%20VAR%20models%20through%20parameter-efficient%20fine-tuning.%20The%20core%20module%20in%0AScaleWeaver%20is%20the%20improved%20MMDiT%20block%20with%20the%20proposed%20Reference%20Attention%0Amodule%2C%20which%20efficiently%20and%20effectively%20incorporates%20conditional%20information.%0ADifferent%20from%20MM%20Attention%2C%20the%20proposed%20Reference%20Attention%20module%20discards%0Athe%20unnecessary%20attention%20from%20image%24%5Crightarrow%24condition%2C%20reducing%0Acomputational%20cost%20while%20stabilizing%20control%20injection.%20Besides%2C%20it%0Astrategically%20emphasizes%20parameter%20reuse%2C%20leveraging%20the%20capability%20of%20the%20VAR%0Abackbone%20itself%20with%20a%20few%20introduced%20parameters%20to%20process%20control%0Ainformation%2C%20and%20equipping%20a%20zero-initialized%20linear%20projection%20to%20ensure%20that%0Acontrol%20signals%20are%20incorporated%20effectively%20without%20disrupting%20the%20generative%0Acapability%20of%20the%20base%20model.%20Extensive%20experiments%20show%20that%20ScaleWeaver%0Adelivers%20high-quality%20generation%20and%20precise%20control%20while%20attaining%20superior%0Aefficiency%20over%20diffusion-based%20methods%2C%20making%20ScaleWeaver%20a%20practical%20and%0Aeffective%20solution%20for%20controllable%20text-to-image%20generation%20within%20the%20visual%0Aautoregressive%20paradigm.%20Code%20and%20models%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14882v1&entry.124074799=Read"},
{"title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "author": "Yufei Zhu and Shih-Min Yang and Andrey Rudenko and Tomasz P. Kucner and Achim J. Lilienthal and Martin Magnusson", "abstract": "  Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.\n", "link": "http://arxiv.org/abs/2510.14827v1", "date": "2025-10-16", "relevancy": 2.3908, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6269}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6129}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Implicit%20Flow%20Fields%20for%20Spatio-Temporal%20Motion%20Mapping&body=Title%3A%20Neural%20Implicit%20Flow%20Fields%20for%20Spatio-Temporal%20Motion%20Mapping%0AAuthor%3A%20Yufei%20Zhu%20and%20Shih-Min%20Yang%20and%20Andrey%20Rudenko%20and%20Tomasz%20P.%20Kucner%20and%20Achim%20J.%20Lilienthal%20and%20Martin%20Magnusson%0AAbstract%3A%20%20%20Safe%20and%20efficient%20robot%20operation%20in%20complex%20human%20environments%20can%20benefit%0Afrom%20good%20models%20of%20site-specific%20motion%20patterns.%20Maps%20of%20Dynamics%20%28MoDs%29%0Aprovide%20such%20models%20by%20encoding%20statistical%20motion%20patterns%20in%20a%20map%2C%20but%0Aexisting%20representations%20use%20discrete%20spatial%20sampling%20and%20typically%20require%0Acostly%20offline%20construction.%20We%20propose%20a%20continuous%20spatio-temporal%20MoD%0Arepresentation%20based%20on%20implicit%20neural%20functions%20that%20directly%20map%20coordinates%0Ato%20the%20parameters%20of%20a%20Semi-Wrapped%20Gaussian%20Mixture%20Model.%20This%20removes%20the%0Aneed%20for%20discretization%20and%20imputation%20for%20unevenly%20sampled%20regions%2C%20enabling%0Asmooth%20generalization%20across%20both%20space%20and%20time.%20Evaluated%20on%20a%20large%20public%0Adataset%20with%20long-term%20real-world%20people%20tracking%20data%2C%20our%20method%20achieves%0Abetter%20accuracy%20of%20motion%20representation%20and%20smoother%20velocity%20distributions%20in%0Asparse%20regions%20while%20still%20being%20computationally%20efficient%2C%20compared%20to%0Aavailable%20baselines.%20The%20proposed%20approach%20demonstrates%20a%20powerful%20and%0Aefficient%20way%20of%20modeling%20complex%20human%20motion%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Implicit%2520Flow%2520Fields%2520for%2520Spatio-Temporal%2520Motion%2520Mapping%26entry.906535625%3DYufei%2520Zhu%2520and%2520Shih-Min%2520Yang%2520and%2520Andrey%2520Rudenko%2520and%2520Tomasz%2520P.%2520Kucner%2520and%2520Achim%2520J.%2520Lilienthal%2520and%2520Martin%2520Magnusson%26entry.1292438233%3D%2520%2520Safe%2520and%2520efficient%2520robot%2520operation%2520in%2520complex%2520human%2520environments%2520can%2520benefit%250Afrom%2520good%2520models%2520of%2520site-specific%2520motion%2520patterns.%2520Maps%2520of%2520Dynamics%2520%2528MoDs%2529%250Aprovide%2520such%2520models%2520by%2520encoding%2520statistical%2520motion%2520patterns%2520in%2520a%2520map%252C%2520but%250Aexisting%2520representations%2520use%2520discrete%2520spatial%2520sampling%2520and%2520typically%2520require%250Acostly%2520offline%2520construction.%2520We%2520propose%2520a%2520continuous%2520spatio-temporal%2520MoD%250Arepresentation%2520based%2520on%2520implicit%2520neural%2520functions%2520that%2520directly%2520map%2520coordinates%250Ato%2520the%2520parameters%2520of%2520a%2520Semi-Wrapped%2520Gaussian%2520Mixture%2520Model.%2520This%2520removes%2520the%250Aneed%2520for%2520discretization%2520and%2520imputation%2520for%2520unevenly%2520sampled%2520regions%252C%2520enabling%250Asmooth%2520generalization%2520across%2520both%2520space%2520and%2520time.%2520Evaluated%2520on%2520a%2520large%2520public%250Adataset%2520with%2520long-term%2520real-world%2520people%2520tracking%2520data%252C%2520our%2520method%2520achieves%250Abetter%2520accuracy%2520of%2520motion%2520representation%2520and%2520smoother%2520velocity%2520distributions%2520in%250Asparse%2520regions%2520while%2520still%2520being%2520computationally%2520efficient%252C%2520compared%2520to%250Aavailable%2520baselines.%2520The%2520proposed%2520approach%2520demonstrates%2520a%2520powerful%2520and%250Aefficient%2520way%2520of%2520modeling%2520complex%2520human%2520motion%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Implicit%20Flow%20Fields%20for%20Spatio-Temporal%20Motion%20Mapping&entry.906535625=Yufei%20Zhu%20and%20Shih-Min%20Yang%20and%20Andrey%20Rudenko%20and%20Tomasz%20P.%20Kucner%20and%20Achim%20J.%20Lilienthal%20and%20Martin%20Magnusson&entry.1292438233=%20%20Safe%20and%20efficient%20robot%20operation%20in%20complex%20human%20environments%20can%20benefit%0Afrom%20good%20models%20of%20site-specific%20motion%20patterns.%20Maps%20of%20Dynamics%20%28MoDs%29%0Aprovide%20such%20models%20by%20encoding%20statistical%20motion%20patterns%20in%20a%20map%2C%20but%0Aexisting%20representations%20use%20discrete%20spatial%20sampling%20and%20typically%20require%0Acostly%20offline%20construction.%20We%20propose%20a%20continuous%20spatio-temporal%20MoD%0Arepresentation%20based%20on%20implicit%20neural%20functions%20that%20directly%20map%20coordinates%0Ato%20the%20parameters%20of%20a%20Semi-Wrapped%20Gaussian%20Mixture%20Model.%20This%20removes%20the%0Aneed%20for%20discretization%20and%20imputation%20for%20unevenly%20sampled%20regions%2C%20enabling%0Asmooth%20generalization%20across%20both%20space%20and%20time.%20Evaluated%20on%20a%20large%20public%0Adataset%20with%20long-term%20real-world%20people%20tracking%20data%2C%20our%20method%20achieves%0Abetter%20accuracy%20of%20motion%20representation%20and%20smoother%20velocity%20distributions%20in%0Asparse%20regions%20while%20still%20being%20computationally%20efficient%2C%20compared%20to%0Aavailable%20baselines.%20The%20proposed%20approach%20demonstrates%20a%20powerful%20and%0Aefficient%20way%20of%20modeling%20complex%20human%20motion%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14827v1&entry.124074799=Read"},
{"title": "Terra: Explorable Native 3D World Model with Point Latents", "author": "Yuanhui Huang and Weiliang Chen and Wenzhao Zheng and Xin Tao and Pengfei Wan and Jie Zhou and Jiwen Lu", "abstract": "  World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.\n", "link": "http://arxiv.org/abs/2510.14977v1", "date": "2025-10-16", "relevancy": 2.3892, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5843}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Terra%3A%20Explorable%20Native%203D%20World%20Model%20with%20Point%20Latents&body=Title%3A%20Terra%3A%20Explorable%20Native%203D%20World%20Model%20with%20Point%20Latents%0AAuthor%3A%20Yuanhui%20Huang%20and%20Weiliang%20Chen%20and%20Wenzhao%20Zheng%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20World%20models%20have%20garnered%20increasing%20attention%20for%20comprehensive%20modeling%20of%0Athe%20real%20world.%20However%2C%20most%20existing%20methods%20still%20rely%20on%20pixel-aligned%0Arepresentations%20as%20the%20basis%20for%20world%20evolution%2C%20neglecting%20the%20inherent%203D%0Anature%20of%20the%20physical%20world.%20This%20could%20undermine%20the%203D%20consistency%20and%0Adiminish%20the%20modeling%20efficiency%20of%20world%20models.%20In%20this%20paper%2C%20we%20present%0ATerra%2C%20a%20native%203D%20world%20model%20that%20represents%20and%20generates%20explorable%0Aenvironments%20in%20an%20intrinsic%203D%20latent%20space.%20Specifically%2C%20we%20propose%20a%20novel%0Apoint-to-Gaussian%20variational%20autoencoder%20%28P2G-VAE%29%20that%20encodes%203D%20inputs%20into%0Aa%20latent%20point%20representation%2C%20which%20is%20subsequently%20decoded%20as%203D%20Gaussian%0Aprimitives%20to%20jointly%20model%20geometry%20and%20appearance.%20We%20then%20introduce%20a%20sparse%0Apoint%20flow%20matching%20network%20%28SPFlow%29%20for%20generating%20the%20latent%20point%0Arepresentation%2C%20which%20simultaneously%20denoises%20the%20positions%20and%20features%20of%20the%0Apoint%20latents.%20Our%20Terra%20enables%20exact%20multi-view%20consistency%20with%20native%203D%0Arepresentation%20and%20architecture%2C%20and%20supports%20flexible%20rendering%20from%20any%0Aviewpoint%20with%20only%20a%20single%20generation%20process.%20Furthermore%2C%20Terra%20achieves%0Aexplorable%20world%20modeling%20through%20progressive%20generation%20in%20the%20point%20latent%0Aspace.%20We%20conduct%20extensive%20experiments%20on%20the%20challenging%20indoor%20scenes%20from%0AScanNet%20v2.%20Terra%20achieves%20state-of-the-art%20performance%20in%20both%20reconstruction%0Aand%20generation%20with%20high%203D%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerra%253A%2520Explorable%2520Native%25203D%2520World%2520Model%2520with%2520Point%2520Latents%26entry.906535625%3DYuanhui%2520Huang%2520and%2520Weiliang%2520Chen%2520and%2520Wenzhao%2520Zheng%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520World%2520models%2520have%2520garnered%2520increasing%2520attention%2520for%2520comprehensive%2520modeling%2520of%250Athe%2520real%2520world.%2520However%252C%2520most%2520existing%2520methods%2520still%2520rely%2520on%2520pixel-aligned%250Arepresentations%2520as%2520the%2520basis%2520for%2520world%2520evolution%252C%2520neglecting%2520the%2520inherent%25203D%250Anature%2520of%2520the%2520physical%2520world.%2520This%2520could%2520undermine%2520the%25203D%2520consistency%2520and%250Adiminish%2520the%2520modeling%2520efficiency%2520of%2520world%2520models.%2520In%2520this%2520paper%252C%2520we%2520present%250ATerra%252C%2520a%2520native%25203D%2520world%2520model%2520that%2520represents%2520and%2520generates%2520explorable%250Aenvironments%2520in%2520an%2520intrinsic%25203D%2520latent%2520space.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%250Apoint-to-Gaussian%2520variational%2520autoencoder%2520%2528P2G-VAE%2529%2520that%2520encodes%25203D%2520inputs%2520into%250Aa%2520latent%2520point%2520representation%252C%2520which%2520is%2520subsequently%2520decoded%2520as%25203D%2520Gaussian%250Aprimitives%2520to%2520jointly%2520model%2520geometry%2520and%2520appearance.%2520We%2520then%2520introduce%2520a%2520sparse%250Apoint%2520flow%2520matching%2520network%2520%2528SPFlow%2529%2520for%2520generating%2520the%2520latent%2520point%250Arepresentation%252C%2520which%2520simultaneously%2520denoises%2520the%2520positions%2520and%2520features%2520of%2520the%250Apoint%2520latents.%2520Our%2520Terra%2520enables%2520exact%2520multi-view%2520consistency%2520with%2520native%25203D%250Arepresentation%2520and%2520architecture%252C%2520and%2520supports%2520flexible%2520rendering%2520from%2520any%250Aviewpoint%2520with%2520only%2520a%2520single%2520generation%2520process.%2520Furthermore%252C%2520Terra%2520achieves%250Aexplorable%2520world%2520modeling%2520through%2520progressive%2520generation%2520in%2520the%2520point%2520latent%250Aspace.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520challenging%2520indoor%2520scenes%2520from%250AScanNet%2520v2.%2520Terra%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520reconstruction%250Aand%2520generation%2520with%2520high%25203D%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Terra%3A%20Explorable%20Native%203D%20World%20Model%20with%20Point%20Latents&entry.906535625=Yuanhui%20Huang%20and%20Weiliang%20Chen%20and%20Wenzhao%20Zheng%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20World%20models%20have%20garnered%20increasing%20attention%20for%20comprehensive%20modeling%20of%0Athe%20real%20world.%20However%2C%20most%20existing%20methods%20still%20rely%20on%20pixel-aligned%0Arepresentations%20as%20the%20basis%20for%20world%20evolution%2C%20neglecting%20the%20inherent%203D%0Anature%20of%20the%20physical%20world.%20This%20could%20undermine%20the%203D%20consistency%20and%0Adiminish%20the%20modeling%20efficiency%20of%20world%20models.%20In%20this%20paper%2C%20we%20present%0ATerra%2C%20a%20native%203D%20world%20model%20that%20represents%20and%20generates%20explorable%0Aenvironments%20in%20an%20intrinsic%203D%20latent%20space.%20Specifically%2C%20we%20propose%20a%20novel%0Apoint-to-Gaussian%20variational%20autoencoder%20%28P2G-VAE%29%20that%20encodes%203D%20inputs%20into%0Aa%20latent%20point%20representation%2C%20which%20is%20subsequently%20decoded%20as%203D%20Gaussian%0Aprimitives%20to%20jointly%20model%20geometry%20and%20appearance.%20We%20then%20introduce%20a%20sparse%0Apoint%20flow%20matching%20network%20%28SPFlow%29%20for%20generating%20the%20latent%20point%0Arepresentation%2C%20which%20simultaneously%20denoises%20the%20positions%20and%20features%20of%20the%0Apoint%20latents.%20Our%20Terra%20enables%20exact%20multi-view%20consistency%20with%20native%203D%0Arepresentation%20and%20architecture%2C%20and%20supports%20flexible%20rendering%20from%20any%0Aviewpoint%20with%20only%20a%20single%20generation%20process.%20Furthermore%2C%20Terra%20achieves%0Aexplorable%20world%20modeling%20through%20progressive%20generation%20in%20the%20point%20latent%0Aspace.%20We%20conduct%20extensive%20experiments%20on%20the%20challenging%20indoor%20scenes%20from%0AScanNet%20v2.%20Terra%20achieves%20state-of-the-art%20performance%20in%20both%20reconstruction%0Aand%20generation%20with%20high%203D%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14977v1&entry.124074799=Read"},
{"title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for\n  Vision-Language-Action Models", "author": "Yixuan Li and Yuhui Chen and Mingcai Zhou and Haoran Li", "abstract": "  Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks.\n", "link": "http://arxiv.org/abs/2510.14836v1", "date": "2025-10-16", "relevancy": 2.3866, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QDepth-VLA%3A%20Quantized%20Depth%20Prediction%20as%20Auxiliary%20Supervision%20for%0A%20%20Vision-Language-Action%20Models&body=Title%3A%20QDepth-VLA%3A%20Quantized%20Depth%20Prediction%20as%20Auxiliary%20Supervision%20for%0A%20%20Vision-Language-Action%20Models%0AAuthor%3A%20Yixuan%20Li%20and%20Yuhui%20Chen%20and%20Mingcai%20Zhou%20and%20Haoran%20Li%0AAbstract%3A%20%20%20Spatial%20perception%20and%20reasoning%20are%20crucial%20for%20Vision-Language-Action%20%28VLA%29%0Amodels%20to%20accomplish%20fine-grained%20manipulation%20tasks.%20However%2C%20existing%0Aapproaches%20often%20lack%20the%20ability%20to%20understand%20and%20reason%20over%20the%20essential%0A3D%20structures%20necessary%20for%20precise%20control.%20To%20address%20this%20limitation%2C%20we%0Apropose%20QDepth-VLA%2C%20a%20general%20framework%20that%20augments%20VLA%20models%20with%20an%0Aauxiliary%20depth%20prediction%20task.%20A%20dedicated%20depth%20expert%20is%20designed%20to%0Apredict%20quantized%20latent%20tokens%20of%20depth%20maps%20obtained%20from%20a%20VQ-VAE%20encoder%2C%0Aenabling%20the%20model%20to%20learn%20depth-aware%20representations%20that%20capture%20critical%0Ageometric%20cues.%20Experimental%20results%20on%20the%20simulation%20benchmarks%20and%0Areal-world%20tasks%20demonstrate%20that%20QDepth-VLA%20yields%20strong%20spatial%20reasoning%0Aand%20competitive%20performance%20on%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQDepth-VLA%253A%2520Quantized%2520Depth%2520Prediction%2520as%2520Auxiliary%2520Supervision%2520for%250A%2520%2520Vision-Language-Action%2520Models%26entry.906535625%3DYixuan%2520Li%2520and%2520Yuhui%2520Chen%2520and%2520Mingcai%2520Zhou%2520and%2520Haoran%2520Li%26entry.1292438233%3D%2520%2520Spatial%2520perception%2520and%2520reasoning%2520are%2520crucial%2520for%2520Vision-Language-Action%2520%2528VLA%2529%250Amodels%2520to%2520accomplish%2520fine-grained%2520manipulation%2520tasks.%2520However%252C%2520existing%250Aapproaches%2520often%2520lack%2520the%2520ability%2520to%2520understand%2520and%2520reason%2520over%2520the%2520essential%250A3D%2520structures%2520necessary%2520for%2520precise%2520control.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520QDepth-VLA%252C%2520a%2520general%2520framework%2520that%2520augments%2520VLA%2520models%2520with%2520an%250Aauxiliary%2520depth%2520prediction%2520task.%2520A%2520dedicated%2520depth%2520expert%2520is%2520designed%2520to%250Apredict%2520quantized%2520latent%2520tokens%2520of%2520depth%2520maps%2520obtained%2520from%2520a%2520VQ-VAE%2520encoder%252C%250Aenabling%2520the%2520model%2520to%2520learn%2520depth-aware%2520representations%2520that%2520capture%2520critical%250Ageometric%2520cues.%2520Experimental%2520results%2520on%2520the%2520simulation%2520benchmarks%2520and%250Areal-world%2520tasks%2520demonstrate%2520that%2520QDepth-VLA%2520yields%2520strong%2520spatial%2520reasoning%250Aand%2520competitive%2520performance%2520on%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QDepth-VLA%3A%20Quantized%20Depth%20Prediction%20as%20Auxiliary%20Supervision%20for%0A%20%20Vision-Language-Action%20Models&entry.906535625=Yixuan%20Li%20and%20Yuhui%20Chen%20and%20Mingcai%20Zhou%20and%20Haoran%20Li&entry.1292438233=%20%20Spatial%20perception%20and%20reasoning%20are%20crucial%20for%20Vision-Language-Action%20%28VLA%29%0Amodels%20to%20accomplish%20fine-grained%20manipulation%20tasks.%20However%2C%20existing%0Aapproaches%20often%20lack%20the%20ability%20to%20understand%20and%20reason%20over%20the%20essential%0A3D%20structures%20necessary%20for%20precise%20control.%20To%20address%20this%20limitation%2C%20we%0Apropose%20QDepth-VLA%2C%20a%20general%20framework%20that%20augments%20VLA%20models%20with%20an%0Aauxiliary%20depth%20prediction%20task.%20A%20dedicated%20depth%20expert%20is%20designed%20to%0Apredict%20quantized%20latent%20tokens%20of%20depth%20maps%20obtained%20from%20a%20VQ-VAE%20encoder%2C%0Aenabling%20the%20model%20to%20learn%20depth-aware%20representations%20that%20capture%20critical%0Ageometric%20cues.%20Experimental%20results%20on%20the%20simulation%20benchmarks%20and%0Areal-world%20tasks%20demonstrate%20that%20QDepth-VLA%20yields%20strong%20spatial%20reasoning%0Aand%20competitive%20performance%20on%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14836v1&entry.124074799=Read"},
{"title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through\n  an Encoder-Decoder Architecture", "author": "Wenfeng Feng and Hongxiang Wang and Jianlong Wang and Xin Zhang and Jingjing Zhao and Yueyue Liang and Xiang Chen and Duokui Han", "abstract": "  In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel\narchitecture designed to mitigate the attention sink phenomenon observed in\nVision Transformer models. Attention sink occurs when an excessive amount of\nattention is allocated to the [CLS] token, distorting the model's ability to\neffectively process image patches. To address this, we introduce a\nlayer-aligned encoder-decoder architecture, where the encoder utilizes\nself-attention to process image patches, while the decoder uses cross-attention\nto focus on the [CLS] token. Unlike traditional encoder-decoder framework,\nwhere the decoder depends solely on high-level encoder representations, EDIT\nallows the decoder to extract information starting from low-level features,\nprogressively refining the representation layer by layer. EDIT is naturally\ninterpretable demonstrated through sequential attention maps, illustrating the\nrefined, layer-by-layer focus on key image features. Experiments on ImageNet-1k\nand ImageNet-21k, along with transfer learning tasks, show that EDIT achieves\nconsistent performance improvements over DeiT3 models. These results highlight\nthe effectiveness of EDIT's design in addressing attention sink and improving\nvisual feature extraction.\n", "link": "http://arxiv.org/abs/2504.06738v2", "date": "2025-10-16", "relevancy": 2.3772, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.613}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDIT%3A%20Enhancing%20Vision%20Transformers%20by%20Mitigating%20Attention%20Sink%20through%0A%20%20an%20Encoder-Decoder%20Architecture&body=Title%3A%20EDIT%3A%20Enhancing%20Vision%20Transformers%20by%20Mitigating%20Attention%20Sink%20through%0A%20%20an%20Encoder-Decoder%20Architecture%0AAuthor%3A%20Wenfeng%20Feng%20and%20Hongxiang%20Wang%20and%20Jianlong%20Wang%20and%20Xin%20Zhang%20and%20Jingjing%20Zhao%20and%20Yueyue%20Liang%20and%20Xiang%20Chen%20and%20Duokui%20Han%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20EDIT%20%28Encoder-Decoder%20Image%20Transformer%29%2C%20a%20novel%0Aarchitecture%20designed%20to%20mitigate%20the%20attention%20sink%20phenomenon%20observed%20in%0AVision%20Transformer%20models.%20Attention%20sink%20occurs%20when%20an%20excessive%20amount%20of%0Aattention%20is%20allocated%20to%20the%20%5BCLS%5D%20token%2C%20distorting%20the%20model%27s%20ability%20to%0Aeffectively%20process%20image%20patches.%20To%20address%20this%2C%20we%20introduce%20a%0Alayer-aligned%20encoder-decoder%20architecture%2C%20where%20the%20encoder%20utilizes%0Aself-attention%20to%20process%20image%20patches%2C%20while%20the%20decoder%20uses%20cross-attention%0Ato%20focus%20on%20the%20%5BCLS%5D%20token.%20Unlike%20traditional%20encoder-decoder%20framework%2C%0Awhere%20the%20decoder%20depends%20solely%20on%20high-level%20encoder%20representations%2C%20EDIT%0Aallows%20the%20decoder%20to%20extract%20information%20starting%20from%20low-level%20features%2C%0Aprogressively%20refining%20the%20representation%20layer%20by%20layer.%20EDIT%20is%20naturally%0Ainterpretable%20demonstrated%20through%20sequential%20attention%20maps%2C%20illustrating%20the%0Arefined%2C%20layer-by-layer%20focus%20on%20key%20image%20features.%20Experiments%20on%20ImageNet-1k%0Aand%20ImageNet-21k%2C%20along%20with%20transfer%20learning%20tasks%2C%20show%20that%20EDIT%20achieves%0Aconsistent%20performance%20improvements%20over%20DeiT3%20models.%20These%20results%20highlight%0Athe%20effectiveness%20of%20EDIT%27s%20design%20in%20addressing%20attention%20sink%20and%20improving%0Avisual%20feature%20extraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06738v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDIT%253A%2520Enhancing%2520Vision%2520Transformers%2520by%2520Mitigating%2520Attention%2520Sink%2520through%250A%2520%2520an%2520Encoder-Decoder%2520Architecture%26entry.906535625%3DWenfeng%2520Feng%2520and%2520Hongxiang%2520Wang%2520and%2520Jianlong%2520Wang%2520and%2520Xin%2520Zhang%2520and%2520Jingjing%2520Zhao%2520and%2520Yueyue%2520Liang%2520and%2520Xiang%2520Chen%2520and%2520Duokui%2520Han%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520EDIT%2520%2528Encoder-Decoder%2520Image%2520Transformer%2529%252C%2520a%2520novel%250Aarchitecture%2520designed%2520to%2520mitigate%2520the%2520attention%2520sink%2520phenomenon%2520observed%2520in%250AVision%2520Transformer%2520models.%2520Attention%2520sink%2520occurs%2520when%2520an%2520excessive%2520amount%2520of%250Aattention%2520is%2520allocated%2520to%2520the%2520%255BCLS%255D%2520token%252C%2520distorting%2520the%2520model%2527s%2520ability%2520to%250Aeffectively%2520process%2520image%2520patches.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Alayer-aligned%2520encoder-decoder%2520architecture%252C%2520where%2520the%2520encoder%2520utilizes%250Aself-attention%2520to%2520process%2520image%2520patches%252C%2520while%2520the%2520decoder%2520uses%2520cross-attention%250Ato%2520focus%2520on%2520the%2520%255BCLS%255D%2520token.%2520Unlike%2520traditional%2520encoder-decoder%2520framework%252C%250Awhere%2520the%2520decoder%2520depends%2520solely%2520on%2520high-level%2520encoder%2520representations%252C%2520EDIT%250Aallows%2520the%2520decoder%2520to%2520extract%2520information%2520starting%2520from%2520low-level%2520features%252C%250Aprogressively%2520refining%2520the%2520representation%2520layer%2520by%2520layer.%2520EDIT%2520is%2520naturally%250Ainterpretable%2520demonstrated%2520through%2520sequential%2520attention%2520maps%252C%2520illustrating%2520the%250Arefined%252C%2520layer-by-layer%2520focus%2520on%2520key%2520image%2520features.%2520Experiments%2520on%2520ImageNet-1k%250Aand%2520ImageNet-21k%252C%2520along%2520with%2520transfer%2520learning%2520tasks%252C%2520show%2520that%2520EDIT%2520achieves%250Aconsistent%2520performance%2520improvements%2520over%2520DeiT3%2520models.%2520These%2520results%2520highlight%250Athe%2520effectiveness%2520of%2520EDIT%2527s%2520design%2520in%2520addressing%2520attention%2520sink%2520and%2520improving%250Avisual%2520feature%2520extraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06738v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDIT%3A%20Enhancing%20Vision%20Transformers%20by%20Mitigating%20Attention%20Sink%20through%0A%20%20an%20Encoder-Decoder%20Architecture&entry.906535625=Wenfeng%20Feng%20and%20Hongxiang%20Wang%20and%20Jianlong%20Wang%20and%20Xin%20Zhang%20and%20Jingjing%20Zhao%20and%20Yueyue%20Liang%20and%20Xiang%20Chen%20and%20Duokui%20Han&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20EDIT%20%28Encoder-Decoder%20Image%20Transformer%29%2C%20a%20novel%0Aarchitecture%20designed%20to%20mitigate%20the%20attention%20sink%20phenomenon%20observed%20in%0AVision%20Transformer%20models.%20Attention%20sink%20occurs%20when%20an%20excessive%20amount%20of%0Aattention%20is%20allocated%20to%20the%20%5BCLS%5D%20token%2C%20distorting%20the%20model%27s%20ability%20to%0Aeffectively%20process%20image%20patches.%20To%20address%20this%2C%20we%20introduce%20a%0Alayer-aligned%20encoder-decoder%20architecture%2C%20where%20the%20encoder%20utilizes%0Aself-attention%20to%20process%20image%20patches%2C%20while%20the%20decoder%20uses%20cross-attention%0Ato%20focus%20on%20the%20%5BCLS%5D%20token.%20Unlike%20traditional%20encoder-decoder%20framework%2C%0Awhere%20the%20decoder%20depends%20solely%20on%20high-level%20encoder%20representations%2C%20EDIT%0Aallows%20the%20decoder%20to%20extract%20information%20starting%20from%20low-level%20features%2C%0Aprogressively%20refining%20the%20representation%20layer%20by%20layer.%20EDIT%20is%20naturally%0Ainterpretable%20demonstrated%20through%20sequential%20attention%20maps%2C%20illustrating%20the%0Arefined%2C%20layer-by-layer%20focus%20on%20key%20image%20features.%20Experiments%20on%20ImageNet-1k%0Aand%20ImageNet-21k%2C%20along%20with%20transfer%20learning%20tasks%2C%20show%20that%20EDIT%20achieves%0Aconsistent%20performance%20improvements%20over%20DeiT3%20models.%20These%20results%20highlight%0Athe%20effectiveness%20of%20EDIT%27s%20design%20in%20addressing%20attention%20sink%20and%20improving%0Avisual%20feature%20extraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06738v2&entry.124074799=Read"},
{"title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,\n  Long Context, and Next Generation Agentic Capabilities", "author": "Gheorghe Comanici and Eric Bieber and Mike Schaekermann and Ice Pasupat and Noveen Sachdeva and Inderjit Dhillon and Marcel Blistein and Ori Ram and Dan Zhang and Evan Rosen and Luke Marris and Sam Petulla and Colin Gaffney and Asaf Aharoni and Nathan Lintz and Tiago Cardal Pais and Henrik Jacobsson and Idan Szpektor and Nan-Jiang Jiang and Krishna Haridasan and Ahmed Omran and Nikunj Saunshi and Dara Bahri and Gaurav Mishra and Eric Chu and Toby Boyd and Brad Hekman and Aaron Parisi and Chaoyi Zhang and Kornraphop Kawintiranon and Tania Bedrax-Weiss and Oliver Wang and Ya Xu and Ollie Purkiss and Uri Mendlovic and Ila\u00ef Deutel and Nam Nguyen and Adam Langley and Flip Korn and Lucia Rossazza and Alexandre Ram\u00e9 and Sagar Waghmare and Helen Miller and Nathan Byrd and Ashrith Sheshan and Raia Hadsell and Sangnie Bhardwaj and Pawel Janus and Tero Rissa and Dan Horgan and Alvin Abdagic and Lior Belenki and James Allingham and Anima Singh and Theo Guidroz and Srivatsan Srinivasan and Herman Schmit and Kristen Chiafullo and Andre Elisseeff and Nilpa Jha and Prateek Kolhar and Leonard Berrada and Frank Ding and Xiance Si and Shrestha Basu Mallick and Franz Och and Sofia Erell and Eric Ni and Tejasi Latkar and Sherry Yang and Petar Sirkovic and Ziqiang Feng and Robert Leland and Rachel Hornung and Gang Wu and Charles Blundell and Hamidreza Alvari and Po-Sen Huang and Cathy Yip and Sanja Deur and Li Liu and Gabriela Surita and Pablo Duque and Dima Damen and Johnson Jia and Arthur Guez and Markus Mircea and Animesh Sinha and Alberto Magni and Pawe\u0142 Stradomski and Tal Marian and Vlado Gali\u0107 and Wenhu Chen and Hisham Husain and Achintya Singhal and Dominik Grewe and Fran\u00e7ois-Xavier Aubet and Shuang Song and Lorenzo Blanco and Leland Rechis and Lewis Ho and Rich Munoz and Kelvin Zheng and Jessica Hamrick and Kevin Mather and Hagai Taitelbaum and Eliza Rutherford and Yun Lei and Kuangyuan Chen and Anand Shukla and Erica Moreira and Eric Doi and Berivan Isik and Nir Shabat and Dominika Rogozi\u0144ska and Kashyap Kolipaka and Jason Chang and Eugen Vu\u0161ak and Srinivasan Venkatachary and Shadi Noghabi and Tarun Bharti and Younghoon Jun and Aleksandr Zaks and Simon Green and Jeshwanth Challagundla and William Wong and Muqthar Mohammad and Dean Hirsch and Yong Cheng and Iftekhar Naim and Lev Proleev and Damien Vincent and Aayush Singh and Maxim Krikun and Dilip Krishnan and Zoubin Ghahramani and Aviel Atias and Rajeev Aggarwal and Christo Kirov and Dimitrios Vytiniotis and Christy Koh and Alexandra Chronopoulou and Pawan Dogra and Vlad-Doru Ion and Gladys Tyen and Jason Lee and Felix Weissenberger and Trevor Strohman and Ashwin Balakrishna and Jack Rae and Marko Velic and Raoul de Liedekerke and Oded Elyada and Wentao Yuan and Canoee Liu and Lior Shani and Sergey Kishchenko and Bea Alessio and Yandong Li and Richard Song and Sam Kwei and Orion Jankowski and Aneesh Pappu and Youhei Namiki and Yenai Ma and Nilesh Tripuraneni and Colin Cherry and Marissa Ikonomidis and Yu-Cheng Ling and Colin Ji and Beka Westberg and Auriel Wright and Da Yu and David Parkinson and Swaroop Ramaswamy and Jerome Connor and Soheil Hassas Yeganeh and Snchit Grover and George Kenwright and Lubo Litchev and Chris Apps and Alex Tomala and Felix Halim and Alex Castro-Ros and Zefei Li and Anudhyan Boral and Pauline Sho and Michal Yarom and Eric Malmi and David Klinghoffer and Rebecca Lin and Alan Ansell and Pradeep Kumar S and Shubin Zhao and Siqi Zuo and Adam Santoro and Heng-Tze Cheng and Solomon Demmessie and Yuchi Liu and Nicole Brichtova and Allie Culp and Nathaniel Braun and Dan Graur and Will Ng and Nikhil Mehta and Aaron Phillips and Patrik Sundberg and Varun Godbole and Fangyu Liu and Yash Katariya and David Rim and Mojtaba Seyedhosseini and Sean Ammirati and Jonas Valfridsson and Mahan Malihi and Timothy Knight and Andeep Toor and Thomas Lampe and Abe Ittycheriah and Lewis Chiang and Chak Yeung and Alexandre Fr\u00e9chette and Jinmeng Rao and Huisheng Wang and Himanshu Srivastava and Richard Zhang and Rocky Rhodes and Ariel Brand and Dean Weesner and Ilya Figotin and Felix Gimeno and Rachana Fellinger and Pierre Marcenac and Jos\u00e9 Leal and Eyal Marcus and Victor Cotruta and Rodrigo Cabrera and Sheryl Luo and Dan Garrette and Vera Axelrod and Sorin Baltateanu and David Barker and Dongkai Chen and Horia Toma and Ben Ingram and Jason Riesa and Chinmay Kulkarni and Yujing Zhang and Hongbin Liu and Chao Wang and Martin Polacek and Will Wu and Kai Hui and Adrian N Reyes and Yi Su and Megan Barnes and Ishaan Malhi and Anfal Siddiqui and Qixuan Feng and Mihai Damaschin and Daniele Pighin and Andreas Steiner and Samuel Yang and Ramya Sree Boppana and Simeon Ivanov and Arun Kandoor and Aditya Shah and Asier Mujika and Da Huang and Christopher A. Choquette-Choo and Mohak Patel and Tianhe Yu and Toni Creswell and  Jerry and  Liu and Catarina Barros and Yasaman Razeghi and Aurko Roy and Phil Culliton and Binbin Xiong and Jiaqi Pan and Thomas Strohmann and Tolly Powell and Babi Seal and Doug DeCarlo and Pranav Shyam and Kaan Katircioglu and Xuezhi Wang and Cassidy Hardin and Immanuel Odisho and Josef Broder and Oscar Chang and Arun Nair and Artem Shtefan and Maura O'Brien and Manu Agarwal and Sahitya Potluri and Siddharth Goyal and Amit Jhindal and Saksham Thakur and Yury Stuken and James Lyon and Kristina Toutanova and Fangxiaoyu Feng and Austin Wu and Ben Horn and Alek Wang and Alex Cullum and Gabe Taubman and Disha Shrivastava and Chongyang Shi and Hamish Tomlinson and Roma Patel and Tao Tu and Ada Maksutaj Oflazer and Francesco Pongetti and Mingyao Yang and Adrien Ali Ta\u00efga and Vincent Perot and Nuo Wang Pierse and Feng Han and Yoel Drori and I\u00f1aki Iturrate and Ayan Chakrabarti and Legg Yeung and Dave Dopson and Yi-ting Chen and Apoorv Kulshreshtha and Tongfei Guo and Philip Pham and Tal Schuster and Junquan Chen and Alex Polozov and Jinwei Xing and Huanjie Zhou and Praneeth Kacham and Doron Kukliansky and Antoine Miech and Sergey Yaroshenko and Ed Chi and Sholto Douglas and Hongliang Fei and Mathieu Blondel and Preethi Myla and Lior Madmoni and Xing Wu and Daniel Keysers and Kristian Kjems and Isabela Albuquerque and Lijun Yu and Joel D'sa and Michelle Plantan and Vlad Ionescu and Jaume Sanchez Elias and Abhirut Gupta and Manish Reddy Vuyyuru and Fred Alcober and Tong Zhou and Kaiyang Ji and Florian Hartmann and Subha Puttagunta and Hugo Song and Ehsan Amid and Anca Stefanoiu and Andrew Lee and Paul Pucciarelli and Emma Wang and Amit Raul and Slav Petrov and Isaac Tian and Valentin Anklin and Nana Nti and Victor Gomes and Max Schumacher and Grace Vesom and Alex Panagopoulos and Konstantinos Bousmalis and Daniel Andor and Josh Jacob and Yuan Zhang and Bill Rosgen and Matija Kecman and Matthew Tung and Alexandra Belias and Noah Goodman and Paul Covington and Brian Wieder and Nikita Saxena and Elnaz Davoodi and Muhuan Huang and Sharath Maddineni and Vincent Roulet and Folawiyo Campbell-Ajala and Pier Giuseppe Sessa and  Xintian and  Wu and Guangda Lai and Paul Collins and Alex Haig and Vytenis Sakenas and Xiaowei Xu and Marissa Giustina and Laurent El Shafey and Pichi Charoenpanit and Shefali Garg and Joshua Ainslie and Boone Severson and Montse Gonzalez Arenas and Shreya Pathak and Sujee Rajayogam and Jie Feng and Michiel Bakker and Sheng Li and Nevan Wichers and Jamie Rogers and Xinyang Geng and Yeqing Li and Rolf Jagerman and Chao Jia and Nadav Olmert and David Sharon and Matthew Mauger and Sandeep Mariserla and Hongxu Ma and Megha Mohabey and Kyuyeun Kim and Alek Andreev and Scott Pollom and Juliette Love and Vihan Jain and Priyanka Agrawal and Yannick Schroecker and Alisa Fortin and Manfred Warmuth and Ji Liu and Andrew Leach and Irina Blok and Ganesh Poomal Girirajan and Roee Aharoni and Benigno Uria and Andrei Sozanschi and Dan Goldberg and Lucian Ionita and Marco Tulio Ribeiro and Martin Zlocha and Vighnesh Birodkar and Sami Lachgar and Liangzhe Yuan and Himadri Choudhury and Matt Ginsberg and Fei Zheng and Gregory Dibb and Emily Graves and Swachhand Lokhande and Gabriel Rasskin and George-Cristian Muraru and Corbin Quick and Sandeep Tata and Pierre Sermanet and Aditya Chawla and Itay Karo and Yan Wang and Susan Zhang and Orgad Keller and Anca Dragan and Guolong Su and Ian Chou and Xi Liu and Yiqing Tao and Shruthi Prabhakara and Marc Wilson and Ruibo Liu and Shibo Wang and Georgie Evans and David Du and Alfonso Casta\u00f1o and Gautam Prasad and Mona El Mahdy and Sebastian Gerlach and Machel Reid and Jarrod Kahn and Amir Zait and Thanumalayan Sankaranarayana Pillai and Thatcher Ulrich and Guanyu Wang and Jan Wassenberg and Efrat Farkash and Kiran Yalasangi and Congchao Wang and Maria Bauza and Simon Bucher and Ting Liu and Jun Yan and Gary Leung and Vikas Sindhwani and Parker Barnes and Avi Singh and Ivan Jurin and Jichuan Chang and Niket Kumar Bhumihar and Sivan Eiger and Gui Citovsky and Ben Withbroe and Zhang Li and Siyang Xue and Niccol\u00f2 Dal Santo and Georgi Stoyanov and Yves Raimond and Steven Zheng and Yilin Gao and V\u00edt List\u00edk and S\u0142awek Kwasiborski and Rachel Saputro and Adnan Ozturel and Ganesh Mallya and Kushal Majmundar and Ross West and Paul Caron and Jinliang Wei and Lluis Castrejon and Sharad Vikram and Deepak Ramachandran and Nikhil Dhawan and Jiho Park and Sara Smoot and George van den Driessche and Yochai Blau and Chase Malik and Wei Liang and Roy Hirsch and Cicero Nogueira dos Santos and Eugene Weinstein and A\u00e4ron van den Oord and Sid Lall and Nicholas FitzGerald and Zixuan Jiang and Xuan Yang and Dale Webster and Ali Elqursh and Aedan Pope and Georges Rotival and David Raposo and Wanzheng Zhu and Jeff Dean and Sami Alabed and Dustin Tran and Arushi Gupta and Zach Gleicher and Jessica Austin and Edouard Rosseel and Megh Umekar and Dipanjan Das and Yinghao Sun and Kai Chen and Karolis Misiunas and Xiang Zhou and Yixian Di and Alyssa Loo and Josh Newlan and Bo Li and Vinay Ramasesh and Ying Xu and Alex Chen and Sudeep Gandhe and Radu Soricut and Nikita Gupta and Shuguang Hu and Seliem El-Sayed and Xavier Garcia and Idan Brusilovsky and Pu-Chin Chen and Andrew Bolt and Lu Huang and Alex Gurney and Zhiying Zhang and Alexander Pritzel and Jarek Wilkiewicz and Bryan Seybold and Bhargav Kanagal Shamanna and Felix Fischer and Josef Dean and Karan Gill and Ross Mcilroy and Abhishek Bhowmick and Jeremy Selier and Antoine Yang and Derek Cheng and Vladimir Magay and Jie Tan and Dhriti Varma and Christian Walder and Tomas Kocisky and Ryo Nakashima and Paul Natsev and Mike Kwong and Ionel Gog and Chiyuan Zhang and Sander Dieleman and Thomas Jimma and Andrey Ryabtsev and Siddhartha Brahma and David Steiner and Dayou Du and Ante \u017du\u017eul and Mislav \u017dani\u0107 and Mukund Raghavachari and Willi Gierke and Zeyu Zheng and Dessie Petrova and Yann Dauphin and Yuchuan Liu and Ido Kessler and Steven Hand and Chris Duvarney and Seokhwan Kim and Hyo Lee and L\u00e9onard Hussenot and Jeffrey Hui and Josh Smith and Deepali Jain and Jiawei Xia and Gaurav Singh Tomar and Keyvan Amiri and Du Phan and Fabian Fuchs and Tobias Weyand and Nenad Tomasev and Alexandra Cordell and Xin Liu and Jonathan Mallinson and Pankaj Joshi and Andy Crawford and Arun Suggala and Steve Chien and Nick Fernando and Mariella Sanchez-Vargas and Duncan Williams and Phil Crone and Xiyang Luo and Igor Karpov and Jyn Shan and Terry Thurk and Robin Strudel and Paul Voigtlaender and Piyush Patil and Tim Dozat and Ali Khodaei and Sahil Singla and Piotr Ambroszczyk and Qiyin Wu and Yifan Chang and Brian Roark and Chaitra Hegde and Tianli Ding and Angelos Filos and Zhongru Wu and Andr\u00e9 Susano Pinto and Shuang Liu and Saarthak Khanna and Aditya Pandey and Siobhan Mcloughlin and Qiujia Li and Sam Haves and Allan Zhou and Elena Buchatskaya and Isabel Leal and Peter de Boursac and Nami Akazawa and Nina Anderson and Terry Chen and Krishna Somandepalli and Chen Liang and Sheela Goenka and Stephanie Winkler and Alexander Grushetsky and Yifan Ding and Jamie Smith and Fan Ye and Jordi Pont-Tuset and Eric Li and Ruichao Li and Tomer Golany and Dawid Wegner and Tao Jiang and Omer Barak and Yuan Shangguan and Eszter V\u00e9rtes and Renee Wong and J\u00f6rg Bornschein and Alex Tudor and Michele Bevilacqua and Tom Schaul and Ankit Singh Rawat and Yang Zhao and Kyriakos Axiotis and Lei Meng and Cory McLean and Jonathan Lai and Jennifer Beattie and Nate Kushman and Yaxin Liu and Blair Kutzman and Fiona Lang and Jingchen Ye and Praneeth Netrapalli and Pushkar Mishra and Myriam Khan and Megha Goel and Rob Willoughby and David Tian and Honglei Zhuang and JD Chen and Zak Tsai and Tasos Kementsietsidis and Arjun Khare and James Keeling and Keyang Xu and Nathan Waters and Florent Altch\u00e9 and Ashok Popat and Bhavishya Mittal and David Saxton and Dalia El Badawy and Michael Mathieu and Zheng Zheng and Hao Zhou and Nishant Ranka and Richard Shin and Qingnan Duan and Tim Salimans and Ioana Mihailescu and Uri Shaham and Ming-Wei Chang and Yannis Assael and Nishanth Dikkala and Martin Izzard and Vincent Cohen-Addad and Cat Graves and Vlad Feinberg and Grace Chung and DJ Strouse and Danny Karmon and Sahand Sharifzadeh and Zoe Ashwood and Khiem Pham and Jon Blanton and Alex Vasiloff and Jarred Barber and Mark Geller and Aurick Zhou and Fedir Zubach and Tzu-Kuo Huang and Lei Zhang and Himanshu Gupta and Matt Young and Julia Proskurnia and Ronny Votel and Valentin Gabeur and Gabriel Barcik and Aditya Tripathi and Hongkun Yu and Geng Yan and Beer Changpinyo and Filip Paveti\u0107 and Amy Coyle and Yasuhisa Fujii and Jorge Gonzalez Mendez and Tianhao Zhou and Harish Rajamani and Blake Hechtman and Eddie Cao and Da-Cheng Juan and Yi-Xuan Tan and Valentin Dalibard and Yilun Du and Natalie Clay and Kaisheng Yao and Wenhao Jia and Dimple Vijaykumar and Yuxiang Zhou and Xinyi Bai and Wei-Chih Hung and Steven Pecht and Georgi Todorov and Nikhil Khadke and Pramod Gupta and Preethi Lahoti and Arnaud Autef and Karthik Duddu and James Lee-Thorp and Alexander Bykovsky and Tautvydas Misiunas and Sebastian Flennerhag and Santhosh Thangaraj and Jed McGiffin and Zack Nado and Markus Kunesch and Andreas Noever and Amir Hertz and Marco Liang and Victor Stone and Evan Palmer and Samira Daruki and Arijit Pramanik and Siim P\u00f5der and Austin Kyker and Mina Khan and Evgeny Sluzhaev and Marvin Ritter and Avraham Ruderman and Wenlei Zhou and Chirag Nagpal and Kiran Vodrahalli and George Necula and Paul Barham and Ellie Pavlick and Jay Hartford and Izhak Shafran and Long Zhao and Maciej Miku\u0142a and Tom Eccles and Hidetoshi Shimokawa and Kanav Garg and Luke Vilnis and Hanwen Chen and Ilia Shumailov and Kuang-Huei Lee and Abdelrahman Abdelhamed and Meiyan Xie and Vered Cohen and Ester Hlavnova and Dan Malkin and Chawin Sitawarin and James Lottes and Pauline Coquinot and Tianli Yu and Sandeep Kumar and Jingwei Zhang and Aroma Mahendru and Zafarali Ahmed and James Martens and Tao Chen and Aviel Boag and Daiyi Peng and Coline Devin and Arseniy Klimovskiy and Mary Phuong and Danny Vainstein and Jin Xie and Bhuvana Ramabhadran and Nathan Howard and Xinxin Yu and Gitartha Goswami and Jingyu Cui and Sam Shleifer and Mario Pinto and Chih-Kuan Yeh and Ming-Hsuan Yang and Sara Javanmardi and Dan Ethier and Chace Lee and Jordi Orbay and Suyog Kotecha and Carla Bromberg and Pete Shaw and James Thornton and Adi Gerzi Rosenthal and Shane Gu and Matt Thomas and Ian Gemp and Aditya Ayyar and Asahi Ushio and Aarush Selvan and Joel Wee and Chenxi Liu and Maryam Majzoubi and Weiren Yu and Jake Abernethy and Tyler Liechty and Renke Pan and Hoang Nguyen and  Qiong and  Hu and Sarah Perrin and Abhinav Arora and Emily Pitler and Weiyi Wang and Kaushik Shivakumar and Flavien Prost and Ben Limonchik and Jing Wang and Yi Gao and Timothee Cour and Shyamal Buch and Huan Gui and Maria Ivanova and Philipp Neubeck and Kelvin Chan and Lucy Kim and Huizhong Chen and Naman Goyal and Da-Woon Chung and Lu Liu and Yao Su and Anastasia Petrushkina and Jiajun Shen and Armand Joulin and Yuanzhong Xu and Stein Xudong Lin and Yana Kulizhskaya and Ciprian Chelba and Shobha Vasudevan and Eli Collins and Vasilisa Bashlovkina and Tony Lu and Doug Fritz and Jongbin Park and Yanqi Zhou and Chen Su and Richard Tanburn and Mikhail Sushkov and Mitchelle Rasquinha and Jinning Li and Jennifer Prendki and Yiming Li and Pallavi LV and Shriya Sharma and Hen Fitoussi and Hui Huang and Andrew Dai and Phuong Dao and Mike Burrows and Henry Prior and Danfeng Qin and Golan Pundak and Lars Lowe Sjoesund and Art Khurshudov and Zhenkai Zhu and Albert Webson and Elizabeth Kemp and Tat Tan and Saurabh Agrawal and Susie Sargsyan and Liqun Cheng and Jim Stephan and Tom Kwiatkowski and David Reid and Arunkumar Byravan and Assaf Hurwitz Michaely and Nicolas Heess and Luowei Zhou and Sonam Goenka and Viral Carpenter and Anselm Levskaya and Bo Wang and Reed Roberts and R\u00e9mi Leblond and Sharat Chikkerur and Stav Ginzburg and Max Chang and Robert Riachi and  Chuqiao and  Xu and Zal\u00e1n Borsos and Michael Pliskin and Julia Pawar and Morgane Lustman and Hannah Kirkwood and Ankit Anand and Aditi Chaudhary and Norbert Kalb and Kieran Milan and Sean Augenstein and Anna Goldie and Laurel Prince and Karthik Raman and Yanhua Sun and Vivian Xia and Aaron Cohen and Zhouyuan Huo and Josh Camp and Seher Ellis and Lukas Zilka and David Vilar Torres and Lisa Patel and Sho Arora and Betty Chan and Jonas Adler and Kareem Ayoub and Jacky Liang and Fayaz Jamil and Jiepu Jiang and Simon Baumgartner and Haitian Sun and Yael Karov and Yaroslav Akulov and Hui Zheng and Irene Cai and Claudio Fantacci and James Rubin and Alex Rav Acha and Mengchao Wang and Nina D'Souza and Rohit Sathyanarayana and Shengyang Dai and Simon Rowe and Andrey Simanovsky and Omer Goldman and Yuheng Kuang and Xiaoyue Pan and Andrew Rosenberg and Tania Rojas-Esponda and Praneet Dutta and Amy Zeng and Irina Jurenka and Greg Farquhar and Yamini Bansal and Shariq Iqbal and Becca Roelofs and Ga-Young Joung and Parker Beak and Changwan Ryu and Ryan Poplin and Yan Wu and Jean-Baptiste Alayrac and Senaka Buthpitiya and Olaf Ronneberger and Caleb Habtegebriel and Wei Li and Paul Cavallaro and Aurora Wei and Guy Bensky and Timo Denk and Harish Ganapathy and Jeff Stanway and Pratik Joshi and Francesco Bertolini and Jessica Lo and Olivia Ma and Zachary Charles and Geta Sampemane and Himanshu Sahni and Xu Chen and Harry Askham and David Gaddy and Peter Young and Jiewen Tan and Matan Eyal and Arthur Bra\u017einskas and Li Zhong and Zhichun Wu and Mark Epstein and Kai Bailey and Andrew Hard and Kamyu Lee and Sasha Goldshtein and Alex Ruiz and Mohammed Badawi and Matthias Lochbrunner and JK Kearns and Ashley Brown and Fabio Pardo and Theophane Weber and Haichuan Yang and Pan-Pan Jiang and Berkin Akin and Zhao Fu and Marcus Wainwright and Chi Zou and Meenu Gaba and Pierre-Antoine Manzagol and Wendy Kan and Yang Song and Karina Zainullina and Rui Lin and Jeongwoo Ko and Salil Deshmukh and Apoorv Jindal and James Svensson and Divya Tyam and Heri Zhao and Christine Kaeser-Chen and Scott Baird and Pooya Moradi and Jamie Hall and Qiuchen Guo and Vincent Tsang and Bowen Liang and Fernando Pereira and Suhas Ganesh and Ivan Korotkov and Jakub Adamek and Sridhar Thiagarajan and Vinh Tran and Charles Chen and Chris Tar and Sanil Jain and Ishita Dasgupta and Taylan Bilal and David Reitter and Kai Zhao and Giulia Vezzani and Yasmin Gehman and Pulkit Mehta and Lauren Beltrone and Xerxes Dotiwalla and Sergio Guadarrama and Zaheer Abbas and Stefani Karp and Petko Georgiev and Chun-Sung Ferng and Marc Brockschmidt and Liqian Peng and Christoph Hirnschall and Vikas Verma and Yingying Bi and Ying Xiao and Avigail Dabush and Kelvin Xu and Phil Wallis and Randall Parker and Qifei Wang and Yang Xu and Ilkin Safarli and Dinesh Tewari and Yin Zhang and Seungyeon Kim and Andrea Gesmundo and Mackenzie Thomas and Sergey Levi and Ahmed Chowdhury and Kanishka Rao and Peter Garst and Sam Conway-Rahman and Helen Ran and Kay McKinney and Zhisheng Xiao and Wenhao Yu and Rohan Agrawal and Axel Stjerngren and Catalin Ionescu and Jingjing Chen and Vivek Sharma and Justin Chiu and Fei Liu and Ken Franko and Clayton Sanford and Xingyu Cai and Paul Michel and Sanjay Ganapathy and Jane Labanowski and Zachary Garrett and Ben Vargas and Sean Sun and Bryan Gale and Thomas Buschmann and Guillaume Desjardins and Nimesh Ghelani and Palak Jain and Mudit Verma and Chulayuth Asawaroengchai and Julian Eisenschlos and Jitendra Harlalka and Hideto Kazawa and Don Metzler and Joshua Howland and Ying Jian and Jake Ades and Viral Shah and Tynan Gangwani and Seungji Lee and Roman Ring and Steven M. Hernandez and Dean Reich and Amer Sinha and Ashutosh Sathe and Joe Kovac and Ashleah Gill and Ajay Kannan and Andrea D'olimpio and Martin Sevenich and Jay Whang and Been Kim and Khe Chai Sim and Jilin Chen and Jiageng Zhang and Shuba Lall and Yossi Matias and Bill Jia and Abe Friesen and Sara Nasso and Ashish Thapliyal and Bryan Perozzi and Ting Yu and Anna Shekhawat and Safeen Huda and Peter Grabowski and Eric Wang and Ashwin Sreevatsa and Hilal Dib and Mehadi Hassen and Parker Schuh and Vedrana Milutinovic and Chris Welty and Michael Quinn and Ali Shah and Bangju Wang and Gabe Barth-Maron and Justin Frye and Natalie Axelsson and Tao Zhu and Yukun Ma and Irene Giannoumis and Hanie Sedghi and Chang Ye and Yi Luan and Kevin Aydin and Bilva Chandra and Vivek Sampathkumar and Ronny Huang and Victor Lavrenko and Ahmed Eleryan and Zhi Hong and Steven Hansen and Sara Mc Carthy and Bidisha Samanta and Domagoj \u0106evid and Xin Wang and Fangtao Li and Michael Voznesensky and Matt Hoffman and Andreas Terzis and Vikash Sehwag and Gil Fidel and Luheng He and Mu Cai and Yanzhang He and Alex Feng and Martin Nikoltchev and Samrat Phatale and Jason Chase and Rory Lawton and Ming Zhang and Tom Ouyang and Manuel Tragut and Mehdi Hafezi Manshadi and Arjun Narayanan and Jiaming Shen and Xu Gao and Tolga Bolukbasi and Nick Roy and Xin Li and Daniel Golovin and Liviu Panait and Zhen Qin and Guangxing Han and Thomas Anthony and Sneha Kudugunta and Viorica Patraucean and Aniket Ray and Xinyun Chen and Xiaochen Yang and Tanuj Bhatia and Pranav Talluri and Alex Morris and Andrija Ra\u017enatovi\u0107 and Bethanie Brownfield and James An and Sheng Peng and Patrick Kane and Ce Zheng and Nico Duduta and Joshua Kessinger and James Noraky and Siqi Liu and Keran Rong and Petar Veli\u010dkovi\u0107 and Keith Rush and Alex Goldin and Fanny Wei and Shiva Mohan Reddy Garlapati and Caroline Pantofaru and Okwan Kwon and Jianmo Ni and Eric Noland and Julia Di Trapani and Fran\u00e7oise Beaufays and Abhijit Guha Roy and Yinlam Chow and Aybuke Turker and Geoffrey Cideron and Lantao Mei and Jon Clark and Qingyun Dou and Matko Bo\u0161njak and Ralph Leith and Yuqing Du and Amir Yazdanbakhsh and Milad Nasr and Chester Kwak and Suraj Satishkumar Sheth and Alex Kaskasoli and Ankesh Anand and Balaji Lakshminarayanan and Sammy Jerome and David Bieber and Chun-Te Chu and Alexandre Senges and Tianxiao Shen and Mukund Sridhar and Ndaba Ndebele and Benjamin Beyret and Shakir Mohamed and Mia Chen and Markus Freitag and Jiaxian Guo and Luyang Liu and Paul Roit and Heng Chen and Shen Yan and Tom Stone and JD Co-Reyes and Jeremy Cole and Salvatore Scellato and Shekoofeh Azizi and Hadi Hashemi and Alicia Jin and Anand Iyer and Marcella Valentine and Andr\u00e1s Gy\u00f6rgy and Arun Ahuja and Daniel Hernandez Diaz and Chen-Yu Lee and Nathan Clement and Weize Kong and Drew Garmon and Ishaan Watts and Kush Bhatia and Khyatti Gupta and Matt Miecnikowski and Hugo Vallet and Ankur Taly and Edward Loper and Saket Joshi and James Atwood and Jo Chick and Mark Collier and Fotis Iliopoulos and Ryan Trostle and Beliz Gunel and Ramiro Leal-Cavazos and Arnar Mar Hrafnkelsson and Michael Guzman and Xiaoen Ju and Andy Forbes and Jesse Emond and Kushal Chauhan and Ben Caine and Li Xiao and Wenjun Zeng and Alexandre Moufarek and Daniel Murphy and Maya Meng and Nitish Gupta and Felix Riedel and Anil Das and Elijah Lawal and Shashi Narayan and Tiberiu Sosea and James Swirhun and Linda Friso and Behnam Neyshabur and Jing Lu and Sertan Girgin and Michael Wunder and Edouard Yvinec and Aroonalok Pyne and Victor Carbune and Shruti Rijhwani and Yang Guo and Tulsee Doshi and Anton Briukhov and Max Bain and Ayal Hitron and Xuanhui Wang and Ashish Gupta and Ke Chen and Cosmo Du and Weiyang Zhang and Dhruv Shah and Arjun Akula and Max Dylla and Ashyana Kachra and Weicheng Kuo and Tingting Zou and Lily Wang and Luyao Xu and Jifan Zhu and Justin Snyder and Sachit Menon and Orhan Firat and Igor Mordatch and Yuan Yuan and Natalia Ponomareva and Rory Blevins and Lawrence Moore and Weijun Wang and Phil Chen and Martin Scholz and Artur Dwornik and Jason Lin and Sicheng Li and Diego Antognini and Te I and Xiaodan Song and Matt Miller and Uday Kalra and Adam Raveret and Oscar Akerlund and Felix Wu and Andrew Nystrom and Namrata Godbole and Tianqi Liu and Hannah DeBalsi and Jewel Zhao and Buhuang Liu and Avi Caciularu and Lauren Lax and Urvashi Khandelwal and Victoria Langston and Eric Bailey and Silvio Lattanzi and Yufei Wang and Neel Kovelamudi and Sneha Mondal and Guru Guruganesh and Nan Hua and Ofir Roval and Pawe\u0142 Weso\u0142owski and Rishikesh Ingale and Jonathan Halcrow and Tim Sohn and Christof Angermueller and Bahram Raad and Eli Stickgold and Eva Lu and Alec Kosik and Jing Xie and Timothy Lillicrap and Austin Huang and Lydia Lihui Zhang and Dominik Paulus and Clement Farabet and Alex Wertheim and Bing Wang and Rishabh Joshi and Chu-ling Ko and Yonghui Wu and Shubham Agrawal and Lily Lin and XiangHai Sheng and Peter Sung and Tyler Breland-King and Christina Butterfield and Swapnil Gawde and Sumeet Singh and Qiao Zhang and Raj Apte and Shilpa Shetty and Adrian Hutter and Tao Li and Elizabeth Salesky and Federico Lebron and Jonni Kanerva and Michela Paganini and Arthur Nguyen and Rohith Vallu and Jan-Thorsten Peter and Sarmishta Velury and David Kao and Jay Hoover and Anna Bortsova and Colton Bishop and Shoshana Jakobovits and Alessandro Agostini and Alekh Agarwal and Chang Liu and Charles Kwong and Sasan Tavakkol and Ioana Bica and Alex Greve and Anirudh GP and Jake Marcus and Le Hou and Tom Duerig and Rivka Moroshko and Dave Lacey and Andy Davis and Julien Amelot and Guohui Wang and Frank Kim and Theofilos Strinopoulos and Hui Wan and Charline Le Lan and Shankar Krishnan and Haotian Tang and Peter Humphreys and Junwen Bai and Idan Heimlich Shtacher and Diego Machado and Chenxi Pang and Ken Burke and Dangyi Liu and Renga Aravamudhan and Yue Song and Ed Hirst and Abhimanyu Singh and Brendan Jou and Liang Bai and Francesco Piccinno and Chuyuan Kelly Fu and Robin Alazard and Barak Meiri and Daniel Winter and Charlie Chen and Mingda Zhang and Jens Heitkaemper and John Lambert and Jinhyuk Lee and Alexander Fr\u00f6mmgen and Sergey Rogulenko and Pranav Nair and Paul Niemczyk and Anton Bulyenov and Bibo Xu and Hadar Shemtov and Morteza Zadimoghaddam and Serge Toropov and Mateo Wirth and Hanjun Dai and Sreenivas Gollapudi and Daniel Zheng and Alex Kurakin and Chansoo Lee and Kalesha Bullard and Nicolas Serrano and Ivana Balazevic and Yang Li and Johan Schalkwyk and Mark Murphy and Mingyang Zhang and Kevin Sequeira and Romina Datta and Nishant Agrawal and Charles Sutton and Nithya Attaluri and Mencher Chiang and Wael Farhan and Gregory Thornton and Kate Lin and Travis Choma and Hung Nguyen and Kingshuk Dasgupta and Dirk Robinson and Iulia Com\u015fa and Michael Riley and Arjun Pillai and Basil Mustafa and Ben Golan and Amir Zandieh and Jean-Baptiste Lespiau and Billy Porter and David Ross and Sujeevan Rajayogam and Mohit Agarwal and Subhashini Venugopalan and Bobak Shahriari and Qiqi Yan and Hao Xu and Taylor Tobin and Pavel Dubov and Hongzhi Shi and Adri\u00e0 Recasens and Anton Kovsharov and Sebastian Borgeaud and Lucio Dery and Shanthal Vasanth and Elena Gribovskaya and Linhai Qiu and Mahdis Mahdieh and Wojtek Skut and Elizabeth Nielsen and CJ Zheng and Adams Yu and Carrie Grimes Bostock and Shaleen Gupta and Aaron Archer and Chris Rawles and Elinor Davies and Alexey Svyatkovskiy and Tomy Tsai and Yoni Halpern and Christian Reisswig and Bartek Wydrowski and Bo Chang and Joan Puigcerver and Mor Hazan Taege and Jian Li and Eva Schnider and Xinjian Li and Dragos Dena and Yunhan Xu and Umesh Telang and Tianze Shi and Heiga Zen and Kyle Kastner and Yeongil Ko and Neesha Subramaniam and Aviral Kumar and Pete Blois and Zhuyun Dai and John Wieting and Yifeng Lu and Yoel Zeldes and Tian Xie and Anja Hauth and Alexandru \u0162ifrea and Yuqi Li and Sam El-Husseini and Dan Abolafia and Howard Zhou and Wen Ding and Sahra Ghalebikesabi and Carlos Gu\u00eda and Andrii Maksai and \u00c1goston Weisz and Sercan Arik and Nick Sukhanov and Aga \u015awietlik and Xuhui Jia and Luo Yu and Weiyue Wang and Mark Brand and Dawn Bloxwich and Sean Kirmani and Zhe Chen and Alec Go and Pablo Sprechmann and Nithish Kannen and Alen Carin and Paramjit Sandhu and Isabel Edkins and Leslie Nooteboom and Jai Gupta and Loren Maggiore and Javad Azizi and Yael Pritch and Pengcheng Yin and Mansi Gupta and Danny Tarlow and Duncan Smith and Desi Ivanov and Mohammad Babaeizadeh and Ankita Goel and Satish Kambala and Grace Chu and Matej Kastelic and Michelle Liu and Hagen Soltau and Austin Stone and Shivani Agrawal and Min Kim and Kedar Soparkar and Srinivas Tadepalli and Oskar Bunyan and Rachel Soh and Arvind Kannan and DY Kim and Blake JianHang Chen and Afief Halumi and Sudeshna Roy and Yulong Wang and Olcan Sercinoglu and Gena Gibson and Sijal Bhatnagar and Motoki Sano and Daniel von Dincklage and Qingchun Ren and Blagoj Mitrevski and Mirek Ol\u0161\u00e1k and Jennifer She and Carl Doersch and  Jilei and  Wang and Bingyuan Liu and Qijun Tan and Tamar Yakar and Tris Warkentin and Alex Ramirez and Carl Lebsack and Josh Dillon and Rajiv Mathews and Tom Cobley and Zelin Wu and Zhuoyuan Chen and Jon Simon and Swaroop Nath and Tara Sainath and Alexei Bendebury and Ryan Julian and Bharath Mankalale and Daria \u0106urko and Paulo Zacchello and Adam R. Brown and Kiranbir Sodhia and Heidi Howard and Sergi Caelles and Abhinav Gupta and Gareth Evans and Anna Bulanova and Lesley Katzen and Roman Goldenberg and Anton Tsitsulin and Joe Stanton and Benoit Schillings and Vitaly Kovalev and Corey Fry and Rushin Shah and Kuo Lin and Shyam Upadhyay and Cheng Li and Soroush Radpour and Marcello Maggioni and Jing Xiong and Lukas Haas and Jenny Brennan and Aishwarya Kamath and Nikolay Savinov and Arsha Nagrani and Trevor Yacovone and Ryan Kappedal and Kostas Andriopoulos and Li Lao and YaGuang Li and Grigory Rozhdestvenskiy and Kazuma Hashimoto and Andrew Audibert and Sophia Austin and Daniel Rodriguez and Anian Ruoss and Garrett Honke and Deep Karkhanis and Xi Xiong and Qing Wei and James Huang and Zhaoqi Leng and Vittal Premachandran and Stan Bileschi and Georgios Evangelopoulos and Thomas Mensink and Jay Pavagadhi and Denis Teplyashin and Paul Chang and Linting Xue and Garrett Tanzer and Sally Goldman and Kaushal Patel and Shixin Li and Jeremy Wiesner and Ivy Zheng and Ian Stewart-Binks and Jie Han and Zhi Li and Liangchen Luo and Karel Lenc and Mario Lu\u010di\u0107 and Fuzhao Xue and Ryan Mullins and Alexey Guseynov and Chung-Ching Chang and Isaac Galatzer-Levy and Adam Zhang and Garrett Bingham and Grace Hu and Ale Hartman and Yue Ma and Jordan Griffith and Alex Irpan and Carey Radebaugh and Summer Yue and Lijie Fan and Victor Ungureanu and Christina Sorokin and Hannah Teufel and Peiran Li and Rohan Anil and Dimitris Paparas and Todd Wang and Chu-Cheng Lin and Hui Peng and Megan Shum and Goran Petrovic and Demetra Brady and Richard Nguyen and Klaus Macherey and Zhihao Li and Harman Singh and Madhavi Yenugula and Mariko Iinuma and Xinyi Chen and Kavya Kopparapu and Alexey Stern and Shachi Dave and Chandu Thekkath and Florence Perot and Anurag Kumar and Fangda Li and Yang Xiao and Matthew Bilotti and Mohammad Hossein Bateni and Isaac Noble and Lisa Lee and Amelio V\u00e1zquez-Reina and Julian Salazar and Xiaomeng Yang and Boyu Wang and Ela Gruzewska and Anand Rao and Sindhu Raghuram and Zheng Xu and Eyal Ben-David and Jieru Mei and Sid Dalmia and Zhaoyi Zhang and Yuchen Liu and Gagan Bansal and Helena Pankov and Steven Schwarcz and Andrea Burns and Christine Chan and Sumit Sanghai and Ricky Liang and Ethan Liang and Antoine He and Amy Stuart and Arun Narayanan and Yukun Zhu and Christian Frank and Bahar Fatemi and Amit Sabne and Oran Lang and Indro Bhattacharya and Shane Settle and Maria Wang and Brendan McMahan and Andrea Tacchetti and Livio Baldini Soares and Majid Hadian and Serkan Cabi and Timothy Chung and Nikita Putikhin and Gang Li and Jeremy Chen and Austin Tarango and Henryk Michalewski and Mehran Kazemi and Hussain Masoom and Hila Sheftel and Rakesh Shivanna and Archita Vadali and Ramona Comanescu and Doug Reid and Joss Moore and Arvind Neelakantan and Micha\u00ebl Sander and Jonathan Herzig and Aviv Rosenberg and Mostafa Dehghani and JD Choi and Michael Fink and Reid Hayes and Eric Ge and Shitao Weng and Chia-Hua Ho and John Karro and Kalpesh Krishna and Lam Nguyen Thiet and Amy Skerry-Ryan and Daniel Eppens and Marco Andreetto and Navin Sarma and Silvano Bonacina and Burcu Karagol Ayan and Megha Nawhal and Zhihao Shan and Mike Dusenberry and Shantanu Thakoor and Sagar Gubbi and Duc Dung Nguyen and Reut Tsarfaty and Samuel Albanie and Jovana Mitrovi\u0107 and Meet Gandhi and Bo-Juen Chen and Alessandro Epasto and Georgi Stephanov and Ye Jin and Samuel Gehman and Aida Amini and Jack Weber and Feryal Behbahani and Shawn Xu and Miltos Allamanis and Xi Chen and Myle Ott and Claire Sha and Michal Jastrzebski and Hang Qi and David Greene and Xinyi Wu and Abodunrinwa Toki and Daniel Vlasic and Jane Shapiro and Ragha Kotikalapudi and Zhe Shen and Takaaki Saeki and Sirui Xie and Albin Cassirer and Shikhar Bharadwaj and Tatsuya Kiyono and Srinadh Bhojanapalli and Elan Rosenfeld and Sam Ritter and Jieming Mao and Jo\u00e3o Gabriel Oliveira and Zoltan Egyed and Bernd Bandemer and Emilio Parisotto and Keisuke Kinoshita and Juliette Pluto and Petros Maniatis and Steve Li and Yaohui Guo and Golnaz Ghiasi and Jean Tarbouriech and Srimon Chatterjee and Julie Jin and  Katrina and  Xu and Jennimaria Palomaki and S\u00e9b Arnold and Madhavi Sewak and Federico Piccinini and Mohit Sharma and Ben Albrecht and Sean Purser-haskell and Ashwin Vaswani and Chongyan Chen and Matheus Wisniewski and Qin Cao and John Aslanides and Nguyet Minh Phu and Maximilian Sieb and Lauren Agubuzu and Anne Zheng and Daniel Sohn and Marco Selvi and Anders Andreassen and Krishan Subudhi and Prem Eruvbetine and Oliver Woodman and Tomas Mery and Sebastian Krause and Xiaoqi Ren and Xiao Ma and Jincheng Luo and Dawn Chen and Wei Fan and Henry Griffiths and Christian Schuler and Alice Li and Shujian Zhang and Jean-Michel Sarr and Shixin Luo and Riccardo Patana and Matthew Watson and Dani Naboulsi and Michael Collins and Sailesh Sidhwani and Emiel Hoogeboom and Sharon Silver and Emily Caveness and Xiaokai Zhao and Mikel Rodriguez and Maxine Deines and Libin Bai and Patrick Griffin and Marco Tagliasacchi and Emily Xue and Spandana Raj Babbula and Bo Pang and Nan Ding and Gloria Shen and Elijah Peake and Remi Crocker and Shubha Srinivas Raghvendra and Danny Swisher and Woohyun Han and Richa Singh and Ling Wu and Vladimir Pchelin and Tsendsuren Munkhdalai and Dana Alon and Geoff Bacon and Efren Robles and Jannis Bulian and Melvin Johnson and George Powell and Felipe Tiengo Ferreira and Yaoyiran Li and Frederik Benzing and Mihajlo Velimirovi\u0107 and Hubert Soyer and William Kong and  Tony and  Nguy\u00ean and Zhen Yang and Jeremiah Liu and Joost van Amersfoort and Daniel Gillick and Baochen Sun and Nathalie Rauschmayr and Katie Zhang and Serena Zhan and Tao Zhou and Alexey Frolov and Chengrun Yang and Denis Vnukov and Louis Rouillard and Hongji Li and Amol Mandhane and Nova Fallen and Rajesh Venkataraman and Clara Huiyi Hu and Jennifer Brennan and Jenny Lee and Jerry Chang and Martin Sundermeyer and Zhufeng Pan and Rosemary Ke and Simon Tong and Alex Fabrikant and William Bono and Jindong Gu and Ryan Foley and Yiran Mao and Manolis Delakis and Dhruva Bhaswar and Roy Frostig and Nick Li and Avital Zipori and Cath Hope and Olga Kozlova and Swaroop Mishra and Josip Djolonga and Craig Schiff and Majd Al Merey and Eleftheria Briakou and Peter Morgan and Andy Wan and Avinatan Hassidim and RJ Skerry-Ryan and Kuntal Sengupta and Mary Jasarevic and Praveen Kallakuri and Paige Kunkle and Hannah Brennan and Tom Lieber and Hassan Mansoor and Julian Walker and Bing Zhang and Annie Xie and Goran \u017du\u017ei\u0107 and Adaeze Chukwuka and Alex Druinsky and Donghyun Cho and Rui Yao and Ferjad Naeem and Shiraz Butt and Eunyoung Kim and Zhipeng Jia and Mandy Jordan and Adam Lelkes and Mark Kurzeja and Sophie Wang and James Zhao and Andrew Over and Abhishek Chakladar and Marcel Prasetya and Neha Jha and Sriram Ganapathy and Yale Cong and Prakash Shroff and Carl Saroufim and Sobhan Miryoosefi and Mohamed Hammad and Tajwar Nasir and Weijuan Xi and Yang Gao and Young Maeng and Ben Hora and Chin-Yi Cheng and Parisa Haghani and Yoad Lewenberg and Caden Lu and Martin Matysiak and Naina Raisinghani and Huiyu Wang and Lexi Baugher and Rahul Sukthankar and Minh Giang and John Schultz and Noah Fiedel and Minmin Chen and Cheng-Chun Lee and Tapomay Dey and Hao Zheng and Shachi Paul and Celine Smith and Andy Ly and Yicheng Wang and Rishabh Bansal and Bartek Perz and Susanna Ricco and Stasha Blank and Vaishakh Keshava and Deepak Sharma and Marvin Chow and Kunal Lad and Komal Jalan and Simon Osindero and Craig Swanson and Jacob Scott and Anastasija Ili\u0107 and Xiaowei Li and Siddhartha Reddy Jonnalagadda and Afzal Shama Soudagar and Yan Xiong and Bat-Orgil Batsaikhan and Daniel Jarrett and Naveen Kumar and Maulik Shah and Matt Lawlor and Austin Waters and Mark Graham and Rhys May and Sabela Ramos and Sandra Lefdal and Zeynep Cankara and Nacho Cano and Brendan O'Donoghue and Jed Borovik and Frederick Liu and Jordan Grimstad and Mahmoud Alnahlawi and Katerina Tsihlas and Tom Hudson and Nikolai Grigorev and Yiling Jia and Terry Huang and Tobenna Peter Igwe and Sergei Lebedev and Xiaodan Tang and Igor Krivokon and Frankie Garcia and Melissa Tan and Eric Jia and Peter Stys and Shikhar Vashishth and Yu Liang and Balaji Venkatraman and Chenjie Gu and Anastasios Kementsietsidis and Chen Zhu and Junehyuk Jung and Yunfei Bai and Mohammad Javad Hosseini and Faruk Ahmed and Aditya Gupta and Xin Yuan and Shereen Ashraf and Shitij Nigam and Gautam Vasudevan and Pranjal Awasthi and Adi Mayrav Gilady and Zelda Mariet and Ramy Eskander and Haiguang Li and Hexiang Hu and Guillermo Garrido and Philippe Schlattner and George Zhang and Rohun Saxena and Petar Devi\u0107 and Kritika Muralidharan and Ashwin Murthy and Yiqian Zhou and Min Choi and Arissa Wongpanich and Zhengdong Wang and Premal Shah and Yuntao Xu and Yiling Huang and Stephen Spencer and Alice Chen and James Cohan and Junjie Wang and Jonathan Tompson and Junru Wu and Ruba Haroun and Haiqiong Li and Blanca Huergo and Fan Yang and Tongxin Yin and James Wendt and Michael Bendersky and Rahma Chaabouni and Javier Snaider and Johan Ferret and Abhishek Jindal and Tara Thompson and Andrew Xue and Will Bishop and Shubham Milind Phal and Archit Sharma and Yunhsuan Sung and Prabakar Radhakrishnan and Mo Shomrat and Reeve Ingle and Roopali Vij and Justin Gilmer and Mihai Dorin Istin and Sam Sobell and Yang Lu and Emily Nottage and Dorsa Sadigh and Jeremiah Willcock and Tingnan Zhang and Steve Xu and Sasha Brown and Katherine Lee and Gary Wang and Yun Zhu and Yi Tay and Cheolmin Kim and Audrey Gutierrez and Abhanshu Sharma and Yongqin Xian and Sungyong Seo and Claire Cui and Elena Pochernina and Cip Baetu and Krzysztof Jastrz\u0119bski and Mimi Ly and Mohamed Elhawaty and Dan Suh and Eren Sezener and Pidong Wang and Nancy Yuen and George Tucker and Jiahao Cai and Zuguang Yang and Cindy Wang and Alex Muzio and Hai Qian and Jae Yoo and Derek Lockhart and Kevin R. McKee and Mandy Guo and Malika Mehrotra and Artur Mendon\u00e7a and Sanket Vaibhav Mehta and Sherry Ben and Chetan Tekur and Jiaqi Mu and Muye Zhu and Victoria Krakovna and Hongrae Lee and AJ Maschinot and S\u00e9bastien Cevey and HyunJeong Choe and Aijun Bai and Hansa Srinivasan and Derek Gasaway and Nick Young and Patrick Siegler and Dan Holtmann-Rice and Vihari Piratla and Kate Baumli and Roey Yogev and Alex Hofer and Hado van Hasselt and Svetlana Grant and Yuri Chervonyi and David Silver and Andrew Hogue and Ayushi Agarwal and Kathie Wang and Preeti Singh and Four Flynn and Josh Lipschultz and Robert David and Lizzetth Bellot and Yao-Yuan Yang and Long Le and Filippo Graziano and Kate Olszewska and Kevin Hui and Akanksha Maurya and Nikos Parotsidis and Weijie Chen and Tayo Oguntebi and Joe Kelley and Anirudh Baddepudi and Johannes Mauerer and Gregory Shaw and Alex Siegman and Lin Yang and Shravya Shetty and Subhrajit Roy and Yunting Song and Wojciech Stokowiec and Ryan Burnell and Omkar Savant and Robert Busa-Fekete and Jin Miao and Samrat Ghosh and Liam MacDermed and Phillip Lippe and Mikhail Dektiarev and Zach Behrman and Fabian Mentzer and Kelvin Nguyen and Meng Wei and Siddharth Verma and Chris Knutsen and Sudeep Dasari and Zhipeng Yan and Petr Mitrichev and Xingyu Wang and Virat Shejwalkar and Jacob Austin and Srinivas Sunkara and Navneet Potti and Yan Virin and Christian Wright and Ga\u00ebl Liu and Oriana Riva and Etienne Pot and Greg Kochanski and Quoc Le and Gargi Balasubramaniam and Arka Dhar and Yuguo Liao and Adam Bloniarz and Divyansh Shukla and Elizabeth Cole and Jong Lee and Sheng Zhang and Sushant Kafle and Siddharth Vashishtha and Parsa Mahmoudieh and Grace Chen and Raphael Hoffmann and Pranesh Srinivasan and Agustin Dal Lago and Yoav Ben Shalom and Zi Wang and Michael Elabd and Anuj Sharma and Junhyuk Oh and Suraj Kothawade and Maigo Le and Marianne Monteiro and Shentao Yang and Kaiz Alarakyia and Robert Geirhos and Diana Mincu and H\u00e5vard Garnes and Hayato Kobayashi and Soroosh Mariooryad and Kacper Krasowiak and  Zhixin and  Lai and Shibl Mourad and Mingqiu Wang and Fan Bu and Ophir Aharoni and Guanjie Chen and Abhimanyu Goyal and Vadim Zubov and Ankur Bapna and Elahe Dabir and Nisarg Kothari and Kay Lamerigts and Nicola De Cao and Jeremy Shar and Christopher Yew and Nitish Kulkarni and Dre Mahaarachchi and Mandar Joshi and Zhenhai Zhu and Jared Lichtarge and Yichao Zhou and Hannah Muckenhirn and Vittorio Selo and Oriol Vinyals and Peter Chen and Anthony Brohan and Vaibhav Mehta and Sarah Cogan and Ruth Wang and Ty Geri and Wei-Jen Ko and Wei Chen and Fabio Viola and Keshav Shivam and Lisa Wang and Madeleine Clare Elish and Raluca Ada Popa and S\u00e9bastien Pereira and Jianqiao Liu and Raphael Koster and Donnie Kim and Gufeng Zhang and Sayna Ebrahimi and Partha Talukdar and Yanyan Zheng and Petra Poklukar and Ales Mikhalap and Dale Johnson and Anitha Vijayakumar and Mark Omernick and Matt Dibb and Ayush Dubey and Qiong Hu and Apurv Suman and Vaibhav Aggarwal and Ilya Kornakov and Fei Xia and Wing Lowe and Alexey Kolganov and Ted Xiao and Vitaly Nikolaev and Steven Hemingray and Bonnie Li and Joana Iljazi and Miko\u0142aj Rybi\u0144ski and Ballie Sandhu and Peggy Lu and Thang Luong and Rodolphe Jenatton and Vineetha Govindaraj and  Hui and  Li and Gabriel Dulac-Arnold and Wonpyo Park and Henry Wang and Abhinit Modi and Jean Pouget-Abadie and Kristina Greller and Rahul Gupta and Robert Berry and Prajit Ramachandran and Jinyu Xie and Liam McCafferty and Jianling Wang and Kilol Gupta and Hyeontaek Lim and Bla\u017e Bratani\u010d and Andy Brock and Ilia Akolzin and Jim Sproch and Dan Karliner and Duhyeon Kim and Adrian Goedeckemeyer and Noam Shazeer and Cordelia Schmid and Daniele Calandriello and Parul Bhatia and Krzysztof Choromanski and Ceslee Montgomery and Dheeru Dua and Ana Ramalho and Helen King and Yue Gao and Lynn Nguyen and David Lindner and Divya Pitta and Oleaser Johnson and Khalid Salama and Diego Ardila and Michael Han and Erin Farnese and Seth Odoom and Ziyue Wang and Xiangzhuo Ding and Norman Rink and Ray Smith and Harshal Tushar Lehri and Eden Cohen and Neera Vats and Tong He and Parthasarathy Gopavarapu and Adam Paszke and Miteyan Patel and Wouter Van Gansbeke and Lucia Loher and Luis Castro and Maria Voitovich and Tamara von Glehn and Nelson George and Simon Niklaus and Zach Eaton-Rosen and Nemanja Raki\u0107evi\u0107 and Erik Jue and Sagi Perel and Carrie Zhang and Yuval Bahat and Ang\u00e9line Pouget and Zhi Xing and Fantine Huot and Ashish Shenoy and Taylor Bos and Vincent Coriou and Bryan Richter and Natasha Noy and Yaqing Wang and Santiago Ontanon and Siyang Qin and Gleb Makarchuk and Demis Hassabis and Zhuowan Li and Mandar Sharma and Kumaran Venkatesan and Iurii Kemaev and Roxanne Daniel and Shiyu Huang and Saloni Shah and Octavio Ponce and  Warren and  Chen and Manaal Faruqui and Jialin Wu and Slavica Anda\u010di\u0107 and Szabolcs Payrits and Daniel McDuff and Tom Hume and Yuan Cao and MH Tessler and Qingze Wang and Yinan Wang and Ivor Rendulic and Eirikur Agustsson and Matthew Johnson and Tanya Lando and Andrew Howard and Sri Gayatri Sundara Padmanabhan and Mayank Daswani and Andrea Banino and Michael Kilgore and Jonathan Heek and Ziwei Ji and Alvaro Caceres and Conglong Li and Nora Kassner and Alexey Vlaskin and Zeyu Liu and Alex Grills and Yanhan Hou and Roykrong Sukkerd and Gowoon Cheon and Nishita Shetty and Larisa Markeeva and Piotr Stanczyk and Tejas Iyer and Yuan Gong and Shawn Gao and Keerthana Gopalakrishnan and Tim Blyth and Malcolm Reynolds and Avishkar Bhoopchand and Misha Bilenko and Dero Gharibian and Vicky Zayats and Aleksandra Faust and Abhinav Singh and Min Ma and Hongyang Jiao and Sudheendra Vijayanarasimhan and Lora Aroyo and Vikas Yadav and Sarah Chakera and Ashwin Kakarla and Vilobh Meshram and Karol Gregor and Gabriela Botea and Evan Senter and Dawei Jia and Geza Kovacs and Neha Sharma and Sebastien Baur and Kai Kang and Yifan He and Lin Zhuo and Marija Kostelac and Itay Laish and Songyou Peng and Louis O'Bryan and Daniel Kasenberg and Girish Ramchandra Rao and Edouard Leurent and Biao Zhang and Sage Stevens and Ana Salazar and Ye Zhang and Ivan Lobov and Jake Walker and Allen Porter and Morgan Redshaw and Han Ke and Abhishek Rao and Alex Lee and Hoi Lam and Michael Moffitt and Jaeyoun Kim and Siyuan Qiao and Terry Koo and Robert Dadashi and Xinying Song and Mukund Sundararajan and Peng Xu and Chizu Kawamoto and Yan Zhong and Clara Barbu and Apoorv Reddy and Mauro Verzetti and Leon Li and George Papamakarios and Hanna Klimczak-Pluci\u0144ska and Mary Cassin and Koray Kavukcuoglu and Rigel Swavely and Alain Vaucher and Jeffrey Zhao and Ross Hemsley and Michael Tschannen and Heming Ge and Gaurav Menghani and Yang Yu and Natalie Ha and Wei He and Xiao Wu and Maggie Song and Rachel Sterneck and Stefan Zinke and Dan A. Calian and Annie Marsden and Alejandro Cruzado Ruiz and Matteo Hessel and Almog Gueta and Benjamin Lee and Brian Farris and Manish Gupta and Yunjie Li and Mohammad Saleh and Vedant Misra and Kefan Xiao and Piermaria Mendolicchio and Gavin Buttimore and Varvara Krayvanova and Nigamaa Nayakanti and Matthew Wiethoff and Yash Pande and Azalia Mirhoseini and Ni Lao and Jasmine Liu and Yiqing Hua and Angie Chen and Yury Malkov and Dmitry Kalashnikov and Shubham Gupta and Kartik Audhkhasi and Yuexiang Zhai and Sudhindra Kopalle and Prateek Jain and Eran Ofek and Clemens Meyer and Khuslen Baatarsukh and Hana Strej\u010dek and Jun Qian and James Freedman and Ricardo Figueira and Michal Sokolik and Olivier Bachem and Raymond Lin and Dia Kharrat and Chris Hidey and Pingmei Xu and Dennis Duan and Yin Li and Muge Ersoy and Richard Everett and Kevin Cen and Rebeca Santamaria-Fernandez and Amir Taubenfeld and Ian Mackinnon and Linda Deng and Polina Zablotskaia and Shashank Viswanadha and Shivanker Goel and Damion Yates and Yunxiao Deng and Peter Choy and Mingqing Chen and Abhishek Sinha and Alex Mossin and Yiming Wang and Arthur Szlam and Susan Hao and Paul Kishan Rubenstein and Metin Toksoz-Exley and Miranda Aperghis and Yin Zhong and Junwhan Ahn and Michael Isard and Olivier Lacombe and Florian Luisier and Chrysovalantis Anastasiou and Yogesh Kalley and Utsav Prabhu and Emma Dunleavy and Shaan Bijwadia and Justin Mao-Jones and Kelly Chen and Rama Pasumarthi and Emily Wood and Adil Dostmohamed and Nate Hurley and Jiri Simsa and Alicia Parrish and Mantas Pajarskas and Matt Harvey and Ondrej Skopek and Yony Kochinski and Javier Rey and Verena Rieser and Denny Zhou and Sun Jae Lee and Trilok Acharya and Guowang Li and Joe Jiang and Xiaofan Zhang and Bryant Gipson and Ethan Mahintorabi and Marco Gelmi and Nima Khajehnouri and Angel Yeh and Kayi Lee and Loic Matthey and Leslie Baker and Trang Pham and Han Fu and Alex Pak and Prakhar Gupta and Cristina Vasconcelos and Adam Sadovsky and Brian Walker and Sissie Hsiao and Patrik Zochbauer and Andreea Marzoca and Noam Velan and Junhao Zeng and Gilles Baechler and Danny Driess and Divya Jain and Yanping Huang and Lizzie Tao and John Maggs and Nir Levine and Jon Schneider and Erika Gemzer and Samuel Petit and Shan Han and Zach Fisher and Dustin Zelle and Courtney Biles and Eugene Ie and Asya Fadeeva and Casper Liu and Juliana Vicente Franco and Adrian Collister and Hao Zhang and Renshen Wang and Ruizhe Zhao and Leandro Kieliger and Kurt Shuster and Rui Zhu and Boqing Gong and Lawrence Chan and Ruoxi Sun and Sujoy Basu and Roland Zimmermann and Jamie Hayes and Abhishek Bapna and Jasper Snoek and Weel Yang and Puranjay Datta and Jad Al Abdallah and Kevin Kilgour and Lu Li and SQ Mah and Yennie Jun and Morgane Rivi\u00e8re and Abhijit Karmarkar and Tammo Spalink and Tao Huang and Lucas Gonzalez and Duc-Hieu Tran and Averi Nowak and John Palowitch and Martin Chadwick and Ellie Talius and Harsh Mehta and Thibault Sellam and Philipp Fr\u00e4nken and Massimo Nicosia and Kyle He and Aditya Kini and David Amos and Sugato Basu and Harrison Jobe and Eleni Shaw and Qiantong Xu and Colin Evans and Daisuke Ikeda and Chaochao Yan and Larry Jin and Lun Wang and Sachin Yadav and Ilia Labzovsky and Ramesh Sampath and Ada Ma and Candice Schumann and Aditya Siddhant and Rohin Shah and John Youssef and Rishabh Agarwal and Natalie Dabney and Alessio Tonioni and Moran Ambar and Jing Li and Isabelle Guyon and Benny Li and David Soergel and Boya Fang and Georgi Karadzhov and Cristian Udrescu and Trieu Trinh and Vikas Raunak and Seb Noury and Dee Guo and Sonal Gupta and Mara Finkelstein and Denis Petek and Lihao Liang and Greg Billock and Pei Sun and David Wood and Yiwen Song and Xiaobin Yu and Tatiana Matejovicova and Regev Cohen and Kalyan Andra and David D'Ambrosio and Zhiwei Deng and Vincent Nallatamby and Ebrahim Songhori and Rumen Dangovski and Andrew Lampinen and Pankil Botadra and Adam Hillier and Jiawei Cao and Nagabhushan Baddi and Adhi Kuncoro and Toshihiro Yoshino and Ankit Bhagatwala and Marc\u00e1urelio Ranzato and Rylan Schaeffer and Tianlin Liu and Shuai Ye and Obaid Sarvana and John Nham and Chenkai Kuang and Isabel Gao and Jinoo Baek and Shubham Mittal and Ayzaan Wahid and Anita Gergely and Bin Ni and Josh Feldman and Carrie Muir and Pascal Lamblin and Wolfgang Macherey and Ethan Dyer and Logan Kilpatrick and V\u00edctor Campos and Mukul Bhutani and Stanislav Fort and Yanif Ahmad and Aliaksei Severyn and Kleopatra Chatziprimou and Oleksandr Ferludin and Mason Dimarco and Aditya Kusupati and Joe Heyward and Dan Bahir and Kevin Villela and Katie Millican and Dror Marcus and Sanaz Bahargam and Caglar Unlu and Nicholas Roth and Zichuan Wei and Siddharth Gopal and Deepanway Ghoshal and Edward Lee and Sharon Lin and Jennie Lees and Dayeong Lee and Anahita Hosseini and Connie Fan and Seth Neel and Marcus Wu and Yasemin Altun and Honglong Cai and Enrique Piqueras and Josh Woodward and Alessandro Bissacco and Salem Haykal and Mahyar Bordbar and Prasha Sundaram and Sarah Hodkinson and Daniel Toyama and George Polovets and Austin Myers and Anu Sinha and Tomer Levinboim and Kashyap Krishnakumar and Rachita Chhaparia and Tatiana Sholokhova and Nitesh Bharadwaj Gundavarapu and Ganesh Jawahar and Haroon Qureshi and Jieru Hu and Nikola Momchev and Matthew Rahtz and Renjie Wu and Aishwarya P S and Kedar Dhamdhere and Meiqi Guo and Umang Gupta and Ali Eslami and Mariano Schain and Michiel Blokzijl and David Welling and Dave Orr and Levent Bolelli and Nicolas Perez-Nieves and Mikhail Sirotenko and Aman Prasad and Arjun Kar and Borja De Balle Pigem and Tayfun Terzi and Gell\u00e9rt Weisz and Dipankar Ghosh and Aditi Mavalankar and Dhruv Madeka and Kaspar Daugaard and Hartwig Adam and Viraj Shah and Dana Berman and Maggie Tran and Steven Baker and Ewa Andrejczuk and Grishma Chole and Ganna Raboshchuk and Mahdi Mirzazadeh and Thais Kagohara and Shimu Wu and Christian Schallhart and Bernett Orlando and Chen Wang and Alban Rrustemi and Hao Xiong and Hao Liu and Arpi Vezer and Nolan Ramsden and Shuo-yiin Chang and Sidharth Mudgal and Yan Li and Nino Vieillard and Yedid Hoshen and Farooq Ahmad and Ambrose Slone and Amy Hua and Natan Potikha and Mirko Rossini and Jon Stritar and Sushant Prakash and Zifeng Wang and Xuanyi Dong and Alireza Nazari and Efrat Nehoran and Kaan Tekelioglu and Yinxiao Li and Kartikeya Badola and Tom Funkhouser and Yuanzhen Li and Varun Yerram and Ramya Ganeshan and Daniel Formoso and Karol Langner and Tian Shi and Huijian Li and Yumeya Yamamori and Amayika Panda and Alaa Saade and Angelo Scorza Scarpati and Chris Breaux and CJ Carey and Zongwei Zhou and Cho-Jui Hsieh and Sophie Bridgers and Alena Butryna and Nishesh Gupta and Vaibhav Tulsyan and Sanghyun Woo and Evgenii Eltyshev and Will Grathwohl and Chanel Parks and Seth Benjamin and Rina Panigrahy and Shenil Dodhia and Daniel De Freitas and Chris Sauer and Will Song and Ferran Alet and Jackson Tolins and Cosmin Paduraru and Xingyi Zhou and Brian Albert and Zizhao Zhang and Lei Shu and Mudit Bansal and Sarah Nguyen and Amir Globerson and Owen Xiao and James Manyika and Tom Hennigan and Rong Rong and Josip Matak and Anton Bakalov and Ankur Sharma and Danila Sinopalnikov and Andrew Pierson and Stephen Roller and Geoff Brown and Mingcen Gao and Toshiyuki Fukuzawa and Amin Ghafouri and Kenny Vassigh and Iain Barr and Zhicheng Wang and Anna Korsun and Rajesh Jayaram and Lijie Ren and Tim Zaman and Samira Khan and Yana Lunts and Dan Deutsch and Dave Uthus and Nitzan Katz and Masha Samsikova and Amr Khalifa and Nikhil Sethi and Jiao Sun and Luming Tang and Uri Alon and Xianghong Luo and Dian Yu and Abhishek Nayyar and Bryce Petrini and Will Truong and Vincent Hellendoorn and Nikolai Chinaev and Chris Alberti and Wei Wang and Jingcao Hu and Vahab Mirrokni and Ananth Balashankar and Avia Aharon and Aahil Mehta and Ahmet Iscen and Joseph Kready and Lucas Manning and Anhad Mohananey and Yuankai Chen and Anshuman Tripathi and Allen Wu and Igor Petrovski and Dawsen Hwang and Martin Baeuml and Shreyas Chandrakaladharan and Yuan Liu and Rey Coaguila and Maxwell Chen and Sally Ma and Pouya Tafti and Susheel Tatineni and Terry Spitz and Jiayu Ye and Paul Vicol and Mihaela Rosca and Adri\u00e0 Puigdom\u00e8nech and Zohar Yahav and Sanjay Ghemawat and Hanzhao Lin and Phoebe Kirk and Zaid Nabulsi and Sergey Brin and Bernd Bohnet and Ken Caluwaerts and Aditya Srikanth Veerubhotla and Dan Zheng and Zihang Dai and Petre Petrov and Yichong Xu and Ramin Mehran and Zhuo Xu and Luisa Zintgraf and Jiho Choi and Spurthi Amba Hombaiah and Romal Thoppilan and Sashank Reddi and Lukasz Lew and Li Li and Kellie Webster and KP Sawhney and Lampros Lamprou and Siamak Shakeri and Mayank Lunayach and Jianmin Chen and Sumit Bagri and Alex Salcianu and Ying Chen and Yani Donchev and Charlotte Magister and Signe N\u00f8rly and Vitor Rodrigues and Tomas Izo and Hila Noga and Joe Zou and Thomas K\u00f6ppe and Wenxuan Zhou and Kenton Lee and Xiangzhu Long and Danielle Eisenbud and Anthony Chen and Connor Schenck and Chi Ming To and Peilin Zhong and Emanuel Taropa and Minh Truong and Omer Levy and Danilo Martins and Zhiyuan Zhang and Christopher Semturs and Kelvin Zhang and Alex Yakubovich and Pol Moreno and Lara McConnaughey and Di Lu and Sam Redmond and Lotte Weerts and Yonatan Bitton and Tiziana Refice and Nicolas Lacasse and Arthur Conmy and Corentin Tallec and Julian Odell and Hannah Forbes-Pollard and Arkadiusz Socala and Jonathan Hoech and Pushmeet Kohli and Alanna Walton and Rui Wang and Mikita Sazanovich and Kexin Zhu and Andrei Kapishnikov and Rich Galt and Matthew Denton and Ben Murdoch and Caitlin Sikora and Kareem Mohamed and Wei Wei and Uri First and Tim McConnell and Luis C. Cobo and James Qin and Thi Avrahami and Daniel Balle and Yu Watanabe and Annie Louis and Adam Kraft and Setareh Ariafar and Yiming Gu and Eug\u00e9nie Rives and Charles Yoon and Andrei Rusu and James Cobon-Kerr and Chris Hahn and Jiaming Luo and  Yuvein and  Zhu and Niharika Ahuja and Rodrigo Benenson and Rapha\u00ebl Lopez Kaufman and Honglin Yu and Lloyd Hightower and Junlin Zhang and Darren Ni and Lisa Anne Hendricks and Gabby Wang and Gal Yona and Lalit Jain and Pablo Barrio and Surya Bhupatiraju and Siva Velusamy and Allan Dafoe and Sebastian Riedel and Tara Thomas and Zhe Yuan and Mathias Bellaiche and Sheena Panthaplackel and Klemen Kloboves and Sarthak Jauhari and Canfer Akbulut and Todor Davchev and Evgeny Gladchenko and David Madras and Aleksandr Chuklin and Tyrone Hill and Quan Yuan and Mukundan Madhavan and Luke Leonhard and Dylan Scandinaro and Qihang Chen and Ning Niu and Arthur Douillard and Bogdan Damoc and Yasumasa Onoe and Fabian Pedregosa and Fred Bertsch and Chas Leichner and Joseph Pagadora and Jonathan Malmaud and Sameera Ponda and Andy Twigg and Oleksii Duzhyi and Jingwei Shen and Miaosen Wang and Roopal Garg and Jing Chen and Utku Evci and Jonathan Lee and Leon Liu and Koji Kojima and Masa Yamaguchi and Arunkumar Rajendran and AJ Piergiovanni and Vinodh Kumar Rajendran and Marco Fornoni and Gabriel Ibagon and Harry Ragan and Sadh MNM Khan and John Blitzer and Andrew Bunner and Guan Sun and Takahiro Kosakai and Scott Lundberg and Ndidi Elue and Kelvin Guu and SK Park and Jane Park and Arunachalam Narayanaswamy and Chengda Wu and Jayaram Mudigonda and Trevor Cohn and Hairong Mu and Ravi Kumar and Laura Graesser and Yichi Zhang and Richard Killam and Vincent Zhuang and Mai Gim\u00e9nez and Wael Al Jishi and Ruy Ley-Wild and Alex Zhai and Kazuki Osawa and Diego Cedillo and Jialu Liu and Mayank Upadhyay and Marcin Sieniek and Roshan Sharma and Tom Paine and Anelia Angelova and Sravanti Addepalli and Carolina Parada and Kingshuk Majumder and Avery Lamp and Sanjiv Kumar and Xiang Deng and Artiom Myaskovsky and Tea Saboli\u0107 and Jeffrey Dudek and Sarah York and F\u00e9lix de Chaumont Quitry and Jiazhong Nie and Dee Cattle and Alok Gunjan and Bilal Piot and Waleed Khawaja and Seojin Bang and Simon Wang and Siavash Khodadadeh and Raghavender R and Praynaa Rawlani and Richard Powell and Kevin Lee and Johannes Griesser and GS Oh and Cesar Magalhaes and Yujia Li and Simon Tokumine and Hadas Natalie Vogel and Dennis Hsu and Arturo BC and Disha Jindal and Matan Cohen and Zi Yang and Junwei Yuan and Dario de Cesare and Tony Bruguier and Jun Xu and Monica Roy and Alon Jacovi and Dan Belov and Rahul Arya and Phoenix Meadowlark and Shlomi Cohen-Ganor and Wenting Ye and Patrick Morris-Suzuki and Praseem Banzal and Gan Song and Pranavaraj Ponnuramu and Fred Zhang and George Scrivener and Salah Zaiem and Alif Raditya Rochman and Kehang Han and Badih Ghazi and Kate Lee and Shahar Drath and Daniel Suo and Antonious Girgis and Pradeep Shenoy and Duy Nguyen and Douglas Eck and Somit Gupta and Le Yan and Joao Carreira and Anmol Gulati and Ruoxin Sang and Daniil Mirylenka and Emma Cooney and Edward Chou and Mingyang Ling and Cindy Fan and Ben Coleman and Guilherme Tubone and Ravin Kumar and Jason Baldridge and Felix Hernandez-Campos and Angeliki Lazaridou and James Besley and Itay Yona and Neslihan Bulut and Quentin Wellens and AJ Pierigiovanni and Jasmine George and Richard Green and Pu Han and Connie Tao and Geoff Clark and Chong You and Abbas Abdolmaleki and Justin Fu and Tongzhou Chen and Ashwin Chaugule and Angad Chandorkar and Altaf Rahman and Will Thompson and Penporn Koanantakool and Mike Bernico and Jie Ren and Andrey Vlasov and Sergei Vassilvitskii and Maciej Kula and Yizhong Liang and Dahun Kim and Yangsibo Huang and Chengxi Ye and Dmitry Lepikhin and Wesley Helmholz", "abstract": "  In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.\n", "link": "http://arxiv.org/abs/2507.06261v5", "date": "2025-10-16", "relevancy": 2.374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%0A%20%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities&body=Title%3A%20Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%0A%20%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities%0AAuthor%3A%20Gheorghe%20Comanici%20and%20Eric%20Bieber%20and%20Mike%20Schaekermann%20and%20Ice%20Pasupat%20and%20Noveen%20Sachdeva%20and%20Inderjit%20Dhillon%20and%20Marcel%20Blistein%20and%20Ori%20Ram%20and%20Dan%20Zhang%20and%20Evan%20Rosen%20and%20Luke%20Marris%20and%20Sam%20Petulla%20and%20Colin%20Gaffney%20and%20Asaf%20Aharoni%20and%20Nathan%20Lintz%20and%20Tiago%20Cardal%20Pais%20and%20Henrik%20Jacobsson%20and%20Idan%20Szpektor%20and%20Nan-Jiang%20Jiang%20and%20Krishna%20Haridasan%20and%20Ahmed%20Omran%20and%20Nikunj%20Saunshi%20and%20Dara%20Bahri%20and%20Gaurav%20Mishra%20and%20Eric%20Chu%20and%20Toby%20Boyd%20and%20Brad%20Hekman%20and%20Aaron%20Parisi%20and%20Chaoyi%20Zhang%20and%20Kornraphop%20Kawintiranon%20and%20Tania%20Bedrax-Weiss%20and%20Oliver%20Wang%20and%20Ya%20Xu%20and%20Ollie%20Purkiss%20and%20Uri%20Mendlovic%20and%20Ila%C3%AF%20Deutel%20and%20Nam%20Nguyen%20and%20Adam%20Langley%20and%20Flip%20Korn%20and%20Lucia%20Rossazza%20and%20Alexandre%20Ram%C3%A9%20and%20Sagar%20Waghmare%20and%20Helen%20Miller%20and%20Nathan%20Byrd%20and%20Ashrith%20Sheshan%20and%20Raia%20Hadsell%20and%20Sangnie%20Bhardwaj%20and%20Pawel%20Janus%20and%20Tero%20Rissa%20and%20Dan%20Horgan%20and%20Alvin%20Abdagic%20and%20Lior%20Belenki%20and%20James%20Allingham%20and%20Anima%20Singh%20and%20Theo%20Guidroz%20and%20Srivatsan%20Srinivasan%20and%20Herman%20Schmit%20and%20Kristen%20Chiafullo%20and%20Andre%20Elisseeff%20and%20Nilpa%20Jha%20and%20Prateek%20Kolhar%20and%20Leonard%20Berrada%20and%20Frank%20Ding%20and%20Xiance%20Si%20and%20Shrestha%20Basu%20Mallick%20and%20Franz%20Och%20and%20Sofia%20Erell%20and%20Eric%20Ni%20and%20Tejasi%20Latkar%20and%20Sherry%20Yang%20and%20Petar%20Sirkovic%20and%20Ziqiang%20Feng%20and%20Robert%20Leland%20and%20Rachel%20Hornung%20and%20Gang%20Wu%20and%20Charles%20Blundell%20and%20Hamidreza%20Alvari%20and%20Po-Sen%20Huang%20and%20Cathy%20Yip%20and%20Sanja%20Deur%20and%20Li%20Liu%20and%20Gabriela%20Surita%20and%20Pablo%20Duque%20and%20Dima%20Damen%20and%20Johnson%20Jia%20and%20Arthur%20Guez%20and%20Markus%20Mircea%20and%20Animesh%20Sinha%20and%20Alberto%20Magni%20and%20Pawe%C5%82%20Stradomski%20and%20Tal%20Marian%20and%20Vlado%20Gali%C4%87%20and%20Wenhu%20Chen%20and%20Hisham%20Husain%20and%20Achintya%20Singhal%20and%20Dominik%20Grewe%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Shuang%20Song%20and%20Lorenzo%20Blanco%20and%20Leland%20Rechis%20and%20Lewis%20Ho%20and%20Rich%20Munoz%20and%20Kelvin%20Zheng%20and%20Jessica%20Hamrick%20and%20Kevin%20Mather%20and%20Hagai%20Taitelbaum%20and%20Eliza%20Rutherford%20and%20Yun%20Lei%20and%20Kuangyuan%20Chen%20and%20Anand%20Shukla%20and%20Erica%20Moreira%20and%20Eric%20Doi%20and%20Berivan%20Isik%20and%20Nir%20Shabat%20and%20Dominika%20Rogozi%C5%84ska%20and%20Kashyap%20Kolipaka%20and%20Jason%20Chang%20and%20Eugen%20Vu%C5%A1ak%20and%20Srinivasan%20Venkatachary%20and%20Shadi%20Noghabi%20and%20Tarun%20Bharti%20and%20Younghoon%20Jun%20and%20Aleksandr%20Zaks%20and%20Simon%20Green%20and%20Jeshwanth%20Challagundla%20and%20William%20Wong%20and%20Muqthar%20Mohammad%20and%20Dean%20Hirsch%20and%20Yong%20Cheng%20and%20Iftekhar%20Naim%20and%20Lev%20Proleev%20and%20Damien%20Vincent%20and%20Aayush%20Singh%20and%20Maxim%20Krikun%20and%20Dilip%20Krishnan%20and%20Zoubin%20Ghahramani%20and%20Aviel%20Atias%20and%20Rajeev%20Aggarwal%20and%20Christo%20Kirov%20and%20Dimitrios%20Vytiniotis%20and%20Christy%20Koh%20and%20Alexandra%20Chronopoulou%20and%20Pawan%20Dogra%20and%20Vlad-Doru%20Ion%20and%20Gladys%20Tyen%20and%20Jason%20Lee%20and%20Felix%20Weissenberger%20and%20Trevor%20Strohman%20and%20Ashwin%20Balakrishna%20and%20Jack%20Rae%20and%20Marko%20Velic%20and%20Raoul%20de%20Liedekerke%20and%20Oded%20Elyada%20and%20Wentao%20Yuan%20and%20Canoee%20Liu%20and%20Lior%20Shani%20and%20Sergey%20Kishchenko%20and%20Bea%20Alessio%20and%20Yandong%20Li%20and%20Richard%20Song%20and%20Sam%20Kwei%20and%20Orion%20Jankowski%20and%20Aneesh%20Pappu%20and%20Youhei%20Namiki%20and%20Yenai%20Ma%20and%20Nilesh%20Tripuraneni%20and%20Colin%20Cherry%20and%20Marissa%20Ikonomidis%20and%20Yu-Cheng%20Ling%20and%20Colin%20Ji%20and%20Beka%20Westberg%20and%20Auriel%20Wright%20and%20Da%20Yu%20and%20David%20Parkinson%20and%20Swaroop%20Ramaswamy%20and%20Jerome%20Connor%20and%20Soheil%20Hassas%20Yeganeh%20and%20Snchit%20Grover%20and%20George%20Kenwright%20and%20Lubo%20Litchev%20and%20Chris%20Apps%20and%20Alex%20Tomala%20and%20Felix%20Halim%20and%20Alex%20Castro-Ros%20and%20Zefei%20Li%20and%20Anudhyan%20Boral%20and%20Pauline%20Sho%20and%20Michal%20Yarom%20and%20Eric%20Malmi%20and%20David%20Klinghoffer%20and%20Rebecca%20Lin%20and%20Alan%20Ansell%20and%20Pradeep%20Kumar%20S%20and%20Shubin%20Zhao%20and%20Siqi%20Zuo%20and%20Adam%20Santoro%20and%20Heng-Tze%20Cheng%20and%20Solomon%20Demmessie%20and%20Yuchi%20Liu%20and%20Nicole%20Brichtova%20and%20Allie%20Culp%20and%20Nathaniel%20Braun%20and%20Dan%20Graur%20and%20Will%20Ng%20and%20Nikhil%20Mehta%20and%20Aaron%20Phillips%20and%20Patrik%20Sundberg%20and%20Varun%20Godbole%20and%20Fangyu%20Liu%20and%20Yash%20Katariya%20and%20David%20Rim%20and%20Mojtaba%20Seyedhosseini%20and%20Sean%20Ammirati%20and%20Jonas%20Valfridsson%20and%20Mahan%20Malihi%20and%20Timothy%20Knight%20and%20Andeep%20Toor%20and%20Thomas%20Lampe%20and%20Abe%20Ittycheriah%20and%20Lewis%20Chiang%20and%20Chak%20Yeung%20and%20Alexandre%20Fr%C3%A9chette%20and%20Jinmeng%20Rao%20and%20Huisheng%20Wang%20and%20Himanshu%20Srivastava%20and%20Richard%20Zhang%20and%20Rocky%20Rhodes%20and%20Ariel%20Brand%20and%20Dean%20Weesner%20and%20Ilya%20Figotin%20and%20Felix%20Gimeno%20and%20Rachana%20Fellinger%20and%20Pierre%20Marcenac%20and%20Jos%C3%A9%20Leal%20and%20Eyal%20Marcus%20and%20Victor%20Cotruta%20and%20Rodrigo%20Cabrera%20and%20Sheryl%20Luo%20and%20Dan%20Garrette%20and%20Vera%20Axelrod%20and%20Sorin%20Baltateanu%20and%20David%20Barker%20and%20Dongkai%20Chen%20and%20Horia%20Toma%20and%20Ben%20Ingram%20and%20Jason%20Riesa%20and%20Chinmay%20Kulkarni%20and%20Yujing%20Zhang%20and%20Hongbin%20Liu%20and%20Chao%20Wang%20and%20Martin%20Polacek%20and%20Will%20Wu%20and%20Kai%20Hui%20and%20Adrian%20N%20Reyes%20and%20Yi%20Su%20and%20Megan%20Barnes%20and%20Ishaan%20Malhi%20and%20Anfal%20Siddiqui%20and%20Qixuan%20Feng%20and%20Mihai%20Damaschin%20and%20Daniele%20Pighin%20and%20Andreas%20Steiner%20and%20Samuel%20Yang%20and%20Ramya%20Sree%20Boppana%20and%20Simeon%20Ivanov%20and%20Arun%20Kandoor%20and%20Aditya%20Shah%20and%20Asier%20Mujika%20and%20Da%20Huang%20and%20Christopher%20A.%20Choquette-Choo%20and%20Mohak%20Patel%20and%20Tianhe%20Yu%20and%20Toni%20Creswell%20and%20%20Jerry%20and%20%20Liu%20and%20Catarina%20Barros%20and%20Yasaman%20Razeghi%20and%20Aurko%20Roy%20and%20Phil%20Culliton%20and%20Binbin%20Xiong%20and%20Jiaqi%20Pan%20and%20Thomas%20Strohmann%20and%20Tolly%20Powell%20and%20Babi%20Seal%20and%20Doug%20DeCarlo%20and%20Pranav%20Shyam%20and%20Kaan%20Katircioglu%20and%20Xuezhi%20Wang%20and%20Cassidy%20Hardin%20and%20Immanuel%20Odisho%20and%20Josef%20Broder%20and%20Oscar%20Chang%20and%20Arun%20Nair%20and%20Artem%20Shtefan%20and%20Maura%20O%27Brien%20and%20Manu%20Agarwal%20and%20Sahitya%20Potluri%20and%20Siddharth%20Goyal%20and%20Amit%20Jhindal%20and%20Saksham%20Thakur%20and%20Yury%20Stuken%20and%20James%20Lyon%20and%20Kristina%20Toutanova%20and%20Fangxiaoyu%20Feng%20and%20Austin%20Wu%20and%20Ben%20Horn%20and%20Alek%20Wang%20and%20Alex%20Cullum%20and%20Gabe%20Taubman%20and%20Disha%20Shrivastava%20and%20Chongyang%20Shi%20and%20Hamish%20Tomlinson%20and%20Roma%20Patel%20and%20Tao%20Tu%20and%20Ada%20Maksutaj%20Oflazer%20and%20Francesco%20Pongetti%20and%20Mingyao%20Yang%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20Vincent%20Perot%20and%20Nuo%20Wang%20Pierse%20and%20Feng%20Han%20and%20Yoel%20Drori%20and%20I%C3%B1aki%20Iturrate%20and%20Ayan%20Chakrabarti%20and%20Legg%20Yeung%20and%20Dave%20Dopson%20and%20Yi-ting%20Chen%20and%20Apoorv%20Kulshreshtha%20and%20Tongfei%20Guo%20and%20Philip%20Pham%20and%20Tal%20Schuster%20and%20Junquan%20Chen%20and%20Alex%20Polozov%20and%20Jinwei%20Xing%20and%20Huanjie%20Zhou%20and%20Praneeth%20Kacham%20and%20Doron%20Kukliansky%20and%20Antoine%20Miech%20and%20Sergey%20Yaroshenko%20and%20Ed%20Chi%20and%20Sholto%20Douglas%20and%20Hongliang%20Fei%20and%20Mathieu%20Blondel%20and%20Preethi%20Myla%20and%20Lior%20Madmoni%20and%20Xing%20Wu%20and%20Daniel%20Keysers%20and%20Kristian%20Kjems%20and%20Isabela%20Albuquerque%20and%20Lijun%20Yu%20and%20Joel%20D%27sa%20and%20Michelle%20Plantan%20and%20Vlad%20Ionescu%20and%20Jaume%20Sanchez%20Elias%20and%20Abhirut%20Gupta%20and%20Manish%20Reddy%20Vuyyuru%20and%20Fred%20Alcober%20and%20Tong%20Zhou%20and%20Kaiyang%20Ji%20and%20Florian%20Hartmann%20and%20Subha%20Puttagunta%20and%20Hugo%20Song%20and%20Ehsan%20Amid%20and%20Anca%20Stefanoiu%20and%20Andrew%20Lee%20and%20Paul%20Pucciarelli%20and%20Emma%20Wang%20and%20Amit%20Raul%20and%20Slav%20Petrov%20and%20Isaac%20Tian%20and%20Valentin%20Anklin%20and%20Nana%20Nti%20and%20Victor%20Gomes%20and%20Max%20Schumacher%20and%20Grace%20Vesom%20and%20Alex%20Panagopoulos%20and%20Konstantinos%20Bousmalis%20and%20Daniel%20Andor%20and%20Josh%20Jacob%20and%20Yuan%20Zhang%20and%20Bill%20Rosgen%20and%20Matija%20Kecman%20and%20Matthew%20Tung%20and%20Alexandra%20Belias%20and%20Noah%20Goodman%20and%20Paul%20Covington%20and%20Brian%20Wieder%20and%20Nikita%20Saxena%20and%20Elnaz%20Davoodi%20and%20Muhuan%20Huang%20and%20Sharath%20Maddineni%20and%20Vincent%20Roulet%20and%20Folawiyo%20Campbell-Ajala%20and%20Pier%20Giuseppe%20Sessa%20and%20%20Xintian%20and%20%20Wu%20and%20Guangda%20Lai%20and%20Paul%20Collins%20and%20Alex%20Haig%20and%20Vytenis%20Sakenas%20and%20Xiaowei%20Xu%20and%20Marissa%20Giustina%20and%20Laurent%20El%20Shafey%20and%20Pichi%20Charoenpanit%20and%20Shefali%20Garg%20and%20Joshua%20Ainslie%20and%20Boone%20Severson%20and%20Montse%20Gonzalez%20Arenas%20and%20Shreya%20Pathak%20and%20Sujee%20Rajayogam%20and%20Jie%20Feng%20and%20Michiel%20Bakker%20and%20Sheng%20Li%20and%20Nevan%20Wichers%20and%20Jamie%20Rogers%20and%20Xinyang%20Geng%20and%20Yeqing%20Li%20and%20Rolf%20Jagerman%20and%20Chao%20Jia%20and%20Nadav%20Olmert%20and%20David%20Sharon%20and%20Matthew%20Mauger%20and%20Sandeep%20Mariserla%20and%20Hongxu%20Ma%20and%20Megha%20Mohabey%20and%20Kyuyeun%20Kim%20and%20Alek%20Andreev%20and%20Scott%20Pollom%20and%20Juliette%20Love%20and%20Vihan%20Jain%20and%20Priyanka%20Agrawal%20and%20Yannick%20Schroecker%20and%20Alisa%20Fortin%20and%20Manfred%20Warmuth%20and%20Ji%20Liu%20and%20Andrew%20Leach%20and%20Irina%20Blok%20and%20Ganesh%20Poomal%20Girirajan%20and%20Roee%20Aharoni%20and%20Benigno%20Uria%20and%20Andrei%20Sozanschi%20and%20Dan%20Goldberg%20and%20Lucian%20Ionita%20and%20Marco%20Tulio%20Ribeiro%20and%20Martin%20Zlocha%20and%20Vighnesh%20Birodkar%20and%20Sami%20Lachgar%20and%20Liangzhe%20Yuan%20and%20Himadri%20Choudhury%20and%20Matt%20Ginsberg%20and%20Fei%20Zheng%20and%20Gregory%20Dibb%20and%20Emily%20Graves%20and%20Swachhand%20Lokhande%20and%20Gabriel%20Rasskin%20and%20George-Cristian%20Muraru%20and%20Corbin%20Quick%20and%20Sandeep%20Tata%20and%20Pierre%20Sermanet%20and%20Aditya%20Chawla%20and%20Itay%20Karo%20and%20Yan%20Wang%20and%20Susan%20Zhang%20and%20Orgad%20Keller%20and%20Anca%20Dragan%20and%20Guolong%20Su%20and%20Ian%20Chou%20and%20Xi%20Liu%20and%20Yiqing%20Tao%20and%20Shruthi%20Prabhakara%20and%20Marc%20Wilson%20and%20Ruibo%20Liu%20and%20Shibo%20Wang%20and%20Georgie%20Evans%20and%20David%20Du%20and%20Alfonso%20Casta%C3%B1o%20and%20Gautam%20Prasad%20and%20Mona%20El%20Mahdy%20and%20Sebastian%20Gerlach%20and%20Machel%20Reid%20and%20Jarrod%20Kahn%20and%20Amir%20Zait%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Thatcher%20Ulrich%20and%20Guanyu%20Wang%20and%20Jan%20Wassenberg%20and%20Efrat%20Farkash%20and%20Kiran%20Yalasangi%20and%20Congchao%20Wang%20and%20Maria%20Bauza%20and%20Simon%20Bucher%20and%20Ting%20Liu%20and%20Jun%20Yan%20and%20Gary%20Leung%20and%20Vikas%20Sindhwani%20and%20Parker%20Barnes%20and%20Avi%20Singh%20and%20Ivan%20Jurin%20and%20Jichuan%20Chang%20and%20Niket%20Kumar%20Bhumihar%20and%20Sivan%20Eiger%20and%20Gui%20Citovsky%20and%20Ben%20Withbroe%20and%20Zhang%20Li%20and%20Siyang%20Xue%20and%20Niccol%C3%B2%20Dal%20Santo%20and%20Georgi%20Stoyanov%20and%20Yves%20Raimond%20and%20Steven%20Zheng%20and%20Yilin%20Gao%20and%20V%C3%ADt%20List%C3%ADk%20and%20S%C5%82awek%20Kwasiborski%20and%20Rachel%20Saputro%20and%20Adnan%20Ozturel%20and%20Ganesh%20Mallya%20and%20Kushal%20Majmundar%20and%20Ross%20West%20and%20Paul%20Caron%20and%20Jinliang%20Wei%20and%20Lluis%20Castrejon%20and%20Sharad%20Vikram%20and%20Deepak%20Ramachandran%20and%20Nikhil%20Dhawan%20and%20Jiho%20Park%20and%20Sara%20Smoot%20and%20George%20van%20den%20Driessche%20and%20Yochai%20Blau%20and%20Chase%20Malik%20and%20Wei%20Liang%20and%20Roy%20Hirsch%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Eugene%20Weinstein%20and%20A%C3%A4ron%20van%20den%20Oord%20and%20Sid%20Lall%20and%20Nicholas%20FitzGerald%20and%20Zixuan%20Jiang%20and%20Xuan%20Yang%20and%20Dale%20Webster%20and%20Ali%20Elqursh%20and%20Aedan%20Pope%20and%20Georges%20Rotival%20and%20David%20Raposo%20and%20Wanzheng%20Zhu%20and%20Jeff%20Dean%20and%20Sami%20Alabed%20and%20Dustin%20Tran%20and%20Arushi%20Gupta%20and%20Zach%20Gleicher%20and%20Jessica%20Austin%20and%20Edouard%20Rosseel%20and%20Megh%20Umekar%20and%20Dipanjan%20Das%20and%20Yinghao%20Sun%20and%20Kai%20Chen%20and%20Karolis%20Misiunas%20and%20Xiang%20Zhou%20and%20Yixian%20Di%20and%20Alyssa%20Loo%20and%20Josh%20Newlan%20and%20Bo%20Li%20and%20Vinay%20Ramasesh%20and%20Ying%20Xu%20and%20Alex%20Chen%20and%20Sudeep%20Gandhe%20and%20Radu%20Soricut%20and%20Nikita%20Gupta%20and%20Shuguang%20Hu%20and%20Seliem%20El-Sayed%20and%20Xavier%20Garcia%20and%20Idan%20Brusilovsky%20and%20Pu-Chin%20Chen%20and%20Andrew%20Bolt%20and%20Lu%20Huang%20and%20Alex%20Gurney%20and%20Zhiying%20Zhang%20and%20Alexander%20Pritzel%20and%20Jarek%20Wilkiewicz%20and%20Bryan%20Seybold%20and%20Bhargav%20Kanagal%20Shamanna%20and%20Felix%20Fischer%20and%20Josef%20Dean%20and%20Karan%20Gill%20and%20Ross%20Mcilroy%20and%20Abhishek%20Bhowmick%20and%20Jeremy%20Selier%20and%20Antoine%20Yang%20and%20Derek%20Cheng%20and%20Vladimir%20Magay%20and%20Jie%20Tan%20and%20Dhriti%20Varma%20and%20Christian%20Walder%20and%20Tomas%20Kocisky%20and%20Ryo%20Nakashima%20and%20Paul%20Natsev%20and%20Mike%20Kwong%20and%20Ionel%20Gog%20and%20Chiyuan%20Zhang%20and%20Sander%20Dieleman%20and%20Thomas%20Jimma%20and%20Andrey%20Ryabtsev%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Dayou%20Du%20and%20Ante%20%C5%BDu%C5%BEul%20and%20Mislav%20%C5%BDani%C4%87%20and%20Mukund%20Raghavachari%20and%20Willi%20Gierke%20and%20Zeyu%20Zheng%20and%20Dessie%20Petrova%20and%20Yann%20Dauphin%20and%20Yuchuan%20Liu%20and%20Ido%20Kessler%20and%20Steven%20Hand%20and%20Chris%20Duvarney%20and%20Seokhwan%20Kim%20and%20Hyo%20Lee%20and%20L%C3%A9onard%20Hussenot%20and%20Jeffrey%20Hui%20and%20Josh%20Smith%20and%20Deepali%20Jain%20and%20Jiawei%20Xia%20and%20Gaurav%20Singh%20Tomar%20and%20Keyvan%20Amiri%20and%20Du%20Phan%20and%20Fabian%20Fuchs%20and%20Tobias%20Weyand%20and%20Nenad%20Tomasev%20and%20Alexandra%20Cordell%20and%20Xin%20Liu%20and%20Jonathan%20Mallinson%20and%20Pankaj%20Joshi%20and%20Andy%20Crawford%20and%20Arun%20Suggala%20and%20Steve%20Chien%20and%20Nick%20Fernando%20and%20Mariella%20Sanchez-Vargas%20and%20Duncan%20Williams%20and%20Phil%20Crone%20and%20Xiyang%20Luo%20and%20Igor%20Karpov%20and%20Jyn%20Shan%20and%20Terry%20Thurk%20and%20Robin%20Strudel%20and%20Paul%20Voigtlaender%20and%20Piyush%20Patil%20and%20Tim%20Dozat%20and%20Ali%20Khodaei%20and%20Sahil%20Singla%20and%20Piotr%20Ambroszczyk%20and%20Qiyin%20Wu%20and%20Yifan%20Chang%20and%20Brian%20Roark%20and%20Chaitra%20Hegde%20and%20Tianli%20Ding%20and%20Angelos%20Filos%20and%20Zhongru%20Wu%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Shuang%20Liu%20and%20Saarthak%20Khanna%20and%20Aditya%20Pandey%20and%20Siobhan%20Mcloughlin%20and%20Qiujia%20Li%20and%20Sam%20Haves%20and%20Allan%20Zhou%20and%20Elena%20Buchatskaya%20and%20Isabel%20Leal%20and%20Peter%20de%20Boursac%20and%20Nami%20Akazawa%20and%20Nina%20Anderson%20and%20Terry%20Chen%20and%20Krishna%20Somandepalli%20and%20Chen%20Liang%20and%20Sheela%20Goenka%20and%20Stephanie%20Winkler%20and%20Alexander%20Grushetsky%20and%20Yifan%20Ding%20and%20Jamie%20Smith%20and%20Fan%20Ye%20and%20Jordi%20Pont-Tuset%20and%20Eric%20Li%20and%20Ruichao%20Li%20and%20Tomer%20Golany%20and%20Dawid%20Wegner%20and%20Tao%20Jiang%20and%20Omer%20Barak%20and%20Yuan%20Shangguan%20and%20Eszter%20V%C3%A9rtes%20and%20Renee%20Wong%20and%20J%C3%B6rg%20Bornschein%20and%20Alex%20Tudor%20and%20Michele%20Bevilacqua%20and%20Tom%20Schaul%20and%20Ankit%20Singh%20Rawat%20and%20Yang%20Zhao%20and%20Kyriakos%20Axiotis%20and%20Lei%20Meng%20and%20Cory%20McLean%20and%20Jonathan%20Lai%20and%20Jennifer%20Beattie%20and%20Nate%20Kushman%20and%20Yaxin%20Liu%20and%20Blair%20Kutzman%20and%20Fiona%20Lang%20and%20Jingchen%20Ye%20and%20Praneeth%20Netrapalli%20and%20Pushkar%20Mishra%20and%20Myriam%20Khan%20and%20Megha%20Goel%20and%20Rob%20Willoughby%20and%20David%20Tian%20and%20Honglei%20Zhuang%20and%20JD%20Chen%20and%20Zak%20Tsai%20and%20Tasos%20Kementsietsidis%20and%20Arjun%20Khare%20and%20James%20Keeling%20and%20Keyang%20Xu%20and%20Nathan%20Waters%20and%20Florent%20Altch%C3%A9%20and%20Ashok%20Popat%20and%20Bhavishya%20Mittal%20and%20David%20Saxton%20and%20Dalia%20El%20Badawy%20and%20Michael%20Mathieu%20and%20Zheng%20Zheng%20and%20Hao%20Zhou%20and%20Nishant%20Ranka%20and%20Richard%20Shin%20and%20Qingnan%20Duan%20and%20Tim%20Salimans%20and%20Ioana%20Mihailescu%20and%20Uri%20Shaham%20and%20Ming-Wei%20Chang%20and%20Yannis%20Assael%20and%20Nishanth%20Dikkala%20and%20Martin%20Izzard%20and%20Vincent%20Cohen-Addad%20and%20Cat%20Graves%20and%20Vlad%20Feinberg%20and%20Grace%20Chung%20and%20DJ%20Strouse%20and%20Danny%20Karmon%20and%20Sahand%20Sharifzadeh%20and%20Zoe%20Ashwood%20and%20Khiem%20Pham%20and%20Jon%20Blanton%20and%20Alex%20Vasiloff%20and%20Jarred%20Barber%20and%20Mark%20Geller%20and%20Aurick%20Zhou%20and%20Fedir%20Zubach%20and%20Tzu-Kuo%20Huang%20and%20Lei%20Zhang%20and%20Himanshu%20Gupta%20and%20Matt%20Young%20and%20Julia%20Proskurnia%20and%20Ronny%20Votel%20and%20Valentin%20Gabeur%20and%20Gabriel%20Barcik%20and%20Aditya%20Tripathi%20and%20Hongkun%20Yu%20and%20Geng%20Yan%20and%20Beer%20Changpinyo%20and%20Filip%20Paveti%C4%87%20and%20Amy%20Coyle%20and%20Yasuhisa%20Fujii%20and%20Jorge%20Gonzalez%20Mendez%20and%20Tianhao%20Zhou%20and%20Harish%20Rajamani%20and%20Blake%20Hechtman%20and%20Eddie%20Cao%20and%20Da-Cheng%20Juan%20and%20Yi-Xuan%20Tan%20and%20Valentin%20Dalibard%20and%20Yilun%20Du%20and%20Natalie%20Clay%20and%20Kaisheng%20Yao%20and%20Wenhao%20Jia%20and%20Dimple%20Vijaykumar%20and%20Yuxiang%20Zhou%20and%20Xinyi%20Bai%20and%20Wei-Chih%20Hung%20and%20Steven%20Pecht%20and%20Georgi%20Todorov%20and%20Nikhil%20Khadke%20and%20Pramod%20Gupta%20and%20Preethi%20Lahoti%20and%20Arnaud%20Autef%20and%20Karthik%20Duddu%20and%20James%20Lee-Thorp%20and%20Alexander%20Bykovsky%20and%20Tautvydas%20Misiunas%20and%20Sebastian%20Flennerhag%20and%20Santhosh%20Thangaraj%20and%20Jed%20McGiffin%20and%20Zack%20Nado%20and%20Markus%20Kunesch%20and%20Andreas%20Noever%20and%20Amir%20Hertz%20and%20Marco%20Liang%20and%20Victor%20Stone%20and%20Evan%20Palmer%20and%20Samira%20Daruki%20and%20Arijit%20Pramanik%20and%20Siim%20P%C3%B5der%20and%20Austin%20Kyker%20and%20Mina%20Khan%20and%20Evgeny%20Sluzhaev%20and%20Marvin%20Ritter%20and%20Avraham%20Ruderman%20and%20Wenlei%20Zhou%20and%20Chirag%20Nagpal%20and%20Kiran%20Vodrahalli%20and%20George%20Necula%20and%20Paul%20Barham%20and%20Ellie%20Pavlick%20and%20Jay%20Hartford%20and%20Izhak%20Shafran%20and%20Long%20Zhao%20and%20Maciej%20Miku%C5%82a%20and%20Tom%20Eccles%20and%20Hidetoshi%20Shimokawa%20and%20Kanav%20Garg%20and%20Luke%20Vilnis%20and%20Hanwen%20Chen%20and%20Ilia%20Shumailov%20and%20Kuang-Huei%20Lee%20and%20Abdelrahman%20Abdelhamed%20and%20Meiyan%20Xie%20and%20Vered%20Cohen%20and%20Ester%20Hlavnova%20and%20Dan%20Malkin%20and%20Chawin%20Sitawarin%20and%20James%20Lottes%20and%20Pauline%20Coquinot%20and%20Tianli%20Yu%20and%20Sandeep%20Kumar%20and%20Jingwei%20Zhang%20and%20Aroma%20Mahendru%20and%20Zafarali%20Ahmed%20and%20James%20Martens%20and%20Tao%20Chen%20and%20Aviel%20Boag%20and%20Daiyi%20Peng%20and%20Coline%20Devin%20and%20Arseniy%20Klimovskiy%20and%20Mary%20Phuong%20and%20Danny%20Vainstein%20and%20Jin%20Xie%20and%20Bhuvana%20Ramabhadran%20and%20Nathan%20Howard%20and%20Xinxin%20Yu%20and%20Gitartha%20Goswami%20and%20Jingyu%20Cui%20and%20Sam%20Shleifer%20and%20Mario%20Pinto%20and%20Chih-Kuan%20Yeh%20and%20Ming-Hsuan%20Yang%20and%20Sara%20Javanmardi%20and%20Dan%20Ethier%20and%20Chace%20Lee%20and%20Jordi%20Orbay%20and%20Suyog%20Kotecha%20and%20Carla%20Bromberg%20and%20Pete%20Shaw%20and%20James%20Thornton%20and%20Adi%20Gerzi%20Rosenthal%20and%20Shane%20Gu%20and%20Matt%20Thomas%20and%20Ian%20Gemp%20and%20Aditya%20Ayyar%20and%20Asahi%20Ushio%20and%20Aarush%20Selvan%20and%20Joel%20Wee%20and%20Chenxi%20Liu%20and%20Maryam%20Majzoubi%20and%20Weiren%20Yu%20and%20Jake%20Abernethy%20and%20Tyler%20Liechty%20and%20Renke%20Pan%20and%20Hoang%20Nguyen%20and%20%20Qiong%20and%20%20Hu%20and%20Sarah%20Perrin%20and%20Abhinav%20Arora%20and%20Emily%20Pitler%20and%20Weiyi%20Wang%20and%20Kaushik%20Shivakumar%20and%20Flavien%20Prost%20and%20Ben%20Limonchik%20and%20Jing%20Wang%20and%20Yi%20Gao%20and%20Timothee%20Cour%20and%20Shyamal%20Buch%20and%20Huan%20Gui%20and%20Maria%20Ivanova%20and%20Philipp%20Neubeck%20and%20Kelvin%20Chan%20and%20Lucy%20Kim%20and%20Huizhong%20Chen%20and%20Naman%20Goyal%20and%20Da-Woon%20Chung%20and%20Lu%20Liu%20and%20Yao%20Su%20and%20Anastasia%20Petrushkina%20and%20Jiajun%20Shen%20and%20Armand%20Joulin%20and%20Yuanzhong%20Xu%20and%20Stein%20Xudong%20Lin%20and%20Yana%20Kulizhskaya%20and%20Ciprian%20Chelba%20and%20Shobha%20Vasudevan%20and%20Eli%20Collins%20and%20Vasilisa%20Bashlovkina%20and%20Tony%20Lu%20and%20Doug%20Fritz%20and%20Jongbin%20Park%20and%20Yanqi%20Zhou%20and%20Chen%20Su%20and%20Richard%20Tanburn%20and%20Mikhail%20Sushkov%20and%20Mitchelle%20Rasquinha%20and%20Jinning%20Li%20and%20Jennifer%20Prendki%20and%20Yiming%20Li%20and%20Pallavi%20LV%20and%20Shriya%20Sharma%20and%20Hen%20Fitoussi%20and%20Hui%20Huang%20and%20Andrew%20Dai%20and%20Phuong%20Dao%20and%20Mike%20Burrows%20and%20Henry%20Prior%20and%20Danfeng%20Qin%20and%20Golan%20Pundak%20and%20Lars%20Lowe%20Sjoesund%20and%20Art%20Khurshudov%20and%20Zhenkai%20Zhu%20and%20Albert%20Webson%20and%20Elizabeth%20Kemp%20and%20Tat%20Tan%20and%20Saurabh%20Agrawal%20and%20Susie%20Sargsyan%20and%20Liqun%20Cheng%20and%20Jim%20Stephan%20and%20Tom%20Kwiatkowski%20and%20David%20Reid%20and%20Arunkumar%20Byravan%20and%20Assaf%20Hurwitz%20Michaely%20and%20Nicolas%20Heess%20and%20Luowei%20Zhou%20and%20Sonam%20Goenka%20and%20Viral%20Carpenter%20and%20Anselm%20Levskaya%20and%20Bo%20Wang%20and%20Reed%20Roberts%20and%20R%C3%A9mi%20Leblond%20and%20Sharat%20Chikkerur%20and%20Stav%20Ginzburg%20and%20Max%20Chang%20and%20Robert%20Riachi%20and%20%20Chuqiao%20and%20%20Xu%20and%20Zal%C3%A1n%20Borsos%20and%20Michael%20Pliskin%20and%20Julia%20Pawar%20and%20Morgane%20Lustman%20and%20Hannah%20Kirkwood%20and%20Ankit%20Anand%20and%20Aditi%20Chaudhary%20and%20Norbert%20Kalb%20and%20Kieran%20Milan%20and%20Sean%20Augenstein%20and%20Anna%20Goldie%20and%20Laurel%20Prince%20and%20Karthik%20Raman%20and%20Yanhua%20Sun%20and%20Vivian%20Xia%20and%20Aaron%20Cohen%20and%20Zhouyuan%20Huo%20and%20Josh%20Camp%20and%20Seher%20Ellis%20and%20Lukas%20Zilka%20and%20David%20Vilar%20Torres%20and%20Lisa%20Patel%20and%20Sho%20Arora%20and%20Betty%20Chan%20and%20Jonas%20Adler%20and%20Kareem%20Ayoub%20and%20Jacky%20Liang%20and%20Fayaz%20Jamil%20and%20Jiepu%20Jiang%20and%20Simon%20Baumgartner%20and%20Haitian%20Sun%20and%20Yael%20Karov%20and%20Yaroslav%20Akulov%20and%20Hui%20Zheng%20and%20Irene%20Cai%20and%20Claudio%20Fantacci%20and%20James%20Rubin%20and%20Alex%20Rav%20Acha%20and%20Mengchao%20Wang%20and%20Nina%20D%27Souza%20and%20Rohit%20Sathyanarayana%20and%20Shengyang%20Dai%20and%20Simon%20Rowe%20and%20Andrey%20Simanovsky%20and%20Omer%20Goldman%20and%20Yuheng%20Kuang%20and%20Xiaoyue%20Pan%20and%20Andrew%20Rosenberg%20and%20Tania%20Rojas-Esponda%20and%20Praneet%20Dutta%20and%20Amy%20Zeng%20and%20Irina%20Jurenka%20and%20Greg%20Farquhar%20and%20Yamini%20Bansal%20and%20Shariq%20Iqbal%20and%20Becca%20Roelofs%20and%20Ga-Young%20Joung%20and%20Parker%20Beak%20and%20Changwan%20Ryu%20and%20Ryan%20Poplin%20and%20Yan%20Wu%20and%20Jean-Baptiste%20Alayrac%20and%20Senaka%20Buthpitiya%20and%20Olaf%20Ronneberger%20and%20Caleb%20Habtegebriel%20and%20Wei%20Li%20and%20Paul%20Cavallaro%20and%20Aurora%20Wei%20and%20Guy%20Bensky%20and%20Timo%20Denk%20and%20Harish%20Ganapathy%20and%20Jeff%20Stanway%20and%20Pratik%20Joshi%20and%20Francesco%20Bertolini%20and%20Jessica%20Lo%20and%20Olivia%20Ma%20and%20Zachary%20Charles%20and%20Geta%20Sampemane%20and%20Himanshu%20Sahni%20and%20Xu%20Chen%20and%20Harry%20Askham%20and%20David%20Gaddy%20and%20Peter%20Young%20and%20Jiewen%20Tan%20and%20Matan%20Eyal%20and%20Arthur%20Bra%C5%BEinskas%20and%20Li%20Zhong%20and%20Zhichun%20Wu%20and%20Mark%20Epstein%20and%20Kai%20Bailey%20and%20Andrew%20Hard%20and%20Kamyu%20Lee%20and%20Sasha%20Goldshtein%20and%20Alex%20Ruiz%20and%20Mohammed%20Badawi%20and%20Matthias%20Lochbrunner%20and%20JK%20Kearns%20and%20Ashley%20Brown%20and%20Fabio%20Pardo%20and%20Theophane%20Weber%20and%20Haichuan%20Yang%20and%20Pan-Pan%20Jiang%20and%20Berkin%20Akin%20and%20Zhao%20Fu%20and%20Marcus%20Wainwright%20and%20Chi%20Zou%20and%20Meenu%20Gaba%20and%20Pierre-Antoine%20Manzagol%20and%20Wendy%20Kan%20and%20Yang%20Song%20and%20Karina%20Zainullina%20and%20Rui%20Lin%20and%20Jeongwoo%20Ko%20and%20Salil%20Deshmukh%20and%20Apoorv%20Jindal%20and%20James%20Svensson%20and%20Divya%20Tyam%20and%20Heri%20Zhao%20and%20Christine%20Kaeser-Chen%20and%20Scott%20Baird%20and%20Pooya%20Moradi%20and%20Jamie%20Hall%20and%20Qiuchen%20Guo%20and%20Vincent%20Tsang%20and%20Bowen%20Liang%20and%20Fernando%20Pereira%20and%20Suhas%20Ganesh%20and%20Ivan%20Korotkov%20and%20Jakub%20Adamek%20and%20Sridhar%20Thiagarajan%20and%20Vinh%20Tran%20and%20Charles%20Chen%20and%20Chris%20Tar%20and%20Sanil%20Jain%20and%20Ishita%20Dasgupta%20and%20Taylan%20Bilal%20and%20David%20Reitter%20and%20Kai%20Zhao%20and%20Giulia%20Vezzani%20and%20Yasmin%20Gehman%20and%20Pulkit%20Mehta%20and%20Lauren%20Beltrone%20and%20Xerxes%20Dotiwalla%20and%20Sergio%20Guadarrama%20and%20Zaheer%20Abbas%20and%20Stefani%20Karp%20and%20Petko%20Georgiev%20and%20Chun-Sung%20Ferng%20and%20Marc%20Brockschmidt%20and%20Liqian%20Peng%20and%20Christoph%20Hirnschall%20and%20Vikas%20Verma%20and%20Yingying%20Bi%20and%20Ying%20Xiao%20and%20Avigail%20Dabush%20and%20Kelvin%20Xu%20and%20Phil%20Wallis%20and%20Randall%20Parker%20and%20Qifei%20Wang%20and%20Yang%20Xu%20and%20Ilkin%20Safarli%20and%20Dinesh%20Tewari%20and%20Yin%20Zhang%20and%20Seungyeon%20Kim%20and%20Andrea%20Gesmundo%20and%20Mackenzie%20Thomas%20and%20Sergey%20Levi%20and%20Ahmed%20Chowdhury%20and%20Kanishka%20Rao%20and%20Peter%20Garst%20and%20Sam%20Conway-Rahman%20and%20Helen%20Ran%20and%20Kay%20McKinney%20and%20Zhisheng%20Xiao%20and%20Wenhao%20Yu%20and%20Rohan%20Agrawal%20and%20Axel%20Stjerngren%20and%20Catalin%20Ionescu%20and%20Jingjing%20Chen%20and%20Vivek%20Sharma%20and%20Justin%20Chiu%20and%20Fei%20Liu%20and%20Ken%20Franko%20and%20Clayton%20Sanford%20and%20Xingyu%20Cai%20and%20Paul%20Michel%20and%20Sanjay%20Ganapathy%20and%20Jane%20Labanowski%20and%20Zachary%20Garrett%20and%20Ben%20Vargas%20and%20Sean%20Sun%20and%20Bryan%20Gale%20and%20Thomas%20Buschmann%20and%20Guillaume%20Desjardins%20and%20Nimesh%20Ghelani%20and%20Palak%20Jain%20and%20Mudit%20Verma%20and%20Chulayuth%20Asawaroengchai%20and%20Julian%20Eisenschlos%20and%20Jitendra%20Harlalka%20and%20Hideto%20Kazawa%20and%20Don%20Metzler%20and%20Joshua%20Howland%20and%20Ying%20Jian%20and%20Jake%20Ades%20and%20Viral%20Shah%20and%20Tynan%20Gangwani%20and%20Seungji%20Lee%20and%20Roman%20Ring%20and%20Steven%20M.%20Hernandez%20and%20Dean%20Reich%20and%20Amer%20Sinha%20and%20Ashutosh%20Sathe%20and%20Joe%20Kovac%20and%20Ashleah%20Gill%20and%20Ajay%20Kannan%20and%20Andrea%20D%27olimpio%20and%20Martin%20Sevenich%20and%20Jay%20Whang%20and%20Been%20Kim%20and%20Khe%20Chai%20Sim%20and%20Jilin%20Chen%20and%20Jiageng%20Zhang%20and%20Shuba%20Lall%20and%20Yossi%20Matias%20and%20Bill%20Jia%20and%20Abe%20Friesen%20and%20Sara%20Nasso%20and%20Ashish%20Thapliyal%20and%20Bryan%20Perozzi%20and%20Ting%20Yu%20and%20Anna%20Shekhawat%20and%20Safeen%20Huda%20and%20Peter%20Grabowski%20and%20Eric%20Wang%20and%20Ashwin%20Sreevatsa%20and%20Hilal%20Dib%20and%20Mehadi%20Hassen%20and%20Parker%20Schuh%20and%20Vedrana%20Milutinovic%20and%20Chris%20Welty%20and%20Michael%20Quinn%20and%20Ali%20Shah%20and%20Bangju%20Wang%20and%20Gabe%20Barth-Maron%20and%20Justin%20Frye%20and%20Natalie%20Axelsson%20and%20Tao%20Zhu%20and%20Yukun%20Ma%20and%20Irene%20Giannoumis%20and%20Hanie%20Sedghi%20and%20Chang%20Ye%20and%20Yi%20Luan%20and%20Kevin%20Aydin%20and%20Bilva%20Chandra%20and%20Vivek%20Sampathkumar%20and%20Ronny%20Huang%20and%20Victor%20Lavrenko%20and%20Ahmed%20Eleryan%20and%20Zhi%20Hong%20and%20Steven%20Hansen%20and%20Sara%20Mc%20Carthy%20and%20Bidisha%20Samanta%20and%20Domagoj%20%C4%86evid%20and%20Xin%20Wang%20and%20Fangtao%20Li%20and%20Michael%20Voznesensky%20and%20Matt%20Hoffman%20and%20Andreas%20Terzis%20and%20Vikash%20Sehwag%20and%20Gil%20Fidel%20and%20Luheng%20He%20and%20Mu%20Cai%20and%20Yanzhang%20He%20and%20Alex%20Feng%20and%20Martin%20Nikoltchev%20and%20Samrat%20Phatale%20and%20Jason%20Chase%20and%20Rory%20Lawton%20and%20Ming%20Zhang%20and%20Tom%20Ouyang%20and%20Manuel%20Tragut%20and%20Mehdi%20Hafezi%20Manshadi%20and%20Arjun%20Narayanan%20and%20Jiaming%20Shen%20and%20Xu%20Gao%20and%20Tolga%20Bolukbasi%20and%20Nick%20Roy%20and%20Xin%20Li%20and%20Daniel%20Golovin%20and%20Liviu%20Panait%20and%20Zhen%20Qin%20and%20Guangxing%20Han%20and%20Thomas%20Anthony%20and%20Sneha%20Kudugunta%20and%20Viorica%20Patraucean%20and%20Aniket%20Ray%20and%20Xinyun%20Chen%20and%20Xiaochen%20Yang%20and%20Tanuj%20Bhatia%20and%20Pranav%20Talluri%20and%20Alex%20Morris%20and%20Andrija%20Ra%C5%BEnatovi%C4%87%20and%20Bethanie%20Brownfield%20and%20James%20An%20and%20Sheng%20Peng%20and%20Patrick%20Kane%20and%20Ce%20Zheng%20and%20Nico%20Duduta%20and%20Joshua%20Kessinger%20and%20James%20Noraky%20and%20Siqi%20Liu%20and%20Keran%20Rong%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Keith%20Rush%20and%20Alex%20Goldin%20and%20Fanny%20Wei%20and%20Shiva%20Mohan%20Reddy%20Garlapati%20and%20Caroline%20Pantofaru%20and%20Okwan%20Kwon%20and%20Jianmo%20Ni%20and%20Eric%20Noland%20and%20Julia%20Di%20Trapani%20and%20Fran%C3%A7oise%20Beaufays%20and%20Abhijit%20Guha%20Roy%20and%20Yinlam%20Chow%20and%20Aybuke%20Turker%20and%20Geoffrey%20Cideron%20and%20Lantao%20Mei%20and%20Jon%20Clark%20and%20Qingyun%20Dou%20and%20Matko%20Bo%C5%A1njak%20and%20Ralph%20Leith%20and%20Yuqing%20Du%20and%20Amir%20Yazdanbakhsh%20and%20Milad%20Nasr%20and%20Chester%20Kwak%20and%20Suraj%20Satishkumar%20Sheth%20and%20Alex%20Kaskasoli%20and%20Ankesh%20Anand%20and%20Balaji%20Lakshminarayanan%20and%20Sammy%20Jerome%20and%20David%20Bieber%20and%20Chun-Te%20Chu%20and%20Alexandre%20Senges%20and%20Tianxiao%20Shen%20and%20Mukund%20Sridhar%20and%20Ndaba%20Ndebele%20and%20Benjamin%20Beyret%20and%20Shakir%20Mohamed%20and%20Mia%20Chen%20and%20Markus%20Freitag%20and%20Jiaxian%20Guo%20and%20Luyang%20Liu%20and%20Paul%20Roit%20and%20Heng%20Chen%20and%20Shen%20Yan%20and%20Tom%20Stone%20and%20JD%20Co-Reyes%20and%20Jeremy%20Cole%20and%20Salvatore%20Scellato%20and%20Shekoofeh%20Azizi%20and%20Hadi%20Hashemi%20and%20Alicia%20Jin%20and%20Anand%20Iyer%20and%20Marcella%20Valentine%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Arun%20Ahuja%20and%20Daniel%20Hernandez%20Diaz%20and%20Chen-Yu%20Lee%20and%20Nathan%20Clement%20and%20Weize%20Kong%20and%20Drew%20Garmon%20and%20Ishaan%20Watts%20and%20Kush%20Bhatia%20and%20Khyatti%20Gupta%20and%20Matt%20Miecnikowski%20and%20Hugo%20Vallet%20and%20Ankur%20Taly%20and%20Edward%20Loper%20and%20Saket%20Joshi%20and%20James%20Atwood%20and%20Jo%20Chick%20and%20Mark%20Collier%20and%20Fotis%20Iliopoulos%20and%20Ryan%20Trostle%20and%20Beliz%20Gunel%20and%20Ramiro%20Leal-Cavazos%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Michael%20Guzman%20and%20Xiaoen%20Ju%20and%20Andy%20Forbes%20and%20Jesse%20Emond%20and%20Kushal%20Chauhan%20and%20Ben%20Caine%20and%20Li%20Xiao%20and%20Wenjun%20Zeng%20and%20Alexandre%20Moufarek%20and%20Daniel%20Murphy%20and%20Maya%20Meng%20and%20Nitish%20Gupta%20and%20Felix%20Riedel%20and%20Anil%20Das%20and%20Elijah%20Lawal%20and%20Shashi%20Narayan%20and%20Tiberiu%20Sosea%20and%20James%20Swirhun%20and%20Linda%20Friso%20and%20Behnam%20Neyshabur%20and%20Jing%20Lu%20and%20Sertan%20Girgin%20and%20Michael%20Wunder%20and%20Edouard%20Yvinec%20and%20Aroonalok%20Pyne%20and%20Victor%20Carbune%20and%20Shruti%20Rijhwani%20and%20Yang%20Guo%20and%20Tulsee%20Doshi%20and%20Anton%20Briukhov%20and%20Max%20Bain%20and%20Ayal%20Hitron%20and%20Xuanhui%20Wang%20and%20Ashish%20Gupta%20and%20Ke%20Chen%20and%20Cosmo%20Du%20and%20Weiyang%20Zhang%20and%20Dhruv%20Shah%20and%20Arjun%20Akula%20and%20Max%20Dylla%20and%20Ashyana%20Kachra%20and%20Weicheng%20Kuo%20and%20Tingting%20Zou%20and%20Lily%20Wang%20and%20Luyao%20Xu%20and%20Jifan%20Zhu%20and%20Justin%20Snyder%20and%20Sachit%20Menon%20and%20Orhan%20Firat%20and%20Igor%20Mordatch%20and%20Yuan%20Yuan%20and%20Natalia%20Ponomareva%20and%20Rory%20Blevins%20and%20Lawrence%20Moore%20and%20Weijun%20Wang%20and%20Phil%20Chen%20and%20Martin%20Scholz%20and%20Artur%20Dwornik%20and%20Jason%20Lin%20and%20Sicheng%20Li%20and%20Diego%20Antognini%20and%20Te%20I%20and%20Xiaodan%20Song%20and%20Matt%20Miller%20and%20Uday%20Kalra%20and%20Adam%20Raveret%20and%20Oscar%20Akerlund%20and%20Felix%20Wu%20and%20Andrew%20Nystrom%20and%20Namrata%20Godbole%20and%20Tianqi%20Liu%20and%20Hannah%20DeBalsi%20and%20Jewel%20Zhao%20and%20Buhuang%20Liu%20and%20Avi%20Caciularu%20and%20Lauren%20Lax%20and%20Urvashi%20Khandelwal%20and%20Victoria%20Langston%20and%20Eric%20Bailey%20and%20Silvio%20Lattanzi%20and%20Yufei%20Wang%20and%20Neel%20Kovelamudi%20and%20Sneha%20Mondal%20and%20Guru%20Guruganesh%20and%20Nan%20Hua%20and%20Ofir%20Roval%20and%20Pawe%C5%82%20Weso%C5%82owski%20and%20Rishikesh%20Ingale%20and%20Jonathan%20Halcrow%20and%20Tim%20Sohn%20and%20Christof%20Angermueller%20and%20Bahram%20Raad%20and%20Eli%20Stickgold%20and%20Eva%20Lu%20and%20Alec%20Kosik%20and%20Jing%20Xie%20and%20Timothy%20Lillicrap%20and%20Austin%20Huang%20and%20Lydia%20Lihui%20Zhang%20and%20Dominik%20Paulus%20and%20Clement%20Farabet%20and%20Alex%20Wertheim%20and%20Bing%20Wang%20and%20Rishabh%20Joshi%20and%20Chu-ling%20Ko%20and%20Yonghui%20Wu%20and%20Shubham%20Agrawal%20and%20Lily%20Lin%20and%20XiangHai%20Sheng%20and%20Peter%20Sung%20and%20Tyler%20Breland-King%20and%20Christina%20Butterfield%20and%20Swapnil%20Gawde%20and%20Sumeet%20Singh%20and%20Qiao%20Zhang%20and%20Raj%20Apte%20and%20Shilpa%20Shetty%20and%20Adrian%20Hutter%20and%20Tao%20Li%20and%20Elizabeth%20Salesky%20and%20Federico%20Lebron%20and%20Jonni%20Kanerva%20and%20Michela%20Paganini%20and%20Arthur%20Nguyen%20and%20Rohith%20Vallu%20and%20Jan-Thorsten%20Peter%20and%20Sarmishta%20Velury%20and%20David%20Kao%20and%20Jay%20Hoover%20and%20Anna%20Bortsova%20and%20Colton%20Bishop%20and%20Shoshana%20Jakobovits%20and%20Alessandro%20Agostini%20and%20Alekh%20Agarwal%20and%20Chang%20Liu%20and%20Charles%20Kwong%20and%20Sasan%20Tavakkol%20and%20Ioana%20Bica%20and%20Alex%20Greve%20and%20Anirudh%20GP%20and%20Jake%20Marcus%20and%20Le%20Hou%20and%20Tom%20Duerig%20and%20Rivka%20Moroshko%20and%20Dave%20Lacey%20and%20Andy%20Davis%20and%20Julien%20Amelot%20and%20Guohui%20Wang%20and%20Frank%20Kim%20and%20Theofilos%20Strinopoulos%20and%20Hui%20Wan%20and%20Charline%20Le%20Lan%20and%20Shankar%20Krishnan%20and%20Haotian%20Tang%20and%20Peter%20Humphreys%20and%20Junwen%20Bai%20and%20Idan%20Heimlich%20Shtacher%20and%20Diego%20Machado%20and%20Chenxi%20Pang%20and%20Ken%20Burke%20and%20Dangyi%20Liu%20and%20Renga%20Aravamudhan%20and%20Yue%20Song%20and%20Ed%20Hirst%20and%20Abhimanyu%20Singh%20and%20Brendan%20Jou%20and%20Liang%20Bai%20and%20Francesco%20Piccinno%20and%20Chuyuan%20Kelly%20Fu%20and%20Robin%20Alazard%20and%20Barak%20Meiri%20and%20Daniel%20Winter%20and%20Charlie%20Chen%20and%20Mingda%20Zhang%20and%20Jens%20Heitkaemper%20and%20John%20Lambert%20and%20Jinhyuk%20Lee%20and%20Alexander%20Fr%C3%B6mmgen%20and%20Sergey%20Rogulenko%20and%20Pranav%20Nair%20and%20Paul%20Niemczyk%20and%20Anton%20Bulyenov%20and%20Bibo%20Xu%20and%20Hadar%20Shemtov%20and%20Morteza%20Zadimoghaddam%20and%20Serge%20Toropov%20and%20Mateo%20Wirth%20and%20Hanjun%20Dai%20and%20Sreenivas%20Gollapudi%20and%20Daniel%20Zheng%20and%20Alex%20Kurakin%20and%20Chansoo%20Lee%20and%20Kalesha%20Bullard%20and%20Nicolas%20Serrano%20and%20Ivana%20Balazevic%20and%20Yang%20Li%20and%20Johan%20Schalkwyk%20and%20Mark%20Murphy%20and%20Mingyang%20Zhang%20and%20Kevin%20Sequeira%20and%20Romina%20Datta%20and%20Nishant%20Agrawal%20and%20Charles%20Sutton%20and%20Nithya%20Attaluri%20and%20Mencher%20Chiang%20and%20Wael%20Farhan%20and%20Gregory%20Thornton%20and%20Kate%20Lin%20and%20Travis%20Choma%20and%20Hung%20Nguyen%20and%20Kingshuk%20Dasgupta%20and%20Dirk%20Robinson%20and%20Iulia%20Com%C5%9Fa%20and%20Michael%20Riley%20and%20Arjun%20Pillai%20and%20Basil%20Mustafa%20and%20Ben%20Golan%20and%20Amir%20Zandieh%20and%20Jean-Baptiste%20Lespiau%20and%20Billy%20Porter%20and%20David%20Ross%20and%20Sujeevan%20Rajayogam%20and%20Mohit%20Agarwal%20and%20Subhashini%20Venugopalan%20and%20Bobak%20Shahriari%20and%20Qiqi%20Yan%20and%20Hao%20Xu%20and%20Taylor%20Tobin%20and%20Pavel%20Dubov%20and%20Hongzhi%20Shi%20and%20Adri%C3%A0%20Recasens%20and%20Anton%20Kovsharov%20and%20Sebastian%20Borgeaud%20and%20Lucio%20Dery%20and%20Shanthal%20Vasanth%20and%20Elena%20Gribovskaya%20and%20Linhai%20Qiu%20and%20Mahdis%20Mahdieh%20and%20Wojtek%20Skut%20and%20Elizabeth%20Nielsen%20and%20CJ%20Zheng%20and%20Adams%20Yu%20and%20Carrie%20Grimes%20Bostock%20and%20Shaleen%20Gupta%20and%20Aaron%20Archer%20and%20Chris%20Rawles%20and%20Elinor%20Davies%20and%20Alexey%20Svyatkovskiy%20and%20Tomy%20Tsai%20and%20Yoni%20Halpern%20and%20Christian%20Reisswig%20and%20Bartek%20Wydrowski%20and%20Bo%20Chang%20and%20Joan%20Puigcerver%20and%20Mor%20Hazan%20Taege%20and%20Jian%20Li%20and%20Eva%20Schnider%20and%20Xinjian%20Li%20and%20Dragos%20Dena%20and%20Yunhan%20Xu%20and%20Umesh%20Telang%20and%20Tianze%20Shi%20and%20Heiga%20Zen%20and%20Kyle%20Kastner%20and%20Yeongil%20Ko%20and%20Neesha%20Subramaniam%20and%20Aviral%20Kumar%20and%20Pete%20Blois%20and%20Zhuyun%20Dai%20and%20John%20Wieting%20and%20Yifeng%20Lu%20and%20Yoel%20Zeldes%20and%20Tian%20Xie%20and%20Anja%20Hauth%20and%20Alexandru%20%C5%A2ifrea%20and%20Yuqi%20Li%20and%20Sam%20El-Husseini%20and%20Dan%20Abolafia%20and%20Howard%20Zhou%20and%20Wen%20Ding%20and%20Sahra%20Ghalebikesabi%20and%20Carlos%20Gu%C3%ADa%20and%20Andrii%20Maksai%20and%20%C3%81goston%20Weisz%20and%20Sercan%20Arik%20and%20Nick%20Sukhanov%20and%20Aga%20%C5%9Awietlik%20and%20Xuhui%20Jia%20and%20Luo%20Yu%20and%20Weiyue%20Wang%20and%20Mark%20Brand%20and%20Dawn%20Bloxwich%20and%20Sean%20Kirmani%20and%20Zhe%20Chen%20and%20Alec%20Go%20and%20Pablo%20Sprechmann%20and%20Nithish%20Kannen%20and%20Alen%20Carin%20and%20Paramjit%20Sandhu%20and%20Isabel%20Edkins%20and%20Leslie%20Nooteboom%20and%20Jai%20Gupta%20and%20Loren%20Maggiore%20and%20Javad%20Azizi%20and%20Yael%20Pritch%20and%20Pengcheng%20Yin%20and%20Mansi%20Gupta%20and%20Danny%20Tarlow%20and%20Duncan%20Smith%20and%20Desi%20Ivanov%20and%20Mohammad%20Babaeizadeh%20and%20Ankita%20Goel%20and%20Satish%20Kambala%20and%20Grace%20Chu%20and%20Matej%20Kastelic%20and%20Michelle%20Liu%20and%20Hagen%20Soltau%20and%20Austin%20Stone%20and%20Shivani%20Agrawal%20and%20Min%20Kim%20and%20Kedar%20Soparkar%20and%20Srinivas%20Tadepalli%20and%20Oskar%20Bunyan%20and%20Rachel%20Soh%20and%20Arvind%20Kannan%20and%20DY%20Kim%20and%20Blake%20JianHang%20Chen%20and%20Afief%20Halumi%20and%20Sudeshna%20Roy%20and%20Yulong%20Wang%20and%20Olcan%20Sercinoglu%20and%20Gena%20Gibson%20and%20Sijal%20Bhatnagar%20and%20Motoki%20Sano%20and%20Daniel%20von%20Dincklage%20and%20Qingchun%20Ren%20and%20Blagoj%20Mitrevski%20and%20Mirek%20Ol%C5%A1%C3%A1k%20and%20Jennifer%20She%20and%20Carl%20Doersch%20and%20%20Jilei%20and%20%20Wang%20and%20Bingyuan%20Liu%20and%20Qijun%20Tan%20and%20Tamar%20Yakar%20and%20Tris%20Warkentin%20and%20Alex%20Ramirez%20and%20Carl%20Lebsack%20and%20Josh%20Dillon%20and%20Rajiv%20Mathews%20and%20Tom%20Cobley%20and%20Zelin%20Wu%20and%20Zhuoyuan%20Chen%20and%20Jon%20Simon%20and%20Swaroop%20Nath%20and%20Tara%20Sainath%20and%20Alexei%20Bendebury%20and%20Ryan%20Julian%20and%20Bharath%20Mankalale%20and%20Daria%20%C4%86urko%20and%20Paulo%20Zacchello%20and%20Adam%20R.%20Brown%20and%20Kiranbir%20Sodhia%20and%20Heidi%20Howard%20and%20Sergi%20Caelles%20and%20Abhinav%20Gupta%20and%20Gareth%20Evans%20and%20Anna%20Bulanova%20and%20Lesley%20Katzen%20and%20Roman%20Goldenberg%20and%20Anton%20Tsitsulin%20and%20Joe%20Stanton%20and%20Benoit%20Schillings%20and%20Vitaly%20Kovalev%20and%20Corey%20Fry%20and%20Rushin%20Shah%20and%20Kuo%20Lin%20and%20Shyam%20Upadhyay%20and%20Cheng%20Li%20and%20Soroush%20Radpour%20and%20Marcello%20Maggioni%20and%20Jing%20Xiong%20and%20Lukas%20Haas%20and%20Jenny%20Brennan%20and%20Aishwarya%20Kamath%20and%20Nikolay%20Savinov%20and%20Arsha%20Nagrani%20and%20Trevor%20Yacovone%20and%20Ryan%20Kappedal%20and%20Kostas%20Andriopoulos%20and%20Li%20Lao%20and%20YaGuang%20Li%20and%20Grigory%20Rozhdestvenskiy%20and%20Kazuma%20Hashimoto%20and%20Andrew%20Audibert%20and%20Sophia%20Austin%20and%20Daniel%20Rodriguez%20and%20Anian%20Ruoss%20and%20Garrett%20Honke%20and%20Deep%20Karkhanis%20and%20Xi%20Xiong%20and%20Qing%20Wei%20and%20James%20Huang%20and%20Zhaoqi%20Leng%20and%20Vittal%20Premachandran%20and%20Stan%20Bileschi%20and%20Georgios%20Evangelopoulos%20and%20Thomas%20Mensink%20and%20Jay%20Pavagadhi%20and%20Denis%20Teplyashin%20and%20Paul%20Chang%20and%20Linting%20Xue%20and%20Garrett%20Tanzer%20and%20Sally%20Goldman%20and%20Kaushal%20Patel%20and%20Shixin%20Li%20and%20Jeremy%20Wiesner%20and%20Ivy%20Zheng%20and%20Ian%20Stewart-Binks%20and%20Jie%20Han%20and%20Zhi%20Li%20and%20Liangchen%20Luo%20and%20Karel%20Lenc%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Fuzhao%20Xue%20and%20Ryan%20Mullins%20and%20Alexey%20Guseynov%20and%20Chung-Ching%20Chang%20and%20Isaac%20Galatzer-Levy%20and%20Adam%20Zhang%20and%20Garrett%20Bingham%20and%20Grace%20Hu%20and%20Ale%20Hartman%20and%20Yue%20Ma%20and%20Jordan%20Griffith%20and%20Alex%20Irpan%20and%20Carey%20Radebaugh%20and%20Summer%20Yue%20and%20Lijie%20Fan%20and%20Victor%20Ungureanu%20and%20Christina%20Sorokin%20and%20Hannah%20Teufel%20and%20Peiran%20Li%20and%20Rohan%20Anil%20and%20Dimitris%20Paparas%20and%20Todd%20Wang%20and%20Chu-Cheng%20Lin%20and%20Hui%20Peng%20and%20Megan%20Shum%20and%20Goran%20Petrovic%20and%20Demetra%20Brady%20and%20Richard%20Nguyen%20and%20Klaus%20Macherey%20and%20Zhihao%20Li%20and%20Harman%20Singh%20and%20Madhavi%20Yenugula%20and%20Mariko%20Iinuma%20and%20Xinyi%20Chen%20and%20Kavya%20Kopparapu%20and%20Alexey%20Stern%20and%20Shachi%20Dave%20and%20Chandu%20Thekkath%20and%20Florence%20Perot%20and%20Anurag%20Kumar%20and%20Fangda%20Li%20and%20Yang%20Xiao%20and%20Matthew%20Bilotti%20and%20Mohammad%20Hossein%20Bateni%20and%20Isaac%20Noble%20and%20Lisa%20Lee%20and%20Amelio%20V%C3%A1zquez-Reina%20and%20Julian%20Salazar%20and%20Xiaomeng%20Yang%20and%20Boyu%20Wang%20and%20Ela%20Gruzewska%20and%20Anand%20Rao%20and%20Sindhu%20Raghuram%20and%20Zheng%20Xu%20and%20Eyal%20Ben-David%20and%20Jieru%20Mei%20and%20Sid%20Dalmia%20and%20Zhaoyi%20Zhang%20and%20Yuchen%20Liu%20and%20Gagan%20Bansal%20and%20Helena%20Pankov%20and%20Steven%20Schwarcz%20and%20Andrea%20Burns%20and%20Christine%20Chan%20and%20Sumit%20Sanghai%20and%20Ricky%20Liang%20and%20Ethan%20Liang%20and%20Antoine%20He%20and%20Amy%20Stuart%20and%20Arun%20Narayanan%20and%20Yukun%20Zhu%20and%20Christian%20Frank%20and%20Bahar%20Fatemi%20and%20Amit%20Sabne%20and%20Oran%20Lang%20and%20Indro%20Bhattacharya%20and%20Shane%20Settle%20and%20Maria%20Wang%20and%20Brendan%20McMahan%20and%20Andrea%20Tacchetti%20and%20Livio%20Baldini%20Soares%20and%20Majid%20Hadian%20and%20Serkan%20Cabi%20and%20Timothy%20Chung%20and%20Nikita%20Putikhin%20and%20Gang%20Li%20and%20Jeremy%20Chen%20and%20Austin%20Tarango%20and%20Henryk%20Michalewski%20and%20Mehran%20Kazemi%20and%20Hussain%20Masoom%20and%20Hila%20Sheftel%20and%20Rakesh%20Shivanna%20and%20Archita%20Vadali%20and%20Ramona%20Comanescu%20and%20Doug%20Reid%20and%20Joss%20Moore%20and%20Arvind%20Neelakantan%20and%20Micha%C3%ABl%20Sander%20and%20Jonathan%20Herzig%20and%20Aviv%20Rosenberg%20and%20Mostafa%20Dehghani%20and%20JD%20Choi%20and%20Michael%20Fink%20and%20Reid%20Hayes%20and%20Eric%20Ge%20and%20Shitao%20Weng%20and%20Chia-Hua%20Ho%20and%20John%20Karro%20and%20Kalpesh%20Krishna%20and%20Lam%20Nguyen%20Thiet%20and%20Amy%20Skerry-Ryan%20and%20Daniel%20Eppens%20and%20Marco%20Andreetto%20and%20Navin%20Sarma%20and%20Silvano%20Bonacina%20and%20Burcu%20Karagol%20Ayan%20and%20Megha%20Nawhal%20and%20Zhihao%20Shan%20and%20Mike%20Dusenberry%20and%20Shantanu%20Thakoor%20and%20Sagar%20Gubbi%20and%20Duc%20Dung%20Nguyen%20and%20Reut%20Tsarfaty%20and%20Samuel%20Albanie%20and%20Jovana%20Mitrovi%C4%87%20and%20Meet%20Gandhi%20and%20Bo-Juen%20Chen%20and%20Alessandro%20Epasto%20and%20Georgi%20Stephanov%20and%20Ye%20Jin%20and%20Samuel%20Gehman%20and%20Aida%20Amini%20and%20Jack%20Weber%20and%20Feryal%20Behbahani%20and%20Shawn%20Xu%20and%20Miltos%20Allamanis%20and%20Xi%20Chen%20and%20Myle%20Ott%20and%20Claire%20Sha%20and%20Michal%20Jastrzebski%20and%20Hang%20Qi%20and%20David%20Greene%20and%20Xinyi%20Wu%20and%20Abodunrinwa%20Toki%20and%20Daniel%20Vlasic%20and%20Jane%20Shapiro%20and%20Ragha%20Kotikalapudi%20and%20Zhe%20Shen%20and%20Takaaki%20Saeki%20and%20Sirui%20Xie%20and%20Albin%20Cassirer%20and%20Shikhar%20Bharadwaj%20and%20Tatsuya%20Kiyono%20and%20Srinadh%20Bhojanapalli%20and%20Elan%20Rosenfeld%20and%20Sam%20Ritter%20and%20Jieming%20Mao%20and%20Jo%C3%A3o%20Gabriel%20Oliveira%20and%20Zoltan%20Egyed%20and%20Bernd%20Bandemer%20and%20Emilio%20Parisotto%20and%20Keisuke%20Kinoshita%20and%20Juliette%20Pluto%20and%20Petros%20Maniatis%20and%20Steve%20Li%20and%20Yaohui%20Guo%20and%20Golnaz%20Ghiasi%20and%20Jean%20Tarbouriech%20and%20Srimon%20Chatterjee%20and%20Julie%20Jin%20and%20%20Katrina%20and%20%20Xu%20and%20Jennimaria%20Palomaki%20and%20S%C3%A9b%20Arnold%20and%20Madhavi%20Sewak%20and%20Federico%20Piccinini%20and%20Mohit%20Sharma%20and%20Ben%20Albrecht%20and%20Sean%20Purser-haskell%20and%20Ashwin%20Vaswani%20and%20Chongyan%20Chen%20and%20Matheus%20Wisniewski%20and%20Qin%20Cao%20and%20John%20Aslanides%20and%20Nguyet%20Minh%20Phu%20and%20Maximilian%20Sieb%20and%20Lauren%20Agubuzu%20and%20Anne%20Zheng%20and%20Daniel%20Sohn%20and%20Marco%20Selvi%20and%20Anders%20Andreassen%20and%20Krishan%20Subudhi%20and%20Prem%20Eruvbetine%20and%20Oliver%20Woodman%20and%20Tomas%20Mery%20and%20Sebastian%20Krause%20and%20Xiaoqi%20Ren%20and%20Xiao%20Ma%20and%20Jincheng%20Luo%20and%20Dawn%20Chen%20and%20Wei%20Fan%20and%20Henry%20Griffiths%20and%20Christian%20Schuler%20and%20Alice%20Li%20and%20Shujian%20Zhang%20and%20Jean-Michel%20Sarr%20and%20Shixin%20Luo%20and%20Riccardo%20Patana%20and%20Matthew%20Watson%20and%20Dani%20Naboulsi%20and%20Michael%20Collins%20and%20Sailesh%20Sidhwani%20and%20Emiel%20Hoogeboom%20and%20Sharon%20Silver%20and%20Emily%20Caveness%20and%20Xiaokai%20Zhao%20and%20Mikel%20Rodriguez%20and%20Maxine%20Deines%20and%20Libin%20Bai%20and%20Patrick%20Griffin%20and%20Marco%20Tagliasacchi%20and%20Emily%20Xue%20and%20Spandana%20Raj%20Babbula%20and%20Bo%20Pang%20and%20Nan%20Ding%20and%20Gloria%20Shen%20and%20Elijah%20Peake%20and%20Remi%20Crocker%20and%20Shubha%20Srinivas%20Raghvendra%20and%20Danny%20Swisher%20and%20Woohyun%20Han%20and%20Richa%20Singh%20and%20Ling%20Wu%20and%20Vladimir%20Pchelin%20and%20Tsendsuren%20Munkhdalai%20and%20Dana%20Alon%20and%20Geoff%20Bacon%20and%20Efren%20Robles%20and%20Jannis%20Bulian%20and%20Melvin%20Johnson%20and%20George%20Powell%20and%20Felipe%20Tiengo%20Ferreira%20and%20Yaoyiran%20Li%20and%20Frederik%20Benzing%20and%20Mihajlo%20Velimirovi%C4%87%20and%20Hubert%20Soyer%20and%20William%20Kong%20and%20%20Tony%20and%20%20Nguy%C3%AAn%20and%20Zhen%20Yang%20and%20Jeremiah%20Liu%20and%20Joost%20van%20Amersfoort%20and%20Daniel%20Gillick%20and%20Baochen%20Sun%20and%20Nathalie%20Rauschmayr%20and%20Katie%20Zhang%20and%20Serena%20Zhan%20and%20Tao%20Zhou%20and%20Alexey%20Frolov%20and%20Chengrun%20Yang%20and%20Denis%20Vnukov%20and%20Louis%20Rouillard%20and%20Hongji%20Li%20and%20Amol%20Mandhane%20and%20Nova%20Fallen%20and%20Rajesh%20Venkataraman%20and%20Clara%20Huiyi%20Hu%20and%20Jennifer%20Brennan%20and%20Jenny%20Lee%20and%20Jerry%20Chang%20and%20Martin%20Sundermeyer%20and%20Zhufeng%20Pan%20and%20Rosemary%20Ke%20and%20Simon%20Tong%20and%20Alex%20Fabrikant%20and%20William%20Bono%20and%20Jindong%20Gu%20and%20Ryan%20Foley%20and%20Yiran%20Mao%20and%20Manolis%20Delakis%20and%20Dhruva%20Bhaswar%20and%20Roy%20Frostig%20and%20Nick%20Li%20and%20Avital%20Zipori%20and%20Cath%20Hope%20and%20Olga%20Kozlova%20and%20Swaroop%20Mishra%20and%20Josip%20Djolonga%20and%20Craig%20Schiff%20and%20Majd%20Al%20Merey%20and%20Eleftheria%20Briakou%20and%20Peter%20Morgan%20and%20Andy%20Wan%20and%20Avinatan%20Hassidim%20and%20RJ%20Skerry-Ryan%20and%20Kuntal%20Sengupta%20and%20Mary%20Jasarevic%20and%20Praveen%20Kallakuri%20and%20Paige%20Kunkle%20and%20Hannah%20Brennan%20and%20Tom%20Lieber%20and%20Hassan%20Mansoor%20and%20Julian%20Walker%20and%20Bing%20Zhang%20and%20Annie%20Xie%20and%20Goran%20%C5%BDu%C5%BEi%C4%87%20and%20Adaeze%20Chukwuka%20and%20Alex%20Druinsky%20and%20Donghyun%20Cho%20and%20Rui%20Yao%20and%20Ferjad%20Naeem%20and%20Shiraz%20Butt%20and%20Eunyoung%20Kim%20and%20Zhipeng%20Jia%20and%20Mandy%20Jordan%20and%20Adam%20Lelkes%20and%20Mark%20Kurzeja%20and%20Sophie%20Wang%20and%20James%20Zhao%20and%20Andrew%20Over%20and%20Abhishek%20Chakladar%20and%20Marcel%20Prasetya%20and%20Neha%20Jha%20and%20Sriram%20Ganapathy%20and%20Yale%20Cong%20and%20Prakash%20Shroff%20and%20Carl%20Saroufim%20and%20Sobhan%20Miryoosefi%20and%20Mohamed%20Hammad%20and%20Tajwar%20Nasir%20and%20Weijuan%20Xi%20and%20Yang%20Gao%20and%20Young%20Maeng%20and%20Ben%20Hora%20and%20Chin-Yi%20Cheng%20and%20Parisa%20Haghani%20and%20Yoad%20Lewenberg%20and%20Caden%20Lu%20and%20Martin%20Matysiak%20and%20Naina%20Raisinghani%20and%20Huiyu%20Wang%20and%20Lexi%20Baugher%20and%20Rahul%20Sukthankar%20and%20Minh%20Giang%20and%20John%20Schultz%20and%20Noah%20Fiedel%20and%20Minmin%20Chen%20and%20Cheng-Chun%20Lee%20and%20Tapomay%20Dey%20and%20Hao%20Zheng%20and%20Shachi%20Paul%20and%20Celine%20Smith%20and%20Andy%20Ly%20and%20Yicheng%20Wang%20and%20Rishabh%20Bansal%20and%20Bartek%20Perz%20and%20Susanna%20Ricco%20and%20Stasha%20Blank%20and%20Vaishakh%20Keshava%20and%20Deepak%20Sharma%20and%20Marvin%20Chow%20and%20Kunal%20Lad%20and%20Komal%20Jalan%20and%20Simon%20Osindero%20and%20Craig%20Swanson%20and%20Jacob%20Scott%20and%20Anastasija%20Ili%C4%87%20and%20Xiaowei%20Li%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Afzal%20Shama%20Soudagar%20and%20Yan%20Xiong%20and%20Bat-Orgil%20Batsaikhan%20and%20Daniel%20Jarrett%20and%20Naveen%20Kumar%20and%20Maulik%20Shah%20and%20Matt%20Lawlor%20and%20Austin%20Waters%20and%20Mark%20Graham%20and%20Rhys%20May%20and%20Sabela%20Ramos%20and%20Sandra%20Lefdal%20and%20Zeynep%20Cankara%20and%20Nacho%20Cano%20and%20Brendan%20O%27Donoghue%20and%20Jed%20Borovik%20and%20Frederick%20Liu%20and%20Jordan%20Grimstad%20and%20Mahmoud%20Alnahlawi%20and%20Katerina%20Tsihlas%20and%20Tom%20Hudson%20and%20Nikolai%20Grigorev%20and%20Yiling%20Jia%20and%20Terry%20Huang%20and%20Tobenna%20Peter%20Igwe%20and%20Sergei%20Lebedev%20and%20Xiaodan%20Tang%20and%20Igor%20Krivokon%20and%20Frankie%20Garcia%20and%20Melissa%20Tan%20and%20Eric%20Jia%20and%20Peter%20Stys%20and%20Shikhar%20Vashishth%20and%20Yu%20Liang%20and%20Balaji%20Venkatraman%20and%20Chenjie%20Gu%20and%20Anastasios%20Kementsietsidis%20and%20Chen%20Zhu%20and%20Junehyuk%20Jung%20and%20Yunfei%20Bai%20and%20Mohammad%20Javad%20Hosseini%20and%20Faruk%20Ahmed%20and%20Aditya%20Gupta%20and%20Xin%20Yuan%20and%20Shereen%20Ashraf%20and%20Shitij%20Nigam%20and%20Gautam%20Vasudevan%20and%20Pranjal%20Awasthi%20and%20Adi%20Mayrav%20Gilady%20and%20Zelda%20Mariet%20and%20Ramy%20Eskander%20and%20Haiguang%20Li%20and%20Hexiang%20Hu%20and%20Guillermo%20Garrido%20and%20Philippe%20Schlattner%20and%20George%20Zhang%20and%20Rohun%20Saxena%20and%20Petar%20Devi%C4%87%20and%20Kritika%20Muralidharan%20and%20Ashwin%20Murthy%20and%20Yiqian%20Zhou%20and%20Min%20Choi%20and%20Arissa%20Wongpanich%20and%20Zhengdong%20Wang%20and%20Premal%20Shah%20and%20Yuntao%20Xu%20and%20Yiling%20Huang%20and%20Stephen%20Spencer%20and%20Alice%20Chen%20and%20James%20Cohan%20and%20Junjie%20Wang%20and%20Jonathan%20Tompson%20and%20Junru%20Wu%20and%20Ruba%20Haroun%20and%20Haiqiong%20Li%20and%20Blanca%20Huergo%20and%20Fan%20Yang%20and%20Tongxin%20Yin%20and%20James%20Wendt%20and%20Michael%20Bendersky%20and%20Rahma%20Chaabouni%20and%20Javier%20Snaider%20and%20Johan%20Ferret%20and%20Abhishek%20Jindal%20and%20Tara%20Thompson%20and%20Andrew%20Xue%20and%20Will%20Bishop%20and%20Shubham%20Milind%20Phal%20and%20Archit%20Sharma%20and%20Yunhsuan%20Sung%20and%20Prabakar%20Radhakrishnan%20and%20Mo%20Shomrat%20and%20Reeve%20Ingle%20and%20Roopali%20Vij%20and%20Justin%20Gilmer%20and%20Mihai%20Dorin%20Istin%20and%20Sam%20Sobell%20and%20Yang%20Lu%20and%20Emily%20Nottage%20and%20Dorsa%20Sadigh%20and%20Jeremiah%20Willcock%20and%20Tingnan%20Zhang%20and%20Steve%20Xu%20and%20Sasha%20Brown%20and%20Katherine%20Lee%20and%20Gary%20Wang%20and%20Yun%20Zhu%20and%20Yi%20Tay%20and%20Cheolmin%20Kim%20and%20Audrey%20Gutierrez%20and%20Abhanshu%20Sharma%20and%20Yongqin%20Xian%20and%20Sungyong%20Seo%20and%20Claire%20Cui%20and%20Elena%20Pochernina%20and%20Cip%20Baetu%20and%20Krzysztof%20Jastrz%C4%99bski%20and%20Mimi%20Ly%20and%20Mohamed%20Elhawaty%20and%20Dan%20Suh%20and%20Eren%20Sezener%20and%20Pidong%20Wang%20and%20Nancy%20Yuen%20and%20George%20Tucker%20and%20Jiahao%20Cai%20and%20Zuguang%20Yang%20and%20Cindy%20Wang%20and%20Alex%20Muzio%20and%20Hai%20Qian%20and%20Jae%20Yoo%20and%20Derek%20Lockhart%20and%20Kevin%20R.%20McKee%20and%20Mandy%20Guo%20and%20Malika%20Mehrotra%20and%20Artur%20Mendon%C3%A7a%20and%20Sanket%20Vaibhav%20Mehta%20and%20Sherry%20Ben%20and%20Chetan%20Tekur%20and%20Jiaqi%20Mu%20and%20Muye%20Zhu%20and%20Victoria%20Krakovna%20and%20Hongrae%20Lee%20and%20AJ%20Maschinot%20and%20S%C3%A9bastien%20Cevey%20and%20HyunJeong%20Choe%20and%20Aijun%20Bai%20and%20Hansa%20Srinivasan%20and%20Derek%20Gasaway%20and%20Nick%20Young%20and%20Patrick%20Siegler%20and%20Dan%20Holtmann-Rice%20and%20Vihari%20Piratla%20and%20Kate%20Baumli%20and%20Roey%20Yogev%20and%20Alex%20Hofer%20and%20Hado%20van%20Hasselt%20and%20Svetlana%20Grant%20and%20Yuri%20Chervonyi%20and%20David%20Silver%20and%20Andrew%20Hogue%20and%20Ayushi%20Agarwal%20and%20Kathie%20Wang%20and%20Preeti%20Singh%20and%20Four%20Flynn%20and%20Josh%20Lipschultz%20and%20Robert%20David%20and%20Lizzetth%20Bellot%20and%20Yao-Yuan%20Yang%20and%20Long%20Le%20and%20Filippo%20Graziano%20and%20Kate%20Olszewska%20and%20Kevin%20Hui%20and%20Akanksha%20Maurya%20and%20Nikos%20Parotsidis%20and%20Weijie%20Chen%20and%20Tayo%20Oguntebi%20and%20Joe%20Kelley%20and%20Anirudh%20Baddepudi%20and%20Johannes%20Mauerer%20and%20Gregory%20Shaw%20and%20Alex%20Siegman%20and%20Lin%20Yang%20and%20Shravya%20Shetty%20and%20Subhrajit%20Roy%20and%20Yunting%20Song%20and%20Wojciech%20Stokowiec%20and%20Ryan%20Burnell%20and%20Omkar%20Savant%20and%20Robert%20Busa-Fekete%20and%20Jin%20Miao%20and%20Samrat%20Ghosh%20and%20Liam%20MacDermed%20and%20Phillip%20Lippe%20and%20Mikhail%20Dektiarev%20and%20Zach%20Behrman%20and%20Fabian%20Mentzer%20and%20Kelvin%20Nguyen%20and%20Meng%20Wei%20and%20Siddharth%20Verma%20and%20Chris%20Knutsen%20and%20Sudeep%20Dasari%20and%20Zhipeng%20Yan%20and%20Petr%20Mitrichev%20and%20Xingyu%20Wang%20and%20Virat%20Shejwalkar%20and%20Jacob%20Austin%20and%20Srinivas%20Sunkara%20and%20Navneet%20Potti%20and%20Yan%20Virin%20and%20Christian%20Wright%20and%20Ga%C3%ABl%20Liu%20and%20Oriana%20Riva%20and%20Etienne%20Pot%20and%20Greg%20Kochanski%20and%20Quoc%20Le%20and%20Gargi%20Balasubramaniam%20and%20Arka%20Dhar%20and%20Yuguo%20Liao%20and%20Adam%20Bloniarz%20and%20Divyansh%20Shukla%20and%20Elizabeth%20Cole%20and%20Jong%20Lee%20and%20Sheng%20Zhang%20and%20Sushant%20Kafle%20and%20Siddharth%20Vashishtha%20and%20Parsa%20Mahmoudieh%20and%20Grace%20Chen%20and%20Raphael%20Hoffmann%20and%20Pranesh%20Srinivasan%20and%20Agustin%20Dal%20Lago%20and%20Yoav%20Ben%20Shalom%20and%20Zi%20Wang%20and%20Michael%20Elabd%20and%20Anuj%20Sharma%20and%20Junhyuk%20Oh%20and%20Suraj%20Kothawade%20and%20Maigo%20Le%20and%20Marianne%20Monteiro%20and%20Shentao%20Yang%20and%20Kaiz%20Alarakyia%20and%20Robert%20Geirhos%20and%20Diana%20Mincu%20and%20H%C3%A5vard%20Garnes%20and%20Hayato%20Kobayashi%20and%20Soroosh%20Mariooryad%20and%20Kacper%20Krasowiak%20and%20%20Zhixin%20and%20%20Lai%20and%20Shibl%20Mourad%20and%20Mingqiu%20Wang%20and%20Fan%20Bu%20and%20Ophir%20Aharoni%20and%20Guanjie%20Chen%20and%20Abhimanyu%20Goyal%20and%20Vadim%20Zubov%20and%20Ankur%20Bapna%20and%20Elahe%20Dabir%20and%20Nisarg%20Kothari%20and%20Kay%20Lamerigts%20and%20Nicola%20De%20Cao%20and%20Jeremy%20Shar%20and%20Christopher%20Yew%20and%20Nitish%20Kulkarni%20and%20Dre%20Mahaarachchi%20and%20Mandar%20Joshi%20and%20Zhenhai%20Zhu%20and%20Jared%20Lichtarge%20and%20Yichao%20Zhou%20and%20Hannah%20Muckenhirn%20and%20Vittorio%20Selo%20and%20Oriol%20Vinyals%20and%20Peter%20Chen%20and%20Anthony%20Brohan%20and%20Vaibhav%20Mehta%20and%20Sarah%20Cogan%20and%20Ruth%20Wang%20and%20Ty%20Geri%20and%20Wei-Jen%20Ko%20and%20Wei%20Chen%20and%20Fabio%20Viola%20and%20Keshav%20Shivam%20and%20Lisa%20Wang%20and%20Madeleine%20Clare%20Elish%20and%20Raluca%20Ada%20Popa%20and%20S%C3%A9bastien%20Pereira%20and%20Jianqiao%20Liu%20and%20Raphael%20Koster%20and%20Donnie%20Kim%20and%20Gufeng%20Zhang%20and%20Sayna%20Ebrahimi%20and%20Partha%20Talukdar%20and%20Yanyan%20Zheng%20and%20Petra%20Poklukar%20and%20Ales%20Mikhalap%20and%20Dale%20Johnson%20and%20Anitha%20Vijayakumar%20and%20Mark%20Omernick%20and%20Matt%20Dibb%20and%20Ayush%20Dubey%20and%20Qiong%20Hu%20and%20Apurv%20Suman%20and%20Vaibhav%20Aggarwal%20and%20Ilya%20Kornakov%20and%20Fei%20Xia%20and%20Wing%20Lowe%20and%20Alexey%20Kolganov%20and%20Ted%20Xiao%20and%20Vitaly%20Nikolaev%20and%20Steven%20Hemingray%20and%20Bonnie%20Li%20and%20Joana%20Iljazi%20and%20Miko%C5%82aj%20Rybi%C5%84ski%20and%20Ballie%20Sandhu%20and%20Peggy%20Lu%20and%20Thang%20Luong%20and%20Rodolphe%20Jenatton%20and%20Vineetha%20Govindaraj%20and%20%20Hui%20and%20%20Li%20and%20Gabriel%20Dulac-Arnold%20and%20Wonpyo%20Park%20and%20Henry%20Wang%20and%20Abhinit%20Modi%20and%20Jean%20Pouget-Abadie%20and%20Kristina%20Greller%20and%20Rahul%20Gupta%20and%20Robert%20Berry%20and%20Prajit%20Ramachandran%20and%20Jinyu%20Xie%20and%20Liam%20McCafferty%20and%20Jianling%20Wang%20and%20Kilol%20Gupta%20and%20Hyeontaek%20Lim%20and%20Bla%C5%BE%20Bratani%C4%8D%20and%20Andy%20Brock%20and%20Ilia%20Akolzin%20and%20Jim%20Sproch%20and%20Dan%20Karliner%20and%20Duhyeon%20Kim%20and%20Adrian%20Goedeckemeyer%20and%20Noam%20Shazeer%20and%20Cordelia%20Schmid%20and%20Daniele%20Calandriello%20and%20Parul%20Bhatia%20and%20Krzysztof%20Choromanski%20and%20Ceslee%20Montgomery%20and%20Dheeru%20Dua%20and%20Ana%20Ramalho%20and%20Helen%20King%20and%20Yue%20Gao%20and%20Lynn%20Nguyen%20and%20David%20Lindner%20and%20Divya%20Pitta%20and%20Oleaser%20Johnson%20and%20Khalid%20Salama%20and%20Diego%20Ardila%20and%20Michael%20Han%20and%20Erin%20Farnese%20and%20Seth%20Odoom%20and%20Ziyue%20Wang%20and%20Xiangzhuo%20Ding%20and%20Norman%20Rink%20and%20Ray%20Smith%20and%20Harshal%20Tushar%20Lehri%20and%20Eden%20Cohen%20and%20Neera%20Vats%20and%20Tong%20He%20and%20Parthasarathy%20Gopavarapu%20and%20Adam%20Paszke%20and%20Miteyan%20Patel%20and%20Wouter%20Van%20Gansbeke%20and%20Lucia%20Loher%20and%20Luis%20Castro%20and%20Maria%20Voitovich%20and%20Tamara%20von%20Glehn%20and%20Nelson%20George%20and%20Simon%20Niklaus%20and%20Zach%20Eaton-Rosen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Erik%20Jue%20and%20Sagi%20Perel%20and%20Carrie%20Zhang%20and%20Yuval%20Bahat%20and%20Ang%C3%A9line%20Pouget%20and%20Zhi%20Xing%20and%20Fantine%20Huot%20and%20Ashish%20Shenoy%20and%20Taylor%20Bos%20and%20Vincent%20Coriou%20and%20Bryan%20Richter%20and%20Natasha%20Noy%20and%20Yaqing%20Wang%20and%20Santiago%20Ontanon%20and%20Siyang%20Qin%20and%20Gleb%20Makarchuk%20and%20Demis%20Hassabis%20and%20Zhuowan%20Li%20and%20Mandar%20Sharma%20and%20Kumaran%20Venkatesan%20and%20Iurii%20Kemaev%20and%20Roxanne%20Daniel%20and%20Shiyu%20Huang%20and%20Saloni%20Shah%20and%20Octavio%20Ponce%20and%20%20Warren%20and%20%20Chen%20and%20Manaal%20Faruqui%20and%20Jialin%20Wu%20and%20Slavica%20Anda%C4%8Di%C4%87%20and%20Szabolcs%20Payrits%20and%20Daniel%20McDuff%20and%20Tom%20Hume%20and%20Yuan%20Cao%20and%20MH%20Tessler%20and%20Qingze%20Wang%20and%20Yinan%20Wang%20and%20Ivor%20Rendulic%20and%20Eirikur%20Agustsson%20and%20Matthew%20Johnson%20and%20Tanya%20Lando%20and%20Andrew%20Howard%20and%20Sri%20Gayatri%20Sundara%20Padmanabhan%20and%20Mayank%20Daswani%20and%20Andrea%20Banino%20and%20Michael%20Kilgore%20and%20Jonathan%20Heek%20and%20Ziwei%20Ji%20and%20Alvaro%20Caceres%20and%20Conglong%20Li%20and%20Nora%20Kassner%20and%20Alexey%20Vlaskin%20and%20Zeyu%20Liu%20and%20Alex%20Grills%20and%20Yanhan%20Hou%20and%20Roykrong%20Sukkerd%20and%20Gowoon%20Cheon%20and%20Nishita%20Shetty%20and%20Larisa%20Markeeva%20and%20Piotr%20Stanczyk%20and%20Tejas%20Iyer%20and%20Yuan%20Gong%20and%20Shawn%20Gao%20and%20Keerthana%20Gopalakrishnan%20and%20Tim%20Blyth%20and%20Malcolm%20Reynolds%20and%20Avishkar%20Bhoopchand%20and%20Misha%20Bilenko%20and%20Dero%20Gharibian%20and%20Vicky%20Zayats%20and%20Aleksandra%20Faust%20and%20Abhinav%20Singh%20and%20Min%20Ma%20and%20Hongyang%20Jiao%20and%20Sudheendra%20Vijayanarasimhan%20and%20Lora%20Aroyo%20and%20Vikas%20Yadav%20and%20Sarah%20Chakera%20and%20Ashwin%20Kakarla%20and%20Vilobh%20Meshram%20and%20Karol%20Gregor%20and%20Gabriela%20Botea%20and%20Evan%20Senter%20and%20Dawei%20Jia%20and%20Geza%20Kovacs%20and%20Neha%20Sharma%20and%20Sebastien%20Baur%20and%20Kai%20Kang%20and%20Yifan%20He%20and%20Lin%20Zhuo%20and%20Marija%20Kostelac%20and%20Itay%20Laish%20and%20Songyou%20Peng%20and%20Louis%20O%27Bryan%20and%20Daniel%20Kasenberg%20and%20Girish%20Ramchandra%20Rao%20and%20Edouard%20Leurent%20and%20Biao%20Zhang%20and%20Sage%20Stevens%20and%20Ana%20Salazar%20and%20Ye%20Zhang%20and%20Ivan%20Lobov%20and%20Jake%20Walker%20and%20Allen%20Porter%20and%20Morgan%20Redshaw%20and%20Han%20Ke%20and%20Abhishek%20Rao%20and%20Alex%20Lee%20and%20Hoi%20Lam%20and%20Michael%20Moffitt%20and%20Jaeyoun%20Kim%20and%20Siyuan%20Qiao%20and%20Terry%20Koo%20and%20Robert%20Dadashi%20and%20Xinying%20Song%20and%20Mukund%20Sundararajan%20and%20Peng%20Xu%20and%20Chizu%20Kawamoto%20and%20Yan%20Zhong%20and%20Clara%20Barbu%20and%20Apoorv%20Reddy%20and%20Mauro%20Verzetti%20and%20Leon%20Li%20and%20George%20Papamakarios%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Mary%20Cassin%20and%20Koray%20Kavukcuoglu%20and%20Rigel%20Swavely%20and%20Alain%20Vaucher%20and%20Jeffrey%20Zhao%20and%20Ross%20Hemsley%20and%20Michael%20Tschannen%20and%20Heming%20Ge%20and%20Gaurav%20Menghani%20and%20Yang%20Yu%20and%20Natalie%20Ha%20and%20Wei%20He%20and%20Xiao%20Wu%20and%20Maggie%20Song%20and%20Rachel%20Sterneck%20and%20Stefan%20Zinke%20and%20Dan%20A.%20Calian%20and%20Annie%20Marsden%20and%20Alejandro%20Cruzado%20Ruiz%20and%20Matteo%20Hessel%20and%20Almog%20Gueta%20and%20Benjamin%20Lee%20and%20Brian%20Farris%20and%20Manish%20Gupta%20and%20Yunjie%20Li%20and%20Mohammad%20Saleh%20and%20Vedant%20Misra%20and%20Kefan%20Xiao%20and%20Piermaria%20Mendolicchio%20and%20Gavin%20Buttimore%20and%20Varvara%20Krayvanova%20and%20Nigamaa%20Nayakanti%20and%20Matthew%20Wiethoff%20and%20Yash%20Pande%20and%20Azalia%20Mirhoseini%20and%20Ni%20Lao%20and%20Jasmine%20Liu%20and%20Yiqing%20Hua%20and%20Angie%20Chen%20and%20Yury%20Malkov%20and%20Dmitry%20Kalashnikov%20and%20Shubham%20Gupta%20and%20Kartik%20Audhkhasi%20and%20Yuexiang%20Zhai%20and%20Sudhindra%20Kopalle%20and%20Prateek%20Jain%20and%20Eran%20Ofek%20and%20Clemens%20Meyer%20and%20Khuslen%20Baatarsukh%20and%20Hana%20Strej%C4%8Dek%20and%20Jun%20Qian%20and%20James%20Freedman%20and%20Ricardo%20Figueira%20and%20Michal%20Sokolik%20and%20Olivier%20Bachem%20and%20Raymond%20Lin%20and%20Dia%20Kharrat%20and%20Chris%20Hidey%20and%20Pingmei%20Xu%20and%20Dennis%20Duan%20and%20Yin%20Li%20and%20Muge%20Ersoy%20and%20Richard%20Everett%20and%20Kevin%20Cen%20and%20Rebeca%20Santamaria-Fernandez%20and%20Amir%20Taubenfeld%20and%20Ian%20Mackinnon%20and%20Linda%20Deng%20and%20Polina%20Zablotskaia%20and%20Shashank%20Viswanadha%20and%20Shivanker%20Goel%20and%20Damion%20Yates%20and%20Yunxiao%20Deng%20and%20Peter%20Choy%20and%20Mingqing%20Chen%20and%20Abhishek%20Sinha%20and%20Alex%20Mossin%20and%20Yiming%20Wang%20and%20Arthur%20Szlam%20and%20Susan%20Hao%20and%20Paul%20Kishan%20Rubenstein%20and%20Metin%20Toksoz-Exley%20and%20Miranda%20Aperghis%20and%20Yin%20Zhong%20and%20Junwhan%20Ahn%20and%20Michael%20Isard%20and%20Olivier%20Lacombe%20and%20Florian%20Luisier%20and%20Chrysovalantis%20Anastasiou%20and%20Yogesh%20Kalley%20and%20Utsav%20Prabhu%20and%20Emma%20Dunleavy%20and%20Shaan%20Bijwadia%20and%20Justin%20Mao-Jones%20and%20Kelly%20Chen%20and%20Rama%20Pasumarthi%20and%20Emily%20Wood%20and%20Adil%20Dostmohamed%20and%20Nate%20Hurley%20and%20Jiri%20Simsa%20and%20Alicia%20Parrish%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Ondrej%20Skopek%20and%20Yony%20Kochinski%20and%20Javier%20Rey%20and%20Verena%20Rieser%20and%20Denny%20Zhou%20and%20Sun%20Jae%20Lee%20and%20Trilok%20Acharya%20and%20Guowang%20Li%20and%20Joe%20Jiang%20and%20Xiaofan%20Zhang%20and%20Bryant%20Gipson%20and%20Ethan%20Mahintorabi%20and%20Marco%20Gelmi%20and%20Nima%20Khajehnouri%20and%20Angel%20Yeh%20and%20Kayi%20Lee%20and%20Loic%20Matthey%20and%20Leslie%20Baker%20and%20Trang%20Pham%20and%20Han%20Fu%20and%20Alex%20Pak%20and%20Prakhar%20Gupta%20and%20Cristina%20Vasconcelos%20and%20Adam%20Sadovsky%20and%20Brian%20Walker%20and%20Sissie%20Hsiao%20and%20Patrik%20Zochbauer%20and%20Andreea%20Marzoca%20and%20Noam%20Velan%20and%20Junhao%20Zeng%20and%20Gilles%20Baechler%20and%20Danny%20Driess%20and%20Divya%20Jain%20and%20Yanping%20Huang%20and%20Lizzie%20Tao%20and%20John%20Maggs%20and%20Nir%20Levine%20and%20Jon%20Schneider%20and%20Erika%20Gemzer%20and%20Samuel%20Petit%20and%20Shan%20Han%20and%20Zach%20Fisher%20and%20Dustin%20Zelle%20and%20Courtney%20Biles%20and%20Eugene%20Ie%20and%20Asya%20Fadeeva%20and%20Casper%20Liu%20and%20Juliana%20Vicente%20Franco%20and%20Adrian%20Collister%20and%20Hao%20Zhang%20and%20Renshen%20Wang%20and%20Ruizhe%20Zhao%20and%20Leandro%20Kieliger%20and%20Kurt%20Shuster%20and%20Rui%20Zhu%20and%20Boqing%20Gong%20and%20Lawrence%20Chan%20and%20Ruoxi%20Sun%20and%20Sujoy%20Basu%20and%20Roland%20Zimmermann%20and%20Jamie%20Hayes%20and%20Abhishek%20Bapna%20and%20Jasper%20Snoek%20and%20Weel%20Yang%20and%20Puranjay%20Datta%20and%20Jad%20Al%20Abdallah%20and%20Kevin%20Kilgour%20and%20Lu%20Li%20and%20SQ%20Mah%20and%20Yennie%20Jun%20and%20Morgane%20Rivi%C3%A8re%20and%20Abhijit%20Karmarkar%20and%20Tammo%20Spalink%20and%20Tao%20Huang%20and%20Lucas%20Gonzalez%20and%20Duc-Hieu%20Tran%20and%20Averi%20Nowak%20and%20John%20Palowitch%20and%20Martin%20Chadwick%20and%20Ellie%20Talius%20and%20Harsh%20Mehta%20and%20Thibault%20Sellam%20and%20Philipp%20Fr%C3%A4nken%20and%20Massimo%20Nicosia%20and%20Kyle%20He%20and%20Aditya%20Kini%20and%20David%20Amos%20and%20Sugato%20Basu%20and%20Harrison%20Jobe%20and%20Eleni%20Shaw%20and%20Qiantong%20Xu%20and%20Colin%20Evans%20and%20Daisuke%20Ikeda%20and%20Chaochao%20Yan%20and%20Larry%20Jin%20and%20Lun%20Wang%20and%20Sachin%20Yadav%20and%20Ilia%20Labzovsky%20and%20Ramesh%20Sampath%20and%20Ada%20Ma%20and%20Candice%20Schumann%20and%20Aditya%20Siddhant%20and%20Rohin%20Shah%20and%20John%20Youssef%20and%20Rishabh%20Agarwal%20and%20Natalie%20Dabney%20and%20Alessio%20Tonioni%20and%20Moran%20Ambar%20and%20Jing%20Li%20and%20Isabelle%20Guyon%20and%20Benny%20Li%20and%20David%20Soergel%20and%20Boya%20Fang%20and%20Georgi%20Karadzhov%20and%20Cristian%20Udrescu%20and%20Trieu%20Trinh%20and%20Vikas%20Raunak%20and%20Seb%20Noury%20and%20Dee%20Guo%20and%20Sonal%20Gupta%20and%20Mara%20Finkelstein%20and%20Denis%20Petek%20and%20Lihao%20Liang%20and%20Greg%20Billock%20and%20Pei%20Sun%20and%20David%20Wood%20and%20Yiwen%20Song%20and%20Xiaobin%20Yu%20and%20Tatiana%20Matejovicova%20and%20Regev%20Cohen%20and%20Kalyan%20Andra%20and%20David%20D%27Ambrosio%20and%20Zhiwei%20Deng%20and%20Vincent%20Nallatamby%20and%20Ebrahim%20Songhori%20and%20Rumen%20Dangovski%20and%20Andrew%20Lampinen%20and%20Pankil%20Botadra%20and%20Adam%20Hillier%20and%20Jiawei%20Cao%20and%20Nagabhushan%20Baddi%20and%20Adhi%20Kuncoro%20and%20Toshihiro%20Yoshino%20and%20Ankit%20Bhagatwala%20and%20Marc%C3%A1urelio%20Ranzato%20and%20Rylan%20Schaeffer%20and%20Tianlin%20Liu%20and%20Shuai%20Ye%20and%20Obaid%20Sarvana%20and%20John%20Nham%20and%20Chenkai%20Kuang%20and%20Isabel%20Gao%20and%20Jinoo%20Baek%20and%20Shubham%20Mittal%20and%20Ayzaan%20Wahid%20and%20Anita%20Gergely%20and%20Bin%20Ni%20and%20Josh%20Feldman%20and%20Carrie%20Muir%20and%20Pascal%20Lamblin%20and%20Wolfgang%20Macherey%20and%20Ethan%20Dyer%20and%20Logan%20Kilpatrick%20and%20V%C3%ADctor%20Campos%20and%20Mukul%20Bhutani%20and%20Stanislav%20Fort%20and%20Yanif%20Ahmad%20and%20Aliaksei%20Severyn%20and%20Kleopatra%20Chatziprimou%20and%20Oleksandr%20Ferludin%20and%20Mason%20Dimarco%20and%20Aditya%20Kusupati%20and%20Joe%20Heyward%20and%20Dan%20Bahir%20and%20Kevin%20Villela%20and%20Katie%20Millican%20and%20Dror%20Marcus%20and%20Sanaz%20Bahargam%20and%20Caglar%20Unlu%20and%20Nicholas%20Roth%20and%20Zichuan%20Wei%20and%20Siddharth%20Gopal%20and%20Deepanway%20Ghoshal%20and%20Edward%20Lee%20and%20Sharon%20Lin%20and%20Jennie%20Lees%20and%20Dayeong%20Lee%20and%20Anahita%20Hosseini%20and%20Connie%20Fan%20and%20Seth%20Neel%20and%20Marcus%20Wu%20and%20Yasemin%20Altun%20and%20Honglong%20Cai%20and%20Enrique%20Piqueras%20and%20Josh%20Woodward%20and%20Alessandro%20Bissacco%20and%20Salem%20Haykal%20and%20Mahyar%20Bordbar%20and%20Prasha%20Sundaram%20and%20Sarah%20Hodkinson%20and%20Daniel%20Toyama%20and%20George%20Polovets%20and%20Austin%20Myers%20and%20Anu%20Sinha%20and%20Tomer%20Levinboim%20and%20Kashyap%20Krishnakumar%20and%20Rachita%20Chhaparia%20and%20Tatiana%20Sholokhova%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Ganesh%20Jawahar%20and%20Haroon%20Qureshi%20and%20Jieru%20Hu%20and%20Nikola%20Momchev%20and%20Matthew%20Rahtz%20and%20Renjie%20Wu%20and%20Aishwarya%20P%20S%20and%20Kedar%20Dhamdhere%20and%20Meiqi%20Guo%20and%20Umang%20Gupta%20and%20Ali%20Eslami%20and%20Mariano%20Schain%20and%20Michiel%20Blokzijl%20and%20David%20Welling%20and%20Dave%20Orr%20and%20Levent%20Bolelli%20and%20Nicolas%20Perez-Nieves%20and%20Mikhail%20Sirotenko%20and%20Aman%20Prasad%20and%20Arjun%20Kar%20and%20Borja%20De%20Balle%20Pigem%20and%20Tayfun%20Terzi%20and%20Gell%C3%A9rt%20Weisz%20and%20Dipankar%20Ghosh%20and%20Aditi%20Mavalankar%20and%20Dhruv%20Madeka%20and%20Kaspar%20Daugaard%20and%20Hartwig%20Adam%20and%20Viraj%20Shah%20and%20Dana%20Berman%20and%20Maggie%20Tran%20and%20Steven%20Baker%20and%20Ewa%20Andrejczuk%20and%20Grishma%20Chole%20and%20Ganna%20Raboshchuk%20and%20Mahdi%20Mirzazadeh%20and%20Thais%20Kagohara%20and%20Shimu%20Wu%20and%20Christian%20Schallhart%20and%20Bernett%20Orlando%20and%20Chen%20Wang%20and%20Alban%20Rrustemi%20and%20Hao%20Xiong%20and%20Hao%20Liu%20and%20Arpi%20Vezer%20and%20Nolan%20Ramsden%20and%20Shuo-yiin%20Chang%20and%20Sidharth%20Mudgal%20and%20Yan%20Li%20and%20Nino%20Vieillard%20and%20Yedid%20Hoshen%20and%20Farooq%20Ahmad%20and%20Ambrose%20Slone%20and%20Amy%20Hua%20and%20Natan%20Potikha%20and%20Mirko%20Rossini%20and%20Jon%20Stritar%20and%20Sushant%20Prakash%20and%20Zifeng%20Wang%20and%20Xuanyi%20Dong%20and%20Alireza%20Nazari%20and%20Efrat%20Nehoran%20and%20Kaan%20Tekelioglu%20and%20Yinxiao%20Li%20and%20Kartikeya%20Badola%20and%20Tom%20Funkhouser%20and%20Yuanzhen%20Li%20and%20Varun%20Yerram%20and%20Ramya%20Ganeshan%20and%20Daniel%20Formoso%20and%20Karol%20Langner%20and%20Tian%20Shi%20and%20Huijian%20Li%20and%20Yumeya%20Yamamori%20and%20Amayika%20Panda%20and%20Alaa%20Saade%20and%20Angelo%20Scorza%20Scarpati%20and%20Chris%20Breaux%20and%20CJ%20Carey%20and%20Zongwei%20Zhou%20and%20Cho-Jui%20Hsieh%20and%20Sophie%20Bridgers%20and%20Alena%20Butryna%20and%20Nishesh%20Gupta%20and%20Vaibhav%20Tulsyan%20and%20Sanghyun%20Woo%20and%20Evgenii%20Eltyshev%20and%20Will%20Grathwohl%20and%20Chanel%20Parks%20and%20Seth%20Benjamin%20and%20Rina%20Panigrahy%20and%20Shenil%20Dodhia%20and%20Daniel%20De%20Freitas%20and%20Chris%20Sauer%20and%20Will%20Song%20and%20Ferran%20Alet%20and%20Jackson%20Tolins%20and%20Cosmin%20Paduraru%20and%20Xingyi%20Zhou%20and%20Brian%20Albert%20and%20Zizhao%20Zhang%20and%20Lei%20Shu%20and%20Mudit%20Bansal%20and%20Sarah%20Nguyen%20and%20Amir%20Globerson%20and%20Owen%20Xiao%20and%20James%20Manyika%20and%20Tom%20Hennigan%20and%20Rong%20Rong%20and%20Josip%20Matak%20and%20Anton%20Bakalov%20and%20Ankur%20Sharma%20and%20Danila%20Sinopalnikov%20and%20Andrew%20Pierson%20and%20Stephen%20Roller%20and%20Geoff%20Brown%20and%20Mingcen%20Gao%20and%20Toshiyuki%20Fukuzawa%20and%20Amin%20Ghafouri%20and%20Kenny%20Vassigh%20and%20Iain%20Barr%20and%20Zhicheng%20Wang%20and%20Anna%20Korsun%20and%20Rajesh%20Jayaram%20and%20Lijie%20Ren%20and%20Tim%20Zaman%20and%20Samira%20Khan%20and%20Yana%20Lunts%20and%20Dan%20Deutsch%20and%20Dave%20Uthus%20and%20Nitzan%20Katz%20and%20Masha%20Samsikova%20and%20Amr%20Khalifa%20and%20Nikhil%20Sethi%20and%20Jiao%20Sun%20and%20Luming%20Tang%20and%20Uri%20Alon%20and%20Xianghong%20Luo%20and%20Dian%20Yu%20and%20Abhishek%20Nayyar%20and%20Bryce%20Petrini%20and%20Will%20Truong%20and%20Vincent%20Hellendoorn%20and%20Nikolai%20Chinaev%20and%20Chris%20Alberti%20and%20Wei%20Wang%20and%20Jingcao%20Hu%20and%20Vahab%20Mirrokni%20and%20Ananth%20Balashankar%20and%20Avia%20Aharon%20and%20Aahil%20Mehta%20and%20Ahmet%20Iscen%20and%20Joseph%20Kready%20and%20Lucas%20Manning%20and%20Anhad%20Mohananey%20and%20Yuankai%20Chen%20and%20Anshuman%20Tripathi%20and%20Allen%20Wu%20and%20Igor%20Petrovski%20and%20Dawsen%20Hwang%20and%20Martin%20Baeuml%20and%20Shreyas%20Chandrakaladharan%20and%20Yuan%20Liu%20and%20Rey%20Coaguila%20and%20Maxwell%20Chen%20and%20Sally%20Ma%20and%20Pouya%20Tafti%20and%20Susheel%20Tatineni%20and%20Terry%20Spitz%20and%20Jiayu%20Ye%20and%20Paul%20Vicol%20and%20Mihaela%20Rosca%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20and%20Zohar%20Yahav%20and%20Sanjay%20Ghemawat%20and%20Hanzhao%20Lin%20and%20Phoebe%20Kirk%20and%20Zaid%20Nabulsi%20and%20Sergey%20Brin%20and%20Bernd%20Bohnet%20and%20Ken%20Caluwaerts%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Dan%20Zheng%20and%20Zihang%20Dai%20and%20Petre%20Petrov%20and%20Yichong%20Xu%20and%20Ramin%20Mehran%20and%20Zhuo%20Xu%20and%20Luisa%20Zintgraf%20and%20Jiho%20Choi%20and%20Spurthi%20Amba%20Hombaiah%20and%20Romal%20Thoppilan%20and%20Sashank%20Reddi%20and%20Lukasz%20Lew%20and%20Li%20Li%20and%20Kellie%20Webster%20and%20KP%20Sawhney%20and%20Lampros%20Lamprou%20and%20Siamak%20Shakeri%20and%20Mayank%20Lunayach%20and%20Jianmin%20Chen%20and%20Sumit%20Bagri%20and%20Alex%20Salcianu%20and%20Ying%20Chen%20and%20Yani%20Donchev%20and%20Charlotte%20Magister%20and%20Signe%20N%C3%B8rly%20and%20Vitor%20Rodrigues%20and%20Tomas%20Izo%20and%20Hila%20Noga%20and%20Joe%20Zou%20and%20Thomas%20K%C3%B6ppe%20and%20Wenxuan%20Zhou%20and%20Kenton%20Lee%20and%20Xiangzhu%20Long%20and%20Danielle%20Eisenbud%20and%20Anthony%20Chen%20and%20Connor%20Schenck%20and%20Chi%20Ming%20To%20and%20Peilin%20Zhong%20and%20Emanuel%20Taropa%20and%20Minh%20Truong%20and%20Omer%20Levy%20and%20Danilo%20Martins%20and%20Zhiyuan%20Zhang%20and%20Christopher%20Semturs%20and%20Kelvin%20Zhang%20and%20Alex%20Yakubovich%20and%20Pol%20Moreno%20and%20Lara%20McConnaughey%20and%20Di%20Lu%20and%20Sam%20Redmond%20and%20Lotte%20Weerts%20and%20Yonatan%20Bitton%20and%20Tiziana%20Refice%20and%20Nicolas%20Lacasse%20and%20Arthur%20Conmy%20and%20Corentin%20Tallec%20and%20Julian%20Odell%20and%20Hannah%20Forbes-Pollard%20and%20Arkadiusz%20Socala%20and%20Jonathan%20Hoech%20and%20Pushmeet%20Kohli%20and%20Alanna%20Walton%20and%20Rui%20Wang%20and%20Mikita%20Sazanovich%20and%20Kexin%20Zhu%20and%20Andrei%20Kapishnikov%20and%20Rich%20Galt%20and%20Matthew%20Denton%20and%20Ben%20Murdoch%20and%20Caitlin%20Sikora%20and%20Kareem%20Mohamed%20and%20Wei%20Wei%20and%20Uri%20First%20and%20Tim%20McConnell%20and%20Luis%20C.%20Cobo%20and%20James%20Qin%20and%20Thi%20Avrahami%20and%20Daniel%20Balle%20and%20Yu%20Watanabe%20and%20Annie%20Louis%20and%20Adam%20Kraft%20and%20Setareh%20Ariafar%20and%20Yiming%20Gu%20and%20Eug%C3%A9nie%20Rives%20and%20Charles%20Yoon%20and%20Andrei%20Rusu%20and%20James%20Cobon-Kerr%20and%20Chris%20Hahn%20and%20Jiaming%20Luo%20and%20%20Yuvein%20and%20%20Zhu%20and%20Niharika%20Ahuja%20and%20Rodrigo%20Benenson%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Honglin%20Yu%20and%20Lloyd%20Hightower%20and%20Junlin%20Zhang%20and%20Darren%20Ni%20and%20Lisa%20Anne%20Hendricks%20and%20Gabby%20Wang%20and%20Gal%20Yona%20and%20Lalit%20Jain%20and%20Pablo%20Barrio%20and%20Surya%20Bhupatiraju%20and%20Siva%20Velusamy%20and%20Allan%20Dafoe%20and%20Sebastian%20Riedel%20and%20Tara%20Thomas%20and%20Zhe%20Yuan%20and%20Mathias%20Bellaiche%20and%20Sheena%20Panthaplackel%20and%20Klemen%20Kloboves%20and%20Sarthak%20Jauhari%20and%20Canfer%20Akbulut%20and%20Todor%20Davchev%20and%20Evgeny%20Gladchenko%20and%20David%20Madras%20and%20Aleksandr%20Chuklin%20and%20Tyrone%20Hill%20and%20Quan%20Yuan%20and%20Mukundan%20Madhavan%20and%20Luke%20Leonhard%20and%20Dylan%20Scandinaro%20and%20Qihang%20Chen%20and%20Ning%20Niu%20and%20Arthur%20Douillard%20and%20Bogdan%20Damoc%20and%20Yasumasa%20Onoe%20and%20Fabian%20Pedregosa%20and%20Fred%20Bertsch%20and%20Chas%20Leichner%20and%20Joseph%20Pagadora%20and%20Jonathan%20Malmaud%20and%20Sameera%20Ponda%20and%20Andy%20Twigg%20and%20Oleksii%20Duzhyi%20and%20Jingwei%20Shen%20and%20Miaosen%20Wang%20and%20Roopal%20Garg%20and%20Jing%20Chen%20and%20Utku%20Evci%20and%20Jonathan%20Lee%20and%20Leon%20Liu%20and%20Koji%20Kojima%20and%20Masa%20Yamaguchi%20and%20Arunkumar%20Rajendran%20and%20AJ%20Piergiovanni%20and%20Vinodh%20Kumar%20Rajendran%20and%20Marco%20Fornoni%20and%20Gabriel%20Ibagon%20and%20Harry%20Ragan%20and%20Sadh%20MNM%20Khan%20and%20John%20Blitzer%20and%20Andrew%20Bunner%20and%20Guan%20Sun%20and%20Takahiro%20Kosakai%20and%20Scott%20Lundberg%20and%20Ndidi%20Elue%20and%20Kelvin%20Guu%20and%20SK%20Park%20and%20Jane%20Park%20and%20Arunachalam%20Narayanaswamy%20and%20Chengda%20Wu%20and%20Jayaram%20Mudigonda%20and%20Trevor%20Cohn%20and%20Hairong%20Mu%20and%20Ravi%20Kumar%20and%20Laura%20Graesser%20and%20Yichi%20Zhang%20and%20Richard%20Killam%20and%20Vincent%20Zhuang%20and%20Mai%20Gim%C3%A9nez%20and%20Wael%20Al%20Jishi%20and%20Ruy%20Ley-Wild%20and%20Alex%20Zhai%20and%20Kazuki%20Osawa%20and%20Diego%20Cedillo%20and%20Jialu%20Liu%20and%20Mayank%20Upadhyay%20and%20Marcin%20Sieniek%20and%20Roshan%20Sharma%20and%20Tom%20Paine%20and%20Anelia%20Angelova%20and%20Sravanti%20Addepalli%20and%20Carolina%20Parada%20and%20Kingshuk%20Majumder%20and%20Avery%20Lamp%20and%20Sanjiv%20Kumar%20and%20Xiang%20Deng%20and%20Artiom%20Myaskovsky%20and%20Tea%20Saboli%C4%87%20and%20Jeffrey%20Dudek%20and%20Sarah%20York%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Jiazhong%20Nie%20and%20Dee%20Cattle%20and%20Alok%20Gunjan%20and%20Bilal%20Piot%20and%20Waleed%20Khawaja%20and%20Seojin%20Bang%20and%20Simon%20Wang%20and%20Siavash%20Khodadadeh%20and%20Raghavender%20R%20and%20Praynaa%20Rawlani%20and%20Richard%20Powell%20and%20Kevin%20Lee%20and%20Johannes%20Griesser%20and%20GS%20Oh%20and%20Cesar%20Magalhaes%20and%20Yujia%20Li%20and%20Simon%20Tokumine%20and%20Hadas%20Natalie%20Vogel%20and%20Dennis%20Hsu%20and%20Arturo%20BC%20and%20Disha%20Jindal%20and%20Matan%20Cohen%20and%20Zi%20Yang%20and%20Junwei%20Yuan%20and%20Dario%20de%20Cesare%20and%20Tony%20Bruguier%20and%20Jun%20Xu%20and%20Monica%20Roy%20and%20Alon%20Jacovi%20and%20Dan%20Belov%20and%20Rahul%20Arya%20and%20Phoenix%20Meadowlark%20and%20Shlomi%20Cohen-Ganor%20and%20Wenting%20Ye%20and%20Patrick%20Morris-Suzuki%20and%20Praseem%20Banzal%20and%20Gan%20Song%20and%20Pranavaraj%20Ponnuramu%20and%20Fred%20Zhang%20and%20George%20Scrivener%20and%20Salah%20Zaiem%20and%20Alif%20Raditya%20Rochman%20and%20Kehang%20Han%20and%20Badih%20Ghazi%20and%20Kate%20Lee%20and%20Shahar%20Drath%20and%20Daniel%20Suo%20and%20Antonious%20Girgis%20and%20Pradeep%20Shenoy%20and%20Duy%20Nguyen%20and%20Douglas%20Eck%20and%20Somit%20Gupta%20and%20Le%20Yan%20and%20Joao%20Carreira%20and%20Anmol%20Gulati%20and%20Ruoxin%20Sang%20and%20Daniil%20Mirylenka%20and%20Emma%20Cooney%20and%20Edward%20Chou%20and%20Mingyang%20Ling%20and%20Cindy%20Fan%20and%20Ben%20Coleman%20and%20Guilherme%20Tubone%20and%20Ravin%20Kumar%20and%20Jason%20Baldridge%20and%20Felix%20Hernandez-Campos%20and%20Angeliki%20Lazaridou%20and%20James%20Besley%20and%20Itay%20Yona%20and%20Neslihan%20Bulut%20and%20Quentin%20Wellens%20and%20AJ%20Pierigiovanni%20and%20Jasmine%20George%20and%20Richard%20Green%20and%20Pu%20Han%20and%20Connie%20Tao%20and%20Geoff%20Clark%20and%20Chong%20You%20and%20Abbas%20Abdolmaleki%20and%20Justin%20Fu%20and%20Tongzhou%20Chen%20and%20Ashwin%20Chaugule%20and%20Angad%20Chandorkar%20and%20Altaf%20Rahman%20and%20Will%20Thompson%20and%20Penporn%20Koanantakool%20and%20Mike%20Bernico%20and%20Jie%20Ren%20and%20Andrey%20Vlasov%20and%20Sergei%20Vassilvitskii%20and%20Maciej%20Kula%20and%20Yizhong%20Liang%20and%20Dahun%20Kim%20and%20Yangsibo%20Huang%20and%20Chengxi%20Ye%20and%20Dmitry%20Lepikhin%20and%20Wesley%20Helmholz%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%202.X%20model%20family%3A%20Gemini%202.5%20Pro%20and%0AGemini%202.5%20Flash%2C%20as%20well%20as%20our%20earlier%20Gemini%202.0%20Flash%20and%20Flash-Lite%0Amodels.%20Gemini%202.5%20Pro%20is%20our%20most%20capable%20model%20yet%2C%20achieving%20SoTA%0Aperformance%20on%20frontier%20coding%20and%20reasoning%20benchmarks.%20In%20addition%20to%20its%0Aincredible%20coding%20and%20reasoning%20skills%2C%20Gemini%202.5%20Pro%20is%20a%20thinking%20model%20that%0Aexcels%20at%20multimodal%20understanding%20and%20it%20is%20now%20able%20to%20process%20up%20to%203%20hours%0Aof%20video%20content.%20Its%20unique%20combination%20of%20long%20context%2C%20multimodal%20and%0Areasoning%20capabilities%20can%20be%20combined%20to%20unlock%20new%20agentic%20workflows.%20Gemini%0A2.5%20Flash%20provides%20excellent%20reasoning%20abilities%20at%20a%20fraction%20of%20the%20compute%0Aand%20latency%20requirements%20and%20Gemini%202.0%20Flash%20and%20Flash-Lite%20provide%20high%0Aperformance%20at%20low%20latency%20and%20cost.%20Taken%20together%2C%20the%20Gemini%202.X%20model%0Ageneration%20spans%20the%20full%20Pareto%20frontier%20of%20model%20capability%20vs%20cost%2C%20allowing%0Ausers%20to%20explore%20the%20boundaries%20of%20what%20is%20possible%20with%20complex%20agentic%0Aproblem%20solving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06261v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25202.5%253A%2520Pushing%2520the%2520Frontier%2520with%2520Advanced%2520Reasoning%252C%2520Multimodality%252C%250A%2520%2520Long%2520Context%252C%2520and%2520Next%2520Generation%2520Agentic%2520Capabilities%26entry.906535625%3DGheorghe%2520Comanici%2520and%2520Eric%2520Bieber%2520and%2520Mike%2520Schaekermann%2520and%2520Ice%2520Pasupat%2520and%2520Noveen%2520Sachdeva%2520and%2520Inderjit%2520Dhillon%2520and%2520Marcel%2520Blistein%2520and%2520Ori%2520Ram%2520and%2520Dan%2520Zhang%2520and%2520Evan%2520Rosen%2520and%2520Luke%2520Marris%2520and%2520Sam%2520Petulla%2520and%2520Colin%2520Gaffney%2520and%2520Asaf%2520Aharoni%2520and%2520Nathan%2520Lintz%2520and%2520Tiago%2520Cardal%2520Pais%2520and%2520Henrik%2520Jacobsson%2520and%2520Idan%2520Szpektor%2520and%2520Nan-Jiang%2520Jiang%2520and%2520Krishna%2520Haridasan%2520and%2520Ahmed%2520Omran%2520and%2520Nikunj%2520Saunshi%2520and%2520Dara%2520Bahri%2520and%2520Gaurav%2520Mishra%2520and%2520Eric%2520Chu%2520and%2520Toby%2520Boyd%2520and%2520Brad%2520Hekman%2520and%2520Aaron%2520Parisi%2520and%2520Chaoyi%2520Zhang%2520and%2520Kornraphop%2520Kawintiranon%2520and%2520Tania%2520Bedrax-Weiss%2520and%2520Oliver%2520Wang%2520and%2520Ya%2520Xu%2520and%2520Ollie%2520Purkiss%2520and%2520Uri%2520Mendlovic%2520and%2520Ila%25C3%25AF%2520Deutel%2520and%2520Nam%2520Nguyen%2520and%2520Adam%2520Langley%2520and%2520Flip%2520Korn%2520and%2520Lucia%2520Rossazza%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Sagar%2520Waghmare%2520and%2520Helen%2520Miller%2520and%2520Nathan%2520Byrd%2520and%2520Ashrith%2520Sheshan%2520and%2520Raia%2520Hadsell%2520and%2520Sangnie%2520Bhardwaj%2520and%2520Pawel%2520Janus%2520and%2520Tero%2520Rissa%2520and%2520Dan%2520Horgan%2520and%2520Alvin%2520Abdagic%2520and%2520Lior%2520Belenki%2520and%2520James%2520Allingham%2520and%2520Anima%2520Singh%2520and%2520Theo%2520Guidroz%2520and%2520Srivatsan%2520Srinivasan%2520and%2520Herman%2520Schmit%2520and%2520Kristen%2520Chiafullo%2520and%2520Andre%2520Elisseeff%2520and%2520Nilpa%2520Jha%2520and%2520Prateek%2520Kolhar%2520and%2520Leonard%2520Berrada%2520and%2520Frank%2520Ding%2520and%2520Xiance%2520Si%2520and%2520Shrestha%2520Basu%2520Mallick%2520and%2520Franz%2520Och%2520and%2520Sofia%2520Erell%2520and%2520Eric%2520Ni%2520and%2520Tejasi%2520Latkar%2520and%2520Sherry%2520Yang%2520and%2520Petar%2520Sirkovic%2520and%2520Ziqiang%2520Feng%2520and%2520Robert%2520Leland%2520and%2520Rachel%2520Hornung%2520and%2520Gang%2520Wu%2520and%2520Charles%2520Blundell%2520and%2520Hamidreza%2520Alvari%2520and%2520Po-Sen%2520Huang%2520and%2520Cathy%2520Yip%2520and%2520Sanja%2520Deur%2520and%2520Li%2520Liu%2520and%2520Gabriela%2520Surita%2520and%2520Pablo%2520Duque%2520and%2520Dima%2520Damen%2520and%2520Johnson%2520Jia%2520and%2520Arthur%2520Guez%2520and%2520Markus%2520Mircea%2520and%2520Animesh%2520Sinha%2520and%2520Alberto%2520Magni%2520and%2520Pawe%25C5%2582%2520Stradomski%2520and%2520Tal%2520Marian%2520and%2520Vlado%2520Gali%25C4%2587%2520and%2520Wenhu%2520Chen%2520and%2520Hisham%2520Husain%2520and%2520Achintya%2520Singhal%2520and%2520Dominik%2520Grewe%2520and%2520Fran%25C3%25A7ois-Xavier%2520Aubet%2520and%2520Shuang%2520Song%2520and%2520Lorenzo%2520Blanco%2520and%2520Leland%2520Rechis%2520and%2520Lewis%2520Ho%2520and%2520Rich%2520Munoz%2520and%2520Kelvin%2520Zheng%2520and%2520Jessica%2520Hamrick%2520and%2520Kevin%2520Mather%2520and%2520Hagai%2520Taitelbaum%2520and%2520Eliza%2520Rutherford%2520and%2520Yun%2520Lei%2520and%2520Kuangyuan%2520Chen%2520and%2520Anand%2520Shukla%2520and%2520Erica%2520Moreira%2520and%2520Eric%2520Doi%2520and%2520Berivan%2520Isik%2520and%2520Nir%2520Shabat%2520and%2520Dominika%2520Rogozi%25C5%2584ska%2520and%2520Kashyap%2520Kolipaka%2520and%2520Jason%2520Chang%2520and%2520Eugen%2520Vu%25C5%25A1ak%2520and%2520Srinivasan%2520Venkatachary%2520and%2520Shadi%2520Noghabi%2520and%2520Tarun%2520Bharti%2520and%2520Younghoon%2520Jun%2520and%2520Aleksandr%2520Zaks%2520and%2520Simon%2520Green%2520and%2520Jeshwanth%2520Challagundla%2520and%2520William%2520Wong%2520and%2520Muqthar%2520Mohammad%2520and%2520Dean%2520Hirsch%2520and%2520Yong%2520Cheng%2520and%2520Iftekhar%2520Naim%2520and%2520Lev%2520Proleev%2520and%2520Damien%2520Vincent%2520and%2520Aayush%2520Singh%2520and%2520Maxim%2520Krikun%2520and%2520Dilip%2520Krishnan%2520and%2520Zoubin%2520Ghahramani%2520and%2520Aviel%2520Atias%2520and%2520Rajeev%2520Aggarwal%2520and%2520Christo%2520Kirov%2520and%2520Dimitrios%2520Vytiniotis%2520and%2520Christy%2520Koh%2520and%2520Alexandra%2520Chronopoulou%2520and%2520Pawan%2520Dogra%2520and%2520Vlad-Doru%2520Ion%2520and%2520Gladys%2520Tyen%2520and%2520Jason%2520Lee%2520and%2520Felix%2520Weissenberger%2520and%2520Trevor%2520Strohman%2520and%2520Ashwin%2520Balakrishna%2520and%2520Jack%2520Rae%2520and%2520Marko%2520Velic%2520and%2520Raoul%2520de%2520Liedekerke%2520and%2520Oded%2520Elyada%2520and%2520Wentao%2520Yuan%2520and%2520Canoee%2520Liu%2520and%2520Lior%2520Shani%2520and%2520Sergey%2520Kishchenko%2520and%2520Bea%2520Alessio%2520and%2520Yandong%2520Li%2520and%2520Richard%2520Song%2520and%2520Sam%2520Kwei%2520and%2520Orion%2520Jankowski%2520and%2520Aneesh%2520Pappu%2520and%2520Youhei%2520Namiki%2520and%2520Yenai%2520Ma%2520and%2520Nilesh%2520Tripuraneni%2520and%2520Colin%2520Cherry%2520and%2520Marissa%2520Ikonomidis%2520and%2520Yu-Cheng%2520Ling%2520and%2520Colin%2520Ji%2520and%2520Beka%2520Westberg%2520and%2520Auriel%2520Wright%2520and%2520Da%2520Yu%2520and%2520David%2520Parkinson%2520and%2520Swaroop%2520Ramaswamy%2520and%2520Jerome%2520Connor%2520and%2520Soheil%2520Hassas%2520Yeganeh%2520and%2520Snchit%2520Grover%2520and%2520George%2520Kenwright%2520and%2520Lubo%2520Litchev%2520and%2520Chris%2520Apps%2520and%2520Alex%2520Tomala%2520and%2520Felix%2520Halim%2520and%2520Alex%2520Castro-Ros%2520and%2520Zefei%2520Li%2520and%2520Anudhyan%2520Boral%2520and%2520Pauline%2520Sho%2520and%2520Michal%2520Yarom%2520and%2520Eric%2520Malmi%2520and%2520David%2520Klinghoffer%2520and%2520Rebecca%2520Lin%2520and%2520Alan%2520Ansell%2520and%2520Pradeep%2520Kumar%2520S%2520and%2520Shubin%2520Zhao%2520and%2520Siqi%2520Zuo%2520and%2520Adam%2520Santoro%2520and%2520Heng-Tze%2520Cheng%2520and%2520Solomon%2520Demmessie%2520and%2520Yuchi%2520Liu%2520and%2520Nicole%2520Brichtova%2520and%2520Allie%2520Culp%2520and%2520Nathaniel%2520Braun%2520and%2520Dan%2520Graur%2520and%2520Will%2520Ng%2520and%2520Nikhil%2520Mehta%2520and%2520Aaron%2520Phillips%2520and%2520Patrik%2520Sundberg%2520and%2520Varun%2520Godbole%2520and%2520Fangyu%2520Liu%2520and%2520Yash%2520Katariya%2520and%2520David%2520Rim%2520and%2520Mojtaba%2520Seyedhosseini%2520and%2520Sean%2520Ammirati%2520and%2520Jonas%2520Valfridsson%2520and%2520Mahan%2520Malihi%2520and%2520Timothy%2520Knight%2520and%2520Andeep%2520Toor%2520and%2520Thomas%2520Lampe%2520and%2520Abe%2520Ittycheriah%2520and%2520Lewis%2520Chiang%2520and%2520Chak%2520Yeung%2520and%2520Alexandre%2520Fr%25C3%25A9chette%2520and%2520Jinmeng%2520Rao%2520and%2520Huisheng%2520Wang%2520and%2520Himanshu%2520Srivastava%2520and%2520Richard%2520Zhang%2520and%2520Rocky%2520Rhodes%2520and%2520Ariel%2520Brand%2520and%2520Dean%2520Weesner%2520and%2520Ilya%2520Figotin%2520and%2520Felix%2520Gimeno%2520and%2520Rachana%2520Fellinger%2520and%2520Pierre%2520Marcenac%2520and%2520Jos%25C3%25A9%2520Leal%2520and%2520Eyal%2520Marcus%2520and%2520Victor%2520Cotruta%2520and%2520Rodrigo%2520Cabrera%2520and%2520Sheryl%2520Luo%2520and%2520Dan%2520Garrette%2520and%2520Vera%2520Axelrod%2520and%2520Sorin%2520Baltateanu%2520and%2520David%2520Barker%2520and%2520Dongkai%2520Chen%2520and%2520Horia%2520Toma%2520and%2520Ben%2520Ingram%2520and%2520Jason%2520Riesa%2520and%2520Chinmay%2520Kulkarni%2520and%2520Yujing%2520Zhang%2520and%2520Hongbin%2520Liu%2520and%2520Chao%2520Wang%2520and%2520Martin%2520Polacek%2520and%2520Will%2520Wu%2520and%2520Kai%2520Hui%2520and%2520Adrian%2520N%2520Reyes%2520and%2520Yi%2520Su%2520and%2520Megan%2520Barnes%2520and%2520Ishaan%2520Malhi%2520and%2520Anfal%2520Siddiqui%2520and%2520Qixuan%2520Feng%2520and%2520Mihai%2520Damaschin%2520and%2520Daniele%2520Pighin%2520and%2520Andreas%2520Steiner%2520and%2520Samuel%2520Yang%2520and%2520Ramya%2520Sree%2520Boppana%2520and%2520Simeon%2520Ivanov%2520and%2520Arun%2520Kandoor%2520and%2520Aditya%2520Shah%2520and%2520Asier%2520Mujika%2520and%2520Da%2520Huang%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Mohak%2520Patel%2520and%2520Tianhe%2520Yu%2520and%2520Toni%2520Creswell%2520and%2520%2520Jerry%2520and%2520%2520Liu%2520and%2520Catarina%2520Barros%2520and%2520Yasaman%2520Razeghi%2520and%2520Aurko%2520Roy%2520and%2520Phil%2520Culliton%2520and%2520Binbin%2520Xiong%2520and%2520Jiaqi%2520Pan%2520and%2520Thomas%2520Strohmann%2520and%2520Tolly%2520Powell%2520and%2520Babi%2520Seal%2520and%2520Doug%2520DeCarlo%2520and%2520Pranav%2520Shyam%2520and%2520Kaan%2520Katircioglu%2520and%2520Xuezhi%2520Wang%2520and%2520Cassidy%2520Hardin%2520and%2520Immanuel%2520Odisho%2520and%2520Josef%2520Broder%2520and%2520Oscar%2520Chang%2520and%2520Arun%2520Nair%2520and%2520Artem%2520Shtefan%2520and%2520Maura%2520O%2527Brien%2520and%2520Manu%2520Agarwal%2520and%2520Sahitya%2520Potluri%2520and%2520Siddharth%2520Goyal%2520and%2520Amit%2520Jhindal%2520and%2520Saksham%2520Thakur%2520and%2520Yury%2520Stuken%2520and%2520James%2520Lyon%2520and%2520Kristina%2520Toutanova%2520and%2520Fangxiaoyu%2520Feng%2520and%2520Austin%2520Wu%2520and%2520Ben%2520Horn%2520and%2520Alek%2520Wang%2520and%2520Alex%2520Cullum%2520and%2520Gabe%2520Taubman%2520and%2520Disha%2520Shrivastava%2520and%2520Chongyang%2520Shi%2520and%2520Hamish%2520Tomlinson%2520and%2520Roma%2520Patel%2520and%2520Tao%2520Tu%2520and%2520Ada%2520Maksutaj%2520Oflazer%2520and%2520Francesco%2520Pongetti%2520and%2520Mingyao%2520Yang%2520and%2520Adrien%2520Ali%2520Ta%25C3%25AFga%2520and%2520Vincent%2520Perot%2520and%2520Nuo%2520Wang%2520Pierse%2520and%2520Feng%2520Han%2520and%2520Yoel%2520Drori%2520and%2520I%25C3%25B1aki%2520Iturrate%2520and%2520Ayan%2520Chakrabarti%2520and%2520Legg%2520Yeung%2520and%2520Dave%2520Dopson%2520and%2520Yi-ting%2520Chen%2520and%2520Apoorv%2520Kulshreshtha%2520and%2520Tongfei%2520Guo%2520and%2520Philip%2520Pham%2520and%2520Tal%2520Schuster%2520and%2520Junquan%2520Chen%2520and%2520Alex%2520Polozov%2520and%2520Jinwei%2520Xing%2520and%2520Huanjie%2520Zhou%2520and%2520Praneeth%2520Kacham%2520and%2520Doron%2520Kukliansky%2520and%2520Antoine%2520Miech%2520and%2520Sergey%2520Yaroshenko%2520and%2520Ed%2520Chi%2520and%2520Sholto%2520Douglas%2520and%2520Hongliang%2520Fei%2520and%2520Mathieu%2520Blondel%2520and%2520Preethi%2520Myla%2520and%2520Lior%2520Madmoni%2520and%2520Xing%2520Wu%2520and%2520Daniel%2520Keysers%2520and%2520Kristian%2520Kjems%2520and%2520Isabela%2520Albuquerque%2520and%2520Lijun%2520Yu%2520and%2520Joel%2520D%2527sa%2520and%2520Michelle%2520Plantan%2520and%2520Vlad%2520Ionescu%2520and%2520Jaume%2520Sanchez%2520Elias%2520and%2520Abhirut%2520Gupta%2520and%2520Manish%2520Reddy%2520Vuyyuru%2520and%2520Fred%2520Alcober%2520and%2520Tong%2520Zhou%2520and%2520Kaiyang%2520Ji%2520and%2520Florian%2520Hartmann%2520and%2520Subha%2520Puttagunta%2520and%2520Hugo%2520Song%2520and%2520Ehsan%2520Amid%2520and%2520Anca%2520Stefanoiu%2520and%2520Andrew%2520Lee%2520and%2520Paul%2520Pucciarelli%2520and%2520Emma%2520Wang%2520and%2520Amit%2520Raul%2520and%2520Slav%2520Petrov%2520and%2520Isaac%2520Tian%2520and%2520Valentin%2520Anklin%2520and%2520Nana%2520Nti%2520and%2520Victor%2520Gomes%2520and%2520Max%2520Schumacher%2520and%2520Grace%2520Vesom%2520and%2520Alex%2520Panagopoulos%2520and%2520Konstantinos%2520Bousmalis%2520and%2520Daniel%2520Andor%2520and%2520Josh%2520Jacob%2520and%2520Yuan%2520Zhang%2520and%2520Bill%2520Rosgen%2520and%2520Matija%2520Kecman%2520and%2520Matthew%2520Tung%2520and%2520Alexandra%2520Belias%2520and%2520Noah%2520Goodman%2520and%2520Paul%2520Covington%2520and%2520Brian%2520Wieder%2520and%2520Nikita%2520Saxena%2520and%2520Elnaz%2520Davoodi%2520and%2520Muhuan%2520Huang%2520and%2520Sharath%2520Maddineni%2520and%2520Vincent%2520Roulet%2520and%2520Folawiyo%2520Campbell-Ajala%2520and%2520Pier%2520Giuseppe%2520Sessa%2520and%2520%2520Xintian%2520and%2520%2520Wu%2520and%2520Guangda%2520Lai%2520and%2520Paul%2520Collins%2520and%2520Alex%2520Haig%2520and%2520Vytenis%2520Sakenas%2520and%2520Xiaowei%2520Xu%2520and%2520Marissa%2520Giustina%2520and%2520Laurent%2520El%2520Shafey%2520and%2520Pichi%2520Charoenpanit%2520and%2520Shefali%2520Garg%2520and%2520Joshua%2520Ainslie%2520and%2520Boone%2520Severson%2520and%2520Montse%2520Gonzalez%2520Arenas%2520and%2520Shreya%2520Pathak%2520and%2520Sujee%2520Rajayogam%2520and%2520Jie%2520Feng%2520and%2520Michiel%2520Bakker%2520and%2520Sheng%2520Li%2520and%2520Nevan%2520Wichers%2520and%2520Jamie%2520Rogers%2520and%2520Xinyang%2520Geng%2520and%2520Yeqing%2520Li%2520and%2520Rolf%2520Jagerman%2520and%2520Chao%2520Jia%2520and%2520Nadav%2520Olmert%2520and%2520David%2520Sharon%2520and%2520Matthew%2520Mauger%2520and%2520Sandeep%2520Mariserla%2520and%2520Hongxu%2520Ma%2520and%2520Megha%2520Mohabey%2520and%2520Kyuyeun%2520Kim%2520and%2520Alek%2520Andreev%2520and%2520Scott%2520Pollom%2520and%2520Juliette%2520Love%2520and%2520Vihan%2520Jain%2520and%2520Priyanka%2520Agrawal%2520and%2520Yannick%2520Schroecker%2520and%2520Alisa%2520Fortin%2520and%2520Manfred%2520Warmuth%2520and%2520Ji%2520Liu%2520and%2520Andrew%2520Leach%2520and%2520Irina%2520Blok%2520and%2520Ganesh%2520Poomal%2520Girirajan%2520and%2520Roee%2520Aharoni%2520and%2520Benigno%2520Uria%2520and%2520Andrei%2520Sozanschi%2520and%2520Dan%2520Goldberg%2520and%2520Lucian%2520Ionita%2520and%2520Marco%2520Tulio%2520Ribeiro%2520and%2520Martin%2520Zlocha%2520and%2520Vighnesh%2520Birodkar%2520and%2520Sami%2520Lachgar%2520and%2520Liangzhe%2520Yuan%2520and%2520Himadri%2520Choudhury%2520and%2520Matt%2520Ginsberg%2520and%2520Fei%2520Zheng%2520and%2520Gregory%2520Dibb%2520and%2520Emily%2520Graves%2520and%2520Swachhand%2520Lokhande%2520and%2520Gabriel%2520Rasskin%2520and%2520George-Cristian%2520Muraru%2520and%2520Corbin%2520Quick%2520and%2520Sandeep%2520Tata%2520and%2520Pierre%2520Sermanet%2520and%2520Aditya%2520Chawla%2520and%2520Itay%2520Karo%2520and%2520Yan%2520Wang%2520and%2520Susan%2520Zhang%2520and%2520Orgad%2520Keller%2520and%2520Anca%2520Dragan%2520and%2520Guolong%2520Su%2520and%2520Ian%2520Chou%2520and%2520Xi%2520Liu%2520and%2520Yiqing%2520Tao%2520and%2520Shruthi%2520Prabhakara%2520and%2520Marc%2520Wilson%2520and%2520Ruibo%2520Liu%2520and%2520Shibo%2520Wang%2520and%2520Georgie%2520Evans%2520and%2520David%2520Du%2520and%2520Alfonso%2520Casta%25C3%25B1o%2520and%2520Gautam%2520Prasad%2520and%2520Mona%2520El%2520Mahdy%2520and%2520Sebastian%2520Gerlach%2520and%2520Machel%2520Reid%2520and%2520Jarrod%2520Kahn%2520and%2520Amir%2520Zait%2520and%2520Thanumalayan%2520Sankaranarayana%2520Pillai%2520and%2520Thatcher%2520Ulrich%2520and%2520Guanyu%2520Wang%2520and%2520Jan%2520Wassenberg%2520and%2520Efrat%2520Farkash%2520and%2520Kiran%2520Yalasangi%2520and%2520Congchao%2520Wang%2520and%2520Maria%2520Bauza%2520and%2520Simon%2520Bucher%2520and%2520Ting%2520Liu%2520and%2520Jun%2520Yan%2520and%2520Gary%2520Leung%2520and%2520Vikas%2520Sindhwani%2520and%2520Parker%2520Barnes%2520and%2520Avi%2520Singh%2520and%2520Ivan%2520Jurin%2520and%2520Jichuan%2520Chang%2520and%2520Niket%2520Kumar%2520Bhumihar%2520and%2520Sivan%2520Eiger%2520and%2520Gui%2520Citovsky%2520and%2520Ben%2520Withbroe%2520and%2520Zhang%2520Li%2520and%2520Siyang%2520Xue%2520and%2520Niccol%25C3%25B2%2520Dal%2520Santo%2520and%2520Georgi%2520Stoyanov%2520and%2520Yves%2520Raimond%2520and%2520Steven%2520Zheng%2520and%2520Yilin%2520Gao%2520and%2520V%25C3%25ADt%2520List%25C3%25ADk%2520and%2520S%25C5%2582awek%2520Kwasiborski%2520and%2520Rachel%2520Saputro%2520and%2520Adnan%2520Ozturel%2520and%2520Ganesh%2520Mallya%2520and%2520Kushal%2520Majmundar%2520and%2520Ross%2520West%2520and%2520Paul%2520Caron%2520and%2520Jinliang%2520Wei%2520and%2520Lluis%2520Castrejon%2520and%2520Sharad%2520Vikram%2520and%2520Deepak%2520Ramachandran%2520and%2520Nikhil%2520Dhawan%2520and%2520Jiho%2520Park%2520and%2520Sara%2520Smoot%2520and%2520George%2520van%2520den%2520Driessche%2520and%2520Yochai%2520Blau%2520and%2520Chase%2520Malik%2520and%2520Wei%2520Liang%2520and%2520Roy%2520Hirsch%2520and%2520Cicero%2520Nogueira%2520dos%2520Santos%2520and%2520Eugene%2520Weinstein%2520and%2520A%25C3%25A4ron%2520van%2520den%2520Oord%2520and%2520Sid%2520Lall%2520and%2520Nicholas%2520FitzGerald%2520and%2520Zixuan%2520Jiang%2520and%2520Xuan%2520Yang%2520and%2520Dale%2520Webster%2520and%2520Ali%2520Elqursh%2520and%2520Aedan%2520Pope%2520and%2520Georges%2520Rotival%2520and%2520David%2520Raposo%2520and%2520Wanzheng%2520Zhu%2520and%2520Jeff%2520Dean%2520and%2520Sami%2520Alabed%2520and%2520Dustin%2520Tran%2520and%2520Arushi%2520Gupta%2520and%2520Zach%2520Gleicher%2520and%2520Jessica%2520Austin%2520and%2520Edouard%2520Rosseel%2520and%2520Megh%2520Umekar%2520and%2520Dipanjan%2520Das%2520and%2520Yinghao%2520Sun%2520and%2520Kai%2520Chen%2520and%2520Karolis%2520Misiunas%2520and%2520Xiang%2520Zhou%2520and%2520Yixian%2520Di%2520and%2520Alyssa%2520Loo%2520and%2520Josh%2520Newlan%2520and%2520Bo%2520Li%2520and%2520Vinay%2520Ramasesh%2520and%2520Ying%2520Xu%2520and%2520Alex%2520Chen%2520and%2520Sudeep%2520Gandhe%2520and%2520Radu%2520Soricut%2520and%2520Nikita%2520Gupta%2520and%2520Shuguang%2520Hu%2520and%2520Seliem%2520El-Sayed%2520and%2520Xavier%2520Garcia%2520and%2520Idan%2520Brusilovsky%2520and%2520Pu-Chin%2520Chen%2520and%2520Andrew%2520Bolt%2520and%2520Lu%2520Huang%2520and%2520Alex%2520Gurney%2520and%2520Zhiying%2520Zhang%2520and%2520Alexander%2520Pritzel%2520and%2520Jarek%2520Wilkiewicz%2520and%2520Bryan%2520Seybold%2520and%2520Bhargav%2520Kanagal%2520Shamanna%2520and%2520Felix%2520Fischer%2520and%2520Josef%2520Dean%2520and%2520Karan%2520Gill%2520and%2520Ross%2520Mcilroy%2520and%2520Abhishek%2520Bhowmick%2520and%2520Jeremy%2520Selier%2520and%2520Antoine%2520Yang%2520and%2520Derek%2520Cheng%2520and%2520Vladimir%2520Magay%2520and%2520Jie%2520Tan%2520and%2520Dhriti%2520Varma%2520and%2520Christian%2520Walder%2520and%2520Tomas%2520Kocisky%2520and%2520Ryo%2520Nakashima%2520and%2520Paul%2520Natsev%2520and%2520Mike%2520Kwong%2520and%2520Ionel%2520Gog%2520and%2520Chiyuan%2520Zhang%2520and%2520Sander%2520Dieleman%2520and%2520Thomas%2520Jimma%2520and%2520Andrey%2520Ryabtsev%2520and%2520Siddhartha%2520Brahma%2520and%2520David%2520Steiner%2520and%2520Dayou%2520Du%2520and%2520Ante%2520%25C5%25BDu%25C5%25BEul%2520and%2520Mislav%2520%25C5%25BDani%25C4%2587%2520and%2520Mukund%2520Raghavachari%2520and%2520Willi%2520Gierke%2520and%2520Zeyu%2520Zheng%2520and%2520Dessie%2520Petrova%2520and%2520Yann%2520Dauphin%2520and%2520Yuchuan%2520Liu%2520and%2520Ido%2520Kessler%2520and%2520Steven%2520Hand%2520and%2520Chris%2520Duvarney%2520and%2520Seokhwan%2520Kim%2520and%2520Hyo%2520Lee%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Jeffrey%2520Hui%2520and%2520Josh%2520Smith%2520and%2520Deepali%2520Jain%2520and%2520Jiawei%2520Xia%2520and%2520Gaurav%2520Singh%2520Tomar%2520and%2520Keyvan%2520Amiri%2520and%2520Du%2520Phan%2520and%2520Fabian%2520Fuchs%2520and%2520Tobias%2520Weyand%2520and%2520Nenad%2520Tomasev%2520and%2520Alexandra%2520Cordell%2520and%2520Xin%2520Liu%2520and%2520Jonathan%2520Mallinson%2520and%2520Pankaj%2520Joshi%2520and%2520Andy%2520Crawford%2520and%2520Arun%2520Suggala%2520and%2520Steve%2520Chien%2520and%2520Nick%2520Fernando%2520and%2520Mariella%2520Sanchez-Vargas%2520and%2520Duncan%2520Williams%2520and%2520Phil%2520Crone%2520and%2520Xiyang%2520Luo%2520and%2520Igor%2520Karpov%2520and%2520Jyn%2520Shan%2520and%2520Terry%2520Thurk%2520and%2520Robin%2520Strudel%2520and%2520Paul%2520Voigtlaender%2520and%2520Piyush%2520Patil%2520and%2520Tim%2520Dozat%2520and%2520Ali%2520Khodaei%2520and%2520Sahil%2520Singla%2520and%2520Piotr%2520Ambroszczyk%2520and%2520Qiyin%2520Wu%2520and%2520Yifan%2520Chang%2520and%2520Brian%2520Roark%2520and%2520Chaitra%2520Hegde%2520and%2520Tianli%2520Ding%2520and%2520Angelos%2520Filos%2520and%2520Zhongru%2520Wu%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Shuang%2520Liu%2520and%2520Saarthak%2520Khanna%2520and%2520Aditya%2520Pandey%2520and%2520Siobhan%2520Mcloughlin%2520and%2520Qiujia%2520Li%2520and%2520Sam%2520Haves%2520and%2520Allan%2520Zhou%2520and%2520Elena%2520Buchatskaya%2520and%2520Isabel%2520Leal%2520and%2520Peter%2520de%2520Boursac%2520and%2520Nami%2520Akazawa%2520and%2520Nina%2520Anderson%2520and%2520Terry%2520Chen%2520and%2520Krishna%2520Somandepalli%2520and%2520Chen%2520Liang%2520and%2520Sheela%2520Goenka%2520and%2520Stephanie%2520Winkler%2520and%2520Alexander%2520Grushetsky%2520and%2520Yifan%2520Ding%2520and%2520Jamie%2520Smith%2520and%2520Fan%2520Ye%2520and%2520Jordi%2520Pont-Tuset%2520and%2520Eric%2520Li%2520and%2520Ruichao%2520Li%2520and%2520Tomer%2520Golany%2520and%2520Dawid%2520Wegner%2520and%2520Tao%2520Jiang%2520and%2520Omer%2520Barak%2520and%2520Yuan%2520Shangguan%2520and%2520Eszter%2520V%25C3%25A9rtes%2520and%2520Renee%2520Wong%2520and%2520J%25C3%25B6rg%2520Bornschein%2520and%2520Alex%2520Tudor%2520and%2520Michele%2520Bevilacqua%2520and%2520Tom%2520Schaul%2520and%2520Ankit%2520Singh%2520Rawat%2520and%2520Yang%2520Zhao%2520and%2520Kyriakos%2520Axiotis%2520and%2520Lei%2520Meng%2520and%2520Cory%2520McLean%2520and%2520Jonathan%2520Lai%2520and%2520Jennifer%2520Beattie%2520and%2520Nate%2520Kushman%2520and%2520Yaxin%2520Liu%2520and%2520Blair%2520Kutzman%2520and%2520Fiona%2520Lang%2520and%2520Jingchen%2520Ye%2520and%2520Praneeth%2520Netrapalli%2520and%2520Pushkar%2520Mishra%2520and%2520Myriam%2520Khan%2520and%2520Megha%2520Goel%2520and%2520Rob%2520Willoughby%2520and%2520David%2520Tian%2520and%2520Honglei%2520Zhuang%2520and%2520JD%2520Chen%2520and%2520Zak%2520Tsai%2520and%2520Tasos%2520Kementsietsidis%2520and%2520Arjun%2520Khare%2520and%2520James%2520Keeling%2520and%2520Keyang%2520Xu%2520and%2520Nathan%2520Waters%2520and%2520Florent%2520Altch%25C3%25A9%2520and%2520Ashok%2520Popat%2520and%2520Bhavishya%2520Mittal%2520and%2520David%2520Saxton%2520and%2520Dalia%2520El%2520Badawy%2520and%2520Michael%2520Mathieu%2520and%2520Zheng%2520Zheng%2520and%2520Hao%2520Zhou%2520and%2520Nishant%2520Ranka%2520and%2520Richard%2520Shin%2520and%2520Qingnan%2520Duan%2520and%2520Tim%2520Salimans%2520and%2520Ioana%2520Mihailescu%2520and%2520Uri%2520Shaham%2520and%2520Ming-Wei%2520Chang%2520and%2520Yannis%2520Assael%2520and%2520Nishanth%2520Dikkala%2520and%2520Martin%2520Izzard%2520and%2520Vincent%2520Cohen-Addad%2520and%2520Cat%2520Graves%2520and%2520Vlad%2520Feinberg%2520and%2520Grace%2520Chung%2520and%2520DJ%2520Strouse%2520and%2520Danny%2520Karmon%2520and%2520Sahand%2520Sharifzadeh%2520and%2520Zoe%2520Ashwood%2520and%2520Khiem%2520Pham%2520and%2520Jon%2520Blanton%2520and%2520Alex%2520Vasiloff%2520and%2520Jarred%2520Barber%2520and%2520Mark%2520Geller%2520and%2520Aurick%2520Zhou%2520and%2520Fedir%2520Zubach%2520and%2520Tzu-Kuo%2520Huang%2520and%2520Lei%2520Zhang%2520and%2520Himanshu%2520Gupta%2520and%2520Matt%2520Young%2520and%2520Julia%2520Proskurnia%2520and%2520Ronny%2520Votel%2520and%2520Valentin%2520Gabeur%2520and%2520Gabriel%2520Barcik%2520and%2520Aditya%2520Tripathi%2520and%2520Hongkun%2520Yu%2520and%2520Geng%2520Yan%2520and%2520Beer%2520Changpinyo%2520and%2520Filip%2520Paveti%25C4%2587%2520and%2520Amy%2520Coyle%2520and%2520Yasuhisa%2520Fujii%2520and%2520Jorge%2520Gonzalez%2520Mendez%2520and%2520Tianhao%2520Zhou%2520and%2520Harish%2520Rajamani%2520and%2520Blake%2520Hechtman%2520and%2520Eddie%2520Cao%2520and%2520Da-Cheng%2520Juan%2520and%2520Yi-Xuan%2520Tan%2520and%2520Valentin%2520Dalibard%2520and%2520Yilun%2520Du%2520and%2520Natalie%2520Clay%2520and%2520Kaisheng%2520Yao%2520and%2520Wenhao%2520Jia%2520and%2520Dimple%2520Vijaykumar%2520and%2520Yuxiang%2520Zhou%2520and%2520Xinyi%2520Bai%2520and%2520Wei-Chih%2520Hung%2520and%2520Steven%2520Pecht%2520and%2520Georgi%2520Todorov%2520and%2520Nikhil%2520Khadke%2520and%2520Pramod%2520Gupta%2520and%2520Preethi%2520Lahoti%2520and%2520Arnaud%2520Autef%2520and%2520Karthik%2520Duddu%2520and%2520James%2520Lee-Thorp%2520and%2520Alexander%2520Bykovsky%2520and%2520Tautvydas%2520Misiunas%2520and%2520Sebastian%2520Flennerhag%2520and%2520Santhosh%2520Thangaraj%2520and%2520Jed%2520McGiffin%2520and%2520Zack%2520Nado%2520and%2520Markus%2520Kunesch%2520and%2520Andreas%2520Noever%2520and%2520Amir%2520Hertz%2520and%2520Marco%2520Liang%2520and%2520Victor%2520Stone%2520and%2520Evan%2520Palmer%2520and%2520Samira%2520Daruki%2520and%2520Arijit%2520Pramanik%2520and%2520Siim%2520P%25C3%25B5der%2520and%2520Austin%2520Kyker%2520and%2520Mina%2520Khan%2520and%2520Evgeny%2520Sluzhaev%2520and%2520Marvin%2520Ritter%2520and%2520Avraham%2520Ruderman%2520and%2520Wenlei%2520Zhou%2520and%2520Chirag%2520Nagpal%2520and%2520Kiran%2520Vodrahalli%2520and%2520George%2520Necula%2520and%2520Paul%2520Barham%2520and%2520Ellie%2520Pavlick%2520and%2520Jay%2520Hartford%2520and%2520Izhak%2520Shafran%2520and%2520Long%2520Zhao%2520and%2520Maciej%2520Miku%25C5%2582a%2520and%2520Tom%2520Eccles%2520and%2520Hidetoshi%2520Shimokawa%2520and%2520Kanav%2520Garg%2520and%2520Luke%2520Vilnis%2520and%2520Hanwen%2520Chen%2520and%2520Ilia%2520Shumailov%2520and%2520Kuang-Huei%2520Lee%2520and%2520Abdelrahman%2520Abdelhamed%2520and%2520Meiyan%2520Xie%2520and%2520Vered%2520Cohen%2520and%2520Ester%2520Hlavnova%2520and%2520Dan%2520Malkin%2520and%2520Chawin%2520Sitawarin%2520and%2520James%2520Lottes%2520and%2520Pauline%2520Coquinot%2520and%2520Tianli%2520Yu%2520and%2520Sandeep%2520Kumar%2520and%2520Jingwei%2520Zhang%2520and%2520Aroma%2520Mahendru%2520and%2520Zafarali%2520Ahmed%2520and%2520James%2520Martens%2520and%2520Tao%2520Chen%2520and%2520Aviel%2520Boag%2520and%2520Daiyi%2520Peng%2520and%2520Coline%2520Devin%2520and%2520Arseniy%2520Klimovskiy%2520and%2520Mary%2520Phuong%2520and%2520Danny%2520Vainstein%2520and%2520Jin%2520Xie%2520and%2520Bhuvana%2520Ramabhadran%2520and%2520Nathan%2520Howard%2520and%2520Xinxin%2520Yu%2520and%2520Gitartha%2520Goswami%2520and%2520Jingyu%2520Cui%2520and%2520Sam%2520Shleifer%2520and%2520Mario%2520Pinto%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Sara%2520Javanmardi%2520and%2520Dan%2520Ethier%2520and%2520Chace%2520Lee%2520and%2520Jordi%2520Orbay%2520and%2520Suyog%2520Kotecha%2520and%2520Carla%2520Bromberg%2520and%2520Pete%2520Shaw%2520and%2520James%2520Thornton%2520and%2520Adi%2520Gerzi%2520Rosenthal%2520and%2520Shane%2520Gu%2520and%2520Matt%2520Thomas%2520and%2520Ian%2520Gemp%2520and%2520Aditya%2520Ayyar%2520and%2520Asahi%2520Ushio%2520and%2520Aarush%2520Selvan%2520and%2520Joel%2520Wee%2520and%2520Chenxi%2520Liu%2520and%2520Maryam%2520Majzoubi%2520and%2520Weiren%2520Yu%2520and%2520Jake%2520Abernethy%2520and%2520Tyler%2520Liechty%2520and%2520Renke%2520Pan%2520and%2520Hoang%2520Nguyen%2520and%2520%2520Qiong%2520and%2520%2520Hu%2520and%2520Sarah%2520Perrin%2520and%2520Abhinav%2520Arora%2520and%2520Emily%2520Pitler%2520and%2520Weiyi%2520Wang%2520and%2520Kaushik%2520Shivakumar%2520and%2520Flavien%2520Prost%2520and%2520Ben%2520Limonchik%2520and%2520Jing%2520Wang%2520and%2520Yi%2520Gao%2520and%2520Timothee%2520Cour%2520and%2520Shyamal%2520Buch%2520and%2520Huan%2520Gui%2520and%2520Maria%2520Ivanova%2520and%2520Philipp%2520Neubeck%2520and%2520Kelvin%2520Chan%2520and%2520Lucy%2520Kim%2520and%2520Huizhong%2520Chen%2520and%2520Naman%2520Goyal%2520and%2520Da-Woon%2520Chung%2520and%2520Lu%2520Liu%2520and%2520Yao%2520Su%2520and%2520Anastasia%2520Petrushkina%2520and%2520Jiajun%2520Shen%2520and%2520Armand%2520Joulin%2520and%2520Yuanzhong%2520Xu%2520and%2520Stein%2520Xudong%2520Lin%2520and%2520Yana%2520Kulizhskaya%2520and%2520Ciprian%2520Chelba%2520and%2520Shobha%2520Vasudevan%2520and%2520Eli%2520Collins%2520and%2520Vasilisa%2520Bashlovkina%2520and%2520Tony%2520Lu%2520and%2520Doug%2520Fritz%2520and%2520Jongbin%2520Park%2520and%2520Yanqi%2520Zhou%2520and%2520Chen%2520Su%2520and%2520Richard%2520Tanburn%2520and%2520Mikhail%2520Sushkov%2520and%2520Mitchelle%2520Rasquinha%2520and%2520Jinning%2520Li%2520and%2520Jennifer%2520Prendki%2520and%2520Yiming%2520Li%2520and%2520Pallavi%2520LV%2520and%2520Shriya%2520Sharma%2520and%2520Hen%2520Fitoussi%2520and%2520Hui%2520Huang%2520and%2520Andrew%2520Dai%2520and%2520Phuong%2520Dao%2520and%2520Mike%2520Burrows%2520and%2520Henry%2520Prior%2520and%2520Danfeng%2520Qin%2520and%2520Golan%2520Pundak%2520and%2520Lars%2520Lowe%2520Sjoesund%2520and%2520Art%2520Khurshudov%2520and%2520Zhenkai%2520Zhu%2520and%2520Albert%2520Webson%2520and%2520Elizabeth%2520Kemp%2520and%2520Tat%2520Tan%2520and%2520Saurabh%2520Agrawal%2520and%2520Susie%2520Sargsyan%2520and%2520Liqun%2520Cheng%2520and%2520Jim%2520Stephan%2520and%2520Tom%2520Kwiatkowski%2520and%2520David%2520Reid%2520and%2520Arunkumar%2520Byravan%2520and%2520Assaf%2520Hurwitz%2520Michaely%2520and%2520Nicolas%2520Heess%2520and%2520Luowei%2520Zhou%2520and%2520Sonam%2520Goenka%2520and%2520Viral%2520Carpenter%2520and%2520Anselm%2520Levskaya%2520and%2520Bo%2520Wang%2520and%2520Reed%2520Roberts%2520and%2520R%25C3%25A9mi%2520Leblond%2520and%2520Sharat%2520Chikkerur%2520and%2520Stav%2520Ginzburg%2520and%2520Max%2520Chang%2520and%2520Robert%2520Riachi%2520and%2520%2520Chuqiao%2520and%2520%2520Xu%2520and%2520Zal%25C3%25A1n%2520Borsos%2520and%2520Michael%2520Pliskin%2520and%2520Julia%2520Pawar%2520and%2520Morgane%2520Lustman%2520and%2520Hannah%2520Kirkwood%2520and%2520Ankit%2520Anand%2520and%2520Aditi%2520Chaudhary%2520and%2520Norbert%2520Kalb%2520and%2520Kieran%2520Milan%2520and%2520Sean%2520Augenstein%2520and%2520Anna%2520Goldie%2520and%2520Laurel%2520Prince%2520and%2520Karthik%2520Raman%2520and%2520Yanhua%2520Sun%2520and%2520Vivian%2520Xia%2520and%2520Aaron%2520Cohen%2520and%2520Zhouyuan%2520Huo%2520and%2520Josh%2520Camp%2520and%2520Seher%2520Ellis%2520and%2520Lukas%2520Zilka%2520and%2520David%2520Vilar%2520Torres%2520and%2520Lisa%2520Patel%2520and%2520Sho%2520Arora%2520and%2520Betty%2520Chan%2520and%2520Jonas%2520Adler%2520and%2520Kareem%2520Ayoub%2520and%2520Jacky%2520Liang%2520and%2520Fayaz%2520Jamil%2520and%2520Jiepu%2520Jiang%2520and%2520Simon%2520Baumgartner%2520and%2520Haitian%2520Sun%2520and%2520Yael%2520Karov%2520and%2520Yaroslav%2520Akulov%2520and%2520Hui%2520Zheng%2520and%2520Irene%2520Cai%2520and%2520Claudio%2520Fantacci%2520and%2520James%2520Rubin%2520and%2520Alex%2520Rav%2520Acha%2520and%2520Mengchao%2520Wang%2520and%2520Nina%2520D%2527Souza%2520and%2520Rohit%2520Sathyanarayana%2520and%2520Shengyang%2520Dai%2520and%2520Simon%2520Rowe%2520and%2520Andrey%2520Simanovsky%2520and%2520Omer%2520Goldman%2520and%2520Yuheng%2520Kuang%2520and%2520Xiaoyue%2520Pan%2520and%2520Andrew%2520Rosenberg%2520and%2520Tania%2520Rojas-Esponda%2520and%2520Praneet%2520Dutta%2520and%2520Amy%2520Zeng%2520and%2520Irina%2520Jurenka%2520and%2520Greg%2520Farquhar%2520and%2520Yamini%2520Bansal%2520and%2520Shariq%2520Iqbal%2520and%2520Becca%2520Roelofs%2520and%2520Ga-Young%2520Joung%2520and%2520Parker%2520Beak%2520and%2520Changwan%2520Ryu%2520and%2520Ryan%2520Poplin%2520and%2520Yan%2520Wu%2520and%2520Jean-Baptiste%2520Alayrac%2520and%2520Senaka%2520Buthpitiya%2520and%2520Olaf%2520Ronneberger%2520and%2520Caleb%2520Habtegebriel%2520and%2520Wei%2520Li%2520and%2520Paul%2520Cavallaro%2520and%2520Aurora%2520Wei%2520and%2520Guy%2520Bensky%2520and%2520Timo%2520Denk%2520and%2520Harish%2520Ganapathy%2520and%2520Jeff%2520Stanway%2520and%2520Pratik%2520Joshi%2520and%2520Francesco%2520Bertolini%2520and%2520Jessica%2520Lo%2520and%2520Olivia%2520Ma%2520and%2520Zachary%2520Charles%2520and%2520Geta%2520Sampemane%2520and%2520Himanshu%2520Sahni%2520and%2520Xu%2520Chen%2520and%2520Harry%2520Askham%2520and%2520David%2520Gaddy%2520and%2520Peter%2520Young%2520and%2520Jiewen%2520Tan%2520and%2520Matan%2520Eyal%2520and%2520Arthur%2520Bra%25C5%25BEinskas%2520and%2520Li%2520Zhong%2520and%2520Zhichun%2520Wu%2520and%2520Mark%2520Epstein%2520and%2520Kai%2520Bailey%2520and%2520Andrew%2520Hard%2520and%2520Kamyu%2520Lee%2520and%2520Sasha%2520Goldshtein%2520and%2520Alex%2520Ruiz%2520and%2520Mohammed%2520Badawi%2520and%2520Matthias%2520Lochbrunner%2520and%2520JK%2520Kearns%2520and%2520Ashley%2520Brown%2520and%2520Fabio%2520Pardo%2520and%2520Theophane%2520Weber%2520and%2520Haichuan%2520Yang%2520and%2520Pan-Pan%2520Jiang%2520and%2520Berkin%2520Akin%2520and%2520Zhao%2520Fu%2520and%2520Marcus%2520Wainwright%2520and%2520Chi%2520Zou%2520and%2520Meenu%2520Gaba%2520and%2520Pierre-Antoine%2520Manzagol%2520and%2520Wendy%2520Kan%2520and%2520Yang%2520Song%2520and%2520Karina%2520Zainullina%2520and%2520Rui%2520Lin%2520and%2520Jeongwoo%2520Ko%2520and%2520Salil%2520Deshmukh%2520and%2520Apoorv%2520Jindal%2520and%2520James%2520Svensson%2520and%2520Divya%2520Tyam%2520and%2520Heri%2520Zhao%2520and%2520Christine%2520Kaeser-Chen%2520and%2520Scott%2520Baird%2520and%2520Pooya%2520Moradi%2520and%2520Jamie%2520Hall%2520and%2520Qiuchen%2520Guo%2520and%2520Vincent%2520Tsang%2520and%2520Bowen%2520Liang%2520and%2520Fernando%2520Pereira%2520and%2520Suhas%2520Ganesh%2520and%2520Ivan%2520Korotkov%2520and%2520Jakub%2520Adamek%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Vinh%2520Tran%2520and%2520Charles%2520Chen%2520and%2520Chris%2520Tar%2520and%2520Sanil%2520Jain%2520and%2520Ishita%2520Dasgupta%2520and%2520Taylan%2520Bilal%2520and%2520David%2520Reitter%2520and%2520Kai%2520Zhao%2520and%2520Giulia%2520Vezzani%2520and%2520Yasmin%2520Gehman%2520and%2520Pulkit%2520Mehta%2520and%2520Lauren%2520Beltrone%2520and%2520Xerxes%2520Dotiwalla%2520and%2520Sergio%2520Guadarrama%2520and%2520Zaheer%2520Abbas%2520and%2520Stefani%2520Karp%2520and%2520Petko%2520Georgiev%2520and%2520Chun-Sung%2520Ferng%2520and%2520Marc%2520Brockschmidt%2520and%2520Liqian%2520Peng%2520and%2520Christoph%2520Hirnschall%2520and%2520Vikas%2520Verma%2520and%2520Yingying%2520Bi%2520and%2520Ying%2520Xiao%2520and%2520Avigail%2520Dabush%2520and%2520Kelvin%2520Xu%2520and%2520Phil%2520Wallis%2520and%2520Randall%2520Parker%2520and%2520Qifei%2520Wang%2520and%2520Yang%2520Xu%2520and%2520Ilkin%2520Safarli%2520and%2520Dinesh%2520Tewari%2520and%2520Yin%2520Zhang%2520and%2520Seungyeon%2520Kim%2520and%2520Andrea%2520Gesmundo%2520and%2520Mackenzie%2520Thomas%2520and%2520Sergey%2520Levi%2520and%2520Ahmed%2520Chowdhury%2520and%2520Kanishka%2520Rao%2520and%2520Peter%2520Garst%2520and%2520Sam%2520Conway-Rahman%2520and%2520Helen%2520Ran%2520and%2520Kay%2520McKinney%2520and%2520Zhisheng%2520Xiao%2520and%2520Wenhao%2520Yu%2520and%2520Rohan%2520Agrawal%2520and%2520Axel%2520Stjerngren%2520and%2520Catalin%2520Ionescu%2520and%2520Jingjing%2520Chen%2520and%2520Vivek%2520Sharma%2520and%2520Justin%2520Chiu%2520and%2520Fei%2520Liu%2520and%2520Ken%2520Franko%2520and%2520Clayton%2520Sanford%2520and%2520Xingyu%2520Cai%2520and%2520Paul%2520Michel%2520and%2520Sanjay%2520Ganapathy%2520and%2520Jane%2520Labanowski%2520and%2520Zachary%2520Garrett%2520and%2520Ben%2520Vargas%2520and%2520Sean%2520Sun%2520and%2520Bryan%2520Gale%2520and%2520Thomas%2520Buschmann%2520and%2520Guillaume%2520Desjardins%2520and%2520Nimesh%2520Ghelani%2520and%2520Palak%2520Jain%2520and%2520Mudit%2520Verma%2520and%2520Chulayuth%2520Asawaroengchai%2520and%2520Julian%2520Eisenschlos%2520and%2520Jitendra%2520Harlalka%2520and%2520Hideto%2520Kazawa%2520and%2520Don%2520Metzler%2520and%2520Joshua%2520Howland%2520and%2520Ying%2520Jian%2520and%2520Jake%2520Ades%2520and%2520Viral%2520Shah%2520and%2520Tynan%2520Gangwani%2520and%2520Seungji%2520Lee%2520and%2520Roman%2520Ring%2520and%2520Steven%2520M.%2520Hernandez%2520and%2520Dean%2520Reich%2520and%2520Amer%2520Sinha%2520and%2520Ashutosh%2520Sathe%2520and%2520Joe%2520Kovac%2520and%2520Ashleah%2520Gill%2520and%2520Ajay%2520Kannan%2520and%2520Andrea%2520D%2527olimpio%2520and%2520Martin%2520Sevenich%2520and%2520Jay%2520Whang%2520and%2520Been%2520Kim%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Jilin%2520Chen%2520and%2520Jiageng%2520Zhang%2520and%2520Shuba%2520Lall%2520and%2520Yossi%2520Matias%2520and%2520Bill%2520Jia%2520and%2520Abe%2520Friesen%2520and%2520Sara%2520Nasso%2520and%2520Ashish%2520Thapliyal%2520and%2520Bryan%2520Perozzi%2520and%2520Ting%2520Yu%2520and%2520Anna%2520Shekhawat%2520and%2520Safeen%2520Huda%2520and%2520Peter%2520Grabowski%2520and%2520Eric%2520Wang%2520and%2520Ashwin%2520Sreevatsa%2520and%2520Hilal%2520Dib%2520and%2520Mehadi%2520Hassen%2520and%2520Parker%2520Schuh%2520and%2520Vedrana%2520Milutinovic%2520and%2520Chris%2520Welty%2520and%2520Michael%2520Quinn%2520and%2520Ali%2520Shah%2520and%2520Bangju%2520Wang%2520and%2520Gabe%2520Barth-Maron%2520and%2520Justin%2520Frye%2520and%2520Natalie%2520Axelsson%2520and%2520Tao%2520Zhu%2520and%2520Yukun%2520Ma%2520and%2520Irene%2520Giannoumis%2520and%2520Hanie%2520Sedghi%2520and%2520Chang%2520Ye%2520and%2520Yi%2520Luan%2520and%2520Kevin%2520Aydin%2520and%2520Bilva%2520Chandra%2520and%2520Vivek%2520Sampathkumar%2520and%2520Ronny%2520Huang%2520and%2520Victor%2520Lavrenko%2520and%2520Ahmed%2520Eleryan%2520and%2520Zhi%2520Hong%2520and%2520Steven%2520Hansen%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Bidisha%2520Samanta%2520and%2520Domagoj%2520%25C4%2586evid%2520and%2520Xin%2520Wang%2520and%2520Fangtao%2520Li%2520and%2520Michael%2520Voznesensky%2520and%2520Matt%2520Hoffman%2520and%2520Andreas%2520Terzis%2520and%2520Vikash%2520Sehwag%2520and%2520Gil%2520Fidel%2520and%2520Luheng%2520He%2520and%2520Mu%2520Cai%2520and%2520Yanzhang%2520He%2520and%2520Alex%2520Feng%2520and%2520Martin%2520Nikoltchev%2520and%2520Samrat%2520Phatale%2520and%2520Jason%2520Chase%2520and%2520Rory%2520Lawton%2520and%2520Ming%2520Zhang%2520and%2520Tom%2520Ouyang%2520and%2520Manuel%2520Tragut%2520and%2520Mehdi%2520Hafezi%2520Manshadi%2520and%2520Arjun%2520Narayanan%2520and%2520Jiaming%2520Shen%2520and%2520Xu%2520Gao%2520and%2520Tolga%2520Bolukbasi%2520and%2520Nick%2520Roy%2520and%2520Xin%2520Li%2520and%2520Daniel%2520Golovin%2520and%2520Liviu%2520Panait%2520and%2520Zhen%2520Qin%2520and%2520Guangxing%2520Han%2520and%2520Thomas%2520Anthony%2520and%2520Sneha%2520Kudugunta%2520and%2520Viorica%2520Patraucean%2520and%2520Aniket%2520Ray%2520and%2520Xinyun%2520Chen%2520and%2520Xiaochen%2520Yang%2520and%2520Tanuj%2520Bhatia%2520and%2520Pranav%2520Talluri%2520and%2520Alex%2520Morris%2520and%2520Andrija%2520Ra%25C5%25BEnatovi%25C4%2587%2520and%2520Bethanie%2520Brownfield%2520and%2520James%2520An%2520and%2520Sheng%2520Peng%2520and%2520Patrick%2520Kane%2520and%2520Ce%2520Zheng%2520and%2520Nico%2520Duduta%2520and%2520Joshua%2520Kessinger%2520and%2520James%2520Noraky%2520and%2520Siqi%2520Liu%2520and%2520Keran%2520Rong%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%2520and%2520Keith%2520Rush%2520and%2520Alex%2520Goldin%2520and%2520Fanny%2520Wei%2520and%2520Shiva%2520Mohan%2520Reddy%2520Garlapati%2520and%2520Caroline%2520Pantofaru%2520and%2520Okwan%2520Kwon%2520and%2520Jianmo%2520Ni%2520and%2520Eric%2520Noland%2520and%2520Julia%2520Di%2520Trapani%2520and%2520Fran%25C3%25A7oise%2520Beaufays%2520and%2520Abhijit%2520Guha%2520Roy%2520and%2520Yinlam%2520Chow%2520and%2520Aybuke%2520Turker%2520and%2520Geoffrey%2520Cideron%2520and%2520Lantao%2520Mei%2520and%2520Jon%2520Clark%2520and%2520Qingyun%2520Dou%2520and%2520Matko%2520Bo%25C5%25A1njak%2520and%2520Ralph%2520Leith%2520and%2520Yuqing%2520Du%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520Milad%2520Nasr%2520and%2520Chester%2520Kwak%2520and%2520Suraj%2520Satishkumar%2520Sheth%2520and%2520Alex%2520Kaskasoli%2520and%2520Ankesh%2520Anand%2520and%2520Balaji%2520Lakshminarayanan%2520and%2520Sammy%2520Jerome%2520and%2520David%2520Bieber%2520and%2520Chun-Te%2520Chu%2520and%2520Alexandre%2520Senges%2520and%2520Tianxiao%2520Shen%2520and%2520Mukund%2520Sridhar%2520and%2520Ndaba%2520Ndebele%2520and%2520Benjamin%2520Beyret%2520and%2520Shakir%2520Mohamed%2520and%2520Mia%2520Chen%2520and%2520Markus%2520Freitag%2520and%2520Jiaxian%2520Guo%2520and%2520Luyang%2520Liu%2520and%2520Paul%2520Roit%2520and%2520Heng%2520Chen%2520and%2520Shen%2520Yan%2520and%2520Tom%2520Stone%2520and%2520JD%2520Co-Reyes%2520and%2520Jeremy%2520Cole%2520and%2520Salvatore%2520Scellato%2520and%2520Shekoofeh%2520Azizi%2520and%2520Hadi%2520Hashemi%2520and%2520Alicia%2520Jin%2520and%2520Anand%2520Iyer%2520and%2520Marcella%2520Valentine%2520and%2520Andr%25C3%25A1s%2520Gy%25C3%25B6rgy%2520and%2520Arun%2520Ahuja%2520and%2520Daniel%2520Hernandez%2520Diaz%2520and%2520Chen-Yu%2520Lee%2520and%2520Nathan%2520Clement%2520and%2520Weize%2520Kong%2520and%2520Drew%2520Garmon%2520and%2520Ishaan%2520Watts%2520and%2520Kush%2520Bhatia%2520and%2520Khyatti%2520Gupta%2520and%2520Matt%2520Miecnikowski%2520and%2520Hugo%2520Vallet%2520and%2520Ankur%2520Taly%2520and%2520Edward%2520Loper%2520and%2520Saket%2520Joshi%2520and%2520James%2520Atwood%2520and%2520Jo%2520Chick%2520and%2520Mark%2520Collier%2520and%2520Fotis%2520Iliopoulos%2520and%2520Ryan%2520Trostle%2520and%2520Beliz%2520Gunel%2520and%2520Ramiro%2520Leal-Cavazos%2520and%2520Arnar%2520Mar%2520Hrafnkelsson%2520and%2520Michael%2520Guzman%2520and%2520Xiaoen%2520Ju%2520and%2520Andy%2520Forbes%2520and%2520Jesse%2520Emond%2520and%2520Kushal%2520Chauhan%2520and%2520Ben%2520Caine%2520and%2520Li%2520Xiao%2520and%2520Wenjun%2520Zeng%2520and%2520Alexandre%2520Moufarek%2520and%2520Daniel%2520Murphy%2520and%2520Maya%2520Meng%2520and%2520Nitish%2520Gupta%2520and%2520Felix%2520Riedel%2520and%2520Anil%2520Das%2520and%2520Elijah%2520Lawal%2520and%2520Shashi%2520Narayan%2520and%2520Tiberiu%2520Sosea%2520and%2520James%2520Swirhun%2520and%2520Linda%2520Friso%2520and%2520Behnam%2520Neyshabur%2520and%2520Jing%2520Lu%2520and%2520Sertan%2520Girgin%2520and%2520Michael%2520Wunder%2520and%2520Edouard%2520Yvinec%2520and%2520Aroonalok%2520Pyne%2520and%2520Victor%2520Carbune%2520and%2520Shruti%2520Rijhwani%2520and%2520Yang%2520Guo%2520and%2520Tulsee%2520Doshi%2520and%2520Anton%2520Briukhov%2520and%2520Max%2520Bain%2520and%2520Ayal%2520Hitron%2520and%2520Xuanhui%2520Wang%2520and%2520Ashish%2520Gupta%2520and%2520Ke%2520Chen%2520and%2520Cosmo%2520Du%2520and%2520Weiyang%2520Zhang%2520and%2520Dhruv%2520Shah%2520and%2520Arjun%2520Akula%2520and%2520Max%2520Dylla%2520and%2520Ashyana%2520Kachra%2520and%2520Weicheng%2520Kuo%2520and%2520Tingting%2520Zou%2520and%2520Lily%2520Wang%2520and%2520Luyao%2520Xu%2520and%2520Jifan%2520Zhu%2520and%2520Justin%2520Snyder%2520and%2520Sachit%2520Menon%2520and%2520Orhan%2520Firat%2520and%2520Igor%2520Mordatch%2520and%2520Yuan%2520Yuan%2520and%2520Natalia%2520Ponomareva%2520and%2520Rory%2520Blevins%2520and%2520Lawrence%2520Moore%2520and%2520Weijun%2520Wang%2520and%2520Phil%2520Chen%2520and%2520Martin%2520Scholz%2520and%2520Artur%2520Dwornik%2520and%2520Jason%2520Lin%2520and%2520Sicheng%2520Li%2520and%2520Diego%2520Antognini%2520and%2520Te%2520I%2520and%2520Xiaodan%2520Song%2520and%2520Matt%2520Miller%2520and%2520Uday%2520Kalra%2520and%2520Adam%2520Raveret%2520and%2520Oscar%2520Akerlund%2520and%2520Felix%2520Wu%2520and%2520Andrew%2520Nystrom%2520and%2520Namrata%2520Godbole%2520and%2520Tianqi%2520Liu%2520and%2520Hannah%2520DeBalsi%2520and%2520Jewel%2520Zhao%2520and%2520Buhuang%2520Liu%2520and%2520Avi%2520Caciularu%2520and%2520Lauren%2520Lax%2520and%2520Urvashi%2520Khandelwal%2520and%2520Victoria%2520Langston%2520and%2520Eric%2520Bailey%2520and%2520Silvio%2520Lattanzi%2520and%2520Yufei%2520Wang%2520and%2520Neel%2520Kovelamudi%2520and%2520Sneha%2520Mondal%2520and%2520Guru%2520Guruganesh%2520and%2520Nan%2520Hua%2520and%2520Ofir%2520Roval%2520and%2520Pawe%25C5%2582%2520Weso%25C5%2582owski%2520and%2520Rishikesh%2520Ingale%2520and%2520Jonathan%2520Halcrow%2520and%2520Tim%2520Sohn%2520and%2520Christof%2520Angermueller%2520and%2520Bahram%2520Raad%2520and%2520Eli%2520Stickgold%2520and%2520Eva%2520Lu%2520and%2520Alec%2520Kosik%2520and%2520Jing%2520Xie%2520and%2520Timothy%2520Lillicrap%2520and%2520Austin%2520Huang%2520and%2520Lydia%2520Lihui%2520Zhang%2520and%2520Dominik%2520Paulus%2520and%2520Clement%2520Farabet%2520and%2520Alex%2520Wertheim%2520and%2520Bing%2520Wang%2520and%2520Rishabh%2520Joshi%2520and%2520Chu-ling%2520Ko%2520and%2520Yonghui%2520Wu%2520and%2520Shubham%2520Agrawal%2520and%2520Lily%2520Lin%2520and%2520XiangHai%2520Sheng%2520and%2520Peter%2520Sung%2520and%2520Tyler%2520Breland-King%2520and%2520Christina%2520Butterfield%2520and%2520Swapnil%2520Gawde%2520and%2520Sumeet%2520Singh%2520and%2520Qiao%2520Zhang%2520and%2520Raj%2520Apte%2520and%2520Shilpa%2520Shetty%2520and%2520Adrian%2520Hutter%2520and%2520Tao%2520Li%2520and%2520Elizabeth%2520Salesky%2520and%2520Federico%2520Lebron%2520and%2520Jonni%2520Kanerva%2520and%2520Michela%2520Paganini%2520and%2520Arthur%2520Nguyen%2520and%2520Rohith%2520Vallu%2520and%2520Jan-Thorsten%2520Peter%2520and%2520Sarmishta%2520Velury%2520and%2520David%2520Kao%2520and%2520Jay%2520Hoover%2520and%2520Anna%2520Bortsova%2520and%2520Colton%2520Bishop%2520and%2520Shoshana%2520Jakobovits%2520and%2520Alessandro%2520Agostini%2520and%2520Alekh%2520Agarwal%2520and%2520Chang%2520Liu%2520and%2520Charles%2520Kwong%2520and%2520Sasan%2520Tavakkol%2520and%2520Ioana%2520Bica%2520and%2520Alex%2520Greve%2520and%2520Anirudh%2520GP%2520and%2520Jake%2520Marcus%2520and%2520Le%2520Hou%2520and%2520Tom%2520Duerig%2520and%2520Rivka%2520Moroshko%2520and%2520Dave%2520Lacey%2520and%2520Andy%2520Davis%2520and%2520Julien%2520Amelot%2520and%2520Guohui%2520Wang%2520and%2520Frank%2520Kim%2520and%2520Theofilos%2520Strinopoulos%2520and%2520Hui%2520Wan%2520and%2520Charline%2520Le%2520Lan%2520and%2520Shankar%2520Krishnan%2520and%2520Haotian%2520Tang%2520and%2520Peter%2520Humphreys%2520and%2520Junwen%2520Bai%2520and%2520Idan%2520Heimlich%2520Shtacher%2520and%2520Diego%2520Machado%2520and%2520Chenxi%2520Pang%2520and%2520Ken%2520Burke%2520and%2520Dangyi%2520Liu%2520and%2520Renga%2520Aravamudhan%2520and%2520Yue%2520Song%2520and%2520Ed%2520Hirst%2520and%2520Abhimanyu%2520Singh%2520and%2520Brendan%2520Jou%2520and%2520Liang%2520Bai%2520and%2520Francesco%2520Piccinno%2520and%2520Chuyuan%2520Kelly%2520Fu%2520and%2520Robin%2520Alazard%2520and%2520Barak%2520Meiri%2520and%2520Daniel%2520Winter%2520and%2520Charlie%2520Chen%2520and%2520Mingda%2520Zhang%2520and%2520Jens%2520Heitkaemper%2520and%2520John%2520Lambert%2520and%2520Jinhyuk%2520Lee%2520and%2520Alexander%2520Fr%25C3%25B6mmgen%2520and%2520Sergey%2520Rogulenko%2520and%2520Pranav%2520Nair%2520and%2520Paul%2520Niemczyk%2520and%2520Anton%2520Bulyenov%2520and%2520Bibo%2520Xu%2520and%2520Hadar%2520Shemtov%2520and%2520Morteza%2520Zadimoghaddam%2520and%2520Serge%2520Toropov%2520and%2520Mateo%2520Wirth%2520and%2520Hanjun%2520Dai%2520and%2520Sreenivas%2520Gollapudi%2520and%2520Daniel%2520Zheng%2520and%2520Alex%2520Kurakin%2520and%2520Chansoo%2520Lee%2520and%2520Kalesha%2520Bullard%2520and%2520Nicolas%2520Serrano%2520and%2520Ivana%2520Balazevic%2520and%2520Yang%2520Li%2520and%2520Johan%2520Schalkwyk%2520and%2520Mark%2520Murphy%2520and%2520Mingyang%2520Zhang%2520and%2520Kevin%2520Sequeira%2520and%2520Romina%2520Datta%2520and%2520Nishant%2520Agrawal%2520and%2520Charles%2520Sutton%2520and%2520Nithya%2520Attaluri%2520and%2520Mencher%2520Chiang%2520and%2520Wael%2520Farhan%2520and%2520Gregory%2520Thornton%2520and%2520Kate%2520Lin%2520and%2520Travis%2520Choma%2520and%2520Hung%2520Nguyen%2520and%2520Kingshuk%2520Dasgupta%2520and%2520Dirk%2520Robinson%2520and%2520Iulia%2520Com%25C5%259Fa%2520and%2520Michael%2520Riley%2520and%2520Arjun%2520Pillai%2520and%2520Basil%2520Mustafa%2520and%2520Ben%2520Golan%2520and%2520Amir%2520Zandieh%2520and%2520Jean-Baptiste%2520Lespiau%2520and%2520Billy%2520Porter%2520and%2520David%2520Ross%2520and%2520Sujeevan%2520Rajayogam%2520and%2520Mohit%2520Agarwal%2520and%2520Subhashini%2520Venugopalan%2520and%2520Bobak%2520Shahriari%2520and%2520Qiqi%2520Yan%2520and%2520Hao%2520Xu%2520and%2520Taylor%2520Tobin%2520and%2520Pavel%2520Dubov%2520and%2520Hongzhi%2520Shi%2520and%2520Adri%25C3%25A0%2520Recasens%2520and%2520Anton%2520Kovsharov%2520and%2520Sebastian%2520Borgeaud%2520and%2520Lucio%2520Dery%2520and%2520Shanthal%2520Vasanth%2520and%2520Elena%2520Gribovskaya%2520and%2520Linhai%2520Qiu%2520and%2520Mahdis%2520Mahdieh%2520and%2520Wojtek%2520Skut%2520and%2520Elizabeth%2520Nielsen%2520and%2520CJ%2520Zheng%2520and%2520Adams%2520Yu%2520and%2520Carrie%2520Grimes%2520Bostock%2520and%2520Shaleen%2520Gupta%2520and%2520Aaron%2520Archer%2520and%2520Chris%2520Rawles%2520and%2520Elinor%2520Davies%2520and%2520Alexey%2520Svyatkovskiy%2520and%2520Tomy%2520Tsai%2520and%2520Yoni%2520Halpern%2520and%2520Christian%2520Reisswig%2520and%2520Bartek%2520Wydrowski%2520and%2520Bo%2520Chang%2520and%2520Joan%2520Puigcerver%2520and%2520Mor%2520Hazan%2520Taege%2520and%2520Jian%2520Li%2520and%2520Eva%2520Schnider%2520and%2520Xinjian%2520Li%2520and%2520Dragos%2520Dena%2520and%2520Yunhan%2520Xu%2520and%2520Umesh%2520Telang%2520and%2520Tianze%2520Shi%2520and%2520Heiga%2520Zen%2520and%2520Kyle%2520Kastner%2520and%2520Yeongil%2520Ko%2520and%2520Neesha%2520Subramaniam%2520and%2520Aviral%2520Kumar%2520and%2520Pete%2520Blois%2520and%2520Zhuyun%2520Dai%2520and%2520John%2520Wieting%2520and%2520Yifeng%2520Lu%2520and%2520Yoel%2520Zeldes%2520and%2520Tian%2520Xie%2520and%2520Anja%2520Hauth%2520and%2520Alexandru%2520%25C5%25A2ifrea%2520and%2520Yuqi%2520Li%2520and%2520Sam%2520El-Husseini%2520and%2520Dan%2520Abolafia%2520and%2520Howard%2520Zhou%2520and%2520Wen%2520Ding%2520and%2520Sahra%2520Ghalebikesabi%2520and%2520Carlos%2520Gu%25C3%25ADa%2520and%2520Andrii%2520Maksai%2520and%2520%25C3%2581goston%2520Weisz%2520and%2520Sercan%2520Arik%2520and%2520Nick%2520Sukhanov%2520and%2520Aga%2520%25C5%259Awietlik%2520and%2520Xuhui%2520Jia%2520and%2520Luo%2520Yu%2520and%2520Weiyue%2520Wang%2520and%2520Mark%2520Brand%2520and%2520Dawn%2520Bloxwich%2520and%2520Sean%2520Kirmani%2520and%2520Zhe%2520Chen%2520and%2520Alec%2520Go%2520and%2520Pablo%2520Sprechmann%2520and%2520Nithish%2520Kannen%2520and%2520Alen%2520Carin%2520and%2520Paramjit%2520Sandhu%2520and%2520Isabel%2520Edkins%2520and%2520Leslie%2520Nooteboom%2520and%2520Jai%2520Gupta%2520and%2520Loren%2520Maggiore%2520and%2520Javad%2520Azizi%2520and%2520Yael%2520Pritch%2520and%2520Pengcheng%2520Yin%2520and%2520Mansi%2520Gupta%2520and%2520Danny%2520Tarlow%2520and%2520Duncan%2520Smith%2520and%2520Desi%2520Ivanov%2520and%2520Mohammad%2520Babaeizadeh%2520and%2520Ankita%2520Goel%2520and%2520Satish%2520Kambala%2520and%2520Grace%2520Chu%2520and%2520Matej%2520Kastelic%2520and%2520Michelle%2520Liu%2520and%2520Hagen%2520Soltau%2520and%2520Austin%2520Stone%2520and%2520Shivani%2520Agrawal%2520and%2520Min%2520Kim%2520and%2520Kedar%2520Soparkar%2520and%2520Srinivas%2520Tadepalli%2520and%2520Oskar%2520Bunyan%2520and%2520Rachel%2520Soh%2520and%2520Arvind%2520Kannan%2520and%2520DY%2520Kim%2520and%2520Blake%2520JianHang%2520Chen%2520and%2520Afief%2520Halumi%2520and%2520Sudeshna%2520Roy%2520and%2520Yulong%2520Wang%2520and%2520Olcan%2520Sercinoglu%2520and%2520Gena%2520Gibson%2520and%2520Sijal%2520Bhatnagar%2520and%2520Motoki%2520Sano%2520and%2520Daniel%2520von%2520Dincklage%2520and%2520Qingchun%2520Ren%2520and%2520Blagoj%2520Mitrevski%2520and%2520Mirek%2520Ol%25C5%25A1%25C3%25A1k%2520and%2520Jennifer%2520She%2520and%2520Carl%2520Doersch%2520and%2520%2520Jilei%2520and%2520%2520Wang%2520and%2520Bingyuan%2520Liu%2520and%2520Qijun%2520Tan%2520and%2520Tamar%2520Yakar%2520and%2520Tris%2520Warkentin%2520and%2520Alex%2520Ramirez%2520and%2520Carl%2520Lebsack%2520and%2520Josh%2520Dillon%2520and%2520Rajiv%2520Mathews%2520and%2520Tom%2520Cobley%2520and%2520Zelin%2520Wu%2520and%2520Zhuoyuan%2520Chen%2520and%2520Jon%2520Simon%2520and%2520Swaroop%2520Nath%2520and%2520Tara%2520Sainath%2520and%2520Alexei%2520Bendebury%2520and%2520Ryan%2520Julian%2520and%2520Bharath%2520Mankalale%2520and%2520Daria%2520%25C4%2586urko%2520and%2520Paulo%2520Zacchello%2520and%2520Adam%2520R.%2520Brown%2520and%2520Kiranbir%2520Sodhia%2520and%2520Heidi%2520Howard%2520and%2520Sergi%2520Caelles%2520and%2520Abhinav%2520Gupta%2520and%2520Gareth%2520Evans%2520and%2520Anna%2520Bulanova%2520and%2520Lesley%2520Katzen%2520and%2520Roman%2520Goldenberg%2520and%2520Anton%2520Tsitsulin%2520and%2520Joe%2520Stanton%2520and%2520Benoit%2520Schillings%2520and%2520Vitaly%2520Kovalev%2520and%2520Corey%2520Fry%2520and%2520Rushin%2520Shah%2520and%2520Kuo%2520Lin%2520and%2520Shyam%2520Upadhyay%2520and%2520Cheng%2520Li%2520and%2520Soroush%2520Radpour%2520and%2520Marcello%2520Maggioni%2520and%2520Jing%2520Xiong%2520and%2520Lukas%2520Haas%2520and%2520Jenny%2520Brennan%2520and%2520Aishwarya%2520Kamath%2520and%2520Nikolay%2520Savinov%2520and%2520Arsha%2520Nagrani%2520and%2520Trevor%2520Yacovone%2520and%2520Ryan%2520Kappedal%2520and%2520Kostas%2520Andriopoulos%2520and%2520Li%2520Lao%2520and%2520YaGuang%2520Li%2520and%2520Grigory%2520Rozhdestvenskiy%2520and%2520Kazuma%2520Hashimoto%2520and%2520Andrew%2520Audibert%2520and%2520Sophia%2520Austin%2520and%2520Daniel%2520Rodriguez%2520and%2520Anian%2520Ruoss%2520and%2520Garrett%2520Honke%2520and%2520Deep%2520Karkhanis%2520and%2520Xi%2520Xiong%2520and%2520Qing%2520Wei%2520and%2520James%2520Huang%2520and%2520Zhaoqi%2520Leng%2520and%2520Vittal%2520Premachandran%2520and%2520Stan%2520Bileschi%2520and%2520Georgios%2520Evangelopoulos%2520and%2520Thomas%2520Mensink%2520and%2520Jay%2520Pavagadhi%2520and%2520Denis%2520Teplyashin%2520and%2520Paul%2520Chang%2520and%2520Linting%2520Xue%2520and%2520Garrett%2520Tanzer%2520and%2520Sally%2520Goldman%2520and%2520Kaushal%2520Patel%2520and%2520Shixin%2520Li%2520and%2520Jeremy%2520Wiesner%2520and%2520Ivy%2520Zheng%2520and%2520Ian%2520Stewart-Binks%2520and%2520Jie%2520Han%2520and%2520Zhi%2520Li%2520and%2520Liangchen%2520Luo%2520and%2520Karel%2520Lenc%2520and%2520Mario%2520Lu%25C4%258Di%25C4%2587%2520and%2520Fuzhao%2520Xue%2520and%2520Ryan%2520Mullins%2520and%2520Alexey%2520Guseynov%2520and%2520Chung-Ching%2520Chang%2520and%2520Isaac%2520Galatzer-Levy%2520and%2520Adam%2520Zhang%2520and%2520Garrett%2520Bingham%2520and%2520Grace%2520Hu%2520and%2520Ale%2520Hartman%2520and%2520Yue%2520Ma%2520and%2520Jordan%2520Griffith%2520and%2520Alex%2520Irpan%2520and%2520Carey%2520Radebaugh%2520and%2520Summer%2520Yue%2520and%2520Lijie%2520Fan%2520and%2520Victor%2520Ungureanu%2520and%2520Christina%2520Sorokin%2520and%2520Hannah%2520Teufel%2520and%2520Peiran%2520Li%2520and%2520Rohan%2520Anil%2520and%2520Dimitris%2520Paparas%2520and%2520Todd%2520Wang%2520and%2520Chu-Cheng%2520Lin%2520and%2520Hui%2520Peng%2520and%2520Megan%2520Shum%2520and%2520Goran%2520Petrovic%2520and%2520Demetra%2520Brady%2520and%2520Richard%2520Nguyen%2520and%2520Klaus%2520Macherey%2520and%2520Zhihao%2520Li%2520and%2520Harman%2520Singh%2520and%2520Madhavi%2520Yenugula%2520and%2520Mariko%2520Iinuma%2520and%2520Xinyi%2520Chen%2520and%2520Kavya%2520Kopparapu%2520and%2520Alexey%2520Stern%2520and%2520Shachi%2520Dave%2520and%2520Chandu%2520Thekkath%2520and%2520Florence%2520Perot%2520and%2520Anurag%2520Kumar%2520and%2520Fangda%2520Li%2520and%2520Yang%2520Xiao%2520and%2520Matthew%2520Bilotti%2520and%2520Mohammad%2520Hossein%2520Bateni%2520and%2520Isaac%2520Noble%2520and%2520Lisa%2520Lee%2520and%2520Amelio%2520V%25C3%25A1zquez-Reina%2520and%2520Julian%2520Salazar%2520and%2520Xiaomeng%2520Yang%2520and%2520Boyu%2520Wang%2520and%2520Ela%2520Gruzewska%2520and%2520Anand%2520Rao%2520and%2520Sindhu%2520Raghuram%2520and%2520Zheng%2520Xu%2520and%2520Eyal%2520Ben-David%2520and%2520Jieru%2520Mei%2520and%2520Sid%2520Dalmia%2520and%2520Zhaoyi%2520Zhang%2520and%2520Yuchen%2520Liu%2520and%2520Gagan%2520Bansal%2520and%2520Helena%2520Pankov%2520and%2520Steven%2520Schwarcz%2520and%2520Andrea%2520Burns%2520and%2520Christine%2520Chan%2520and%2520Sumit%2520Sanghai%2520and%2520Ricky%2520Liang%2520and%2520Ethan%2520Liang%2520and%2520Antoine%2520He%2520and%2520Amy%2520Stuart%2520and%2520Arun%2520Narayanan%2520and%2520Yukun%2520Zhu%2520and%2520Christian%2520Frank%2520and%2520Bahar%2520Fatemi%2520and%2520Amit%2520Sabne%2520and%2520Oran%2520Lang%2520and%2520Indro%2520Bhattacharya%2520and%2520Shane%2520Settle%2520and%2520Maria%2520Wang%2520and%2520Brendan%2520McMahan%2520and%2520Andrea%2520Tacchetti%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Majid%2520Hadian%2520and%2520Serkan%2520Cabi%2520and%2520Timothy%2520Chung%2520and%2520Nikita%2520Putikhin%2520and%2520Gang%2520Li%2520and%2520Jeremy%2520Chen%2520and%2520Austin%2520Tarango%2520and%2520Henryk%2520Michalewski%2520and%2520Mehran%2520Kazemi%2520and%2520Hussain%2520Masoom%2520and%2520Hila%2520Sheftel%2520and%2520Rakesh%2520Shivanna%2520and%2520Archita%2520Vadali%2520and%2520Ramona%2520Comanescu%2520and%2520Doug%2520Reid%2520and%2520Joss%2520Moore%2520and%2520Arvind%2520Neelakantan%2520and%2520Micha%25C3%25ABl%2520Sander%2520and%2520Jonathan%2520Herzig%2520and%2520Aviv%2520Rosenberg%2520and%2520Mostafa%2520Dehghani%2520and%2520JD%2520Choi%2520and%2520Michael%2520Fink%2520and%2520Reid%2520Hayes%2520and%2520Eric%2520Ge%2520and%2520Shitao%2520Weng%2520and%2520Chia-Hua%2520Ho%2520and%2520John%2520Karro%2520and%2520Kalpesh%2520Krishna%2520and%2520Lam%2520Nguyen%2520Thiet%2520and%2520Amy%2520Skerry-Ryan%2520and%2520Daniel%2520Eppens%2520and%2520Marco%2520Andreetto%2520and%2520Navin%2520Sarma%2520and%2520Silvano%2520Bonacina%2520and%2520Burcu%2520Karagol%2520Ayan%2520and%2520Megha%2520Nawhal%2520and%2520Zhihao%2520Shan%2520and%2520Mike%2520Dusenberry%2520and%2520Shantanu%2520Thakoor%2520and%2520Sagar%2520Gubbi%2520and%2520Duc%2520Dung%2520Nguyen%2520and%2520Reut%2520Tsarfaty%2520and%2520Samuel%2520Albanie%2520and%2520Jovana%2520Mitrovi%25C4%2587%2520and%2520Meet%2520Gandhi%2520and%2520Bo-Juen%2520Chen%2520and%2520Alessandro%2520Epasto%2520and%2520Georgi%2520Stephanov%2520and%2520Ye%2520Jin%2520and%2520Samuel%2520Gehman%2520and%2520Aida%2520Amini%2520and%2520Jack%2520Weber%2520and%2520Feryal%2520Behbahani%2520and%2520Shawn%2520Xu%2520and%2520Miltos%2520Allamanis%2520and%2520Xi%2520Chen%2520and%2520Myle%2520Ott%2520and%2520Claire%2520Sha%2520and%2520Michal%2520Jastrzebski%2520and%2520Hang%2520Qi%2520and%2520David%2520Greene%2520and%2520Xinyi%2520Wu%2520and%2520Abodunrinwa%2520Toki%2520and%2520Daniel%2520Vlasic%2520and%2520Jane%2520Shapiro%2520and%2520Ragha%2520Kotikalapudi%2520and%2520Zhe%2520Shen%2520and%2520Takaaki%2520Saeki%2520and%2520Sirui%2520Xie%2520and%2520Albin%2520Cassirer%2520and%2520Shikhar%2520Bharadwaj%2520and%2520Tatsuya%2520Kiyono%2520and%2520Srinadh%2520Bhojanapalli%2520and%2520Elan%2520Rosenfeld%2520and%2520Sam%2520Ritter%2520and%2520Jieming%2520Mao%2520and%2520Jo%25C3%25A3o%2520Gabriel%2520Oliveira%2520and%2520Zoltan%2520Egyed%2520and%2520Bernd%2520Bandemer%2520and%2520Emilio%2520Parisotto%2520and%2520Keisuke%2520Kinoshita%2520and%2520Juliette%2520Pluto%2520and%2520Petros%2520Maniatis%2520and%2520Steve%2520Li%2520and%2520Yaohui%2520Guo%2520and%2520Golnaz%2520Ghiasi%2520and%2520Jean%2520Tarbouriech%2520and%2520Srimon%2520Chatterjee%2520and%2520Julie%2520Jin%2520and%2520%2520Katrina%2520and%2520%2520Xu%2520and%2520Jennimaria%2520Palomaki%2520and%2520S%25C3%25A9b%2520Arnold%2520and%2520Madhavi%2520Sewak%2520and%2520Federico%2520Piccinini%2520and%2520Mohit%2520Sharma%2520and%2520Ben%2520Albrecht%2520and%2520Sean%2520Purser-haskell%2520and%2520Ashwin%2520Vaswani%2520and%2520Chongyan%2520Chen%2520and%2520Matheus%2520Wisniewski%2520and%2520Qin%2520Cao%2520and%2520John%2520Aslanides%2520and%2520Nguyet%2520Minh%2520Phu%2520and%2520Maximilian%2520Sieb%2520and%2520Lauren%2520Agubuzu%2520and%2520Anne%2520Zheng%2520and%2520Daniel%2520Sohn%2520and%2520Marco%2520Selvi%2520and%2520Anders%2520Andreassen%2520and%2520Krishan%2520Subudhi%2520and%2520Prem%2520Eruvbetine%2520and%2520Oliver%2520Woodman%2520and%2520Tomas%2520Mery%2520and%2520Sebastian%2520Krause%2520and%2520Xiaoqi%2520Ren%2520and%2520Xiao%2520Ma%2520and%2520Jincheng%2520Luo%2520and%2520Dawn%2520Chen%2520and%2520Wei%2520Fan%2520and%2520Henry%2520Griffiths%2520and%2520Christian%2520Schuler%2520and%2520Alice%2520Li%2520and%2520Shujian%2520Zhang%2520and%2520Jean-Michel%2520Sarr%2520and%2520Shixin%2520Luo%2520and%2520Riccardo%2520Patana%2520and%2520Matthew%2520Watson%2520and%2520Dani%2520Naboulsi%2520and%2520Michael%2520Collins%2520and%2520Sailesh%2520Sidhwani%2520and%2520Emiel%2520Hoogeboom%2520and%2520Sharon%2520Silver%2520and%2520Emily%2520Caveness%2520and%2520Xiaokai%2520Zhao%2520and%2520Mikel%2520Rodriguez%2520and%2520Maxine%2520Deines%2520and%2520Libin%2520Bai%2520and%2520Patrick%2520Griffin%2520and%2520Marco%2520Tagliasacchi%2520and%2520Emily%2520Xue%2520and%2520Spandana%2520Raj%2520Babbula%2520and%2520Bo%2520Pang%2520and%2520Nan%2520Ding%2520and%2520Gloria%2520Shen%2520and%2520Elijah%2520Peake%2520and%2520Remi%2520Crocker%2520and%2520Shubha%2520Srinivas%2520Raghvendra%2520and%2520Danny%2520Swisher%2520and%2520Woohyun%2520Han%2520and%2520Richa%2520Singh%2520and%2520Ling%2520Wu%2520and%2520Vladimir%2520Pchelin%2520and%2520Tsendsuren%2520Munkhdalai%2520and%2520Dana%2520Alon%2520and%2520Geoff%2520Bacon%2520and%2520Efren%2520Robles%2520and%2520Jannis%2520Bulian%2520and%2520Melvin%2520Johnson%2520and%2520George%2520Powell%2520and%2520Felipe%2520Tiengo%2520Ferreira%2520and%2520Yaoyiran%2520Li%2520and%2520Frederik%2520Benzing%2520and%2520Mihajlo%2520Velimirovi%25C4%2587%2520and%2520Hubert%2520Soyer%2520and%2520William%2520Kong%2520and%2520%2520Tony%2520and%2520%2520Nguy%25C3%25AAn%2520and%2520Zhen%2520Yang%2520and%2520Jeremiah%2520Liu%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Daniel%2520Gillick%2520and%2520Baochen%2520Sun%2520and%2520Nathalie%2520Rauschmayr%2520and%2520Katie%2520Zhang%2520and%2520Serena%2520Zhan%2520and%2520Tao%2520Zhou%2520and%2520Alexey%2520Frolov%2520and%2520Chengrun%2520Yang%2520and%2520Denis%2520Vnukov%2520and%2520Louis%2520Rouillard%2520and%2520Hongji%2520Li%2520and%2520Amol%2520Mandhane%2520and%2520Nova%2520Fallen%2520and%2520Rajesh%2520Venkataraman%2520and%2520Clara%2520Huiyi%2520Hu%2520and%2520Jennifer%2520Brennan%2520and%2520Jenny%2520Lee%2520and%2520Jerry%2520Chang%2520and%2520Martin%2520Sundermeyer%2520and%2520Zhufeng%2520Pan%2520and%2520Rosemary%2520Ke%2520and%2520Simon%2520Tong%2520and%2520Alex%2520Fabrikant%2520and%2520William%2520Bono%2520and%2520Jindong%2520Gu%2520and%2520Ryan%2520Foley%2520and%2520Yiran%2520Mao%2520and%2520Manolis%2520Delakis%2520and%2520Dhruva%2520Bhaswar%2520and%2520Roy%2520Frostig%2520and%2520Nick%2520Li%2520and%2520Avital%2520Zipori%2520and%2520Cath%2520Hope%2520and%2520Olga%2520Kozlova%2520and%2520Swaroop%2520Mishra%2520and%2520Josip%2520Djolonga%2520and%2520Craig%2520Schiff%2520and%2520Majd%2520Al%2520Merey%2520and%2520Eleftheria%2520Briakou%2520and%2520Peter%2520Morgan%2520and%2520Andy%2520Wan%2520and%2520Avinatan%2520Hassidim%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Kuntal%2520Sengupta%2520and%2520Mary%2520Jasarevic%2520and%2520Praveen%2520Kallakuri%2520and%2520Paige%2520Kunkle%2520and%2520Hannah%2520Brennan%2520and%2520Tom%2520Lieber%2520and%2520Hassan%2520Mansoor%2520and%2520Julian%2520Walker%2520and%2520Bing%2520Zhang%2520and%2520Annie%2520Xie%2520and%2520Goran%2520%25C5%25BDu%25C5%25BEi%25C4%2587%2520and%2520Adaeze%2520Chukwuka%2520and%2520Alex%2520Druinsky%2520and%2520Donghyun%2520Cho%2520and%2520Rui%2520Yao%2520and%2520Ferjad%2520Naeem%2520and%2520Shiraz%2520Butt%2520and%2520Eunyoung%2520Kim%2520and%2520Zhipeng%2520Jia%2520and%2520Mandy%2520Jordan%2520and%2520Adam%2520Lelkes%2520and%2520Mark%2520Kurzeja%2520and%2520Sophie%2520Wang%2520and%2520James%2520Zhao%2520and%2520Andrew%2520Over%2520and%2520Abhishek%2520Chakladar%2520and%2520Marcel%2520Prasetya%2520and%2520Neha%2520Jha%2520and%2520Sriram%2520Ganapathy%2520and%2520Yale%2520Cong%2520and%2520Prakash%2520Shroff%2520and%2520Carl%2520Saroufim%2520and%2520Sobhan%2520Miryoosefi%2520and%2520Mohamed%2520Hammad%2520and%2520Tajwar%2520Nasir%2520and%2520Weijuan%2520Xi%2520and%2520Yang%2520Gao%2520and%2520Young%2520Maeng%2520and%2520Ben%2520Hora%2520and%2520Chin-Yi%2520Cheng%2520and%2520Parisa%2520Haghani%2520and%2520Yoad%2520Lewenberg%2520and%2520Caden%2520Lu%2520and%2520Martin%2520Matysiak%2520and%2520Naina%2520Raisinghani%2520and%2520Huiyu%2520Wang%2520and%2520Lexi%2520Baugher%2520and%2520Rahul%2520Sukthankar%2520and%2520Minh%2520Giang%2520and%2520John%2520Schultz%2520and%2520Noah%2520Fiedel%2520and%2520Minmin%2520Chen%2520and%2520Cheng-Chun%2520Lee%2520and%2520Tapomay%2520Dey%2520and%2520Hao%2520Zheng%2520and%2520Shachi%2520Paul%2520and%2520Celine%2520Smith%2520and%2520Andy%2520Ly%2520and%2520Yicheng%2520Wang%2520and%2520Rishabh%2520Bansal%2520and%2520Bartek%2520Perz%2520and%2520Susanna%2520Ricco%2520and%2520Stasha%2520Blank%2520and%2520Vaishakh%2520Keshava%2520and%2520Deepak%2520Sharma%2520and%2520Marvin%2520Chow%2520and%2520Kunal%2520Lad%2520and%2520Komal%2520Jalan%2520and%2520Simon%2520Osindero%2520and%2520Craig%2520Swanson%2520and%2520Jacob%2520Scott%2520and%2520Anastasija%2520Ili%25C4%2587%2520and%2520Xiaowei%2520Li%2520and%2520Siddhartha%2520Reddy%2520Jonnalagadda%2520and%2520Afzal%2520Shama%2520Soudagar%2520and%2520Yan%2520Xiong%2520and%2520Bat-Orgil%2520Batsaikhan%2520and%2520Daniel%2520Jarrett%2520and%2520Naveen%2520Kumar%2520and%2520Maulik%2520Shah%2520and%2520Matt%2520Lawlor%2520and%2520Austin%2520Waters%2520and%2520Mark%2520Graham%2520and%2520Rhys%2520May%2520and%2520Sabela%2520Ramos%2520and%2520Sandra%2520Lefdal%2520and%2520Zeynep%2520Cankara%2520and%2520Nacho%2520Cano%2520and%2520Brendan%2520O%2527Donoghue%2520and%2520Jed%2520Borovik%2520and%2520Frederick%2520Liu%2520and%2520Jordan%2520Grimstad%2520and%2520Mahmoud%2520Alnahlawi%2520and%2520Katerina%2520Tsihlas%2520and%2520Tom%2520Hudson%2520and%2520Nikolai%2520Grigorev%2520and%2520Yiling%2520Jia%2520and%2520Terry%2520Huang%2520and%2520Tobenna%2520Peter%2520Igwe%2520and%2520Sergei%2520Lebedev%2520and%2520Xiaodan%2520Tang%2520and%2520Igor%2520Krivokon%2520and%2520Frankie%2520Garcia%2520and%2520Melissa%2520Tan%2520and%2520Eric%2520Jia%2520and%2520Peter%2520Stys%2520and%2520Shikhar%2520Vashishth%2520and%2520Yu%2520Liang%2520and%2520Balaji%2520Venkatraman%2520and%2520Chenjie%2520Gu%2520and%2520Anastasios%2520Kementsietsidis%2520and%2520Chen%2520Zhu%2520and%2520Junehyuk%2520Jung%2520and%2520Yunfei%2520Bai%2520and%2520Mohammad%2520Javad%2520Hosseini%2520and%2520Faruk%2520Ahmed%2520and%2520Aditya%2520Gupta%2520and%2520Xin%2520Yuan%2520and%2520Shereen%2520Ashraf%2520and%2520Shitij%2520Nigam%2520and%2520Gautam%2520Vasudevan%2520and%2520Pranjal%2520Awasthi%2520and%2520Adi%2520Mayrav%2520Gilady%2520and%2520Zelda%2520Mariet%2520and%2520Ramy%2520Eskander%2520and%2520Haiguang%2520Li%2520and%2520Hexiang%2520Hu%2520and%2520Guillermo%2520Garrido%2520and%2520Philippe%2520Schlattner%2520and%2520George%2520Zhang%2520and%2520Rohun%2520Saxena%2520and%2520Petar%2520Devi%25C4%2587%2520and%2520Kritika%2520Muralidharan%2520and%2520Ashwin%2520Murthy%2520and%2520Yiqian%2520Zhou%2520and%2520Min%2520Choi%2520and%2520Arissa%2520Wongpanich%2520and%2520Zhengdong%2520Wang%2520and%2520Premal%2520Shah%2520and%2520Yuntao%2520Xu%2520and%2520Yiling%2520Huang%2520and%2520Stephen%2520Spencer%2520and%2520Alice%2520Chen%2520and%2520James%2520Cohan%2520and%2520Junjie%2520Wang%2520and%2520Jonathan%2520Tompson%2520and%2520Junru%2520Wu%2520and%2520Ruba%2520Haroun%2520and%2520Haiqiong%2520Li%2520and%2520Blanca%2520Huergo%2520and%2520Fan%2520Yang%2520and%2520Tongxin%2520Yin%2520and%2520James%2520Wendt%2520and%2520Michael%2520Bendersky%2520and%2520Rahma%2520Chaabouni%2520and%2520Javier%2520Snaider%2520and%2520Johan%2520Ferret%2520and%2520Abhishek%2520Jindal%2520and%2520Tara%2520Thompson%2520and%2520Andrew%2520Xue%2520and%2520Will%2520Bishop%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Archit%2520Sharma%2520and%2520Yunhsuan%2520Sung%2520and%2520Prabakar%2520Radhakrishnan%2520and%2520Mo%2520Shomrat%2520and%2520Reeve%2520Ingle%2520and%2520Roopali%2520Vij%2520and%2520Justin%2520Gilmer%2520and%2520Mihai%2520Dorin%2520Istin%2520and%2520Sam%2520Sobell%2520and%2520Yang%2520Lu%2520and%2520Emily%2520Nottage%2520and%2520Dorsa%2520Sadigh%2520and%2520Jeremiah%2520Willcock%2520and%2520Tingnan%2520Zhang%2520and%2520Steve%2520Xu%2520and%2520Sasha%2520Brown%2520and%2520Katherine%2520Lee%2520and%2520Gary%2520Wang%2520and%2520Yun%2520Zhu%2520and%2520Yi%2520Tay%2520and%2520Cheolmin%2520Kim%2520and%2520Audrey%2520Gutierrez%2520and%2520Abhanshu%2520Sharma%2520and%2520Yongqin%2520Xian%2520and%2520Sungyong%2520Seo%2520and%2520Claire%2520Cui%2520and%2520Elena%2520Pochernina%2520and%2520Cip%2520Baetu%2520and%2520Krzysztof%2520Jastrz%25C4%2599bski%2520and%2520Mimi%2520Ly%2520and%2520Mohamed%2520Elhawaty%2520and%2520Dan%2520Suh%2520and%2520Eren%2520Sezener%2520and%2520Pidong%2520Wang%2520and%2520Nancy%2520Yuen%2520and%2520George%2520Tucker%2520and%2520Jiahao%2520Cai%2520and%2520Zuguang%2520Yang%2520and%2520Cindy%2520Wang%2520and%2520Alex%2520Muzio%2520and%2520Hai%2520Qian%2520and%2520Jae%2520Yoo%2520and%2520Derek%2520Lockhart%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Mandy%2520Guo%2520and%2520Malika%2520Mehrotra%2520and%2520Artur%2520Mendon%25C3%25A7a%2520and%2520Sanket%2520Vaibhav%2520Mehta%2520and%2520Sherry%2520Ben%2520and%2520Chetan%2520Tekur%2520and%2520Jiaqi%2520Mu%2520and%2520Muye%2520Zhu%2520and%2520Victoria%2520Krakovna%2520and%2520Hongrae%2520Lee%2520and%2520AJ%2520Maschinot%2520and%2520S%25C3%25A9bastien%2520Cevey%2520and%2520HyunJeong%2520Choe%2520and%2520Aijun%2520Bai%2520and%2520Hansa%2520Srinivasan%2520and%2520Derek%2520Gasaway%2520and%2520Nick%2520Young%2520and%2520Patrick%2520Siegler%2520and%2520Dan%2520Holtmann-Rice%2520and%2520Vihari%2520Piratla%2520and%2520Kate%2520Baumli%2520and%2520Roey%2520Yogev%2520and%2520Alex%2520Hofer%2520and%2520Hado%2520van%2520Hasselt%2520and%2520Svetlana%2520Grant%2520and%2520Yuri%2520Chervonyi%2520and%2520David%2520Silver%2520and%2520Andrew%2520Hogue%2520and%2520Ayushi%2520Agarwal%2520and%2520Kathie%2520Wang%2520and%2520Preeti%2520Singh%2520and%2520Four%2520Flynn%2520and%2520Josh%2520Lipschultz%2520and%2520Robert%2520David%2520and%2520Lizzetth%2520Bellot%2520and%2520Yao-Yuan%2520Yang%2520and%2520Long%2520Le%2520and%2520Filippo%2520Graziano%2520and%2520Kate%2520Olszewska%2520and%2520Kevin%2520Hui%2520and%2520Akanksha%2520Maurya%2520and%2520Nikos%2520Parotsidis%2520and%2520Weijie%2520Chen%2520and%2520Tayo%2520Oguntebi%2520and%2520Joe%2520Kelley%2520and%2520Anirudh%2520Baddepudi%2520and%2520Johannes%2520Mauerer%2520and%2520Gregory%2520Shaw%2520and%2520Alex%2520Siegman%2520and%2520Lin%2520Yang%2520and%2520Shravya%2520Shetty%2520and%2520Subhrajit%2520Roy%2520and%2520Yunting%2520Song%2520and%2520Wojciech%2520Stokowiec%2520and%2520Ryan%2520Burnell%2520and%2520Omkar%2520Savant%2520and%2520Robert%2520Busa-Fekete%2520and%2520Jin%2520Miao%2520and%2520Samrat%2520Ghosh%2520and%2520Liam%2520MacDermed%2520and%2520Phillip%2520Lippe%2520and%2520Mikhail%2520Dektiarev%2520and%2520Zach%2520Behrman%2520and%2520Fabian%2520Mentzer%2520and%2520Kelvin%2520Nguyen%2520and%2520Meng%2520Wei%2520and%2520Siddharth%2520Verma%2520and%2520Chris%2520Knutsen%2520and%2520Sudeep%2520Dasari%2520and%2520Zhipeng%2520Yan%2520and%2520Petr%2520Mitrichev%2520and%2520Xingyu%2520Wang%2520and%2520Virat%2520Shejwalkar%2520and%2520Jacob%2520Austin%2520and%2520Srinivas%2520Sunkara%2520and%2520Navneet%2520Potti%2520and%2520Yan%2520Virin%2520and%2520Christian%2520Wright%2520and%2520Ga%25C3%25ABl%2520Liu%2520and%2520Oriana%2520Riva%2520and%2520Etienne%2520Pot%2520and%2520Greg%2520Kochanski%2520and%2520Quoc%2520Le%2520and%2520Gargi%2520Balasubramaniam%2520and%2520Arka%2520Dhar%2520and%2520Yuguo%2520Liao%2520and%2520Adam%2520Bloniarz%2520and%2520Divyansh%2520Shukla%2520and%2520Elizabeth%2520Cole%2520and%2520Jong%2520Lee%2520and%2520Sheng%2520Zhang%2520and%2520Sushant%2520Kafle%2520and%2520Siddharth%2520Vashishtha%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Grace%2520Chen%2520and%2520Raphael%2520Hoffmann%2520and%2520Pranesh%2520Srinivasan%2520and%2520Agustin%2520Dal%2520Lago%2520and%2520Yoav%2520Ben%2520Shalom%2520and%2520Zi%2520Wang%2520and%2520Michael%2520Elabd%2520and%2520Anuj%2520Sharma%2520and%2520Junhyuk%2520Oh%2520and%2520Suraj%2520Kothawade%2520and%2520Maigo%2520Le%2520and%2520Marianne%2520Monteiro%2520and%2520Shentao%2520Yang%2520and%2520Kaiz%2520Alarakyia%2520and%2520Robert%2520Geirhos%2520and%2520Diana%2520Mincu%2520and%2520H%25C3%25A5vard%2520Garnes%2520and%2520Hayato%2520Kobayashi%2520and%2520Soroosh%2520Mariooryad%2520and%2520Kacper%2520Krasowiak%2520and%2520%2520Zhixin%2520and%2520%2520Lai%2520and%2520Shibl%2520Mourad%2520and%2520Mingqiu%2520Wang%2520and%2520Fan%2520Bu%2520and%2520Ophir%2520Aharoni%2520and%2520Guanjie%2520Chen%2520and%2520Abhimanyu%2520Goyal%2520and%2520Vadim%2520Zubov%2520and%2520Ankur%2520Bapna%2520and%2520Elahe%2520Dabir%2520and%2520Nisarg%2520Kothari%2520and%2520Kay%2520Lamerigts%2520and%2520Nicola%2520De%2520Cao%2520and%2520Jeremy%2520Shar%2520and%2520Christopher%2520Yew%2520and%2520Nitish%2520Kulkarni%2520and%2520Dre%2520Mahaarachchi%2520and%2520Mandar%2520Joshi%2520and%2520Zhenhai%2520Zhu%2520and%2520Jared%2520Lichtarge%2520and%2520Yichao%2520Zhou%2520and%2520Hannah%2520Muckenhirn%2520and%2520Vittorio%2520Selo%2520and%2520Oriol%2520Vinyals%2520and%2520Peter%2520Chen%2520and%2520Anthony%2520Brohan%2520and%2520Vaibhav%2520Mehta%2520and%2520Sarah%2520Cogan%2520and%2520Ruth%2520Wang%2520and%2520Ty%2520Geri%2520and%2520Wei-Jen%2520Ko%2520and%2520Wei%2520Chen%2520and%2520Fabio%2520Viola%2520and%2520Keshav%2520Shivam%2520and%2520Lisa%2520Wang%2520and%2520Madeleine%2520Clare%2520Elish%2520and%2520Raluca%2520Ada%2520Popa%2520and%2520S%25C3%25A9bastien%2520Pereira%2520and%2520Jianqiao%2520Liu%2520and%2520Raphael%2520Koster%2520and%2520Donnie%2520Kim%2520and%2520Gufeng%2520Zhang%2520and%2520Sayna%2520Ebrahimi%2520and%2520Partha%2520Talukdar%2520and%2520Yanyan%2520Zheng%2520and%2520Petra%2520Poklukar%2520and%2520Ales%2520Mikhalap%2520and%2520Dale%2520Johnson%2520and%2520Anitha%2520Vijayakumar%2520and%2520Mark%2520Omernick%2520and%2520Matt%2520Dibb%2520and%2520Ayush%2520Dubey%2520and%2520Qiong%2520Hu%2520and%2520Apurv%2520Suman%2520and%2520Vaibhav%2520Aggarwal%2520and%2520Ilya%2520Kornakov%2520and%2520Fei%2520Xia%2520and%2520Wing%2520Lowe%2520and%2520Alexey%2520Kolganov%2520and%2520Ted%2520Xiao%2520and%2520Vitaly%2520Nikolaev%2520and%2520Steven%2520Hemingray%2520and%2520Bonnie%2520Li%2520and%2520Joana%2520Iljazi%2520and%2520Miko%25C5%2582aj%2520Rybi%25C5%2584ski%2520and%2520Ballie%2520Sandhu%2520and%2520Peggy%2520Lu%2520and%2520Thang%2520Luong%2520and%2520Rodolphe%2520Jenatton%2520and%2520Vineetha%2520Govindaraj%2520and%2520%2520Hui%2520and%2520%2520Li%2520and%2520Gabriel%2520Dulac-Arnold%2520and%2520Wonpyo%2520Park%2520and%2520Henry%2520Wang%2520and%2520Abhinit%2520Modi%2520and%2520Jean%2520Pouget-Abadie%2520and%2520Kristina%2520Greller%2520and%2520Rahul%2520Gupta%2520and%2520Robert%2520Berry%2520and%2520Prajit%2520Ramachandran%2520and%2520Jinyu%2520Xie%2520and%2520Liam%2520McCafferty%2520and%2520Jianling%2520Wang%2520and%2520Kilol%2520Gupta%2520and%2520Hyeontaek%2520Lim%2520and%2520Bla%25C5%25BE%2520Bratani%25C4%258D%2520and%2520Andy%2520Brock%2520and%2520Ilia%2520Akolzin%2520and%2520Jim%2520Sproch%2520and%2520Dan%2520Karliner%2520and%2520Duhyeon%2520Kim%2520and%2520Adrian%2520Goedeckemeyer%2520and%2520Noam%2520Shazeer%2520and%2520Cordelia%2520Schmid%2520and%2520Daniele%2520Calandriello%2520and%2520Parul%2520Bhatia%2520and%2520Krzysztof%2520Choromanski%2520and%2520Ceslee%2520Montgomery%2520and%2520Dheeru%2520Dua%2520and%2520Ana%2520Ramalho%2520and%2520Helen%2520King%2520and%2520Yue%2520Gao%2520and%2520Lynn%2520Nguyen%2520and%2520David%2520Lindner%2520and%2520Divya%2520Pitta%2520and%2520Oleaser%2520Johnson%2520and%2520Khalid%2520Salama%2520and%2520Diego%2520Ardila%2520and%2520Michael%2520Han%2520and%2520Erin%2520Farnese%2520and%2520Seth%2520Odoom%2520and%2520Ziyue%2520Wang%2520and%2520Xiangzhuo%2520Ding%2520and%2520Norman%2520Rink%2520and%2520Ray%2520Smith%2520and%2520Harshal%2520Tushar%2520Lehri%2520and%2520Eden%2520Cohen%2520and%2520Neera%2520Vats%2520and%2520Tong%2520He%2520and%2520Parthasarathy%2520Gopavarapu%2520and%2520Adam%2520Paszke%2520and%2520Miteyan%2520Patel%2520and%2520Wouter%2520Van%2520Gansbeke%2520and%2520Lucia%2520Loher%2520and%2520Luis%2520Castro%2520and%2520Maria%2520Voitovich%2520and%2520Tamara%2520von%2520Glehn%2520and%2520Nelson%2520George%2520and%2520Simon%2520Niklaus%2520and%2520Zach%2520Eaton-Rosen%2520and%2520Nemanja%2520Raki%25C4%2587evi%25C4%2587%2520and%2520Erik%2520Jue%2520and%2520Sagi%2520Perel%2520and%2520Carrie%2520Zhang%2520and%2520Yuval%2520Bahat%2520and%2520Ang%25C3%25A9line%2520Pouget%2520and%2520Zhi%2520Xing%2520and%2520Fantine%2520Huot%2520and%2520Ashish%2520Shenoy%2520and%2520Taylor%2520Bos%2520and%2520Vincent%2520Coriou%2520and%2520Bryan%2520Richter%2520and%2520Natasha%2520Noy%2520and%2520Yaqing%2520Wang%2520and%2520Santiago%2520Ontanon%2520and%2520Siyang%2520Qin%2520and%2520Gleb%2520Makarchuk%2520and%2520Demis%2520Hassabis%2520and%2520Zhuowan%2520Li%2520and%2520Mandar%2520Sharma%2520and%2520Kumaran%2520Venkatesan%2520and%2520Iurii%2520Kemaev%2520and%2520Roxanne%2520Daniel%2520and%2520Shiyu%2520Huang%2520and%2520Saloni%2520Shah%2520and%2520Octavio%2520Ponce%2520and%2520%2520Warren%2520and%2520%2520Chen%2520and%2520Manaal%2520Faruqui%2520and%2520Jialin%2520Wu%2520and%2520Slavica%2520Anda%25C4%258Di%25C4%2587%2520and%2520Szabolcs%2520Payrits%2520and%2520Daniel%2520McDuff%2520and%2520Tom%2520Hume%2520and%2520Yuan%2520Cao%2520and%2520MH%2520Tessler%2520and%2520Qingze%2520Wang%2520and%2520Yinan%2520Wang%2520and%2520Ivor%2520Rendulic%2520and%2520Eirikur%2520Agustsson%2520and%2520Matthew%2520Johnson%2520and%2520Tanya%2520Lando%2520and%2520Andrew%2520Howard%2520and%2520Sri%2520Gayatri%2520Sundara%2520Padmanabhan%2520and%2520Mayank%2520Daswani%2520and%2520Andrea%2520Banino%2520and%2520Michael%2520Kilgore%2520and%2520Jonathan%2520Heek%2520and%2520Ziwei%2520Ji%2520and%2520Alvaro%2520Caceres%2520and%2520Conglong%2520Li%2520and%2520Nora%2520Kassner%2520and%2520Alexey%2520Vlaskin%2520and%2520Zeyu%2520Liu%2520and%2520Alex%2520Grills%2520and%2520Yanhan%2520Hou%2520and%2520Roykrong%2520Sukkerd%2520and%2520Gowoon%2520Cheon%2520and%2520Nishita%2520Shetty%2520and%2520Larisa%2520Markeeva%2520and%2520Piotr%2520Stanczyk%2520and%2520Tejas%2520Iyer%2520and%2520Yuan%2520Gong%2520and%2520Shawn%2520Gao%2520and%2520Keerthana%2520Gopalakrishnan%2520and%2520Tim%2520Blyth%2520and%2520Malcolm%2520Reynolds%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Misha%2520Bilenko%2520and%2520Dero%2520Gharibian%2520and%2520Vicky%2520Zayats%2520and%2520Aleksandra%2520Faust%2520and%2520Abhinav%2520Singh%2520and%2520Min%2520Ma%2520and%2520Hongyang%2520Jiao%2520and%2520Sudheendra%2520Vijayanarasimhan%2520and%2520Lora%2520Aroyo%2520and%2520Vikas%2520Yadav%2520and%2520Sarah%2520Chakera%2520and%2520Ashwin%2520Kakarla%2520and%2520Vilobh%2520Meshram%2520and%2520Karol%2520Gregor%2520and%2520Gabriela%2520Botea%2520and%2520Evan%2520Senter%2520and%2520Dawei%2520Jia%2520and%2520Geza%2520Kovacs%2520and%2520Neha%2520Sharma%2520and%2520Sebastien%2520Baur%2520and%2520Kai%2520Kang%2520and%2520Yifan%2520He%2520and%2520Lin%2520Zhuo%2520and%2520Marija%2520Kostelac%2520and%2520Itay%2520Laish%2520and%2520Songyou%2520Peng%2520and%2520Louis%2520O%2527Bryan%2520and%2520Daniel%2520Kasenberg%2520and%2520Girish%2520Ramchandra%2520Rao%2520and%2520Edouard%2520Leurent%2520and%2520Biao%2520Zhang%2520and%2520Sage%2520Stevens%2520and%2520Ana%2520Salazar%2520and%2520Ye%2520Zhang%2520and%2520Ivan%2520Lobov%2520and%2520Jake%2520Walker%2520and%2520Allen%2520Porter%2520and%2520Morgan%2520Redshaw%2520and%2520Han%2520Ke%2520and%2520Abhishek%2520Rao%2520and%2520Alex%2520Lee%2520and%2520Hoi%2520Lam%2520and%2520Michael%2520Moffitt%2520and%2520Jaeyoun%2520Kim%2520and%2520Siyuan%2520Qiao%2520and%2520Terry%2520Koo%2520and%2520Robert%2520Dadashi%2520and%2520Xinying%2520Song%2520and%2520Mukund%2520Sundararajan%2520and%2520Peng%2520Xu%2520and%2520Chizu%2520Kawamoto%2520and%2520Yan%2520Zhong%2520and%2520Clara%2520Barbu%2520and%2520Apoorv%2520Reddy%2520and%2520Mauro%2520Verzetti%2520and%2520Leon%2520Li%2520and%2520George%2520Papamakarios%2520and%2520Hanna%2520Klimczak-Pluci%25C5%2584ska%2520and%2520Mary%2520Cassin%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Rigel%2520Swavely%2520and%2520Alain%2520Vaucher%2520and%2520Jeffrey%2520Zhao%2520and%2520Ross%2520Hemsley%2520and%2520Michael%2520Tschannen%2520and%2520Heming%2520Ge%2520and%2520Gaurav%2520Menghani%2520and%2520Yang%2520Yu%2520and%2520Natalie%2520Ha%2520and%2520Wei%2520He%2520and%2520Xiao%2520Wu%2520and%2520Maggie%2520Song%2520and%2520Rachel%2520Sterneck%2520and%2520Stefan%2520Zinke%2520and%2520Dan%2520A.%2520Calian%2520and%2520Annie%2520Marsden%2520and%2520Alejandro%2520Cruzado%2520Ruiz%2520and%2520Matteo%2520Hessel%2520and%2520Almog%2520Gueta%2520and%2520Benjamin%2520Lee%2520and%2520Brian%2520Farris%2520and%2520Manish%2520Gupta%2520and%2520Yunjie%2520Li%2520and%2520Mohammad%2520Saleh%2520and%2520Vedant%2520Misra%2520and%2520Kefan%2520Xiao%2520and%2520Piermaria%2520Mendolicchio%2520and%2520Gavin%2520Buttimore%2520and%2520Varvara%2520Krayvanova%2520and%2520Nigamaa%2520Nayakanti%2520and%2520Matthew%2520Wiethoff%2520and%2520Yash%2520Pande%2520and%2520Azalia%2520Mirhoseini%2520and%2520Ni%2520Lao%2520and%2520Jasmine%2520Liu%2520and%2520Yiqing%2520Hua%2520and%2520Angie%2520Chen%2520and%2520Yury%2520Malkov%2520and%2520Dmitry%2520Kalashnikov%2520and%2520Shubham%2520Gupta%2520and%2520Kartik%2520Audhkhasi%2520and%2520Yuexiang%2520Zhai%2520and%2520Sudhindra%2520Kopalle%2520and%2520Prateek%2520Jain%2520and%2520Eran%2520Ofek%2520and%2520Clemens%2520Meyer%2520and%2520Khuslen%2520Baatarsukh%2520and%2520Hana%2520Strej%25C4%258Dek%2520and%2520Jun%2520Qian%2520and%2520James%2520Freedman%2520and%2520Ricardo%2520Figueira%2520and%2520Michal%2520Sokolik%2520and%2520Olivier%2520Bachem%2520and%2520Raymond%2520Lin%2520and%2520Dia%2520Kharrat%2520and%2520Chris%2520Hidey%2520and%2520Pingmei%2520Xu%2520and%2520Dennis%2520Duan%2520and%2520Yin%2520Li%2520and%2520Muge%2520Ersoy%2520and%2520Richard%2520Everett%2520and%2520Kevin%2520Cen%2520and%2520Rebeca%2520Santamaria-Fernandez%2520and%2520Amir%2520Taubenfeld%2520and%2520Ian%2520Mackinnon%2520and%2520Linda%2520Deng%2520and%2520Polina%2520Zablotskaia%2520and%2520Shashank%2520Viswanadha%2520and%2520Shivanker%2520Goel%2520and%2520Damion%2520Yates%2520and%2520Yunxiao%2520Deng%2520and%2520Peter%2520Choy%2520and%2520Mingqing%2520Chen%2520and%2520Abhishek%2520Sinha%2520and%2520Alex%2520Mossin%2520and%2520Yiming%2520Wang%2520and%2520Arthur%2520Szlam%2520and%2520Susan%2520Hao%2520and%2520Paul%2520Kishan%2520Rubenstein%2520and%2520Metin%2520Toksoz-Exley%2520and%2520Miranda%2520Aperghis%2520and%2520Yin%2520Zhong%2520and%2520Junwhan%2520Ahn%2520and%2520Michael%2520Isard%2520and%2520Olivier%2520Lacombe%2520and%2520Florian%2520Luisier%2520and%2520Chrysovalantis%2520Anastasiou%2520and%2520Yogesh%2520Kalley%2520and%2520Utsav%2520Prabhu%2520and%2520Emma%2520Dunleavy%2520and%2520Shaan%2520Bijwadia%2520and%2520Justin%2520Mao-Jones%2520and%2520Kelly%2520Chen%2520and%2520Rama%2520Pasumarthi%2520and%2520Emily%2520Wood%2520and%2520Adil%2520Dostmohamed%2520and%2520Nate%2520Hurley%2520and%2520Jiri%2520Simsa%2520and%2520Alicia%2520Parrish%2520and%2520Mantas%2520Pajarskas%2520and%2520Matt%2520Harvey%2520and%2520Ondrej%2520Skopek%2520and%2520Yony%2520Kochinski%2520and%2520Javier%2520Rey%2520and%2520Verena%2520Rieser%2520and%2520Denny%2520Zhou%2520and%2520Sun%2520Jae%2520Lee%2520and%2520Trilok%2520Acharya%2520and%2520Guowang%2520Li%2520and%2520Joe%2520Jiang%2520and%2520Xiaofan%2520Zhang%2520and%2520Bryant%2520Gipson%2520and%2520Ethan%2520Mahintorabi%2520and%2520Marco%2520Gelmi%2520and%2520Nima%2520Khajehnouri%2520and%2520Angel%2520Yeh%2520and%2520Kayi%2520Lee%2520and%2520Loic%2520Matthey%2520and%2520Leslie%2520Baker%2520and%2520Trang%2520Pham%2520and%2520Han%2520Fu%2520and%2520Alex%2520Pak%2520and%2520Prakhar%2520Gupta%2520and%2520Cristina%2520Vasconcelos%2520and%2520Adam%2520Sadovsky%2520and%2520Brian%2520Walker%2520and%2520Sissie%2520Hsiao%2520and%2520Patrik%2520Zochbauer%2520and%2520Andreea%2520Marzoca%2520and%2520Noam%2520Velan%2520and%2520Junhao%2520Zeng%2520and%2520Gilles%2520Baechler%2520and%2520Danny%2520Driess%2520and%2520Divya%2520Jain%2520and%2520Yanping%2520Huang%2520and%2520Lizzie%2520Tao%2520and%2520John%2520Maggs%2520and%2520Nir%2520Levine%2520and%2520Jon%2520Schneider%2520and%2520Erika%2520Gemzer%2520and%2520Samuel%2520Petit%2520and%2520Shan%2520Han%2520and%2520Zach%2520Fisher%2520and%2520Dustin%2520Zelle%2520and%2520Courtney%2520Biles%2520and%2520Eugene%2520Ie%2520and%2520Asya%2520Fadeeva%2520and%2520Casper%2520Liu%2520and%2520Juliana%2520Vicente%2520Franco%2520and%2520Adrian%2520Collister%2520and%2520Hao%2520Zhang%2520and%2520Renshen%2520Wang%2520and%2520Ruizhe%2520Zhao%2520and%2520Leandro%2520Kieliger%2520and%2520Kurt%2520Shuster%2520and%2520Rui%2520Zhu%2520and%2520Boqing%2520Gong%2520and%2520Lawrence%2520Chan%2520and%2520Ruoxi%2520Sun%2520and%2520Sujoy%2520Basu%2520and%2520Roland%2520Zimmermann%2520and%2520Jamie%2520Hayes%2520and%2520Abhishek%2520Bapna%2520and%2520Jasper%2520Snoek%2520and%2520Weel%2520Yang%2520and%2520Puranjay%2520Datta%2520and%2520Jad%2520Al%2520Abdallah%2520and%2520Kevin%2520Kilgour%2520and%2520Lu%2520Li%2520and%2520SQ%2520Mah%2520and%2520Yennie%2520Jun%2520and%2520Morgane%2520Rivi%25C3%25A8re%2520and%2520Abhijit%2520Karmarkar%2520and%2520Tammo%2520Spalink%2520and%2520Tao%2520Huang%2520and%2520Lucas%2520Gonzalez%2520and%2520Duc-Hieu%2520Tran%2520and%2520Averi%2520Nowak%2520and%2520John%2520Palowitch%2520and%2520Martin%2520Chadwick%2520and%2520Ellie%2520Talius%2520and%2520Harsh%2520Mehta%2520and%2520Thibault%2520Sellam%2520and%2520Philipp%2520Fr%25C3%25A4nken%2520and%2520Massimo%2520Nicosia%2520and%2520Kyle%2520He%2520and%2520Aditya%2520Kini%2520and%2520David%2520Amos%2520and%2520Sugato%2520Basu%2520and%2520Harrison%2520Jobe%2520and%2520Eleni%2520Shaw%2520and%2520Qiantong%2520Xu%2520and%2520Colin%2520Evans%2520and%2520Daisuke%2520Ikeda%2520and%2520Chaochao%2520Yan%2520and%2520Larry%2520Jin%2520and%2520Lun%2520Wang%2520and%2520Sachin%2520Yadav%2520and%2520Ilia%2520Labzovsky%2520and%2520Ramesh%2520Sampath%2520and%2520Ada%2520Ma%2520and%2520Candice%2520Schumann%2520and%2520Aditya%2520Siddhant%2520and%2520Rohin%2520Shah%2520and%2520John%2520Youssef%2520and%2520Rishabh%2520Agarwal%2520and%2520Natalie%2520Dabney%2520and%2520Alessio%2520Tonioni%2520and%2520Moran%2520Ambar%2520and%2520Jing%2520Li%2520and%2520Isabelle%2520Guyon%2520and%2520Benny%2520Li%2520and%2520David%2520Soergel%2520and%2520Boya%2520Fang%2520and%2520Georgi%2520Karadzhov%2520and%2520Cristian%2520Udrescu%2520and%2520Trieu%2520Trinh%2520and%2520Vikas%2520Raunak%2520and%2520Seb%2520Noury%2520and%2520Dee%2520Guo%2520and%2520Sonal%2520Gupta%2520and%2520Mara%2520Finkelstein%2520and%2520Denis%2520Petek%2520and%2520Lihao%2520Liang%2520and%2520Greg%2520Billock%2520and%2520Pei%2520Sun%2520and%2520David%2520Wood%2520and%2520Yiwen%2520Song%2520and%2520Xiaobin%2520Yu%2520and%2520Tatiana%2520Matejovicova%2520and%2520Regev%2520Cohen%2520and%2520Kalyan%2520Andra%2520and%2520David%2520D%2527Ambrosio%2520and%2520Zhiwei%2520Deng%2520and%2520Vincent%2520Nallatamby%2520and%2520Ebrahim%2520Songhori%2520and%2520Rumen%2520Dangovski%2520and%2520Andrew%2520Lampinen%2520and%2520Pankil%2520Botadra%2520and%2520Adam%2520Hillier%2520and%2520Jiawei%2520Cao%2520and%2520Nagabhushan%2520Baddi%2520and%2520Adhi%2520Kuncoro%2520and%2520Toshihiro%2520Yoshino%2520and%2520Ankit%2520Bhagatwala%2520and%2520Marc%25C3%25A1urelio%2520Ranzato%2520and%2520Rylan%2520Schaeffer%2520and%2520Tianlin%2520Liu%2520and%2520Shuai%2520Ye%2520and%2520Obaid%2520Sarvana%2520and%2520John%2520Nham%2520and%2520Chenkai%2520Kuang%2520and%2520Isabel%2520Gao%2520and%2520Jinoo%2520Baek%2520and%2520Shubham%2520Mittal%2520and%2520Ayzaan%2520Wahid%2520and%2520Anita%2520Gergely%2520and%2520Bin%2520Ni%2520and%2520Josh%2520Feldman%2520and%2520Carrie%2520Muir%2520and%2520Pascal%2520Lamblin%2520and%2520Wolfgang%2520Macherey%2520and%2520Ethan%2520Dyer%2520and%2520Logan%2520Kilpatrick%2520and%2520V%25C3%25ADctor%2520Campos%2520and%2520Mukul%2520Bhutani%2520and%2520Stanislav%2520Fort%2520and%2520Yanif%2520Ahmad%2520and%2520Aliaksei%2520Severyn%2520and%2520Kleopatra%2520Chatziprimou%2520and%2520Oleksandr%2520Ferludin%2520and%2520Mason%2520Dimarco%2520and%2520Aditya%2520Kusupati%2520and%2520Joe%2520Heyward%2520and%2520Dan%2520Bahir%2520and%2520Kevin%2520Villela%2520and%2520Katie%2520Millican%2520and%2520Dror%2520Marcus%2520and%2520Sanaz%2520Bahargam%2520and%2520Caglar%2520Unlu%2520and%2520Nicholas%2520Roth%2520and%2520Zichuan%2520Wei%2520and%2520Siddharth%2520Gopal%2520and%2520Deepanway%2520Ghoshal%2520and%2520Edward%2520Lee%2520and%2520Sharon%2520Lin%2520and%2520Jennie%2520Lees%2520and%2520Dayeong%2520Lee%2520and%2520Anahita%2520Hosseini%2520and%2520Connie%2520Fan%2520and%2520Seth%2520Neel%2520and%2520Marcus%2520Wu%2520and%2520Yasemin%2520Altun%2520and%2520Honglong%2520Cai%2520and%2520Enrique%2520Piqueras%2520and%2520Josh%2520Woodward%2520and%2520Alessandro%2520Bissacco%2520and%2520Salem%2520Haykal%2520and%2520Mahyar%2520Bordbar%2520and%2520Prasha%2520Sundaram%2520and%2520Sarah%2520Hodkinson%2520and%2520Daniel%2520Toyama%2520and%2520George%2520Polovets%2520and%2520Austin%2520Myers%2520and%2520Anu%2520Sinha%2520and%2520Tomer%2520Levinboim%2520and%2520Kashyap%2520Krishnakumar%2520and%2520Rachita%2520Chhaparia%2520and%2520Tatiana%2520Sholokhova%2520and%2520Nitesh%2520Bharadwaj%2520Gundavarapu%2520and%2520Ganesh%2520Jawahar%2520and%2520Haroon%2520Qureshi%2520and%2520Jieru%2520Hu%2520and%2520Nikola%2520Momchev%2520and%2520Matthew%2520Rahtz%2520and%2520Renjie%2520Wu%2520and%2520Aishwarya%2520P%2520S%2520and%2520Kedar%2520Dhamdhere%2520and%2520Meiqi%2520Guo%2520and%2520Umang%2520Gupta%2520and%2520Ali%2520Eslami%2520and%2520Mariano%2520Schain%2520and%2520Michiel%2520Blokzijl%2520and%2520David%2520Welling%2520and%2520Dave%2520Orr%2520and%2520Levent%2520Bolelli%2520and%2520Nicolas%2520Perez-Nieves%2520and%2520Mikhail%2520Sirotenko%2520and%2520Aman%2520Prasad%2520and%2520Arjun%2520Kar%2520and%2520Borja%2520De%2520Balle%2520Pigem%2520and%2520Tayfun%2520Terzi%2520and%2520Gell%25C3%25A9rt%2520Weisz%2520and%2520Dipankar%2520Ghosh%2520and%2520Aditi%2520Mavalankar%2520and%2520Dhruv%2520Madeka%2520and%2520Kaspar%2520Daugaard%2520and%2520Hartwig%2520Adam%2520and%2520Viraj%2520Shah%2520and%2520Dana%2520Berman%2520and%2520Maggie%2520Tran%2520and%2520Steven%2520Baker%2520and%2520Ewa%2520Andrejczuk%2520and%2520Grishma%2520Chole%2520and%2520Ganna%2520Raboshchuk%2520and%2520Mahdi%2520Mirzazadeh%2520and%2520Thais%2520Kagohara%2520and%2520Shimu%2520Wu%2520and%2520Christian%2520Schallhart%2520and%2520Bernett%2520Orlando%2520and%2520Chen%2520Wang%2520and%2520Alban%2520Rrustemi%2520and%2520Hao%2520Xiong%2520and%2520Hao%2520Liu%2520and%2520Arpi%2520Vezer%2520and%2520Nolan%2520Ramsden%2520and%2520Shuo-yiin%2520Chang%2520and%2520Sidharth%2520Mudgal%2520and%2520Yan%2520Li%2520and%2520Nino%2520Vieillard%2520and%2520Yedid%2520Hoshen%2520and%2520Farooq%2520Ahmad%2520and%2520Ambrose%2520Slone%2520and%2520Amy%2520Hua%2520and%2520Natan%2520Potikha%2520and%2520Mirko%2520Rossini%2520and%2520Jon%2520Stritar%2520and%2520Sushant%2520Prakash%2520and%2520Zifeng%2520Wang%2520and%2520Xuanyi%2520Dong%2520and%2520Alireza%2520Nazari%2520and%2520Efrat%2520Nehoran%2520and%2520Kaan%2520Tekelioglu%2520and%2520Yinxiao%2520Li%2520and%2520Kartikeya%2520Badola%2520and%2520Tom%2520Funkhouser%2520and%2520Yuanzhen%2520Li%2520and%2520Varun%2520Yerram%2520and%2520Ramya%2520Ganeshan%2520and%2520Daniel%2520Formoso%2520and%2520Karol%2520Langner%2520and%2520Tian%2520Shi%2520and%2520Huijian%2520Li%2520and%2520Yumeya%2520Yamamori%2520and%2520Amayika%2520Panda%2520and%2520Alaa%2520Saade%2520and%2520Angelo%2520Scorza%2520Scarpati%2520and%2520Chris%2520Breaux%2520and%2520CJ%2520Carey%2520and%2520Zongwei%2520Zhou%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Sophie%2520Bridgers%2520and%2520Alena%2520Butryna%2520and%2520Nishesh%2520Gupta%2520and%2520Vaibhav%2520Tulsyan%2520and%2520Sanghyun%2520Woo%2520and%2520Evgenii%2520Eltyshev%2520and%2520Will%2520Grathwohl%2520and%2520Chanel%2520Parks%2520and%2520Seth%2520Benjamin%2520and%2520Rina%2520Panigrahy%2520and%2520Shenil%2520Dodhia%2520and%2520Daniel%2520De%2520Freitas%2520and%2520Chris%2520Sauer%2520and%2520Will%2520Song%2520and%2520Ferran%2520Alet%2520and%2520Jackson%2520Tolins%2520and%2520Cosmin%2520Paduraru%2520and%2520Xingyi%2520Zhou%2520and%2520Brian%2520Albert%2520and%2520Zizhao%2520Zhang%2520and%2520Lei%2520Shu%2520and%2520Mudit%2520Bansal%2520and%2520Sarah%2520Nguyen%2520and%2520Amir%2520Globerson%2520and%2520Owen%2520Xiao%2520and%2520James%2520Manyika%2520and%2520Tom%2520Hennigan%2520and%2520Rong%2520Rong%2520and%2520Josip%2520Matak%2520and%2520Anton%2520Bakalov%2520and%2520Ankur%2520Sharma%2520and%2520Danila%2520Sinopalnikov%2520and%2520Andrew%2520Pierson%2520and%2520Stephen%2520Roller%2520and%2520Geoff%2520Brown%2520and%2520Mingcen%2520Gao%2520and%2520Toshiyuki%2520Fukuzawa%2520and%2520Amin%2520Ghafouri%2520and%2520Kenny%2520Vassigh%2520and%2520Iain%2520Barr%2520and%2520Zhicheng%2520Wang%2520and%2520Anna%2520Korsun%2520and%2520Rajesh%2520Jayaram%2520and%2520Lijie%2520Ren%2520and%2520Tim%2520Zaman%2520and%2520Samira%2520Khan%2520and%2520Yana%2520Lunts%2520and%2520Dan%2520Deutsch%2520and%2520Dave%2520Uthus%2520and%2520Nitzan%2520Katz%2520and%2520Masha%2520Samsikova%2520and%2520Amr%2520Khalifa%2520and%2520Nikhil%2520Sethi%2520and%2520Jiao%2520Sun%2520and%2520Luming%2520Tang%2520and%2520Uri%2520Alon%2520and%2520Xianghong%2520Luo%2520and%2520Dian%2520Yu%2520and%2520Abhishek%2520Nayyar%2520and%2520Bryce%2520Petrini%2520and%2520Will%2520Truong%2520and%2520Vincent%2520Hellendoorn%2520and%2520Nikolai%2520Chinaev%2520and%2520Chris%2520Alberti%2520and%2520Wei%2520Wang%2520and%2520Jingcao%2520Hu%2520and%2520Vahab%2520Mirrokni%2520and%2520Ananth%2520Balashankar%2520and%2520Avia%2520Aharon%2520and%2520Aahil%2520Mehta%2520and%2520Ahmet%2520Iscen%2520and%2520Joseph%2520Kready%2520and%2520Lucas%2520Manning%2520and%2520Anhad%2520Mohananey%2520and%2520Yuankai%2520Chen%2520and%2520Anshuman%2520Tripathi%2520and%2520Allen%2520Wu%2520and%2520Igor%2520Petrovski%2520and%2520Dawsen%2520Hwang%2520and%2520Martin%2520Baeuml%2520and%2520Shreyas%2520Chandrakaladharan%2520and%2520Yuan%2520Liu%2520and%2520Rey%2520Coaguila%2520and%2520Maxwell%2520Chen%2520and%2520Sally%2520Ma%2520and%2520Pouya%2520Tafti%2520and%2520Susheel%2520Tatineni%2520and%2520Terry%2520Spitz%2520and%2520Jiayu%2520Ye%2520and%2520Paul%2520Vicol%2520and%2520Mihaela%2520Rosca%2520and%2520Adri%25C3%25A0%2520Puigdom%25C3%25A8nech%2520and%2520Zohar%2520Yahav%2520and%2520Sanjay%2520Ghemawat%2520and%2520Hanzhao%2520Lin%2520and%2520Phoebe%2520Kirk%2520and%2520Zaid%2520Nabulsi%2520and%2520Sergey%2520Brin%2520and%2520Bernd%2520Bohnet%2520and%2520Ken%2520Caluwaerts%2520and%2520Aditya%2520Srikanth%2520Veerubhotla%2520and%2520Dan%2520Zheng%2520and%2520Zihang%2520Dai%2520and%2520Petre%2520Petrov%2520and%2520Yichong%2520Xu%2520and%2520Ramin%2520Mehran%2520and%2520Zhuo%2520Xu%2520and%2520Luisa%2520Zintgraf%2520and%2520Jiho%2520Choi%2520and%2520Spurthi%2520Amba%2520Hombaiah%2520and%2520Romal%2520Thoppilan%2520and%2520Sashank%2520Reddi%2520and%2520Lukasz%2520Lew%2520and%2520Li%2520Li%2520and%2520Kellie%2520Webster%2520and%2520KP%2520Sawhney%2520and%2520Lampros%2520Lamprou%2520and%2520Siamak%2520Shakeri%2520and%2520Mayank%2520Lunayach%2520and%2520Jianmin%2520Chen%2520and%2520Sumit%2520Bagri%2520and%2520Alex%2520Salcianu%2520and%2520Ying%2520Chen%2520and%2520Yani%2520Donchev%2520and%2520Charlotte%2520Magister%2520and%2520Signe%2520N%25C3%25B8rly%2520and%2520Vitor%2520Rodrigues%2520and%2520Tomas%2520Izo%2520and%2520Hila%2520Noga%2520and%2520Joe%2520Zou%2520and%2520Thomas%2520K%25C3%25B6ppe%2520and%2520Wenxuan%2520Zhou%2520and%2520Kenton%2520Lee%2520and%2520Xiangzhu%2520Long%2520and%2520Danielle%2520Eisenbud%2520and%2520Anthony%2520Chen%2520and%2520Connor%2520Schenck%2520and%2520Chi%2520Ming%2520To%2520and%2520Peilin%2520Zhong%2520and%2520Emanuel%2520Taropa%2520and%2520Minh%2520Truong%2520and%2520Omer%2520Levy%2520and%2520Danilo%2520Martins%2520and%2520Zhiyuan%2520Zhang%2520and%2520Christopher%2520Semturs%2520and%2520Kelvin%2520Zhang%2520and%2520Alex%2520Yakubovich%2520and%2520Pol%2520Moreno%2520and%2520Lara%2520McConnaughey%2520and%2520Di%2520Lu%2520and%2520Sam%2520Redmond%2520and%2520Lotte%2520Weerts%2520and%2520Yonatan%2520Bitton%2520and%2520Tiziana%2520Refice%2520and%2520Nicolas%2520Lacasse%2520and%2520Arthur%2520Conmy%2520and%2520Corentin%2520Tallec%2520and%2520Julian%2520Odell%2520and%2520Hannah%2520Forbes-Pollard%2520and%2520Arkadiusz%2520Socala%2520and%2520Jonathan%2520Hoech%2520and%2520Pushmeet%2520Kohli%2520and%2520Alanna%2520Walton%2520and%2520Rui%2520Wang%2520and%2520Mikita%2520Sazanovich%2520and%2520Kexin%2520Zhu%2520and%2520Andrei%2520Kapishnikov%2520and%2520Rich%2520Galt%2520and%2520Matthew%2520Denton%2520and%2520Ben%2520Murdoch%2520and%2520Caitlin%2520Sikora%2520and%2520Kareem%2520Mohamed%2520and%2520Wei%2520Wei%2520and%2520Uri%2520First%2520and%2520Tim%2520McConnell%2520and%2520Luis%2520C.%2520Cobo%2520and%2520James%2520Qin%2520and%2520Thi%2520Avrahami%2520and%2520Daniel%2520Balle%2520and%2520Yu%2520Watanabe%2520and%2520Annie%2520Louis%2520and%2520Adam%2520Kraft%2520and%2520Setareh%2520Ariafar%2520and%2520Yiming%2520Gu%2520and%2520Eug%25C3%25A9nie%2520Rives%2520and%2520Charles%2520Yoon%2520and%2520Andrei%2520Rusu%2520and%2520James%2520Cobon-Kerr%2520and%2520Chris%2520Hahn%2520and%2520Jiaming%2520Luo%2520and%2520%2520Yuvein%2520and%2520%2520Zhu%2520and%2520Niharika%2520Ahuja%2520and%2520Rodrigo%2520Benenson%2520and%2520Rapha%25C3%25ABl%2520Lopez%2520Kaufman%2520and%2520Honglin%2520Yu%2520and%2520Lloyd%2520Hightower%2520and%2520Junlin%2520Zhang%2520and%2520Darren%2520Ni%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Gabby%2520Wang%2520and%2520Gal%2520Yona%2520and%2520Lalit%2520Jain%2520and%2520Pablo%2520Barrio%2520and%2520Surya%2520Bhupatiraju%2520and%2520Siva%2520Velusamy%2520and%2520Allan%2520Dafoe%2520and%2520Sebastian%2520Riedel%2520and%2520Tara%2520Thomas%2520and%2520Zhe%2520Yuan%2520and%2520Mathias%2520Bellaiche%2520and%2520Sheena%2520Panthaplackel%2520and%2520Klemen%2520Kloboves%2520and%2520Sarthak%2520Jauhari%2520and%2520Canfer%2520Akbulut%2520and%2520Todor%2520Davchev%2520and%2520Evgeny%2520Gladchenko%2520and%2520David%2520Madras%2520and%2520Aleksandr%2520Chuklin%2520and%2520Tyrone%2520Hill%2520and%2520Quan%2520Yuan%2520and%2520Mukundan%2520Madhavan%2520and%2520Luke%2520Leonhard%2520and%2520Dylan%2520Scandinaro%2520and%2520Qihang%2520Chen%2520and%2520Ning%2520Niu%2520and%2520Arthur%2520Douillard%2520and%2520Bogdan%2520Damoc%2520and%2520Yasumasa%2520Onoe%2520and%2520Fabian%2520Pedregosa%2520and%2520Fred%2520Bertsch%2520and%2520Chas%2520Leichner%2520and%2520Joseph%2520Pagadora%2520and%2520Jonathan%2520Malmaud%2520and%2520Sameera%2520Ponda%2520and%2520Andy%2520Twigg%2520and%2520Oleksii%2520Duzhyi%2520and%2520Jingwei%2520Shen%2520and%2520Miaosen%2520Wang%2520and%2520Roopal%2520Garg%2520and%2520Jing%2520Chen%2520and%2520Utku%2520Evci%2520and%2520Jonathan%2520Lee%2520and%2520Leon%2520Liu%2520and%2520Koji%2520Kojima%2520and%2520Masa%2520Yamaguchi%2520and%2520Arunkumar%2520Rajendran%2520and%2520AJ%2520Piergiovanni%2520and%2520Vinodh%2520Kumar%2520Rajendran%2520and%2520Marco%2520Fornoni%2520and%2520Gabriel%2520Ibagon%2520and%2520Harry%2520Ragan%2520and%2520Sadh%2520MNM%2520Khan%2520and%2520John%2520Blitzer%2520and%2520Andrew%2520Bunner%2520and%2520Guan%2520Sun%2520and%2520Takahiro%2520Kosakai%2520and%2520Scott%2520Lundberg%2520and%2520Ndidi%2520Elue%2520and%2520Kelvin%2520Guu%2520and%2520SK%2520Park%2520and%2520Jane%2520Park%2520and%2520Arunachalam%2520Narayanaswamy%2520and%2520Chengda%2520Wu%2520and%2520Jayaram%2520Mudigonda%2520and%2520Trevor%2520Cohn%2520and%2520Hairong%2520Mu%2520and%2520Ravi%2520Kumar%2520and%2520Laura%2520Graesser%2520and%2520Yichi%2520Zhang%2520and%2520Richard%2520Killam%2520and%2520Vincent%2520Zhuang%2520and%2520Mai%2520Gim%25C3%25A9nez%2520and%2520Wael%2520Al%2520Jishi%2520and%2520Ruy%2520Ley-Wild%2520and%2520Alex%2520Zhai%2520and%2520Kazuki%2520Osawa%2520and%2520Diego%2520Cedillo%2520and%2520Jialu%2520Liu%2520and%2520Mayank%2520Upadhyay%2520and%2520Marcin%2520Sieniek%2520and%2520Roshan%2520Sharma%2520and%2520Tom%2520Paine%2520and%2520Anelia%2520Angelova%2520and%2520Sravanti%2520Addepalli%2520and%2520Carolina%2520Parada%2520and%2520Kingshuk%2520Majumder%2520and%2520Avery%2520Lamp%2520and%2520Sanjiv%2520Kumar%2520and%2520Xiang%2520Deng%2520and%2520Artiom%2520Myaskovsky%2520and%2520Tea%2520Saboli%25C4%2587%2520and%2520Jeffrey%2520Dudek%2520and%2520Sarah%2520York%2520and%2520F%25C3%25A9lix%2520de%2520Chaumont%2520Quitry%2520and%2520Jiazhong%2520Nie%2520and%2520Dee%2520Cattle%2520and%2520Alok%2520Gunjan%2520and%2520Bilal%2520Piot%2520and%2520Waleed%2520Khawaja%2520and%2520Seojin%2520Bang%2520and%2520Simon%2520Wang%2520and%2520Siavash%2520Khodadadeh%2520and%2520Raghavender%2520R%2520and%2520Praynaa%2520Rawlani%2520and%2520Richard%2520Powell%2520and%2520Kevin%2520Lee%2520and%2520Johannes%2520Griesser%2520and%2520GS%2520Oh%2520and%2520Cesar%2520Magalhaes%2520and%2520Yujia%2520Li%2520and%2520Simon%2520Tokumine%2520and%2520Hadas%2520Natalie%2520Vogel%2520and%2520Dennis%2520Hsu%2520and%2520Arturo%2520BC%2520and%2520Disha%2520Jindal%2520and%2520Matan%2520Cohen%2520and%2520Zi%2520Yang%2520and%2520Junwei%2520Yuan%2520and%2520Dario%2520de%2520Cesare%2520and%2520Tony%2520Bruguier%2520and%2520Jun%2520Xu%2520and%2520Monica%2520Roy%2520and%2520Alon%2520Jacovi%2520and%2520Dan%2520Belov%2520and%2520Rahul%2520Arya%2520and%2520Phoenix%2520Meadowlark%2520and%2520Shlomi%2520Cohen-Ganor%2520and%2520Wenting%2520Ye%2520and%2520Patrick%2520Morris-Suzuki%2520and%2520Praseem%2520Banzal%2520and%2520Gan%2520Song%2520and%2520Pranavaraj%2520Ponnuramu%2520and%2520Fred%2520Zhang%2520and%2520George%2520Scrivener%2520and%2520Salah%2520Zaiem%2520and%2520Alif%2520Raditya%2520Rochman%2520and%2520Kehang%2520Han%2520and%2520Badih%2520Ghazi%2520and%2520Kate%2520Lee%2520and%2520Shahar%2520Drath%2520and%2520Daniel%2520Suo%2520and%2520Antonious%2520Girgis%2520and%2520Pradeep%2520Shenoy%2520and%2520Duy%2520Nguyen%2520and%2520Douglas%2520Eck%2520and%2520Somit%2520Gupta%2520and%2520Le%2520Yan%2520and%2520Joao%2520Carreira%2520and%2520Anmol%2520Gulati%2520and%2520Ruoxin%2520Sang%2520and%2520Daniil%2520Mirylenka%2520and%2520Emma%2520Cooney%2520and%2520Edward%2520Chou%2520and%2520Mingyang%2520Ling%2520and%2520Cindy%2520Fan%2520and%2520Ben%2520Coleman%2520and%2520Guilherme%2520Tubone%2520and%2520Ravin%2520Kumar%2520and%2520Jason%2520Baldridge%2520and%2520Felix%2520Hernandez-Campos%2520and%2520Angeliki%2520Lazaridou%2520and%2520James%2520Besley%2520and%2520Itay%2520Yona%2520and%2520Neslihan%2520Bulut%2520and%2520Quentin%2520Wellens%2520and%2520AJ%2520Pierigiovanni%2520and%2520Jasmine%2520George%2520and%2520Richard%2520Green%2520and%2520Pu%2520Han%2520and%2520Connie%2520Tao%2520and%2520Geoff%2520Clark%2520and%2520Chong%2520You%2520and%2520Abbas%2520Abdolmaleki%2520and%2520Justin%2520Fu%2520and%2520Tongzhou%2520Chen%2520and%2520Ashwin%2520Chaugule%2520and%2520Angad%2520Chandorkar%2520and%2520Altaf%2520Rahman%2520and%2520Will%2520Thompson%2520and%2520Penporn%2520Koanantakool%2520and%2520Mike%2520Bernico%2520and%2520Jie%2520Ren%2520and%2520Andrey%2520Vlasov%2520and%2520Sergei%2520Vassilvitskii%2520and%2520Maciej%2520Kula%2520and%2520Yizhong%2520Liang%2520and%2520Dahun%2520Kim%2520and%2520Yangsibo%2520Huang%2520and%2520Chengxi%2520Ye%2520and%2520Dmitry%2520Lepikhin%2520and%2520Wesley%2520Helmholz%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520the%2520Gemini%25202.X%2520model%2520family%253A%2520Gemini%25202.5%2520Pro%2520and%250AGemini%25202.5%2520Flash%252C%2520as%2520well%2520as%2520our%2520earlier%2520Gemini%25202.0%2520Flash%2520and%2520Flash-Lite%250Amodels.%2520Gemini%25202.5%2520Pro%2520is%2520our%2520most%2520capable%2520model%2520yet%252C%2520achieving%2520SoTA%250Aperformance%2520on%2520frontier%2520coding%2520and%2520reasoning%2520benchmarks.%2520In%2520addition%2520to%2520its%250Aincredible%2520coding%2520and%2520reasoning%2520skills%252C%2520Gemini%25202.5%2520Pro%2520is%2520a%2520thinking%2520model%2520that%250Aexcels%2520at%2520multimodal%2520understanding%2520and%2520it%2520is%2520now%2520able%2520to%2520process%2520up%2520to%25203%2520hours%250Aof%2520video%2520content.%2520Its%2520unique%2520combination%2520of%2520long%2520context%252C%2520multimodal%2520and%250Areasoning%2520capabilities%2520can%2520be%2520combined%2520to%2520unlock%2520new%2520agentic%2520workflows.%2520Gemini%250A2.5%2520Flash%2520provides%2520excellent%2520reasoning%2520abilities%2520at%2520a%2520fraction%2520of%2520the%2520compute%250Aand%2520latency%2520requirements%2520and%2520Gemini%25202.0%2520Flash%2520and%2520Flash-Lite%2520provide%2520high%250Aperformance%2520at%2520low%2520latency%2520and%2520cost.%2520Taken%2520together%252C%2520the%2520Gemini%25202.X%2520model%250Ageneration%2520spans%2520the%2520full%2520Pareto%2520frontier%2520of%2520model%2520capability%2520vs%2520cost%252C%2520allowing%250Ausers%2520to%2520explore%2520the%2520boundaries%2520of%2520what%2520is%2520possible%2520with%2520complex%2520agentic%250Aproblem%2520solving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06261v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%0A%20%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities&entry.906535625=Gheorghe%20Comanici%20and%20Eric%20Bieber%20and%20Mike%20Schaekermann%20and%20Ice%20Pasupat%20and%20Noveen%20Sachdeva%20and%20Inderjit%20Dhillon%20and%20Marcel%20Blistein%20and%20Ori%20Ram%20and%20Dan%20Zhang%20and%20Evan%20Rosen%20and%20Luke%20Marris%20and%20Sam%20Petulla%20and%20Colin%20Gaffney%20and%20Asaf%20Aharoni%20and%20Nathan%20Lintz%20and%20Tiago%20Cardal%20Pais%20and%20Henrik%20Jacobsson%20and%20Idan%20Szpektor%20and%20Nan-Jiang%20Jiang%20and%20Krishna%20Haridasan%20and%20Ahmed%20Omran%20and%20Nikunj%20Saunshi%20and%20Dara%20Bahri%20and%20Gaurav%20Mishra%20and%20Eric%20Chu%20and%20Toby%20Boyd%20and%20Brad%20Hekman%20and%20Aaron%20Parisi%20and%20Chaoyi%20Zhang%20and%20Kornraphop%20Kawintiranon%20and%20Tania%20Bedrax-Weiss%20and%20Oliver%20Wang%20and%20Ya%20Xu%20and%20Ollie%20Purkiss%20and%20Uri%20Mendlovic%20and%20Ila%C3%AF%20Deutel%20and%20Nam%20Nguyen%20and%20Adam%20Langley%20and%20Flip%20Korn%20and%20Lucia%20Rossazza%20and%20Alexandre%20Ram%C3%A9%20and%20Sagar%20Waghmare%20and%20Helen%20Miller%20and%20Nathan%20Byrd%20and%20Ashrith%20Sheshan%20and%20Raia%20Hadsell%20and%20Sangnie%20Bhardwaj%20and%20Pawel%20Janus%20and%20Tero%20Rissa%20and%20Dan%20Horgan%20and%20Alvin%20Abdagic%20and%20Lior%20Belenki%20and%20James%20Allingham%20and%20Anima%20Singh%20and%20Theo%20Guidroz%20and%20Srivatsan%20Srinivasan%20and%20Herman%20Schmit%20and%20Kristen%20Chiafullo%20and%20Andre%20Elisseeff%20and%20Nilpa%20Jha%20and%20Prateek%20Kolhar%20and%20Leonard%20Berrada%20and%20Frank%20Ding%20and%20Xiance%20Si%20and%20Shrestha%20Basu%20Mallick%20and%20Franz%20Och%20and%20Sofia%20Erell%20and%20Eric%20Ni%20and%20Tejasi%20Latkar%20and%20Sherry%20Yang%20and%20Petar%20Sirkovic%20and%20Ziqiang%20Feng%20and%20Robert%20Leland%20and%20Rachel%20Hornung%20and%20Gang%20Wu%20and%20Charles%20Blundell%20and%20Hamidreza%20Alvari%20and%20Po-Sen%20Huang%20and%20Cathy%20Yip%20and%20Sanja%20Deur%20and%20Li%20Liu%20and%20Gabriela%20Surita%20and%20Pablo%20Duque%20and%20Dima%20Damen%20and%20Johnson%20Jia%20and%20Arthur%20Guez%20and%20Markus%20Mircea%20and%20Animesh%20Sinha%20and%20Alberto%20Magni%20and%20Pawe%C5%82%20Stradomski%20and%20Tal%20Marian%20and%20Vlado%20Gali%C4%87%20and%20Wenhu%20Chen%20and%20Hisham%20Husain%20and%20Achintya%20Singhal%20and%20Dominik%20Grewe%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Shuang%20Song%20and%20Lorenzo%20Blanco%20and%20Leland%20Rechis%20and%20Lewis%20Ho%20and%20Rich%20Munoz%20and%20Kelvin%20Zheng%20and%20Jessica%20Hamrick%20and%20Kevin%20Mather%20and%20Hagai%20Taitelbaum%20and%20Eliza%20Rutherford%20and%20Yun%20Lei%20and%20Kuangyuan%20Chen%20and%20Anand%20Shukla%20and%20Erica%20Moreira%20and%20Eric%20Doi%20and%20Berivan%20Isik%20and%20Nir%20Shabat%20and%20Dominika%20Rogozi%C5%84ska%20and%20Kashyap%20Kolipaka%20and%20Jason%20Chang%20and%20Eugen%20Vu%C5%A1ak%20and%20Srinivasan%20Venkatachary%20and%20Shadi%20Noghabi%20and%20Tarun%20Bharti%20and%20Younghoon%20Jun%20and%20Aleksandr%20Zaks%20and%20Simon%20Green%20and%20Jeshwanth%20Challagundla%20and%20William%20Wong%20and%20Muqthar%20Mohammad%20and%20Dean%20Hirsch%20and%20Yong%20Cheng%20and%20Iftekhar%20Naim%20and%20Lev%20Proleev%20and%20Damien%20Vincent%20and%20Aayush%20Singh%20and%20Maxim%20Krikun%20and%20Dilip%20Krishnan%20and%20Zoubin%20Ghahramani%20and%20Aviel%20Atias%20and%20Rajeev%20Aggarwal%20and%20Christo%20Kirov%20and%20Dimitrios%20Vytiniotis%20and%20Christy%20Koh%20and%20Alexandra%20Chronopoulou%20and%20Pawan%20Dogra%20and%20Vlad-Doru%20Ion%20and%20Gladys%20Tyen%20and%20Jason%20Lee%20and%20Felix%20Weissenberger%20and%20Trevor%20Strohman%20and%20Ashwin%20Balakrishna%20and%20Jack%20Rae%20and%20Marko%20Velic%20and%20Raoul%20de%20Liedekerke%20and%20Oded%20Elyada%20and%20Wentao%20Yuan%20and%20Canoee%20Liu%20and%20Lior%20Shani%20and%20Sergey%20Kishchenko%20and%20Bea%20Alessio%20and%20Yandong%20Li%20and%20Richard%20Song%20and%20Sam%20Kwei%20and%20Orion%20Jankowski%20and%20Aneesh%20Pappu%20and%20Youhei%20Namiki%20and%20Yenai%20Ma%20and%20Nilesh%20Tripuraneni%20and%20Colin%20Cherry%20and%20Marissa%20Ikonomidis%20and%20Yu-Cheng%20Ling%20and%20Colin%20Ji%20and%20Beka%20Westberg%20and%20Auriel%20Wright%20and%20Da%20Yu%20and%20David%20Parkinson%20and%20Swaroop%20Ramaswamy%20and%20Jerome%20Connor%20and%20Soheil%20Hassas%20Yeganeh%20and%20Snchit%20Grover%20and%20George%20Kenwright%20and%20Lubo%20Litchev%20and%20Chris%20Apps%20and%20Alex%20Tomala%20and%20Felix%20Halim%20and%20Alex%20Castro-Ros%20and%20Zefei%20Li%20and%20Anudhyan%20Boral%20and%20Pauline%20Sho%20and%20Michal%20Yarom%20and%20Eric%20Malmi%20and%20David%20Klinghoffer%20and%20Rebecca%20Lin%20and%20Alan%20Ansell%20and%20Pradeep%20Kumar%20S%20and%20Shubin%20Zhao%20and%20Siqi%20Zuo%20and%20Adam%20Santoro%20and%20Heng-Tze%20Cheng%20and%20Solomon%20Demmessie%20and%20Yuchi%20Liu%20and%20Nicole%20Brichtova%20and%20Allie%20Culp%20and%20Nathaniel%20Braun%20and%20Dan%20Graur%20and%20Will%20Ng%20and%20Nikhil%20Mehta%20and%20Aaron%20Phillips%20and%20Patrik%20Sundberg%20and%20Varun%20Godbole%20and%20Fangyu%20Liu%20and%20Yash%20Katariya%20and%20David%20Rim%20and%20Mojtaba%20Seyedhosseini%20and%20Sean%20Ammirati%20and%20Jonas%20Valfridsson%20and%20Mahan%20Malihi%20and%20Timothy%20Knight%20and%20Andeep%20Toor%20and%20Thomas%20Lampe%20and%20Abe%20Ittycheriah%20and%20Lewis%20Chiang%20and%20Chak%20Yeung%20and%20Alexandre%20Fr%C3%A9chette%20and%20Jinmeng%20Rao%20and%20Huisheng%20Wang%20and%20Himanshu%20Srivastava%20and%20Richard%20Zhang%20and%20Rocky%20Rhodes%20and%20Ariel%20Brand%20and%20Dean%20Weesner%20and%20Ilya%20Figotin%20and%20Felix%20Gimeno%20and%20Rachana%20Fellinger%20and%20Pierre%20Marcenac%20and%20Jos%C3%A9%20Leal%20and%20Eyal%20Marcus%20and%20Victor%20Cotruta%20and%20Rodrigo%20Cabrera%20and%20Sheryl%20Luo%20and%20Dan%20Garrette%20and%20Vera%20Axelrod%20and%20Sorin%20Baltateanu%20and%20David%20Barker%20and%20Dongkai%20Chen%20and%20Horia%20Toma%20and%20Ben%20Ingram%20and%20Jason%20Riesa%20and%20Chinmay%20Kulkarni%20and%20Yujing%20Zhang%20and%20Hongbin%20Liu%20and%20Chao%20Wang%20and%20Martin%20Polacek%20and%20Will%20Wu%20and%20Kai%20Hui%20and%20Adrian%20N%20Reyes%20and%20Yi%20Su%20and%20Megan%20Barnes%20and%20Ishaan%20Malhi%20and%20Anfal%20Siddiqui%20and%20Qixuan%20Feng%20and%20Mihai%20Damaschin%20and%20Daniele%20Pighin%20and%20Andreas%20Steiner%20and%20Samuel%20Yang%20and%20Ramya%20Sree%20Boppana%20and%20Simeon%20Ivanov%20and%20Arun%20Kandoor%20and%20Aditya%20Shah%20and%20Asier%20Mujika%20and%20Da%20Huang%20and%20Christopher%20A.%20Choquette-Choo%20and%20Mohak%20Patel%20and%20Tianhe%20Yu%20and%20Toni%20Creswell%20and%20%20Jerry%20and%20%20Liu%20and%20Catarina%20Barros%20and%20Yasaman%20Razeghi%20and%20Aurko%20Roy%20and%20Phil%20Culliton%20and%20Binbin%20Xiong%20and%20Jiaqi%20Pan%20and%20Thomas%20Strohmann%20and%20Tolly%20Powell%20and%20Babi%20Seal%20and%20Doug%20DeCarlo%20and%20Pranav%20Shyam%20and%20Kaan%20Katircioglu%20and%20Xuezhi%20Wang%20and%20Cassidy%20Hardin%20and%20Immanuel%20Odisho%20and%20Josef%20Broder%20and%20Oscar%20Chang%20and%20Arun%20Nair%20and%20Artem%20Shtefan%20and%20Maura%20O%27Brien%20and%20Manu%20Agarwal%20and%20Sahitya%20Potluri%20and%20Siddharth%20Goyal%20and%20Amit%20Jhindal%20and%20Saksham%20Thakur%20and%20Yury%20Stuken%20and%20James%20Lyon%20and%20Kristina%20Toutanova%20and%20Fangxiaoyu%20Feng%20and%20Austin%20Wu%20and%20Ben%20Horn%20and%20Alek%20Wang%20and%20Alex%20Cullum%20and%20Gabe%20Taubman%20and%20Disha%20Shrivastava%20and%20Chongyang%20Shi%20and%20Hamish%20Tomlinson%20and%20Roma%20Patel%20and%20Tao%20Tu%20and%20Ada%20Maksutaj%20Oflazer%20and%20Francesco%20Pongetti%20and%20Mingyao%20Yang%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20Vincent%20Perot%20and%20Nuo%20Wang%20Pierse%20and%20Feng%20Han%20and%20Yoel%20Drori%20and%20I%C3%B1aki%20Iturrate%20and%20Ayan%20Chakrabarti%20and%20Legg%20Yeung%20and%20Dave%20Dopson%20and%20Yi-ting%20Chen%20and%20Apoorv%20Kulshreshtha%20and%20Tongfei%20Guo%20and%20Philip%20Pham%20and%20Tal%20Schuster%20and%20Junquan%20Chen%20and%20Alex%20Polozov%20and%20Jinwei%20Xing%20and%20Huanjie%20Zhou%20and%20Praneeth%20Kacham%20and%20Doron%20Kukliansky%20and%20Antoine%20Miech%20and%20Sergey%20Yaroshenko%20and%20Ed%20Chi%20and%20Sholto%20Douglas%20and%20Hongliang%20Fei%20and%20Mathieu%20Blondel%20and%20Preethi%20Myla%20and%20Lior%20Madmoni%20and%20Xing%20Wu%20and%20Daniel%20Keysers%20and%20Kristian%20Kjems%20and%20Isabela%20Albuquerque%20and%20Lijun%20Yu%20and%20Joel%20D%27sa%20and%20Michelle%20Plantan%20and%20Vlad%20Ionescu%20and%20Jaume%20Sanchez%20Elias%20and%20Abhirut%20Gupta%20and%20Manish%20Reddy%20Vuyyuru%20and%20Fred%20Alcober%20and%20Tong%20Zhou%20and%20Kaiyang%20Ji%20and%20Florian%20Hartmann%20and%20Subha%20Puttagunta%20and%20Hugo%20Song%20and%20Ehsan%20Amid%20and%20Anca%20Stefanoiu%20and%20Andrew%20Lee%20and%20Paul%20Pucciarelli%20and%20Emma%20Wang%20and%20Amit%20Raul%20and%20Slav%20Petrov%20and%20Isaac%20Tian%20and%20Valentin%20Anklin%20and%20Nana%20Nti%20and%20Victor%20Gomes%20and%20Max%20Schumacher%20and%20Grace%20Vesom%20and%20Alex%20Panagopoulos%20and%20Konstantinos%20Bousmalis%20and%20Daniel%20Andor%20and%20Josh%20Jacob%20and%20Yuan%20Zhang%20and%20Bill%20Rosgen%20and%20Matija%20Kecman%20and%20Matthew%20Tung%20and%20Alexandra%20Belias%20and%20Noah%20Goodman%20and%20Paul%20Covington%20and%20Brian%20Wieder%20and%20Nikita%20Saxena%20and%20Elnaz%20Davoodi%20and%20Muhuan%20Huang%20and%20Sharath%20Maddineni%20and%20Vincent%20Roulet%20and%20Folawiyo%20Campbell-Ajala%20and%20Pier%20Giuseppe%20Sessa%20and%20%20Xintian%20and%20%20Wu%20and%20Guangda%20Lai%20and%20Paul%20Collins%20and%20Alex%20Haig%20and%20Vytenis%20Sakenas%20and%20Xiaowei%20Xu%20and%20Marissa%20Giustina%20and%20Laurent%20El%20Shafey%20and%20Pichi%20Charoenpanit%20and%20Shefali%20Garg%20and%20Joshua%20Ainslie%20and%20Boone%20Severson%20and%20Montse%20Gonzalez%20Arenas%20and%20Shreya%20Pathak%20and%20Sujee%20Rajayogam%20and%20Jie%20Feng%20and%20Michiel%20Bakker%20and%20Sheng%20Li%20and%20Nevan%20Wichers%20and%20Jamie%20Rogers%20and%20Xinyang%20Geng%20and%20Yeqing%20Li%20and%20Rolf%20Jagerman%20and%20Chao%20Jia%20and%20Nadav%20Olmert%20and%20David%20Sharon%20and%20Matthew%20Mauger%20and%20Sandeep%20Mariserla%20and%20Hongxu%20Ma%20and%20Megha%20Mohabey%20and%20Kyuyeun%20Kim%20and%20Alek%20Andreev%20and%20Scott%20Pollom%20and%20Juliette%20Love%20and%20Vihan%20Jain%20and%20Priyanka%20Agrawal%20and%20Yannick%20Schroecker%20and%20Alisa%20Fortin%20and%20Manfred%20Warmuth%20and%20Ji%20Liu%20and%20Andrew%20Leach%20and%20Irina%20Blok%20and%20Ganesh%20Poomal%20Girirajan%20and%20Roee%20Aharoni%20and%20Benigno%20Uria%20and%20Andrei%20Sozanschi%20and%20Dan%20Goldberg%20and%20Lucian%20Ionita%20and%20Marco%20Tulio%20Ribeiro%20and%20Martin%20Zlocha%20and%20Vighnesh%20Birodkar%20and%20Sami%20Lachgar%20and%20Liangzhe%20Yuan%20and%20Himadri%20Choudhury%20and%20Matt%20Ginsberg%20and%20Fei%20Zheng%20and%20Gregory%20Dibb%20and%20Emily%20Graves%20and%20Swachhand%20Lokhande%20and%20Gabriel%20Rasskin%20and%20George-Cristian%20Muraru%20and%20Corbin%20Quick%20and%20Sandeep%20Tata%20and%20Pierre%20Sermanet%20and%20Aditya%20Chawla%20and%20Itay%20Karo%20and%20Yan%20Wang%20and%20Susan%20Zhang%20and%20Orgad%20Keller%20and%20Anca%20Dragan%20and%20Guolong%20Su%20and%20Ian%20Chou%20and%20Xi%20Liu%20and%20Yiqing%20Tao%20and%20Shruthi%20Prabhakara%20and%20Marc%20Wilson%20and%20Ruibo%20Liu%20and%20Shibo%20Wang%20and%20Georgie%20Evans%20and%20David%20Du%20and%20Alfonso%20Casta%C3%B1o%20and%20Gautam%20Prasad%20and%20Mona%20El%20Mahdy%20and%20Sebastian%20Gerlach%20and%20Machel%20Reid%20and%20Jarrod%20Kahn%20and%20Amir%20Zait%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Thatcher%20Ulrich%20and%20Guanyu%20Wang%20and%20Jan%20Wassenberg%20and%20Efrat%20Farkash%20and%20Kiran%20Yalasangi%20and%20Congchao%20Wang%20and%20Maria%20Bauza%20and%20Simon%20Bucher%20and%20Ting%20Liu%20and%20Jun%20Yan%20and%20Gary%20Leung%20and%20Vikas%20Sindhwani%20and%20Parker%20Barnes%20and%20Avi%20Singh%20and%20Ivan%20Jurin%20and%20Jichuan%20Chang%20and%20Niket%20Kumar%20Bhumihar%20and%20Sivan%20Eiger%20and%20Gui%20Citovsky%20and%20Ben%20Withbroe%20and%20Zhang%20Li%20and%20Siyang%20Xue%20and%20Niccol%C3%B2%20Dal%20Santo%20and%20Georgi%20Stoyanov%20and%20Yves%20Raimond%20and%20Steven%20Zheng%20and%20Yilin%20Gao%20and%20V%C3%ADt%20List%C3%ADk%20and%20S%C5%82awek%20Kwasiborski%20and%20Rachel%20Saputro%20and%20Adnan%20Ozturel%20and%20Ganesh%20Mallya%20and%20Kushal%20Majmundar%20and%20Ross%20West%20and%20Paul%20Caron%20and%20Jinliang%20Wei%20and%20Lluis%20Castrejon%20and%20Sharad%20Vikram%20and%20Deepak%20Ramachandran%20and%20Nikhil%20Dhawan%20and%20Jiho%20Park%20and%20Sara%20Smoot%20and%20George%20van%20den%20Driessche%20and%20Yochai%20Blau%20and%20Chase%20Malik%20and%20Wei%20Liang%20and%20Roy%20Hirsch%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Eugene%20Weinstein%20and%20A%C3%A4ron%20van%20den%20Oord%20and%20Sid%20Lall%20and%20Nicholas%20FitzGerald%20and%20Zixuan%20Jiang%20and%20Xuan%20Yang%20and%20Dale%20Webster%20and%20Ali%20Elqursh%20and%20Aedan%20Pope%20and%20Georges%20Rotival%20and%20David%20Raposo%20and%20Wanzheng%20Zhu%20and%20Jeff%20Dean%20and%20Sami%20Alabed%20and%20Dustin%20Tran%20and%20Arushi%20Gupta%20and%20Zach%20Gleicher%20and%20Jessica%20Austin%20and%20Edouard%20Rosseel%20and%20Megh%20Umekar%20and%20Dipanjan%20Das%20and%20Yinghao%20Sun%20and%20Kai%20Chen%20and%20Karolis%20Misiunas%20and%20Xiang%20Zhou%20and%20Yixian%20Di%20and%20Alyssa%20Loo%20and%20Josh%20Newlan%20and%20Bo%20Li%20and%20Vinay%20Ramasesh%20and%20Ying%20Xu%20and%20Alex%20Chen%20and%20Sudeep%20Gandhe%20and%20Radu%20Soricut%20and%20Nikita%20Gupta%20and%20Shuguang%20Hu%20and%20Seliem%20El-Sayed%20and%20Xavier%20Garcia%20and%20Idan%20Brusilovsky%20and%20Pu-Chin%20Chen%20and%20Andrew%20Bolt%20and%20Lu%20Huang%20and%20Alex%20Gurney%20and%20Zhiying%20Zhang%20and%20Alexander%20Pritzel%20and%20Jarek%20Wilkiewicz%20and%20Bryan%20Seybold%20and%20Bhargav%20Kanagal%20Shamanna%20and%20Felix%20Fischer%20and%20Josef%20Dean%20and%20Karan%20Gill%20and%20Ross%20Mcilroy%20and%20Abhishek%20Bhowmick%20and%20Jeremy%20Selier%20and%20Antoine%20Yang%20and%20Derek%20Cheng%20and%20Vladimir%20Magay%20and%20Jie%20Tan%20and%20Dhriti%20Varma%20and%20Christian%20Walder%20and%20Tomas%20Kocisky%20and%20Ryo%20Nakashima%20and%20Paul%20Natsev%20and%20Mike%20Kwong%20and%20Ionel%20Gog%20and%20Chiyuan%20Zhang%20and%20Sander%20Dieleman%20and%20Thomas%20Jimma%20and%20Andrey%20Ryabtsev%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Dayou%20Du%20and%20Ante%20%C5%BDu%C5%BEul%20and%20Mislav%20%C5%BDani%C4%87%20and%20Mukund%20Raghavachari%20and%20Willi%20Gierke%20and%20Zeyu%20Zheng%20and%20Dessie%20Petrova%20and%20Yann%20Dauphin%20and%20Yuchuan%20Liu%20and%20Ido%20Kessler%20and%20Steven%20Hand%20and%20Chris%20Duvarney%20and%20Seokhwan%20Kim%20and%20Hyo%20Lee%20and%20L%C3%A9onard%20Hussenot%20and%20Jeffrey%20Hui%20and%20Josh%20Smith%20and%20Deepali%20Jain%20and%20Jiawei%20Xia%20and%20Gaurav%20Singh%20Tomar%20and%20Keyvan%20Amiri%20and%20Du%20Phan%20and%20Fabian%20Fuchs%20and%20Tobias%20Weyand%20and%20Nenad%20Tomasev%20and%20Alexandra%20Cordell%20and%20Xin%20Liu%20and%20Jonathan%20Mallinson%20and%20Pankaj%20Joshi%20and%20Andy%20Crawford%20and%20Arun%20Suggala%20and%20Steve%20Chien%20and%20Nick%20Fernando%20and%20Mariella%20Sanchez-Vargas%20and%20Duncan%20Williams%20and%20Phil%20Crone%20and%20Xiyang%20Luo%20and%20Igor%20Karpov%20and%20Jyn%20Shan%20and%20Terry%20Thurk%20and%20Robin%20Strudel%20and%20Paul%20Voigtlaender%20and%20Piyush%20Patil%20and%20Tim%20Dozat%20and%20Ali%20Khodaei%20and%20Sahil%20Singla%20and%20Piotr%20Ambroszczyk%20and%20Qiyin%20Wu%20and%20Yifan%20Chang%20and%20Brian%20Roark%20and%20Chaitra%20Hegde%20and%20Tianli%20Ding%20and%20Angelos%20Filos%20and%20Zhongru%20Wu%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Shuang%20Liu%20and%20Saarthak%20Khanna%20and%20Aditya%20Pandey%20and%20Siobhan%20Mcloughlin%20and%20Qiujia%20Li%20and%20Sam%20Haves%20and%20Allan%20Zhou%20and%20Elena%20Buchatskaya%20and%20Isabel%20Leal%20and%20Peter%20de%20Boursac%20and%20Nami%20Akazawa%20and%20Nina%20Anderson%20and%20Terry%20Chen%20and%20Krishna%20Somandepalli%20and%20Chen%20Liang%20and%20Sheela%20Goenka%20and%20Stephanie%20Winkler%20and%20Alexander%20Grushetsky%20and%20Yifan%20Ding%20and%20Jamie%20Smith%20and%20Fan%20Ye%20and%20Jordi%20Pont-Tuset%20and%20Eric%20Li%20and%20Ruichao%20Li%20and%20Tomer%20Golany%20and%20Dawid%20Wegner%20and%20Tao%20Jiang%20and%20Omer%20Barak%20and%20Yuan%20Shangguan%20and%20Eszter%20V%C3%A9rtes%20and%20Renee%20Wong%20and%20J%C3%B6rg%20Bornschein%20and%20Alex%20Tudor%20and%20Michele%20Bevilacqua%20and%20Tom%20Schaul%20and%20Ankit%20Singh%20Rawat%20and%20Yang%20Zhao%20and%20Kyriakos%20Axiotis%20and%20Lei%20Meng%20and%20Cory%20McLean%20and%20Jonathan%20Lai%20and%20Jennifer%20Beattie%20and%20Nate%20Kushman%20and%20Yaxin%20Liu%20and%20Blair%20Kutzman%20and%20Fiona%20Lang%20and%20Jingchen%20Ye%20and%20Praneeth%20Netrapalli%20and%20Pushkar%20Mishra%20and%20Myriam%20Khan%20and%20Megha%20Goel%20and%20Rob%20Willoughby%20and%20David%20Tian%20and%20Honglei%20Zhuang%20and%20JD%20Chen%20and%20Zak%20Tsai%20and%20Tasos%20Kementsietsidis%20and%20Arjun%20Khare%20and%20James%20Keeling%20and%20Keyang%20Xu%20and%20Nathan%20Waters%20and%20Florent%20Altch%C3%A9%20and%20Ashok%20Popat%20and%20Bhavishya%20Mittal%20and%20David%20Saxton%20and%20Dalia%20El%20Badawy%20and%20Michael%20Mathieu%20and%20Zheng%20Zheng%20and%20Hao%20Zhou%20and%20Nishant%20Ranka%20and%20Richard%20Shin%20and%20Qingnan%20Duan%20and%20Tim%20Salimans%20and%20Ioana%20Mihailescu%20and%20Uri%20Shaham%20and%20Ming-Wei%20Chang%20and%20Yannis%20Assael%20and%20Nishanth%20Dikkala%20and%20Martin%20Izzard%20and%20Vincent%20Cohen-Addad%20and%20Cat%20Graves%20and%20Vlad%20Feinberg%20and%20Grace%20Chung%20and%20DJ%20Strouse%20and%20Danny%20Karmon%20and%20Sahand%20Sharifzadeh%20and%20Zoe%20Ashwood%20and%20Khiem%20Pham%20and%20Jon%20Blanton%20and%20Alex%20Vasiloff%20and%20Jarred%20Barber%20and%20Mark%20Geller%20and%20Aurick%20Zhou%20and%20Fedir%20Zubach%20and%20Tzu-Kuo%20Huang%20and%20Lei%20Zhang%20and%20Himanshu%20Gupta%20and%20Matt%20Young%20and%20Julia%20Proskurnia%20and%20Ronny%20Votel%20and%20Valentin%20Gabeur%20and%20Gabriel%20Barcik%20and%20Aditya%20Tripathi%20and%20Hongkun%20Yu%20and%20Geng%20Yan%20and%20Beer%20Changpinyo%20and%20Filip%20Paveti%C4%87%20and%20Amy%20Coyle%20and%20Yasuhisa%20Fujii%20and%20Jorge%20Gonzalez%20Mendez%20and%20Tianhao%20Zhou%20and%20Harish%20Rajamani%20and%20Blake%20Hechtman%20and%20Eddie%20Cao%20and%20Da-Cheng%20Juan%20and%20Yi-Xuan%20Tan%20and%20Valentin%20Dalibard%20and%20Yilun%20Du%20and%20Natalie%20Clay%20and%20Kaisheng%20Yao%20and%20Wenhao%20Jia%20and%20Dimple%20Vijaykumar%20and%20Yuxiang%20Zhou%20and%20Xinyi%20Bai%20and%20Wei-Chih%20Hung%20and%20Steven%20Pecht%20and%20Georgi%20Todorov%20and%20Nikhil%20Khadke%20and%20Pramod%20Gupta%20and%20Preethi%20Lahoti%20and%20Arnaud%20Autef%20and%20Karthik%20Duddu%20and%20James%20Lee-Thorp%20and%20Alexander%20Bykovsky%20and%20Tautvydas%20Misiunas%20and%20Sebastian%20Flennerhag%20and%20Santhosh%20Thangaraj%20and%20Jed%20McGiffin%20and%20Zack%20Nado%20and%20Markus%20Kunesch%20and%20Andreas%20Noever%20and%20Amir%20Hertz%20and%20Marco%20Liang%20and%20Victor%20Stone%20and%20Evan%20Palmer%20and%20Samira%20Daruki%20and%20Arijit%20Pramanik%20and%20Siim%20P%C3%B5der%20and%20Austin%20Kyker%20and%20Mina%20Khan%20and%20Evgeny%20Sluzhaev%20and%20Marvin%20Ritter%20and%20Avraham%20Ruderman%20and%20Wenlei%20Zhou%20and%20Chirag%20Nagpal%20and%20Kiran%20Vodrahalli%20and%20George%20Necula%20and%20Paul%20Barham%20and%20Ellie%20Pavlick%20and%20Jay%20Hartford%20and%20Izhak%20Shafran%20and%20Long%20Zhao%20and%20Maciej%20Miku%C5%82a%20and%20Tom%20Eccles%20and%20Hidetoshi%20Shimokawa%20and%20Kanav%20Garg%20and%20Luke%20Vilnis%20and%20Hanwen%20Chen%20and%20Ilia%20Shumailov%20and%20Kuang-Huei%20Lee%20and%20Abdelrahman%20Abdelhamed%20and%20Meiyan%20Xie%20and%20Vered%20Cohen%20and%20Ester%20Hlavnova%20and%20Dan%20Malkin%20and%20Chawin%20Sitawarin%20and%20James%20Lottes%20and%20Pauline%20Coquinot%20and%20Tianli%20Yu%20and%20Sandeep%20Kumar%20and%20Jingwei%20Zhang%20and%20Aroma%20Mahendru%20and%20Zafarali%20Ahmed%20and%20James%20Martens%20and%20Tao%20Chen%20and%20Aviel%20Boag%20and%20Daiyi%20Peng%20and%20Coline%20Devin%20and%20Arseniy%20Klimovskiy%20and%20Mary%20Phuong%20and%20Danny%20Vainstein%20and%20Jin%20Xie%20and%20Bhuvana%20Ramabhadran%20and%20Nathan%20Howard%20and%20Xinxin%20Yu%20and%20Gitartha%20Goswami%20and%20Jingyu%20Cui%20and%20Sam%20Shleifer%20and%20Mario%20Pinto%20and%20Chih-Kuan%20Yeh%20and%20Ming-Hsuan%20Yang%20and%20Sara%20Javanmardi%20and%20Dan%20Ethier%20and%20Chace%20Lee%20and%20Jordi%20Orbay%20and%20Suyog%20Kotecha%20and%20Carla%20Bromberg%20and%20Pete%20Shaw%20and%20James%20Thornton%20and%20Adi%20Gerzi%20Rosenthal%20and%20Shane%20Gu%20and%20Matt%20Thomas%20and%20Ian%20Gemp%20and%20Aditya%20Ayyar%20and%20Asahi%20Ushio%20and%20Aarush%20Selvan%20and%20Joel%20Wee%20and%20Chenxi%20Liu%20and%20Maryam%20Majzoubi%20and%20Weiren%20Yu%20and%20Jake%20Abernethy%20and%20Tyler%20Liechty%20and%20Renke%20Pan%20and%20Hoang%20Nguyen%20and%20%20Qiong%20and%20%20Hu%20and%20Sarah%20Perrin%20and%20Abhinav%20Arora%20and%20Emily%20Pitler%20and%20Weiyi%20Wang%20and%20Kaushik%20Shivakumar%20and%20Flavien%20Prost%20and%20Ben%20Limonchik%20and%20Jing%20Wang%20and%20Yi%20Gao%20and%20Timothee%20Cour%20and%20Shyamal%20Buch%20and%20Huan%20Gui%20and%20Maria%20Ivanova%20and%20Philipp%20Neubeck%20and%20Kelvin%20Chan%20and%20Lucy%20Kim%20and%20Huizhong%20Chen%20and%20Naman%20Goyal%20and%20Da-Woon%20Chung%20and%20Lu%20Liu%20and%20Yao%20Su%20and%20Anastasia%20Petrushkina%20and%20Jiajun%20Shen%20and%20Armand%20Joulin%20and%20Yuanzhong%20Xu%20and%20Stein%20Xudong%20Lin%20and%20Yana%20Kulizhskaya%20and%20Ciprian%20Chelba%20and%20Shobha%20Vasudevan%20and%20Eli%20Collins%20and%20Vasilisa%20Bashlovkina%20and%20Tony%20Lu%20and%20Doug%20Fritz%20and%20Jongbin%20Park%20and%20Yanqi%20Zhou%20and%20Chen%20Su%20and%20Richard%20Tanburn%20and%20Mikhail%20Sushkov%20and%20Mitchelle%20Rasquinha%20and%20Jinning%20Li%20and%20Jennifer%20Prendki%20and%20Yiming%20Li%20and%20Pallavi%20LV%20and%20Shriya%20Sharma%20and%20Hen%20Fitoussi%20and%20Hui%20Huang%20and%20Andrew%20Dai%20and%20Phuong%20Dao%20and%20Mike%20Burrows%20and%20Henry%20Prior%20and%20Danfeng%20Qin%20and%20Golan%20Pundak%20and%20Lars%20Lowe%20Sjoesund%20and%20Art%20Khurshudov%20and%20Zhenkai%20Zhu%20and%20Albert%20Webson%20and%20Elizabeth%20Kemp%20and%20Tat%20Tan%20and%20Saurabh%20Agrawal%20and%20Susie%20Sargsyan%20and%20Liqun%20Cheng%20and%20Jim%20Stephan%20and%20Tom%20Kwiatkowski%20and%20David%20Reid%20and%20Arunkumar%20Byravan%20and%20Assaf%20Hurwitz%20Michaely%20and%20Nicolas%20Heess%20and%20Luowei%20Zhou%20and%20Sonam%20Goenka%20and%20Viral%20Carpenter%20and%20Anselm%20Levskaya%20and%20Bo%20Wang%20and%20Reed%20Roberts%20and%20R%C3%A9mi%20Leblond%20and%20Sharat%20Chikkerur%20and%20Stav%20Ginzburg%20and%20Max%20Chang%20and%20Robert%20Riachi%20and%20%20Chuqiao%20and%20%20Xu%20and%20Zal%C3%A1n%20Borsos%20and%20Michael%20Pliskin%20and%20Julia%20Pawar%20and%20Morgane%20Lustman%20and%20Hannah%20Kirkwood%20and%20Ankit%20Anand%20and%20Aditi%20Chaudhary%20and%20Norbert%20Kalb%20and%20Kieran%20Milan%20and%20Sean%20Augenstein%20and%20Anna%20Goldie%20and%20Laurel%20Prince%20and%20Karthik%20Raman%20and%20Yanhua%20Sun%20and%20Vivian%20Xia%20and%20Aaron%20Cohen%20and%20Zhouyuan%20Huo%20and%20Josh%20Camp%20and%20Seher%20Ellis%20and%20Lukas%20Zilka%20and%20David%20Vilar%20Torres%20and%20Lisa%20Patel%20and%20Sho%20Arora%20and%20Betty%20Chan%20and%20Jonas%20Adler%20and%20Kareem%20Ayoub%20and%20Jacky%20Liang%20and%20Fayaz%20Jamil%20and%20Jiepu%20Jiang%20and%20Simon%20Baumgartner%20and%20Haitian%20Sun%20and%20Yael%20Karov%20and%20Yaroslav%20Akulov%20and%20Hui%20Zheng%20and%20Irene%20Cai%20and%20Claudio%20Fantacci%20and%20James%20Rubin%20and%20Alex%20Rav%20Acha%20and%20Mengchao%20Wang%20and%20Nina%20D%27Souza%20and%20Rohit%20Sathyanarayana%20and%20Shengyang%20Dai%20and%20Simon%20Rowe%20and%20Andrey%20Simanovsky%20and%20Omer%20Goldman%20and%20Yuheng%20Kuang%20and%20Xiaoyue%20Pan%20and%20Andrew%20Rosenberg%20and%20Tania%20Rojas-Esponda%20and%20Praneet%20Dutta%20and%20Amy%20Zeng%20and%20Irina%20Jurenka%20and%20Greg%20Farquhar%20and%20Yamini%20Bansal%20and%20Shariq%20Iqbal%20and%20Becca%20Roelofs%20and%20Ga-Young%20Joung%20and%20Parker%20Beak%20and%20Changwan%20Ryu%20and%20Ryan%20Poplin%20and%20Yan%20Wu%20and%20Jean-Baptiste%20Alayrac%20and%20Senaka%20Buthpitiya%20and%20Olaf%20Ronneberger%20and%20Caleb%20Habtegebriel%20and%20Wei%20Li%20and%20Paul%20Cavallaro%20and%20Aurora%20Wei%20and%20Guy%20Bensky%20and%20Timo%20Denk%20and%20Harish%20Ganapathy%20and%20Jeff%20Stanway%20and%20Pratik%20Joshi%20and%20Francesco%20Bertolini%20and%20Jessica%20Lo%20and%20Olivia%20Ma%20and%20Zachary%20Charles%20and%20Geta%20Sampemane%20and%20Himanshu%20Sahni%20and%20Xu%20Chen%20and%20Harry%20Askham%20and%20David%20Gaddy%20and%20Peter%20Young%20and%20Jiewen%20Tan%20and%20Matan%20Eyal%20and%20Arthur%20Bra%C5%BEinskas%20and%20Li%20Zhong%20and%20Zhichun%20Wu%20and%20Mark%20Epstein%20and%20Kai%20Bailey%20and%20Andrew%20Hard%20and%20Kamyu%20Lee%20and%20Sasha%20Goldshtein%20and%20Alex%20Ruiz%20and%20Mohammed%20Badawi%20and%20Matthias%20Lochbrunner%20and%20JK%20Kearns%20and%20Ashley%20Brown%20and%20Fabio%20Pardo%20and%20Theophane%20Weber%20and%20Haichuan%20Yang%20and%20Pan-Pan%20Jiang%20and%20Berkin%20Akin%20and%20Zhao%20Fu%20and%20Marcus%20Wainwright%20and%20Chi%20Zou%20and%20Meenu%20Gaba%20and%20Pierre-Antoine%20Manzagol%20and%20Wendy%20Kan%20and%20Yang%20Song%20and%20Karina%20Zainullina%20and%20Rui%20Lin%20and%20Jeongwoo%20Ko%20and%20Salil%20Deshmukh%20and%20Apoorv%20Jindal%20and%20James%20Svensson%20and%20Divya%20Tyam%20and%20Heri%20Zhao%20and%20Christine%20Kaeser-Chen%20and%20Scott%20Baird%20and%20Pooya%20Moradi%20and%20Jamie%20Hall%20and%20Qiuchen%20Guo%20and%20Vincent%20Tsang%20and%20Bowen%20Liang%20and%20Fernando%20Pereira%20and%20Suhas%20Ganesh%20and%20Ivan%20Korotkov%20and%20Jakub%20Adamek%20and%20Sridhar%20Thiagarajan%20and%20Vinh%20Tran%20and%20Charles%20Chen%20and%20Chris%20Tar%20and%20Sanil%20Jain%20and%20Ishita%20Dasgupta%20and%20Taylan%20Bilal%20and%20David%20Reitter%20and%20Kai%20Zhao%20and%20Giulia%20Vezzani%20and%20Yasmin%20Gehman%20and%20Pulkit%20Mehta%20and%20Lauren%20Beltrone%20and%20Xerxes%20Dotiwalla%20and%20Sergio%20Guadarrama%20and%20Zaheer%20Abbas%20and%20Stefani%20Karp%20and%20Petko%20Georgiev%20and%20Chun-Sung%20Ferng%20and%20Marc%20Brockschmidt%20and%20Liqian%20Peng%20and%20Christoph%20Hirnschall%20and%20Vikas%20Verma%20and%20Yingying%20Bi%20and%20Ying%20Xiao%20and%20Avigail%20Dabush%20and%20Kelvin%20Xu%20and%20Phil%20Wallis%20and%20Randall%20Parker%20and%20Qifei%20Wang%20and%20Yang%20Xu%20and%20Ilkin%20Safarli%20and%20Dinesh%20Tewari%20and%20Yin%20Zhang%20and%20Seungyeon%20Kim%20and%20Andrea%20Gesmundo%20and%20Mackenzie%20Thomas%20and%20Sergey%20Levi%20and%20Ahmed%20Chowdhury%20and%20Kanishka%20Rao%20and%20Peter%20Garst%20and%20Sam%20Conway-Rahman%20and%20Helen%20Ran%20and%20Kay%20McKinney%20and%20Zhisheng%20Xiao%20and%20Wenhao%20Yu%20and%20Rohan%20Agrawal%20and%20Axel%20Stjerngren%20and%20Catalin%20Ionescu%20and%20Jingjing%20Chen%20and%20Vivek%20Sharma%20and%20Justin%20Chiu%20and%20Fei%20Liu%20and%20Ken%20Franko%20and%20Clayton%20Sanford%20and%20Xingyu%20Cai%20and%20Paul%20Michel%20and%20Sanjay%20Ganapathy%20and%20Jane%20Labanowski%20and%20Zachary%20Garrett%20and%20Ben%20Vargas%20and%20Sean%20Sun%20and%20Bryan%20Gale%20and%20Thomas%20Buschmann%20and%20Guillaume%20Desjardins%20and%20Nimesh%20Ghelani%20and%20Palak%20Jain%20and%20Mudit%20Verma%20and%20Chulayuth%20Asawaroengchai%20and%20Julian%20Eisenschlos%20and%20Jitendra%20Harlalka%20and%20Hideto%20Kazawa%20and%20Don%20Metzler%20and%20Joshua%20Howland%20and%20Ying%20Jian%20and%20Jake%20Ades%20and%20Viral%20Shah%20and%20Tynan%20Gangwani%20and%20Seungji%20Lee%20and%20Roman%20Ring%20and%20Steven%20M.%20Hernandez%20and%20Dean%20Reich%20and%20Amer%20Sinha%20and%20Ashutosh%20Sathe%20and%20Joe%20Kovac%20and%20Ashleah%20Gill%20and%20Ajay%20Kannan%20and%20Andrea%20D%27olimpio%20and%20Martin%20Sevenich%20and%20Jay%20Whang%20and%20Been%20Kim%20and%20Khe%20Chai%20Sim%20and%20Jilin%20Chen%20and%20Jiageng%20Zhang%20and%20Shuba%20Lall%20and%20Yossi%20Matias%20and%20Bill%20Jia%20and%20Abe%20Friesen%20and%20Sara%20Nasso%20and%20Ashish%20Thapliyal%20and%20Bryan%20Perozzi%20and%20Ting%20Yu%20and%20Anna%20Shekhawat%20and%20Safeen%20Huda%20and%20Peter%20Grabowski%20and%20Eric%20Wang%20and%20Ashwin%20Sreevatsa%20and%20Hilal%20Dib%20and%20Mehadi%20Hassen%20and%20Parker%20Schuh%20and%20Vedrana%20Milutinovic%20and%20Chris%20Welty%20and%20Michael%20Quinn%20and%20Ali%20Shah%20and%20Bangju%20Wang%20and%20Gabe%20Barth-Maron%20and%20Justin%20Frye%20and%20Natalie%20Axelsson%20and%20Tao%20Zhu%20and%20Yukun%20Ma%20and%20Irene%20Giannoumis%20and%20Hanie%20Sedghi%20and%20Chang%20Ye%20and%20Yi%20Luan%20and%20Kevin%20Aydin%20and%20Bilva%20Chandra%20and%20Vivek%20Sampathkumar%20and%20Ronny%20Huang%20and%20Victor%20Lavrenko%20and%20Ahmed%20Eleryan%20and%20Zhi%20Hong%20and%20Steven%20Hansen%20and%20Sara%20Mc%20Carthy%20and%20Bidisha%20Samanta%20and%20Domagoj%20%C4%86evid%20and%20Xin%20Wang%20and%20Fangtao%20Li%20and%20Michael%20Voznesensky%20and%20Matt%20Hoffman%20and%20Andreas%20Terzis%20and%20Vikash%20Sehwag%20and%20Gil%20Fidel%20and%20Luheng%20He%20and%20Mu%20Cai%20and%20Yanzhang%20He%20and%20Alex%20Feng%20and%20Martin%20Nikoltchev%20and%20Samrat%20Phatale%20and%20Jason%20Chase%20and%20Rory%20Lawton%20and%20Ming%20Zhang%20and%20Tom%20Ouyang%20and%20Manuel%20Tragut%20and%20Mehdi%20Hafezi%20Manshadi%20and%20Arjun%20Narayanan%20and%20Jiaming%20Shen%20and%20Xu%20Gao%20and%20Tolga%20Bolukbasi%20and%20Nick%20Roy%20and%20Xin%20Li%20and%20Daniel%20Golovin%20and%20Liviu%20Panait%20and%20Zhen%20Qin%20and%20Guangxing%20Han%20and%20Thomas%20Anthony%20and%20Sneha%20Kudugunta%20and%20Viorica%20Patraucean%20and%20Aniket%20Ray%20and%20Xinyun%20Chen%20and%20Xiaochen%20Yang%20and%20Tanuj%20Bhatia%20and%20Pranav%20Talluri%20and%20Alex%20Morris%20and%20Andrija%20Ra%C5%BEnatovi%C4%87%20and%20Bethanie%20Brownfield%20and%20James%20An%20and%20Sheng%20Peng%20and%20Patrick%20Kane%20and%20Ce%20Zheng%20and%20Nico%20Duduta%20and%20Joshua%20Kessinger%20and%20James%20Noraky%20and%20Siqi%20Liu%20and%20Keran%20Rong%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Keith%20Rush%20and%20Alex%20Goldin%20and%20Fanny%20Wei%20and%20Shiva%20Mohan%20Reddy%20Garlapati%20and%20Caroline%20Pantofaru%20and%20Okwan%20Kwon%20and%20Jianmo%20Ni%20and%20Eric%20Noland%20and%20Julia%20Di%20Trapani%20and%20Fran%C3%A7oise%20Beaufays%20and%20Abhijit%20Guha%20Roy%20and%20Yinlam%20Chow%20and%20Aybuke%20Turker%20and%20Geoffrey%20Cideron%20and%20Lantao%20Mei%20and%20Jon%20Clark%20and%20Qingyun%20Dou%20and%20Matko%20Bo%C5%A1njak%20and%20Ralph%20Leith%20and%20Yuqing%20Du%20and%20Amir%20Yazdanbakhsh%20and%20Milad%20Nasr%20and%20Chester%20Kwak%20and%20Suraj%20Satishkumar%20Sheth%20and%20Alex%20Kaskasoli%20and%20Ankesh%20Anand%20and%20Balaji%20Lakshminarayanan%20and%20Sammy%20Jerome%20and%20David%20Bieber%20and%20Chun-Te%20Chu%20and%20Alexandre%20Senges%20and%20Tianxiao%20Shen%20and%20Mukund%20Sridhar%20and%20Ndaba%20Ndebele%20and%20Benjamin%20Beyret%20and%20Shakir%20Mohamed%20and%20Mia%20Chen%20and%20Markus%20Freitag%20and%20Jiaxian%20Guo%20and%20Luyang%20Liu%20and%20Paul%20Roit%20and%20Heng%20Chen%20and%20Shen%20Yan%20and%20Tom%20Stone%20and%20JD%20Co-Reyes%20and%20Jeremy%20Cole%20and%20Salvatore%20Scellato%20and%20Shekoofeh%20Azizi%20and%20Hadi%20Hashemi%20and%20Alicia%20Jin%20and%20Anand%20Iyer%20and%20Marcella%20Valentine%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Arun%20Ahuja%20and%20Daniel%20Hernandez%20Diaz%20and%20Chen-Yu%20Lee%20and%20Nathan%20Clement%20and%20Weize%20Kong%20and%20Drew%20Garmon%20and%20Ishaan%20Watts%20and%20Kush%20Bhatia%20and%20Khyatti%20Gupta%20and%20Matt%20Miecnikowski%20and%20Hugo%20Vallet%20and%20Ankur%20Taly%20and%20Edward%20Loper%20and%20Saket%20Joshi%20and%20James%20Atwood%20and%20Jo%20Chick%20and%20Mark%20Collier%20and%20Fotis%20Iliopoulos%20and%20Ryan%20Trostle%20and%20Beliz%20Gunel%20and%20Ramiro%20Leal-Cavazos%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Michael%20Guzman%20and%20Xiaoen%20Ju%20and%20Andy%20Forbes%20and%20Jesse%20Emond%20and%20Kushal%20Chauhan%20and%20Ben%20Caine%20and%20Li%20Xiao%20and%20Wenjun%20Zeng%20and%20Alexandre%20Moufarek%20and%20Daniel%20Murphy%20and%20Maya%20Meng%20and%20Nitish%20Gupta%20and%20Felix%20Riedel%20and%20Anil%20Das%20and%20Elijah%20Lawal%20and%20Shashi%20Narayan%20and%20Tiberiu%20Sosea%20and%20James%20Swirhun%20and%20Linda%20Friso%20and%20Behnam%20Neyshabur%20and%20Jing%20Lu%20and%20Sertan%20Girgin%20and%20Michael%20Wunder%20and%20Edouard%20Yvinec%20and%20Aroonalok%20Pyne%20and%20Victor%20Carbune%20and%20Shruti%20Rijhwani%20and%20Yang%20Guo%20and%20Tulsee%20Doshi%20and%20Anton%20Briukhov%20and%20Max%20Bain%20and%20Ayal%20Hitron%20and%20Xuanhui%20Wang%20and%20Ashish%20Gupta%20and%20Ke%20Chen%20and%20Cosmo%20Du%20and%20Weiyang%20Zhang%20and%20Dhruv%20Shah%20and%20Arjun%20Akula%20and%20Max%20Dylla%20and%20Ashyana%20Kachra%20and%20Weicheng%20Kuo%20and%20Tingting%20Zou%20and%20Lily%20Wang%20and%20Luyao%20Xu%20and%20Jifan%20Zhu%20and%20Justin%20Snyder%20and%20Sachit%20Menon%20and%20Orhan%20Firat%20and%20Igor%20Mordatch%20and%20Yuan%20Yuan%20and%20Natalia%20Ponomareva%20and%20Rory%20Blevins%20and%20Lawrence%20Moore%20and%20Weijun%20Wang%20and%20Phil%20Chen%20and%20Martin%20Scholz%20and%20Artur%20Dwornik%20and%20Jason%20Lin%20and%20Sicheng%20Li%20and%20Diego%20Antognini%20and%20Te%20I%20and%20Xiaodan%20Song%20and%20Matt%20Miller%20and%20Uday%20Kalra%20and%20Adam%20Raveret%20and%20Oscar%20Akerlund%20and%20Felix%20Wu%20and%20Andrew%20Nystrom%20and%20Namrata%20Godbole%20and%20Tianqi%20Liu%20and%20Hannah%20DeBalsi%20and%20Jewel%20Zhao%20and%20Buhuang%20Liu%20and%20Avi%20Caciularu%20and%20Lauren%20Lax%20and%20Urvashi%20Khandelwal%20and%20Victoria%20Langston%20and%20Eric%20Bailey%20and%20Silvio%20Lattanzi%20and%20Yufei%20Wang%20and%20Neel%20Kovelamudi%20and%20Sneha%20Mondal%20and%20Guru%20Guruganesh%20and%20Nan%20Hua%20and%20Ofir%20Roval%20and%20Pawe%C5%82%20Weso%C5%82owski%20and%20Rishikesh%20Ingale%20and%20Jonathan%20Halcrow%20and%20Tim%20Sohn%20and%20Christof%20Angermueller%20and%20Bahram%20Raad%20and%20Eli%20Stickgold%20and%20Eva%20Lu%20and%20Alec%20Kosik%20and%20Jing%20Xie%20and%20Timothy%20Lillicrap%20and%20Austin%20Huang%20and%20Lydia%20Lihui%20Zhang%20and%20Dominik%20Paulus%20and%20Clement%20Farabet%20and%20Alex%20Wertheim%20and%20Bing%20Wang%20and%20Rishabh%20Joshi%20and%20Chu-ling%20Ko%20and%20Yonghui%20Wu%20and%20Shubham%20Agrawal%20and%20Lily%20Lin%20and%20XiangHai%20Sheng%20and%20Peter%20Sung%20and%20Tyler%20Breland-King%20and%20Christina%20Butterfield%20and%20Swapnil%20Gawde%20and%20Sumeet%20Singh%20and%20Qiao%20Zhang%20and%20Raj%20Apte%20and%20Shilpa%20Shetty%20and%20Adrian%20Hutter%20and%20Tao%20Li%20and%20Elizabeth%20Salesky%20and%20Federico%20Lebron%20and%20Jonni%20Kanerva%20and%20Michela%20Paganini%20and%20Arthur%20Nguyen%20and%20Rohith%20Vallu%20and%20Jan-Thorsten%20Peter%20and%20Sarmishta%20Velury%20and%20David%20Kao%20and%20Jay%20Hoover%20and%20Anna%20Bortsova%20and%20Colton%20Bishop%20and%20Shoshana%20Jakobovits%20and%20Alessandro%20Agostini%20and%20Alekh%20Agarwal%20and%20Chang%20Liu%20and%20Charles%20Kwong%20and%20Sasan%20Tavakkol%20and%20Ioana%20Bica%20and%20Alex%20Greve%20and%20Anirudh%20GP%20and%20Jake%20Marcus%20and%20Le%20Hou%20and%20Tom%20Duerig%20and%20Rivka%20Moroshko%20and%20Dave%20Lacey%20and%20Andy%20Davis%20and%20Julien%20Amelot%20and%20Guohui%20Wang%20and%20Frank%20Kim%20and%20Theofilos%20Strinopoulos%20and%20Hui%20Wan%20and%20Charline%20Le%20Lan%20and%20Shankar%20Krishnan%20and%20Haotian%20Tang%20and%20Peter%20Humphreys%20and%20Junwen%20Bai%20and%20Idan%20Heimlich%20Shtacher%20and%20Diego%20Machado%20and%20Chenxi%20Pang%20and%20Ken%20Burke%20and%20Dangyi%20Liu%20and%20Renga%20Aravamudhan%20and%20Yue%20Song%20and%20Ed%20Hirst%20and%20Abhimanyu%20Singh%20and%20Brendan%20Jou%20and%20Liang%20Bai%20and%20Francesco%20Piccinno%20and%20Chuyuan%20Kelly%20Fu%20and%20Robin%20Alazard%20and%20Barak%20Meiri%20and%20Daniel%20Winter%20and%20Charlie%20Chen%20and%20Mingda%20Zhang%20and%20Jens%20Heitkaemper%20and%20John%20Lambert%20and%20Jinhyuk%20Lee%20and%20Alexander%20Fr%C3%B6mmgen%20and%20Sergey%20Rogulenko%20and%20Pranav%20Nair%20and%20Paul%20Niemczyk%20and%20Anton%20Bulyenov%20and%20Bibo%20Xu%20and%20Hadar%20Shemtov%20and%20Morteza%20Zadimoghaddam%20and%20Serge%20Toropov%20and%20Mateo%20Wirth%20and%20Hanjun%20Dai%20and%20Sreenivas%20Gollapudi%20and%20Daniel%20Zheng%20and%20Alex%20Kurakin%20and%20Chansoo%20Lee%20and%20Kalesha%20Bullard%20and%20Nicolas%20Serrano%20and%20Ivana%20Balazevic%20and%20Yang%20Li%20and%20Johan%20Schalkwyk%20and%20Mark%20Murphy%20and%20Mingyang%20Zhang%20and%20Kevin%20Sequeira%20and%20Romina%20Datta%20and%20Nishant%20Agrawal%20and%20Charles%20Sutton%20and%20Nithya%20Attaluri%20and%20Mencher%20Chiang%20and%20Wael%20Farhan%20and%20Gregory%20Thornton%20and%20Kate%20Lin%20and%20Travis%20Choma%20and%20Hung%20Nguyen%20and%20Kingshuk%20Dasgupta%20and%20Dirk%20Robinson%20and%20Iulia%20Com%C5%9Fa%20and%20Michael%20Riley%20and%20Arjun%20Pillai%20and%20Basil%20Mustafa%20and%20Ben%20Golan%20and%20Amir%20Zandieh%20and%20Jean-Baptiste%20Lespiau%20and%20Billy%20Porter%20and%20David%20Ross%20and%20Sujeevan%20Rajayogam%20and%20Mohit%20Agarwal%20and%20Subhashini%20Venugopalan%20and%20Bobak%20Shahriari%20and%20Qiqi%20Yan%20and%20Hao%20Xu%20and%20Taylor%20Tobin%20and%20Pavel%20Dubov%20and%20Hongzhi%20Shi%20and%20Adri%C3%A0%20Recasens%20and%20Anton%20Kovsharov%20and%20Sebastian%20Borgeaud%20and%20Lucio%20Dery%20and%20Shanthal%20Vasanth%20and%20Elena%20Gribovskaya%20and%20Linhai%20Qiu%20and%20Mahdis%20Mahdieh%20and%20Wojtek%20Skut%20and%20Elizabeth%20Nielsen%20and%20CJ%20Zheng%20and%20Adams%20Yu%20and%20Carrie%20Grimes%20Bostock%20and%20Shaleen%20Gupta%20and%20Aaron%20Archer%20and%20Chris%20Rawles%20and%20Elinor%20Davies%20and%20Alexey%20Svyatkovskiy%20and%20Tomy%20Tsai%20and%20Yoni%20Halpern%20and%20Christian%20Reisswig%20and%20Bartek%20Wydrowski%20and%20Bo%20Chang%20and%20Joan%20Puigcerver%20and%20Mor%20Hazan%20Taege%20and%20Jian%20Li%20and%20Eva%20Schnider%20and%20Xinjian%20Li%20and%20Dragos%20Dena%20and%20Yunhan%20Xu%20and%20Umesh%20Telang%20and%20Tianze%20Shi%20and%20Heiga%20Zen%20and%20Kyle%20Kastner%20and%20Yeongil%20Ko%20and%20Neesha%20Subramaniam%20and%20Aviral%20Kumar%20and%20Pete%20Blois%20and%20Zhuyun%20Dai%20and%20John%20Wieting%20and%20Yifeng%20Lu%20and%20Yoel%20Zeldes%20and%20Tian%20Xie%20and%20Anja%20Hauth%20and%20Alexandru%20%C5%A2ifrea%20and%20Yuqi%20Li%20and%20Sam%20El-Husseini%20and%20Dan%20Abolafia%20and%20Howard%20Zhou%20and%20Wen%20Ding%20and%20Sahra%20Ghalebikesabi%20and%20Carlos%20Gu%C3%ADa%20and%20Andrii%20Maksai%20and%20%C3%81goston%20Weisz%20and%20Sercan%20Arik%20and%20Nick%20Sukhanov%20and%20Aga%20%C5%9Awietlik%20and%20Xuhui%20Jia%20and%20Luo%20Yu%20and%20Weiyue%20Wang%20and%20Mark%20Brand%20and%20Dawn%20Bloxwich%20and%20Sean%20Kirmani%20and%20Zhe%20Chen%20and%20Alec%20Go%20and%20Pablo%20Sprechmann%20and%20Nithish%20Kannen%20and%20Alen%20Carin%20and%20Paramjit%20Sandhu%20and%20Isabel%20Edkins%20and%20Leslie%20Nooteboom%20and%20Jai%20Gupta%20and%20Loren%20Maggiore%20and%20Javad%20Azizi%20and%20Yael%20Pritch%20and%20Pengcheng%20Yin%20and%20Mansi%20Gupta%20and%20Danny%20Tarlow%20and%20Duncan%20Smith%20and%20Desi%20Ivanov%20and%20Mohammad%20Babaeizadeh%20and%20Ankita%20Goel%20and%20Satish%20Kambala%20and%20Grace%20Chu%20and%20Matej%20Kastelic%20and%20Michelle%20Liu%20and%20Hagen%20Soltau%20and%20Austin%20Stone%20and%20Shivani%20Agrawal%20and%20Min%20Kim%20and%20Kedar%20Soparkar%20and%20Srinivas%20Tadepalli%20and%20Oskar%20Bunyan%20and%20Rachel%20Soh%20and%20Arvind%20Kannan%20and%20DY%20Kim%20and%20Blake%20JianHang%20Chen%20and%20Afief%20Halumi%20and%20Sudeshna%20Roy%20and%20Yulong%20Wang%20and%20Olcan%20Sercinoglu%20and%20Gena%20Gibson%20and%20Sijal%20Bhatnagar%20and%20Motoki%20Sano%20and%20Daniel%20von%20Dincklage%20and%20Qingchun%20Ren%20and%20Blagoj%20Mitrevski%20and%20Mirek%20Ol%C5%A1%C3%A1k%20and%20Jennifer%20She%20and%20Carl%20Doersch%20and%20%20Jilei%20and%20%20Wang%20and%20Bingyuan%20Liu%20and%20Qijun%20Tan%20and%20Tamar%20Yakar%20and%20Tris%20Warkentin%20and%20Alex%20Ramirez%20and%20Carl%20Lebsack%20and%20Josh%20Dillon%20and%20Rajiv%20Mathews%20and%20Tom%20Cobley%20and%20Zelin%20Wu%20and%20Zhuoyuan%20Chen%20and%20Jon%20Simon%20and%20Swaroop%20Nath%20and%20Tara%20Sainath%20and%20Alexei%20Bendebury%20and%20Ryan%20Julian%20and%20Bharath%20Mankalale%20and%20Daria%20%C4%86urko%20and%20Paulo%20Zacchello%20and%20Adam%20R.%20Brown%20and%20Kiranbir%20Sodhia%20and%20Heidi%20Howard%20and%20Sergi%20Caelles%20and%20Abhinav%20Gupta%20and%20Gareth%20Evans%20and%20Anna%20Bulanova%20and%20Lesley%20Katzen%20and%20Roman%20Goldenberg%20and%20Anton%20Tsitsulin%20and%20Joe%20Stanton%20and%20Benoit%20Schillings%20and%20Vitaly%20Kovalev%20and%20Corey%20Fry%20and%20Rushin%20Shah%20and%20Kuo%20Lin%20and%20Shyam%20Upadhyay%20and%20Cheng%20Li%20and%20Soroush%20Radpour%20and%20Marcello%20Maggioni%20and%20Jing%20Xiong%20and%20Lukas%20Haas%20and%20Jenny%20Brennan%20and%20Aishwarya%20Kamath%20and%20Nikolay%20Savinov%20and%20Arsha%20Nagrani%20and%20Trevor%20Yacovone%20and%20Ryan%20Kappedal%20and%20Kostas%20Andriopoulos%20and%20Li%20Lao%20and%20YaGuang%20Li%20and%20Grigory%20Rozhdestvenskiy%20and%20Kazuma%20Hashimoto%20and%20Andrew%20Audibert%20and%20Sophia%20Austin%20and%20Daniel%20Rodriguez%20and%20Anian%20Ruoss%20and%20Garrett%20Honke%20and%20Deep%20Karkhanis%20and%20Xi%20Xiong%20and%20Qing%20Wei%20and%20James%20Huang%20and%20Zhaoqi%20Leng%20and%20Vittal%20Premachandran%20and%20Stan%20Bileschi%20and%20Georgios%20Evangelopoulos%20and%20Thomas%20Mensink%20and%20Jay%20Pavagadhi%20and%20Denis%20Teplyashin%20and%20Paul%20Chang%20and%20Linting%20Xue%20and%20Garrett%20Tanzer%20and%20Sally%20Goldman%20and%20Kaushal%20Patel%20and%20Shixin%20Li%20and%20Jeremy%20Wiesner%20and%20Ivy%20Zheng%20and%20Ian%20Stewart-Binks%20and%20Jie%20Han%20and%20Zhi%20Li%20and%20Liangchen%20Luo%20and%20Karel%20Lenc%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Fuzhao%20Xue%20and%20Ryan%20Mullins%20and%20Alexey%20Guseynov%20and%20Chung-Ching%20Chang%20and%20Isaac%20Galatzer-Levy%20and%20Adam%20Zhang%20and%20Garrett%20Bingham%20and%20Grace%20Hu%20and%20Ale%20Hartman%20and%20Yue%20Ma%20and%20Jordan%20Griffith%20and%20Alex%20Irpan%20and%20Carey%20Radebaugh%20and%20Summer%20Yue%20and%20Lijie%20Fan%20and%20Victor%20Ungureanu%20and%20Christina%20Sorokin%20and%20Hannah%20Teufel%20and%20Peiran%20Li%20and%20Rohan%20Anil%20and%20Dimitris%20Paparas%20and%20Todd%20Wang%20and%20Chu-Cheng%20Lin%20and%20Hui%20Peng%20and%20Megan%20Shum%20and%20Goran%20Petrovic%20and%20Demetra%20Brady%20and%20Richard%20Nguyen%20and%20Klaus%20Macherey%20and%20Zhihao%20Li%20and%20Harman%20Singh%20and%20Madhavi%20Yenugula%20and%20Mariko%20Iinuma%20and%20Xinyi%20Chen%20and%20Kavya%20Kopparapu%20and%20Alexey%20Stern%20and%20Shachi%20Dave%20and%20Chandu%20Thekkath%20and%20Florence%20Perot%20and%20Anurag%20Kumar%20and%20Fangda%20Li%20and%20Yang%20Xiao%20and%20Matthew%20Bilotti%20and%20Mohammad%20Hossein%20Bateni%20and%20Isaac%20Noble%20and%20Lisa%20Lee%20and%20Amelio%20V%C3%A1zquez-Reina%20and%20Julian%20Salazar%20and%20Xiaomeng%20Yang%20and%20Boyu%20Wang%20and%20Ela%20Gruzewska%20and%20Anand%20Rao%20and%20Sindhu%20Raghuram%20and%20Zheng%20Xu%20and%20Eyal%20Ben-David%20and%20Jieru%20Mei%20and%20Sid%20Dalmia%20and%20Zhaoyi%20Zhang%20and%20Yuchen%20Liu%20and%20Gagan%20Bansal%20and%20Helena%20Pankov%20and%20Steven%20Schwarcz%20and%20Andrea%20Burns%20and%20Christine%20Chan%20and%20Sumit%20Sanghai%20and%20Ricky%20Liang%20and%20Ethan%20Liang%20and%20Antoine%20He%20and%20Amy%20Stuart%20and%20Arun%20Narayanan%20and%20Yukun%20Zhu%20and%20Christian%20Frank%20and%20Bahar%20Fatemi%20and%20Amit%20Sabne%20and%20Oran%20Lang%20and%20Indro%20Bhattacharya%20and%20Shane%20Settle%20and%20Maria%20Wang%20and%20Brendan%20McMahan%20and%20Andrea%20Tacchetti%20and%20Livio%20Baldini%20Soares%20and%20Majid%20Hadian%20and%20Serkan%20Cabi%20and%20Timothy%20Chung%20and%20Nikita%20Putikhin%20and%20Gang%20Li%20and%20Jeremy%20Chen%20and%20Austin%20Tarango%20and%20Henryk%20Michalewski%20and%20Mehran%20Kazemi%20and%20Hussain%20Masoom%20and%20Hila%20Sheftel%20and%20Rakesh%20Shivanna%20and%20Archita%20Vadali%20and%20Ramona%20Comanescu%20and%20Doug%20Reid%20and%20Joss%20Moore%20and%20Arvind%20Neelakantan%20and%20Micha%C3%ABl%20Sander%20and%20Jonathan%20Herzig%20and%20Aviv%20Rosenberg%20and%20Mostafa%20Dehghani%20and%20JD%20Choi%20and%20Michael%20Fink%20and%20Reid%20Hayes%20and%20Eric%20Ge%20and%20Shitao%20Weng%20and%20Chia-Hua%20Ho%20and%20John%20Karro%20and%20Kalpesh%20Krishna%20and%20Lam%20Nguyen%20Thiet%20and%20Amy%20Skerry-Ryan%20and%20Daniel%20Eppens%20and%20Marco%20Andreetto%20and%20Navin%20Sarma%20and%20Silvano%20Bonacina%20and%20Burcu%20Karagol%20Ayan%20and%20Megha%20Nawhal%20and%20Zhihao%20Shan%20and%20Mike%20Dusenberry%20and%20Shantanu%20Thakoor%20and%20Sagar%20Gubbi%20and%20Duc%20Dung%20Nguyen%20and%20Reut%20Tsarfaty%20and%20Samuel%20Albanie%20and%20Jovana%20Mitrovi%C4%87%20and%20Meet%20Gandhi%20and%20Bo-Juen%20Chen%20and%20Alessandro%20Epasto%20and%20Georgi%20Stephanov%20and%20Ye%20Jin%20and%20Samuel%20Gehman%20and%20Aida%20Amini%20and%20Jack%20Weber%20and%20Feryal%20Behbahani%20and%20Shawn%20Xu%20and%20Miltos%20Allamanis%20and%20Xi%20Chen%20and%20Myle%20Ott%20and%20Claire%20Sha%20and%20Michal%20Jastrzebski%20and%20Hang%20Qi%20and%20David%20Greene%20and%20Xinyi%20Wu%20and%20Abodunrinwa%20Toki%20and%20Daniel%20Vlasic%20and%20Jane%20Shapiro%20and%20Ragha%20Kotikalapudi%20and%20Zhe%20Shen%20and%20Takaaki%20Saeki%20and%20Sirui%20Xie%20and%20Albin%20Cassirer%20and%20Shikhar%20Bharadwaj%20and%20Tatsuya%20Kiyono%20and%20Srinadh%20Bhojanapalli%20and%20Elan%20Rosenfeld%20and%20Sam%20Ritter%20and%20Jieming%20Mao%20and%20Jo%C3%A3o%20Gabriel%20Oliveira%20and%20Zoltan%20Egyed%20and%20Bernd%20Bandemer%20and%20Emilio%20Parisotto%20and%20Keisuke%20Kinoshita%20and%20Juliette%20Pluto%20and%20Petros%20Maniatis%20and%20Steve%20Li%20and%20Yaohui%20Guo%20and%20Golnaz%20Ghiasi%20and%20Jean%20Tarbouriech%20and%20Srimon%20Chatterjee%20and%20Julie%20Jin%20and%20%20Katrina%20and%20%20Xu%20and%20Jennimaria%20Palomaki%20and%20S%C3%A9b%20Arnold%20and%20Madhavi%20Sewak%20and%20Federico%20Piccinini%20and%20Mohit%20Sharma%20and%20Ben%20Albrecht%20and%20Sean%20Purser-haskell%20and%20Ashwin%20Vaswani%20and%20Chongyan%20Chen%20and%20Matheus%20Wisniewski%20and%20Qin%20Cao%20and%20John%20Aslanides%20and%20Nguyet%20Minh%20Phu%20and%20Maximilian%20Sieb%20and%20Lauren%20Agubuzu%20and%20Anne%20Zheng%20and%20Daniel%20Sohn%20and%20Marco%20Selvi%20and%20Anders%20Andreassen%20and%20Krishan%20Subudhi%20and%20Prem%20Eruvbetine%20and%20Oliver%20Woodman%20and%20Tomas%20Mery%20and%20Sebastian%20Krause%20and%20Xiaoqi%20Ren%20and%20Xiao%20Ma%20and%20Jincheng%20Luo%20and%20Dawn%20Chen%20and%20Wei%20Fan%20and%20Henry%20Griffiths%20and%20Christian%20Schuler%20and%20Alice%20Li%20and%20Shujian%20Zhang%20and%20Jean-Michel%20Sarr%20and%20Shixin%20Luo%20and%20Riccardo%20Patana%20and%20Matthew%20Watson%20and%20Dani%20Naboulsi%20and%20Michael%20Collins%20and%20Sailesh%20Sidhwani%20and%20Emiel%20Hoogeboom%20and%20Sharon%20Silver%20and%20Emily%20Caveness%20and%20Xiaokai%20Zhao%20and%20Mikel%20Rodriguez%20and%20Maxine%20Deines%20and%20Libin%20Bai%20and%20Patrick%20Griffin%20and%20Marco%20Tagliasacchi%20and%20Emily%20Xue%20and%20Spandana%20Raj%20Babbula%20and%20Bo%20Pang%20and%20Nan%20Ding%20and%20Gloria%20Shen%20and%20Elijah%20Peake%20and%20Remi%20Crocker%20and%20Shubha%20Srinivas%20Raghvendra%20and%20Danny%20Swisher%20and%20Woohyun%20Han%20and%20Richa%20Singh%20and%20Ling%20Wu%20and%20Vladimir%20Pchelin%20and%20Tsendsuren%20Munkhdalai%20and%20Dana%20Alon%20and%20Geoff%20Bacon%20and%20Efren%20Robles%20and%20Jannis%20Bulian%20and%20Melvin%20Johnson%20and%20George%20Powell%20and%20Felipe%20Tiengo%20Ferreira%20and%20Yaoyiran%20Li%20and%20Frederik%20Benzing%20and%20Mihajlo%20Velimirovi%C4%87%20and%20Hubert%20Soyer%20and%20William%20Kong%20and%20%20Tony%20and%20%20Nguy%C3%AAn%20and%20Zhen%20Yang%20and%20Jeremiah%20Liu%20and%20Joost%20van%20Amersfoort%20and%20Daniel%20Gillick%20and%20Baochen%20Sun%20and%20Nathalie%20Rauschmayr%20and%20Katie%20Zhang%20and%20Serena%20Zhan%20and%20Tao%20Zhou%20and%20Alexey%20Frolov%20and%20Chengrun%20Yang%20and%20Denis%20Vnukov%20and%20Louis%20Rouillard%20and%20Hongji%20Li%20and%20Amol%20Mandhane%20and%20Nova%20Fallen%20and%20Rajesh%20Venkataraman%20and%20Clara%20Huiyi%20Hu%20and%20Jennifer%20Brennan%20and%20Jenny%20Lee%20and%20Jerry%20Chang%20and%20Martin%20Sundermeyer%20and%20Zhufeng%20Pan%20and%20Rosemary%20Ke%20and%20Simon%20Tong%20and%20Alex%20Fabrikant%20and%20William%20Bono%20and%20Jindong%20Gu%20and%20Ryan%20Foley%20and%20Yiran%20Mao%20and%20Manolis%20Delakis%20and%20Dhruva%20Bhaswar%20and%20Roy%20Frostig%20and%20Nick%20Li%20and%20Avital%20Zipori%20and%20Cath%20Hope%20and%20Olga%20Kozlova%20and%20Swaroop%20Mishra%20and%20Josip%20Djolonga%20and%20Craig%20Schiff%20and%20Majd%20Al%20Merey%20and%20Eleftheria%20Briakou%20and%20Peter%20Morgan%20and%20Andy%20Wan%20and%20Avinatan%20Hassidim%20and%20RJ%20Skerry-Ryan%20and%20Kuntal%20Sengupta%20and%20Mary%20Jasarevic%20and%20Praveen%20Kallakuri%20and%20Paige%20Kunkle%20and%20Hannah%20Brennan%20and%20Tom%20Lieber%20and%20Hassan%20Mansoor%20and%20Julian%20Walker%20and%20Bing%20Zhang%20and%20Annie%20Xie%20and%20Goran%20%C5%BDu%C5%BEi%C4%87%20and%20Adaeze%20Chukwuka%20and%20Alex%20Druinsky%20and%20Donghyun%20Cho%20and%20Rui%20Yao%20and%20Ferjad%20Naeem%20and%20Shiraz%20Butt%20and%20Eunyoung%20Kim%20and%20Zhipeng%20Jia%20and%20Mandy%20Jordan%20and%20Adam%20Lelkes%20and%20Mark%20Kurzeja%20and%20Sophie%20Wang%20and%20James%20Zhao%20and%20Andrew%20Over%20and%20Abhishek%20Chakladar%20and%20Marcel%20Prasetya%20and%20Neha%20Jha%20and%20Sriram%20Ganapathy%20and%20Yale%20Cong%20and%20Prakash%20Shroff%20and%20Carl%20Saroufim%20and%20Sobhan%20Miryoosefi%20and%20Mohamed%20Hammad%20and%20Tajwar%20Nasir%20and%20Weijuan%20Xi%20and%20Yang%20Gao%20and%20Young%20Maeng%20and%20Ben%20Hora%20and%20Chin-Yi%20Cheng%20and%20Parisa%20Haghani%20and%20Yoad%20Lewenberg%20and%20Caden%20Lu%20and%20Martin%20Matysiak%20and%20Naina%20Raisinghani%20and%20Huiyu%20Wang%20and%20Lexi%20Baugher%20and%20Rahul%20Sukthankar%20and%20Minh%20Giang%20and%20John%20Schultz%20and%20Noah%20Fiedel%20and%20Minmin%20Chen%20and%20Cheng-Chun%20Lee%20and%20Tapomay%20Dey%20and%20Hao%20Zheng%20and%20Shachi%20Paul%20and%20Celine%20Smith%20and%20Andy%20Ly%20and%20Yicheng%20Wang%20and%20Rishabh%20Bansal%20and%20Bartek%20Perz%20and%20Susanna%20Ricco%20and%20Stasha%20Blank%20and%20Vaishakh%20Keshava%20and%20Deepak%20Sharma%20and%20Marvin%20Chow%20and%20Kunal%20Lad%20and%20Komal%20Jalan%20and%20Simon%20Osindero%20and%20Craig%20Swanson%20and%20Jacob%20Scott%20and%20Anastasija%20Ili%C4%87%20and%20Xiaowei%20Li%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Afzal%20Shama%20Soudagar%20and%20Yan%20Xiong%20and%20Bat-Orgil%20Batsaikhan%20and%20Daniel%20Jarrett%20and%20Naveen%20Kumar%20and%20Maulik%20Shah%20and%20Matt%20Lawlor%20and%20Austin%20Waters%20and%20Mark%20Graham%20and%20Rhys%20May%20and%20Sabela%20Ramos%20and%20Sandra%20Lefdal%20and%20Zeynep%20Cankara%20and%20Nacho%20Cano%20and%20Brendan%20O%27Donoghue%20and%20Jed%20Borovik%20and%20Frederick%20Liu%20and%20Jordan%20Grimstad%20and%20Mahmoud%20Alnahlawi%20and%20Katerina%20Tsihlas%20and%20Tom%20Hudson%20and%20Nikolai%20Grigorev%20and%20Yiling%20Jia%20and%20Terry%20Huang%20and%20Tobenna%20Peter%20Igwe%20and%20Sergei%20Lebedev%20and%20Xiaodan%20Tang%20and%20Igor%20Krivokon%20and%20Frankie%20Garcia%20and%20Melissa%20Tan%20and%20Eric%20Jia%20and%20Peter%20Stys%20and%20Shikhar%20Vashishth%20and%20Yu%20Liang%20and%20Balaji%20Venkatraman%20and%20Chenjie%20Gu%20and%20Anastasios%20Kementsietsidis%20and%20Chen%20Zhu%20and%20Junehyuk%20Jung%20and%20Yunfei%20Bai%20and%20Mohammad%20Javad%20Hosseini%20and%20Faruk%20Ahmed%20and%20Aditya%20Gupta%20and%20Xin%20Yuan%20and%20Shereen%20Ashraf%20and%20Shitij%20Nigam%20and%20Gautam%20Vasudevan%20and%20Pranjal%20Awasthi%20and%20Adi%20Mayrav%20Gilady%20and%20Zelda%20Mariet%20and%20Ramy%20Eskander%20and%20Haiguang%20Li%20and%20Hexiang%20Hu%20and%20Guillermo%20Garrido%20and%20Philippe%20Schlattner%20and%20George%20Zhang%20and%20Rohun%20Saxena%20and%20Petar%20Devi%C4%87%20and%20Kritika%20Muralidharan%20and%20Ashwin%20Murthy%20and%20Yiqian%20Zhou%20and%20Min%20Choi%20and%20Arissa%20Wongpanich%20and%20Zhengdong%20Wang%20and%20Premal%20Shah%20and%20Yuntao%20Xu%20and%20Yiling%20Huang%20and%20Stephen%20Spencer%20and%20Alice%20Chen%20and%20James%20Cohan%20and%20Junjie%20Wang%20and%20Jonathan%20Tompson%20and%20Junru%20Wu%20and%20Ruba%20Haroun%20and%20Haiqiong%20Li%20and%20Blanca%20Huergo%20and%20Fan%20Yang%20and%20Tongxin%20Yin%20and%20James%20Wendt%20and%20Michael%20Bendersky%20and%20Rahma%20Chaabouni%20and%20Javier%20Snaider%20and%20Johan%20Ferret%20and%20Abhishek%20Jindal%20and%20Tara%20Thompson%20and%20Andrew%20Xue%20and%20Will%20Bishop%20and%20Shubham%20Milind%20Phal%20and%20Archit%20Sharma%20and%20Yunhsuan%20Sung%20and%20Prabakar%20Radhakrishnan%20and%20Mo%20Shomrat%20and%20Reeve%20Ingle%20and%20Roopali%20Vij%20and%20Justin%20Gilmer%20and%20Mihai%20Dorin%20Istin%20and%20Sam%20Sobell%20and%20Yang%20Lu%20and%20Emily%20Nottage%20and%20Dorsa%20Sadigh%20and%20Jeremiah%20Willcock%20and%20Tingnan%20Zhang%20and%20Steve%20Xu%20and%20Sasha%20Brown%20and%20Katherine%20Lee%20and%20Gary%20Wang%20and%20Yun%20Zhu%20and%20Yi%20Tay%20and%20Cheolmin%20Kim%20and%20Audrey%20Gutierrez%20and%20Abhanshu%20Sharma%20and%20Yongqin%20Xian%20and%20Sungyong%20Seo%20and%20Claire%20Cui%20and%20Elena%20Pochernina%20and%20Cip%20Baetu%20and%20Krzysztof%20Jastrz%C4%99bski%20and%20Mimi%20Ly%20and%20Mohamed%20Elhawaty%20and%20Dan%20Suh%20and%20Eren%20Sezener%20and%20Pidong%20Wang%20and%20Nancy%20Yuen%20and%20George%20Tucker%20and%20Jiahao%20Cai%20and%20Zuguang%20Yang%20and%20Cindy%20Wang%20and%20Alex%20Muzio%20and%20Hai%20Qian%20and%20Jae%20Yoo%20and%20Derek%20Lockhart%20and%20Kevin%20R.%20McKee%20and%20Mandy%20Guo%20and%20Malika%20Mehrotra%20and%20Artur%20Mendon%C3%A7a%20and%20Sanket%20Vaibhav%20Mehta%20and%20Sherry%20Ben%20and%20Chetan%20Tekur%20and%20Jiaqi%20Mu%20and%20Muye%20Zhu%20and%20Victoria%20Krakovna%20and%20Hongrae%20Lee%20and%20AJ%20Maschinot%20and%20S%C3%A9bastien%20Cevey%20and%20HyunJeong%20Choe%20and%20Aijun%20Bai%20and%20Hansa%20Srinivasan%20and%20Derek%20Gasaway%20and%20Nick%20Young%20and%20Patrick%20Siegler%20and%20Dan%20Holtmann-Rice%20and%20Vihari%20Piratla%20and%20Kate%20Baumli%20and%20Roey%20Yogev%20and%20Alex%20Hofer%20and%20Hado%20van%20Hasselt%20and%20Svetlana%20Grant%20and%20Yuri%20Chervonyi%20and%20David%20Silver%20and%20Andrew%20Hogue%20and%20Ayushi%20Agarwal%20and%20Kathie%20Wang%20and%20Preeti%20Singh%20and%20Four%20Flynn%20and%20Josh%20Lipschultz%20and%20Robert%20David%20and%20Lizzetth%20Bellot%20and%20Yao-Yuan%20Yang%20and%20Long%20Le%20and%20Filippo%20Graziano%20and%20Kate%20Olszewska%20and%20Kevin%20Hui%20and%20Akanksha%20Maurya%20and%20Nikos%20Parotsidis%20and%20Weijie%20Chen%20and%20Tayo%20Oguntebi%20and%20Joe%20Kelley%20and%20Anirudh%20Baddepudi%20and%20Johannes%20Mauerer%20and%20Gregory%20Shaw%20and%20Alex%20Siegman%20and%20Lin%20Yang%20and%20Shravya%20Shetty%20and%20Subhrajit%20Roy%20and%20Yunting%20Song%20and%20Wojciech%20Stokowiec%20and%20Ryan%20Burnell%20and%20Omkar%20Savant%20and%20Robert%20Busa-Fekete%20and%20Jin%20Miao%20and%20Samrat%20Ghosh%20and%20Liam%20MacDermed%20and%20Phillip%20Lippe%20and%20Mikhail%20Dektiarev%20and%20Zach%20Behrman%20and%20Fabian%20Mentzer%20and%20Kelvin%20Nguyen%20and%20Meng%20Wei%20and%20Siddharth%20Verma%20and%20Chris%20Knutsen%20and%20Sudeep%20Dasari%20and%20Zhipeng%20Yan%20and%20Petr%20Mitrichev%20and%20Xingyu%20Wang%20and%20Virat%20Shejwalkar%20and%20Jacob%20Austin%20and%20Srinivas%20Sunkara%20and%20Navneet%20Potti%20and%20Yan%20Virin%20and%20Christian%20Wright%20and%20Ga%C3%ABl%20Liu%20and%20Oriana%20Riva%20and%20Etienne%20Pot%20and%20Greg%20Kochanski%20and%20Quoc%20Le%20and%20Gargi%20Balasubramaniam%20and%20Arka%20Dhar%20and%20Yuguo%20Liao%20and%20Adam%20Bloniarz%20and%20Divyansh%20Shukla%20and%20Elizabeth%20Cole%20and%20Jong%20Lee%20and%20Sheng%20Zhang%20and%20Sushant%20Kafle%20and%20Siddharth%20Vashishtha%20and%20Parsa%20Mahmoudieh%20and%20Grace%20Chen%20and%20Raphael%20Hoffmann%20and%20Pranesh%20Srinivasan%20and%20Agustin%20Dal%20Lago%20and%20Yoav%20Ben%20Shalom%20and%20Zi%20Wang%20and%20Michael%20Elabd%20and%20Anuj%20Sharma%20and%20Junhyuk%20Oh%20and%20Suraj%20Kothawade%20and%20Maigo%20Le%20and%20Marianne%20Monteiro%20and%20Shentao%20Yang%20and%20Kaiz%20Alarakyia%20and%20Robert%20Geirhos%20and%20Diana%20Mincu%20and%20H%C3%A5vard%20Garnes%20and%20Hayato%20Kobayashi%20and%20Soroosh%20Mariooryad%20and%20Kacper%20Krasowiak%20and%20%20Zhixin%20and%20%20Lai%20and%20Shibl%20Mourad%20and%20Mingqiu%20Wang%20and%20Fan%20Bu%20and%20Ophir%20Aharoni%20and%20Guanjie%20Chen%20and%20Abhimanyu%20Goyal%20and%20Vadim%20Zubov%20and%20Ankur%20Bapna%20and%20Elahe%20Dabir%20and%20Nisarg%20Kothari%20and%20Kay%20Lamerigts%20and%20Nicola%20De%20Cao%20and%20Jeremy%20Shar%20and%20Christopher%20Yew%20and%20Nitish%20Kulkarni%20and%20Dre%20Mahaarachchi%20and%20Mandar%20Joshi%20and%20Zhenhai%20Zhu%20and%20Jared%20Lichtarge%20and%20Yichao%20Zhou%20and%20Hannah%20Muckenhirn%20and%20Vittorio%20Selo%20and%20Oriol%20Vinyals%20and%20Peter%20Chen%20and%20Anthony%20Brohan%20and%20Vaibhav%20Mehta%20and%20Sarah%20Cogan%20and%20Ruth%20Wang%20and%20Ty%20Geri%20and%20Wei-Jen%20Ko%20and%20Wei%20Chen%20and%20Fabio%20Viola%20and%20Keshav%20Shivam%20and%20Lisa%20Wang%20and%20Madeleine%20Clare%20Elish%20and%20Raluca%20Ada%20Popa%20and%20S%C3%A9bastien%20Pereira%20and%20Jianqiao%20Liu%20and%20Raphael%20Koster%20and%20Donnie%20Kim%20and%20Gufeng%20Zhang%20and%20Sayna%20Ebrahimi%20and%20Partha%20Talukdar%20and%20Yanyan%20Zheng%20and%20Petra%20Poklukar%20and%20Ales%20Mikhalap%20and%20Dale%20Johnson%20and%20Anitha%20Vijayakumar%20and%20Mark%20Omernick%20and%20Matt%20Dibb%20and%20Ayush%20Dubey%20and%20Qiong%20Hu%20and%20Apurv%20Suman%20and%20Vaibhav%20Aggarwal%20and%20Ilya%20Kornakov%20and%20Fei%20Xia%20and%20Wing%20Lowe%20and%20Alexey%20Kolganov%20and%20Ted%20Xiao%20and%20Vitaly%20Nikolaev%20and%20Steven%20Hemingray%20and%20Bonnie%20Li%20and%20Joana%20Iljazi%20and%20Miko%C5%82aj%20Rybi%C5%84ski%20and%20Ballie%20Sandhu%20and%20Peggy%20Lu%20and%20Thang%20Luong%20and%20Rodolphe%20Jenatton%20and%20Vineetha%20Govindaraj%20and%20%20Hui%20and%20%20Li%20and%20Gabriel%20Dulac-Arnold%20and%20Wonpyo%20Park%20and%20Henry%20Wang%20and%20Abhinit%20Modi%20and%20Jean%20Pouget-Abadie%20and%20Kristina%20Greller%20and%20Rahul%20Gupta%20and%20Robert%20Berry%20and%20Prajit%20Ramachandran%20and%20Jinyu%20Xie%20and%20Liam%20McCafferty%20and%20Jianling%20Wang%20and%20Kilol%20Gupta%20and%20Hyeontaek%20Lim%20and%20Bla%C5%BE%20Bratani%C4%8D%20and%20Andy%20Brock%20and%20Ilia%20Akolzin%20and%20Jim%20Sproch%20and%20Dan%20Karliner%20and%20Duhyeon%20Kim%20and%20Adrian%20Goedeckemeyer%20and%20Noam%20Shazeer%20and%20Cordelia%20Schmid%20and%20Daniele%20Calandriello%20and%20Parul%20Bhatia%20and%20Krzysztof%20Choromanski%20and%20Ceslee%20Montgomery%20and%20Dheeru%20Dua%20and%20Ana%20Ramalho%20and%20Helen%20King%20and%20Yue%20Gao%20and%20Lynn%20Nguyen%20and%20David%20Lindner%20and%20Divya%20Pitta%20and%20Oleaser%20Johnson%20and%20Khalid%20Salama%20and%20Diego%20Ardila%20and%20Michael%20Han%20and%20Erin%20Farnese%20and%20Seth%20Odoom%20and%20Ziyue%20Wang%20and%20Xiangzhuo%20Ding%20and%20Norman%20Rink%20and%20Ray%20Smith%20and%20Harshal%20Tushar%20Lehri%20and%20Eden%20Cohen%20and%20Neera%20Vats%20and%20Tong%20He%20and%20Parthasarathy%20Gopavarapu%20and%20Adam%20Paszke%20and%20Miteyan%20Patel%20and%20Wouter%20Van%20Gansbeke%20and%20Lucia%20Loher%20and%20Luis%20Castro%20and%20Maria%20Voitovich%20and%20Tamara%20von%20Glehn%20and%20Nelson%20George%20and%20Simon%20Niklaus%20and%20Zach%20Eaton-Rosen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Erik%20Jue%20and%20Sagi%20Perel%20and%20Carrie%20Zhang%20and%20Yuval%20Bahat%20and%20Ang%C3%A9line%20Pouget%20and%20Zhi%20Xing%20and%20Fantine%20Huot%20and%20Ashish%20Shenoy%20and%20Taylor%20Bos%20and%20Vincent%20Coriou%20and%20Bryan%20Richter%20and%20Natasha%20Noy%20and%20Yaqing%20Wang%20and%20Santiago%20Ontanon%20and%20Siyang%20Qin%20and%20Gleb%20Makarchuk%20and%20Demis%20Hassabis%20and%20Zhuowan%20Li%20and%20Mandar%20Sharma%20and%20Kumaran%20Venkatesan%20and%20Iurii%20Kemaev%20and%20Roxanne%20Daniel%20and%20Shiyu%20Huang%20and%20Saloni%20Shah%20and%20Octavio%20Ponce%20and%20%20Warren%20and%20%20Chen%20and%20Manaal%20Faruqui%20and%20Jialin%20Wu%20and%20Slavica%20Anda%C4%8Di%C4%87%20and%20Szabolcs%20Payrits%20and%20Daniel%20McDuff%20and%20Tom%20Hume%20and%20Yuan%20Cao%20and%20MH%20Tessler%20and%20Qingze%20Wang%20and%20Yinan%20Wang%20and%20Ivor%20Rendulic%20and%20Eirikur%20Agustsson%20and%20Matthew%20Johnson%20and%20Tanya%20Lando%20and%20Andrew%20Howard%20and%20Sri%20Gayatri%20Sundara%20Padmanabhan%20and%20Mayank%20Daswani%20and%20Andrea%20Banino%20and%20Michael%20Kilgore%20and%20Jonathan%20Heek%20and%20Ziwei%20Ji%20and%20Alvaro%20Caceres%20and%20Conglong%20Li%20and%20Nora%20Kassner%20and%20Alexey%20Vlaskin%20and%20Zeyu%20Liu%20and%20Alex%20Grills%20and%20Yanhan%20Hou%20and%20Roykrong%20Sukkerd%20and%20Gowoon%20Cheon%20and%20Nishita%20Shetty%20and%20Larisa%20Markeeva%20and%20Piotr%20Stanczyk%20and%20Tejas%20Iyer%20and%20Yuan%20Gong%20and%20Shawn%20Gao%20and%20Keerthana%20Gopalakrishnan%20and%20Tim%20Blyth%20and%20Malcolm%20Reynolds%20and%20Avishkar%20Bhoopchand%20and%20Misha%20Bilenko%20and%20Dero%20Gharibian%20and%20Vicky%20Zayats%20and%20Aleksandra%20Faust%20and%20Abhinav%20Singh%20and%20Min%20Ma%20and%20Hongyang%20Jiao%20and%20Sudheendra%20Vijayanarasimhan%20and%20Lora%20Aroyo%20and%20Vikas%20Yadav%20and%20Sarah%20Chakera%20and%20Ashwin%20Kakarla%20and%20Vilobh%20Meshram%20and%20Karol%20Gregor%20and%20Gabriela%20Botea%20and%20Evan%20Senter%20and%20Dawei%20Jia%20and%20Geza%20Kovacs%20and%20Neha%20Sharma%20and%20Sebastien%20Baur%20and%20Kai%20Kang%20and%20Yifan%20He%20and%20Lin%20Zhuo%20and%20Marija%20Kostelac%20and%20Itay%20Laish%20and%20Songyou%20Peng%20and%20Louis%20O%27Bryan%20and%20Daniel%20Kasenberg%20and%20Girish%20Ramchandra%20Rao%20and%20Edouard%20Leurent%20and%20Biao%20Zhang%20and%20Sage%20Stevens%20and%20Ana%20Salazar%20and%20Ye%20Zhang%20and%20Ivan%20Lobov%20and%20Jake%20Walker%20and%20Allen%20Porter%20and%20Morgan%20Redshaw%20and%20Han%20Ke%20and%20Abhishek%20Rao%20and%20Alex%20Lee%20and%20Hoi%20Lam%20and%20Michael%20Moffitt%20and%20Jaeyoun%20Kim%20and%20Siyuan%20Qiao%20and%20Terry%20Koo%20and%20Robert%20Dadashi%20and%20Xinying%20Song%20and%20Mukund%20Sundararajan%20and%20Peng%20Xu%20and%20Chizu%20Kawamoto%20and%20Yan%20Zhong%20and%20Clara%20Barbu%20and%20Apoorv%20Reddy%20and%20Mauro%20Verzetti%20and%20Leon%20Li%20and%20George%20Papamakarios%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Mary%20Cassin%20and%20Koray%20Kavukcuoglu%20and%20Rigel%20Swavely%20and%20Alain%20Vaucher%20and%20Jeffrey%20Zhao%20and%20Ross%20Hemsley%20and%20Michael%20Tschannen%20and%20Heming%20Ge%20and%20Gaurav%20Menghani%20and%20Yang%20Yu%20and%20Natalie%20Ha%20and%20Wei%20He%20and%20Xiao%20Wu%20and%20Maggie%20Song%20and%20Rachel%20Sterneck%20and%20Stefan%20Zinke%20and%20Dan%20A.%20Calian%20and%20Annie%20Marsden%20and%20Alejandro%20Cruzado%20Ruiz%20and%20Matteo%20Hessel%20and%20Almog%20Gueta%20and%20Benjamin%20Lee%20and%20Brian%20Farris%20and%20Manish%20Gupta%20and%20Yunjie%20Li%20and%20Mohammad%20Saleh%20and%20Vedant%20Misra%20and%20Kefan%20Xiao%20and%20Piermaria%20Mendolicchio%20and%20Gavin%20Buttimore%20and%20Varvara%20Krayvanova%20and%20Nigamaa%20Nayakanti%20and%20Matthew%20Wiethoff%20and%20Yash%20Pande%20and%20Azalia%20Mirhoseini%20and%20Ni%20Lao%20and%20Jasmine%20Liu%20and%20Yiqing%20Hua%20and%20Angie%20Chen%20and%20Yury%20Malkov%20and%20Dmitry%20Kalashnikov%20and%20Shubham%20Gupta%20and%20Kartik%20Audhkhasi%20and%20Yuexiang%20Zhai%20and%20Sudhindra%20Kopalle%20and%20Prateek%20Jain%20and%20Eran%20Ofek%20and%20Clemens%20Meyer%20and%20Khuslen%20Baatarsukh%20and%20Hana%20Strej%C4%8Dek%20and%20Jun%20Qian%20and%20James%20Freedman%20and%20Ricardo%20Figueira%20and%20Michal%20Sokolik%20and%20Olivier%20Bachem%20and%20Raymond%20Lin%20and%20Dia%20Kharrat%20and%20Chris%20Hidey%20and%20Pingmei%20Xu%20and%20Dennis%20Duan%20and%20Yin%20Li%20and%20Muge%20Ersoy%20and%20Richard%20Everett%20and%20Kevin%20Cen%20and%20Rebeca%20Santamaria-Fernandez%20and%20Amir%20Taubenfeld%20and%20Ian%20Mackinnon%20and%20Linda%20Deng%20and%20Polina%20Zablotskaia%20and%20Shashank%20Viswanadha%20and%20Shivanker%20Goel%20and%20Damion%20Yates%20and%20Yunxiao%20Deng%20and%20Peter%20Choy%20and%20Mingqing%20Chen%20and%20Abhishek%20Sinha%20and%20Alex%20Mossin%20and%20Yiming%20Wang%20and%20Arthur%20Szlam%20and%20Susan%20Hao%20and%20Paul%20Kishan%20Rubenstein%20and%20Metin%20Toksoz-Exley%20and%20Miranda%20Aperghis%20and%20Yin%20Zhong%20and%20Junwhan%20Ahn%20and%20Michael%20Isard%20and%20Olivier%20Lacombe%20and%20Florian%20Luisier%20and%20Chrysovalantis%20Anastasiou%20and%20Yogesh%20Kalley%20and%20Utsav%20Prabhu%20and%20Emma%20Dunleavy%20and%20Shaan%20Bijwadia%20and%20Justin%20Mao-Jones%20and%20Kelly%20Chen%20and%20Rama%20Pasumarthi%20and%20Emily%20Wood%20and%20Adil%20Dostmohamed%20and%20Nate%20Hurley%20and%20Jiri%20Simsa%20and%20Alicia%20Parrish%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Ondrej%20Skopek%20and%20Yony%20Kochinski%20and%20Javier%20Rey%20and%20Verena%20Rieser%20and%20Denny%20Zhou%20and%20Sun%20Jae%20Lee%20and%20Trilok%20Acharya%20and%20Guowang%20Li%20and%20Joe%20Jiang%20and%20Xiaofan%20Zhang%20and%20Bryant%20Gipson%20and%20Ethan%20Mahintorabi%20and%20Marco%20Gelmi%20and%20Nima%20Khajehnouri%20and%20Angel%20Yeh%20and%20Kayi%20Lee%20and%20Loic%20Matthey%20and%20Leslie%20Baker%20and%20Trang%20Pham%20and%20Han%20Fu%20and%20Alex%20Pak%20and%20Prakhar%20Gupta%20and%20Cristina%20Vasconcelos%20and%20Adam%20Sadovsky%20and%20Brian%20Walker%20and%20Sissie%20Hsiao%20and%20Patrik%20Zochbauer%20and%20Andreea%20Marzoca%20and%20Noam%20Velan%20and%20Junhao%20Zeng%20and%20Gilles%20Baechler%20and%20Danny%20Driess%20and%20Divya%20Jain%20and%20Yanping%20Huang%20and%20Lizzie%20Tao%20and%20John%20Maggs%20and%20Nir%20Levine%20and%20Jon%20Schneider%20and%20Erika%20Gemzer%20and%20Samuel%20Petit%20and%20Shan%20Han%20and%20Zach%20Fisher%20and%20Dustin%20Zelle%20and%20Courtney%20Biles%20and%20Eugene%20Ie%20and%20Asya%20Fadeeva%20and%20Casper%20Liu%20and%20Juliana%20Vicente%20Franco%20and%20Adrian%20Collister%20and%20Hao%20Zhang%20and%20Renshen%20Wang%20and%20Ruizhe%20Zhao%20and%20Leandro%20Kieliger%20and%20Kurt%20Shuster%20and%20Rui%20Zhu%20and%20Boqing%20Gong%20and%20Lawrence%20Chan%20and%20Ruoxi%20Sun%20and%20Sujoy%20Basu%20and%20Roland%20Zimmermann%20and%20Jamie%20Hayes%20and%20Abhishek%20Bapna%20and%20Jasper%20Snoek%20and%20Weel%20Yang%20and%20Puranjay%20Datta%20and%20Jad%20Al%20Abdallah%20and%20Kevin%20Kilgour%20and%20Lu%20Li%20and%20SQ%20Mah%20and%20Yennie%20Jun%20and%20Morgane%20Rivi%C3%A8re%20and%20Abhijit%20Karmarkar%20and%20Tammo%20Spalink%20and%20Tao%20Huang%20and%20Lucas%20Gonzalez%20and%20Duc-Hieu%20Tran%20and%20Averi%20Nowak%20and%20John%20Palowitch%20and%20Martin%20Chadwick%20and%20Ellie%20Talius%20and%20Harsh%20Mehta%20and%20Thibault%20Sellam%20and%20Philipp%20Fr%C3%A4nken%20and%20Massimo%20Nicosia%20and%20Kyle%20He%20and%20Aditya%20Kini%20and%20David%20Amos%20and%20Sugato%20Basu%20and%20Harrison%20Jobe%20and%20Eleni%20Shaw%20and%20Qiantong%20Xu%20and%20Colin%20Evans%20and%20Daisuke%20Ikeda%20and%20Chaochao%20Yan%20and%20Larry%20Jin%20and%20Lun%20Wang%20and%20Sachin%20Yadav%20and%20Ilia%20Labzovsky%20and%20Ramesh%20Sampath%20and%20Ada%20Ma%20and%20Candice%20Schumann%20and%20Aditya%20Siddhant%20and%20Rohin%20Shah%20and%20John%20Youssef%20and%20Rishabh%20Agarwal%20and%20Natalie%20Dabney%20and%20Alessio%20Tonioni%20and%20Moran%20Ambar%20and%20Jing%20Li%20and%20Isabelle%20Guyon%20and%20Benny%20Li%20and%20David%20Soergel%20and%20Boya%20Fang%20and%20Georgi%20Karadzhov%20and%20Cristian%20Udrescu%20and%20Trieu%20Trinh%20and%20Vikas%20Raunak%20and%20Seb%20Noury%20and%20Dee%20Guo%20and%20Sonal%20Gupta%20and%20Mara%20Finkelstein%20and%20Denis%20Petek%20and%20Lihao%20Liang%20and%20Greg%20Billock%20and%20Pei%20Sun%20and%20David%20Wood%20and%20Yiwen%20Song%20and%20Xiaobin%20Yu%20and%20Tatiana%20Matejovicova%20and%20Regev%20Cohen%20and%20Kalyan%20Andra%20and%20David%20D%27Ambrosio%20and%20Zhiwei%20Deng%20and%20Vincent%20Nallatamby%20and%20Ebrahim%20Songhori%20and%20Rumen%20Dangovski%20and%20Andrew%20Lampinen%20and%20Pankil%20Botadra%20and%20Adam%20Hillier%20and%20Jiawei%20Cao%20and%20Nagabhushan%20Baddi%20and%20Adhi%20Kuncoro%20and%20Toshihiro%20Yoshino%20and%20Ankit%20Bhagatwala%20and%20Marc%C3%A1urelio%20Ranzato%20and%20Rylan%20Schaeffer%20and%20Tianlin%20Liu%20and%20Shuai%20Ye%20and%20Obaid%20Sarvana%20and%20John%20Nham%20and%20Chenkai%20Kuang%20and%20Isabel%20Gao%20and%20Jinoo%20Baek%20and%20Shubham%20Mittal%20and%20Ayzaan%20Wahid%20and%20Anita%20Gergely%20and%20Bin%20Ni%20and%20Josh%20Feldman%20and%20Carrie%20Muir%20and%20Pascal%20Lamblin%20and%20Wolfgang%20Macherey%20and%20Ethan%20Dyer%20and%20Logan%20Kilpatrick%20and%20V%C3%ADctor%20Campos%20and%20Mukul%20Bhutani%20and%20Stanislav%20Fort%20and%20Yanif%20Ahmad%20and%20Aliaksei%20Severyn%20and%20Kleopatra%20Chatziprimou%20and%20Oleksandr%20Ferludin%20and%20Mason%20Dimarco%20and%20Aditya%20Kusupati%20and%20Joe%20Heyward%20and%20Dan%20Bahir%20and%20Kevin%20Villela%20and%20Katie%20Millican%20and%20Dror%20Marcus%20and%20Sanaz%20Bahargam%20and%20Caglar%20Unlu%20and%20Nicholas%20Roth%20and%20Zichuan%20Wei%20and%20Siddharth%20Gopal%20and%20Deepanway%20Ghoshal%20and%20Edward%20Lee%20and%20Sharon%20Lin%20and%20Jennie%20Lees%20and%20Dayeong%20Lee%20and%20Anahita%20Hosseini%20and%20Connie%20Fan%20and%20Seth%20Neel%20and%20Marcus%20Wu%20and%20Yasemin%20Altun%20and%20Honglong%20Cai%20and%20Enrique%20Piqueras%20and%20Josh%20Woodward%20and%20Alessandro%20Bissacco%20and%20Salem%20Haykal%20and%20Mahyar%20Bordbar%20and%20Prasha%20Sundaram%20and%20Sarah%20Hodkinson%20and%20Daniel%20Toyama%20and%20George%20Polovets%20and%20Austin%20Myers%20and%20Anu%20Sinha%20and%20Tomer%20Levinboim%20and%20Kashyap%20Krishnakumar%20and%20Rachita%20Chhaparia%20and%20Tatiana%20Sholokhova%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Ganesh%20Jawahar%20and%20Haroon%20Qureshi%20and%20Jieru%20Hu%20and%20Nikola%20Momchev%20and%20Matthew%20Rahtz%20and%20Renjie%20Wu%20and%20Aishwarya%20P%20S%20and%20Kedar%20Dhamdhere%20and%20Meiqi%20Guo%20and%20Umang%20Gupta%20and%20Ali%20Eslami%20and%20Mariano%20Schain%20and%20Michiel%20Blokzijl%20and%20David%20Welling%20and%20Dave%20Orr%20and%20Levent%20Bolelli%20and%20Nicolas%20Perez-Nieves%20and%20Mikhail%20Sirotenko%20and%20Aman%20Prasad%20and%20Arjun%20Kar%20and%20Borja%20De%20Balle%20Pigem%20and%20Tayfun%20Terzi%20and%20Gell%C3%A9rt%20Weisz%20and%20Dipankar%20Ghosh%20and%20Aditi%20Mavalankar%20and%20Dhruv%20Madeka%20and%20Kaspar%20Daugaard%20and%20Hartwig%20Adam%20and%20Viraj%20Shah%20and%20Dana%20Berman%20and%20Maggie%20Tran%20and%20Steven%20Baker%20and%20Ewa%20Andrejczuk%20and%20Grishma%20Chole%20and%20Ganna%20Raboshchuk%20and%20Mahdi%20Mirzazadeh%20and%20Thais%20Kagohara%20and%20Shimu%20Wu%20and%20Christian%20Schallhart%20and%20Bernett%20Orlando%20and%20Chen%20Wang%20and%20Alban%20Rrustemi%20and%20Hao%20Xiong%20and%20Hao%20Liu%20and%20Arpi%20Vezer%20and%20Nolan%20Ramsden%20and%20Shuo-yiin%20Chang%20and%20Sidharth%20Mudgal%20and%20Yan%20Li%20and%20Nino%20Vieillard%20and%20Yedid%20Hoshen%20and%20Farooq%20Ahmad%20and%20Ambrose%20Slone%20and%20Amy%20Hua%20and%20Natan%20Potikha%20and%20Mirko%20Rossini%20and%20Jon%20Stritar%20and%20Sushant%20Prakash%20and%20Zifeng%20Wang%20and%20Xuanyi%20Dong%20and%20Alireza%20Nazari%20and%20Efrat%20Nehoran%20and%20Kaan%20Tekelioglu%20and%20Yinxiao%20Li%20and%20Kartikeya%20Badola%20and%20Tom%20Funkhouser%20and%20Yuanzhen%20Li%20and%20Varun%20Yerram%20and%20Ramya%20Ganeshan%20and%20Daniel%20Formoso%20and%20Karol%20Langner%20and%20Tian%20Shi%20and%20Huijian%20Li%20and%20Yumeya%20Yamamori%20and%20Amayika%20Panda%20and%20Alaa%20Saade%20and%20Angelo%20Scorza%20Scarpati%20and%20Chris%20Breaux%20and%20CJ%20Carey%20and%20Zongwei%20Zhou%20and%20Cho-Jui%20Hsieh%20and%20Sophie%20Bridgers%20and%20Alena%20Butryna%20and%20Nishesh%20Gupta%20and%20Vaibhav%20Tulsyan%20and%20Sanghyun%20Woo%20and%20Evgenii%20Eltyshev%20and%20Will%20Grathwohl%20and%20Chanel%20Parks%20and%20Seth%20Benjamin%20and%20Rina%20Panigrahy%20and%20Shenil%20Dodhia%20and%20Daniel%20De%20Freitas%20and%20Chris%20Sauer%20and%20Will%20Song%20and%20Ferran%20Alet%20and%20Jackson%20Tolins%20and%20Cosmin%20Paduraru%20and%20Xingyi%20Zhou%20and%20Brian%20Albert%20and%20Zizhao%20Zhang%20and%20Lei%20Shu%20and%20Mudit%20Bansal%20and%20Sarah%20Nguyen%20and%20Amir%20Globerson%20and%20Owen%20Xiao%20and%20James%20Manyika%20and%20Tom%20Hennigan%20and%20Rong%20Rong%20and%20Josip%20Matak%20and%20Anton%20Bakalov%20and%20Ankur%20Sharma%20and%20Danila%20Sinopalnikov%20and%20Andrew%20Pierson%20and%20Stephen%20Roller%20and%20Geoff%20Brown%20and%20Mingcen%20Gao%20and%20Toshiyuki%20Fukuzawa%20and%20Amin%20Ghafouri%20and%20Kenny%20Vassigh%20and%20Iain%20Barr%20and%20Zhicheng%20Wang%20and%20Anna%20Korsun%20and%20Rajesh%20Jayaram%20and%20Lijie%20Ren%20and%20Tim%20Zaman%20and%20Samira%20Khan%20and%20Yana%20Lunts%20and%20Dan%20Deutsch%20and%20Dave%20Uthus%20and%20Nitzan%20Katz%20and%20Masha%20Samsikova%20and%20Amr%20Khalifa%20and%20Nikhil%20Sethi%20and%20Jiao%20Sun%20and%20Luming%20Tang%20and%20Uri%20Alon%20and%20Xianghong%20Luo%20and%20Dian%20Yu%20and%20Abhishek%20Nayyar%20and%20Bryce%20Petrini%20and%20Will%20Truong%20and%20Vincent%20Hellendoorn%20and%20Nikolai%20Chinaev%20and%20Chris%20Alberti%20and%20Wei%20Wang%20and%20Jingcao%20Hu%20and%20Vahab%20Mirrokni%20and%20Ananth%20Balashankar%20and%20Avia%20Aharon%20and%20Aahil%20Mehta%20and%20Ahmet%20Iscen%20and%20Joseph%20Kready%20and%20Lucas%20Manning%20and%20Anhad%20Mohananey%20and%20Yuankai%20Chen%20and%20Anshuman%20Tripathi%20and%20Allen%20Wu%20and%20Igor%20Petrovski%20and%20Dawsen%20Hwang%20and%20Martin%20Baeuml%20and%20Shreyas%20Chandrakaladharan%20and%20Yuan%20Liu%20and%20Rey%20Coaguila%20and%20Maxwell%20Chen%20and%20Sally%20Ma%20and%20Pouya%20Tafti%20and%20Susheel%20Tatineni%20and%20Terry%20Spitz%20and%20Jiayu%20Ye%20and%20Paul%20Vicol%20and%20Mihaela%20Rosca%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20and%20Zohar%20Yahav%20and%20Sanjay%20Ghemawat%20and%20Hanzhao%20Lin%20and%20Phoebe%20Kirk%20and%20Zaid%20Nabulsi%20and%20Sergey%20Brin%20and%20Bernd%20Bohnet%20and%20Ken%20Caluwaerts%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Dan%20Zheng%20and%20Zihang%20Dai%20and%20Petre%20Petrov%20and%20Yichong%20Xu%20and%20Ramin%20Mehran%20and%20Zhuo%20Xu%20and%20Luisa%20Zintgraf%20and%20Jiho%20Choi%20and%20Spurthi%20Amba%20Hombaiah%20and%20Romal%20Thoppilan%20and%20Sashank%20Reddi%20and%20Lukasz%20Lew%20and%20Li%20Li%20and%20Kellie%20Webster%20and%20KP%20Sawhney%20and%20Lampros%20Lamprou%20and%20Siamak%20Shakeri%20and%20Mayank%20Lunayach%20and%20Jianmin%20Chen%20and%20Sumit%20Bagri%20and%20Alex%20Salcianu%20and%20Ying%20Chen%20and%20Yani%20Donchev%20and%20Charlotte%20Magister%20and%20Signe%20N%C3%B8rly%20and%20Vitor%20Rodrigues%20and%20Tomas%20Izo%20and%20Hila%20Noga%20and%20Joe%20Zou%20and%20Thomas%20K%C3%B6ppe%20and%20Wenxuan%20Zhou%20and%20Kenton%20Lee%20and%20Xiangzhu%20Long%20and%20Danielle%20Eisenbud%20and%20Anthony%20Chen%20and%20Connor%20Schenck%20and%20Chi%20Ming%20To%20and%20Peilin%20Zhong%20and%20Emanuel%20Taropa%20and%20Minh%20Truong%20and%20Omer%20Levy%20and%20Danilo%20Martins%20and%20Zhiyuan%20Zhang%20and%20Christopher%20Semturs%20and%20Kelvin%20Zhang%20and%20Alex%20Yakubovich%20and%20Pol%20Moreno%20and%20Lara%20McConnaughey%20and%20Di%20Lu%20and%20Sam%20Redmond%20and%20Lotte%20Weerts%20and%20Yonatan%20Bitton%20and%20Tiziana%20Refice%20and%20Nicolas%20Lacasse%20and%20Arthur%20Conmy%20and%20Corentin%20Tallec%20and%20Julian%20Odell%20and%20Hannah%20Forbes-Pollard%20and%20Arkadiusz%20Socala%20and%20Jonathan%20Hoech%20and%20Pushmeet%20Kohli%20and%20Alanna%20Walton%20and%20Rui%20Wang%20and%20Mikita%20Sazanovich%20and%20Kexin%20Zhu%20and%20Andrei%20Kapishnikov%20and%20Rich%20Galt%20and%20Matthew%20Denton%20and%20Ben%20Murdoch%20and%20Caitlin%20Sikora%20and%20Kareem%20Mohamed%20and%20Wei%20Wei%20and%20Uri%20First%20and%20Tim%20McConnell%20and%20Luis%20C.%20Cobo%20and%20James%20Qin%20and%20Thi%20Avrahami%20and%20Daniel%20Balle%20and%20Yu%20Watanabe%20and%20Annie%20Louis%20and%20Adam%20Kraft%20and%20Setareh%20Ariafar%20and%20Yiming%20Gu%20and%20Eug%C3%A9nie%20Rives%20and%20Charles%20Yoon%20and%20Andrei%20Rusu%20and%20James%20Cobon-Kerr%20and%20Chris%20Hahn%20and%20Jiaming%20Luo%20and%20%20Yuvein%20and%20%20Zhu%20and%20Niharika%20Ahuja%20and%20Rodrigo%20Benenson%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Honglin%20Yu%20and%20Lloyd%20Hightower%20and%20Junlin%20Zhang%20and%20Darren%20Ni%20and%20Lisa%20Anne%20Hendricks%20and%20Gabby%20Wang%20and%20Gal%20Yona%20and%20Lalit%20Jain%20and%20Pablo%20Barrio%20and%20Surya%20Bhupatiraju%20and%20Siva%20Velusamy%20and%20Allan%20Dafoe%20and%20Sebastian%20Riedel%20and%20Tara%20Thomas%20and%20Zhe%20Yuan%20and%20Mathias%20Bellaiche%20and%20Sheena%20Panthaplackel%20and%20Klemen%20Kloboves%20and%20Sarthak%20Jauhari%20and%20Canfer%20Akbulut%20and%20Todor%20Davchev%20and%20Evgeny%20Gladchenko%20and%20David%20Madras%20and%20Aleksandr%20Chuklin%20and%20Tyrone%20Hill%20and%20Quan%20Yuan%20and%20Mukundan%20Madhavan%20and%20Luke%20Leonhard%20and%20Dylan%20Scandinaro%20and%20Qihang%20Chen%20and%20Ning%20Niu%20and%20Arthur%20Douillard%20and%20Bogdan%20Damoc%20and%20Yasumasa%20Onoe%20and%20Fabian%20Pedregosa%20and%20Fred%20Bertsch%20and%20Chas%20Leichner%20and%20Joseph%20Pagadora%20and%20Jonathan%20Malmaud%20and%20Sameera%20Ponda%20and%20Andy%20Twigg%20and%20Oleksii%20Duzhyi%20and%20Jingwei%20Shen%20and%20Miaosen%20Wang%20and%20Roopal%20Garg%20and%20Jing%20Chen%20and%20Utku%20Evci%20and%20Jonathan%20Lee%20and%20Leon%20Liu%20and%20Koji%20Kojima%20and%20Masa%20Yamaguchi%20and%20Arunkumar%20Rajendran%20and%20AJ%20Piergiovanni%20and%20Vinodh%20Kumar%20Rajendran%20and%20Marco%20Fornoni%20and%20Gabriel%20Ibagon%20and%20Harry%20Ragan%20and%20Sadh%20MNM%20Khan%20and%20John%20Blitzer%20and%20Andrew%20Bunner%20and%20Guan%20Sun%20and%20Takahiro%20Kosakai%20and%20Scott%20Lundberg%20and%20Ndidi%20Elue%20and%20Kelvin%20Guu%20and%20SK%20Park%20and%20Jane%20Park%20and%20Arunachalam%20Narayanaswamy%20and%20Chengda%20Wu%20and%20Jayaram%20Mudigonda%20and%20Trevor%20Cohn%20and%20Hairong%20Mu%20and%20Ravi%20Kumar%20and%20Laura%20Graesser%20and%20Yichi%20Zhang%20and%20Richard%20Killam%20and%20Vincent%20Zhuang%20and%20Mai%20Gim%C3%A9nez%20and%20Wael%20Al%20Jishi%20and%20Ruy%20Ley-Wild%20and%20Alex%20Zhai%20and%20Kazuki%20Osawa%20and%20Diego%20Cedillo%20and%20Jialu%20Liu%20and%20Mayank%20Upadhyay%20and%20Marcin%20Sieniek%20and%20Roshan%20Sharma%20and%20Tom%20Paine%20and%20Anelia%20Angelova%20and%20Sravanti%20Addepalli%20and%20Carolina%20Parada%20and%20Kingshuk%20Majumder%20and%20Avery%20Lamp%20and%20Sanjiv%20Kumar%20and%20Xiang%20Deng%20and%20Artiom%20Myaskovsky%20and%20Tea%20Saboli%C4%87%20and%20Jeffrey%20Dudek%20and%20Sarah%20York%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Jiazhong%20Nie%20and%20Dee%20Cattle%20and%20Alok%20Gunjan%20and%20Bilal%20Piot%20and%20Waleed%20Khawaja%20and%20Seojin%20Bang%20and%20Simon%20Wang%20and%20Siavash%20Khodadadeh%20and%20Raghavender%20R%20and%20Praynaa%20Rawlani%20and%20Richard%20Powell%20and%20Kevin%20Lee%20and%20Johannes%20Griesser%20and%20GS%20Oh%20and%20Cesar%20Magalhaes%20and%20Yujia%20Li%20and%20Simon%20Tokumine%20and%20Hadas%20Natalie%20Vogel%20and%20Dennis%20Hsu%20and%20Arturo%20BC%20and%20Disha%20Jindal%20and%20Matan%20Cohen%20and%20Zi%20Yang%20and%20Junwei%20Yuan%20and%20Dario%20de%20Cesare%20and%20Tony%20Bruguier%20and%20Jun%20Xu%20and%20Monica%20Roy%20and%20Alon%20Jacovi%20and%20Dan%20Belov%20and%20Rahul%20Arya%20and%20Phoenix%20Meadowlark%20and%20Shlomi%20Cohen-Ganor%20and%20Wenting%20Ye%20and%20Patrick%20Morris-Suzuki%20and%20Praseem%20Banzal%20and%20Gan%20Song%20and%20Pranavaraj%20Ponnuramu%20and%20Fred%20Zhang%20and%20George%20Scrivener%20and%20Salah%20Zaiem%20and%20Alif%20Raditya%20Rochman%20and%20Kehang%20Han%20and%20Badih%20Ghazi%20and%20Kate%20Lee%20and%20Shahar%20Drath%20and%20Daniel%20Suo%20and%20Antonious%20Girgis%20and%20Pradeep%20Shenoy%20and%20Duy%20Nguyen%20and%20Douglas%20Eck%20and%20Somit%20Gupta%20and%20Le%20Yan%20and%20Joao%20Carreira%20and%20Anmol%20Gulati%20and%20Ruoxin%20Sang%20and%20Daniil%20Mirylenka%20and%20Emma%20Cooney%20and%20Edward%20Chou%20and%20Mingyang%20Ling%20and%20Cindy%20Fan%20and%20Ben%20Coleman%20and%20Guilherme%20Tubone%20and%20Ravin%20Kumar%20and%20Jason%20Baldridge%20and%20Felix%20Hernandez-Campos%20and%20Angeliki%20Lazaridou%20and%20James%20Besley%20and%20Itay%20Yona%20and%20Neslihan%20Bulut%20and%20Quentin%20Wellens%20and%20AJ%20Pierigiovanni%20and%20Jasmine%20George%20and%20Richard%20Green%20and%20Pu%20Han%20and%20Connie%20Tao%20and%20Geoff%20Clark%20and%20Chong%20You%20and%20Abbas%20Abdolmaleki%20and%20Justin%20Fu%20and%20Tongzhou%20Chen%20and%20Ashwin%20Chaugule%20and%20Angad%20Chandorkar%20and%20Altaf%20Rahman%20and%20Will%20Thompson%20and%20Penporn%20Koanantakool%20and%20Mike%20Bernico%20and%20Jie%20Ren%20and%20Andrey%20Vlasov%20and%20Sergei%20Vassilvitskii%20and%20Maciej%20Kula%20and%20Yizhong%20Liang%20and%20Dahun%20Kim%20and%20Yangsibo%20Huang%20and%20Chengxi%20Ye%20and%20Dmitry%20Lepikhin%20and%20Wesley%20Helmholz&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%202.X%20model%20family%3A%20Gemini%202.5%20Pro%20and%0AGemini%202.5%20Flash%2C%20as%20well%20as%20our%20earlier%20Gemini%202.0%20Flash%20and%20Flash-Lite%0Amodels.%20Gemini%202.5%20Pro%20is%20our%20most%20capable%20model%20yet%2C%20achieving%20SoTA%0Aperformance%20on%20frontier%20coding%20and%20reasoning%20benchmarks.%20In%20addition%20to%20its%0Aincredible%20coding%20and%20reasoning%20skills%2C%20Gemini%202.5%20Pro%20is%20a%20thinking%20model%20that%0Aexcels%20at%20multimodal%20understanding%20and%20it%20is%20now%20able%20to%20process%20up%20to%203%20hours%0Aof%20video%20content.%20Its%20unique%20combination%20of%20long%20context%2C%20multimodal%20and%0Areasoning%20capabilities%20can%20be%20combined%20to%20unlock%20new%20agentic%20workflows.%20Gemini%0A2.5%20Flash%20provides%20excellent%20reasoning%20abilities%20at%20a%20fraction%20of%20the%20compute%0Aand%20latency%20requirements%20and%20Gemini%202.0%20Flash%20and%20Flash-Lite%20provide%20high%0Aperformance%20at%20low%20latency%20and%20cost.%20Taken%20together%2C%20the%20Gemini%202.X%20model%0Ageneration%20spans%20the%20full%20Pareto%20frontier%20of%20model%20capability%20vs%20cost%2C%20allowing%0Ausers%20to%20explore%20the%20boundaries%20of%20what%20is%20possible%20with%20complex%20agentic%0Aproblem%20solving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06261v5&entry.124074799=Read"},
{"title": "WeightLoRA: Keep Only Necessary Adapters", "author": "Andrey Veprikov and Vladimir Solodkin and Alexander Zyl and Andrey Savchenko and Aleksandr Beznosikov", "abstract": "  The widespread utilization of language models in modern applications is\ninconceivable without Parameter-Efficient Fine-Tuning techniques, such as\nlow-rank adaptation ($\\texttt{LoRA}$), which adds trainable adapters to\nselected layers. Although $\\texttt{LoRA}$ may obtain accurate solutions, it\nrequires significant memory to train large models and intuition on which layers\nto add adapters. In this paper, we propose a novel method,\n$\\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the\nmost critical $\\texttt{LoRA}$ heads throughout the optimization process. As a\nresult, we can significantly reduce the number of trainable parameters while\nmaintaining the capability to obtain consistent or even superior metric values.\nWe conduct experiments for a series of competitive benchmarks and DeBERTa,\nBART, and Llama models, comparing our method with different adaptive\napproaches. The experimental results demonstrate the efficacy of\n$\\texttt{WeightLoRA}$ and the superior performance of $\\texttt{WeightLoRA+}$ in\nalmost all cases.\n", "link": "http://arxiv.org/abs/2506.02724v3", "date": "2025-10-16", "relevancy": 2.3557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4723}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters&body=Title%3A%20WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters%0AAuthor%3A%20Andrey%20Veprikov%20and%20Vladimir%20Solodkin%20and%20Alexander%20Zyl%20and%20Andrey%20Savchenko%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20The%20widespread%20utilization%20of%20language%20models%20in%20modern%20applications%20is%0Ainconceivable%20without%20Parameter-Efficient%20Fine-Tuning%20techniques%2C%20such%20as%0Alow-rank%20adaptation%20%28%24%5Ctexttt%7BLoRA%7D%24%29%2C%20which%20adds%20trainable%20adapters%20to%0Aselected%20layers.%20Although%20%24%5Ctexttt%7BLoRA%7D%24%20may%20obtain%20accurate%20solutions%2C%20it%0Arequires%20significant%20memory%20to%20train%20large%20models%20and%20intuition%20on%20which%20layers%0Ato%20add%20adapters.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0A%24%5Ctexttt%7BWeightLoRA%7D%24%2C%20which%20overcomes%20this%20issue%20by%20adaptive%20selection%20of%20the%0Amost%20critical%20%24%5Ctexttt%7BLoRA%7D%24%20heads%20throughout%20the%20optimization%20process.%20As%20a%0Aresult%2C%20we%20can%20significantly%20reduce%20the%20number%20of%20trainable%20parameters%20while%0Amaintaining%20the%20capability%20to%20obtain%20consistent%20or%20even%20superior%20metric%20values.%0AWe%20conduct%20experiments%20for%20a%20series%20of%20competitive%20benchmarks%20and%20DeBERTa%2C%0ABART%2C%20and%20Llama%20models%2C%20comparing%20our%20method%20with%20different%20adaptive%0Aapproaches.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%0A%24%5Ctexttt%7BWeightLoRA%7D%24%20and%20the%20superior%20performance%20of%20%24%5Ctexttt%7BWeightLoRA%2B%7D%24%20in%0Aalmost%20all%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02724v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeightLoRA%253A%2520Keep%2520Only%2520Necessary%2520Adapters%26entry.906535625%3DAndrey%2520Veprikov%2520and%2520Vladimir%2520Solodkin%2520and%2520Alexander%2520Zyl%2520and%2520Andrey%2520Savchenko%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520The%2520widespread%2520utilization%2520of%2520language%2520models%2520in%2520modern%2520applications%2520is%250Ainconceivable%2520without%2520Parameter-Efficient%2520Fine-Tuning%2520techniques%252C%2520such%2520as%250Alow-rank%2520adaptation%2520%2528%2524%255Ctexttt%257BLoRA%257D%2524%2529%252C%2520which%2520adds%2520trainable%2520adapters%2520to%250Aselected%2520layers.%2520Although%2520%2524%255Ctexttt%257BLoRA%257D%2524%2520may%2520obtain%2520accurate%2520solutions%252C%2520it%250Arequires%2520significant%2520memory%2520to%2520train%2520large%2520models%2520and%2520intuition%2520on%2520which%2520layers%250Ato%2520add%2520adapters.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250A%2524%255Ctexttt%257BWeightLoRA%257D%2524%252C%2520which%2520overcomes%2520this%2520issue%2520by%2520adaptive%2520selection%2520of%2520the%250Amost%2520critical%2520%2524%255Ctexttt%257BLoRA%257D%2524%2520heads%2520throughout%2520the%2520optimization%2520process.%2520As%2520a%250Aresult%252C%2520we%2520can%2520significantly%2520reduce%2520the%2520number%2520of%2520trainable%2520parameters%2520while%250Amaintaining%2520the%2520capability%2520to%2520obtain%2520consistent%2520or%2520even%2520superior%2520metric%2520values.%250AWe%2520conduct%2520experiments%2520for%2520a%2520series%2520of%2520competitive%2520benchmarks%2520and%2520DeBERTa%252C%250ABART%252C%2520and%2520Llama%2520models%252C%2520comparing%2520our%2520method%2520with%2520different%2520adaptive%250Aapproaches.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%250A%2524%255Ctexttt%257BWeightLoRA%257D%2524%2520and%2520the%2520superior%2520performance%2520of%2520%2524%255Ctexttt%257BWeightLoRA%252B%257D%2524%2520in%250Aalmost%2520all%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02724v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters&entry.906535625=Andrey%20Veprikov%20and%20Vladimir%20Solodkin%20and%20Alexander%20Zyl%20and%20Andrey%20Savchenko%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20The%20widespread%20utilization%20of%20language%20models%20in%20modern%20applications%20is%0Ainconceivable%20without%20Parameter-Efficient%20Fine-Tuning%20techniques%2C%20such%20as%0Alow-rank%20adaptation%20%28%24%5Ctexttt%7BLoRA%7D%24%29%2C%20which%20adds%20trainable%20adapters%20to%0Aselected%20layers.%20Although%20%24%5Ctexttt%7BLoRA%7D%24%20may%20obtain%20accurate%20solutions%2C%20it%0Arequires%20significant%20memory%20to%20train%20large%20models%20and%20intuition%20on%20which%20layers%0Ato%20add%20adapters.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0A%24%5Ctexttt%7BWeightLoRA%7D%24%2C%20which%20overcomes%20this%20issue%20by%20adaptive%20selection%20of%20the%0Amost%20critical%20%24%5Ctexttt%7BLoRA%7D%24%20heads%20throughout%20the%20optimization%20process.%20As%20a%0Aresult%2C%20we%20can%20significantly%20reduce%20the%20number%20of%20trainable%20parameters%20while%0Amaintaining%20the%20capability%20to%20obtain%20consistent%20or%20even%20superior%20metric%20values.%0AWe%20conduct%20experiments%20for%20a%20series%20of%20competitive%20benchmarks%20and%20DeBERTa%2C%0ABART%2C%20and%20Llama%20models%2C%20comparing%20our%20method%20with%20different%20adaptive%0Aapproaches.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%0A%24%5Ctexttt%7BWeightLoRA%7D%24%20and%20the%20superior%20performance%20of%20%24%5Ctexttt%7BWeightLoRA%2B%7D%24%20in%0Aalmost%20all%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02724v3&entry.124074799=Read"},
{"title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis", "author": "Ayush Khaitan and Vijay Ganesh", "abstract": "  Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians.\n", "link": "http://arxiv.org/abs/2510.12350v2", "date": "2025-10-16", "relevancy": 2.3476, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4732}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20O-Forge%3A%20An%20LLM%20%2B%20Computer%20Algebra%20Framework%20for%20Asymptotic%20Analysis&body=Title%3A%20O-Forge%3A%20An%20LLM%20%2B%20Computer%20Algebra%20Framework%20for%20Asymptotic%20Analysis%0AAuthor%3A%20Ayush%20Khaitan%20and%20Vijay%20Ganesh%0AAbstract%3A%20%20%20Large%20language%20models%20have%20recently%20demonstrated%20advanced%20capabilities%20in%0Asolving%20IMO%20and%20Putnam%20problems%3B%20yet%20their%20role%20in%20research%20mathematics%20has%0Aremained%20fairly%20limited.%20The%20key%20difficulty%20is%20verification%3A%20suggested%20proofs%0Amay%20look%20plausible%2C%20but%20cannot%20be%20trusted%20without%20rigorous%20checking.%20We%20present%0Aa%20framework%2C%20called%20LLM%2BCAS%2C%20and%20an%20associated%20tool%2C%20O-Forge%2C%20that%20couples%0Afrontier%20LLMs%20with%20a%20computer%20algebra%20systems%20%28CAS%29%20in%20an%20In-Context%20Symbolic%0AFeedback%20loop%20to%20produce%20proofs%20that%20are%20both%20creative%20and%20symbolically%0Averified.%20Our%20focus%20is%20on%20asymptotic%20inequalities%2C%20a%20topic%20that%20often%20involves%0Adifficult%20proofs%20and%20appropriate%20decomposition%20of%20the%20domain%20into%20the%20%22right%22%0Asubdomains.%20Many%20mathematicians%2C%20including%20Terry%20Tao%2C%20have%20suggested%20that%20using%0AAI%20tools%20to%20find%20the%20right%20decompositions%20can%20be%20very%20useful%20for%20research-level%0Aasymptotic%20analysis.%20In%20this%20paper%2C%20we%20show%20that%20our%20framework%20LLM%2BCAS%20turns%0Aout%20to%20be%20remarkably%20effective%20at%20proposing%20such%20decompositions%20via%20a%0Acombination%20of%20a%20frontier%20LLM%20and%20a%20CAS.%20More%20precisely%2C%20we%20use%20an%20LLM%20to%0Asuggest%20domain%20decomposition%2C%20and%20a%20CAS%20%28such%20as%20Mathematica%29%20that%20provides%20a%0Averification%20of%20each%20piece%20axiomatically.%20Using%20this%20loop%2C%20we%20answer%20a%20question%0Aposed%20by%20Terence%20Tao%3A%20whether%20LLMs%20coupled%20with%20a%20verifier%20can%20be%20used%20to%20help%0Aprove%20intricate%20asymptotic%20inequalities.%20More%20broadly%2C%20we%20show%20how%20AI%20can%20move%0Abeyond%20contest%20math%20towards%20research-level%20tools%20for%20professional%0Amathematicians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12350v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DO-Forge%253A%2520An%2520LLM%2520%252B%2520Computer%2520Algebra%2520Framework%2520for%2520Asymptotic%2520Analysis%26entry.906535625%3DAyush%2520Khaitan%2520and%2520Vijay%2520Ganesh%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520recently%2520demonstrated%2520advanced%2520capabilities%2520in%250Asolving%2520IMO%2520and%2520Putnam%2520problems%253B%2520yet%2520their%2520role%2520in%2520research%2520mathematics%2520has%250Aremained%2520fairly%2520limited.%2520The%2520key%2520difficulty%2520is%2520verification%253A%2520suggested%2520proofs%250Amay%2520look%2520plausible%252C%2520but%2520cannot%2520be%2520trusted%2520without%2520rigorous%2520checking.%2520We%2520present%250Aa%2520framework%252C%2520called%2520LLM%252BCAS%252C%2520and%2520an%2520associated%2520tool%252C%2520O-Forge%252C%2520that%2520couples%250Afrontier%2520LLMs%2520with%2520a%2520computer%2520algebra%2520systems%2520%2528CAS%2529%2520in%2520an%2520In-Context%2520Symbolic%250AFeedback%2520loop%2520to%2520produce%2520proofs%2520that%2520are%2520both%2520creative%2520and%2520symbolically%250Averified.%2520Our%2520focus%2520is%2520on%2520asymptotic%2520inequalities%252C%2520a%2520topic%2520that%2520often%2520involves%250Adifficult%2520proofs%2520and%2520appropriate%2520decomposition%2520of%2520the%2520domain%2520into%2520the%2520%2522right%2522%250Asubdomains.%2520Many%2520mathematicians%252C%2520including%2520Terry%2520Tao%252C%2520have%2520suggested%2520that%2520using%250AAI%2520tools%2520to%2520find%2520the%2520right%2520decompositions%2520can%2520be%2520very%2520useful%2520for%2520research-level%250Aasymptotic%2520analysis.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520our%2520framework%2520LLM%252BCAS%2520turns%250Aout%2520to%2520be%2520remarkably%2520effective%2520at%2520proposing%2520such%2520decompositions%2520via%2520a%250Acombination%2520of%2520a%2520frontier%2520LLM%2520and%2520a%2520CAS.%2520More%2520precisely%252C%2520we%2520use%2520an%2520LLM%2520to%250Asuggest%2520domain%2520decomposition%252C%2520and%2520a%2520CAS%2520%2528such%2520as%2520Mathematica%2529%2520that%2520provides%2520a%250Averification%2520of%2520each%2520piece%2520axiomatically.%2520Using%2520this%2520loop%252C%2520we%2520answer%2520a%2520question%250Aposed%2520by%2520Terence%2520Tao%253A%2520whether%2520LLMs%2520coupled%2520with%2520a%2520verifier%2520can%2520be%2520used%2520to%2520help%250Aprove%2520intricate%2520asymptotic%2520inequalities.%2520More%2520broadly%252C%2520we%2520show%2520how%2520AI%2520can%2520move%250Abeyond%2520contest%2520math%2520towards%2520research-level%2520tools%2520for%2520professional%250Amathematicians.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12350v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=O-Forge%3A%20An%20LLM%20%2B%20Computer%20Algebra%20Framework%20for%20Asymptotic%20Analysis&entry.906535625=Ayush%20Khaitan%20and%20Vijay%20Ganesh&entry.1292438233=%20%20Large%20language%20models%20have%20recently%20demonstrated%20advanced%20capabilities%20in%0Asolving%20IMO%20and%20Putnam%20problems%3B%20yet%20their%20role%20in%20research%20mathematics%20has%0Aremained%20fairly%20limited.%20The%20key%20difficulty%20is%20verification%3A%20suggested%20proofs%0Amay%20look%20plausible%2C%20but%20cannot%20be%20trusted%20without%20rigorous%20checking.%20We%20present%0Aa%20framework%2C%20called%20LLM%2BCAS%2C%20and%20an%20associated%20tool%2C%20O-Forge%2C%20that%20couples%0Afrontier%20LLMs%20with%20a%20computer%20algebra%20systems%20%28CAS%29%20in%20an%20In-Context%20Symbolic%0AFeedback%20loop%20to%20produce%20proofs%20that%20are%20both%20creative%20and%20symbolically%0Averified.%20Our%20focus%20is%20on%20asymptotic%20inequalities%2C%20a%20topic%20that%20often%20involves%0Adifficult%20proofs%20and%20appropriate%20decomposition%20of%20the%20domain%20into%20the%20%22right%22%0Asubdomains.%20Many%20mathematicians%2C%20including%20Terry%20Tao%2C%20have%20suggested%20that%20using%0AAI%20tools%20to%20find%20the%20right%20decompositions%20can%20be%20very%20useful%20for%20research-level%0Aasymptotic%20analysis.%20In%20this%20paper%2C%20we%20show%20that%20our%20framework%20LLM%2BCAS%20turns%0Aout%20to%20be%20remarkably%20effective%20at%20proposing%20such%20decompositions%20via%20a%0Acombination%20of%20a%20frontier%20LLM%20and%20a%20CAS.%20More%20precisely%2C%20we%20use%20an%20LLM%20to%0Asuggest%20domain%20decomposition%2C%20and%20a%20CAS%20%28such%20as%20Mathematica%29%20that%20provides%20a%0Averification%20of%20each%20piece%20axiomatically.%20Using%20this%20loop%2C%20we%20answer%20a%20question%0Aposed%20by%20Terence%20Tao%3A%20whether%20LLMs%20coupled%20with%20a%20verifier%20can%20be%20used%20to%20help%0Aprove%20intricate%20asymptotic%20inequalities.%20More%20broadly%2C%20we%20show%20how%20AI%20can%20move%0Abeyond%20contest%20math%20towards%20research-level%20tools%20for%20professional%0Amathematicians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12350v2&entry.124074799=Read"},
{"title": "GOPLA: Generalizable Object Placement Learning via Synthetic\n  Augmentation of Human Arrangement", "author": "Yao Zhong and Hanzhi Chen and Simon Schaefer and Anran Zhang and Stefan Leutenegger", "abstract": "  Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.\n", "link": "http://arxiv.org/abs/2510.14627v1", "date": "2025-10-16", "relevancy": 2.3476, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6179}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5852}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOPLA%3A%20Generalizable%20Object%20Placement%20Learning%20via%20Synthetic%0A%20%20Augmentation%20of%20Human%20Arrangement&body=Title%3A%20GOPLA%3A%20Generalizable%20Object%20Placement%20Learning%20via%20Synthetic%0A%20%20Augmentation%20of%20Human%20Arrangement%0AAuthor%3A%20Yao%20Zhong%20and%20Hanzhi%20Chen%20and%20Simon%20Schaefer%20and%20Anran%20Zhang%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20Robots%20are%20expected%20to%20serve%20as%20intelligent%20assistants%2C%20helping%20humans%20with%0Aeveryday%20household%20organization.%20A%20central%20challenge%20in%20this%20setting%20is%20the%0Atask%20of%20object%20placement%2C%20which%20requires%20reasoning%20about%20both%20semantic%0Apreferences%20%28e.g.%2C%20common-sense%20object%20relations%29%20and%20geometric%20feasibility%0A%28e.g.%2C%20collision%20avoidance%29.%20We%20present%20GOPLA%2C%20a%20hierarchical%20framework%20that%0Alearns%20generalizable%20object%20placement%20from%20augmented%20human%20demonstrations.%20A%0Amulti-modal%20large%20language%20model%20translates%20human%20instructions%20and%20visual%0Ainputs%20into%20structured%20plans%20that%20specify%20pairwise%20object%20relationships.%20These%0Aplans%20are%20then%20converted%20into%203D%20affordance%20maps%20with%20geometric%20common%20sense%20by%0Aa%20spatial%20mapper%2C%20while%20a%20diffusion-based%20planner%20generates%20placement%20poses%0Aguided%20by%20test-time%20costs%2C%20considering%20multi-plan%20distributions%20and%20collision%0Aavoidance.%20To%20overcome%20data%20scarcity%2C%20we%20introduce%20a%20scalable%20pipeline%20that%0Aexpands%20human%20placement%20demonstrations%20into%20diverse%20synthetic%20training%20data.%0AExtensive%20experiments%20show%20that%20our%20approach%20improves%20placement%20success%20rates%0Aby%2030.04%20percentage%20points%20over%20the%20runner-up%2C%20evaluated%20on%20positioning%0Aaccuracy%20and%20physical%20plausibility%2C%20demonstrating%20strong%20generalization%20across%0Aa%20wide%20range%20of%20real-world%20robotic%20placement%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOPLA%253A%2520Generalizable%2520Object%2520Placement%2520Learning%2520via%2520Synthetic%250A%2520%2520Augmentation%2520of%2520Human%2520Arrangement%26entry.906535625%3DYao%2520Zhong%2520and%2520Hanzhi%2520Chen%2520and%2520Simon%2520Schaefer%2520and%2520Anran%2520Zhang%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3D%2520%2520Robots%2520are%2520expected%2520to%2520serve%2520as%2520intelligent%2520assistants%252C%2520helping%2520humans%2520with%250Aeveryday%2520household%2520organization.%2520A%2520central%2520challenge%2520in%2520this%2520setting%2520is%2520the%250Atask%2520of%2520object%2520placement%252C%2520which%2520requires%2520reasoning%2520about%2520both%2520semantic%250Apreferences%2520%2528e.g.%252C%2520common-sense%2520object%2520relations%2529%2520and%2520geometric%2520feasibility%250A%2528e.g.%252C%2520collision%2520avoidance%2529.%2520We%2520present%2520GOPLA%252C%2520a%2520hierarchical%2520framework%2520that%250Alearns%2520generalizable%2520object%2520placement%2520from%2520augmented%2520human%2520demonstrations.%2520A%250Amulti-modal%2520large%2520language%2520model%2520translates%2520human%2520instructions%2520and%2520visual%250Ainputs%2520into%2520structured%2520plans%2520that%2520specify%2520pairwise%2520object%2520relationships.%2520These%250Aplans%2520are%2520then%2520converted%2520into%25203D%2520affordance%2520maps%2520with%2520geometric%2520common%2520sense%2520by%250Aa%2520spatial%2520mapper%252C%2520while%2520a%2520diffusion-based%2520planner%2520generates%2520placement%2520poses%250Aguided%2520by%2520test-time%2520costs%252C%2520considering%2520multi-plan%2520distributions%2520and%2520collision%250Aavoidance.%2520To%2520overcome%2520data%2520scarcity%252C%2520we%2520introduce%2520a%2520scalable%2520pipeline%2520that%250Aexpands%2520human%2520placement%2520demonstrations%2520into%2520diverse%2520synthetic%2520training%2520data.%250AExtensive%2520experiments%2520show%2520that%2520our%2520approach%2520improves%2520placement%2520success%2520rates%250Aby%252030.04%2520percentage%2520points%2520over%2520the%2520runner-up%252C%2520evaluated%2520on%2520positioning%250Aaccuracy%2520and%2520physical%2520plausibility%252C%2520demonstrating%2520strong%2520generalization%2520across%250Aa%2520wide%2520range%2520of%2520real-world%2520robotic%2520placement%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOPLA%3A%20Generalizable%20Object%20Placement%20Learning%20via%20Synthetic%0A%20%20Augmentation%20of%20Human%20Arrangement&entry.906535625=Yao%20Zhong%20and%20Hanzhi%20Chen%20and%20Simon%20Schaefer%20and%20Anran%20Zhang%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20Robots%20are%20expected%20to%20serve%20as%20intelligent%20assistants%2C%20helping%20humans%20with%0Aeveryday%20household%20organization.%20A%20central%20challenge%20in%20this%20setting%20is%20the%0Atask%20of%20object%20placement%2C%20which%20requires%20reasoning%20about%20both%20semantic%0Apreferences%20%28e.g.%2C%20common-sense%20object%20relations%29%20and%20geometric%20feasibility%0A%28e.g.%2C%20collision%20avoidance%29.%20We%20present%20GOPLA%2C%20a%20hierarchical%20framework%20that%0Alearns%20generalizable%20object%20placement%20from%20augmented%20human%20demonstrations.%20A%0Amulti-modal%20large%20language%20model%20translates%20human%20instructions%20and%20visual%0Ainputs%20into%20structured%20plans%20that%20specify%20pairwise%20object%20relationships.%20These%0Aplans%20are%20then%20converted%20into%203D%20affordance%20maps%20with%20geometric%20common%20sense%20by%0Aa%20spatial%20mapper%2C%20while%20a%20diffusion-based%20planner%20generates%20placement%20poses%0Aguided%20by%20test-time%20costs%2C%20considering%20multi-plan%20distributions%20and%20collision%0Aavoidance.%20To%20overcome%20data%20scarcity%2C%20we%20introduce%20a%20scalable%20pipeline%20that%0Aexpands%20human%20placement%20demonstrations%20into%20diverse%20synthetic%20training%20data.%0AExtensive%20experiments%20show%20that%20our%20approach%20improves%20placement%20success%20rates%0Aby%2030.04%20percentage%20points%20over%20the%20runner-up%2C%20evaluated%20on%20positioning%0Aaccuracy%20and%20physical%20plausibility%2C%20demonstrating%20strong%20generalization%20across%0Aa%20wide%20range%20of%20real-world%20robotic%20placement%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14627v1&entry.124074799=Read"},
{"title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation", "author": "Han Zhao and Jiaxuan Zhang and Wenxuan Song and Pengxiang Ding and Donglin Wang", "abstract": "  Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.\n", "link": "http://arxiv.org/abs/2510.14902v1", "date": "2025-10-16", "relevancy": 2.3206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA%5E2%3A%20Empowering%20Vision-Language-Action%20Models%20with%20an%20Agentic%0A%20%20Framework%20for%20Unseen%20Concept%20Manipulation&body=Title%3A%20VLA%5E2%3A%20Empowering%20Vision-Language-Action%20Models%20with%20an%20Agentic%0A%20%20Framework%20for%20Unseen%20Concept%20Manipulation%0AAuthor%3A%20Han%20Zhao%20and%20Jiaxuan%20Zhang%20and%20Wenxuan%20Song%20and%20Pengxiang%20Ding%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20Current%20vision-language-action%20%28VLA%29%20models%2C%20pre-trained%20on%20large-scale%0Arobotic%20data%2C%20exhibit%20strong%20multi-task%20capabilities%20and%20generalize%20well%20to%0Avariations%20in%20visual%20and%20language%20instructions%20for%20manipulation.%20However%2C%20their%0Asuccess%20rate%20drops%20significantly%20when%20faced%20with%20object%20concepts%20outside%20the%0Atraining%20data%2C%20such%20as%20unseen%20object%20descriptions%20and%20textures%20in%20the%20dataset.%0ATo%20address%20this%2C%20we%20propose%20a%20novel%20agentic%20framework%2C%20VLA%5E2%2C%20which%20leverages%0AOpenVLA%20as%20the%20execution%20backbone%20and%20effectively%20leverages%20external%20modules%0Asuch%20as%20web%20retrieval%20and%20object%20detection%20to%20provide%20visual%20and%20textual%0Aknowledge%20about%20target%20objects%20to%20the%20VLA.%20This%20approach%20mitigates%0Ageneralization%20failure%20when%20handling%20out-of-distribution%20objects.%20Based%20on%20the%0ALIBERO%20simulation%20environment%2C%20we%20introduced%20novel%20objects%20and%20object%0Adescriptions%20to%20construct%20a%20new%20evaluation%20benchmark%20with%20three%20difficulty%0Alevels%20to%20test%20the%20effectiveness%20of%20our%20method.%20Our%20framework%20successfully%0Aoutperformed%20the%20current%20state-of-the-art%20models%20on%20our%20designed%20hard-level%0Ageneralization%20benchmark.%20Compared%20to%20the%20standalone%20OpenVLA%20baseline%2C%20VLA%5E2%0Aachieves%20a%2044.2%25%20improvement%20in%20the%20success%20rate%20in%20the%20hard-level%20benchmark%0Aand%20an%20average%20improvement%20of%2020.2%25%20in%20all%20customized%20environments%20without%20any%0Aperformance%20degradation%20on%20in-domain%20tasks.%20Project%20website%3A%0Ahttps%3A//vla-2.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA%255E2%253A%2520Empowering%2520Vision-Language-Action%2520Models%2520with%2520an%2520Agentic%250A%2520%2520Framework%2520for%2520Unseen%2520Concept%2520Manipulation%26entry.906535625%3DHan%2520Zhao%2520and%2520Jiaxuan%2520Zhang%2520and%2520Wenxuan%2520Song%2520and%2520Pengxiang%2520Ding%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520Current%2520vision-language-action%2520%2528VLA%2529%2520models%252C%2520pre-trained%2520on%2520large-scale%250Arobotic%2520data%252C%2520exhibit%2520strong%2520multi-task%2520capabilities%2520and%2520generalize%2520well%2520to%250Avariations%2520in%2520visual%2520and%2520language%2520instructions%2520for%2520manipulation.%2520However%252C%2520their%250Asuccess%2520rate%2520drops%2520significantly%2520when%2520faced%2520with%2520object%2520concepts%2520outside%2520the%250Atraining%2520data%252C%2520such%2520as%2520unseen%2520object%2520descriptions%2520and%2520textures%2520in%2520the%2520dataset.%250ATo%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520agentic%2520framework%252C%2520VLA%255E2%252C%2520which%2520leverages%250AOpenVLA%2520as%2520the%2520execution%2520backbone%2520and%2520effectively%2520leverages%2520external%2520modules%250Asuch%2520as%2520web%2520retrieval%2520and%2520object%2520detection%2520to%2520provide%2520visual%2520and%2520textual%250Aknowledge%2520about%2520target%2520objects%2520to%2520the%2520VLA.%2520This%2520approach%2520mitigates%250Ageneralization%2520failure%2520when%2520handling%2520out-of-distribution%2520objects.%2520Based%2520on%2520the%250ALIBERO%2520simulation%2520environment%252C%2520we%2520introduced%2520novel%2520objects%2520and%2520object%250Adescriptions%2520to%2520construct%2520a%2520new%2520evaluation%2520benchmark%2520with%2520three%2520difficulty%250Alevels%2520to%2520test%2520the%2520effectiveness%2520of%2520our%2520method.%2520Our%2520framework%2520successfully%250Aoutperformed%2520the%2520current%2520state-of-the-art%2520models%2520on%2520our%2520designed%2520hard-level%250Ageneralization%2520benchmark.%2520Compared%2520to%2520the%2520standalone%2520OpenVLA%2520baseline%252C%2520VLA%255E2%250Aachieves%2520a%252044.2%2525%2520improvement%2520in%2520the%2520success%2520rate%2520in%2520the%2520hard-level%2520benchmark%250Aand%2520an%2520average%2520improvement%2520of%252020.2%2525%2520in%2520all%2520customized%2520environments%2520without%2520any%250Aperformance%2520degradation%2520on%2520in-domain%2520tasks.%2520Project%2520website%253A%250Ahttps%253A//vla-2.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA%5E2%3A%20Empowering%20Vision-Language-Action%20Models%20with%20an%20Agentic%0A%20%20Framework%20for%20Unseen%20Concept%20Manipulation&entry.906535625=Han%20Zhao%20and%20Jiaxuan%20Zhang%20and%20Wenxuan%20Song%20and%20Pengxiang%20Ding%20and%20Donglin%20Wang&entry.1292438233=%20%20Current%20vision-language-action%20%28VLA%29%20models%2C%20pre-trained%20on%20large-scale%0Arobotic%20data%2C%20exhibit%20strong%20multi-task%20capabilities%20and%20generalize%20well%20to%0Avariations%20in%20visual%20and%20language%20instructions%20for%20manipulation.%20However%2C%20their%0Asuccess%20rate%20drops%20significantly%20when%20faced%20with%20object%20concepts%20outside%20the%0Atraining%20data%2C%20such%20as%20unseen%20object%20descriptions%20and%20textures%20in%20the%20dataset.%0ATo%20address%20this%2C%20we%20propose%20a%20novel%20agentic%20framework%2C%20VLA%5E2%2C%20which%20leverages%0AOpenVLA%20as%20the%20execution%20backbone%20and%20effectively%20leverages%20external%20modules%0Asuch%20as%20web%20retrieval%20and%20object%20detection%20to%20provide%20visual%20and%20textual%0Aknowledge%20about%20target%20objects%20to%20the%20VLA.%20This%20approach%20mitigates%0Ageneralization%20failure%20when%20handling%20out-of-distribution%20objects.%20Based%20on%20the%0ALIBERO%20simulation%20environment%2C%20we%20introduced%20novel%20objects%20and%20object%0Adescriptions%20to%20construct%20a%20new%20evaluation%20benchmark%20with%20three%20difficulty%0Alevels%20to%20test%20the%20effectiveness%20of%20our%20method.%20Our%20framework%20successfully%0Aoutperformed%20the%20current%20state-of-the-art%20models%20on%20our%20designed%20hard-level%0Ageneralization%20benchmark.%20Compared%20to%20the%20standalone%20OpenVLA%20baseline%2C%20VLA%5E2%0Aachieves%20a%2044.2%25%20improvement%20in%20the%20success%20rate%20in%20the%20hard-level%20benchmark%0Aand%20an%20average%20improvement%20of%2020.2%25%20in%20all%20customized%20environments%20without%20any%0Aperformance%20degradation%20on%20in-domain%20tasks.%20Project%20website%3A%0Ahttps%3A//vla-2.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14902v1&entry.124074799=Read"},
{"title": "Redundancy-Aware Test-Time Graph Out-of-Distribution Detection", "author": "Yue Hou and He Zhu and Ruomei Liu and Yingke Su and Junran Wu and Ke Xu", "abstract": "  Distributional discrepancy between training and test data can lead models to\nmake inaccurate predictions when encountering out-of-distribution (OOD) samples\nin real-world applications. Although existing graph OOD detection methods\nleverage data-centric techniques to extract effective representations, their\nperformance remains compromised by structural redundancy that induces semantic\nshifts. To address this dilemma, we propose RedOUT, an unsupervised framework\nthat integrates structural entropy into test-time OOD detection for graph\nclassification. Concretely, we introduce the Redundancy-aware Graph Information\nBottleneck (ReGIB) and decompose the objective into essential information and\nirrelevant redundancy. By minimizing structural entropy, the decoupled\nredundancy is reduced, and theoretically grounded upper and lower bounds are\nproposed for optimization. Extensive experiments on real-world datasets\ndemonstrate the superior performance of RedOUT on OOD detection. Specifically,\nour method achieves an average improvement of 6.7%, significantly surpassing\nthe best competitor by 17.3% on the ClinTox/LIPO dataset pair.\n", "link": "http://arxiv.org/abs/2510.14562v1", "date": "2025-10-16", "relevancy": 2.3119, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4636}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redundancy-Aware%20Test-Time%20Graph%20Out-of-Distribution%20Detection&body=Title%3A%20Redundancy-Aware%20Test-Time%20Graph%20Out-of-Distribution%20Detection%0AAuthor%3A%20Yue%20Hou%20and%20He%20Zhu%20and%20Ruomei%20Liu%20and%20Yingke%20Su%20and%20Junran%20Wu%20and%20Ke%20Xu%0AAbstract%3A%20%20%20Distributional%20discrepancy%20between%20training%20and%20test%20data%20can%20lead%20models%20to%0Amake%20inaccurate%20predictions%20when%20encountering%20out-of-distribution%20%28OOD%29%20samples%0Ain%20real-world%20applications.%20Although%20existing%20graph%20OOD%20detection%20methods%0Aleverage%20data-centric%20techniques%20to%20extract%20effective%20representations%2C%20their%0Aperformance%20remains%20compromised%20by%20structural%20redundancy%20that%20induces%20semantic%0Ashifts.%20To%20address%20this%20dilemma%2C%20we%20propose%20RedOUT%2C%20an%20unsupervised%20framework%0Athat%20integrates%20structural%20entropy%20into%20test-time%20OOD%20detection%20for%20graph%0Aclassification.%20Concretely%2C%20we%20introduce%20the%20Redundancy-aware%20Graph%20Information%0ABottleneck%20%28ReGIB%29%20and%20decompose%20the%20objective%20into%20essential%20information%20and%0Airrelevant%20redundancy.%20By%20minimizing%20structural%20entropy%2C%20the%20decoupled%0Aredundancy%20is%20reduced%2C%20and%20theoretically%20grounded%20upper%20and%20lower%20bounds%20are%0Aproposed%20for%20optimization.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20RedOUT%20on%20OOD%20detection.%20Specifically%2C%0Aour%20method%20achieves%20an%20average%20improvement%20of%206.7%25%2C%20significantly%20surpassing%0Athe%20best%20competitor%20by%2017.3%25%20on%20the%20ClinTox/LIPO%20dataset%20pair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedundancy-Aware%2520Test-Time%2520Graph%2520Out-of-Distribution%2520Detection%26entry.906535625%3DYue%2520Hou%2520and%2520He%2520Zhu%2520and%2520Ruomei%2520Liu%2520and%2520Yingke%2520Su%2520and%2520Junran%2520Wu%2520and%2520Ke%2520Xu%26entry.1292438233%3D%2520%2520Distributional%2520discrepancy%2520between%2520training%2520and%2520test%2520data%2520can%2520lead%2520models%2520to%250Amake%2520inaccurate%2520predictions%2520when%2520encountering%2520out-of-distribution%2520%2528OOD%2529%2520samples%250Ain%2520real-world%2520applications.%2520Although%2520existing%2520graph%2520OOD%2520detection%2520methods%250Aleverage%2520data-centric%2520techniques%2520to%2520extract%2520effective%2520representations%252C%2520their%250Aperformance%2520remains%2520compromised%2520by%2520structural%2520redundancy%2520that%2520induces%2520semantic%250Ashifts.%2520To%2520address%2520this%2520dilemma%252C%2520we%2520propose%2520RedOUT%252C%2520an%2520unsupervised%2520framework%250Athat%2520integrates%2520structural%2520entropy%2520into%2520test-time%2520OOD%2520detection%2520for%2520graph%250Aclassification.%2520Concretely%252C%2520we%2520introduce%2520the%2520Redundancy-aware%2520Graph%2520Information%250ABottleneck%2520%2528ReGIB%2529%2520and%2520decompose%2520the%2520objective%2520into%2520essential%2520information%2520and%250Airrelevant%2520redundancy.%2520By%2520minimizing%2520structural%2520entropy%252C%2520the%2520decoupled%250Aredundancy%2520is%2520reduced%252C%2520and%2520theoretically%2520grounded%2520upper%2520and%2520lower%2520bounds%2520are%250Aproposed%2520for%2520optimization.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520RedOUT%2520on%2520OOD%2520detection.%2520Specifically%252C%250Aour%2520method%2520achieves%2520an%2520average%2520improvement%2520of%25206.7%2525%252C%2520significantly%2520surpassing%250Athe%2520best%2520competitor%2520by%252017.3%2525%2520on%2520the%2520ClinTox/LIPO%2520dataset%2520pair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redundancy-Aware%20Test-Time%20Graph%20Out-of-Distribution%20Detection&entry.906535625=Yue%20Hou%20and%20He%20Zhu%20and%20Ruomei%20Liu%20and%20Yingke%20Su%20and%20Junran%20Wu%20and%20Ke%20Xu&entry.1292438233=%20%20Distributional%20discrepancy%20between%20training%20and%20test%20data%20can%20lead%20models%20to%0Amake%20inaccurate%20predictions%20when%20encountering%20out-of-distribution%20%28OOD%29%20samples%0Ain%20real-world%20applications.%20Although%20existing%20graph%20OOD%20detection%20methods%0Aleverage%20data-centric%20techniques%20to%20extract%20effective%20representations%2C%20their%0Aperformance%20remains%20compromised%20by%20structural%20redundancy%20that%20induces%20semantic%0Ashifts.%20To%20address%20this%20dilemma%2C%20we%20propose%20RedOUT%2C%20an%20unsupervised%20framework%0Athat%20integrates%20structural%20entropy%20into%20test-time%20OOD%20detection%20for%20graph%0Aclassification.%20Concretely%2C%20we%20introduce%20the%20Redundancy-aware%20Graph%20Information%0ABottleneck%20%28ReGIB%29%20and%20decompose%20the%20objective%20into%20essential%20information%20and%0Airrelevant%20redundancy.%20By%20minimizing%20structural%20entropy%2C%20the%20decoupled%0Aredundancy%20is%20reduced%2C%20and%20theoretically%20grounded%20upper%20and%20lower%20bounds%20are%0Aproposed%20for%20optimization.%20Extensive%20experiments%20on%20real-world%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20RedOUT%20on%20OOD%20detection.%20Specifically%2C%0Aour%20method%20achieves%20an%20average%20improvement%20of%206.7%25%2C%20significantly%20surpassing%0Athe%20best%20competitor%20by%2017.3%25%20on%20the%20ClinTox/LIPO%20dataset%20pair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14562v1&entry.124074799=Read"},
{"title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained\n  MLLM Perception", "author": "Yuheng Shi and Xiaohuan Pei and Minjing Dong and Chang Xu", "abstract": "  Multimodal Large Language Models (MLLMs) require high-resolution visual\ninformation to perform fine-grained perception, yet processing entire\nhigh-resolution images is computationally prohibitive. While recent methods\nleverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they\ntypically present a difficult trade-off: training-based approaches depend on\nlarge-scale annotated datasets, while training-free methods that utilize the\nmodel's internal attention are computationally inefficient and less accurate,\nrequiring either multi-pass prefill stages or reliance on the slow\nauto-regressive decoding process. In this paper, we propose an efficient,\nannotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves\nthis trade-off. The SD-RPN is built around a pipeline that transforms the noisy\nattention maps from the MLLM's middle layers into high-quality pseudo-RoI\nlabels by explicitly denoising the signal and resolving ambiguity. We use these\nlabels to train a lightweight Region Proposal Network (RPN) that learns a more\nprecise localization. This RPN is also highly efficient, predicting the RoI in\na single forward pass using features from the MLLM's middle layers, decoupling\nRoI identification from the auto-regressive generation and avoiding costly\nmulti-pass operations. To validate our approach, we integrate the framework\ninto multiple MLLM families. Despite being trained on only a few (e.g. 10K)\nquestion-answer pairs, our method demonstrates exceptional data efficiency and\ngeneralization, achieving over a 10% absolute accuracy improvement on unseen\nbenchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a\npractical and scalable solution for enhancing the fine-grained perception of\nMLLMs without requiring costly supervision or full model fine-tuning. Code is\navailable at https://github.com/YuHengsss/SD-RPN.\n", "link": "http://arxiv.org/abs/2509.16944v2", "date": "2025-10-16", "relevancy": 2.3116, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5763}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Catching%20the%20Details%3A%20Self-Distilled%20RoI%20Predictors%20for%20Fine-Grained%0A%20%20MLLM%20Perception&body=Title%3A%20Catching%20the%20Details%3A%20Self-Distilled%20RoI%20Predictors%20for%20Fine-Grained%0A%20%20MLLM%20Perception%0AAuthor%3A%20Yuheng%20Shi%20and%20Xiaohuan%20Pei%20and%20Minjing%20Dong%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20require%20high-resolution%20visual%0Ainformation%20to%20perform%20fine-grained%20perception%2C%20yet%20processing%20entire%0Ahigh-resolution%20images%20is%20computationally%20prohibitive.%20While%20recent%20methods%0Aleverage%20a%20Region-of-Interest%20%28RoI%29%20mechanism%20to%20focus%20on%20salient%20areas%2C%20they%0Atypically%20present%20a%20difficult%20trade-off%3A%20training-based%20approaches%20depend%20on%0Alarge-scale%20annotated%20datasets%2C%20while%20training-free%20methods%20that%20utilize%20the%0Amodel%27s%20internal%20attention%20are%20computationally%20inefficient%20and%20less%20accurate%2C%0Arequiring%20either%20multi-pass%20prefill%20stages%20or%20reliance%20on%20the%20slow%0Aauto-regressive%20decoding%20process.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%2C%0Aannotation-free%20Self-Distilled%20Region%20Proposal%20Network%20%28SD-RPN%29%20that%20resolves%0Athis%20trade-off.%20The%20SD-RPN%20is%20built%20around%20a%20pipeline%20that%20transforms%20the%20noisy%0Aattention%20maps%20from%20the%20MLLM%27s%20middle%20layers%20into%20high-quality%20pseudo-RoI%0Alabels%20by%20explicitly%20denoising%20the%20signal%20and%20resolving%20ambiguity.%20We%20use%20these%0Alabels%20to%20train%20a%20lightweight%20Region%20Proposal%20Network%20%28RPN%29%20that%20learns%20a%20more%0Aprecise%20localization.%20This%20RPN%20is%20also%20highly%20efficient%2C%20predicting%20the%20RoI%20in%0Aa%20single%20forward%20pass%20using%20features%20from%20the%20MLLM%27s%20middle%20layers%2C%20decoupling%0ARoI%20identification%20from%20the%20auto-regressive%20generation%20and%20avoiding%20costly%0Amulti-pass%20operations.%20To%20validate%20our%20approach%2C%20we%20integrate%20the%20framework%0Ainto%20multiple%20MLLM%20families.%20Despite%20being%20trained%20on%20only%20a%20few%20%28e.g.%2010K%29%0Aquestion-answer%20pairs%2C%20our%20method%20demonstrates%20exceptional%20data%20efficiency%20and%0Ageneralization%2C%20achieving%20over%20a%2010%25%20absolute%20accuracy%20improvement%20on%20unseen%0Abenchmarks%2C%20including%20TextVQA%2C%20DocVQA%2C%20and%20V-Star.%20Our%20work%20presents%20a%0Apractical%20and%20scalable%20solution%20for%20enhancing%20the%20fine-grained%20perception%20of%0AMLLMs%20without%20requiring%20costly%20supervision%20or%20full%20model%20fine-tuning.%20Code%20is%0Aavailable%20at%20https%3A//github.com/YuHengsss/SD-RPN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16944v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatching%2520the%2520Details%253A%2520Self-Distilled%2520RoI%2520Predictors%2520for%2520Fine-Grained%250A%2520%2520MLLM%2520Perception%26entry.906535625%3DYuheng%2520Shi%2520and%2520Xiaohuan%2520Pei%2520and%2520Minjing%2520Dong%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520require%2520high-resolution%2520visual%250Ainformation%2520to%2520perform%2520fine-grained%2520perception%252C%2520yet%2520processing%2520entire%250Ahigh-resolution%2520images%2520is%2520computationally%2520prohibitive.%2520While%2520recent%2520methods%250Aleverage%2520a%2520Region-of-Interest%2520%2528RoI%2529%2520mechanism%2520to%2520focus%2520on%2520salient%2520areas%252C%2520they%250Atypically%2520present%2520a%2520difficult%2520trade-off%253A%2520training-based%2520approaches%2520depend%2520on%250Alarge-scale%2520annotated%2520datasets%252C%2520while%2520training-free%2520methods%2520that%2520utilize%2520the%250Amodel%2527s%2520internal%2520attention%2520are%2520computationally%2520inefficient%2520and%2520less%2520accurate%252C%250Arequiring%2520either%2520multi-pass%2520prefill%2520stages%2520or%2520reliance%2520on%2520the%2520slow%250Aauto-regressive%2520decoding%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%252C%250Aannotation-free%2520Self-Distilled%2520Region%2520Proposal%2520Network%2520%2528SD-RPN%2529%2520that%2520resolves%250Athis%2520trade-off.%2520The%2520SD-RPN%2520is%2520built%2520around%2520a%2520pipeline%2520that%2520transforms%2520the%2520noisy%250Aattention%2520maps%2520from%2520the%2520MLLM%2527s%2520middle%2520layers%2520into%2520high-quality%2520pseudo-RoI%250Alabels%2520by%2520explicitly%2520denoising%2520the%2520signal%2520and%2520resolving%2520ambiguity.%2520We%2520use%2520these%250Alabels%2520to%2520train%2520a%2520lightweight%2520Region%2520Proposal%2520Network%2520%2528RPN%2529%2520that%2520learns%2520a%2520more%250Aprecise%2520localization.%2520This%2520RPN%2520is%2520also%2520highly%2520efficient%252C%2520predicting%2520the%2520RoI%2520in%250Aa%2520single%2520forward%2520pass%2520using%2520features%2520from%2520the%2520MLLM%2527s%2520middle%2520layers%252C%2520decoupling%250ARoI%2520identification%2520from%2520the%2520auto-regressive%2520generation%2520and%2520avoiding%2520costly%250Amulti-pass%2520operations.%2520To%2520validate%2520our%2520approach%252C%2520we%2520integrate%2520the%2520framework%250Ainto%2520multiple%2520MLLM%2520families.%2520Despite%2520being%2520trained%2520on%2520only%2520a%2520few%2520%2528e.g.%252010K%2529%250Aquestion-answer%2520pairs%252C%2520our%2520method%2520demonstrates%2520exceptional%2520data%2520efficiency%2520and%250Ageneralization%252C%2520achieving%2520over%2520a%252010%2525%2520absolute%2520accuracy%2520improvement%2520on%2520unseen%250Abenchmarks%252C%2520including%2520TextVQA%252C%2520DocVQA%252C%2520and%2520V-Star.%2520Our%2520work%2520presents%2520a%250Apractical%2520and%2520scalable%2520solution%2520for%2520enhancing%2520the%2520fine-grained%2520perception%2520of%250AMLLMs%2520without%2520requiring%2520costly%2520supervision%2520or%2520full%2520model%2520fine-tuning.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/YuHengsss/SD-RPN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16944v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Catching%20the%20Details%3A%20Self-Distilled%20RoI%20Predictors%20for%20Fine-Grained%0A%20%20MLLM%20Perception&entry.906535625=Yuheng%20Shi%20and%20Xiaohuan%20Pei%20and%20Minjing%20Dong%20and%20Chang%20Xu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20require%20high-resolution%20visual%0Ainformation%20to%20perform%20fine-grained%20perception%2C%20yet%20processing%20entire%0Ahigh-resolution%20images%20is%20computationally%20prohibitive.%20While%20recent%20methods%0Aleverage%20a%20Region-of-Interest%20%28RoI%29%20mechanism%20to%20focus%20on%20salient%20areas%2C%20they%0Atypically%20present%20a%20difficult%20trade-off%3A%20training-based%20approaches%20depend%20on%0Alarge-scale%20annotated%20datasets%2C%20while%20training-free%20methods%20that%20utilize%20the%0Amodel%27s%20internal%20attention%20are%20computationally%20inefficient%20and%20less%20accurate%2C%0Arequiring%20either%20multi-pass%20prefill%20stages%20or%20reliance%20on%20the%20slow%0Aauto-regressive%20decoding%20process.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%2C%0Aannotation-free%20Self-Distilled%20Region%20Proposal%20Network%20%28SD-RPN%29%20that%20resolves%0Athis%20trade-off.%20The%20SD-RPN%20is%20built%20around%20a%20pipeline%20that%20transforms%20the%20noisy%0Aattention%20maps%20from%20the%20MLLM%27s%20middle%20layers%20into%20high-quality%20pseudo-RoI%0Alabels%20by%20explicitly%20denoising%20the%20signal%20and%20resolving%20ambiguity.%20We%20use%20these%0Alabels%20to%20train%20a%20lightweight%20Region%20Proposal%20Network%20%28RPN%29%20that%20learns%20a%20more%0Aprecise%20localization.%20This%20RPN%20is%20also%20highly%20efficient%2C%20predicting%20the%20RoI%20in%0Aa%20single%20forward%20pass%20using%20features%20from%20the%20MLLM%27s%20middle%20layers%2C%20decoupling%0ARoI%20identification%20from%20the%20auto-regressive%20generation%20and%20avoiding%20costly%0Amulti-pass%20operations.%20To%20validate%20our%20approach%2C%20we%20integrate%20the%20framework%0Ainto%20multiple%20MLLM%20families.%20Despite%20being%20trained%20on%20only%20a%20few%20%28e.g.%2010K%29%0Aquestion-answer%20pairs%2C%20our%20method%20demonstrates%20exceptional%20data%20efficiency%20and%0Ageneralization%2C%20achieving%20over%20a%2010%25%20absolute%20accuracy%20improvement%20on%20unseen%0Abenchmarks%2C%20including%20TextVQA%2C%20DocVQA%2C%20and%20V-Star.%20Our%20work%20presents%20a%0Apractical%20and%20scalable%20solution%20for%20enhancing%20the%20fine-grained%20perception%20of%0AMLLMs%20without%20requiring%20costly%20supervision%20or%20full%20model%20fine-tuning.%20Code%20is%0Aavailable%20at%20https%3A//github.com/YuHengsss/SD-RPN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16944v2&entry.124074799=Read"},
{"title": "Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging", "author": "Bang An and Yibo Yang and Philip Torr and Bernard Ghanem", "abstract": "  Model merging aims to integrate task-specific abilities from individually\nfine-tuned models into a single model without extra training. In recent model\nmerging methods, task vector has become a fundamental building block, as it can\nencapsulate the residual information from finetuning. However, the merged model\noften suffers from notable performance degradation due to the conflicts caused\nby task-irrelevant redundancy in task vectors. Existing efforts in overcoming\nredundancy by randomly dropping elements in the parameter space involves\nrandomness and lacks knowledge awareness. To address these challenges, in this\nstudy, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.\nConcretely, we sample some training examples from each task, and feed them into\ntheir corresponding fine-tuned models to acquire the covariance matrices before\nlinear layers. We then perform a context-oriented singular value decomposition,\nwhich accentuates the weight components most relevant to the target knowledge.\nAs a result, we can split fine-tuned model weights into task-relevant and\nredundant components in the knowledge-aware subspace, and purify the task\nvector by pruning the redundant components. To induce fair pruning efforts\nacross models, we further introduce a spectral rank allocation strategy by\noptimizing a normalized activated pruning error. The task vector purification\nby our method as a plug-and-play scheme is applicable across various task\nvector-based merging methods to improve their performance. In experiments, we\ndemonstrate the effectiveness of PAVE across a diverse set of merging methods,\ntasks, and model architectures.\n", "link": "http://arxiv.org/abs/2510.14697v1", "date": "2025-10-16", "relevancy": 2.296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4657}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Purifying%20Task%20Vectors%20in%20Knowledge-Aware%20Subspace%20for%20Model%20Merging&body=Title%3A%20Purifying%20Task%20Vectors%20in%20Knowledge-Aware%20Subspace%20for%20Model%20Merging%0AAuthor%3A%20Bang%20An%20and%20Yibo%20Yang%20and%20Philip%20Torr%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Model%20merging%20aims%20to%20integrate%20task-specific%20abilities%20from%20individually%0Afine-tuned%20models%20into%20a%20single%20model%20without%20extra%20training.%20In%20recent%20model%0Amerging%20methods%2C%20task%20vector%20has%20become%20a%20fundamental%20building%20block%2C%20as%20it%20can%0Aencapsulate%20the%20residual%20information%20from%20finetuning.%20However%2C%20the%20merged%20model%0Aoften%20suffers%20from%20notable%20performance%20degradation%20due%20to%20the%20conflicts%20caused%0Aby%20task-irrelevant%20redundancy%20in%20task%20vectors.%20Existing%20efforts%20in%20overcoming%0Aredundancy%20by%20randomly%20dropping%20elements%20in%20the%20parameter%20space%20involves%0Arandomness%20and%20lacks%20knowledge%20awareness.%20To%20address%20these%20challenges%2C%20in%20this%0Astudy%2C%20we%20propose%20Purifying%20TAsk%20Vectors%20%28PAVE%29%20in%20knowledge-aware%20subspace.%0AConcretely%2C%20we%20sample%20some%20training%20examples%20from%20each%20task%2C%20and%20feed%20them%20into%0Atheir%20corresponding%20fine-tuned%20models%20to%20acquire%20the%20covariance%20matrices%20before%0Alinear%20layers.%20We%20then%20perform%20a%20context-oriented%20singular%20value%20decomposition%2C%0Awhich%20accentuates%20the%20weight%20components%20most%20relevant%20to%20the%20target%20knowledge.%0AAs%20a%20result%2C%20we%20can%20split%20fine-tuned%20model%20weights%20into%20task-relevant%20and%0Aredundant%20components%20in%20the%20knowledge-aware%20subspace%2C%20and%20purify%20the%20task%0Avector%20by%20pruning%20the%20redundant%20components.%20To%20induce%20fair%20pruning%20efforts%0Aacross%20models%2C%20we%20further%20introduce%20a%20spectral%20rank%20allocation%20strategy%20by%0Aoptimizing%20a%20normalized%20activated%20pruning%20error.%20The%20task%20vector%20purification%0Aby%20our%20method%20as%20a%20plug-and-play%20scheme%20is%20applicable%20across%20various%20task%0Avector-based%20merging%20methods%20to%20improve%20their%20performance.%20In%20experiments%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20PAVE%20across%20a%20diverse%20set%20of%20merging%20methods%2C%0Atasks%2C%20and%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPurifying%2520Task%2520Vectors%2520in%2520Knowledge-Aware%2520Subspace%2520for%2520Model%2520Merging%26entry.906535625%3DBang%2520An%2520and%2520Yibo%2520Yang%2520and%2520Philip%2520Torr%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3D%2520%2520Model%2520merging%2520aims%2520to%2520integrate%2520task-specific%2520abilities%2520from%2520individually%250Afine-tuned%2520models%2520into%2520a%2520single%2520model%2520without%2520extra%2520training.%2520In%2520recent%2520model%250Amerging%2520methods%252C%2520task%2520vector%2520has%2520become%2520a%2520fundamental%2520building%2520block%252C%2520as%2520it%2520can%250Aencapsulate%2520the%2520residual%2520information%2520from%2520finetuning.%2520However%252C%2520the%2520merged%2520model%250Aoften%2520suffers%2520from%2520notable%2520performance%2520degradation%2520due%2520to%2520the%2520conflicts%2520caused%250Aby%2520task-irrelevant%2520redundancy%2520in%2520task%2520vectors.%2520Existing%2520efforts%2520in%2520overcoming%250Aredundancy%2520by%2520randomly%2520dropping%2520elements%2520in%2520the%2520parameter%2520space%2520involves%250Arandomness%2520and%2520lacks%2520knowledge%2520awareness.%2520To%2520address%2520these%2520challenges%252C%2520in%2520this%250Astudy%252C%2520we%2520propose%2520Purifying%2520TAsk%2520Vectors%2520%2528PAVE%2529%2520in%2520knowledge-aware%2520subspace.%250AConcretely%252C%2520we%2520sample%2520some%2520training%2520examples%2520from%2520each%2520task%252C%2520and%2520feed%2520them%2520into%250Atheir%2520corresponding%2520fine-tuned%2520models%2520to%2520acquire%2520the%2520covariance%2520matrices%2520before%250Alinear%2520layers.%2520We%2520then%2520perform%2520a%2520context-oriented%2520singular%2520value%2520decomposition%252C%250Awhich%2520accentuates%2520the%2520weight%2520components%2520most%2520relevant%2520to%2520the%2520target%2520knowledge.%250AAs%2520a%2520result%252C%2520we%2520can%2520split%2520fine-tuned%2520model%2520weights%2520into%2520task-relevant%2520and%250Aredundant%2520components%2520in%2520the%2520knowledge-aware%2520subspace%252C%2520and%2520purify%2520the%2520task%250Avector%2520by%2520pruning%2520the%2520redundant%2520components.%2520To%2520induce%2520fair%2520pruning%2520efforts%250Aacross%2520models%252C%2520we%2520further%2520introduce%2520a%2520spectral%2520rank%2520allocation%2520strategy%2520by%250Aoptimizing%2520a%2520normalized%2520activated%2520pruning%2520error.%2520The%2520task%2520vector%2520purification%250Aby%2520our%2520method%2520as%2520a%2520plug-and-play%2520scheme%2520is%2520applicable%2520across%2520various%2520task%250Avector-based%2520merging%2520methods%2520to%2520improve%2520their%2520performance.%2520In%2520experiments%252C%2520we%250Ademonstrate%2520the%2520effectiveness%2520of%2520PAVE%2520across%2520a%2520diverse%2520set%2520of%2520merging%2520methods%252C%250Atasks%252C%2520and%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Purifying%20Task%20Vectors%20in%20Knowledge-Aware%20Subspace%20for%20Model%20Merging&entry.906535625=Bang%20An%20and%20Yibo%20Yang%20and%20Philip%20Torr%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Model%20merging%20aims%20to%20integrate%20task-specific%20abilities%20from%20individually%0Afine-tuned%20models%20into%20a%20single%20model%20without%20extra%20training.%20In%20recent%20model%0Amerging%20methods%2C%20task%20vector%20has%20become%20a%20fundamental%20building%20block%2C%20as%20it%20can%0Aencapsulate%20the%20residual%20information%20from%20finetuning.%20However%2C%20the%20merged%20model%0Aoften%20suffers%20from%20notable%20performance%20degradation%20due%20to%20the%20conflicts%20caused%0Aby%20task-irrelevant%20redundancy%20in%20task%20vectors.%20Existing%20efforts%20in%20overcoming%0Aredundancy%20by%20randomly%20dropping%20elements%20in%20the%20parameter%20space%20involves%0Arandomness%20and%20lacks%20knowledge%20awareness.%20To%20address%20these%20challenges%2C%20in%20this%0Astudy%2C%20we%20propose%20Purifying%20TAsk%20Vectors%20%28PAVE%29%20in%20knowledge-aware%20subspace.%0AConcretely%2C%20we%20sample%20some%20training%20examples%20from%20each%20task%2C%20and%20feed%20them%20into%0Atheir%20corresponding%20fine-tuned%20models%20to%20acquire%20the%20covariance%20matrices%20before%0Alinear%20layers.%20We%20then%20perform%20a%20context-oriented%20singular%20value%20decomposition%2C%0Awhich%20accentuates%20the%20weight%20components%20most%20relevant%20to%20the%20target%20knowledge.%0AAs%20a%20result%2C%20we%20can%20split%20fine-tuned%20model%20weights%20into%20task-relevant%20and%0Aredundant%20components%20in%20the%20knowledge-aware%20subspace%2C%20and%20purify%20the%20task%0Avector%20by%20pruning%20the%20redundant%20components.%20To%20induce%20fair%20pruning%20efforts%0Aacross%20models%2C%20we%20further%20introduce%20a%20spectral%20rank%20allocation%20strategy%20by%0Aoptimizing%20a%20normalized%20activated%20pruning%20error.%20The%20task%20vector%20purification%0Aby%20our%20method%20as%20a%20plug-and-play%20scheme%20is%20applicable%20across%20various%20task%0Avector-based%20merging%20methods%20to%20improve%20their%20performance.%20In%20experiments%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20PAVE%20across%20a%20diverse%20set%20of%20merging%20methods%2C%0Atasks%2C%20and%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14697v1&entry.124074799=Read"},
{"title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language\n  Robotic Maps", "author": "Matti Pekkanen and Francesco Verdoja and Ville Kyrki", "abstract": "  Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.\n", "link": "http://arxiv.org/abs/2510.14546v1", "date": "2025-10-16", "relevancy": 2.2826, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuASH%3A%20Using%20Natural-Language%20Heuristics%20to%20Query%20Visual-Language%0A%20%20Robotic%20Maps&body=Title%3A%20QuASH%3A%20Using%20Natural-Language%20Heuristics%20to%20Query%20Visual-Language%0A%20%20Robotic%20Maps%0AAuthor%3A%20Matti%20Pekkanen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki%0AAbstract%3A%20%20%20Embeddings%20from%20Visual-Language%20Models%20are%20increasingly%20utilized%20to%20represent%0Asemantics%20in%20robotic%20maps%2C%20offering%20an%20open-vocabulary%20scene%20understanding%20that%0Asurpasses%20traditional%2C%20limited%20labels.%20Embeddings%20enable%20on-demand%20querying%20by%0Acomparing%20embedded%20user%20text%20prompts%20to%20map%20embeddings%20via%20a%20similarity%20metric.%0AThe%20key%20challenge%20in%20performing%20the%20task%20indicated%20in%20a%20query%20is%20that%20the%20robot%0Amust%20determine%20the%20parts%20of%20the%20environment%20relevant%20to%20the%20query.%0A%20%20This%20paper%20proposes%20a%20solution%20to%20this%20challenge.%20We%20leverage%0Anatural-language%20synonyms%20and%20antonyms%20associated%20with%20the%20query%20within%20the%0Aembedding%20space%2C%20applying%20heuristics%20to%20estimate%20the%20language%20space%20relevant%20to%0Athe%20query%2C%20and%20use%20that%20to%20train%20a%20classifier%20to%20partition%20the%20environment%20into%0Amatches%20and%20non-matches.%20We%20evaluate%20our%20method%20through%20extensive%20experiments%2C%0Aquerying%20both%20maps%20and%20standard%20image%20benchmarks.%20The%20results%20demonstrate%0Aincreased%20queryability%20of%20maps%20and%20images.%20Our%20querying%20technique%20is%20agnostic%0Ato%20the%20representation%20and%20encoder%20used%2C%20and%20requires%20limited%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuASH%253A%2520Using%2520Natural-Language%2520Heuristics%2520to%2520Query%2520Visual-Language%250A%2520%2520Robotic%2520Maps%26entry.906535625%3DMatti%2520Pekkanen%2520and%2520Francesco%2520Verdoja%2520and%2520Ville%2520Kyrki%26entry.1292438233%3D%2520%2520Embeddings%2520from%2520Visual-Language%2520Models%2520are%2520increasingly%2520utilized%2520to%2520represent%250Asemantics%2520in%2520robotic%2520maps%252C%2520offering%2520an%2520open-vocabulary%2520scene%2520understanding%2520that%250Asurpasses%2520traditional%252C%2520limited%2520labels.%2520Embeddings%2520enable%2520on-demand%2520querying%2520by%250Acomparing%2520embedded%2520user%2520text%2520prompts%2520to%2520map%2520embeddings%2520via%2520a%2520similarity%2520metric.%250AThe%2520key%2520challenge%2520in%2520performing%2520the%2520task%2520indicated%2520in%2520a%2520query%2520is%2520that%2520the%2520robot%250Amust%2520determine%2520the%2520parts%2520of%2520the%2520environment%2520relevant%2520to%2520the%2520query.%250A%2520%2520This%2520paper%2520proposes%2520a%2520solution%2520to%2520this%2520challenge.%2520We%2520leverage%250Anatural-language%2520synonyms%2520and%2520antonyms%2520associated%2520with%2520the%2520query%2520within%2520the%250Aembedding%2520space%252C%2520applying%2520heuristics%2520to%2520estimate%2520the%2520language%2520space%2520relevant%2520to%250Athe%2520query%252C%2520and%2520use%2520that%2520to%2520train%2520a%2520classifier%2520to%2520partition%2520the%2520environment%2520into%250Amatches%2520and%2520non-matches.%2520We%2520evaluate%2520our%2520method%2520through%2520extensive%2520experiments%252C%250Aquerying%2520both%2520maps%2520and%2520standard%2520image%2520benchmarks.%2520The%2520results%2520demonstrate%250Aincreased%2520queryability%2520of%2520maps%2520and%2520images.%2520Our%2520querying%2520technique%2520is%2520agnostic%250Ato%2520the%2520representation%2520and%2520encoder%2520used%252C%2520and%2520requires%2520limited%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuASH%3A%20Using%20Natural-Language%20Heuristics%20to%20Query%20Visual-Language%0A%20%20Robotic%20Maps&entry.906535625=Matti%20Pekkanen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki&entry.1292438233=%20%20Embeddings%20from%20Visual-Language%20Models%20are%20increasingly%20utilized%20to%20represent%0Asemantics%20in%20robotic%20maps%2C%20offering%20an%20open-vocabulary%20scene%20understanding%20that%0Asurpasses%20traditional%2C%20limited%20labels.%20Embeddings%20enable%20on-demand%20querying%20by%0Acomparing%20embedded%20user%20text%20prompts%20to%20map%20embeddings%20via%20a%20similarity%20metric.%0AThe%20key%20challenge%20in%20performing%20the%20task%20indicated%20in%20a%20query%20is%20that%20the%20robot%0Amust%20determine%20the%20parts%20of%20the%20environment%20relevant%20to%20the%20query.%0A%20%20This%20paper%20proposes%20a%20solution%20to%20this%20challenge.%20We%20leverage%0Anatural-language%20synonyms%20and%20antonyms%20associated%20with%20the%20query%20within%20the%0Aembedding%20space%2C%20applying%20heuristics%20to%20estimate%20the%20language%20space%20relevant%20to%0Athe%20query%2C%20and%20use%20that%20to%20train%20a%20classifier%20to%20partition%20the%20environment%20into%0Amatches%20and%20non-matches.%20We%20evaluate%20our%20method%20through%20extensive%20experiments%2C%0Aquerying%20both%20maps%20and%20standard%20image%20benchmarks.%20The%20results%20demonstrate%0Aincreased%20queryability%20of%20maps%20and%20images.%20Our%20querying%20technique%20is%20agnostic%0Ato%20the%20representation%20and%20encoder%20used%2C%20and%20requires%20limited%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14546v1&entry.124074799=Read"},
{"title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning", "author": "Jinglei Zhang and Yuanfan Guo and Rolandos Alexandros Potamias and Jiankang Deng and Hang Xu and Chao Ma", "abstract": "  In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io\n", "link": "http://arxiv.org/abs/2510.14672v1", "date": "2025-10-16", "relevancy": 2.281, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5744}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTimeCoT%3A%20Thinking%20by%20Drawing%20for%20Video%20Temporal%20Grounding%20and%20Reasoning&body=Title%3A%20VTimeCoT%3A%20Thinking%20by%20Drawing%20for%20Video%20Temporal%20Grounding%20and%20Reasoning%0AAuthor%3A%20Jinglei%20Zhang%20and%20Yuanfan%20Guo%20and%20Rolandos%20Alexandros%20Potamias%20and%20Jiankang%20Deng%20and%20Hang%20Xu%20and%20Chao%20Ma%0AAbstract%3A%20%20%20In%20recent%20years%2C%20video%20question%20answering%20based%20on%20multimodal%20large%20language%0Amodels%20%28MLLM%29%20has%20garnered%20considerable%20attention%2C%20due%20to%20the%20benefits%20from%20the%0Asubstantial%20advancements%20in%20LLMs.%20However%2C%20these%20models%20have%20a%20notable%0Adeficiency%20in%20the%20domains%20of%20video%20temporal%20grounding%20and%20reasoning%2C%20posing%0Achallenges%20to%20the%20development%20of%20effective%20real-world%20video%20understanding%0Asystems.%20Inspired%20by%20how%20humans%20use%20video%20players%20to%20interact%20with%20the%20progress%0Abar%20for%20video%20comprehension%2C%20we%20introduce%20VTimeCoT%2C%20a%20simple%20yet%20effective%0Atraining-free%20framework%2C%20designed%20for%20high-performance%20video%20grounding%20and%0Areasoning.%20The%20proposed%20framework%20incorporates%20two%20novel%20visual%20tools%20of%20the%0Aprogress%20bar%3A%20a%20plug-and-play%20progress%20bar%20integration%20tool%20and%20a%0Ahigh-efficiency%20highlighting%20tool.%20In%20addition%2C%20to%20address%20the%20limitations%20of%0Aconventional%20text-based%20chain-of-thought%20%28CoT%29%20approaches%2C%20we%20introduce%20a%0Avisuotemporal%20CoT%20process%20that%20integrates%20cross-modality%20reasoning%20across%20both%0Avideo%20and%20text.%20Our%20approach%20demonstrates%20significant%20performance%20improvements%0Aon%20both%20Qwen2VL-7B%20and%20GPT4o%20baselines%20in%20tasks%20of%20video%20temporal%20grounding%20and%0Areasoning-based%20question%20answering.%20Finally%2C%20we%20showcase%20that%20the%20proposed%0Aframework%20achieves%20a%20compositional%20and%20interpretable%20reasoning%20process.%20Project%0Apage%3A%20https%3A//vtimecot.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTimeCoT%253A%2520Thinking%2520by%2520Drawing%2520for%2520Video%2520Temporal%2520Grounding%2520and%2520Reasoning%26entry.906535625%3DJinglei%2520Zhang%2520and%2520Yuanfan%2520Guo%2520and%2520Rolandos%2520Alexandros%2520Potamias%2520and%2520Jiankang%2520Deng%2520and%2520Hang%2520Xu%2520and%2520Chao%2520Ma%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520video%2520question%2520answering%2520based%2520on%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLM%2529%2520has%2520garnered%2520considerable%2520attention%252C%2520due%2520to%2520the%2520benefits%2520from%2520the%250Asubstantial%2520advancements%2520in%2520LLMs.%2520However%252C%2520these%2520models%2520have%2520a%2520notable%250Adeficiency%2520in%2520the%2520domains%2520of%2520video%2520temporal%2520grounding%2520and%2520reasoning%252C%2520posing%250Achallenges%2520to%2520the%2520development%2520of%2520effective%2520real-world%2520video%2520understanding%250Asystems.%2520Inspired%2520by%2520how%2520humans%2520use%2520video%2520players%2520to%2520interact%2520with%2520the%2520progress%250Abar%2520for%2520video%2520comprehension%252C%2520we%2520introduce%2520VTimeCoT%252C%2520a%2520simple%2520yet%2520effective%250Atraining-free%2520framework%252C%2520designed%2520for%2520high-performance%2520video%2520grounding%2520and%250Areasoning.%2520The%2520proposed%2520framework%2520incorporates%2520two%2520novel%2520visual%2520tools%2520of%2520the%250Aprogress%2520bar%253A%2520a%2520plug-and-play%2520progress%2520bar%2520integration%2520tool%2520and%2520a%250Ahigh-efficiency%2520highlighting%2520tool.%2520In%2520addition%252C%2520to%2520address%2520the%2520limitations%2520of%250Aconventional%2520text-based%2520chain-of-thought%2520%2528CoT%2529%2520approaches%252C%2520we%2520introduce%2520a%250Avisuotemporal%2520CoT%2520process%2520that%2520integrates%2520cross-modality%2520reasoning%2520across%2520both%250Avideo%2520and%2520text.%2520Our%2520approach%2520demonstrates%2520significant%2520performance%2520improvements%250Aon%2520both%2520Qwen2VL-7B%2520and%2520GPT4o%2520baselines%2520in%2520tasks%2520of%2520video%2520temporal%2520grounding%2520and%250Areasoning-based%2520question%2520answering.%2520Finally%252C%2520we%2520showcase%2520that%2520the%2520proposed%250Aframework%2520achieves%2520a%2520compositional%2520and%2520interpretable%2520reasoning%2520process.%2520Project%250Apage%253A%2520https%253A//vtimecot.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTimeCoT%3A%20Thinking%20by%20Drawing%20for%20Video%20Temporal%20Grounding%20and%20Reasoning&entry.906535625=Jinglei%20Zhang%20and%20Yuanfan%20Guo%20and%20Rolandos%20Alexandros%20Potamias%20and%20Jiankang%20Deng%20and%20Hang%20Xu%20and%20Chao%20Ma&entry.1292438233=%20%20In%20recent%20years%2C%20video%20question%20answering%20based%20on%20multimodal%20large%20language%0Amodels%20%28MLLM%29%20has%20garnered%20considerable%20attention%2C%20due%20to%20the%20benefits%20from%20the%0Asubstantial%20advancements%20in%20LLMs.%20However%2C%20these%20models%20have%20a%20notable%0Adeficiency%20in%20the%20domains%20of%20video%20temporal%20grounding%20and%20reasoning%2C%20posing%0Achallenges%20to%20the%20development%20of%20effective%20real-world%20video%20understanding%0Asystems.%20Inspired%20by%20how%20humans%20use%20video%20players%20to%20interact%20with%20the%20progress%0Abar%20for%20video%20comprehension%2C%20we%20introduce%20VTimeCoT%2C%20a%20simple%20yet%20effective%0Atraining-free%20framework%2C%20designed%20for%20high-performance%20video%20grounding%20and%0Areasoning.%20The%20proposed%20framework%20incorporates%20two%20novel%20visual%20tools%20of%20the%0Aprogress%20bar%3A%20a%20plug-and-play%20progress%20bar%20integration%20tool%20and%20a%0Ahigh-efficiency%20highlighting%20tool.%20In%20addition%2C%20to%20address%20the%20limitations%20of%0Aconventional%20text-based%20chain-of-thought%20%28CoT%29%20approaches%2C%20we%20introduce%20a%0Avisuotemporal%20CoT%20process%20that%20integrates%20cross-modality%20reasoning%20across%20both%0Avideo%20and%20text.%20Our%20approach%20demonstrates%20significant%20performance%20improvements%0Aon%20both%20Qwen2VL-7B%20and%20GPT4o%20baselines%20in%20tasks%20of%20video%20temporal%20grounding%20and%0Areasoning-based%20question%20answering.%20Finally%2C%20we%20showcase%20that%20the%20proposed%0Aframework%20achieves%20a%20compositional%20and%20interpretable%20reasoning%20process.%20Project%0Apage%3A%20https%3A//vtimecot.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14672v1&entry.124074799=Read"},
{"title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place\n  Reasoning", "author": "Benno Wingender and Nils Dengler and Rohit Menon and Sicong Pan and Maren Bennewitz", "abstract": "  To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.\n", "link": "http://arxiv.org/abs/2510.14584v1", "date": "2025-10-16", "relevancy": 2.2791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5703}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generalized%20Placeability%20Metric%20for%20Model-Free%20Unified%20Pick-and-Place%0A%20%20Reasoning&body=Title%3A%20A%20Generalized%20Placeability%20Metric%20for%20Model-Free%20Unified%20Pick-and-Place%0A%20%20Reasoning%0AAuthor%3A%20Benno%20Wingender%20and%20Nils%20Dengler%20and%20Rohit%20Menon%20and%20Sicong%20Pan%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20To%20reliably%20pick%20and%20place%20unknown%20objects%20under%20real-world%20sensing%20noise%0Aremains%20a%20challenging%20task%2C%20as%20existing%20methods%20rely%20on%20strong%20object%20priors%0A%28e.g.%2C%20CAD%20models%29%2C%20or%20planar-support%20assumptions%2C%20limiting%20generalization%20and%0Aunified%20reasoning%20between%20grasping%20and%20placing.%20In%20this%20work%2C%20we%20introduce%20a%0Ageneralized%20placeability%20metric%20that%20evaluates%20placement%20poses%20directly%20from%0Anoisy%20point%20clouds%2C%20without%20any%20shape%20priors.%20The%20metric%20jointly%20scores%0Astability%2C%20graspability%2C%20and%20clearance.%20From%20raw%20geometry%2C%20we%20extract%20the%0Asupport%20surfaces%20of%20the%20object%20to%20generate%20diverse%20candidates%20for%0Amulti-orientation%20placement%20and%20sample%20contacts%20that%20satisfy%20collision%20and%0Astability%20constraints.%20By%20conditioning%20grasp%20scores%20on%20each%20candidate%0Aplacement%2C%20our%20proposed%20method%20enables%20model-free%20unified%20pick-and-place%0Areasoning%20and%20selects%20grasp-place%20pairs%20that%20lead%20to%20stable%2C%20collision-free%0Aplacements.%20On%20unseen%20real%20objects%20and%20non-planar%20object%20supports%2C%20our%20metric%0Adelivers%20CAD-comparable%20accuracy%20in%20predicting%20stability%20loss%20and%20generally%0Aproduces%20more%20physically%20plausible%20placements%20than%20learning-based%20predictors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generalized%2520Placeability%2520Metric%2520for%2520Model-Free%2520Unified%2520Pick-and-Place%250A%2520%2520Reasoning%26entry.906535625%3DBenno%2520Wingender%2520and%2520Nils%2520Dengler%2520and%2520Rohit%2520Menon%2520and%2520Sicong%2520Pan%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520To%2520reliably%2520pick%2520and%2520place%2520unknown%2520objects%2520under%2520real-world%2520sensing%2520noise%250Aremains%2520a%2520challenging%2520task%252C%2520as%2520existing%2520methods%2520rely%2520on%2520strong%2520object%2520priors%250A%2528e.g.%252C%2520CAD%2520models%2529%252C%2520or%2520planar-support%2520assumptions%252C%2520limiting%2520generalization%2520and%250Aunified%2520reasoning%2520between%2520grasping%2520and%2520placing.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Ageneralized%2520placeability%2520metric%2520that%2520evaluates%2520placement%2520poses%2520directly%2520from%250Anoisy%2520point%2520clouds%252C%2520without%2520any%2520shape%2520priors.%2520The%2520metric%2520jointly%2520scores%250Astability%252C%2520graspability%252C%2520and%2520clearance.%2520From%2520raw%2520geometry%252C%2520we%2520extract%2520the%250Asupport%2520surfaces%2520of%2520the%2520object%2520to%2520generate%2520diverse%2520candidates%2520for%250Amulti-orientation%2520placement%2520and%2520sample%2520contacts%2520that%2520satisfy%2520collision%2520and%250Astability%2520constraints.%2520By%2520conditioning%2520grasp%2520scores%2520on%2520each%2520candidate%250Aplacement%252C%2520our%2520proposed%2520method%2520enables%2520model-free%2520unified%2520pick-and-place%250Areasoning%2520and%2520selects%2520grasp-place%2520pairs%2520that%2520lead%2520to%2520stable%252C%2520collision-free%250Aplacements.%2520On%2520unseen%2520real%2520objects%2520and%2520non-planar%2520object%2520supports%252C%2520our%2520metric%250Adelivers%2520CAD-comparable%2520accuracy%2520in%2520predicting%2520stability%2520loss%2520and%2520generally%250Aproduces%2520more%2520physically%2520plausible%2520placements%2520than%2520learning-based%2520predictors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalized%20Placeability%20Metric%20for%20Model-Free%20Unified%20Pick-and-Place%0A%20%20Reasoning&entry.906535625=Benno%20Wingender%20and%20Nils%20Dengler%20and%20Rohit%20Menon%20and%20Sicong%20Pan%20and%20Maren%20Bennewitz&entry.1292438233=%20%20To%20reliably%20pick%20and%20place%20unknown%20objects%20under%20real-world%20sensing%20noise%0Aremains%20a%20challenging%20task%2C%20as%20existing%20methods%20rely%20on%20strong%20object%20priors%0A%28e.g.%2C%20CAD%20models%29%2C%20or%20planar-support%20assumptions%2C%20limiting%20generalization%20and%0Aunified%20reasoning%20between%20grasping%20and%20placing.%20In%20this%20work%2C%20we%20introduce%20a%0Ageneralized%20placeability%20metric%20that%20evaluates%20placement%20poses%20directly%20from%0Anoisy%20point%20clouds%2C%20without%20any%20shape%20priors.%20The%20metric%20jointly%20scores%0Astability%2C%20graspability%2C%20and%20clearance.%20From%20raw%20geometry%2C%20we%20extract%20the%0Asupport%20surfaces%20of%20the%20object%20to%20generate%20diverse%20candidates%20for%0Amulti-orientation%20placement%20and%20sample%20contacts%20that%20satisfy%20collision%20and%0Astability%20constraints.%20By%20conditioning%20grasp%20scores%20on%20each%20candidate%0Aplacement%2C%20our%20proposed%20method%20enables%20model-free%20unified%20pick-and-place%0Areasoning%20and%20selects%20grasp-place%20pairs%20that%20lead%20to%20stable%2C%20collision-free%0Aplacements.%20On%20unseen%20real%20objects%20and%20non-planar%20object%20supports%2C%20our%20metric%0Adelivers%20CAD-comparable%20accuracy%20in%20predicting%20stability%20loss%20and%20generally%0Aproduces%20more%20physically%20plausible%20placements%20than%20learning-based%20predictors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14584v1&entry.124074799=Read"},
{"title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection", "author": "Furkan Mumcu and Michael J. Jones and Anoop Cherian and Yasin Yilmaz", "abstract": "  Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.\n", "link": "http://arxiv.org/abs/2510.14896v1", "date": "2025-10-16", "relevancy": 2.266, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5698}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.566}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Multimodal%20LLM%20Descriptions%20of%20Activity%20for%20Explainable%0A%20%20Semi-Supervised%20Video%20Anomaly%20Detection&body=Title%3A%20Leveraging%20Multimodal%20LLM%20Descriptions%20of%20Activity%20for%20Explainable%0A%20%20Semi-Supervised%20Video%20Anomaly%20Detection%0AAuthor%3A%20Furkan%20Mumcu%20and%20Michael%20J.%20Jones%20and%20Anoop%20Cherian%20and%20Yasin%20Yilmaz%0AAbstract%3A%20%20%20Existing%20semi-supervised%20video%20anomaly%20detection%20%28VAD%29%20methods%20often%20struggle%0Awith%20detecting%20complex%20anomalies%20involving%20object%20interactions%20and%20generally%0Alack%20explainability.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20VAD%0Aframework%20leveraging%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Unlike%20previous%0AMLLM-based%20approaches%20that%20make%20direct%20anomaly%20judgments%20at%20the%20frame%20level%2C%0Aour%20method%20focuses%20on%20extracting%20and%20interpreting%20object%20activity%20and%0Ainteractions%20over%20time.%20By%20querying%20an%20MLLM%20with%20visual%20inputs%20of%20object%20pairs%0Aat%20different%20moments%2C%20we%20generate%20textual%20descriptions%20of%20the%20activity%20and%0Ainteractions%20from%20nominal%20videos.%20These%20textual%20descriptions%20serve%20as%20a%0Ahigh-level%20representation%20of%20the%20activity%20and%20interactions%20of%20objects%20in%20a%0Avideo.%20They%20are%20used%20to%20detect%20anomalies%20during%20test%20time%20by%20comparing%20them%20to%0Atextual%20descriptions%20found%20in%20nominal%20training%20videos.%20Our%20approach%20inherently%0Aprovides%20explainability%20and%20can%20be%20combined%20with%20many%20traditional%20VAD%20methods%0Ato%20further%20enhance%20their%20interpretability.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20demonstrate%20that%20our%20method%20not%20only%20detects%20complex%20interaction-based%0Aanomalies%20effectively%20but%20also%20achieves%20state-of-the-art%20performance%20on%0Adatasets%20without%20interaction%20anomalies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Multimodal%2520LLM%2520Descriptions%2520of%2520Activity%2520for%2520Explainable%250A%2520%2520Semi-Supervised%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DFurkan%2520Mumcu%2520and%2520Michael%2520J.%2520Jones%2520and%2520Anoop%2520Cherian%2520and%2520Yasin%2520Yilmaz%26entry.1292438233%3D%2520%2520Existing%2520semi-supervised%2520video%2520anomaly%2520detection%2520%2528VAD%2529%2520methods%2520often%2520struggle%250Awith%2520detecting%2520complex%2520anomalies%2520involving%2520object%2520interactions%2520and%2520generally%250Alack%2520explainability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520VAD%250Aframework%2520leveraging%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Unlike%2520previous%250AMLLM-based%2520approaches%2520that%2520make%2520direct%2520anomaly%2520judgments%2520at%2520the%2520frame%2520level%252C%250Aour%2520method%2520focuses%2520on%2520extracting%2520and%2520interpreting%2520object%2520activity%2520and%250Ainteractions%2520over%2520time.%2520By%2520querying%2520an%2520MLLM%2520with%2520visual%2520inputs%2520of%2520object%2520pairs%250Aat%2520different%2520moments%252C%2520we%2520generate%2520textual%2520descriptions%2520of%2520the%2520activity%2520and%250Ainteractions%2520from%2520nominal%2520videos.%2520These%2520textual%2520descriptions%2520serve%2520as%2520a%250Ahigh-level%2520representation%2520of%2520the%2520activity%2520and%2520interactions%2520of%2520objects%2520in%2520a%250Avideo.%2520They%2520are%2520used%2520to%2520detect%2520anomalies%2520during%2520test%2520time%2520by%2520comparing%2520them%2520to%250Atextual%2520descriptions%2520found%2520in%2520nominal%2520training%2520videos.%2520Our%2520approach%2520inherently%250Aprovides%2520explainability%2520and%2520can%2520be%2520combined%2520with%2520many%2520traditional%2520VAD%2520methods%250Ato%2520further%2520enhance%2520their%2520interpretability.%2520Extensive%2520experiments%2520on%2520benchmark%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520not%2520only%2520detects%2520complex%2520interaction-based%250Aanomalies%2520effectively%2520but%2520also%2520achieves%2520state-of-the-art%2520performance%2520on%250Adatasets%2520without%2520interaction%2520anomalies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Multimodal%20LLM%20Descriptions%20of%20Activity%20for%20Explainable%0A%20%20Semi-Supervised%20Video%20Anomaly%20Detection&entry.906535625=Furkan%20Mumcu%20and%20Michael%20J.%20Jones%20and%20Anoop%20Cherian%20and%20Yasin%20Yilmaz&entry.1292438233=%20%20Existing%20semi-supervised%20video%20anomaly%20detection%20%28VAD%29%20methods%20often%20struggle%0Awith%20detecting%20complex%20anomalies%20involving%20object%20interactions%20and%20generally%0Alack%20explainability.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20VAD%0Aframework%20leveraging%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Unlike%20previous%0AMLLM-based%20approaches%20that%20make%20direct%20anomaly%20judgments%20at%20the%20frame%20level%2C%0Aour%20method%20focuses%20on%20extracting%20and%20interpreting%20object%20activity%20and%0Ainteractions%20over%20time.%20By%20querying%20an%20MLLM%20with%20visual%20inputs%20of%20object%20pairs%0Aat%20different%20moments%2C%20we%20generate%20textual%20descriptions%20of%20the%20activity%20and%0Ainteractions%20from%20nominal%20videos.%20These%20textual%20descriptions%20serve%20as%20a%0Ahigh-level%20representation%20of%20the%20activity%20and%20interactions%20of%20objects%20in%20a%0Avideo.%20They%20are%20used%20to%20detect%20anomalies%20during%20test%20time%20by%20comparing%20them%20to%0Atextual%20descriptions%20found%20in%20nominal%20training%20videos.%20Our%20approach%20inherently%0Aprovides%20explainability%20and%20can%20be%20combined%20with%20many%20traditional%20VAD%20methods%0Ato%20further%20enhance%20their%20interpretability.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20demonstrate%20that%20our%20method%20not%20only%20detects%20complex%20interaction-based%0Aanomalies%20effectively%20but%20also%20achieves%20state-of-the-art%20performance%20on%0Adatasets%20without%20interaction%20anomalies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14896v1&entry.124074799=Read"},
{"title": "Boosting Instruction Following at Scale", "author": "Ben Elder and Evelyn Duesterwald and Vinod Muthusamy", "abstract": "  A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.\n", "link": "http://arxiv.org/abs/2510.14842v1", "date": "2025-10-16", "relevancy": 2.2592, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4749}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Instruction%20Following%20at%20Scale&body=Title%3A%20Boosting%20Instruction%20Following%20at%20Scale%0AAuthor%3A%20Ben%20Elder%20and%20Evelyn%20Duesterwald%20and%20Vinod%20Muthusamy%0AAbstract%3A%20%20%20A%20typical%20approach%20developers%20follow%20to%20influence%20an%20LLM%27s%20behavior%20in%20an%0Aapplication%20is%20through%20careful%20manipulation%20of%20the%20prompt%2C%20such%20as%20by%20adding%20or%0Amodifying%20instructions.%20However%2C%20merely%20adding%20more%20instructions%20provides%0Alittle%20assurance%20that%20they%20will%20actually%20be%20followed.%20We%20introduce%20Instruction%0ABoosting%20as%20a%20post-generation%20method%20to%20increase%20the%20reliability%20of%20LLM%20prompt%0Ainstructions.%20We%20show%20that%20Instruction%20Boosting%20improves%20the%20instruction%0Afollowing%20rate%20by%20up%20to%207%20points%20for%20two%20instructions%20and%20up%20to%204%20points%20for%0Aten%20instructions.%20To%20demonstrate%20these%20results%20we%20introduce%20SCALEDIF%2C%20a%0Abenchmark%20with%20a%20scaled%20instruction%20volume%20of%20up%20to%20ten%20instructions%20per%20data%0Asample.%20We%20also%20present%20an%20analysis%20of%20the%20commonly%20observed%20trend%20that%0Aperformance%20degrades%20as%20more%20instructions%20are%20added.%20We%20show%20that%20an%20important%0Afactor%20contributing%20to%20this%20trend%20is%20the%20degree%20of%20tension%20and%20conflict%20that%0Aarises%20as%20the%20number%20of%20instructions%20is%20increased.%20We%20contribute%20a%20quantitative%0Aconflict%20scoring%20tool%20that%20explains%20the%20observed%20performance%20trends%20and%0Aprovides%20feedback%20to%20developers%20on%20the%20impact%20that%20additional%20prompt%0Ainstructions%20have%20on%20a%20model%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Instruction%2520Following%2520at%2520Scale%26entry.906535625%3DBen%2520Elder%2520and%2520Evelyn%2520Duesterwald%2520and%2520Vinod%2520Muthusamy%26entry.1292438233%3D%2520%2520A%2520typical%2520approach%2520developers%2520follow%2520to%2520influence%2520an%2520LLM%2527s%2520behavior%2520in%2520an%250Aapplication%2520is%2520through%2520careful%2520manipulation%2520of%2520the%2520prompt%252C%2520such%2520as%2520by%2520adding%2520or%250Amodifying%2520instructions.%2520However%252C%2520merely%2520adding%2520more%2520instructions%2520provides%250Alittle%2520assurance%2520that%2520they%2520will%2520actually%2520be%2520followed.%2520We%2520introduce%2520Instruction%250ABoosting%2520as%2520a%2520post-generation%2520method%2520to%2520increase%2520the%2520reliability%2520of%2520LLM%2520prompt%250Ainstructions.%2520We%2520show%2520that%2520Instruction%2520Boosting%2520improves%2520the%2520instruction%250Afollowing%2520rate%2520by%2520up%2520to%25207%2520points%2520for%2520two%2520instructions%2520and%2520up%2520to%25204%2520points%2520for%250Aten%2520instructions.%2520To%2520demonstrate%2520these%2520results%2520we%2520introduce%2520SCALEDIF%252C%2520a%250Abenchmark%2520with%2520a%2520scaled%2520instruction%2520volume%2520of%2520up%2520to%2520ten%2520instructions%2520per%2520data%250Asample.%2520We%2520also%2520present%2520an%2520analysis%2520of%2520the%2520commonly%2520observed%2520trend%2520that%250Aperformance%2520degrades%2520as%2520more%2520instructions%2520are%2520added.%2520We%2520show%2520that%2520an%2520important%250Afactor%2520contributing%2520to%2520this%2520trend%2520is%2520the%2520degree%2520of%2520tension%2520and%2520conflict%2520that%250Aarises%2520as%2520the%2520number%2520of%2520instructions%2520is%2520increased.%2520We%2520contribute%2520a%2520quantitative%250Aconflict%2520scoring%2520tool%2520that%2520explains%2520the%2520observed%2520performance%2520trends%2520and%250Aprovides%2520feedback%2520to%2520developers%2520on%2520the%2520impact%2520that%2520additional%2520prompt%250Ainstructions%2520have%2520on%2520a%2520model%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Instruction%20Following%20at%20Scale&entry.906535625=Ben%20Elder%20and%20Evelyn%20Duesterwald%20and%20Vinod%20Muthusamy&entry.1292438233=%20%20A%20typical%20approach%20developers%20follow%20to%20influence%20an%20LLM%27s%20behavior%20in%20an%0Aapplication%20is%20through%20careful%20manipulation%20of%20the%20prompt%2C%20such%20as%20by%20adding%20or%0Amodifying%20instructions.%20However%2C%20merely%20adding%20more%20instructions%20provides%0Alittle%20assurance%20that%20they%20will%20actually%20be%20followed.%20We%20introduce%20Instruction%0ABoosting%20as%20a%20post-generation%20method%20to%20increase%20the%20reliability%20of%20LLM%20prompt%0Ainstructions.%20We%20show%20that%20Instruction%20Boosting%20improves%20the%20instruction%0Afollowing%20rate%20by%20up%20to%207%20points%20for%20two%20instructions%20and%20up%20to%204%20points%20for%0Aten%20instructions.%20To%20demonstrate%20these%20results%20we%20introduce%20SCALEDIF%2C%20a%0Abenchmark%20with%20a%20scaled%20instruction%20volume%20of%20up%20to%20ten%20instructions%20per%20data%0Asample.%20We%20also%20present%20an%20analysis%20of%20the%20commonly%20observed%20trend%20that%0Aperformance%20degrades%20as%20more%20instructions%20are%20added.%20We%20show%20that%20an%20important%0Afactor%20contributing%20to%20this%20trend%20is%20the%20degree%20of%20tension%20and%20conflict%20that%0Aarises%20as%20the%20number%20of%20instructions%20is%20increased.%20We%20contribute%20a%20quantitative%0Aconflict%20scoring%20tool%20that%20explains%20the%20observed%20performance%20trends%20and%0Aprovides%20feedback%20to%20developers%20on%20the%20impact%20that%20additional%20prompt%0Ainstructions%20have%20on%20a%20model%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14842v1&entry.124074799=Read"},
{"title": "Proprioceptive Image: An Image Representation of Proprioceptive Data\n  from Quadruped Robots for Contact Estimation Learning", "author": "Gabriel Fischer Abati and Jo\u00e3o Carlos Virgolino Soares and Giulio Turrisi and Victor Barasuol and Claudio Semini", "abstract": "  This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size.\n", "link": "http://arxiv.org/abs/2510.14612v1", "date": "2025-10-16", "relevancy": 2.2557, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6621}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5525}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proprioceptive%20Image%3A%20An%20Image%20Representation%20of%20Proprioceptive%20Data%0A%20%20from%20Quadruped%20Robots%20for%20Contact%20Estimation%20Learning&body=Title%3A%20Proprioceptive%20Image%3A%20An%20Image%20Representation%20of%20Proprioceptive%20Data%0A%20%20from%20Quadruped%20Robots%20for%20Contact%20Estimation%20Learning%0AAuthor%3A%20Gabriel%20Fischer%20Abati%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Giulio%20Turrisi%20and%20Victor%20Barasuol%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20representing%20proprioceptive%0Atime-series%20data%20from%20quadruped%20robots%20as%20structured%20two-dimensional%20images%2C%0Aenabling%20the%20use%20of%20convolutional%20neural%20networks%20for%20learning%0Alocomotion-related%20tasks.%20The%20proposed%20method%20encodes%20temporal%20dynamics%20from%0Amultiple%20proprioceptive%20signals%2C%20such%20as%20joint%20positions%2C%20IMU%20readings%2C%20and%0Afoot%20velocities%2C%20while%20preserving%20the%20robot%27s%20morphological%20structure%20in%20the%0Aspatial%20arrangement%20of%20the%20image.%20This%20transformation%20captures%20inter-signal%0Acorrelations%20and%20gait-dependent%20patterns%2C%20providing%20a%20richer%20feature%20space%20than%0Adirect%20time-series%20processing.%20We%20apply%20this%20concept%20in%20the%20problem%20of%20contact%0Aestimation%2C%20a%20key%20capability%20for%20stable%20and%20adaptive%20locomotion%20on%20diverse%0Aterrains.%20Experimental%20evaluations%20on%20both%20real-world%20datasets%20and%20simulated%0Aenvironments%20show%20that%20our%20image-based%20representation%20consistently%20enhances%0Aprediction%20accuracy%20and%20generalization%20over%20conventional%20sequence-based%20models%2C%0Aunderscoring%20the%20potential%20of%20cross-modal%20encoding%20strategies%20for%20robotic%20state%0Alearning.%20Our%20method%20achieves%20superior%20performance%20on%20the%20contact%20dataset%2C%0Aimproving%20contact%20state%20accuracy%20from%2087.7%25%20to%2094.5%25%20over%20the%20recently%20proposed%0AMI-HGNN%20method%2C%20using%20a%2015%20times%20shorter%20window%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProprioceptive%2520Image%253A%2520An%2520Image%2520Representation%2520of%2520Proprioceptive%2520Data%250A%2520%2520from%2520Quadruped%2520Robots%2520for%2520Contact%2520Estimation%2520Learning%26entry.906535625%3DGabriel%2520Fischer%2520Abati%2520and%2520Jo%25C3%25A3o%2520Carlos%2520Virgolino%2520Soares%2520and%2520Giulio%2520Turrisi%2520and%2520Victor%2520Barasuol%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520representing%2520proprioceptive%250Atime-series%2520data%2520from%2520quadruped%2520robots%2520as%2520structured%2520two-dimensional%2520images%252C%250Aenabling%2520the%2520use%2520of%2520convolutional%2520neural%2520networks%2520for%2520learning%250Alocomotion-related%2520tasks.%2520The%2520proposed%2520method%2520encodes%2520temporal%2520dynamics%2520from%250Amultiple%2520proprioceptive%2520signals%252C%2520such%2520as%2520joint%2520positions%252C%2520IMU%2520readings%252C%2520and%250Afoot%2520velocities%252C%2520while%2520preserving%2520the%2520robot%2527s%2520morphological%2520structure%2520in%2520the%250Aspatial%2520arrangement%2520of%2520the%2520image.%2520This%2520transformation%2520captures%2520inter-signal%250Acorrelations%2520and%2520gait-dependent%2520patterns%252C%2520providing%2520a%2520richer%2520feature%2520space%2520than%250Adirect%2520time-series%2520processing.%2520We%2520apply%2520this%2520concept%2520in%2520the%2520problem%2520of%2520contact%250Aestimation%252C%2520a%2520key%2520capability%2520for%2520stable%2520and%2520adaptive%2520locomotion%2520on%2520diverse%250Aterrains.%2520Experimental%2520evaluations%2520on%2520both%2520real-world%2520datasets%2520and%2520simulated%250Aenvironments%2520show%2520that%2520our%2520image-based%2520representation%2520consistently%2520enhances%250Aprediction%2520accuracy%2520and%2520generalization%2520over%2520conventional%2520sequence-based%2520models%252C%250Aunderscoring%2520the%2520potential%2520of%2520cross-modal%2520encoding%2520strategies%2520for%2520robotic%2520state%250Alearning.%2520Our%2520method%2520achieves%2520superior%2520performance%2520on%2520the%2520contact%2520dataset%252C%250Aimproving%2520contact%2520state%2520accuracy%2520from%252087.7%2525%2520to%252094.5%2525%2520over%2520the%2520recently%2520proposed%250AMI-HGNN%2520method%252C%2520using%2520a%252015%2520times%2520shorter%2520window%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proprioceptive%20Image%3A%20An%20Image%20Representation%20of%20Proprioceptive%20Data%0A%20%20from%20Quadruped%20Robots%20for%20Contact%20Estimation%20Learning&entry.906535625=Gabriel%20Fischer%20Abati%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Giulio%20Turrisi%20and%20Victor%20Barasuol%20and%20Claudio%20Semini&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20representing%20proprioceptive%0Atime-series%20data%20from%20quadruped%20robots%20as%20structured%20two-dimensional%20images%2C%0Aenabling%20the%20use%20of%20convolutional%20neural%20networks%20for%20learning%0Alocomotion-related%20tasks.%20The%20proposed%20method%20encodes%20temporal%20dynamics%20from%0Amultiple%20proprioceptive%20signals%2C%20such%20as%20joint%20positions%2C%20IMU%20readings%2C%20and%0Afoot%20velocities%2C%20while%20preserving%20the%20robot%27s%20morphological%20structure%20in%20the%0Aspatial%20arrangement%20of%20the%20image.%20This%20transformation%20captures%20inter-signal%0Acorrelations%20and%20gait-dependent%20patterns%2C%20providing%20a%20richer%20feature%20space%20than%0Adirect%20time-series%20processing.%20We%20apply%20this%20concept%20in%20the%20problem%20of%20contact%0Aestimation%2C%20a%20key%20capability%20for%20stable%20and%20adaptive%20locomotion%20on%20diverse%0Aterrains.%20Experimental%20evaluations%20on%20both%20real-world%20datasets%20and%20simulated%0Aenvironments%20show%20that%20our%20image-based%20representation%20consistently%20enhances%0Aprediction%20accuracy%20and%20generalization%20over%20conventional%20sequence-based%20models%2C%0Aunderscoring%20the%20potential%20of%20cross-modal%20encoding%20strategies%20for%20robotic%20state%0Alearning.%20Our%20method%20achieves%20superior%20performance%20on%20the%20contact%20dataset%2C%0Aimproving%20contact%20state%20accuracy%20from%2087.7%25%20to%2094.5%25%20over%20the%20recently%20proposed%0AMI-HGNN%20method%2C%20using%20a%2015%20times%20shorter%20window%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14612v1&entry.124074799=Read"},
{"title": "SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal\n  Video Action Grounding", "author": "Tanveer Hannan and Shuaicong Wu and Mark Weber and Suprosanna Shit and Jindong Gu and Rajat Koner and Aljo\u0161a O\u0161ep and Laura Leal-Taix\u00e9 and Thomas Seidl", "abstract": "  Understanding fine-grained actions and accurately localizing their\ncorresponding actors in space and time are fundamental capabilities for\nadvancing next-generation AI systems, including embodied agents, autonomous\nplatforms, and human-AI interaction frameworks. Despite recent progress in\nvideo understanding, existing methods predominantly address either\ncoarse-grained action recognition or generic object tracking, thereby\noverlooking the challenge of jointly detecting and tracking multiple objects\naccording to their actions while grounding them temporally. To address this\ngap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task\nthat requires models to simultaneously detect, track, and temporally localize\nall referent objects in videos based on natural language descriptions of their\nactions. To support this task, we construct SVAG-Bench, a large-scale benchmark\ncomprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering\na diverse range of objects, actions, and real-world scenes. We further propose\nSVAGFormer, a baseline framework that adapts state of the art vision language\nmodels for joint spatial and temporal grounding, and introduce SVAGEval, a\nstandardized evaluation toolkit for fair and reproducible benchmarking.\nEmpirical results show that existing models perform poorly on SVAG,\nparticularly in dense or complex scenes, underscoring the need for more\nadvanced reasoning over fine-grained object-action interactions in long videos.\n", "link": "http://arxiv.org/abs/2510.13016v2", "date": "2025-10-16", "relevancy": 2.2523, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5852}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVAG-Bench%3A%20A%20Large-Scale%20Benchmark%20for%20Multi-Instance%20Spatio-temporal%0A%20%20Video%20Action%20Grounding&body=Title%3A%20SVAG-Bench%3A%20A%20Large-Scale%20Benchmark%20for%20Multi-Instance%20Spatio-temporal%0A%20%20Video%20Action%20Grounding%0AAuthor%3A%20Tanveer%20Hannan%20and%20Shuaicong%20Wu%20and%20Mark%20Weber%20and%20Suprosanna%20Shit%20and%20Jindong%20Gu%20and%20Rajat%20Koner%20and%20Aljo%C5%A1a%20O%C5%A1ep%20and%20Laura%20Leal-Taix%C3%A9%20and%20Thomas%20Seidl%0AAbstract%3A%20%20%20Understanding%20fine-grained%20actions%20and%20accurately%20localizing%20their%0Acorresponding%20actors%20in%20space%20and%20time%20are%20fundamental%20capabilities%20for%0Aadvancing%20next-generation%20AI%20systems%2C%20including%20embodied%20agents%2C%20autonomous%0Aplatforms%2C%20and%20human-AI%20interaction%20frameworks.%20Despite%20recent%20progress%20in%0Avideo%20understanding%2C%20existing%20methods%20predominantly%20address%20either%0Acoarse-grained%20action%20recognition%20or%20generic%20object%20tracking%2C%20thereby%0Aoverlooking%20the%20challenge%20of%20jointly%20detecting%20and%20tracking%20multiple%20objects%0Aaccording%20to%20their%20actions%20while%20grounding%20them%20temporally.%20To%20address%20this%0Agap%2C%20we%20introduce%20Spatio-temporal%20Video%20Action%20Grounding%20%28SVAG%29%2C%20a%20novel%20task%0Athat%20requires%20models%20to%20simultaneously%20detect%2C%20track%2C%20and%20temporally%20localize%0Aall%20referent%20objects%20in%20videos%20based%20on%20natural%20language%20descriptions%20of%20their%0Aactions.%20To%20support%20this%20task%2C%20we%20construct%20SVAG-Bench%2C%20a%20large-scale%20benchmark%0Acomprising%20688%20videos%2C%2019%2C590%20annotated%20records%2C%20and%20903%20unique%20verbs%2C%20covering%0Aa%20diverse%20range%20of%20objects%2C%20actions%2C%20and%20real-world%20scenes.%20We%20further%20propose%0ASVAGFormer%2C%20a%20baseline%20framework%20that%20adapts%20state%20of%20the%20art%20vision%20language%0Amodels%20for%20joint%20spatial%20and%20temporal%20grounding%2C%20and%20introduce%20SVAGEval%2C%20a%0Astandardized%20evaluation%20toolkit%20for%20fair%20and%20reproducible%20benchmarking.%0AEmpirical%20results%20show%20that%20existing%20models%20perform%20poorly%20on%20SVAG%2C%0Aparticularly%20in%20dense%20or%20complex%20scenes%2C%20underscoring%20the%20need%20for%20more%0Aadvanced%20reasoning%20over%20fine-grained%20object-action%20interactions%20in%20long%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVAG-Bench%253A%2520A%2520Large-Scale%2520Benchmark%2520for%2520Multi-Instance%2520Spatio-temporal%250A%2520%2520Video%2520Action%2520Grounding%26entry.906535625%3DTanveer%2520Hannan%2520and%2520Shuaicong%2520Wu%2520and%2520Mark%2520Weber%2520and%2520Suprosanna%2520Shit%2520and%2520Jindong%2520Gu%2520and%2520Rajat%2520Koner%2520and%2520Aljo%25C5%25A1a%2520O%25C5%25A1ep%2520and%2520Laura%2520Leal-Taix%25C3%25A9%2520and%2520Thomas%2520Seidl%26entry.1292438233%3D%2520%2520Understanding%2520fine-grained%2520actions%2520and%2520accurately%2520localizing%2520their%250Acorresponding%2520actors%2520in%2520space%2520and%2520time%2520are%2520fundamental%2520capabilities%2520for%250Aadvancing%2520next-generation%2520AI%2520systems%252C%2520including%2520embodied%2520agents%252C%2520autonomous%250Aplatforms%252C%2520and%2520human-AI%2520interaction%2520frameworks.%2520Despite%2520recent%2520progress%2520in%250Avideo%2520understanding%252C%2520existing%2520methods%2520predominantly%2520address%2520either%250Acoarse-grained%2520action%2520recognition%2520or%2520generic%2520object%2520tracking%252C%2520thereby%250Aoverlooking%2520the%2520challenge%2520of%2520jointly%2520detecting%2520and%2520tracking%2520multiple%2520objects%250Aaccording%2520to%2520their%2520actions%2520while%2520grounding%2520them%2520temporally.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520Spatio-temporal%2520Video%2520Action%2520Grounding%2520%2528SVAG%2529%252C%2520a%2520novel%2520task%250Athat%2520requires%2520models%2520to%2520simultaneously%2520detect%252C%2520track%252C%2520and%2520temporally%2520localize%250Aall%2520referent%2520objects%2520in%2520videos%2520based%2520on%2520natural%2520language%2520descriptions%2520of%2520their%250Aactions.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520SVAG-Bench%252C%2520a%2520large-scale%2520benchmark%250Acomprising%2520688%2520videos%252C%252019%252C590%2520annotated%2520records%252C%2520and%2520903%2520unique%2520verbs%252C%2520covering%250Aa%2520diverse%2520range%2520of%2520objects%252C%2520actions%252C%2520and%2520real-world%2520scenes.%2520We%2520further%2520propose%250ASVAGFormer%252C%2520a%2520baseline%2520framework%2520that%2520adapts%2520state%2520of%2520the%2520art%2520vision%2520language%250Amodels%2520for%2520joint%2520spatial%2520and%2520temporal%2520grounding%252C%2520and%2520introduce%2520SVAGEval%252C%2520a%250Astandardized%2520evaluation%2520toolkit%2520for%2520fair%2520and%2520reproducible%2520benchmarking.%250AEmpirical%2520results%2520show%2520that%2520existing%2520models%2520perform%2520poorly%2520on%2520SVAG%252C%250Aparticularly%2520in%2520dense%2520or%2520complex%2520scenes%252C%2520underscoring%2520the%2520need%2520for%2520more%250Aadvanced%2520reasoning%2520over%2520fine-grained%2520object-action%2520interactions%2520in%2520long%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVAG-Bench%3A%20A%20Large-Scale%20Benchmark%20for%20Multi-Instance%20Spatio-temporal%0A%20%20Video%20Action%20Grounding&entry.906535625=Tanveer%20Hannan%20and%20Shuaicong%20Wu%20and%20Mark%20Weber%20and%20Suprosanna%20Shit%20and%20Jindong%20Gu%20and%20Rajat%20Koner%20and%20Aljo%C5%A1a%20O%C5%A1ep%20and%20Laura%20Leal-Taix%C3%A9%20and%20Thomas%20Seidl&entry.1292438233=%20%20Understanding%20fine-grained%20actions%20and%20accurately%20localizing%20their%0Acorresponding%20actors%20in%20space%20and%20time%20are%20fundamental%20capabilities%20for%0Aadvancing%20next-generation%20AI%20systems%2C%20including%20embodied%20agents%2C%20autonomous%0Aplatforms%2C%20and%20human-AI%20interaction%20frameworks.%20Despite%20recent%20progress%20in%0Avideo%20understanding%2C%20existing%20methods%20predominantly%20address%20either%0Acoarse-grained%20action%20recognition%20or%20generic%20object%20tracking%2C%20thereby%0Aoverlooking%20the%20challenge%20of%20jointly%20detecting%20and%20tracking%20multiple%20objects%0Aaccording%20to%20their%20actions%20while%20grounding%20them%20temporally.%20To%20address%20this%0Agap%2C%20we%20introduce%20Spatio-temporal%20Video%20Action%20Grounding%20%28SVAG%29%2C%20a%20novel%20task%0Athat%20requires%20models%20to%20simultaneously%20detect%2C%20track%2C%20and%20temporally%20localize%0Aall%20referent%20objects%20in%20videos%20based%20on%20natural%20language%20descriptions%20of%20their%0Aactions.%20To%20support%20this%20task%2C%20we%20construct%20SVAG-Bench%2C%20a%20large-scale%20benchmark%0Acomprising%20688%20videos%2C%2019%2C590%20annotated%20records%2C%20and%20903%20unique%20verbs%2C%20covering%0Aa%20diverse%20range%20of%20objects%2C%20actions%2C%20and%20real-world%20scenes.%20We%20further%20propose%0ASVAGFormer%2C%20a%20baseline%20framework%20that%20adapts%20state%20of%20the%20art%20vision%20language%0Amodels%20for%20joint%20spatial%20and%20temporal%20grounding%2C%20and%20introduce%20SVAGEval%2C%20a%0Astandardized%20evaluation%20toolkit%20for%20fair%20and%20reproducible%20benchmarking.%0AEmpirical%20results%20show%20that%20existing%20models%20perform%20poorly%20on%20SVAG%2C%0Aparticularly%20in%20dense%20or%20complex%20scenes%2C%20underscoring%20the%20need%20for%20more%0Aadvanced%20reasoning%20over%20fine-grained%20object-action%20interactions%20in%20long%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13016v2&entry.124074799=Read"},
{"title": "Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D\n  Ultrasound Localization Microscopy", "author": "Brice Rauby and Paul Xing and Jonathan Por\u00e9e and Maxime Gasse and Jean Provost", "abstract": "  Ultrasound Localization Microscopy (ULM) is a non-invasive technique that\nallows for the imaging of micro-vessels in vivo, at depth and with a resolution\non the order of ten microns. ULM is based on the sub-resolution localization of\nindividual microbubbles injected in the bloodstream. Mapping the whole\nangioarchitecture requires the accumulation of microbubbles trajectories from\nthousands of frames, typically acquired over a few minutes. ULM acquisition\ntimes can be reduced by increasing the microbubble concentration, but requires\nmore advanced algorithms to detect them individually. Several deep learning\napproaches have been proposed for this task, but they remain limited to 2D\nimaging, in part due to the associated large memory requirements. Herein, we\npropose to use sparse tensor neural networks to reduce memory usage in 2D and\nto improve the scaling of the memory requirement for the extension of deep\nlearning architecture to 3D. We study several approaches to efficiently convert\nultrasound data into a sparse format and study the impact of the associated\nloss of information. When applied in 2D, the sparse formulation reduces the\nmemory requirements by a factor 2 at the cost of a small reduction of\nperformance when compared against dense networks. In 3D, the proposed approach\nreduces memory requirements by two order of magnitude while largely\noutperforming conventional ULM in high concentration settings. We show that\nSparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense\ndeep learning based method in 2D ULM i.e. the use of higher concentration in\nsilico and reduced acquisition time.\n", "link": "http://arxiv.org/abs/2402.09359v2", "date": "2025-10-16", "relevancy": 2.2457, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5655}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20Sparse%20Tensor%20Neural%20Networks%20Enables%20Deep%20Learning%20for%203D%0A%20%20Ultrasound%20Localization%20Microscopy&body=Title%3A%20Pruning%20Sparse%20Tensor%20Neural%20Networks%20Enables%20Deep%20Learning%20for%203D%0A%20%20Ultrasound%20Localization%20Microscopy%0AAuthor%3A%20Brice%20Rauby%20and%20Paul%20Xing%20and%20Jonathan%20Por%C3%A9e%20and%20Maxime%20Gasse%20and%20Jean%20Provost%0AAbstract%3A%20%20%20Ultrasound%20Localization%20Microscopy%20%28ULM%29%20is%20a%20non-invasive%20technique%20that%0Aallows%20for%20the%20imaging%20of%20micro-vessels%20in%20vivo%2C%20at%20depth%20and%20with%20a%20resolution%0Aon%20the%20order%20of%20ten%20microns.%20ULM%20is%20based%20on%20the%20sub-resolution%20localization%20of%0Aindividual%20microbubbles%20injected%20in%20the%20bloodstream.%20Mapping%20the%20whole%0Aangioarchitecture%20requires%20the%20accumulation%20of%20microbubbles%20trajectories%20from%0Athousands%20of%20frames%2C%20typically%20acquired%20over%20a%20few%20minutes.%20ULM%20acquisition%0Atimes%20can%20be%20reduced%20by%20increasing%20the%20microbubble%20concentration%2C%20but%20requires%0Amore%20advanced%20algorithms%20to%20detect%20them%20individually.%20Several%20deep%20learning%0Aapproaches%20have%20been%20proposed%20for%20this%20task%2C%20but%20they%20remain%20limited%20to%202D%0Aimaging%2C%20in%20part%20due%20to%20the%20associated%20large%20memory%20requirements.%20Herein%2C%20we%0Apropose%20to%20use%20sparse%20tensor%20neural%20networks%20to%20reduce%20memory%20usage%20in%202D%20and%0Ato%20improve%20the%20scaling%20of%20the%20memory%20requirement%20for%20the%20extension%20of%20deep%0Alearning%20architecture%20to%203D.%20We%20study%20several%20approaches%20to%20efficiently%20convert%0Aultrasound%20data%20into%20a%20sparse%20format%20and%20study%20the%20impact%20of%20the%20associated%0Aloss%20of%20information.%20When%20applied%20in%202D%2C%20the%20sparse%20formulation%20reduces%20the%0Amemory%20requirements%20by%20a%20factor%202%20at%20the%20cost%20of%20a%20small%20reduction%20of%0Aperformance%20when%20compared%20against%20dense%20networks.%20In%203D%2C%20the%20proposed%20approach%0Areduces%20memory%20requirements%20by%20two%20order%20of%20magnitude%20while%20largely%0Aoutperforming%20conventional%20ULM%20in%20high%20concentration%20settings.%20We%20show%20that%0ASparse%20Tensor%20Neural%20Networks%20in%203D%20ULM%20allow%20for%20the%20same%20benefits%20as%20dense%0Adeep%20learning%20based%20method%20in%202D%20ULM%20i.e.%20the%20use%20of%20higher%20concentration%20in%0Asilico%20and%20reduced%20acquisition%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09359v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520Sparse%2520Tensor%2520Neural%2520Networks%2520Enables%2520Deep%2520Learning%2520for%25203D%250A%2520%2520Ultrasound%2520Localization%2520Microscopy%26entry.906535625%3DBrice%2520Rauby%2520and%2520Paul%2520Xing%2520and%2520Jonathan%2520Por%25C3%25A9e%2520and%2520Maxime%2520Gasse%2520and%2520Jean%2520Provost%26entry.1292438233%3D%2520%2520Ultrasound%2520Localization%2520Microscopy%2520%2528ULM%2529%2520is%2520a%2520non-invasive%2520technique%2520that%250Aallows%2520for%2520the%2520imaging%2520of%2520micro-vessels%2520in%2520vivo%252C%2520at%2520depth%2520and%2520with%2520a%2520resolution%250Aon%2520the%2520order%2520of%2520ten%2520microns.%2520ULM%2520is%2520based%2520on%2520the%2520sub-resolution%2520localization%2520of%250Aindividual%2520microbubbles%2520injected%2520in%2520the%2520bloodstream.%2520Mapping%2520the%2520whole%250Aangioarchitecture%2520requires%2520the%2520accumulation%2520of%2520microbubbles%2520trajectories%2520from%250Athousands%2520of%2520frames%252C%2520typically%2520acquired%2520over%2520a%2520few%2520minutes.%2520ULM%2520acquisition%250Atimes%2520can%2520be%2520reduced%2520by%2520increasing%2520the%2520microbubble%2520concentration%252C%2520but%2520requires%250Amore%2520advanced%2520algorithms%2520to%2520detect%2520them%2520individually.%2520Several%2520deep%2520learning%250Aapproaches%2520have%2520been%2520proposed%2520for%2520this%2520task%252C%2520but%2520they%2520remain%2520limited%2520to%25202D%250Aimaging%252C%2520in%2520part%2520due%2520to%2520the%2520associated%2520large%2520memory%2520requirements.%2520Herein%252C%2520we%250Apropose%2520to%2520use%2520sparse%2520tensor%2520neural%2520networks%2520to%2520reduce%2520memory%2520usage%2520in%25202D%2520and%250Ato%2520improve%2520the%2520scaling%2520of%2520the%2520memory%2520requirement%2520for%2520the%2520extension%2520of%2520deep%250Alearning%2520architecture%2520to%25203D.%2520We%2520study%2520several%2520approaches%2520to%2520efficiently%2520convert%250Aultrasound%2520data%2520into%2520a%2520sparse%2520format%2520and%2520study%2520the%2520impact%2520of%2520the%2520associated%250Aloss%2520of%2520information.%2520When%2520applied%2520in%25202D%252C%2520the%2520sparse%2520formulation%2520reduces%2520the%250Amemory%2520requirements%2520by%2520a%2520factor%25202%2520at%2520the%2520cost%2520of%2520a%2520small%2520reduction%2520of%250Aperformance%2520when%2520compared%2520against%2520dense%2520networks.%2520In%25203D%252C%2520the%2520proposed%2520approach%250Areduces%2520memory%2520requirements%2520by%2520two%2520order%2520of%2520magnitude%2520while%2520largely%250Aoutperforming%2520conventional%2520ULM%2520in%2520high%2520concentration%2520settings.%2520We%2520show%2520that%250ASparse%2520Tensor%2520Neural%2520Networks%2520in%25203D%2520ULM%2520allow%2520for%2520the%2520same%2520benefits%2520as%2520dense%250Adeep%2520learning%2520based%2520method%2520in%25202D%2520ULM%2520i.e.%2520the%2520use%2520of%2520higher%2520concentration%2520in%250Asilico%2520and%2520reduced%2520acquisition%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09359v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20Sparse%20Tensor%20Neural%20Networks%20Enables%20Deep%20Learning%20for%203D%0A%20%20Ultrasound%20Localization%20Microscopy&entry.906535625=Brice%20Rauby%20and%20Paul%20Xing%20and%20Jonathan%20Por%C3%A9e%20and%20Maxime%20Gasse%20and%20Jean%20Provost&entry.1292438233=%20%20Ultrasound%20Localization%20Microscopy%20%28ULM%29%20is%20a%20non-invasive%20technique%20that%0Aallows%20for%20the%20imaging%20of%20micro-vessels%20in%20vivo%2C%20at%20depth%20and%20with%20a%20resolution%0Aon%20the%20order%20of%20ten%20microns.%20ULM%20is%20based%20on%20the%20sub-resolution%20localization%20of%0Aindividual%20microbubbles%20injected%20in%20the%20bloodstream.%20Mapping%20the%20whole%0Aangioarchitecture%20requires%20the%20accumulation%20of%20microbubbles%20trajectories%20from%0Athousands%20of%20frames%2C%20typically%20acquired%20over%20a%20few%20minutes.%20ULM%20acquisition%0Atimes%20can%20be%20reduced%20by%20increasing%20the%20microbubble%20concentration%2C%20but%20requires%0Amore%20advanced%20algorithms%20to%20detect%20them%20individually.%20Several%20deep%20learning%0Aapproaches%20have%20been%20proposed%20for%20this%20task%2C%20but%20they%20remain%20limited%20to%202D%0Aimaging%2C%20in%20part%20due%20to%20the%20associated%20large%20memory%20requirements.%20Herein%2C%20we%0Apropose%20to%20use%20sparse%20tensor%20neural%20networks%20to%20reduce%20memory%20usage%20in%202D%20and%0Ato%20improve%20the%20scaling%20of%20the%20memory%20requirement%20for%20the%20extension%20of%20deep%0Alearning%20architecture%20to%203D.%20We%20study%20several%20approaches%20to%20efficiently%20convert%0Aultrasound%20data%20into%20a%20sparse%20format%20and%20study%20the%20impact%20of%20the%20associated%0Aloss%20of%20information.%20When%20applied%20in%202D%2C%20the%20sparse%20formulation%20reduces%20the%0Amemory%20requirements%20by%20a%20factor%202%20at%20the%20cost%20of%20a%20small%20reduction%20of%0Aperformance%20when%20compared%20against%20dense%20networks.%20In%203D%2C%20the%20proposed%20approach%0Areduces%20memory%20requirements%20by%20two%20order%20of%20magnitude%20while%20largely%0Aoutperforming%20conventional%20ULM%20in%20high%20concentration%20settings.%20We%20show%20that%0ASparse%20Tensor%20Neural%20Networks%20in%203D%20ULM%20allow%20for%20the%20same%20benefits%20as%20dense%0Adeep%20learning%20based%20method%20in%202D%20ULM%20i.e.%20the%20use%20of%20higher%20concentration%20in%0Asilico%20and%20reduced%20acquisition%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09359v2&entry.124074799=Read"},
{"title": "RealDPO: Real or Not Real, that is the Preference", "author": "Guo Cheng and Danni Yang and Ziqi Huang and Jianlou Si and Chenyang Si and Ziwei Liu", "abstract": "  Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.\n", "link": "http://arxiv.org/abs/2510.14955v1", "date": "2025-10-16", "relevancy": 2.2354, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5702}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5693}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealDPO%3A%20Real%20or%20Not%20Real%2C%20that%20is%20the%20Preference&body=Title%3A%20RealDPO%3A%20Real%20or%20Not%20Real%2C%20that%20is%20the%20Preference%0AAuthor%3A%20Guo%20Cheng%20and%20Danni%20Yang%20and%20Ziqi%20Huang%20and%20Jianlou%20Si%20and%20Chenyang%20Si%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Video%20generative%20models%20have%20recently%20achieved%20notable%20advancements%20in%0Asynthesis%20quality.%20However%2C%20generating%20complex%20motions%20remains%20a%20critical%0Achallenge%2C%20as%20existing%20models%20often%20struggle%20to%20produce%20natural%2C%20smooth%2C%20and%0Acontextually%20consistent%20movements.%20This%20gap%20between%20generated%20and%20real-world%0Amotions%20limits%20their%20practical%20applicability.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20RealDPO%2C%20a%20novel%20alignment%20paradigm%20that%20leverages%20real-world%20data%20as%0Apositive%20samples%20for%20preference%20learning%2C%20enabling%20more%20accurate%20motion%0Asynthesis.%20Unlike%20traditional%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20offers%0Alimited%20corrective%20feedback%2C%20RealDPO%20employs%20Direct%20Preference%20Optimization%0A%28DPO%29%20with%20a%20tailored%20loss%20function%20to%20enhance%20motion%20realism.%20By%20contrasting%0Areal-world%20videos%20with%20erroneous%20model%20outputs%2C%20RealDPO%20enables%20iterative%0Aself-correction%2C%20progressively%20refining%20motion%20quality.%20To%20support%0Apost-training%20in%20complex%20motion%20synthesis%2C%20we%20propose%20RealAction-5K%2C%20a%20curated%0Adataset%20of%20high-quality%20videos%20capturing%20human%20daily%20activities%20with%20rich%20and%0Aprecise%20motion%20details.%20Extensive%20experiments%20demonstrate%20that%20RealDPO%0Asignificantly%20improves%20video%20quality%2C%20text%20alignment%2C%20and%20motion%20realism%0Acompared%20to%20state-of-the-art%20models%20and%20existing%20preference%20optimization%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealDPO%253A%2520Real%2520or%2520Not%2520Real%252C%2520that%2520is%2520the%2520Preference%26entry.906535625%3DGuo%2520Cheng%2520and%2520Danni%2520Yang%2520and%2520Ziqi%2520Huang%2520and%2520Jianlou%2520Si%2520and%2520Chenyang%2520Si%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Video%2520generative%2520models%2520have%2520recently%2520achieved%2520notable%2520advancements%2520in%250Asynthesis%2520quality.%2520However%252C%2520generating%2520complex%2520motions%2520remains%2520a%2520critical%250Achallenge%252C%2520as%2520existing%2520models%2520often%2520struggle%2520to%2520produce%2520natural%252C%2520smooth%252C%2520and%250Acontextually%2520consistent%2520movements.%2520This%2520gap%2520between%2520generated%2520and%2520real-world%250Amotions%2520limits%2520their%2520practical%2520applicability.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520RealDPO%252C%2520a%2520novel%2520alignment%2520paradigm%2520that%2520leverages%2520real-world%2520data%2520as%250Apositive%2520samples%2520for%2520preference%2520learning%252C%2520enabling%2520more%2520accurate%2520motion%250Asynthesis.%2520Unlike%2520traditional%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520which%2520offers%250Alimited%2520corrective%2520feedback%252C%2520RealDPO%2520employs%2520Direct%2520Preference%2520Optimization%250A%2528DPO%2529%2520with%2520a%2520tailored%2520loss%2520function%2520to%2520enhance%2520motion%2520realism.%2520By%2520contrasting%250Areal-world%2520videos%2520with%2520erroneous%2520model%2520outputs%252C%2520RealDPO%2520enables%2520iterative%250Aself-correction%252C%2520progressively%2520refining%2520motion%2520quality.%2520To%2520support%250Apost-training%2520in%2520complex%2520motion%2520synthesis%252C%2520we%2520propose%2520RealAction-5K%252C%2520a%2520curated%250Adataset%2520of%2520high-quality%2520videos%2520capturing%2520human%2520daily%2520activities%2520with%2520rich%2520and%250Aprecise%2520motion%2520details.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RealDPO%250Asignificantly%2520improves%2520video%2520quality%252C%2520text%2520alignment%252C%2520and%2520motion%2520realism%250Acompared%2520to%2520state-of-the-art%2520models%2520and%2520existing%2520preference%2520optimization%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealDPO%3A%20Real%20or%20Not%20Real%2C%20that%20is%20the%20Preference&entry.906535625=Guo%20Cheng%20and%20Danni%20Yang%20and%20Ziqi%20Huang%20and%20Jianlou%20Si%20and%20Chenyang%20Si%20and%20Ziwei%20Liu&entry.1292438233=%20%20Video%20generative%20models%20have%20recently%20achieved%20notable%20advancements%20in%0Asynthesis%20quality.%20However%2C%20generating%20complex%20motions%20remains%20a%20critical%0Achallenge%2C%20as%20existing%20models%20often%20struggle%20to%20produce%20natural%2C%20smooth%2C%20and%0Acontextually%20consistent%20movements.%20This%20gap%20between%20generated%20and%20real-world%0Amotions%20limits%20their%20practical%20applicability.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20RealDPO%2C%20a%20novel%20alignment%20paradigm%20that%20leverages%20real-world%20data%20as%0Apositive%20samples%20for%20preference%20learning%2C%20enabling%20more%20accurate%20motion%0Asynthesis.%20Unlike%20traditional%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20offers%0Alimited%20corrective%20feedback%2C%20RealDPO%20employs%20Direct%20Preference%20Optimization%0A%28DPO%29%20with%20a%20tailored%20loss%20function%20to%20enhance%20motion%20realism.%20By%20contrasting%0Areal-world%20videos%20with%20erroneous%20model%20outputs%2C%20RealDPO%20enables%20iterative%0Aself-correction%2C%20progressively%20refining%20motion%20quality.%20To%20support%0Apost-training%20in%20complex%20motion%20synthesis%2C%20we%20propose%20RealAction-5K%2C%20a%20curated%0Adataset%20of%20high-quality%20videos%20capturing%20human%20daily%20activities%20with%20rich%20and%0Aprecise%20motion%20details.%20Extensive%20experiments%20demonstrate%20that%20RealDPO%0Asignificantly%20improves%20video%20quality%2C%20text%20alignment%2C%20and%20motion%20realism%0Acompared%20to%20state-of-the-art%20models%20and%20existing%20preference%20optimization%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14955v1&entry.124074799=Read"},
{"title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models", "author": "Konrad L\u00f6hr and Shuzhou Yuan and Michael F\u00e4rber", "abstract": "  Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs.\n", "link": "http://arxiv.org/abs/2510.08236v2", "date": "2025-10-16", "relevancy": 2.2352, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Hidden%20Bias%3A%20A%20Study%20on%20Explicit%20and%20Implicit%20Political%20Stereotypes%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20The%20Hidden%20Bias%3A%20A%20Study%20on%20Explicit%20and%20Implicit%20Political%20Stereotypes%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Konrad%20L%C3%B6hr%20and%20Shuzhou%20Yuan%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20integral%20to%20information%0Adissemination%20and%20decision-making%20processes.%20Given%20their%20growing%20societal%0Ainfluence%2C%20understanding%20potential%20biases%2C%20particularly%20within%20the%20political%0Adomain%2C%20is%20crucial%20to%20prevent%20undue%20influence%20on%20public%20opinion%20and%20democratic%0Aprocesses.%20This%20work%20investigates%20political%20bias%20and%20stereotype%20propagation%0Aacross%20eight%20prominent%20LLMs%20using%20the%20two-dimensional%20Political%20Compass%20Test%0A%28PCT%29.%20Initially%2C%20the%20PCT%20is%20employed%20to%20assess%20the%20inherent%20political%20leanings%0Aof%20these%20models.%20Subsequently%2C%20persona%20prompting%20with%20the%20PCT%20is%20used%20to%0Aexplore%20explicit%20stereotypes%20across%20various%20social%20dimensions.%20In%20a%20final%20step%2C%0Aimplicit%20stereotypes%20are%20uncovered%20by%20evaluating%20models%20with%20multilingual%0Aversions%20of%20the%20PCT.%20Key%20findings%20reveal%20a%20consistent%20left-leaning%20political%0Aalignment%20across%20all%20investigated%20models.%20Furthermore%2C%20while%20the%20nature%20and%0Aextent%20of%20stereotypes%20vary%20considerably%20between%20models%2C%20implicit%20stereotypes%0Aelicited%20through%20language%20variation%20are%20more%20pronounced%20than%20those%20identified%0Avia%20explicit%20persona%20prompting.%20Interestingly%2C%20for%20most%20models%2C%20implicit%20and%0Aexplicit%20stereotypes%20show%20a%20notable%20alignment%2C%20suggesting%20a%20degree%20of%0Atransparency%20or%20%22awareness%22%20regarding%20their%20inherent%20biases.%20This%20study%0Aunderscores%20the%20complex%20interplay%20of%20political%20bias%20and%20stereotypes%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08236v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Hidden%2520Bias%253A%2520A%2520Study%2520on%2520Explicit%2520and%2520Implicit%2520Political%2520Stereotypes%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DKonrad%2520L%25C3%25B6hr%2520and%2520Shuzhou%2520Yuan%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520integral%2520to%2520information%250Adissemination%2520and%2520decision-making%2520processes.%2520Given%2520their%2520growing%2520societal%250Ainfluence%252C%2520understanding%2520potential%2520biases%252C%2520particularly%2520within%2520the%2520political%250Adomain%252C%2520is%2520crucial%2520to%2520prevent%2520undue%2520influence%2520on%2520public%2520opinion%2520and%2520democratic%250Aprocesses.%2520This%2520work%2520investigates%2520political%2520bias%2520and%2520stereotype%2520propagation%250Aacross%2520eight%2520prominent%2520LLMs%2520using%2520the%2520two-dimensional%2520Political%2520Compass%2520Test%250A%2528PCT%2529.%2520Initially%252C%2520the%2520PCT%2520is%2520employed%2520to%2520assess%2520the%2520inherent%2520political%2520leanings%250Aof%2520these%2520models.%2520Subsequently%252C%2520persona%2520prompting%2520with%2520the%2520PCT%2520is%2520used%2520to%250Aexplore%2520explicit%2520stereotypes%2520across%2520various%2520social%2520dimensions.%2520In%2520a%2520final%2520step%252C%250Aimplicit%2520stereotypes%2520are%2520uncovered%2520by%2520evaluating%2520models%2520with%2520multilingual%250Aversions%2520of%2520the%2520PCT.%2520Key%2520findings%2520reveal%2520a%2520consistent%2520left-leaning%2520political%250Aalignment%2520across%2520all%2520investigated%2520models.%2520Furthermore%252C%2520while%2520the%2520nature%2520and%250Aextent%2520of%2520stereotypes%2520vary%2520considerably%2520between%2520models%252C%2520implicit%2520stereotypes%250Aelicited%2520through%2520language%2520variation%2520are%2520more%2520pronounced%2520than%2520those%2520identified%250Avia%2520explicit%2520persona%2520prompting.%2520Interestingly%252C%2520for%2520most%2520models%252C%2520implicit%2520and%250Aexplicit%2520stereotypes%2520show%2520a%2520notable%2520alignment%252C%2520suggesting%2520a%2520degree%2520of%250Atransparency%2520or%2520%2522awareness%2522%2520regarding%2520their%2520inherent%2520biases.%2520This%2520study%250Aunderscores%2520the%2520complex%2520interplay%2520of%2520political%2520bias%2520and%2520stereotypes%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08236v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Hidden%20Bias%3A%20A%20Study%20on%20Explicit%20and%20Implicit%20Political%20Stereotypes%0A%20%20in%20Large%20Language%20Models&entry.906535625=Konrad%20L%C3%B6hr%20and%20Shuzhou%20Yuan%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20integral%20to%20information%0Adissemination%20and%20decision-making%20processes.%20Given%20their%20growing%20societal%0Ainfluence%2C%20understanding%20potential%20biases%2C%20particularly%20within%20the%20political%0Adomain%2C%20is%20crucial%20to%20prevent%20undue%20influence%20on%20public%20opinion%20and%20democratic%0Aprocesses.%20This%20work%20investigates%20political%20bias%20and%20stereotype%20propagation%0Aacross%20eight%20prominent%20LLMs%20using%20the%20two-dimensional%20Political%20Compass%20Test%0A%28PCT%29.%20Initially%2C%20the%20PCT%20is%20employed%20to%20assess%20the%20inherent%20political%20leanings%0Aof%20these%20models.%20Subsequently%2C%20persona%20prompting%20with%20the%20PCT%20is%20used%20to%0Aexplore%20explicit%20stereotypes%20across%20various%20social%20dimensions.%20In%20a%20final%20step%2C%0Aimplicit%20stereotypes%20are%20uncovered%20by%20evaluating%20models%20with%20multilingual%0Aversions%20of%20the%20PCT.%20Key%20findings%20reveal%20a%20consistent%20left-leaning%20political%0Aalignment%20across%20all%20investigated%20models.%20Furthermore%2C%20while%20the%20nature%20and%0Aextent%20of%20stereotypes%20vary%20considerably%20between%20models%2C%20implicit%20stereotypes%0Aelicited%20through%20language%20variation%20are%20more%20pronounced%20than%20those%20identified%0Avia%20explicit%20persona%20prompting.%20Interestingly%2C%20for%20most%20models%2C%20implicit%20and%0Aexplicit%20stereotypes%20show%20a%20notable%20alignment%2C%20suggesting%20a%20degree%20of%0Atransparency%20or%20%22awareness%22%20regarding%20their%20inherent%20biases.%20This%20study%0Aunderscores%20the%20complex%20interplay%20of%20political%20bias%20and%20stereotypes%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08236v2&entry.124074799=Read"},
{"title": "Analysis of Hyperparameter Optimization Effects on Lightweight Deep\n  Models for Real-Time Image Classification", "author": "Vineet Kumar Rakesh and Soumya Mazumdar and Tapas Samanta and Hemendra Kumar Pandey and Amitabha Das", "abstract": "  Lightweight convolutional and transformer-based networks are increasingly\npreferred for real-time image classification, especially on\nresource-constrained devices. This study evaluates the impact of hyperparameter\noptimization on the accuracy and deployment feasibility of seven modern\nlightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L,\nMobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced\nsubset of 90,000 images from ImageNet-1K. Under standardized training settings,\nthis paper investigates the influence of learning rate schedules, augmentation,\noptimizers, and initialization on model performance. Inference benchmarks are\nperformed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512,\ncapturing latency and throughput in real-time conditions. This work\ndemonstrates that controlled hyperparameter variation significantly alters\nconvergence dynamics in lightweight CNN and transformer backbones, providing\ninsight into stability regions and deployment feasibility in edge artificial\nintelligence. Our results reveal that tuning alone leads to a top-1 accuracy\nimprovement of 1.5 to 3.5 percent over baselines, and select models (e.g.,\nRepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800\nframes per second, making them ideal for edge deployment. This work provides\nreproducible, subset-based insights into lightweight hyperparameter tuning and\nits role in balancing speed and accuracy. The code and logs may be seen at:\nhttps://vineetkumarrakesh.github.io/lcnn-opt\n", "link": "http://arxiv.org/abs/2507.23315v2", "date": "2025-10-16", "relevancy": 2.2309, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5843}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.558}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Hyperparameter%20Optimization%20Effects%20on%20Lightweight%20Deep%0A%20%20Models%20for%20Real-Time%20Image%20Classification&body=Title%3A%20Analysis%20of%20Hyperparameter%20Optimization%20Effects%20on%20Lightweight%20Deep%0A%20%20Models%20for%20Real-Time%20Image%20Classification%0AAuthor%3A%20Vineet%20Kumar%20Rakesh%20and%20Soumya%20Mazumdar%20and%20Tapas%20Samanta%20and%20Hemendra%20Kumar%20Pandey%20and%20Amitabha%20Das%0AAbstract%3A%20%20%20Lightweight%20convolutional%20and%20transformer-based%20networks%20are%20increasingly%0Apreferred%20for%20real-time%20image%20classification%2C%20especially%20on%0Aresource-constrained%20devices.%20This%20study%20evaluates%20the%20impact%20of%20hyperparameter%0Aoptimization%20on%20the%20accuracy%20and%20deployment%20feasibility%20of%20seven%20modern%0Alightweight%20architectures%3A%20ConvNeXt-T%2C%20EfficientNetV2-S%2C%20MobileNetV3-L%2C%0AMobileViT%20v2%20%28S/XS%29%2C%20RepVGG-A2%2C%20and%20TinyViT-21M%2C%20trained%20on%20a%20class-balanced%0Asubset%20of%2090%2C000%20images%20from%20ImageNet-1K.%20Under%20standardized%20training%20settings%2C%0Athis%20paper%20investigates%20the%20influence%20of%20learning%20rate%20schedules%2C%20augmentation%2C%0Aoptimizers%2C%20and%20initialization%20on%20model%20performance.%20Inference%20benchmarks%20are%0Aperformed%20using%20an%20NVIDIA%20L40s%20GPU%20with%20batch%20sizes%20ranging%20from%201%20to%20512%2C%0Acapturing%20latency%20and%20throughput%20in%20real-time%20conditions.%20This%20work%0Ademonstrates%20that%20controlled%20hyperparameter%20variation%20significantly%20alters%0Aconvergence%20dynamics%20in%20lightweight%20CNN%20and%20transformer%20backbones%2C%20providing%0Ainsight%20into%20stability%20regions%20and%20deployment%20feasibility%20in%20edge%20artificial%0Aintelligence.%20Our%20results%20reveal%20that%20tuning%20alone%20leads%20to%20a%20top-1%20accuracy%0Aimprovement%20of%201.5%20to%203.5%20percent%20over%20baselines%2C%20and%20select%20models%20%28e.g.%2C%0ARepVGG-A2%2C%20MobileNetV3-L%29%20deliver%20latency%20under%205%20milliseconds%20and%20over%209%2C800%0Aframes%20per%20second%2C%20making%20them%20ideal%20for%20edge%20deployment.%20This%20work%20provides%0Areproducible%2C%20subset-based%20insights%20into%20lightweight%20hyperparameter%20tuning%20and%0Aits%20role%20in%20balancing%20speed%20and%20accuracy.%20The%20code%20and%20logs%20may%20be%20seen%20at%3A%0Ahttps%3A//vineetkumarrakesh.github.io/lcnn-opt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Hyperparameter%2520Optimization%2520Effects%2520on%2520Lightweight%2520Deep%250A%2520%2520Models%2520for%2520Real-Time%2520Image%2520Classification%26entry.906535625%3DVineet%2520Kumar%2520Rakesh%2520and%2520Soumya%2520Mazumdar%2520and%2520Tapas%2520Samanta%2520and%2520Hemendra%2520Kumar%2520Pandey%2520and%2520Amitabha%2520Das%26entry.1292438233%3D%2520%2520Lightweight%2520convolutional%2520and%2520transformer-based%2520networks%2520are%2520increasingly%250Apreferred%2520for%2520real-time%2520image%2520classification%252C%2520especially%2520on%250Aresource-constrained%2520devices.%2520This%2520study%2520evaluates%2520the%2520impact%2520of%2520hyperparameter%250Aoptimization%2520on%2520the%2520accuracy%2520and%2520deployment%2520feasibility%2520of%2520seven%2520modern%250Alightweight%2520architectures%253A%2520ConvNeXt-T%252C%2520EfficientNetV2-S%252C%2520MobileNetV3-L%252C%250AMobileViT%2520v2%2520%2528S/XS%2529%252C%2520RepVGG-A2%252C%2520and%2520TinyViT-21M%252C%2520trained%2520on%2520a%2520class-balanced%250Asubset%2520of%252090%252C000%2520images%2520from%2520ImageNet-1K.%2520Under%2520standardized%2520training%2520settings%252C%250Athis%2520paper%2520investigates%2520the%2520influence%2520of%2520learning%2520rate%2520schedules%252C%2520augmentation%252C%250Aoptimizers%252C%2520and%2520initialization%2520on%2520model%2520performance.%2520Inference%2520benchmarks%2520are%250Aperformed%2520using%2520an%2520NVIDIA%2520L40s%2520GPU%2520with%2520batch%2520sizes%2520ranging%2520from%25201%2520to%2520512%252C%250Acapturing%2520latency%2520and%2520throughput%2520in%2520real-time%2520conditions.%2520This%2520work%250Ademonstrates%2520that%2520controlled%2520hyperparameter%2520variation%2520significantly%2520alters%250Aconvergence%2520dynamics%2520in%2520lightweight%2520CNN%2520and%2520transformer%2520backbones%252C%2520providing%250Ainsight%2520into%2520stability%2520regions%2520and%2520deployment%2520feasibility%2520in%2520edge%2520artificial%250Aintelligence.%2520Our%2520results%2520reveal%2520that%2520tuning%2520alone%2520leads%2520to%2520a%2520top-1%2520accuracy%250Aimprovement%2520of%25201.5%2520to%25203.5%2520percent%2520over%2520baselines%252C%2520and%2520select%2520models%2520%2528e.g.%252C%250ARepVGG-A2%252C%2520MobileNetV3-L%2529%2520deliver%2520latency%2520under%25205%2520milliseconds%2520and%2520over%25209%252C800%250Aframes%2520per%2520second%252C%2520making%2520them%2520ideal%2520for%2520edge%2520deployment.%2520This%2520work%2520provides%250Areproducible%252C%2520subset-based%2520insights%2520into%2520lightweight%2520hyperparameter%2520tuning%2520and%250Aits%2520role%2520in%2520balancing%2520speed%2520and%2520accuracy.%2520The%2520code%2520and%2520logs%2520may%2520be%2520seen%2520at%253A%250Ahttps%253A//vineetkumarrakesh.github.io/lcnn-opt%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Hyperparameter%20Optimization%20Effects%20on%20Lightweight%20Deep%0A%20%20Models%20for%20Real-Time%20Image%20Classification&entry.906535625=Vineet%20Kumar%20Rakesh%20and%20Soumya%20Mazumdar%20and%20Tapas%20Samanta%20and%20Hemendra%20Kumar%20Pandey%20and%20Amitabha%20Das&entry.1292438233=%20%20Lightweight%20convolutional%20and%20transformer-based%20networks%20are%20increasingly%0Apreferred%20for%20real-time%20image%20classification%2C%20especially%20on%0Aresource-constrained%20devices.%20This%20study%20evaluates%20the%20impact%20of%20hyperparameter%0Aoptimization%20on%20the%20accuracy%20and%20deployment%20feasibility%20of%20seven%20modern%0Alightweight%20architectures%3A%20ConvNeXt-T%2C%20EfficientNetV2-S%2C%20MobileNetV3-L%2C%0AMobileViT%20v2%20%28S/XS%29%2C%20RepVGG-A2%2C%20and%20TinyViT-21M%2C%20trained%20on%20a%20class-balanced%0Asubset%20of%2090%2C000%20images%20from%20ImageNet-1K.%20Under%20standardized%20training%20settings%2C%0Athis%20paper%20investigates%20the%20influence%20of%20learning%20rate%20schedules%2C%20augmentation%2C%0Aoptimizers%2C%20and%20initialization%20on%20model%20performance.%20Inference%20benchmarks%20are%0Aperformed%20using%20an%20NVIDIA%20L40s%20GPU%20with%20batch%20sizes%20ranging%20from%201%20to%20512%2C%0Acapturing%20latency%20and%20throughput%20in%20real-time%20conditions.%20This%20work%0Ademonstrates%20that%20controlled%20hyperparameter%20variation%20significantly%20alters%0Aconvergence%20dynamics%20in%20lightweight%20CNN%20and%20transformer%20backbones%2C%20providing%0Ainsight%20into%20stability%20regions%20and%20deployment%20feasibility%20in%20edge%20artificial%0Aintelligence.%20Our%20results%20reveal%20that%20tuning%20alone%20leads%20to%20a%20top-1%20accuracy%0Aimprovement%20of%201.5%20to%203.5%20percent%20over%20baselines%2C%20and%20select%20models%20%28e.g.%2C%0ARepVGG-A2%2C%20MobileNetV3-L%29%20deliver%20latency%20under%205%20milliseconds%20and%20over%209%2C800%0Aframes%20per%20second%2C%20making%20them%20ideal%20for%20edge%20deployment.%20This%20work%20provides%0Areproducible%2C%20subset-based%20insights%20into%20lightweight%20hyperparameter%20tuning%20and%0Aits%20role%20in%20balancing%20speed%20and%20accuracy.%20The%20code%20and%20logs%20may%20be%20seen%20at%3A%0Ahttps%3A//vineetkumarrakesh.github.io/lcnn-opt%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23315v2&entry.124074799=Read"},
{"title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in\n  Colorectal Cancer from H&E Whole Slide Images", "author": "Usama Sajjad and Abdul Rehman Akbar and Ziyu Su and Deborah Knight and Wendy L. Frankel and Metin N. Gurcan and Wei Chen and Muhammad Khalid Khan Niazi", "abstract": "  Colorectal cancer (CRC) remains the third most prevalent malignancy globally,\nwith approximately 154,000 new cases and 54,000 projected deaths anticipated\nfor 2025. The recent advancement of foundation models in computational\npathology has been largely propelled by task agnostic methodologies that can\noverlook organ-specific crucial morphological patterns that represent distinct\nbiological processes that can fundamentally influence tumor behavior,\ntherapeutic response, and patient outcomes. The aim of this study is to develop\na novel, interpretable AI model, PRISM (Prognostic Representation of Integrated\nSpatial Morphology), that incorporates a continuous variability spectrum within\neach distinct morphology to characterize phenotypic diversity and reflecting\nthe principle that malignant transformation occurs through incremental\nevolutionary processes rather than abrupt phenotypic shifts. PRISM is trained\non 8.74 million histological images extracted from surgical resection specimens\nof 424 patients with stage III CRC. PRISM achieved superior prognostic\nperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;\nHR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific\nmethods by 15% and AI foundation models by ~23% accuracy. It showed\nsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable\nperformance across clinicopathological subgroups, with minimal accuracy\nfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,\nreplicating the Alliance cohort finding of no survival difference between\ntreatments.\n", "link": "http://arxiv.org/abs/2510.14800v1", "date": "2025-10-16", "relevancy": 2.2274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphology-Aware%20Prognostic%20model%20for%20Five-Year%20Survival%20Prediction%20in%0A%20%20Colorectal%20Cancer%20from%20H%26E%20Whole%20Slide%20Images&body=Title%3A%20Morphology-Aware%20Prognostic%20model%20for%20Five-Year%20Survival%20Prediction%20in%0A%20%20Colorectal%20Cancer%20from%20H%26E%20Whole%20Slide%20Images%0AAuthor%3A%20Usama%20Sajjad%20and%20Abdul%20Rehman%20Akbar%20and%20Ziyu%20Su%20and%20Deborah%20Knight%20and%20Wendy%20L.%20Frankel%20and%20Metin%20N.%20Gurcan%20and%20Wei%20Chen%20and%20Muhammad%20Khalid%20Khan%20Niazi%0AAbstract%3A%20%20%20Colorectal%20cancer%20%28CRC%29%20remains%20the%20third%20most%20prevalent%20malignancy%20globally%2C%0Awith%20approximately%20154%2C000%20new%20cases%20and%2054%2C000%20projected%20deaths%20anticipated%0Afor%202025.%20The%20recent%20advancement%20of%20foundation%20models%20in%20computational%0Apathology%20has%20been%20largely%20propelled%20by%20task%20agnostic%20methodologies%20that%20can%0Aoverlook%20organ-specific%20crucial%20morphological%20patterns%20that%20represent%20distinct%0Abiological%20processes%20that%20can%20fundamentally%20influence%20tumor%20behavior%2C%0Atherapeutic%20response%2C%20and%20patient%20outcomes.%20The%20aim%20of%20this%20study%20is%20to%20develop%0Aa%20novel%2C%20interpretable%20AI%20model%2C%20PRISM%20%28Prognostic%20Representation%20of%20Integrated%0ASpatial%20Morphology%29%2C%20that%20incorporates%20a%20continuous%20variability%20spectrum%20within%0Aeach%20distinct%20morphology%20to%20characterize%20phenotypic%20diversity%20and%20reflecting%0Athe%20principle%20that%20malignant%20transformation%20occurs%20through%20incremental%0Aevolutionary%20processes%20rather%20than%20abrupt%20phenotypic%20shifts.%20PRISM%20is%20trained%0Aon%208.74%20million%20histological%20images%20extracted%20from%20surgical%20resection%20specimens%0Aof%20424%20patients%20with%20stage%20III%20CRC.%20PRISM%20achieved%20superior%20prognostic%0Aperformance%20for%20five-year%20OS%20%28AUC%20%3D%200.70%20%2B-%200.04%3B%20accuracy%20%3D%2068.37%25%20%2B-%204.75%25%3B%0AHR%20%3D%203.34%2C%2095%25%20CI%20%3D%202.28-4.90%3B%20p%20%3C%200.0001%29%2C%20outperforming%20existing%20CRC-specific%0Amethods%20by%2015%25%20and%20AI%20foundation%20models%20by%20~23%25%20accuracy.%20It%20showed%0Asex-agnostic%20robustness%20%28AUC%20delta%20%3D%200.02%3B%20accuracy%20delta%20%3D%200.15%25%29%20and%20stable%0Aperformance%20across%20clinicopathological%20subgroups%2C%20with%20minimal%20accuracy%0Afluctuation%20%28delta%20%3D%201.44%25%29%20between%205FU/LV%20and%20CPT-11/5FU/LV%20regimens%2C%0Areplicating%20the%20Alliance%20cohort%20finding%20of%20no%20survival%20difference%20between%0Atreatments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphology-Aware%2520Prognostic%2520model%2520for%2520Five-Year%2520Survival%2520Prediction%2520in%250A%2520%2520Colorectal%2520Cancer%2520from%2520H%2526E%2520Whole%2520Slide%2520Images%26entry.906535625%3DUsama%2520Sajjad%2520and%2520Abdul%2520Rehman%2520Akbar%2520and%2520Ziyu%2520Su%2520and%2520Deborah%2520Knight%2520and%2520Wendy%2520L.%2520Frankel%2520and%2520Metin%2520N.%2520Gurcan%2520and%2520Wei%2520Chen%2520and%2520Muhammad%2520Khalid%2520Khan%2520Niazi%26entry.1292438233%3D%2520%2520Colorectal%2520cancer%2520%2528CRC%2529%2520remains%2520the%2520third%2520most%2520prevalent%2520malignancy%2520globally%252C%250Awith%2520approximately%2520154%252C000%2520new%2520cases%2520and%252054%252C000%2520projected%2520deaths%2520anticipated%250Afor%25202025.%2520The%2520recent%2520advancement%2520of%2520foundation%2520models%2520in%2520computational%250Apathology%2520has%2520been%2520largely%2520propelled%2520by%2520task%2520agnostic%2520methodologies%2520that%2520can%250Aoverlook%2520organ-specific%2520crucial%2520morphological%2520patterns%2520that%2520represent%2520distinct%250Abiological%2520processes%2520that%2520can%2520fundamentally%2520influence%2520tumor%2520behavior%252C%250Atherapeutic%2520response%252C%2520and%2520patient%2520outcomes.%2520The%2520aim%2520of%2520this%2520study%2520is%2520to%2520develop%250Aa%2520novel%252C%2520interpretable%2520AI%2520model%252C%2520PRISM%2520%2528Prognostic%2520Representation%2520of%2520Integrated%250ASpatial%2520Morphology%2529%252C%2520that%2520incorporates%2520a%2520continuous%2520variability%2520spectrum%2520within%250Aeach%2520distinct%2520morphology%2520to%2520characterize%2520phenotypic%2520diversity%2520and%2520reflecting%250Athe%2520principle%2520that%2520malignant%2520transformation%2520occurs%2520through%2520incremental%250Aevolutionary%2520processes%2520rather%2520than%2520abrupt%2520phenotypic%2520shifts.%2520PRISM%2520is%2520trained%250Aon%25208.74%2520million%2520histological%2520images%2520extracted%2520from%2520surgical%2520resection%2520specimens%250Aof%2520424%2520patients%2520with%2520stage%2520III%2520CRC.%2520PRISM%2520achieved%2520superior%2520prognostic%250Aperformance%2520for%2520five-year%2520OS%2520%2528AUC%2520%253D%25200.70%2520%252B-%25200.04%253B%2520accuracy%2520%253D%252068.37%2525%2520%252B-%25204.75%2525%253B%250AHR%2520%253D%25203.34%252C%252095%2525%2520CI%2520%253D%25202.28-4.90%253B%2520p%2520%253C%25200.0001%2529%252C%2520outperforming%2520existing%2520CRC-specific%250Amethods%2520by%252015%2525%2520and%2520AI%2520foundation%2520models%2520by%2520~23%2525%2520accuracy.%2520It%2520showed%250Asex-agnostic%2520robustness%2520%2528AUC%2520delta%2520%253D%25200.02%253B%2520accuracy%2520delta%2520%253D%25200.15%2525%2529%2520and%2520stable%250Aperformance%2520across%2520clinicopathological%2520subgroups%252C%2520with%2520minimal%2520accuracy%250Afluctuation%2520%2528delta%2520%253D%25201.44%2525%2529%2520between%25205FU/LV%2520and%2520CPT-11/5FU/LV%2520regimens%252C%250Areplicating%2520the%2520Alliance%2520cohort%2520finding%2520of%2520no%2520survival%2520difference%2520between%250Atreatments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphology-Aware%20Prognostic%20model%20for%20Five-Year%20Survival%20Prediction%20in%0A%20%20Colorectal%20Cancer%20from%20H%26E%20Whole%20Slide%20Images&entry.906535625=Usama%20Sajjad%20and%20Abdul%20Rehman%20Akbar%20and%20Ziyu%20Su%20and%20Deborah%20Knight%20and%20Wendy%20L.%20Frankel%20and%20Metin%20N.%20Gurcan%20and%20Wei%20Chen%20and%20Muhammad%20Khalid%20Khan%20Niazi&entry.1292438233=%20%20Colorectal%20cancer%20%28CRC%29%20remains%20the%20third%20most%20prevalent%20malignancy%20globally%2C%0Awith%20approximately%20154%2C000%20new%20cases%20and%2054%2C000%20projected%20deaths%20anticipated%0Afor%202025.%20The%20recent%20advancement%20of%20foundation%20models%20in%20computational%0Apathology%20has%20been%20largely%20propelled%20by%20task%20agnostic%20methodologies%20that%20can%0Aoverlook%20organ-specific%20crucial%20morphological%20patterns%20that%20represent%20distinct%0Abiological%20processes%20that%20can%20fundamentally%20influence%20tumor%20behavior%2C%0Atherapeutic%20response%2C%20and%20patient%20outcomes.%20The%20aim%20of%20this%20study%20is%20to%20develop%0Aa%20novel%2C%20interpretable%20AI%20model%2C%20PRISM%20%28Prognostic%20Representation%20of%20Integrated%0ASpatial%20Morphology%29%2C%20that%20incorporates%20a%20continuous%20variability%20spectrum%20within%0Aeach%20distinct%20morphology%20to%20characterize%20phenotypic%20diversity%20and%20reflecting%0Athe%20principle%20that%20malignant%20transformation%20occurs%20through%20incremental%0Aevolutionary%20processes%20rather%20than%20abrupt%20phenotypic%20shifts.%20PRISM%20is%20trained%0Aon%208.74%20million%20histological%20images%20extracted%20from%20surgical%20resection%20specimens%0Aof%20424%20patients%20with%20stage%20III%20CRC.%20PRISM%20achieved%20superior%20prognostic%0Aperformance%20for%20five-year%20OS%20%28AUC%20%3D%200.70%20%2B-%200.04%3B%20accuracy%20%3D%2068.37%25%20%2B-%204.75%25%3B%0AHR%20%3D%203.34%2C%2095%25%20CI%20%3D%202.28-4.90%3B%20p%20%3C%200.0001%29%2C%20outperforming%20existing%20CRC-specific%0Amethods%20by%2015%25%20and%20AI%20foundation%20models%20by%20~23%25%20accuracy.%20It%20showed%0Asex-agnostic%20robustness%20%28AUC%20delta%20%3D%200.02%3B%20accuracy%20delta%20%3D%200.15%25%29%20and%20stable%0Aperformance%20across%20clinicopathological%20subgroups%2C%20with%20minimal%20accuracy%0Afluctuation%20%28delta%20%3D%201.44%25%29%20between%205FU/LV%20and%20CPT-11/5FU/LV%20regimens%2C%0Areplicating%20the%20Alliance%20cohort%20finding%20of%20no%20survival%20difference%20between%0Atreatments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14800v1&entry.124074799=Read"},
{"title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training", "author": "Yiming Wang and Da Yin and Yuedong Cui and Ruichen Zheng and Zhiqian Li and Zongyu Lin and Di Wu and Xueqing Wu and Chenchen Ye and Yu Zhou and Kai-Wei Chang", "abstract": "  Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.\n", "link": "http://arxiv.org/abs/2510.14969v1", "date": "2025-10-16", "relevancy": 2.2244, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5632}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5586}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Scalable%2C%20General-Purpose%20Simulators%20For%20Evolving%20Digital%20Agent%0A%20%20Training&body=Title%3A%20LLMs%20as%20Scalable%2C%20General-Purpose%20Simulators%20For%20Evolving%20Digital%20Agent%0A%20%20Training%0AAuthor%3A%20Yiming%20Wang%20and%20Da%20Yin%20and%20Yuedong%20Cui%20and%20Ruichen%20Zheng%20and%20Zhiqian%20Li%20and%20Zongyu%20Lin%20and%20Di%20Wu%20and%20Xueqing%20Wu%20and%20Chenchen%20Ye%20and%20Yu%20Zhou%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20Digital%20agents%20require%20diverse%2C%20large-scale%20UI%20trajectories%20to%20generalize%0Aacross%20real-world%20tasks%2C%20yet%20collecting%20such%20data%20is%20prohibitively%20expensive%20in%0Aboth%20human%20annotation%2C%20infra%20and%20engineering%20perspectives.%20To%20this%20end%2C%20we%0Aintroduce%20%24%5Ctextbf%7BUI-Simulator%7D%24%2C%20a%20scalable%20paradigm%20that%20generates%0Astructured%20UI%20states%20and%20transitions%20to%20synthesize%20training%20trajectories%20at%0Ascale.%20Our%20paradigm%20integrates%20a%20digital%20world%20simulator%20for%20diverse%20UI%20states%2C%0Aa%20guided%20rollout%20process%20for%20coherent%20exploration%2C%20and%20a%20trajectory%20wrapper%0Athat%20produces%20high-quality%20and%20diverse%20trajectories%20for%20agent%20training.%20We%0Afurther%20propose%20%24%5Ctextbf%7BUI-Simulator-Grow%7D%24%2C%20a%20targeted%20scaling%20strategy%20that%0Aenables%20more%20rapid%20and%20data-efficient%20scaling%20by%20prioritizing%20high-impact%20tasks%0Aand%20synthesizes%20informative%20trajectory%20variants.%20Experiments%20on%20WebArena%20and%0AAndroidWorld%20show%20that%20UI-Simulator%20rivals%20or%20surpasses%20open-source%20agents%0Atrained%20on%20real%20UIs%20with%20significantly%20better%20robustness%2C%20despite%20using%20weaker%0Ateacher%20models.%20Moreover%2C%20UI-Simulator-Grow%20matches%20the%20performance%20of%0ALlama-3-70B-Instruct%20using%20only%20Llama-3-8B-Instruct%20as%20the%20base%20model%2C%0Ahighlighting%20the%20potential%20of%20targeted%20synthesis%20scaling%20paradigm%20to%0Acontinuously%20and%20efficiently%20enhance%20the%20digital%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520Scalable%252C%2520General-Purpose%2520Simulators%2520For%2520Evolving%2520Digital%2520Agent%250A%2520%2520Training%26entry.906535625%3DYiming%2520Wang%2520and%2520Da%2520Yin%2520and%2520Yuedong%2520Cui%2520and%2520Ruichen%2520Zheng%2520and%2520Zhiqian%2520Li%2520and%2520Zongyu%2520Lin%2520and%2520Di%2520Wu%2520and%2520Xueqing%2520Wu%2520and%2520Chenchen%2520Ye%2520and%2520Yu%2520Zhou%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520Digital%2520agents%2520require%2520diverse%252C%2520large-scale%2520UI%2520trajectories%2520to%2520generalize%250Aacross%2520real-world%2520tasks%252C%2520yet%2520collecting%2520such%2520data%2520is%2520prohibitively%2520expensive%2520in%250Aboth%2520human%2520annotation%252C%2520infra%2520and%2520engineering%2520perspectives.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520%2524%255Ctextbf%257BUI-Simulator%257D%2524%252C%2520a%2520scalable%2520paradigm%2520that%2520generates%250Astructured%2520UI%2520states%2520and%2520transitions%2520to%2520synthesize%2520training%2520trajectories%2520at%250Ascale.%2520Our%2520paradigm%2520integrates%2520a%2520digital%2520world%2520simulator%2520for%2520diverse%2520UI%2520states%252C%250Aa%2520guided%2520rollout%2520process%2520for%2520coherent%2520exploration%252C%2520and%2520a%2520trajectory%2520wrapper%250Athat%2520produces%2520high-quality%2520and%2520diverse%2520trajectories%2520for%2520agent%2520training.%2520We%250Afurther%2520propose%2520%2524%255Ctextbf%257BUI-Simulator-Grow%257D%2524%252C%2520a%2520targeted%2520scaling%2520strategy%2520that%250Aenables%2520more%2520rapid%2520and%2520data-efficient%2520scaling%2520by%2520prioritizing%2520high-impact%2520tasks%250Aand%2520synthesizes%2520informative%2520trajectory%2520variants.%2520Experiments%2520on%2520WebArena%2520and%250AAndroidWorld%2520show%2520that%2520UI-Simulator%2520rivals%2520or%2520surpasses%2520open-source%2520agents%250Atrained%2520on%2520real%2520UIs%2520with%2520significantly%2520better%2520robustness%252C%2520despite%2520using%2520weaker%250Ateacher%2520models.%2520Moreover%252C%2520UI-Simulator-Grow%2520matches%2520the%2520performance%2520of%250ALlama-3-70B-Instruct%2520using%2520only%2520Llama-3-8B-Instruct%2520as%2520the%2520base%2520model%252C%250Ahighlighting%2520the%2520potential%2520of%2520targeted%2520synthesis%2520scaling%2520paradigm%2520to%250Acontinuously%2520and%2520efficiently%2520enhance%2520the%2520digital%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Scalable%2C%20General-Purpose%20Simulators%20For%20Evolving%20Digital%20Agent%0A%20%20Training&entry.906535625=Yiming%20Wang%20and%20Da%20Yin%20and%20Yuedong%20Cui%20and%20Ruichen%20Zheng%20and%20Zhiqian%20Li%20and%20Zongyu%20Lin%20and%20Di%20Wu%20and%20Xueqing%20Wu%20and%20Chenchen%20Ye%20and%20Yu%20Zhou%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20Digital%20agents%20require%20diverse%2C%20large-scale%20UI%20trajectories%20to%20generalize%0Aacross%20real-world%20tasks%2C%20yet%20collecting%20such%20data%20is%20prohibitively%20expensive%20in%0Aboth%20human%20annotation%2C%20infra%20and%20engineering%20perspectives.%20To%20this%20end%2C%20we%0Aintroduce%20%24%5Ctextbf%7BUI-Simulator%7D%24%2C%20a%20scalable%20paradigm%20that%20generates%0Astructured%20UI%20states%20and%20transitions%20to%20synthesize%20training%20trajectories%20at%0Ascale.%20Our%20paradigm%20integrates%20a%20digital%20world%20simulator%20for%20diverse%20UI%20states%2C%0Aa%20guided%20rollout%20process%20for%20coherent%20exploration%2C%20and%20a%20trajectory%20wrapper%0Athat%20produces%20high-quality%20and%20diverse%20trajectories%20for%20agent%20training.%20We%0Afurther%20propose%20%24%5Ctextbf%7BUI-Simulator-Grow%7D%24%2C%20a%20targeted%20scaling%20strategy%20that%0Aenables%20more%20rapid%20and%20data-efficient%20scaling%20by%20prioritizing%20high-impact%20tasks%0Aand%20synthesizes%20informative%20trajectory%20variants.%20Experiments%20on%20WebArena%20and%0AAndroidWorld%20show%20that%20UI-Simulator%20rivals%20or%20surpasses%20open-source%20agents%0Atrained%20on%20real%20UIs%20with%20significantly%20better%20robustness%2C%20despite%20using%20weaker%0Ateacher%20models.%20Moreover%2C%20UI-Simulator-Grow%20matches%20the%20performance%20of%0ALlama-3-70B-Instruct%20using%20only%20Llama-3-8B-Instruct%20as%20the%20base%20model%2C%0Ahighlighting%20the%20potential%20of%20targeted%20synthesis%20scaling%20paradigm%20to%0Acontinuously%20and%20efficiently%20enhance%20the%20digital%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14969v1&entry.124074799=Read"},
{"title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source\n  Suites", "author": "Zhenxin Lei and Zhangwei Gao and Changyao Tian and Erfei Cui and Guanzhou Chen and Danni Yang and Yuchen Duan and Zhaokai Wang and Wenhao Li and Weiyun Wang and Xiangyu Zhao and Jiayi Ji and Yu Qiao and Wenhai Wang and Gen Luo", "abstract": "  Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.\n", "link": "http://arxiv.org/abs/2510.12126v3", "date": "2025-10-16", "relevancy": 2.219, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaCaptioner%3A%20Towards%20Generalist%20Visual%20Captioning%20with%20Open-source%0A%20%20Suites&body=Title%3A%20MetaCaptioner%3A%20Towards%20Generalist%20Visual%20Captioning%20with%20Open-source%0A%20%20Suites%0AAuthor%3A%20Zhenxin%20Lei%20and%20Zhangwei%20Gao%20and%20Changyao%20Tian%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Danni%20Yang%20and%20Yuchen%20Duan%20and%20Zhaokai%20Wang%20and%20Wenhao%20Li%20and%20Weiyun%20Wang%20and%20Xiangyu%20Zhao%20and%20Jiayi%20Ji%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo%0AAbstract%3A%20%20%20Generalist%20visual%20captioning%20goes%20beyond%20a%20simple%20appearance%20description%0Atask%2C%20but%20requires%20integrating%20a%20series%20of%20visual%20cues%20into%20a%20caption%20and%0Ahandling%20various%20visual%20domains.%20In%20this%20task%2C%20current%20open-source%20models%0Apresent%20a%20large%20performance%20gap%20with%20commercial%20ones%2C%20which%20limits%20various%0Aapplications%20such%20as%20data%20synthesis.%20To%20bridge%20the%20gap%2C%20this%20paper%20proposes%0ACapFlow%2C%20a%20novel%20multi-agent%20collaboration%20workflow.%20CapFlow%20demonstrates%20for%0Athe%20first%20time%20that%2C%20by%20capitalizing%20on%20open-source%20models%2C%20it%20is%20possible%20to%0Aachieve%20caption%20quality%20on%20par%20with%20GPT-4.1%20in%20various%20domains%20with%20an%2089.5%25%0Areduction%20in%20costs.%20By%20leveraging%20CapFlow%20as%20the%20data%20synthesizer%2C%20we%20produce%0Ahigh-quality%20visual%20captions%20from%20image%20and%20video%20domains%20at%20scale%2C%20and%20obtain%0Aa%20generalist%20visual%20captioner%20via%20fine-tuning%2C%20namely%20MetaCaptioner.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20MetaCaptioner%20not%20only%20achieves%20comparable%0Acaptioning%20capabilities%20with%20commercial%20models%20but%20also%20reaches%20top-tier%0Amultimodal%20performance%20in%20the%20open-source%20community.%20We%20hope%20CapFlow%20and%0AMetaCaptioner%20can%20benefit%20future%20multimodal%20research%20by%20providing%20a%20strong%20and%0Acost-effective%20visual%20captioning%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12126v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaCaptioner%253A%2520Towards%2520Generalist%2520Visual%2520Captioning%2520with%2520Open-source%250A%2520%2520Suites%26entry.906535625%3DZhenxin%2520Lei%2520and%2520Zhangwei%2520Gao%2520and%2520Changyao%2520Tian%2520and%2520Erfei%2520Cui%2520and%2520Guanzhou%2520Chen%2520and%2520Danni%2520Yang%2520and%2520Yuchen%2520Duan%2520and%2520Zhaokai%2520Wang%2520and%2520Wenhao%2520Li%2520and%2520Weiyun%2520Wang%2520and%2520Xiangyu%2520Zhao%2520and%2520Jiayi%2520Ji%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Gen%2520Luo%26entry.1292438233%3D%2520%2520Generalist%2520visual%2520captioning%2520goes%2520beyond%2520a%2520simple%2520appearance%2520description%250Atask%252C%2520but%2520requires%2520integrating%2520a%2520series%2520of%2520visual%2520cues%2520into%2520a%2520caption%2520and%250Ahandling%2520various%2520visual%2520domains.%2520In%2520this%2520task%252C%2520current%2520open-source%2520models%250Apresent%2520a%2520large%2520performance%2520gap%2520with%2520commercial%2520ones%252C%2520which%2520limits%2520various%250Aapplications%2520such%2520as%2520data%2520synthesis.%2520To%2520bridge%2520the%2520gap%252C%2520this%2520paper%2520proposes%250ACapFlow%252C%2520a%2520novel%2520multi-agent%2520collaboration%2520workflow.%2520CapFlow%2520demonstrates%2520for%250Athe%2520first%2520time%2520that%252C%2520by%2520capitalizing%2520on%2520open-source%2520models%252C%2520it%2520is%2520possible%2520to%250Aachieve%2520caption%2520quality%2520on%2520par%2520with%2520GPT-4.1%2520in%2520various%2520domains%2520with%2520an%252089.5%2525%250Areduction%2520in%2520costs.%2520By%2520leveraging%2520CapFlow%2520as%2520the%2520data%2520synthesizer%252C%2520we%2520produce%250Ahigh-quality%2520visual%2520captions%2520from%2520image%2520and%2520video%2520domains%2520at%2520scale%252C%2520and%2520obtain%250Aa%2520generalist%2520visual%2520captioner%2520via%2520fine-tuning%252C%2520namely%2520MetaCaptioner.%2520Through%250Aextensive%2520experiments%252C%2520we%2520show%2520that%2520MetaCaptioner%2520not%2520only%2520achieves%2520comparable%250Acaptioning%2520capabilities%2520with%2520commercial%2520models%2520but%2520also%2520reaches%2520top-tier%250Amultimodal%2520performance%2520in%2520the%2520open-source%2520community.%2520We%2520hope%2520CapFlow%2520and%250AMetaCaptioner%2520can%2520benefit%2520future%2520multimodal%2520research%2520by%2520providing%2520a%2520strong%2520and%250Acost-effective%2520visual%2520captioning%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12126v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaCaptioner%3A%20Towards%20Generalist%20Visual%20Captioning%20with%20Open-source%0A%20%20Suites&entry.906535625=Zhenxin%20Lei%20and%20Zhangwei%20Gao%20and%20Changyao%20Tian%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Danni%20Yang%20and%20Yuchen%20Duan%20and%20Zhaokai%20Wang%20and%20Wenhao%20Li%20and%20Weiyun%20Wang%20and%20Xiangyu%20Zhao%20and%20Jiayi%20Ji%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo&entry.1292438233=%20%20Generalist%20visual%20captioning%20goes%20beyond%20a%20simple%20appearance%20description%0Atask%2C%20but%20requires%20integrating%20a%20series%20of%20visual%20cues%20into%20a%20caption%20and%0Ahandling%20various%20visual%20domains.%20In%20this%20task%2C%20current%20open-source%20models%0Apresent%20a%20large%20performance%20gap%20with%20commercial%20ones%2C%20which%20limits%20various%0Aapplications%20such%20as%20data%20synthesis.%20To%20bridge%20the%20gap%2C%20this%20paper%20proposes%0ACapFlow%2C%20a%20novel%20multi-agent%20collaboration%20workflow.%20CapFlow%20demonstrates%20for%0Athe%20first%20time%20that%2C%20by%20capitalizing%20on%20open-source%20models%2C%20it%20is%20possible%20to%0Aachieve%20caption%20quality%20on%20par%20with%20GPT-4.1%20in%20various%20domains%20with%20an%2089.5%25%0Areduction%20in%20costs.%20By%20leveraging%20CapFlow%20as%20the%20data%20synthesizer%2C%20we%20produce%0Ahigh-quality%20visual%20captions%20from%20image%20and%20video%20domains%20at%20scale%2C%20and%20obtain%0Aa%20generalist%20visual%20captioner%20via%20fine-tuning%2C%20namely%20MetaCaptioner.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20MetaCaptioner%20not%20only%20achieves%20comparable%0Acaptioning%20capabilities%20with%20commercial%20models%20but%20also%20reaches%20top-tier%0Amultimodal%20performance%20in%20the%20open-source%20community.%20We%20hope%20CapFlow%20and%0AMetaCaptioner%20can%20benefit%20future%20multimodal%20research%20by%20providing%20a%20strong%20and%0Acost-effective%20visual%20captioning%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12126v3&entry.124074799=Read"},
{"title": "Rethinking Hebbian Principle: Low-Dimensional Structural Projection for\n  Unsupervised Learning", "author": "Shikuang Deng and Jiayuan Zhang and Yuhang Wu and Ting Chen and Shi Gu", "abstract": "  Hebbian learning is a biological principle that intuitively describes how\nneurons adapt their connections through repeated stimuli. However, when applied\nto machine learning, it suffers serious issues due to the unconstrained updates\nof the connections and the lack of accounting for feedback mediation. Such\nshortcomings limit its effective scaling to complex network architectures and\ntasks. To this end, here we introduce the Structural Projection Hebbian\nRepresentation (SPHeRe), a novel unsupervised learning method that integrates\northogonality and structural information preservation through a local auxiliary\nnonlinear block. The loss for structural information preservation\nbackpropagates to the input through an auxiliary lightweight projection that\nconceptually serves as feedback mediation while the orthogonality constraints\naccount for the boundedness of updating magnitude. Extensive experimental\nresults show that SPHeRe achieves SOTA performance among unsupervised synaptic\nplasticity approaches on standard image classification benchmarks, including\nCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong\neffectiveness in continual learning and transfer learning scenarios, and image\nreconstruction tasks show the robustness and generalizability of the extracted\nfeatures. This work demonstrates the competitiveness and potential of Hebbian\nunsupervised learning rules within modern deep learning frameworks,\ndemonstrating the possibility of efficient and biologically inspired learning\nalgorithms without the strong dependence on strict backpropagation. Our code is\navailable at https://github.com/brain-intelligence-lab/SPHeRe.\n", "link": "http://arxiv.org/abs/2510.14810v1", "date": "2025-10-16", "relevancy": 2.2115, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5234}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Hebbian%20Principle%3A%20Low-Dimensional%20Structural%20Projection%20for%0A%20%20Unsupervised%20Learning&body=Title%3A%20Rethinking%20Hebbian%20Principle%3A%20Low-Dimensional%20Structural%20Projection%20for%0A%20%20Unsupervised%20Learning%0AAuthor%3A%20Shikuang%20Deng%20and%20Jiayuan%20Zhang%20and%20Yuhang%20Wu%20and%20Ting%20Chen%20and%20Shi%20Gu%0AAbstract%3A%20%20%20Hebbian%20learning%20is%20a%20biological%20principle%20that%20intuitively%20describes%20how%0Aneurons%20adapt%20their%20connections%20through%20repeated%20stimuli.%20However%2C%20when%20applied%0Ato%20machine%20learning%2C%20it%20suffers%20serious%20issues%20due%20to%20the%20unconstrained%20updates%0Aof%20the%20connections%20and%20the%20lack%20of%20accounting%20for%20feedback%20mediation.%20Such%0Ashortcomings%20limit%20its%20effective%20scaling%20to%20complex%20network%20architectures%20and%0Atasks.%20To%20this%20end%2C%20here%20we%20introduce%20the%20Structural%20Projection%20Hebbian%0ARepresentation%20%28SPHeRe%29%2C%20a%20novel%20unsupervised%20learning%20method%20that%20integrates%0Aorthogonality%20and%20structural%20information%20preservation%20through%20a%20local%20auxiliary%0Anonlinear%20block.%20The%20loss%20for%20structural%20information%20preservation%0Abackpropagates%20to%20the%20input%20through%20an%20auxiliary%20lightweight%20projection%20that%0Aconceptually%20serves%20as%20feedback%20mediation%20while%20the%20orthogonality%20constraints%0Aaccount%20for%20the%20boundedness%20of%20updating%20magnitude.%20Extensive%20experimental%0Aresults%20show%20that%20SPHeRe%20achieves%20SOTA%20performance%20among%20unsupervised%20synaptic%0Aplasticity%20approaches%20on%20standard%20image%20classification%20benchmarks%2C%20including%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny-ImageNet.%20Furthermore%2C%20the%20method%20exhibits%20strong%0Aeffectiveness%20in%20continual%20learning%20and%20transfer%20learning%20scenarios%2C%20and%20image%0Areconstruction%20tasks%20show%20the%20robustness%20and%20generalizability%20of%20the%20extracted%0Afeatures.%20This%20work%20demonstrates%20the%20competitiveness%20and%20potential%20of%20Hebbian%0Aunsupervised%20learning%20rules%20within%20modern%20deep%20learning%20frameworks%2C%0Ademonstrating%20the%20possibility%20of%20efficient%20and%20biologically%20inspired%20learning%0Aalgorithms%20without%20the%20strong%20dependence%20on%20strict%20backpropagation.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/brain-intelligence-lab/SPHeRe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Hebbian%2520Principle%253A%2520Low-Dimensional%2520Structural%2520Projection%2520for%250A%2520%2520Unsupervised%2520Learning%26entry.906535625%3DShikuang%2520Deng%2520and%2520Jiayuan%2520Zhang%2520and%2520Yuhang%2520Wu%2520and%2520Ting%2520Chen%2520and%2520Shi%2520Gu%26entry.1292438233%3D%2520%2520Hebbian%2520learning%2520is%2520a%2520biological%2520principle%2520that%2520intuitively%2520describes%2520how%250Aneurons%2520adapt%2520their%2520connections%2520through%2520repeated%2520stimuli.%2520However%252C%2520when%2520applied%250Ato%2520machine%2520learning%252C%2520it%2520suffers%2520serious%2520issues%2520due%2520to%2520the%2520unconstrained%2520updates%250Aof%2520the%2520connections%2520and%2520the%2520lack%2520of%2520accounting%2520for%2520feedback%2520mediation.%2520Such%250Ashortcomings%2520limit%2520its%2520effective%2520scaling%2520to%2520complex%2520network%2520architectures%2520and%250Atasks.%2520To%2520this%2520end%252C%2520here%2520we%2520introduce%2520the%2520Structural%2520Projection%2520Hebbian%250ARepresentation%2520%2528SPHeRe%2529%252C%2520a%2520novel%2520unsupervised%2520learning%2520method%2520that%2520integrates%250Aorthogonality%2520and%2520structural%2520information%2520preservation%2520through%2520a%2520local%2520auxiliary%250Anonlinear%2520block.%2520The%2520loss%2520for%2520structural%2520information%2520preservation%250Abackpropagates%2520to%2520the%2520input%2520through%2520an%2520auxiliary%2520lightweight%2520projection%2520that%250Aconceptually%2520serves%2520as%2520feedback%2520mediation%2520while%2520the%2520orthogonality%2520constraints%250Aaccount%2520for%2520the%2520boundedness%2520of%2520updating%2520magnitude.%2520Extensive%2520experimental%250Aresults%2520show%2520that%2520SPHeRe%2520achieves%2520SOTA%2520performance%2520among%2520unsupervised%2520synaptic%250Aplasticity%2520approaches%2520on%2520standard%2520image%2520classification%2520benchmarks%252C%2520including%250ACIFAR-10%252C%2520CIFAR-100%252C%2520and%2520Tiny-ImageNet.%2520Furthermore%252C%2520the%2520method%2520exhibits%2520strong%250Aeffectiveness%2520in%2520continual%2520learning%2520and%2520transfer%2520learning%2520scenarios%252C%2520and%2520image%250Areconstruction%2520tasks%2520show%2520the%2520robustness%2520and%2520generalizability%2520of%2520the%2520extracted%250Afeatures.%2520This%2520work%2520demonstrates%2520the%2520competitiveness%2520and%2520potential%2520of%2520Hebbian%250Aunsupervised%2520learning%2520rules%2520within%2520modern%2520deep%2520learning%2520frameworks%252C%250Ademonstrating%2520the%2520possibility%2520of%2520efficient%2520and%2520biologically%2520inspired%2520learning%250Aalgorithms%2520without%2520the%2520strong%2520dependence%2520on%2520strict%2520backpropagation.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/brain-intelligence-lab/SPHeRe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Hebbian%20Principle%3A%20Low-Dimensional%20Structural%20Projection%20for%0A%20%20Unsupervised%20Learning&entry.906535625=Shikuang%20Deng%20and%20Jiayuan%20Zhang%20and%20Yuhang%20Wu%20and%20Ting%20Chen%20and%20Shi%20Gu&entry.1292438233=%20%20Hebbian%20learning%20is%20a%20biological%20principle%20that%20intuitively%20describes%20how%0Aneurons%20adapt%20their%20connections%20through%20repeated%20stimuli.%20However%2C%20when%20applied%0Ato%20machine%20learning%2C%20it%20suffers%20serious%20issues%20due%20to%20the%20unconstrained%20updates%0Aof%20the%20connections%20and%20the%20lack%20of%20accounting%20for%20feedback%20mediation.%20Such%0Ashortcomings%20limit%20its%20effective%20scaling%20to%20complex%20network%20architectures%20and%0Atasks.%20To%20this%20end%2C%20here%20we%20introduce%20the%20Structural%20Projection%20Hebbian%0ARepresentation%20%28SPHeRe%29%2C%20a%20novel%20unsupervised%20learning%20method%20that%20integrates%0Aorthogonality%20and%20structural%20information%20preservation%20through%20a%20local%20auxiliary%0Anonlinear%20block.%20The%20loss%20for%20structural%20information%20preservation%0Abackpropagates%20to%20the%20input%20through%20an%20auxiliary%20lightweight%20projection%20that%0Aconceptually%20serves%20as%20feedback%20mediation%20while%20the%20orthogonality%20constraints%0Aaccount%20for%20the%20boundedness%20of%20updating%20magnitude.%20Extensive%20experimental%0Aresults%20show%20that%20SPHeRe%20achieves%20SOTA%20performance%20among%20unsupervised%20synaptic%0Aplasticity%20approaches%20on%20standard%20image%20classification%20benchmarks%2C%20including%0ACIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny-ImageNet.%20Furthermore%2C%20the%20method%20exhibits%20strong%0Aeffectiveness%20in%20continual%20learning%20and%20transfer%20learning%20scenarios%2C%20and%20image%0Areconstruction%20tasks%20show%20the%20robustness%20and%20generalizability%20of%20the%20extracted%0Afeatures.%20This%20work%20demonstrates%20the%20competitiveness%20and%20potential%20of%20Hebbian%0Aunsupervised%20learning%20rules%20within%20modern%20deep%20learning%20frameworks%2C%0Ademonstrating%20the%20possibility%20of%20efficient%20and%20biologically%20inspired%20learning%0Aalgorithms%20without%20the%20strong%20dependence%20on%20strict%20backpropagation.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/brain-intelligence-lab/SPHeRe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14810v1&entry.124074799=Read"},
{"title": "Attention Is All You Need for KV Cache in Diffusion LLMs", "author": "Quan Nguyen-Tri and Mukul Ranjan and Zhiqiang Shen", "abstract": "  This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.\n", "link": "http://arxiv.org/abs/2510.14973v1", "date": "2025-10-16", "relevancy": 2.2041, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5923}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5883}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Is%20All%20You%20Need%20for%20KV%20Cache%20in%20Diffusion%20LLMs&body=Title%3A%20Attention%20Is%20All%20You%20Need%20for%20KV%20Cache%20in%20Diffusion%20LLMs%0AAuthor%3A%20Quan%20Nguyen-Tri%20and%20Mukul%20Ranjan%20and%20Zhiqiang%20Shen%0AAbstract%3A%20%20%20This%20work%20studies%20how%20to%20adaptively%20recompute%20key-value%20%28KV%29%20caches%20for%0Adiffusion%20large%20language%20models%20%28DLMs%29%20to%20maximize%20prediction%20accuracy%20while%0Aminimizing%20decoding%20latency.%20Prior%20methods%27%20decoders%20recompute%20QKV%20for%20all%0Atokens%20at%20every%20denoising%20step%20and%20layer%2C%20despite%20KV%20states%20changing%20little%0Aacross%20most%20steps%2C%20especially%20in%20shallow%20layers%2C%20leading%20to%20substantial%0Aredundancy.%20We%20make%20three%20observations%3A%20%281%29%20distant%20%24%7B%5Cbf%20MASK%7D%24%20tokens%0Aprimarily%20act%20as%20a%20length-bias%20and%20can%20be%20cached%20block-wise%20beyond%20the%20active%0Aprediction%20window%3B%20%282%29%20KV%20dynamics%20increase%20with%20depth%2C%20suggesting%20that%0Aselective%20refresh%20starting%20from%20deeper%20layers%20is%20sufficient%3B%20and%20%283%29%20the%0Amost-attended%20token%20exhibits%20the%20smallest%20KV%20drift%2C%20providing%20a%20conservative%0Alower%20bound%20on%20cache%20change%20for%20other%20tokens.%20Building%20on%20these%2C%20we%20propose%0A%24%7B%5Cbf%20Elastic-Cache%7D%24%2C%20a%20training-free%2C%20architecture-agnostic%20strategy%20that%0Ajointly%20decides%20%24%7Bwhen%7D%24%20to%20refresh%20%28via%20an%20attention-aware%20drift%20test%20on%20the%0Amost-attended%20token%29%20and%20%24%7Bwhere%7D%24%20to%20refresh%20%28via%20a%20depth-aware%20schedule%20that%0Arecomputes%20from%20a%20chosen%20layer%20onward%20while%20reusing%20shallow-layer%20caches%20and%0Aoff-window%20MASK%20caches%29.%20Unlike%20fixed-period%20schemes%2C%20Elastic-Cache%20performs%0Aadaptive%2C%20layer-aware%20cache%20updates%20for%20diffusion%20LLMs%2C%20reducing%20redundant%0Acomputation%20and%20accelerating%20decoding%20with%20negligible%20loss%20in%20generation%0Aquality.%20Experiments%20on%20LLaDA-Instruct%2C%20LLaDA-1.5%2C%20and%20LLaDA-V%20across%0Amathematical%20reasoning%20and%20code%20generation%20tasks%20demonstrate%20consistent%0Aspeedups%3A%20%248.7%5Ctimes%24%20on%20GSM8K%20%28256%20tokens%29%2C%20%2445.1%5Ctimes%24%20on%20longer%20sequences%2C%0Aand%20%244.8%5Ctimes%24%20on%20HumanEval%2C%20while%20consistently%20maintaining%20higher%20accuracy%0Athan%20the%20baseline.%20Our%20method%20achieves%20significantly%20higher%20throughput%0A%28%246.8%5Ctimes%24%20on%20GSM8K%29%20than%20existing%20confidence-based%20approaches%20while%0Apreserving%20generation%20quality%2C%20enabling%20practical%20deployment%20of%20diffusion%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Is%2520All%2520You%2520Need%2520for%2520KV%2520Cache%2520in%2520Diffusion%2520LLMs%26entry.906535625%3DQuan%2520Nguyen-Tri%2520and%2520Mukul%2520Ranjan%2520and%2520Zhiqiang%2520Shen%26entry.1292438233%3D%2520%2520This%2520work%2520studies%2520how%2520to%2520adaptively%2520recompute%2520key-value%2520%2528KV%2529%2520caches%2520for%250Adiffusion%2520large%2520language%2520models%2520%2528DLMs%2529%2520to%2520maximize%2520prediction%2520accuracy%2520while%250Aminimizing%2520decoding%2520latency.%2520Prior%2520methods%2527%2520decoders%2520recompute%2520QKV%2520for%2520all%250Atokens%2520at%2520every%2520denoising%2520step%2520and%2520layer%252C%2520despite%2520KV%2520states%2520changing%2520little%250Aacross%2520most%2520steps%252C%2520especially%2520in%2520shallow%2520layers%252C%2520leading%2520to%2520substantial%250Aredundancy.%2520We%2520make%2520three%2520observations%253A%2520%25281%2529%2520distant%2520%2524%257B%255Cbf%2520MASK%257D%2524%2520tokens%250Aprimarily%2520act%2520as%2520a%2520length-bias%2520and%2520can%2520be%2520cached%2520block-wise%2520beyond%2520the%2520active%250Aprediction%2520window%253B%2520%25282%2529%2520KV%2520dynamics%2520increase%2520with%2520depth%252C%2520suggesting%2520that%250Aselective%2520refresh%2520starting%2520from%2520deeper%2520layers%2520is%2520sufficient%253B%2520and%2520%25283%2529%2520the%250Amost-attended%2520token%2520exhibits%2520the%2520smallest%2520KV%2520drift%252C%2520providing%2520a%2520conservative%250Alower%2520bound%2520on%2520cache%2520change%2520for%2520other%2520tokens.%2520Building%2520on%2520these%252C%2520we%2520propose%250A%2524%257B%255Cbf%2520Elastic-Cache%257D%2524%252C%2520a%2520training-free%252C%2520architecture-agnostic%2520strategy%2520that%250Ajointly%2520decides%2520%2524%257Bwhen%257D%2524%2520to%2520refresh%2520%2528via%2520an%2520attention-aware%2520drift%2520test%2520on%2520the%250Amost-attended%2520token%2529%2520and%2520%2524%257Bwhere%257D%2524%2520to%2520refresh%2520%2528via%2520a%2520depth-aware%2520schedule%2520that%250Arecomputes%2520from%2520a%2520chosen%2520layer%2520onward%2520while%2520reusing%2520shallow-layer%2520caches%2520and%250Aoff-window%2520MASK%2520caches%2529.%2520Unlike%2520fixed-period%2520schemes%252C%2520Elastic-Cache%2520performs%250Aadaptive%252C%2520layer-aware%2520cache%2520updates%2520for%2520diffusion%2520LLMs%252C%2520reducing%2520redundant%250Acomputation%2520and%2520accelerating%2520decoding%2520with%2520negligible%2520loss%2520in%2520generation%250Aquality.%2520Experiments%2520on%2520LLaDA-Instruct%252C%2520LLaDA-1.5%252C%2520and%2520LLaDA-V%2520across%250Amathematical%2520reasoning%2520and%2520code%2520generation%2520tasks%2520demonstrate%2520consistent%250Aspeedups%253A%2520%25248.7%255Ctimes%2524%2520on%2520GSM8K%2520%2528256%2520tokens%2529%252C%2520%252445.1%255Ctimes%2524%2520on%2520longer%2520sequences%252C%250Aand%2520%25244.8%255Ctimes%2524%2520on%2520HumanEval%252C%2520while%2520consistently%2520maintaining%2520higher%2520accuracy%250Athan%2520the%2520baseline.%2520Our%2520method%2520achieves%2520significantly%2520higher%2520throughput%250A%2528%25246.8%255Ctimes%2524%2520on%2520GSM8K%2529%2520than%2520existing%2520confidence-based%2520approaches%2520while%250Apreserving%2520generation%2520quality%252C%2520enabling%2520practical%2520deployment%2520of%2520diffusion%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Is%20All%20You%20Need%20for%20KV%20Cache%20in%20Diffusion%20LLMs&entry.906535625=Quan%20Nguyen-Tri%20and%20Mukul%20Ranjan%20and%20Zhiqiang%20Shen&entry.1292438233=%20%20This%20work%20studies%20how%20to%20adaptively%20recompute%20key-value%20%28KV%29%20caches%20for%0Adiffusion%20large%20language%20models%20%28DLMs%29%20to%20maximize%20prediction%20accuracy%20while%0Aminimizing%20decoding%20latency.%20Prior%20methods%27%20decoders%20recompute%20QKV%20for%20all%0Atokens%20at%20every%20denoising%20step%20and%20layer%2C%20despite%20KV%20states%20changing%20little%0Aacross%20most%20steps%2C%20especially%20in%20shallow%20layers%2C%20leading%20to%20substantial%0Aredundancy.%20We%20make%20three%20observations%3A%20%281%29%20distant%20%24%7B%5Cbf%20MASK%7D%24%20tokens%0Aprimarily%20act%20as%20a%20length-bias%20and%20can%20be%20cached%20block-wise%20beyond%20the%20active%0Aprediction%20window%3B%20%282%29%20KV%20dynamics%20increase%20with%20depth%2C%20suggesting%20that%0Aselective%20refresh%20starting%20from%20deeper%20layers%20is%20sufficient%3B%20and%20%283%29%20the%0Amost-attended%20token%20exhibits%20the%20smallest%20KV%20drift%2C%20providing%20a%20conservative%0Alower%20bound%20on%20cache%20change%20for%20other%20tokens.%20Building%20on%20these%2C%20we%20propose%0A%24%7B%5Cbf%20Elastic-Cache%7D%24%2C%20a%20training-free%2C%20architecture-agnostic%20strategy%20that%0Ajointly%20decides%20%24%7Bwhen%7D%24%20to%20refresh%20%28via%20an%20attention-aware%20drift%20test%20on%20the%0Amost-attended%20token%29%20and%20%24%7Bwhere%7D%24%20to%20refresh%20%28via%20a%20depth-aware%20schedule%20that%0Arecomputes%20from%20a%20chosen%20layer%20onward%20while%20reusing%20shallow-layer%20caches%20and%0Aoff-window%20MASK%20caches%29.%20Unlike%20fixed-period%20schemes%2C%20Elastic-Cache%20performs%0Aadaptive%2C%20layer-aware%20cache%20updates%20for%20diffusion%20LLMs%2C%20reducing%20redundant%0Acomputation%20and%20accelerating%20decoding%20with%20negligible%20loss%20in%20generation%0Aquality.%20Experiments%20on%20LLaDA-Instruct%2C%20LLaDA-1.5%2C%20and%20LLaDA-V%20across%0Amathematical%20reasoning%20and%20code%20generation%20tasks%20demonstrate%20consistent%0Aspeedups%3A%20%248.7%5Ctimes%24%20on%20GSM8K%20%28256%20tokens%29%2C%20%2445.1%5Ctimes%24%20on%20longer%20sequences%2C%0Aand%20%244.8%5Ctimes%24%20on%20HumanEval%2C%20while%20consistently%20maintaining%20higher%20accuracy%0Athan%20the%20baseline.%20Our%20method%20achieves%20significantly%20higher%20throughput%0A%28%246.8%5Ctimes%24%20on%20GSM8K%29%20than%20existing%20confidence-based%20approaches%20while%0Apreserving%20generation%20quality%2C%20enabling%20practical%20deployment%20of%20diffusion%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14973v1&entry.124074799=Read"},
{"title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic\n  Recovery", "author": "Fan Yang and Zixuan Huang and Abhinav Kumar and Sergio Aguilera Marinovic and Soshi Iba and Rana Soltani Zarrin and Dmitry Berenson", "abstract": "  Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.\n", "link": "http://arxiv.org/abs/2510.14768v1", "date": "2025-10-16", "relevancy": 2.196, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5574}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5464}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Neural%20Descriptor%20Fields%20for%20Learning%20Contact-Aware%20Dynamic%0A%20%20Recovery&body=Title%3A%20Leveraging%20Neural%20Descriptor%20Fields%20for%20Learning%20Contact-Aware%20Dynamic%0A%20%20Recovery%0AAuthor%3A%20Fan%20Yang%20and%20Zixuan%20Huang%20and%20Abhinav%20Kumar%20and%20Sergio%20Aguilera%20Marinovic%20and%20Soshi%20Iba%20and%20Rana%20Soltani%20Zarrin%20and%20Dmitry%20Berenson%0AAbstract%3A%20%20%20Real-world%20dexterous%20manipulation%20often%20encounters%20unexpected%20errors%20and%0Adisturbances%2C%20which%20can%20lead%20to%20catastrophic%20failures%2C%20such%20as%20dropping%20the%0Amanipulated%20object.%20To%20address%20this%20challenge%2C%20we%20focus%20on%20the%20problem%20of%0Acatching%20a%20falling%20object%20while%20it%20remains%20within%20grasping%20range%20and%2C%0Aimportantly%2C%20resetting%20the%20system%20to%20a%20configuration%20favorable%20for%20resuming%20the%0Aprimary%20manipulation%20task.%20We%20propose%20Contact-Aware%20Dynamic%20Recovery%20%28CADRE%29%2C%20a%0Areinforcement%20learning%20framework%20that%20incorporates%20a%20Neural%20Descriptor%20Field%0A%28NDF%29-inspired%20module%20to%20extract%20implicit%20contact%20features.%20Compared%20to%20methods%0Athat%20rely%20solely%20on%20object%20pose%20or%20point%20cloud%20input%2C%20NDFs%20can%20directly%20reason%0Aabout%20finger-object%20correspondence%20and%20adapt%20to%20different%20object%20geometries.%0AOur%20experiments%20show%20that%20incorporating%20contact%20features%20improves%20training%0Aefficiency%2C%20enhances%20convergence%20performance%20for%20RL%20training%2C%20and%20ultimately%0Aleads%20to%20more%20successful%20recoveries.%20Additionally%2C%20we%20demonstrate%20that%20CADRE%0Acan%20generalize%20zero-shot%20to%20unseen%20objects%20with%20different%20geometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Neural%2520Descriptor%2520Fields%2520for%2520Learning%2520Contact-Aware%2520Dynamic%250A%2520%2520Recovery%26entry.906535625%3DFan%2520Yang%2520and%2520Zixuan%2520Huang%2520and%2520Abhinav%2520Kumar%2520and%2520Sergio%2520Aguilera%2520Marinovic%2520and%2520Soshi%2520Iba%2520and%2520Rana%2520Soltani%2520Zarrin%2520and%2520Dmitry%2520Berenson%26entry.1292438233%3D%2520%2520Real-world%2520dexterous%2520manipulation%2520often%2520encounters%2520unexpected%2520errors%2520and%250Adisturbances%252C%2520which%2520can%2520lead%2520to%2520catastrophic%2520failures%252C%2520such%2520as%2520dropping%2520the%250Amanipulated%2520object.%2520To%2520address%2520this%2520challenge%252C%2520we%2520focus%2520on%2520the%2520problem%2520of%250Acatching%2520a%2520falling%2520object%2520while%2520it%2520remains%2520within%2520grasping%2520range%2520and%252C%250Aimportantly%252C%2520resetting%2520the%2520system%2520to%2520a%2520configuration%2520favorable%2520for%2520resuming%2520the%250Aprimary%2520manipulation%2520task.%2520We%2520propose%2520Contact-Aware%2520Dynamic%2520Recovery%2520%2528CADRE%2529%252C%2520a%250Areinforcement%2520learning%2520framework%2520that%2520incorporates%2520a%2520Neural%2520Descriptor%2520Field%250A%2528NDF%2529-inspired%2520module%2520to%2520extract%2520implicit%2520contact%2520features.%2520Compared%2520to%2520methods%250Athat%2520rely%2520solely%2520on%2520object%2520pose%2520or%2520point%2520cloud%2520input%252C%2520NDFs%2520can%2520directly%2520reason%250Aabout%2520finger-object%2520correspondence%2520and%2520adapt%2520to%2520different%2520object%2520geometries.%250AOur%2520experiments%2520show%2520that%2520incorporating%2520contact%2520features%2520improves%2520training%250Aefficiency%252C%2520enhances%2520convergence%2520performance%2520for%2520RL%2520training%252C%2520and%2520ultimately%250Aleads%2520to%2520more%2520successful%2520recoveries.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520CADRE%250Acan%2520generalize%2520zero-shot%2520to%2520unseen%2520objects%2520with%2520different%2520geometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Neural%20Descriptor%20Fields%20for%20Learning%20Contact-Aware%20Dynamic%0A%20%20Recovery&entry.906535625=Fan%20Yang%20and%20Zixuan%20Huang%20and%20Abhinav%20Kumar%20and%20Sergio%20Aguilera%20Marinovic%20and%20Soshi%20Iba%20and%20Rana%20Soltani%20Zarrin%20and%20Dmitry%20Berenson&entry.1292438233=%20%20Real-world%20dexterous%20manipulation%20often%20encounters%20unexpected%20errors%20and%0Adisturbances%2C%20which%20can%20lead%20to%20catastrophic%20failures%2C%20such%20as%20dropping%20the%0Amanipulated%20object.%20To%20address%20this%20challenge%2C%20we%20focus%20on%20the%20problem%20of%0Acatching%20a%20falling%20object%20while%20it%20remains%20within%20grasping%20range%20and%2C%0Aimportantly%2C%20resetting%20the%20system%20to%20a%20configuration%20favorable%20for%20resuming%20the%0Aprimary%20manipulation%20task.%20We%20propose%20Contact-Aware%20Dynamic%20Recovery%20%28CADRE%29%2C%20a%0Areinforcement%20learning%20framework%20that%20incorporates%20a%20Neural%20Descriptor%20Field%0A%28NDF%29-inspired%20module%20to%20extract%20implicit%20contact%20features.%20Compared%20to%20methods%0Athat%20rely%20solely%20on%20object%20pose%20or%20point%20cloud%20input%2C%20NDFs%20can%20directly%20reason%0Aabout%20finger-object%20correspondence%20and%20adapt%20to%20different%20object%20geometries.%0AOur%20experiments%20show%20that%20incorporating%20contact%20features%20improves%20training%0Aefficiency%2C%20enhances%20convergence%20performance%20for%20RL%20training%2C%20and%20ultimately%0Aleads%20to%20more%20successful%20recoveries.%20Additionally%2C%20we%20demonstrate%20that%20CADRE%0Acan%20generalize%20zero-shot%20to%20unseen%20objects%20with%20different%20geometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14768v1&entry.124074799=Read"},
{"title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video\n  Generation", "author": "JoungBin Lee and Jaewoo Jung and Jisang Han and Takuya Narihira and Kazumi Fukuda and Junyoung Seo and Sunghwan Hong and Yuki Mitsufuji and Seungryong Kim", "abstract": "  We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/\n", "link": "http://arxiv.org/abs/2510.14945v1", "date": "2025-10-16", "relevancy": 1.984, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7043}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6892}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Scene%20Prompting%20for%20Scene-Consistent%20Camera-Controllable%20Video%0A%20%20Generation&body=Title%3A%203D%20Scene%20Prompting%20for%20Scene-Consistent%20Camera-Controllable%20Video%0A%20%20Generation%0AAuthor%3A%20JoungBin%20Lee%20and%20Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Junyoung%20Seo%20and%20Sunghwan%20Hong%20and%20Yuki%20Mitsufuji%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20We%20present%203DScenePrompt%2C%20a%20framework%20that%20generates%20the%20next%20video%20chunk%0Afrom%20arbitrary-length%20input%20while%20enabling%20precise%20camera%20control%20and%0Apreserving%20scene%20consistency.%20Unlike%20methods%20conditioned%20on%20a%20single%20image%20or%20a%0Ashort%20clip%2C%20we%20employ%20dual%20spatio-temporal%20conditioning%20that%20reformulates%0Acontext-view%20referencing%20across%20the%20input%20video.%20Our%20approach%20conditions%20on%0Aboth%20temporally%20adjacent%20frames%20for%20motion%20continuity%20and%20spatially%20adjacent%0Acontent%20for%20scene%20consistency.%20However%2C%20when%20generating%20beyond%20temporal%0Aboundaries%2C%20directly%20using%20spatially%20adjacent%20frames%20would%20incorrectly%20preserve%0Adynamic%20elements%20from%20the%20past.%20We%20address%20this%20by%20introducing%20a%203D%20scene%0Amemory%20that%20represents%20exclusively%20the%20static%20geometry%20extracted%20from%20the%0Aentire%20input%20video.%20To%20construct%20this%20memory%2C%20we%20leverage%20dynamic%20SLAM%20with%20our%0Anewly%20introduced%20dynamic%20masking%20strategy%20that%20explicitly%20separates%20static%0Ascene%20geometry%20from%20moving%20elements.%20The%20static%20scene%20representation%20can%20then%0Abe%20projected%20to%20any%20target%20viewpoint%2C%20providing%20geometrically%20consistent%20warped%0Aviews%20that%20serve%20as%20strong%203D%20spatial%20prompts%20while%20allowing%20dynamic%20regions%20to%0Aevolve%20naturally%20from%20temporal%20context.%20This%20enables%20our%20model%20to%20maintain%0Along-range%20spatial%20coherence%20and%20precise%20camera%20control%20without%20sacrificing%0Acomputational%20efficiency%20or%20motion%20realism.%20Extensive%20experiments%20demonstrate%0Athat%20our%20framework%20significantly%20outperforms%20existing%20methods%20in%20scene%0Aconsistency%2C%20camera%20controllability%2C%20and%20generation%20quality.%20Project%20page%20%3A%0Ahttps%3A//cvlab-kaist.github.io/3DScenePrompt/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Scene%2520Prompting%2520for%2520Scene-Consistent%2520Camera-Controllable%2520Video%250A%2520%2520Generation%26entry.906535625%3DJoungBin%2520Lee%2520and%2520Jaewoo%2520Jung%2520and%2520Jisang%2520Han%2520and%2520Takuya%2520Narihira%2520and%2520Kazumi%2520Fukuda%2520and%2520Junyoung%2520Seo%2520and%2520Sunghwan%2520Hong%2520and%2520Yuki%2520Mitsufuji%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%25203DScenePrompt%252C%2520a%2520framework%2520that%2520generates%2520the%2520next%2520video%2520chunk%250Afrom%2520arbitrary-length%2520input%2520while%2520enabling%2520precise%2520camera%2520control%2520and%250Apreserving%2520scene%2520consistency.%2520Unlike%2520methods%2520conditioned%2520on%2520a%2520single%2520image%2520or%2520a%250Ashort%2520clip%252C%2520we%2520employ%2520dual%2520spatio-temporal%2520conditioning%2520that%2520reformulates%250Acontext-view%2520referencing%2520across%2520the%2520input%2520video.%2520Our%2520approach%2520conditions%2520on%250Aboth%2520temporally%2520adjacent%2520frames%2520for%2520motion%2520continuity%2520and%2520spatially%2520adjacent%250Acontent%2520for%2520scene%2520consistency.%2520However%252C%2520when%2520generating%2520beyond%2520temporal%250Aboundaries%252C%2520directly%2520using%2520spatially%2520adjacent%2520frames%2520would%2520incorrectly%2520preserve%250Adynamic%2520elements%2520from%2520the%2520past.%2520We%2520address%2520this%2520by%2520introducing%2520a%25203D%2520scene%250Amemory%2520that%2520represents%2520exclusively%2520the%2520static%2520geometry%2520extracted%2520from%2520the%250Aentire%2520input%2520video.%2520To%2520construct%2520this%2520memory%252C%2520we%2520leverage%2520dynamic%2520SLAM%2520with%2520our%250Anewly%2520introduced%2520dynamic%2520masking%2520strategy%2520that%2520explicitly%2520separates%2520static%250Ascene%2520geometry%2520from%2520moving%2520elements.%2520The%2520static%2520scene%2520representation%2520can%2520then%250Abe%2520projected%2520to%2520any%2520target%2520viewpoint%252C%2520providing%2520geometrically%2520consistent%2520warped%250Aviews%2520that%2520serve%2520as%2520strong%25203D%2520spatial%2520prompts%2520while%2520allowing%2520dynamic%2520regions%2520to%250Aevolve%2520naturally%2520from%2520temporal%2520context.%2520This%2520enables%2520our%2520model%2520to%2520maintain%250Along-range%2520spatial%2520coherence%2520and%2520precise%2520camera%2520control%2520without%2520sacrificing%250Acomputational%2520efficiency%2520or%2520motion%2520realism.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520framework%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520scene%250Aconsistency%252C%2520camera%2520controllability%252C%2520and%2520generation%2520quality.%2520Project%2520page%2520%253A%250Ahttps%253A//cvlab-kaist.github.io/3DScenePrompt/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Scene%20Prompting%20for%20Scene-Consistent%20Camera-Controllable%20Video%0A%20%20Generation&entry.906535625=JoungBin%20Lee%20and%20Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Junyoung%20Seo%20and%20Sunghwan%20Hong%20and%20Yuki%20Mitsufuji%20and%20Seungryong%20Kim&entry.1292438233=%20%20We%20present%203DScenePrompt%2C%20a%20framework%20that%20generates%20the%20next%20video%20chunk%0Afrom%20arbitrary-length%20input%20while%20enabling%20precise%20camera%20control%20and%0Apreserving%20scene%20consistency.%20Unlike%20methods%20conditioned%20on%20a%20single%20image%20or%20a%0Ashort%20clip%2C%20we%20employ%20dual%20spatio-temporal%20conditioning%20that%20reformulates%0Acontext-view%20referencing%20across%20the%20input%20video.%20Our%20approach%20conditions%20on%0Aboth%20temporally%20adjacent%20frames%20for%20motion%20continuity%20and%20spatially%20adjacent%0Acontent%20for%20scene%20consistency.%20However%2C%20when%20generating%20beyond%20temporal%0Aboundaries%2C%20directly%20using%20spatially%20adjacent%20frames%20would%20incorrectly%20preserve%0Adynamic%20elements%20from%20the%20past.%20We%20address%20this%20by%20introducing%20a%203D%20scene%0Amemory%20that%20represents%20exclusively%20the%20static%20geometry%20extracted%20from%20the%0Aentire%20input%20video.%20To%20construct%20this%20memory%2C%20we%20leverage%20dynamic%20SLAM%20with%20our%0Anewly%20introduced%20dynamic%20masking%20strategy%20that%20explicitly%20separates%20static%0Ascene%20geometry%20from%20moving%20elements.%20The%20static%20scene%20representation%20can%20then%0Abe%20projected%20to%20any%20target%20viewpoint%2C%20providing%20geometrically%20consistent%20warped%0Aviews%20that%20serve%20as%20strong%203D%20spatial%20prompts%20while%20allowing%20dynamic%20regions%20to%0Aevolve%20naturally%20from%20temporal%20context.%20This%20enables%20our%20model%20to%20maintain%0Along-range%20spatial%20coherence%20and%20precise%20camera%20control%20without%20sacrificing%0Acomputational%20efficiency%20or%20motion%20realism.%20Extensive%20experiments%20demonstrate%0Athat%20our%20framework%20significantly%20outperforms%20existing%20methods%20in%20scene%0Aconsistency%2C%20camera%20controllability%2C%20and%20generation%20quality.%20Project%20page%20%3A%0Ahttps%3A//cvlab-kaist.github.io/3DScenePrompt/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14945v1&entry.124074799=Read"},
{"title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via\n  Multi-Agent Collaboration", "author": "Haoyuan Li and Mathias Funk and Aaqib Saeed", "abstract": "  Federated Learning (FL) offers a powerful paradigm for training models on\ndecentralized data, but its promise is often undermined by the immense\ncomplexity of designing and deploying robust systems. The need to select,\ncombine, and tune strategies for multifaceted challenges like data\nheterogeneity and system constraints has become a critical bottleneck,\nresulting in brittle, bespoke solutions. To address this, we introduce\nHelmsman, a novel multi-agent system that automates the end-to-end synthesis of\nfederated learning systems from high-level user specifications. It emulates a\nprincipled research and development workflow through three collaborative\nphases: (1) interactive human-in-the-loop planning to formulate a sound\nresearch plan, (2) modular code generation by supervised agent teams, and (3) a\nclosed-loop of autonomous evaluation and refinement in a sandboxed simulation\nenvironment. To facilitate rigorous evaluation, we also introduce\nAgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess\nthe system-level generation capabilities of agentic systems in FL. Extensive\nexperiments demonstrate that our approach generates solutions competitive with,\nand often superior to, established hand-crafted baselines. Our work represents\na significant step towards the automated engineering of complex decentralized\nAI systems.\n", "link": "http://arxiv.org/abs/2510.14512v1", "date": "2025-10-16", "relevancy": 1.5653, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5295}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.517}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helmsman%3A%20Autonomous%20Synthesis%20of%20Federated%20Learning%20Systems%20via%0A%20%20Multi-Agent%20Collaboration&body=Title%3A%20Helmsman%3A%20Autonomous%20Synthesis%20of%20Federated%20Learning%20Systems%20via%0A%20%20Multi-Agent%20Collaboration%0AAuthor%3A%20Haoyuan%20Li%20and%20Mathias%20Funk%20and%20Aaqib%20Saeed%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20powerful%20paradigm%20for%20training%20models%20on%0Adecentralized%20data%2C%20but%20its%20promise%20is%20often%20undermined%20by%20the%20immense%0Acomplexity%20of%20designing%20and%20deploying%20robust%20systems.%20The%20need%20to%20select%2C%0Acombine%2C%20and%20tune%20strategies%20for%20multifaceted%20challenges%20like%20data%0Aheterogeneity%20and%20system%20constraints%20has%20become%20a%20critical%20bottleneck%2C%0Aresulting%20in%20brittle%2C%20bespoke%20solutions.%20To%20address%20this%2C%20we%20introduce%0AHelmsman%2C%20a%20novel%20multi-agent%20system%20that%20automates%20the%20end-to-end%20synthesis%20of%0Afederated%20learning%20systems%20from%20high-level%20user%20specifications.%20It%20emulates%20a%0Aprincipled%20research%20and%20development%20workflow%20through%20three%20collaborative%0Aphases%3A%20%281%29%20interactive%20human-in-the-loop%20planning%20to%20formulate%20a%20sound%0Aresearch%20plan%2C%20%282%29%20modular%20code%20generation%20by%20supervised%20agent%20teams%2C%20and%20%283%29%20a%0Aclosed-loop%20of%20autonomous%20evaluation%20and%20refinement%20in%20a%20sandboxed%20simulation%0Aenvironment.%20To%20facilitate%20rigorous%20evaluation%2C%20we%20also%20introduce%0AAgentFL-Bench%2C%20a%20new%20benchmark%20comprising%2016%20diverse%20tasks%20designed%20to%20assess%0Athe%20system-level%20generation%20capabilities%20of%20agentic%20systems%20in%20FL.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20generates%20solutions%20competitive%20with%2C%0Aand%20often%20superior%20to%2C%20established%20hand-crafted%20baselines.%20Our%20work%20represents%0Aa%20significant%20step%20towards%20the%20automated%20engineering%20of%20complex%20decentralized%0AAI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelmsman%253A%2520Autonomous%2520Synthesis%2520of%2520Federated%2520Learning%2520Systems%2520via%250A%2520%2520Multi-Agent%2520Collaboration%26entry.906535625%3DHaoyuan%2520Li%2520and%2520Mathias%2520Funk%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520offers%2520a%2520powerful%2520paradigm%2520for%2520training%2520models%2520on%250Adecentralized%2520data%252C%2520but%2520its%2520promise%2520is%2520often%2520undermined%2520by%2520the%2520immense%250Acomplexity%2520of%2520designing%2520and%2520deploying%2520robust%2520systems.%2520The%2520need%2520to%2520select%252C%250Acombine%252C%2520and%2520tune%2520strategies%2520for%2520multifaceted%2520challenges%2520like%2520data%250Aheterogeneity%2520and%2520system%2520constraints%2520has%2520become%2520a%2520critical%2520bottleneck%252C%250Aresulting%2520in%2520brittle%252C%2520bespoke%2520solutions.%2520To%2520address%2520this%252C%2520we%2520introduce%250AHelmsman%252C%2520a%2520novel%2520multi-agent%2520system%2520that%2520automates%2520the%2520end-to-end%2520synthesis%2520of%250Afederated%2520learning%2520systems%2520from%2520high-level%2520user%2520specifications.%2520It%2520emulates%2520a%250Aprincipled%2520research%2520and%2520development%2520workflow%2520through%2520three%2520collaborative%250Aphases%253A%2520%25281%2529%2520interactive%2520human-in-the-loop%2520planning%2520to%2520formulate%2520a%2520sound%250Aresearch%2520plan%252C%2520%25282%2529%2520modular%2520code%2520generation%2520by%2520supervised%2520agent%2520teams%252C%2520and%2520%25283%2529%2520a%250Aclosed-loop%2520of%2520autonomous%2520evaluation%2520and%2520refinement%2520in%2520a%2520sandboxed%2520simulation%250Aenvironment.%2520To%2520facilitate%2520rigorous%2520evaluation%252C%2520we%2520also%2520introduce%250AAgentFL-Bench%252C%2520a%2520new%2520benchmark%2520comprising%252016%2520diverse%2520tasks%2520designed%2520to%2520assess%250Athe%2520system-level%2520generation%2520capabilities%2520of%2520agentic%2520systems%2520in%2520FL.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520generates%2520solutions%2520competitive%2520with%252C%250Aand%2520often%2520superior%2520to%252C%2520established%2520hand-crafted%2520baselines.%2520Our%2520work%2520represents%250Aa%2520significant%2520step%2520towards%2520the%2520automated%2520engineering%2520of%2520complex%2520decentralized%250AAI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helmsman%3A%20Autonomous%20Synthesis%20of%20Federated%20Learning%20Systems%20via%0A%20%20Multi-Agent%20Collaboration&entry.906535625=Haoyuan%20Li%20and%20Mathias%20Funk%20and%20Aaqib%20Saeed&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20powerful%20paradigm%20for%20training%20models%20on%0Adecentralized%20data%2C%20but%20its%20promise%20is%20often%20undermined%20by%20the%20immense%0Acomplexity%20of%20designing%20and%20deploying%20robust%20systems.%20The%20need%20to%20select%2C%0Acombine%2C%20and%20tune%20strategies%20for%20multifaceted%20challenges%20like%20data%0Aheterogeneity%20and%20system%20constraints%20has%20become%20a%20critical%20bottleneck%2C%0Aresulting%20in%20brittle%2C%20bespoke%20solutions.%20To%20address%20this%2C%20we%20introduce%0AHelmsman%2C%20a%20novel%20multi-agent%20system%20that%20automates%20the%20end-to-end%20synthesis%20of%0Afederated%20learning%20systems%20from%20high-level%20user%20specifications.%20It%20emulates%20a%0Aprincipled%20research%20and%20development%20workflow%20through%20three%20collaborative%0Aphases%3A%20%281%29%20interactive%20human-in-the-loop%20planning%20to%20formulate%20a%20sound%0Aresearch%20plan%2C%20%282%29%20modular%20code%20generation%20by%20supervised%20agent%20teams%2C%20and%20%283%29%20a%0Aclosed-loop%20of%20autonomous%20evaluation%20and%20refinement%20in%20a%20sandboxed%20simulation%0Aenvironment.%20To%20facilitate%20rigorous%20evaluation%2C%20we%20also%20introduce%0AAgentFL-Bench%2C%20a%20new%20benchmark%20comprising%2016%20diverse%20tasks%20designed%20to%20assess%0Athe%20system-level%20generation%20capabilities%20of%20agentic%20systems%20in%20FL.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20generates%20solutions%20competitive%20with%2C%0Aand%20often%20superior%20to%2C%20established%20hand-crafted%20baselines.%20Our%20work%20represents%0Aa%20significant%20step%20towards%20the%20automated%20engineering%20of%20complex%20decentralized%0AAI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14512v1&entry.124074799=Read"},
{"title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence", "author": "Marco Simoni and Aleksandar Fontana and Andrea Saracino and Paolo Mori", "abstract": "  TITAN (Threat Intelligence Through Automated Navigation) is a framework that\nconnects natural-language cyber threat queries with executable reasoning over a\nstructured knowledge graph. It integrates a path planner model, which predicts\nlogical relation chains from text, and a graph executor that traverses the\nTITAN Ontology to retrieve factual answers and supporting evidence. Unlike\ntraditional retrieval systems, TITAN operates on a typed, bidirectional graph\nderived from MITRE, allowing reasoning to move clearly and reversibly between\nthreats, behaviors, and defenses. To support training and evaluation, we\nintroduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:\n13951) pairing natural language questions with executable reasoning paths and\nstep by step Chain of Thought explanations. Empirical evaluations show that\nTITAN enables models to generate syntactically valid and semantically coherent\nreasoning paths that can be deterministically executed on the underlying graph.\n", "link": "http://arxiv.org/abs/2510.14670v1", "date": "2025-10-16", "relevancy": 1.7671, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4771}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TITAN%3A%20Graph-Executable%20Reasoning%20for%20Cyber%20Threat%20Intelligence&body=Title%3A%20TITAN%3A%20Graph-Executable%20Reasoning%20for%20Cyber%20Threat%20Intelligence%0AAuthor%3A%20Marco%20Simoni%20and%20Aleksandar%20Fontana%20and%20Andrea%20Saracino%20and%20Paolo%20Mori%0AAbstract%3A%20%20%20TITAN%20%28Threat%20Intelligence%20Through%20Automated%20Navigation%29%20is%20a%20framework%20that%0Aconnects%20natural-language%20cyber%20threat%20queries%20with%20executable%20reasoning%20over%20a%0Astructured%20knowledge%20graph.%20It%20integrates%20a%20path%20planner%20model%2C%20which%20predicts%0Alogical%20relation%20chains%20from%20text%2C%20and%20a%20graph%20executor%20that%20traverses%20the%0ATITAN%20Ontology%20to%20retrieve%20factual%20answers%20and%20supporting%20evidence.%20Unlike%0Atraditional%20retrieval%20systems%2C%20TITAN%20operates%20on%20a%20typed%2C%20bidirectional%20graph%0Aderived%20from%20MITRE%2C%20allowing%20reasoning%20to%20move%20clearly%20and%20reversibly%20between%0Athreats%2C%20behaviors%2C%20and%20defenses.%20To%20support%20training%20and%20evaluation%2C%20we%0Aintroduce%20the%20TITAN%20Dataset%2C%20a%20corpus%20of%2088209%20examples%20%28Train%3A%2074258%3B%20Test%3A%0A13951%29%20pairing%20natural%20language%20questions%20with%20executable%20reasoning%20paths%20and%0Astep%20by%20step%20Chain%20of%20Thought%20explanations.%20Empirical%20evaluations%20show%20that%0ATITAN%20enables%20models%20to%20generate%20syntactically%20valid%20and%20semantically%20coherent%0Areasoning%20paths%20that%20can%20be%20deterministically%20executed%20on%20the%20underlying%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTITAN%253A%2520Graph-Executable%2520Reasoning%2520for%2520Cyber%2520Threat%2520Intelligence%26entry.906535625%3DMarco%2520Simoni%2520and%2520Aleksandar%2520Fontana%2520and%2520Andrea%2520Saracino%2520and%2520Paolo%2520Mori%26entry.1292438233%3D%2520%2520TITAN%2520%2528Threat%2520Intelligence%2520Through%2520Automated%2520Navigation%2529%2520is%2520a%2520framework%2520that%250Aconnects%2520natural-language%2520cyber%2520threat%2520queries%2520with%2520executable%2520reasoning%2520over%2520a%250Astructured%2520knowledge%2520graph.%2520It%2520integrates%2520a%2520path%2520planner%2520model%252C%2520which%2520predicts%250Alogical%2520relation%2520chains%2520from%2520text%252C%2520and%2520a%2520graph%2520executor%2520that%2520traverses%2520the%250ATITAN%2520Ontology%2520to%2520retrieve%2520factual%2520answers%2520and%2520supporting%2520evidence.%2520Unlike%250Atraditional%2520retrieval%2520systems%252C%2520TITAN%2520operates%2520on%2520a%2520typed%252C%2520bidirectional%2520graph%250Aderived%2520from%2520MITRE%252C%2520allowing%2520reasoning%2520to%2520move%2520clearly%2520and%2520reversibly%2520between%250Athreats%252C%2520behaviors%252C%2520and%2520defenses.%2520To%2520support%2520training%2520and%2520evaluation%252C%2520we%250Aintroduce%2520the%2520TITAN%2520Dataset%252C%2520a%2520corpus%2520of%252088209%2520examples%2520%2528Train%253A%252074258%253B%2520Test%253A%250A13951%2529%2520pairing%2520natural%2520language%2520questions%2520with%2520executable%2520reasoning%2520paths%2520and%250Astep%2520by%2520step%2520Chain%2520of%2520Thought%2520explanations.%2520Empirical%2520evaluations%2520show%2520that%250ATITAN%2520enables%2520models%2520to%2520generate%2520syntactically%2520valid%2520and%2520semantically%2520coherent%250Areasoning%2520paths%2520that%2520can%2520be%2520deterministically%2520executed%2520on%2520the%2520underlying%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TITAN%3A%20Graph-Executable%20Reasoning%20for%20Cyber%20Threat%20Intelligence&entry.906535625=Marco%20Simoni%20and%20Aleksandar%20Fontana%20and%20Andrea%20Saracino%20and%20Paolo%20Mori&entry.1292438233=%20%20TITAN%20%28Threat%20Intelligence%20Through%20Automated%20Navigation%29%20is%20a%20framework%20that%0Aconnects%20natural-language%20cyber%20threat%20queries%20with%20executable%20reasoning%20over%20a%0Astructured%20knowledge%20graph.%20It%20integrates%20a%20path%20planner%20model%2C%20which%20predicts%0Alogical%20relation%20chains%20from%20text%2C%20and%20a%20graph%20executor%20that%20traverses%20the%0ATITAN%20Ontology%20to%20retrieve%20factual%20answers%20and%20supporting%20evidence.%20Unlike%0Atraditional%20retrieval%20systems%2C%20TITAN%20operates%20on%20a%20typed%2C%20bidirectional%20graph%0Aderived%20from%20MITRE%2C%20allowing%20reasoning%20to%20move%20clearly%20and%20reversibly%20between%0Athreats%2C%20behaviors%2C%20and%20defenses.%20To%20support%20training%20and%20evaluation%2C%20we%0Aintroduce%20the%20TITAN%20Dataset%2C%20a%20corpus%20of%2088209%20examples%20%28Train%3A%2074258%3B%20Test%3A%0A13951%29%20pairing%20natural%20language%20questions%20with%20executable%20reasoning%20paths%20and%0Astep%20by%20step%20Chain%20of%20Thought%20explanations.%20Empirical%20evaluations%20show%20that%0ATITAN%20enables%20models%20to%20generate%20syntactically%20valid%20and%20semantically%20coherent%0Areasoning%20paths%20that%20can%20be%20deterministically%20executed%20on%20the%20underlying%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14670v1&entry.124074799=Read"},
{"title": "Consistent text-to-image generation via scene de-contextualization", "author": "Song Tang and Peihao Gong and Kunyu Li and Kai Guo and Boyu Wang and Mao Ye and Jianwei Zhang and Xiatian Zhu", "abstract": "  Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity.\n", "link": "http://arxiv.org/abs/2510.14553v1", "date": "2025-10-16", "relevancy": 1.8945, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6754}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5986}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20text-to-image%20generation%20via%20scene%20de-contextualization&body=Title%3A%20Consistent%20text-to-image%20generation%20via%20scene%20de-contextualization%0AAuthor%3A%20Song%20Tang%20and%20Peihao%20Gong%20and%20Kunyu%20Li%20and%20Kai%20Guo%20and%20Boyu%20Wang%20and%20Mao%20Ye%20and%20Jianwei%20Zhang%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20Consistent%20text-to-image%20%28T2I%29%20generation%20seeks%20to%20produce%0Aidentity-preserving%20images%20of%20the%20same%20subject%20across%20diverse%20scenes%2C%20yet%20it%0Aoften%20fails%20due%20to%20a%20phenomenon%20called%20identity%20%28ID%29%20shift.%20Previous%20methods%0Ahave%20tackled%20this%20issue%2C%20but%20typically%20rely%20on%20the%20unrealistic%20assumption%20of%0Aknowing%20all%20target%20scenes%20in%20advance.%20This%20paper%20reveals%20that%20a%20key%20source%20of%0AID%20shift%20is%20the%20native%20correlation%20between%20subject%20and%20scene%20context%2C%20called%0Ascene%20contextualization%2C%20which%20arises%20naturally%20as%20T2I%20models%20fit%20the%20training%0Adistribution%20of%20vast%20natural%20images.%20We%20formally%20prove%20the%20near-universality%20of%0Athis%20scene-ID%20correlation%20and%20derive%20theoretical%20bounds%20on%20its%20strength.%20On%0Athis%20basis%2C%20we%20propose%20a%20novel%2C%20efficient%2C%20training-free%20prompt%20embedding%0Aediting%20approach%2C%20called%20Scene%20De-Contextualization%20%28SDeC%29%2C%20that%20imposes%20an%0Ainversion%20process%20of%20T2I%27s%20built-in%20scene%20contextualization.%20Specifically%2C%20it%0Aidentifies%20and%20suppresses%20the%20latent%20scene-ID%20correlation%20within%20the%20ID%0Aprompt%27s%20embedding%20by%20quantifying%20the%20SVD%20directional%20stability%20to%20adaptively%0Are-weight%20the%20corresponding%20eigenvalues.%20Critically%2C%20SDeC%20allows%20for%20per-scene%0Ause%20%28one%20scene%20per%20prompt%29%20without%20requiring%20prior%20access%20to%20all%20target%20scenes.%0AThis%20makes%20it%20a%20highly%20flexible%20and%20general%20solution%20well-suited%20to%20real-world%0Aapplications%20where%20such%20prior%20knowledge%20is%20often%20unavailable%20or%20varies%20over%0Atime.%20Experiments%20demonstrate%20that%20SDeC%20significantly%20enhances%20identity%0Apreservation%20while%20maintaining%20scene%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520text-to-image%2520generation%2520via%2520scene%2520de-contextualization%26entry.906535625%3DSong%2520Tang%2520and%2520Peihao%2520Gong%2520and%2520Kunyu%2520Li%2520and%2520Kai%2520Guo%2520and%2520Boyu%2520Wang%2520and%2520Mao%2520Ye%2520and%2520Jianwei%2520Zhang%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520Consistent%2520text-to-image%2520%2528T2I%2529%2520generation%2520seeks%2520to%2520produce%250Aidentity-preserving%2520images%2520of%2520the%2520same%2520subject%2520across%2520diverse%2520scenes%252C%2520yet%2520it%250Aoften%2520fails%2520due%2520to%2520a%2520phenomenon%2520called%2520identity%2520%2528ID%2529%2520shift.%2520Previous%2520methods%250Ahave%2520tackled%2520this%2520issue%252C%2520but%2520typically%2520rely%2520on%2520the%2520unrealistic%2520assumption%2520of%250Aknowing%2520all%2520target%2520scenes%2520in%2520advance.%2520This%2520paper%2520reveals%2520that%2520a%2520key%2520source%2520of%250AID%2520shift%2520is%2520the%2520native%2520correlation%2520between%2520subject%2520and%2520scene%2520context%252C%2520called%250Ascene%2520contextualization%252C%2520which%2520arises%2520naturally%2520as%2520T2I%2520models%2520fit%2520the%2520training%250Adistribution%2520of%2520vast%2520natural%2520images.%2520We%2520formally%2520prove%2520the%2520near-universality%2520of%250Athis%2520scene-ID%2520correlation%2520and%2520derive%2520theoretical%2520bounds%2520on%2520its%2520strength.%2520On%250Athis%2520basis%252C%2520we%2520propose%2520a%2520novel%252C%2520efficient%252C%2520training-free%2520prompt%2520embedding%250Aediting%2520approach%252C%2520called%2520Scene%2520De-Contextualization%2520%2528SDeC%2529%252C%2520that%2520imposes%2520an%250Ainversion%2520process%2520of%2520T2I%2527s%2520built-in%2520scene%2520contextualization.%2520Specifically%252C%2520it%250Aidentifies%2520and%2520suppresses%2520the%2520latent%2520scene-ID%2520correlation%2520within%2520the%2520ID%250Aprompt%2527s%2520embedding%2520by%2520quantifying%2520the%2520SVD%2520directional%2520stability%2520to%2520adaptively%250Are-weight%2520the%2520corresponding%2520eigenvalues.%2520Critically%252C%2520SDeC%2520allows%2520for%2520per-scene%250Ause%2520%2528one%2520scene%2520per%2520prompt%2529%2520without%2520requiring%2520prior%2520access%2520to%2520all%2520target%2520scenes.%250AThis%2520makes%2520it%2520a%2520highly%2520flexible%2520and%2520general%2520solution%2520well-suited%2520to%2520real-world%250Aapplications%2520where%2520such%2520prior%2520knowledge%2520is%2520often%2520unavailable%2520or%2520varies%2520over%250Atime.%2520Experiments%2520demonstrate%2520that%2520SDeC%2520significantly%2520enhances%2520identity%250Apreservation%2520while%2520maintaining%2520scene%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20text-to-image%20generation%20via%20scene%20de-contextualization&entry.906535625=Song%20Tang%20and%20Peihao%20Gong%20and%20Kunyu%20Li%20and%20Kai%20Guo%20and%20Boyu%20Wang%20and%20Mao%20Ye%20and%20Jianwei%20Zhang%20and%20Xiatian%20Zhu&entry.1292438233=%20%20Consistent%20text-to-image%20%28T2I%29%20generation%20seeks%20to%20produce%0Aidentity-preserving%20images%20of%20the%20same%20subject%20across%20diverse%20scenes%2C%20yet%20it%0Aoften%20fails%20due%20to%20a%20phenomenon%20called%20identity%20%28ID%29%20shift.%20Previous%20methods%0Ahave%20tackled%20this%20issue%2C%20but%20typically%20rely%20on%20the%20unrealistic%20assumption%20of%0Aknowing%20all%20target%20scenes%20in%20advance.%20This%20paper%20reveals%20that%20a%20key%20source%20of%0AID%20shift%20is%20the%20native%20correlation%20between%20subject%20and%20scene%20context%2C%20called%0Ascene%20contextualization%2C%20which%20arises%20naturally%20as%20T2I%20models%20fit%20the%20training%0Adistribution%20of%20vast%20natural%20images.%20We%20formally%20prove%20the%20near-universality%20of%0Athis%20scene-ID%20correlation%20and%20derive%20theoretical%20bounds%20on%20its%20strength.%20On%0Athis%20basis%2C%20we%20propose%20a%20novel%2C%20efficient%2C%20training-free%20prompt%20embedding%0Aediting%20approach%2C%20called%20Scene%20De-Contextualization%20%28SDeC%29%2C%20that%20imposes%20an%0Ainversion%20process%20of%20T2I%27s%20built-in%20scene%20contextualization.%20Specifically%2C%20it%0Aidentifies%20and%20suppresses%20the%20latent%20scene-ID%20correlation%20within%20the%20ID%0Aprompt%27s%20embedding%20by%20quantifying%20the%20SVD%20directional%20stability%20to%20adaptively%0Are-weight%20the%20corresponding%20eigenvalues.%20Critically%2C%20SDeC%20allows%20for%20per-scene%0Ause%20%28one%20scene%20per%20prompt%29%20without%20requiring%20prior%20access%20to%20all%20target%20scenes.%0AThis%20makes%20it%20a%20highly%20flexible%20and%20general%20solution%20well-suited%20to%20real-world%0Aapplications%20where%20such%20prior%20knowledge%20is%20often%20unavailable%20or%20varies%20over%0Atime.%20Experiments%20demonstrate%20that%20SDeC%20significantly%20enhances%20identity%0Apreservation%20while%20maintaining%20scene%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14553v1&entry.124074799=Read"},
{"title": "Large Language Models Enable Design of Personalized Nudges across\n  Cultures", "author": "Vladimir Maksimenko and Qingyao Xin and Prateek Gupta and Bin Zhang and Prateek Bansal", "abstract": "  Nudge strategies are effective tools for influencing behaviour, but their\nimpact depends on individual preferences. Strategies that work for some\nindividuals may be counterproductive for others. We hypothesize that large\nlanguage models (LLMs) can facilitate the design of individual-specific nudges\nwithout the need for costly and time-intensive behavioural data collection and\nmodelling. To test this, we use LLMs to design personalized decoy-based nudges\ntailored to individual profiles and cultural contexts, aimed at encouraging air\ntravellers to voluntarily offset CO$_2$ emissions from flights. We evaluate\ntheir effectiveness through a large-scale survey experiment ($n=3495$)\nconducted across five countries. Results show that LLM-informed personalized\nnudges are more effective than uniform settings, raising offsetting rates by\n3-7$\\%$ in Germany, Singapore, and the US, though not in China or India. Our\nstudy highlights the potential of LLM as a low-cost testbed for piloting nudge\nstrategies. At the same time, cultural heterogeneity constrains their\ngeneralizability underscoring the need for combining LLM-based simulations with\ntargeted empirical validation.\n", "link": "http://arxiv.org/abs/2508.12045v2", "date": "2025-10-16", "relevancy": 0.8473, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4612}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Enable%20Design%20of%20Personalized%20Nudges%20across%0A%20%20Cultures&body=Title%3A%20Large%20Language%20Models%20Enable%20Design%20of%20Personalized%20Nudges%20across%0A%20%20Cultures%0AAuthor%3A%20Vladimir%20Maksimenko%20and%20Qingyao%20Xin%20and%20Prateek%20Gupta%20and%20Bin%20Zhang%20and%20Prateek%20Bansal%0AAbstract%3A%20%20%20Nudge%20strategies%20are%20effective%20tools%20for%20influencing%20behaviour%2C%20but%20their%0Aimpact%20depends%20on%20individual%20preferences.%20Strategies%20that%20work%20for%20some%0Aindividuals%20may%20be%20counterproductive%20for%20others.%20We%20hypothesize%20that%20large%0Alanguage%20models%20%28LLMs%29%20can%20facilitate%20the%20design%20of%20individual-specific%20nudges%0Awithout%20the%20need%20for%20costly%20and%20time-intensive%20behavioural%20data%20collection%20and%0Amodelling.%20To%20test%20this%2C%20we%20use%20LLMs%20to%20design%20personalized%20decoy-based%20nudges%0Atailored%20to%20individual%20profiles%20and%20cultural%20contexts%2C%20aimed%20at%20encouraging%20air%0Atravellers%20to%20voluntarily%20offset%20CO%24_2%24%20emissions%20from%20flights.%20We%20evaluate%0Atheir%20effectiveness%20through%20a%20large-scale%20survey%20experiment%20%28%24n%3D3495%24%29%0Aconducted%20across%20five%20countries.%20Results%20show%20that%20LLM-informed%20personalized%0Anudges%20are%20more%20effective%20than%20uniform%20settings%2C%20raising%20offsetting%20rates%20by%0A3-7%24%5C%25%24%20in%20Germany%2C%20Singapore%2C%20and%20the%20US%2C%20though%20not%20in%20China%20or%20India.%20Our%0Astudy%20highlights%20the%20potential%20of%20LLM%20as%20a%20low-cost%20testbed%20for%20piloting%20nudge%0Astrategies.%20At%20the%20same%20time%2C%20cultural%20heterogeneity%20constrains%20their%0Ageneralizability%20underscoring%20the%20need%20for%20combining%20LLM-based%20simulations%20with%0Atargeted%20empirical%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Enable%2520Design%2520of%2520Personalized%2520Nudges%2520across%250A%2520%2520Cultures%26entry.906535625%3DVladimir%2520Maksimenko%2520and%2520Qingyao%2520Xin%2520and%2520Prateek%2520Gupta%2520and%2520Bin%2520Zhang%2520and%2520Prateek%2520Bansal%26entry.1292438233%3D%2520%2520Nudge%2520strategies%2520are%2520effective%2520tools%2520for%2520influencing%2520behaviour%252C%2520but%2520their%250Aimpact%2520depends%2520on%2520individual%2520preferences.%2520Strategies%2520that%2520work%2520for%2520some%250Aindividuals%2520may%2520be%2520counterproductive%2520for%2520others.%2520We%2520hypothesize%2520that%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520can%2520facilitate%2520the%2520design%2520of%2520individual-specific%2520nudges%250Awithout%2520the%2520need%2520for%2520costly%2520and%2520time-intensive%2520behavioural%2520data%2520collection%2520and%250Amodelling.%2520To%2520test%2520this%252C%2520we%2520use%2520LLMs%2520to%2520design%2520personalized%2520decoy-based%2520nudges%250Atailored%2520to%2520individual%2520profiles%2520and%2520cultural%2520contexts%252C%2520aimed%2520at%2520encouraging%2520air%250Atravellers%2520to%2520voluntarily%2520offset%2520CO%2524_2%2524%2520emissions%2520from%2520flights.%2520We%2520evaluate%250Atheir%2520effectiveness%2520through%2520a%2520large-scale%2520survey%2520experiment%2520%2528%2524n%253D3495%2524%2529%250Aconducted%2520across%2520five%2520countries.%2520Results%2520show%2520that%2520LLM-informed%2520personalized%250Anudges%2520are%2520more%2520effective%2520than%2520uniform%2520settings%252C%2520raising%2520offsetting%2520rates%2520by%250A3-7%2524%255C%2525%2524%2520in%2520Germany%252C%2520Singapore%252C%2520and%2520the%2520US%252C%2520though%2520not%2520in%2520China%2520or%2520India.%2520Our%250Astudy%2520highlights%2520the%2520potential%2520of%2520LLM%2520as%2520a%2520low-cost%2520testbed%2520for%2520piloting%2520nudge%250Astrategies.%2520At%2520the%2520same%2520time%252C%2520cultural%2520heterogeneity%2520constrains%2520their%250Ageneralizability%2520underscoring%2520the%2520need%2520for%2520combining%2520LLM-based%2520simulations%2520with%250Atargeted%2520empirical%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Enable%20Design%20of%20Personalized%20Nudges%20across%0A%20%20Cultures&entry.906535625=Vladimir%20Maksimenko%20and%20Qingyao%20Xin%20and%20Prateek%20Gupta%20and%20Bin%20Zhang%20and%20Prateek%20Bansal&entry.1292438233=%20%20Nudge%20strategies%20are%20effective%20tools%20for%20influencing%20behaviour%2C%20but%20their%0Aimpact%20depends%20on%20individual%20preferences.%20Strategies%20that%20work%20for%20some%0Aindividuals%20may%20be%20counterproductive%20for%20others.%20We%20hypothesize%20that%20large%0Alanguage%20models%20%28LLMs%29%20can%20facilitate%20the%20design%20of%20individual-specific%20nudges%0Awithout%20the%20need%20for%20costly%20and%20time-intensive%20behavioural%20data%20collection%20and%0Amodelling.%20To%20test%20this%2C%20we%20use%20LLMs%20to%20design%20personalized%20decoy-based%20nudges%0Atailored%20to%20individual%20profiles%20and%20cultural%20contexts%2C%20aimed%20at%20encouraging%20air%0Atravellers%20to%20voluntarily%20offset%20CO%24_2%24%20emissions%20from%20flights.%20We%20evaluate%0Atheir%20effectiveness%20through%20a%20large-scale%20survey%20experiment%20%28%24n%3D3495%24%29%0Aconducted%20across%20five%20countries.%20Results%20show%20that%20LLM-informed%20personalized%0Anudges%20are%20more%20effective%20than%20uniform%20settings%2C%20raising%20offsetting%20rates%20by%0A3-7%24%5C%25%24%20in%20Germany%2C%20Singapore%2C%20and%20the%20US%2C%20though%20not%20in%20China%20or%20India.%20Our%0Astudy%20highlights%20the%20potential%20of%20LLM%20as%20a%20low-cost%20testbed%20for%20piloting%20nudge%0Astrategies.%20At%20the%20same%20time%2C%20cultural%20heterogeneity%20constrains%20their%0Ageneralizability%20underscoring%20the%20need%20for%20combining%20LLM-based%20simulations%20with%0Atargeted%20empirical%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12045v2&entry.124074799=Read"},
{"title": "ECG-Soup: Harnessing Multi-Layer Synergy for ECG Foundation Models", "author": "Phu X. Nguyen and Huy Phan and Hieu Pham and Christos Chatzichristos and Bert Vandenberk and Maarten De Vos", "abstract": "  Transformer-based foundation models for Electrocardiograms (ECGs) have\nrecently achieved impressive performance in many downstream applications.\n", "link": "http://arxiv.org/abs/2509.00102v2", "date": "2025-10-16", "relevancy": 1.4714, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5239}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4822}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECG-Soup%3A%20Harnessing%20Multi-Layer%20Synergy%20for%20ECG%20Foundation%20Models&body=Title%3A%20ECG-Soup%3A%20Harnessing%20Multi-Layer%20Synergy%20for%20ECG%20Foundation%20Models%0AAuthor%3A%20Phu%20X.%20Nguyen%20and%20Huy%20Phan%20and%20Hieu%20Pham%20and%20Christos%20Chatzichristos%20and%20Bert%20Vandenberk%20and%20Maarten%20De%20Vos%0AAbstract%3A%20%20%20Transformer-based%20foundation%20models%20for%20Electrocardiograms%20%28ECGs%29%20have%0Arecently%20achieved%20impressive%20performance%20in%20many%20downstream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00102v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECG-Soup%253A%2520Harnessing%2520Multi-Layer%2520Synergy%2520for%2520ECG%2520Foundation%2520Models%26entry.906535625%3DPhu%2520X.%2520Nguyen%2520and%2520Huy%2520Phan%2520and%2520Hieu%2520Pham%2520and%2520Christos%2520Chatzichristos%2520and%2520Bert%2520Vandenberk%2520and%2520Maarten%2520De%2520Vos%26entry.1292438233%3D%2520%2520Transformer-based%2520foundation%2520models%2520for%2520Electrocardiograms%2520%2528ECGs%2529%2520have%250Arecently%2520achieved%2520impressive%2520performance%2520in%2520many%2520downstream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00102v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECG-Soup%3A%20Harnessing%20Multi-Layer%20Synergy%20for%20ECG%20Foundation%20Models&entry.906535625=Phu%20X.%20Nguyen%20and%20Huy%20Phan%20and%20Hieu%20Pham%20and%20Christos%20Chatzichristos%20and%20Bert%20Vandenberk%20and%20Maarten%20De%20Vos&entry.1292438233=%20%20Transformer-based%20foundation%20models%20for%20Electrocardiograms%20%28ECGs%29%20have%0Arecently%20achieved%20impressive%20performance%20in%20many%20downstream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00102v2&entry.124074799=Read"},
{"title": "Reinforcement Learning with Stochastic Reward Machines", "author": "Jan Corazza and Ivan Gavran and Daniel Neider", "abstract": "  Reward machines are an established tool for dealing with reinforcement\nlearning problems in which rewards are sparse and depend on complex sequences\nof actions. However, existing algorithms for learning reward machines assume an\noverly idealized setting where rewards have to be free of noise. To overcome\nthis practical limitation, we introduce a novel type of reward machines, called\nstochastic reward machines, and an algorithm for learning them. Our algorithm,\nbased on constraint solving, learns minimal stochastic reward machines from the\nexplorations of a reinforcement learning agent. This algorithm can easily be\npaired with existing reinforcement learning algorithms for reward machines and\nguarantees to converge to an optimal policy in the limit. We demonstrate the\neffectiveness of our algorithm in two case studies and show that it outperforms\nboth existing methods and a naive approach for handling noisy reward functions.\n", "link": "http://arxiv.org/abs/2510.14837v1", "date": "2025-10-16", "relevancy": 1.3349, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4421}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Stochastic%20Reward%20Machines&body=Title%3A%20Reinforcement%20Learning%20with%20Stochastic%20Reward%20Machines%0AAuthor%3A%20Jan%20Corazza%20and%20Ivan%20Gavran%20and%20Daniel%20Neider%0AAbstract%3A%20%20%20Reward%20machines%20are%20an%20established%20tool%20for%20dealing%20with%20reinforcement%0Alearning%20problems%20in%20which%20rewards%20are%20sparse%20and%20depend%20on%20complex%20sequences%0Aof%20actions.%20However%2C%20existing%20algorithms%20for%20learning%20reward%20machines%20assume%20an%0Aoverly%20idealized%20setting%20where%20rewards%20have%20to%20be%20free%20of%20noise.%20To%20overcome%0Athis%20practical%20limitation%2C%20we%20introduce%20a%20novel%20type%20of%20reward%20machines%2C%20called%0Astochastic%20reward%20machines%2C%20and%20an%20algorithm%20for%20learning%20them.%20Our%20algorithm%2C%0Abased%20on%20constraint%20solving%2C%20learns%20minimal%20stochastic%20reward%20machines%20from%20the%0Aexplorations%20of%20a%20reinforcement%20learning%20agent.%20This%20algorithm%20can%20easily%20be%0Apaired%20with%20existing%20reinforcement%20learning%20algorithms%20for%20reward%20machines%20and%0Aguarantees%20to%20converge%20to%20an%20optimal%20policy%20in%20the%20limit.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithm%20in%20two%20case%20studies%20and%20show%20that%20it%20outperforms%0Aboth%20existing%20methods%20and%20a%20naive%20approach%20for%20handling%20noisy%20reward%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Stochastic%2520Reward%2520Machines%26entry.906535625%3DJan%2520Corazza%2520and%2520Ivan%2520Gavran%2520and%2520Daniel%2520Neider%26entry.1292438233%3D%2520%2520Reward%2520machines%2520are%2520an%2520established%2520tool%2520for%2520dealing%2520with%2520reinforcement%250Alearning%2520problems%2520in%2520which%2520rewards%2520are%2520sparse%2520and%2520depend%2520on%2520complex%2520sequences%250Aof%2520actions.%2520However%252C%2520existing%2520algorithms%2520for%2520learning%2520reward%2520machines%2520assume%2520an%250Aoverly%2520idealized%2520setting%2520where%2520rewards%2520have%2520to%2520be%2520free%2520of%2520noise.%2520To%2520overcome%250Athis%2520practical%2520limitation%252C%2520we%2520introduce%2520a%2520novel%2520type%2520of%2520reward%2520machines%252C%2520called%250Astochastic%2520reward%2520machines%252C%2520and%2520an%2520algorithm%2520for%2520learning%2520them.%2520Our%2520algorithm%252C%250Abased%2520on%2520constraint%2520solving%252C%2520learns%2520minimal%2520stochastic%2520reward%2520machines%2520from%2520the%250Aexplorations%2520of%2520a%2520reinforcement%2520learning%2520agent.%2520This%2520algorithm%2520can%2520easily%2520be%250Apaired%2520with%2520existing%2520reinforcement%2520learning%2520algorithms%2520for%2520reward%2520machines%2520and%250Aguarantees%2520to%2520converge%2520to%2520an%2520optimal%2520policy%2520in%2520the%2520limit.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520algorithm%2520in%2520two%2520case%2520studies%2520and%2520show%2520that%2520it%2520outperforms%250Aboth%2520existing%2520methods%2520and%2520a%2520naive%2520approach%2520for%2520handling%2520noisy%2520reward%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Stochastic%20Reward%20Machines&entry.906535625=Jan%20Corazza%20and%20Ivan%20Gavran%20and%20Daniel%20Neider&entry.1292438233=%20%20Reward%20machines%20are%20an%20established%20tool%20for%20dealing%20with%20reinforcement%0Alearning%20problems%20in%20which%20rewards%20are%20sparse%20and%20depend%20on%20complex%20sequences%0Aof%20actions.%20However%2C%20existing%20algorithms%20for%20learning%20reward%20machines%20assume%20an%0Aoverly%20idealized%20setting%20where%20rewards%20have%20to%20be%20free%20of%20noise.%20To%20overcome%0Athis%20practical%20limitation%2C%20we%20introduce%20a%20novel%20type%20of%20reward%20machines%2C%20called%0Astochastic%20reward%20machines%2C%20and%20an%20algorithm%20for%20learning%20them.%20Our%20algorithm%2C%0Abased%20on%20constraint%20solving%2C%20learns%20minimal%20stochastic%20reward%20machines%20from%20the%0Aexplorations%20of%20a%20reinforcement%20learning%20agent.%20This%20algorithm%20can%20easily%20be%0Apaired%20with%20existing%20reinforcement%20learning%20algorithms%20for%20reward%20machines%20and%0Aguarantees%20to%20converge%20to%20an%20optimal%20policy%20in%20the%20limit.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithm%20in%20two%20case%20studies%20and%20show%20that%20it%20outperforms%0Aboth%20existing%20methods%20and%20a%20naive%20approach%20for%20handling%20noisy%20reward%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14837v1&entry.124074799=Read"},
{"title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order", "author": "Egor Petrov and Grigoriy Evseev and Aleksey Antonov and Andrey Veprikov and Nikolay Bushkov and Stanislav Moiseev and Aleksandr Beznosikov", "abstract": "  Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM\n", "link": "http://arxiv.org/abs/2506.04430v4", "date": "2025-10-16", "relevancy": 1.4514, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4871}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4841}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Coordinate%20Momentum%20in%20SignSGD%20and%20Muon%3A%20Memory-Optimized%0A%20%20Zero-Order&body=Title%3A%20Leveraging%20Coordinate%20Momentum%20in%20SignSGD%20and%20Muon%3A%20Memory-Optimized%0A%20%20Zero-Order%0AAuthor%3A%20Egor%20Petrov%20and%20Grigoriy%20Evseev%20and%20Aleksey%20Antonov%20and%20Andrey%20Veprikov%20and%20Nikolay%20Bushkov%20and%20Stanislav%20Moiseev%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20for%20adapting%0Apre-trained%20models%20to%20downstream%20tasks.%20Yet%20traditional%20first-order%20optimizers%0Asuch%20as%20Stochastic%20Gradient%20Descent%20%28SGD%29%20and%20Adam%20incur%20prohibitive%20memory%20and%0Acomputational%20costs%20that%20scale%20poorly%20with%20model%20size.%20In%20this%20paper%2C%20we%0Ainvestigate%20zero-order%20%28ZO%29%20optimization%20methods%20as%20a%20memory-%20and%0Acompute-efficient%20alternative%2C%20particularly%20in%20the%20context%20of%0Aparameter-efficient%20fine-tuning%20techniques%20like%20LoRA.%20We%20propose%0A%24%5Ctexttt%7BJAGUAR%20SignSGD%7D%24%2C%20a%20ZO%20momentum-based%20algorithm%20that%20extends%20ZO%0ASignSGD%2C%20requiring%20the%20same%20number%20of%20parameters%20as%20the%20standard%20ZO%20SGD%20and%0Aonly%20%24%5Cmathcal%7BO%7D%281%29%24%20function%20evaluations%20per%20iteration.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20establish%20rigorous%20convergence%20guarantees%0Afor%20SignSGD%20in%20the%20stochastic%20ZO%20case.%20We%20further%20propose%20%24%5Ctexttt%7BJAGUAR%0AMuon%7D%24%2C%20a%20novel%20ZO%20extension%20of%20the%20Muon%20optimizer%20that%20leverages%20the%20matrix%0Astructure%20of%20model%20parameters%2C%20and%20we%20provide%20its%20convergence%20rate%20under%0Aarbitrary%20stochastic%20noise.%20Through%20extensive%20experiments%20on%20challenging%20LLM%0Afine-tuning%20benchmarks%2C%20we%20demonstrate%20that%20the%20proposed%20algorithms%20meet%20or%0Aexceed%20the%20convergence%20quality%20of%20standard%20first-order%20methods%2C%20achieving%0Asignificant%20memory%20reduction.%20Our%20theoretical%20and%20empirical%20results%20establish%0Anew%20ZO%20optimization%20methods%20as%20a%20practical%20and%20theoretically%20grounded%20approach%0Afor%20resource-constrained%20LLM%20adaptation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/brain-mmo-lab/ZO_LLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04430v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Coordinate%2520Momentum%2520in%2520SignSGD%2520and%2520Muon%253A%2520Memory-Optimized%250A%2520%2520Zero-Order%26entry.906535625%3DEgor%2520Petrov%2520and%2520Grigoriy%2520Evseev%2520and%2520Aleksey%2520Antonov%2520and%2520Andrey%2520Veprikov%2520and%2520Nikolay%2520Bushkov%2520and%2520Stanislav%2520Moiseev%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520Fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520essential%2520for%2520adapting%250Apre-trained%2520models%2520to%2520downstream%2520tasks.%2520Yet%2520traditional%2520first-order%2520optimizers%250Asuch%2520as%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%2520and%2520Adam%2520incur%2520prohibitive%2520memory%2520and%250Acomputational%2520costs%2520that%2520scale%2520poorly%2520with%2520model%2520size.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520zero-order%2520%2528ZO%2529%2520optimization%2520methods%2520as%2520a%2520memory-%2520and%250Acompute-efficient%2520alternative%252C%2520particularly%2520in%2520the%2520context%2520of%250Aparameter-efficient%2520fine-tuning%2520techniques%2520like%2520LoRA.%2520We%2520propose%250A%2524%255Ctexttt%257BJAGUAR%2520SignSGD%257D%2524%252C%2520a%2520ZO%2520momentum-based%2520algorithm%2520that%2520extends%2520ZO%250ASignSGD%252C%2520requiring%2520the%2520same%2520number%2520of%2520parameters%2520as%2520the%2520standard%2520ZO%2520SGD%2520and%250Aonly%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520function%2520evaluations%2520per%2520iteration.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520establish%2520rigorous%2520convergence%2520guarantees%250Afor%2520SignSGD%2520in%2520the%2520stochastic%2520ZO%2520case.%2520We%2520further%2520propose%2520%2524%255Ctexttt%257BJAGUAR%250AMuon%257D%2524%252C%2520a%2520novel%2520ZO%2520extension%2520of%2520the%2520Muon%2520optimizer%2520that%2520leverages%2520the%2520matrix%250Astructure%2520of%2520model%2520parameters%252C%2520and%2520we%2520provide%2520its%2520convergence%2520rate%2520under%250Aarbitrary%2520stochastic%2520noise.%2520Through%2520extensive%2520experiments%2520on%2520challenging%2520LLM%250Afine-tuning%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520algorithms%2520meet%2520or%250Aexceed%2520the%2520convergence%2520quality%2520of%2520standard%2520first-order%2520methods%252C%2520achieving%250Asignificant%2520memory%2520reduction.%2520Our%2520theoretical%2520and%2520empirical%2520results%2520establish%250Anew%2520ZO%2520optimization%2520methods%2520as%2520a%2520practical%2520and%2520theoretically%2520grounded%2520approach%250Afor%2520resource-constrained%2520LLM%2520adaptation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/brain-mmo-lab/ZO_LLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04430v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Coordinate%20Momentum%20in%20SignSGD%20and%20Muon%3A%20Memory-Optimized%0A%20%20Zero-Order&entry.906535625=Egor%20Petrov%20and%20Grigoriy%20Evseev%20and%20Aleksey%20Antonov%20and%20Andrey%20Veprikov%20and%20Nikolay%20Bushkov%20and%20Stanislav%20Moiseev%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20Fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20is%20essential%20for%20adapting%0Apre-trained%20models%20to%20downstream%20tasks.%20Yet%20traditional%20first-order%20optimizers%0Asuch%20as%20Stochastic%20Gradient%20Descent%20%28SGD%29%20and%20Adam%20incur%20prohibitive%20memory%20and%0Acomputational%20costs%20that%20scale%20poorly%20with%20model%20size.%20In%20this%20paper%2C%20we%0Ainvestigate%20zero-order%20%28ZO%29%20optimization%20methods%20as%20a%20memory-%20and%0Acompute-efficient%20alternative%2C%20particularly%20in%20the%20context%20of%0Aparameter-efficient%20fine-tuning%20techniques%20like%20LoRA.%20We%20propose%0A%24%5Ctexttt%7BJAGUAR%20SignSGD%7D%24%2C%20a%20ZO%20momentum-based%20algorithm%20that%20extends%20ZO%0ASignSGD%2C%20requiring%20the%20same%20number%20of%20parameters%20as%20the%20standard%20ZO%20SGD%20and%0Aonly%20%24%5Cmathcal%7BO%7D%281%29%24%20function%20evaluations%20per%20iteration.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20establish%20rigorous%20convergence%20guarantees%0Afor%20SignSGD%20in%20the%20stochastic%20ZO%20case.%20We%20further%20propose%20%24%5Ctexttt%7BJAGUAR%0AMuon%7D%24%2C%20a%20novel%20ZO%20extension%20of%20the%20Muon%20optimizer%20that%20leverages%20the%20matrix%0Astructure%20of%20model%20parameters%2C%20and%20we%20provide%20its%20convergence%20rate%20under%0Aarbitrary%20stochastic%20noise.%20Through%20extensive%20experiments%20on%20challenging%20LLM%0Afine-tuning%20benchmarks%2C%20we%20demonstrate%20that%20the%20proposed%20algorithms%20meet%20or%0Aexceed%20the%20convergence%20quality%20of%20standard%20first-order%20methods%2C%20achieving%0Asignificant%20memory%20reduction.%20Our%20theoretical%20and%20empirical%20results%20establish%0Anew%20ZO%20optimization%20methods%20as%20a%20practical%20and%20theoretically%20grounded%20approach%0Afor%20resource-constrained%20LLM%20adaptation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/brain-mmo-lab/ZO_LLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04430v4&entry.124074799=Read"},
{"title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with\n  Model-Based Reinforcement Learning", "author": "Aderik Verraest and Stavrow Bahnam and Robin Ferede and Guido de Croon and Christophe De Wagter", "abstract": "  Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.\n", "link": "http://arxiv.org/abs/2510.14783v1", "date": "2025-10-16", "relevancy": 1.0631, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5363}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkyDreamer%3A%20Interpretable%20End-to-End%20Vision-Based%20Drone%20Racing%20with%0A%20%20Model-Based%20Reinforcement%20Learning&body=Title%3A%20SkyDreamer%3A%20Interpretable%20End-to-End%20Vision-Based%20Drone%20Racing%20with%0A%20%20Model-Based%20Reinforcement%20Learning%0AAuthor%3A%20Aderik%20Verraest%20and%20Stavrow%20Bahnam%20and%20Robin%20Ferede%20and%20Guido%20de%20Croon%20and%20Christophe%20De%20Wagter%0AAbstract%3A%20%20%20Autonomous%20drone%20racing%20%28ADR%29%20systems%20have%20recently%20achieved%20champion-level%0Aperformance%2C%20yet%20remain%20highly%20specific%20to%20drone%20racing.%20While%20end-to-end%0Avision-based%20methods%20promise%20broader%20applicability%2C%20no%20system%20to%20date%0Asimultaneously%20achieves%20full%20sim-to-real%20transfer%2C%20onboard%20execution%2C%20and%0Achampion-level%20performance.%20In%20this%20work%2C%20we%20present%20SkyDreamer%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20the%20first%20end-to-end%20vision-based%20ADR%20policy%20that%20maps%20directly%0Afrom%20pixel-level%20representations%20to%20motor%20commands.%20SkyDreamer%20builds%20on%0Ainformed%20Dreamer%2C%20a%20model-based%20reinforcement%20learning%20approach%20where%20the%20world%0Amodel%20decodes%20to%20privileged%20information%20only%20available%20during%20training.%20By%0Aextending%20this%20concept%20to%20end-to-end%20vision-based%20ADR%2C%20the%20world%20model%0Aeffectively%20functions%20as%20an%20implicit%20state%20and%20parameter%20estimator%2C%20greatly%0Aimproving%20interpretability.%20SkyDreamer%20runs%20fully%20onboard%20without%20external%20aid%2C%0Aresolves%20visual%20ambiguities%20by%20tracking%20progress%20using%20the%20state%20decoded%20from%0Athe%20world%20model%27s%20hidden%20state%2C%20and%20requires%20no%20extrinsic%20camera%20calibration%2C%0Aenabling%20rapid%20deployment%20across%20different%20drones%20without%20retraining.%0AReal-world%20experiments%20show%20that%20SkyDreamer%20achieves%20robust%2C%20high-speed%20flight%2C%0Aexecuting%20tight%20maneuvers%20such%20as%20an%20inverted%20loop%2C%20a%20split-S%20and%20a%20ladder%2C%0Areaching%20speeds%20of%20up%20to%2021%20m/s%20and%20accelerations%20of%20up%20to%206%20g.%20It%20further%0Ademonstrates%20a%20non-trivial%20visual%20sim-to-real%20transfer%20by%20operating%20on%0Apoor-quality%20segmentation%20masks%2C%20and%20exhibits%20robustness%20to%20battery%20depletion%0Aby%20accurately%20estimating%20the%20maximum%20attainable%20motor%20RPM%20and%20adjusting%20its%0Aflight%20path%20in%20real-time.%20These%20results%20highlight%20SkyDreamer%27s%20adaptability%20to%0Aimportant%20aspects%20of%20the%20reality%20gap%2C%20bringing%20robustness%20while%20still%20achieving%0Aextremely%20high-speed%2C%20agile%20flight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyDreamer%253A%2520Interpretable%2520End-to-End%2520Vision-Based%2520Drone%2520Racing%2520with%250A%2520%2520Model-Based%2520Reinforcement%2520Learning%26entry.906535625%3DAderik%2520Verraest%2520and%2520Stavrow%2520Bahnam%2520and%2520Robin%2520Ferede%2520and%2520Guido%2520de%2520Croon%2520and%2520Christophe%2520De%2520Wagter%26entry.1292438233%3D%2520%2520Autonomous%2520drone%2520racing%2520%2528ADR%2529%2520systems%2520have%2520recently%2520achieved%2520champion-level%250Aperformance%252C%2520yet%2520remain%2520highly%2520specific%2520to%2520drone%2520racing.%2520While%2520end-to-end%250Avision-based%2520methods%2520promise%2520broader%2520applicability%252C%2520no%2520system%2520to%2520date%250Asimultaneously%2520achieves%2520full%2520sim-to-real%2520transfer%252C%2520onboard%2520execution%252C%2520and%250Achampion-level%2520performance.%2520In%2520this%2520work%252C%2520we%2520present%2520SkyDreamer%252C%2520to%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520the%2520first%2520end-to-end%2520vision-based%2520ADR%2520policy%2520that%2520maps%2520directly%250Afrom%2520pixel-level%2520representations%2520to%2520motor%2520commands.%2520SkyDreamer%2520builds%2520on%250Ainformed%2520Dreamer%252C%2520a%2520model-based%2520reinforcement%2520learning%2520approach%2520where%2520the%2520world%250Amodel%2520decodes%2520to%2520privileged%2520information%2520only%2520available%2520during%2520training.%2520By%250Aextending%2520this%2520concept%2520to%2520end-to-end%2520vision-based%2520ADR%252C%2520the%2520world%2520model%250Aeffectively%2520functions%2520as%2520an%2520implicit%2520state%2520and%2520parameter%2520estimator%252C%2520greatly%250Aimproving%2520interpretability.%2520SkyDreamer%2520runs%2520fully%2520onboard%2520without%2520external%2520aid%252C%250Aresolves%2520visual%2520ambiguities%2520by%2520tracking%2520progress%2520using%2520the%2520state%2520decoded%2520from%250Athe%2520world%2520model%2527s%2520hidden%2520state%252C%2520and%2520requires%2520no%2520extrinsic%2520camera%2520calibration%252C%250Aenabling%2520rapid%2520deployment%2520across%2520different%2520drones%2520without%2520retraining.%250AReal-world%2520experiments%2520show%2520that%2520SkyDreamer%2520achieves%2520robust%252C%2520high-speed%2520flight%252C%250Aexecuting%2520tight%2520maneuvers%2520such%2520as%2520an%2520inverted%2520loop%252C%2520a%2520split-S%2520and%2520a%2520ladder%252C%250Areaching%2520speeds%2520of%2520up%2520to%252021%2520m/s%2520and%2520accelerations%2520of%2520up%2520to%25206%2520g.%2520It%2520further%250Ademonstrates%2520a%2520non-trivial%2520visual%2520sim-to-real%2520transfer%2520by%2520operating%2520on%250Apoor-quality%2520segmentation%2520masks%252C%2520and%2520exhibits%2520robustness%2520to%2520battery%2520depletion%250Aby%2520accurately%2520estimating%2520the%2520maximum%2520attainable%2520motor%2520RPM%2520and%2520adjusting%2520its%250Aflight%2520path%2520in%2520real-time.%2520These%2520results%2520highlight%2520SkyDreamer%2527s%2520adaptability%2520to%250Aimportant%2520aspects%2520of%2520the%2520reality%2520gap%252C%2520bringing%2520robustness%2520while%2520still%2520achieving%250Aextremely%2520high-speed%252C%2520agile%2520flight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkyDreamer%3A%20Interpretable%20End-to-End%20Vision-Based%20Drone%20Racing%20with%0A%20%20Model-Based%20Reinforcement%20Learning&entry.906535625=Aderik%20Verraest%20and%20Stavrow%20Bahnam%20and%20Robin%20Ferede%20and%20Guido%20de%20Croon%20and%20Christophe%20De%20Wagter&entry.1292438233=%20%20Autonomous%20drone%20racing%20%28ADR%29%20systems%20have%20recently%20achieved%20champion-level%0Aperformance%2C%20yet%20remain%20highly%20specific%20to%20drone%20racing.%20While%20end-to-end%0Avision-based%20methods%20promise%20broader%20applicability%2C%20no%20system%20to%20date%0Asimultaneously%20achieves%20full%20sim-to-real%20transfer%2C%20onboard%20execution%2C%20and%0Achampion-level%20performance.%20In%20this%20work%2C%20we%20present%20SkyDreamer%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20the%20first%20end-to-end%20vision-based%20ADR%20policy%20that%20maps%20directly%0Afrom%20pixel-level%20representations%20to%20motor%20commands.%20SkyDreamer%20builds%20on%0Ainformed%20Dreamer%2C%20a%20model-based%20reinforcement%20learning%20approach%20where%20the%20world%0Amodel%20decodes%20to%20privileged%20information%20only%20available%20during%20training.%20By%0Aextending%20this%20concept%20to%20end-to-end%20vision-based%20ADR%2C%20the%20world%20model%0Aeffectively%20functions%20as%20an%20implicit%20state%20and%20parameter%20estimator%2C%20greatly%0Aimproving%20interpretability.%20SkyDreamer%20runs%20fully%20onboard%20without%20external%20aid%2C%0Aresolves%20visual%20ambiguities%20by%20tracking%20progress%20using%20the%20state%20decoded%20from%0Athe%20world%20model%27s%20hidden%20state%2C%20and%20requires%20no%20extrinsic%20camera%20calibration%2C%0Aenabling%20rapid%20deployment%20across%20different%20drones%20without%20retraining.%0AReal-world%20experiments%20show%20that%20SkyDreamer%20achieves%20robust%2C%20high-speed%20flight%2C%0Aexecuting%20tight%20maneuvers%20such%20as%20an%20inverted%20loop%2C%20a%20split-S%20and%20a%20ladder%2C%0Areaching%20speeds%20of%20up%20to%2021%20m/s%20and%20accelerations%20of%20up%20to%206%20g.%20It%20further%0Ademonstrates%20a%20non-trivial%20visual%20sim-to-real%20transfer%20by%20operating%20on%0Apoor-quality%20segmentation%20masks%2C%20and%20exhibits%20robustness%20to%20battery%20depletion%0Aby%20accurately%20estimating%20the%20maximum%20attainable%20motor%20RPM%20and%20adjusting%20its%0Aflight%20path%20in%20real-time.%20These%20results%20highlight%20SkyDreamer%27s%20adaptability%20to%0Aimportant%20aspects%20of%20the%20reality%20gap%2C%20bringing%20robustness%20while%20still%20achieving%0Aextremely%20high-speed%2C%20agile%20flight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14783v1&entry.124074799=Read"},
{"title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift\n  nuPlan Benchmarks", "author": "Steffen Hagedorn and Luka Donkov and Aron Distelzweig and Alexandru P. Condurache", "abstract": "  Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.\n", "link": "http://arxiv.org/abs/2510.14677v1", "date": "2025-10-16", "relevancy": 1.9574, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Planners%20Meet%20Reality%3A%20How%20Learned%2C%20Reactive%20Traffic%20Agents%20Shift%0A%20%20nuPlan%20Benchmarks&body=Title%3A%20When%20Planners%20Meet%20Reality%3A%20How%20Learned%2C%20Reactive%20Traffic%20Agents%20Shift%0A%20%20nuPlan%20Benchmarks%0AAuthor%3A%20Steffen%20Hagedorn%20and%20Luka%20Donkov%20and%20Aron%20Distelzweig%20and%20Alexandru%20P.%20Condurache%0AAbstract%3A%20%20%20Planner%20evaluation%20in%20closed-loop%20simulation%20often%20uses%20rule-based%20traffic%0Aagents%2C%20whose%20simplistic%20and%20passive%20behavior%20can%20hide%20planner%20deficiencies%20and%0Abias%20rankings.%20Widely%20used%20IDM%20agents%20simply%20follow%20a%20lead%20vehicle%20and%20cannot%0Areact%20to%20vehicles%20in%20adjacent%20lanes%2C%20hindering%20tests%20of%20complex%20interaction%0Acapabilities.%20We%20address%20this%20issue%20by%20integrating%20the%20state-of-the-art%20learned%0Atraffic%20agent%20model%20SMART%20into%20nuPlan.%20Thus%2C%20we%20are%20the%20first%20to%20evaluate%0Aplanners%20under%20more%20realistic%20conditions%20and%20quantify%20how%20conclusions%20shift%0Awhen%20narrowing%20the%20sim-to-real%20gap.%20Our%20analysis%20covers%2014%20recent%20planners%20and%0Aestablished%20baselines%20and%20shows%20that%20IDM-based%20simulation%20overestimates%0Aplanning%20performance%3A%20nearly%20all%20scores%20deteriorate.%20In%20contrast%2C%20many%20planners%0Ainteract%20better%20than%20previously%20assumed%20and%20even%20improve%20in%20multi-lane%2C%0Ainteraction-heavy%20scenarios%20like%20lane%20changes%20or%20turns.%20Methods%20trained%20in%0Aclosed-loop%20demonstrate%20the%20best%20and%20most%20stable%20driving%20performance.%20However%2C%0Awhen%20reaching%20their%20limits%20in%20augmented%20edge-case%20scenarios%2C%20all%20learned%0Aplanners%20degrade%20abruptly%2C%20whereas%20rule-based%20planners%20maintain%20reasonable%0Abasic%20behavior.%20Based%20on%20our%20results%2C%20we%20suggest%20SMART-reactive%20simulation%20as%20a%0Anew%20standard%20closed-loop%20benchmark%20in%20nuPlan%20and%20release%20the%20SMART%20agents%20as%20a%0Adrop-in%20alternative%20to%20IDM%20at%20https%3A//github.com/shgd95/InteractiveClosedLoop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Planners%2520Meet%2520Reality%253A%2520How%2520Learned%252C%2520Reactive%2520Traffic%2520Agents%2520Shift%250A%2520%2520nuPlan%2520Benchmarks%26entry.906535625%3DSteffen%2520Hagedorn%2520and%2520Luka%2520Donkov%2520and%2520Aron%2520Distelzweig%2520and%2520Alexandru%2520P.%2520Condurache%26entry.1292438233%3D%2520%2520Planner%2520evaluation%2520in%2520closed-loop%2520simulation%2520often%2520uses%2520rule-based%2520traffic%250Aagents%252C%2520whose%2520simplistic%2520and%2520passive%2520behavior%2520can%2520hide%2520planner%2520deficiencies%2520and%250Abias%2520rankings.%2520Widely%2520used%2520IDM%2520agents%2520simply%2520follow%2520a%2520lead%2520vehicle%2520and%2520cannot%250Areact%2520to%2520vehicles%2520in%2520adjacent%2520lanes%252C%2520hindering%2520tests%2520of%2520complex%2520interaction%250Acapabilities.%2520We%2520address%2520this%2520issue%2520by%2520integrating%2520the%2520state-of-the-art%2520learned%250Atraffic%2520agent%2520model%2520SMART%2520into%2520nuPlan.%2520Thus%252C%2520we%2520are%2520the%2520first%2520to%2520evaluate%250Aplanners%2520under%2520more%2520realistic%2520conditions%2520and%2520quantify%2520how%2520conclusions%2520shift%250Awhen%2520narrowing%2520the%2520sim-to-real%2520gap.%2520Our%2520analysis%2520covers%252014%2520recent%2520planners%2520and%250Aestablished%2520baselines%2520and%2520shows%2520that%2520IDM-based%2520simulation%2520overestimates%250Aplanning%2520performance%253A%2520nearly%2520all%2520scores%2520deteriorate.%2520In%2520contrast%252C%2520many%2520planners%250Ainteract%2520better%2520than%2520previously%2520assumed%2520and%2520even%2520improve%2520in%2520multi-lane%252C%250Ainteraction-heavy%2520scenarios%2520like%2520lane%2520changes%2520or%2520turns.%2520Methods%2520trained%2520in%250Aclosed-loop%2520demonstrate%2520the%2520best%2520and%2520most%2520stable%2520driving%2520performance.%2520However%252C%250Awhen%2520reaching%2520their%2520limits%2520in%2520augmented%2520edge-case%2520scenarios%252C%2520all%2520learned%250Aplanners%2520degrade%2520abruptly%252C%2520whereas%2520rule-based%2520planners%2520maintain%2520reasonable%250Abasic%2520behavior.%2520Based%2520on%2520our%2520results%252C%2520we%2520suggest%2520SMART-reactive%2520simulation%2520as%2520a%250Anew%2520standard%2520closed-loop%2520benchmark%2520in%2520nuPlan%2520and%2520release%2520the%2520SMART%2520agents%2520as%2520a%250Adrop-in%2520alternative%2520to%2520IDM%2520at%2520https%253A//github.com/shgd95/InteractiveClosedLoop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Planners%20Meet%20Reality%3A%20How%20Learned%2C%20Reactive%20Traffic%20Agents%20Shift%0A%20%20nuPlan%20Benchmarks&entry.906535625=Steffen%20Hagedorn%20and%20Luka%20Donkov%20and%20Aron%20Distelzweig%20and%20Alexandru%20P.%20Condurache&entry.1292438233=%20%20Planner%20evaluation%20in%20closed-loop%20simulation%20often%20uses%20rule-based%20traffic%0Aagents%2C%20whose%20simplistic%20and%20passive%20behavior%20can%20hide%20planner%20deficiencies%20and%0Abias%20rankings.%20Widely%20used%20IDM%20agents%20simply%20follow%20a%20lead%20vehicle%20and%20cannot%0Areact%20to%20vehicles%20in%20adjacent%20lanes%2C%20hindering%20tests%20of%20complex%20interaction%0Acapabilities.%20We%20address%20this%20issue%20by%20integrating%20the%20state-of-the-art%20learned%0Atraffic%20agent%20model%20SMART%20into%20nuPlan.%20Thus%2C%20we%20are%20the%20first%20to%20evaluate%0Aplanners%20under%20more%20realistic%20conditions%20and%20quantify%20how%20conclusions%20shift%0Awhen%20narrowing%20the%20sim-to-real%20gap.%20Our%20analysis%20covers%2014%20recent%20planners%20and%0Aestablished%20baselines%20and%20shows%20that%20IDM-based%20simulation%20overestimates%0Aplanning%20performance%3A%20nearly%20all%20scores%20deteriorate.%20In%20contrast%2C%20many%20planners%0Ainteract%20better%20than%20previously%20assumed%20and%20even%20improve%20in%20multi-lane%2C%0Ainteraction-heavy%20scenarios%20like%20lane%20changes%20or%20turns.%20Methods%20trained%20in%0Aclosed-loop%20demonstrate%20the%20best%20and%20most%20stable%20driving%20performance.%20However%2C%0Awhen%20reaching%20their%20limits%20in%20augmented%20edge-case%20scenarios%2C%20all%20learned%0Aplanners%20degrade%20abruptly%2C%20whereas%20rule-based%20planners%20maintain%20reasonable%0Abasic%20behavior.%20Based%20on%20our%20results%2C%20we%20suggest%20SMART-reactive%20simulation%20as%20a%0Anew%20standard%20closed-loop%20benchmark%20in%20nuPlan%20and%20release%20the%20SMART%20agents%20as%20a%0Adrop-in%20alternative%20to%20IDM%20at%20https%3A//github.com/shgd95/InteractiveClosedLoop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14677v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


