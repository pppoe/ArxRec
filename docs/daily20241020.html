<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241017.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes", "author": "Xinjie Zhang and Zhening Liu and Yifan Zhang and Xingtong Ge and Dailan He and Tongda Xu and Yan Wang and Zehong Lin and Shuicheng Yan and Jun Zhang", "abstract": "  4D Gaussian Splatting (4DGS) has recently emerged as a promising technique\nfor capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D\nGaussian representation and a GPU-friendly rasterizer, enabling rapid rendering\nspeeds. Despite its advantages, 4DGS faces significant challenges, notably the\nrequirement of millions of 4D Gaussians, each with extensive associated\nattributes, leading to substantial memory and storage cost. This paper\nintroduces a memory-efficient framework for 4DGS. We streamline the color\nattribute by decomposing it into a per-Gaussian direct color component with\nonly 3 parameters and a shared lightweight alternating current color predictor.\nThis approach eliminates the need for spherical harmonics coefficients, which\ntypically involve up to 144 parameters in classic 4DGS, thereby creating a\nmemory-efficient 4D Gaussian representation. Furthermore, we introduce an\nentropy-constrained Gaussian deformation technique that uses a deformation\nfield to expand the action range of each Gaussian and integrates an\nopacity-based entropy loss to limit the number of Gaussians, thus forcing our\nmodel to use as few Gaussians as possible to fit a dynamic scene well. With\nsimple half-precision storage and zip compression, our framework achieves a\nstorage reduction by approximately 190$\\times$ and 125$\\times$ on the\nTechnicolor and Neural 3D Video datasets, respectively, compared to the\noriginal 4DGS. Meanwhile, it maintains comparable rendering speeds and scene\nrepresentation quality, setting a new standard in the field.\n", "link": "http://arxiv.org/abs/2410.13613v1", "date": "2024-10-17", "relevancy": 3.3616, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.711}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6714}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Memory-Efficient%204D%20Gaussian%20Splatting%20for%20Dynamic%20Scenes&body=Title%3A%20MEGA%3A%20Memory-Efficient%204D%20Gaussian%20Splatting%20for%20Dynamic%20Scenes%0AAuthor%3A%20Xinjie%20Zhang%20and%20Zhening%20Liu%20and%20Yifan%20Zhang%20and%20Xingtong%20Ge%20and%20Dailan%20He%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Zehong%20Lin%20and%20Shuicheng%20Yan%20and%20Jun%20Zhang%0AAbstract%3A%20%20%204D%20Gaussian%20Splatting%20%284DGS%29%20has%20recently%20emerged%20as%20a%20promising%20technique%0Afor%20capturing%20complex%20dynamic%203D%20scenes%20with%20high%20fidelity.%20It%20utilizes%20a%204D%0AGaussian%20representation%20and%20a%20GPU-friendly%20rasterizer%2C%20enabling%20rapid%20rendering%0Aspeeds.%20Despite%20its%20advantages%2C%204DGS%20faces%20significant%20challenges%2C%20notably%20the%0Arequirement%20of%20millions%20of%204D%20Gaussians%2C%20each%20with%20extensive%20associated%0Aattributes%2C%20leading%20to%20substantial%20memory%20and%20storage%20cost.%20This%20paper%0Aintroduces%20a%20memory-efficient%20framework%20for%204DGS.%20We%20streamline%20the%20color%0Aattribute%20by%20decomposing%20it%20into%20a%20per-Gaussian%20direct%20color%20component%20with%0Aonly%203%20parameters%20and%20a%20shared%20lightweight%20alternating%20current%20color%20predictor.%0AThis%20approach%20eliminates%20the%20need%20for%20spherical%20harmonics%20coefficients%2C%20which%0Atypically%20involve%20up%20to%20144%20parameters%20in%20classic%204DGS%2C%20thereby%20creating%20a%0Amemory-efficient%204D%20Gaussian%20representation.%20Furthermore%2C%20we%20introduce%20an%0Aentropy-constrained%20Gaussian%20deformation%20technique%20that%20uses%20a%20deformation%0Afield%20to%20expand%20the%20action%20range%20of%20each%20Gaussian%20and%20integrates%20an%0Aopacity-based%20entropy%20loss%20to%20limit%20the%20number%20of%20Gaussians%2C%20thus%20forcing%20our%0Amodel%20to%20use%20as%20few%20Gaussians%20as%20possible%20to%20fit%20a%20dynamic%20scene%20well.%20With%0Asimple%20half-precision%20storage%20and%20zip%20compression%2C%20our%20framework%20achieves%20a%0Astorage%20reduction%20by%20approximately%20190%24%5Ctimes%24%20and%20125%24%5Ctimes%24%20on%20the%0ATechnicolor%20and%20Neural%203D%20Video%20datasets%2C%20respectively%2C%20compared%20to%20the%0Aoriginal%204DGS.%20Meanwhile%2C%20it%20maintains%20comparable%20rendering%20speeds%20and%20scene%0Arepresentation%20quality%2C%20setting%20a%20new%20standard%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Memory-Efficient%25204D%2520Gaussian%2520Splatting%2520for%2520Dynamic%2520Scenes%26entry.906535625%3DXinjie%2520Zhang%2520and%2520Zhening%2520Liu%2520and%2520Yifan%2520Zhang%2520and%2520Xingtong%2520Ge%2520and%2520Dailan%2520He%2520and%2520Tongda%2520Xu%2520and%2520Yan%2520Wang%2520and%2520Zehong%2520Lin%2520and%2520Shuicheng%2520Yan%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%25204D%2520Gaussian%2520Splatting%2520%25284DGS%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520technique%250Afor%2520capturing%2520complex%2520dynamic%25203D%2520scenes%2520with%2520high%2520fidelity.%2520It%2520utilizes%2520a%25204D%250AGaussian%2520representation%2520and%2520a%2520GPU-friendly%2520rasterizer%252C%2520enabling%2520rapid%2520rendering%250Aspeeds.%2520Despite%2520its%2520advantages%252C%25204DGS%2520faces%2520significant%2520challenges%252C%2520notably%2520the%250Arequirement%2520of%2520millions%2520of%25204D%2520Gaussians%252C%2520each%2520with%2520extensive%2520associated%250Aattributes%252C%2520leading%2520to%2520substantial%2520memory%2520and%2520storage%2520cost.%2520This%2520paper%250Aintroduces%2520a%2520memory-efficient%2520framework%2520for%25204DGS.%2520We%2520streamline%2520the%2520color%250Aattribute%2520by%2520decomposing%2520it%2520into%2520a%2520per-Gaussian%2520direct%2520color%2520component%2520with%250Aonly%25203%2520parameters%2520and%2520a%2520shared%2520lightweight%2520alternating%2520current%2520color%2520predictor.%250AThis%2520approach%2520eliminates%2520the%2520need%2520for%2520spherical%2520harmonics%2520coefficients%252C%2520which%250Atypically%2520involve%2520up%2520to%2520144%2520parameters%2520in%2520classic%25204DGS%252C%2520thereby%2520creating%2520a%250Amemory-efficient%25204D%2520Gaussian%2520representation.%2520Furthermore%252C%2520we%2520introduce%2520an%250Aentropy-constrained%2520Gaussian%2520deformation%2520technique%2520that%2520uses%2520a%2520deformation%250Afield%2520to%2520expand%2520the%2520action%2520range%2520of%2520each%2520Gaussian%2520and%2520integrates%2520an%250Aopacity-based%2520entropy%2520loss%2520to%2520limit%2520the%2520number%2520of%2520Gaussians%252C%2520thus%2520forcing%2520our%250Amodel%2520to%2520use%2520as%2520few%2520Gaussians%2520as%2520possible%2520to%2520fit%2520a%2520dynamic%2520scene%2520well.%2520With%250Asimple%2520half-precision%2520storage%2520and%2520zip%2520compression%252C%2520our%2520framework%2520achieves%2520a%250Astorage%2520reduction%2520by%2520approximately%2520190%2524%255Ctimes%2524%2520and%2520125%2524%255Ctimes%2524%2520on%2520the%250ATechnicolor%2520and%2520Neural%25203D%2520Video%2520datasets%252C%2520respectively%252C%2520compared%2520to%2520the%250Aoriginal%25204DGS.%2520Meanwhile%252C%2520it%2520maintains%2520comparable%2520rendering%2520speeds%2520and%2520scene%250Arepresentation%2520quality%252C%2520setting%2520a%2520new%2520standard%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Memory-Efficient%204D%20Gaussian%20Splatting%20for%20Dynamic%20Scenes&entry.906535625=Xinjie%20Zhang%20and%20Zhening%20Liu%20and%20Yifan%20Zhang%20and%20Xingtong%20Ge%20and%20Dailan%20He%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Zehong%20Lin%20and%20Shuicheng%20Yan%20and%20Jun%20Zhang&entry.1292438233=%20%204D%20Gaussian%20Splatting%20%284DGS%29%20has%20recently%20emerged%20as%20a%20promising%20technique%0Afor%20capturing%20complex%20dynamic%203D%20scenes%20with%20high%20fidelity.%20It%20utilizes%20a%204D%0AGaussian%20representation%20and%20a%20GPU-friendly%20rasterizer%2C%20enabling%20rapid%20rendering%0Aspeeds.%20Despite%20its%20advantages%2C%204DGS%20faces%20significant%20challenges%2C%20notably%20the%0Arequirement%20of%20millions%20of%204D%20Gaussians%2C%20each%20with%20extensive%20associated%0Aattributes%2C%20leading%20to%20substantial%20memory%20and%20storage%20cost.%20This%20paper%0Aintroduces%20a%20memory-efficient%20framework%20for%204DGS.%20We%20streamline%20the%20color%0Aattribute%20by%20decomposing%20it%20into%20a%20per-Gaussian%20direct%20color%20component%20with%0Aonly%203%20parameters%20and%20a%20shared%20lightweight%20alternating%20current%20color%20predictor.%0AThis%20approach%20eliminates%20the%20need%20for%20spherical%20harmonics%20coefficients%2C%20which%0Atypically%20involve%20up%20to%20144%20parameters%20in%20classic%204DGS%2C%20thereby%20creating%20a%0Amemory-efficient%204D%20Gaussian%20representation.%20Furthermore%2C%20we%20introduce%20an%0Aentropy-constrained%20Gaussian%20deformation%20technique%20that%20uses%20a%20deformation%0Afield%20to%20expand%20the%20action%20range%20of%20each%20Gaussian%20and%20integrates%20an%0Aopacity-based%20entropy%20loss%20to%20limit%20the%20number%20of%20Gaussians%2C%20thus%20forcing%20our%0Amodel%20to%20use%20as%20few%20Gaussians%20as%20possible%20to%20fit%20a%20dynamic%20scene%20well.%20With%0Asimple%20half-precision%20storage%20and%20zip%20compression%2C%20our%20framework%20achieves%20a%0Astorage%20reduction%20by%20approximately%20190%24%5Ctimes%24%20and%20125%24%5Ctimes%24%20on%20the%0ATechnicolor%20and%20Neural%203D%20Video%20datasets%2C%20respectively%2C%20compared%20to%20the%0Aoriginal%204DGS.%20Meanwhile%2C%20it%20maintains%20comparable%20rendering%20speeds%20and%20scene%0Arepresentation%20quality%2C%20setting%20a%20new%20standard%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13613v1&entry.124074799=Read"},
{"title": "DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation\n  for Dynamic Scene Rendering", "author": "Jiahao Lu and Jiacheng Deng and Ruijie Zhu and Yanzhe Liang and Wenfei Yang and Tianzhu Zhang and Xu Zhou", "abstract": "  Dynamic scenes rendering is an intriguing yet challenging problem. Although\ncurrent methods based on NeRF have achieved satisfactory performance, they\nstill can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS)\nhas gar?nered researchers attention due to their outstanding rendering quality\nand real?time speed. Therefore, a new paradigm has been proposed: defining a\ncanonical 3D gaussians and deforming it to individual frames in deformable\nfields. How?ever, since the coordinates of canonical 3D gaussians are filled\nwith noise, which can transfer noise into the deformable fields, and there is\ncurrently no method that adequately considers the aggregation of 4D\ninformation. Therefore, we pro?pose Denoised Deformable Network with\nTemporal-Spatial Aggregation for Dy?namic Scene Rendering (DN-4DGS).\nSpecifically, a Noise Suppression Strategy is introduced to change the\ndistribution of the coordinates of the canonical 3D gaussians and suppress\nnoise. Additionally, a Decoupled Temporal-Spatial Ag?gregation Module is\ndesigned to aggregate information from adjacent points and frames. Extensive\nexperiments on various real-world datasets demonstrate that our method achieves\nstate-of-the-art rendering quality under a real-time level.\n", "link": "http://arxiv.org/abs/2410.13607v1", "date": "2024-10-17", "relevancy": 3.1932, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6757}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6481}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DN-4DGS%3A%20Denoised%20Deformable%20Network%20with%20Temporal-Spatial%20Aggregation%0A%20%20for%20Dynamic%20Scene%20Rendering&body=Title%3A%20DN-4DGS%3A%20Denoised%20Deformable%20Network%20with%20Temporal-Spatial%20Aggregation%0A%20%20for%20Dynamic%20Scene%20Rendering%0AAuthor%3A%20Jiahao%20Lu%20and%20Jiacheng%20Deng%20and%20Ruijie%20Zhu%20and%20Yanzhe%20Liang%20and%20Wenfei%20Yang%20and%20Tianzhu%20Zhang%20and%20Xu%20Zhou%0AAbstract%3A%20%20%20Dynamic%20scenes%20rendering%20is%20an%20intriguing%20yet%20challenging%20problem.%20Although%0Acurrent%20methods%20based%20on%20NeRF%20have%20achieved%20satisfactory%20performance%2C%20they%0Astill%20can%20not%20reach%20real-time%20levels.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%0Ahas%20gar%3Fnered%20researchers%20attention%20due%20to%20their%20outstanding%20rendering%20quality%0Aand%20real%3Ftime%20speed.%20Therefore%2C%20a%20new%20paradigm%20has%20been%20proposed%3A%20defining%20a%0Acanonical%203D%20gaussians%20and%20deforming%20it%20to%20individual%20frames%20in%20deformable%0Afields.%20How%3Fever%2C%20since%20the%20coordinates%20of%20canonical%203D%20gaussians%20are%20filled%0Awith%20noise%2C%20which%20can%20transfer%20noise%20into%20the%20deformable%20fields%2C%20and%20there%20is%0Acurrently%20no%20method%20that%20adequately%20considers%20the%20aggregation%20of%204D%0Ainformation.%20Therefore%2C%20we%20pro%3Fpose%20Denoised%20Deformable%20Network%20with%0ATemporal-Spatial%20Aggregation%20for%20Dy%3Fnamic%20Scene%20Rendering%20%28DN-4DGS%29.%0ASpecifically%2C%20a%20Noise%20Suppression%20Strategy%20is%20introduced%20to%20change%20the%0Adistribution%20of%20the%20coordinates%20of%20the%20canonical%203D%20gaussians%20and%20suppress%0Anoise.%20Additionally%2C%20a%20Decoupled%20Temporal-Spatial%20Ag%3Fgregation%20Module%20is%0Adesigned%20to%20aggregate%20information%20from%20adjacent%20points%20and%20frames.%20Extensive%0Aexperiments%20on%20various%20real-world%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20rendering%20quality%20under%20a%20real-time%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDN-4DGS%253A%2520Denoised%2520Deformable%2520Network%2520with%2520Temporal-Spatial%2520Aggregation%250A%2520%2520for%2520Dynamic%2520Scene%2520Rendering%26entry.906535625%3DJiahao%2520Lu%2520and%2520Jiacheng%2520Deng%2520and%2520Ruijie%2520Zhu%2520and%2520Yanzhe%2520Liang%2520and%2520Wenfei%2520Yang%2520and%2520Tianzhu%2520Zhang%2520and%2520Xu%2520Zhou%26entry.1292438233%3D%2520%2520Dynamic%2520scenes%2520rendering%2520is%2520an%2520intriguing%2520yet%2520challenging%2520problem.%2520Although%250Acurrent%2520methods%2520based%2520on%2520NeRF%2520have%2520achieved%2520satisfactory%2520performance%252C%2520they%250Astill%2520can%2520not%2520reach%2520real-time%2520levels.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Ahas%2520gar%253Fnered%2520researchers%2520attention%2520due%2520to%2520their%2520outstanding%2520rendering%2520quality%250Aand%2520real%253Ftime%2520speed.%2520Therefore%252C%2520a%2520new%2520paradigm%2520has%2520been%2520proposed%253A%2520defining%2520a%250Acanonical%25203D%2520gaussians%2520and%2520deforming%2520it%2520to%2520individual%2520frames%2520in%2520deformable%250Afields.%2520How%253Fever%252C%2520since%2520the%2520coordinates%2520of%2520canonical%25203D%2520gaussians%2520are%2520filled%250Awith%2520noise%252C%2520which%2520can%2520transfer%2520noise%2520into%2520the%2520deformable%2520fields%252C%2520and%2520there%2520is%250Acurrently%2520no%2520method%2520that%2520adequately%2520considers%2520the%2520aggregation%2520of%25204D%250Ainformation.%2520Therefore%252C%2520we%2520pro%253Fpose%2520Denoised%2520Deformable%2520Network%2520with%250ATemporal-Spatial%2520Aggregation%2520for%2520Dy%253Fnamic%2520Scene%2520Rendering%2520%2528DN-4DGS%2529.%250ASpecifically%252C%2520a%2520Noise%2520Suppression%2520Strategy%2520is%2520introduced%2520to%2520change%2520the%250Adistribution%2520of%2520the%2520coordinates%2520of%2520the%2520canonical%25203D%2520gaussians%2520and%2520suppress%250Anoise.%2520Additionally%252C%2520a%2520Decoupled%2520Temporal-Spatial%2520Ag%253Fgregation%2520Module%2520is%250Adesigned%2520to%2520aggregate%2520information%2520from%2520adjacent%2520points%2520and%2520frames.%2520Extensive%250Aexperiments%2520on%2520various%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520rendering%2520quality%2520under%2520a%2520real-time%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DN-4DGS%3A%20Denoised%20Deformable%20Network%20with%20Temporal-Spatial%20Aggregation%0A%20%20for%20Dynamic%20Scene%20Rendering&entry.906535625=Jiahao%20Lu%20and%20Jiacheng%20Deng%20and%20Ruijie%20Zhu%20and%20Yanzhe%20Liang%20and%20Wenfei%20Yang%20and%20Tianzhu%20Zhang%20and%20Xu%20Zhou&entry.1292438233=%20%20Dynamic%20scenes%20rendering%20is%20an%20intriguing%20yet%20challenging%20problem.%20Although%0Acurrent%20methods%20based%20on%20NeRF%20have%20achieved%20satisfactory%20performance%2C%20they%0Astill%20can%20not%20reach%20real-time%20levels.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%0Ahas%20gar%3Fnered%20researchers%20attention%20due%20to%20their%20outstanding%20rendering%20quality%0Aand%20real%3Ftime%20speed.%20Therefore%2C%20a%20new%20paradigm%20has%20been%20proposed%3A%20defining%20a%0Acanonical%203D%20gaussians%20and%20deforming%20it%20to%20individual%20frames%20in%20deformable%0Afields.%20How%3Fever%2C%20since%20the%20coordinates%20of%20canonical%203D%20gaussians%20are%20filled%0Awith%20noise%2C%20which%20can%20transfer%20noise%20into%20the%20deformable%20fields%2C%20and%20there%20is%0Acurrently%20no%20method%20that%20adequately%20considers%20the%20aggregation%20of%204D%0Ainformation.%20Therefore%2C%20we%20pro%3Fpose%20Denoised%20Deformable%20Network%20with%0ATemporal-Spatial%20Aggregation%20for%20Dy%3Fnamic%20Scene%20Rendering%20%28DN-4DGS%29.%0ASpecifically%2C%20a%20Noise%20Suppression%20Strategy%20is%20introduced%20to%20change%20the%0Adistribution%20of%20the%20coordinates%20of%20the%20canonical%203D%20gaussians%20and%20suppress%0Anoise.%20Additionally%2C%20a%20Decoupled%20Temporal-Spatial%20Ag%3Fgregation%20Module%20is%0Adesigned%20to%20aggregate%20information%20from%20adjacent%20points%20and%20frames.%20Extensive%0Aexperiments%20on%20various%20real-world%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20rendering%20quality%20under%20a%20real-time%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13607v1&entry.124074799=Read"},
{"title": "DepthSplat: Connecting Gaussian Splatting and Depth", "author": "Haofei Xu and Songyou Peng and Fangjinhua Wang and Hermann Blum and Daniel Barath and Andreas Geiger and Marc Pollefeys", "abstract": "  Gaussian splatting and single/multi-view depth estimation are typically\nstudied in isolation. In this paper, we present DepthSplat to connect Gaussian\nsplatting and depth estimation and study their interactions. More specifically,\nwe first contribute a robust multi-view depth model by leveraging pre-trained\nmonocular depth features, leading to high-quality feed-forward 3D Gaussian\nsplatting reconstructions. We also show that Gaussian splatting can serve as an\nunsupervised pre-training objective for learning powerful depth models from\nlarge-scale unlabelled datasets. We validate the synergy between Gaussian\nsplatting and depth estimation through extensive ablation and cross-task\ntransfer experiments. Our DepthSplat achieves state-of-the-art performance on\nScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and\nnovel view synthesis, demonstrating the mutual benefits of connecting both\ntasks. Our code, models, and video results are available at\nhttps://haofeixu.github.io/depthsplat/.\n", "link": "http://arxiv.org/abs/2410.13862v1", "date": "2024-10-17", "relevancy": 3.1714, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.664}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthSplat%3A%20Connecting%20Gaussian%20Splatting%20and%20Depth&body=Title%3A%20DepthSplat%3A%20Connecting%20Gaussian%20Splatting%20and%20Depth%0AAuthor%3A%20Haofei%20Xu%20and%20Songyou%20Peng%20and%20Fangjinhua%20Wang%20and%20Hermann%20Blum%20and%20Daniel%20Barath%20and%20Andreas%20Geiger%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Gaussian%20splatting%20and%20single/multi-view%20depth%20estimation%20are%20typically%0Astudied%20in%20isolation.%20In%20this%20paper%2C%20we%20present%20DepthSplat%20to%20connect%20Gaussian%0Asplatting%20and%20depth%20estimation%20and%20study%20their%20interactions.%20More%20specifically%2C%0Awe%20first%20contribute%20a%20robust%20multi-view%20depth%20model%20by%20leveraging%20pre-trained%0Amonocular%20depth%20features%2C%20leading%20to%20high-quality%20feed-forward%203D%20Gaussian%0Asplatting%20reconstructions.%20We%20also%20show%20that%20Gaussian%20splatting%20can%20serve%20as%20an%0Aunsupervised%20pre-training%20objective%20for%20learning%20powerful%20depth%20models%20from%0Alarge-scale%20unlabelled%20datasets.%20We%20validate%20the%20synergy%20between%20Gaussian%0Asplatting%20and%20depth%20estimation%20through%20extensive%20ablation%20and%20cross-task%0Atransfer%20experiments.%20Our%20DepthSplat%20achieves%20state-of-the-art%20performance%20on%0AScanNet%2C%20RealEstate10K%20and%20DL3DV%20datasets%20in%20terms%20of%20both%20depth%20estimation%20and%0Anovel%20view%20synthesis%2C%20demonstrating%20the%20mutual%20benefits%20of%20connecting%20both%0Atasks.%20Our%20code%2C%20models%2C%20and%20video%20results%20are%20available%20at%0Ahttps%3A//haofeixu.github.io/depthsplat/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthSplat%253A%2520Connecting%2520Gaussian%2520Splatting%2520and%2520Depth%26entry.906535625%3DHaofei%2520Xu%2520and%2520Songyou%2520Peng%2520and%2520Fangjinhua%2520Wang%2520and%2520Hermann%2520Blum%2520and%2520Daniel%2520Barath%2520and%2520Andreas%2520Geiger%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Gaussian%2520splatting%2520and%2520single/multi-view%2520depth%2520estimation%2520are%2520typically%250Astudied%2520in%2520isolation.%2520In%2520this%2520paper%252C%2520we%2520present%2520DepthSplat%2520to%2520connect%2520Gaussian%250Asplatting%2520and%2520depth%2520estimation%2520and%2520study%2520their%2520interactions.%2520More%2520specifically%252C%250Awe%2520first%2520contribute%2520a%2520robust%2520multi-view%2520depth%2520model%2520by%2520leveraging%2520pre-trained%250Amonocular%2520depth%2520features%252C%2520leading%2520to%2520high-quality%2520feed-forward%25203D%2520Gaussian%250Asplatting%2520reconstructions.%2520We%2520also%2520show%2520that%2520Gaussian%2520splatting%2520can%2520serve%2520as%2520an%250Aunsupervised%2520pre-training%2520objective%2520for%2520learning%2520powerful%2520depth%2520models%2520from%250Alarge-scale%2520unlabelled%2520datasets.%2520We%2520validate%2520the%2520synergy%2520between%2520Gaussian%250Asplatting%2520and%2520depth%2520estimation%2520through%2520extensive%2520ablation%2520and%2520cross-task%250Atransfer%2520experiments.%2520Our%2520DepthSplat%2520achieves%2520state-of-the-art%2520performance%2520on%250AScanNet%252C%2520RealEstate10K%2520and%2520DL3DV%2520datasets%2520in%2520terms%2520of%2520both%2520depth%2520estimation%2520and%250Anovel%2520view%2520synthesis%252C%2520demonstrating%2520the%2520mutual%2520benefits%2520of%2520connecting%2520both%250Atasks.%2520Our%2520code%252C%2520models%252C%2520and%2520video%2520results%2520are%2520available%2520at%250Ahttps%253A//haofeixu.github.io/depthsplat/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthSplat%3A%20Connecting%20Gaussian%20Splatting%20and%20Depth&entry.906535625=Haofei%20Xu%20and%20Songyou%20Peng%20and%20Fangjinhua%20Wang%20and%20Hermann%20Blum%20and%20Daniel%20Barath%20and%20Andreas%20Geiger%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Gaussian%20splatting%20and%20single/multi-view%20depth%20estimation%20are%20typically%0Astudied%20in%20isolation.%20In%20this%20paper%2C%20we%20present%20DepthSplat%20to%20connect%20Gaussian%0Asplatting%20and%20depth%20estimation%20and%20study%20their%20interactions.%20More%20specifically%2C%0Awe%20first%20contribute%20a%20robust%20multi-view%20depth%20model%20by%20leveraging%20pre-trained%0Amonocular%20depth%20features%2C%20leading%20to%20high-quality%20feed-forward%203D%20Gaussian%0Asplatting%20reconstructions.%20We%20also%20show%20that%20Gaussian%20splatting%20can%20serve%20as%20an%0Aunsupervised%20pre-training%20objective%20for%20learning%20powerful%20depth%20models%20from%0Alarge-scale%20unlabelled%20datasets.%20We%20validate%20the%20synergy%20between%20Gaussian%0Asplatting%20and%20depth%20estimation%20through%20extensive%20ablation%20and%20cross-task%0Atransfer%20experiments.%20Our%20DepthSplat%20achieves%20state-of-the-art%20performance%20on%0AScanNet%2C%20RealEstate10K%20and%20DL3DV%20datasets%20in%20terms%20of%20both%20depth%20estimation%20and%0Anovel%20view%20synthesis%2C%20demonstrating%20the%20mutual%20benefits%20of%20connecting%20both%0Atasks.%20Our%20code%2C%20models%2C%20and%20video%20results%20are%20available%20at%0Ahttps%3A//haofeixu.github.io/depthsplat/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13862v1&entry.124074799=Read"},
{"title": "Movie Gen: A Cast of Media Foundation Models", "author": "Adam Polyak and Amit Zohar and Andrew Brown and Andros Tjandra and Animesh Sinha and Ann Lee and Apoorv Vyas and Bowen Shi and Chih-Yao Ma and Ching-Yao Chuang and David Yan and Dhruv Choudhary and Dingkang Wang and Geet Sethi and Guan Pang and Haoyu Ma and Ishan Misra and Ji Hou and Jialiang Wang and Kiran Jagadeesh and Kunpeng Li and Luxin Zhang and Mannat Singh and Mary Williamson and Matt Le and Matthew Yu and Mitesh Kumar Singh and Peizhao Zhang and Peter Vajda and Quentin Duval and Rohit Girdhar and Roshan Sumbaly and Sai Saketh Rambhatla and Sam Tsai and Samaneh Azadi and Samyak Datta and Sanyuan Chen and Sean Bell and Sharadh Ramaswamy and Shelly Sheynin and Siddharth Bhattacharya and Simran Motwani and Tao Xu and Tianhe Li and Tingbo Hou and Wei-Ning Hsu and Xi Yin and Xiaoliang Dai and Yaniv Taigman and Yaqiao Luo and Yen-Cheng Liu and Yi-Chiao Wu and Yue Zhao and Yuval Kirstain and Zecheng He and Zijian He and Albert Pumarola and Ali Thabet and Artsiom Sanakoyeu and Arun Mallya and Baishan Guo and Boris Araya and Breena Kerr and Carleigh Wood and Ce Liu and Cen Peng and Dimitry Vengertsev and Edgar Schonfeld and Elliot Blanchard and Felix Juefei-Xu and Fraylie Nord and Jeff Liang and John Hoffman and Jonas Kohler and Kaolin Fire and Karthik Sivakumar and Lawrence Chen and Licheng Yu and Luya Gao and Markos Georgopoulos and Rashel Moritz and Sara K. Sampson and Shikai Li and Simone Parmeggiani and Steve Fine and Tara Fowler and Vladan Petrovic and Yuming Du", "abstract": "  We present Movie Gen, a cast of foundation models that generates\nhigh-quality, 1080p HD videos with different aspect ratios and synchronized\naudio. We also show additional capabilities such as precise instruction-based\nvideo editing and generation of personalized videos based on a user's image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video\nsynthesis, video personalization, video editing, video-to-audio generation, and\ntext-to-audio generation. Our largest video generation model is a 30B parameter\ntransformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We\nshow multiple technical innovations and simplifications on the architecture,\nlatent spaces, training objectives and recipes, data curation, evaluation\nprotocols, parallelization techniques, and inference optimizations that allow\nus to reap the benefits of scaling pre-training data, model size, and training\ncompute for training large scale media generation models. We hope this paper\nhelps the research community to accelerate progress and innovation in media\ngeneration models. All videos from this paper are available at\nhttps://go.fb.me/MovieGenResearchVideos.\n", "link": "http://arxiv.org/abs/2410.13720v1", "date": "2024-10-17", "relevancy": 3.1434, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6386}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6268}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Movie%20Gen%3A%20A%20Cast%20of%20Media%20Foundation%20Models&body=Title%3A%20Movie%20Gen%3A%20A%20Cast%20of%20Media%20Foundation%20Models%0AAuthor%3A%20Adam%20Polyak%20and%20Amit%20Zohar%20and%20Andrew%20Brown%20and%20Andros%20Tjandra%20and%20Animesh%20Sinha%20and%20Ann%20Lee%20and%20Apoorv%20Vyas%20and%20Bowen%20Shi%20and%20Chih-Yao%20Ma%20and%20Ching-Yao%20Chuang%20and%20David%20Yan%20and%20Dhruv%20Choudhary%20and%20Dingkang%20Wang%20and%20Geet%20Sethi%20and%20Guan%20Pang%20and%20Haoyu%20Ma%20and%20Ishan%20Misra%20and%20Ji%20Hou%20and%20Jialiang%20Wang%20and%20Kiran%20Jagadeesh%20and%20Kunpeng%20Li%20and%20Luxin%20Zhang%20and%20Mannat%20Singh%20and%20Mary%20Williamson%20and%20Matt%20Le%20and%20Matthew%20Yu%20and%20Mitesh%20Kumar%20Singh%20and%20Peizhao%20Zhang%20and%20Peter%20Vajda%20and%20Quentin%20Duval%20and%20Rohit%20Girdhar%20and%20Roshan%20Sumbaly%20and%20Sai%20Saketh%20Rambhatla%20and%20Sam%20Tsai%20and%20Samaneh%20Azadi%20and%20Samyak%20Datta%20and%20Sanyuan%20Chen%20and%20Sean%20Bell%20and%20Sharadh%20Ramaswamy%20and%20Shelly%20Sheynin%20and%20Siddharth%20Bhattacharya%20and%20Simran%20Motwani%20and%20Tao%20Xu%20and%20Tianhe%20Li%20and%20Tingbo%20Hou%20and%20Wei-Ning%20Hsu%20and%20Xi%20Yin%20and%20Xiaoliang%20Dai%20and%20Yaniv%20Taigman%20and%20Yaqiao%20Luo%20and%20Yen-Cheng%20Liu%20and%20Yi-Chiao%20Wu%20and%20Yue%20Zhao%20and%20Yuval%20Kirstain%20and%20Zecheng%20He%20and%20Zijian%20He%20and%20Albert%20Pumarola%20and%20Ali%20Thabet%20and%20Artsiom%20Sanakoyeu%20and%20Arun%20Mallya%20and%20Baishan%20Guo%20and%20Boris%20Araya%20and%20Breena%20Kerr%20and%20Carleigh%20Wood%20and%20Ce%20Liu%20and%20Cen%20Peng%20and%20Dimitry%20Vengertsev%20and%20Edgar%20Schonfeld%20and%20Elliot%20Blanchard%20and%20Felix%20Juefei-Xu%20and%20Fraylie%20Nord%20and%20Jeff%20Liang%20and%20John%20Hoffman%20and%20Jonas%20Kohler%20and%20Kaolin%20Fire%20and%20Karthik%20Sivakumar%20and%20Lawrence%20Chen%20and%20Licheng%20Yu%20and%20Luya%20Gao%20and%20Markos%20Georgopoulos%20and%20Rashel%20Moritz%20and%20Sara%20K.%20Sampson%20and%20Shikai%20Li%20and%20Simone%20Parmeggiani%20and%20Steve%20Fine%20and%20Tara%20Fowler%20and%20Vladan%20Petrovic%20and%20Yuming%20Du%0AAbstract%3A%20%20%20We%20present%20Movie%20Gen%2C%20a%20cast%20of%20foundation%20models%20that%20generates%0Ahigh-quality%2C%201080p%20HD%20videos%20with%20different%20aspect%20ratios%20and%20synchronized%0Aaudio.%20We%20also%20show%20additional%20capabilities%20such%20as%20precise%20instruction-based%0Avideo%20editing%20and%20generation%20of%20personalized%20videos%20based%20on%20a%20user%27s%20image.%0AOur%20models%20set%20a%20new%20state-of-the-art%20on%20multiple%20tasks%3A%20text-to-video%0Asynthesis%2C%20video%20personalization%2C%20video%20editing%2C%20video-to-audio%20generation%2C%20and%0Atext-to-audio%20generation.%20Our%20largest%20video%20generation%20model%20is%20a%2030B%20parameter%0Atransformer%20trained%20with%20a%20maximum%20context%20length%20of%2073K%20video%20tokens%2C%0Acorresponding%20to%20a%20generated%20video%20of%2016%20seconds%20at%2016%20frames-per-second.%20We%0Ashow%20multiple%20technical%20innovations%20and%20simplifications%20on%20the%20architecture%2C%0Alatent%20spaces%2C%20training%20objectives%20and%20recipes%2C%20data%20curation%2C%20evaluation%0Aprotocols%2C%20parallelization%20techniques%2C%20and%20inference%20optimizations%20that%20allow%0Aus%20to%20reap%20the%20benefits%20of%20scaling%20pre-training%20data%2C%20model%20size%2C%20and%20training%0Acompute%20for%20training%20large%20scale%20media%20generation%20models.%20We%20hope%20this%20paper%0Ahelps%20the%20research%20community%20to%20accelerate%20progress%20and%20innovation%20in%20media%0Ageneration%20models.%20All%20videos%20from%20this%20paper%20are%20available%20at%0Ahttps%3A//go.fb.me/MovieGenResearchVideos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovie%2520Gen%253A%2520A%2520Cast%2520of%2520Media%2520Foundation%2520Models%26entry.906535625%3DAdam%2520Polyak%2520and%2520Amit%2520Zohar%2520and%2520Andrew%2520Brown%2520and%2520Andros%2520Tjandra%2520and%2520Animesh%2520Sinha%2520and%2520Ann%2520Lee%2520and%2520Apoorv%2520Vyas%2520and%2520Bowen%2520Shi%2520and%2520Chih-Yao%2520Ma%2520and%2520Ching-Yao%2520Chuang%2520and%2520David%2520Yan%2520and%2520Dhruv%2520Choudhary%2520and%2520Dingkang%2520Wang%2520and%2520Geet%2520Sethi%2520and%2520Guan%2520Pang%2520and%2520Haoyu%2520Ma%2520and%2520Ishan%2520Misra%2520and%2520Ji%2520Hou%2520and%2520Jialiang%2520Wang%2520and%2520Kiran%2520Jagadeesh%2520and%2520Kunpeng%2520Li%2520and%2520Luxin%2520Zhang%2520and%2520Mannat%2520Singh%2520and%2520Mary%2520Williamson%2520and%2520Matt%2520Le%2520and%2520Matthew%2520Yu%2520and%2520Mitesh%2520Kumar%2520Singh%2520and%2520Peizhao%2520Zhang%2520and%2520Peter%2520Vajda%2520and%2520Quentin%2520Duval%2520and%2520Rohit%2520Girdhar%2520and%2520Roshan%2520Sumbaly%2520and%2520Sai%2520Saketh%2520Rambhatla%2520and%2520Sam%2520Tsai%2520and%2520Samaneh%2520Azadi%2520and%2520Samyak%2520Datta%2520and%2520Sanyuan%2520Chen%2520and%2520Sean%2520Bell%2520and%2520Sharadh%2520Ramaswamy%2520and%2520Shelly%2520Sheynin%2520and%2520Siddharth%2520Bhattacharya%2520and%2520Simran%2520Motwani%2520and%2520Tao%2520Xu%2520and%2520Tianhe%2520Li%2520and%2520Tingbo%2520Hou%2520and%2520Wei-Ning%2520Hsu%2520and%2520Xi%2520Yin%2520and%2520Xiaoliang%2520Dai%2520and%2520Yaniv%2520Taigman%2520and%2520Yaqiao%2520Luo%2520and%2520Yen-Cheng%2520Liu%2520and%2520Yi-Chiao%2520Wu%2520and%2520Yue%2520Zhao%2520and%2520Yuval%2520Kirstain%2520and%2520Zecheng%2520He%2520and%2520Zijian%2520He%2520and%2520Albert%2520Pumarola%2520and%2520Ali%2520Thabet%2520and%2520Artsiom%2520Sanakoyeu%2520and%2520Arun%2520Mallya%2520and%2520Baishan%2520Guo%2520and%2520Boris%2520Araya%2520and%2520Breena%2520Kerr%2520and%2520Carleigh%2520Wood%2520and%2520Ce%2520Liu%2520and%2520Cen%2520Peng%2520and%2520Dimitry%2520Vengertsev%2520and%2520Edgar%2520Schonfeld%2520and%2520Elliot%2520Blanchard%2520and%2520Felix%2520Juefei-Xu%2520and%2520Fraylie%2520Nord%2520and%2520Jeff%2520Liang%2520and%2520John%2520Hoffman%2520and%2520Jonas%2520Kohler%2520and%2520Kaolin%2520Fire%2520and%2520Karthik%2520Sivakumar%2520and%2520Lawrence%2520Chen%2520and%2520Licheng%2520Yu%2520and%2520Luya%2520Gao%2520and%2520Markos%2520Georgopoulos%2520and%2520Rashel%2520Moritz%2520and%2520Sara%2520K.%2520Sampson%2520and%2520Shikai%2520Li%2520and%2520Simone%2520Parmeggiani%2520and%2520Steve%2520Fine%2520and%2520Tara%2520Fowler%2520and%2520Vladan%2520Petrovic%2520and%2520Yuming%2520Du%26entry.1292438233%3D%2520%2520We%2520present%2520Movie%2520Gen%252C%2520a%2520cast%2520of%2520foundation%2520models%2520that%2520generates%250Ahigh-quality%252C%25201080p%2520HD%2520videos%2520with%2520different%2520aspect%2520ratios%2520and%2520synchronized%250Aaudio.%2520We%2520also%2520show%2520additional%2520capabilities%2520such%2520as%2520precise%2520instruction-based%250Avideo%2520editing%2520and%2520generation%2520of%2520personalized%2520videos%2520based%2520on%2520a%2520user%2527s%2520image.%250AOur%2520models%2520set%2520a%2520new%2520state-of-the-art%2520on%2520multiple%2520tasks%253A%2520text-to-video%250Asynthesis%252C%2520video%2520personalization%252C%2520video%2520editing%252C%2520video-to-audio%2520generation%252C%2520and%250Atext-to-audio%2520generation.%2520Our%2520largest%2520video%2520generation%2520model%2520is%2520a%252030B%2520parameter%250Atransformer%2520trained%2520with%2520a%2520maximum%2520context%2520length%2520of%252073K%2520video%2520tokens%252C%250Acorresponding%2520to%2520a%2520generated%2520video%2520of%252016%2520seconds%2520at%252016%2520frames-per-second.%2520We%250Ashow%2520multiple%2520technical%2520innovations%2520and%2520simplifications%2520on%2520the%2520architecture%252C%250Alatent%2520spaces%252C%2520training%2520objectives%2520and%2520recipes%252C%2520data%2520curation%252C%2520evaluation%250Aprotocols%252C%2520parallelization%2520techniques%252C%2520and%2520inference%2520optimizations%2520that%2520allow%250Aus%2520to%2520reap%2520the%2520benefits%2520of%2520scaling%2520pre-training%2520data%252C%2520model%2520size%252C%2520and%2520training%250Acompute%2520for%2520training%2520large%2520scale%2520media%2520generation%2520models.%2520We%2520hope%2520this%2520paper%250Ahelps%2520the%2520research%2520community%2520to%2520accelerate%2520progress%2520and%2520innovation%2520in%2520media%250Ageneration%2520models.%2520All%2520videos%2520from%2520this%2520paper%2520are%2520available%2520at%250Ahttps%253A//go.fb.me/MovieGenResearchVideos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Movie%20Gen%3A%20A%20Cast%20of%20Media%20Foundation%20Models&entry.906535625=Adam%20Polyak%20and%20Amit%20Zohar%20and%20Andrew%20Brown%20and%20Andros%20Tjandra%20and%20Animesh%20Sinha%20and%20Ann%20Lee%20and%20Apoorv%20Vyas%20and%20Bowen%20Shi%20and%20Chih-Yao%20Ma%20and%20Ching-Yao%20Chuang%20and%20David%20Yan%20and%20Dhruv%20Choudhary%20and%20Dingkang%20Wang%20and%20Geet%20Sethi%20and%20Guan%20Pang%20and%20Haoyu%20Ma%20and%20Ishan%20Misra%20and%20Ji%20Hou%20and%20Jialiang%20Wang%20and%20Kiran%20Jagadeesh%20and%20Kunpeng%20Li%20and%20Luxin%20Zhang%20and%20Mannat%20Singh%20and%20Mary%20Williamson%20and%20Matt%20Le%20and%20Matthew%20Yu%20and%20Mitesh%20Kumar%20Singh%20and%20Peizhao%20Zhang%20and%20Peter%20Vajda%20and%20Quentin%20Duval%20and%20Rohit%20Girdhar%20and%20Roshan%20Sumbaly%20and%20Sai%20Saketh%20Rambhatla%20and%20Sam%20Tsai%20and%20Samaneh%20Azadi%20and%20Samyak%20Datta%20and%20Sanyuan%20Chen%20and%20Sean%20Bell%20and%20Sharadh%20Ramaswamy%20and%20Shelly%20Sheynin%20and%20Siddharth%20Bhattacharya%20and%20Simran%20Motwani%20and%20Tao%20Xu%20and%20Tianhe%20Li%20and%20Tingbo%20Hou%20and%20Wei-Ning%20Hsu%20and%20Xi%20Yin%20and%20Xiaoliang%20Dai%20and%20Yaniv%20Taigman%20and%20Yaqiao%20Luo%20and%20Yen-Cheng%20Liu%20and%20Yi-Chiao%20Wu%20and%20Yue%20Zhao%20and%20Yuval%20Kirstain%20and%20Zecheng%20He%20and%20Zijian%20He%20and%20Albert%20Pumarola%20and%20Ali%20Thabet%20and%20Artsiom%20Sanakoyeu%20and%20Arun%20Mallya%20and%20Baishan%20Guo%20and%20Boris%20Araya%20and%20Breena%20Kerr%20and%20Carleigh%20Wood%20and%20Ce%20Liu%20and%20Cen%20Peng%20and%20Dimitry%20Vengertsev%20and%20Edgar%20Schonfeld%20and%20Elliot%20Blanchard%20and%20Felix%20Juefei-Xu%20and%20Fraylie%20Nord%20and%20Jeff%20Liang%20and%20John%20Hoffman%20and%20Jonas%20Kohler%20and%20Kaolin%20Fire%20and%20Karthik%20Sivakumar%20and%20Lawrence%20Chen%20and%20Licheng%20Yu%20and%20Luya%20Gao%20and%20Markos%20Georgopoulos%20and%20Rashel%20Moritz%20and%20Sara%20K.%20Sampson%20and%20Shikai%20Li%20and%20Simone%20Parmeggiani%20and%20Steve%20Fine%20and%20Tara%20Fowler%20and%20Vladan%20Petrovic%20and%20Yuming%20Du&entry.1292438233=%20%20We%20present%20Movie%20Gen%2C%20a%20cast%20of%20foundation%20models%20that%20generates%0Ahigh-quality%2C%201080p%20HD%20videos%20with%20different%20aspect%20ratios%20and%20synchronized%0Aaudio.%20We%20also%20show%20additional%20capabilities%20such%20as%20precise%20instruction-based%0Avideo%20editing%20and%20generation%20of%20personalized%20videos%20based%20on%20a%20user%27s%20image.%0AOur%20models%20set%20a%20new%20state-of-the-art%20on%20multiple%20tasks%3A%20text-to-video%0Asynthesis%2C%20video%20personalization%2C%20video%20editing%2C%20video-to-audio%20generation%2C%20and%0Atext-to-audio%20generation.%20Our%20largest%20video%20generation%20model%20is%20a%2030B%20parameter%0Atransformer%20trained%20with%20a%20maximum%20context%20length%20of%2073K%20video%20tokens%2C%0Acorresponding%20to%20a%20generated%20video%20of%2016%20seconds%20at%2016%20frames-per-second.%20We%0Ashow%20multiple%20technical%20innovations%20and%20simplifications%20on%20the%20architecture%2C%0Alatent%20spaces%2C%20training%20objectives%20and%20recipes%2C%20data%20curation%2C%20evaluation%0Aprotocols%2C%20parallelization%20techniques%2C%20and%20inference%20optimizations%20that%20allow%0Aus%20to%20reap%20the%20benefits%20of%20scaling%20pre-training%20data%2C%20model%20size%2C%20and%20training%0Acompute%20for%20training%20large%20scale%20media%20generation%20models.%20We%20hope%20this%20paper%0Ahelps%20the%20research%20community%20to%20accelerate%20progress%20and%20innovation%20in%20media%0Ageneration%20models.%20All%20videos%20from%20this%20paper%20are%20available%20at%0Ahttps%3A//go.fb.me/MovieGenResearchVideos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13720v1&entry.124074799=Read"},
{"title": "Improving Multi-modal Large Language Model through Boosting Vision\n  Capabilities", "author": "Yanpeng Sun and Huaxin Zhang and Qiang Chen and Xinyu Zhang and Nong Sang and Gang Zhang and Jingdong Wang and Zechao Li", "abstract": "  We focus on improving the visual understanding capability for boosting the\nvision-language models. We propose \\textbf{Arcana}, a multiModal language\nmodel, which introduces two crucial techniques. First, we present Multimodal\nLoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional\nlanguage-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for\nvision and one for language -- each with its own parameters. This disentangled\nparameters design allows for more specialized learning in each modality and\nbetter integration of multimodal information. Second, we introduce the Query\nLadder adapter (QLadder) to improve the visual encoder. QLadder employs a\nlearnable ``\\textit{ladder}'' structure to deeply aggregates the intermediate\nrepresentations from the frozen pretrained visual encoder (e.g., CLIP image\nencoder). This enables the model to learn new and informative visual features,\nas well as remaining the powerful capabilities of the pretrained visual\nencoder. These techniques collectively enhance Arcana's visual perception\npower, enabling it to leverage improved visual information for more accurate\nand contextually relevant outputs across various multimodal scenarios.\nExtensive experiments and ablation studies demonstrate the effectiveness and\ngeneralization capability of our Arcana. The code and re-annotated data are\navailable at \\url{https://arcana-project-page.github.io}.\n", "link": "http://arxiv.org/abs/2410.13733v1", "date": "2024-10-17", "relevancy": 3.1208, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Multi-modal%20Large%20Language%20Model%20through%20Boosting%20Vision%0A%20%20Capabilities&body=Title%3A%20Improving%20Multi-modal%20Large%20Language%20Model%20through%20Boosting%20Vision%0A%20%20Capabilities%0AAuthor%3A%20Yanpeng%20Sun%20and%20Huaxin%20Zhang%20and%20Qiang%20Chen%20and%20Xinyu%20Zhang%20and%20Nong%20Sang%20and%20Gang%20Zhang%20and%20Jingdong%20Wang%20and%20Zechao%20Li%0AAbstract%3A%20%20%20We%20focus%20on%20improving%20the%20visual%20understanding%20capability%20for%20boosting%20the%0Avision-language%20models.%20We%20propose%20%5Ctextbf%7BArcana%7D%2C%20a%20multiModal%20language%0Amodel%2C%20which%20introduces%20two%20crucial%20techniques.%20First%2C%20we%20present%20Multimodal%0ALoRA%20%28MM-LoRA%29%2C%20a%20module%20designed%20to%20enhance%20the%20decoder.%20Unlike%20traditional%0Alanguage-driven%20decoders%2C%20MM-LoRA%20consists%20of%20two%20parallel%20LoRAs%20--%20one%20for%0Avision%20and%20one%20for%20language%20--%20each%20with%20its%20own%20parameters.%20This%20disentangled%0Aparameters%20design%20allows%20for%20more%20specialized%20learning%20in%20each%20modality%20and%0Abetter%20integration%20of%20multimodal%20information.%20Second%2C%20we%20introduce%20the%20Query%0ALadder%20adapter%20%28QLadder%29%20to%20improve%20the%20visual%20encoder.%20QLadder%20employs%20a%0Alearnable%20%60%60%5Ctextit%7Bladder%7D%27%27%20structure%20to%20deeply%20aggregates%20the%20intermediate%0Arepresentations%20from%20the%20frozen%20pretrained%20visual%20encoder%20%28e.g.%2C%20CLIP%20image%0Aencoder%29.%20This%20enables%20the%20model%20to%20learn%20new%20and%20informative%20visual%20features%2C%0Aas%20well%20as%20remaining%20the%20powerful%20capabilities%20of%20the%20pretrained%20visual%0Aencoder.%20These%20techniques%20collectively%20enhance%20Arcana%27s%20visual%20perception%0Apower%2C%20enabling%20it%20to%20leverage%20improved%20visual%20information%20for%20more%20accurate%0Aand%20contextually%20relevant%20outputs%20across%20various%20multimodal%20scenarios.%0AExtensive%20experiments%20and%20ablation%20studies%20demonstrate%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20our%20Arcana.%20The%20code%20and%20re-annotated%20data%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//arcana-project-page.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Multi-modal%2520Large%2520Language%2520Model%2520through%2520Boosting%2520Vision%250A%2520%2520Capabilities%26entry.906535625%3DYanpeng%2520Sun%2520and%2520Huaxin%2520Zhang%2520and%2520Qiang%2520Chen%2520and%2520Xinyu%2520Zhang%2520and%2520Nong%2520Sang%2520and%2520Gang%2520Zhang%2520and%2520Jingdong%2520Wang%2520and%2520Zechao%2520Li%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520improving%2520the%2520visual%2520understanding%2520capability%2520for%2520boosting%2520the%250Avision-language%2520models.%2520We%2520propose%2520%255Ctextbf%257BArcana%257D%252C%2520a%2520multiModal%2520language%250Amodel%252C%2520which%2520introduces%2520two%2520crucial%2520techniques.%2520First%252C%2520we%2520present%2520Multimodal%250ALoRA%2520%2528MM-LoRA%2529%252C%2520a%2520module%2520designed%2520to%2520enhance%2520the%2520decoder.%2520Unlike%2520traditional%250Alanguage-driven%2520decoders%252C%2520MM-LoRA%2520consists%2520of%2520two%2520parallel%2520LoRAs%2520--%2520one%2520for%250Avision%2520and%2520one%2520for%2520language%2520--%2520each%2520with%2520its%2520own%2520parameters.%2520This%2520disentangled%250Aparameters%2520design%2520allows%2520for%2520more%2520specialized%2520learning%2520in%2520each%2520modality%2520and%250Abetter%2520integration%2520of%2520multimodal%2520information.%2520Second%252C%2520we%2520introduce%2520the%2520Query%250ALadder%2520adapter%2520%2528QLadder%2529%2520to%2520improve%2520the%2520visual%2520encoder.%2520QLadder%2520employs%2520a%250Alearnable%2520%2560%2560%255Ctextit%257Bladder%257D%2527%2527%2520structure%2520to%2520deeply%2520aggregates%2520the%2520intermediate%250Arepresentations%2520from%2520the%2520frozen%2520pretrained%2520visual%2520encoder%2520%2528e.g.%252C%2520CLIP%2520image%250Aencoder%2529.%2520This%2520enables%2520the%2520model%2520to%2520learn%2520new%2520and%2520informative%2520visual%2520features%252C%250Aas%2520well%2520as%2520remaining%2520the%2520powerful%2520capabilities%2520of%2520the%2520pretrained%2520visual%250Aencoder.%2520These%2520techniques%2520collectively%2520enhance%2520Arcana%2527s%2520visual%2520perception%250Apower%252C%2520enabling%2520it%2520to%2520leverage%2520improved%2520visual%2520information%2520for%2520more%2520accurate%250Aand%2520contextually%2520relevant%2520outputs%2520across%2520various%2520multimodal%2520scenarios.%250AExtensive%2520experiments%2520and%2520ablation%2520studies%2520demonstrate%2520the%2520effectiveness%2520and%250Ageneralization%2520capability%2520of%2520our%2520Arcana.%2520The%2520code%2520and%2520re-annotated%2520data%2520are%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//arcana-project-page.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Multi-modal%20Large%20Language%20Model%20through%20Boosting%20Vision%0A%20%20Capabilities&entry.906535625=Yanpeng%20Sun%20and%20Huaxin%20Zhang%20and%20Qiang%20Chen%20and%20Xinyu%20Zhang%20and%20Nong%20Sang%20and%20Gang%20Zhang%20and%20Jingdong%20Wang%20and%20Zechao%20Li&entry.1292438233=%20%20We%20focus%20on%20improving%20the%20visual%20understanding%20capability%20for%20boosting%20the%0Avision-language%20models.%20We%20propose%20%5Ctextbf%7BArcana%7D%2C%20a%20multiModal%20language%0Amodel%2C%20which%20introduces%20two%20crucial%20techniques.%20First%2C%20we%20present%20Multimodal%0ALoRA%20%28MM-LoRA%29%2C%20a%20module%20designed%20to%20enhance%20the%20decoder.%20Unlike%20traditional%0Alanguage-driven%20decoders%2C%20MM-LoRA%20consists%20of%20two%20parallel%20LoRAs%20--%20one%20for%0Avision%20and%20one%20for%20language%20--%20each%20with%20its%20own%20parameters.%20This%20disentangled%0Aparameters%20design%20allows%20for%20more%20specialized%20learning%20in%20each%20modality%20and%0Abetter%20integration%20of%20multimodal%20information.%20Second%2C%20we%20introduce%20the%20Query%0ALadder%20adapter%20%28QLadder%29%20to%20improve%20the%20visual%20encoder.%20QLadder%20employs%20a%0Alearnable%20%60%60%5Ctextit%7Bladder%7D%27%27%20structure%20to%20deeply%20aggregates%20the%20intermediate%0Arepresentations%20from%20the%20frozen%20pretrained%20visual%20encoder%20%28e.g.%2C%20CLIP%20image%0Aencoder%29.%20This%20enables%20the%20model%20to%20learn%20new%20and%20informative%20visual%20features%2C%0Aas%20well%20as%20remaining%20the%20powerful%20capabilities%20of%20the%20pretrained%20visual%0Aencoder.%20These%20techniques%20collectively%20enhance%20Arcana%27s%20visual%20perception%0Apower%2C%20enabling%20it%20to%20leverage%20improved%20visual%20information%20for%20more%20accurate%0Aand%20contextually%20relevant%20outputs%20across%20various%20multimodal%20scenarios.%0AExtensive%20experiments%20and%20ablation%20studies%20demonstrate%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20our%20Arcana.%20The%20code%20and%20re-annotated%20data%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//arcana-project-page.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13733v1&entry.124074799=Read"},
{"title": "UniDrive: Towards Universal Driving Perception Across Camera\n  Configurations", "author": "Ye Li and Wenzhao Zheng and Xiaonan Huang and Kurt Keutzer", "abstract": "  Vision-centric autonomous driving has demonstrated excellent performance with\neconomical sensors. As the fundamental step, 3D perception aims to infer 3D\ninformation from 2D images based on 3D-2D projection. This makes driving\nperception models susceptible to sensor configuration (e.g., camera intrinsics\nand extrinsics) variations. However, generalizing across camera configurations\nis important for deploying autonomous driving models on different car models.\nIn this paper, we present UniDrive, a novel framework for vision-centric\nautonomous driving to achieve universal perception across camera\nconfigurations. We deploy a set of unified virtual cameras and propose a\nground-aware projection method to effectively transform the original images\ninto these unified virtual views. We further propose a virtual configuration\noptimization method by minimizing the expected projection error between\noriginal cameras and virtual cameras. The proposed virtual camera projection\ncan be applied to existing 3D perception methods as a plug-and-play module to\nmitigate the challenges posed by camera parameter variability, resulting in\nmore adaptable and reliable driving perception models. To evaluate the\neffectiveness of our framework, we collect a dataset on Carla by driving the\nsame routes while only modifying the camera configurations. Experimental\nresults demonstrate that our method trained on one specific camera\nconfiguration can generalize to varying configurations with minor performance\ndegradation.\n", "link": "http://arxiv.org/abs/2410.13864v1", "date": "2024-10-17", "relevancy": 3.0555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniDrive%3A%20Towards%20Universal%20Driving%20Perception%20Across%20Camera%0A%20%20Configurations&body=Title%3A%20UniDrive%3A%20Towards%20Universal%20Driving%20Perception%20Across%20Camera%0A%20%20Configurations%0AAuthor%3A%20Ye%20Li%20and%20Wenzhao%20Zheng%20and%20Xiaonan%20Huang%20and%20Kurt%20Keutzer%0AAbstract%3A%20%20%20Vision-centric%20autonomous%20driving%20has%20demonstrated%20excellent%20performance%20with%0Aeconomical%20sensors.%20As%20the%20fundamental%20step%2C%203D%20perception%20aims%20to%20infer%203D%0Ainformation%20from%202D%20images%20based%20on%203D-2D%20projection.%20This%20makes%20driving%0Aperception%20models%20susceptible%20to%20sensor%20configuration%20%28e.g.%2C%20camera%20intrinsics%0Aand%20extrinsics%29%20variations.%20However%2C%20generalizing%20across%20camera%20configurations%0Ais%20important%20for%20deploying%20autonomous%20driving%20models%20on%20different%20car%20models.%0AIn%20this%20paper%2C%20we%20present%20UniDrive%2C%20a%20novel%20framework%20for%20vision-centric%0Aautonomous%20driving%20to%20achieve%20universal%20perception%20across%20camera%0Aconfigurations.%20We%20deploy%20a%20set%20of%20unified%20virtual%20cameras%20and%20propose%20a%0Aground-aware%20projection%20method%20to%20effectively%20transform%20the%20original%20images%0Ainto%20these%20unified%20virtual%20views.%20We%20further%20propose%20a%20virtual%20configuration%0Aoptimization%20method%20by%20minimizing%20the%20expected%20projection%20error%20between%0Aoriginal%20cameras%20and%20virtual%20cameras.%20The%20proposed%20virtual%20camera%20projection%0Acan%20be%20applied%20to%20existing%203D%20perception%20methods%20as%20a%20plug-and-play%20module%20to%0Amitigate%20the%20challenges%20posed%20by%20camera%20parameter%20variability%2C%20resulting%20in%0Amore%20adaptable%20and%20reliable%20driving%20perception%20models.%20To%20evaluate%20the%0Aeffectiveness%20of%20our%20framework%2C%20we%20collect%20a%20dataset%20on%20Carla%20by%20driving%20the%0Asame%20routes%20while%20only%20modifying%20the%20camera%20configurations.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20trained%20on%20one%20specific%20camera%0Aconfiguration%20can%20generalize%20to%20varying%20configurations%20with%20minor%20performance%0Adegradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniDrive%253A%2520Towards%2520Universal%2520Driving%2520Perception%2520Across%2520Camera%250A%2520%2520Configurations%26entry.906535625%3DYe%2520Li%2520and%2520Wenzhao%2520Zheng%2520and%2520Xiaonan%2520Huang%2520and%2520Kurt%2520Keutzer%26entry.1292438233%3D%2520%2520Vision-centric%2520autonomous%2520driving%2520has%2520demonstrated%2520excellent%2520performance%2520with%250Aeconomical%2520sensors.%2520As%2520the%2520fundamental%2520step%252C%25203D%2520perception%2520aims%2520to%2520infer%25203D%250Ainformation%2520from%25202D%2520images%2520based%2520on%25203D-2D%2520projection.%2520This%2520makes%2520driving%250Aperception%2520models%2520susceptible%2520to%2520sensor%2520configuration%2520%2528e.g.%252C%2520camera%2520intrinsics%250Aand%2520extrinsics%2529%2520variations.%2520However%252C%2520generalizing%2520across%2520camera%2520configurations%250Ais%2520important%2520for%2520deploying%2520autonomous%2520driving%2520models%2520on%2520different%2520car%2520models.%250AIn%2520this%2520paper%252C%2520we%2520present%2520UniDrive%252C%2520a%2520novel%2520framework%2520for%2520vision-centric%250Aautonomous%2520driving%2520to%2520achieve%2520universal%2520perception%2520across%2520camera%250Aconfigurations.%2520We%2520deploy%2520a%2520set%2520of%2520unified%2520virtual%2520cameras%2520and%2520propose%2520a%250Aground-aware%2520projection%2520method%2520to%2520effectively%2520transform%2520the%2520original%2520images%250Ainto%2520these%2520unified%2520virtual%2520views.%2520We%2520further%2520propose%2520a%2520virtual%2520configuration%250Aoptimization%2520method%2520by%2520minimizing%2520the%2520expected%2520projection%2520error%2520between%250Aoriginal%2520cameras%2520and%2520virtual%2520cameras.%2520The%2520proposed%2520virtual%2520camera%2520projection%250Acan%2520be%2520applied%2520to%2520existing%25203D%2520perception%2520methods%2520as%2520a%2520plug-and-play%2520module%2520to%250Amitigate%2520the%2520challenges%2520posed%2520by%2520camera%2520parameter%2520variability%252C%2520resulting%2520in%250Amore%2520adaptable%2520and%2520reliable%2520driving%2520perception%2520models.%2520To%2520evaluate%2520the%250Aeffectiveness%2520of%2520our%2520framework%252C%2520we%2520collect%2520a%2520dataset%2520on%2520Carla%2520by%2520driving%2520the%250Asame%2520routes%2520while%2520only%2520modifying%2520the%2520camera%2520configurations.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520trained%2520on%2520one%2520specific%2520camera%250Aconfiguration%2520can%2520generalize%2520to%2520varying%2520configurations%2520with%2520minor%2520performance%250Adegradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniDrive%3A%20Towards%20Universal%20Driving%20Perception%20Across%20Camera%0A%20%20Configurations&entry.906535625=Ye%20Li%20and%20Wenzhao%20Zheng%20and%20Xiaonan%20Huang%20and%20Kurt%20Keutzer&entry.1292438233=%20%20Vision-centric%20autonomous%20driving%20has%20demonstrated%20excellent%20performance%20with%0Aeconomical%20sensors.%20As%20the%20fundamental%20step%2C%203D%20perception%20aims%20to%20infer%203D%0Ainformation%20from%202D%20images%20based%20on%203D-2D%20projection.%20This%20makes%20driving%0Aperception%20models%20susceptible%20to%20sensor%20configuration%20%28e.g.%2C%20camera%20intrinsics%0Aand%20extrinsics%29%20variations.%20However%2C%20generalizing%20across%20camera%20configurations%0Ais%20important%20for%20deploying%20autonomous%20driving%20models%20on%20different%20car%20models.%0AIn%20this%20paper%2C%20we%20present%20UniDrive%2C%20a%20novel%20framework%20for%20vision-centric%0Aautonomous%20driving%20to%20achieve%20universal%20perception%20across%20camera%0Aconfigurations.%20We%20deploy%20a%20set%20of%20unified%20virtual%20cameras%20and%20propose%20a%0Aground-aware%20projection%20method%20to%20effectively%20transform%20the%20original%20images%0Ainto%20these%20unified%20virtual%20views.%20We%20further%20propose%20a%20virtual%20configuration%0Aoptimization%20method%20by%20minimizing%20the%20expected%20projection%20error%20between%0Aoriginal%20cameras%20and%20virtual%20cameras.%20The%20proposed%20virtual%20camera%20projection%0Acan%20be%20applied%20to%20existing%203D%20perception%20methods%20as%20a%20plug-and-play%20module%20to%0Amitigate%20the%20challenges%20posed%20by%20camera%20parameter%20variability%2C%20resulting%20in%0Amore%20adaptable%20and%20reliable%20driving%20perception%20models.%20To%20evaluate%20the%0Aeffectiveness%20of%20our%20framework%2C%20we%20collect%20a%20dataset%20on%20Carla%20by%20driving%20the%0Asame%20routes%20while%20only%20modifying%20the%20camera%20configurations.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20trained%20on%20one%20specific%20camera%0Aconfiguration%20can%20generalize%20to%20varying%20configurations%20with%20minor%20performance%0Adegradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13864v1&entry.124074799=Read"},
{"title": "Help Me Identify: Is an LLM+VQA System All We Need to Identify Visual\n  Concepts?", "author": "Shailaja Keyur Sampat and Maitreya Patel and Yezhou Yang and Chitta Baral", "abstract": "  An ability to learn about new objects from a small amount of visual data and\nproduce convincing linguistic justification about the presence/absence of\ncertain concepts (that collectively compose the object) in novel scenarios is\nan important characteristic of human cognition. This is possible due to\nabstraction of attributes/properties that an object is composed of e.g. an\nobject `bird' can be identified by the presence of a beak, feathers, legs,\nwings, etc. Inspired by this aspect of human reasoning, in this work, we\npresent a zero-shot framework for fine-grained visual concept learning by\nleveraging large language model and Visual Question Answering (VQA) system.\nSpecifically, we prompt GPT-3 to obtain a rich linguistic description of visual\nobjects in the dataset. We convert the obtained concept descriptions into a set\nof binary questions. We pose these questions along with the query image to a\nVQA system and aggregate the answers to determine the presence or absence of an\nobject in the test images. Our experiments demonstrate comparable performance\nwith existing zero-shot visual classification methods and few-shot concept\nlearning approaches, without substantial computational overhead, yet being\nfully explainable from the reasoning perspective.\n", "link": "http://arxiv.org/abs/2410.13651v1", "date": "2024-10-17", "relevancy": 2.9383, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Help%20Me%20Identify%3A%20Is%20an%20LLM%2BVQA%20System%20All%20We%20Need%20to%20Identify%20Visual%0A%20%20Concepts%3F&body=Title%3A%20Help%20Me%20Identify%3A%20Is%20an%20LLM%2BVQA%20System%20All%20We%20Need%20to%20Identify%20Visual%0A%20%20Concepts%3F%0AAuthor%3A%20Shailaja%20Keyur%20Sampat%20and%20Maitreya%20Patel%20and%20Yezhou%20Yang%20and%20Chitta%20Baral%0AAbstract%3A%20%20%20An%20ability%20to%20learn%20about%20new%20objects%20from%20a%20small%20amount%20of%20visual%20data%20and%0Aproduce%20convincing%20linguistic%20justification%20about%20the%20presence/absence%20of%0Acertain%20concepts%20%28that%20collectively%20compose%20the%20object%29%20in%20novel%20scenarios%20is%0Aan%20important%20characteristic%20of%20human%20cognition.%20This%20is%20possible%20due%20to%0Aabstraction%20of%20attributes/properties%20that%20an%20object%20is%20composed%20of%20e.g.%20an%0Aobject%20%60bird%27%20can%20be%20identified%20by%20the%20presence%20of%20a%20beak%2C%20feathers%2C%20legs%2C%0Awings%2C%20etc.%20Inspired%20by%20this%20aspect%20of%20human%20reasoning%2C%20in%20this%20work%2C%20we%0Apresent%20a%20zero-shot%20framework%20for%20fine-grained%20visual%20concept%20learning%20by%0Aleveraging%20large%20language%20model%20and%20Visual%20Question%20Answering%20%28VQA%29%20system.%0ASpecifically%2C%20we%20prompt%20GPT-3%20to%20obtain%20a%20rich%20linguistic%20description%20of%20visual%0Aobjects%20in%20the%20dataset.%20We%20convert%20the%20obtained%20concept%20descriptions%20into%20a%20set%0Aof%20binary%20questions.%20We%20pose%20these%20questions%20along%20with%20the%20query%20image%20to%20a%0AVQA%20system%20and%20aggregate%20the%20answers%20to%20determine%20the%20presence%20or%20absence%20of%20an%0Aobject%20in%20the%20test%20images.%20Our%20experiments%20demonstrate%20comparable%20performance%0Awith%20existing%20zero-shot%20visual%20classification%20methods%20and%20few-shot%20concept%0Alearning%20approaches%2C%20without%20substantial%20computational%20overhead%2C%20yet%20being%0Afully%20explainable%20from%20the%20reasoning%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelp%2520Me%2520Identify%253A%2520Is%2520an%2520LLM%252BVQA%2520System%2520All%2520We%2520Need%2520to%2520Identify%2520Visual%250A%2520%2520Concepts%253F%26entry.906535625%3DShailaja%2520Keyur%2520Sampat%2520and%2520Maitreya%2520Patel%2520and%2520Yezhou%2520Yang%2520and%2520Chitta%2520Baral%26entry.1292438233%3D%2520%2520An%2520ability%2520to%2520learn%2520about%2520new%2520objects%2520from%2520a%2520small%2520amount%2520of%2520visual%2520data%2520and%250Aproduce%2520convincing%2520linguistic%2520justification%2520about%2520the%2520presence/absence%2520of%250Acertain%2520concepts%2520%2528that%2520collectively%2520compose%2520the%2520object%2529%2520in%2520novel%2520scenarios%2520is%250Aan%2520important%2520characteristic%2520of%2520human%2520cognition.%2520This%2520is%2520possible%2520due%2520to%250Aabstraction%2520of%2520attributes/properties%2520that%2520an%2520object%2520is%2520composed%2520of%2520e.g.%2520an%250Aobject%2520%2560bird%2527%2520can%2520be%2520identified%2520by%2520the%2520presence%2520of%2520a%2520beak%252C%2520feathers%252C%2520legs%252C%250Awings%252C%2520etc.%2520Inspired%2520by%2520this%2520aspect%2520of%2520human%2520reasoning%252C%2520in%2520this%2520work%252C%2520we%250Apresent%2520a%2520zero-shot%2520framework%2520for%2520fine-grained%2520visual%2520concept%2520learning%2520by%250Aleveraging%2520large%2520language%2520model%2520and%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520system.%250ASpecifically%252C%2520we%2520prompt%2520GPT-3%2520to%2520obtain%2520a%2520rich%2520linguistic%2520description%2520of%2520visual%250Aobjects%2520in%2520the%2520dataset.%2520We%2520convert%2520the%2520obtained%2520concept%2520descriptions%2520into%2520a%2520set%250Aof%2520binary%2520questions.%2520We%2520pose%2520these%2520questions%2520along%2520with%2520the%2520query%2520image%2520to%2520a%250AVQA%2520system%2520and%2520aggregate%2520the%2520answers%2520to%2520determine%2520the%2520presence%2520or%2520absence%2520of%2520an%250Aobject%2520in%2520the%2520test%2520images.%2520Our%2520experiments%2520demonstrate%2520comparable%2520performance%250Awith%2520existing%2520zero-shot%2520visual%2520classification%2520methods%2520and%2520few-shot%2520concept%250Alearning%2520approaches%252C%2520without%2520substantial%2520computational%2520overhead%252C%2520yet%2520being%250Afully%2520explainable%2520from%2520the%2520reasoning%2520perspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Help%20Me%20Identify%3A%20Is%20an%20LLM%2BVQA%20System%20All%20We%20Need%20to%20Identify%20Visual%0A%20%20Concepts%3F&entry.906535625=Shailaja%20Keyur%20Sampat%20and%20Maitreya%20Patel%20and%20Yezhou%20Yang%20and%20Chitta%20Baral&entry.1292438233=%20%20An%20ability%20to%20learn%20about%20new%20objects%20from%20a%20small%20amount%20of%20visual%20data%20and%0Aproduce%20convincing%20linguistic%20justification%20about%20the%20presence/absence%20of%0Acertain%20concepts%20%28that%20collectively%20compose%20the%20object%29%20in%20novel%20scenarios%20is%0Aan%20important%20characteristic%20of%20human%20cognition.%20This%20is%20possible%20due%20to%0Aabstraction%20of%20attributes/properties%20that%20an%20object%20is%20composed%20of%20e.g.%20an%0Aobject%20%60bird%27%20can%20be%20identified%20by%20the%20presence%20of%20a%20beak%2C%20feathers%2C%20legs%2C%0Awings%2C%20etc.%20Inspired%20by%20this%20aspect%20of%20human%20reasoning%2C%20in%20this%20work%2C%20we%0Apresent%20a%20zero-shot%20framework%20for%20fine-grained%20visual%20concept%20learning%20by%0Aleveraging%20large%20language%20model%20and%20Visual%20Question%20Answering%20%28VQA%29%20system.%0ASpecifically%2C%20we%20prompt%20GPT-3%20to%20obtain%20a%20rich%20linguistic%20description%20of%20visual%0Aobjects%20in%20the%20dataset.%20We%20convert%20the%20obtained%20concept%20descriptions%20into%20a%20set%0Aof%20binary%20questions.%20We%20pose%20these%20questions%20along%20with%20the%20query%20image%20to%20a%0AVQA%20system%20and%20aggregate%20the%20answers%20to%20determine%20the%20presence%20or%20absence%20of%20an%0Aobject%20in%20the%20test%20images.%20Our%20experiments%20demonstrate%20comparable%20performance%0Awith%20existing%20zero-shot%20visual%20classification%20methods%20and%20few-shot%20concept%0Alearning%20approaches%2C%20without%20substantial%20computational%20overhead%2C%20yet%20being%0Afully%20explainable%20from%20the%20reasoning%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13651v1&entry.124074799=Read"},
{"title": "VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding", "author": "Runsen Xu and Zhiwei Huang and Tai Wang and Yilun Chen and Jiangmiao Pang and Dahua Lin", "abstract": "  3D visual grounding is crucial for robots, requiring integration of natural\nlanguage and 3D scene understanding. Traditional methods depending on\nsupervised learning with 3D point clouds are limited by scarce datasets.\nRecently zero-shot methods leveraging LLMs have been proposed to address the\ndata issue. While effective, these methods only use object-centric information,\nlimiting their ability to handle complex queries. In this work, we present\nVLM-Grounder, a novel framework using vision-language models (VLMs) for\nzero-shot 3D visual grounding based solely on 2D images. VLM-Grounder\ndynamically stitches image sequences, employs a grounding and feedback scheme\nto find the target object, and uses a multi-view ensemble projection to\naccurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D\ndatasets show VLM-Grounder outperforms previous zero-shot methods, achieving\n51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D\ngeometry or object priors. Codes are available at\nhttps://github.com/OpenRobotLab/VLM-Grounder .\n", "link": "http://arxiv.org/abs/2410.13860v1", "date": "2024-10-17", "relevancy": 2.9293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-Grounder%3A%20A%20VLM%20Agent%20for%20Zero-Shot%203D%20Visual%20Grounding&body=Title%3A%20VLM-Grounder%3A%20A%20VLM%20Agent%20for%20Zero-Shot%203D%20Visual%20Grounding%0AAuthor%3A%20Runsen%20Xu%20and%20Zhiwei%20Huang%20and%20Tai%20Wang%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%20and%20Dahua%20Lin%0AAbstract%3A%20%20%203D%20visual%20grounding%20is%20crucial%20for%20robots%2C%20requiring%20integration%20of%20natural%0Alanguage%20and%203D%20scene%20understanding.%20Traditional%20methods%20depending%20on%0Asupervised%20learning%20with%203D%20point%20clouds%20are%20limited%20by%20scarce%20datasets.%0ARecently%20zero-shot%20methods%20leveraging%20LLMs%20have%20been%20proposed%20to%20address%20the%0Adata%20issue.%20While%20effective%2C%20these%20methods%20only%20use%20object-centric%20information%2C%0Alimiting%20their%20ability%20to%20handle%20complex%20queries.%20In%20this%20work%2C%20we%20present%0AVLM-Grounder%2C%20a%20novel%20framework%20using%20vision-language%20models%20%28VLMs%29%20for%0Azero-shot%203D%20visual%20grounding%20based%20solely%20on%202D%20images.%20VLM-Grounder%0Adynamically%20stitches%20image%20sequences%2C%20employs%20a%20grounding%20and%20feedback%20scheme%0Ato%20find%20the%20target%20object%2C%20and%20uses%20a%20multi-view%20ensemble%20projection%20to%0Aaccurately%20estimate%203D%20bounding%20boxes.%20Experiments%20on%20ScanRefer%20and%20Nr3D%0Adatasets%20show%20VLM-Grounder%20outperforms%20previous%20zero-shot%20methods%2C%20achieving%0A51.6%25%20Acc%400.25%20on%20ScanRefer%20and%2048.0%25%20Acc%20on%20Nr3D%2C%20without%20relying%20on%203D%0Ageometry%20or%20object%20priors.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/OpenRobotLab/VLM-Grounder%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-Grounder%253A%2520A%2520VLM%2520Agent%2520for%2520Zero-Shot%25203D%2520Visual%2520Grounding%26entry.906535625%3DRunsen%2520Xu%2520and%2520Zhiwei%2520Huang%2520and%2520Tai%2520Wang%2520and%2520Yilun%2520Chen%2520and%2520Jiangmiao%2520Pang%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%25203D%2520visual%2520grounding%2520is%2520crucial%2520for%2520robots%252C%2520requiring%2520integration%2520of%2520natural%250Alanguage%2520and%25203D%2520scene%2520understanding.%2520Traditional%2520methods%2520depending%2520on%250Asupervised%2520learning%2520with%25203D%2520point%2520clouds%2520are%2520limited%2520by%2520scarce%2520datasets.%250ARecently%2520zero-shot%2520methods%2520leveraging%2520LLMs%2520have%2520been%2520proposed%2520to%2520address%2520the%250Adata%2520issue.%2520While%2520effective%252C%2520these%2520methods%2520only%2520use%2520object-centric%2520information%252C%250Alimiting%2520their%2520ability%2520to%2520handle%2520complex%2520queries.%2520In%2520this%2520work%252C%2520we%2520present%250AVLM-Grounder%252C%2520a%2520novel%2520framework%2520using%2520vision-language%2520models%2520%2528VLMs%2529%2520for%250Azero-shot%25203D%2520visual%2520grounding%2520based%2520solely%2520on%25202D%2520images.%2520VLM-Grounder%250Adynamically%2520stitches%2520image%2520sequences%252C%2520employs%2520a%2520grounding%2520and%2520feedback%2520scheme%250Ato%2520find%2520the%2520target%2520object%252C%2520and%2520uses%2520a%2520multi-view%2520ensemble%2520projection%2520to%250Aaccurately%2520estimate%25203D%2520bounding%2520boxes.%2520Experiments%2520on%2520ScanRefer%2520and%2520Nr3D%250Adatasets%2520show%2520VLM-Grounder%2520outperforms%2520previous%2520zero-shot%2520methods%252C%2520achieving%250A51.6%2525%2520Acc%25400.25%2520on%2520ScanRefer%2520and%252048.0%2525%2520Acc%2520on%2520Nr3D%252C%2520without%2520relying%2520on%25203D%250Ageometry%2520or%2520object%2520priors.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/OpenRobotLab/VLM-Grounder%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-Grounder%3A%20A%20VLM%20Agent%20for%20Zero-Shot%203D%20Visual%20Grounding&entry.906535625=Runsen%20Xu%20and%20Zhiwei%20Huang%20and%20Tai%20Wang%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%20and%20Dahua%20Lin&entry.1292438233=%20%203D%20visual%20grounding%20is%20crucial%20for%20robots%2C%20requiring%20integration%20of%20natural%0Alanguage%20and%203D%20scene%20understanding.%20Traditional%20methods%20depending%20on%0Asupervised%20learning%20with%203D%20point%20clouds%20are%20limited%20by%20scarce%20datasets.%0ARecently%20zero-shot%20methods%20leveraging%20LLMs%20have%20been%20proposed%20to%20address%20the%0Adata%20issue.%20While%20effective%2C%20these%20methods%20only%20use%20object-centric%20information%2C%0Alimiting%20their%20ability%20to%20handle%20complex%20queries.%20In%20this%20work%2C%20we%20present%0AVLM-Grounder%2C%20a%20novel%20framework%20using%20vision-language%20models%20%28VLMs%29%20for%0Azero-shot%203D%20visual%20grounding%20based%20solely%20on%202D%20images.%20VLM-Grounder%0Adynamically%20stitches%20image%20sequences%2C%20employs%20a%20grounding%20and%20feedback%20scheme%0Ato%20find%20the%20target%20object%2C%20and%20uses%20a%20multi-view%20ensemble%20projection%20to%0Aaccurately%20estimate%203D%20bounding%20boxes.%20Experiments%20on%20ScanRefer%20and%20Nr3D%0Adatasets%20show%20VLM-Grounder%20outperforms%20previous%20zero-shot%20methods%2C%20achieving%0A51.6%25%20Acc%400.25%20on%20ScanRefer%20and%2048.0%25%20Acc%20on%20Nr3D%2C%20without%20relying%20on%203D%0Ageometry%20or%20object%20priors.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/OpenRobotLab/VLM-Grounder%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13860v1&entry.124074799=Read"},
{"title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding\n  and Generation", "author": "Chengyue Wu and Xiaokang Chen and Zhiyu Wu and Yiyang Ma and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan and Ping Luo", "abstract": "  In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.\n", "link": "http://arxiv.org/abs/2410.13848v1", "date": "2024-10-17", "relevancy": 2.8808, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Janus%3A%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%0A%20%20and%20Generation&body=Title%3A%20Janus%3A%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%0A%20%20and%20Generation%0AAuthor%3A%20Chengyue%20Wu%20and%20Xiaokang%20Chen%20and%20Zhiyu%20Wu%20and%20Yiyang%20Ma%20and%20Xingchao%20Liu%20and%20Zizheng%20Pan%20and%20Wen%20Liu%20and%20Zhenda%20Xie%20and%20Xingkai%20Yu%20and%20Chong%20Ruan%20and%20Ping%20Luo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Janus%2C%20an%20autoregressive%20framework%20that%20unifies%0Amultimodal%20understanding%20and%20generation.%20Prior%20research%20often%20relies%20on%20a%0Asingle%20visual%20encoder%20for%20both%20tasks%2C%20such%20as%20Chameleon.%20However%2C%20due%20to%20the%0Adiffering%20levels%20of%20information%20granularity%20required%20by%20multimodal%0Aunderstanding%20and%20generation%2C%20this%20approach%20can%20lead%20to%20suboptimal%20performance%2C%0Aparticularly%20in%20multimodal%20understanding.%20To%20address%20this%20issue%2C%20we%20decouple%0Avisual%20encoding%20into%20separate%20pathways%2C%20while%20still%20leveraging%20a%20single%2C%0Aunified%20transformer%20architecture%20for%20processing.%20The%20decoupling%20not%20only%0Aalleviates%20the%20conflict%20between%20the%20visual%20encoder%27s%20roles%20in%20understanding%20and%0Ageneration%2C%20but%20also%20enhances%20the%20framework%27s%20flexibility.%20For%20instance%2C%20both%0Athe%20multimodal%20understanding%20and%20generation%20components%20can%20independently%20select%0Atheir%20most%20suitable%20encoding%20methods.%20Experiments%20show%20that%20Janus%20surpasses%0Aprevious%20unified%20model%20and%20matches%20or%20exceeds%20the%20performance%20of%20task-specific%0Amodels.%20The%20simplicity%2C%20high%20flexibility%2C%20and%20effectiveness%20of%20Janus%20make%20it%20a%0Astrong%20candidate%20for%20next-generation%20unified%20multimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJanus%253A%2520Decoupling%2520Visual%2520Encoding%2520for%2520Unified%2520Multimodal%2520Understanding%250A%2520%2520and%2520Generation%26entry.906535625%3DChengyue%2520Wu%2520and%2520Xiaokang%2520Chen%2520and%2520Zhiyu%2520Wu%2520and%2520Yiyang%2520Ma%2520and%2520Xingchao%2520Liu%2520and%2520Zizheng%2520Pan%2520and%2520Wen%2520Liu%2520and%2520Zhenda%2520Xie%2520and%2520Xingkai%2520Yu%2520and%2520Chong%2520Ruan%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Janus%252C%2520an%2520autoregressive%2520framework%2520that%2520unifies%250Amultimodal%2520understanding%2520and%2520generation.%2520Prior%2520research%2520often%2520relies%2520on%2520a%250Asingle%2520visual%2520encoder%2520for%2520both%2520tasks%252C%2520such%2520as%2520Chameleon.%2520However%252C%2520due%2520to%2520the%250Adiffering%2520levels%2520of%2520information%2520granularity%2520required%2520by%2520multimodal%250Aunderstanding%2520and%2520generation%252C%2520this%2520approach%2520can%2520lead%2520to%2520suboptimal%2520performance%252C%250Aparticularly%2520in%2520multimodal%2520understanding.%2520To%2520address%2520this%2520issue%252C%2520we%2520decouple%250Avisual%2520encoding%2520into%2520separate%2520pathways%252C%2520while%2520still%2520leveraging%2520a%2520single%252C%250Aunified%2520transformer%2520architecture%2520for%2520processing.%2520The%2520decoupling%2520not%2520only%250Aalleviates%2520the%2520conflict%2520between%2520the%2520visual%2520encoder%2527s%2520roles%2520in%2520understanding%2520and%250Ageneration%252C%2520but%2520also%2520enhances%2520the%2520framework%2527s%2520flexibility.%2520For%2520instance%252C%2520both%250Athe%2520multimodal%2520understanding%2520and%2520generation%2520components%2520can%2520independently%2520select%250Atheir%2520most%2520suitable%2520encoding%2520methods.%2520Experiments%2520show%2520that%2520Janus%2520surpasses%250Aprevious%2520unified%2520model%2520and%2520matches%2520or%2520exceeds%2520the%2520performance%2520of%2520task-specific%250Amodels.%2520The%2520simplicity%252C%2520high%2520flexibility%252C%2520and%2520effectiveness%2520of%2520Janus%2520make%2520it%2520a%250Astrong%2520candidate%2520for%2520next-generation%2520unified%2520multimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Janus%3A%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%0A%20%20and%20Generation&entry.906535625=Chengyue%20Wu%20and%20Xiaokang%20Chen%20and%20Zhiyu%20Wu%20and%20Yiyang%20Ma%20and%20Xingchao%20Liu%20and%20Zizheng%20Pan%20and%20Wen%20Liu%20and%20Zhenda%20Xie%20and%20Xingkai%20Yu%20and%20Chong%20Ruan%20and%20Ping%20Luo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Janus%2C%20an%20autoregressive%20framework%20that%20unifies%0Amultimodal%20understanding%20and%20generation.%20Prior%20research%20often%20relies%20on%20a%0Asingle%20visual%20encoder%20for%20both%20tasks%2C%20such%20as%20Chameleon.%20However%2C%20due%20to%20the%0Adiffering%20levels%20of%20information%20granularity%20required%20by%20multimodal%0Aunderstanding%20and%20generation%2C%20this%20approach%20can%20lead%20to%20suboptimal%20performance%2C%0Aparticularly%20in%20multimodal%20understanding.%20To%20address%20this%20issue%2C%20we%20decouple%0Avisual%20encoding%20into%20separate%20pathways%2C%20while%20still%20leveraging%20a%20single%2C%0Aunified%20transformer%20architecture%20for%20processing.%20The%20decoupling%20not%20only%0Aalleviates%20the%20conflict%20between%20the%20visual%20encoder%27s%20roles%20in%20understanding%20and%0Ageneration%2C%20but%20also%20enhances%20the%20framework%27s%20flexibility.%20For%20instance%2C%20both%0Athe%20multimodal%20understanding%20and%20generation%20components%20can%20independently%20select%0Atheir%20most%20suitable%20encoding%20methods.%20Experiments%20show%20that%20Janus%20surpasses%0Aprevious%20unified%20model%20and%20matches%20or%20exceeds%20the%20performance%20of%20task-specific%0Amodels.%20The%20simplicity%2C%20high%20flexibility%2C%20and%20effectiveness%20of%20Janus%20make%20it%20a%0Astrong%20candidate%20for%20next-generation%20unified%20multimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13848v1&entry.124074799=Read"},
{"title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation", "author": "Rongyao Fang and Chengqi Duan and Kun Wang and Hao Li and Hao Tian and Xingyu Zeng and Rui Zhao and Jifeng Dai and Hongsheng Li and Xihui Liu", "abstract": "  Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps://github.com/rongyaofang/PUMA.\n", "link": "http://arxiv.org/abs/2410.13861v1", "date": "2024-10-17", "relevancy": 2.8069, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PUMA%3A%20Empowering%20Unified%20MLLM%20with%20Multi-granular%20Visual%20Generation&body=Title%3A%20PUMA%3A%20Empowering%20Unified%20MLLM%20with%20Multi-granular%20Visual%20Generation%0AAuthor%3A%20Rongyao%20Fang%20and%20Chengqi%20Duan%20and%20Kun%20Wang%20and%20Hao%20Li%20and%20Hao%20Tian%20and%20Xingyu%20Zeng%20and%20Rui%20Zhao%20and%20Jifeng%20Dai%20and%20Hongsheng%20Li%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20foundation%20models%20have%20yielded%20significant%0Aprogress%20in%20vision-language%20understanding.%20Initial%20attempts%20have%20also%20explored%0Athe%20potential%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20visual%20content%0Ageneration.%20However%2C%20existing%20works%20have%20insufficiently%20addressed%20the%20varying%0Agranularity%20demands%20of%20different%20image%20generation%20tasks%20within%20a%20unified%20MLLM%0Aparadigm%20-%20from%20the%20diversity%20required%20in%20text-to-image%20generation%20to%20the%0Aprecise%20controllability%20needed%20in%20image%20manipulation.%20In%20this%20work%2C%20we%20propose%0APUMA%2C%20emPowering%20Unified%20MLLM%20with%20Multi-grAnular%20visual%20generation.%20PUMA%0Aunifies%20multi-granular%20visual%20features%20as%20both%20inputs%20and%20outputs%20of%20MLLMs%2C%0Aelegantly%20addressing%20the%20different%20granularity%20requirements%20of%20various%20image%0Ageneration%20tasks%20within%20a%20unified%20MLLM%20framework.%20Following%20multimodal%0Apretraining%20and%20task-specific%20instruction%20tuning%2C%20PUMA%20demonstrates%20proficiency%0Ain%20a%20wide%20range%20of%20multimodal%20tasks.%20This%20work%20represents%20a%20significant%20step%0Atowards%20a%20truly%20unified%20MLLM%20capable%20of%20adapting%20to%20the%20granularity%20demands%20of%0Avarious%20visual%20tasks.%20The%20code%20and%20model%20will%20be%20released%20in%0Ahttps%3A//github.com/rongyaofang/PUMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPUMA%253A%2520Empowering%2520Unified%2520MLLM%2520with%2520Multi-granular%2520Visual%2520Generation%26entry.906535625%3DRongyao%2520Fang%2520and%2520Chengqi%2520Duan%2520and%2520Kun%2520Wang%2520and%2520Hao%2520Li%2520and%2520Hao%2520Tian%2520and%2520Xingyu%2520Zeng%2520and%2520Rui%2520Zhao%2520and%2520Jifeng%2520Dai%2520and%2520Hongsheng%2520Li%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520foundation%2520models%2520have%2520yielded%2520significant%250Aprogress%2520in%2520vision-language%2520understanding.%2520Initial%2520attempts%2520have%2520also%2520explored%250Athe%2520potential%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520visual%2520content%250Ageneration.%2520However%252C%2520existing%2520works%2520have%2520insufficiently%2520addressed%2520the%2520varying%250Agranularity%2520demands%2520of%2520different%2520image%2520generation%2520tasks%2520within%2520a%2520unified%2520MLLM%250Aparadigm%2520-%2520from%2520the%2520diversity%2520required%2520in%2520text-to-image%2520generation%2520to%2520the%250Aprecise%2520controllability%2520needed%2520in%2520image%2520manipulation.%2520In%2520this%2520work%252C%2520we%2520propose%250APUMA%252C%2520emPowering%2520Unified%2520MLLM%2520with%2520Multi-grAnular%2520visual%2520generation.%2520PUMA%250Aunifies%2520multi-granular%2520visual%2520features%2520as%2520both%2520inputs%2520and%2520outputs%2520of%2520MLLMs%252C%250Aelegantly%2520addressing%2520the%2520different%2520granularity%2520requirements%2520of%2520various%2520image%250Ageneration%2520tasks%2520within%2520a%2520unified%2520MLLM%2520framework.%2520Following%2520multimodal%250Apretraining%2520and%2520task-specific%2520instruction%2520tuning%252C%2520PUMA%2520demonstrates%2520proficiency%250Ain%2520a%2520wide%2520range%2520of%2520multimodal%2520tasks.%2520This%2520work%2520represents%2520a%2520significant%2520step%250Atowards%2520a%2520truly%2520unified%2520MLLM%2520capable%2520of%2520adapting%2520to%2520the%2520granularity%2520demands%2520of%250Avarious%2520visual%2520tasks.%2520The%2520code%2520and%2520model%2520will%2520be%2520released%2520in%250Ahttps%253A//github.com/rongyaofang/PUMA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PUMA%3A%20Empowering%20Unified%20MLLM%20with%20Multi-granular%20Visual%20Generation&entry.906535625=Rongyao%20Fang%20and%20Chengqi%20Duan%20and%20Kun%20Wang%20and%20Hao%20Li%20and%20Hao%20Tian%20and%20Xingyu%20Zeng%20and%20Rui%20Zhao%20and%20Jifeng%20Dai%20and%20Hongsheng%20Li%20and%20Xihui%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20foundation%20models%20have%20yielded%20significant%0Aprogress%20in%20vision-language%20understanding.%20Initial%20attempts%20have%20also%20explored%0Athe%20potential%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20visual%20content%0Ageneration.%20However%2C%20existing%20works%20have%20insufficiently%20addressed%20the%20varying%0Agranularity%20demands%20of%20different%20image%20generation%20tasks%20within%20a%20unified%20MLLM%0Aparadigm%20-%20from%20the%20diversity%20required%20in%20text-to-image%20generation%20to%20the%0Aprecise%20controllability%20needed%20in%20image%20manipulation.%20In%20this%20work%2C%20we%20propose%0APUMA%2C%20emPowering%20Unified%20MLLM%20with%20Multi-grAnular%20visual%20generation.%20PUMA%0Aunifies%20multi-granular%20visual%20features%20as%20both%20inputs%20and%20outputs%20of%20MLLMs%2C%0Aelegantly%20addressing%20the%20different%20granularity%20requirements%20of%20various%20image%0Ageneration%20tasks%20within%20a%20unified%20MLLM%20framework.%20Following%20multimodal%0Apretraining%20and%20task-specific%20instruction%20tuning%2C%20PUMA%20demonstrates%20proficiency%0Ain%20a%20wide%20range%20of%20multimodal%20tasks.%20This%20work%20represents%20a%20significant%20step%0Atowards%20a%20truly%20unified%20MLLM%20capable%20of%20adapting%20to%20the%20granularity%20demands%20of%0Avarious%20visual%20tasks.%20The%20code%20and%20model%20will%20be%20released%20in%0Ahttps%3A//github.com/rongyaofang/PUMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13861v1&entry.124074799=Read"},
{"title": "DPLM-2: A Multimodal Diffusion Protein Language Model", "author": "Xinyou Wang and Zaixiang Zheng and Fei Ye and Dongyu Xue and Shujian Huang and Quanquan Gu", "abstract": "  Proteins are essential macromolecules defined by their amino acid sequences,\nwhich determine their three-dimensional structures and, consequently, their\nfunctions in all living organisms. Therefore, generative protein modeling\nnecessitates a multimodal approach to simultaneously model, understand, and\ngenerate both sequences and structures. However, existing methods typically use\nseparate models for each modality, limiting their ability to capture the\nintricate relationships between sequence and structure. This results in\nsuboptimal performance in tasks that requires joint understanding and\ngeneration of both modalities. In this paper, we introduce DPLM-2, a multimodal\nprotein foundation model that extends discrete diffusion protein language model\n(DPLM) to accommodate both sequences and structures. To enable structural\nlearning with the language model, 3D coordinates are converted to discrete\ntokens using a lookup-free quantization-based tokenizer. By training on both\nexperimental and high-quality synthetic structures, DPLM-2 learns the joint\ndistribution of sequence and structure, as well as their marginals and\nconditionals. We also implement an efficient warm-up strategy to exploit the\nconnection between large-scale evolutionary data and structural inductive\nbiases from pre-trained sequence-based protein language models. Empirical\nevaluation shows that DPLM-2 can simultaneously generate highly compatible\namino acid sequences and their corresponding 3D structures eliminating the need\nfor a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive\nperformance in various conditional generation tasks, including folding, inverse\nfolding, and scaffolding with multimodal motif inputs, as well as providing\nstructure-aware representations for predictive tasks.\n", "link": "http://arxiv.org/abs/2410.13782v1", "date": "2024-10-17", "relevancy": 2.7522, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPLM-2%3A%20A%20Multimodal%20Diffusion%20Protein%20Language%20Model&body=Title%3A%20DPLM-2%3A%20A%20Multimodal%20Diffusion%20Protein%20Language%20Model%0AAuthor%3A%20Xinyou%20Wang%20and%20Zaixiang%20Zheng%20and%20Fei%20Ye%20and%20Dongyu%20Xue%20and%20Shujian%20Huang%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Proteins%20are%20essential%20macromolecules%20defined%20by%20their%20amino%20acid%20sequences%2C%0Awhich%20determine%20their%20three-dimensional%20structures%20and%2C%20consequently%2C%20their%0Afunctions%20in%20all%20living%20organisms.%20Therefore%2C%20generative%20protein%20modeling%0Anecessitates%20a%20multimodal%20approach%20to%20simultaneously%20model%2C%20understand%2C%20and%0Agenerate%20both%20sequences%20and%20structures.%20However%2C%20existing%20methods%20typically%20use%0Aseparate%20models%20for%20each%20modality%2C%20limiting%20their%20ability%20to%20capture%20the%0Aintricate%20relationships%20between%20sequence%20and%20structure.%20This%20results%20in%0Asuboptimal%20performance%20in%20tasks%20that%20requires%20joint%20understanding%20and%0Ageneration%20of%20both%20modalities.%20In%20this%20paper%2C%20we%20introduce%20DPLM-2%2C%20a%20multimodal%0Aprotein%20foundation%20model%20that%20extends%20discrete%20diffusion%20protein%20language%20model%0A%28DPLM%29%20to%20accommodate%20both%20sequences%20and%20structures.%20To%20enable%20structural%0Alearning%20with%20the%20language%20model%2C%203D%20coordinates%20are%20converted%20to%20discrete%0Atokens%20using%20a%20lookup-free%20quantization-based%20tokenizer.%20By%20training%20on%20both%0Aexperimental%20and%20high-quality%20synthetic%20structures%2C%20DPLM-2%20learns%20the%20joint%0Adistribution%20of%20sequence%20and%20structure%2C%20as%20well%20as%20their%20marginals%20and%0Aconditionals.%20We%20also%20implement%20an%20efficient%20warm-up%20strategy%20to%20exploit%20the%0Aconnection%20between%20large-scale%20evolutionary%20data%20and%20structural%20inductive%0Abiases%20from%20pre-trained%20sequence-based%20protein%20language%20models.%20Empirical%0Aevaluation%20shows%20that%20DPLM-2%20can%20simultaneously%20generate%20highly%20compatible%0Aamino%20acid%20sequences%20and%20their%20corresponding%203D%20structures%20eliminating%20the%20need%0Afor%20a%20two-stage%20generation%20approach.%20Moreover%2C%20DPLM-2%20demonstrates%20competitive%0Aperformance%20in%20various%20conditional%20generation%20tasks%2C%20including%20folding%2C%20inverse%0Afolding%2C%20and%20scaffolding%20with%20multimodal%20motif%20inputs%2C%20as%20well%20as%20providing%0Astructure-aware%20representations%20for%20predictive%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPLM-2%253A%2520A%2520Multimodal%2520Diffusion%2520Protein%2520Language%2520Model%26entry.906535625%3DXinyou%2520Wang%2520and%2520Zaixiang%2520Zheng%2520and%2520Fei%2520Ye%2520and%2520Dongyu%2520Xue%2520and%2520Shujian%2520Huang%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Proteins%2520are%2520essential%2520macromolecules%2520defined%2520by%2520their%2520amino%2520acid%2520sequences%252C%250Awhich%2520determine%2520their%2520three-dimensional%2520structures%2520and%252C%2520consequently%252C%2520their%250Afunctions%2520in%2520all%2520living%2520organisms.%2520Therefore%252C%2520generative%2520protein%2520modeling%250Anecessitates%2520a%2520multimodal%2520approach%2520to%2520simultaneously%2520model%252C%2520understand%252C%2520and%250Agenerate%2520both%2520sequences%2520and%2520structures.%2520However%252C%2520existing%2520methods%2520typically%2520use%250Aseparate%2520models%2520for%2520each%2520modality%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520the%250Aintricate%2520relationships%2520between%2520sequence%2520and%2520structure.%2520This%2520results%2520in%250Asuboptimal%2520performance%2520in%2520tasks%2520that%2520requires%2520joint%2520understanding%2520and%250Ageneration%2520of%2520both%2520modalities.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DPLM-2%252C%2520a%2520multimodal%250Aprotein%2520foundation%2520model%2520that%2520extends%2520discrete%2520diffusion%2520protein%2520language%2520model%250A%2528DPLM%2529%2520to%2520accommodate%2520both%2520sequences%2520and%2520structures.%2520To%2520enable%2520structural%250Alearning%2520with%2520the%2520language%2520model%252C%25203D%2520coordinates%2520are%2520converted%2520to%2520discrete%250Atokens%2520using%2520a%2520lookup-free%2520quantization-based%2520tokenizer.%2520By%2520training%2520on%2520both%250Aexperimental%2520and%2520high-quality%2520synthetic%2520structures%252C%2520DPLM-2%2520learns%2520the%2520joint%250Adistribution%2520of%2520sequence%2520and%2520structure%252C%2520as%2520well%2520as%2520their%2520marginals%2520and%250Aconditionals.%2520We%2520also%2520implement%2520an%2520efficient%2520warm-up%2520strategy%2520to%2520exploit%2520the%250Aconnection%2520between%2520large-scale%2520evolutionary%2520data%2520and%2520structural%2520inductive%250Abiases%2520from%2520pre-trained%2520sequence-based%2520protein%2520language%2520models.%2520Empirical%250Aevaluation%2520shows%2520that%2520DPLM-2%2520can%2520simultaneously%2520generate%2520highly%2520compatible%250Aamino%2520acid%2520sequences%2520and%2520their%2520corresponding%25203D%2520structures%2520eliminating%2520the%2520need%250Afor%2520a%2520two-stage%2520generation%2520approach.%2520Moreover%252C%2520DPLM-2%2520demonstrates%2520competitive%250Aperformance%2520in%2520various%2520conditional%2520generation%2520tasks%252C%2520including%2520folding%252C%2520inverse%250Afolding%252C%2520and%2520scaffolding%2520with%2520multimodal%2520motif%2520inputs%252C%2520as%2520well%2520as%2520providing%250Astructure-aware%2520representations%2520for%2520predictive%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPLM-2%3A%20A%20Multimodal%20Diffusion%20Protein%20Language%20Model&entry.906535625=Xinyou%20Wang%20and%20Zaixiang%20Zheng%20and%20Fei%20Ye%20and%20Dongyu%20Xue%20and%20Shujian%20Huang%20and%20Quanquan%20Gu&entry.1292438233=%20%20Proteins%20are%20essential%20macromolecules%20defined%20by%20their%20amino%20acid%20sequences%2C%0Awhich%20determine%20their%20three-dimensional%20structures%20and%2C%20consequently%2C%20their%0Afunctions%20in%20all%20living%20organisms.%20Therefore%2C%20generative%20protein%20modeling%0Anecessitates%20a%20multimodal%20approach%20to%20simultaneously%20model%2C%20understand%2C%20and%0Agenerate%20both%20sequences%20and%20structures.%20However%2C%20existing%20methods%20typically%20use%0Aseparate%20models%20for%20each%20modality%2C%20limiting%20their%20ability%20to%20capture%20the%0Aintricate%20relationships%20between%20sequence%20and%20structure.%20This%20results%20in%0Asuboptimal%20performance%20in%20tasks%20that%20requires%20joint%20understanding%20and%0Ageneration%20of%20both%20modalities.%20In%20this%20paper%2C%20we%20introduce%20DPLM-2%2C%20a%20multimodal%0Aprotein%20foundation%20model%20that%20extends%20discrete%20diffusion%20protein%20language%20model%0A%28DPLM%29%20to%20accommodate%20both%20sequences%20and%20structures.%20To%20enable%20structural%0Alearning%20with%20the%20language%20model%2C%203D%20coordinates%20are%20converted%20to%20discrete%0Atokens%20using%20a%20lookup-free%20quantization-based%20tokenizer.%20By%20training%20on%20both%0Aexperimental%20and%20high-quality%20synthetic%20structures%2C%20DPLM-2%20learns%20the%20joint%0Adistribution%20of%20sequence%20and%20structure%2C%20as%20well%20as%20their%20marginals%20and%0Aconditionals.%20We%20also%20implement%20an%20efficient%20warm-up%20strategy%20to%20exploit%20the%0Aconnection%20between%20large-scale%20evolutionary%20data%20and%20structural%20inductive%0Abiases%20from%20pre-trained%20sequence-based%20protein%20language%20models.%20Empirical%0Aevaluation%20shows%20that%20DPLM-2%20can%20simultaneously%20generate%20highly%20compatible%0Aamino%20acid%20sequences%20and%20their%20corresponding%203D%20structures%20eliminating%20the%20need%0Afor%20a%20two-stage%20generation%20approach.%20Moreover%2C%20DPLM-2%20demonstrates%20competitive%0Aperformance%20in%20various%20conditional%20generation%20tasks%2C%20including%20folding%2C%20inverse%0Afolding%2C%20and%20scaffolding%20with%20multimodal%20motif%20inputs%2C%20as%20well%20as%20providing%0Astructure-aware%20representations%20for%20predictive%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13782v1&entry.124074799=Read"},
{"title": "Can MLLMs Understand the Deep Implication Behind Chinese Images?", "author": "Chenhao Zhang and Xi Feng and Yuelin Bai and Xinrun Du and Jinchang Hou and Kaixin Deng and Guangzeng Han and Qinrui Li and Bingli Wang and Jiaheng Liu and Xingwei Qu and Yifei Zhang and Qixuan Zhao and Yiming Liang and Ziqiang Liu and Feiteng Fang and Min Yang and Wenhao Huang and Chenghua Lin and Ge Zhang and Shiwen Ni", "abstract": "  As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.\n", "link": "http://arxiv.org/abs/2410.13854v1", "date": "2024-10-17", "relevancy": 2.7156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20MLLMs%20Understand%20the%20Deep%20Implication%20Behind%20Chinese%20Images%3F&body=Title%3A%20Can%20MLLMs%20Understand%20the%20Deep%20Implication%20Behind%20Chinese%20Images%3F%0AAuthor%3A%20Chenhao%20Zhang%20and%20Xi%20Feng%20and%20Yuelin%20Bai%20and%20Xinrun%20Du%20and%20Jinchang%20Hou%20and%20Kaixin%20Deng%20and%20Guangzeng%20Han%20and%20Qinrui%20Li%20and%20Bingli%20Wang%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Yifei%20Zhang%20and%20Qixuan%20Zhao%20and%20Yiming%20Liang%20and%20Ziqiang%20Liu%20and%20Feiteng%20Fang%20and%20Min%20Yang%20and%20Wenhao%20Huang%20and%20Chenghua%20Lin%20and%20Ge%20Zhang%20and%20Shiwen%20Ni%0AAbstract%3A%20%20%20As%20the%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20continue%20to%0Aimprove%2C%20the%20need%20for%20higher-order%20capability%20evaluation%20of%20MLLMs%20is%0Aincreasing.%20However%2C%20there%20is%20a%20lack%20of%20work%20evaluating%20MLLM%20for%20higher-order%0Aperception%20and%20understanding%20of%20Chinese%20visual%20content.%20To%20fill%20the%20gap%2C%20we%0Aintroduce%20the%20%2A%2AC%2A%2Ahinese%20%2A%2AI%2A%2Amage%20%2A%2AI%2A%2Amplication%20understanding%0A%2A%2ABench%2A%2Amark%2C%20%2A%2ACII-Bench%2A%2A%2C%20which%20aims%20to%20assess%20the%20higher-order%20perception%0Aand%20understanding%20capabilities%20of%20MLLMs%20for%20Chinese%20images.%20CII-Bench%20stands%0Aout%20in%20several%20ways%20compared%20to%20existing%20benchmarks.%20Firstly%2C%20to%20ensure%20the%0Aauthenticity%20of%20the%20Chinese%20context%2C%20images%20in%20CII-Bench%20are%20sourced%20from%20the%0AChinese%20Internet%20and%20manually%20reviewed%2C%20with%20corresponding%20answers%20also%0Amanually%20crafted.%20Additionally%2C%20CII-Bench%20incorporates%20images%20that%20represent%0AChinese%20traditional%20culture%2C%20such%20as%20famous%20Chinese%20traditional%20paintings%2C%0Awhich%20can%20deeply%20reflect%20the%20model%27s%20understanding%20of%20Chinese%20traditional%0Aculture.%20Through%20extensive%20experiments%20on%20CII-Bench%20across%20multiple%20MLLMs%2C%20we%0Ahave%20made%20significant%20findings.%20Initially%2C%20a%20substantial%20gap%20is%20observed%0Abetween%20the%20performance%20of%20MLLMs%20and%20humans%20on%20CII-Bench.%20The%20highest%20accuracy%0Aof%20MLLMs%20attains%2064.4%25%2C%20where%20as%20human%20accuracy%20averages%2078.2%25%2C%20peaking%20at%20an%0Aimpressive%2081.0%25.%20Subsequently%2C%20MLLMs%20perform%20worse%20on%20Chinese%20traditional%0Aculture%20images%2C%20suggesting%20limitations%20in%20their%20ability%20to%20understand%0Ahigh-level%20semantics%20and%20lack%20a%20deep%20knowledge%20base%20of%20Chinese%20traditional%0Aculture.%20Finally%2C%20it%20is%20observed%20that%20most%20models%20exhibit%20enhanced%20accuracy%0Awhen%20image%20emotion%20hints%20are%20incorporated%20into%20the%20prompts.%20We%20believe%20that%0ACII-Bench%20will%20enable%20MLLMs%20to%20gain%20a%20better%20understanding%20of%20Chinese%20semantics%0Aand%20Chinese-specific%20images%2C%20advancing%20the%20journey%20towards%20expert%20artificial%0Ageneral%20intelligence%20%28AGI%29.%20Our%20project%20is%20publicly%20available%20at%0Ahttps%3A//cii-bench.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520MLLMs%2520Understand%2520the%2520Deep%2520Implication%2520Behind%2520Chinese%2520Images%253F%26entry.906535625%3DChenhao%2520Zhang%2520and%2520Xi%2520Feng%2520and%2520Yuelin%2520Bai%2520and%2520Xinrun%2520Du%2520and%2520Jinchang%2520Hou%2520and%2520Kaixin%2520Deng%2520and%2520Guangzeng%2520Han%2520and%2520Qinrui%2520Li%2520and%2520Bingli%2520Wang%2520and%2520Jiaheng%2520Liu%2520and%2520Xingwei%2520Qu%2520and%2520Yifei%2520Zhang%2520and%2520Qixuan%2520Zhao%2520and%2520Yiming%2520Liang%2520and%2520Ziqiang%2520Liu%2520and%2520Feiteng%2520Fang%2520and%2520Min%2520Yang%2520and%2520Wenhao%2520Huang%2520and%2520Chenghua%2520Lin%2520and%2520Ge%2520Zhang%2520and%2520Shiwen%2520Ni%26entry.1292438233%3D%2520%2520As%2520the%2520capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520continue%2520to%250Aimprove%252C%2520the%2520need%2520for%2520higher-order%2520capability%2520evaluation%2520of%2520MLLMs%2520is%250Aincreasing.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%2520work%2520evaluating%2520MLLM%2520for%2520higher-order%250Aperception%2520and%2520understanding%2520of%2520Chinese%2520visual%2520content.%2520To%2520fill%2520the%2520gap%252C%2520we%250Aintroduce%2520the%2520%252A%252AC%252A%252Ahinese%2520%252A%252AI%252A%252Amage%2520%252A%252AI%252A%252Amplication%2520understanding%250A%252A%252ABench%252A%252Amark%252C%2520%252A%252ACII-Bench%252A%252A%252C%2520which%2520aims%2520to%2520assess%2520the%2520higher-order%2520perception%250Aand%2520understanding%2520capabilities%2520of%2520MLLMs%2520for%2520Chinese%2520images.%2520CII-Bench%2520stands%250Aout%2520in%2520several%2520ways%2520compared%2520to%2520existing%2520benchmarks.%2520Firstly%252C%2520to%2520ensure%2520the%250Aauthenticity%2520of%2520the%2520Chinese%2520context%252C%2520images%2520in%2520CII-Bench%2520are%2520sourced%2520from%2520the%250AChinese%2520Internet%2520and%2520manually%2520reviewed%252C%2520with%2520corresponding%2520answers%2520also%250Amanually%2520crafted.%2520Additionally%252C%2520CII-Bench%2520incorporates%2520images%2520that%2520represent%250AChinese%2520traditional%2520culture%252C%2520such%2520as%2520famous%2520Chinese%2520traditional%2520paintings%252C%250Awhich%2520can%2520deeply%2520reflect%2520the%2520model%2527s%2520understanding%2520of%2520Chinese%2520traditional%250Aculture.%2520Through%2520extensive%2520experiments%2520on%2520CII-Bench%2520across%2520multiple%2520MLLMs%252C%2520we%250Ahave%2520made%2520significant%2520findings.%2520Initially%252C%2520a%2520substantial%2520gap%2520is%2520observed%250Abetween%2520the%2520performance%2520of%2520MLLMs%2520and%2520humans%2520on%2520CII-Bench.%2520The%2520highest%2520accuracy%250Aof%2520MLLMs%2520attains%252064.4%2525%252C%2520where%2520as%2520human%2520accuracy%2520averages%252078.2%2525%252C%2520peaking%2520at%2520an%250Aimpressive%252081.0%2525.%2520Subsequently%252C%2520MLLMs%2520perform%2520worse%2520on%2520Chinese%2520traditional%250Aculture%2520images%252C%2520suggesting%2520limitations%2520in%2520their%2520ability%2520to%2520understand%250Ahigh-level%2520semantics%2520and%2520lack%2520a%2520deep%2520knowledge%2520base%2520of%2520Chinese%2520traditional%250Aculture.%2520Finally%252C%2520it%2520is%2520observed%2520that%2520most%2520models%2520exhibit%2520enhanced%2520accuracy%250Awhen%2520image%2520emotion%2520hints%2520are%2520incorporated%2520into%2520the%2520prompts.%2520We%2520believe%2520that%250ACII-Bench%2520will%2520enable%2520MLLMs%2520to%2520gain%2520a%2520better%2520understanding%2520of%2520Chinese%2520semantics%250Aand%2520Chinese-specific%2520images%252C%2520advancing%2520the%2520journey%2520towards%2520expert%2520artificial%250Ageneral%2520intelligence%2520%2528AGI%2529.%2520Our%2520project%2520is%2520publicly%2520available%2520at%250Ahttps%253A//cii-bench.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20MLLMs%20Understand%20the%20Deep%20Implication%20Behind%20Chinese%20Images%3F&entry.906535625=Chenhao%20Zhang%20and%20Xi%20Feng%20and%20Yuelin%20Bai%20and%20Xinrun%20Du%20and%20Jinchang%20Hou%20and%20Kaixin%20Deng%20and%20Guangzeng%20Han%20and%20Qinrui%20Li%20and%20Bingli%20Wang%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Yifei%20Zhang%20and%20Qixuan%20Zhao%20and%20Yiming%20Liang%20and%20Ziqiang%20Liu%20and%20Feiteng%20Fang%20and%20Min%20Yang%20and%20Wenhao%20Huang%20and%20Chenghua%20Lin%20and%20Ge%20Zhang%20and%20Shiwen%20Ni&entry.1292438233=%20%20As%20the%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20continue%20to%0Aimprove%2C%20the%20need%20for%20higher-order%20capability%20evaluation%20of%20MLLMs%20is%0Aincreasing.%20However%2C%20there%20is%20a%20lack%20of%20work%20evaluating%20MLLM%20for%20higher-order%0Aperception%20and%20understanding%20of%20Chinese%20visual%20content.%20To%20fill%20the%20gap%2C%20we%0Aintroduce%20the%20%2A%2AC%2A%2Ahinese%20%2A%2AI%2A%2Amage%20%2A%2AI%2A%2Amplication%20understanding%0A%2A%2ABench%2A%2Amark%2C%20%2A%2ACII-Bench%2A%2A%2C%20which%20aims%20to%20assess%20the%20higher-order%20perception%0Aand%20understanding%20capabilities%20of%20MLLMs%20for%20Chinese%20images.%20CII-Bench%20stands%0Aout%20in%20several%20ways%20compared%20to%20existing%20benchmarks.%20Firstly%2C%20to%20ensure%20the%0Aauthenticity%20of%20the%20Chinese%20context%2C%20images%20in%20CII-Bench%20are%20sourced%20from%20the%0AChinese%20Internet%20and%20manually%20reviewed%2C%20with%20corresponding%20answers%20also%0Amanually%20crafted.%20Additionally%2C%20CII-Bench%20incorporates%20images%20that%20represent%0AChinese%20traditional%20culture%2C%20such%20as%20famous%20Chinese%20traditional%20paintings%2C%0Awhich%20can%20deeply%20reflect%20the%20model%27s%20understanding%20of%20Chinese%20traditional%0Aculture.%20Through%20extensive%20experiments%20on%20CII-Bench%20across%20multiple%20MLLMs%2C%20we%0Ahave%20made%20significant%20findings.%20Initially%2C%20a%20substantial%20gap%20is%20observed%0Abetween%20the%20performance%20of%20MLLMs%20and%20humans%20on%20CII-Bench.%20The%20highest%20accuracy%0Aof%20MLLMs%20attains%2064.4%25%2C%20where%20as%20human%20accuracy%20averages%2078.2%25%2C%20peaking%20at%20an%0Aimpressive%2081.0%25.%20Subsequently%2C%20MLLMs%20perform%20worse%20on%20Chinese%20traditional%0Aculture%20images%2C%20suggesting%20limitations%20in%20their%20ability%20to%20understand%0Ahigh-level%20semantics%20and%20lack%20a%20deep%20knowledge%20base%20of%20Chinese%20traditional%0Aculture.%20Finally%2C%20it%20is%20observed%20that%20most%20models%20exhibit%20enhanced%20accuracy%0Awhen%20image%20emotion%20hints%20are%20incorporated%20into%20the%20prompts.%20We%20believe%20that%0ACII-Bench%20will%20enable%20MLLMs%20to%20gain%20a%20better%20understanding%20of%20Chinese%20semantics%0Aand%20Chinese-specific%20images%2C%20advancing%20the%20journey%20towards%20expert%20artificial%0Ageneral%20intelligence%20%28AGI%29.%20Our%20project%20is%20publicly%20available%20at%0Ahttps%3A//cii-bench.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13854v1&entry.124074799=Read"},
{"title": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs", "author": "Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael I. Jordan and Song Mei", "abstract": "  Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.\n", "link": "http://arxiv.org/abs/2410.13835v1", "date": "2024-10-17", "relevancy": 2.709, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active-Dormant%20Attention%20Heads%3A%20Mechanistically%20Demystifying%0A%20%20Extreme-Token%20Phenomena%20in%20LLMs&body=Title%3A%20Active-Dormant%20Attention%20Heads%3A%20Mechanistically%20Demystifying%0A%20%20Extreme-Token%20Phenomena%20in%20LLMs%0AAuthor%3A%20Tianyu%20Guo%20and%20Druv%20Pai%20and%20Yu%20Bai%20and%20Jiantao%20Jiao%20and%20Michael%20I.%20Jordan%20and%20Song%20Mei%0AAbstract%3A%20%20%20Practitioners%20have%20consistently%20observed%20three%20puzzling%20phenomena%20in%0Atransformer-based%20large%20language%20models%20%28LLMs%29%3A%20attention%20sinks%2C%20value-state%0Adrains%2C%20and%20residual-state%20peaks%2C%20collectively%20referred%20to%20as%20extreme-token%0Aphenomena.%20These%20phenomena%20are%20characterized%20by%20certain%20so-called%20%22sink%20tokens%22%0Areceiving%20disproportionately%20high%20attention%20weights%2C%20exhibiting%20significantly%0Asmaller%20value%20states%2C%20and%20having%20much%20larger%20residual-state%20norms%20than%20those%20of%0Aother%20tokens.%20These%20extreme%20tokens%20give%20rise%20to%20various%20challenges%20in%20LLM%0Ainference%2C%20quantization%2C%20and%20interpretability.%0A%20%20We%20elucidate%20the%20mechanisms%20behind%20extreme-token%20phenomena.%20First%2C%20we%20show%0Athat%20these%20phenomena%20arise%20in%20very%20simple%20architectures%20--%20transformers%20with%0Aone%20to%20three%20layers%20--%20trained%20on%20a%20toy%20model%2C%20the%20Bigram-Backcopy%20%28BB%29%20task.%0AIn%20this%20setting%2C%20we%20identify%20an%20active-dormant%20mechanism%2C%20where%20attention%20heads%0Abecome%20sinks%20for%20specific%20input%20domains%20while%20remaining%20non-sinks%20for%20others.%0AOur%20theoretical%20analysis%20of%20the%20training%20dynamics%20reveals%20that%20these%20phenomena%0Aare%20driven%20by%20a%20mutual%20reinforcement%20mechanism.%20Building%20on%20these%20insights%2C%20we%0Apropose%20strategies%20to%20mitigate%20extreme-token%20phenomena%20during%20pretraining%2C%0Aincluding%20replacing%20softmax%20with%20ReLU%20and%20Adam%20with%20SGD.%20Next%2C%20we%20extend%20our%0Aanalysis%20to%20pretrained%20LLMs%2C%20including%20Llama%20and%20OLMo%2C%20showing%20that%20many%0Aattention%20heads%20exhibit%20a%20similar%20active-dormant%20mechanism%20as%20in%20the%20BB%20task%2C%0Aand%20that%20the%20mutual%20reinforcement%20mechanism%20also%20governs%20the%20emergence%20of%0Aextreme-token%20phenomena%20during%20LLM%20pretraining.%20Our%20results%20reveal%20that%20many%20of%0Athe%20static%20and%20dynamic%20properties%20of%20extreme-token%20phenomena%20predicted%20by%20the%0ABB%20task%20align%20with%20observations%20in%20pretrained%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive-Dormant%2520Attention%2520Heads%253A%2520Mechanistically%2520Demystifying%250A%2520%2520Extreme-Token%2520Phenomena%2520in%2520LLMs%26entry.906535625%3DTianyu%2520Guo%2520and%2520Druv%2520Pai%2520and%2520Yu%2520Bai%2520and%2520Jiantao%2520Jiao%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Song%2520Mei%26entry.1292438233%3D%2520%2520Practitioners%2520have%2520consistently%2520observed%2520three%2520puzzling%2520phenomena%2520in%250Atransformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%253A%2520attention%2520sinks%252C%2520value-state%250Adrains%252C%2520and%2520residual-state%2520peaks%252C%2520collectively%2520referred%2520to%2520as%2520extreme-token%250Aphenomena.%2520These%2520phenomena%2520are%2520characterized%2520by%2520certain%2520so-called%2520%2522sink%2520tokens%2522%250Areceiving%2520disproportionately%2520high%2520attention%2520weights%252C%2520exhibiting%2520significantly%250Asmaller%2520value%2520states%252C%2520and%2520having%2520much%2520larger%2520residual-state%2520norms%2520than%2520those%2520of%250Aother%2520tokens.%2520These%2520extreme%2520tokens%2520give%2520rise%2520to%2520various%2520challenges%2520in%2520LLM%250Ainference%252C%2520quantization%252C%2520and%2520interpretability.%250A%2520%2520We%2520elucidate%2520the%2520mechanisms%2520behind%2520extreme-token%2520phenomena.%2520First%252C%2520we%2520show%250Athat%2520these%2520phenomena%2520arise%2520in%2520very%2520simple%2520architectures%2520--%2520transformers%2520with%250Aone%2520to%2520three%2520layers%2520--%2520trained%2520on%2520a%2520toy%2520model%252C%2520the%2520Bigram-Backcopy%2520%2528BB%2529%2520task.%250AIn%2520this%2520setting%252C%2520we%2520identify%2520an%2520active-dormant%2520mechanism%252C%2520where%2520attention%2520heads%250Abecome%2520sinks%2520for%2520specific%2520input%2520domains%2520while%2520remaining%2520non-sinks%2520for%2520others.%250AOur%2520theoretical%2520analysis%2520of%2520the%2520training%2520dynamics%2520reveals%2520that%2520these%2520phenomena%250Aare%2520driven%2520by%2520a%2520mutual%2520reinforcement%2520mechanism.%2520Building%2520on%2520these%2520insights%252C%2520we%250Apropose%2520strategies%2520to%2520mitigate%2520extreme-token%2520phenomena%2520during%2520pretraining%252C%250Aincluding%2520replacing%2520softmax%2520with%2520ReLU%2520and%2520Adam%2520with%2520SGD.%2520Next%252C%2520we%2520extend%2520our%250Aanalysis%2520to%2520pretrained%2520LLMs%252C%2520including%2520Llama%2520and%2520OLMo%252C%2520showing%2520that%2520many%250Aattention%2520heads%2520exhibit%2520a%2520similar%2520active-dormant%2520mechanism%2520as%2520in%2520the%2520BB%2520task%252C%250Aand%2520that%2520the%2520mutual%2520reinforcement%2520mechanism%2520also%2520governs%2520the%2520emergence%2520of%250Aextreme-token%2520phenomena%2520during%2520LLM%2520pretraining.%2520Our%2520results%2520reveal%2520that%2520many%2520of%250Athe%2520static%2520and%2520dynamic%2520properties%2520of%2520extreme-token%2520phenomena%2520predicted%2520by%2520the%250ABB%2520task%2520align%2520with%2520observations%2520in%2520pretrained%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active-Dormant%20Attention%20Heads%3A%20Mechanistically%20Demystifying%0A%20%20Extreme-Token%20Phenomena%20in%20LLMs&entry.906535625=Tianyu%20Guo%20and%20Druv%20Pai%20and%20Yu%20Bai%20and%20Jiantao%20Jiao%20and%20Michael%20I.%20Jordan%20and%20Song%20Mei&entry.1292438233=%20%20Practitioners%20have%20consistently%20observed%20three%20puzzling%20phenomena%20in%0Atransformer-based%20large%20language%20models%20%28LLMs%29%3A%20attention%20sinks%2C%20value-state%0Adrains%2C%20and%20residual-state%20peaks%2C%20collectively%20referred%20to%20as%20extreme-token%0Aphenomena.%20These%20phenomena%20are%20characterized%20by%20certain%20so-called%20%22sink%20tokens%22%0Areceiving%20disproportionately%20high%20attention%20weights%2C%20exhibiting%20significantly%0Asmaller%20value%20states%2C%20and%20having%20much%20larger%20residual-state%20norms%20than%20those%20of%0Aother%20tokens.%20These%20extreme%20tokens%20give%20rise%20to%20various%20challenges%20in%20LLM%0Ainference%2C%20quantization%2C%20and%20interpretability.%0A%20%20We%20elucidate%20the%20mechanisms%20behind%20extreme-token%20phenomena.%20First%2C%20we%20show%0Athat%20these%20phenomena%20arise%20in%20very%20simple%20architectures%20--%20transformers%20with%0Aone%20to%20three%20layers%20--%20trained%20on%20a%20toy%20model%2C%20the%20Bigram-Backcopy%20%28BB%29%20task.%0AIn%20this%20setting%2C%20we%20identify%20an%20active-dormant%20mechanism%2C%20where%20attention%20heads%0Abecome%20sinks%20for%20specific%20input%20domains%20while%20remaining%20non-sinks%20for%20others.%0AOur%20theoretical%20analysis%20of%20the%20training%20dynamics%20reveals%20that%20these%20phenomena%0Aare%20driven%20by%20a%20mutual%20reinforcement%20mechanism.%20Building%20on%20these%20insights%2C%20we%0Apropose%20strategies%20to%20mitigate%20extreme-token%20phenomena%20during%20pretraining%2C%0Aincluding%20replacing%20softmax%20with%20ReLU%20and%20Adam%20with%20SGD.%20Next%2C%20we%20extend%20our%0Aanalysis%20to%20pretrained%20LLMs%2C%20including%20Llama%20and%20OLMo%2C%20showing%20that%20many%0Aattention%20heads%20exhibit%20a%20similar%20active-dormant%20mechanism%20as%20in%20the%20BB%20task%2C%0Aand%20that%20the%20mutual%20reinforcement%20mechanism%20also%20governs%20the%20emergence%20of%0Aextreme-token%20phenomena%20during%20LLM%20pretraining.%20Our%20results%20reveal%20that%20many%20of%0Athe%20static%20and%20dynamic%20properties%20of%20extreme-token%20phenomena%20predicted%20by%20the%0ABB%20task%20align%20with%20observations%20in%20pretrained%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13835v1&entry.124074799=Read"},
{"title": "DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise\n  Motion Control", "author": "Yujie Wei and Shiwei Zhang and Hangjie Yuan and Xiang Wang and Haonan Qiu and Rui Zhao and Yutong Feng and Feng Liu and Zhizhong Huang and Jiaxin Ye and Yingya Zhang and Hongming Shan", "abstract": "  Recent advances in customized video generation have enabled users to create\nvideos tailored to both specific subjects and motion trajectories. However,\nexisting methods often require complicated test-time fine-tuning and struggle\nwith balancing subject learning and motion control, limiting their real-world\napplications. In this paper, we present DreamVideo-2, a zero-shot video\ncustomization framework capable of generating videos with a specific subject\nand motion trajectory, guided by a single image and a bounding box sequence,\nrespectively, and without the need for test-time fine-tuning. Specifically, we\nintroduce reference attention, which leverages the model's inherent\ncapabilities for subject learning, and devise a mask-guided motion module to\nachieve precise motion control by fully utilizing the robust motion signal of\nbox masks derived from bounding boxes. While these two components achieve their\nintended functions, we empirically observe that motion control tends to\ndominate over subject learning. To address this, we propose two key designs: 1)\nthe masked reference attention, which integrates a blended latent mask modeling\nscheme into reference attention to enhance subject representations at the\ndesired positions, and 2) a reweighted diffusion loss, which differentiates the\ncontributions of regions inside and outside the bounding boxes to ensure a\nbalance between subject and motion control. Extensive experimental results on a\nnewly curated dataset demonstrate that DreamVideo-2 outperforms\nstate-of-the-art methods in both subject customization and motion control. The\ndataset, code, and models will be made publicly available.\n", "link": "http://arxiv.org/abs/2410.13830v1", "date": "2024-10-17", "relevancy": 2.685, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7346}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6757}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamVideo-2%3A%20Zero-Shot%20Subject-Driven%20Video%20Customization%20with%20Precise%0A%20%20Motion%20Control&body=Title%3A%20DreamVideo-2%3A%20Zero-Shot%20Subject-Driven%20Video%20Customization%20with%20Precise%0A%20%20Motion%20Control%0AAuthor%3A%20Yujie%20Wei%20and%20Shiwei%20Zhang%20and%20Hangjie%20Yuan%20and%20Xiang%20Wang%20and%20Haonan%20Qiu%20and%20Rui%20Zhao%20and%20Yutong%20Feng%20and%20Feng%20Liu%20and%20Zhizhong%20Huang%20and%20Jiaxin%20Ye%20and%20Yingya%20Zhang%20and%20Hongming%20Shan%0AAbstract%3A%20%20%20Recent%20advances%20in%20customized%20video%20generation%20have%20enabled%20users%20to%20create%0Avideos%20tailored%20to%20both%20specific%20subjects%20and%20motion%20trajectories.%20However%2C%0Aexisting%20methods%20often%20require%20complicated%20test-time%20fine-tuning%20and%20struggle%0Awith%20balancing%20subject%20learning%20and%20motion%20control%2C%20limiting%20their%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20present%20DreamVideo-2%2C%20a%20zero-shot%20video%0Acustomization%20framework%20capable%20of%20generating%20videos%20with%20a%20specific%20subject%0Aand%20motion%20trajectory%2C%20guided%20by%20a%20single%20image%20and%20a%20bounding%20box%20sequence%2C%0Arespectively%2C%20and%20without%20the%20need%20for%20test-time%20fine-tuning.%20Specifically%2C%20we%0Aintroduce%20reference%20attention%2C%20which%20leverages%20the%20model%27s%20inherent%0Acapabilities%20for%20subject%20learning%2C%20and%20devise%20a%20mask-guided%20motion%20module%20to%0Aachieve%20precise%20motion%20control%20by%20fully%20utilizing%20the%20robust%20motion%20signal%20of%0Abox%20masks%20derived%20from%20bounding%20boxes.%20While%20these%20two%20components%20achieve%20their%0Aintended%20functions%2C%20we%20empirically%20observe%20that%20motion%20control%20tends%20to%0Adominate%20over%20subject%20learning.%20To%20address%20this%2C%20we%20propose%20two%20key%20designs%3A%201%29%0Athe%20masked%20reference%20attention%2C%20which%20integrates%20a%20blended%20latent%20mask%20modeling%0Ascheme%20into%20reference%20attention%20to%20enhance%20subject%20representations%20at%20the%0Adesired%20positions%2C%20and%202%29%20a%20reweighted%20diffusion%20loss%2C%20which%20differentiates%20the%0Acontributions%20of%20regions%20inside%20and%20outside%20the%20bounding%20boxes%20to%20ensure%20a%0Abalance%20between%20subject%20and%20motion%20control.%20Extensive%20experimental%20results%20on%20a%0Anewly%20curated%20dataset%20demonstrate%20that%20DreamVideo-2%20outperforms%0Astate-of-the-art%20methods%20in%20both%20subject%20customization%20and%20motion%20control.%20The%0Adataset%2C%20code%2C%20and%20models%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamVideo-2%253A%2520Zero-Shot%2520Subject-Driven%2520Video%2520Customization%2520with%2520Precise%250A%2520%2520Motion%2520Control%26entry.906535625%3DYujie%2520Wei%2520and%2520Shiwei%2520Zhang%2520and%2520Hangjie%2520Yuan%2520and%2520Xiang%2520Wang%2520and%2520Haonan%2520Qiu%2520and%2520Rui%2520Zhao%2520and%2520Yutong%2520Feng%2520and%2520Feng%2520Liu%2520and%2520Zhizhong%2520Huang%2520and%2520Jiaxin%2520Ye%2520and%2520Yingya%2520Zhang%2520and%2520Hongming%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520customized%2520video%2520generation%2520have%2520enabled%2520users%2520to%2520create%250Avideos%2520tailored%2520to%2520both%2520specific%2520subjects%2520and%2520motion%2520trajectories.%2520However%252C%250Aexisting%2520methods%2520often%2520require%2520complicated%2520test-time%2520fine-tuning%2520and%2520struggle%250Awith%2520balancing%2520subject%2520learning%2520and%2520motion%2520control%252C%2520limiting%2520their%2520real-world%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520present%2520DreamVideo-2%252C%2520a%2520zero-shot%2520video%250Acustomization%2520framework%2520capable%2520of%2520generating%2520videos%2520with%2520a%2520specific%2520subject%250Aand%2520motion%2520trajectory%252C%2520guided%2520by%2520a%2520single%2520image%2520and%2520a%2520bounding%2520box%2520sequence%252C%250Arespectively%252C%2520and%2520without%2520the%2520need%2520for%2520test-time%2520fine-tuning.%2520Specifically%252C%2520we%250Aintroduce%2520reference%2520attention%252C%2520which%2520leverages%2520the%2520model%2527s%2520inherent%250Acapabilities%2520for%2520subject%2520learning%252C%2520and%2520devise%2520a%2520mask-guided%2520motion%2520module%2520to%250Aachieve%2520precise%2520motion%2520control%2520by%2520fully%2520utilizing%2520the%2520robust%2520motion%2520signal%2520of%250Abox%2520masks%2520derived%2520from%2520bounding%2520boxes.%2520While%2520these%2520two%2520components%2520achieve%2520their%250Aintended%2520functions%252C%2520we%2520empirically%2520observe%2520that%2520motion%2520control%2520tends%2520to%250Adominate%2520over%2520subject%2520learning.%2520To%2520address%2520this%252C%2520we%2520propose%2520two%2520key%2520designs%253A%25201%2529%250Athe%2520masked%2520reference%2520attention%252C%2520which%2520integrates%2520a%2520blended%2520latent%2520mask%2520modeling%250Ascheme%2520into%2520reference%2520attention%2520to%2520enhance%2520subject%2520representations%2520at%2520the%250Adesired%2520positions%252C%2520and%25202%2529%2520a%2520reweighted%2520diffusion%2520loss%252C%2520which%2520differentiates%2520the%250Acontributions%2520of%2520regions%2520inside%2520and%2520outside%2520the%2520bounding%2520boxes%2520to%2520ensure%2520a%250Abalance%2520between%2520subject%2520and%2520motion%2520control.%2520Extensive%2520experimental%2520results%2520on%2520a%250Anewly%2520curated%2520dataset%2520demonstrate%2520that%2520DreamVideo-2%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520both%2520subject%2520customization%2520and%2520motion%2520control.%2520The%250Adataset%252C%2520code%252C%2520and%2520models%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamVideo-2%3A%20Zero-Shot%20Subject-Driven%20Video%20Customization%20with%20Precise%0A%20%20Motion%20Control&entry.906535625=Yujie%20Wei%20and%20Shiwei%20Zhang%20and%20Hangjie%20Yuan%20and%20Xiang%20Wang%20and%20Haonan%20Qiu%20and%20Rui%20Zhao%20and%20Yutong%20Feng%20and%20Feng%20Liu%20and%20Zhizhong%20Huang%20and%20Jiaxin%20Ye%20and%20Yingya%20Zhang%20and%20Hongming%20Shan&entry.1292438233=%20%20Recent%20advances%20in%20customized%20video%20generation%20have%20enabled%20users%20to%20create%0Avideos%20tailored%20to%20both%20specific%20subjects%20and%20motion%20trajectories.%20However%2C%0Aexisting%20methods%20often%20require%20complicated%20test-time%20fine-tuning%20and%20struggle%0Awith%20balancing%20subject%20learning%20and%20motion%20control%2C%20limiting%20their%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20present%20DreamVideo-2%2C%20a%20zero-shot%20video%0Acustomization%20framework%20capable%20of%20generating%20videos%20with%20a%20specific%20subject%0Aand%20motion%20trajectory%2C%20guided%20by%20a%20single%20image%20and%20a%20bounding%20box%20sequence%2C%0Arespectively%2C%20and%20without%20the%20need%20for%20test-time%20fine-tuning.%20Specifically%2C%20we%0Aintroduce%20reference%20attention%2C%20which%20leverages%20the%20model%27s%20inherent%0Acapabilities%20for%20subject%20learning%2C%20and%20devise%20a%20mask-guided%20motion%20module%20to%0Aachieve%20precise%20motion%20control%20by%20fully%20utilizing%20the%20robust%20motion%20signal%20of%0Abox%20masks%20derived%20from%20bounding%20boxes.%20While%20these%20two%20components%20achieve%20their%0Aintended%20functions%2C%20we%20empirically%20observe%20that%20motion%20control%20tends%20to%0Adominate%20over%20subject%20learning.%20To%20address%20this%2C%20we%20propose%20two%20key%20designs%3A%201%29%0Athe%20masked%20reference%20attention%2C%20which%20integrates%20a%20blended%20latent%20mask%20modeling%0Ascheme%20into%20reference%20attention%20to%20enhance%20subject%20representations%20at%20the%0Adesired%20positions%2C%20and%202%29%20a%20reweighted%20diffusion%20loss%2C%20which%20differentiates%20the%0Acontributions%20of%20regions%20inside%20and%20outside%20the%20bounding%20boxes%20to%20ensure%20a%0Abalance%20between%20subject%20and%20motion%20control.%20Extensive%20experimental%20results%20on%20a%0Anewly%20curated%20dataset%20demonstrate%20that%20DreamVideo-2%20outperforms%0Astate-of-the-art%20methods%20in%20both%20subject%20customization%20and%20motion%20control.%20The%0Adataset%2C%20code%2C%20and%20models%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13830v1&entry.124074799=Read"},
{"title": "LieRE: Generalizing Rotary Position Encodings", "author": "Sophie Ostmeier and Brian Axelrod and Michael E. Moseley and Akshay Chaudhari and Curtis Langlotz", "abstract": "  While Rotary Position Embeddings (RoPE) for large language models have become\nwidely adopted, their application for other modalities has been slower. Here,\nwe introduce Lie group Relative position Encodings (LieRE) that goes beyond\nRoPE in supporting n-dimensional inputs. We evaluate the performance of LieRE\non 2D and 3D image classification tasks and observe that LieRE leads to marked\nrelative improvements in performance (up to 9.7% for 2D and up to 25.5% for\n3D), training efficiency (3.5x reduction), data efficiency (30%) compared to\nthe baselines of DeiT III, RoPE-Mixed and Vision-Llama.\nhttps://github.com/Stanford-AIMI/LieRE\n", "link": "http://arxiv.org/abs/2406.10322v2", "date": "2024-10-17", "relevancy": 2.6449, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LieRE%3A%20Generalizing%20Rotary%20Position%20Encodings&body=Title%3A%20LieRE%3A%20Generalizing%20Rotary%20Position%20Encodings%0AAuthor%3A%20Sophie%20Ostmeier%20and%20Brian%20Axelrod%20and%20Michael%20E.%20Moseley%20and%20Akshay%20Chaudhari%20and%20Curtis%20Langlotz%0AAbstract%3A%20%20%20While%20Rotary%20Position%20Embeddings%20%28RoPE%29%20for%20large%20language%20models%20have%20become%0Awidely%20adopted%2C%20their%20application%20for%20other%20modalities%20has%20been%20slower.%20Here%2C%0Awe%20introduce%20Lie%20group%20Relative%20position%20Encodings%20%28LieRE%29%20that%20goes%20beyond%0ARoPE%20in%20supporting%20n-dimensional%20inputs.%20We%20evaluate%20the%20performance%20of%20LieRE%0Aon%202D%20and%203D%20image%20classification%20tasks%20and%20observe%20that%20LieRE%20leads%20to%20marked%0Arelative%20improvements%20in%20performance%20%28up%20to%209.7%25%20for%202D%20and%20up%20to%2025.5%25%20for%0A3D%29%2C%20training%20efficiency%20%283.5x%20reduction%29%2C%20data%20efficiency%20%2830%25%29%20compared%20to%0Athe%20baselines%20of%20DeiT%20III%2C%20RoPE-Mixed%20and%20Vision-Llama.%0Ahttps%3A//github.com/Stanford-AIMI/LieRE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLieRE%253A%2520Generalizing%2520Rotary%2520Position%2520Encodings%26entry.906535625%3DSophie%2520Ostmeier%2520and%2520Brian%2520Axelrod%2520and%2520Michael%2520E.%2520Moseley%2520and%2520Akshay%2520Chaudhari%2520and%2520Curtis%2520Langlotz%26entry.1292438233%3D%2520%2520While%2520Rotary%2520Position%2520Embeddings%2520%2528RoPE%2529%2520for%2520large%2520language%2520models%2520have%2520become%250Awidely%2520adopted%252C%2520their%2520application%2520for%2520other%2520modalities%2520has%2520been%2520slower.%2520Here%252C%250Awe%2520introduce%2520Lie%2520group%2520Relative%2520position%2520Encodings%2520%2528LieRE%2529%2520that%2520goes%2520beyond%250ARoPE%2520in%2520supporting%2520n-dimensional%2520inputs.%2520We%2520evaluate%2520the%2520performance%2520of%2520LieRE%250Aon%25202D%2520and%25203D%2520image%2520classification%2520tasks%2520and%2520observe%2520that%2520LieRE%2520leads%2520to%2520marked%250Arelative%2520improvements%2520in%2520performance%2520%2528up%2520to%25209.7%2525%2520for%25202D%2520and%2520up%2520to%252025.5%2525%2520for%250A3D%2529%252C%2520training%2520efficiency%2520%25283.5x%2520reduction%2529%252C%2520data%2520efficiency%2520%252830%2525%2529%2520compared%2520to%250Athe%2520baselines%2520of%2520DeiT%2520III%252C%2520RoPE-Mixed%2520and%2520Vision-Llama.%250Ahttps%253A//github.com/Stanford-AIMI/LieRE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LieRE%3A%20Generalizing%20Rotary%20Position%20Encodings&entry.906535625=Sophie%20Ostmeier%20and%20Brian%20Axelrod%20and%20Michael%20E.%20Moseley%20and%20Akshay%20Chaudhari%20and%20Curtis%20Langlotz&entry.1292438233=%20%20While%20Rotary%20Position%20Embeddings%20%28RoPE%29%20for%20large%20language%20models%20have%20become%0Awidely%20adopted%2C%20their%20application%20for%20other%20modalities%20has%20been%20slower.%20Here%2C%0Awe%20introduce%20Lie%20group%20Relative%20position%20Encodings%20%28LieRE%29%20that%20goes%20beyond%0ARoPE%20in%20supporting%20n-dimensional%20inputs.%20We%20evaluate%20the%20performance%20of%20LieRE%0Aon%202D%20and%203D%20image%20classification%20tasks%20and%20observe%20that%20LieRE%20leads%20to%20marked%0Arelative%20improvements%20in%20performance%20%28up%20to%209.7%25%20for%202D%20and%20up%20to%2025.5%25%20for%0A3D%29%2C%20training%20efficiency%20%283.5x%20reduction%29%2C%20data%20efficiency%20%2830%25%29%20compared%20to%0Athe%20baselines%20of%20DeiT%20III%2C%20RoPE-Mixed%20and%20Vision-Llama.%0Ahttps%3A//github.com/Stanford-AIMI/LieRE%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10322v2&entry.124074799=Read"},
{"title": "Many-Shot In-Context Learning", "author": "Rishabh Agarwal and Avi Singh and Lei M. Zhang and Bernd Bohnet and Luis Rosias and Stephanie Chan and Biao Zhang and Ankesh Anand and Zaheer Abbas and Azade Nova and John D. Co-Reyes and Eric Chu and Feryal Behbahani and Aleksandra Faust and Hugo Larochelle", "abstract": "  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.\n", "link": "http://arxiv.org/abs/2404.11018v3", "date": "2024-10-17", "relevancy": 2.5938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Many-Shot%20In-Context%20Learning&body=Title%3A%20Many-Shot%20In-Context%20Learning%0AAuthor%3A%20Rishabh%20Agarwal%20and%20Avi%20Singh%20and%20Lei%20M.%20Zhang%20and%20Bernd%20Bohnet%20and%20Luis%20Rosias%20and%20Stephanie%20Chan%20and%20Biao%20Zhang%20and%20Ankesh%20Anand%20and%20Zaheer%20Abbas%20and%20Azade%20Nova%20and%20John%20D.%20Co-Reyes%20and%20Eric%20Chu%20and%20Feryal%20Behbahani%20and%20Aleksandra%20Faust%20and%20Hugo%20Larochelle%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20few-shot%20in-context%20learning%20%28ICL%29%20--%0Alearning%20from%20a%20few%20examples%20provided%20in%20context%20at%20inference%2C%20without%20any%0Aweight%20updates.%20Newly%20expanded%20context%20windows%20allow%20us%20to%20investigate%20ICL%20with%0Ahundreds%20or%20thousands%20of%20examples%20--%20the%20many-shot%20regime.%20Going%20from%20few-shot%0Ato%20many-shot%2C%20we%20observe%20significant%20performance%20gains%20across%20a%20wide%20variety%20of%0Agenerative%20and%20discriminative%20tasks.%20While%20promising%2C%20many-shot%20ICL%20can%20be%0Abottlenecked%20by%20the%20available%20amount%20of%20human-generated%20examples.%20To%20mitigate%0Athis%20limitation%2C%20we%20explore%20two%20new%20settings%3A%20Reinforced%20and%20Unsupervised%20ICL.%0AReinforced%20ICL%20uses%20model-generated%20chain-of-thought%20rationales%20in%20place%20of%0Ahuman%20examples.%20Unsupervised%20ICL%20removes%20rationales%20from%20the%20prompt%20altogether%2C%0Aand%20prompts%20the%20model%20only%20with%20domain-specific%20questions.%20We%20find%20that%20both%0AReinforced%20and%20Unsupervised%20ICL%20can%20be%20quite%20effective%20in%20the%20many-shot%20regime%2C%0Aparticularly%20on%20complex%20reasoning%20tasks.%20Finally%2C%20we%20demonstrate%20that%2C%20unlike%0Afew-shot%20learning%2C%20many-shot%20learning%20is%20effective%20at%20overriding%20pretraining%0Abiases%2C%20can%20learn%20high-dimensional%20functions%20with%20numerical%20inputs%2C%20and%0Aperforms%20comparably%20to%20fine-tuning.%20We%20also%20find%20that%20inference%20cost%20increases%0Alinearly%20in%20the%20many-shot%20regime%2C%20and%20frontier%20LLMs%20benefit%20from%20many-shot%20ICL%0Ato%20varying%20degrees.%20Our%20analysis%20also%20reveals%20the%20limitations%20of%20next-token%0Aprediction%20loss%20as%20an%20indicator%20of%20downstream%20ICL%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11018v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMany-Shot%2520In-Context%2520Learning%26entry.906535625%3DRishabh%2520Agarwal%2520and%2520Avi%2520Singh%2520and%2520Lei%2520M.%2520Zhang%2520and%2520Bernd%2520Bohnet%2520and%2520Luis%2520Rosias%2520and%2520Stephanie%2520Chan%2520and%2520Biao%2520Zhang%2520and%2520Ankesh%2520Anand%2520and%2520Zaheer%2520Abbas%2520and%2520Azade%2520Nova%2520and%2520John%2520D.%2520Co-Reyes%2520and%2520Eric%2520Chu%2520and%2520Feryal%2520Behbahani%2520and%2520Aleksandra%2520Faust%2520and%2520Hugo%2520Larochelle%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520few-shot%2520in-context%2520learning%2520%2528ICL%2529%2520--%250Alearning%2520from%2520a%2520few%2520examples%2520provided%2520in%2520context%2520at%2520inference%252C%2520without%2520any%250Aweight%2520updates.%2520Newly%2520expanded%2520context%2520windows%2520allow%2520us%2520to%2520investigate%2520ICL%2520with%250Ahundreds%2520or%2520thousands%2520of%2520examples%2520--%2520the%2520many-shot%2520regime.%2520Going%2520from%2520few-shot%250Ato%2520many-shot%252C%2520we%2520observe%2520significant%2520performance%2520gains%2520across%2520a%2520wide%2520variety%2520of%250Agenerative%2520and%2520discriminative%2520tasks.%2520While%2520promising%252C%2520many-shot%2520ICL%2520can%2520be%250Abottlenecked%2520by%2520the%2520available%2520amount%2520of%2520human-generated%2520examples.%2520To%2520mitigate%250Athis%2520limitation%252C%2520we%2520explore%2520two%2520new%2520settings%253A%2520Reinforced%2520and%2520Unsupervised%2520ICL.%250AReinforced%2520ICL%2520uses%2520model-generated%2520chain-of-thought%2520rationales%2520in%2520place%2520of%250Ahuman%2520examples.%2520Unsupervised%2520ICL%2520removes%2520rationales%2520from%2520the%2520prompt%2520altogether%252C%250Aand%2520prompts%2520the%2520model%2520only%2520with%2520domain-specific%2520questions.%2520We%2520find%2520that%2520both%250AReinforced%2520and%2520Unsupervised%2520ICL%2520can%2520be%2520quite%2520effective%2520in%2520the%2520many-shot%2520regime%252C%250Aparticularly%2520on%2520complex%2520reasoning%2520tasks.%2520Finally%252C%2520we%2520demonstrate%2520that%252C%2520unlike%250Afew-shot%2520learning%252C%2520many-shot%2520learning%2520is%2520effective%2520at%2520overriding%2520pretraining%250Abiases%252C%2520can%2520learn%2520high-dimensional%2520functions%2520with%2520numerical%2520inputs%252C%2520and%250Aperforms%2520comparably%2520to%2520fine-tuning.%2520We%2520also%2520find%2520that%2520inference%2520cost%2520increases%250Alinearly%2520in%2520the%2520many-shot%2520regime%252C%2520and%2520frontier%2520LLMs%2520benefit%2520from%2520many-shot%2520ICL%250Ato%2520varying%2520degrees.%2520Our%2520analysis%2520also%2520reveals%2520the%2520limitations%2520of%2520next-token%250Aprediction%2520loss%2520as%2520an%2520indicator%2520of%2520downstream%2520ICL%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11018v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Many-Shot%20In-Context%20Learning&entry.906535625=Rishabh%20Agarwal%20and%20Avi%20Singh%20and%20Lei%20M.%20Zhang%20and%20Bernd%20Bohnet%20and%20Luis%20Rosias%20and%20Stephanie%20Chan%20and%20Biao%20Zhang%20and%20Ankesh%20Anand%20and%20Zaheer%20Abbas%20and%20Azade%20Nova%20and%20John%20D.%20Co-Reyes%20and%20Eric%20Chu%20and%20Feryal%20Behbahani%20and%20Aleksandra%20Faust%20and%20Hugo%20Larochelle&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20few-shot%20in-context%20learning%20%28ICL%29%20--%0Alearning%20from%20a%20few%20examples%20provided%20in%20context%20at%20inference%2C%20without%20any%0Aweight%20updates.%20Newly%20expanded%20context%20windows%20allow%20us%20to%20investigate%20ICL%20with%0Ahundreds%20or%20thousands%20of%20examples%20--%20the%20many-shot%20regime.%20Going%20from%20few-shot%0Ato%20many-shot%2C%20we%20observe%20significant%20performance%20gains%20across%20a%20wide%20variety%20of%0Agenerative%20and%20discriminative%20tasks.%20While%20promising%2C%20many-shot%20ICL%20can%20be%0Abottlenecked%20by%20the%20available%20amount%20of%20human-generated%20examples.%20To%20mitigate%0Athis%20limitation%2C%20we%20explore%20two%20new%20settings%3A%20Reinforced%20and%20Unsupervised%20ICL.%0AReinforced%20ICL%20uses%20model-generated%20chain-of-thought%20rationales%20in%20place%20of%0Ahuman%20examples.%20Unsupervised%20ICL%20removes%20rationales%20from%20the%20prompt%20altogether%2C%0Aand%20prompts%20the%20model%20only%20with%20domain-specific%20questions.%20We%20find%20that%20both%0AReinforced%20and%20Unsupervised%20ICL%20can%20be%20quite%20effective%20in%20the%20many-shot%20regime%2C%0Aparticularly%20on%20complex%20reasoning%20tasks.%20Finally%2C%20we%20demonstrate%20that%2C%20unlike%0Afew-shot%20learning%2C%20many-shot%20learning%20is%20effective%20at%20overriding%20pretraining%0Abiases%2C%20can%20learn%20high-dimensional%20functions%20with%20numerical%20inputs%2C%20and%0Aperforms%20comparably%20to%20fine-tuning.%20We%20also%20find%20that%20inference%20cost%20increases%0Alinearly%20in%20the%20many-shot%20regime%2C%20and%20frontier%20LLMs%20benefit%20from%20many-shot%20ICL%0Ato%20varying%20degrees.%20Our%20analysis%20also%20reveals%20the%20limitations%20of%20next-token%0Aprediction%20loss%20as%20an%20indicator%20of%20downstream%20ICL%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11018v3&entry.124074799=Read"},
{"title": "Stratified Domain Adaptation: A Progressive Self-Training Approach for\n  Scene Text Recognition", "author": "Kha Nhat Le and Hoang-Tuan Nguyen and Hung Tien Tran and Thanh Duc Ngo", "abstract": "  Unsupervised domain adaptation (UDA) has become increasingly prevalent in\nscene text recognition (STR), especially where training and testing data reside\nin different domains. The efficacy of existing UDA approaches tends to degrade\nwhen there is a large gap between the source and target domains. To deal with\nthis problem, gradually shifting or progressively learning to shift from domain\nto domain is the key issue. In this paper, we introduce the Stratified Domain\nAdaptation (StrDA) approach, which examines the gradual escalation of the\ndomain gap for the learning process. The objective is to partition the training\ndata into subsets so that the progressively self-trained model can adapt to\ngradual changes. We stratify the training data by evaluating the proximity of\neach data sample to both the source and target domains. We propose a novel\nmethod for employing domain discriminators to estimate the out-of-distribution\nand domain discriminative levels of data samples. Extensive experiments on\nbenchmark scene-text datasets show that our approach significantly improves the\nperformance of baseline (source-trained) STR models.\n", "link": "http://arxiv.org/abs/2410.09913v2", "date": "2024-10-17", "relevancy": 2.5912, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stratified%20Domain%20Adaptation%3A%20A%20Progressive%20Self-Training%20Approach%20for%0A%20%20Scene%20Text%20Recognition&body=Title%3A%20Stratified%20Domain%20Adaptation%3A%20A%20Progressive%20Self-Training%20Approach%20for%0A%20%20Scene%20Text%20Recognition%0AAuthor%3A%20Kha%20Nhat%20Le%20and%20Hoang-Tuan%20Nguyen%20and%20Hung%20Tien%20Tran%20and%20Thanh%20Duc%20Ngo%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20has%20become%20increasingly%20prevalent%20in%0Ascene%20text%20recognition%20%28STR%29%2C%20especially%20where%20training%20and%20testing%20data%20reside%0Ain%20different%20domains.%20The%20efficacy%20of%20existing%20UDA%20approaches%20tends%20to%20degrade%0Awhen%20there%20is%20a%20large%20gap%20between%20the%20source%20and%20target%20domains.%20To%20deal%20with%0Athis%20problem%2C%20gradually%20shifting%20or%20progressively%20learning%20to%20shift%20from%20domain%0Ato%20domain%20is%20the%20key%20issue.%20In%20this%20paper%2C%20we%20introduce%20the%20Stratified%20Domain%0AAdaptation%20%28StrDA%29%20approach%2C%20which%20examines%20the%20gradual%20escalation%20of%20the%0Adomain%20gap%20for%20the%20learning%20process.%20The%20objective%20is%20to%20partition%20the%20training%0Adata%20into%20subsets%20so%20that%20the%20progressively%20self-trained%20model%20can%20adapt%20to%0Agradual%20changes.%20We%20stratify%20the%20training%20data%20by%20evaluating%20the%20proximity%20of%0Aeach%20data%20sample%20to%20both%20the%20source%20and%20target%20domains.%20We%20propose%20a%20novel%0Amethod%20for%20employing%20domain%20discriminators%20to%20estimate%20the%20out-of-distribution%0Aand%20domain%20discriminative%20levels%20of%20data%20samples.%20Extensive%20experiments%20on%0Abenchmark%20scene-text%20datasets%20show%20that%20our%20approach%20significantly%20improves%20the%0Aperformance%20of%20baseline%20%28source-trained%29%20STR%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStratified%2520Domain%2520Adaptation%253A%2520A%2520Progressive%2520Self-Training%2520Approach%2520for%250A%2520%2520Scene%2520Text%2520Recognition%26entry.906535625%3DKha%2520Nhat%2520Le%2520and%2520Hoang-Tuan%2520Nguyen%2520and%2520Hung%2520Tien%2520Tran%2520and%2520Thanh%2520Duc%2520Ngo%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520has%2520become%2520increasingly%2520prevalent%2520in%250Ascene%2520text%2520recognition%2520%2528STR%2529%252C%2520especially%2520where%2520training%2520and%2520testing%2520data%2520reside%250Ain%2520different%2520domains.%2520The%2520efficacy%2520of%2520existing%2520UDA%2520approaches%2520tends%2520to%2520degrade%250Awhen%2520there%2520is%2520a%2520large%2520gap%2520between%2520the%2520source%2520and%2520target%2520domains.%2520To%2520deal%2520with%250Athis%2520problem%252C%2520gradually%2520shifting%2520or%2520progressively%2520learning%2520to%2520shift%2520from%2520domain%250Ato%2520domain%2520is%2520the%2520key%2520issue.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Stratified%2520Domain%250AAdaptation%2520%2528StrDA%2529%2520approach%252C%2520which%2520examines%2520the%2520gradual%2520escalation%2520of%2520the%250Adomain%2520gap%2520for%2520the%2520learning%2520process.%2520The%2520objective%2520is%2520to%2520partition%2520the%2520training%250Adata%2520into%2520subsets%2520so%2520that%2520the%2520progressively%2520self-trained%2520model%2520can%2520adapt%2520to%250Agradual%2520changes.%2520We%2520stratify%2520the%2520training%2520data%2520by%2520evaluating%2520the%2520proximity%2520of%250Aeach%2520data%2520sample%2520to%2520both%2520the%2520source%2520and%2520target%2520domains.%2520We%2520propose%2520a%2520novel%250Amethod%2520for%2520employing%2520domain%2520discriminators%2520to%2520estimate%2520the%2520out-of-distribution%250Aand%2520domain%2520discriminative%2520levels%2520of%2520data%2520samples.%2520Extensive%2520experiments%2520on%250Abenchmark%2520scene-text%2520datasets%2520show%2520that%2520our%2520approach%2520significantly%2520improves%2520the%250Aperformance%2520of%2520baseline%2520%2528source-trained%2529%2520STR%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stratified%20Domain%20Adaptation%3A%20A%20Progressive%20Self-Training%20Approach%20for%0A%20%20Scene%20Text%20Recognition&entry.906535625=Kha%20Nhat%20Le%20and%20Hoang-Tuan%20Nguyen%20and%20Hung%20Tien%20Tran%20and%20Thanh%20Duc%20Ngo&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20has%20become%20increasingly%20prevalent%20in%0Ascene%20text%20recognition%20%28STR%29%2C%20especially%20where%20training%20and%20testing%20data%20reside%0Ain%20different%20domains.%20The%20efficacy%20of%20existing%20UDA%20approaches%20tends%20to%20degrade%0Awhen%20there%20is%20a%20large%20gap%20between%20the%20source%20and%20target%20domains.%20To%20deal%20with%0Athis%20problem%2C%20gradually%20shifting%20or%20progressively%20learning%20to%20shift%20from%20domain%0Ato%20domain%20is%20the%20key%20issue.%20In%20this%20paper%2C%20we%20introduce%20the%20Stratified%20Domain%0AAdaptation%20%28StrDA%29%20approach%2C%20which%20examines%20the%20gradual%20escalation%20of%20the%0Adomain%20gap%20for%20the%20learning%20process.%20The%20objective%20is%20to%20partition%20the%20training%0Adata%20into%20subsets%20so%20that%20the%20progressively%20self-trained%20model%20can%20adapt%20to%0Agradual%20changes.%20We%20stratify%20the%20training%20data%20by%20evaluating%20the%20proximity%20of%0Aeach%20data%20sample%20to%20both%20the%20source%20and%20target%20domains.%20We%20propose%20a%20novel%0Amethod%20for%20employing%20domain%20discriminators%20to%20estimate%20the%20out-of-distribution%0Aand%20domain%20discriminative%20levels%20of%20data%20samples.%20Extensive%20experiments%20on%0Abenchmark%20scene-text%20datasets%20show%20that%20our%20approach%20significantly%20improves%20the%0Aperformance%20of%20baseline%20%28source-trained%29%20STR%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09913v2&entry.124074799=Read"},
{"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks", "author": "Georgios Chochlakis and Niyantha Maruthu Pandiyan and Kristina Lerman and Shrikanth Narayanan", "abstract": "  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.\n", "link": "http://arxiv.org/abs/2409.06173v3", "date": "2024-10-17", "relevancy": 2.59, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Larger%20Language%20Models%20Don%27t%20Care%20How%20You%20Think%3A%20Why%20Chain-of-Thought%0A%20%20Prompting%20Fails%20in%20Subjective%20Tasks&body=Title%3A%20Larger%20Language%20Models%20Don%27t%20Care%20How%20You%20Think%3A%20Why%20Chain-of-Thought%0A%20%20Prompting%20Fails%20in%20Subjective%20Tasks%0AAuthor%3A%20Georgios%20Chochlakis%20and%20Niyantha%20Maruthu%20Pandiyan%20and%20Kristina%20Lerman%20and%20Shrikanth%20Narayanan%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20in%20Large%20Language%20Models%20%28LLM%29%20has%20emerged%20as%20the%0Adominant%20technique%20for%20performing%20natural%20language%20tasks%2C%20as%20it%20does%20not%0Arequire%20updating%20the%20model%20parameters%20with%20gradient-based%20methods.%20ICL%20promises%0Ato%20%22adapt%22%20the%20LLM%20to%20perform%20the%20present%20task%20at%20a%20competitive%20or%0Astate-of-the-art%20level%20at%20a%20fraction%20of%20the%20computational%20cost.%20ICL%20can%20be%0Aaugmented%20by%20incorporating%20the%20reasoning%20process%20to%20arrive%20at%20the%20final%20label%0Aexplicitly%20in%20the%20prompt%2C%20a%20technique%20called%20Chain-of-Thought%20%28CoT%29%20prompting.%0AHowever%2C%20recent%20work%20has%20found%20that%20ICL%20relies%20mostly%20on%20the%20retrieval%20of%20task%0Apriors%20and%20less%20so%20on%20%22learning%22%20to%20perform%20tasks%2C%20especially%20for%20complex%0Asubjective%20domains%20like%20emotion%20and%20morality%2C%20where%20priors%20ossify%20posterior%0Apredictions.%20In%20this%20work%2C%20we%20examine%20whether%20%22enabling%22%20reasoning%20also%20creates%0Athe%20same%20behavior%20in%20LLMs%2C%20wherein%20the%20format%20of%20CoT%20retrieves%20reasoning%20priors%0Athat%20remain%20relatively%20unchanged%20despite%20the%20evidence%20in%20the%20prompt.%20We%20find%0Athat%2C%20surprisingly%2C%20CoT%20indeed%20suffers%20from%20the%20same%20posterior%20collapse%20as%20ICL%0Afor%20larger%20language%20models.%20Code%20is%20avalaible%20at%0Ahttps%3A//github.com/gchochla/cot-priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarger%2520Language%2520Models%2520Don%2527t%2520Care%2520How%2520You%2520Think%253A%2520Why%2520Chain-of-Thought%250A%2520%2520Prompting%2520Fails%2520in%2520Subjective%2520Tasks%26entry.906535625%3DGeorgios%2520Chochlakis%2520and%2520Niyantha%2520Maruthu%2520Pandiyan%2520and%2520Kristina%2520Lerman%2520and%2520Shrikanth%2520Narayanan%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520in%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520has%2520emerged%2520as%2520the%250Adominant%2520technique%2520for%2520performing%2520natural%2520language%2520tasks%252C%2520as%2520it%2520does%2520not%250Arequire%2520updating%2520the%2520model%2520parameters%2520with%2520gradient-based%2520methods.%2520ICL%2520promises%250Ato%2520%2522adapt%2522%2520the%2520LLM%2520to%2520perform%2520the%2520present%2520task%2520at%2520a%2520competitive%2520or%250Astate-of-the-art%2520level%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520cost.%2520ICL%2520can%2520be%250Aaugmented%2520by%2520incorporating%2520the%2520reasoning%2520process%2520to%2520arrive%2520at%2520the%2520final%2520label%250Aexplicitly%2520in%2520the%2520prompt%252C%2520a%2520technique%2520called%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting.%250AHowever%252C%2520recent%2520work%2520has%2520found%2520that%2520ICL%2520relies%2520mostly%2520on%2520the%2520retrieval%2520of%2520task%250Apriors%2520and%2520less%2520so%2520on%2520%2522learning%2522%2520to%2520perform%2520tasks%252C%2520especially%2520for%2520complex%250Asubjective%2520domains%2520like%2520emotion%2520and%2520morality%252C%2520where%2520priors%2520ossify%2520posterior%250Apredictions.%2520In%2520this%2520work%252C%2520we%2520examine%2520whether%2520%2522enabling%2522%2520reasoning%2520also%2520creates%250Athe%2520same%2520behavior%2520in%2520LLMs%252C%2520wherein%2520the%2520format%2520of%2520CoT%2520retrieves%2520reasoning%2520priors%250Athat%2520remain%2520relatively%2520unchanged%2520despite%2520the%2520evidence%2520in%2520the%2520prompt.%2520We%2520find%250Athat%252C%2520surprisingly%252C%2520CoT%2520indeed%2520suffers%2520from%2520the%2520same%2520posterior%2520collapse%2520as%2520ICL%250Afor%2520larger%2520language%2520models.%2520Code%2520is%2520avalaible%2520at%250Ahttps%253A//github.com/gchochla/cot-priors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Larger%20Language%20Models%20Don%27t%20Care%20How%20You%20Think%3A%20Why%20Chain-of-Thought%0A%20%20Prompting%20Fails%20in%20Subjective%20Tasks&entry.906535625=Georgios%20Chochlakis%20and%20Niyantha%20Maruthu%20Pandiyan%20and%20Kristina%20Lerman%20and%20Shrikanth%20Narayanan&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20in%20Large%20Language%20Models%20%28LLM%29%20has%20emerged%20as%20the%0Adominant%20technique%20for%20performing%20natural%20language%20tasks%2C%20as%20it%20does%20not%0Arequire%20updating%20the%20model%20parameters%20with%20gradient-based%20methods.%20ICL%20promises%0Ato%20%22adapt%22%20the%20LLM%20to%20perform%20the%20present%20task%20at%20a%20competitive%20or%0Astate-of-the-art%20level%20at%20a%20fraction%20of%20the%20computational%20cost.%20ICL%20can%20be%0Aaugmented%20by%20incorporating%20the%20reasoning%20process%20to%20arrive%20at%20the%20final%20label%0Aexplicitly%20in%20the%20prompt%2C%20a%20technique%20called%20Chain-of-Thought%20%28CoT%29%20prompting.%0AHowever%2C%20recent%20work%20has%20found%20that%20ICL%20relies%20mostly%20on%20the%20retrieval%20of%20task%0Apriors%20and%20less%20so%20on%20%22learning%22%20to%20perform%20tasks%2C%20especially%20for%20complex%0Asubjective%20domains%20like%20emotion%20and%20morality%2C%20where%20priors%20ossify%20posterior%0Apredictions.%20In%20this%20work%2C%20we%20examine%20whether%20%22enabling%22%20reasoning%20also%20creates%0Athe%20same%20behavior%20in%20LLMs%2C%20wherein%20the%20format%20of%20CoT%20retrieves%20reasoning%20priors%0Athat%20remain%20relatively%20unchanged%20despite%20the%20evidence%20in%20the%20prompt.%20We%20find%0Athat%2C%20surprisingly%2C%20CoT%20indeed%20suffers%20from%20the%20same%20posterior%20collapse%20as%20ICL%0Afor%20larger%20language%20models.%20Code%20is%20avalaible%20at%0Ahttps%3A//github.com/gchochla/cot-priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06173v3&entry.124074799=Read"},
{"title": "GDeR: Safeguarding Efficiency, Balancing, and Robustness via\n  Prototypical Graph Pruning", "author": "Guibin Zhang and Haonan Dong and Yuchen Zhang and Zhixun Li and Dingshuo Chen and Kai Wang and Tianlong Chen and Yuxuan Liang and Dawei Cheng and Kun Wang", "abstract": "  Training high-quality deep models necessitates vast amounts of data,\nresulting in overwhelming computational and memory demands. Recently, data\npruning, distillation, and coreset selection have been developed to streamline\ndata volume by retaining, synthesizing, or selecting a small yet informative\nsubset from the full set. Among these methods, data pruning incurs the least\nadditional training cost and offers the most practical acceleration benefits.\nHowever, it is the most vulnerable, often suffering significant performance\ndegradation with imbalanced or biased data schema, thus raising concerns about\nits accuracy and reliability in on-device deployment. Therefore, there is a\nlooming need for a new data pruning paradigm that maintains the efficiency of\nprevious practices while ensuring balance and robustness. Unlike the fields of\ncomputer vision and natural language processing, where mature solutions have\nbeen developed to address these issues, graph neural networks (GNNs) continue\nto struggle with increasingly large-scale, imbalanced, and noisy datasets,\nlacking a unified dataset pruning solution. To achieve this, we introduce a\nnovel dynamic soft-pruning method, GDeR, designed to update the training\n``basket'' during the process using trainable prototypes. GDeR first constructs\na well-modeled graph embedding hypersphere and then samples\n\\textit{representative, balanced, and unbiased subsets} from this embedding\nspace, which achieves the goal we called Graph Training Debugging. Extensive\nexperiments on five datasets across three GNN backbones, demonstrate that GDeR\n(I) achieves or surpasses the performance of the full dataset with 30%~50%\nfewer training samples, (II) attains up to a 2.81x lossless training speedup,\nand (III) outperforms state-of-the-art pruning methods in imbalanced training\nand noisy training scenarios by 0.3%~4.3% and 3.6%~7.8%, respectively.\n", "link": "http://arxiv.org/abs/2410.13761v1", "date": "2024-10-17", "relevancy": 2.564, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5285}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5066}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GDeR%3A%20Safeguarding%20Efficiency%2C%20Balancing%2C%20and%20Robustness%20via%0A%20%20Prototypical%20Graph%20Pruning&body=Title%3A%20GDeR%3A%20Safeguarding%20Efficiency%2C%20Balancing%2C%20and%20Robustness%20via%0A%20%20Prototypical%20Graph%20Pruning%0AAuthor%3A%20Guibin%20Zhang%20and%20Haonan%20Dong%20and%20Yuchen%20Zhang%20and%20Zhixun%20Li%20and%20Dingshuo%20Chen%20and%20Kai%20Wang%20and%20Tianlong%20Chen%20and%20Yuxuan%20Liang%20and%20Dawei%20Cheng%20and%20Kun%20Wang%0AAbstract%3A%20%20%20Training%20high-quality%20deep%20models%20necessitates%20vast%20amounts%20of%20data%2C%0Aresulting%20in%20overwhelming%20computational%20and%20memory%20demands.%20Recently%2C%20data%0Apruning%2C%20distillation%2C%20and%20coreset%20selection%20have%20been%20developed%20to%20streamline%0Adata%20volume%20by%20retaining%2C%20synthesizing%2C%20or%20selecting%20a%20small%20yet%20informative%0Asubset%20from%20the%20full%20set.%20Among%20these%20methods%2C%20data%20pruning%20incurs%20the%20least%0Aadditional%20training%20cost%20and%20offers%20the%20most%20practical%20acceleration%20benefits.%0AHowever%2C%20it%20is%20the%20most%20vulnerable%2C%20often%20suffering%20significant%20performance%0Adegradation%20with%20imbalanced%20or%20biased%20data%20schema%2C%20thus%20raising%20concerns%20about%0Aits%20accuracy%20and%20reliability%20in%20on-device%20deployment.%20Therefore%2C%20there%20is%20a%0Alooming%20need%20for%20a%20new%20data%20pruning%20paradigm%20that%20maintains%20the%20efficiency%20of%0Aprevious%20practices%20while%20ensuring%20balance%20and%20robustness.%20Unlike%20the%20fields%20of%0Acomputer%20vision%20and%20natural%20language%20processing%2C%20where%20mature%20solutions%20have%0Abeen%20developed%20to%20address%20these%20issues%2C%20graph%20neural%20networks%20%28GNNs%29%20continue%0Ato%20struggle%20with%20increasingly%20large-scale%2C%20imbalanced%2C%20and%20noisy%20datasets%2C%0Alacking%20a%20unified%20dataset%20pruning%20solution.%20To%20achieve%20this%2C%20we%20introduce%20a%0Anovel%20dynamic%20soft-pruning%20method%2C%20GDeR%2C%20designed%20to%20update%20the%20training%0A%60%60basket%27%27%20during%20the%20process%20using%20trainable%20prototypes.%20GDeR%20first%20constructs%0Aa%20well-modeled%20graph%20embedding%20hypersphere%20and%20then%20samples%0A%5Ctextit%7Brepresentative%2C%20balanced%2C%20and%20unbiased%20subsets%7D%20from%20this%20embedding%0Aspace%2C%20which%20achieves%20the%20goal%20we%20called%20Graph%20Training%20Debugging.%20Extensive%0Aexperiments%20on%20five%20datasets%20across%20three%20GNN%20backbones%2C%20demonstrate%20that%20GDeR%0A%28I%29%20achieves%20or%20surpasses%20the%20performance%20of%20the%20full%20dataset%20with%2030%25~50%25%0Afewer%20training%20samples%2C%20%28II%29%20attains%20up%20to%20a%202.81x%20lossless%20training%20speedup%2C%0Aand%20%28III%29%20outperforms%20state-of-the-art%20pruning%20methods%20in%20imbalanced%20training%0Aand%20noisy%20training%20scenarios%20by%200.3%25~4.3%25%20and%203.6%25~7.8%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGDeR%253A%2520Safeguarding%2520Efficiency%252C%2520Balancing%252C%2520and%2520Robustness%2520via%250A%2520%2520Prototypical%2520Graph%2520Pruning%26entry.906535625%3DGuibin%2520Zhang%2520and%2520Haonan%2520Dong%2520and%2520Yuchen%2520Zhang%2520and%2520Zhixun%2520Li%2520and%2520Dingshuo%2520Chen%2520and%2520Kai%2520Wang%2520and%2520Tianlong%2520Chen%2520and%2520Yuxuan%2520Liang%2520and%2520Dawei%2520Cheng%2520and%2520Kun%2520Wang%26entry.1292438233%3D%2520%2520Training%2520high-quality%2520deep%2520models%2520necessitates%2520vast%2520amounts%2520of%2520data%252C%250Aresulting%2520in%2520overwhelming%2520computational%2520and%2520memory%2520demands.%2520Recently%252C%2520data%250Apruning%252C%2520distillation%252C%2520and%2520coreset%2520selection%2520have%2520been%2520developed%2520to%2520streamline%250Adata%2520volume%2520by%2520retaining%252C%2520synthesizing%252C%2520or%2520selecting%2520a%2520small%2520yet%2520informative%250Asubset%2520from%2520the%2520full%2520set.%2520Among%2520these%2520methods%252C%2520data%2520pruning%2520incurs%2520the%2520least%250Aadditional%2520training%2520cost%2520and%2520offers%2520the%2520most%2520practical%2520acceleration%2520benefits.%250AHowever%252C%2520it%2520is%2520the%2520most%2520vulnerable%252C%2520often%2520suffering%2520significant%2520performance%250Adegradation%2520with%2520imbalanced%2520or%2520biased%2520data%2520schema%252C%2520thus%2520raising%2520concerns%2520about%250Aits%2520accuracy%2520and%2520reliability%2520in%2520on-device%2520deployment.%2520Therefore%252C%2520there%2520is%2520a%250Alooming%2520need%2520for%2520a%2520new%2520data%2520pruning%2520paradigm%2520that%2520maintains%2520the%2520efficiency%2520of%250Aprevious%2520practices%2520while%2520ensuring%2520balance%2520and%2520robustness.%2520Unlike%2520the%2520fields%2520of%250Acomputer%2520vision%2520and%2520natural%2520language%2520processing%252C%2520where%2520mature%2520solutions%2520have%250Abeen%2520developed%2520to%2520address%2520these%2520issues%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520continue%250Ato%2520struggle%2520with%2520increasingly%2520large-scale%252C%2520imbalanced%252C%2520and%2520noisy%2520datasets%252C%250Alacking%2520a%2520unified%2520dataset%2520pruning%2520solution.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%250Anovel%2520dynamic%2520soft-pruning%2520method%252C%2520GDeR%252C%2520designed%2520to%2520update%2520the%2520training%250A%2560%2560basket%2527%2527%2520during%2520the%2520process%2520using%2520trainable%2520prototypes.%2520GDeR%2520first%2520constructs%250Aa%2520well-modeled%2520graph%2520embedding%2520hypersphere%2520and%2520then%2520samples%250A%255Ctextit%257Brepresentative%252C%2520balanced%252C%2520and%2520unbiased%2520subsets%257D%2520from%2520this%2520embedding%250Aspace%252C%2520which%2520achieves%2520the%2520goal%2520we%2520called%2520Graph%2520Training%2520Debugging.%2520Extensive%250Aexperiments%2520on%2520five%2520datasets%2520across%2520three%2520GNN%2520backbones%252C%2520demonstrate%2520that%2520GDeR%250A%2528I%2529%2520achieves%2520or%2520surpasses%2520the%2520performance%2520of%2520the%2520full%2520dataset%2520with%252030%2525~50%2525%250Afewer%2520training%2520samples%252C%2520%2528II%2529%2520attains%2520up%2520to%2520a%25202.81x%2520lossless%2520training%2520speedup%252C%250Aand%2520%2528III%2529%2520outperforms%2520state-of-the-art%2520pruning%2520methods%2520in%2520imbalanced%2520training%250Aand%2520noisy%2520training%2520scenarios%2520by%25200.3%2525~4.3%2525%2520and%25203.6%2525~7.8%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDeR%3A%20Safeguarding%20Efficiency%2C%20Balancing%2C%20and%20Robustness%20via%0A%20%20Prototypical%20Graph%20Pruning&entry.906535625=Guibin%20Zhang%20and%20Haonan%20Dong%20and%20Yuchen%20Zhang%20and%20Zhixun%20Li%20and%20Dingshuo%20Chen%20and%20Kai%20Wang%20and%20Tianlong%20Chen%20and%20Yuxuan%20Liang%20and%20Dawei%20Cheng%20and%20Kun%20Wang&entry.1292438233=%20%20Training%20high-quality%20deep%20models%20necessitates%20vast%20amounts%20of%20data%2C%0Aresulting%20in%20overwhelming%20computational%20and%20memory%20demands.%20Recently%2C%20data%0Apruning%2C%20distillation%2C%20and%20coreset%20selection%20have%20been%20developed%20to%20streamline%0Adata%20volume%20by%20retaining%2C%20synthesizing%2C%20or%20selecting%20a%20small%20yet%20informative%0Asubset%20from%20the%20full%20set.%20Among%20these%20methods%2C%20data%20pruning%20incurs%20the%20least%0Aadditional%20training%20cost%20and%20offers%20the%20most%20practical%20acceleration%20benefits.%0AHowever%2C%20it%20is%20the%20most%20vulnerable%2C%20often%20suffering%20significant%20performance%0Adegradation%20with%20imbalanced%20or%20biased%20data%20schema%2C%20thus%20raising%20concerns%20about%0Aits%20accuracy%20and%20reliability%20in%20on-device%20deployment.%20Therefore%2C%20there%20is%20a%0Alooming%20need%20for%20a%20new%20data%20pruning%20paradigm%20that%20maintains%20the%20efficiency%20of%0Aprevious%20practices%20while%20ensuring%20balance%20and%20robustness.%20Unlike%20the%20fields%20of%0Acomputer%20vision%20and%20natural%20language%20processing%2C%20where%20mature%20solutions%20have%0Abeen%20developed%20to%20address%20these%20issues%2C%20graph%20neural%20networks%20%28GNNs%29%20continue%0Ato%20struggle%20with%20increasingly%20large-scale%2C%20imbalanced%2C%20and%20noisy%20datasets%2C%0Alacking%20a%20unified%20dataset%20pruning%20solution.%20To%20achieve%20this%2C%20we%20introduce%20a%0Anovel%20dynamic%20soft-pruning%20method%2C%20GDeR%2C%20designed%20to%20update%20the%20training%0A%60%60basket%27%27%20during%20the%20process%20using%20trainable%20prototypes.%20GDeR%20first%20constructs%0Aa%20well-modeled%20graph%20embedding%20hypersphere%20and%20then%20samples%0A%5Ctextit%7Brepresentative%2C%20balanced%2C%20and%20unbiased%20subsets%7D%20from%20this%20embedding%0Aspace%2C%20which%20achieves%20the%20goal%20we%20called%20Graph%20Training%20Debugging.%20Extensive%0Aexperiments%20on%20five%20datasets%20across%20three%20GNN%20backbones%2C%20demonstrate%20that%20GDeR%0A%28I%29%20achieves%20or%20surpasses%20the%20performance%20of%20the%20full%20dataset%20with%2030%25~50%25%0Afewer%20training%20samples%2C%20%28II%29%20attains%20up%20to%20a%202.81x%20lossless%20training%20speedup%2C%0Aand%20%28III%29%20outperforms%20state-of-the-art%20pruning%20methods%20in%20imbalanced%20training%0Aand%20noisy%20training%20scenarios%20by%200.3%25~4.3%25%20and%203.6%25~7.8%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13761v1&entry.124074799=Read"},
{"title": "Granular Privacy Control for Geolocation with Vision Language Models", "author": "Ethan Mendes and Yang Chen and James Hays and Sauvik Das and Wei Xu and Alan Ritter", "abstract": "  Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.\n", "link": "http://arxiv.org/abs/2407.04952v2", "date": "2024-10-17", "relevancy": 2.523, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Granular%20Privacy%20Control%20for%20Geolocation%20with%20Vision%20Language%20Models&body=Title%3A%20Granular%20Privacy%20Control%20for%20Geolocation%20with%20Vision%20Language%20Models%0AAuthor%3A%20Ethan%20Mendes%20and%20Yang%20Chen%20and%20James%20Hays%20and%20Sauvik%20Das%20and%20Wei%20Xu%20and%20Alan%20Ritter%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20are%20rapidly%20advancing%20in%20their%20capability%20to%0Aanswer%20information-seeking%20questions.%20As%20these%20models%20are%20widely%20deployed%20in%0Aconsumer%20applications%2C%20they%20could%20lead%20to%20new%20privacy%20risks%20due%20to%20emergent%0Aabilities%20to%20identify%20people%20in%20photos%2C%20geolocate%20images%2C%20etc.%20As%20we%0Ademonstrate%2C%20somewhat%20surprisingly%2C%20current%20open-source%20and%20proprietary%20VLMs%0Aare%20very%20capable%20image%20geolocators%2C%20making%20widespread%20geolocation%20with%20VLMs%20an%0Aimmediate%20privacy%20risk%2C%20rather%20than%20merely%20a%20theoretical%20future%20concern.%20As%20a%0Afirst%20step%20to%20address%20this%20challenge%2C%20we%20develop%20a%20new%20benchmark%2C%20GPTGeoChat%2C%0Ato%20test%20the%20ability%20of%20VLMs%20to%20moderate%20geolocation%20dialogues%20with%20users.%20We%0Acollect%20a%20set%20of%201%2C000%20image%20geolocation%20conversations%20between%20in-house%0Aannotators%20and%20GPT-4v%2C%20which%20are%20annotated%20with%20the%20granularity%20of%20location%0Ainformation%20revealed%20at%20each%20turn.%20Using%20this%20new%20dataset%2C%20we%20evaluate%20the%0Aability%20of%20various%20VLMs%20to%20moderate%20GPT-4v%20geolocation%20conversations%20by%0Adetermining%20when%20too%20much%20location%20information%20has%20been%20revealed.%20We%20find%20that%0Acustom%20fine-tuned%20models%20perform%20on%20par%20with%20prompted%20API-based%20models%20when%0Aidentifying%20leaked%20location%20information%20at%20the%20country%20or%20city%20level%3B%20however%2C%0Afine-tuning%20on%20supervised%20data%20appears%20to%20be%20needed%20to%20accurately%20moderate%0Afiner%20granularities%2C%20such%20as%20the%20name%20of%20a%20restaurant%20or%20building.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGranular%2520Privacy%2520Control%2520for%2520Geolocation%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DEthan%2520Mendes%2520and%2520Yang%2520Chen%2520and%2520James%2520Hays%2520and%2520Sauvik%2520Das%2520and%2520Wei%2520Xu%2520and%2520Alan%2520Ritter%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520are%2520rapidly%2520advancing%2520in%2520their%2520capability%2520to%250Aanswer%2520information-seeking%2520questions.%2520As%2520these%2520models%2520are%2520widely%2520deployed%2520in%250Aconsumer%2520applications%252C%2520they%2520could%2520lead%2520to%2520new%2520privacy%2520risks%2520due%2520to%2520emergent%250Aabilities%2520to%2520identify%2520people%2520in%2520photos%252C%2520geolocate%2520images%252C%2520etc.%2520As%2520we%250Ademonstrate%252C%2520somewhat%2520surprisingly%252C%2520current%2520open-source%2520and%2520proprietary%2520VLMs%250Aare%2520very%2520capable%2520image%2520geolocators%252C%2520making%2520widespread%2520geolocation%2520with%2520VLMs%2520an%250Aimmediate%2520privacy%2520risk%252C%2520rather%2520than%2520merely%2520a%2520theoretical%2520future%2520concern.%2520As%2520a%250Afirst%2520step%2520to%2520address%2520this%2520challenge%252C%2520we%2520develop%2520a%2520new%2520benchmark%252C%2520GPTGeoChat%252C%250Ato%2520test%2520the%2520ability%2520of%2520VLMs%2520to%2520moderate%2520geolocation%2520dialogues%2520with%2520users.%2520We%250Acollect%2520a%2520set%2520of%25201%252C000%2520image%2520geolocation%2520conversations%2520between%2520in-house%250Aannotators%2520and%2520GPT-4v%252C%2520which%2520are%2520annotated%2520with%2520the%2520granularity%2520of%2520location%250Ainformation%2520revealed%2520at%2520each%2520turn.%2520Using%2520this%2520new%2520dataset%252C%2520we%2520evaluate%2520the%250Aability%2520of%2520various%2520VLMs%2520to%2520moderate%2520GPT-4v%2520geolocation%2520conversations%2520by%250Adetermining%2520when%2520too%2520much%2520location%2520information%2520has%2520been%2520revealed.%2520We%2520find%2520that%250Acustom%2520fine-tuned%2520models%2520perform%2520on%2520par%2520with%2520prompted%2520API-based%2520models%2520when%250Aidentifying%2520leaked%2520location%2520information%2520at%2520the%2520country%2520or%2520city%2520level%253B%2520however%252C%250Afine-tuning%2520on%2520supervised%2520data%2520appears%2520to%2520be%2520needed%2520to%2520accurately%2520moderate%250Afiner%2520granularities%252C%2520such%2520as%2520the%2520name%2520of%2520a%2520restaurant%2520or%2520building.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Granular%20Privacy%20Control%20for%20Geolocation%20with%20Vision%20Language%20Models&entry.906535625=Ethan%20Mendes%20and%20Yang%20Chen%20and%20James%20Hays%20and%20Sauvik%20Das%20and%20Wei%20Xu%20and%20Alan%20Ritter&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20are%20rapidly%20advancing%20in%20their%20capability%20to%0Aanswer%20information-seeking%20questions.%20As%20these%20models%20are%20widely%20deployed%20in%0Aconsumer%20applications%2C%20they%20could%20lead%20to%20new%20privacy%20risks%20due%20to%20emergent%0Aabilities%20to%20identify%20people%20in%20photos%2C%20geolocate%20images%2C%20etc.%20As%20we%0Ademonstrate%2C%20somewhat%20surprisingly%2C%20current%20open-source%20and%20proprietary%20VLMs%0Aare%20very%20capable%20image%20geolocators%2C%20making%20widespread%20geolocation%20with%20VLMs%20an%0Aimmediate%20privacy%20risk%2C%20rather%20than%20merely%20a%20theoretical%20future%20concern.%20As%20a%0Afirst%20step%20to%20address%20this%20challenge%2C%20we%20develop%20a%20new%20benchmark%2C%20GPTGeoChat%2C%0Ato%20test%20the%20ability%20of%20VLMs%20to%20moderate%20geolocation%20dialogues%20with%20users.%20We%0Acollect%20a%20set%20of%201%2C000%20image%20geolocation%20conversations%20between%20in-house%0Aannotators%20and%20GPT-4v%2C%20which%20are%20annotated%20with%20the%20granularity%20of%20location%0Ainformation%20revealed%20at%20each%20turn.%20Using%20this%20new%20dataset%2C%20we%20evaluate%20the%0Aability%20of%20various%20VLMs%20to%20moderate%20GPT-4v%20geolocation%20conversations%20by%0Adetermining%20when%20too%20much%20location%20information%20has%20been%20revealed.%20We%20find%20that%0Acustom%20fine-tuned%20models%20perform%20on%20par%20with%20prompted%20API-based%20models%20when%0Aidentifying%20leaked%20location%20information%20at%20the%20country%20or%20city%20level%3B%20however%2C%0Afine-tuning%20on%20supervised%20data%20appears%20to%20be%20needed%20to%20accurately%20moderate%0Afiner%20granularities%2C%20such%20as%20the%20name%20of%20a%20restaurant%20or%20building.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04952v2&entry.124074799=Read"},
{"title": "Normalizing self-supervised learning for provably reliable Change Point\n  Detection", "author": "Alexandra Bazarova and Evgenia Romanenkova and Alexey Zaytsev", "abstract": "  Change point detection (CPD) methods aim to identify abrupt shifts in the\ndistribution of input data streams. Accurate estimators for this task are\ncrucial across various real-world scenarios. Yet, traditional unsupervised CPD\ntechniques face significant limitations, often relying on strong assumptions or\nsuffering from low expressive power due to inherent model simplicity. In\ncontrast, representation learning methods overcome these drawbacks by offering\nflexibility and the ability to capture the full complexity of the data without\nimposing restrictive assumptions. However, these approaches are still emerging\nin the CPD field and lack robust theoretical foundations to ensure their\nreliability. Our work addresses this gap by integrating the expressive power of\nrepresentation learning with the groundedness of traditional CPD techniques. We\nadopt spectral normalization (SN) for deep representation learning in CPD tasks\nand prove that the embeddings after SN are highly informative for CPD. Our\nmethod significantly outperforms current state-of-the-art methods during the\ncomprehensive evaluation via three standard CPD datasets.\n", "link": "http://arxiv.org/abs/2410.13637v1", "date": "2024-10-17", "relevancy": 2.5175, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4983}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalizing%20self-supervised%20learning%20for%20provably%20reliable%20Change%20Point%0A%20%20Detection&body=Title%3A%20Normalizing%20self-supervised%20learning%20for%20provably%20reliable%20Change%20Point%0A%20%20Detection%0AAuthor%3A%20Alexandra%20Bazarova%20and%20Evgenia%20Romanenkova%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Change%20point%20detection%20%28CPD%29%20methods%20aim%20to%20identify%20abrupt%20shifts%20in%20the%0Adistribution%20of%20input%20data%20streams.%20Accurate%20estimators%20for%20this%20task%20are%0Acrucial%20across%20various%20real-world%20scenarios.%20Yet%2C%20traditional%20unsupervised%20CPD%0Atechniques%20face%20significant%20limitations%2C%20often%20relying%20on%20strong%20assumptions%20or%0Asuffering%20from%20low%20expressive%20power%20due%20to%20inherent%20model%20simplicity.%20In%0Acontrast%2C%20representation%20learning%20methods%20overcome%20these%20drawbacks%20by%20offering%0Aflexibility%20and%20the%20ability%20to%20capture%20the%20full%20complexity%20of%20the%20data%20without%0Aimposing%20restrictive%20assumptions.%20However%2C%20these%20approaches%20are%20still%20emerging%0Ain%20the%20CPD%20field%20and%20lack%20robust%20theoretical%20foundations%20to%20ensure%20their%0Areliability.%20Our%20work%20addresses%20this%20gap%20by%20integrating%20the%20expressive%20power%20of%0Arepresentation%20learning%20with%20the%20groundedness%20of%20traditional%20CPD%20techniques.%20We%0Aadopt%20spectral%20normalization%20%28SN%29%20for%20deep%20representation%20learning%20in%20CPD%20tasks%0Aand%20prove%20that%20the%20embeddings%20after%20SN%20are%20highly%20informative%20for%20CPD.%20Our%0Amethod%20significantly%20outperforms%20current%20state-of-the-art%20methods%20during%20the%0Acomprehensive%20evaluation%20via%20three%20standard%20CPD%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalizing%2520self-supervised%2520learning%2520for%2520provably%2520reliable%2520Change%2520Point%250A%2520%2520Detection%26entry.906535625%3DAlexandra%2520Bazarova%2520and%2520Evgenia%2520Romanenkova%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Change%2520point%2520detection%2520%2528CPD%2529%2520methods%2520aim%2520to%2520identify%2520abrupt%2520shifts%2520in%2520the%250Adistribution%2520of%2520input%2520data%2520streams.%2520Accurate%2520estimators%2520for%2520this%2520task%2520are%250Acrucial%2520across%2520various%2520real-world%2520scenarios.%2520Yet%252C%2520traditional%2520unsupervised%2520CPD%250Atechniques%2520face%2520significant%2520limitations%252C%2520often%2520relying%2520on%2520strong%2520assumptions%2520or%250Asuffering%2520from%2520low%2520expressive%2520power%2520due%2520to%2520inherent%2520model%2520simplicity.%2520In%250Acontrast%252C%2520representation%2520learning%2520methods%2520overcome%2520these%2520drawbacks%2520by%2520offering%250Aflexibility%2520and%2520the%2520ability%2520to%2520capture%2520the%2520full%2520complexity%2520of%2520the%2520data%2520without%250Aimposing%2520restrictive%2520assumptions.%2520However%252C%2520these%2520approaches%2520are%2520still%2520emerging%250Ain%2520the%2520CPD%2520field%2520and%2520lack%2520robust%2520theoretical%2520foundations%2520to%2520ensure%2520their%250Areliability.%2520Our%2520work%2520addresses%2520this%2520gap%2520by%2520integrating%2520the%2520expressive%2520power%2520of%250Arepresentation%2520learning%2520with%2520the%2520groundedness%2520of%2520traditional%2520CPD%2520techniques.%2520We%250Aadopt%2520spectral%2520normalization%2520%2528SN%2529%2520for%2520deep%2520representation%2520learning%2520in%2520CPD%2520tasks%250Aand%2520prove%2520that%2520the%2520embeddings%2520after%2520SN%2520are%2520highly%2520informative%2520for%2520CPD.%2520Our%250Amethod%2520significantly%2520outperforms%2520current%2520state-of-the-art%2520methods%2520during%2520the%250Acomprehensive%2520evaluation%2520via%2520three%2520standard%2520CPD%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalizing%20self-supervised%20learning%20for%20provably%20reliable%20Change%20Point%0A%20%20Detection&entry.906535625=Alexandra%20Bazarova%20and%20Evgenia%20Romanenkova%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Change%20point%20detection%20%28CPD%29%20methods%20aim%20to%20identify%20abrupt%20shifts%20in%20the%0Adistribution%20of%20input%20data%20streams.%20Accurate%20estimators%20for%20this%20task%20are%0Acrucial%20across%20various%20real-world%20scenarios.%20Yet%2C%20traditional%20unsupervised%20CPD%0Atechniques%20face%20significant%20limitations%2C%20often%20relying%20on%20strong%20assumptions%20or%0Asuffering%20from%20low%20expressive%20power%20due%20to%20inherent%20model%20simplicity.%20In%0Acontrast%2C%20representation%20learning%20methods%20overcome%20these%20drawbacks%20by%20offering%0Aflexibility%20and%20the%20ability%20to%20capture%20the%20full%20complexity%20of%20the%20data%20without%0Aimposing%20restrictive%20assumptions.%20However%2C%20these%20approaches%20are%20still%20emerging%0Ain%20the%20CPD%20field%20and%20lack%20robust%20theoretical%20foundations%20to%20ensure%20their%0Areliability.%20Our%20work%20addresses%20this%20gap%20by%20integrating%20the%20expressive%20power%20of%0Arepresentation%20learning%20with%20the%20groundedness%20of%20traditional%20CPD%20techniques.%20We%0Aadopt%20spectral%20normalization%20%28SN%29%20for%20deep%20representation%20learning%20in%20CPD%20tasks%0Aand%20prove%20that%20the%20embeddings%20after%20SN%20are%20highly%20informative%20for%20CPD.%20Our%0Amethod%20significantly%20outperforms%20current%20state-of-the-art%20methods%20during%20the%0Acomprehensive%20evaluation%20via%20three%20standard%20CPD%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13637v1&entry.124074799=Read"},
{"title": "LLM-Human Pipeline for Cultural Context Grounding of Conversations", "author": "Rajkumar Pujari and Dan Goldwasser", "abstract": "  Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance.\n", "link": "http://arxiv.org/abs/2410.13727v1", "date": "2024-10-17", "relevancy": 2.5158, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Human%20Pipeline%20for%20Cultural%20Context%20Grounding%20of%20Conversations&body=Title%3A%20LLM-Human%20Pipeline%20for%20Cultural%20Context%20Grounding%20of%20Conversations%0AAuthor%3A%20Rajkumar%20Pujari%20and%20Dan%20Goldwasser%0AAbstract%3A%20%20%20Conversations%20often%20adhere%20to%20well-understood%20social%20norms%20that%20vary%20across%0Acultures.%20For%20example%2C%20while%20%22addressing%20parents%20by%20name%22%20is%20commonplace%20in%20the%0AWest%2C%20it%20is%20rare%20in%20most%20Asian%20cultures.%20Adherence%20or%20violation%20of%20such%20norms%0Aoften%20dictates%20the%20tenor%20of%20conversations.%20Humans%20are%20able%20to%20navigate%20social%0Asituations%20requiring%20cultural%20awareness%20quite%20adeptly.%20However%2C%20it%20is%20a%20hard%0Atask%20for%20NLP%20models.%0A%20%20In%20this%20paper%2C%20we%20tackle%20this%20problem%20by%20introducing%20a%20%22Cultural%20Context%0ASchema%22%20for%20conversations.%20It%20comprises%20%281%29%20conversational%20information%20such%20as%0Aemotions%2C%20dialogue%20acts%2C%20etc.%2C%20and%20%282%29%20cultural%20information%20such%20as%20social%0Anorms%2C%20violations%2C%20etc.%20We%20generate%20~110k%20social%20norm%20and%20violation%0Adescriptions%20for%20~23k%20conversations%20from%20Chinese%20culture%20using%20LLMs.%20We%20refine%0Athem%20using%20automated%20verification%20strategies%20which%20are%20evaluated%20against%0Aculturally%20aware%20human%20judgements.%20We%20organize%20these%20descriptions%20into%0Ameaningful%20structures%20we%20call%20%22Norm%20Concepts%22%2C%20using%20an%20interactive%0Ahuman-in-loop%20framework.%20We%20ground%20the%20norm%20concepts%20and%20the%20descriptions%20in%0Aconversations%20using%20symbolic%20annotation.%20Finally%2C%20we%20use%20the%20obtained%20dataset%0Afor%20downstream%20tasks%20such%20as%20emotion%2C%20sentiment%2C%20and%20dialogue%20act%20detection.%20We%0Ashow%20that%20it%20significantly%20improves%20the%20empirical%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Human%2520Pipeline%2520for%2520Cultural%2520Context%2520Grounding%2520of%2520Conversations%26entry.906535625%3DRajkumar%2520Pujari%2520and%2520Dan%2520Goldwasser%26entry.1292438233%3D%2520%2520Conversations%2520often%2520adhere%2520to%2520well-understood%2520social%2520norms%2520that%2520vary%2520across%250Acultures.%2520For%2520example%252C%2520while%2520%2522addressing%2520parents%2520by%2520name%2522%2520is%2520commonplace%2520in%2520the%250AWest%252C%2520it%2520is%2520rare%2520in%2520most%2520Asian%2520cultures.%2520Adherence%2520or%2520violation%2520of%2520such%2520norms%250Aoften%2520dictates%2520the%2520tenor%2520of%2520conversations.%2520Humans%2520are%2520able%2520to%2520navigate%2520social%250Asituations%2520requiring%2520cultural%2520awareness%2520quite%2520adeptly.%2520However%252C%2520it%2520is%2520a%2520hard%250Atask%2520for%2520NLP%2520models.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520tackle%2520this%2520problem%2520by%2520introducing%2520a%2520%2522Cultural%2520Context%250ASchema%2522%2520for%2520conversations.%2520It%2520comprises%2520%25281%2529%2520conversational%2520information%2520such%2520as%250Aemotions%252C%2520dialogue%2520acts%252C%2520etc.%252C%2520and%2520%25282%2529%2520cultural%2520information%2520such%2520as%2520social%250Anorms%252C%2520violations%252C%2520etc.%2520We%2520generate%2520~110k%2520social%2520norm%2520and%2520violation%250Adescriptions%2520for%2520~23k%2520conversations%2520from%2520Chinese%2520culture%2520using%2520LLMs.%2520We%2520refine%250Athem%2520using%2520automated%2520verification%2520strategies%2520which%2520are%2520evaluated%2520against%250Aculturally%2520aware%2520human%2520judgements.%2520We%2520organize%2520these%2520descriptions%2520into%250Ameaningful%2520structures%2520we%2520call%2520%2522Norm%2520Concepts%2522%252C%2520using%2520an%2520interactive%250Ahuman-in-loop%2520framework.%2520We%2520ground%2520the%2520norm%2520concepts%2520and%2520the%2520descriptions%2520in%250Aconversations%2520using%2520symbolic%2520annotation.%2520Finally%252C%2520we%2520use%2520the%2520obtained%2520dataset%250Afor%2520downstream%2520tasks%2520such%2520as%2520emotion%252C%2520sentiment%252C%2520and%2520dialogue%2520act%2520detection.%2520We%250Ashow%2520that%2520it%2520significantly%2520improves%2520the%2520empirical%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Human%20Pipeline%20for%20Cultural%20Context%20Grounding%20of%20Conversations&entry.906535625=Rajkumar%20Pujari%20and%20Dan%20Goldwasser&entry.1292438233=%20%20Conversations%20often%20adhere%20to%20well-understood%20social%20norms%20that%20vary%20across%0Acultures.%20For%20example%2C%20while%20%22addressing%20parents%20by%20name%22%20is%20commonplace%20in%20the%0AWest%2C%20it%20is%20rare%20in%20most%20Asian%20cultures.%20Adherence%20or%20violation%20of%20such%20norms%0Aoften%20dictates%20the%20tenor%20of%20conversations.%20Humans%20are%20able%20to%20navigate%20social%0Asituations%20requiring%20cultural%20awareness%20quite%20adeptly.%20However%2C%20it%20is%20a%20hard%0Atask%20for%20NLP%20models.%0A%20%20In%20this%20paper%2C%20we%20tackle%20this%20problem%20by%20introducing%20a%20%22Cultural%20Context%0ASchema%22%20for%20conversations.%20It%20comprises%20%281%29%20conversational%20information%20such%20as%0Aemotions%2C%20dialogue%20acts%2C%20etc.%2C%20and%20%282%29%20cultural%20information%20such%20as%20social%0Anorms%2C%20violations%2C%20etc.%20We%20generate%20~110k%20social%20norm%20and%20violation%0Adescriptions%20for%20~23k%20conversations%20from%20Chinese%20culture%20using%20LLMs.%20We%20refine%0Athem%20using%20automated%20verification%20strategies%20which%20are%20evaluated%20against%0Aculturally%20aware%20human%20judgements.%20We%20organize%20these%20descriptions%20into%0Ameaningful%20structures%20we%20call%20%22Norm%20Concepts%22%2C%20using%20an%20interactive%0Ahuman-in-loop%20framework.%20We%20ground%20the%20norm%20concepts%20and%20the%20descriptions%20in%0Aconversations%20using%20symbolic%20annotation.%20Finally%2C%20we%20use%20the%20obtained%20dataset%0Afor%20downstream%20tasks%20such%20as%20emotion%2C%20sentiment%2C%20and%20dialogue%20act%20detection.%20We%0Ashow%20that%20it%20significantly%20improves%20the%20empirical%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13727v1&entry.124074799=Read"},
{"title": "Exploring the Design Space of Visual Context Representation in Video\n  MLLMs", "author": "Yifan Du and Yuqi Huo and Kun Zhou and Zijia Zhao and Haoyu Lu and Han Huang and Wayne Xin Zhao and Bingning Wang and Weipeng Chen and Ji-Rong Wen", "abstract": "  Video Multimodal Large Language Models (MLLMs) have shown remarkable\ncapability of understanding the video semantics on various downstream tasks.\nDespite the advancements, there is still a lack of systematic research on\nvisual context representation, which refers to the scheme to select frames from\na video and further select the tokens from a frame. In this paper, we explore\nthe design space for visual context representation, and aim to improve the\nperformance of video MLLMs by finding more effective representation schemes.\nFirstly, we formulate the task of visual context representation as a\nconstrained optimization problem, and model the language modeling loss as a\nfunction of the number of frames and the number of embeddings (or tokens) per\nframe, given the maximum visual context window size. Then, we explore the\nscaling effects in frame selection and token selection respectively, and fit\nthe corresponding function curve by conducting extensive empirical experiments.\nWe examine the effectiveness of typical selection strategies and present\nempirical findings to determine the two factors. Furthermore, we study the\njoint effect of frame selection and token selection, and derive the optimal\nformula for determining the two factors. We demonstrate that the derived\noptimal settings show alignment with the best-performed results of empirical\nexperiments. Our code and model are available at:\nhttps://github.com/RUCAIBox/Opt-Visor.\n", "link": "http://arxiv.org/abs/2410.13694v1", "date": "2024-10-17", "relevancy": 2.4836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.629}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Design%20Space%20of%20Visual%20Context%20Representation%20in%20Video%0A%20%20MLLMs&body=Title%3A%20Exploring%20the%20Design%20Space%20of%20Visual%20Context%20Representation%20in%20Video%0A%20%20MLLMs%0AAuthor%3A%20Yifan%20Du%20and%20Yuqi%20Huo%20and%20Kun%20Zhou%20and%20Zijia%20Zhao%20and%20Haoyu%20Lu%20and%20Han%20Huang%20and%20Wayne%20Xin%20Zhao%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Video%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20remarkable%0Acapability%20of%20understanding%20the%20video%20semantics%20on%20various%20downstream%20tasks.%0ADespite%20the%20advancements%2C%20there%20is%20still%20a%20lack%20of%20systematic%20research%20on%0Avisual%20context%20representation%2C%20which%20refers%20to%20the%20scheme%20to%20select%20frames%20from%0Aa%20video%20and%20further%20select%20the%20tokens%20from%20a%20frame.%20In%20this%20paper%2C%20we%20explore%0Athe%20design%20space%20for%20visual%20context%20representation%2C%20and%20aim%20to%20improve%20the%0Aperformance%20of%20video%20MLLMs%20by%20finding%20more%20effective%20representation%20schemes.%0AFirstly%2C%20we%20formulate%20the%20task%20of%20visual%20context%20representation%20as%20a%0Aconstrained%20optimization%20problem%2C%20and%20model%20the%20language%20modeling%20loss%20as%20a%0Afunction%20of%20the%20number%20of%20frames%20and%20the%20number%20of%20embeddings%20%28or%20tokens%29%20per%0Aframe%2C%20given%20the%20maximum%20visual%20context%20window%20size.%20Then%2C%20we%20explore%20the%0Ascaling%20effects%20in%20frame%20selection%20and%20token%20selection%20respectively%2C%20and%20fit%0Athe%20corresponding%20function%20curve%20by%20conducting%20extensive%20empirical%20experiments.%0AWe%20examine%20the%20effectiveness%20of%20typical%20selection%20strategies%20and%20present%0Aempirical%20findings%20to%20determine%20the%20two%20factors.%20Furthermore%2C%20we%20study%20the%0Ajoint%20effect%20of%20frame%20selection%20and%20token%20selection%2C%20and%20derive%20the%20optimal%0Aformula%20for%20determining%20the%20two%20factors.%20We%20demonstrate%20that%20the%20derived%0Aoptimal%20settings%20show%20alignment%20with%20the%20best-performed%20results%20of%20empirical%0Aexperiments.%20Our%20code%20and%20model%20are%20available%20at%3A%0Ahttps%3A//github.com/RUCAIBox/Opt-Visor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Design%2520Space%2520of%2520Visual%2520Context%2520Representation%2520in%2520Video%250A%2520%2520MLLMs%26entry.906535625%3DYifan%2520Du%2520and%2520Yuqi%2520Huo%2520and%2520Kun%2520Zhou%2520and%2520Zijia%2520Zhao%2520and%2520Haoyu%2520Lu%2520and%2520Han%2520Huang%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Bingning%2520Wang%2520and%2520Weipeng%2520Chen%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Video%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520remarkable%250Acapability%2520of%2520understanding%2520the%2520video%2520semantics%2520on%2520various%2520downstream%2520tasks.%250ADespite%2520the%2520advancements%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520systematic%2520research%2520on%250Avisual%2520context%2520representation%252C%2520which%2520refers%2520to%2520the%2520scheme%2520to%2520select%2520frames%2520from%250Aa%2520video%2520and%2520further%2520select%2520the%2520tokens%2520from%2520a%2520frame.%2520In%2520this%2520paper%252C%2520we%2520explore%250Athe%2520design%2520space%2520for%2520visual%2520context%2520representation%252C%2520and%2520aim%2520to%2520improve%2520the%250Aperformance%2520of%2520video%2520MLLMs%2520by%2520finding%2520more%2520effective%2520representation%2520schemes.%250AFirstly%252C%2520we%2520formulate%2520the%2520task%2520of%2520visual%2520context%2520representation%2520as%2520a%250Aconstrained%2520optimization%2520problem%252C%2520and%2520model%2520the%2520language%2520modeling%2520loss%2520as%2520a%250Afunction%2520of%2520the%2520number%2520of%2520frames%2520and%2520the%2520number%2520of%2520embeddings%2520%2528or%2520tokens%2529%2520per%250Aframe%252C%2520given%2520the%2520maximum%2520visual%2520context%2520window%2520size.%2520Then%252C%2520we%2520explore%2520the%250Ascaling%2520effects%2520in%2520frame%2520selection%2520and%2520token%2520selection%2520respectively%252C%2520and%2520fit%250Athe%2520corresponding%2520function%2520curve%2520by%2520conducting%2520extensive%2520empirical%2520experiments.%250AWe%2520examine%2520the%2520effectiveness%2520of%2520typical%2520selection%2520strategies%2520and%2520present%250Aempirical%2520findings%2520to%2520determine%2520the%2520two%2520factors.%2520Furthermore%252C%2520we%2520study%2520the%250Ajoint%2520effect%2520of%2520frame%2520selection%2520and%2520token%2520selection%252C%2520and%2520derive%2520the%2520optimal%250Aformula%2520for%2520determining%2520the%2520two%2520factors.%2520We%2520demonstrate%2520that%2520the%2520derived%250Aoptimal%2520settings%2520show%2520alignment%2520with%2520the%2520best-performed%2520results%2520of%2520empirical%250Aexperiments.%2520Our%2520code%2520and%2520model%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/RUCAIBox/Opt-Visor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Design%20Space%20of%20Visual%20Context%20Representation%20in%20Video%0A%20%20MLLMs&entry.906535625=Yifan%20Du%20and%20Yuqi%20Huo%20and%20Kun%20Zhou%20and%20Zijia%20Zhao%20and%20Haoyu%20Lu%20and%20Han%20Huang%20and%20Wayne%20Xin%20Zhao%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Video%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20remarkable%0Acapability%20of%20understanding%20the%20video%20semantics%20on%20various%20downstream%20tasks.%0ADespite%20the%20advancements%2C%20there%20is%20still%20a%20lack%20of%20systematic%20research%20on%0Avisual%20context%20representation%2C%20which%20refers%20to%20the%20scheme%20to%20select%20frames%20from%0Aa%20video%20and%20further%20select%20the%20tokens%20from%20a%20frame.%20In%20this%20paper%2C%20we%20explore%0Athe%20design%20space%20for%20visual%20context%20representation%2C%20and%20aim%20to%20improve%20the%0Aperformance%20of%20video%20MLLMs%20by%20finding%20more%20effective%20representation%20schemes.%0AFirstly%2C%20we%20formulate%20the%20task%20of%20visual%20context%20representation%20as%20a%0Aconstrained%20optimization%20problem%2C%20and%20model%20the%20language%20modeling%20loss%20as%20a%0Afunction%20of%20the%20number%20of%20frames%20and%20the%20number%20of%20embeddings%20%28or%20tokens%29%20per%0Aframe%2C%20given%20the%20maximum%20visual%20context%20window%20size.%20Then%2C%20we%20explore%20the%0Ascaling%20effects%20in%20frame%20selection%20and%20token%20selection%20respectively%2C%20and%20fit%0Athe%20corresponding%20function%20curve%20by%20conducting%20extensive%20empirical%20experiments.%0AWe%20examine%20the%20effectiveness%20of%20typical%20selection%20strategies%20and%20present%0Aempirical%20findings%20to%20determine%20the%20two%20factors.%20Furthermore%2C%20we%20study%20the%0Ajoint%20effect%20of%20frame%20selection%20and%20token%20selection%2C%20and%20derive%20the%20optimal%0Aformula%20for%20determining%20the%20two%20factors.%20We%20demonstrate%20that%20the%20derived%0Aoptimal%20settings%20show%20alignment%20with%20the%20best-performed%20results%20of%20empirical%0Aexperiments.%20Our%20code%20and%20model%20are%20available%20at%3A%0Ahttps%3A//github.com/RUCAIBox/Opt-Visor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13694v1&entry.124074799=Read"},
{"title": "The Mystery of the Pathological Path-star Task for Language Models", "author": "Arvid Frydenlund", "abstract": "  The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task.\n", "link": "http://arxiv.org/abs/2410.13779v1", "date": "2024-10-17", "relevancy": 2.4667, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Mystery%20of%20the%20Pathological%20Path-star%20Task%20for%20Language%20Models&body=Title%3A%20The%20Mystery%20of%20the%20Pathological%20Path-star%20Task%20for%20Language%20Models%0AAuthor%3A%20Arvid%20Frydenlund%0AAbstract%3A%20%20%20The%20recently%20introduced%20path-star%20task%20is%20a%20minimal%20task%20designed%20to%0Aexemplify%20limitations%20to%20the%20abilities%20of%20language%20models%20%28Bachmann%20and%0ANagarajan%2C%202024%29.%20It%20involves%20a%20path-star%20graph%20where%20multiple%20arms%20radiate%0Afrom%20a%20single%20starting%20node%20and%20each%20node%20is%20unique.%20Given%20the%20start%20node%20and%20a%0Aspecified%20target%20node%20that%20ends%20an%20arm%2C%20the%20task%20is%20to%20generate%20the%20arm%0Acontaining%20that%20target%20node.%20This%20is%20straightforward%20for%20a%20human%20but%0Asurprisingly%20difficult%20for%20language%20models%2C%20which%20did%20not%20outperform%20the%20random%0Abaseline.%20The%20authors%20hypothesized%20this%20is%20due%20to%20a%20deficiency%20in%0Ateacher-forcing%20and%20the%20next-token%20prediction%20paradigm.%0A%20%20We%20demonstrate%20the%20task%20is%20learnable%20using%20teacher-forcing%20in%20alternative%0Asettings%20and%20that%20the%20issue%20is%20partially%20due%20to%20representation.%20We%20introduce%20a%0Aregularization%20method%20using%20structured%20samples%20of%20the%20same%20graph%20but%20with%0Adiffering%20target%20nodes%2C%20improving%20results%20across%20a%20variety%20of%20model%20types.%20We%0Aprovide%20RASP%20proofs%20showing%20the%20task%20is%20theoretically%20solvable.%20Finally%2C%20we%0Afind%20settings%20where%20an%20encoder-only%20model%20can%20consistently%20solve%20the%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Mystery%2520of%2520the%2520Pathological%2520Path-star%2520Task%2520for%2520Language%2520Models%26entry.906535625%3DArvid%2520Frydenlund%26entry.1292438233%3D%2520%2520The%2520recently%2520introduced%2520path-star%2520task%2520is%2520a%2520minimal%2520task%2520designed%2520to%250Aexemplify%2520limitations%2520to%2520the%2520abilities%2520of%2520language%2520models%2520%2528Bachmann%2520and%250ANagarajan%252C%25202024%2529.%2520It%2520involves%2520a%2520path-star%2520graph%2520where%2520multiple%2520arms%2520radiate%250Afrom%2520a%2520single%2520starting%2520node%2520and%2520each%2520node%2520is%2520unique.%2520Given%2520the%2520start%2520node%2520and%2520a%250Aspecified%2520target%2520node%2520that%2520ends%2520an%2520arm%252C%2520the%2520task%2520is%2520to%2520generate%2520the%2520arm%250Acontaining%2520that%2520target%2520node.%2520This%2520is%2520straightforward%2520for%2520a%2520human%2520but%250Asurprisingly%2520difficult%2520for%2520language%2520models%252C%2520which%2520did%2520not%2520outperform%2520the%2520random%250Abaseline.%2520The%2520authors%2520hypothesized%2520this%2520is%2520due%2520to%2520a%2520deficiency%2520in%250Ateacher-forcing%2520and%2520the%2520next-token%2520prediction%2520paradigm.%250A%2520%2520We%2520demonstrate%2520the%2520task%2520is%2520learnable%2520using%2520teacher-forcing%2520in%2520alternative%250Asettings%2520and%2520that%2520the%2520issue%2520is%2520partially%2520due%2520to%2520representation.%2520We%2520introduce%2520a%250Aregularization%2520method%2520using%2520structured%2520samples%2520of%2520the%2520same%2520graph%2520but%2520with%250Adiffering%2520target%2520nodes%252C%2520improving%2520results%2520across%2520a%2520variety%2520of%2520model%2520types.%2520We%250Aprovide%2520RASP%2520proofs%2520showing%2520the%2520task%2520is%2520theoretically%2520solvable.%2520Finally%252C%2520we%250Afind%2520settings%2520where%2520an%2520encoder-only%2520model%2520can%2520consistently%2520solve%2520the%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Mystery%20of%20the%20Pathological%20Path-star%20Task%20for%20Language%20Models&entry.906535625=Arvid%20Frydenlund&entry.1292438233=%20%20The%20recently%20introduced%20path-star%20task%20is%20a%20minimal%20task%20designed%20to%0Aexemplify%20limitations%20to%20the%20abilities%20of%20language%20models%20%28Bachmann%20and%0ANagarajan%2C%202024%29.%20It%20involves%20a%20path-star%20graph%20where%20multiple%20arms%20radiate%0Afrom%20a%20single%20starting%20node%20and%20each%20node%20is%20unique.%20Given%20the%20start%20node%20and%20a%0Aspecified%20target%20node%20that%20ends%20an%20arm%2C%20the%20task%20is%20to%20generate%20the%20arm%0Acontaining%20that%20target%20node.%20This%20is%20straightforward%20for%20a%20human%20but%0Asurprisingly%20difficult%20for%20language%20models%2C%20which%20did%20not%20outperform%20the%20random%0Abaseline.%20The%20authors%20hypothesized%20this%20is%20due%20to%20a%20deficiency%20in%0Ateacher-forcing%20and%20the%20next-token%20prediction%20paradigm.%0A%20%20We%20demonstrate%20the%20task%20is%20learnable%20using%20teacher-forcing%20in%20alternative%0Asettings%20and%20that%20the%20issue%20is%20partially%20due%20to%20representation.%20We%20introduce%20a%0Aregularization%20method%20using%20structured%20samples%20of%20the%20same%20graph%20but%20with%0Adiffering%20target%20nodes%2C%20improving%20results%20across%20a%20variety%20of%20model%20types.%20We%0Aprovide%20RASP%20proofs%20showing%20the%20task%20is%20theoretically%20solvable.%20Finally%2C%20we%0Afind%20settings%20where%20an%20encoder-only%20model%20can%20consistently%20solve%20the%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13779v1&entry.124074799=Read"},
{"title": "PopAlign: Diversifying Contrasting Patterns for a More Comprehensive\n  Alignment", "author": "Zekun Moore Wang and Shawn Wang and Kang Zhu and Jiaheng Liu and Ke Xu and Jie Fu and Wangchunshu Zhou and Wenhao Huang", "abstract": "  Alignment of large language models (LLMs) involves training models on\npreference-contrastive output pairs to adjust their responses according to\nhuman preferences. To obtain such contrastive pairs, traditional methods like\nRLHF and RLAIF rely on limited contrasting patterns, such as varying model\nvariants or decoding temperatures. This singularity leads to two issues: (1)\nalignment is not comprehensive; and thereby (2) models are susceptible to\njailbreaking attacks. To address these issues, we investigate how to construct\nmore comprehensive and diversified contrasting patterns to enhance preference\ndata (RQ1) and verify the impact of the diversification of contrasting patterns\non model alignment (RQ2). For RQ1, we propose PopAlign, a framework that\nintegrates diversified contrasting patterns across the prompt, model, and\npipeline levels, introducing six contrasting strategies that do not require\nadditional feedback labeling procedures. Regarding RQ2, we conduct thorough\nexperiments demonstrating that PopAlign significantly outperforms existing\nmethods, leading to more comprehensive alignment.\n", "link": "http://arxiv.org/abs/2410.13785v1", "date": "2024-10-17", "relevancy": 2.4656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PopAlign%3A%20Diversifying%20Contrasting%20Patterns%20for%20a%20More%20Comprehensive%0A%20%20Alignment&body=Title%3A%20PopAlign%3A%20Diversifying%20Contrasting%20Patterns%20for%20a%20More%20Comprehensive%0A%20%20Alignment%0AAuthor%3A%20Zekun%20Moore%20Wang%20and%20Shawn%20Wang%20and%20Kang%20Zhu%20and%20Jiaheng%20Liu%20and%20Ke%20Xu%20and%20Jie%20Fu%20and%20Wangchunshu%20Zhou%20and%20Wenhao%20Huang%0AAbstract%3A%20%20%20Alignment%20of%20large%20language%20models%20%28LLMs%29%20involves%20training%20models%20on%0Apreference-contrastive%20output%20pairs%20to%20adjust%20their%20responses%20according%20to%0Ahuman%20preferences.%20To%20obtain%20such%20contrastive%20pairs%2C%20traditional%20methods%20like%0ARLHF%20and%20RLAIF%20rely%20on%20limited%20contrasting%20patterns%2C%20such%20as%20varying%20model%0Avariants%20or%20decoding%20temperatures.%20This%20singularity%20leads%20to%20two%20issues%3A%20%281%29%0Aalignment%20is%20not%20comprehensive%3B%20and%20thereby%20%282%29%20models%20are%20susceptible%20to%0Ajailbreaking%20attacks.%20To%20address%20these%20issues%2C%20we%20investigate%20how%20to%20construct%0Amore%20comprehensive%20and%20diversified%20contrasting%20patterns%20to%20enhance%20preference%0Adata%20%28RQ1%29%20and%20verify%20the%20impact%20of%20the%20diversification%20of%20contrasting%20patterns%0Aon%20model%20alignment%20%28RQ2%29.%20For%20RQ1%2C%20we%20propose%20PopAlign%2C%20a%20framework%20that%0Aintegrates%20diversified%20contrasting%20patterns%20across%20the%20prompt%2C%20model%2C%20and%0Apipeline%20levels%2C%20introducing%20six%20contrasting%20strategies%20that%20do%20not%20require%0Aadditional%20feedback%20labeling%20procedures.%20Regarding%20RQ2%2C%20we%20conduct%20thorough%0Aexperiments%20demonstrating%20that%20PopAlign%20significantly%20outperforms%20existing%0Amethods%2C%20leading%20to%20more%20comprehensive%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPopAlign%253A%2520Diversifying%2520Contrasting%2520Patterns%2520for%2520a%2520More%2520Comprehensive%250A%2520%2520Alignment%26entry.906535625%3DZekun%2520Moore%2520Wang%2520and%2520Shawn%2520Wang%2520and%2520Kang%2520Zhu%2520and%2520Jiaheng%2520Liu%2520and%2520Ke%2520Xu%2520and%2520Jie%2520Fu%2520and%2520Wangchunshu%2520Zhou%2520and%2520Wenhao%2520Huang%26entry.1292438233%3D%2520%2520Alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520involves%2520training%2520models%2520on%250Apreference-contrastive%2520output%2520pairs%2520to%2520adjust%2520their%2520responses%2520according%2520to%250Ahuman%2520preferences.%2520To%2520obtain%2520such%2520contrastive%2520pairs%252C%2520traditional%2520methods%2520like%250ARLHF%2520and%2520RLAIF%2520rely%2520on%2520limited%2520contrasting%2520patterns%252C%2520such%2520as%2520varying%2520model%250Avariants%2520or%2520decoding%2520temperatures.%2520This%2520singularity%2520leads%2520to%2520two%2520issues%253A%2520%25281%2529%250Aalignment%2520is%2520not%2520comprehensive%253B%2520and%2520thereby%2520%25282%2529%2520models%2520are%2520susceptible%2520to%250Ajailbreaking%2520attacks.%2520To%2520address%2520these%2520issues%252C%2520we%2520investigate%2520how%2520to%2520construct%250Amore%2520comprehensive%2520and%2520diversified%2520contrasting%2520patterns%2520to%2520enhance%2520preference%250Adata%2520%2528RQ1%2529%2520and%2520verify%2520the%2520impact%2520of%2520the%2520diversification%2520of%2520contrasting%2520patterns%250Aon%2520model%2520alignment%2520%2528RQ2%2529.%2520For%2520RQ1%252C%2520we%2520propose%2520PopAlign%252C%2520a%2520framework%2520that%250Aintegrates%2520diversified%2520contrasting%2520patterns%2520across%2520the%2520prompt%252C%2520model%252C%2520and%250Apipeline%2520levels%252C%2520introducing%2520six%2520contrasting%2520strategies%2520that%2520do%2520not%2520require%250Aadditional%2520feedback%2520labeling%2520procedures.%2520Regarding%2520RQ2%252C%2520we%2520conduct%2520thorough%250Aexperiments%2520demonstrating%2520that%2520PopAlign%2520significantly%2520outperforms%2520existing%250Amethods%252C%2520leading%2520to%2520more%2520comprehensive%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PopAlign%3A%20Diversifying%20Contrasting%20Patterns%20for%20a%20More%20Comprehensive%0A%20%20Alignment&entry.906535625=Zekun%20Moore%20Wang%20and%20Shawn%20Wang%20and%20Kang%20Zhu%20and%20Jiaheng%20Liu%20and%20Ke%20Xu%20and%20Jie%20Fu%20and%20Wangchunshu%20Zhou%20and%20Wenhao%20Huang&entry.1292438233=%20%20Alignment%20of%20large%20language%20models%20%28LLMs%29%20involves%20training%20models%20on%0Apreference-contrastive%20output%20pairs%20to%20adjust%20their%20responses%20according%20to%0Ahuman%20preferences.%20To%20obtain%20such%20contrastive%20pairs%2C%20traditional%20methods%20like%0ARLHF%20and%20RLAIF%20rely%20on%20limited%20contrasting%20patterns%2C%20such%20as%20varying%20model%0Avariants%20or%20decoding%20temperatures.%20This%20singularity%20leads%20to%20two%20issues%3A%20%281%29%0Aalignment%20is%20not%20comprehensive%3B%20and%20thereby%20%282%29%20models%20are%20susceptible%20to%0Ajailbreaking%20attacks.%20To%20address%20these%20issues%2C%20we%20investigate%20how%20to%20construct%0Amore%20comprehensive%20and%20diversified%20contrasting%20patterns%20to%20enhance%20preference%0Adata%20%28RQ1%29%20and%20verify%20the%20impact%20of%20the%20diversification%20of%20contrasting%20patterns%0Aon%20model%20alignment%20%28RQ2%29.%20For%20RQ1%2C%20we%20propose%20PopAlign%2C%20a%20framework%20that%0Aintegrates%20diversified%20contrasting%20patterns%20across%20the%20prompt%2C%20model%2C%20and%0Apipeline%20levels%2C%20introducing%20six%20contrasting%20strategies%20that%20do%20not%20require%0Aadditional%20feedback%20labeling%20procedures.%20Regarding%20RQ2%2C%20we%20conduct%20thorough%0Aexperiments%20demonstrating%20that%20PopAlign%20significantly%20outperforms%20existing%0Amethods%2C%20leading%20to%20more%20comprehensive%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13785v1&entry.124074799=Read"},
{"title": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation", "author": "Yiming Wang and Pei Zhang and Baosong Yang and Derek F. Wong and Rui Wang", "abstract": "  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensure real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n", "link": "http://arxiv.org/abs/2410.13640v1", "date": "2024-10-17", "relevancy": 2.4575, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Chain-of-Embedding%20Enables%20Output-free%20LLM%20Self-Evaluation&body=Title%3A%20Latent%20Space%20Chain-of-Embedding%20Enables%20Output-free%20LLM%20Self-Evaluation%0AAuthor%3A%20Yiming%20Wang%20and%20Pei%20Zhang%20and%20Baosong%20Yang%20and%20Derek%20F.%20Wong%20and%20Rui%20Wang%0AAbstract%3A%20%20%20LLM%20self-evaluation%20relies%20on%20the%20LLM%27s%20own%20ability%20to%20estimate%20response%0Acorrectness%2C%20which%20can%20greatly%20improve%20its%20deployment%20reliability.%20In%20this%0Aresearch%20track%2C%20we%20propose%20the%20Chain-of-Embedding%20%28CoE%29%20in%20the%20latent%20space%20to%0Aenable%20LLMs%20to%20perform%20output-free%20self-evaluation.%20CoE%20consists%20of%20all%0Aprogressive%20hidden%20states%20produced%20during%20the%20inference%20time%2C%20which%20can%20be%0Atreated%20as%20the%20latent%20thinking%20path%20of%20LLMs.%20We%20find%20that%20when%20LLMs%20respond%0Acorrectly%20and%20incorrectly%2C%20their%20CoE%20features%20differ%2C%20these%20discrepancies%0Aassist%20us%20in%20estimating%20LLM%20response%20correctness.%20Experiments%20in%20four%20diverse%0Adomains%20and%20seven%20LLMs%20fully%20demonstrate%20the%20effectiveness%20of%20our%20method.%0AMeanwhile%2C%20its%20label-free%20design%20intent%20without%20any%20training%20and%0Amillisecond-level%20computational%20cost%20ensure%20real-time%20feedback%20in%20large-scale%0Ascenarios.%20More%20importantly%2C%20we%20provide%20interesting%20insights%20into%20LLM%20response%0Acorrectness%20from%20the%20perspective%20of%20hidden%20state%20changes%20inside%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Space%2520Chain-of-Embedding%2520Enables%2520Output-free%2520LLM%2520Self-Evaluation%26entry.906535625%3DYiming%2520Wang%2520and%2520Pei%2520Zhang%2520and%2520Baosong%2520Yang%2520and%2520Derek%2520F.%2520Wong%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520LLM%2520self-evaluation%2520relies%2520on%2520the%2520LLM%2527s%2520own%2520ability%2520to%2520estimate%2520response%250Acorrectness%252C%2520which%2520can%2520greatly%2520improve%2520its%2520deployment%2520reliability.%2520In%2520this%250Aresearch%2520track%252C%2520we%2520propose%2520the%2520Chain-of-Embedding%2520%2528CoE%2529%2520in%2520the%2520latent%2520space%2520to%250Aenable%2520LLMs%2520to%2520perform%2520output-free%2520self-evaluation.%2520CoE%2520consists%2520of%2520all%250Aprogressive%2520hidden%2520states%2520produced%2520during%2520the%2520inference%2520time%252C%2520which%2520can%2520be%250Atreated%2520as%2520the%2520latent%2520thinking%2520path%2520of%2520LLMs.%2520We%2520find%2520that%2520when%2520LLMs%2520respond%250Acorrectly%2520and%2520incorrectly%252C%2520their%2520CoE%2520features%2520differ%252C%2520these%2520discrepancies%250Aassist%2520us%2520in%2520estimating%2520LLM%2520response%2520correctness.%2520Experiments%2520in%2520four%2520diverse%250Adomains%2520and%2520seven%2520LLMs%2520fully%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%250AMeanwhile%252C%2520its%2520label-free%2520design%2520intent%2520without%2520any%2520training%2520and%250Amillisecond-level%2520computational%2520cost%2520ensure%2520real-time%2520feedback%2520in%2520large-scale%250Ascenarios.%2520More%2520importantly%252C%2520we%2520provide%2520interesting%2520insights%2520into%2520LLM%2520response%250Acorrectness%2520from%2520the%2520perspective%2520of%2520hidden%2520state%2520changes%2520inside%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Chain-of-Embedding%20Enables%20Output-free%20LLM%20Self-Evaluation&entry.906535625=Yiming%20Wang%20and%20Pei%20Zhang%20and%20Baosong%20Yang%20and%20Derek%20F.%20Wong%20and%20Rui%20Wang&entry.1292438233=%20%20LLM%20self-evaluation%20relies%20on%20the%20LLM%27s%20own%20ability%20to%20estimate%20response%0Acorrectness%2C%20which%20can%20greatly%20improve%20its%20deployment%20reliability.%20In%20this%0Aresearch%20track%2C%20we%20propose%20the%20Chain-of-Embedding%20%28CoE%29%20in%20the%20latent%20space%20to%0Aenable%20LLMs%20to%20perform%20output-free%20self-evaluation.%20CoE%20consists%20of%20all%0Aprogressive%20hidden%20states%20produced%20during%20the%20inference%20time%2C%20which%20can%20be%0Atreated%20as%20the%20latent%20thinking%20path%20of%20LLMs.%20We%20find%20that%20when%20LLMs%20respond%0Acorrectly%20and%20incorrectly%2C%20their%20CoE%20features%20differ%2C%20these%20discrepancies%0Aassist%20us%20in%20estimating%20LLM%20response%20correctness.%20Experiments%20in%20four%20diverse%0Adomains%20and%20seven%20LLMs%20fully%20demonstrate%20the%20effectiveness%20of%20our%20method.%0AMeanwhile%2C%20its%20label-free%20design%20intent%20without%20any%20training%20and%0Amillisecond-level%20computational%20cost%20ensure%20real-time%20feedback%20in%20large-scale%0Ascenarios.%20More%20importantly%2C%20we%20provide%20interesting%20insights%20into%20LLM%20response%0Acorrectness%20from%20the%20perspective%20of%20hidden%20state%20changes%20inside%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13640v1&entry.124074799=Read"},
{"title": "VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic\n  Reasoning Tasks", "author": "Shailaja Keyur Sampat and Mutsumi Nakamura and Shankar Kailas and Kartik Aggarwal and Mandy Zhou and Yezhou Yang and Chitta Baral", "abstract": "  Deriving inference from heterogeneous inputs (such as images, text, and\naudio) is an important skill for humans to perform day-to-day tasks. A similar\nability is desirable for the development of advanced Artificial Intelligence\n(AI) systems. While state-of-the-art models are rapidly closing the gap with\nhuman-level performance on diverse computer vision and NLP tasks separately,\nthey struggle to solve tasks that require joint reasoning over visual and\ntextual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask\nbenchmark for natural language understanding, we propose VL-GLUE in this paper.\nVL-GLUE consists of over 100k samples spanned across seven different tasks,\nwhich at their core require visuo-linguistic reasoning. Moreover, our benchmark\ncomprises of diverse image types (from synthetically rendered figures, and\nday-to-day scenes to charts and complex diagrams) and includes a broad variety\nof domain-specific text (from cooking, politics, and sports to high-school\ncurricula), demonstrating the need for multi-modal understanding in the\nreal-world. We show that this benchmark is quite challenging for existing\nlarge-scale vision-language models and encourage development of systems that\npossess robust visuo-linguistic reasoning capabilities.\n", "link": "http://arxiv.org/abs/2410.13666v1", "date": "2024-10-17", "relevancy": 2.395, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6139}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-GLUE%3A%20A%20Suite%20of%20Fundamental%20yet%20Challenging%20Visuo-Linguistic%0A%20%20Reasoning%20Tasks&body=Title%3A%20VL-GLUE%3A%20A%20Suite%20of%20Fundamental%20yet%20Challenging%20Visuo-Linguistic%0A%20%20Reasoning%20Tasks%0AAuthor%3A%20Shailaja%20Keyur%20Sampat%20and%20Mutsumi%20Nakamura%20and%20Shankar%20Kailas%20and%20Kartik%20Aggarwal%20and%20Mandy%20Zhou%20and%20Yezhou%20Yang%20and%20Chitta%20Baral%0AAbstract%3A%20%20%20Deriving%20inference%20from%20heterogeneous%20inputs%20%28such%20as%20images%2C%20text%2C%20and%0Aaudio%29%20is%20an%20important%20skill%20for%20humans%20to%20perform%20day-to-day%20tasks.%20A%20similar%0Aability%20is%20desirable%20for%20the%20development%20of%20advanced%20Artificial%20Intelligence%0A%28AI%29%20systems.%20While%20state-of-the-art%20models%20are%20rapidly%20closing%20the%20gap%20with%0Ahuman-level%20performance%20on%20diverse%20computer%20vision%20and%20NLP%20tasks%20separately%2C%0Athey%20struggle%20to%20solve%20tasks%20that%20require%20joint%20reasoning%20over%20visual%20and%0Atextual%20modalities.%20Inspired%20by%20GLUE%20%28Wang%20et.%20al.%2C%202018%29-%20a%20multitask%0Abenchmark%20for%20natural%20language%20understanding%2C%20we%20propose%20VL-GLUE%20in%20this%20paper.%0AVL-GLUE%20consists%20of%20over%20100k%20samples%20spanned%20across%20seven%20different%20tasks%2C%0Awhich%20at%20their%20core%20require%20visuo-linguistic%20reasoning.%20Moreover%2C%20our%20benchmark%0Acomprises%20of%20diverse%20image%20types%20%28from%20synthetically%20rendered%20figures%2C%20and%0Aday-to-day%20scenes%20to%20charts%20and%20complex%20diagrams%29%20and%20includes%20a%20broad%20variety%0Aof%20domain-specific%20text%20%28from%20cooking%2C%20politics%2C%20and%20sports%20to%20high-school%0Acurricula%29%2C%20demonstrating%20the%20need%20for%20multi-modal%20understanding%20in%20the%0Areal-world.%20We%20show%20that%20this%20benchmark%20is%20quite%20challenging%20for%20existing%0Alarge-scale%20vision-language%20models%20and%20encourage%20development%20of%20systems%20that%0Apossess%20robust%20visuo-linguistic%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-GLUE%253A%2520A%2520Suite%2520of%2520Fundamental%2520yet%2520Challenging%2520Visuo-Linguistic%250A%2520%2520Reasoning%2520Tasks%26entry.906535625%3DShailaja%2520Keyur%2520Sampat%2520and%2520Mutsumi%2520Nakamura%2520and%2520Shankar%2520Kailas%2520and%2520Kartik%2520Aggarwal%2520and%2520Mandy%2520Zhou%2520and%2520Yezhou%2520Yang%2520and%2520Chitta%2520Baral%26entry.1292438233%3D%2520%2520Deriving%2520inference%2520from%2520heterogeneous%2520inputs%2520%2528such%2520as%2520images%252C%2520text%252C%2520and%250Aaudio%2529%2520is%2520an%2520important%2520skill%2520for%2520humans%2520to%2520perform%2520day-to-day%2520tasks.%2520A%2520similar%250Aability%2520is%2520desirable%2520for%2520the%2520development%2520of%2520advanced%2520Artificial%2520Intelligence%250A%2528AI%2529%2520systems.%2520While%2520state-of-the-art%2520models%2520are%2520rapidly%2520closing%2520the%2520gap%2520with%250Ahuman-level%2520performance%2520on%2520diverse%2520computer%2520vision%2520and%2520NLP%2520tasks%2520separately%252C%250Athey%2520struggle%2520to%2520solve%2520tasks%2520that%2520require%2520joint%2520reasoning%2520over%2520visual%2520and%250Atextual%2520modalities.%2520Inspired%2520by%2520GLUE%2520%2528Wang%2520et.%2520al.%252C%25202018%2529-%2520a%2520multitask%250Abenchmark%2520for%2520natural%2520language%2520understanding%252C%2520we%2520propose%2520VL-GLUE%2520in%2520this%2520paper.%250AVL-GLUE%2520consists%2520of%2520over%2520100k%2520samples%2520spanned%2520across%2520seven%2520different%2520tasks%252C%250Awhich%2520at%2520their%2520core%2520require%2520visuo-linguistic%2520reasoning.%2520Moreover%252C%2520our%2520benchmark%250Acomprises%2520of%2520diverse%2520image%2520types%2520%2528from%2520synthetically%2520rendered%2520figures%252C%2520and%250Aday-to-day%2520scenes%2520to%2520charts%2520and%2520complex%2520diagrams%2529%2520and%2520includes%2520a%2520broad%2520variety%250Aof%2520domain-specific%2520text%2520%2528from%2520cooking%252C%2520politics%252C%2520and%2520sports%2520to%2520high-school%250Acurricula%2529%252C%2520demonstrating%2520the%2520need%2520for%2520multi-modal%2520understanding%2520in%2520the%250Areal-world.%2520We%2520show%2520that%2520this%2520benchmark%2520is%2520quite%2520challenging%2520for%2520existing%250Alarge-scale%2520vision-language%2520models%2520and%2520encourage%2520development%2520of%2520systems%2520that%250Apossess%2520robust%2520visuo-linguistic%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-GLUE%3A%20A%20Suite%20of%20Fundamental%20yet%20Challenging%20Visuo-Linguistic%0A%20%20Reasoning%20Tasks&entry.906535625=Shailaja%20Keyur%20Sampat%20and%20Mutsumi%20Nakamura%20and%20Shankar%20Kailas%20and%20Kartik%20Aggarwal%20and%20Mandy%20Zhou%20and%20Yezhou%20Yang%20and%20Chitta%20Baral&entry.1292438233=%20%20Deriving%20inference%20from%20heterogeneous%20inputs%20%28such%20as%20images%2C%20text%2C%20and%0Aaudio%29%20is%20an%20important%20skill%20for%20humans%20to%20perform%20day-to-day%20tasks.%20A%20similar%0Aability%20is%20desirable%20for%20the%20development%20of%20advanced%20Artificial%20Intelligence%0A%28AI%29%20systems.%20While%20state-of-the-art%20models%20are%20rapidly%20closing%20the%20gap%20with%0Ahuman-level%20performance%20on%20diverse%20computer%20vision%20and%20NLP%20tasks%20separately%2C%0Athey%20struggle%20to%20solve%20tasks%20that%20require%20joint%20reasoning%20over%20visual%20and%0Atextual%20modalities.%20Inspired%20by%20GLUE%20%28Wang%20et.%20al.%2C%202018%29-%20a%20multitask%0Abenchmark%20for%20natural%20language%20understanding%2C%20we%20propose%20VL-GLUE%20in%20this%20paper.%0AVL-GLUE%20consists%20of%20over%20100k%20samples%20spanned%20across%20seven%20different%20tasks%2C%0Awhich%20at%20their%20core%20require%20visuo-linguistic%20reasoning.%20Moreover%2C%20our%20benchmark%0Acomprises%20of%20diverse%20image%20types%20%28from%20synthetically%20rendered%20figures%2C%20and%0Aday-to-day%20scenes%20to%20charts%20and%20complex%20diagrams%29%20and%20includes%20a%20broad%20variety%0Aof%20domain-specific%20text%20%28from%20cooking%2C%20politics%2C%20and%20sports%20to%20high-school%0Acurricula%29%2C%20demonstrating%20the%20need%20for%20multi-modal%20understanding%20in%20the%0Areal-world.%20We%20show%20that%20this%20benchmark%20is%20quite%20challenging%20for%20existing%0Alarge-scale%20vision-language%20models%20and%20encourage%20development%20of%20systems%20that%0Apossess%20robust%20visuo-linguistic%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13666v1&entry.124074799=Read"},
{"title": "Fluid: Scaling Autoregressive Text-to-image Generative Models with\n  Continuous Tokens", "author": "Lijie Fan and Tianhong Li and Siyang Qin and Yuanzhen Li and Chen Sun and Michael Rubinstein and Deqing Sun and Kaiming He and Yonglong Tian", "abstract": "  Scaling up autoregressive models in vision has not proven as beneficial as in\nlarge language models. In this work, we investigate this scaling problem in the\ncontext of text-to-image generation, focusing on two critical factors: whether\nmodels use discrete or continuous tokens, and whether tokens are generated in a\nrandom or fixed raster order using BERT- or GPT-like transformer architectures.\nOur empirical results show that, while all models scale effectively in terms of\nvalidation loss, their evaluation performance -- measured by FID, GenEval\nscore, and visual quality -- follows different trends. Models based on\ncontinuous tokens achieve significantly better visual quality than those using\ndiscrete tokens. Furthermore, the generation order and attention mechanisms\nsignificantly affect the GenEval score: random-order models achieve notably\nbetter GenEval scores compared to raster-order models. Inspired by these\nfindings, we train Fluid, a random-order autoregressive model on continuous\ntokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16\non MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our\nfindings and results will encourage future efforts to further bridge the\nscaling gap between vision and language models.\n", "link": "http://arxiv.org/abs/2410.13863v1", "date": "2024-10-17", "relevancy": 2.3658, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.714}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5819}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fluid%3A%20Scaling%20Autoregressive%20Text-to-image%20Generative%20Models%20with%0A%20%20Continuous%20Tokens&body=Title%3A%20Fluid%3A%20Scaling%20Autoregressive%20Text-to-image%20Generative%20Models%20with%0A%20%20Continuous%20Tokens%0AAuthor%3A%20Lijie%20Fan%20and%20Tianhong%20Li%20and%20Siyang%20Qin%20and%20Yuanzhen%20Li%20and%20Chen%20Sun%20and%20Michael%20Rubinstein%20and%20Deqing%20Sun%20and%20Kaiming%20He%20and%20Yonglong%20Tian%0AAbstract%3A%20%20%20Scaling%20up%20autoregressive%20models%20in%20vision%20has%20not%20proven%20as%20beneficial%20as%20in%0Alarge%20language%20models.%20In%20this%20work%2C%20we%20investigate%20this%20scaling%20problem%20in%20the%0Acontext%20of%20text-to-image%20generation%2C%20focusing%20on%20two%20critical%20factors%3A%20whether%0Amodels%20use%20discrete%20or%20continuous%20tokens%2C%20and%20whether%20tokens%20are%20generated%20in%20a%0Arandom%20or%20fixed%20raster%20order%20using%20BERT-%20or%20GPT-like%20transformer%20architectures.%0AOur%20empirical%20results%20show%20that%2C%20while%20all%20models%20scale%20effectively%20in%20terms%20of%0Avalidation%20loss%2C%20their%20evaluation%20performance%20--%20measured%20by%20FID%2C%20GenEval%0Ascore%2C%20and%20visual%20quality%20--%20follows%20different%20trends.%20Models%20based%20on%0Acontinuous%20tokens%20achieve%20significantly%20better%20visual%20quality%20than%20those%20using%0Adiscrete%20tokens.%20Furthermore%2C%20the%20generation%20order%20and%20attention%20mechanisms%0Asignificantly%20affect%20the%20GenEval%20score%3A%20random-order%20models%20achieve%20notably%0Abetter%20GenEval%20scores%20compared%20to%20raster-order%20models.%20Inspired%20by%20these%0Afindings%2C%20we%20train%20Fluid%2C%20a%20random-order%20autoregressive%20model%20on%20continuous%0Atokens.%20Fluid%2010.5B%20model%20achieves%20a%20new%20state-of-the-art%20zero-shot%20FID%20of%206.16%0Aon%20MS-COCO%2030K%2C%20and%200.69%20overall%20score%20on%20the%20GenEval%20benchmark.%20We%20hope%20our%0Afindings%20and%20results%20will%20encourage%20future%20efforts%20to%20further%20bridge%20the%0Ascaling%20gap%20between%20vision%20and%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFluid%253A%2520Scaling%2520Autoregressive%2520Text-to-image%2520Generative%2520Models%2520with%250A%2520%2520Continuous%2520Tokens%26entry.906535625%3DLijie%2520Fan%2520and%2520Tianhong%2520Li%2520and%2520Siyang%2520Qin%2520and%2520Yuanzhen%2520Li%2520and%2520Chen%2520Sun%2520and%2520Michael%2520Rubinstein%2520and%2520Deqing%2520Sun%2520and%2520Kaiming%2520He%2520and%2520Yonglong%2520Tian%26entry.1292438233%3D%2520%2520Scaling%2520up%2520autoregressive%2520models%2520in%2520vision%2520has%2520not%2520proven%2520as%2520beneficial%2520as%2520in%250Alarge%2520language%2520models.%2520In%2520this%2520work%252C%2520we%2520investigate%2520this%2520scaling%2520problem%2520in%2520the%250Acontext%2520of%2520text-to-image%2520generation%252C%2520focusing%2520on%2520two%2520critical%2520factors%253A%2520whether%250Amodels%2520use%2520discrete%2520or%2520continuous%2520tokens%252C%2520and%2520whether%2520tokens%2520are%2520generated%2520in%2520a%250Arandom%2520or%2520fixed%2520raster%2520order%2520using%2520BERT-%2520or%2520GPT-like%2520transformer%2520architectures.%250AOur%2520empirical%2520results%2520show%2520that%252C%2520while%2520all%2520models%2520scale%2520effectively%2520in%2520terms%2520of%250Avalidation%2520loss%252C%2520their%2520evaluation%2520performance%2520--%2520measured%2520by%2520FID%252C%2520GenEval%250Ascore%252C%2520and%2520visual%2520quality%2520--%2520follows%2520different%2520trends.%2520Models%2520based%2520on%250Acontinuous%2520tokens%2520achieve%2520significantly%2520better%2520visual%2520quality%2520than%2520those%2520using%250Adiscrete%2520tokens.%2520Furthermore%252C%2520the%2520generation%2520order%2520and%2520attention%2520mechanisms%250Asignificantly%2520affect%2520the%2520GenEval%2520score%253A%2520random-order%2520models%2520achieve%2520notably%250Abetter%2520GenEval%2520scores%2520compared%2520to%2520raster-order%2520models.%2520Inspired%2520by%2520these%250Afindings%252C%2520we%2520train%2520Fluid%252C%2520a%2520random-order%2520autoregressive%2520model%2520on%2520continuous%250Atokens.%2520Fluid%252010.5B%2520model%2520achieves%2520a%2520new%2520state-of-the-art%2520zero-shot%2520FID%2520of%25206.16%250Aon%2520MS-COCO%252030K%252C%2520and%25200.69%2520overall%2520score%2520on%2520the%2520GenEval%2520benchmark.%2520We%2520hope%2520our%250Afindings%2520and%2520results%2520will%2520encourage%2520future%2520efforts%2520to%2520further%2520bridge%2520the%250Ascaling%2520gap%2520between%2520vision%2520and%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fluid%3A%20Scaling%20Autoregressive%20Text-to-image%20Generative%20Models%20with%0A%20%20Continuous%20Tokens&entry.906535625=Lijie%20Fan%20and%20Tianhong%20Li%20and%20Siyang%20Qin%20and%20Yuanzhen%20Li%20and%20Chen%20Sun%20and%20Michael%20Rubinstein%20and%20Deqing%20Sun%20and%20Kaiming%20He%20and%20Yonglong%20Tian&entry.1292438233=%20%20Scaling%20up%20autoregressive%20models%20in%20vision%20has%20not%20proven%20as%20beneficial%20as%20in%0Alarge%20language%20models.%20In%20this%20work%2C%20we%20investigate%20this%20scaling%20problem%20in%20the%0Acontext%20of%20text-to-image%20generation%2C%20focusing%20on%20two%20critical%20factors%3A%20whether%0Amodels%20use%20discrete%20or%20continuous%20tokens%2C%20and%20whether%20tokens%20are%20generated%20in%20a%0Arandom%20or%20fixed%20raster%20order%20using%20BERT-%20or%20GPT-like%20transformer%20architectures.%0AOur%20empirical%20results%20show%20that%2C%20while%20all%20models%20scale%20effectively%20in%20terms%20of%0Avalidation%20loss%2C%20their%20evaluation%20performance%20--%20measured%20by%20FID%2C%20GenEval%0Ascore%2C%20and%20visual%20quality%20--%20follows%20different%20trends.%20Models%20based%20on%0Acontinuous%20tokens%20achieve%20significantly%20better%20visual%20quality%20than%20those%20using%0Adiscrete%20tokens.%20Furthermore%2C%20the%20generation%20order%20and%20attention%20mechanisms%0Asignificantly%20affect%20the%20GenEval%20score%3A%20random-order%20models%20achieve%20notably%0Abetter%20GenEval%20scores%20compared%20to%20raster-order%20models.%20Inspired%20by%20these%0Afindings%2C%20we%20train%20Fluid%2C%20a%20random-order%20autoregressive%20model%20on%20continuous%0Atokens.%20Fluid%2010.5B%20model%20achieves%20a%20new%20state-of-the-art%20zero-shot%20FID%20of%206.16%0Aon%20MS-COCO%2030K%2C%20and%200.69%20overall%20score%20on%20the%20GenEval%20benchmark.%20We%20hope%20our%0Afindings%20and%20results%20will%20encourage%20future%20efforts%20to%20further%20bridge%20the%0Ascaling%20gap%20between%20vision%20and%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13863v1&entry.124074799=Read"},
{"title": "DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework\n  for Talking Head Video Generation", "author": "Hanbo Cheng and Limin Lin and Chenyu Liu and Pengcheng Xia and Pengfei Hu and Jiefeng Ma and Jun Du and Jia Pan", "abstract": "  Talking head generation intends to produce vivid and realistic talking head\nvideos from a single portrait and speech audio clip. Although significant\nprogress has been made in diffusion-based talking head generation, almost all\nmethods rely on autoregressive strategies, which suffer from limited context\nutilization beyond the current generation step, error accumulation, and slower\ngeneration speed. To address these challenges, we present DAWN (Dynamic frame\nAvatar With Non-autoregressive diffusion), a framework that enables all-at-once\ngeneration of dynamic-length video sequences. Specifically, it consists of two\nmain components: (1) audio-driven holistic facial dynamics generation in the\nlatent motion space, and (2) audio-driven head pose and blink generation.\nExtensive experiments demonstrate that our method generates authentic and vivid\nvideos with precise lip motions, and natural pose/blink movements.\nAdditionally, with a high generation speed, DAWN possesses strong extrapolation\ncapabilities, ensuring the stable production of high-quality long videos. These\nresults highlight the considerable promise and potential impact of DAWN in the\nfield of talking head video generation. Furthermore, we hope that DAWN sparks\nfurther exploration of non-autoregressive approaches in diffusion models. Our\ncode will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.\n", "link": "http://arxiv.org/abs/2410.13726v1", "date": "2024-10-17", "relevancy": 2.3561, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6032}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5967}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAWN%3A%20Dynamic%20Frame%20Avatar%20with%20Non-autoregressive%20Diffusion%20Framework%0A%20%20for%20Talking%20Head%20Video%20Generation&body=Title%3A%20DAWN%3A%20Dynamic%20Frame%20Avatar%20with%20Non-autoregressive%20Diffusion%20Framework%0A%20%20for%20Talking%20Head%20Video%20Generation%0AAuthor%3A%20Hanbo%20Cheng%20and%20Limin%20Lin%20and%20Chenyu%20Liu%20and%20Pengcheng%20Xia%20and%20Pengfei%20Hu%20and%20Jiefeng%20Ma%20and%20Jun%20Du%20and%20Jia%20Pan%0AAbstract%3A%20%20%20Talking%20head%20generation%20intends%20to%20produce%20vivid%20and%20realistic%20talking%20head%0Avideos%20from%20a%20single%20portrait%20and%20speech%20audio%20clip.%20Although%20significant%0Aprogress%20has%20been%20made%20in%20diffusion-based%20talking%20head%20generation%2C%20almost%20all%0Amethods%20rely%20on%20autoregressive%20strategies%2C%20which%20suffer%20from%20limited%20context%0Autilization%20beyond%20the%20current%20generation%20step%2C%20error%20accumulation%2C%20and%20slower%0Ageneration%20speed.%20To%20address%20these%20challenges%2C%20we%20present%20DAWN%20%28Dynamic%20frame%0AAvatar%20With%20Non-autoregressive%20diffusion%29%2C%20a%20framework%20that%20enables%20all-at-once%0Ageneration%20of%20dynamic-length%20video%20sequences.%20Specifically%2C%20it%20consists%20of%20two%0Amain%20components%3A%20%281%29%20audio-driven%20holistic%20facial%20dynamics%20generation%20in%20the%0Alatent%20motion%20space%2C%20and%20%282%29%20audio-driven%20head%20pose%20and%20blink%20generation.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20generates%20authentic%20and%20vivid%0Avideos%20with%20precise%20lip%20motions%2C%20and%20natural%20pose/blink%20movements.%0AAdditionally%2C%20with%20a%20high%20generation%20speed%2C%20DAWN%20possesses%20strong%20extrapolation%0Acapabilities%2C%20ensuring%20the%20stable%20production%20of%20high-quality%20long%20videos.%20These%0Aresults%20highlight%20the%20considerable%20promise%20and%20potential%20impact%20of%20DAWN%20in%20the%0Afield%20of%20talking%20head%20video%20generation.%20Furthermore%2C%20we%20hope%20that%20DAWN%20sparks%0Afurther%20exploration%20of%20non-autoregressive%20approaches%20in%20diffusion%20models.%20Our%0Acode%20will%20be%20publicly%20at%20https%3A//github.com/Hanbo-Cheng/DAWN-pytorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAWN%253A%2520Dynamic%2520Frame%2520Avatar%2520with%2520Non-autoregressive%2520Diffusion%2520Framework%250A%2520%2520for%2520Talking%2520Head%2520Video%2520Generation%26entry.906535625%3DHanbo%2520Cheng%2520and%2520Limin%2520Lin%2520and%2520Chenyu%2520Liu%2520and%2520Pengcheng%2520Xia%2520and%2520Pengfei%2520Hu%2520and%2520Jiefeng%2520Ma%2520and%2520Jun%2520Du%2520and%2520Jia%2520Pan%26entry.1292438233%3D%2520%2520Talking%2520head%2520generation%2520intends%2520to%2520produce%2520vivid%2520and%2520realistic%2520talking%2520head%250Avideos%2520from%2520a%2520single%2520portrait%2520and%2520speech%2520audio%2520clip.%2520Although%2520significant%250Aprogress%2520has%2520been%2520made%2520in%2520diffusion-based%2520talking%2520head%2520generation%252C%2520almost%2520all%250Amethods%2520rely%2520on%2520autoregressive%2520strategies%252C%2520which%2520suffer%2520from%2520limited%2520context%250Autilization%2520beyond%2520the%2520current%2520generation%2520step%252C%2520error%2520accumulation%252C%2520and%2520slower%250Ageneration%2520speed.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520DAWN%2520%2528Dynamic%2520frame%250AAvatar%2520With%2520Non-autoregressive%2520diffusion%2529%252C%2520a%2520framework%2520that%2520enables%2520all-at-once%250Ageneration%2520of%2520dynamic-length%2520video%2520sequences.%2520Specifically%252C%2520it%2520consists%2520of%2520two%250Amain%2520components%253A%2520%25281%2529%2520audio-driven%2520holistic%2520facial%2520dynamics%2520generation%2520in%2520the%250Alatent%2520motion%2520space%252C%2520and%2520%25282%2529%2520audio-driven%2520head%2520pose%2520and%2520blink%2520generation.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520generates%2520authentic%2520and%2520vivid%250Avideos%2520with%2520precise%2520lip%2520motions%252C%2520and%2520natural%2520pose/blink%2520movements.%250AAdditionally%252C%2520with%2520a%2520high%2520generation%2520speed%252C%2520DAWN%2520possesses%2520strong%2520extrapolation%250Acapabilities%252C%2520ensuring%2520the%2520stable%2520production%2520of%2520high-quality%2520long%2520videos.%2520These%250Aresults%2520highlight%2520the%2520considerable%2520promise%2520and%2520potential%2520impact%2520of%2520DAWN%2520in%2520the%250Afield%2520of%2520talking%2520head%2520video%2520generation.%2520Furthermore%252C%2520we%2520hope%2520that%2520DAWN%2520sparks%250Afurther%2520exploration%2520of%2520non-autoregressive%2520approaches%2520in%2520diffusion%2520models.%2520Our%250Acode%2520will%2520be%2520publicly%2520at%2520https%253A//github.com/Hanbo-Cheng/DAWN-pytorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAWN%3A%20Dynamic%20Frame%20Avatar%20with%20Non-autoregressive%20Diffusion%20Framework%0A%20%20for%20Talking%20Head%20Video%20Generation&entry.906535625=Hanbo%20Cheng%20and%20Limin%20Lin%20and%20Chenyu%20Liu%20and%20Pengcheng%20Xia%20and%20Pengfei%20Hu%20and%20Jiefeng%20Ma%20and%20Jun%20Du%20and%20Jia%20Pan&entry.1292438233=%20%20Talking%20head%20generation%20intends%20to%20produce%20vivid%20and%20realistic%20talking%20head%0Avideos%20from%20a%20single%20portrait%20and%20speech%20audio%20clip.%20Although%20significant%0Aprogress%20has%20been%20made%20in%20diffusion-based%20talking%20head%20generation%2C%20almost%20all%0Amethods%20rely%20on%20autoregressive%20strategies%2C%20which%20suffer%20from%20limited%20context%0Autilization%20beyond%20the%20current%20generation%20step%2C%20error%20accumulation%2C%20and%20slower%0Ageneration%20speed.%20To%20address%20these%20challenges%2C%20we%20present%20DAWN%20%28Dynamic%20frame%0AAvatar%20With%20Non-autoregressive%20diffusion%29%2C%20a%20framework%20that%20enables%20all-at-once%0Ageneration%20of%20dynamic-length%20video%20sequences.%20Specifically%2C%20it%20consists%20of%20two%0Amain%20components%3A%20%281%29%20audio-driven%20holistic%20facial%20dynamics%20generation%20in%20the%0Alatent%20motion%20space%2C%20and%20%282%29%20audio-driven%20head%20pose%20and%20blink%20generation.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20generates%20authentic%20and%20vivid%0Avideos%20with%20precise%20lip%20motions%2C%20and%20natural%20pose/blink%20movements.%0AAdditionally%2C%20with%20a%20high%20generation%20speed%2C%20DAWN%20possesses%20strong%20extrapolation%0Acapabilities%2C%20ensuring%20the%20stable%20production%20of%20high-quality%20long%20videos.%20These%0Aresults%20highlight%20the%20considerable%20promise%20and%20potential%20impact%20of%20DAWN%20in%20the%0Afield%20of%20talking%20head%20video%20generation.%20Furthermore%2C%20we%20hope%20that%20DAWN%20sparks%0Afurther%20exploration%20of%20non-autoregressive%20approaches%20in%20diffusion%20models.%20Our%0Acode%20will%20be%20publicly%20at%20https%3A//github.com/Hanbo-Cheng/DAWN-pytorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13726v1&entry.124074799=Read"},
{"title": "Looking Inward: Language Models Can Learn About Themselves by\n  Introspection", "author": "Felix J Binder and James Chua and Tomek Korbak and Henry Sleight and John Hughes and Robert Long and Ethan Perez and Miles Turpin and Owain Evans", "abstract": "  Humans acquire knowledge by observing the external world, but also by\nintrospection. Introspection gives a person privileged access to their current\nstate of mind (e.g., thoughts and feelings) that is not accessible to external\nobservers. Can LLMs introspect? We define introspection as acquiring knowledge\nthat is not contained in or derived from training data but instead originates\nfrom internal states. Such a capability could enhance model interpretability.\nInstead of painstakingly analyzing a model's internal workings, we could simply\nask the model about its beliefs, world models, and goals. More speculatively,\nan introspective model might self-report on whether it possesses certain\ninternal states such as subjective feelings or desires and this could inform us\nabout the moral status of these states. Such self-reports would not be entirely\ndictated by the model's training data.\n  We study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \"Given the input P, would your\noutput favor the short- or long-term option?\" If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior even if M2\nis trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself\nbetter than M2 (even if M2 is generally stronger).\n  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to\npredict itself), we find that the model M1 outperforms M2 in predicting itself,\nproviding evidence for introspection. Notably, M1 continues to predict its\nbehavior accurately even after we intentionally modify its ground-truth\nbehavior. However, while we successfully elicit introspection on simple tasks,\nwe are unsuccessful on more complex tasks or those requiring\nout-of-distribution generalization.\n", "link": "http://arxiv.org/abs/2410.13787v1", "date": "2024-10-17", "relevancy": 2.3513, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20Inward%3A%20Language%20Models%20Can%20Learn%20About%20Themselves%20by%0A%20%20Introspection&body=Title%3A%20Looking%20Inward%3A%20Language%20Models%20Can%20Learn%20About%20Themselves%20by%0A%20%20Introspection%0AAuthor%3A%20Felix%20J%20Binder%20and%20James%20Chua%20and%20Tomek%20Korbak%20and%20Henry%20Sleight%20and%20John%20Hughes%20and%20Robert%20Long%20and%20Ethan%20Perez%20and%20Miles%20Turpin%20and%20Owain%20Evans%0AAbstract%3A%20%20%20Humans%20acquire%20knowledge%20by%20observing%20the%20external%20world%2C%20but%20also%20by%0Aintrospection.%20Introspection%20gives%20a%20person%20privileged%20access%20to%20their%20current%0Astate%20of%20mind%20%28e.g.%2C%20thoughts%20and%20feelings%29%20that%20is%20not%20accessible%20to%20external%0Aobservers.%20Can%20LLMs%20introspect%3F%20We%20define%20introspection%20as%20acquiring%20knowledge%0Athat%20is%20not%20contained%20in%20or%20derived%20from%20training%20data%20but%20instead%20originates%0Afrom%20internal%20states.%20Such%20a%20capability%20could%20enhance%20model%20interpretability.%0AInstead%20of%20painstakingly%20analyzing%20a%20model%27s%20internal%20workings%2C%20we%20could%20simply%0Aask%20the%20model%20about%20its%20beliefs%2C%20world%20models%2C%20and%20goals.%20More%20speculatively%2C%0Aan%20introspective%20model%20might%20self-report%20on%20whether%20it%20possesses%20certain%0Ainternal%20states%20such%20as%20subjective%20feelings%20or%20desires%20and%20this%20could%20inform%20us%0Aabout%20the%20moral%20status%20of%20these%20states.%20Such%20self-reports%20would%20not%20be%20entirely%0Adictated%20by%20the%20model%27s%20training%20data.%0A%20%20We%20study%20introspection%20by%20finetuning%20LLMs%20to%20predict%20properties%20of%20their%20own%0Abehavior%20in%20hypothetical%20scenarios.%20For%20example%2C%20%22Given%20the%20input%20P%2C%20would%20your%0Aoutput%20favor%20the%20short-%20or%20long-term%20option%3F%22%20If%20a%20model%20M1%20can%20introspect%2C%20it%0Ashould%20outperform%20a%20different%20model%20M2%20in%20predicting%20M1%27s%20behavior%20even%20if%20M2%0Ais%20trained%20on%20M1%27s%20ground-truth%20behavior.%20The%20idea%20is%20that%20M1%20has%20privileged%0Aaccess%20to%20its%20own%20behavioral%20tendencies%2C%20and%20this%20enables%20it%20to%20predict%20itself%0Abetter%20than%20M2%20%28even%20if%20M2%20is%20generally%20stronger%29.%0A%20%20In%20experiments%20with%20GPT-4%2C%20GPT-4o%2C%20and%20Llama-3%20models%20%28each%20finetuned%20to%0Apredict%20itself%29%2C%20we%20find%20that%20the%20model%20M1%20outperforms%20M2%20in%20predicting%20itself%2C%0Aproviding%20evidence%20for%20introspection.%20Notably%2C%20M1%20continues%20to%20predict%20its%0Abehavior%20accurately%20even%20after%20we%20intentionally%20modify%20its%20ground-truth%0Abehavior.%20However%2C%20while%20we%20successfully%20elicit%20introspection%20on%20simple%20tasks%2C%0Awe%20are%20unsuccessful%20on%20more%20complex%20tasks%20or%20those%20requiring%0Aout-of-distribution%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520Inward%253A%2520Language%2520Models%2520Can%2520Learn%2520About%2520Themselves%2520by%250A%2520%2520Introspection%26entry.906535625%3DFelix%2520J%2520Binder%2520and%2520James%2520Chua%2520and%2520Tomek%2520Korbak%2520and%2520Henry%2520Sleight%2520and%2520John%2520Hughes%2520and%2520Robert%2520Long%2520and%2520Ethan%2520Perez%2520and%2520Miles%2520Turpin%2520and%2520Owain%2520Evans%26entry.1292438233%3D%2520%2520Humans%2520acquire%2520knowledge%2520by%2520observing%2520the%2520external%2520world%252C%2520but%2520also%2520by%250Aintrospection.%2520Introspection%2520gives%2520a%2520person%2520privileged%2520access%2520to%2520their%2520current%250Astate%2520of%2520mind%2520%2528e.g.%252C%2520thoughts%2520and%2520feelings%2529%2520that%2520is%2520not%2520accessible%2520to%2520external%250Aobservers.%2520Can%2520LLMs%2520introspect%253F%2520We%2520define%2520introspection%2520as%2520acquiring%2520knowledge%250Athat%2520is%2520not%2520contained%2520in%2520or%2520derived%2520from%2520training%2520data%2520but%2520instead%2520originates%250Afrom%2520internal%2520states.%2520Such%2520a%2520capability%2520could%2520enhance%2520model%2520interpretability.%250AInstead%2520of%2520painstakingly%2520analyzing%2520a%2520model%2527s%2520internal%2520workings%252C%2520we%2520could%2520simply%250Aask%2520the%2520model%2520about%2520its%2520beliefs%252C%2520world%2520models%252C%2520and%2520goals.%2520More%2520speculatively%252C%250Aan%2520introspective%2520model%2520might%2520self-report%2520on%2520whether%2520it%2520possesses%2520certain%250Ainternal%2520states%2520such%2520as%2520subjective%2520feelings%2520or%2520desires%2520and%2520this%2520could%2520inform%2520us%250Aabout%2520the%2520moral%2520status%2520of%2520these%2520states.%2520Such%2520self-reports%2520would%2520not%2520be%2520entirely%250Adictated%2520by%2520the%2520model%2527s%2520training%2520data.%250A%2520%2520We%2520study%2520introspection%2520by%2520finetuning%2520LLMs%2520to%2520predict%2520properties%2520of%2520their%2520own%250Abehavior%2520in%2520hypothetical%2520scenarios.%2520For%2520example%252C%2520%2522Given%2520the%2520input%2520P%252C%2520would%2520your%250Aoutput%2520favor%2520the%2520short-%2520or%2520long-term%2520option%253F%2522%2520If%2520a%2520model%2520M1%2520can%2520introspect%252C%2520it%250Ashould%2520outperform%2520a%2520different%2520model%2520M2%2520in%2520predicting%2520M1%2527s%2520behavior%2520even%2520if%2520M2%250Ais%2520trained%2520on%2520M1%2527s%2520ground-truth%2520behavior.%2520The%2520idea%2520is%2520that%2520M1%2520has%2520privileged%250Aaccess%2520to%2520its%2520own%2520behavioral%2520tendencies%252C%2520and%2520this%2520enables%2520it%2520to%2520predict%2520itself%250Abetter%2520than%2520M2%2520%2528even%2520if%2520M2%2520is%2520generally%2520stronger%2529.%250A%2520%2520In%2520experiments%2520with%2520GPT-4%252C%2520GPT-4o%252C%2520and%2520Llama-3%2520models%2520%2528each%2520finetuned%2520to%250Apredict%2520itself%2529%252C%2520we%2520find%2520that%2520the%2520model%2520M1%2520outperforms%2520M2%2520in%2520predicting%2520itself%252C%250Aproviding%2520evidence%2520for%2520introspection.%2520Notably%252C%2520M1%2520continues%2520to%2520predict%2520its%250Abehavior%2520accurately%2520even%2520after%2520we%2520intentionally%2520modify%2520its%2520ground-truth%250Abehavior.%2520However%252C%2520while%2520we%2520successfully%2520elicit%2520introspection%2520on%2520simple%2520tasks%252C%250Awe%2520are%2520unsuccessful%2520on%2520more%2520complex%2520tasks%2520or%2520those%2520requiring%250Aout-of-distribution%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20Inward%3A%20Language%20Models%20Can%20Learn%20About%20Themselves%20by%0A%20%20Introspection&entry.906535625=Felix%20J%20Binder%20and%20James%20Chua%20and%20Tomek%20Korbak%20and%20Henry%20Sleight%20and%20John%20Hughes%20and%20Robert%20Long%20and%20Ethan%20Perez%20and%20Miles%20Turpin%20and%20Owain%20Evans&entry.1292438233=%20%20Humans%20acquire%20knowledge%20by%20observing%20the%20external%20world%2C%20but%20also%20by%0Aintrospection.%20Introspection%20gives%20a%20person%20privileged%20access%20to%20their%20current%0Astate%20of%20mind%20%28e.g.%2C%20thoughts%20and%20feelings%29%20that%20is%20not%20accessible%20to%20external%0Aobservers.%20Can%20LLMs%20introspect%3F%20We%20define%20introspection%20as%20acquiring%20knowledge%0Athat%20is%20not%20contained%20in%20or%20derived%20from%20training%20data%20but%20instead%20originates%0Afrom%20internal%20states.%20Such%20a%20capability%20could%20enhance%20model%20interpretability.%0AInstead%20of%20painstakingly%20analyzing%20a%20model%27s%20internal%20workings%2C%20we%20could%20simply%0Aask%20the%20model%20about%20its%20beliefs%2C%20world%20models%2C%20and%20goals.%20More%20speculatively%2C%0Aan%20introspective%20model%20might%20self-report%20on%20whether%20it%20possesses%20certain%0Ainternal%20states%20such%20as%20subjective%20feelings%20or%20desires%20and%20this%20could%20inform%20us%0Aabout%20the%20moral%20status%20of%20these%20states.%20Such%20self-reports%20would%20not%20be%20entirely%0Adictated%20by%20the%20model%27s%20training%20data.%0A%20%20We%20study%20introspection%20by%20finetuning%20LLMs%20to%20predict%20properties%20of%20their%20own%0Abehavior%20in%20hypothetical%20scenarios.%20For%20example%2C%20%22Given%20the%20input%20P%2C%20would%20your%0Aoutput%20favor%20the%20short-%20or%20long-term%20option%3F%22%20If%20a%20model%20M1%20can%20introspect%2C%20it%0Ashould%20outperform%20a%20different%20model%20M2%20in%20predicting%20M1%27s%20behavior%20even%20if%20M2%0Ais%20trained%20on%20M1%27s%20ground-truth%20behavior.%20The%20idea%20is%20that%20M1%20has%20privileged%0Aaccess%20to%20its%20own%20behavioral%20tendencies%2C%20and%20this%20enables%20it%20to%20predict%20itself%0Abetter%20than%20M2%20%28even%20if%20M2%20is%20generally%20stronger%29.%0A%20%20In%20experiments%20with%20GPT-4%2C%20GPT-4o%2C%20and%20Llama-3%20models%20%28each%20finetuned%20to%0Apredict%20itself%29%2C%20we%20find%20that%20the%20model%20M1%20outperforms%20M2%20in%20predicting%20itself%2C%0Aproviding%20evidence%20for%20introspection.%20Notably%2C%20M1%20continues%20to%20predict%20its%0Abehavior%20accurately%20even%20after%20we%20intentionally%20modify%20its%20ground-truth%0Abehavior.%20However%2C%20while%20we%20successfully%20elicit%20introspection%20on%20simple%20tasks%2C%0Awe%20are%20unsuccessful%20on%20more%20complex%20tasks%20or%20those%20requiring%0Aout-of-distribution%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13787v1&entry.124074799=Read"},
{"title": "VidPanos: Generative Panoramic Videos from Casual Panning Videos", "author": "Jingwei Ma and Erika Lu and Roni Paiss and Shiran Zada and Aleksander Holynski and Tali Dekel and Brian Curless and Michael Rubinstein and Forrester Cole", "abstract": "  Panoramic image stitching provides a unified, wide-angle view of a scene that\nextends beyond the camera's field of view. Stitching frames of a panning video\ninto a panoramic photograph is a well-understood problem for stationary scenes,\nbut when objects are moving, a still panorama cannot capture the scene. We\npresent a method for synthesizing a panoramic video from a casually-captured\npanning video, as if the original video were captured with a wide-angle camera.\nWe pose panorama synthesis as a space-time outpainting problem, where we aim to\ncreate a full panoramic video of the same length as the input video. Consistent\ncompletion of the space-time volume requires a powerful, realistic prior over\nvideo content and motion, for which we adapt generative video models. Existing\ngenerative models do not, however, immediately extend to panorama completion,\nas we show. We instead apply video generation as a component of our panorama\nsynthesis system, and demonstrate how to exploit the strengths of the models\nwhile minimizing their limitations. Our system can create video panoramas for a\nrange of in-the-wild scenes including people, vehicles, and flowing water, as\nwell as stationary background features.\n", "link": "http://arxiv.org/abs/2410.13832v1", "date": "2024-10-17", "relevancy": 2.3123, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6117}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5753}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidPanos%3A%20Generative%20Panoramic%20Videos%20from%20Casual%20Panning%20Videos&body=Title%3A%20VidPanos%3A%20Generative%20Panoramic%20Videos%20from%20Casual%20Panning%20Videos%0AAuthor%3A%20Jingwei%20Ma%20and%20Erika%20Lu%20and%20Roni%20Paiss%20and%20Shiran%20Zada%20and%20Aleksander%20Holynski%20and%20Tali%20Dekel%20and%20Brian%20Curless%20and%20Michael%20Rubinstein%20and%20Forrester%20Cole%0AAbstract%3A%20%20%20Panoramic%20image%20stitching%20provides%20a%20unified%2C%20wide-angle%20view%20of%20a%20scene%20that%0Aextends%20beyond%20the%20camera%27s%20field%20of%20view.%20Stitching%20frames%20of%20a%20panning%20video%0Ainto%20a%20panoramic%20photograph%20is%20a%20well-understood%20problem%20for%20stationary%20scenes%2C%0Abut%20when%20objects%20are%20moving%2C%20a%20still%20panorama%20cannot%20capture%20the%20scene.%20We%0Apresent%20a%20method%20for%20synthesizing%20a%20panoramic%20video%20from%20a%20casually-captured%0Apanning%20video%2C%20as%20if%20the%20original%20video%20were%20captured%20with%20a%20wide-angle%20camera.%0AWe%20pose%20panorama%20synthesis%20as%20a%20space-time%20outpainting%20problem%2C%20where%20we%20aim%20to%0Acreate%20a%20full%20panoramic%20video%20of%20the%20same%20length%20as%20the%20input%20video.%20Consistent%0Acompletion%20of%20the%20space-time%20volume%20requires%20a%20powerful%2C%20realistic%20prior%20over%0Avideo%20content%20and%20motion%2C%20for%20which%20we%20adapt%20generative%20video%20models.%20Existing%0Agenerative%20models%20do%20not%2C%20however%2C%20immediately%20extend%20to%20panorama%20completion%2C%0Aas%20we%20show.%20We%20instead%20apply%20video%20generation%20as%20a%20component%20of%20our%20panorama%0Asynthesis%20system%2C%20and%20demonstrate%20how%20to%20exploit%20the%20strengths%20of%20the%20models%0Awhile%20minimizing%20their%20limitations.%20Our%20system%20can%20create%20video%20panoramas%20for%20a%0Arange%20of%20in-the-wild%20scenes%20including%20people%2C%20vehicles%2C%20and%20flowing%20water%2C%20as%0Awell%20as%20stationary%20background%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidPanos%253A%2520Generative%2520Panoramic%2520Videos%2520from%2520Casual%2520Panning%2520Videos%26entry.906535625%3DJingwei%2520Ma%2520and%2520Erika%2520Lu%2520and%2520Roni%2520Paiss%2520and%2520Shiran%2520Zada%2520and%2520Aleksander%2520Holynski%2520and%2520Tali%2520Dekel%2520and%2520Brian%2520Curless%2520and%2520Michael%2520Rubinstein%2520and%2520Forrester%2520Cole%26entry.1292438233%3D%2520%2520Panoramic%2520image%2520stitching%2520provides%2520a%2520unified%252C%2520wide-angle%2520view%2520of%2520a%2520scene%2520that%250Aextends%2520beyond%2520the%2520camera%2527s%2520field%2520of%2520view.%2520Stitching%2520frames%2520of%2520a%2520panning%2520video%250Ainto%2520a%2520panoramic%2520photograph%2520is%2520a%2520well-understood%2520problem%2520for%2520stationary%2520scenes%252C%250Abut%2520when%2520objects%2520are%2520moving%252C%2520a%2520still%2520panorama%2520cannot%2520capture%2520the%2520scene.%2520We%250Apresent%2520a%2520method%2520for%2520synthesizing%2520a%2520panoramic%2520video%2520from%2520a%2520casually-captured%250Apanning%2520video%252C%2520as%2520if%2520the%2520original%2520video%2520were%2520captured%2520with%2520a%2520wide-angle%2520camera.%250AWe%2520pose%2520panorama%2520synthesis%2520as%2520a%2520space-time%2520outpainting%2520problem%252C%2520where%2520we%2520aim%2520to%250Acreate%2520a%2520full%2520panoramic%2520video%2520of%2520the%2520same%2520length%2520as%2520the%2520input%2520video.%2520Consistent%250Acompletion%2520of%2520the%2520space-time%2520volume%2520requires%2520a%2520powerful%252C%2520realistic%2520prior%2520over%250Avideo%2520content%2520and%2520motion%252C%2520for%2520which%2520we%2520adapt%2520generative%2520video%2520models.%2520Existing%250Agenerative%2520models%2520do%2520not%252C%2520however%252C%2520immediately%2520extend%2520to%2520panorama%2520completion%252C%250Aas%2520we%2520show.%2520We%2520instead%2520apply%2520video%2520generation%2520as%2520a%2520component%2520of%2520our%2520panorama%250Asynthesis%2520system%252C%2520and%2520demonstrate%2520how%2520to%2520exploit%2520the%2520strengths%2520of%2520the%2520models%250Awhile%2520minimizing%2520their%2520limitations.%2520Our%2520system%2520can%2520create%2520video%2520panoramas%2520for%2520a%250Arange%2520of%2520in-the-wild%2520scenes%2520including%2520people%252C%2520vehicles%252C%2520and%2520flowing%2520water%252C%2520as%250Awell%2520as%2520stationary%2520background%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidPanos%3A%20Generative%20Panoramic%20Videos%20from%20Casual%20Panning%20Videos&entry.906535625=Jingwei%20Ma%20and%20Erika%20Lu%20and%20Roni%20Paiss%20and%20Shiran%20Zada%20and%20Aleksander%20Holynski%20and%20Tali%20Dekel%20and%20Brian%20Curless%20and%20Michael%20Rubinstein%20and%20Forrester%20Cole&entry.1292438233=%20%20Panoramic%20image%20stitching%20provides%20a%20unified%2C%20wide-angle%20view%20of%20a%20scene%20that%0Aextends%20beyond%20the%20camera%27s%20field%20of%20view.%20Stitching%20frames%20of%20a%20panning%20video%0Ainto%20a%20panoramic%20photograph%20is%20a%20well-understood%20problem%20for%20stationary%20scenes%2C%0Abut%20when%20objects%20are%20moving%2C%20a%20still%20panorama%20cannot%20capture%20the%20scene.%20We%0Apresent%20a%20method%20for%20synthesizing%20a%20panoramic%20video%20from%20a%20casually-captured%0Apanning%20video%2C%20as%20if%20the%20original%20video%20were%20captured%20with%20a%20wide-angle%20camera.%0AWe%20pose%20panorama%20synthesis%20as%20a%20space-time%20outpainting%20problem%2C%20where%20we%20aim%20to%0Acreate%20a%20full%20panoramic%20video%20of%20the%20same%20length%20as%20the%20input%20video.%20Consistent%0Acompletion%20of%20the%20space-time%20volume%20requires%20a%20powerful%2C%20realistic%20prior%20over%0Avideo%20content%20and%20motion%2C%20for%20which%20we%20adapt%20generative%20video%20models.%20Existing%0Agenerative%20models%20do%20not%2C%20however%2C%20immediately%20extend%20to%20panorama%20completion%2C%0Aas%20we%20show.%20We%20instead%20apply%20video%20generation%20as%20a%20component%20of%20our%20panorama%0Asynthesis%20system%2C%20and%20demonstrate%20how%20to%20exploit%20the%20strengths%20of%20the%20models%0Awhile%20minimizing%20their%20limitations.%20Our%20system%20can%20create%20video%20panoramas%20for%20a%0Arange%20of%20in-the-wild%20scenes%20including%20people%2C%20vehicles%2C%20and%20flowing%20water%2C%20as%0Awell%20as%20stationary%20background%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13832v1&entry.124074799=Read"},
{"title": "From Gradient Clipping to Normalization for Heavy Tailed SGD", "author": "Florian H\u00fcbler and Ilyas Fatkhullin and Niao He", "abstract": "  Recent empirical evidence indicates that many machine learning applications\ninvolve heavy-tailed gradient noise, which challenges the standard assumptions\nof bounded variance in stochastic optimization. Gradient clipping has emerged\nas a popular tool to handle this heavy-tailed noise, as it achieves good\nperformance in this setting both theoretically and practically. However, our\ncurrent theoretical understanding of non-convex gradient clipping has three\nmain shortcomings. First, the theory hinges on large, increasing clipping\nthresholds, which are in stark contrast to the small constant clipping\nthresholds employed in practice. Second, clipping thresholds require knowledge\nof problem-dependent parameters to guarantee convergence. Lastly, even with\nthis knowledge, current sampling complexity upper bounds for the method are\nsub-optimal in nearly all parameters. To address these issues, we study\nconvergence of Normalized SGD (NSGD). First, we establish a parameter-free\nsample complexity for NSGD of\n$\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$ to find an\n$\\varepsilon$-stationary point. Furthermore, we prove tightness of this result,\nby providing a matching algorithm-specific lower bound. In the setting where\nall problem parameters are known, we show this complexity is improved to\n$\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$, matching the\npreviously known lower bound for all first-order methods in all problem\ndependent parameters. Finally, we establish high-probability convergence of\nNSGD with a mild logarithmic dependence on the failure probability. Our work\ncomplements the studies of gradient clipping under heavy tailed noise improving\nthe sample complexities of existing algorithms and offering an alternative\nmechanism to achieve high probability convergence.\n", "link": "http://arxiv.org/abs/2410.13849v1", "date": "2024-10-17", "relevancy": 2.3059, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4744}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4549}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Gradient%20Clipping%20to%20Normalization%20for%20Heavy%20Tailed%20SGD&body=Title%3A%20From%20Gradient%20Clipping%20to%20Normalization%20for%20Heavy%20Tailed%20SGD%0AAuthor%3A%20Florian%20H%C3%BCbler%20and%20Ilyas%20Fatkhullin%20and%20Niao%20He%0AAbstract%3A%20%20%20Recent%20empirical%20evidence%20indicates%20that%20many%20machine%20learning%20applications%0Ainvolve%20heavy-tailed%20gradient%20noise%2C%20which%20challenges%20the%20standard%20assumptions%0Aof%20bounded%20variance%20in%20stochastic%20optimization.%20Gradient%20clipping%20has%20emerged%0Aas%20a%20popular%20tool%20to%20handle%20this%20heavy-tailed%20noise%2C%20as%20it%20achieves%20good%0Aperformance%20in%20this%20setting%20both%20theoretically%20and%20practically.%20However%2C%20our%0Acurrent%20theoretical%20understanding%20of%20non-convex%20gradient%20clipping%20has%20three%0Amain%20shortcomings.%20First%2C%20the%20theory%20hinges%20on%20large%2C%20increasing%20clipping%0Athresholds%2C%20which%20are%20in%20stark%20contrast%20to%20the%20small%20constant%20clipping%0Athresholds%20employed%20in%20practice.%20Second%2C%20clipping%20thresholds%20require%20knowledge%0Aof%20problem-dependent%20parameters%20to%20guarantee%20convergence.%20Lastly%2C%20even%20with%0Athis%20knowledge%2C%20current%20sampling%20complexity%20upper%20bounds%20for%20the%20method%20are%0Asub-optimal%20in%20nearly%20all%20parameters.%20To%20address%20these%20issues%2C%20we%20study%0Aconvergence%20of%20Normalized%20SGD%20%28NSGD%29.%20First%2C%20we%20establish%20a%20parameter-free%0Asample%20complexity%20for%20NSGD%20of%0A%24%5Cmathcal%7BO%7D%5Cleft%28%5Cvarepsilon%5E%7B-%5Cfrac%7B2p%7D%7Bp-1%7D%7D%5Cright%29%24%20to%20find%20an%0A%24%5Cvarepsilon%24-stationary%20point.%20Furthermore%2C%20we%20prove%20tightness%20of%20this%20result%2C%0Aby%20providing%20a%20matching%20algorithm-specific%20lower%20bound.%20In%20the%20setting%20where%0Aall%20problem%20parameters%20are%20known%2C%20we%20show%20this%20complexity%20is%20improved%20to%0A%24%5Cmathcal%7BO%7D%5Cleft%28%5Cvarepsilon%5E%7B-%5Cfrac%7B3p-2%7D%7Bp-1%7D%7D%5Cright%29%24%2C%20matching%20the%0Apreviously%20known%20lower%20bound%20for%20all%20first-order%20methods%20in%20all%20problem%0Adependent%20parameters.%20Finally%2C%20we%20establish%20high-probability%20convergence%20of%0ANSGD%20with%20a%20mild%20logarithmic%20dependence%20on%20the%20failure%20probability.%20Our%20work%0Acomplements%20the%20studies%20of%20gradient%20clipping%20under%20heavy%20tailed%20noise%20improving%0Athe%20sample%20complexities%20of%20existing%20algorithms%20and%20offering%20an%20alternative%0Amechanism%20to%20achieve%20high%20probability%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Gradient%2520Clipping%2520to%2520Normalization%2520for%2520Heavy%2520Tailed%2520SGD%26entry.906535625%3DFlorian%2520H%25C3%25BCbler%2520and%2520Ilyas%2520Fatkhullin%2520and%2520Niao%2520He%26entry.1292438233%3D%2520%2520Recent%2520empirical%2520evidence%2520indicates%2520that%2520many%2520machine%2520learning%2520applications%250Ainvolve%2520heavy-tailed%2520gradient%2520noise%252C%2520which%2520challenges%2520the%2520standard%2520assumptions%250Aof%2520bounded%2520variance%2520in%2520stochastic%2520optimization.%2520Gradient%2520clipping%2520has%2520emerged%250Aas%2520a%2520popular%2520tool%2520to%2520handle%2520this%2520heavy-tailed%2520noise%252C%2520as%2520it%2520achieves%2520good%250Aperformance%2520in%2520this%2520setting%2520both%2520theoretically%2520and%2520practically.%2520However%252C%2520our%250Acurrent%2520theoretical%2520understanding%2520of%2520non-convex%2520gradient%2520clipping%2520has%2520three%250Amain%2520shortcomings.%2520First%252C%2520the%2520theory%2520hinges%2520on%2520large%252C%2520increasing%2520clipping%250Athresholds%252C%2520which%2520are%2520in%2520stark%2520contrast%2520to%2520the%2520small%2520constant%2520clipping%250Athresholds%2520employed%2520in%2520practice.%2520Second%252C%2520clipping%2520thresholds%2520require%2520knowledge%250Aof%2520problem-dependent%2520parameters%2520to%2520guarantee%2520convergence.%2520Lastly%252C%2520even%2520with%250Athis%2520knowledge%252C%2520current%2520sampling%2520complexity%2520upper%2520bounds%2520for%2520the%2520method%2520are%250Asub-optimal%2520in%2520nearly%2520all%2520parameters.%2520To%2520address%2520these%2520issues%252C%2520we%2520study%250Aconvergence%2520of%2520Normalized%2520SGD%2520%2528NSGD%2529.%2520First%252C%2520we%2520establish%2520a%2520parameter-free%250Asample%2520complexity%2520for%2520NSGD%2520of%250A%2524%255Cmathcal%257BO%257D%255Cleft%2528%255Cvarepsilon%255E%257B-%255Cfrac%257B2p%257D%257Bp-1%257D%257D%255Cright%2529%2524%2520to%2520find%2520an%250A%2524%255Cvarepsilon%2524-stationary%2520point.%2520Furthermore%252C%2520we%2520prove%2520tightness%2520of%2520this%2520result%252C%250Aby%2520providing%2520a%2520matching%2520algorithm-specific%2520lower%2520bound.%2520In%2520the%2520setting%2520where%250Aall%2520problem%2520parameters%2520are%2520known%252C%2520we%2520show%2520this%2520complexity%2520is%2520improved%2520to%250A%2524%255Cmathcal%257BO%257D%255Cleft%2528%255Cvarepsilon%255E%257B-%255Cfrac%257B3p-2%257D%257Bp-1%257D%257D%255Cright%2529%2524%252C%2520matching%2520the%250Apreviously%2520known%2520lower%2520bound%2520for%2520all%2520first-order%2520methods%2520in%2520all%2520problem%250Adependent%2520parameters.%2520Finally%252C%2520we%2520establish%2520high-probability%2520convergence%2520of%250ANSGD%2520with%2520a%2520mild%2520logarithmic%2520dependence%2520on%2520the%2520failure%2520probability.%2520Our%2520work%250Acomplements%2520the%2520studies%2520of%2520gradient%2520clipping%2520under%2520heavy%2520tailed%2520noise%2520improving%250Athe%2520sample%2520complexities%2520of%2520existing%2520algorithms%2520and%2520offering%2520an%2520alternative%250Amechanism%2520to%2520achieve%2520high%2520probability%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Gradient%20Clipping%20to%20Normalization%20for%20Heavy%20Tailed%20SGD&entry.906535625=Florian%20H%C3%BCbler%20and%20Ilyas%20Fatkhullin%20and%20Niao%20He&entry.1292438233=%20%20Recent%20empirical%20evidence%20indicates%20that%20many%20machine%20learning%20applications%0Ainvolve%20heavy-tailed%20gradient%20noise%2C%20which%20challenges%20the%20standard%20assumptions%0Aof%20bounded%20variance%20in%20stochastic%20optimization.%20Gradient%20clipping%20has%20emerged%0Aas%20a%20popular%20tool%20to%20handle%20this%20heavy-tailed%20noise%2C%20as%20it%20achieves%20good%0Aperformance%20in%20this%20setting%20both%20theoretically%20and%20practically.%20However%2C%20our%0Acurrent%20theoretical%20understanding%20of%20non-convex%20gradient%20clipping%20has%20three%0Amain%20shortcomings.%20First%2C%20the%20theory%20hinges%20on%20large%2C%20increasing%20clipping%0Athresholds%2C%20which%20are%20in%20stark%20contrast%20to%20the%20small%20constant%20clipping%0Athresholds%20employed%20in%20practice.%20Second%2C%20clipping%20thresholds%20require%20knowledge%0Aof%20problem-dependent%20parameters%20to%20guarantee%20convergence.%20Lastly%2C%20even%20with%0Athis%20knowledge%2C%20current%20sampling%20complexity%20upper%20bounds%20for%20the%20method%20are%0Asub-optimal%20in%20nearly%20all%20parameters.%20To%20address%20these%20issues%2C%20we%20study%0Aconvergence%20of%20Normalized%20SGD%20%28NSGD%29.%20First%2C%20we%20establish%20a%20parameter-free%0Asample%20complexity%20for%20NSGD%20of%0A%24%5Cmathcal%7BO%7D%5Cleft%28%5Cvarepsilon%5E%7B-%5Cfrac%7B2p%7D%7Bp-1%7D%7D%5Cright%29%24%20to%20find%20an%0A%24%5Cvarepsilon%24-stationary%20point.%20Furthermore%2C%20we%20prove%20tightness%20of%20this%20result%2C%0Aby%20providing%20a%20matching%20algorithm-specific%20lower%20bound.%20In%20the%20setting%20where%0Aall%20problem%20parameters%20are%20known%2C%20we%20show%20this%20complexity%20is%20improved%20to%0A%24%5Cmathcal%7BO%7D%5Cleft%28%5Cvarepsilon%5E%7B-%5Cfrac%7B3p-2%7D%7Bp-1%7D%7D%5Cright%29%24%2C%20matching%20the%0Apreviously%20known%20lower%20bound%20for%20all%20first-order%20methods%20in%20all%20problem%0Adependent%20parameters.%20Finally%2C%20we%20establish%20high-probability%20convergence%20of%0ANSGD%20with%20a%20mild%20logarithmic%20dependence%20on%20the%20failure%20probability.%20Our%20work%0Acomplements%20the%20studies%20of%20gradient%20clipping%20under%20heavy%20tailed%20noise%20improving%0Athe%20sample%20complexities%20of%20existing%20algorithms%20and%20offering%20an%20alternative%0Amechanism%20to%20achieve%20high%20probability%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13849v1&entry.124074799=Read"},
{"title": "GPTreeO: An R package for continual regression with dividing local\n  Gaussian processes", "author": "Timo Braun and Anders Kvellestad and Riccardo De Bin", "abstract": "  We introduce GPTreeO, a flexible R package for scalable Gaussian process (GP)\nregression, particularly tailored to continual learning problems. GPTreeO\nbuilds upon the Dividing Local Gaussian Processes (DLGP) algorithm, in which a\nbinary tree of local GP regressors is dynamically constructed using a continual\nstream of input data. In GPTreeO we extend the original DLGP algorithm by\nallowing continual optimisation of the GP hyperparameters, incorporating\nuncertainty calibration, and introducing new strategies for how the local\npartitions are created. Moreover, the modular code structure allows users to\ninterface their favourite GP library to perform the local GP regression in\nGPTreeO. The flexibility of GPTreeO gives the user fine-grained control of the\nbalance between computational speed, accuracy, stability and smoothness. We\nconduct a sensitivity analysis to show how GPTreeO's configurable features\nimpact the regression performance in a continual learning setting.\n", "link": "http://arxiv.org/abs/2410.01024v2", "date": "2024-10-17", "relevancy": 2.2999, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4671}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4664}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPTreeO%3A%20An%20R%20package%20for%20continual%20regression%20with%20dividing%20local%0A%20%20Gaussian%20processes&body=Title%3A%20GPTreeO%3A%20An%20R%20package%20for%20continual%20regression%20with%20dividing%20local%0A%20%20Gaussian%20processes%0AAuthor%3A%20Timo%20Braun%20and%20Anders%20Kvellestad%20and%20Riccardo%20De%20Bin%0AAbstract%3A%20%20%20We%20introduce%20GPTreeO%2C%20a%20flexible%20R%20package%20for%20scalable%20Gaussian%20process%20%28GP%29%0Aregression%2C%20particularly%20tailored%20to%20continual%20learning%20problems.%20GPTreeO%0Abuilds%20upon%20the%20Dividing%20Local%20Gaussian%20Processes%20%28DLGP%29%20algorithm%2C%20in%20which%20a%0Abinary%20tree%20of%20local%20GP%20regressors%20is%20dynamically%20constructed%20using%20a%20continual%0Astream%20of%20input%20data.%20In%20GPTreeO%20we%20extend%20the%20original%20DLGP%20algorithm%20by%0Aallowing%20continual%20optimisation%20of%20the%20GP%20hyperparameters%2C%20incorporating%0Auncertainty%20calibration%2C%20and%20introducing%20new%20strategies%20for%20how%20the%20local%0Apartitions%20are%20created.%20Moreover%2C%20the%20modular%20code%20structure%20allows%20users%20to%0Ainterface%20their%20favourite%20GP%20library%20to%20perform%20the%20local%20GP%20regression%20in%0AGPTreeO.%20The%20flexibility%20of%20GPTreeO%20gives%20the%20user%20fine-grained%20control%20of%20the%0Abalance%20between%20computational%20speed%2C%20accuracy%2C%20stability%20and%20smoothness.%20We%0Aconduct%20a%20sensitivity%20analysis%20to%20show%20how%20GPTreeO%27s%20configurable%20features%0Aimpact%20the%20regression%20performance%20in%20a%20continual%20learning%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPTreeO%253A%2520An%2520R%2520package%2520for%2520continual%2520regression%2520with%2520dividing%2520local%250A%2520%2520Gaussian%2520processes%26entry.906535625%3DTimo%2520Braun%2520and%2520Anders%2520Kvellestad%2520and%2520Riccardo%2520De%2520Bin%26entry.1292438233%3D%2520%2520We%2520introduce%2520GPTreeO%252C%2520a%2520flexible%2520R%2520package%2520for%2520scalable%2520Gaussian%2520process%2520%2528GP%2529%250Aregression%252C%2520particularly%2520tailored%2520to%2520continual%2520learning%2520problems.%2520GPTreeO%250Abuilds%2520upon%2520the%2520Dividing%2520Local%2520Gaussian%2520Processes%2520%2528DLGP%2529%2520algorithm%252C%2520in%2520which%2520a%250Abinary%2520tree%2520of%2520local%2520GP%2520regressors%2520is%2520dynamically%2520constructed%2520using%2520a%2520continual%250Astream%2520of%2520input%2520data.%2520In%2520GPTreeO%2520we%2520extend%2520the%2520original%2520DLGP%2520algorithm%2520by%250Aallowing%2520continual%2520optimisation%2520of%2520the%2520GP%2520hyperparameters%252C%2520incorporating%250Auncertainty%2520calibration%252C%2520and%2520introducing%2520new%2520strategies%2520for%2520how%2520the%2520local%250Apartitions%2520are%2520created.%2520Moreover%252C%2520the%2520modular%2520code%2520structure%2520allows%2520users%2520to%250Ainterface%2520their%2520favourite%2520GP%2520library%2520to%2520perform%2520the%2520local%2520GP%2520regression%2520in%250AGPTreeO.%2520The%2520flexibility%2520of%2520GPTreeO%2520gives%2520the%2520user%2520fine-grained%2520control%2520of%2520the%250Abalance%2520between%2520computational%2520speed%252C%2520accuracy%252C%2520stability%2520and%2520smoothness.%2520We%250Aconduct%2520a%2520sensitivity%2520analysis%2520to%2520show%2520how%2520GPTreeO%2527s%2520configurable%2520features%250Aimpact%2520the%2520regression%2520performance%2520in%2520a%2520continual%2520learning%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPTreeO%3A%20An%20R%20package%20for%20continual%20regression%20with%20dividing%20local%0A%20%20Gaussian%20processes&entry.906535625=Timo%20Braun%20and%20Anders%20Kvellestad%20and%20Riccardo%20De%20Bin&entry.1292438233=%20%20We%20introduce%20GPTreeO%2C%20a%20flexible%20R%20package%20for%20scalable%20Gaussian%20process%20%28GP%29%0Aregression%2C%20particularly%20tailored%20to%20continual%20learning%20problems.%20GPTreeO%0Abuilds%20upon%20the%20Dividing%20Local%20Gaussian%20Processes%20%28DLGP%29%20algorithm%2C%20in%20which%20a%0Abinary%20tree%20of%20local%20GP%20regressors%20is%20dynamically%20constructed%20using%20a%20continual%0Astream%20of%20input%20data.%20In%20GPTreeO%20we%20extend%20the%20original%20DLGP%20algorithm%20by%0Aallowing%20continual%20optimisation%20of%20the%20GP%20hyperparameters%2C%20incorporating%0Auncertainty%20calibration%2C%20and%20introducing%20new%20strategies%20for%20how%20the%20local%0Apartitions%20are%20created.%20Moreover%2C%20the%20modular%20code%20structure%20allows%20users%20to%0Ainterface%20their%20favourite%20GP%20library%20to%20perform%20the%20local%20GP%20regression%20in%0AGPTreeO.%20The%20flexibility%20of%20GPTreeO%20gives%20the%20user%20fine-grained%20control%20of%20the%0Abalance%20between%20computational%20speed%2C%20accuracy%2C%20stability%20and%20smoothness.%20We%0Aconduct%20a%20sensitivity%20analysis%20to%20show%20how%20GPTreeO%27s%20configurable%20features%0Aimpact%20the%20regression%20performance%20in%20a%20continual%20learning%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01024v2&entry.124074799=Read"},
{"title": "Beyond Coarse-Grained Matching in Video-Text Retrieval", "author": "Aozhu Chen and Hazel Doughty and Xirong Li and Cees G. M. Snoek", "abstract": "  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n", "link": "http://arxiv.org/abs/2410.12407v2", "date": "2024-10-17", "relevancy": 2.2703, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Coarse-Grained%20Matching%20in%20Video-Text%20Retrieval&body=Title%3A%20Beyond%20Coarse-Grained%20Matching%20in%20Video-Text%20Retrieval%0AAuthor%3A%20Aozhu%20Chen%20and%20Hazel%20Doughty%20and%20Xirong%20Li%20and%20Cees%20G.%20M.%20Snoek%0AAbstract%3A%20%20%20Video-text%20retrieval%20has%20seen%20significant%20advancements%2C%20yet%20the%20ability%20of%0Amodels%20to%20discern%20subtle%20differences%20in%20captions%20still%20requires%20verification.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20approach%20for%20fine-grained%20evaluation.%20Our%0Aapproach%20can%20be%20applied%20to%20existing%20datasets%20by%20automatically%20generating%20hard%0Anegative%20test%20captions%20with%20subtle%20single-word%20variations%20across%20nouns%2C%20verbs%2C%0Aadjectives%2C%20adverbs%2C%20and%20prepositions.%20We%20perform%20comprehensive%20experiments%0Ausing%20four%20state-of-the-art%20models%20across%20two%20standard%20benchmarks%20%28MSR-VTT%20and%0AVATEX%29%20and%20two%20specially%20curated%20datasets%20enriched%20with%20detailed%20descriptions%0A%28VLN-UVO%20and%20VLN-OOPS%29%2C%20resulting%20in%20a%20number%20of%20novel%20insights%3A%201%29%20our%0Aanalyses%20show%20that%20the%20current%20evaluation%20benchmarks%20fall%20short%20in%20detecting%20a%0Amodel%27s%20ability%20to%20perceive%20subtle%20single-word%20differences%2C%202%29%20our%20fine-grained%0Aevaluation%20highlights%20the%20difficulty%20models%20face%20in%20distinguishing%20such%20subtle%0Avariations.%20To%20enhance%20fine-grained%20understanding%2C%20we%20propose%20a%20new%20baseline%0Athat%20can%20be%20easily%20combined%20with%20current%20methods.%20Experiments%20on%20our%0Afine-grained%20evaluations%20demonstrate%20that%20this%20approach%20enhances%20a%20model%27s%0Aability%20to%20understand%20fine-grained%20differences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Coarse-Grained%2520Matching%2520in%2520Video-Text%2520Retrieval%26entry.906535625%3DAozhu%2520Chen%2520and%2520Hazel%2520Doughty%2520and%2520Xirong%2520Li%2520and%2520Cees%2520G.%2520M.%2520Snoek%26entry.1292438233%3D%2520%2520Video-text%2520retrieval%2520has%2520seen%2520significant%2520advancements%252C%2520yet%2520the%2520ability%2520of%250Amodels%2520to%2520discern%2520subtle%2520differences%2520in%2520captions%2520still%2520requires%2520verification.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520approach%2520for%2520fine-grained%2520evaluation.%2520Our%250Aapproach%2520can%2520be%2520applied%2520to%2520existing%2520datasets%2520by%2520automatically%2520generating%2520hard%250Anegative%2520test%2520captions%2520with%2520subtle%2520single-word%2520variations%2520across%2520nouns%252C%2520verbs%252C%250Aadjectives%252C%2520adverbs%252C%2520and%2520prepositions.%2520We%2520perform%2520comprehensive%2520experiments%250Ausing%2520four%2520state-of-the-art%2520models%2520across%2520two%2520standard%2520benchmarks%2520%2528MSR-VTT%2520and%250AVATEX%2529%2520and%2520two%2520specially%2520curated%2520datasets%2520enriched%2520with%2520detailed%2520descriptions%250A%2528VLN-UVO%2520and%2520VLN-OOPS%2529%252C%2520resulting%2520in%2520a%2520number%2520of%2520novel%2520insights%253A%25201%2529%2520our%250Aanalyses%2520show%2520that%2520the%2520current%2520evaluation%2520benchmarks%2520fall%2520short%2520in%2520detecting%2520a%250Amodel%2527s%2520ability%2520to%2520perceive%2520subtle%2520single-word%2520differences%252C%25202%2529%2520our%2520fine-grained%250Aevaluation%2520highlights%2520the%2520difficulty%2520models%2520face%2520in%2520distinguishing%2520such%2520subtle%250Avariations.%2520To%2520enhance%2520fine-grained%2520understanding%252C%2520we%2520propose%2520a%2520new%2520baseline%250Athat%2520can%2520be%2520easily%2520combined%2520with%2520current%2520methods.%2520Experiments%2520on%2520our%250Afine-grained%2520evaluations%2520demonstrate%2520that%2520this%2520approach%2520enhances%2520a%2520model%2527s%250Aability%2520to%2520understand%2520fine-grained%2520differences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Coarse-Grained%20Matching%20in%20Video-Text%20Retrieval&entry.906535625=Aozhu%20Chen%20and%20Hazel%20Doughty%20and%20Xirong%20Li%20and%20Cees%20G.%20M.%20Snoek&entry.1292438233=%20%20Video-text%20retrieval%20has%20seen%20significant%20advancements%2C%20yet%20the%20ability%20of%0Amodels%20to%20discern%20subtle%20differences%20in%20captions%20still%20requires%20verification.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20approach%20for%20fine-grained%20evaluation.%20Our%0Aapproach%20can%20be%20applied%20to%20existing%20datasets%20by%20automatically%20generating%20hard%0Anegative%20test%20captions%20with%20subtle%20single-word%20variations%20across%20nouns%2C%20verbs%2C%0Aadjectives%2C%20adverbs%2C%20and%20prepositions.%20We%20perform%20comprehensive%20experiments%0Ausing%20four%20state-of-the-art%20models%20across%20two%20standard%20benchmarks%20%28MSR-VTT%20and%0AVATEX%29%20and%20two%20specially%20curated%20datasets%20enriched%20with%20detailed%20descriptions%0A%28VLN-UVO%20and%20VLN-OOPS%29%2C%20resulting%20in%20a%20number%20of%20novel%20insights%3A%201%29%20our%0Aanalyses%20show%20that%20the%20current%20evaluation%20benchmarks%20fall%20short%20in%20detecting%20a%0Amodel%27s%20ability%20to%20perceive%20subtle%20single-word%20differences%2C%202%29%20our%20fine-grained%0Aevaluation%20highlights%20the%20difficulty%20models%20face%20in%20distinguishing%20such%20subtle%0Avariations.%20To%20enhance%20fine-grained%20understanding%2C%20we%20propose%20a%20new%20baseline%0Athat%20can%20be%20easily%20combined%20with%20current%20methods.%20Experiments%20on%20our%0Afine-grained%20evaluations%20demonstrate%20that%20this%20approach%20enhances%20a%20model%27s%0Aability%20to%20understand%20fine-grained%20differences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12407v2&entry.124074799=Read"},
{"title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free", "author": "Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan", "abstract": "  A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting\nincorrect geometries when given an insufficient number of input views. One\npotential reason is that standard volumetric rendering does not enforce the\nconstraint that most of a scene's geometry consist of empty space and opaque\nsurfaces. We formalize the above assumption through DS-NeRF (Depth-supervised\nNeural Radiance Fields), a loss for learning radiance fields that takes\nadvantage of readily-available depth supervision. We leverage the fact that\ncurrent NeRF pipelines require images with known camera poses that are\ntypically estimated by running structure-from-motion (SFM). Crucially, SFM also\nproduces sparse 3D points that can be used as \"free\" depth supervision during\ntraining: we add a loss to encourage the distribution of a ray's terminating\ndepth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can\nrender better images given fewer training views while training 2-3x faster.\nFurther, we show that our loss is compatible with other recently proposed NeRF\nmethods, demonstrating that depth is a cheap and easily digestible supervisory\nsignal. And finally, we find that DS-NeRF can support other types of depth\nsupervision such as scanned depth sensors and RGB-D reconstruction outputs.\n", "link": "http://arxiv.org/abs/2107.02791v3", "date": "2024-10-17", "relevancy": 2.246, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5725}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-supervised%20NeRF%3A%20Fewer%20Views%20and%20Faster%20Training%20for%20Free&body=Title%3A%20Depth-supervised%20NeRF%3A%20Fewer%20Views%20and%20Faster%20Training%20for%20Free%0AAuthor%3A%20Kangle%20Deng%20and%20Andrew%20Liu%20and%20Jun-Yan%20Zhu%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20A%20commonly%20observed%20failure%20mode%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20is%20fitting%0Aincorrect%20geometries%20when%20given%20an%20insufficient%20number%20of%20input%20views.%20One%0Apotential%20reason%20is%20that%20standard%20volumetric%20rendering%20does%20not%20enforce%20the%0Aconstraint%20that%20most%20of%20a%20scene%27s%20geometry%20consist%20of%20empty%20space%20and%20opaque%0Asurfaces.%20We%20formalize%20the%20above%20assumption%20through%20DS-NeRF%20%28Depth-supervised%0ANeural%20Radiance%20Fields%29%2C%20a%20loss%20for%20learning%20radiance%20fields%20that%20takes%0Aadvantage%20of%20readily-available%20depth%20supervision.%20We%20leverage%20the%20fact%20that%0Acurrent%20NeRF%20pipelines%20require%20images%20with%20known%20camera%20poses%20that%20are%0Atypically%20estimated%20by%20running%20structure-from-motion%20%28SFM%29.%20Crucially%2C%20SFM%20also%0Aproduces%20sparse%203D%20points%20that%20can%20be%20used%20as%20%22free%22%20depth%20supervision%20during%0Atraining%3A%20we%20add%20a%20loss%20to%20encourage%20the%20distribution%20of%20a%20ray%27s%20terminating%0Adepth%20matches%20a%20given%203D%20keypoint%2C%20incorporating%20depth%20uncertainty.%20DS-NeRF%20can%0Arender%20better%20images%20given%20fewer%20training%20views%20while%20training%202-3x%20faster.%0AFurther%2C%20we%20show%20that%20our%20loss%20is%20compatible%20with%20other%20recently%20proposed%20NeRF%0Amethods%2C%20demonstrating%20that%20depth%20is%20a%20cheap%20and%20easily%20digestible%20supervisory%0Asignal.%20And%20finally%2C%20we%20find%20that%20DS-NeRF%20can%20support%20other%20types%20of%20depth%0Asupervision%20such%20as%20scanned%20depth%20sensors%20and%20RGB-D%20reconstruction%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2107.02791v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-supervised%2520NeRF%253A%2520Fewer%2520Views%2520and%2520Faster%2520Training%2520for%2520Free%26entry.906535625%3DKangle%2520Deng%2520and%2520Andrew%2520Liu%2520and%2520Jun-Yan%2520Zhu%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520A%2520commonly%2520observed%2520failure%2520mode%2520of%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520is%2520fitting%250Aincorrect%2520geometries%2520when%2520given%2520an%2520insufficient%2520number%2520of%2520input%2520views.%2520One%250Apotential%2520reason%2520is%2520that%2520standard%2520volumetric%2520rendering%2520does%2520not%2520enforce%2520the%250Aconstraint%2520that%2520most%2520of%2520a%2520scene%2527s%2520geometry%2520consist%2520of%2520empty%2520space%2520and%2520opaque%250Asurfaces.%2520We%2520formalize%2520the%2520above%2520assumption%2520through%2520DS-NeRF%2520%2528Depth-supervised%250ANeural%2520Radiance%2520Fields%2529%252C%2520a%2520loss%2520for%2520learning%2520radiance%2520fields%2520that%2520takes%250Aadvantage%2520of%2520readily-available%2520depth%2520supervision.%2520We%2520leverage%2520the%2520fact%2520that%250Acurrent%2520NeRF%2520pipelines%2520require%2520images%2520with%2520known%2520camera%2520poses%2520that%2520are%250Atypically%2520estimated%2520by%2520running%2520structure-from-motion%2520%2528SFM%2529.%2520Crucially%252C%2520SFM%2520also%250Aproduces%2520sparse%25203D%2520points%2520that%2520can%2520be%2520used%2520as%2520%2522free%2522%2520depth%2520supervision%2520during%250Atraining%253A%2520we%2520add%2520a%2520loss%2520to%2520encourage%2520the%2520distribution%2520of%2520a%2520ray%2527s%2520terminating%250Adepth%2520matches%2520a%2520given%25203D%2520keypoint%252C%2520incorporating%2520depth%2520uncertainty.%2520DS-NeRF%2520can%250Arender%2520better%2520images%2520given%2520fewer%2520training%2520views%2520while%2520training%25202-3x%2520faster.%250AFurther%252C%2520we%2520show%2520that%2520our%2520loss%2520is%2520compatible%2520with%2520other%2520recently%2520proposed%2520NeRF%250Amethods%252C%2520demonstrating%2520that%2520depth%2520is%2520a%2520cheap%2520and%2520easily%2520digestible%2520supervisory%250Asignal.%2520And%2520finally%252C%2520we%2520find%2520that%2520DS-NeRF%2520can%2520support%2520other%2520types%2520of%2520depth%250Asupervision%2520such%2520as%2520scanned%2520depth%2520sensors%2520and%2520RGB-D%2520reconstruction%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2107.02791v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-supervised%20NeRF%3A%20Fewer%20Views%20and%20Faster%20Training%20for%20Free&entry.906535625=Kangle%20Deng%20and%20Andrew%20Liu%20and%20Jun-Yan%20Zhu%20and%20Deva%20Ramanan&entry.1292438233=%20%20A%20commonly%20observed%20failure%20mode%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20is%20fitting%0Aincorrect%20geometries%20when%20given%20an%20insufficient%20number%20of%20input%20views.%20One%0Apotential%20reason%20is%20that%20standard%20volumetric%20rendering%20does%20not%20enforce%20the%0Aconstraint%20that%20most%20of%20a%20scene%27s%20geometry%20consist%20of%20empty%20space%20and%20opaque%0Asurfaces.%20We%20formalize%20the%20above%20assumption%20through%20DS-NeRF%20%28Depth-supervised%0ANeural%20Radiance%20Fields%29%2C%20a%20loss%20for%20learning%20radiance%20fields%20that%20takes%0Aadvantage%20of%20readily-available%20depth%20supervision.%20We%20leverage%20the%20fact%20that%0Acurrent%20NeRF%20pipelines%20require%20images%20with%20known%20camera%20poses%20that%20are%0Atypically%20estimated%20by%20running%20structure-from-motion%20%28SFM%29.%20Crucially%2C%20SFM%20also%0Aproduces%20sparse%203D%20points%20that%20can%20be%20used%20as%20%22free%22%20depth%20supervision%20during%0Atraining%3A%20we%20add%20a%20loss%20to%20encourage%20the%20distribution%20of%20a%20ray%27s%20terminating%0Adepth%20matches%20a%20given%203D%20keypoint%2C%20incorporating%20depth%20uncertainty.%20DS-NeRF%20can%0Arender%20better%20images%20given%20fewer%20training%20views%20while%20training%202-3x%20faster.%0AFurther%2C%20we%20show%20that%20our%20loss%20is%20compatible%20with%20other%20recently%20proposed%20NeRF%0Amethods%2C%20demonstrating%20that%20depth%20is%20a%20cheap%20and%20easily%20digestible%20supervisory%0Asignal.%20And%20finally%2C%20we%20find%20that%20DS-NeRF%20can%20support%20other%20types%20of%20depth%0Asupervision%20such%20as%20scanned%20depth%20sensors%20and%20RGB-D%20reconstruction%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2107.02791v3&entry.124074799=Read"},
{"title": "Learning Action and Reasoning-Centric Image Editing from Videos and\n  Simulations", "author": "Benno Krojer and Dheeraj Vattikonda and Luis Lara and Varun Jampani and Eva Portelance and Christopher Pal and Siva Reddy", "abstract": "  An image editing model should be able to perform diverse edits, ranging from\nobject replacement, changing attributes or style, to performing actions or\nmovement, which require many forms of reasoning. Current general\ninstruction-guided editing models have significant shortcomings with action and\nreasoning-centric edits. Object, attribute or stylistic changes can be learned\nfrom visually static datasets. On the other hand, high-quality data for action\nand reasoning-centric edits is scarce and has to come from entirely different\nsources that cover e.g. physical dynamics, temporality and spatial reasoning.\nTo this end, we meticulously curate the AURORA Dataset\n(Action-Reasoning-Object-Attribute), a collection of high-quality training\ndata, human-annotated and curated from videos and simulation engines. We focus\non a key aspect of quality training data: triplets (source image, prompt,\ntarget image) contain a single meaningful visual change described by the\nprompt, i.e., truly minimal changes between source and target images. To\ndemonstrate the value of our dataset, we evaluate an AURORA-finetuned model on\na new expert-curated benchmark (AURORA-Bench) covering 8 diverse editing tasks.\nOur model significantly outperforms previous editing models as judged by human\nraters. For automatic evaluations, we find important flaws in previous metrics\nand caution their use for semantically hard editing tasks. Instead, we propose\na new automatic metric that focuses on discriminative understanding. We hope\nthat our efforts : (1) curating a quality training dataset and an evaluation\nbenchmark, (2) developing critical evaluations, and (3) releasing a\nstate-of-the-art model, will fuel further progress on general image editing.\n", "link": "http://arxiv.org/abs/2407.03471v3", "date": "2024-10-17", "relevancy": 2.2416, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Action%20and%20Reasoning-Centric%20Image%20Editing%20from%20Videos%20and%0A%20%20Simulations&body=Title%3A%20Learning%20Action%20and%20Reasoning-Centric%20Image%20Editing%20from%20Videos%20and%0A%20%20Simulations%0AAuthor%3A%20Benno%20Krojer%20and%20Dheeraj%20Vattikonda%20and%20Luis%20Lara%20and%20Varun%20Jampani%20and%20Eva%20Portelance%20and%20Christopher%20Pal%20and%20Siva%20Reddy%0AAbstract%3A%20%20%20An%20image%20editing%20model%20should%20be%20able%20to%20perform%20diverse%20edits%2C%20ranging%20from%0Aobject%20replacement%2C%20changing%20attributes%20or%20style%2C%20to%20performing%20actions%20or%0Amovement%2C%20which%20require%20many%20forms%20of%20reasoning.%20Current%20general%0Ainstruction-guided%20editing%20models%20have%20significant%20shortcomings%20with%20action%20and%0Areasoning-centric%20edits.%20Object%2C%20attribute%20or%20stylistic%20changes%20can%20be%20learned%0Afrom%20visually%20static%20datasets.%20On%20the%20other%20hand%2C%20high-quality%20data%20for%20action%0Aand%20reasoning-centric%20edits%20is%20scarce%20and%20has%20to%20come%20from%20entirely%20different%0Asources%20that%20cover%20e.g.%20physical%20dynamics%2C%20temporality%20and%20spatial%20reasoning.%0ATo%20this%20end%2C%20we%20meticulously%20curate%20the%20AURORA%20Dataset%0A%28Action-Reasoning-Object-Attribute%29%2C%20a%20collection%20of%20high-quality%20training%0Adata%2C%20human-annotated%20and%20curated%20from%20videos%20and%20simulation%20engines.%20We%20focus%0Aon%20a%20key%20aspect%20of%20quality%20training%20data%3A%20triplets%20%28source%20image%2C%20prompt%2C%0Atarget%20image%29%20contain%20a%20single%20meaningful%20visual%20change%20described%20by%20the%0Aprompt%2C%20i.e.%2C%20truly%20minimal%20changes%20between%20source%20and%20target%20images.%20To%0Ademonstrate%20the%20value%20of%20our%20dataset%2C%20we%20evaluate%20an%20AURORA-finetuned%20model%20on%0Aa%20new%20expert-curated%20benchmark%20%28AURORA-Bench%29%20covering%208%20diverse%20editing%20tasks.%0AOur%20model%20significantly%20outperforms%20previous%20editing%20models%20as%20judged%20by%20human%0Araters.%20For%20automatic%20evaluations%2C%20we%20find%20important%20flaws%20in%20previous%20metrics%0Aand%20caution%20their%20use%20for%20semantically%20hard%20editing%20tasks.%20Instead%2C%20we%20propose%0Aa%20new%20automatic%20metric%20that%20focuses%20on%20discriminative%20understanding.%20We%20hope%0Athat%20our%20efforts%20%3A%20%281%29%20curating%20a%20quality%20training%20dataset%20and%20an%20evaluation%0Abenchmark%2C%20%282%29%20developing%20critical%20evaluations%2C%20and%20%283%29%20releasing%20a%0Astate-of-the-art%20model%2C%20will%20fuel%20further%20progress%20on%20general%20image%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03471v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Action%2520and%2520Reasoning-Centric%2520Image%2520Editing%2520from%2520Videos%2520and%250A%2520%2520Simulations%26entry.906535625%3DBenno%2520Krojer%2520and%2520Dheeraj%2520Vattikonda%2520and%2520Luis%2520Lara%2520and%2520Varun%2520Jampani%2520and%2520Eva%2520Portelance%2520and%2520Christopher%2520Pal%2520and%2520Siva%2520Reddy%26entry.1292438233%3D%2520%2520An%2520image%2520editing%2520model%2520should%2520be%2520able%2520to%2520perform%2520diverse%2520edits%252C%2520ranging%2520from%250Aobject%2520replacement%252C%2520changing%2520attributes%2520or%2520style%252C%2520to%2520performing%2520actions%2520or%250Amovement%252C%2520which%2520require%2520many%2520forms%2520of%2520reasoning.%2520Current%2520general%250Ainstruction-guided%2520editing%2520models%2520have%2520significant%2520shortcomings%2520with%2520action%2520and%250Areasoning-centric%2520edits.%2520Object%252C%2520attribute%2520or%2520stylistic%2520changes%2520can%2520be%2520learned%250Afrom%2520visually%2520static%2520datasets.%2520On%2520the%2520other%2520hand%252C%2520high-quality%2520data%2520for%2520action%250Aand%2520reasoning-centric%2520edits%2520is%2520scarce%2520and%2520has%2520to%2520come%2520from%2520entirely%2520different%250Asources%2520that%2520cover%2520e.g.%2520physical%2520dynamics%252C%2520temporality%2520and%2520spatial%2520reasoning.%250ATo%2520this%2520end%252C%2520we%2520meticulously%2520curate%2520the%2520AURORA%2520Dataset%250A%2528Action-Reasoning-Object-Attribute%2529%252C%2520a%2520collection%2520of%2520high-quality%2520training%250Adata%252C%2520human-annotated%2520and%2520curated%2520from%2520videos%2520and%2520simulation%2520engines.%2520We%2520focus%250Aon%2520a%2520key%2520aspect%2520of%2520quality%2520training%2520data%253A%2520triplets%2520%2528source%2520image%252C%2520prompt%252C%250Atarget%2520image%2529%2520contain%2520a%2520single%2520meaningful%2520visual%2520change%2520described%2520by%2520the%250Aprompt%252C%2520i.e.%252C%2520truly%2520minimal%2520changes%2520between%2520source%2520and%2520target%2520images.%2520To%250Ademonstrate%2520the%2520value%2520of%2520our%2520dataset%252C%2520we%2520evaluate%2520an%2520AURORA-finetuned%2520model%2520on%250Aa%2520new%2520expert-curated%2520benchmark%2520%2528AURORA-Bench%2529%2520covering%25208%2520diverse%2520editing%2520tasks.%250AOur%2520model%2520significantly%2520outperforms%2520previous%2520editing%2520models%2520as%2520judged%2520by%2520human%250Araters.%2520For%2520automatic%2520evaluations%252C%2520we%2520find%2520important%2520flaws%2520in%2520previous%2520metrics%250Aand%2520caution%2520their%2520use%2520for%2520semantically%2520hard%2520editing%2520tasks.%2520Instead%252C%2520we%2520propose%250Aa%2520new%2520automatic%2520metric%2520that%2520focuses%2520on%2520discriminative%2520understanding.%2520We%2520hope%250Athat%2520our%2520efforts%2520%253A%2520%25281%2529%2520curating%2520a%2520quality%2520training%2520dataset%2520and%2520an%2520evaluation%250Abenchmark%252C%2520%25282%2529%2520developing%2520critical%2520evaluations%252C%2520and%2520%25283%2529%2520releasing%2520a%250Astate-of-the-art%2520model%252C%2520will%2520fuel%2520further%2520progress%2520on%2520general%2520image%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03471v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Action%20and%20Reasoning-Centric%20Image%20Editing%20from%20Videos%20and%0A%20%20Simulations&entry.906535625=Benno%20Krojer%20and%20Dheeraj%20Vattikonda%20and%20Luis%20Lara%20and%20Varun%20Jampani%20and%20Eva%20Portelance%20and%20Christopher%20Pal%20and%20Siva%20Reddy&entry.1292438233=%20%20An%20image%20editing%20model%20should%20be%20able%20to%20perform%20diverse%20edits%2C%20ranging%20from%0Aobject%20replacement%2C%20changing%20attributes%20or%20style%2C%20to%20performing%20actions%20or%0Amovement%2C%20which%20require%20many%20forms%20of%20reasoning.%20Current%20general%0Ainstruction-guided%20editing%20models%20have%20significant%20shortcomings%20with%20action%20and%0Areasoning-centric%20edits.%20Object%2C%20attribute%20or%20stylistic%20changes%20can%20be%20learned%0Afrom%20visually%20static%20datasets.%20On%20the%20other%20hand%2C%20high-quality%20data%20for%20action%0Aand%20reasoning-centric%20edits%20is%20scarce%20and%20has%20to%20come%20from%20entirely%20different%0Asources%20that%20cover%20e.g.%20physical%20dynamics%2C%20temporality%20and%20spatial%20reasoning.%0ATo%20this%20end%2C%20we%20meticulously%20curate%20the%20AURORA%20Dataset%0A%28Action-Reasoning-Object-Attribute%29%2C%20a%20collection%20of%20high-quality%20training%0Adata%2C%20human-annotated%20and%20curated%20from%20videos%20and%20simulation%20engines.%20We%20focus%0Aon%20a%20key%20aspect%20of%20quality%20training%20data%3A%20triplets%20%28source%20image%2C%20prompt%2C%0Atarget%20image%29%20contain%20a%20single%20meaningful%20visual%20change%20described%20by%20the%0Aprompt%2C%20i.e.%2C%20truly%20minimal%20changes%20between%20source%20and%20target%20images.%20To%0Ademonstrate%20the%20value%20of%20our%20dataset%2C%20we%20evaluate%20an%20AURORA-finetuned%20model%20on%0Aa%20new%20expert-curated%20benchmark%20%28AURORA-Bench%29%20covering%208%20diverse%20editing%20tasks.%0AOur%20model%20significantly%20outperforms%20previous%20editing%20models%20as%20judged%20by%20human%0Araters.%20For%20automatic%20evaluations%2C%20we%20find%20important%20flaws%20in%20previous%20metrics%0Aand%20caution%20their%20use%20for%20semantically%20hard%20editing%20tasks.%20Instead%2C%20we%20propose%0Aa%20new%20automatic%20metric%20that%20focuses%20on%20discriminative%20understanding.%20We%20hope%0Athat%20our%20efforts%20%3A%20%281%29%20curating%20a%20quality%20training%20dataset%20and%20an%20evaluation%0Abenchmark%2C%20%282%29%20developing%20critical%20evaluations%2C%20and%20%283%29%20releasing%20a%0Astate-of-the-art%20model%2C%20will%20fuel%20further%20progress%20on%20general%20image%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03471v3&entry.124074799=Read"},
{"title": "EchoApex: A General-Purpose Vision Foundation Model for Echocardiography", "author": "Abdoul Aziz Amadou and Yue Zhang and Sebastien Piat and Paul Klein and Ingo Schmuecking and Tiziano Passerini and Puneet Sharma", "abstract": "  Quantitative evaluation of echocardiography is essential for precise\nassessment of cardiac condition, monitoring disease progression, and guiding\ntreatment decisions. The diverse nature of echo images, including variations in\nprobe types, manufacturers, and pathologies, poses challenges for developing\nartificial intelligent models that can generalize across different clinical\npractice. We introduce EchoApex, the first general-purpose vision foundation\nmodel echocardiography with applications on a variety of clinical practice.\nLeveraging self-supervised learning, EchoApex is pretrained on over 20 million\necho images from 11 clinical centres. By incorporating task-specific decoders\nand adapter modules, we demonstrate the effectiveness of EchoApex on 4\ndifferent kind of clinical applications with 28 sub-tasks, including view\nclassification, interactive structure segmentation, left ventricle hypertrophy\ndetection and automated ejection fraction estimation from view sequences.\nCompared to state-of-the-art task-specific models, EchoApex attains improved\nperformance with a unified image encoding architecture, demonstrating the\nbenefits of model pretraining at scale with in-domain data. Furthermore,\nEchoApex illustrates the potential for developing a general-purpose vision\nfoundation model tailored specifically for echocardiography, capable of\naddressing a diverse range of clinical applications with high efficiency and\nefficacy.\n", "link": "http://arxiv.org/abs/2410.11092v2", "date": "2024-10-17", "relevancy": 2.2161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoApex%3A%20A%20General-Purpose%20Vision%20Foundation%20Model%20for%20Echocardiography&body=Title%3A%20EchoApex%3A%20A%20General-Purpose%20Vision%20Foundation%20Model%20for%20Echocardiography%0AAuthor%3A%20Abdoul%20Aziz%20Amadou%20and%20Yue%20Zhang%20and%20Sebastien%20Piat%20and%20Paul%20Klein%20and%20Ingo%20Schmuecking%20and%20Tiziano%20Passerini%20and%20Puneet%20Sharma%0AAbstract%3A%20%20%20Quantitative%20evaluation%20of%20echocardiography%20is%20essential%20for%20precise%0Aassessment%20of%20cardiac%20condition%2C%20monitoring%20disease%20progression%2C%20and%20guiding%0Atreatment%20decisions.%20The%20diverse%20nature%20of%20echo%20images%2C%20including%20variations%20in%0Aprobe%20types%2C%20manufacturers%2C%20and%20pathologies%2C%20poses%20challenges%20for%20developing%0Aartificial%20intelligent%20models%20that%20can%20generalize%20across%20different%20clinical%0Apractice.%20We%20introduce%20EchoApex%2C%20the%20first%20general-purpose%20vision%20foundation%0Amodel%20echocardiography%20with%20applications%20on%20a%20variety%20of%20clinical%20practice.%0ALeveraging%20self-supervised%20learning%2C%20EchoApex%20is%20pretrained%20on%20over%2020%20million%0Aecho%20images%20from%2011%20clinical%20centres.%20By%20incorporating%20task-specific%20decoders%0Aand%20adapter%20modules%2C%20we%20demonstrate%20the%20effectiveness%20of%20EchoApex%20on%204%0Adifferent%20kind%20of%20clinical%20applications%20with%2028%20sub-tasks%2C%20including%20view%0Aclassification%2C%20interactive%20structure%20segmentation%2C%20left%20ventricle%20hypertrophy%0Adetection%20and%20automated%20ejection%20fraction%20estimation%20from%20view%20sequences.%0ACompared%20to%20state-of-the-art%20task-specific%20models%2C%20EchoApex%20attains%20improved%0Aperformance%20with%20a%20unified%20image%20encoding%20architecture%2C%20demonstrating%20the%0Abenefits%20of%20model%20pretraining%20at%20scale%20with%20in-domain%20data.%20Furthermore%2C%0AEchoApex%20illustrates%20the%20potential%20for%20developing%20a%20general-purpose%20vision%0Afoundation%20model%20tailored%20specifically%20for%20echocardiography%2C%20capable%20of%0Aaddressing%20a%20diverse%20range%20of%20clinical%20applications%20with%20high%20efficiency%20and%0Aefficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoApex%253A%2520A%2520General-Purpose%2520Vision%2520Foundation%2520Model%2520for%2520Echocardiography%26entry.906535625%3DAbdoul%2520Aziz%2520Amadou%2520and%2520Yue%2520Zhang%2520and%2520Sebastien%2520Piat%2520and%2520Paul%2520Klein%2520and%2520Ingo%2520Schmuecking%2520and%2520Tiziano%2520Passerini%2520and%2520Puneet%2520Sharma%26entry.1292438233%3D%2520%2520Quantitative%2520evaluation%2520of%2520echocardiography%2520is%2520essential%2520for%2520precise%250Aassessment%2520of%2520cardiac%2520condition%252C%2520monitoring%2520disease%2520progression%252C%2520and%2520guiding%250Atreatment%2520decisions.%2520The%2520diverse%2520nature%2520of%2520echo%2520images%252C%2520including%2520variations%2520in%250Aprobe%2520types%252C%2520manufacturers%252C%2520and%2520pathologies%252C%2520poses%2520challenges%2520for%2520developing%250Aartificial%2520intelligent%2520models%2520that%2520can%2520generalize%2520across%2520different%2520clinical%250Apractice.%2520We%2520introduce%2520EchoApex%252C%2520the%2520first%2520general-purpose%2520vision%2520foundation%250Amodel%2520echocardiography%2520with%2520applications%2520on%2520a%2520variety%2520of%2520clinical%2520practice.%250ALeveraging%2520self-supervised%2520learning%252C%2520EchoApex%2520is%2520pretrained%2520on%2520over%252020%2520million%250Aecho%2520images%2520from%252011%2520clinical%2520centres.%2520By%2520incorporating%2520task-specific%2520decoders%250Aand%2520adapter%2520modules%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520EchoApex%2520on%25204%250Adifferent%2520kind%2520of%2520clinical%2520applications%2520with%252028%2520sub-tasks%252C%2520including%2520view%250Aclassification%252C%2520interactive%2520structure%2520segmentation%252C%2520left%2520ventricle%2520hypertrophy%250Adetection%2520and%2520automated%2520ejection%2520fraction%2520estimation%2520from%2520view%2520sequences.%250ACompared%2520to%2520state-of-the-art%2520task-specific%2520models%252C%2520EchoApex%2520attains%2520improved%250Aperformance%2520with%2520a%2520unified%2520image%2520encoding%2520architecture%252C%2520demonstrating%2520the%250Abenefits%2520of%2520model%2520pretraining%2520at%2520scale%2520with%2520in-domain%2520data.%2520Furthermore%252C%250AEchoApex%2520illustrates%2520the%2520potential%2520for%2520developing%2520a%2520general-purpose%2520vision%250Afoundation%2520model%2520tailored%2520specifically%2520for%2520echocardiography%252C%2520capable%2520of%250Aaddressing%2520a%2520diverse%2520range%2520of%2520clinical%2520applications%2520with%2520high%2520efficiency%2520and%250Aefficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoApex%3A%20A%20General-Purpose%20Vision%20Foundation%20Model%20for%20Echocardiography&entry.906535625=Abdoul%20Aziz%20Amadou%20and%20Yue%20Zhang%20and%20Sebastien%20Piat%20and%20Paul%20Klein%20and%20Ingo%20Schmuecking%20and%20Tiziano%20Passerini%20and%20Puneet%20Sharma&entry.1292438233=%20%20Quantitative%20evaluation%20of%20echocardiography%20is%20essential%20for%20precise%0Aassessment%20of%20cardiac%20condition%2C%20monitoring%20disease%20progression%2C%20and%20guiding%0Atreatment%20decisions.%20The%20diverse%20nature%20of%20echo%20images%2C%20including%20variations%20in%0Aprobe%20types%2C%20manufacturers%2C%20and%20pathologies%2C%20poses%20challenges%20for%20developing%0Aartificial%20intelligent%20models%20that%20can%20generalize%20across%20different%20clinical%0Apractice.%20We%20introduce%20EchoApex%2C%20the%20first%20general-purpose%20vision%20foundation%0Amodel%20echocardiography%20with%20applications%20on%20a%20variety%20of%20clinical%20practice.%0ALeveraging%20self-supervised%20learning%2C%20EchoApex%20is%20pretrained%20on%20over%2020%20million%0Aecho%20images%20from%2011%20clinical%20centres.%20By%20incorporating%20task-specific%20decoders%0Aand%20adapter%20modules%2C%20we%20demonstrate%20the%20effectiveness%20of%20EchoApex%20on%204%0Adifferent%20kind%20of%20clinical%20applications%20with%2028%20sub-tasks%2C%20including%20view%0Aclassification%2C%20interactive%20structure%20segmentation%2C%20left%20ventricle%20hypertrophy%0Adetection%20and%20automated%20ejection%20fraction%20estimation%20from%20view%20sequences.%0ACompared%20to%20state-of-the-art%20task-specific%20models%2C%20EchoApex%20attains%20improved%0Aperformance%20with%20a%20unified%20image%20encoding%20architecture%2C%20demonstrating%20the%0Abenefits%20of%20model%20pretraining%20at%20scale%20with%20in-domain%20data.%20Furthermore%2C%0AEchoApex%20illustrates%20the%20potential%20for%20developing%20a%20general-purpose%20vision%0Afoundation%20model%20tailored%20specifically%20for%20echocardiography%2C%20capable%20of%0Aaddressing%20a%20diverse%20range%20of%20clinical%20applications%20with%20high%20efficiency%20and%0Aefficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11092v2&entry.124074799=Read"},
{"title": "Persistent Pre-Training Poisoning of LLMs", "author": "Yiming Zhang and Javier Rando and Ivan Evtimov and Jianfeng Chi and Eric Michael Smith and Nicholas Carlini and Florian Tram\u00e8r and Daphne Ippolito", "abstract": "  Large language models are pre-trained on uncurated text datasets consisting\nof trillions of tokens scraped from the Web. Prior work has shown that: (1)\nweb-scraped pre-training datasets can be practically poisoned by malicious\nactors; and (2) adversaries can compromise language models after poisoning\nfine-tuning datasets. Our work evaluates for the first time whether language\nmodels can also be compromised during pre-training, with a focus on the\npersistence of pre-training attacks after models are fine-tuned as helpful and\nharmless chatbots (i.e., after SFT and DPO). We pre-train a series of LLMs from\nscratch to measure the impact of a potential poisoning adversary under four\ndifferent attack objectives (denial-of-service, belief manipulation,\njailbreaking, and prompt stealing), and across a wide range of model sizes\n(from 600M to 7B). Our main result is that poisoning only 0.1% of a model's\npre-training dataset is sufficient for three out of four attacks to measurably\npersist through post-training. Moreover, simple attacks like denial-of-service\npersist through post-training with a poisoning rate of only 0.001%.\n", "link": "http://arxiv.org/abs/2410.13722v1", "date": "2024-10-17", "relevancy": 2.2154, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4578}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4416}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Pre-Training%20Poisoning%20of%20LLMs&body=Title%3A%20Persistent%20Pre-Training%20Poisoning%20of%20LLMs%0AAuthor%3A%20Yiming%20Zhang%20and%20Javier%20Rando%20and%20Ivan%20Evtimov%20and%20Jianfeng%20Chi%20and%20Eric%20Michael%20Smith%20and%20Nicholas%20Carlini%20and%20Florian%20Tram%C3%A8r%20and%20Daphne%20Ippolito%0AAbstract%3A%20%20%20Large%20language%20models%20are%20pre-trained%20on%20uncurated%20text%20datasets%20consisting%0Aof%20trillions%20of%20tokens%20scraped%20from%20the%20Web.%20Prior%20work%20has%20shown%20that%3A%20%281%29%0Aweb-scraped%20pre-training%20datasets%20can%20be%20practically%20poisoned%20by%20malicious%0Aactors%3B%20and%20%282%29%20adversaries%20can%20compromise%20language%20models%20after%20poisoning%0Afine-tuning%20datasets.%20Our%20work%20evaluates%20for%20the%20first%20time%20whether%20language%0Amodels%20can%20also%20be%20compromised%20during%20pre-training%2C%20with%20a%20focus%20on%20the%0Apersistence%20of%20pre-training%20attacks%20after%20models%20are%20fine-tuned%20as%20helpful%20and%0Aharmless%20chatbots%20%28i.e.%2C%20after%20SFT%20and%20DPO%29.%20We%20pre-train%20a%20series%20of%20LLMs%20from%0Ascratch%20to%20measure%20the%20impact%20of%20a%20potential%20poisoning%20adversary%20under%20four%0Adifferent%20attack%20objectives%20%28denial-of-service%2C%20belief%20manipulation%2C%0Ajailbreaking%2C%20and%20prompt%20stealing%29%2C%20and%20across%20a%20wide%20range%20of%20model%20sizes%0A%28from%20600M%20to%207B%29.%20Our%20main%20result%20is%20that%20poisoning%20only%200.1%25%20of%20a%20model%27s%0Apre-training%20dataset%20is%20sufficient%20for%20three%20out%20of%20four%20attacks%20to%20measurably%0Apersist%20through%20post-training.%20Moreover%2C%20simple%20attacks%20like%20denial-of-service%0Apersist%20through%20post-training%20with%20a%20poisoning%20rate%20of%20only%200.001%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Pre-Training%2520Poisoning%2520of%2520LLMs%26entry.906535625%3DYiming%2520Zhang%2520and%2520Javier%2520Rando%2520and%2520Ivan%2520Evtimov%2520and%2520Jianfeng%2520Chi%2520and%2520Eric%2520Michael%2520Smith%2520and%2520Nicholas%2520Carlini%2520and%2520Florian%2520Tram%25C3%25A8r%2520and%2520Daphne%2520Ippolito%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520pre-trained%2520on%2520uncurated%2520text%2520datasets%2520consisting%250Aof%2520trillions%2520of%2520tokens%2520scraped%2520from%2520the%2520Web.%2520Prior%2520work%2520has%2520shown%2520that%253A%2520%25281%2529%250Aweb-scraped%2520pre-training%2520datasets%2520can%2520be%2520practically%2520poisoned%2520by%2520malicious%250Aactors%253B%2520and%2520%25282%2529%2520adversaries%2520can%2520compromise%2520language%2520models%2520after%2520poisoning%250Afine-tuning%2520datasets.%2520Our%2520work%2520evaluates%2520for%2520the%2520first%2520time%2520whether%2520language%250Amodels%2520can%2520also%2520be%2520compromised%2520during%2520pre-training%252C%2520with%2520a%2520focus%2520on%2520the%250Apersistence%2520of%2520pre-training%2520attacks%2520after%2520models%2520are%2520fine-tuned%2520as%2520helpful%2520and%250Aharmless%2520chatbots%2520%2528i.e.%252C%2520after%2520SFT%2520and%2520DPO%2529.%2520We%2520pre-train%2520a%2520series%2520of%2520LLMs%2520from%250Ascratch%2520to%2520measure%2520the%2520impact%2520of%2520a%2520potential%2520poisoning%2520adversary%2520under%2520four%250Adifferent%2520attack%2520objectives%2520%2528denial-of-service%252C%2520belief%2520manipulation%252C%250Ajailbreaking%252C%2520and%2520prompt%2520stealing%2529%252C%2520and%2520across%2520a%2520wide%2520range%2520of%2520model%2520sizes%250A%2528from%2520600M%2520to%25207B%2529.%2520Our%2520main%2520result%2520is%2520that%2520poisoning%2520only%25200.1%2525%2520of%2520a%2520model%2527s%250Apre-training%2520dataset%2520is%2520sufficient%2520for%2520three%2520out%2520of%2520four%2520attacks%2520to%2520measurably%250Apersist%2520through%2520post-training.%2520Moreover%252C%2520simple%2520attacks%2520like%2520denial-of-service%250Apersist%2520through%2520post-training%2520with%2520a%2520poisoning%2520rate%2520of%2520only%25200.001%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Pre-Training%20Poisoning%20of%20LLMs&entry.906535625=Yiming%20Zhang%20and%20Javier%20Rando%20and%20Ivan%20Evtimov%20and%20Jianfeng%20Chi%20and%20Eric%20Michael%20Smith%20and%20Nicholas%20Carlini%20and%20Florian%20Tram%C3%A8r%20and%20Daphne%20Ippolito&entry.1292438233=%20%20Large%20language%20models%20are%20pre-trained%20on%20uncurated%20text%20datasets%20consisting%0Aof%20trillions%20of%20tokens%20scraped%20from%20the%20Web.%20Prior%20work%20has%20shown%20that%3A%20%281%29%0Aweb-scraped%20pre-training%20datasets%20can%20be%20practically%20poisoned%20by%20malicious%0Aactors%3B%20and%20%282%29%20adversaries%20can%20compromise%20language%20models%20after%20poisoning%0Afine-tuning%20datasets.%20Our%20work%20evaluates%20for%20the%20first%20time%20whether%20language%0Amodels%20can%20also%20be%20compromised%20during%20pre-training%2C%20with%20a%20focus%20on%20the%0Apersistence%20of%20pre-training%20attacks%20after%20models%20are%20fine-tuned%20as%20helpful%20and%0Aharmless%20chatbots%20%28i.e.%2C%20after%20SFT%20and%20DPO%29.%20We%20pre-train%20a%20series%20of%20LLMs%20from%0Ascratch%20to%20measure%20the%20impact%20of%20a%20potential%20poisoning%20adversary%20under%20four%0Adifferent%20attack%20objectives%20%28denial-of-service%2C%20belief%20manipulation%2C%0Ajailbreaking%2C%20and%20prompt%20stealing%29%2C%20and%20across%20a%20wide%20range%20of%20model%20sizes%0A%28from%20600M%20to%207B%29.%20Our%20main%20result%20is%20that%20poisoning%20only%200.1%25%20of%20a%20model%27s%0Apre-training%20dataset%20is%20sufficient%20for%20three%20out%20of%20four%20attacks%20to%20measurably%0Apersist%20through%20post-training.%20Moreover%2C%20simple%20attacks%20like%20denial-of-service%0Apersist%20through%20post-training%20with%20a%20poisoning%20rate%20of%20only%200.001%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13722v1&entry.124074799=Read"},
{"title": "Emphasizing Semantic Consistency of Salient Posture for Speech-Driven\n  Gesture Generation", "author": "Fengqi Liu and Hexiang Wang and Jingyu Gong and Ran Yi and Qianyu Zhou and Xuequan Lu and Jiangbo Lu and Lizhuang Ma", "abstract": "  Speech-driven gesture generation aims at synthesizing a gesture sequence\nsynchronized with the input speech signal. Previous methods leverage neural\nnetworks to directly map a compact audio representation to the gesture\nsequence, ignoring the semantic association of different modalities and failing\nto deal with salient gestures. In this paper, we propose a novel speech-driven\ngesture generation method by emphasizing the semantic consistency of salient\nposture. Specifically, we first learn a joint manifold space for the individual\nrepresentation of audio and body pose to exploit the inherent semantic\nassociation between two modalities, and propose to enforce semantic consistency\nvia a consistency loss. Furthermore, we emphasize the semantic consistency of\nsalient postures by introducing a weakly-supervised detector to identify\nsalient postures, and reweighting the consistency loss to focus more on\nlearning the correspondence between salient postures and the high-level\nsemantics of speech content. In addition, we propose to extract audio features\ndedicated to facial expression and body gesture separately, and design separate\nbranches for face and body gesture synthesis. Extensive experimental results\ndemonstrate the superiority of our method over the state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2410.13786v1", "date": "2024-10-17", "relevancy": 2.1915, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5542}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emphasizing%20Semantic%20Consistency%20of%20Salient%20Posture%20for%20Speech-Driven%0A%20%20Gesture%20Generation&body=Title%3A%20Emphasizing%20Semantic%20Consistency%20of%20Salient%20Posture%20for%20Speech-Driven%0A%20%20Gesture%20Generation%0AAuthor%3A%20Fengqi%20Liu%20and%20Hexiang%20Wang%20and%20Jingyu%20Gong%20and%20Ran%20Yi%20and%20Qianyu%20Zhou%20and%20Xuequan%20Lu%20and%20Jiangbo%20Lu%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Speech-driven%20gesture%20generation%20aims%20at%20synthesizing%20a%20gesture%20sequence%0Asynchronized%20with%20the%20input%20speech%20signal.%20Previous%20methods%20leverage%20neural%0Anetworks%20to%20directly%20map%20a%20compact%20audio%20representation%20to%20the%20gesture%0Asequence%2C%20ignoring%20the%20semantic%20association%20of%20different%20modalities%20and%20failing%0Ato%20deal%20with%20salient%20gestures.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20speech-driven%0Agesture%20generation%20method%20by%20emphasizing%20the%20semantic%20consistency%20of%20salient%0Aposture.%20Specifically%2C%20we%20first%20learn%20a%20joint%20manifold%20space%20for%20the%20individual%0Arepresentation%20of%20audio%20and%20body%20pose%20to%20exploit%20the%20inherent%20semantic%0Aassociation%20between%20two%20modalities%2C%20and%20propose%20to%20enforce%20semantic%20consistency%0Avia%20a%20consistency%20loss.%20Furthermore%2C%20we%20emphasize%20the%20semantic%20consistency%20of%0Asalient%20postures%20by%20introducing%20a%20weakly-supervised%20detector%20to%20identify%0Asalient%20postures%2C%20and%20reweighting%20the%20consistency%20loss%20to%20focus%20more%20on%0Alearning%20the%20correspondence%20between%20salient%20postures%20and%20the%20high-level%0Asemantics%20of%20speech%20content.%20In%20addition%2C%20we%20propose%20to%20extract%20audio%20features%0Adedicated%20to%20facial%20expression%20and%20body%20gesture%20separately%2C%20and%20design%20separate%0Abranches%20for%20face%20and%20body%20gesture%20synthesis.%20Extensive%20experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20the%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmphasizing%2520Semantic%2520Consistency%2520of%2520Salient%2520Posture%2520for%2520Speech-Driven%250A%2520%2520Gesture%2520Generation%26entry.906535625%3DFengqi%2520Liu%2520and%2520Hexiang%2520Wang%2520and%2520Jingyu%2520Gong%2520and%2520Ran%2520Yi%2520and%2520Qianyu%2520Zhou%2520and%2520Xuequan%2520Lu%2520and%2520Jiangbo%2520Lu%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Speech-driven%2520gesture%2520generation%2520aims%2520at%2520synthesizing%2520a%2520gesture%2520sequence%250Asynchronized%2520with%2520the%2520input%2520speech%2520signal.%2520Previous%2520methods%2520leverage%2520neural%250Anetworks%2520to%2520directly%2520map%2520a%2520compact%2520audio%2520representation%2520to%2520the%2520gesture%250Asequence%252C%2520ignoring%2520the%2520semantic%2520association%2520of%2520different%2520modalities%2520and%2520failing%250Ato%2520deal%2520with%2520salient%2520gestures.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520speech-driven%250Agesture%2520generation%2520method%2520by%2520emphasizing%2520the%2520semantic%2520consistency%2520of%2520salient%250Aposture.%2520Specifically%252C%2520we%2520first%2520learn%2520a%2520joint%2520manifold%2520space%2520for%2520the%2520individual%250Arepresentation%2520of%2520audio%2520and%2520body%2520pose%2520to%2520exploit%2520the%2520inherent%2520semantic%250Aassociation%2520between%2520two%2520modalities%252C%2520and%2520propose%2520to%2520enforce%2520semantic%2520consistency%250Avia%2520a%2520consistency%2520loss.%2520Furthermore%252C%2520we%2520emphasize%2520the%2520semantic%2520consistency%2520of%250Asalient%2520postures%2520by%2520introducing%2520a%2520weakly-supervised%2520detector%2520to%2520identify%250Asalient%2520postures%252C%2520and%2520reweighting%2520the%2520consistency%2520loss%2520to%2520focus%2520more%2520on%250Alearning%2520the%2520correspondence%2520between%2520salient%2520postures%2520and%2520the%2520high-level%250Asemantics%2520of%2520speech%2520content.%2520In%2520addition%252C%2520we%2520propose%2520to%2520extract%2520audio%2520features%250Adedicated%2520to%2520facial%2520expression%2520and%2520body%2520gesture%2520separately%252C%2520and%2520design%2520separate%250Abranches%2520for%2520face%2520and%2520body%2520gesture%2520synthesis.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520the%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emphasizing%20Semantic%20Consistency%20of%20Salient%20Posture%20for%20Speech-Driven%0A%20%20Gesture%20Generation&entry.906535625=Fengqi%20Liu%20and%20Hexiang%20Wang%20and%20Jingyu%20Gong%20and%20Ran%20Yi%20and%20Qianyu%20Zhou%20and%20Xuequan%20Lu%20and%20Jiangbo%20Lu%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Speech-driven%20gesture%20generation%20aims%20at%20synthesizing%20a%20gesture%20sequence%0Asynchronized%20with%20the%20input%20speech%20signal.%20Previous%20methods%20leverage%20neural%0Anetworks%20to%20directly%20map%20a%20compact%20audio%20representation%20to%20the%20gesture%0Asequence%2C%20ignoring%20the%20semantic%20association%20of%20different%20modalities%20and%20failing%0Ato%20deal%20with%20salient%20gestures.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20speech-driven%0Agesture%20generation%20method%20by%20emphasizing%20the%20semantic%20consistency%20of%20salient%0Aposture.%20Specifically%2C%20we%20first%20learn%20a%20joint%20manifold%20space%20for%20the%20individual%0Arepresentation%20of%20audio%20and%20body%20pose%20to%20exploit%20the%20inherent%20semantic%0Aassociation%20between%20two%20modalities%2C%20and%20propose%20to%20enforce%20semantic%20consistency%0Avia%20a%20consistency%20loss.%20Furthermore%2C%20we%20emphasize%20the%20semantic%20consistency%20of%0Asalient%20postures%20by%20introducing%20a%20weakly-supervised%20detector%20to%20identify%0Asalient%20postures%2C%20and%20reweighting%20the%20consistency%20loss%20to%20focus%20more%20on%0Alearning%20the%20correspondence%20between%20salient%20postures%20and%20the%20high-level%0Asemantics%20of%20speech%20content.%20In%20addition%2C%20we%20propose%20to%20extract%20audio%20features%0Adedicated%20to%20facial%20expression%20and%20body%20gesture%20separately%2C%20and%20design%20separate%0Abranches%20for%20face%20and%20body%20gesture%20synthesis.%20Extensive%20experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20the%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13786v1&entry.124074799=Read"},
{"title": "Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and\n  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments", "author": "Ranjan Sapkota and Zhichao Meng and Martin Churuvija and Xiaoqiang Du and Zenghong Ma and Manoj Karkee", "abstract": "  This study extensively evaluated You Only Look Once (YOLO) object detection\nalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and\nYOLO11 for green fruit detection in commercial orchards. The research also\nvalidated in-field fruitlet counting using an iPhone and machine vision sensors\nacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.\nAmong the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base\noutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In\nterms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9\nconfigurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n\nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,\nsignificantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,\nand YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This\ncomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,\noffering researchers essential insights to choose the best-suited model for\nfruitlet detection and possible automation in commercial orchards. For\nreal-time automation related work in relevant datasets, we recommend using\nYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,\nYOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet\nDetection, Greenfruit Detection, Green Apple Detection, Agricultural\nAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot\nDetection\n", "link": "http://arxiv.org/abs/2407.12040v5", "date": "2024-10-17", "relevancy": 2.1816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Performance%20Evaluation%20of%20YOLO11%2C%20YOLOv10%2C%20YOLOv9%20and%0A%20%20YOLOv8%20on%20Detecting%20and%20Counting%20Fruitlet%20in%20Complex%20Orchard%20Environments&body=Title%3A%20Comprehensive%20Performance%20Evaluation%20of%20YOLO11%2C%20YOLOv10%2C%20YOLOv9%20and%0A%20%20YOLOv8%20on%20Detecting%20and%20Counting%20Fruitlet%20in%20Complex%20Orchard%20Environments%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Zhichao%20Meng%20and%20Martin%20Churuvija%20and%20Xiaoqiang%20Du%20and%20Zenghong%20Ma%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20study%20extensively%20evaluated%20You%20Only%20Look%20Once%20%28YOLO%29%20object%20detection%0Aalgorithms%20across%20all%20configurations%20%28total%2022%29%20of%20YOLOv8%2C%20YOLOv9%2C%20YOLOv10%2C%20and%0AYOLO11%20for%20green%20fruit%20detection%20in%20commercial%20orchards.%20The%20research%20also%0Avalidated%20in-field%20fruitlet%20counting%20using%20an%20iPhone%20and%20machine%20vision%20sensors%0Aacross%20four%20apple%20varieties%3A%20Scifresh%2C%20Scilate%2C%20Honeycrisp%20and%20Cosmic%20Crisp.%0AAmong%20the%2022%20configurations%20evaluated%2C%20YOLO11s%20and%20YOLOv9%20gelan-base%0Aoutperformed%20others%20with%20mAP%4050%20scores%20of%200.933%20and%200.935%20respectively.%20In%0Aterms%20of%20recall%2C%20YOLOv9%20gelan-base%20achieved%20the%20highest%20value%20among%20YOLOv9%0Aconfigurations%20at%200.899%2C%20while%20YOLO11m%20led%20YOLO11%20variants%20with%200.897.%20YOLO11n%0Aemerged%20as%20the%20fastest%20model%2C%20achieving%20fastest%20inference%20speed%20of%20only%202.4%20ms%2C%0Asignificantly%20outpacing%20the%20leading%20configurations%20of%20YOLOv10n%2C%20YOLOv9%20gelan-s%2C%0Aand%20YOLOv8n%2C%20with%20speeds%20of%205.5%2C%2011.5%2C%20and%204.1%20ms%2C%20respectively.%20This%0Acomparative%20evaluation%20highlights%20the%20strengths%20of%20YOLO11%2C%20YOLOv9%2C%20and%20YOLOv10%2C%0Aoffering%20researchers%20essential%20insights%20to%20choose%20the%20best-suited%20model%20for%0Afruitlet%20detection%20and%20possible%20automation%20in%20commercial%20orchards.%20For%0Areal-time%20automation%20related%20work%20in%20relevant%20datasets%2C%20we%20recommend%20using%0AYOLO11n%20due%20to%20its%20high%20detection%20and%20image%20processing%20speed.%20Keywords%3A%20YOLO11%2C%0AYOLO11%20Object%20Detection%2C%20YOLOv10%2C%20YOLOv9%2C%20YOLOv8%2C%20You%20Only%20Look%20Once%2C%20Fruitlet%0ADetection%2C%20Greenfruit%20Detection%2C%20Green%20Apple%20Detection%2C%20Agricultural%0AAutomation%2C%20Artificial%20Intelligence%2C%20Deep%20Learning%2C%20Machine%20Learning%2C%20Zero-shot%0ADetection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12040v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Performance%2520Evaluation%2520of%2520YOLO11%252C%2520YOLOv10%252C%2520YOLOv9%2520and%250A%2520%2520YOLOv8%2520on%2520Detecting%2520and%2520Counting%2520Fruitlet%2520in%2520Complex%2520Orchard%2520Environments%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Zhichao%2520Meng%2520and%2520Martin%2520Churuvija%2520and%2520Xiaoqiang%2520Du%2520and%2520Zenghong%2520Ma%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520study%2520extensively%2520evaluated%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520object%2520detection%250Aalgorithms%2520across%2520all%2520configurations%2520%2528total%252022%2529%2520of%2520YOLOv8%252C%2520YOLOv9%252C%2520YOLOv10%252C%2520and%250AYOLO11%2520for%2520green%2520fruit%2520detection%2520in%2520commercial%2520orchards.%2520The%2520research%2520also%250Avalidated%2520in-field%2520fruitlet%2520counting%2520using%2520an%2520iPhone%2520and%2520machine%2520vision%2520sensors%250Aacross%2520four%2520apple%2520varieties%253A%2520Scifresh%252C%2520Scilate%252C%2520Honeycrisp%2520and%2520Cosmic%2520Crisp.%250AAmong%2520the%252022%2520configurations%2520evaluated%252C%2520YOLO11s%2520and%2520YOLOv9%2520gelan-base%250Aoutperformed%2520others%2520with%2520mAP%254050%2520scores%2520of%25200.933%2520and%25200.935%2520respectively.%2520In%250Aterms%2520of%2520recall%252C%2520YOLOv9%2520gelan-base%2520achieved%2520the%2520highest%2520value%2520among%2520YOLOv9%250Aconfigurations%2520at%25200.899%252C%2520while%2520YOLO11m%2520led%2520YOLO11%2520variants%2520with%25200.897.%2520YOLO11n%250Aemerged%2520as%2520the%2520fastest%2520model%252C%2520achieving%2520fastest%2520inference%2520speed%2520of%2520only%25202.4%2520ms%252C%250Asignificantly%2520outpacing%2520the%2520leading%2520configurations%2520of%2520YOLOv10n%252C%2520YOLOv9%2520gelan-s%252C%250Aand%2520YOLOv8n%252C%2520with%2520speeds%2520of%25205.5%252C%252011.5%252C%2520and%25204.1%2520ms%252C%2520respectively.%2520This%250Acomparative%2520evaluation%2520highlights%2520the%2520strengths%2520of%2520YOLO11%252C%2520YOLOv9%252C%2520and%2520YOLOv10%252C%250Aoffering%2520researchers%2520essential%2520insights%2520to%2520choose%2520the%2520best-suited%2520model%2520for%250Afruitlet%2520detection%2520and%2520possible%2520automation%2520in%2520commercial%2520orchards.%2520For%250Areal-time%2520automation%2520related%2520work%2520in%2520relevant%2520datasets%252C%2520we%2520recommend%2520using%250AYOLO11n%2520due%2520to%2520its%2520high%2520detection%2520and%2520image%2520processing%2520speed.%2520Keywords%253A%2520YOLO11%252C%250AYOLO11%2520Object%2520Detection%252C%2520YOLOv10%252C%2520YOLOv9%252C%2520YOLOv8%252C%2520You%2520Only%2520Look%2520Once%252C%2520Fruitlet%250ADetection%252C%2520Greenfruit%2520Detection%252C%2520Green%2520Apple%2520Detection%252C%2520Agricultural%250AAutomation%252C%2520Artificial%2520Intelligence%252C%2520Deep%2520Learning%252C%2520Machine%2520Learning%252C%2520Zero-shot%250ADetection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12040v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Performance%20Evaluation%20of%20YOLO11%2C%20YOLOv10%2C%20YOLOv9%20and%0A%20%20YOLOv8%20on%20Detecting%20and%20Counting%20Fruitlet%20in%20Complex%20Orchard%20Environments&entry.906535625=Ranjan%20Sapkota%20and%20Zhichao%20Meng%20and%20Martin%20Churuvija%20and%20Xiaoqiang%20Du%20and%20Zenghong%20Ma%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20study%20extensively%20evaluated%20You%20Only%20Look%20Once%20%28YOLO%29%20object%20detection%0Aalgorithms%20across%20all%20configurations%20%28total%2022%29%20of%20YOLOv8%2C%20YOLOv9%2C%20YOLOv10%2C%20and%0AYOLO11%20for%20green%20fruit%20detection%20in%20commercial%20orchards.%20The%20research%20also%0Avalidated%20in-field%20fruitlet%20counting%20using%20an%20iPhone%20and%20machine%20vision%20sensors%0Aacross%20four%20apple%20varieties%3A%20Scifresh%2C%20Scilate%2C%20Honeycrisp%20and%20Cosmic%20Crisp.%0AAmong%20the%2022%20configurations%20evaluated%2C%20YOLO11s%20and%20YOLOv9%20gelan-base%0Aoutperformed%20others%20with%20mAP%4050%20scores%20of%200.933%20and%200.935%20respectively.%20In%0Aterms%20of%20recall%2C%20YOLOv9%20gelan-base%20achieved%20the%20highest%20value%20among%20YOLOv9%0Aconfigurations%20at%200.899%2C%20while%20YOLO11m%20led%20YOLO11%20variants%20with%200.897.%20YOLO11n%0Aemerged%20as%20the%20fastest%20model%2C%20achieving%20fastest%20inference%20speed%20of%20only%202.4%20ms%2C%0Asignificantly%20outpacing%20the%20leading%20configurations%20of%20YOLOv10n%2C%20YOLOv9%20gelan-s%2C%0Aand%20YOLOv8n%2C%20with%20speeds%20of%205.5%2C%2011.5%2C%20and%204.1%20ms%2C%20respectively.%20This%0Acomparative%20evaluation%20highlights%20the%20strengths%20of%20YOLO11%2C%20YOLOv9%2C%20and%20YOLOv10%2C%0Aoffering%20researchers%20essential%20insights%20to%20choose%20the%20best-suited%20model%20for%0Afruitlet%20detection%20and%20possible%20automation%20in%20commercial%20orchards.%20For%0Areal-time%20automation%20related%20work%20in%20relevant%20datasets%2C%20we%20recommend%20using%0AYOLO11n%20due%20to%20its%20high%20detection%20and%20image%20processing%20speed.%20Keywords%3A%20YOLO11%2C%0AYOLO11%20Object%20Detection%2C%20YOLOv10%2C%20YOLOv9%2C%20YOLOv8%2C%20You%20Only%20Look%20Once%2C%20Fruitlet%0ADetection%2C%20Greenfruit%20Detection%2C%20Green%20Apple%20Detection%2C%20Agricultural%0AAutomation%2C%20Artificial%20Intelligence%2C%20Deep%20Learning%2C%20Machine%20Learning%2C%20Zero-shot%0ADetection%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12040v5&entry.124074799=Read"},
{"title": "Harnessing Webpage UIs for Text-Rich Visual Understanding", "author": "Junpeng Liu and Tianyue Ou and Yifan Song and Yuxiao Qu and Wai Lam and Chenyan Xiong and Wenhu Chen and Graham Neubig and Xiang Yue", "abstract": "  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on\nVisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n", "link": "http://arxiv.org/abs/2410.13824v1", "date": "2024-10-17", "relevancy": 2.1729, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Webpage%20UIs%20for%20Text-Rich%20Visual%20Understanding&body=Title%3A%20Harnessing%20Webpage%20UIs%20for%20Text-Rich%20Visual%20Understanding%0AAuthor%3A%20Junpeng%20Liu%20and%20Tianyue%20Ou%20and%20Yifan%20Song%20and%20Yuxiao%20Qu%20and%20Wai%20Lam%20and%20Chenyan%20Xiong%20and%20Wenhu%20Chen%20and%20Graham%20Neubig%20and%20Xiang%20Yue%0AAbstract%3A%20%20%20Text-rich%20visual%20understanding-the%20ability%20to%20process%20environments%20where%0Adense%20textual%20content%20is%20integrated%20with%20visuals-is%20crucial%20for%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20to%20interact%20effectively%20with%20structured%0Aenvironments.%20To%20enhance%20this%20capability%2C%20we%20propose%20synthesizing%20general%0Amultimodal%20instructions%20from%20webpage%20UIs%20using%20text-based%20large%20language%20models%0A%28LLMs%29.%20Despite%20lacking%20direct%20visual%20input%2C%20text-based%20LLMs%20are%20able%20to%0Aprocess%20structured%20text%20representations%20from%20webpage%20accessibility%20trees.%20These%0Ainstructions%20are%20then%20paired%20with%20UI%20screenshots%20to%20train%20multimodal%20models.%20We%0Aintroduce%20MultiUI%2C%20a%20dataset%20containing%207.3%20million%20samples%20from%201%20million%0Awebsites%2C%20covering%20diverse%20multimodal%20tasks%20and%20UI%20layouts.%20Models%20trained%20on%0AMultiUI%20not%20only%20excel%20in%20web%20UI%20tasks-achieving%20up%20to%20a%2048%5C%25%20improvement%20on%0AVisualWebBench%20and%20a%2019.1%5C%25%20boost%20in%20action%20accuracy%20on%20a%20web%20agent%20dataset%0AMind2Web-but%20also%20generalize%20surprisingly%20well%20to%20non-web%20UI%20tasks%20and%20even%20to%0Anon-UI%20domains%2C%20such%20as%20document%20understanding%2C%20OCR%2C%20and%20chart%20interpretation.%0AThese%20results%20highlight%20the%20broad%20applicability%20of%20web%20UI%20data%20for%20advancing%0Atext-rich%20visual%20understanding%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Webpage%2520UIs%2520for%2520Text-Rich%2520Visual%2520Understanding%26entry.906535625%3DJunpeng%2520Liu%2520and%2520Tianyue%2520Ou%2520and%2520Yifan%2520Song%2520and%2520Yuxiao%2520Qu%2520and%2520Wai%2520Lam%2520and%2520Chenyan%2520Xiong%2520and%2520Wenhu%2520Chen%2520and%2520Graham%2520Neubig%2520and%2520Xiang%2520Yue%26entry.1292438233%3D%2520%2520Text-rich%2520visual%2520understanding-the%2520ability%2520to%2520process%2520environments%2520where%250Adense%2520textual%2520content%2520is%2520integrated%2520with%2520visuals-is%2520crucial%2520for%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520to%2520interact%2520effectively%2520with%2520structured%250Aenvironments.%2520To%2520enhance%2520this%2520capability%252C%2520we%2520propose%2520synthesizing%2520general%250Amultimodal%2520instructions%2520from%2520webpage%2520UIs%2520using%2520text-based%2520large%2520language%2520models%250A%2528LLMs%2529.%2520Despite%2520lacking%2520direct%2520visual%2520input%252C%2520text-based%2520LLMs%2520are%2520able%2520to%250Aprocess%2520structured%2520text%2520representations%2520from%2520webpage%2520accessibility%2520trees.%2520These%250Ainstructions%2520are%2520then%2520paired%2520with%2520UI%2520screenshots%2520to%2520train%2520multimodal%2520models.%2520We%250Aintroduce%2520MultiUI%252C%2520a%2520dataset%2520containing%25207.3%2520million%2520samples%2520from%25201%2520million%250Awebsites%252C%2520covering%2520diverse%2520multimodal%2520tasks%2520and%2520UI%2520layouts.%2520Models%2520trained%2520on%250AMultiUI%2520not%2520only%2520excel%2520in%2520web%2520UI%2520tasks-achieving%2520up%2520to%2520a%252048%255C%2525%2520improvement%2520on%250AVisualWebBench%2520and%2520a%252019.1%255C%2525%2520boost%2520in%2520action%2520accuracy%2520on%2520a%2520web%2520agent%2520dataset%250AMind2Web-but%2520also%2520generalize%2520surprisingly%2520well%2520to%2520non-web%2520UI%2520tasks%2520and%2520even%2520to%250Anon-UI%2520domains%252C%2520such%2520as%2520document%2520understanding%252C%2520OCR%252C%2520and%2520chart%2520interpretation.%250AThese%2520results%2520highlight%2520the%2520broad%2520applicability%2520of%2520web%2520UI%2520data%2520for%2520advancing%250Atext-rich%2520visual%2520understanding%2520across%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Webpage%20UIs%20for%20Text-Rich%20Visual%20Understanding&entry.906535625=Junpeng%20Liu%20and%20Tianyue%20Ou%20and%20Yifan%20Song%20and%20Yuxiao%20Qu%20and%20Wai%20Lam%20and%20Chenyan%20Xiong%20and%20Wenhu%20Chen%20and%20Graham%20Neubig%20and%20Xiang%20Yue&entry.1292438233=%20%20Text-rich%20visual%20understanding-the%20ability%20to%20process%20environments%20where%0Adense%20textual%20content%20is%20integrated%20with%20visuals-is%20crucial%20for%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20to%20interact%20effectively%20with%20structured%0Aenvironments.%20To%20enhance%20this%20capability%2C%20we%20propose%20synthesizing%20general%0Amultimodal%20instructions%20from%20webpage%20UIs%20using%20text-based%20large%20language%20models%0A%28LLMs%29.%20Despite%20lacking%20direct%20visual%20input%2C%20text-based%20LLMs%20are%20able%20to%0Aprocess%20structured%20text%20representations%20from%20webpage%20accessibility%20trees.%20These%0Ainstructions%20are%20then%20paired%20with%20UI%20screenshots%20to%20train%20multimodal%20models.%20We%0Aintroduce%20MultiUI%2C%20a%20dataset%20containing%207.3%20million%20samples%20from%201%20million%0Awebsites%2C%20covering%20diverse%20multimodal%20tasks%20and%20UI%20layouts.%20Models%20trained%20on%0AMultiUI%20not%20only%20excel%20in%20web%20UI%20tasks-achieving%20up%20to%20a%2048%5C%25%20improvement%20on%0AVisualWebBench%20and%20a%2019.1%5C%25%20boost%20in%20action%20accuracy%20on%20a%20web%20agent%20dataset%0AMind2Web-but%20also%20generalize%20surprisingly%20well%20to%20non-web%20UI%20tasks%20and%20even%20to%0Anon-UI%20domains%2C%20such%20as%20document%20understanding%2C%20OCR%2C%20and%20chart%20interpretation.%0AThese%20results%20highlight%20the%20broad%20applicability%20of%20web%20UI%20data%20for%20advancing%0Atext-rich%20visual%20understanding%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13824v1&entry.124074799=Read"},
{"title": "Spatiotemporal Object Detection for Improved Aerial Vehicle Detection in\n  Traffic Monitoring", "author": "Kristina Telegraph and Christos Kyrkou", "abstract": "  This work presents advancements in multi-class vehicle detection using UAV\ncameras through the development of spatiotemporal object detection models. The\nstudy introduces a Spatio-Temporal Vehicle Detection Dataset (STVD) containing\n6, 600 annotated sequential frame images captured by UAVs, enabling\ncomprehensive training and evaluation of algorithms for holistic spatiotemporal\nperception. A YOLO-based object detection algorithm is enhanced to incorporate\ntemporal dynamics, resulting in improved performance over single frame models.\nThe integration of attention mechanisms into spatiotemporal models is shown to\nfurther enhance performance. Experimental validation demonstrates significant\nprogress, with the best spatiotemporal model exhibiting a 16.22% improvement\nover single frame models, while it is demonstrated that attention mechanisms\nhold the potential for additional performance gains.\n", "link": "http://arxiv.org/abs/2410.13616v1", "date": "2024-10-17", "relevancy": 2.17, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5676}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5344}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20Object%20Detection%20for%20Improved%20Aerial%20Vehicle%20Detection%20in%0A%20%20Traffic%20Monitoring&body=Title%3A%20Spatiotemporal%20Object%20Detection%20for%20Improved%20Aerial%20Vehicle%20Detection%20in%0A%20%20Traffic%20Monitoring%0AAuthor%3A%20Kristina%20Telegraph%20and%20Christos%20Kyrkou%0AAbstract%3A%20%20%20This%20work%20presents%20advancements%20in%20multi-class%20vehicle%20detection%20using%20UAV%0Acameras%20through%20the%20development%20of%20spatiotemporal%20object%20detection%20models.%20The%0Astudy%20introduces%20a%20Spatio-Temporal%20Vehicle%20Detection%20Dataset%20%28STVD%29%20containing%0A6%2C%20600%20annotated%20sequential%20frame%20images%20captured%20by%20UAVs%2C%20enabling%0Acomprehensive%20training%20and%20evaluation%20of%20algorithms%20for%20holistic%20spatiotemporal%0Aperception.%20A%20YOLO-based%20object%20detection%20algorithm%20is%20enhanced%20to%20incorporate%0Atemporal%20dynamics%2C%20resulting%20in%20improved%20performance%20over%20single%20frame%20models.%0AThe%20integration%20of%20attention%20mechanisms%20into%20spatiotemporal%20models%20is%20shown%20to%0Afurther%20enhance%20performance.%20Experimental%20validation%20demonstrates%20significant%0Aprogress%2C%20with%20the%20best%20spatiotemporal%20model%20exhibiting%20a%2016.22%25%20improvement%0Aover%20single%20frame%20models%2C%20while%20it%20is%20demonstrated%20that%20attention%20mechanisms%0Ahold%20the%20potential%20for%20additional%20performance%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatiotemporal%2520Object%2520Detection%2520for%2520Improved%2520Aerial%2520Vehicle%2520Detection%2520in%250A%2520%2520Traffic%2520Monitoring%26entry.906535625%3DKristina%2520Telegraph%2520and%2520Christos%2520Kyrkou%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520advancements%2520in%2520multi-class%2520vehicle%2520detection%2520using%2520UAV%250Acameras%2520through%2520the%2520development%2520of%2520spatiotemporal%2520object%2520detection%2520models.%2520The%250Astudy%2520introduces%2520a%2520Spatio-Temporal%2520Vehicle%2520Detection%2520Dataset%2520%2528STVD%2529%2520containing%250A6%252C%2520600%2520annotated%2520sequential%2520frame%2520images%2520captured%2520by%2520UAVs%252C%2520enabling%250Acomprehensive%2520training%2520and%2520evaluation%2520of%2520algorithms%2520for%2520holistic%2520spatiotemporal%250Aperception.%2520A%2520YOLO-based%2520object%2520detection%2520algorithm%2520is%2520enhanced%2520to%2520incorporate%250Atemporal%2520dynamics%252C%2520resulting%2520in%2520improved%2520performance%2520over%2520single%2520frame%2520models.%250AThe%2520integration%2520of%2520attention%2520mechanisms%2520into%2520spatiotemporal%2520models%2520is%2520shown%2520to%250Afurther%2520enhance%2520performance.%2520Experimental%2520validation%2520demonstrates%2520significant%250Aprogress%252C%2520with%2520the%2520best%2520spatiotemporal%2520model%2520exhibiting%2520a%252016.22%2525%2520improvement%250Aover%2520single%2520frame%2520models%252C%2520while%2520it%2520is%2520demonstrated%2520that%2520attention%2520mechanisms%250Ahold%2520the%2520potential%2520for%2520additional%2520performance%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Object%20Detection%20for%20Improved%20Aerial%20Vehicle%20Detection%20in%0A%20%20Traffic%20Monitoring&entry.906535625=Kristina%20Telegraph%20and%20Christos%20Kyrkou&entry.1292438233=%20%20This%20work%20presents%20advancements%20in%20multi-class%20vehicle%20detection%20using%20UAV%0Acameras%20through%20the%20development%20of%20spatiotemporal%20object%20detection%20models.%20The%0Astudy%20introduces%20a%20Spatio-Temporal%20Vehicle%20Detection%20Dataset%20%28STVD%29%20containing%0A6%2C%20600%20annotated%20sequential%20frame%20images%20captured%20by%20UAVs%2C%20enabling%0Acomprehensive%20training%20and%20evaluation%20of%20algorithms%20for%20holistic%20spatiotemporal%0Aperception.%20A%20YOLO-based%20object%20detection%20algorithm%20is%20enhanced%20to%20incorporate%0Atemporal%20dynamics%2C%20resulting%20in%20improved%20performance%20over%20single%20frame%20models.%0AThe%20integration%20of%20attention%20mechanisms%20into%20spatiotemporal%20models%20is%20shown%20to%0Afurther%20enhance%20performance.%20Experimental%20validation%20demonstrates%20significant%0Aprogress%2C%20with%20the%20best%20spatiotemporal%20model%20exhibiting%20a%2016.22%25%20improvement%0Aover%20single%20frame%20models%2C%20while%20it%20is%20demonstrated%20that%20attention%20mechanisms%0Ahold%20the%20potential%20for%20additional%20performance%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13616v1&entry.124074799=Read"},
{"title": "Generation through the lens of learning theory", "author": "Vinod Raman and Ambuj Tewari", "abstract": "  We study generation through the lens of statistical learning theory. First,\nwe abstract and formalize the results of Gold [1967], Angluin [1979, 1980], and\nKleinberg and Mullainathan [2024] for language identification/generation in the\nlimit in terms of a binary hypothesis class defined over an abstract instance\nspace. Then, we formalize a different paradigm of generation studied by\nKleinberg and Mullainathan [2024], which we call ``uniform generation,\" and\nprovide a characterization of which hypothesis classes are uniformly\ngeneratable. As is standard in statistical learning theory, our\ncharacterization is in terms of the finiteness of a new combinatorial dimension\nwe call the Closure dimension. By doing so, we are able to compare\ngeneratability with predictability (captured via PAC and online learnability)\nand show that these two properties of hypothesis classes are\n\\emph{incompatible} - there are classes that are generatable but not\npredictable and vice versa.\n", "link": "http://arxiv.org/abs/2410.13714v1", "date": "2024-10-17", "relevancy": 2.1489, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5807}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5117}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20through%20the%20lens%20of%20learning%20theory&body=Title%3A%20Generation%20through%20the%20lens%20of%20learning%20theory%0AAuthor%3A%20Vinod%20Raman%20and%20Ambuj%20Tewari%0AAbstract%3A%20%20%20We%20study%20generation%20through%20the%20lens%20of%20statistical%20learning%20theory.%20First%2C%0Awe%20abstract%20and%20formalize%20the%20results%20of%20Gold%20%5B1967%5D%2C%20Angluin%20%5B1979%2C%201980%5D%2C%20and%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%20for%20language%20identification/generation%20in%20the%0Alimit%20in%20terms%20of%20a%20binary%20hypothesis%20class%20defined%20over%20an%20abstract%20instance%0Aspace.%20Then%2C%20we%20formalize%20a%20different%20paradigm%20of%20generation%20studied%20by%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%2C%20which%20we%20call%20%60%60uniform%20generation%2C%22%20and%0Aprovide%20a%20characterization%20of%20which%20hypothesis%20classes%20are%20uniformly%0Ageneratable.%20As%20is%20standard%20in%20statistical%20learning%20theory%2C%20our%0Acharacterization%20is%20in%20terms%20of%20the%20finiteness%20of%20a%20new%20combinatorial%20dimension%0Awe%20call%20the%20Closure%20dimension.%20By%20doing%20so%2C%20we%20are%20able%20to%20compare%0Ageneratability%20with%20predictability%20%28captured%20via%20PAC%20and%20online%20learnability%29%0Aand%20show%20that%20these%20two%20properties%20of%20hypothesis%20classes%20are%0A%5Cemph%7Bincompatible%7D%20-%20there%20are%20classes%20that%20are%20generatable%20but%20not%0Apredictable%20and%20vice%20versa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520through%2520the%2520lens%2520of%2520learning%2520theory%26entry.906535625%3DVinod%2520Raman%2520and%2520Ambuj%2520Tewari%26entry.1292438233%3D%2520%2520We%2520study%2520generation%2520through%2520the%2520lens%2520of%2520statistical%2520learning%2520theory.%2520First%252C%250Awe%2520abstract%2520and%2520formalize%2520the%2520results%2520of%2520Gold%2520%255B1967%255D%252C%2520Angluin%2520%255B1979%252C%25201980%255D%252C%2520and%250AKleinberg%2520and%2520Mullainathan%2520%255B2024%255D%2520for%2520language%2520identification/generation%2520in%2520the%250Alimit%2520in%2520terms%2520of%2520a%2520binary%2520hypothesis%2520class%2520defined%2520over%2520an%2520abstract%2520instance%250Aspace.%2520Then%252C%2520we%2520formalize%2520a%2520different%2520paradigm%2520of%2520generation%2520studied%2520by%250AKleinberg%2520and%2520Mullainathan%2520%255B2024%255D%252C%2520which%2520we%2520call%2520%2560%2560uniform%2520generation%252C%2522%2520and%250Aprovide%2520a%2520characterization%2520of%2520which%2520hypothesis%2520classes%2520are%2520uniformly%250Ageneratable.%2520As%2520is%2520standard%2520in%2520statistical%2520learning%2520theory%252C%2520our%250Acharacterization%2520is%2520in%2520terms%2520of%2520the%2520finiteness%2520of%2520a%2520new%2520combinatorial%2520dimension%250Awe%2520call%2520the%2520Closure%2520dimension.%2520By%2520doing%2520so%252C%2520we%2520are%2520able%2520to%2520compare%250Ageneratability%2520with%2520predictability%2520%2528captured%2520via%2520PAC%2520and%2520online%2520learnability%2529%250Aand%2520show%2520that%2520these%2520two%2520properties%2520of%2520hypothesis%2520classes%2520are%250A%255Cemph%257Bincompatible%257D%2520-%2520there%2520are%2520classes%2520that%2520are%2520generatable%2520but%2520not%250Apredictable%2520and%2520vice%2520versa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20through%20the%20lens%20of%20learning%20theory&entry.906535625=Vinod%20Raman%20and%20Ambuj%20Tewari&entry.1292438233=%20%20We%20study%20generation%20through%20the%20lens%20of%20statistical%20learning%20theory.%20First%2C%0Awe%20abstract%20and%20formalize%20the%20results%20of%20Gold%20%5B1967%5D%2C%20Angluin%20%5B1979%2C%201980%5D%2C%20and%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%20for%20language%20identification/generation%20in%20the%0Alimit%20in%20terms%20of%20a%20binary%20hypothesis%20class%20defined%20over%20an%20abstract%20instance%0Aspace.%20Then%2C%20we%20formalize%20a%20different%20paradigm%20of%20generation%20studied%20by%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%2C%20which%20we%20call%20%60%60uniform%20generation%2C%22%20and%0Aprovide%20a%20characterization%20of%20which%20hypothesis%20classes%20are%20uniformly%0Ageneratable.%20As%20is%20standard%20in%20statistical%20learning%20theory%2C%20our%0Acharacterization%20is%20in%20terms%20of%20the%20finiteness%20of%20a%20new%20combinatorial%20dimension%0Awe%20call%20the%20Closure%20dimension.%20By%20doing%20so%2C%20we%20are%20able%20to%20compare%0Ageneratability%20with%20predictability%20%28captured%20via%20PAC%20and%20online%20learnability%29%0Aand%20show%20that%20these%20two%20properties%20of%20hypothesis%20classes%20are%0A%5Cemph%7Bincompatible%7D%20-%20there%20are%20classes%20that%20are%20generatable%20but%20not%0Apredictable%20and%20vice%20versa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13714v1&entry.124074799=Read"},
{"title": "SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit\n  ToM Application in LLMs", "author": "Yuling Gu and Oyvind Tafjord and Hyunwoo Kim and Jared Moore and Ronan Le Bras and Peter Clark and Yejin Choi", "abstract": "  While prior work has explored whether large language models (LLMs) possess a\n\"theory of mind\" (ToM) - the ability to attribute mental states to oneself and\nothers - there has been little work testing whether LLMs can implicitly apply\nsuch knowledge to predict behavior, or to judge whether an observed behavior is\nrational. Such skills are critical for appropriate interaction in social\nenvironments. We create a new dataset, SimpleTom, containing concise, diverse\nstories (e.g., \"The can of Pringles has moldy chips in it. Mary picks up the\ncan in the supermarket and walks to the cashier.\"), each with three questions\nthat test different degrees of ToM reasoning, asking models to predict (a)\nmental state (\"Is Mary aware of the mold?\"), (b) behavior (\"Will Mary pay for\nthe chips or report the mold?\"), and (c) judgment (\"Mary paid for the chips.\nWas that reasonable?\"). To our knowledge, SimpleToM is the first dataset to\nsystematically explore downstream reasoning requiring knowledge of mental\nstates in realistic scenarios. Our experimental results are intriguing: While\nmost models can reliably predict mental state on our dataset (a), they often\nfail to correctly predict the behavior (b), and fare even worse at judging\nwhether given behaviors are reasonable (c), despite being correctly aware of\nthe protagonist's mental state should make such secondary predictions obvious.\nWe further show that we can help models do better at (b) and (c) via\ninterventions such as reminding the model of its earlier mental state answer\nand mental-state-specific chain-of-thought prompting, raising the action\nprediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment\naccuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models\ncan be coaxed to perform well, it requires task-specific interventions, and the\nnatural model performances remain low, a cautionary tale for LLM deployment.\n", "link": "http://arxiv.org/abs/2410.13648v1", "date": "2024-10-17", "relevancy": 2.1393, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimpleToM%3A%20Exposing%20the%20Gap%20between%20Explicit%20ToM%20Inference%20and%20Implicit%0A%20%20ToM%20Application%20in%20LLMs&body=Title%3A%20SimpleToM%3A%20Exposing%20the%20Gap%20between%20Explicit%20ToM%20Inference%20and%20Implicit%0A%20%20ToM%20Application%20in%20LLMs%0AAuthor%3A%20Yuling%20Gu%20and%20Oyvind%20Tafjord%20and%20Hyunwoo%20Kim%20and%20Jared%20Moore%20and%20Ronan%20Le%20Bras%20and%20Peter%20Clark%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20While%20prior%20work%20has%20explored%20whether%20large%20language%20models%20%28LLMs%29%20possess%20a%0A%22theory%20of%20mind%22%20%28ToM%29%20-%20the%20ability%20to%20attribute%20mental%20states%20to%20oneself%20and%0Aothers%20-%20there%20has%20been%20little%20work%20testing%20whether%20LLMs%20can%20implicitly%20apply%0Asuch%20knowledge%20to%20predict%20behavior%2C%20or%20to%20judge%20whether%20an%20observed%20behavior%20is%0Arational.%20Such%20skills%20are%20critical%20for%20appropriate%20interaction%20in%20social%0Aenvironments.%20We%20create%20a%20new%20dataset%2C%20SimpleTom%2C%20containing%20concise%2C%20diverse%0Astories%20%28e.g.%2C%20%22The%20can%20of%20Pringles%20has%20moldy%20chips%20in%20it.%20Mary%20picks%20up%20the%0Acan%20in%20the%20supermarket%20and%20walks%20to%20the%20cashier.%22%29%2C%20each%20with%20three%20questions%0Athat%20test%20different%20degrees%20of%20ToM%20reasoning%2C%20asking%20models%20to%20predict%20%28a%29%0Amental%20state%20%28%22Is%20Mary%20aware%20of%20the%20mold%3F%22%29%2C%20%28b%29%20behavior%20%28%22Will%20Mary%20pay%20for%0Athe%20chips%20or%20report%20the%20mold%3F%22%29%2C%20and%20%28c%29%20judgment%20%28%22Mary%20paid%20for%20the%20chips.%0AWas%20that%20reasonable%3F%22%29.%20To%20our%20knowledge%2C%20SimpleToM%20is%20the%20first%20dataset%20to%0Asystematically%20explore%20downstream%20reasoning%20requiring%20knowledge%20of%20mental%0Astates%20in%20realistic%20scenarios.%20Our%20experimental%20results%20are%20intriguing%3A%20While%0Amost%20models%20can%20reliably%20predict%20mental%20state%20on%20our%20dataset%20%28a%29%2C%20they%20often%0Afail%20to%20correctly%20predict%20the%20behavior%20%28b%29%2C%20and%20fare%20even%20worse%20at%20judging%0Awhether%20given%20behaviors%20are%20reasonable%20%28c%29%2C%20despite%20being%20correctly%20aware%20of%0Athe%20protagonist%27s%20mental%20state%20should%20make%20such%20secondary%20predictions%20obvious.%0AWe%20further%20show%20that%20we%20can%20help%20models%20do%20better%20at%20%28b%29%20and%20%28c%29%20via%0Ainterventions%20such%20as%20reminding%20the%20model%20of%20its%20earlier%20mental%20state%20answer%0Aand%20mental-state-specific%20chain-of-thought%20prompting%2C%20raising%20the%20action%0Aprediction%20accuracies%20%28e.g.%2C%20from%2049.5%25%20to%2093.5%25%20for%20GPT-4o%29%20and%20judgment%0Aaccuracies%20%28e.g.%2C%20from%2015.3%25%20to%2094.7%25%20in%20GPT-4o%29.%20While%20this%20shows%20that%20models%0Acan%20be%20coaxed%20to%20perform%20well%2C%20it%20requires%20task-specific%20interventions%2C%20and%20the%0Anatural%20model%20performances%20remain%20low%2C%20a%20cautionary%20tale%20for%20LLM%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimpleToM%253A%2520Exposing%2520the%2520Gap%2520between%2520Explicit%2520ToM%2520Inference%2520and%2520Implicit%250A%2520%2520ToM%2520Application%2520in%2520LLMs%26entry.906535625%3DYuling%2520Gu%2520and%2520Oyvind%2520Tafjord%2520and%2520Hyunwoo%2520Kim%2520and%2520Jared%2520Moore%2520and%2520Ronan%2520Le%2520Bras%2520and%2520Peter%2520Clark%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520While%2520prior%2520work%2520has%2520explored%2520whether%2520large%2520language%2520models%2520%2528LLMs%2529%2520possess%2520a%250A%2522theory%2520of%2520mind%2522%2520%2528ToM%2529%2520-%2520the%2520ability%2520to%2520attribute%2520mental%2520states%2520to%2520oneself%2520and%250Aothers%2520-%2520there%2520has%2520been%2520little%2520work%2520testing%2520whether%2520LLMs%2520can%2520implicitly%2520apply%250Asuch%2520knowledge%2520to%2520predict%2520behavior%252C%2520or%2520to%2520judge%2520whether%2520an%2520observed%2520behavior%2520is%250Arational.%2520Such%2520skills%2520are%2520critical%2520for%2520appropriate%2520interaction%2520in%2520social%250Aenvironments.%2520We%2520create%2520a%2520new%2520dataset%252C%2520SimpleTom%252C%2520containing%2520concise%252C%2520diverse%250Astories%2520%2528e.g.%252C%2520%2522The%2520can%2520of%2520Pringles%2520has%2520moldy%2520chips%2520in%2520it.%2520Mary%2520picks%2520up%2520the%250Acan%2520in%2520the%2520supermarket%2520and%2520walks%2520to%2520the%2520cashier.%2522%2529%252C%2520each%2520with%2520three%2520questions%250Athat%2520test%2520different%2520degrees%2520of%2520ToM%2520reasoning%252C%2520asking%2520models%2520to%2520predict%2520%2528a%2529%250Amental%2520state%2520%2528%2522Is%2520Mary%2520aware%2520of%2520the%2520mold%253F%2522%2529%252C%2520%2528b%2529%2520behavior%2520%2528%2522Will%2520Mary%2520pay%2520for%250Athe%2520chips%2520or%2520report%2520the%2520mold%253F%2522%2529%252C%2520and%2520%2528c%2529%2520judgment%2520%2528%2522Mary%2520paid%2520for%2520the%2520chips.%250AWas%2520that%2520reasonable%253F%2522%2529.%2520To%2520our%2520knowledge%252C%2520SimpleToM%2520is%2520the%2520first%2520dataset%2520to%250Asystematically%2520explore%2520downstream%2520reasoning%2520requiring%2520knowledge%2520of%2520mental%250Astates%2520in%2520realistic%2520scenarios.%2520Our%2520experimental%2520results%2520are%2520intriguing%253A%2520While%250Amost%2520models%2520can%2520reliably%2520predict%2520mental%2520state%2520on%2520our%2520dataset%2520%2528a%2529%252C%2520they%2520often%250Afail%2520to%2520correctly%2520predict%2520the%2520behavior%2520%2528b%2529%252C%2520and%2520fare%2520even%2520worse%2520at%2520judging%250Awhether%2520given%2520behaviors%2520are%2520reasonable%2520%2528c%2529%252C%2520despite%2520being%2520correctly%2520aware%2520of%250Athe%2520protagonist%2527s%2520mental%2520state%2520should%2520make%2520such%2520secondary%2520predictions%2520obvious.%250AWe%2520further%2520show%2520that%2520we%2520can%2520help%2520models%2520do%2520better%2520at%2520%2528b%2529%2520and%2520%2528c%2529%2520via%250Ainterventions%2520such%2520as%2520reminding%2520the%2520model%2520of%2520its%2520earlier%2520mental%2520state%2520answer%250Aand%2520mental-state-specific%2520chain-of-thought%2520prompting%252C%2520raising%2520the%2520action%250Aprediction%2520accuracies%2520%2528e.g.%252C%2520from%252049.5%2525%2520to%252093.5%2525%2520for%2520GPT-4o%2529%2520and%2520judgment%250Aaccuracies%2520%2528e.g.%252C%2520from%252015.3%2525%2520to%252094.7%2525%2520in%2520GPT-4o%2529.%2520While%2520this%2520shows%2520that%2520models%250Acan%2520be%2520coaxed%2520to%2520perform%2520well%252C%2520it%2520requires%2520task-specific%2520interventions%252C%2520and%2520the%250Anatural%2520model%2520performances%2520remain%2520low%252C%2520a%2520cautionary%2520tale%2520for%2520LLM%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimpleToM%3A%20Exposing%20the%20Gap%20between%20Explicit%20ToM%20Inference%20and%20Implicit%0A%20%20ToM%20Application%20in%20LLMs&entry.906535625=Yuling%20Gu%20and%20Oyvind%20Tafjord%20and%20Hyunwoo%20Kim%20and%20Jared%20Moore%20and%20Ronan%20Le%20Bras%20and%20Peter%20Clark%20and%20Yejin%20Choi&entry.1292438233=%20%20While%20prior%20work%20has%20explored%20whether%20large%20language%20models%20%28LLMs%29%20possess%20a%0A%22theory%20of%20mind%22%20%28ToM%29%20-%20the%20ability%20to%20attribute%20mental%20states%20to%20oneself%20and%0Aothers%20-%20there%20has%20been%20little%20work%20testing%20whether%20LLMs%20can%20implicitly%20apply%0Asuch%20knowledge%20to%20predict%20behavior%2C%20or%20to%20judge%20whether%20an%20observed%20behavior%20is%0Arational.%20Such%20skills%20are%20critical%20for%20appropriate%20interaction%20in%20social%0Aenvironments.%20We%20create%20a%20new%20dataset%2C%20SimpleTom%2C%20containing%20concise%2C%20diverse%0Astories%20%28e.g.%2C%20%22The%20can%20of%20Pringles%20has%20moldy%20chips%20in%20it.%20Mary%20picks%20up%20the%0Acan%20in%20the%20supermarket%20and%20walks%20to%20the%20cashier.%22%29%2C%20each%20with%20three%20questions%0Athat%20test%20different%20degrees%20of%20ToM%20reasoning%2C%20asking%20models%20to%20predict%20%28a%29%0Amental%20state%20%28%22Is%20Mary%20aware%20of%20the%20mold%3F%22%29%2C%20%28b%29%20behavior%20%28%22Will%20Mary%20pay%20for%0Athe%20chips%20or%20report%20the%20mold%3F%22%29%2C%20and%20%28c%29%20judgment%20%28%22Mary%20paid%20for%20the%20chips.%0AWas%20that%20reasonable%3F%22%29.%20To%20our%20knowledge%2C%20SimpleToM%20is%20the%20first%20dataset%20to%0Asystematically%20explore%20downstream%20reasoning%20requiring%20knowledge%20of%20mental%0Astates%20in%20realistic%20scenarios.%20Our%20experimental%20results%20are%20intriguing%3A%20While%0Amost%20models%20can%20reliably%20predict%20mental%20state%20on%20our%20dataset%20%28a%29%2C%20they%20often%0Afail%20to%20correctly%20predict%20the%20behavior%20%28b%29%2C%20and%20fare%20even%20worse%20at%20judging%0Awhether%20given%20behaviors%20are%20reasonable%20%28c%29%2C%20despite%20being%20correctly%20aware%20of%0Athe%20protagonist%27s%20mental%20state%20should%20make%20such%20secondary%20predictions%20obvious.%0AWe%20further%20show%20that%20we%20can%20help%20models%20do%20better%20at%20%28b%29%20and%20%28c%29%20via%0Ainterventions%20such%20as%20reminding%20the%20model%20of%20its%20earlier%20mental%20state%20answer%0Aand%20mental-state-specific%20chain-of-thought%20prompting%2C%20raising%20the%20action%0Aprediction%20accuracies%20%28e.g.%2C%20from%2049.5%25%20to%2093.5%25%20for%20GPT-4o%29%20and%20judgment%0Aaccuracies%20%28e.g.%2C%20from%2015.3%25%20to%2094.7%25%20in%20GPT-4o%29.%20While%20this%20shows%20that%20models%0Acan%20be%20coaxed%20to%20perform%20well%2C%20it%20requires%20task-specific%20interventions%2C%20and%20the%0Anatural%20model%20performances%20remain%20low%2C%20a%20cautionary%20tale%20for%20LLM%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13648v1&entry.124074799=Read"},
{"title": "Representing Model Weights with Language using Tree Experts", "author": "Eliahu Horwitz and Bar Cavia and Jonathan Kahana and Yedid Hoshen", "abstract": "  The increasing availability of public models begs the question: can we train\nneural networks that use other networks as input? This paper learns to\nrepresent models within a joint space that embeds both model weights and\nlanguage. However, machine learning on model weights is challenging as model\nweights often exhibit significant variation unrelated to the models' semantic\nproperties (nuisance variation). We identify a key property of real-world\nmodels: most public models belong to a small set of Model Trees, where all\nmodels within a tree are fine-tuned from a common ancestor (e.g., a foundation\nmodel). Importantly, we find that within each tree there is less nuisance\nvariation between models. For example, while classifying models according to\ntheir training dataset generally requires complex architectures, in our case,\neven a linear classifier trained on a single layer is often effective. While\neffective, linear layers are computationally expensive as model weights are\nvery high dimensional. To address this, we introduce Probing Experts (ProbeX),\na theoretically motivated, lightweight probing method. Notably, ProbeX is the\nfirst probing method designed to learn from the weights of just a single model\nlayer. We also construct and release a dataset that simulates the structure of\npublic model repositories. Our results show that ProbeX can effectively map the\nweights of large models into a shared weight-language embedding space.\nFurthermore, we demonstrate the impressive generalization of our method,\nachieving zero-shot model classification and retrieval.\n", "link": "http://arxiv.org/abs/2410.13569v1", "date": "2024-10-17", "relevancy": 2.1356, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20Model%20Weights%20with%20Language%20using%20Tree%20Experts&body=Title%3A%20Representing%20Model%20Weights%20with%20Language%20using%20Tree%20Experts%0AAuthor%3A%20Eliahu%20Horwitz%20and%20Bar%20Cavia%20and%20Jonathan%20Kahana%20and%20Yedid%20Hoshen%0AAbstract%3A%20%20%20The%20increasing%20availability%20of%20public%20models%20begs%20the%20question%3A%20can%20we%20train%0Aneural%20networks%20that%20use%20other%20networks%20as%20input%3F%20This%20paper%20learns%20to%0Arepresent%20models%20within%20a%20joint%20space%20that%20embeds%20both%20model%20weights%20and%0Alanguage.%20However%2C%20machine%20learning%20on%20model%20weights%20is%20challenging%20as%20model%0Aweights%20often%20exhibit%20significant%20variation%20unrelated%20to%20the%20models%27%20semantic%0Aproperties%20%28nuisance%20variation%29.%20We%20identify%20a%20key%20property%20of%20real-world%0Amodels%3A%20most%20public%20models%20belong%20to%20a%20small%20set%20of%20Model%20Trees%2C%20where%20all%0Amodels%20within%20a%20tree%20are%20fine-tuned%20from%20a%20common%20ancestor%20%28e.g.%2C%20a%20foundation%0Amodel%29.%20Importantly%2C%20we%20find%20that%20within%20each%20tree%20there%20is%20less%20nuisance%0Avariation%20between%20models.%20For%20example%2C%20while%20classifying%20models%20according%20to%0Atheir%20training%20dataset%20generally%20requires%20complex%20architectures%2C%20in%20our%20case%2C%0Aeven%20a%20linear%20classifier%20trained%20on%20a%20single%20layer%20is%20often%20effective.%20While%0Aeffective%2C%20linear%20layers%20are%20computationally%20expensive%20as%20model%20weights%20are%0Avery%20high%20dimensional.%20To%20address%20this%2C%20we%20introduce%20Probing%20Experts%20%28ProbeX%29%2C%0Aa%20theoretically%20motivated%2C%20lightweight%20probing%20method.%20Notably%2C%20ProbeX%20is%20the%0Afirst%20probing%20method%20designed%20to%20learn%20from%20the%20weights%20of%20just%20a%20single%20model%0Alayer.%20We%20also%20construct%20and%20release%20a%20dataset%20that%20simulates%20the%20structure%20of%0Apublic%20model%20repositories.%20Our%20results%20show%20that%20ProbeX%20can%20effectively%20map%20the%0Aweights%20of%20large%20models%20into%20a%20shared%20weight-language%20embedding%20space.%0AFurthermore%2C%20we%20demonstrate%20the%20impressive%20generalization%20of%20our%20method%2C%0Aachieving%20zero-shot%20model%20classification%20and%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520Model%2520Weights%2520with%2520Language%2520using%2520Tree%2520Experts%26entry.906535625%3DEliahu%2520Horwitz%2520and%2520Bar%2520Cavia%2520and%2520Jonathan%2520Kahana%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3D%2520%2520The%2520increasing%2520availability%2520of%2520public%2520models%2520begs%2520the%2520question%253A%2520can%2520we%2520train%250Aneural%2520networks%2520that%2520use%2520other%2520networks%2520as%2520input%253F%2520This%2520paper%2520learns%2520to%250Arepresent%2520models%2520within%2520a%2520joint%2520space%2520that%2520embeds%2520both%2520model%2520weights%2520and%250Alanguage.%2520However%252C%2520machine%2520learning%2520on%2520model%2520weights%2520is%2520challenging%2520as%2520model%250Aweights%2520often%2520exhibit%2520significant%2520variation%2520unrelated%2520to%2520the%2520models%2527%2520semantic%250Aproperties%2520%2528nuisance%2520variation%2529.%2520We%2520identify%2520a%2520key%2520property%2520of%2520real-world%250Amodels%253A%2520most%2520public%2520models%2520belong%2520to%2520a%2520small%2520set%2520of%2520Model%2520Trees%252C%2520where%2520all%250Amodels%2520within%2520a%2520tree%2520are%2520fine-tuned%2520from%2520a%2520common%2520ancestor%2520%2528e.g.%252C%2520a%2520foundation%250Amodel%2529.%2520Importantly%252C%2520we%2520find%2520that%2520within%2520each%2520tree%2520there%2520is%2520less%2520nuisance%250Avariation%2520between%2520models.%2520For%2520example%252C%2520while%2520classifying%2520models%2520according%2520to%250Atheir%2520training%2520dataset%2520generally%2520requires%2520complex%2520architectures%252C%2520in%2520our%2520case%252C%250Aeven%2520a%2520linear%2520classifier%2520trained%2520on%2520a%2520single%2520layer%2520is%2520often%2520effective.%2520While%250Aeffective%252C%2520linear%2520layers%2520are%2520computationally%2520expensive%2520as%2520model%2520weights%2520are%250Avery%2520high%2520dimensional.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Probing%2520Experts%2520%2528ProbeX%2529%252C%250Aa%2520theoretically%2520motivated%252C%2520lightweight%2520probing%2520method.%2520Notably%252C%2520ProbeX%2520is%2520the%250Afirst%2520probing%2520method%2520designed%2520to%2520learn%2520from%2520the%2520weights%2520of%2520just%2520a%2520single%2520model%250Alayer.%2520We%2520also%2520construct%2520and%2520release%2520a%2520dataset%2520that%2520simulates%2520the%2520structure%2520of%250Apublic%2520model%2520repositories.%2520Our%2520results%2520show%2520that%2520ProbeX%2520can%2520effectively%2520map%2520the%250Aweights%2520of%2520large%2520models%2520into%2520a%2520shared%2520weight-language%2520embedding%2520space.%250AFurthermore%252C%2520we%2520demonstrate%2520the%2520impressive%2520generalization%2520of%2520our%2520method%252C%250Aachieving%2520zero-shot%2520model%2520classification%2520and%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20Model%20Weights%20with%20Language%20using%20Tree%20Experts&entry.906535625=Eliahu%20Horwitz%20and%20Bar%20Cavia%20and%20Jonathan%20Kahana%20and%20Yedid%20Hoshen&entry.1292438233=%20%20The%20increasing%20availability%20of%20public%20models%20begs%20the%20question%3A%20can%20we%20train%0Aneural%20networks%20that%20use%20other%20networks%20as%20input%3F%20This%20paper%20learns%20to%0Arepresent%20models%20within%20a%20joint%20space%20that%20embeds%20both%20model%20weights%20and%0Alanguage.%20However%2C%20machine%20learning%20on%20model%20weights%20is%20challenging%20as%20model%0Aweights%20often%20exhibit%20significant%20variation%20unrelated%20to%20the%20models%27%20semantic%0Aproperties%20%28nuisance%20variation%29.%20We%20identify%20a%20key%20property%20of%20real-world%0Amodels%3A%20most%20public%20models%20belong%20to%20a%20small%20set%20of%20Model%20Trees%2C%20where%20all%0Amodels%20within%20a%20tree%20are%20fine-tuned%20from%20a%20common%20ancestor%20%28e.g.%2C%20a%20foundation%0Amodel%29.%20Importantly%2C%20we%20find%20that%20within%20each%20tree%20there%20is%20less%20nuisance%0Avariation%20between%20models.%20For%20example%2C%20while%20classifying%20models%20according%20to%0Atheir%20training%20dataset%20generally%20requires%20complex%20architectures%2C%20in%20our%20case%2C%0Aeven%20a%20linear%20classifier%20trained%20on%20a%20single%20layer%20is%20often%20effective.%20While%0Aeffective%2C%20linear%20layers%20are%20computationally%20expensive%20as%20model%20weights%20are%0Avery%20high%20dimensional.%20To%20address%20this%2C%20we%20introduce%20Probing%20Experts%20%28ProbeX%29%2C%0Aa%20theoretically%20motivated%2C%20lightweight%20probing%20method.%20Notably%2C%20ProbeX%20is%20the%0Afirst%20probing%20method%20designed%20to%20learn%20from%20the%20weights%20of%20just%20a%20single%20model%0Alayer.%20We%20also%20construct%20and%20release%20a%20dataset%20that%20simulates%20the%20structure%20of%0Apublic%20model%20repositories.%20Our%20results%20show%20that%20ProbeX%20can%20effectively%20map%20the%0Aweights%20of%20large%20models%20into%20a%20shared%20weight-language%20embedding%20space.%0AFurthermore%2C%20we%20demonstrate%20the%20impressive%20generalization%20of%20our%20method%2C%0Aachieving%20zero-shot%20model%20classification%20and%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13569v1&entry.124074799=Read"},
{"title": "Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on\n  Segment Anything", "author": "Joonhyeon Song and Seohwan Yun and Seongho Yoon and Joohyeok Kim and Sangmin Lee", "abstract": "  This work proposes a novel approach beyond supervised learning for effective\npathological image analysis, addressing the challenge of limited robust labeled\ndata. Pathological diagnosis of diseases like cancer has conventionally relied\non the evaluation of morphological features by physicians and pathologists.\nHowever, recent advancements in compute-aided diagnosis (CAD) systems are\ngaining significant attention as diagnostic support tools. Although the\nadvancement of deep learning has improved CAD significantly, segmentation\nmodels typically require large pixel-level annotated dataset, and such labeling\nis expensive. Existing studies not based on supervised approaches still\nstruggle with limited generalization, and no practical approach has emerged\nyet. To address this issue, we present a weakly supervised semantic\nsegmentation (WSSS) model by combining class activation map and Segment\nAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt\nthe SAM-a foundation model that is pretrained on large datasets and operates in\nzero-shot configurations using only coarse prompts. The proposed approach\ntransfer enhanced Attention Dropout Layer's knowledge to SAM, thereby\ngenerating pseudo-labels. To demonstrate the superiority of the proposed\nmethod, experimental studies are conducted on histopathological breast cancer\ndatasets. The proposed method outperformed other WSSS methods across three\ndatasets, demonstrating its efficiency by achieving this with only 12GB of GPU\nmemory during training. Our code is available at :\nhttps://github.com/QI-NemoSong/EPLC-SAM\n", "link": "http://arxiv.org/abs/2410.13621v1", "date": "2024-10-17", "relevancy": 2.1325, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Prompt-leveraged%20Weakly%20Supervised%20Cancer%20Segmentation%20based%20on%0A%20%20Segment%20Anything&body=Title%3A%20Enhanced%20Prompt-leveraged%20Weakly%20Supervised%20Cancer%20Segmentation%20based%20on%0A%20%20Segment%20Anything%0AAuthor%3A%20Joonhyeon%20Song%20and%20Seohwan%20Yun%20and%20Seongho%20Yoon%20and%20Joohyeok%20Kim%20and%20Sangmin%20Lee%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20novel%20approach%20beyond%20supervised%20learning%20for%20effective%0Apathological%20image%20analysis%2C%20addressing%20the%20challenge%20of%20limited%20robust%20labeled%0Adata.%20Pathological%20diagnosis%20of%20diseases%20like%20cancer%20has%20conventionally%20relied%0Aon%20the%20evaluation%20of%20morphological%20features%20by%20physicians%20and%20pathologists.%0AHowever%2C%20recent%20advancements%20in%20compute-aided%20diagnosis%20%28CAD%29%20systems%20are%0Againing%20significant%20attention%20as%20diagnostic%20support%20tools.%20Although%20the%0Aadvancement%20of%20deep%20learning%20has%20improved%20CAD%20significantly%2C%20segmentation%0Amodels%20typically%20require%20large%20pixel-level%20annotated%20dataset%2C%20and%20such%20labeling%0Ais%20expensive.%20Existing%20studies%20not%20based%20on%20supervised%20approaches%20still%0Astruggle%20with%20limited%20generalization%2C%20and%20no%20practical%20approach%20has%20emerged%0Ayet.%20To%20address%20this%20issue%2C%20we%20present%20a%20weakly%20supervised%20semantic%0Asegmentation%20%28WSSS%29%20model%20by%20combining%20class%20activation%20map%20and%20Segment%0AAnything%20Model%20%28SAM%29-based%20pseudo-labeling.%20For%20effective%20pretraining%2C%20we%20adopt%0Athe%20SAM-a%20foundation%20model%20that%20is%20pretrained%20on%20large%20datasets%20and%20operates%20in%0Azero-shot%20configurations%20using%20only%20coarse%20prompts.%20The%20proposed%20approach%0Atransfer%20enhanced%20Attention%20Dropout%20Layer%27s%20knowledge%20to%20SAM%2C%20thereby%0Agenerating%20pseudo-labels.%20To%20demonstrate%20the%20superiority%20of%20the%20proposed%0Amethod%2C%20experimental%20studies%20are%20conducted%20on%20histopathological%20breast%20cancer%0Adatasets.%20The%20proposed%20method%20outperformed%20other%20WSSS%20methods%20across%20three%0Adatasets%2C%20demonstrating%20its%20efficiency%20by%20achieving%20this%20with%20only%2012GB%20of%20GPU%0Amemory%20during%20training.%20Our%20code%20is%20available%20at%20%3A%0Ahttps%3A//github.com/QI-NemoSong/EPLC-SAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Prompt-leveraged%2520Weakly%2520Supervised%2520Cancer%2520Segmentation%2520based%2520on%250A%2520%2520Segment%2520Anything%26entry.906535625%3DJoonhyeon%2520Song%2520and%2520Seohwan%2520Yun%2520and%2520Seongho%2520Yoon%2520and%2520Joohyeok%2520Kim%2520and%2520Sangmin%2520Lee%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520beyond%2520supervised%2520learning%2520for%2520effective%250Apathological%2520image%2520analysis%252C%2520addressing%2520the%2520challenge%2520of%2520limited%2520robust%2520labeled%250Adata.%2520Pathological%2520diagnosis%2520of%2520diseases%2520like%2520cancer%2520has%2520conventionally%2520relied%250Aon%2520the%2520evaluation%2520of%2520morphological%2520features%2520by%2520physicians%2520and%2520pathologists.%250AHowever%252C%2520recent%2520advancements%2520in%2520compute-aided%2520diagnosis%2520%2528CAD%2529%2520systems%2520are%250Againing%2520significant%2520attention%2520as%2520diagnostic%2520support%2520tools.%2520Although%2520the%250Aadvancement%2520of%2520deep%2520learning%2520has%2520improved%2520CAD%2520significantly%252C%2520segmentation%250Amodels%2520typically%2520require%2520large%2520pixel-level%2520annotated%2520dataset%252C%2520and%2520such%2520labeling%250Ais%2520expensive.%2520Existing%2520studies%2520not%2520based%2520on%2520supervised%2520approaches%2520still%250Astruggle%2520with%2520limited%2520generalization%252C%2520and%2520no%2520practical%2520approach%2520has%2520emerged%250Ayet.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520a%2520weakly%2520supervised%2520semantic%250Asegmentation%2520%2528WSSS%2529%2520model%2520by%2520combining%2520class%2520activation%2520map%2520and%2520Segment%250AAnything%2520Model%2520%2528SAM%2529-based%2520pseudo-labeling.%2520For%2520effective%2520pretraining%252C%2520we%2520adopt%250Athe%2520SAM-a%2520foundation%2520model%2520that%2520is%2520pretrained%2520on%2520large%2520datasets%2520and%2520operates%2520in%250Azero-shot%2520configurations%2520using%2520only%2520coarse%2520prompts.%2520The%2520proposed%2520approach%250Atransfer%2520enhanced%2520Attention%2520Dropout%2520Layer%2527s%2520knowledge%2520to%2520SAM%252C%2520thereby%250Agenerating%2520pseudo-labels.%2520To%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%250Amethod%252C%2520experimental%2520studies%2520are%2520conducted%2520on%2520histopathological%2520breast%2520cancer%250Adatasets.%2520The%2520proposed%2520method%2520outperformed%2520other%2520WSSS%2520methods%2520across%2520three%250Adatasets%252C%2520demonstrating%2520its%2520efficiency%2520by%2520achieving%2520this%2520with%2520only%252012GB%2520of%2520GPU%250Amemory%2520during%2520training.%2520Our%2520code%2520is%2520available%2520at%2520%253A%250Ahttps%253A//github.com/QI-NemoSong/EPLC-SAM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Prompt-leveraged%20Weakly%20Supervised%20Cancer%20Segmentation%20based%20on%0A%20%20Segment%20Anything&entry.906535625=Joonhyeon%20Song%20and%20Seohwan%20Yun%20and%20Seongho%20Yoon%20and%20Joohyeok%20Kim%20and%20Sangmin%20Lee&entry.1292438233=%20%20This%20work%20proposes%20a%20novel%20approach%20beyond%20supervised%20learning%20for%20effective%0Apathological%20image%20analysis%2C%20addressing%20the%20challenge%20of%20limited%20robust%20labeled%0Adata.%20Pathological%20diagnosis%20of%20diseases%20like%20cancer%20has%20conventionally%20relied%0Aon%20the%20evaluation%20of%20morphological%20features%20by%20physicians%20and%20pathologists.%0AHowever%2C%20recent%20advancements%20in%20compute-aided%20diagnosis%20%28CAD%29%20systems%20are%0Againing%20significant%20attention%20as%20diagnostic%20support%20tools.%20Although%20the%0Aadvancement%20of%20deep%20learning%20has%20improved%20CAD%20significantly%2C%20segmentation%0Amodels%20typically%20require%20large%20pixel-level%20annotated%20dataset%2C%20and%20such%20labeling%0Ais%20expensive.%20Existing%20studies%20not%20based%20on%20supervised%20approaches%20still%0Astruggle%20with%20limited%20generalization%2C%20and%20no%20practical%20approach%20has%20emerged%0Ayet.%20To%20address%20this%20issue%2C%20we%20present%20a%20weakly%20supervised%20semantic%0Asegmentation%20%28WSSS%29%20model%20by%20combining%20class%20activation%20map%20and%20Segment%0AAnything%20Model%20%28SAM%29-based%20pseudo-labeling.%20For%20effective%20pretraining%2C%20we%20adopt%0Athe%20SAM-a%20foundation%20model%20that%20is%20pretrained%20on%20large%20datasets%20and%20operates%20in%0Azero-shot%20configurations%20using%20only%20coarse%20prompts.%20The%20proposed%20approach%0Atransfer%20enhanced%20Attention%20Dropout%20Layer%27s%20knowledge%20to%20SAM%2C%20thereby%0Agenerating%20pseudo-labels.%20To%20demonstrate%20the%20superiority%20of%20the%20proposed%0Amethod%2C%20experimental%20studies%20are%20conducted%20on%20histopathological%20breast%20cancer%0Adatasets.%20The%20proposed%20method%20outperformed%20other%20WSSS%20methods%20across%20three%0Adatasets%2C%20demonstrating%20its%20efficiency%20by%20achieving%20this%20with%20only%2012GB%20of%20GPU%0Amemory%20during%20training.%20Our%20code%20is%20available%20at%20%3A%0Ahttps%3A//github.com/QI-NemoSong/EPLC-SAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13621v1&entry.124074799=Read"},
{"title": "ActionCOMET: A Zero-shot Approach to Learn Image-specific Commonsense\n  Concepts about Actions", "author": "Shailaja Keyur Sampat and Yezhou Yang and Chitta Baral", "abstract": "  Humans observe various actions being performed by other humans (physically or\nin videos/images) and can draw a wide range of inferences about it beyond what\nthey can visually perceive. Such inferences include determining the aspects of\nthe world that make action execution possible (e.g. liquid objects can undergo\npouring), predicting how the world will change as a result of the action (e.g.\npotatoes being golden and crispy after frying), high-level goals associated\nwith the action (e.g. beat the eggs to make an omelet) and reasoning about\nactions that possibly precede or follow the current action (e.g. crack eggs\nbefore whisking or draining pasta after boiling). Similar reasoning ability is\nhighly desirable in autonomous systems that would assist us in performing\neveryday tasks. To that end, we propose a multi-modal task to learn\naforementioned concepts about actions being performed in images. We develop a\ndataset consisting of 8.5k images and 59.3k inferences about actions grounded\nin those images, collected from an annotated cooking-video dataset. We propose\nActionCOMET, a zero-shot framework to discern knowledge present in language\nmodels specific to the provided visual input. We present baseline results of\nActionCOMET over the collected dataset and compare them with the performance of\nthe best existing VQA approaches.\n", "link": "http://arxiv.org/abs/2410.13662v1", "date": "2024-10-17", "relevancy": 2.128, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5313}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActionCOMET%3A%20A%20Zero-shot%20Approach%20to%20Learn%20Image-specific%20Commonsense%0A%20%20Concepts%20about%20Actions&body=Title%3A%20ActionCOMET%3A%20A%20Zero-shot%20Approach%20to%20Learn%20Image-specific%20Commonsense%0A%20%20Concepts%20about%20Actions%0AAuthor%3A%20Shailaja%20Keyur%20Sampat%20and%20Yezhou%20Yang%20and%20Chitta%20Baral%0AAbstract%3A%20%20%20Humans%20observe%20various%20actions%20being%20performed%20by%20other%20humans%20%28physically%20or%0Ain%20videos/images%29%20and%20can%20draw%20a%20wide%20range%20of%20inferences%20about%20it%20beyond%20what%0Athey%20can%20visually%20perceive.%20Such%20inferences%20include%20determining%20the%20aspects%20of%0Athe%20world%20that%20make%20action%20execution%20possible%20%28e.g.%20liquid%20objects%20can%20undergo%0Apouring%29%2C%20predicting%20how%20the%20world%20will%20change%20as%20a%20result%20of%20the%20action%20%28e.g.%0Apotatoes%20being%20golden%20and%20crispy%20after%20frying%29%2C%20high-level%20goals%20associated%0Awith%20the%20action%20%28e.g.%20beat%20the%20eggs%20to%20make%20an%20omelet%29%20and%20reasoning%20about%0Aactions%20that%20possibly%20precede%20or%20follow%20the%20current%20action%20%28e.g.%20crack%20eggs%0Abefore%20whisking%20or%20draining%20pasta%20after%20boiling%29.%20Similar%20reasoning%20ability%20is%0Ahighly%20desirable%20in%20autonomous%20systems%20that%20would%20assist%20us%20in%20performing%0Aeveryday%20tasks.%20To%20that%20end%2C%20we%20propose%20a%20multi-modal%20task%20to%20learn%0Aaforementioned%20concepts%20about%20actions%20being%20performed%20in%20images.%20We%20develop%20a%0Adataset%20consisting%20of%208.5k%20images%20and%2059.3k%20inferences%20about%20actions%20grounded%0Ain%20those%20images%2C%20collected%20from%20an%20annotated%20cooking-video%20dataset.%20We%20propose%0AActionCOMET%2C%20a%20zero-shot%20framework%20to%20discern%20knowledge%20present%20in%20language%0Amodels%20specific%20to%20the%20provided%20visual%20input.%20We%20present%20baseline%20results%20of%0AActionCOMET%20over%20the%20collected%20dataset%20and%20compare%20them%20with%20the%20performance%20of%0Athe%20best%20existing%20VQA%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActionCOMET%253A%2520A%2520Zero-shot%2520Approach%2520to%2520Learn%2520Image-specific%2520Commonsense%250A%2520%2520Concepts%2520about%2520Actions%26entry.906535625%3DShailaja%2520Keyur%2520Sampat%2520and%2520Yezhou%2520Yang%2520and%2520Chitta%2520Baral%26entry.1292438233%3D%2520%2520Humans%2520observe%2520various%2520actions%2520being%2520performed%2520by%2520other%2520humans%2520%2528physically%2520or%250Ain%2520videos/images%2529%2520and%2520can%2520draw%2520a%2520wide%2520range%2520of%2520inferences%2520about%2520it%2520beyond%2520what%250Athey%2520can%2520visually%2520perceive.%2520Such%2520inferences%2520include%2520determining%2520the%2520aspects%2520of%250Athe%2520world%2520that%2520make%2520action%2520execution%2520possible%2520%2528e.g.%2520liquid%2520objects%2520can%2520undergo%250Apouring%2529%252C%2520predicting%2520how%2520the%2520world%2520will%2520change%2520as%2520a%2520result%2520of%2520the%2520action%2520%2528e.g.%250Apotatoes%2520being%2520golden%2520and%2520crispy%2520after%2520frying%2529%252C%2520high-level%2520goals%2520associated%250Awith%2520the%2520action%2520%2528e.g.%2520beat%2520the%2520eggs%2520to%2520make%2520an%2520omelet%2529%2520and%2520reasoning%2520about%250Aactions%2520that%2520possibly%2520precede%2520or%2520follow%2520the%2520current%2520action%2520%2528e.g.%2520crack%2520eggs%250Abefore%2520whisking%2520or%2520draining%2520pasta%2520after%2520boiling%2529.%2520Similar%2520reasoning%2520ability%2520is%250Ahighly%2520desirable%2520in%2520autonomous%2520systems%2520that%2520would%2520assist%2520us%2520in%2520performing%250Aeveryday%2520tasks.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%2520multi-modal%2520task%2520to%2520learn%250Aaforementioned%2520concepts%2520about%2520actions%2520being%2520performed%2520in%2520images.%2520We%2520develop%2520a%250Adataset%2520consisting%2520of%25208.5k%2520images%2520and%252059.3k%2520inferences%2520about%2520actions%2520grounded%250Ain%2520those%2520images%252C%2520collected%2520from%2520an%2520annotated%2520cooking-video%2520dataset.%2520We%2520propose%250AActionCOMET%252C%2520a%2520zero-shot%2520framework%2520to%2520discern%2520knowledge%2520present%2520in%2520language%250Amodels%2520specific%2520to%2520the%2520provided%2520visual%2520input.%2520We%2520present%2520baseline%2520results%2520of%250AActionCOMET%2520over%2520the%2520collected%2520dataset%2520and%2520compare%2520them%2520with%2520the%2520performance%2520of%250Athe%2520best%2520existing%2520VQA%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActionCOMET%3A%20A%20Zero-shot%20Approach%20to%20Learn%20Image-specific%20Commonsense%0A%20%20Concepts%20about%20Actions&entry.906535625=Shailaja%20Keyur%20Sampat%20and%20Yezhou%20Yang%20and%20Chitta%20Baral&entry.1292438233=%20%20Humans%20observe%20various%20actions%20being%20performed%20by%20other%20humans%20%28physically%20or%0Ain%20videos/images%29%20and%20can%20draw%20a%20wide%20range%20of%20inferences%20about%20it%20beyond%20what%0Athey%20can%20visually%20perceive.%20Such%20inferences%20include%20determining%20the%20aspects%20of%0Athe%20world%20that%20make%20action%20execution%20possible%20%28e.g.%20liquid%20objects%20can%20undergo%0Apouring%29%2C%20predicting%20how%20the%20world%20will%20change%20as%20a%20result%20of%20the%20action%20%28e.g.%0Apotatoes%20being%20golden%20and%20crispy%20after%20frying%29%2C%20high-level%20goals%20associated%0Awith%20the%20action%20%28e.g.%20beat%20the%20eggs%20to%20make%20an%20omelet%29%20and%20reasoning%20about%0Aactions%20that%20possibly%20precede%20or%20follow%20the%20current%20action%20%28e.g.%20crack%20eggs%0Abefore%20whisking%20or%20draining%20pasta%20after%20boiling%29.%20Similar%20reasoning%20ability%20is%0Ahighly%20desirable%20in%20autonomous%20systems%20that%20would%20assist%20us%20in%20performing%0Aeveryday%20tasks.%20To%20that%20end%2C%20we%20propose%20a%20multi-modal%20task%20to%20learn%0Aaforementioned%20concepts%20about%20actions%20being%20performed%20in%20images.%20We%20develop%20a%0Adataset%20consisting%20of%208.5k%20images%20and%2059.3k%20inferences%20about%20actions%20grounded%0Ain%20those%20images%2C%20collected%20from%20an%20annotated%20cooking-video%20dataset.%20We%20propose%0AActionCOMET%2C%20a%20zero-shot%20framework%20to%20discern%20knowledge%20present%20in%20language%0Amodels%20specific%20to%20the%20provided%20visual%20input.%20We%20present%20baseline%20results%20of%0AActionCOMET%20over%20the%20collected%20dataset%20and%20compare%20them%20with%20the%20performance%20of%0Athe%20best%20existing%20VQA%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13662v1&entry.124074799=Read"},
{"title": "Aggregation Artifacts in Subjective Tasks Collapse Large Language\n  Models' Posteriors", "author": "Georgios Chochlakis and Alexandros Potamianos and Kristina Lerman and Shrikanth Narayanan", "abstract": "  In-context Learning (ICL) has become the primary method for performing\nnatural language tasks with Large Language Models (LLMs). The knowledge\nacquired during pre-training is crucial for this few-shot capability, providing\nthe model with task priors. However, recent studies have shown that ICL\npredominantly relies on retrieving task priors rather than \"learning\" to\nperform tasks. This limitation is particularly evident in complex subjective\ndomains such as emotion and morality, where priors significantly influence\nposterior predictions. In this work, we examine whether this is the result of\nthe aggregation used in corresponding datasets, where trying to combine\nlow-agreement, disparate annotations might lead to annotation artifacts that\ncreate detrimental noise in the prompt. Moreover, we evaluate the posterior\nbias towards certain annotators by grounding our study in appropriate,\nquantitative measures of LLM priors. Our results indicate that aggregation is a\nconfounding factor in the modeling of subjective tasks, and advocate focusing\non modeling individuals instead. However, aggregation does not explain the\nentire gap between ICL and the state of the art, meaning other factors in such\ntasks also account for the observed phenomena. Finally, by rigorously studying\nannotator-level labels, we find that it is possible for minority annotators to\nboth better align with LLMs and have their perspectives further amplified.\n", "link": "http://arxiv.org/abs/2410.13776v1", "date": "2024-10-17", "relevancy": 2.127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aggregation%20Artifacts%20in%20Subjective%20Tasks%20Collapse%20Large%20Language%0A%20%20Models%27%20Posteriors&body=Title%3A%20Aggregation%20Artifacts%20in%20Subjective%20Tasks%20Collapse%20Large%20Language%0A%20%20Models%27%20Posteriors%0AAuthor%3A%20Georgios%20Chochlakis%20and%20Alexandros%20Potamianos%20and%20Kristina%20Lerman%20and%20Shrikanth%20Narayanan%0AAbstract%3A%20%20%20In-context%20Learning%20%28ICL%29%20has%20become%20the%20primary%20method%20for%20performing%0Anatural%20language%20tasks%20with%20Large%20Language%20Models%20%28LLMs%29.%20The%20knowledge%0Aacquired%20during%20pre-training%20is%20crucial%20for%20this%20few-shot%20capability%2C%20providing%0Athe%20model%20with%20task%20priors.%20However%2C%20recent%20studies%20have%20shown%20that%20ICL%0Apredominantly%20relies%20on%20retrieving%20task%20priors%20rather%20than%20%22learning%22%20to%0Aperform%20tasks.%20This%20limitation%20is%20particularly%20evident%20in%20complex%20subjective%0Adomains%20such%20as%20emotion%20and%20morality%2C%20where%20priors%20significantly%20influence%0Aposterior%20predictions.%20In%20this%20work%2C%20we%20examine%20whether%20this%20is%20the%20result%20of%0Athe%20aggregation%20used%20in%20corresponding%20datasets%2C%20where%20trying%20to%20combine%0Alow-agreement%2C%20disparate%20annotations%20might%20lead%20to%20annotation%20artifacts%20that%0Acreate%20detrimental%20noise%20in%20the%20prompt.%20Moreover%2C%20we%20evaluate%20the%20posterior%0Abias%20towards%20certain%20annotators%20by%20grounding%20our%20study%20in%20appropriate%2C%0Aquantitative%20measures%20of%20LLM%20priors.%20Our%20results%20indicate%20that%20aggregation%20is%20a%0Aconfounding%20factor%20in%20the%20modeling%20of%20subjective%20tasks%2C%20and%20advocate%20focusing%0Aon%20modeling%20individuals%20instead.%20However%2C%20aggregation%20does%20not%20explain%20the%0Aentire%20gap%20between%20ICL%20and%20the%20state%20of%20the%20art%2C%20meaning%20other%20factors%20in%20such%0Atasks%20also%20account%20for%20the%20observed%20phenomena.%20Finally%2C%20by%20rigorously%20studying%0Aannotator-level%20labels%2C%20we%20find%20that%20it%20is%20possible%20for%20minority%20annotators%20to%0Aboth%20better%20align%20with%20LLMs%20and%20have%20their%20perspectives%20further%20amplified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAggregation%2520Artifacts%2520in%2520Subjective%2520Tasks%2520Collapse%2520Large%2520Language%250A%2520%2520Models%2527%2520Posteriors%26entry.906535625%3DGeorgios%2520Chochlakis%2520and%2520Alexandros%2520Potamianos%2520and%2520Kristina%2520Lerman%2520and%2520Shrikanth%2520Narayanan%26entry.1292438233%3D%2520%2520In-context%2520Learning%2520%2528ICL%2529%2520has%2520become%2520the%2520primary%2520method%2520for%2520performing%250Anatural%2520language%2520tasks%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520knowledge%250Aacquired%2520during%2520pre-training%2520is%2520crucial%2520for%2520this%2520few-shot%2520capability%252C%2520providing%250Athe%2520model%2520with%2520task%2520priors.%2520However%252C%2520recent%2520studies%2520have%2520shown%2520that%2520ICL%250Apredominantly%2520relies%2520on%2520retrieving%2520task%2520priors%2520rather%2520than%2520%2522learning%2522%2520to%250Aperform%2520tasks.%2520This%2520limitation%2520is%2520particularly%2520evident%2520in%2520complex%2520subjective%250Adomains%2520such%2520as%2520emotion%2520and%2520morality%252C%2520where%2520priors%2520significantly%2520influence%250Aposterior%2520predictions.%2520In%2520this%2520work%252C%2520we%2520examine%2520whether%2520this%2520is%2520the%2520result%2520of%250Athe%2520aggregation%2520used%2520in%2520corresponding%2520datasets%252C%2520where%2520trying%2520to%2520combine%250Alow-agreement%252C%2520disparate%2520annotations%2520might%2520lead%2520to%2520annotation%2520artifacts%2520that%250Acreate%2520detrimental%2520noise%2520in%2520the%2520prompt.%2520Moreover%252C%2520we%2520evaluate%2520the%2520posterior%250Abias%2520towards%2520certain%2520annotators%2520by%2520grounding%2520our%2520study%2520in%2520appropriate%252C%250Aquantitative%2520measures%2520of%2520LLM%2520priors.%2520Our%2520results%2520indicate%2520that%2520aggregation%2520is%2520a%250Aconfounding%2520factor%2520in%2520the%2520modeling%2520of%2520subjective%2520tasks%252C%2520and%2520advocate%2520focusing%250Aon%2520modeling%2520individuals%2520instead.%2520However%252C%2520aggregation%2520does%2520not%2520explain%2520the%250Aentire%2520gap%2520between%2520ICL%2520and%2520the%2520state%2520of%2520the%2520art%252C%2520meaning%2520other%2520factors%2520in%2520such%250Atasks%2520also%2520account%2520for%2520the%2520observed%2520phenomena.%2520Finally%252C%2520by%2520rigorously%2520studying%250Aannotator-level%2520labels%252C%2520we%2520find%2520that%2520it%2520is%2520possible%2520for%2520minority%2520annotators%2520to%250Aboth%2520better%2520align%2520with%2520LLMs%2520and%2520have%2520their%2520perspectives%2520further%2520amplified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aggregation%20Artifacts%20in%20Subjective%20Tasks%20Collapse%20Large%20Language%0A%20%20Models%27%20Posteriors&entry.906535625=Georgios%20Chochlakis%20and%20Alexandros%20Potamianos%20and%20Kristina%20Lerman%20and%20Shrikanth%20Narayanan&entry.1292438233=%20%20In-context%20Learning%20%28ICL%29%20has%20become%20the%20primary%20method%20for%20performing%0Anatural%20language%20tasks%20with%20Large%20Language%20Models%20%28LLMs%29.%20The%20knowledge%0Aacquired%20during%20pre-training%20is%20crucial%20for%20this%20few-shot%20capability%2C%20providing%0Athe%20model%20with%20task%20priors.%20However%2C%20recent%20studies%20have%20shown%20that%20ICL%0Apredominantly%20relies%20on%20retrieving%20task%20priors%20rather%20than%20%22learning%22%20to%0Aperform%20tasks.%20This%20limitation%20is%20particularly%20evident%20in%20complex%20subjective%0Adomains%20such%20as%20emotion%20and%20morality%2C%20where%20priors%20significantly%20influence%0Aposterior%20predictions.%20In%20this%20work%2C%20we%20examine%20whether%20this%20is%20the%20result%20of%0Athe%20aggregation%20used%20in%20corresponding%20datasets%2C%20where%20trying%20to%20combine%0Alow-agreement%2C%20disparate%20annotations%20might%20lead%20to%20annotation%20artifacts%20that%0Acreate%20detrimental%20noise%20in%20the%20prompt.%20Moreover%2C%20we%20evaluate%20the%20posterior%0Abias%20towards%20certain%20annotators%20by%20grounding%20our%20study%20in%20appropriate%2C%0Aquantitative%20measures%20of%20LLM%20priors.%20Our%20results%20indicate%20that%20aggregation%20is%20a%0Aconfounding%20factor%20in%20the%20modeling%20of%20subjective%20tasks%2C%20and%20advocate%20focusing%0Aon%20modeling%20individuals%20instead.%20However%2C%20aggregation%20does%20not%20explain%20the%0Aentire%20gap%20between%20ICL%20and%20the%20state%20of%20the%20art%2C%20meaning%20other%20factors%20in%20such%0Atasks%20also%20account%20for%20the%20observed%20phenomena.%20Finally%2C%20by%20rigorously%20studying%0Aannotator-level%20labels%2C%20we%20find%20that%20it%20is%20possible%20for%20minority%20annotators%20to%0Aboth%20better%20align%20with%20LLMs%20and%20have%20their%20perspectives%20further%20amplified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13776v1&entry.124074799=Read"},
{"title": "A Unified View of Delta Parameter Editing in Post-Trained Large-Scale\n  Models", "author": "Qiaoyu Tang and Le Yu and Bowen Yu and Hongyu Lin and Keming Lu and Yaojie Lu and Xianpei Han and Le Sun", "abstract": "  Post-training has emerged as a crucial paradigm for adapting large-scale\npre-trained models to various tasks, whose effects are fully reflected by delta\nparameters (i.e., the disparity between post-trained and pre-trained\nparameters). While numerous studies have explored delta parameter properties\nvia operations like pruning, quantization, low-rank approximation, and\nextrapolation, a unified framework for systematically examining these\ncharacteristics has been lacking. In this paper, we propose a novel perspective\nbased on Riemann sum approximation of the loss function to elucidate delta\nparameter editing operations. Our analysis categorizes existing methods into\nthree classes based on their post-editing performance: competitive, decreased,\nand improved, explaining how they are expressed by the Riemann sum\napproximation term and how they alter the model performance. Extensive\nexperiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,\nand Mistral, corroborate our theoretical findings. Furthermore, we introduce\nextensions to existing techniques like DARE and BitDelta, highlighting their\nlimitations in leveraging the properties of delta parameters and reorganizing\nthem into general expressions to enhance the applicability and effectiveness of\ndelta parameter editing in post-trained models.\n", "link": "http://arxiv.org/abs/2410.13841v1", "date": "2024-10-17", "relevancy": 2.1209, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20View%20of%20Delta%20Parameter%20Editing%20in%20Post-Trained%20Large-Scale%0A%20%20Models&body=Title%3A%20A%20Unified%20View%20of%20Delta%20Parameter%20Editing%20in%20Post-Trained%20Large-Scale%0A%20%20Models%0AAuthor%3A%20Qiaoyu%20Tang%20and%20Le%20Yu%20and%20Bowen%20Yu%20and%20Hongyu%20Lin%20and%20Keming%20Lu%20and%20Yaojie%20Lu%20and%20Xianpei%20Han%20and%20Le%20Sun%0AAbstract%3A%20%20%20Post-training%20has%20emerged%20as%20a%20crucial%20paradigm%20for%20adapting%20large-scale%0Apre-trained%20models%20to%20various%20tasks%2C%20whose%20effects%20are%20fully%20reflected%20by%20delta%0Aparameters%20%28i.e.%2C%20the%20disparity%20between%20post-trained%20and%20pre-trained%0Aparameters%29.%20While%20numerous%20studies%20have%20explored%20delta%20parameter%20properties%0Avia%20operations%20like%20pruning%2C%20quantization%2C%20low-rank%20approximation%2C%20and%0Aextrapolation%2C%20a%20unified%20framework%20for%20systematically%20examining%20these%0Acharacteristics%20has%20been%20lacking.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20perspective%0Abased%20on%20Riemann%20sum%20approximation%20of%20the%20loss%20function%20to%20elucidate%20delta%0Aparameter%20editing%20operations.%20Our%20analysis%20categorizes%20existing%20methods%20into%0Athree%20classes%20based%20on%20their%20post-editing%20performance%3A%20competitive%2C%20decreased%2C%0Aand%20improved%2C%20explaining%20how%20they%20are%20expressed%20by%20the%20Riemann%20sum%0Aapproximation%20term%20and%20how%20they%20alter%20the%20model%20performance.%20Extensive%0Aexperiments%20on%20both%20visual%20and%20language%20models%2C%20including%20ViT%2C%20LLaMA%203%2C%20Qwen%202%2C%0Aand%20Mistral%2C%20corroborate%20our%20theoretical%20findings.%20Furthermore%2C%20we%20introduce%0Aextensions%20to%20existing%20techniques%20like%20DARE%20and%20BitDelta%2C%20highlighting%20their%0Alimitations%20in%20leveraging%20the%20properties%20of%20delta%20parameters%20and%20reorganizing%0Athem%20into%20general%20expressions%20to%20enhance%20the%20applicability%20and%20effectiveness%20of%0Adelta%20parameter%20editing%20in%20post-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520View%2520of%2520Delta%2520Parameter%2520Editing%2520in%2520Post-Trained%2520Large-Scale%250A%2520%2520Models%26entry.906535625%3DQiaoyu%2520Tang%2520and%2520Le%2520Yu%2520and%2520Bowen%2520Yu%2520and%2520Hongyu%2520Lin%2520and%2520Keming%2520Lu%2520and%2520Yaojie%2520Lu%2520and%2520Xianpei%2520Han%2520and%2520Le%2520Sun%26entry.1292438233%3D%2520%2520Post-training%2520has%2520emerged%2520as%2520a%2520crucial%2520paradigm%2520for%2520adapting%2520large-scale%250Apre-trained%2520models%2520to%2520various%2520tasks%252C%2520whose%2520effects%2520are%2520fully%2520reflected%2520by%2520delta%250Aparameters%2520%2528i.e.%252C%2520the%2520disparity%2520between%2520post-trained%2520and%2520pre-trained%250Aparameters%2529.%2520While%2520numerous%2520studies%2520have%2520explored%2520delta%2520parameter%2520properties%250Avia%2520operations%2520like%2520pruning%252C%2520quantization%252C%2520low-rank%2520approximation%252C%2520and%250Aextrapolation%252C%2520a%2520unified%2520framework%2520for%2520systematically%2520examining%2520these%250Acharacteristics%2520has%2520been%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520perspective%250Abased%2520on%2520Riemann%2520sum%2520approximation%2520of%2520the%2520loss%2520function%2520to%2520elucidate%2520delta%250Aparameter%2520editing%2520operations.%2520Our%2520analysis%2520categorizes%2520existing%2520methods%2520into%250Athree%2520classes%2520based%2520on%2520their%2520post-editing%2520performance%253A%2520competitive%252C%2520decreased%252C%250Aand%2520improved%252C%2520explaining%2520how%2520they%2520are%2520expressed%2520by%2520the%2520Riemann%2520sum%250Aapproximation%2520term%2520and%2520how%2520they%2520alter%2520the%2520model%2520performance.%2520Extensive%250Aexperiments%2520on%2520both%2520visual%2520and%2520language%2520models%252C%2520including%2520ViT%252C%2520LLaMA%25203%252C%2520Qwen%25202%252C%250Aand%2520Mistral%252C%2520corroborate%2520our%2520theoretical%2520findings.%2520Furthermore%252C%2520we%2520introduce%250Aextensions%2520to%2520existing%2520techniques%2520like%2520DARE%2520and%2520BitDelta%252C%2520highlighting%2520their%250Alimitations%2520in%2520leveraging%2520the%2520properties%2520of%2520delta%2520parameters%2520and%2520reorganizing%250Athem%2520into%2520general%2520expressions%2520to%2520enhance%2520the%2520applicability%2520and%2520effectiveness%2520of%250Adelta%2520parameter%2520editing%2520in%2520post-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20View%20of%20Delta%20Parameter%20Editing%20in%20Post-Trained%20Large-Scale%0A%20%20Models&entry.906535625=Qiaoyu%20Tang%20and%20Le%20Yu%20and%20Bowen%20Yu%20and%20Hongyu%20Lin%20and%20Keming%20Lu%20and%20Yaojie%20Lu%20and%20Xianpei%20Han%20and%20Le%20Sun&entry.1292438233=%20%20Post-training%20has%20emerged%20as%20a%20crucial%20paradigm%20for%20adapting%20large-scale%0Apre-trained%20models%20to%20various%20tasks%2C%20whose%20effects%20are%20fully%20reflected%20by%20delta%0Aparameters%20%28i.e.%2C%20the%20disparity%20between%20post-trained%20and%20pre-trained%0Aparameters%29.%20While%20numerous%20studies%20have%20explored%20delta%20parameter%20properties%0Avia%20operations%20like%20pruning%2C%20quantization%2C%20low-rank%20approximation%2C%20and%0Aextrapolation%2C%20a%20unified%20framework%20for%20systematically%20examining%20these%0Acharacteristics%20has%20been%20lacking.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20perspective%0Abased%20on%20Riemann%20sum%20approximation%20of%20the%20loss%20function%20to%20elucidate%20delta%0Aparameter%20editing%20operations.%20Our%20analysis%20categorizes%20existing%20methods%20into%0Athree%20classes%20based%20on%20their%20post-editing%20performance%3A%20competitive%2C%20decreased%2C%0Aand%20improved%2C%20explaining%20how%20they%20are%20expressed%20by%20the%20Riemann%20sum%0Aapproximation%20term%20and%20how%20they%20alter%20the%20model%20performance.%20Extensive%0Aexperiments%20on%20both%20visual%20and%20language%20models%2C%20including%20ViT%2C%20LLaMA%203%2C%20Qwen%202%2C%0Aand%20Mistral%2C%20corroborate%20our%20theoretical%20findings.%20Furthermore%2C%20we%20introduce%0Aextensions%20to%20existing%20techniques%20like%20DARE%20and%20BitDelta%2C%20highlighting%20their%0Alimitations%20in%20leveraging%20the%20properties%20of%20delta%20parameters%20and%20reorganizing%0Athem%20into%20general%20expressions%20to%20enhance%20the%20applicability%20and%20effectiveness%20of%0Adelta%20parameter%20editing%20in%20post-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13841v1&entry.124074799=Read"},
{"title": "LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for\n  Parameter-Efficient Fine-Tuning", "author": "Yiming Shi and Jiwei Wei and Yujia Wu and Ran Ran and Chengwei Sun and Shiyuan He and Yang Yang", "abstract": "  The rapid growth of model scale has necessitated substantial computational\nresources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA)\nhas sought to address the problem of handling the large updated parameters in\nfull fine-tuning. However, LoRA utilize random initialization and optimization\nof low-rank matrices to approximate updated weights, which can result in\nsuboptimal convergence and an accuracy gap compared to full fine-tuning. To\naddress these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning\n(PEFT) approach that significantly reduces trainable parameters by 2600 times\ncompared to regular PEFT methods while maintaining comparable performance.\nLoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank\nmatrices for faster convergence and orthogonality. We focus on optimizing the\ndiagonal matrix for scaling transformations. To the best of our knowledge,\nLoLDU has the fewest parameters among all PEFT approaches. We conducted\nextensive experiments across 4 instruction-following datasets, 6 natural\nlanguage understanding (NLU) datasets, 8 image classification datasets, and\nimage generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and\nStable Diffusion), providing a comprehensive and detailed analysis. Our\nopen-source code can be accessed at\n\\href{https://github.com/SKDDJ/LoLDU}{https://github.com/SKDDJ/LoLDU}.\n", "link": "http://arxiv.org/abs/2410.13618v1", "date": "2024-10-17", "relevancy": 2.1185, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5637}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5337}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoLDU%3A%20Low-Rank%20Adaptation%20via%20Lower-Diag-Upper%20Decomposition%20for%0A%20%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20LoLDU%3A%20Low-Rank%20Adaptation%20via%20Lower-Diag-Upper%20Decomposition%20for%0A%20%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Yiming%20Shi%20and%20Jiwei%20Wei%20and%20Yujia%20Wu%20and%20Ran%20Ran%20and%20Chengwei%20Sun%20and%20Shiyuan%20He%20and%20Yang%20Yang%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20model%20scale%20has%20necessitated%20substantial%20computational%0Aresources%20for%20fine-tuning.%20Existing%20approach%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%0Ahas%20sought%20to%20address%20the%20problem%20of%20handling%20the%20large%20updated%20parameters%20in%0Afull%20fine-tuning.%20However%2C%20LoRA%20utilize%20random%20initialization%20and%20optimization%0Aof%20low-rank%20matrices%20to%20approximate%20updated%20weights%2C%20which%20can%20result%20in%0Asuboptimal%20convergence%20and%20an%20accuracy%20gap%20compared%20to%20full%20fine-tuning.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20LoLDU%2C%20a%20Parameter-Efficient%20Fine-Tuning%0A%28PEFT%29%20approach%20that%20significantly%20reduces%20trainable%20parameters%20by%202600%20times%0Acompared%20to%20regular%20PEFT%20methods%20while%20maintaining%20comparable%20performance.%0ALoLDU%20leverages%20Lower-Diag-Upper%20Decomposition%20%28LDU%29%20to%20initialize%20low-rank%0Amatrices%20for%20faster%20convergence%20and%20orthogonality.%20We%20focus%20on%20optimizing%20the%0Adiagonal%20matrix%20for%20scaling%20transformations.%20To%20the%20best%20of%20our%20knowledge%2C%0ALoLDU%20has%20the%20fewest%20parameters%20among%20all%20PEFT%20approaches.%20We%20conducted%0Aextensive%20experiments%20across%204%20instruction-following%20datasets%2C%206%20natural%0Alanguage%20understanding%20%28NLU%29%20datasets%2C%208%20image%20classification%20datasets%2C%20and%0Aimage%20generation%20datasets%20with%20multiple%20model%20types%20%28LLaMA2%2C%20RoBERTa%2C%20ViT%2C%20and%0AStable%20Diffusion%29%2C%20providing%20a%20comprehensive%20and%20detailed%20analysis.%20Our%0Aopen-source%20code%20can%20be%20accessed%20at%0A%5Chref%7Bhttps%3A//github.com/SKDDJ/LoLDU%7D%7Bhttps%3A//github.com/SKDDJ/LoLDU%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoLDU%253A%2520Low-Rank%2520Adaptation%2520via%2520Lower-Diag-Upper%2520Decomposition%2520for%250A%2520%2520Parameter-Efficient%2520Fine-Tuning%26entry.906535625%3DYiming%2520Shi%2520and%2520Jiwei%2520Wei%2520and%2520Yujia%2520Wu%2520and%2520Ran%2520Ran%2520and%2520Chengwei%2520Sun%2520and%2520Shiyuan%2520He%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520model%2520scale%2520has%2520necessitated%2520substantial%2520computational%250Aresources%2520for%2520fine-tuning.%2520Existing%2520approach%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Ahas%2520sought%2520to%2520address%2520the%2520problem%2520of%2520handling%2520the%2520large%2520updated%2520parameters%2520in%250Afull%2520fine-tuning.%2520However%252C%2520LoRA%2520utilize%2520random%2520initialization%2520and%2520optimization%250Aof%2520low-rank%2520matrices%2520to%2520approximate%2520updated%2520weights%252C%2520which%2520can%2520result%2520in%250Asuboptimal%2520convergence%2520and%2520an%2520accuracy%2520gap%2520compared%2520to%2520full%2520fine-tuning.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520LoLDU%252C%2520a%2520Parameter-Efficient%2520Fine-Tuning%250A%2528PEFT%2529%2520approach%2520that%2520significantly%2520reduces%2520trainable%2520parameters%2520by%25202600%2520times%250Acompared%2520to%2520regular%2520PEFT%2520methods%2520while%2520maintaining%2520comparable%2520performance.%250ALoLDU%2520leverages%2520Lower-Diag-Upper%2520Decomposition%2520%2528LDU%2529%2520to%2520initialize%2520low-rank%250Amatrices%2520for%2520faster%2520convergence%2520and%2520orthogonality.%2520We%2520focus%2520on%2520optimizing%2520the%250Adiagonal%2520matrix%2520for%2520scaling%2520transformations.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250ALoLDU%2520has%2520the%2520fewest%2520parameters%2520among%2520all%2520PEFT%2520approaches.%2520We%2520conducted%250Aextensive%2520experiments%2520across%25204%2520instruction-following%2520datasets%252C%25206%2520natural%250Alanguage%2520understanding%2520%2528NLU%2529%2520datasets%252C%25208%2520image%2520classification%2520datasets%252C%2520and%250Aimage%2520generation%2520datasets%2520with%2520multiple%2520model%2520types%2520%2528LLaMA2%252C%2520RoBERTa%252C%2520ViT%252C%2520and%250AStable%2520Diffusion%2529%252C%2520providing%2520a%2520comprehensive%2520and%2520detailed%2520analysis.%2520Our%250Aopen-source%2520code%2520can%2520be%2520accessed%2520at%250A%255Chref%257Bhttps%253A//github.com/SKDDJ/LoLDU%257D%257Bhttps%253A//github.com/SKDDJ/LoLDU%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoLDU%3A%20Low-Rank%20Adaptation%20via%20Lower-Diag-Upper%20Decomposition%20for%0A%20%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Yiming%20Shi%20and%20Jiwei%20Wei%20and%20Yujia%20Wu%20and%20Ran%20Ran%20and%20Chengwei%20Sun%20and%20Shiyuan%20He%20and%20Yang%20Yang&entry.1292438233=%20%20The%20rapid%20growth%20of%20model%20scale%20has%20necessitated%20substantial%20computational%0Aresources%20for%20fine-tuning.%20Existing%20approach%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%0Ahas%20sought%20to%20address%20the%20problem%20of%20handling%20the%20large%20updated%20parameters%20in%0Afull%20fine-tuning.%20However%2C%20LoRA%20utilize%20random%20initialization%20and%20optimization%0Aof%20low-rank%20matrices%20to%20approximate%20updated%20weights%2C%20which%20can%20result%20in%0Asuboptimal%20convergence%20and%20an%20accuracy%20gap%20compared%20to%20full%20fine-tuning.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20LoLDU%2C%20a%20Parameter-Efficient%20Fine-Tuning%0A%28PEFT%29%20approach%20that%20significantly%20reduces%20trainable%20parameters%20by%202600%20times%0Acompared%20to%20regular%20PEFT%20methods%20while%20maintaining%20comparable%20performance.%0ALoLDU%20leverages%20Lower-Diag-Upper%20Decomposition%20%28LDU%29%20to%20initialize%20low-rank%0Amatrices%20for%20faster%20convergence%20and%20orthogonality.%20We%20focus%20on%20optimizing%20the%0Adiagonal%20matrix%20for%20scaling%20transformations.%20To%20the%20best%20of%20our%20knowledge%2C%0ALoLDU%20has%20the%20fewest%20parameters%20among%20all%20PEFT%20approaches.%20We%20conducted%0Aextensive%20experiments%20across%204%20instruction-following%20datasets%2C%206%20natural%0Alanguage%20understanding%20%28NLU%29%20datasets%2C%208%20image%20classification%20datasets%2C%20and%0Aimage%20generation%20datasets%20with%20multiple%20model%20types%20%28LLaMA2%2C%20RoBERTa%2C%20ViT%2C%20and%0AStable%20Diffusion%29%2C%20providing%20a%20comprehensive%20and%20detailed%20analysis.%20Our%0Aopen-source%20code%20can%20be%20accessed%20at%0A%5Chref%7Bhttps%3A//github.com/SKDDJ/LoLDU%7D%7Bhttps%3A//github.com/SKDDJ/LoLDU%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13618v1&entry.124074799=Read"},
{"title": "Change Detection in Multivariate data streams: Online Analysis with\n  Kernel-QuantTree", "author": "Michelangelo Olmo Nogara Notarianni and Filippo Leveni and Diego Stucchi and Luca Frittoli and Giacomo Boracchi", "abstract": "  We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.\n", "link": "http://arxiv.org/abs/2410.13778v1", "date": "2024-10-17", "relevancy": 2.1158, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4306}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4273}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Change%20Detection%20in%20Multivariate%20data%20streams%3A%20Online%20Analysis%20with%0A%20%20Kernel-QuantTree&body=Title%3A%20Change%20Detection%20in%20Multivariate%20data%20streams%3A%20Online%20Analysis%20with%0A%20%20Kernel-QuantTree%0AAuthor%3A%20Michelangelo%20Olmo%20Nogara%20Notarianni%20and%20Filippo%20Leveni%20and%20Diego%20Stucchi%20and%20Luca%20Frittoli%20and%20Giacomo%20Boracchi%0AAbstract%3A%20%20%20We%20present%20Kernel-QuantTree%20Exponentially%20Weighted%20Moving%20Average%20%28KQT-EWMA%29%2C%0Aa%20non-parametric%20change-detection%20algorithm%20that%20combines%20the%20Kernel-QuantTree%0A%28KQT%29%20histogram%20and%20the%20EWMA%20statistic%20to%20monitor%20multivariate%20data%20streams%0Aonline.%20The%20resulting%20monitoring%20scheme%20is%20very%20flexible%2C%20since%20histograms%20can%0Abe%20used%20to%20model%20any%20stationary%20distribution%2C%20and%20practical%2C%20since%20the%0Adistribution%20of%20test%20statistics%20does%20not%20depend%20on%20the%20distribution%20of%0Adatastream%20in%20stationary%20conditions%20%28non-parametric%20monitoring%29.%20KQT-EWMA%0Aenables%20controlling%20false%20alarms%20by%20operating%20at%20a%20pre-determined%20Average%20Run%0ALength%20%28%24ARL_0%24%29%2C%20which%20measures%20the%20expected%20number%20of%20stationary%20samples%20to%0Abe%20monitored%20before%20triggering%20a%20false%20alarm.%20The%20latter%20peculiarity%20is%20in%0Acontrast%20with%20most%20non-parametric%20change-detection%20tests%2C%20which%20rarely%20can%0Acontrol%20the%20%24ARL_0%24%20a%20priori.%20Our%20experiments%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20that%20KQT-EWMA%20can%20control%20%24ARL_0%24%20while%20achieving%0Adetection%20delays%20comparable%20to%20or%20lower%20than%20state-of-the-art%20methods%20designed%0Ato%20work%20in%20the%20same%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChange%2520Detection%2520in%2520Multivariate%2520data%2520streams%253A%2520Online%2520Analysis%2520with%250A%2520%2520Kernel-QuantTree%26entry.906535625%3DMichelangelo%2520Olmo%2520Nogara%2520Notarianni%2520and%2520Filippo%2520Leveni%2520and%2520Diego%2520Stucchi%2520and%2520Luca%2520Frittoli%2520and%2520Giacomo%2520Boracchi%26entry.1292438233%3D%2520%2520We%2520present%2520Kernel-QuantTree%2520Exponentially%2520Weighted%2520Moving%2520Average%2520%2528KQT-EWMA%2529%252C%250Aa%2520non-parametric%2520change-detection%2520algorithm%2520that%2520combines%2520the%2520Kernel-QuantTree%250A%2528KQT%2529%2520histogram%2520and%2520the%2520EWMA%2520statistic%2520to%2520monitor%2520multivariate%2520data%2520streams%250Aonline.%2520The%2520resulting%2520monitoring%2520scheme%2520is%2520very%2520flexible%252C%2520since%2520histograms%2520can%250Abe%2520used%2520to%2520model%2520any%2520stationary%2520distribution%252C%2520and%2520practical%252C%2520since%2520the%250Adistribution%2520of%2520test%2520statistics%2520does%2520not%2520depend%2520on%2520the%2520distribution%2520of%250Adatastream%2520in%2520stationary%2520conditions%2520%2528non-parametric%2520monitoring%2529.%2520KQT-EWMA%250Aenables%2520controlling%2520false%2520alarms%2520by%2520operating%2520at%2520a%2520pre-determined%2520Average%2520Run%250ALength%2520%2528%2524ARL_0%2524%2529%252C%2520which%2520measures%2520the%2520expected%2520number%2520of%2520stationary%2520samples%2520to%250Abe%2520monitored%2520before%2520triggering%2520a%2520false%2520alarm.%2520The%2520latter%2520peculiarity%2520is%2520in%250Acontrast%2520with%2520most%2520non-parametric%2520change-detection%2520tests%252C%2520which%2520rarely%2520can%250Acontrol%2520the%2520%2524ARL_0%2524%2520a%2520priori.%2520Our%2520experiments%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520demonstrate%2520that%2520KQT-EWMA%2520can%2520control%2520%2524ARL_0%2524%2520while%2520achieving%250Adetection%2520delays%2520comparable%2520to%2520or%2520lower%2520than%2520state-of-the-art%2520methods%2520designed%250Ato%2520work%2520in%2520the%2520same%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Change%20Detection%20in%20Multivariate%20data%20streams%3A%20Online%20Analysis%20with%0A%20%20Kernel-QuantTree&entry.906535625=Michelangelo%20Olmo%20Nogara%20Notarianni%20and%20Filippo%20Leveni%20and%20Diego%20Stucchi%20and%20Luca%20Frittoli%20and%20Giacomo%20Boracchi&entry.1292438233=%20%20We%20present%20Kernel-QuantTree%20Exponentially%20Weighted%20Moving%20Average%20%28KQT-EWMA%29%2C%0Aa%20non-parametric%20change-detection%20algorithm%20that%20combines%20the%20Kernel-QuantTree%0A%28KQT%29%20histogram%20and%20the%20EWMA%20statistic%20to%20monitor%20multivariate%20data%20streams%0Aonline.%20The%20resulting%20monitoring%20scheme%20is%20very%20flexible%2C%20since%20histograms%20can%0Abe%20used%20to%20model%20any%20stationary%20distribution%2C%20and%20practical%2C%20since%20the%0Adistribution%20of%20test%20statistics%20does%20not%20depend%20on%20the%20distribution%20of%0Adatastream%20in%20stationary%20conditions%20%28non-parametric%20monitoring%29.%20KQT-EWMA%0Aenables%20controlling%20false%20alarms%20by%20operating%20at%20a%20pre-determined%20Average%20Run%0ALength%20%28%24ARL_0%24%29%2C%20which%20measures%20the%20expected%20number%20of%20stationary%20samples%20to%0Abe%20monitored%20before%20triggering%20a%20false%20alarm.%20The%20latter%20peculiarity%20is%20in%0Acontrast%20with%20most%20non-parametric%20change-detection%20tests%2C%20which%20rarely%20can%0Acontrol%20the%20%24ARL_0%24%20a%20priori.%20Our%20experiments%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20that%20KQT-EWMA%20can%20control%20%24ARL_0%24%20while%20achieving%0Adetection%20delays%20comparable%20to%20or%20lower%20than%20state-of-the-art%20methods%20designed%0Ato%20work%20in%20the%20same%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13778v1&entry.124074799=Read"},
{"title": "Privacy-Preserving Decentralized AI with Confidential Computing", "author": "Dayeol Lee and Jorge Antonio and Hisham Khan", "abstract": "  This paper addresses privacy protection in decentralized Artificial\nIntelligence (AI) using Confidential Computing (CC) within the Atoma Network, a\ndecentralized AI platform designed for the Web3 domain. Decentralized AI\ndistributes AI services among multiple entities without centralized oversight,\nfostering transparency and robustness. However, this structure introduces\nsignificant privacy challenges, as sensitive assets such as proprietary models\nand personal data may be exposed to untrusted participants. Cryptography-based\nprivacy protection techniques such as zero-knowledge machine learning (zkML)\nsuffers prohibitive computational overhead. To address the limitation, we\npropose leveraging Confidential Computing (CC). Confidential Computing\nleverages hardware-based Trusted Execution Environments (TEEs) to provide\nisolation for processing sensitive data, ensuring that both model parameters\nand user data remain secure, even in decentralized, potentially untrusted\nenvironments. While TEEs face a few limitations, we believe they can bridge the\nprivacy gap in decentralized AI. We explore how we can integrate TEEs into\nAtoma's decentralized framework.\n", "link": "http://arxiv.org/abs/2410.13752v1", "date": "2024-10-17", "relevancy": 2.1126, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4279}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4208}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy-Preserving%20Decentralized%20AI%20with%20Confidential%20Computing&body=Title%3A%20Privacy-Preserving%20Decentralized%20AI%20with%20Confidential%20Computing%0AAuthor%3A%20Dayeol%20Lee%20and%20Jorge%20Antonio%20and%20Hisham%20Khan%0AAbstract%3A%20%20%20This%20paper%20addresses%20privacy%20protection%20in%20decentralized%20Artificial%0AIntelligence%20%28AI%29%20using%20Confidential%20Computing%20%28CC%29%20within%20the%20Atoma%20Network%2C%20a%0Adecentralized%20AI%20platform%20designed%20for%20the%20Web3%20domain.%20Decentralized%20AI%0Adistributes%20AI%20services%20among%20multiple%20entities%20without%20centralized%20oversight%2C%0Afostering%20transparency%20and%20robustness.%20However%2C%20this%20structure%20introduces%0Asignificant%20privacy%20challenges%2C%20as%20sensitive%20assets%20such%20as%20proprietary%20models%0Aand%20personal%20data%20may%20be%20exposed%20to%20untrusted%20participants.%20Cryptography-based%0Aprivacy%20protection%20techniques%20such%20as%20zero-knowledge%20machine%20learning%20%28zkML%29%0Asuffers%20prohibitive%20computational%20overhead.%20To%20address%20the%20limitation%2C%20we%0Apropose%20leveraging%20Confidential%20Computing%20%28CC%29.%20Confidential%20Computing%0Aleverages%20hardware-based%20Trusted%20Execution%20Environments%20%28TEEs%29%20to%20provide%0Aisolation%20for%20processing%20sensitive%20data%2C%20ensuring%20that%20both%20model%20parameters%0Aand%20user%20data%20remain%20secure%2C%20even%20in%20decentralized%2C%20potentially%20untrusted%0Aenvironments.%20While%20TEEs%20face%20a%20few%20limitations%2C%20we%20believe%20they%20can%20bridge%20the%0Aprivacy%20gap%20in%20decentralized%20AI.%20We%20explore%20how%20we%20can%20integrate%20TEEs%20into%0AAtoma%27s%20decentralized%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy-Preserving%2520Decentralized%2520AI%2520with%2520Confidential%2520Computing%26entry.906535625%3DDayeol%2520Lee%2520and%2520Jorge%2520Antonio%2520and%2520Hisham%2520Khan%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520privacy%2520protection%2520in%2520decentralized%2520Artificial%250AIntelligence%2520%2528AI%2529%2520using%2520Confidential%2520Computing%2520%2528CC%2529%2520within%2520the%2520Atoma%2520Network%252C%2520a%250Adecentralized%2520AI%2520platform%2520designed%2520for%2520the%2520Web3%2520domain.%2520Decentralized%2520AI%250Adistributes%2520AI%2520services%2520among%2520multiple%2520entities%2520without%2520centralized%2520oversight%252C%250Afostering%2520transparency%2520and%2520robustness.%2520However%252C%2520this%2520structure%2520introduces%250Asignificant%2520privacy%2520challenges%252C%2520as%2520sensitive%2520assets%2520such%2520as%2520proprietary%2520models%250Aand%2520personal%2520data%2520may%2520be%2520exposed%2520to%2520untrusted%2520participants.%2520Cryptography-based%250Aprivacy%2520protection%2520techniques%2520such%2520as%2520zero-knowledge%2520machine%2520learning%2520%2528zkML%2529%250Asuffers%2520prohibitive%2520computational%2520overhead.%2520To%2520address%2520the%2520limitation%252C%2520we%250Apropose%2520leveraging%2520Confidential%2520Computing%2520%2528CC%2529.%2520Confidential%2520Computing%250Aleverages%2520hardware-based%2520Trusted%2520Execution%2520Environments%2520%2528TEEs%2529%2520to%2520provide%250Aisolation%2520for%2520processing%2520sensitive%2520data%252C%2520ensuring%2520that%2520both%2520model%2520parameters%250Aand%2520user%2520data%2520remain%2520secure%252C%2520even%2520in%2520decentralized%252C%2520potentially%2520untrusted%250Aenvironments.%2520While%2520TEEs%2520face%2520a%2520few%2520limitations%252C%2520we%2520believe%2520they%2520can%2520bridge%2520the%250Aprivacy%2520gap%2520in%2520decentralized%2520AI.%2520We%2520explore%2520how%2520we%2520can%2520integrate%2520TEEs%2520into%250AAtoma%2527s%2520decentralized%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Preserving%20Decentralized%20AI%20with%20Confidential%20Computing&entry.906535625=Dayeol%20Lee%20and%20Jorge%20Antonio%20and%20Hisham%20Khan&entry.1292438233=%20%20This%20paper%20addresses%20privacy%20protection%20in%20decentralized%20Artificial%0AIntelligence%20%28AI%29%20using%20Confidential%20Computing%20%28CC%29%20within%20the%20Atoma%20Network%2C%20a%0Adecentralized%20AI%20platform%20designed%20for%20the%20Web3%20domain.%20Decentralized%20AI%0Adistributes%20AI%20services%20among%20multiple%20entities%20without%20centralized%20oversight%2C%0Afostering%20transparency%20and%20robustness.%20However%2C%20this%20structure%20introduces%0Asignificant%20privacy%20challenges%2C%20as%20sensitive%20assets%20such%20as%20proprietary%20models%0Aand%20personal%20data%20may%20be%20exposed%20to%20untrusted%20participants.%20Cryptography-based%0Aprivacy%20protection%20techniques%20such%20as%20zero-knowledge%20machine%20learning%20%28zkML%29%0Asuffers%20prohibitive%20computational%20overhead.%20To%20address%20the%20limitation%2C%20we%0Apropose%20leveraging%20Confidential%20Computing%20%28CC%29.%20Confidential%20Computing%0Aleverages%20hardware-based%20Trusted%20Execution%20Environments%20%28TEEs%29%20to%20provide%0Aisolation%20for%20processing%20sensitive%20data%2C%20ensuring%20that%20both%20model%20parameters%0Aand%20user%20data%20remain%20secure%2C%20even%20in%20decentralized%2C%20potentially%20untrusted%0Aenvironments.%20While%20TEEs%20face%20a%20few%20limitations%2C%20we%20believe%20they%20can%20bridge%20the%0Aprivacy%20gap%20in%20decentralized%20AI.%20We%20explore%20how%20we%20can%20integrate%20TEEs%20into%0AAtoma%27s%20decentralized%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13752v1&entry.124074799=Read"},
{"title": "Guided Multi-objective Generative AI to Enhance Structure-based Drug\n  Design", "author": "Amit Kadan and Kevin Ryczko and Erika Lloyd and Adrian Roitberg and Takeshi Yamazaki", "abstract": "  Generative AI has the potential to revolutionize drug discovery. Yet, despite\nrecent advances in deep learning, existing models cannot generate molecules\nthat satisfy all desired physicochemical properties. Herein, we describe\nIDOLpro, a generative chemistry AI combining diffusion with multi-objective\noptimization for structure-based drug design. Differentiable scoring functions\nguide the latent variables of the diffusion model to explore uncharted chemical\nspace and generate novel ligands in silico, optimizing a plurality of target\nphysicochemical properties. We demonstrate our platform's effectiveness by\ngenerating ligands with optimized binding affinity and synthetic accessibility\non two benchmark sets. IDOLpro produces ligands with binding affinities over\n10%-20% better than the next best state-of-the-art method on each test set,\nproducing more drug-like molecules with generally better synthetic\naccessibility scores than other methods. We do a head-to-head comparison of\nIDOLpro against a classic virtual screen of a large database of drug-like\nmolecules. We show that IDOLpro can generate molecules for a range of important\ndisease-related targets with better binding affinity and synthetic\naccessibility than any molecule found in the virtual screen while being over\n100x faster and less expensive to run. On a test set of experimental complexes,\nIDOLpro is the first to produce molecules with better binding affinities than\nexperimentally observed ligands. IDOLpro can accommodate other scoring\nfunctions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead\noptimization for drug discovery.\n", "link": "http://arxiv.org/abs/2405.11785v2", "date": "2024-10-17", "relevancy": 2.102, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5376}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5299}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Multi-objective%20Generative%20AI%20to%20Enhance%20Structure-based%20Drug%0A%20%20Design&body=Title%3A%20Guided%20Multi-objective%20Generative%20AI%20to%20Enhance%20Structure-based%20Drug%0A%20%20Design%0AAuthor%3A%20Amit%20Kadan%20and%20Kevin%20Ryczko%20and%20Erika%20Lloyd%20and%20Adrian%20Roitberg%20and%20Takeshi%20Yamazaki%0AAbstract%3A%20%20%20Generative%20AI%20has%20the%20potential%20to%20revolutionize%20drug%20discovery.%20Yet%2C%20despite%0Arecent%20advances%20in%20deep%20learning%2C%20existing%20models%20cannot%20generate%20molecules%0Athat%20satisfy%20all%20desired%20physicochemical%20properties.%20Herein%2C%20we%20describe%0AIDOLpro%2C%20a%20generative%20chemistry%20AI%20combining%20diffusion%20with%20multi-objective%0Aoptimization%20for%20structure-based%20drug%20design.%20Differentiable%20scoring%20functions%0Aguide%20the%20latent%20variables%20of%20the%20diffusion%20model%20to%20explore%20uncharted%20chemical%0Aspace%20and%20generate%20novel%20ligands%20in%20silico%2C%20optimizing%20a%20plurality%20of%20target%0Aphysicochemical%20properties.%20We%20demonstrate%20our%20platform%27s%20effectiveness%20by%0Agenerating%20ligands%20with%20optimized%20binding%20affinity%20and%20synthetic%20accessibility%0Aon%20two%20benchmark%20sets.%20IDOLpro%20produces%20ligands%20with%20binding%20affinities%20over%0A10%25-20%25%20better%20than%20the%20next%20best%20state-of-the-art%20method%20on%20each%20test%20set%2C%0Aproducing%20more%20drug-like%20molecules%20with%20generally%20better%20synthetic%0Aaccessibility%20scores%20than%20other%20methods.%20We%20do%20a%20head-to-head%20comparison%20of%0AIDOLpro%20against%20a%20classic%20virtual%20screen%20of%20a%20large%20database%20of%20drug-like%0Amolecules.%20We%20show%20that%20IDOLpro%20can%20generate%20molecules%20for%20a%20range%20of%20important%0Adisease-related%20targets%20with%20better%20binding%20affinity%20and%20synthetic%0Aaccessibility%20than%20any%20molecule%20found%20in%20the%20virtual%20screen%20while%20being%20over%0A100x%20faster%20and%20less%20expensive%20to%20run.%20On%20a%20test%20set%20of%20experimental%20complexes%2C%0AIDOLpro%20is%20the%20first%20to%20produce%20molecules%20with%20better%20binding%20affinities%20than%0Aexperimentally%20observed%20ligands.%20IDOLpro%20can%20accommodate%20other%20scoring%0Afunctions%20%28e.g.%20ADME-Tox%29%20to%20accelerate%20hit-finding%2C%20hit-to-lead%2C%20and%20lead%0Aoptimization%20for%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Multi-objective%2520Generative%2520AI%2520to%2520Enhance%2520Structure-based%2520Drug%250A%2520%2520Design%26entry.906535625%3DAmit%2520Kadan%2520and%2520Kevin%2520Ryczko%2520and%2520Erika%2520Lloyd%2520and%2520Adrian%2520Roitberg%2520and%2520Takeshi%2520Yamazaki%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520the%2520potential%2520to%2520revolutionize%2520drug%2520discovery.%2520Yet%252C%2520despite%250Arecent%2520advances%2520in%2520deep%2520learning%252C%2520existing%2520models%2520cannot%2520generate%2520molecules%250Athat%2520satisfy%2520all%2520desired%2520physicochemical%2520properties.%2520Herein%252C%2520we%2520describe%250AIDOLpro%252C%2520a%2520generative%2520chemistry%2520AI%2520combining%2520diffusion%2520with%2520multi-objective%250Aoptimization%2520for%2520structure-based%2520drug%2520design.%2520Differentiable%2520scoring%2520functions%250Aguide%2520the%2520latent%2520variables%2520of%2520the%2520diffusion%2520model%2520to%2520explore%2520uncharted%2520chemical%250Aspace%2520and%2520generate%2520novel%2520ligands%2520in%2520silico%252C%2520optimizing%2520a%2520plurality%2520of%2520target%250Aphysicochemical%2520properties.%2520We%2520demonstrate%2520our%2520platform%2527s%2520effectiveness%2520by%250Agenerating%2520ligands%2520with%2520optimized%2520binding%2520affinity%2520and%2520synthetic%2520accessibility%250Aon%2520two%2520benchmark%2520sets.%2520IDOLpro%2520produces%2520ligands%2520with%2520binding%2520affinities%2520over%250A10%2525-20%2525%2520better%2520than%2520the%2520next%2520best%2520state-of-the-art%2520method%2520on%2520each%2520test%2520set%252C%250Aproducing%2520more%2520drug-like%2520molecules%2520with%2520generally%2520better%2520synthetic%250Aaccessibility%2520scores%2520than%2520other%2520methods.%2520We%2520do%2520a%2520head-to-head%2520comparison%2520of%250AIDOLpro%2520against%2520a%2520classic%2520virtual%2520screen%2520of%2520a%2520large%2520database%2520of%2520drug-like%250Amolecules.%2520We%2520show%2520that%2520IDOLpro%2520can%2520generate%2520molecules%2520for%2520a%2520range%2520of%2520important%250Adisease-related%2520targets%2520with%2520better%2520binding%2520affinity%2520and%2520synthetic%250Aaccessibility%2520than%2520any%2520molecule%2520found%2520in%2520the%2520virtual%2520screen%2520while%2520being%2520over%250A100x%2520faster%2520and%2520less%2520expensive%2520to%2520run.%2520On%2520a%2520test%2520set%2520of%2520experimental%2520complexes%252C%250AIDOLpro%2520is%2520the%2520first%2520to%2520produce%2520molecules%2520with%2520better%2520binding%2520affinities%2520than%250Aexperimentally%2520observed%2520ligands.%2520IDOLpro%2520can%2520accommodate%2520other%2520scoring%250Afunctions%2520%2528e.g.%2520ADME-Tox%2529%2520to%2520accelerate%2520hit-finding%252C%2520hit-to-lead%252C%2520and%2520lead%250Aoptimization%2520for%2520drug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Multi-objective%20Generative%20AI%20to%20Enhance%20Structure-based%20Drug%0A%20%20Design&entry.906535625=Amit%20Kadan%20and%20Kevin%20Ryczko%20and%20Erika%20Lloyd%20and%20Adrian%20Roitberg%20and%20Takeshi%20Yamazaki&entry.1292438233=%20%20Generative%20AI%20has%20the%20potential%20to%20revolutionize%20drug%20discovery.%20Yet%2C%20despite%0Arecent%20advances%20in%20deep%20learning%2C%20existing%20models%20cannot%20generate%20molecules%0Athat%20satisfy%20all%20desired%20physicochemical%20properties.%20Herein%2C%20we%20describe%0AIDOLpro%2C%20a%20generative%20chemistry%20AI%20combining%20diffusion%20with%20multi-objective%0Aoptimization%20for%20structure-based%20drug%20design.%20Differentiable%20scoring%20functions%0Aguide%20the%20latent%20variables%20of%20the%20diffusion%20model%20to%20explore%20uncharted%20chemical%0Aspace%20and%20generate%20novel%20ligands%20in%20silico%2C%20optimizing%20a%20plurality%20of%20target%0Aphysicochemical%20properties.%20We%20demonstrate%20our%20platform%27s%20effectiveness%20by%0Agenerating%20ligands%20with%20optimized%20binding%20affinity%20and%20synthetic%20accessibility%0Aon%20two%20benchmark%20sets.%20IDOLpro%20produces%20ligands%20with%20binding%20affinities%20over%0A10%25-20%25%20better%20than%20the%20next%20best%20state-of-the-art%20method%20on%20each%20test%20set%2C%0Aproducing%20more%20drug-like%20molecules%20with%20generally%20better%20synthetic%0Aaccessibility%20scores%20than%20other%20methods.%20We%20do%20a%20head-to-head%20comparison%20of%0AIDOLpro%20against%20a%20classic%20virtual%20screen%20of%20a%20large%20database%20of%20drug-like%0Amolecules.%20We%20show%20that%20IDOLpro%20can%20generate%20molecules%20for%20a%20range%20of%20important%0Adisease-related%20targets%20with%20better%20binding%20affinity%20and%20synthetic%0Aaccessibility%20than%20any%20molecule%20found%20in%20the%20virtual%20screen%20while%20being%20over%0A100x%20faster%20and%20less%20expensive%20to%20run.%20On%20a%20test%20set%20of%20experimental%20complexes%2C%0AIDOLpro%20is%20the%20first%20to%20produce%20molecules%20with%20better%20binding%20affinities%20than%0Aexperimentally%20observed%20ligands.%20IDOLpro%20can%20accommodate%20other%20scoring%0Afunctions%20%28e.g.%20ADME-Tox%29%20to%20accelerate%20hit-finding%2C%20hit-to-lead%2C%20and%20lead%0Aoptimization%20for%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11785v2&entry.124074799=Read"},
{"title": "Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models", "author": "Mazda Moayeri and Vidhisha Balachandran and Varun Chandrasekaran and Safoora Yousefi and Thomas Fel and Soheil Feizi and Besmira Nushi and Neel Joshi and Vibhav Vineet", "abstract": "  With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities.\n", "link": "http://arxiv.org/abs/2410.13826v1", "date": "2024-10-17", "relevancy": 2.0815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unearthing%20Skill-Level%20Insights%20for%20Understanding%20Trade-Offs%20of%0A%20%20Foundation%20Models&body=Title%3A%20Unearthing%20Skill-Level%20Insights%20for%20Understanding%20Trade-Offs%20of%0A%20%20Foundation%20Models%0AAuthor%3A%20Mazda%20Moayeri%20and%20Vidhisha%20Balachandran%20and%20Varun%20Chandrasekaran%20and%20Safoora%20Yousefi%20and%20Thomas%20Fel%20and%20Soheil%20Feizi%20and%20Besmira%20Nushi%20and%20Neel%20Joshi%20and%20Vibhav%20Vineet%0AAbstract%3A%20%20%20With%20models%20getting%20stronger%2C%20evaluations%20have%20grown%20more%20complex%2C%20testing%0Amultiple%20skills%20in%20one%20benchmark%20and%20even%20in%20the%20same%20instance%20at%20once.%0AHowever%2C%20skill-wise%20performance%20is%20obscured%20when%20inspecting%20aggregate%20accuracy%2C%0Aunder-utilizing%20the%20rich%20signal%20modern%20benchmarks%20contain.%20We%20propose%20an%0Aautomatic%20approach%20to%20recover%20the%20underlying%20skills%20relevant%20for%20any%20evaluation%0Ainstance%2C%20by%20way%20of%20inspecting%20model-generated%20rationales.%20After%20validating%20the%0Arelevance%20of%20rationale-parsed%20skills%20and%20inferring%20skills%20for%20%2446%24k%20instances%0Aover%20%2412%24%20benchmarks%2C%20we%20observe%20many%20skills%20to%20be%20common%20across%20benchmarks%2C%0Aresulting%20in%20the%20curation%20of%20hundreds%20of%20skill-slices%20%28i.e.%20sets%20of%20instances%0Atesting%20a%20common%20skill%29.%20Inspecting%20accuracy%20over%20these%20slices%20yields%20novel%0Ainsights%20on%20model%20trade-offs%3A%20e.g.%2C%20compared%20to%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%0Aon%20average%2C%20Gemini%201.5%20Pro%20is%20%2418%5C%25%24%20more%20accurate%20in%20%22computing%20molar%20mass%22%2C%0Abut%20%2419%5C%25%24%20less%20accurate%20in%20%22applying%20constitutional%20law%22%2C%20despite%20the%20overall%0Aaccuracies%20of%20the%20three%20models%20differing%20by%20a%20mere%20%240.4%5C%25%24.%20Furthermore%2C%20we%0Ademonstrate%20the%20practical%20utility%20of%20our%20approach%20by%20showing%20that%20insights%0Aderived%20from%20skill%20slice%20analysis%20can%20generalize%20to%20held-out%20instances%3A%20when%0Arouting%20each%20instance%20to%20the%20model%20strongest%20on%20the%20relevant%20skills%2C%20we%20see%20a%0A%243%5C%25%24%20accuracy%20improvement%20over%20our%20%2412%24%20dataset%20corpus.%20Our%20skill-slices%20and%0Aframework%20open%20a%20new%20avenue%20in%20model%20evaluation%2C%20leveraging%20skill-specific%0Aanalyses%20to%20unlock%20a%20more%20granular%20and%20actionable%20understanding%20of%20model%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnearthing%2520Skill-Level%2520Insights%2520for%2520Understanding%2520Trade-Offs%2520of%250A%2520%2520Foundation%2520Models%26entry.906535625%3DMazda%2520Moayeri%2520and%2520Vidhisha%2520Balachandran%2520and%2520Varun%2520Chandrasekaran%2520and%2520Safoora%2520Yousefi%2520and%2520Thomas%2520Fel%2520and%2520Soheil%2520Feizi%2520and%2520Besmira%2520Nushi%2520and%2520Neel%2520Joshi%2520and%2520Vibhav%2520Vineet%26entry.1292438233%3D%2520%2520With%2520models%2520getting%2520stronger%252C%2520evaluations%2520have%2520grown%2520more%2520complex%252C%2520testing%250Amultiple%2520skills%2520in%2520one%2520benchmark%2520and%2520even%2520in%2520the%2520same%2520instance%2520at%2520once.%250AHowever%252C%2520skill-wise%2520performance%2520is%2520obscured%2520when%2520inspecting%2520aggregate%2520accuracy%252C%250Aunder-utilizing%2520the%2520rich%2520signal%2520modern%2520benchmarks%2520contain.%2520We%2520propose%2520an%250Aautomatic%2520approach%2520to%2520recover%2520the%2520underlying%2520skills%2520relevant%2520for%2520any%2520evaluation%250Ainstance%252C%2520by%2520way%2520of%2520inspecting%2520model-generated%2520rationales.%2520After%2520validating%2520the%250Arelevance%2520of%2520rationale-parsed%2520skills%2520and%2520inferring%2520skills%2520for%2520%252446%2524k%2520instances%250Aover%2520%252412%2524%2520benchmarks%252C%2520we%2520observe%2520many%2520skills%2520to%2520be%2520common%2520across%2520benchmarks%252C%250Aresulting%2520in%2520the%2520curation%2520of%2520hundreds%2520of%2520skill-slices%2520%2528i.e.%2520sets%2520of%2520instances%250Atesting%2520a%2520common%2520skill%2529.%2520Inspecting%2520accuracy%2520over%2520these%2520slices%2520yields%2520novel%250Ainsights%2520on%2520model%2520trade-offs%253A%2520e.g.%252C%2520compared%2520to%2520GPT-4o%2520and%2520Claude%25203.5%2520Sonnet%252C%250Aon%2520average%252C%2520Gemini%25201.5%2520Pro%2520is%2520%252418%255C%2525%2524%2520more%2520accurate%2520in%2520%2522computing%2520molar%2520mass%2522%252C%250Abut%2520%252419%255C%2525%2524%2520less%2520accurate%2520in%2520%2522applying%2520constitutional%2520law%2522%252C%2520despite%2520the%2520overall%250Aaccuracies%2520of%2520the%2520three%2520models%2520differing%2520by%2520a%2520mere%2520%25240.4%255C%2525%2524.%2520Furthermore%252C%2520we%250Ademonstrate%2520the%2520practical%2520utility%2520of%2520our%2520approach%2520by%2520showing%2520that%2520insights%250Aderived%2520from%2520skill%2520slice%2520analysis%2520can%2520generalize%2520to%2520held-out%2520instances%253A%2520when%250Arouting%2520each%2520instance%2520to%2520the%2520model%2520strongest%2520on%2520the%2520relevant%2520skills%252C%2520we%2520see%2520a%250A%25243%255C%2525%2524%2520accuracy%2520improvement%2520over%2520our%2520%252412%2524%2520dataset%2520corpus.%2520Our%2520skill-slices%2520and%250Aframework%2520open%2520a%2520new%2520avenue%2520in%2520model%2520evaluation%252C%2520leveraging%2520skill-specific%250Aanalyses%2520to%2520unlock%2520a%2520more%2520granular%2520and%2520actionable%2520understanding%2520of%2520model%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unearthing%20Skill-Level%20Insights%20for%20Understanding%20Trade-Offs%20of%0A%20%20Foundation%20Models&entry.906535625=Mazda%20Moayeri%20and%20Vidhisha%20Balachandran%20and%20Varun%20Chandrasekaran%20and%20Safoora%20Yousefi%20and%20Thomas%20Fel%20and%20Soheil%20Feizi%20and%20Besmira%20Nushi%20and%20Neel%20Joshi%20and%20Vibhav%20Vineet&entry.1292438233=%20%20With%20models%20getting%20stronger%2C%20evaluations%20have%20grown%20more%20complex%2C%20testing%0Amultiple%20skills%20in%20one%20benchmark%20and%20even%20in%20the%20same%20instance%20at%20once.%0AHowever%2C%20skill-wise%20performance%20is%20obscured%20when%20inspecting%20aggregate%20accuracy%2C%0Aunder-utilizing%20the%20rich%20signal%20modern%20benchmarks%20contain.%20We%20propose%20an%0Aautomatic%20approach%20to%20recover%20the%20underlying%20skills%20relevant%20for%20any%20evaluation%0Ainstance%2C%20by%20way%20of%20inspecting%20model-generated%20rationales.%20After%20validating%20the%0Arelevance%20of%20rationale-parsed%20skills%20and%20inferring%20skills%20for%20%2446%24k%20instances%0Aover%20%2412%24%20benchmarks%2C%20we%20observe%20many%20skills%20to%20be%20common%20across%20benchmarks%2C%0Aresulting%20in%20the%20curation%20of%20hundreds%20of%20skill-slices%20%28i.e.%20sets%20of%20instances%0Atesting%20a%20common%20skill%29.%20Inspecting%20accuracy%20over%20these%20slices%20yields%20novel%0Ainsights%20on%20model%20trade-offs%3A%20e.g.%2C%20compared%20to%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%0Aon%20average%2C%20Gemini%201.5%20Pro%20is%20%2418%5C%25%24%20more%20accurate%20in%20%22computing%20molar%20mass%22%2C%0Abut%20%2419%5C%25%24%20less%20accurate%20in%20%22applying%20constitutional%20law%22%2C%20despite%20the%20overall%0Aaccuracies%20of%20the%20three%20models%20differing%20by%20a%20mere%20%240.4%5C%25%24.%20Furthermore%2C%20we%0Ademonstrate%20the%20practical%20utility%20of%20our%20approach%20by%20showing%20that%20insights%0Aderived%20from%20skill%20slice%20analysis%20can%20generalize%20to%20held-out%20instances%3A%20when%0Arouting%20each%20instance%20to%20the%20model%20strongest%20on%20the%20relevant%20skills%2C%20we%20see%20a%0A%243%5C%25%24%20accuracy%20improvement%20over%20our%20%2412%24%20dataset%20corpus.%20Our%20skill-slices%20and%0Aframework%20open%20a%20new%20avenue%20in%20model%20evaluation%2C%20leveraging%20skill-specific%0Aanalyses%20to%20unlock%20a%20more%20granular%20and%20actionable%20understanding%20of%20model%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13826v1&entry.124074799=Read"},
{"title": "H2OVL-Mississippi Vision Language Models Technical Report", "author": "Shaikat Galib and Shanshan Wang and Guanshuo Xu and Pascal Pfeiffer and Ryan Chesler and Mark Landry and Sri Satish Ambati", "abstract": "  Smaller vision-language models (VLMs) are becoming increasingly important for\nprivacy-focused, on-device applications due to their ability to run efficiently\non consumer hardware for processing enterprise commercial documents and images.\nThese models require strong language understanding and visual capabilities to\nenhance human-machine interaction. To address this need, we present\nH2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs\nusing 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny\nmodel with 0.8 billion parameters that specializes in text recognition,\nachieving state of the art performance on the Text Recognition portion of\nOCRBench and surpassing much larger models in this area. Additionally, we are\nreleasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use\ncases, exhibiting highly competitive metrics across various academic\nbenchmarks. Both models build upon our prior work with H2O-Danube language\nmodels, extending their capabilities into the visual domain. We release them\nunder the Apache 2.0 license, making VLMs accessible to everyone, democratizing\ndocument AI and visual LLMs.\n", "link": "http://arxiv.org/abs/2410.13611v1", "date": "2024-10-17", "relevancy": 2.0792, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H2OVL-Mississippi%20Vision%20Language%20Models%20Technical%20Report&body=Title%3A%20H2OVL-Mississippi%20Vision%20Language%20Models%20Technical%20Report%0AAuthor%3A%20Shaikat%20Galib%20and%20Shanshan%20Wang%20and%20Guanshuo%20Xu%20and%20Pascal%20Pfeiffer%20and%20Ryan%20Chesler%20and%20Mark%20Landry%20and%20Sri%20Satish%20Ambati%0AAbstract%3A%20%20%20Smaller%20vision-language%20models%20%28VLMs%29%20are%20becoming%20increasingly%20important%20for%0Aprivacy-focused%2C%20on-device%20applications%20due%20to%20their%20ability%20to%20run%20efficiently%0Aon%20consumer%20hardware%20for%20processing%20enterprise%20commercial%20documents%20and%20images.%0AThese%20models%20require%20strong%20language%20understanding%20and%20visual%20capabilities%20to%0Aenhance%20human-machine%20interaction.%20To%20address%20this%20need%2C%20we%20present%0AH2OVL-Mississippi%2C%20a%20pair%20of%20small%20VLMs%20trained%20on%2037%20million%20image-text%20pairs%0Ausing%20240%20hours%20of%20compute%20on%208%20x%20H100%20GPUs.%20H2OVL-Mississippi-0.8B%20is%20a%20tiny%0Amodel%20with%200.8%20billion%20parameters%20that%20specializes%20in%20text%20recognition%2C%0Aachieving%20state%20of%20the%20art%20performance%20on%20the%20Text%20Recognition%20portion%20of%0AOCRBench%20and%20surpassing%20much%20larger%20models%20in%20this%20area.%20Additionally%2C%20we%20are%0Areleasing%20H2OVL-Mississippi-2B%2C%20a%202%20billion%20parameter%20model%20for%20general%20use%0Acases%2C%20exhibiting%20highly%20competitive%20metrics%20across%20various%20academic%0Abenchmarks.%20Both%20models%20build%20upon%20our%20prior%20work%20with%20H2O-Danube%20language%0Amodels%2C%20extending%20their%20capabilities%20into%20the%20visual%20domain.%20We%20release%20them%0Aunder%20the%20Apache%202.0%20license%2C%20making%20VLMs%20accessible%20to%20everyone%2C%20democratizing%0Adocument%20AI%20and%20visual%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH2OVL-Mississippi%2520Vision%2520Language%2520Models%2520Technical%2520Report%26entry.906535625%3DShaikat%2520Galib%2520and%2520Shanshan%2520Wang%2520and%2520Guanshuo%2520Xu%2520and%2520Pascal%2520Pfeiffer%2520and%2520Ryan%2520Chesler%2520and%2520Mark%2520Landry%2520and%2520Sri%2520Satish%2520Ambati%26entry.1292438233%3D%2520%2520Smaller%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520becoming%2520increasingly%2520important%2520for%250Aprivacy-focused%252C%2520on-device%2520applications%2520due%2520to%2520their%2520ability%2520to%2520run%2520efficiently%250Aon%2520consumer%2520hardware%2520for%2520processing%2520enterprise%2520commercial%2520documents%2520and%2520images.%250AThese%2520models%2520require%2520strong%2520language%2520understanding%2520and%2520visual%2520capabilities%2520to%250Aenhance%2520human-machine%2520interaction.%2520To%2520address%2520this%2520need%252C%2520we%2520present%250AH2OVL-Mississippi%252C%2520a%2520pair%2520of%2520small%2520VLMs%2520trained%2520on%252037%2520million%2520image-text%2520pairs%250Ausing%2520240%2520hours%2520of%2520compute%2520on%25208%2520x%2520H100%2520GPUs.%2520H2OVL-Mississippi-0.8B%2520is%2520a%2520tiny%250Amodel%2520with%25200.8%2520billion%2520parameters%2520that%2520specializes%2520in%2520text%2520recognition%252C%250Aachieving%2520state%2520of%2520the%2520art%2520performance%2520on%2520the%2520Text%2520Recognition%2520portion%2520of%250AOCRBench%2520and%2520surpassing%2520much%2520larger%2520models%2520in%2520this%2520area.%2520Additionally%252C%2520we%2520are%250Areleasing%2520H2OVL-Mississippi-2B%252C%2520a%25202%2520billion%2520parameter%2520model%2520for%2520general%2520use%250Acases%252C%2520exhibiting%2520highly%2520competitive%2520metrics%2520across%2520various%2520academic%250Abenchmarks.%2520Both%2520models%2520build%2520upon%2520our%2520prior%2520work%2520with%2520H2O-Danube%2520language%250Amodels%252C%2520extending%2520their%2520capabilities%2520into%2520the%2520visual%2520domain.%2520We%2520release%2520them%250Aunder%2520the%2520Apache%25202.0%2520license%252C%2520making%2520VLMs%2520accessible%2520to%2520everyone%252C%2520democratizing%250Adocument%2520AI%2520and%2520visual%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H2OVL-Mississippi%20Vision%20Language%20Models%20Technical%20Report&entry.906535625=Shaikat%20Galib%20and%20Shanshan%20Wang%20and%20Guanshuo%20Xu%20and%20Pascal%20Pfeiffer%20and%20Ryan%20Chesler%20and%20Mark%20Landry%20and%20Sri%20Satish%20Ambati&entry.1292438233=%20%20Smaller%20vision-language%20models%20%28VLMs%29%20are%20becoming%20increasingly%20important%20for%0Aprivacy-focused%2C%20on-device%20applications%20due%20to%20their%20ability%20to%20run%20efficiently%0Aon%20consumer%20hardware%20for%20processing%20enterprise%20commercial%20documents%20and%20images.%0AThese%20models%20require%20strong%20language%20understanding%20and%20visual%20capabilities%20to%0Aenhance%20human-machine%20interaction.%20To%20address%20this%20need%2C%20we%20present%0AH2OVL-Mississippi%2C%20a%20pair%20of%20small%20VLMs%20trained%20on%2037%20million%20image-text%20pairs%0Ausing%20240%20hours%20of%20compute%20on%208%20x%20H100%20GPUs.%20H2OVL-Mississippi-0.8B%20is%20a%20tiny%0Amodel%20with%200.8%20billion%20parameters%20that%20specializes%20in%20text%20recognition%2C%0Aachieving%20state%20of%20the%20art%20performance%20on%20the%20Text%20Recognition%20portion%20of%0AOCRBench%20and%20surpassing%20much%20larger%20models%20in%20this%20area.%20Additionally%2C%20we%20are%0Areleasing%20H2OVL-Mississippi-2B%2C%20a%202%20billion%20parameter%20model%20for%20general%20use%0Acases%2C%20exhibiting%20highly%20competitive%20metrics%20across%20various%20academic%0Abenchmarks.%20Both%20models%20build%20upon%20our%20prior%20work%20with%20H2O-Danube%20language%0Amodels%2C%20extending%20their%20capabilities%20into%20the%20visual%20domain.%20We%20release%20them%0Aunder%20the%20Apache%202.0%20license%2C%20making%20VLMs%20accessible%20to%20everyone%2C%20democratizing%0Adocument%20AI%20and%20visual%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13611v1&entry.124074799=Read"},
{"title": "Pose-Based Sign Language Appearance Transfer", "author": "Amit Moryossef and Gerard Sant and Zifan Jiang", "abstract": "  We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\n\\url{https://github.com/sign-language-processing/pose-anonymization}.\n", "link": "http://arxiv.org/abs/2410.13675v1", "date": "2024-10-17", "relevancy": 2.0772, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5653}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4868}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose-Based%20Sign%20Language%20Appearance%20Transfer&body=Title%3A%20Pose-Based%20Sign%20Language%20Appearance%20Transfer%0AAuthor%3A%20Amit%20Moryossef%20and%20Gerard%20Sant%20and%20Zifan%20Jiang%0AAbstract%3A%20%20%20We%20introduce%20a%20method%20for%20transferring%20the%20signer%27s%20appearance%20in%20sign%0Alanguage%20skeletal%20poses%20while%20preserving%20the%20sign%20content.%20Using%20estimated%0Aposes%2C%20we%20transfer%20the%20appearance%20of%20one%20signer%20to%20another%2C%20maintaining%20natural%0Amovements%20and%20transitions.%20This%20approach%20improves%20pose-based%20rendering%20and%20sign%0Astitching%20while%20obfuscating%20identity.%20Our%20experiments%20show%20that%20while%20the%0Amethod%20reduces%20signer%20identification%20accuracy%2C%20it%20slightly%20harms%20sign%0Arecognition%20performance%2C%20highlighting%20a%20tradeoff%20between%20privacy%20and%20utility.%0AOur%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sign-language-processing/pose-anonymization%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose-Based%2520Sign%2520Language%2520Appearance%2520Transfer%26entry.906535625%3DAmit%2520Moryossef%2520and%2520Gerard%2520Sant%2520and%2520Zifan%2520Jiang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520method%2520for%2520transferring%2520the%2520signer%2527s%2520appearance%2520in%2520sign%250Alanguage%2520skeletal%2520poses%2520while%2520preserving%2520the%2520sign%2520content.%2520Using%2520estimated%250Aposes%252C%2520we%2520transfer%2520the%2520appearance%2520of%2520one%2520signer%2520to%2520another%252C%2520maintaining%2520natural%250Amovements%2520and%2520transitions.%2520This%2520approach%2520improves%2520pose-based%2520rendering%2520and%2520sign%250Astitching%2520while%2520obfuscating%2520identity.%2520Our%2520experiments%2520show%2520that%2520while%2520the%250Amethod%2520reduces%2520signer%2520identification%2520accuracy%252C%2520it%2520slightly%2520harms%2520sign%250Arecognition%2520performance%252C%2520highlighting%2520a%2520tradeoff%2520between%2520privacy%2520and%2520utility.%250AOur%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/sign-language-processing/pose-anonymization%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose-Based%20Sign%20Language%20Appearance%20Transfer&entry.906535625=Amit%20Moryossef%20and%20Gerard%20Sant%20and%20Zifan%20Jiang&entry.1292438233=%20%20We%20introduce%20a%20method%20for%20transferring%20the%20signer%27s%20appearance%20in%20sign%0Alanguage%20skeletal%20poses%20while%20preserving%20the%20sign%20content.%20Using%20estimated%0Aposes%2C%20we%20transfer%20the%20appearance%20of%20one%20signer%20to%20another%2C%20maintaining%20natural%0Amovements%20and%20transitions.%20This%20approach%20improves%20pose-based%20rendering%20and%20sign%0Astitching%20while%20obfuscating%20identity.%20Our%20experiments%20show%20that%20while%20the%0Amethod%20reduces%20signer%20identification%20accuracy%2C%20it%20slightly%20harms%20sign%0Arecognition%20performance%2C%20highlighting%20a%20tradeoff%20between%20privacy%20and%20utility.%0AOur%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sign-language-processing/pose-anonymization%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13675v1&entry.124074799=Read"},
{"title": "Learning Graph Quantized Tokenizers for Transformers", "author": "Limei Wang and Kaveh Hassani and Si Zhang and Dongqi Fu and Baichuan Yuan and Weilin Cong and Zhigang Hua and Hao Wu and Ning Yao and Bo Long", "abstract": "  Transformers serve as the backbone architectures of Foundational Models,\nwhere a domain-specific tokenizer helps them adapt to various domains. Graph\nTransformers (GTs) have recently emerged as a leading model in geometric deep\nlearning, outperforming Graph Neural Networks (GNNs) in various graph learning\ntasks. However, the development of tokenizers for graphs has lagged behind\nother modalities, with existing approaches relying on heuristics or GNNs\nco-trained with Transformers. To address this, we introduce GQT (\\textbf{G}raph\n\\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from\nTransformer training by leveraging multi-task graph self-supervised learning,\nyielding robust and generalizable graph tokens. Furthermore, the GQT utilizes\nResidual Vector Quantization (RVQ) to learn hierarchical discrete tokens,\nresulting in significantly reduced memory requirements and improved\ngeneralization capabilities. By combining the GQT with token modulation, a\nTransformer encoder achieves state-of-the-art performance on 16 out of 18\nbenchmarks, including large-scale homophilic and heterophilic datasets. The\ncode is available at: https://github.com/limei0307/graph-tokenizer\n", "link": "http://arxiv.org/abs/2410.13798v1", "date": "2024-10-17", "relevancy": 2.0737, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5662}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5436}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Graph%20Quantized%20Tokenizers%20for%20Transformers&body=Title%3A%20Learning%20Graph%20Quantized%20Tokenizers%20for%20Transformers%0AAuthor%3A%20Limei%20Wang%20and%20Kaveh%20Hassani%20and%20Si%20Zhang%20and%20Dongqi%20Fu%20and%20Baichuan%20Yuan%20and%20Weilin%20Cong%20and%20Zhigang%20Hua%20and%20Hao%20Wu%20and%20Ning%20Yao%20and%20Bo%20Long%0AAbstract%3A%20%20%20Transformers%20serve%20as%20the%20backbone%20architectures%20of%20Foundational%20Models%2C%0Awhere%20a%20domain-specific%20tokenizer%20helps%20them%20adapt%20to%20various%20domains.%20Graph%0ATransformers%20%28GTs%29%20have%20recently%20emerged%20as%20a%20leading%20model%20in%20geometric%20deep%0Alearning%2C%20outperforming%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20various%20graph%20learning%0Atasks.%20However%2C%20the%20development%20of%20tokenizers%20for%20graphs%20has%20lagged%20behind%0Aother%20modalities%2C%20with%20existing%20approaches%20relying%20on%20heuristics%20or%20GNNs%0Aco-trained%20with%20Transformers.%20To%20address%20this%2C%20we%20introduce%20GQT%20%28%5Ctextbf%7BG%7Draph%0A%5Ctextbf%7BQ%7Duantized%20%5Ctextbf%7BT%7Dokenizer%29%2C%20which%20decouples%20tokenizer%20training%20from%0ATransformer%20training%20by%20leveraging%20multi-task%20graph%20self-supervised%20learning%2C%0Ayielding%20robust%20and%20generalizable%20graph%20tokens.%20Furthermore%2C%20the%20GQT%20utilizes%0AResidual%20Vector%20Quantization%20%28RVQ%29%20to%20learn%20hierarchical%20discrete%20tokens%2C%0Aresulting%20in%20significantly%20reduced%20memory%20requirements%20and%20improved%0Ageneralization%20capabilities.%20By%20combining%20the%20GQT%20with%20token%20modulation%2C%20a%0ATransformer%20encoder%20achieves%20state-of-the-art%20performance%20on%2016%20out%20of%2018%0Abenchmarks%2C%20including%20large-scale%20homophilic%20and%20heterophilic%20datasets.%20The%0Acode%20is%20available%20at%3A%20https%3A//github.com/limei0307/graph-tokenizer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Graph%2520Quantized%2520Tokenizers%2520for%2520Transformers%26entry.906535625%3DLimei%2520Wang%2520and%2520Kaveh%2520Hassani%2520and%2520Si%2520Zhang%2520and%2520Dongqi%2520Fu%2520and%2520Baichuan%2520Yuan%2520and%2520Weilin%2520Cong%2520and%2520Zhigang%2520Hua%2520and%2520Hao%2520Wu%2520and%2520Ning%2520Yao%2520and%2520Bo%2520Long%26entry.1292438233%3D%2520%2520Transformers%2520serve%2520as%2520the%2520backbone%2520architectures%2520of%2520Foundational%2520Models%252C%250Awhere%2520a%2520domain-specific%2520tokenizer%2520helps%2520them%2520adapt%2520to%2520various%2520domains.%2520Graph%250ATransformers%2520%2528GTs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520leading%2520model%2520in%2520geometric%2520deep%250Alearning%252C%2520outperforming%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520in%2520various%2520graph%2520learning%250Atasks.%2520However%252C%2520the%2520development%2520of%2520tokenizers%2520for%2520graphs%2520has%2520lagged%2520behind%250Aother%2520modalities%252C%2520with%2520existing%2520approaches%2520relying%2520on%2520heuristics%2520or%2520GNNs%250Aco-trained%2520with%2520Transformers.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GQT%2520%2528%255Ctextbf%257BG%257Draph%250A%255Ctextbf%257BQ%257Duantized%2520%255Ctextbf%257BT%257Dokenizer%2529%252C%2520which%2520decouples%2520tokenizer%2520training%2520from%250ATransformer%2520training%2520by%2520leveraging%2520multi-task%2520graph%2520self-supervised%2520learning%252C%250Ayielding%2520robust%2520and%2520generalizable%2520graph%2520tokens.%2520Furthermore%252C%2520the%2520GQT%2520utilizes%250AResidual%2520Vector%2520Quantization%2520%2528RVQ%2529%2520to%2520learn%2520hierarchical%2520discrete%2520tokens%252C%250Aresulting%2520in%2520significantly%2520reduced%2520memory%2520requirements%2520and%2520improved%250Ageneralization%2520capabilities.%2520By%2520combining%2520the%2520GQT%2520with%2520token%2520modulation%252C%2520a%250ATransformer%2520encoder%2520achieves%2520state-of-the-art%2520performance%2520on%252016%2520out%2520of%252018%250Abenchmarks%252C%2520including%2520large-scale%2520homophilic%2520and%2520heterophilic%2520datasets.%2520The%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/limei0307/graph-tokenizer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Graph%20Quantized%20Tokenizers%20for%20Transformers&entry.906535625=Limei%20Wang%20and%20Kaveh%20Hassani%20and%20Si%20Zhang%20and%20Dongqi%20Fu%20and%20Baichuan%20Yuan%20and%20Weilin%20Cong%20and%20Zhigang%20Hua%20and%20Hao%20Wu%20and%20Ning%20Yao%20and%20Bo%20Long&entry.1292438233=%20%20Transformers%20serve%20as%20the%20backbone%20architectures%20of%20Foundational%20Models%2C%0Awhere%20a%20domain-specific%20tokenizer%20helps%20them%20adapt%20to%20various%20domains.%20Graph%0ATransformers%20%28GTs%29%20have%20recently%20emerged%20as%20a%20leading%20model%20in%20geometric%20deep%0Alearning%2C%20outperforming%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20various%20graph%20learning%0Atasks.%20However%2C%20the%20development%20of%20tokenizers%20for%20graphs%20has%20lagged%20behind%0Aother%20modalities%2C%20with%20existing%20approaches%20relying%20on%20heuristics%20or%20GNNs%0Aco-trained%20with%20Transformers.%20To%20address%20this%2C%20we%20introduce%20GQT%20%28%5Ctextbf%7BG%7Draph%0A%5Ctextbf%7BQ%7Duantized%20%5Ctextbf%7BT%7Dokenizer%29%2C%20which%20decouples%20tokenizer%20training%20from%0ATransformer%20training%20by%20leveraging%20multi-task%20graph%20self-supervised%20learning%2C%0Ayielding%20robust%20and%20generalizable%20graph%20tokens.%20Furthermore%2C%20the%20GQT%20utilizes%0AResidual%20Vector%20Quantization%20%28RVQ%29%20to%20learn%20hierarchical%20discrete%20tokens%2C%0Aresulting%20in%20significantly%20reduced%20memory%20requirements%20and%20improved%0Ageneralization%20capabilities.%20By%20combining%20the%20GQT%20with%20token%20modulation%2C%20a%0ATransformer%20encoder%20achieves%20state-of-the-art%20performance%20on%2016%20out%20of%2018%0Abenchmarks%2C%20including%20large-scale%20homophilic%20and%20heterophilic%20datasets.%20The%0Acode%20is%20available%20at%3A%20https%3A//github.com/limei0307/graph-tokenizer%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13798v1&entry.124074799=Read"},
{"title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language\n  Models for Strategic Reasoning", "author": "Yadong Zhang and Shaoguang Mao and Tao Ge and Xun Wang and Yan Xia and Man Lan and Furu Wei", "abstract": "  Strategic reasoning is a complex yet essential capability for intelligent\nagents. It requires Large Language Model (LLM) agents to adapt their strategies\ndynamically in multi-agent environments. Unlike static reasoning tasks, success\nin these contexts depends on anticipating other agents' beliefs and actions\nwhile continuously adjusting strategies to achieve individual goals. LLMs and\nLLM agents often struggle with strategic reasoning due to the absence of a\nreasoning framework that enables them to dynamically infer others' perspectives\nand adapt to changing environments. Inspired by the Level-K framework from game\ntheory and behavioral economics, which extends reasoning from simple reactions\nto structured strategic depth, we propose a novel framework: \"K-Level Reasoning\nwith Large Language Models (K-R).\" This framework employs recursive mechanisms\nto enable LLMs to achieve varying levels of strategic depth, allowing agents to\nform higher order beliefs - beliefs about others' beliefs. We validate this\nframework through rigorous testing on four testbeds: two classical game theory\nproblems and two social intelligence tasks. The results demonstrate the\nadvantages of K-R in strategic reasoning. Our work presents the first recursive\nimplementation of strategic depth in large language models (LLMs). It\nestablishes a foundation for future research into theory of mind and strategic\nreasoning in LLMs.\n", "link": "http://arxiv.org/abs/2402.01521v2", "date": "2024-10-17", "relevancy": 2.0605, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20K-Level%20Reasoning%3A%20Establishing%20Higher%20Order%20Beliefs%20in%20Large%20Language%0A%20%20Models%20for%20Strategic%20Reasoning&body=Title%3A%20K-Level%20Reasoning%3A%20Establishing%20Higher%20Order%20Beliefs%20in%20Large%20Language%0A%20%20Models%20for%20Strategic%20Reasoning%0AAuthor%3A%20Yadong%20Zhang%20and%20Shaoguang%20Mao%20and%20Tao%20Ge%20and%20Xun%20Wang%20and%20Yan%20Xia%20and%20Man%20Lan%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Strategic%20reasoning%20is%20a%20complex%20yet%20essential%20capability%20for%20intelligent%0Aagents.%20It%20requires%20Large%20Language%20Model%20%28LLM%29%20agents%20to%20adapt%20their%20strategies%0Adynamically%20in%20multi-agent%20environments.%20Unlike%20static%20reasoning%20tasks%2C%20success%0Ain%20these%20contexts%20depends%20on%20anticipating%20other%20agents%27%20beliefs%20and%20actions%0Awhile%20continuously%20adjusting%20strategies%20to%20achieve%20individual%20goals.%20LLMs%20and%0ALLM%20agents%20often%20struggle%20with%20strategic%20reasoning%20due%20to%20the%20absence%20of%20a%0Areasoning%20framework%20that%20enables%20them%20to%20dynamically%20infer%20others%27%20perspectives%0Aand%20adapt%20to%20changing%20environments.%20Inspired%20by%20the%20Level-K%20framework%20from%20game%0Atheory%20and%20behavioral%20economics%2C%20which%20extends%20reasoning%20from%20simple%20reactions%0Ato%20structured%20strategic%20depth%2C%20we%20propose%20a%20novel%20framework%3A%20%22K-Level%20Reasoning%0Awith%20Large%20Language%20Models%20%28K-R%29.%22%20This%20framework%20employs%20recursive%20mechanisms%0Ato%20enable%20LLMs%20to%20achieve%20varying%20levels%20of%20strategic%20depth%2C%20allowing%20agents%20to%0Aform%20higher%20order%20beliefs%20-%20beliefs%20about%20others%27%20beliefs.%20We%20validate%20this%0Aframework%20through%20rigorous%20testing%20on%20four%20testbeds%3A%20two%20classical%20game%20theory%0Aproblems%20and%20two%20social%20intelligence%20tasks.%20The%20results%20demonstrate%20the%0Aadvantages%20of%20K-R%20in%20strategic%20reasoning.%20Our%20work%20presents%20the%20first%20recursive%0Aimplementation%20of%20strategic%20depth%20in%20large%20language%20models%20%28LLMs%29.%20It%0Aestablishes%20a%20foundation%20for%20future%20research%20into%20theory%20of%20mind%20and%20strategic%0Areasoning%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DK-Level%2520Reasoning%253A%2520Establishing%2520Higher%2520Order%2520Beliefs%2520in%2520Large%2520Language%250A%2520%2520Models%2520for%2520Strategic%2520Reasoning%26entry.906535625%3DYadong%2520Zhang%2520and%2520Shaoguang%2520Mao%2520and%2520Tao%2520Ge%2520and%2520Xun%2520Wang%2520and%2520Yan%2520Xia%2520and%2520Man%2520Lan%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Strategic%2520reasoning%2520is%2520a%2520complex%2520yet%2520essential%2520capability%2520for%2520intelligent%250Aagents.%2520It%2520requires%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520to%2520adapt%2520their%2520strategies%250Adynamically%2520in%2520multi-agent%2520environments.%2520Unlike%2520static%2520reasoning%2520tasks%252C%2520success%250Ain%2520these%2520contexts%2520depends%2520on%2520anticipating%2520other%2520agents%2527%2520beliefs%2520and%2520actions%250Awhile%2520continuously%2520adjusting%2520strategies%2520to%2520achieve%2520individual%2520goals.%2520LLMs%2520and%250ALLM%2520agents%2520often%2520struggle%2520with%2520strategic%2520reasoning%2520due%2520to%2520the%2520absence%2520of%2520a%250Areasoning%2520framework%2520that%2520enables%2520them%2520to%2520dynamically%2520infer%2520others%2527%2520perspectives%250Aand%2520adapt%2520to%2520changing%2520environments.%2520Inspired%2520by%2520the%2520Level-K%2520framework%2520from%2520game%250Atheory%2520and%2520behavioral%2520economics%252C%2520which%2520extends%2520reasoning%2520from%2520simple%2520reactions%250Ato%2520structured%2520strategic%2520depth%252C%2520we%2520propose%2520a%2520novel%2520framework%253A%2520%2522K-Level%2520Reasoning%250Awith%2520Large%2520Language%2520Models%2520%2528K-R%2529.%2522%2520This%2520framework%2520employs%2520recursive%2520mechanisms%250Ato%2520enable%2520LLMs%2520to%2520achieve%2520varying%2520levels%2520of%2520strategic%2520depth%252C%2520allowing%2520agents%2520to%250Aform%2520higher%2520order%2520beliefs%2520-%2520beliefs%2520about%2520others%2527%2520beliefs.%2520We%2520validate%2520this%250Aframework%2520through%2520rigorous%2520testing%2520on%2520four%2520testbeds%253A%2520two%2520classical%2520game%2520theory%250Aproblems%2520and%2520two%2520social%2520intelligence%2520tasks.%2520The%2520results%2520demonstrate%2520the%250Aadvantages%2520of%2520K-R%2520in%2520strategic%2520reasoning.%2520Our%2520work%2520presents%2520the%2520first%2520recursive%250Aimplementation%2520of%2520strategic%2520depth%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520It%250Aestablishes%2520a%2520foundation%2520for%2520future%2520research%2520into%2520theory%2520of%2520mind%2520and%2520strategic%250Areasoning%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=K-Level%20Reasoning%3A%20Establishing%20Higher%20Order%20Beliefs%20in%20Large%20Language%0A%20%20Models%20for%20Strategic%20Reasoning&entry.906535625=Yadong%20Zhang%20and%20Shaoguang%20Mao%20and%20Tao%20Ge%20and%20Xun%20Wang%20and%20Yan%20Xia%20and%20Man%20Lan%20and%20Furu%20Wei&entry.1292438233=%20%20Strategic%20reasoning%20is%20a%20complex%20yet%20essential%20capability%20for%20intelligent%0Aagents.%20It%20requires%20Large%20Language%20Model%20%28LLM%29%20agents%20to%20adapt%20their%20strategies%0Adynamically%20in%20multi-agent%20environments.%20Unlike%20static%20reasoning%20tasks%2C%20success%0Ain%20these%20contexts%20depends%20on%20anticipating%20other%20agents%27%20beliefs%20and%20actions%0Awhile%20continuously%20adjusting%20strategies%20to%20achieve%20individual%20goals.%20LLMs%20and%0ALLM%20agents%20often%20struggle%20with%20strategic%20reasoning%20due%20to%20the%20absence%20of%20a%0Areasoning%20framework%20that%20enables%20them%20to%20dynamically%20infer%20others%27%20perspectives%0Aand%20adapt%20to%20changing%20environments.%20Inspired%20by%20the%20Level-K%20framework%20from%20game%0Atheory%20and%20behavioral%20economics%2C%20which%20extends%20reasoning%20from%20simple%20reactions%0Ato%20structured%20strategic%20depth%2C%20we%20propose%20a%20novel%20framework%3A%20%22K-Level%20Reasoning%0Awith%20Large%20Language%20Models%20%28K-R%29.%22%20This%20framework%20employs%20recursive%20mechanisms%0Ato%20enable%20LLMs%20to%20achieve%20varying%20levels%20of%20strategic%20depth%2C%20allowing%20agents%20to%0Aform%20higher%20order%20beliefs%20-%20beliefs%20about%20others%27%20beliefs.%20We%20validate%20this%0Aframework%20through%20rigorous%20testing%20on%20four%20testbeds%3A%20two%20classical%20game%20theory%0Aproblems%20and%20two%20social%20intelligence%20tasks.%20The%20results%20demonstrate%20the%0Aadvantages%20of%20K-R%20in%20strategic%20reasoning.%20Our%20work%20presents%20the%20first%20recursive%0Aimplementation%20of%20strategic%20depth%20in%20large%20language%20models%20%28LLMs%29.%20It%0Aestablishes%20a%20foundation%20for%20future%20research%20into%20theory%20of%20mind%20and%20strategic%0Areasoning%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01521v2&entry.124074799=Read"},
{"title": "AutoAL: Automated Active Learning with Differentiable Query Strategy\n  Search", "author": "Yifeng Wang and Xueying Zhan and Siyu Huang", "abstract": "  As deep learning continues to evolve, the need for data efficiency becomes\nincreasingly important. Considering labeling large datasets is both\ntime-consuming and expensive, active learning (AL) provides a promising\nsolution to this challenge by iteratively selecting the most informative\nsubsets of examples to train deep neural networks, thereby reducing the\nlabeling cost. However, the effectiveness of different AL algorithms can vary\nsignificantly across data scenarios, and determining which AL algorithm best\nfits a given task remains a challenging problem. This work presents the first\ndifferentiable AL strategy search method, named AutoAL, which is designed on\ntop of existing AL sampling strategies. AutoAL consists of two neural nets,\nnamed SearchNet and FitNet, which are optimized concurrently under a\ndifferentiable bi-level optimization framework. For any given task, SearchNet\nand FitNet are iteratively co-optimized using the labeled data, learning how\nwell a set of candidate AL algorithms perform on that task. With the optimal AL\nstrategies identified, SearchNet selects a small subset from the unlabeled pool\nfor querying their annotations, enabling efficient training of the task model.\nExperimental results demonstrate that AutoAL consistently achieves superior\naccuracy compared to all candidate AL algorithms and other selective AL\napproaches, showcasing its potential for adapting and integrating multiple\nexisting AL methods across diverse tasks and domains. Code will be available\nat: https://github.com/haizailache999/AutoAL.\n", "link": "http://arxiv.org/abs/2410.13853v1", "date": "2024-10-17", "relevancy": 2.0593, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5334}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5173}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoAL%3A%20Automated%20Active%20Learning%20with%20Differentiable%20Query%20Strategy%0A%20%20Search&body=Title%3A%20AutoAL%3A%20Automated%20Active%20Learning%20with%20Differentiable%20Query%20Strategy%0A%20%20Search%0AAuthor%3A%20Yifeng%20Wang%20and%20Xueying%20Zhan%20and%20Siyu%20Huang%0AAbstract%3A%20%20%20As%20deep%20learning%20continues%20to%20evolve%2C%20the%20need%20for%20data%20efficiency%20becomes%0Aincreasingly%20important.%20Considering%20labeling%20large%20datasets%20is%20both%0Atime-consuming%20and%20expensive%2C%20active%20learning%20%28AL%29%20provides%20a%20promising%0Asolution%20to%20this%20challenge%20by%20iteratively%20selecting%20the%20most%20informative%0Asubsets%20of%20examples%20to%20train%20deep%20neural%20networks%2C%20thereby%20reducing%20the%0Alabeling%20cost.%20However%2C%20the%20effectiveness%20of%20different%20AL%20algorithms%20can%20vary%0Asignificantly%20across%20data%20scenarios%2C%20and%20determining%20which%20AL%20algorithm%20best%0Afits%20a%20given%20task%20remains%20a%20challenging%20problem.%20This%20work%20presents%20the%20first%0Adifferentiable%20AL%20strategy%20search%20method%2C%20named%20AutoAL%2C%20which%20is%20designed%20on%0Atop%20of%20existing%20AL%20sampling%20strategies.%20AutoAL%20consists%20of%20two%20neural%20nets%2C%0Anamed%20SearchNet%20and%20FitNet%2C%20which%20are%20optimized%20concurrently%20under%20a%0Adifferentiable%20bi-level%20optimization%20framework.%20For%20any%20given%20task%2C%20SearchNet%0Aand%20FitNet%20are%20iteratively%20co-optimized%20using%20the%20labeled%20data%2C%20learning%20how%0Awell%20a%20set%20of%20candidate%20AL%20algorithms%20perform%20on%20that%20task.%20With%20the%20optimal%20AL%0Astrategies%20identified%2C%20SearchNet%20selects%20a%20small%20subset%20from%20the%20unlabeled%20pool%0Afor%20querying%20their%20annotations%2C%20enabling%20efficient%20training%20of%20the%20task%20model.%0AExperimental%20results%20demonstrate%20that%20AutoAL%20consistently%20achieves%20superior%0Aaccuracy%20compared%20to%20all%20candidate%20AL%20algorithms%20and%20other%20selective%20AL%0Aapproaches%2C%20showcasing%20its%20potential%20for%20adapting%20and%20integrating%20multiple%0Aexisting%20AL%20methods%20across%20diverse%20tasks%20and%20domains.%20Code%20will%20be%20available%0Aat%3A%20https%3A//github.com/haizailache999/AutoAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoAL%253A%2520Automated%2520Active%2520Learning%2520with%2520Differentiable%2520Query%2520Strategy%250A%2520%2520Search%26entry.906535625%3DYifeng%2520Wang%2520and%2520Xueying%2520Zhan%2520and%2520Siyu%2520Huang%26entry.1292438233%3D%2520%2520As%2520deep%2520learning%2520continues%2520to%2520evolve%252C%2520the%2520need%2520for%2520data%2520efficiency%2520becomes%250Aincreasingly%2520important.%2520Considering%2520labeling%2520large%2520datasets%2520is%2520both%250Atime-consuming%2520and%2520expensive%252C%2520active%2520learning%2520%2528AL%2529%2520provides%2520a%2520promising%250Asolution%2520to%2520this%2520challenge%2520by%2520iteratively%2520selecting%2520the%2520most%2520informative%250Asubsets%2520of%2520examples%2520to%2520train%2520deep%2520neural%2520networks%252C%2520thereby%2520reducing%2520the%250Alabeling%2520cost.%2520However%252C%2520the%2520effectiveness%2520of%2520different%2520AL%2520algorithms%2520can%2520vary%250Asignificantly%2520across%2520data%2520scenarios%252C%2520and%2520determining%2520which%2520AL%2520algorithm%2520best%250Afits%2520a%2520given%2520task%2520remains%2520a%2520challenging%2520problem.%2520This%2520work%2520presents%2520the%2520first%250Adifferentiable%2520AL%2520strategy%2520search%2520method%252C%2520named%2520AutoAL%252C%2520which%2520is%2520designed%2520on%250Atop%2520of%2520existing%2520AL%2520sampling%2520strategies.%2520AutoAL%2520consists%2520of%2520two%2520neural%2520nets%252C%250Anamed%2520SearchNet%2520and%2520FitNet%252C%2520which%2520are%2520optimized%2520concurrently%2520under%2520a%250Adifferentiable%2520bi-level%2520optimization%2520framework.%2520For%2520any%2520given%2520task%252C%2520SearchNet%250Aand%2520FitNet%2520are%2520iteratively%2520co-optimized%2520using%2520the%2520labeled%2520data%252C%2520learning%2520how%250Awell%2520a%2520set%2520of%2520candidate%2520AL%2520algorithms%2520perform%2520on%2520that%2520task.%2520With%2520the%2520optimal%2520AL%250Astrategies%2520identified%252C%2520SearchNet%2520selects%2520a%2520small%2520subset%2520from%2520the%2520unlabeled%2520pool%250Afor%2520querying%2520their%2520annotations%252C%2520enabling%2520efficient%2520training%2520of%2520the%2520task%2520model.%250AExperimental%2520results%2520demonstrate%2520that%2520AutoAL%2520consistently%2520achieves%2520superior%250Aaccuracy%2520compared%2520to%2520all%2520candidate%2520AL%2520algorithms%2520and%2520other%2520selective%2520AL%250Aapproaches%252C%2520showcasing%2520its%2520potential%2520for%2520adapting%2520and%2520integrating%2520multiple%250Aexisting%2520AL%2520methods%2520across%2520diverse%2520tasks%2520and%2520domains.%2520Code%2520will%2520be%2520available%250Aat%253A%2520https%253A//github.com/haizailache999/AutoAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoAL%3A%20Automated%20Active%20Learning%20with%20Differentiable%20Query%20Strategy%0A%20%20Search&entry.906535625=Yifeng%20Wang%20and%20Xueying%20Zhan%20and%20Siyu%20Huang&entry.1292438233=%20%20As%20deep%20learning%20continues%20to%20evolve%2C%20the%20need%20for%20data%20efficiency%20becomes%0Aincreasingly%20important.%20Considering%20labeling%20large%20datasets%20is%20both%0Atime-consuming%20and%20expensive%2C%20active%20learning%20%28AL%29%20provides%20a%20promising%0Asolution%20to%20this%20challenge%20by%20iteratively%20selecting%20the%20most%20informative%0Asubsets%20of%20examples%20to%20train%20deep%20neural%20networks%2C%20thereby%20reducing%20the%0Alabeling%20cost.%20However%2C%20the%20effectiveness%20of%20different%20AL%20algorithms%20can%20vary%0Asignificantly%20across%20data%20scenarios%2C%20and%20determining%20which%20AL%20algorithm%20best%0Afits%20a%20given%20task%20remains%20a%20challenging%20problem.%20This%20work%20presents%20the%20first%0Adifferentiable%20AL%20strategy%20search%20method%2C%20named%20AutoAL%2C%20which%20is%20designed%20on%0Atop%20of%20existing%20AL%20sampling%20strategies.%20AutoAL%20consists%20of%20two%20neural%20nets%2C%0Anamed%20SearchNet%20and%20FitNet%2C%20which%20are%20optimized%20concurrently%20under%20a%0Adifferentiable%20bi-level%20optimization%20framework.%20For%20any%20given%20task%2C%20SearchNet%0Aand%20FitNet%20are%20iteratively%20co-optimized%20using%20the%20labeled%20data%2C%20learning%20how%0Awell%20a%20set%20of%20candidate%20AL%20algorithms%20perform%20on%20that%20task.%20With%20the%20optimal%20AL%0Astrategies%20identified%2C%20SearchNet%20selects%20a%20small%20subset%20from%20the%20unlabeled%20pool%0Afor%20querying%20their%20annotations%2C%20enabling%20efficient%20training%20of%20the%20task%20model.%0AExperimental%20results%20demonstrate%20that%20AutoAL%20consistently%20achieves%20superior%0Aaccuracy%20compared%20to%20all%20candidate%20AL%20algorithms%20and%20other%20selective%20AL%0Aapproaches%2C%20showcasing%20its%20potential%20for%20adapting%20and%20integrating%20multiple%0Aexisting%20AL%20methods%20across%20diverse%20tasks%20and%20domains.%20Code%20will%20be%20available%0Aat%3A%20https%3A//github.com/haizailache999/AutoAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13853v1&entry.124074799=Read"},
{"title": "Reducing the Transformer Architecture to a Minimum", "author": "Bernhard Bermeitinger and Tomas Hrycej and Massimo Pavone and Julianus Kath and Siegfried Handschuh", "abstract": "  Transformers are a widespread and successful model architecture, particularly\nin Natural Language Processing (NLP) and Computer Vision (CV). The essential\ninnovation of this architecture is the Attention Mechanism, which solves the\nproblem of extracting relevant context information from long sequences in NLP\nand realistic scenes in CV. A classical neural network component, a Multi-Layer\nPerceptron (MLP), complements the attention mechanism. Its necessity is\nfrequently justified by its capability of modeling nonlinear relationships.\nHowever, the attention mechanism itself is nonlinear through its internal use\nof similarity measures. A possible hypothesis is that this nonlinearity is\nsufficient for modeling typical application problems. As the MLPs usually\ncontain the most trainable parameters of the whole model, their omission would\nsubstantially reduce the parameter set size. Further components can also be\nreorganized to reduce the number of parameters. Under some conditions, query\nand key matrices can be collapsed into a single matrix of the same size. The\nsame is true about value and projection matrices, which can also be omitted\nwithout eliminating the substance of the attention mechanism. Initially, the\nsimilarity measure was defined asymmetrically, with peculiar properties such as\nthat a token is possibly dissimilar to itself. A possible symmetric definition\nrequires only half of the parameters. We have laid the groundwork by testing\nwidespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that\nsimplified transformer architectures (a) without MLP, (b) with collapsed\nmatrices, and (c) symmetric similarity matrices exhibit similar performance as\nthe original architecture, saving up to 90% of parameters without hurting the\nclassification performance.\n", "link": "http://arxiv.org/abs/2410.13732v1", "date": "2024-10-17", "relevancy": 2.0559, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5495}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20the%20Transformer%20Architecture%20to%20a%20Minimum&body=Title%3A%20Reducing%20the%20Transformer%20Architecture%20to%20a%20Minimum%0AAuthor%3A%20Bernhard%20Bermeitinger%20and%20Tomas%20Hrycej%20and%20Massimo%20Pavone%20and%20Julianus%20Kath%20and%20Siegfried%20Handschuh%0AAbstract%3A%20%20%20Transformers%20are%20a%20widespread%20and%20successful%20model%20architecture%2C%20particularly%0Ain%20Natural%20Language%20Processing%20%28NLP%29%20and%20Computer%20Vision%20%28CV%29.%20The%20essential%0Ainnovation%20of%20this%20architecture%20is%20the%20Attention%20Mechanism%2C%20which%20solves%20the%0Aproblem%20of%20extracting%20relevant%20context%20information%20from%20long%20sequences%20in%20NLP%0Aand%20realistic%20scenes%20in%20CV.%20A%20classical%20neural%20network%20component%2C%20a%20Multi-Layer%0APerceptron%20%28MLP%29%2C%20complements%20the%20attention%20mechanism.%20Its%20necessity%20is%0Afrequently%20justified%20by%20its%20capability%20of%20modeling%20nonlinear%20relationships.%0AHowever%2C%20the%20attention%20mechanism%20itself%20is%20nonlinear%20through%20its%20internal%20use%0Aof%20similarity%20measures.%20A%20possible%20hypothesis%20is%20that%20this%20nonlinearity%20is%0Asufficient%20for%20modeling%20typical%20application%20problems.%20As%20the%20MLPs%20usually%0Acontain%20the%20most%20trainable%20parameters%20of%20the%20whole%20model%2C%20their%20omission%20would%0Asubstantially%20reduce%20the%20parameter%20set%20size.%20Further%20components%20can%20also%20be%0Areorganized%20to%20reduce%20the%20number%20of%20parameters.%20Under%20some%20conditions%2C%20query%0Aand%20key%20matrices%20can%20be%20collapsed%20into%20a%20single%20matrix%20of%20the%20same%20size.%20The%0Asame%20is%20true%20about%20value%20and%20projection%20matrices%2C%20which%20can%20also%20be%20omitted%0Awithout%20eliminating%20the%20substance%20of%20the%20attention%20mechanism.%20Initially%2C%20the%0Asimilarity%20measure%20was%20defined%20asymmetrically%2C%20with%20peculiar%20properties%20such%20as%0Athat%20a%20token%20is%20possibly%20dissimilar%20to%20itself.%20A%20possible%20symmetric%20definition%0Arequires%20only%20half%20of%20the%20parameters.%20We%20have%20laid%20the%20groundwork%20by%20testing%0Awidespread%20CV%20benchmarks%3A%20MNIST%20and%20CIFAR-10.%20The%20tests%20have%20shown%20that%0Asimplified%20transformer%20architectures%20%28a%29%20without%20MLP%2C%20%28b%29%20with%20collapsed%0Amatrices%2C%20and%20%28c%29%20symmetric%20similarity%20matrices%20exhibit%20similar%20performance%20as%0Athe%20original%20architecture%2C%20saving%20up%20to%2090%25%20of%20parameters%20without%20hurting%20the%0Aclassification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520the%2520Transformer%2520Architecture%2520to%2520a%2520Minimum%26entry.906535625%3DBernhard%2520Bermeitinger%2520and%2520Tomas%2520Hrycej%2520and%2520Massimo%2520Pavone%2520and%2520Julianus%2520Kath%2520and%2520Siegfried%2520Handschuh%26entry.1292438233%3D%2520%2520Transformers%2520are%2520a%2520widespread%2520and%2520successful%2520model%2520architecture%252C%2520particularly%250Ain%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520and%2520Computer%2520Vision%2520%2528CV%2529.%2520The%2520essential%250Ainnovation%2520of%2520this%2520architecture%2520is%2520the%2520Attention%2520Mechanism%252C%2520which%2520solves%2520the%250Aproblem%2520of%2520extracting%2520relevant%2520context%2520information%2520from%2520long%2520sequences%2520in%2520NLP%250Aand%2520realistic%2520scenes%2520in%2520CV.%2520A%2520classical%2520neural%2520network%2520component%252C%2520a%2520Multi-Layer%250APerceptron%2520%2528MLP%2529%252C%2520complements%2520the%2520attention%2520mechanism.%2520Its%2520necessity%2520is%250Afrequently%2520justified%2520by%2520its%2520capability%2520of%2520modeling%2520nonlinear%2520relationships.%250AHowever%252C%2520the%2520attention%2520mechanism%2520itself%2520is%2520nonlinear%2520through%2520its%2520internal%2520use%250Aof%2520similarity%2520measures.%2520A%2520possible%2520hypothesis%2520is%2520that%2520this%2520nonlinearity%2520is%250Asufficient%2520for%2520modeling%2520typical%2520application%2520problems.%2520As%2520the%2520MLPs%2520usually%250Acontain%2520the%2520most%2520trainable%2520parameters%2520of%2520the%2520whole%2520model%252C%2520their%2520omission%2520would%250Asubstantially%2520reduce%2520the%2520parameter%2520set%2520size.%2520Further%2520components%2520can%2520also%2520be%250Areorganized%2520to%2520reduce%2520the%2520number%2520of%2520parameters.%2520Under%2520some%2520conditions%252C%2520query%250Aand%2520key%2520matrices%2520can%2520be%2520collapsed%2520into%2520a%2520single%2520matrix%2520of%2520the%2520same%2520size.%2520The%250Asame%2520is%2520true%2520about%2520value%2520and%2520projection%2520matrices%252C%2520which%2520can%2520also%2520be%2520omitted%250Awithout%2520eliminating%2520the%2520substance%2520of%2520the%2520attention%2520mechanism.%2520Initially%252C%2520the%250Asimilarity%2520measure%2520was%2520defined%2520asymmetrically%252C%2520with%2520peculiar%2520properties%2520such%2520as%250Athat%2520a%2520token%2520is%2520possibly%2520dissimilar%2520to%2520itself.%2520A%2520possible%2520symmetric%2520definition%250Arequires%2520only%2520half%2520of%2520the%2520parameters.%2520We%2520have%2520laid%2520the%2520groundwork%2520by%2520testing%250Awidespread%2520CV%2520benchmarks%253A%2520MNIST%2520and%2520CIFAR-10.%2520The%2520tests%2520have%2520shown%2520that%250Asimplified%2520transformer%2520architectures%2520%2528a%2529%2520without%2520MLP%252C%2520%2528b%2529%2520with%2520collapsed%250Amatrices%252C%2520and%2520%2528c%2529%2520symmetric%2520similarity%2520matrices%2520exhibit%2520similar%2520performance%2520as%250Athe%2520original%2520architecture%252C%2520saving%2520up%2520to%252090%2525%2520of%2520parameters%2520without%2520hurting%2520the%250Aclassification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20the%20Transformer%20Architecture%20to%20a%20Minimum&entry.906535625=Bernhard%20Bermeitinger%20and%20Tomas%20Hrycej%20and%20Massimo%20Pavone%20and%20Julianus%20Kath%20and%20Siegfried%20Handschuh&entry.1292438233=%20%20Transformers%20are%20a%20widespread%20and%20successful%20model%20architecture%2C%20particularly%0Ain%20Natural%20Language%20Processing%20%28NLP%29%20and%20Computer%20Vision%20%28CV%29.%20The%20essential%0Ainnovation%20of%20this%20architecture%20is%20the%20Attention%20Mechanism%2C%20which%20solves%20the%0Aproblem%20of%20extracting%20relevant%20context%20information%20from%20long%20sequences%20in%20NLP%0Aand%20realistic%20scenes%20in%20CV.%20A%20classical%20neural%20network%20component%2C%20a%20Multi-Layer%0APerceptron%20%28MLP%29%2C%20complements%20the%20attention%20mechanism.%20Its%20necessity%20is%0Afrequently%20justified%20by%20its%20capability%20of%20modeling%20nonlinear%20relationships.%0AHowever%2C%20the%20attention%20mechanism%20itself%20is%20nonlinear%20through%20its%20internal%20use%0Aof%20similarity%20measures.%20A%20possible%20hypothesis%20is%20that%20this%20nonlinearity%20is%0Asufficient%20for%20modeling%20typical%20application%20problems.%20As%20the%20MLPs%20usually%0Acontain%20the%20most%20trainable%20parameters%20of%20the%20whole%20model%2C%20their%20omission%20would%0Asubstantially%20reduce%20the%20parameter%20set%20size.%20Further%20components%20can%20also%20be%0Areorganized%20to%20reduce%20the%20number%20of%20parameters.%20Under%20some%20conditions%2C%20query%0Aand%20key%20matrices%20can%20be%20collapsed%20into%20a%20single%20matrix%20of%20the%20same%20size.%20The%0Asame%20is%20true%20about%20value%20and%20projection%20matrices%2C%20which%20can%20also%20be%20omitted%0Awithout%20eliminating%20the%20substance%20of%20the%20attention%20mechanism.%20Initially%2C%20the%0Asimilarity%20measure%20was%20defined%20asymmetrically%2C%20with%20peculiar%20properties%20such%20as%0Athat%20a%20token%20is%20possibly%20dissimilar%20to%20itself.%20A%20possible%20symmetric%20definition%0Arequires%20only%20half%20of%20the%20parameters.%20We%20have%20laid%20the%20groundwork%20by%20testing%0Awidespread%20CV%20benchmarks%3A%20MNIST%20and%20CIFAR-10.%20The%20tests%20have%20shown%20that%0Asimplified%20transformer%20architectures%20%28a%29%20without%20MLP%2C%20%28b%29%20with%20collapsed%0Amatrices%2C%20and%20%28c%29%20symmetric%20similarity%20matrices%20exhibit%20similar%20performance%20as%0Athe%20original%20architecture%2C%20saving%20up%20to%2090%25%20of%20parameters%20without%20hurting%20the%0Aclassification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13732v1&entry.124074799=Read"},
{"title": "Retrospective Learning from Interactions", "author": "Zizhao Chen and Mustafa Omer Gul and Yiwei Chen and Gloria Geng and Anne Wu and Yoav Artzi", "abstract": "  Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.\n", "link": "http://arxiv.org/abs/2410.13852v1", "date": "2024-10-17", "relevancy": 2.0497, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrospective%20Learning%20from%20Interactions&body=Title%3A%20Retrospective%20Learning%20from%20Interactions%0AAuthor%3A%20Zizhao%20Chen%20and%20Mustafa%20Omer%20Gul%20and%20Yiwei%20Chen%20and%20Gloria%20Geng%20and%20Anne%20Wu%20and%20Yoav%20Artzi%0AAbstract%3A%20%20%20Multi-turn%20interactions%20between%20large%20language%20models%20%28LLMs%29%20and%20users%0Anaturally%20include%20implicit%20feedback%20signals.%20If%20an%20LLM%20responds%20in%20an%0Aunexpected%20way%20to%20an%20instruction%2C%20the%20user%20is%20likely%20to%20signal%20it%20by%20rephrasing%0Athe%20request%2C%20expressing%20frustration%2C%20or%20pivoting%20to%20an%20alternative%20task.%20Such%0Asignals%20are%20task-independent%20and%20occupy%20a%20relatively%20constrained%20subspace%20of%0Alanguage%2C%20allowing%20the%20LLM%20to%20identify%20them%20even%20if%20it%20fails%20on%20the%20actual%0Atask.%20This%20creates%20an%20avenue%20for%20continually%20learning%20from%20interactions%20without%0Aadditional%20annotations.%20We%20introduce%20ReSpect%2C%20a%20method%20to%20learn%20from%20such%0Asignals%20in%20past%20interactions%20via%20retrospection.%20We%20deploy%20ReSpect%20in%20a%20new%0Amultimodal%20interaction%20scenario%2C%20where%20humans%20instruct%20an%20LLM%20to%20solve%20an%0Aabstract%20reasoning%20task%20with%20a%20combinatorial%20solution%20space.%20Through%20thousands%0Aof%20interactions%20with%20humans%2C%20we%20show%20how%20ReSpect%20gradually%20improves%20task%0Acompletion%20rate%20from%2031%25%20to%2082%25%2C%20all%20without%20any%20external%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrospective%2520Learning%2520from%2520Interactions%26entry.906535625%3DZizhao%2520Chen%2520and%2520Mustafa%2520Omer%2520Gul%2520and%2520Yiwei%2520Chen%2520and%2520Gloria%2520Geng%2520and%2520Anne%2520Wu%2520and%2520Yoav%2520Artzi%26entry.1292438233%3D%2520%2520Multi-turn%2520interactions%2520between%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520users%250Anaturally%2520include%2520implicit%2520feedback%2520signals.%2520If%2520an%2520LLM%2520responds%2520in%2520an%250Aunexpected%2520way%2520to%2520an%2520instruction%252C%2520the%2520user%2520is%2520likely%2520to%2520signal%2520it%2520by%2520rephrasing%250Athe%2520request%252C%2520expressing%2520frustration%252C%2520or%2520pivoting%2520to%2520an%2520alternative%2520task.%2520Such%250Asignals%2520are%2520task-independent%2520and%2520occupy%2520a%2520relatively%2520constrained%2520subspace%2520of%250Alanguage%252C%2520allowing%2520the%2520LLM%2520to%2520identify%2520them%2520even%2520if%2520it%2520fails%2520on%2520the%2520actual%250Atask.%2520This%2520creates%2520an%2520avenue%2520for%2520continually%2520learning%2520from%2520interactions%2520without%250Aadditional%2520annotations.%2520We%2520introduce%2520ReSpect%252C%2520a%2520method%2520to%2520learn%2520from%2520such%250Asignals%2520in%2520past%2520interactions%2520via%2520retrospection.%2520We%2520deploy%2520ReSpect%2520in%2520a%2520new%250Amultimodal%2520interaction%2520scenario%252C%2520where%2520humans%2520instruct%2520an%2520LLM%2520to%2520solve%2520an%250Aabstract%2520reasoning%2520task%2520with%2520a%2520combinatorial%2520solution%2520space.%2520Through%2520thousands%250Aof%2520interactions%2520with%2520humans%252C%2520we%2520show%2520how%2520ReSpect%2520gradually%2520improves%2520task%250Acompletion%2520rate%2520from%252031%2525%2520to%252082%2525%252C%2520all%2520without%2520any%2520external%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrospective%20Learning%20from%20Interactions&entry.906535625=Zizhao%20Chen%20and%20Mustafa%20Omer%20Gul%20and%20Yiwei%20Chen%20and%20Gloria%20Geng%20and%20Anne%20Wu%20and%20Yoav%20Artzi&entry.1292438233=%20%20Multi-turn%20interactions%20between%20large%20language%20models%20%28LLMs%29%20and%20users%0Anaturally%20include%20implicit%20feedback%20signals.%20If%20an%20LLM%20responds%20in%20an%0Aunexpected%20way%20to%20an%20instruction%2C%20the%20user%20is%20likely%20to%20signal%20it%20by%20rephrasing%0Athe%20request%2C%20expressing%20frustration%2C%20or%20pivoting%20to%20an%20alternative%20task.%20Such%0Asignals%20are%20task-independent%20and%20occupy%20a%20relatively%20constrained%20subspace%20of%0Alanguage%2C%20allowing%20the%20LLM%20to%20identify%20them%20even%20if%20it%20fails%20on%20the%20actual%0Atask.%20This%20creates%20an%20avenue%20for%20continually%20learning%20from%20interactions%20without%0Aadditional%20annotations.%20We%20introduce%20ReSpect%2C%20a%20method%20to%20learn%20from%20such%0Asignals%20in%20past%20interactions%20via%20retrospection.%20We%20deploy%20ReSpect%20in%20a%20new%0Amultimodal%20interaction%20scenario%2C%20where%20humans%20instruct%20an%20LLM%20to%20solve%20an%0Aabstract%20reasoning%20task%20with%20a%20combinatorial%20solution%20space.%20Through%20thousands%0Aof%20interactions%20with%20humans%2C%20we%20show%20how%20ReSpect%20gradually%20improves%20task%0Acompletion%20rate%20from%2031%25%20to%2082%25%2C%20all%20without%20any%20external%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13852v1&entry.124074799=Read"},
{"title": "Transformer-Based Approaches for Sensor-Based Human Activity\n  Recognition: Opportunities and Challenges", "author": "Clayton Souza Leite and Henry Mauranen and Aziza Zhanabatyrova and Yu Xiao", "abstract": "  Transformers have excelled in natural language processing and computer\nvision, paving their way to sensor-based Human Activity Recognition (HAR).\nPrevious studies show that transformers outperform their counterparts\nexclusively when they harness abundant data or employ compute-intensive\noptimization algorithms. However, neither of these scenarios is viable in\nsensor-based HAR due to the scarcity of data in this field and the frequent\nneed to perform training and inference on resource-constrained devices. Our\nextensive investigation into various implementations of transformer-based\nversus non-transformer-based HAR using wearable sensors, encompassing more than\n500 experiments, corroborates these concerns. We observe that transformer-based\nsolutions pose higher computational demands, consistently yield inferior\nperformance, and experience significant performance degradation when quantized\nto accommodate resource-constrained devices. Additionally, transformers\ndemonstrate lower robustness to adversarial attacks, posing a potential threat\nto user trust in HAR.\n", "link": "http://arxiv.org/abs/2410.13605v1", "date": "2024-10-17", "relevancy": 2.0374, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5211}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5198}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-Based%20Approaches%20for%20Sensor-Based%20Human%20Activity%0A%20%20Recognition%3A%20Opportunities%20and%20Challenges&body=Title%3A%20Transformer-Based%20Approaches%20for%20Sensor-Based%20Human%20Activity%0A%20%20Recognition%3A%20Opportunities%20and%20Challenges%0AAuthor%3A%20Clayton%20Souza%20Leite%20and%20Henry%20Mauranen%20and%20Aziza%20Zhanabatyrova%20and%20Yu%20Xiao%0AAbstract%3A%20%20%20Transformers%20have%20excelled%20in%20natural%20language%20processing%20and%20computer%0Avision%2C%20paving%20their%20way%20to%20sensor-based%20Human%20Activity%20Recognition%20%28HAR%29.%0APrevious%20studies%20show%20that%20transformers%20outperform%20their%20counterparts%0Aexclusively%20when%20they%20harness%20abundant%20data%20or%20employ%20compute-intensive%0Aoptimization%20algorithms.%20However%2C%20neither%20of%20these%20scenarios%20is%20viable%20in%0Asensor-based%20HAR%20due%20to%20the%20scarcity%20of%20data%20in%20this%20field%20and%20the%20frequent%0Aneed%20to%20perform%20training%20and%20inference%20on%20resource-constrained%20devices.%20Our%0Aextensive%20investigation%20into%20various%20implementations%20of%20transformer-based%0Aversus%20non-transformer-based%20HAR%20using%20wearable%20sensors%2C%20encompassing%20more%20than%0A500%20experiments%2C%20corroborates%20these%20concerns.%20We%20observe%20that%20transformer-based%0Asolutions%20pose%20higher%20computational%20demands%2C%20consistently%20yield%20inferior%0Aperformance%2C%20and%20experience%20significant%20performance%20degradation%20when%20quantized%0Ato%20accommodate%20resource-constrained%20devices.%20Additionally%2C%20transformers%0Ademonstrate%20lower%20robustness%20to%20adversarial%20attacks%2C%20posing%20a%20potential%20threat%0Ato%20user%20trust%20in%20HAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-Based%2520Approaches%2520for%2520Sensor-Based%2520Human%2520Activity%250A%2520%2520Recognition%253A%2520Opportunities%2520and%2520Challenges%26entry.906535625%3DClayton%2520Souza%2520Leite%2520and%2520Henry%2520Mauranen%2520and%2520Aziza%2520Zhanabatyrova%2520and%2520Yu%2520Xiao%26entry.1292438233%3D%2520%2520Transformers%2520have%2520excelled%2520in%2520natural%2520language%2520processing%2520and%2520computer%250Avision%252C%2520paving%2520their%2520way%2520to%2520sensor-based%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529.%250APrevious%2520studies%2520show%2520that%2520transformers%2520outperform%2520their%2520counterparts%250Aexclusively%2520when%2520they%2520harness%2520abundant%2520data%2520or%2520employ%2520compute-intensive%250Aoptimization%2520algorithms.%2520However%252C%2520neither%2520of%2520these%2520scenarios%2520is%2520viable%2520in%250Asensor-based%2520HAR%2520due%2520to%2520the%2520scarcity%2520of%2520data%2520in%2520this%2520field%2520and%2520the%2520frequent%250Aneed%2520to%2520perform%2520training%2520and%2520inference%2520on%2520resource-constrained%2520devices.%2520Our%250Aextensive%2520investigation%2520into%2520various%2520implementations%2520of%2520transformer-based%250Aversus%2520non-transformer-based%2520HAR%2520using%2520wearable%2520sensors%252C%2520encompassing%2520more%2520than%250A500%2520experiments%252C%2520corroborates%2520these%2520concerns.%2520We%2520observe%2520that%2520transformer-based%250Asolutions%2520pose%2520higher%2520computational%2520demands%252C%2520consistently%2520yield%2520inferior%250Aperformance%252C%2520and%2520experience%2520significant%2520performance%2520degradation%2520when%2520quantized%250Ato%2520accommodate%2520resource-constrained%2520devices.%2520Additionally%252C%2520transformers%250Ademonstrate%2520lower%2520robustness%2520to%2520adversarial%2520attacks%252C%2520posing%2520a%2520potential%2520threat%250Ato%2520user%2520trust%2520in%2520HAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-Based%20Approaches%20for%20Sensor-Based%20Human%20Activity%0A%20%20Recognition%3A%20Opportunities%20and%20Challenges&entry.906535625=Clayton%20Souza%20Leite%20and%20Henry%20Mauranen%20and%20Aziza%20Zhanabatyrova%20and%20Yu%20Xiao&entry.1292438233=%20%20Transformers%20have%20excelled%20in%20natural%20language%20processing%20and%20computer%0Avision%2C%20paving%20their%20way%20to%20sensor-based%20Human%20Activity%20Recognition%20%28HAR%29.%0APrevious%20studies%20show%20that%20transformers%20outperform%20their%20counterparts%0Aexclusively%20when%20they%20harness%20abundant%20data%20or%20employ%20compute-intensive%0Aoptimization%20algorithms.%20However%2C%20neither%20of%20these%20scenarios%20is%20viable%20in%0Asensor-based%20HAR%20due%20to%20the%20scarcity%20of%20data%20in%20this%20field%20and%20the%20frequent%0Aneed%20to%20perform%20training%20and%20inference%20on%20resource-constrained%20devices.%20Our%0Aextensive%20investigation%20into%20various%20implementations%20of%20transformer-based%0Aversus%20non-transformer-based%20HAR%20using%20wearable%20sensors%2C%20encompassing%20more%20than%0A500%20experiments%2C%20corroborates%20these%20concerns.%20We%20observe%20that%20transformer-based%0Asolutions%20pose%20higher%20computational%20demands%2C%20consistently%20yield%20inferior%0Aperformance%2C%20and%20experience%20significant%20performance%20degradation%20when%20quantized%0Ato%20accommodate%20resource-constrained%20devices.%20Additionally%2C%20transformers%0Ademonstrate%20lower%20robustness%20to%20adversarial%20attacks%2C%20posing%20a%20potential%20threat%0Ato%20user%20trust%20in%20HAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13605v1&entry.124074799=Read"},
{"title": "Large Language Models as Narrative-Driven Recommenders", "author": "Lukas Eberhard and Thorsten Ruprechter and Denis Helic", "abstract": "  Narrative-driven recommenders aim to provide personalized suggestions for\nuser requests expressed in free-form text such as \"I want to watch a thriller\nwith a mind-bending story, like Shutter Island.\" Although large language models\n(LLMs) have been shown to excel in processing general natural language queries,\ntheir effectiveness for handling such recommendation requests remains\nrelatively unexplored. To close this gap, we compare the performance of 38\nopen- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in\na movie recommendation setting. For this, we utilize a gold-standard,\ncrowdworker-annotated dataset of posts from reddit's movie suggestion community\nand employ various prompting strategies, including zero-shot, identity, and\nfew-shot prompting. Our findings demonstrate the ability of LLMs to generate\ncontextually relevant movie recommendations, significantly outperforming other\nstate-of-the-art approaches, such as doc2vec. While we find that closed-source\nand large-parameterized models generally perform best, medium-sized open-source\nmodels remain competitive, being only slightly outperformed by their more\ncomputationally expensive counterparts. Furthermore, we observe no significant\ndifferences across prompting strategies for most models, underscoring the\neffectiveness of simple approaches such as zero-shot prompting for\nnarrative-driven recommendations. Overall, this work offers valuable insights\nfor recommender system researchers as well as practitioners aiming to integrate\nLLMs into real-world recommendation tools.\n", "link": "http://arxiv.org/abs/2410.13604v1", "date": "2024-10-17", "relevancy": 2.0234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Narrative-Driven%20Recommenders&body=Title%3A%20Large%20Language%20Models%20as%20Narrative-Driven%20Recommenders%0AAuthor%3A%20Lukas%20Eberhard%20and%20Thorsten%20Ruprechter%20and%20Denis%20Helic%0AAbstract%3A%20%20%20Narrative-driven%20recommenders%20aim%20to%20provide%20personalized%20suggestions%20for%0Auser%20requests%20expressed%20in%20free-form%20text%20such%20as%20%22I%20want%20to%20watch%20a%20thriller%0Awith%20a%20mind-bending%20story%2C%20like%20Shutter%20Island.%22%20Although%20large%20language%20models%0A%28LLMs%29%20have%20been%20shown%20to%20excel%20in%20processing%20general%20natural%20language%20queries%2C%0Atheir%20effectiveness%20for%20handling%20such%20recommendation%20requests%20remains%0Arelatively%20unexplored.%20To%20close%20this%20gap%2C%20we%20compare%20the%20performance%20of%2038%0Aopen-%20and%20closed-source%20LLMs%20of%20various%20sizes%2C%20such%20as%20LLama%203.2%20and%20GPT-4o%2C%20in%0Aa%20movie%20recommendation%20setting.%20For%20this%2C%20we%20utilize%20a%20gold-standard%2C%0Acrowdworker-annotated%20dataset%20of%20posts%20from%20reddit%27s%20movie%20suggestion%20community%0Aand%20employ%20various%20prompting%20strategies%2C%20including%20zero-shot%2C%20identity%2C%20and%0Afew-shot%20prompting.%20Our%20findings%20demonstrate%20the%20ability%20of%20LLMs%20to%20generate%0Acontextually%20relevant%20movie%20recommendations%2C%20significantly%20outperforming%20other%0Astate-of-the-art%20approaches%2C%20such%20as%20doc2vec.%20While%20we%20find%20that%20closed-source%0Aand%20large-parameterized%20models%20generally%20perform%20best%2C%20medium-sized%20open-source%0Amodels%20remain%20competitive%2C%20being%20only%20slightly%20outperformed%20by%20their%20more%0Acomputationally%20expensive%20counterparts.%20Furthermore%2C%20we%20observe%20no%20significant%0Adifferences%20across%20prompting%20strategies%20for%20most%20models%2C%20underscoring%20the%0Aeffectiveness%20of%20simple%20approaches%20such%20as%20zero-shot%20prompting%20for%0Anarrative-driven%20recommendations.%20Overall%2C%20this%20work%20offers%20valuable%20insights%0Afor%20recommender%20system%20researchers%20as%20well%20as%20practitioners%20aiming%20to%20integrate%0ALLMs%20into%20real-world%20recommendation%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520as%2520Narrative-Driven%2520Recommenders%26entry.906535625%3DLukas%2520Eberhard%2520and%2520Thorsten%2520Ruprechter%2520and%2520Denis%2520Helic%26entry.1292438233%3D%2520%2520Narrative-driven%2520recommenders%2520aim%2520to%2520provide%2520personalized%2520suggestions%2520for%250Auser%2520requests%2520expressed%2520in%2520free-form%2520text%2520such%2520as%2520%2522I%2520want%2520to%2520watch%2520a%2520thriller%250Awith%2520a%2520mind-bending%2520story%252C%2520like%2520Shutter%2520Island.%2522%2520Although%2520large%2520language%2520models%250A%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520excel%2520in%2520processing%2520general%2520natural%2520language%2520queries%252C%250Atheir%2520effectiveness%2520for%2520handling%2520such%2520recommendation%2520requests%2520remains%250Arelatively%2520unexplored.%2520To%2520close%2520this%2520gap%252C%2520we%2520compare%2520the%2520performance%2520of%252038%250Aopen-%2520and%2520closed-source%2520LLMs%2520of%2520various%2520sizes%252C%2520such%2520as%2520LLama%25203.2%2520and%2520GPT-4o%252C%2520in%250Aa%2520movie%2520recommendation%2520setting.%2520For%2520this%252C%2520we%2520utilize%2520a%2520gold-standard%252C%250Acrowdworker-annotated%2520dataset%2520of%2520posts%2520from%2520reddit%2527s%2520movie%2520suggestion%2520community%250Aand%2520employ%2520various%2520prompting%2520strategies%252C%2520including%2520zero-shot%252C%2520identity%252C%2520and%250Afew-shot%2520prompting.%2520Our%2520findings%2520demonstrate%2520the%2520ability%2520of%2520LLMs%2520to%2520generate%250Acontextually%2520relevant%2520movie%2520recommendations%252C%2520significantly%2520outperforming%2520other%250Astate-of-the-art%2520approaches%252C%2520such%2520as%2520doc2vec.%2520While%2520we%2520find%2520that%2520closed-source%250Aand%2520large-parameterized%2520models%2520generally%2520perform%2520best%252C%2520medium-sized%2520open-source%250Amodels%2520remain%2520competitive%252C%2520being%2520only%2520slightly%2520outperformed%2520by%2520their%2520more%250Acomputationally%2520expensive%2520counterparts.%2520Furthermore%252C%2520we%2520observe%2520no%2520significant%250Adifferences%2520across%2520prompting%2520strategies%2520for%2520most%2520models%252C%2520underscoring%2520the%250Aeffectiveness%2520of%2520simple%2520approaches%2520such%2520as%2520zero-shot%2520prompting%2520for%250Anarrative-driven%2520recommendations.%2520Overall%252C%2520this%2520work%2520offers%2520valuable%2520insights%250Afor%2520recommender%2520system%2520researchers%2520as%2520well%2520as%2520practitioners%2520aiming%2520to%2520integrate%250ALLMs%2520into%2520real-world%2520recommendation%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Narrative-Driven%20Recommenders&entry.906535625=Lukas%20Eberhard%20and%20Thorsten%20Ruprechter%20and%20Denis%20Helic&entry.1292438233=%20%20Narrative-driven%20recommenders%20aim%20to%20provide%20personalized%20suggestions%20for%0Auser%20requests%20expressed%20in%20free-form%20text%20such%20as%20%22I%20want%20to%20watch%20a%20thriller%0Awith%20a%20mind-bending%20story%2C%20like%20Shutter%20Island.%22%20Although%20large%20language%20models%0A%28LLMs%29%20have%20been%20shown%20to%20excel%20in%20processing%20general%20natural%20language%20queries%2C%0Atheir%20effectiveness%20for%20handling%20such%20recommendation%20requests%20remains%0Arelatively%20unexplored.%20To%20close%20this%20gap%2C%20we%20compare%20the%20performance%20of%2038%0Aopen-%20and%20closed-source%20LLMs%20of%20various%20sizes%2C%20such%20as%20LLama%203.2%20and%20GPT-4o%2C%20in%0Aa%20movie%20recommendation%20setting.%20For%20this%2C%20we%20utilize%20a%20gold-standard%2C%0Acrowdworker-annotated%20dataset%20of%20posts%20from%20reddit%27s%20movie%20suggestion%20community%0Aand%20employ%20various%20prompting%20strategies%2C%20including%20zero-shot%2C%20identity%2C%20and%0Afew-shot%20prompting.%20Our%20findings%20demonstrate%20the%20ability%20of%20LLMs%20to%20generate%0Acontextually%20relevant%20movie%20recommendations%2C%20significantly%20outperforming%20other%0Astate-of-the-art%20approaches%2C%20such%20as%20doc2vec.%20While%20we%20find%20that%20closed-source%0Aand%20large-parameterized%20models%20generally%20perform%20best%2C%20medium-sized%20open-source%0Amodels%20remain%20competitive%2C%20being%20only%20slightly%20outperformed%20by%20their%20more%0Acomputationally%20expensive%20counterparts.%20Furthermore%2C%20we%20observe%20no%20significant%0Adifferences%20across%20prompting%20strategies%20for%20most%20models%2C%20underscoring%20the%0Aeffectiveness%20of%20simple%20approaches%20such%20as%20zero-shot%20prompting%20for%0Anarrative-driven%20recommendations.%20Overall%2C%20this%20work%20offers%20valuable%20insights%0Afor%20recommender%20system%20researchers%20as%20well%20as%20practitioners%20aiming%20to%20integrate%0ALLMs%20into%20real-world%20recommendation%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13604v1&entry.124074799=Read"},
{"title": "LifeGPT: Topology-Agnostic Generative Pretrained Transformer Model for\n  Cellular Automata", "author": "Jaime A. Berkovich and Markus J. Buehler", "abstract": "  Conway's Game of Life (Life), a well known algorithm within the broader class\nof cellular automata (CA), exhibits complex emergent dynamics, with extreme\nsensitivity to initial conditions. Modeling and predicting such intricate\nbehavior without explicit knowledge of the system's underlying topology\npresents a significant challenge, motivating the development of algorithms that\ncan generalize across various grid configurations and boundary conditions. We\ndevelop a decoder-only generative pretrained transformer (GPT) model to solve\nthis problem, showing that our model can simulate Life on a toroidal grid with\nno prior knowledge on the size of the grid, or its periodic boundary conditions\n(LifeGPT). LifeGPT is topology-agnostic with respect to its training data and\nour results show that a GPT model is capable of capturing the deterministic\nrules of a Turing-complete system with near-perfect accuracy, given\nsufficiently diverse training data. We also introduce the idea of an\n`autoregressive autoregressor' to recursively implement Life using LifeGPT. Our\nresults pave the path towards true universal computation within a large\nlanguage model framework, synthesizing of mathematical analysis with natural\nlanguage processing, and probing AI systems for situational awareness about the\nevolution of such algorithms without ever having to compute them. Similar GPTs\ncould potentially solve inverse problems in multicellular self-assembly by\nextracting CA-compatible rulesets from real-world biological systems to create\nnew predictive models, which would have significant consequences for the fields\nof bioinspired materials, tissue engineering, and architected materials design.\n", "link": "http://arxiv.org/abs/2409.12182v2", "date": "2024-10-17", "relevancy": 2.0122, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5273}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5223}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LifeGPT%3A%20Topology-Agnostic%20Generative%20Pretrained%20Transformer%20Model%20for%0A%20%20Cellular%20Automata&body=Title%3A%20LifeGPT%3A%20Topology-Agnostic%20Generative%20Pretrained%20Transformer%20Model%20for%0A%20%20Cellular%20Automata%0AAuthor%3A%20Jaime%20A.%20Berkovich%20and%20Markus%20J.%20Buehler%0AAbstract%3A%20%20%20Conway%27s%20Game%20of%20Life%20%28Life%29%2C%20a%20well%20known%20algorithm%20within%20the%20broader%20class%0Aof%20cellular%20automata%20%28CA%29%2C%20exhibits%20complex%20emergent%20dynamics%2C%20with%20extreme%0Asensitivity%20to%20initial%20conditions.%20Modeling%20and%20predicting%20such%20intricate%0Abehavior%20without%20explicit%20knowledge%20of%20the%20system%27s%20underlying%20topology%0Apresents%20a%20significant%20challenge%2C%20motivating%20the%20development%20of%20algorithms%20that%0Acan%20generalize%20across%20various%20grid%20configurations%20and%20boundary%20conditions.%20We%0Adevelop%20a%20decoder-only%20generative%20pretrained%20transformer%20%28GPT%29%20model%20to%20solve%0Athis%20problem%2C%20showing%20that%20our%20model%20can%20simulate%20Life%20on%20a%20toroidal%20grid%20with%0Ano%20prior%20knowledge%20on%20the%20size%20of%20the%20grid%2C%20or%20its%20periodic%20boundary%20conditions%0A%28LifeGPT%29.%20LifeGPT%20is%20topology-agnostic%20with%20respect%20to%20its%20training%20data%20and%0Aour%20results%20show%20that%20a%20GPT%20model%20is%20capable%20of%20capturing%20the%20deterministic%0Arules%20of%20a%20Turing-complete%20system%20with%20near-perfect%20accuracy%2C%20given%0Asufficiently%20diverse%20training%20data.%20We%20also%20introduce%20the%20idea%20of%20an%0A%60autoregressive%20autoregressor%27%20to%20recursively%20implement%20Life%20using%20LifeGPT.%20Our%0Aresults%20pave%20the%20path%20towards%20true%20universal%20computation%20within%20a%20large%0Alanguage%20model%20framework%2C%20synthesizing%20of%20mathematical%20analysis%20with%20natural%0Alanguage%20processing%2C%20and%20probing%20AI%20systems%20for%20situational%20awareness%20about%20the%0Aevolution%20of%20such%20algorithms%20without%20ever%20having%20to%20compute%20them.%20Similar%20GPTs%0Acould%20potentially%20solve%20inverse%20problems%20in%20multicellular%20self-assembly%20by%0Aextracting%20CA-compatible%20rulesets%20from%20real-world%20biological%20systems%20to%20create%0Anew%20predictive%20models%2C%20which%20would%20have%20significant%20consequences%20for%20the%20fields%0Aof%20bioinspired%20materials%2C%20tissue%20engineering%2C%20and%20architected%20materials%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifeGPT%253A%2520Topology-Agnostic%2520Generative%2520Pretrained%2520Transformer%2520Model%2520for%250A%2520%2520Cellular%2520Automata%26entry.906535625%3DJaime%2520A.%2520Berkovich%2520and%2520Markus%2520J.%2520Buehler%26entry.1292438233%3D%2520%2520Conway%2527s%2520Game%2520of%2520Life%2520%2528Life%2529%252C%2520a%2520well%2520known%2520algorithm%2520within%2520the%2520broader%2520class%250Aof%2520cellular%2520automata%2520%2528CA%2529%252C%2520exhibits%2520complex%2520emergent%2520dynamics%252C%2520with%2520extreme%250Asensitivity%2520to%2520initial%2520conditions.%2520Modeling%2520and%2520predicting%2520such%2520intricate%250Abehavior%2520without%2520explicit%2520knowledge%2520of%2520the%2520system%2527s%2520underlying%2520topology%250Apresents%2520a%2520significant%2520challenge%252C%2520motivating%2520the%2520development%2520of%2520algorithms%2520that%250Acan%2520generalize%2520across%2520various%2520grid%2520configurations%2520and%2520boundary%2520conditions.%2520We%250Adevelop%2520a%2520decoder-only%2520generative%2520pretrained%2520transformer%2520%2528GPT%2529%2520model%2520to%2520solve%250Athis%2520problem%252C%2520showing%2520that%2520our%2520model%2520can%2520simulate%2520Life%2520on%2520a%2520toroidal%2520grid%2520with%250Ano%2520prior%2520knowledge%2520on%2520the%2520size%2520of%2520the%2520grid%252C%2520or%2520its%2520periodic%2520boundary%2520conditions%250A%2528LifeGPT%2529.%2520LifeGPT%2520is%2520topology-agnostic%2520with%2520respect%2520to%2520its%2520training%2520data%2520and%250Aour%2520results%2520show%2520that%2520a%2520GPT%2520model%2520is%2520capable%2520of%2520capturing%2520the%2520deterministic%250Arules%2520of%2520a%2520Turing-complete%2520system%2520with%2520near-perfect%2520accuracy%252C%2520given%250Asufficiently%2520diverse%2520training%2520data.%2520We%2520also%2520introduce%2520the%2520idea%2520of%2520an%250A%2560autoregressive%2520autoregressor%2527%2520to%2520recursively%2520implement%2520Life%2520using%2520LifeGPT.%2520Our%250Aresults%2520pave%2520the%2520path%2520towards%2520true%2520universal%2520computation%2520within%2520a%2520large%250Alanguage%2520model%2520framework%252C%2520synthesizing%2520of%2520mathematical%2520analysis%2520with%2520natural%250Alanguage%2520processing%252C%2520and%2520probing%2520AI%2520systems%2520for%2520situational%2520awareness%2520about%2520the%250Aevolution%2520of%2520such%2520algorithms%2520without%2520ever%2520having%2520to%2520compute%2520them.%2520Similar%2520GPTs%250Acould%2520potentially%2520solve%2520inverse%2520problems%2520in%2520multicellular%2520self-assembly%2520by%250Aextracting%2520CA-compatible%2520rulesets%2520from%2520real-world%2520biological%2520systems%2520to%2520create%250Anew%2520predictive%2520models%252C%2520which%2520would%2520have%2520significant%2520consequences%2520for%2520the%2520fields%250Aof%2520bioinspired%2520materials%252C%2520tissue%2520engineering%252C%2520and%2520architected%2520materials%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LifeGPT%3A%20Topology-Agnostic%20Generative%20Pretrained%20Transformer%20Model%20for%0A%20%20Cellular%20Automata&entry.906535625=Jaime%20A.%20Berkovich%20and%20Markus%20J.%20Buehler&entry.1292438233=%20%20Conway%27s%20Game%20of%20Life%20%28Life%29%2C%20a%20well%20known%20algorithm%20within%20the%20broader%20class%0Aof%20cellular%20automata%20%28CA%29%2C%20exhibits%20complex%20emergent%20dynamics%2C%20with%20extreme%0Asensitivity%20to%20initial%20conditions.%20Modeling%20and%20predicting%20such%20intricate%0Abehavior%20without%20explicit%20knowledge%20of%20the%20system%27s%20underlying%20topology%0Apresents%20a%20significant%20challenge%2C%20motivating%20the%20development%20of%20algorithms%20that%0Acan%20generalize%20across%20various%20grid%20configurations%20and%20boundary%20conditions.%20We%0Adevelop%20a%20decoder-only%20generative%20pretrained%20transformer%20%28GPT%29%20model%20to%20solve%0Athis%20problem%2C%20showing%20that%20our%20model%20can%20simulate%20Life%20on%20a%20toroidal%20grid%20with%0Ano%20prior%20knowledge%20on%20the%20size%20of%20the%20grid%2C%20or%20its%20periodic%20boundary%20conditions%0A%28LifeGPT%29.%20LifeGPT%20is%20topology-agnostic%20with%20respect%20to%20its%20training%20data%20and%0Aour%20results%20show%20that%20a%20GPT%20model%20is%20capable%20of%20capturing%20the%20deterministic%0Arules%20of%20a%20Turing-complete%20system%20with%20near-perfect%20accuracy%2C%20given%0Asufficiently%20diverse%20training%20data.%20We%20also%20introduce%20the%20idea%20of%20an%0A%60autoregressive%20autoregressor%27%20to%20recursively%20implement%20Life%20using%20LifeGPT.%20Our%0Aresults%20pave%20the%20path%20towards%20true%20universal%20computation%20within%20a%20large%0Alanguage%20model%20framework%2C%20synthesizing%20of%20mathematical%20analysis%20with%20natural%0Alanguage%20processing%2C%20and%20probing%20AI%20systems%20for%20situational%20awareness%20about%20the%0Aevolution%20of%20such%20algorithms%20without%20ever%20having%20to%20compute%20them.%20Similar%20GPTs%0Acould%20potentially%20solve%20inverse%20problems%20in%20multicellular%20self-assembly%20by%0Aextracting%20CA-compatible%20rulesets%20from%20real-world%20biological%20systems%20to%20create%0Anew%20predictive%20models%2C%20which%20would%20have%20significant%20consequences%20for%20the%20fields%0Aof%20bioinspired%20materials%2C%20tissue%20engineering%2C%20and%20architected%20materials%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12182v2&entry.124074799=Read"},
{"title": "A new approach for fine-tuning sentence transformers for intent\n  classification and out-of-scope detection tasks", "author": "Tianyi Zhang and Atta Norouzian and Aanchan Mohan and Frederick Ducatelle", "abstract": "  In virtual assistant (VA) systems it is important to reject or redirect user\nqueries that fall outside the scope of the system. One of the most accurate\napproaches for out-of-scope (OOS) rejection is to combine it with the task of\nintent classification on in-scope queries, and to use methods based on the\nsimilarity of embeddings produced by transformer-based sentence encoders.\nTypically, such encoders are fine-tuned for the intent-classification task,\nusing cross-entropy loss. Recent work has shown that while this produces\nsuitable embeddings for the intent-classification task, it also tends to\ndisperse in-scope embeddings over the full sentence embedding space. This\ncauses the in-scope embeddings to potentially overlap with OOS embeddings,\nthereby making OOS rejection difficult. This is compounded when OOS data is\nunknown. To mitigate this issue our work proposes to regularize the\ncross-entropy loss with an in-scope embedding reconstruction loss learned using\nan auto-encoder. Our method achieves a 1-4% improvement in the area under the\nprecision-recall curve for rejecting out-of-sample (OOS) instances, without\ncompromising intent classification performance.\n", "link": "http://arxiv.org/abs/2410.13649v1", "date": "2024-10-17", "relevancy": 2.0036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20approach%20for%20fine-tuning%20sentence%20transformers%20for%20intent%0A%20%20classification%20and%20out-of-scope%20detection%20tasks&body=Title%3A%20A%20new%20approach%20for%20fine-tuning%20sentence%20transformers%20for%20intent%0A%20%20classification%20and%20out-of-scope%20detection%20tasks%0AAuthor%3A%20Tianyi%20Zhang%20and%20Atta%20Norouzian%20and%20Aanchan%20Mohan%20and%20Frederick%20Ducatelle%0AAbstract%3A%20%20%20In%20virtual%20assistant%20%28VA%29%20systems%20it%20is%20important%20to%20reject%20or%20redirect%20user%0Aqueries%20that%20fall%20outside%20the%20scope%20of%20the%20system.%20One%20of%20the%20most%20accurate%0Aapproaches%20for%20out-of-scope%20%28OOS%29%20rejection%20is%20to%20combine%20it%20with%20the%20task%20of%0Aintent%20classification%20on%20in-scope%20queries%2C%20and%20to%20use%20methods%20based%20on%20the%0Asimilarity%20of%20embeddings%20produced%20by%20transformer-based%20sentence%20encoders.%0ATypically%2C%20such%20encoders%20are%20fine-tuned%20for%20the%20intent-classification%20task%2C%0Ausing%20cross-entropy%20loss.%20Recent%20work%20has%20shown%20that%20while%20this%20produces%0Asuitable%20embeddings%20for%20the%20intent-classification%20task%2C%20it%20also%20tends%20to%0Adisperse%20in-scope%20embeddings%20over%20the%20full%20sentence%20embedding%20space.%20This%0Acauses%20the%20in-scope%20embeddings%20to%20potentially%20overlap%20with%20OOS%20embeddings%2C%0Athereby%20making%20OOS%20rejection%20difficult.%20This%20is%20compounded%20when%20OOS%20data%20is%0Aunknown.%20To%20mitigate%20this%20issue%20our%20work%20proposes%20to%20regularize%20the%0Across-entropy%20loss%20with%20an%20in-scope%20embedding%20reconstruction%20loss%20learned%20using%0Aan%20auto-encoder.%20Our%20method%20achieves%20a%201-4%25%20improvement%20in%20the%20area%20under%20the%0Aprecision-recall%20curve%20for%20rejecting%20out-of-sample%20%28OOS%29%20instances%2C%20without%0Acompromising%20intent%20classification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520approach%2520for%2520fine-tuning%2520sentence%2520transformers%2520for%2520intent%250A%2520%2520classification%2520and%2520out-of-scope%2520detection%2520tasks%26entry.906535625%3DTianyi%2520Zhang%2520and%2520Atta%2520Norouzian%2520and%2520Aanchan%2520Mohan%2520and%2520Frederick%2520Ducatelle%26entry.1292438233%3D%2520%2520In%2520virtual%2520assistant%2520%2528VA%2529%2520systems%2520it%2520is%2520important%2520to%2520reject%2520or%2520redirect%2520user%250Aqueries%2520that%2520fall%2520outside%2520the%2520scope%2520of%2520the%2520system.%2520One%2520of%2520the%2520most%2520accurate%250Aapproaches%2520for%2520out-of-scope%2520%2528OOS%2529%2520rejection%2520is%2520to%2520combine%2520it%2520with%2520the%2520task%2520of%250Aintent%2520classification%2520on%2520in-scope%2520queries%252C%2520and%2520to%2520use%2520methods%2520based%2520on%2520the%250Asimilarity%2520of%2520embeddings%2520produced%2520by%2520transformer-based%2520sentence%2520encoders.%250ATypically%252C%2520such%2520encoders%2520are%2520fine-tuned%2520for%2520the%2520intent-classification%2520task%252C%250Ausing%2520cross-entropy%2520loss.%2520Recent%2520work%2520has%2520shown%2520that%2520while%2520this%2520produces%250Asuitable%2520embeddings%2520for%2520the%2520intent-classification%2520task%252C%2520it%2520also%2520tends%2520to%250Adisperse%2520in-scope%2520embeddings%2520over%2520the%2520full%2520sentence%2520embedding%2520space.%2520This%250Acauses%2520the%2520in-scope%2520embeddings%2520to%2520potentially%2520overlap%2520with%2520OOS%2520embeddings%252C%250Athereby%2520making%2520OOS%2520rejection%2520difficult.%2520This%2520is%2520compounded%2520when%2520OOS%2520data%2520is%250Aunknown.%2520To%2520mitigate%2520this%2520issue%2520our%2520work%2520proposes%2520to%2520regularize%2520the%250Across-entropy%2520loss%2520with%2520an%2520in-scope%2520embedding%2520reconstruction%2520loss%2520learned%2520using%250Aan%2520auto-encoder.%2520Our%2520method%2520achieves%2520a%25201-4%2525%2520improvement%2520in%2520the%2520area%2520under%2520the%250Aprecision-recall%2520curve%2520for%2520rejecting%2520out-of-sample%2520%2528OOS%2529%2520instances%252C%2520without%250Acompromising%2520intent%2520classification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20approach%20for%20fine-tuning%20sentence%20transformers%20for%20intent%0A%20%20classification%20and%20out-of-scope%20detection%20tasks&entry.906535625=Tianyi%20Zhang%20and%20Atta%20Norouzian%20and%20Aanchan%20Mohan%20and%20Frederick%20Ducatelle&entry.1292438233=%20%20In%20virtual%20assistant%20%28VA%29%20systems%20it%20is%20important%20to%20reject%20or%20redirect%20user%0Aqueries%20that%20fall%20outside%20the%20scope%20of%20the%20system.%20One%20of%20the%20most%20accurate%0Aapproaches%20for%20out-of-scope%20%28OOS%29%20rejection%20is%20to%20combine%20it%20with%20the%20task%20of%0Aintent%20classification%20on%20in-scope%20queries%2C%20and%20to%20use%20methods%20based%20on%20the%0Asimilarity%20of%20embeddings%20produced%20by%20transformer-based%20sentence%20encoders.%0ATypically%2C%20such%20encoders%20are%20fine-tuned%20for%20the%20intent-classification%20task%2C%0Ausing%20cross-entropy%20loss.%20Recent%20work%20has%20shown%20that%20while%20this%20produces%0Asuitable%20embeddings%20for%20the%20intent-classification%20task%2C%20it%20also%20tends%20to%0Adisperse%20in-scope%20embeddings%20over%20the%20full%20sentence%20embedding%20space.%20This%0Acauses%20the%20in-scope%20embeddings%20to%20potentially%20overlap%20with%20OOS%20embeddings%2C%0Athereby%20making%20OOS%20rejection%20difficult.%20This%20is%20compounded%20when%20OOS%20data%20is%0Aunknown.%20To%20mitigate%20this%20issue%20our%20work%20proposes%20to%20regularize%20the%0Across-entropy%20loss%20with%20an%20in-scope%20embedding%20reconstruction%20loss%20learned%20using%0Aan%20auto-encoder.%20Our%20method%20achieves%20a%201-4%25%20improvement%20in%20the%20area%20under%20the%0Aprecision-recall%20curve%20for%20rejecting%20out-of-sample%20%28OOS%29%20instances%2C%20without%0Acompromising%20intent%20classification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13649v1&entry.124074799=Read"},
{"title": "Automated Model Discovery for Tensional Homeostasis: Constitutive\n  Machine Learning in Growth and Remodeling", "author": "Hagen Holthusen and Tim Brepols and Kevin Linka and Ellen Kuhl", "abstract": "  Soft biological tissues exhibit a tendency to maintain a preferred state of\ntensile stress, known as tensional homeostasis, which is restored even after\nexternal mechanical stimuli. This macroscopic behavior can be described using\nthe theory of kinematic growth, where the deformation gradient is\nmultiplicatively decomposed into an elastic part and a part related to growth\nand remodeling. Recently, the concept of homeostatic surfaces was introduced to\ndefine the state of homeostasis and the evolution equations for inelastic\ndeformations.\n  However, identifying the optimal model and material parameters to accurately\ncapture the macroscopic behavior of inelastic materials can only be\naccomplished with significant expertise, is often time-consuming, and prone to\nerror, regardless of the specific inelastic phenomenon. To address this\nchallenge, built-in physics machine learning algorithms offer significant\npotential.\n  In this work, we extend our inelastic Constitutive Artificial Neural Networks\n(iCANNs) by incorporating kinematic growth and homeostatic surfaces to discover\nthe scalar model equations, namely the Helmholtz free energy and the pseudo\npotential. The latter describes the state of homeostasis in a smeared sense. We\nevaluate the ability of the proposed network to learn from experimentally\nobtained tissue equivalent data at the material point level, assess its\npredictive accuracy beyond the training regime, and discuss its current\nlimitations when applied at the structural level.\n  Our source code, data, examples, and an implementation of the corresponding\nmaterial subroutine are made accessible to the public at\nhttps://doi.org/10.5281/zenodo.13946282.\n", "link": "http://arxiv.org/abs/2410.13645v1", "date": "2024-10-17", "relevancy": 2.0033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5047}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Model%20Discovery%20for%20Tensional%20Homeostasis%3A%20Constitutive%0A%20%20Machine%20Learning%20in%20Growth%20and%20Remodeling&body=Title%3A%20Automated%20Model%20Discovery%20for%20Tensional%20Homeostasis%3A%20Constitutive%0A%20%20Machine%20Learning%20in%20Growth%20and%20Remodeling%0AAuthor%3A%20Hagen%20Holthusen%20and%20Tim%20Brepols%20and%20Kevin%20Linka%20and%20Ellen%20Kuhl%0AAbstract%3A%20%20%20Soft%20biological%20tissues%20exhibit%20a%20tendency%20to%20maintain%20a%20preferred%20state%20of%0Atensile%20stress%2C%20known%20as%20tensional%20homeostasis%2C%20which%20is%20restored%20even%20after%0Aexternal%20mechanical%20stimuli.%20This%20macroscopic%20behavior%20can%20be%20described%20using%0Athe%20theory%20of%20kinematic%20growth%2C%20where%20the%20deformation%20gradient%20is%0Amultiplicatively%20decomposed%20into%20an%20elastic%20part%20and%20a%20part%20related%20to%20growth%0Aand%20remodeling.%20Recently%2C%20the%20concept%20of%20homeostatic%20surfaces%20was%20introduced%20to%0Adefine%20the%20state%20of%20homeostasis%20and%20the%20evolution%20equations%20for%20inelastic%0Adeformations.%0A%20%20However%2C%20identifying%20the%20optimal%20model%20and%20material%20parameters%20to%20accurately%0Acapture%20the%20macroscopic%20behavior%20of%20inelastic%20materials%20can%20only%20be%0Aaccomplished%20with%20significant%20expertise%2C%20is%20often%20time-consuming%2C%20and%20prone%20to%0Aerror%2C%20regardless%20of%20the%20specific%20inelastic%20phenomenon.%20To%20address%20this%0Achallenge%2C%20built-in%20physics%20machine%20learning%20algorithms%20offer%20significant%0Apotential.%0A%20%20In%20this%20work%2C%20we%20extend%20our%20inelastic%20Constitutive%20Artificial%20Neural%20Networks%0A%28iCANNs%29%20by%20incorporating%20kinematic%20growth%20and%20homeostatic%20surfaces%20to%20discover%0Athe%20scalar%20model%20equations%2C%20namely%20the%20Helmholtz%20free%20energy%20and%20the%20pseudo%0Apotential.%20The%20latter%20describes%20the%20state%20of%20homeostasis%20in%20a%20smeared%20sense.%20We%0Aevaluate%20the%20ability%20of%20the%20proposed%20network%20to%20learn%20from%20experimentally%0Aobtained%20tissue%20equivalent%20data%20at%20the%20material%20point%20level%2C%20assess%20its%0Apredictive%20accuracy%20beyond%20the%20training%20regime%2C%20and%20discuss%20its%20current%0Alimitations%20when%20applied%20at%20the%20structural%20level.%0A%20%20Our%20source%20code%2C%20data%2C%20examples%2C%20and%20an%20implementation%20of%20the%20corresponding%0Amaterial%20subroutine%20are%20made%20accessible%20to%20the%20public%20at%0Ahttps%3A//doi.org/10.5281/zenodo.13946282.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Model%2520Discovery%2520for%2520Tensional%2520Homeostasis%253A%2520Constitutive%250A%2520%2520Machine%2520Learning%2520in%2520Growth%2520and%2520Remodeling%26entry.906535625%3DHagen%2520Holthusen%2520and%2520Tim%2520Brepols%2520and%2520Kevin%2520Linka%2520and%2520Ellen%2520Kuhl%26entry.1292438233%3D%2520%2520Soft%2520biological%2520tissues%2520exhibit%2520a%2520tendency%2520to%2520maintain%2520a%2520preferred%2520state%2520of%250Atensile%2520stress%252C%2520known%2520as%2520tensional%2520homeostasis%252C%2520which%2520is%2520restored%2520even%2520after%250Aexternal%2520mechanical%2520stimuli.%2520This%2520macroscopic%2520behavior%2520can%2520be%2520described%2520using%250Athe%2520theory%2520of%2520kinematic%2520growth%252C%2520where%2520the%2520deformation%2520gradient%2520is%250Amultiplicatively%2520decomposed%2520into%2520an%2520elastic%2520part%2520and%2520a%2520part%2520related%2520to%2520growth%250Aand%2520remodeling.%2520Recently%252C%2520the%2520concept%2520of%2520homeostatic%2520surfaces%2520was%2520introduced%2520to%250Adefine%2520the%2520state%2520of%2520homeostasis%2520and%2520the%2520evolution%2520equations%2520for%2520inelastic%250Adeformations.%250A%2520%2520However%252C%2520identifying%2520the%2520optimal%2520model%2520and%2520material%2520parameters%2520to%2520accurately%250Acapture%2520the%2520macroscopic%2520behavior%2520of%2520inelastic%2520materials%2520can%2520only%2520be%250Aaccomplished%2520with%2520significant%2520expertise%252C%2520is%2520often%2520time-consuming%252C%2520and%2520prone%2520to%250Aerror%252C%2520regardless%2520of%2520the%2520specific%2520inelastic%2520phenomenon.%2520To%2520address%2520this%250Achallenge%252C%2520built-in%2520physics%2520machine%2520learning%2520algorithms%2520offer%2520significant%250Apotential.%250A%2520%2520In%2520this%2520work%252C%2520we%2520extend%2520our%2520inelastic%2520Constitutive%2520Artificial%2520Neural%2520Networks%250A%2528iCANNs%2529%2520by%2520incorporating%2520kinematic%2520growth%2520and%2520homeostatic%2520surfaces%2520to%2520discover%250Athe%2520scalar%2520model%2520equations%252C%2520namely%2520the%2520Helmholtz%2520free%2520energy%2520and%2520the%2520pseudo%250Apotential.%2520The%2520latter%2520describes%2520the%2520state%2520of%2520homeostasis%2520in%2520a%2520smeared%2520sense.%2520We%250Aevaluate%2520the%2520ability%2520of%2520the%2520proposed%2520network%2520to%2520learn%2520from%2520experimentally%250Aobtained%2520tissue%2520equivalent%2520data%2520at%2520the%2520material%2520point%2520level%252C%2520assess%2520its%250Apredictive%2520accuracy%2520beyond%2520the%2520training%2520regime%252C%2520and%2520discuss%2520its%2520current%250Alimitations%2520when%2520applied%2520at%2520the%2520structural%2520level.%250A%2520%2520Our%2520source%2520code%252C%2520data%252C%2520examples%252C%2520and%2520an%2520implementation%2520of%2520the%2520corresponding%250Amaterial%2520subroutine%2520are%2520made%2520accessible%2520to%2520the%2520public%2520at%250Ahttps%253A//doi.org/10.5281/zenodo.13946282.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Model%20Discovery%20for%20Tensional%20Homeostasis%3A%20Constitutive%0A%20%20Machine%20Learning%20in%20Growth%20and%20Remodeling&entry.906535625=Hagen%20Holthusen%20and%20Tim%20Brepols%20and%20Kevin%20Linka%20and%20Ellen%20Kuhl&entry.1292438233=%20%20Soft%20biological%20tissues%20exhibit%20a%20tendency%20to%20maintain%20a%20preferred%20state%20of%0Atensile%20stress%2C%20known%20as%20tensional%20homeostasis%2C%20which%20is%20restored%20even%20after%0Aexternal%20mechanical%20stimuli.%20This%20macroscopic%20behavior%20can%20be%20described%20using%0Athe%20theory%20of%20kinematic%20growth%2C%20where%20the%20deformation%20gradient%20is%0Amultiplicatively%20decomposed%20into%20an%20elastic%20part%20and%20a%20part%20related%20to%20growth%0Aand%20remodeling.%20Recently%2C%20the%20concept%20of%20homeostatic%20surfaces%20was%20introduced%20to%0Adefine%20the%20state%20of%20homeostasis%20and%20the%20evolution%20equations%20for%20inelastic%0Adeformations.%0A%20%20However%2C%20identifying%20the%20optimal%20model%20and%20material%20parameters%20to%20accurately%0Acapture%20the%20macroscopic%20behavior%20of%20inelastic%20materials%20can%20only%20be%0Aaccomplished%20with%20significant%20expertise%2C%20is%20often%20time-consuming%2C%20and%20prone%20to%0Aerror%2C%20regardless%20of%20the%20specific%20inelastic%20phenomenon.%20To%20address%20this%0Achallenge%2C%20built-in%20physics%20machine%20learning%20algorithms%20offer%20significant%0Apotential.%0A%20%20In%20this%20work%2C%20we%20extend%20our%20inelastic%20Constitutive%20Artificial%20Neural%20Networks%0A%28iCANNs%29%20by%20incorporating%20kinematic%20growth%20and%20homeostatic%20surfaces%20to%20discover%0Athe%20scalar%20model%20equations%2C%20namely%20the%20Helmholtz%20free%20energy%20and%20the%20pseudo%0Apotential.%20The%20latter%20describes%20the%20state%20of%20homeostasis%20in%20a%20smeared%20sense.%20We%0Aevaluate%20the%20ability%20of%20the%20proposed%20network%20to%20learn%20from%20experimentally%0Aobtained%20tissue%20equivalent%20data%20at%20the%20material%20point%20level%2C%20assess%20its%0Apredictive%20accuracy%20beyond%20the%20training%20regime%2C%20and%20discuss%20its%20current%0Alimitations%20when%20applied%20at%20the%20structural%20level.%0A%20%20Our%20source%20code%2C%20data%2C%20examples%2C%20and%20an%20implementation%20of%20the%20corresponding%0Amaterial%20subroutine%20are%20made%20accessible%20to%20the%20public%20at%0Ahttps%3A//doi.org/10.5281/zenodo.13946282.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13645v1&entry.124074799=Read"},
{"title": "Analyzing Deep Transformer Models for Time Series Forecasting via\n  Manifold Learning", "author": "Ilya Kaufman and Omri Azencot", "abstract": "  Transformer models have consistently achieved remarkable results in various\ndomains such as natural language processing and computer vision. However,\ndespite ongoing research efforts to better understand these models, the field\nstill lacks a comprehensive understanding. This is particularly true for deep\ntime series forecasting methods, where analysis and understanding work is\nrelatively limited. Time series data, unlike image and text information, can be\nmore challenging to interpret and analyze. To address this, we approach the\nproblem from a manifold learning perspective, assuming that the latent\nrepresentations of time series forecasting models lie next to a low-dimensional\nmanifold. In our study, we focus on analyzing the geometric features of these\nlatent data manifolds, including intrinsic dimension and principal curvatures.\nOur findings reveal that deep transformer models exhibit similar geometric\nbehavior across layers, and these geometric features are correlated with model\nperformance. Additionally, we observe that untrained models initially have\ndifferent structures, but they rapidly converge during training. By leveraging\nour geometric analysis and differentiable tools, we can potentially design new\nand improved deep forecasting neural networks. This approach complements\nexisting analysis studies and contributes to a better understanding of\ntransformer models in the context of time series forecasting. Code is released\nat https://github.com/azencot-group/GATLM.\n", "link": "http://arxiv.org/abs/2410.13792v1", "date": "2024-10-17", "relevancy": 1.9829, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5267}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Deep%20Transformer%20Models%20for%20Time%20Series%20Forecasting%20via%0A%20%20Manifold%20Learning&body=Title%3A%20Analyzing%20Deep%20Transformer%20Models%20for%20Time%20Series%20Forecasting%20via%0A%20%20Manifold%20Learning%0AAuthor%3A%20Ilya%20Kaufman%20and%20Omri%20Azencot%0AAbstract%3A%20%20%20Transformer%20models%20have%20consistently%20achieved%20remarkable%20results%20in%20various%0Adomains%20such%20as%20natural%20language%20processing%20and%20computer%20vision.%20However%2C%0Adespite%20ongoing%20research%20efforts%20to%20better%20understand%20these%20models%2C%20the%20field%0Astill%20lacks%20a%20comprehensive%20understanding.%20This%20is%20particularly%20true%20for%20deep%0Atime%20series%20forecasting%20methods%2C%20where%20analysis%20and%20understanding%20work%20is%0Arelatively%20limited.%20Time%20series%20data%2C%20unlike%20image%20and%20text%20information%2C%20can%20be%0Amore%20challenging%20to%20interpret%20and%20analyze.%20To%20address%20this%2C%20we%20approach%20the%0Aproblem%20from%20a%20manifold%20learning%20perspective%2C%20assuming%20that%20the%20latent%0Arepresentations%20of%20time%20series%20forecasting%20models%20lie%20next%20to%20a%20low-dimensional%0Amanifold.%20In%20our%20study%2C%20we%20focus%20on%20analyzing%20the%20geometric%20features%20of%20these%0Alatent%20data%20manifolds%2C%20including%20intrinsic%20dimension%20and%20principal%20curvatures.%0AOur%20findings%20reveal%20that%20deep%20transformer%20models%20exhibit%20similar%20geometric%0Abehavior%20across%20layers%2C%20and%20these%20geometric%20features%20are%20correlated%20with%20model%0Aperformance.%20Additionally%2C%20we%20observe%20that%20untrained%20models%20initially%20have%0Adifferent%20structures%2C%20but%20they%20rapidly%20converge%20during%20training.%20By%20leveraging%0Aour%20geometric%20analysis%20and%20differentiable%20tools%2C%20we%20can%20potentially%20design%20new%0Aand%20improved%20deep%20forecasting%20neural%20networks.%20This%20approach%20complements%0Aexisting%20analysis%20studies%20and%20contributes%20to%20a%20better%20understanding%20of%0Atransformer%20models%20in%20the%20context%20of%20time%20series%20forecasting.%20Code%20is%20released%0Aat%20https%3A//github.com/azencot-group/GATLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Deep%2520Transformer%2520Models%2520for%2520Time%2520Series%2520Forecasting%2520via%250A%2520%2520Manifold%2520Learning%26entry.906535625%3DIlya%2520Kaufman%2520and%2520Omri%2520Azencot%26entry.1292438233%3D%2520%2520Transformer%2520models%2520have%2520consistently%2520achieved%2520remarkable%2520results%2520in%2520various%250Adomains%2520such%2520as%2520natural%2520language%2520processing%2520and%2520computer%2520vision.%2520However%252C%250Adespite%2520ongoing%2520research%2520efforts%2520to%2520better%2520understand%2520these%2520models%252C%2520the%2520field%250Astill%2520lacks%2520a%2520comprehensive%2520understanding.%2520This%2520is%2520particularly%2520true%2520for%2520deep%250Atime%2520series%2520forecasting%2520methods%252C%2520where%2520analysis%2520and%2520understanding%2520work%2520is%250Arelatively%2520limited.%2520Time%2520series%2520data%252C%2520unlike%2520image%2520and%2520text%2520information%252C%2520can%2520be%250Amore%2520challenging%2520to%2520interpret%2520and%2520analyze.%2520To%2520address%2520this%252C%2520we%2520approach%2520the%250Aproblem%2520from%2520a%2520manifold%2520learning%2520perspective%252C%2520assuming%2520that%2520the%2520latent%250Arepresentations%2520of%2520time%2520series%2520forecasting%2520models%2520lie%2520next%2520to%2520a%2520low-dimensional%250Amanifold.%2520In%2520our%2520study%252C%2520we%2520focus%2520on%2520analyzing%2520the%2520geometric%2520features%2520of%2520these%250Alatent%2520data%2520manifolds%252C%2520including%2520intrinsic%2520dimension%2520and%2520principal%2520curvatures.%250AOur%2520findings%2520reveal%2520that%2520deep%2520transformer%2520models%2520exhibit%2520similar%2520geometric%250Abehavior%2520across%2520layers%252C%2520and%2520these%2520geometric%2520features%2520are%2520correlated%2520with%2520model%250Aperformance.%2520Additionally%252C%2520we%2520observe%2520that%2520untrained%2520models%2520initially%2520have%250Adifferent%2520structures%252C%2520but%2520they%2520rapidly%2520converge%2520during%2520training.%2520By%2520leveraging%250Aour%2520geometric%2520analysis%2520and%2520differentiable%2520tools%252C%2520we%2520can%2520potentially%2520design%2520new%250Aand%2520improved%2520deep%2520forecasting%2520neural%2520networks.%2520This%2520approach%2520complements%250Aexisting%2520analysis%2520studies%2520and%2520contributes%2520to%2520a%2520better%2520understanding%2520of%250Atransformer%2520models%2520in%2520the%2520context%2520of%2520time%2520series%2520forecasting.%2520Code%2520is%2520released%250Aat%2520https%253A//github.com/azencot-group/GATLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Deep%20Transformer%20Models%20for%20Time%20Series%20Forecasting%20via%0A%20%20Manifold%20Learning&entry.906535625=Ilya%20Kaufman%20and%20Omri%20Azencot&entry.1292438233=%20%20Transformer%20models%20have%20consistently%20achieved%20remarkable%20results%20in%20various%0Adomains%20such%20as%20natural%20language%20processing%20and%20computer%20vision.%20However%2C%0Adespite%20ongoing%20research%20efforts%20to%20better%20understand%20these%20models%2C%20the%20field%0Astill%20lacks%20a%20comprehensive%20understanding.%20This%20is%20particularly%20true%20for%20deep%0Atime%20series%20forecasting%20methods%2C%20where%20analysis%20and%20understanding%20work%20is%0Arelatively%20limited.%20Time%20series%20data%2C%20unlike%20image%20and%20text%20information%2C%20can%20be%0Amore%20challenging%20to%20interpret%20and%20analyze.%20To%20address%20this%2C%20we%20approach%20the%0Aproblem%20from%20a%20manifold%20learning%20perspective%2C%20assuming%20that%20the%20latent%0Arepresentations%20of%20time%20series%20forecasting%20models%20lie%20next%20to%20a%20low-dimensional%0Amanifold.%20In%20our%20study%2C%20we%20focus%20on%20analyzing%20the%20geometric%20features%20of%20these%0Alatent%20data%20manifolds%2C%20including%20intrinsic%20dimension%20and%20principal%20curvatures.%0AOur%20findings%20reveal%20that%20deep%20transformer%20models%20exhibit%20similar%20geometric%0Abehavior%20across%20layers%2C%20and%20these%20geometric%20features%20are%20correlated%20with%20model%0Aperformance.%20Additionally%2C%20we%20observe%20that%20untrained%20models%20initially%20have%0Adifferent%20structures%2C%20but%20they%20rapidly%20converge%20during%20training.%20By%20leveraging%0Aour%20geometric%20analysis%20and%20differentiable%20tools%2C%20we%20can%20potentially%20design%20new%0Aand%20improved%20deep%20forecasting%20neural%20networks.%20This%20approach%20complements%0Aexisting%20analysis%20studies%20and%20contributes%20to%20a%20better%20understanding%20of%0Atransformer%20models%20in%20the%20context%20of%20time%20series%20forecasting.%20Code%20is%20released%0Aat%20https%3A//github.com/azencot-group/GATLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13792v1&entry.124074799=Read"},
{"title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "author": "Yash Akhauri and Ahmed F AbouElhamayed and Jordan Dotzel and Zhiru Zhang and Alexander M Rush and Safeen Huda and Mohamed S Abdelfattah", "abstract": "  The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated efficiency techniques like quantization\nand sparsity. Contextual sparsity, where the sparsity pattern is\ninput-dependent, is crucial in LLMs because the permanent removal of attention\nheads or neurons from LLMs can significantly degrade accuracy. Prior work has\nattempted to model contextual sparsity using neural networks trained to predict\nactivation magnitudes, which can be used to dynamically prune structures with\nlow predicted activation magnitude. In this paper, we look beyond\nmagnitude-based pruning criteria to assess attention head and neuron importance\nin LLMs. We develop a novel predictor called ShadowLLM, which can shadow the\nLLM behavior and enforce better sparsity patterns, resulting in over 15%\nimprovement in end-to-end accuracy compared to prior methods. In addition,\nShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on Llama-2 and OPT models with up\nto 30 billion parameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.\n", "link": "http://arxiv.org/abs/2406.16635v2", "date": "2024-10-17", "relevancy": 1.9819, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5134}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShadowLLM%3A%20Predictor-based%20Contextual%20Sparsity%20for%20Large%20Language%20Models&body=Title%3A%20ShadowLLM%3A%20Predictor-based%20Contextual%20Sparsity%20for%20Large%20Language%20Models%0AAuthor%3A%20Yash%20Akhauri%20and%20Ahmed%20F%20AbouElhamayed%20and%20Jordan%20Dotzel%20and%20Zhiru%20Zhang%20and%20Alexander%20M%20Rush%20and%20Safeen%20Huda%20and%20Mohamed%20S%20Abdelfattah%0AAbstract%3A%20%20%20The%20high%20power%20consumption%20and%20latency-sensitive%20deployments%20of%20large%0Alanguage%20models%20%28LLMs%29%20have%20motivated%20efficiency%20techniques%20like%20quantization%0Aand%20sparsity.%20Contextual%20sparsity%2C%20where%20the%20sparsity%20pattern%20is%0Ainput-dependent%2C%20is%20crucial%20in%20LLMs%20because%20the%20permanent%20removal%20of%20attention%0Aheads%20or%20neurons%20from%20LLMs%20can%20significantly%20degrade%20accuracy.%20Prior%20work%20has%0Aattempted%20to%20model%20contextual%20sparsity%20using%20neural%20networks%20trained%20to%20predict%0Aactivation%20magnitudes%2C%20which%20can%20be%20used%20to%20dynamically%20prune%20structures%20with%0Alow%20predicted%20activation%20magnitude.%20In%20this%20paper%2C%20we%20look%20beyond%0Amagnitude-based%20pruning%20criteria%20to%20assess%20attention%20head%20and%20neuron%20importance%0Ain%20LLMs.%20We%20develop%20a%20novel%20predictor%20called%20ShadowLLM%2C%20which%20can%20shadow%20the%0ALLM%20behavior%20and%20enforce%20better%20sparsity%20patterns%2C%20resulting%20in%20over%2015%25%0Aimprovement%20in%20end-to-end%20accuracy%20compared%20to%20prior%20methods.%20In%20addition%2C%0AShadowLLM%20achieves%20up%20to%20a%2020%25%20speed-up%20over%20the%20state-of-the-art%20DejaVu%0Aframework.%20These%20enhancements%20are%20validated%20on%20Llama-2%20and%20OPT%20models%20with%20up%0Ato%2030%20billion%20parameters.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/abdelfattah-lab/shadow_llm/%7D%7BShadowLLM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShadowLLM%253A%2520Predictor-based%2520Contextual%2520Sparsity%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYash%2520Akhauri%2520and%2520Ahmed%2520F%2520AbouElhamayed%2520and%2520Jordan%2520Dotzel%2520and%2520Zhiru%2520Zhang%2520and%2520Alexander%2520M%2520Rush%2520and%2520Safeen%2520Huda%2520and%2520Mohamed%2520S%2520Abdelfattah%26entry.1292438233%3D%2520%2520The%2520high%2520power%2520consumption%2520and%2520latency-sensitive%2520deployments%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520motivated%2520efficiency%2520techniques%2520like%2520quantization%250Aand%2520sparsity.%2520Contextual%2520sparsity%252C%2520where%2520the%2520sparsity%2520pattern%2520is%250Ainput-dependent%252C%2520is%2520crucial%2520in%2520LLMs%2520because%2520the%2520permanent%2520removal%2520of%2520attention%250Aheads%2520or%2520neurons%2520from%2520LLMs%2520can%2520significantly%2520degrade%2520accuracy.%2520Prior%2520work%2520has%250Aattempted%2520to%2520model%2520contextual%2520sparsity%2520using%2520neural%2520networks%2520trained%2520to%2520predict%250Aactivation%2520magnitudes%252C%2520which%2520can%2520be%2520used%2520to%2520dynamically%2520prune%2520structures%2520with%250Alow%2520predicted%2520activation%2520magnitude.%2520In%2520this%2520paper%252C%2520we%2520look%2520beyond%250Amagnitude-based%2520pruning%2520criteria%2520to%2520assess%2520attention%2520head%2520and%2520neuron%2520importance%250Ain%2520LLMs.%2520We%2520develop%2520a%2520novel%2520predictor%2520called%2520ShadowLLM%252C%2520which%2520can%2520shadow%2520the%250ALLM%2520behavior%2520and%2520enforce%2520better%2520sparsity%2520patterns%252C%2520resulting%2520in%2520over%252015%2525%250Aimprovement%2520in%2520end-to-end%2520accuracy%2520compared%2520to%2520prior%2520methods.%2520In%2520addition%252C%250AShadowLLM%2520achieves%2520up%2520to%2520a%252020%2525%2520speed-up%2520over%2520the%2520state-of-the-art%2520DejaVu%250Aframework.%2520These%2520enhancements%2520are%2520validated%2520on%2520Llama-2%2520and%2520OPT%2520models%2520with%2520up%250Ato%252030%2520billion%2520parameters.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/abdelfattah-lab/shadow_llm/%257D%257BShadowLLM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShadowLLM%3A%20Predictor-based%20Contextual%20Sparsity%20for%20Large%20Language%20Models&entry.906535625=Yash%20Akhauri%20and%20Ahmed%20F%20AbouElhamayed%20and%20Jordan%20Dotzel%20and%20Zhiru%20Zhang%20and%20Alexander%20M%20Rush%20and%20Safeen%20Huda%20and%20Mohamed%20S%20Abdelfattah&entry.1292438233=%20%20The%20high%20power%20consumption%20and%20latency-sensitive%20deployments%20of%20large%0Alanguage%20models%20%28LLMs%29%20have%20motivated%20efficiency%20techniques%20like%20quantization%0Aand%20sparsity.%20Contextual%20sparsity%2C%20where%20the%20sparsity%20pattern%20is%0Ainput-dependent%2C%20is%20crucial%20in%20LLMs%20because%20the%20permanent%20removal%20of%20attention%0Aheads%20or%20neurons%20from%20LLMs%20can%20significantly%20degrade%20accuracy.%20Prior%20work%20has%0Aattempted%20to%20model%20contextual%20sparsity%20using%20neural%20networks%20trained%20to%20predict%0Aactivation%20magnitudes%2C%20which%20can%20be%20used%20to%20dynamically%20prune%20structures%20with%0Alow%20predicted%20activation%20magnitude.%20In%20this%20paper%2C%20we%20look%20beyond%0Amagnitude-based%20pruning%20criteria%20to%20assess%20attention%20head%20and%20neuron%20importance%0Ain%20LLMs.%20We%20develop%20a%20novel%20predictor%20called%20ShadowLLM%2C%20which%20can%20shadow%20the%0ALLM%20behavior%20and%20enforce%20better%20sparsity%20patterns%2C%20resulting%20in%20over%2015%25%0Aimprovement%20in%20end-to-end%20accuracy%20compared%20to%20prior%20methods.%20In%20addition%2C%0AShadowLLM%20achieves%20up%20to%20a%2020%25%20speed-up%20over%20the%20state-of-the-art%20DejaVu%0Aframework.%20These%20enhancements%20are%20validated%20on%20Llama-2%20and%20OPT%20models%20with%20up%0Ato%2030%20billion%20parameters.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/abdelfattah-lab/shadow_llm/%7D%7BShadowLLM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16635v2&entry.124074799=Read"},
{"title": "Transformer Guided Coevolution: Improved Team Formation in Multiagent\n  Adversarial Games", "author": "Pranav Rajbhandari and Prithviraj Dasgupta and Donald Sofge", "abstract": "  We consider the problem of team formation within multiagent adversarial\ngames. We propose BERTeam, a novel algorithm that uses a transformer-based deep\nneural network with Masked Language Model training to select the best team of\nplayers from a trained population. We integrate this with coevolutionary deep\nreinforcement learning, which trains a diverse set of individual players to\nchoose teams from. We test our algorithm in the multiagent adversarial game\nMarine Capture-The-Flag, and we find that BERTeam learns non-trivial team\ncompositions that perform well against unseen opponents. For this game, we find\nthat BERTeam outperforms MCAA, an algorithm that similarly optimizes team\nformation.\n", "link": "http://arxiv.org/abs/2410.13769v1", "date": "2024-10-17", "relevancy": 1.9467, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4854}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20Guided%20Coevolution%3A%20Improved%20Team%20Formation%20in%20Multiagent%0A%20%20Adversarial%20Games&body=Title%3A%20Transformer%20Guided%20Coevolution%3A%20Improved%20Team%20Formation%20in%20Multiagent%0A%20%20Adversarial%20Games%0AAuthor%3A%20Pranav%20Rajbhandari%20and%20Prithviraj%20Dasgupta%20and%20Donald%20Sofge%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20team%20formation%20within%20multiagent%20adversarial%0Agames.%20We%20propose%20BERTeam%2C%20a%20novel%20algorithm%20that%20uses%20a%20transformer-based%20deep%0Aneural%20network%20with%20Masked%20Language%20Model%20training%20to%20select%20the%20best%20team%20of%0Aplayers%20from%20a%20trained%20population.%20We%20integrate%20this%20with%20coevolutionary%20deep%0Areinforcement%20learning%2C%20which%20trains%20a%20diverse%20set%20of%20individual%20players%20to%0Achoose%20teams%20from.%20We%20test%20our%20algorithm%20in%20the%20multiagent%20adversarial%20game%0AMarine%20Capture-The-Flag%2C%20and%20we%20find%20that%20BERTeam%20learns%20non-trivial%20team%0Acompositions%20that%20perform%20well%20against%20unseen%20opponents.%20For%20this%20game%2C%20we%20find%0Athat%20BERTeam%20outperforms%20MCAA%2C%20an%20algorithm%20that%20similarly%20optimizes%20team%0Aformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520Guided%2520Coevolution%253A%2520Improved%2520Team%2520Formation%2520in%2520Multiagent%250A%2520%2520Adversarial%2520Games%26entry.906535625%3DPranav%2520Rajbhandari%2520and%2520Prithviraj%2520Dasgupta%2520and%2520Donald%2520Sofge%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520team%2520formation%2520within%2520multiagent%2520adversarial%250Agames.%2520We%2520propose%2520BERTeam%252C%2520a%2520novel%2520algorithm%2520that%2520uses%2520a%2520transformer-based%2520deep%250Aneural%2520network%2520with%2520Masked%2520Language%2520Model%2520training%2520to%2520select%2520the%2520best%2520team%2520of%250Aplayers%2520from%2520a%2520trained%2520population.%2520We%2520integrate%2520this%2520with%2520coevolutionary%2520deep%250Areinforcement%2520learning%252C%2520which%2520trains%2520a%2520diverse%2520set%2520of%2520individual%2520players%2520to%250Achoose%2520teams%2520from.%2520We%2520test%2520our%2520algorithm%2520in%2520the%2520multiagent%2520adversarial%2520game%250AMarine%2520Capture-The-Flag%252C%2520and%2520we%2520find%2520that%2520BERTeam%2520learns%2520non-trivial%2520team%250Acompositions%2520that%2520perform%2520well%2520against%2520unseen%2520opponents.%2520For%2520this%2520game%252C%2520we%2520find%250Athat%2520BERTeam%2520outperforms%2520MCAA%252C%2520an%2520algorithm%2520that%2520similarly%2520optimizes%2520team%250Aformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20Guided%20Coevolution%3A%20Improved%20Team%20Formation%20in%20Multiagent%0A%20%20Adversarial%20Games&entry.906535625=Pranav%20Rajbhandari%20and%20Prithviraj%20Dasgupta%20and%20Donald%20Sofge&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20team%20formation%20within%20multiagent%20adversarial%0Agames.%20We%20propose%20BERTeam%2C%20a%20novel%20algorithm%20that%20uses%20a%20transformer-based%20deep%0Aneural%20network%20with%20Masked%20Language%20Model%20training%20to%20select%20the%20best%20team%20of%0Aplayers%20from%20a%20trained%20population.%20We%20integrate%20this%20with%20coevolutionary%20deep%0Areinforcement%20learning%2C%20which%20trains%20a%20diverse%20set%20of%20individual%20players%20to%0Achoose%20teams%20from.%20We%20test%20our%20algorithm%20in%20the%20multiagent%20adversarial%20game%0AMarine%20Capture-The-Flag%2C%20and%20we%20find%20that%20BERTeam%20learns%20non-trivial%20team%0Acompositions%20that%20perform%20well%20against%20unseen%20opponents.%20For%20this%20game%2C%20we%20find%0Athat%20BERTeam%20outperforms%20MCAA%2C%20an%20algorithm%20that%20similarly%20optimizes%20team%0Aformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13769v1&entry.124074799=Read"},
{"title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach", "author": "Zhuowan Li and Cheng Li and Mingyang Zhang and Qiaozhu Mei and Michael Bendersky", "abstract": "  Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.\n", "link": "http://arxiv.org/abs/2407.16833v2", "date": "2024-10-17", "relevancy": 1.9428, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4893}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Augmented%20Generation%20or%20Long-Context%20LLMs%3F%20A%20Comprehensive%0A%20%20Study%20and%20Hybrid%20Approach&body=Title%3A%20Retrieval%20Augmented%20Generation%20or%20Long-Context%20LLMs%3F%20A%20Comprehensive%0A%20%20Study%20and%20Hybrid%20Approach%0AAuthor%3A%20Zhuowan%20Li%20and%20Cheng%20Li%20and%20Mingyang%20Zhang%20and%20Qiaozhu%20Mei%20and%20Michael%20Bendersky%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20been%20a%20powerful%20tool%20for%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20efficiently%20process%20overly%20lengthy%20contexts.%20However%2C%0Arecent%20LLMs%20like%20Gemini-1.5%20and%20GPT-4%20show%20exceptional%20capabilities%20to%0Aunderstand%20long%20contexts%20directly.%20We%20conduct%20a%20comprehensive%20comparison%0Abetween%20RAG%20and%20long-context%20%28LC%29%20LLMs%2C%20aiming%20to%20leverage%20the%20strengths%20of%0Aboth.%20We%20benchmark%20RAG%20and%20LC%20across%20various%20public%20datasets%20using%20three%20latest%0ALLMs.%20Results%20reveal%20that%20when%20resourced%20sufficiently%2C%20LC%20consistently%0Aoutperforms%20RAG%20in%20terms%20of%20average%20performance.%20However%2C%20RAG%27s%20significantly%0Alower%20cost%20remains%20a%20distinct%20advantage.%20Based%20on%20this%20observation%2C%20we%20propose%0ASelf-Route%2C%20a%20simple%20yet%20effective%20method%20that%20routes%20queries%20to%20RAG%20or%20LC%0Abased%20on%20model%20self-reflection.%20Self-Route%20significantly%20reduces%20the%0Acomputation%20cost%20while%20maintaining%20a%20comparable%20performance%20to%20LC.%20Our%20findings%0Aprovide%20a%20guideline%20for%20long-context%20applications%20of%20LLMs%20using%20RAG%20and%20LC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Augmented%2520Generation%2520or%2520Long-Context%2520LLMs%253F%2520A%2520Comprehensive%250A%2520%2520Study%2520and%2520Hybrid%2520Approach%26entry.906535625%3DZhuowan%2520Li%2520and%2520Cheng%2520Li%2520and%2520Mingyang%2520Zhang%2520and%2520Qiaozhu%2520Mei%2520and%2520Michael%2520Bendersky%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520has%2520been%2520a%2520powerful%2520tool%2520for%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520efficiently%2520process%2520overly%2520lengthy%2520contexts.%2520However%252C%250Arecent%2520LLMs%2520like%2520Gemini-1.5%2520and%2520GPT-4%2520show%2520exceptional%2520capabilities%2520to%250Aunderstand%2520long%2520contexts%2520directly.%2520We%2520conduct%2520a%2520comprehensive%2520comparison%250Abetween%2520RAG%2520and%2520long-context%2520%2528LC%2529%2520LLMs%252C%2520aiming%2520to%2520leverage%2520the%2520strengths%2520of%250Aboth.%2520We%2520benchmark%2520RAG%2520and%2520LC%2520across%2520various%2520public%2520datasets%2520using%2520three%2520latest%250ALLMs.%2520Results%2520reveal%2520that%2520when%2520resourced%2520sufficiently%252C%2520LC%2520consistently%250Aoutperforms%2520RAG%2520in%2520terms%2520of%2520average%2520performance.%2520However%252C%2520RAG%2527s%2520significantly%250Alower%2520cost%2520remains%2520a%2520distinct%2520advantage.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%250ASelf-Route%252C%2520a%2520simple%2520yet%2520effective%2520method%2520that%2520routes%2520queries%2520to%2520RAG%2520or%2520LC%250Abased%2520on%2520model%2520self-reflection.%2520Self-Route%2520significantly%2520reduces%2520the%250Acomputation%2520cost%2520while%2520maintaining%2520a%2520comparable%2520performance%2520to%2520LC.%2520Our%2520findings%250Aprovide%2520a%2520guideline%2520for%2520long-context%2520applications%2520of%2520LLMs%2520using%2520RAG%2520and%2520LC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Augmented%20Generation%20or%20Long-Context%20LLMs%3F%20A%20Comprehensive%0A%20%20Study%20and%20Hybrid%20Approach&entry.906535625=Zhuowan%20Li%20and%20Cheng%20Li%20and%20Mingyang%20Zhang%20and%20Qiaozhu%20Mei%20and%20Michael%20Bendersky&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20been%20a%20powerful%20tool%20for%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20efficiently%20process%20overly%20lengthy%20contexts.%20However%2C%0Arecent%20LLMs%20like%20Gemini-1.5%20and%20GPT-4%20show%20exceptional%20capabilities%20to%0Aunderstand%20long%20contexts%20directly.%20We%20conduct%20a%20comprehensive%20comparison%0Abetween%20RAG%20and%20long-context%20%28LC%29%20LLMs%2C%20aiming%20to%20leverage%20the%20strengths%20of%0Aboth.%20We%20benchmark%20RAG%20and%20LC%20across%20various%20public%20datasets%20using%20three%20latest%0ALLMs.%20Results%20reveal%20that%20when%20resourced%20sufficiently%2C%20LC%20consistently%0Aoutperforms%20RAG%20in%20terms%20of%20average%20performance.%20However%2C%20RAG%27s%20significantly%0Alower%20cost%20remains%20a%20distinct%20advantage.%20Based%20on%20this%20observation%2C%20we%20propose%0ASelf-Route%2C%20a%20simple%20yet%20effective%20method%20that%20routes%20queries%20to%20RAG%20or%20LC%0Abased%20on%20model%20self-reflection.%20Self-Route%20significantly%20reduces%20the%0Acomputation%20cost%20while%20maintaining%20a%20comparable%20performance%20to%20LC.%20Our%20findings%0Aprovide%20a%20guideline%20for%20long-context%20applications%20of%20LLMs%20using%20RAG%20and%20LC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16833v2&entry.124074799=Read"},
{"title": "Block-Attention for Efficient RAG", "author": "East Sun and Yan Wang and Lan Tian", "abstract": "  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n", "link": "http://arxiv.org/abs/2409.15355v4", "date": "2024-10-17", "relevancy": 1.9398, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.523}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Block-Attention%20for%20Efficient%20RAG&body=Title%3A%20Block-Attention%20for%20Efficient%20RAG%0AAuthor%3A%20East%20Sun%20and%20Yan%20Wang%20and%20Lan%20Tian%0AAbstract%3A%20%20%20We%20introduce%20Block-Attention%2C%20an%20attention%20mechanism%20designed%20to%20address%20the%0Aincreased%20inference%20latency%20and%20cost%20in%20Retrieval-Augmented%20Generation%20%28RAG%29%0Ascenarios.%20Traditional%20approaches%20often%20encode%20the%20entire%20context.%20Instead%2C%0ABlock-Attention%20divides%20retrieved%20documents%20into%20discrete%20blocks%2C%20with%20each%0Ablock%20independently%20calculating%20key-value%20%28KV%29%20states%20except%20for%20the%20final%0Ablock.%20In%20RAG%20scenarios%2C%20by%20defining%20each%20passage%20as%20a%20block%2C%20Block-Attention%0Aenables%20us%20to%20reuse%20the%20KV%20states%20of%20passages%20that%20have%20been%20seen%20before%2C%0Athereby%20significantly%20reducing%20the%20latency%20and%20the%20computation%20overhead%20during%0Ainference.%20The%20implementation%20of%20Block-Attention%20involves%20block%20segmentation%2C%0Aposition%20re-encoding%2C%20and%20fine-tuning%20the%20LLM%20to%20adapt%20to%20the%20Block-Attention%0Amechanism.%20Experiments%20on%20four%20RAG%20benchmarks%20demonstrate%20that%20after%20block%0Afine-tuning%2C%20the%20Block-Attention%20model%20achieves%20performance%20comparable%20to%0Aself-attention%20models%20%2868.4%5C%25%20vs%2067.9%5C%25%20on%20Llama3%29%20or%20even%20superior%20performance%0A%2862.8%5C%25%20vs%2059.6%5C%25%20on%20Mistral%29.%20Notably%2C%20Block-Attention%20significantly%20reduces%0Athe%20time%20to%20first%20token%20%28TTFT%29%20and%20floating%20point%20operations%20%28FLOPs%29%20to%20a%20very%0Alow%20level.%20It%20only%20takes%2045%20ms%20to%20output%20the%20first%20token%20for%20an%20input%20sequence%0Awith%20a%20total%20length%20of%2032K.%20Compared%20to%20the%20self-attention%20models%2C%20the%20time%0Aconsumption%20and%20corresponding%20FLOPs%20are%20reduced%20by%2098.7%5C%25%20and%2099.8%5C%25%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15355v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlock-Attention%2520for%2520Efficient%2520RAG%26entry.906535625%3DEast%2520Sun%2520and%2520Yan%2520Wang%2520and%2520Lan%2520Tian%26entry.1292438233%3D%2520%2520We%2520introduce%2520Block-Attention%252C%2520an%2520attention%2520mechanism%2520designed%2520to%2520address%2520the%250Aincreased%2520inference%2520latency%2520and%2520cost%2520in%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%250Ascenarios.%2520Traditional%2520approaches%2520often%2520encode%2520the%2520entire%2520context.%2520Instead%252C%250ABlock-Attention%2520divides%2520retrieved%2520documents%2520into%2520discrete%2520blocks%252C%2520with%2520each%250Ablock%2520independently%2520calculating%2520key-value%2520%2528KV%2529%2520states%2520except%2520for%2520the%2520final%250Ablock.%2520In%2520RAG%2520scenarios%252C%2520by%2520defining%2520each%2520passage%2520as%2520a%2520block%252C%2520Block-Attention%250Aenables%2520us%2520to%2520reuse%2520the%2520KV%2520states%2520of%2520passages%2520that%2520have%2520been%2520seen%2520before%252C%250Athereby%2520significantly%2520reducing%2520the%2520latency%2520and%2520the%2520computation%2520overhead%2520during%250Ainference.%2520The%2520implementation%2520of%2520Block-Attention%2520involves%2520block%2520segmentation%252C%250Aposition%2520re-encoding%252C%2520and%2520fine-tuning%2520the%2520LLM%2520to%2520adapt%2520to%2520the%2520Block-Attention%250Amechanism.%2520Experiments%2520on%2520four%2520RAG%2520benchmarks%2520demonstrate%2520that%2520after%2520block%250Afine-tuning%252C%2520the%2520Block-Attention%2520model%2520achieves%2520performance%2520comparable%2520to%250Aself-attention%2520models%2520%252868.4%255C%2525%2520vs%252067.9%255C%2525%2520on%2520Llama3%2529%2520or%2520even%2520superior%2520performance%250A%252862.8%255C%2525%2520vs%252059.6%255C%2525%2520on%2520Mistral%2529.%2520Notably%252C%2520Block-Attention%2520significantly%2520reduces%250Athe%2520time%2520to%2520first%2520token%2520%2528TTFT%2529%2520and%2520floating%2520point%2520operations%2520%2528FLOPs%2529%2520to%2520a%2520very%250Alow%2520level.%2520It%2520only%2520takes%252045%2520ms%2520to%2520output%2520the%2520first%2520token%2520for%2520an%2520input%2520sequence%250Awith%2520a%2520total%2520length%2520of%252032K.%2520Compared%2520to%2520the%2520self-attention%2520models%252C%2520the%2520time%250Aconsumption%2520and%2520corresponding%2520FLOPs%2520are%2520reduced%2520by%252098.7%255C%2525%2520and%252099.8%255C%2525%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15355v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Block-Attention%20for%20Efficient%20RAG&entry.906535625=East%20Sun%20and%20Yan%20Wang%20and%20Lan%20Tian&entry.1292438233=%20%20We%20introduce%20Block-Attention%2C%20an%20attention%20mechanism%20designed%20to%20address%20the%0Aincreased%20inference%20latency%20and%20cost%20in%20Retrieval-Augmented%20Generation%20%28RAG%29%0Ascenarios.%20Traditional%20approaches%20often%20encode%20the%20entire%20context.%20Instead%2C%0ABlock-Attention%20divides%20retrieved%20documents%20into%20discrete%20blocks%2C%20with%20each%0Ablock%20independently%20calculating%20key-value%20%28KV%29%20states%20except%20for%20the%20final%0Ablock.%20In%20RAG%20scenarios%2C%20by%20defining%20each%20passage%20as%20a%20block%2C%20Block-Attention%0Aenables%20us%20to%20reuse%20the%20KV%20states%20of%20passages%20that%20have%20been%20seen%20before%2C%0Athereby%20significantly%20reducing%20the%20latency%20and%20the%20computation%20overhead%20during%0Ainference.%20The%20implementation%20of%20Block-Attention%20involves%20block%20segmentation%2C%0Aposition%20re-encoding%2C%20and%20fine-tuning%20the%20LLM%20to%20adapt%20to%20the%20Block-Attention%0Amechanism.%20Experiments%20on%20four%20RAG%20benchmarks%20demonstrate%20that%20after%20block%0Afine-tuning%2C%20the%20Block-Attention%20model%20achieves%20performance%20comparable%20to%0Aself-attention%20models%20%2868.4%5C%25%20vs%2067.9%5C%25%20on%20Llama3%29%20or%20even%20superior%20performance%0A%2862.8%5C%25%20vs%2059.6%5C%25%20on%20Mistral%29.%20Notably%2C%20Block-Attention%20significantly%20reduces%0Athe%20time%20to%20first%20token%20%28TTFT%29%20and%20floating%20point%20operations%20%28FLOPs%29%20to%20a%20very%0Alow%20level.%20It%20only%20takes%2045%20ms%20to%20output%20the%20first%20token%20for%20an%20input%20sequence%0Awith%20a%20total%20length%20of%2032K.%20Compared%20to%20the%20self-attention%20models%2C%20the%20time%0Aconsumption%20and%20corresponding%20FLOPs%20are%20reduced%20by%2098.7%5C%25%20and%2099.8%5C%25%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15355v4&entry.124074799=Read"},
{"title": "Towards Multilingual LLM Evaluation for European Languages", "author": "Klaudia Thellmann and Bernhard Stadler and Michael Fromm and Jasper Schulze Buschhoff and Alex Jude and Fabio Barth and Johannes Leveling and Nicolas Flores-Herr and Joachim K\u00f6hler and Ren\u00e9 J\u00e4kel and Mehdi Ali", "abstract": "  The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of\nlanguage-parallel multilingual benchmarks. We introduce a multilingual\nevaluation approach tailored for European languages. We employ translated\nversions of five widely-used benchmarks to assess the capabilities of 40 LLMs\nacross 21 European languages. Our contributions include examining the\neffectiveness of translated benchmarks, assessing the impact of different\ntranslation services, and offering a multilingual evaluation framework for LLMs\nthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,\nEU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly\navailable to encourage further research in multilingual LLM evaluation.\n", "link": "http://arxiv.org/abs/2410.08928v2", "date": "2024-10-17", "relevancy": 1.9345, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Multilingual%20LLM%20Evaluation%20for%20European%20Languages&body=Title%3A%20Towards%20Multilingual%20LLM%20Evaluation%20for%20European%20Languages%0AAuthor%3A%20Klaudia%20Thellmann%20and%20Bernhard%20Stadler%20and%20Michael%20Fromm%20and%20Jasper%20Schulze%20Buschhoff%20and%20Alex%20Jude%20and%20Fabio%20Barth%20and%20Johannes%20Leveling%20and%20Nicolas%20Flores-Herr%20and%20Joachim%20K%C3%B6hler%20and%20Ren%C3%A9%20J%C3%A4kel%20and%20Mehdi%20Ali%0AAbstract%3A%20%20%20The%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20natural%20language%0Aprocessing%20across%20numerous%20languages%20and%20tasks.%20However%2C%20evaluating%20LLM%0Aperformance%20in%20a%20consistent%20and%20meaningful%20way%20across%20multiple%20European%0Alanguages%20remains%20challenging%2C%20especially%20due%20to%20the%20scarcity%20of%0Alanguage-parallel%20multilingual%20benchmarks.%20We%20introduce%20a%20multilingual%0Aevaluation%20approach%20tailored%20for%20European%20languages.%20We%20employ%20translated%0Aversions%20of%20five%20widely-used%20benchmarks%20to%20assess%20the%20capabilities%20of%2040%20LLMs%0Aacross%2021%20European%20languages.%20Our%20contributions%20include%20examining%20the%0Aeffectiveness%20of%20translated%20benchmarks%2C%20assessing%20the%20impact%20of%20different%0Atranslation%20services%2C%20and%20offering%20a%20multilingual%20evaluation%20framework%20for%20LLMs%0Athat%20includes%20newly%20created%20datasets%3A%20EU20-MMLU%2C%20EU20-HellaSwag%2C%20EU20-ARC%2C%0AEU20-TruthfulQA%2C%20and%20EU20-GSM8K.%20The%20benchmarks%20and%20results%20are%20made%20publicly%0Aavailable%20to%20encourage%20further%20research%20in%20multilingual%20LLM%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Multilingual%2520LLM%2520Evaluation%2520for%2520European%2520Languages%26entry.906535625%3DKlaudia%2520Thellmann%2520and%2520Bernhard%2520Stadler%2520and%2520Michael%2520Fromm%2520and%2520Jasper%2520Schulze%2520Buschhoff%2520and%2520Alex%2520Jude%2520and%2520Fabio%2520Barth%2520and%2520Johannes%2520Leveling%2520and%2520Nicolas%2520Flores-Herr%2520and%2520Joachim%2520K%25C3%25B6hler%2520and%2520Ren%25C3%25A9%2520J%25C3%25A4kel%2520and%2520Mehdi%2520Ali%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520revolutionized%2520natural%2520language%250Aprocessing%2520across%2520numerous%2520languages%2520and%2520tasks.%2520However%252C%2520evaluating%2520LLM%250Aperformance%2520in%2520a%2520consistent%2520and%2520meaningful%2520way%2520across%2520multiple%2520European%250Alanguages%2520remains%2520challenging%252C%2520especially%2520due%2520to%2520the%2520scarcity%2520of%250Alanguage-parallel%2520multilingual%2520benchmarks.%2520We%2520introduce%2520a%2520multilingual%250Aevaluation%2520approach%2520tailored%2520for%2520European%2520languages.%2520We%2520employ%2520translated%250Aversions%2520of%2520five%2520widely-used%2520benchmarks%2520to%2520assess%2520the%2520capabilities%2520of%252040%2520LLMs%250Aacross%252021%2520European%2520languages.%2520Our%2520contributions%2520include%2520examining%2520the%250Aeffectiveness%2520of%2520translated%2520benchmarks%252C%2520assessing%2520the%2520impact%2520of%2520different%250Atranslation%2520services%252C%2520and%2520offering%2520a%2520multilingual%2520evaluation%2520framework%2520for%2520LLMs%250Athat%2520includes%2520newly%2520created%2520datasets%253A%2520EU20-MMLU%252C%2520EU20-HellaSwag%252C%2520EU20-ARC%252C%250AEU20-TruthfulQA%252C%2520and%2520EU20-GSM8K.%2520The%2520benchmarks%2520and%2520results%2520are%2520made%2520publicly%250Aavailable%2520to%2520encourage%2520further%2520research%2520in%2520multilingual%2520LLM%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Multilingual%20LLM%20Evaluation%20for%20European%20Languages&entry.906535625=Klaudia%20Thellmann%20and%20Bernhard%20Stadler%20and%20Michael%20Fromm%20and%20Jasper%20Schulze%20Buschhoff%20and%20Alex%20Jude%20and%20Fabio%20Barth%20and%20Johannes%20Leveling%20and%20Nicolas%20Flores-Herr%20and%20Joachim%20K%C3%B6hler%20and%20Ren%C3%A9%20J%C3%A4kel%20and%20Mehdi%20Ali&entry.1292438233=%20%20The%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20natural%20language%0Aprocessing%20across%20numerous%20languages%20and%20tasks.%20However%2C%20evaluating%20LLM%0Aperformance%20in%20a%20consistent%20and%20meaningful%20way%20across%20multiple%20European%0Alanguages%20remains%20challenging%2C%20especially%20due%20to%20the%20scarcity%20of%0Alanguage-parallel%20multilingual%20benchmarks.%20We%20introduce%20a%20multilingual%0Aevaluation%20approach%20tailored%20for%20European%20languages.%20We%20employ%20translated%0Aversions%20of%20five%20widely-used%20benchmarks%20to%20assess%20the%20capabilities%20of%2040%20LLMs%0Aacross%2021%20European%20languages.%20Our%20contributions%20include%20examining%20the%0Aeffectiveness%20of%20translated%20benchmarks%2C%20assessing%20the%20impact%20of%20different%0Atranslation%20services%2C%20and%20offering%20a%20multilingual%20evaluation%20framework%20for%20LLMs%0Athat%20includes%20newly%20created%20datasets%3A%20EU20-MMLU%2C%20EU20-HellaSwag%2C%20EU20-ARC%2C%0AEU20-TruthfulQA%2C%20and%20EU20-GSM8K.%20The%20benchmarks%20and%20results%20are%20made%20publicly%0Aavailable%20to%20encourage%20further%20research%20in%20multilingual%20LLM%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08928v2&entry.124074799=Read"},
{"title": "MIRAGE-Bench: Automatic Multilingual Benchmark Arena for\n  Retrieval-Augmented Generation Systems", "author": "Nandan Thakur and Suleman Kazi and Ge Luo and Jimmy Lin and Amin Ahmad", "abstract": "  Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different\nheuristic-based metrics for evaluation, but these require human preferences as\nground truth for reference. In contrast, arena-based benchmarks, where two\nmodels compete each other, require an expensive Large Language Model (LLM) as a\njudge for a reliable evaluation. We present an easy and efficient technique to\nget the best of both worlds. The idea is to train a learning to rank model as a\n\"surrogate\" judge using RAG-based evaluation heuristics as input, to produce a\nsynthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a\nstandardized arena-based multilingual RAG benchmark for 18 diverse languages on\nWikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and\nextended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG\nextensively coupling both heuristic features and LLM as a judge evaluator. In\nour work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high\ncorrelation (Kendall Tau ($\\tau$) = 0.909) using our surrogate judge learned\nusing heuristic features with pairwise evaluations and between GPT-4o as a\nteacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We\nobserve proprietary and large open-source LLMs currently dominate in\nmultilingual RAG. MIRAGE-Bench is available at:\nhttps://github.com/vectara/mirage-bench.\n", "link": "http://arxiv.org/abs/2410.13716v1", "date": "2024-10-17", "relevancy": 1.9344, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.485}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIRAGE-Bench%3A%20Automatic%20Multilingual%20Benchmark%20Arena%20for%0A%20%20Retrieval-Augmented%20Generation%20Systems&body=Title%3A%20MIRAGE-Bench%3A%20Automatic%20Multilingual%20Benchmark%20Arena%20for%0A%20%20Retrieval-Augmented%20Generation%20Systems%0AAuthor%3A%20Nandan%20Thakur%20and%20Suleman%20Kazi%20and%20Ge%20Luo%20and%20Jimmy%20Lin%20and%20Amin%20Ahmad%0AAbstract%3A%20%20%20Traditional%20Retrieval-Augmented%20Generation%20%28RAG%29%20benchmarks%20rely%20on%20different%0Aheuristic-based%20metrics%20for%20evaluation%2C%20but%20these%20require%20human%20preferences%20as%0Aground%20truth%20for%20reference.%20In%20contrast%2C%20arena-based%20benchmarks%2C%20where%20two%0Amodels%20compete%20each%20other%2C%20require%20an%20expensive%20Large%20Language%20Model%20%28LLM%29%20as%20a%0Ajudge%20for%20a%20reliable%20evaluation.%20We%20present%20an%20easy%20and%20efficient%20technique%20to%0Aget%20the%20best%20of%20both%20worlds.%20The%20idea%20is%20to%20train%20a%20learning%20to%20rank%20model%20as%20a%0A%22surrogate%22%20judge%20using%20RAG-based%20evaluation%20heuristics%20as%20input%2C%20to%20produce%20a%0Asynthetic%20arena-based%20leaderboard.%20Using%20this%20idea%2C%20We%20develop%20MIRAGE-Bench%2C%20a%0Astandardized%20arena-based%20multilingual%20RAG%20benchmark%20for%2018%20diverse%20languages%20on%0AWikipedia.%20The%20benchmark%20is%20constructed%20using%20MIRACL%2C%20a%20retrieval%20dataset%2C%20and%0Aextended%20for%20multilingual%20generation%20evaluation.%20MIRAGE-Bench%20evaluates%20RAG%0Aextensively%20coupling%20both%20heuristic%20features%20and%20LLM%20as%20a%20judge%20evaluator.%20In%0Aour%20work%2C%20we%20benchmark%2019%20diverse%20multilingual-focused%20LLMs%2C%20and%20achieve%20a%20high%0Acorrelation%20%28Kendall%20Tau%20%28%24%5Ctau%24%29%20%3D%200.909%29%20using%20our%20surrogate%20judge%20learned%0Ausing%20heuristic%20features%20with%20pairwise%20evaluations%20and%20between%20GPT-4o%20as%20a%0Ateacher%20on%20the%20MIRAGE-Bench%20leaderboard%20using%20the%20Bradley-Terry%20framework.%20We%0Aobserve%20proprietary%20and%20large%20open-source%20LLMs%20currently%20dominate%20in%0Amultilingual%20RAG.%20MIRAGE-Bench%20is%20available%20at%3A%0Ahttps%3A//github.com/vectara/mirage-bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIRAGE-Bench%253A%2520Automatic%2520Multilingual%2520Benchmark%2520Arena%2520for%250A%2520%2520Retrieval-Augmented%2520Generation%2520Systems%26entry.906535625%3DNandan%2520Thakur%2520and%2520Suleman%2520Kazi%2520and%2520Ge%2520Luo%2520and%2520Jimmy%2520Lin%2520and%2520Amin%2520Ahmad%26entry.1292438233%3D%2520%2520Traditional%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520benchmarks%2520rely%2520on%2520different%250Aheuristic-based%2520metrics%2520for%2520evaluation%252C%2520but%2520these%2520require%2520human%2520preferences%2520as%250Aground%2520truth%2520for%2520reference.%2520In%2520contrast%252C%2520arena-based%2520benchmarks%252C%2520where%2520two%250Amodels%2520compete%2520each%2520other%252C%2520require%2520an%2520expensive%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520as%2520a%250Ajudge%2520for%2520a%2520reliable%2520evaluation.%2520We%2520present%2520an%2520easy%2520and%2520efficient%2520technique%2520to%250Aget%2520the%2520best%2520of%2520both%2520worlds.%2520The%2520idea%2520is%2520to%2520train%2520a%2520learning%2520to%2520rank%2520model%2520as%2520a%250A%2522surrogate%2522%2520judge%2520using%2520RAG-based%2520evaluation%2520heuristics%2520as%2520input%252C%2520to%2520produce%2520a%250Asynthetic%2520arena-based%2520leaderboard.%2520Using%2520this%2520idea%252C%2520We%2520develop%2520MIRAGE-Bench%252C%2520a%250Astandardized%2520arena-based%2520multilingual%2520RAG%2520benchmark%2520for%252018%2520diverse%2520languages%2520on%250AWikipedia.%2520The%2520benchmark%2520is%2520constructed%2520using%2520MIRACL%252C%2520a%2520retrieval%2520dataset%252C%2520and%250Aextended%2520for%2520multilingual%2520generation%2520evaluation.%2520MIRAGE-Bench%2520evaluates%2520RAG%250Aextensively%2520coupling%2520both%2520heuristic%2520features%2520and%2520LLM%2520as%2520a%2520judge%2520evaluator.%2520In%250Aour%2520work%252C%2520we%2520benchmark%252019%2520diverse%2520multilingual-focused%2520LLMs%252C%2520and%2520achieve%2520a%2520high%250Acorrelation%2520%2528Kendall%2520Tau%2520%2528%2524%255Ctau%2524%2529%2520%253D%25200.909%2529%2520using%2520our%2520surrogate%2520judge%2520learned%250Ausing%2520heuristic%2520features%2520with%2520pairwise%2520evaluations%2520and%2520between%2520GPT-4o%2520as%2520a%250Ateacher%2520on%2520the%2520MIRAGE-Bench%2520leaderboard%2520using%2520the%2520Bradley-Terry%2520framework.%2520We%250Aobserve%2520proprietary%2520and%2520large%2520open-source%2520LLMs%2520currently%2520dominate%2520in%250Amultilingual%2520RAG.%2520MIRAGE-Bench%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/vectara/mirage-bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIRAGE-Bench%3A%20Automatic%20Multilingual%20Benchmark%20Arena%20for%0A%20%20Retrieval-Augmented%20Generation%20Systems&entry.906535625=Nandan%20Thakur%20and%20Suleman%20Kazi%20and%20Ge%20Luo%20and%20Jimmy%20Lin%20and%20Amin%20Ahmad&entry.1292438233=%20%20Traditional%20Retrieval-Augmented%20Generation%20%28RAG%29%20benchmarks%20rely%20on%20different%0Aheuristic-based%20metrics%20for%20evaluation%2C%20but%20these%20require%20human%20preferences%20as%0Aground%20truth%20for%20reference.%20In%20contrast%2C%20arena-based%20benchmarks%2C%20where%20two%0Amodels%20compete%20each%20other%2C%20require%20an%20expensive%20Large%20Language%20Model%20%28LLM%29%20as%20a%0Ajudge%20for%20a%20reliable%20evaluation.%20We%20present%20an%20easy%20and%20efficient%20technique%20to%0Aget%20the%20best%20of%20both%20worlds.%20The%20idea%20is%20to%20train%20a%20learning%20to%20rank%20model%20as%20a%0A%22surrogate%22%20judge%20using%20RAG-based%20evaluation%20heuristics%20as%20input%2C%20to%20produce%20a%0Asynthetic%20arena-based%20leaderboard.%20Using%20this%20idea%2C%20We%20develop%20MIRAGE-Bench%2C%20a%0Astandardized%20arena-based%20multilingual%20RAG%20benchmark%20for%2018%20diverse%20languages%20on%0AWikipedia.%20The%20benchmark%20is%20constructed%20using%20MIRACL%2C%20a%20retrieval%20dataset%2C%20and%0Aextended%20for%20multilingual%20generation%20evaluation.%20MIRAGE-Bench%20evaluates%20RAG%0Aextensively%20coupling%20both%20heuristic%20features%20and%20LLM%20as%20a%20judge%20evaluator.%20In%0Aour%20work%2C%20we%20benchmark%2019%20diverse%20multilingual-focused%20LLMs%2C%20and%20achieve%20a%20high%0Acorrelation%20%28Kendall%20Tau%20%28%24%5Ctau%24%29%20%3D%200.909%29%20using%20our%20surrogate%20judge%20learned%0Ausing%20heuristic%20features%20with%20pairwise%20evaluations%20and%20between%20GPT-4o%20as%20a%0Ateacher%20on%20the%20MIRAGE-Bench%20leaderboard%20using%20the%20Bradley-Terry%20framework.%20We%0Aobserve%20proprietary%20and%20large%20open-source%20LLMs%20currently%20dominate%20in%0Amultilingual%20RAG.%20MIRAGE-Bench%20is%20available%20at%3A%0Ahttps%3A//github.com/vectara/mirage-bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13716v1&entry.124074799=Read"},
{"title": "Eyelid Fold Consistency in Facial Modeling", "author": "Lohit Petikam and Charlie Hewitt and Fatemeh Saleh and Tadas Baltru\u0161aitis", "abstract": "  Eyelid shape is integral to identity and likeness in human facial modeling.\nHuman eyelids are diverse in appearance with varied skin fold and epicanthal\nfold morphology between individuals. Existing parametric face models express\neyelid shape variation to an extent, but do not preserve sufficient likeness\nacross a diverse range of individuals. We propose a new definition of eyelid\nfold consistency and implement geometric processing techniques to model diverse\neyelid shapes in a unified topology. Using this method we reprocess data used\nto train a parametric face model and demonstrate significant improvements in\nface-related machine learning tasks.\n", "link": "http://arxiv.org/abs/2410.13760v1", "date": "2024-10-17", "relevancy": 1.9223, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4855}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4855}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyelid%20Fold%20Consistency%20in%20Facial%20Modeling&body=Title%3A%20Eyelid%20Fold%20Consistency%20in%20Facial%20Modeling%0AAuthor%3A%20Lohit%20Petikam%20and%20Charlie%20Hewitt%20and%20Fatemeh%20Saleh%20and%20Tadas%20Baltru%C5%A1aitis%0AAbstract%3A%20%20%20Eyelid%20shape%20is%20integral%20to%20identity%20and%20likeness%20in%20human%20facial%20modeling.%0AHuman%20eyelids%20are%20diverse%20in%20appearance%20with%20varied%20skin%20fold%20and%20epicanthal%0Afold%20morphology%20between%20individuals.%20Existing%20parametric%20face%20models%20express%0Aeyelid%20shape%20variation%20to%20an%20extent%2C%20but%20do%20not%20preserve%20sufficient%20likeness%0Aacross%20a%20diverse%20range%20of%20individuals.%20We%20propose%20a%20new%20definition%20of%20eyelid%0Afold%20consistency%20and%20implement%20geometric%20processing%20techniques%20to%20model%20diverse%0Aeyelid%20shapes%20in%20a%20unified%20topology.%20Using%20this%20method%20we%20reprocess%20data%20used%0Ato%20train%20a%20parametric%20face%20model%20and%20demonstrate%20significant%20improvements%20in%0Aface-related%20machine%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyelid%2520Fold%2520Consistency%2520in%2520Facial%2520Modeling%26entry.906535625%3DLohit%2520Petikam%2520and%2520Charlie%2520Hewitt%2520and%2520Fatemeh%2520Saleh%2520and%2520Tadas%2520Baltru%25C5%25A1aitis%26entry.1292438233%3D%2520%2520Eyelid%2520shape%2520is%2520integral%2520to%2520identity%2520and%2520likeness%2520in%2520human%2520facial%2520modeling.%250AHuman%2520eyelids%2520are%2520diverse%2520in%2520appearance%2520with%2520varied%2520skin%2520fold%2520and%2520epicanthal%250Afold%2520morphology%2520between%2520individuals.%2520Existing%2520parametric%2520face%2520models%2520express%250Aeyelid%2520shape%2520variation%2520to%2520an%2520extent%252C%2520but%2520do%2520not%2520preserve%2520sufficient%2520likeness%250Aacross%2520a%2520diverse%2520range%2520of%2520individuals.%2520We%2520propose%2520a%2520new%2520definition%2520of%2520eyelid%250Afold%2520consistency%2520and%2520implement%2520geometric%2520processing%2520techniques%2520to%2520model%2520diverse%250Aeyelid%2520shapes%2520in%2520a%2520unified%2520topology.%2520Using%2520this%2520method%2520we%2520reprocess%2520data%2520used%250Ato%2520train%2520a%2520parametric%2520face%2520model%2520and%2520demonstrate%2520significant%2520improvements%2520in%250Aface-related%2520machine%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyelid%20Fold%20Consistency%20in%20Facial%20Modeling&entry.906535625=Lohit%20Petikam%20and%20Charlie%20Hewitt%20and%20Fatemeh%20Saleh%20and%20Tadas%20Baltru%C5%A1aitis&entry.1292438233=%20%20Eyelid%20shape%20is%20integral%20to%20identity%20and%20likeness%20in%20human%20facial%20modeling.%0AHuman%20eyelids%20are%20diverse%20in%20appearance%20with%20varied%20skin%20fold%20and%20epicanthal%0Afold%20morphology%20between%20individuals.%20Existing%20parametric%20face%20models%20express%0Aeyelid%20shape%20variation%20to%20an%20extent%2C%20but%20do%20not%20preserve%20sufficient%20likeness%0Aacross%20a%20diverse%20range%20of%20individuals.%20We%20propose%20a%20new%20definition%20of%20eyelid%0Afold%20consistency%20and%20implement%20geometric%20processing%20techniques%20to%20model%20diverse%0Aeyelid%20shapes%20in%20a%20unified%20topology.%20Using%20this%20method%20we%20reprocess%20data%20used%0Ato%20train%20a%20parametric%20face%20model%20and%20demonstrate%20significant%20improvements%20in%0Aface-related%20machine%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13760v1&entry.124074799=Read"},
{"title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction", "author": "Xuan Zhang and Cunxiao Du and Chao Du and Tianyu Pang and Wei Gao and Min Lin", "abstract": "  Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.\n", "link": "http://arxiv.org/abs/2410.13846v1", "date": "2024-10-17", "relevancy": 1.9215, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4823}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimLayerKV%3A%20A%20Simple%20Framework%20for%20Layer-Level%20KV%20Cache%20Reduction&body=Title%3A%20SimLayerKV%3A%20A%20Simple%20Framework%20for%20Layer-Level%20KV%20Cache%20Reduction%0AAuthor%3A%20Xuan%20Zhang%20and%20Cunxiao%20Du%20and%20Chao%20Du%20and%20Tianyu%20Pang%20and%20Wei%20Gao%20and%20Min%20Lin%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20extended%20their%0Acapabilities%20to%20handle%20long%20contexts.%20However%2C%20increasing%20the%20number%20of%20model%0Alayers%20and%20the%20length%20of%20input%20sequences%20significantly%20escalates%20the%20memory%0Arequired%20to%20store%20key-value%20%28KV%29%20cache%2C%20posing%20challenges%20for%20efficient%0Ainference.%20To%20mitigate%20this%20issue%2C%20we%20present%20SimLayerKV%2C%20a%20simple%20yet%0Aeffective%20method%20that%20reduces%20inter-layer%20KV%20cache%20redundancies%20by%20selectively%0Adropping%20cache%20in%20identified%20lazy%20layers.%20Our%20approach%20is%20based%20on%20the%0Aobservation%20that%20certain%20layers%20in%20long-context%20LLMs%20exhibit%20%22lazy%22%20behavior%2C%0Acontributing%20less%20to%20modeling%20long-range%20dependencies%20compared%20to%20non-lazy%0Alayers.%20By%20analyzing%20attention%20weight%20patterns%2C%20we%20find%20that%20the%20behavior%20of%0Athese%20lazy%20layers%20is%20consistent%20across%20tokens%20during%20generation%20for%20a%20given%0Ainput.%20This%20insight%20motivates%20our%20SimLayerKV%2C%20which%20identifies%20lazy%20layers%20and%0Areduces%20their%20KV%20cache%20accordingly.%20SimLayerKV%20is%20training-free%2C%20generalizable%2C%0Aand%20can%20be%20implemented%20with%20only%20seven%20lines%20of%20code.%20We%20conduct%20extensive%0Aexperiments%20on%20three%20representative%20LLMs%2C%20e.g.%2C%20LLaMA2-7B%2C%20LLaMA3-8B%2C%20and%0AMistral-7B%20across%2016%20tasks%20from%20the%20LongBench%20benchmark.%20The%20results%0Ademonstrate%20that%20SimLayerKV%20achieves%20a%20KV%20cache%20compression%20ratio%20of%205%24%5Ctimes%24%0Awith%20only%20a%201.2%25%20performance%20drop%20when%20combined%20with%204-bit%20quantization.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/sail-sg/SimLayerKV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimLayerKV%253A%2520A%2520Simple%2520Framework%2520for%2520Layer-Level%2520KV%2520Cache%2520Reduction%26entry.906535625%3DXuan%2520Zhang%2520and%2520Cunxiao%2520Du%2520and%2520Chao%2520Du%2520and%2520Tianyu%2520Pang%2520and%2520Wei%2520Gao%2520and%2520Min%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520extended%2520their%250Acapabilities%2520to%2520handle%2520long%2520contexts.%2520However%252C%2520increasing%2520the%2520number%2520of%2520model%250Alayers%2520and%2520the%2520length%2520of%2520input%2520sequences%2520significantly%2520escalates%2520the%2520memory%250Arequired%2520to%2520store%2520key-value%2520%2528KV%2529%2520cache%252C%2520posing%2520challenges%2520for%2520efficient%250Ainference.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520present%2520SimLayerKV%252C%2520a%2520simple%2520yet%250Aeffective%2520method%2520that%2520reduces%2520inter-layer%2520KV%2520cache%2520redundancies%2520by%2520selectively%250Adropping%2520cache%2520in%2520identified%2520lazy%2520layers.%2520Our%2520approach%2520is%2520based%2520on%2520the%250Aobservation%2520that%2520certain%2520layers%2520in%2520long-context%2520LLMs%2520exhibit%2520%2522lazy%2522%2520behavior%252C%250Acontributing%2520less%2520to%2520modeling%2520long-range%2520dependencies%2520compared%2520to%2520non-lazy%250Alayers.%2520By%2520analyzing%2520attention%2520weight%2520patterns%252C%2520we%2520find%2520that%2520the%2520behavior%2520of%250Athese%2520lazy%2520layers%2520is%2520consistent%2520across%2520tokens%2520during%2520generation%2520for%2520a%2520given%250Ainput.%2520This%2520insight%2520motivates%2520our%2520SimLayerKV%252C%2520which%2520identifies%2520lazy%2520layers%2520and%250Areduces%2520their%2520KV%2520cache%2520accordingly.%2520SimLayerKV%2520is%2520training-free%252C%2520generalizable%252C%250Aand%2520can%2520be%2520implemented%2520with%2520only%2520seven%2520lines%2520of%2520code.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520three%2520representative%2520LLMs%252C%2520e.g.%252C%2520LLaMA2-7B%252C%2520LLaMA3-8B%252C%2520and%250AMistral-7B%2520across%252016%2520tasks%2520from%2520the%2520LongBench%2520benchmark.%2520The%2520results%250Ademonstrate%2520that%2520SimLayerKV%2520achieves%2520a%2520KV%2520cache%2520compression%2520ratio%2520of%25205%2524%255Ctimes%2524%250Awith%2520only%2520a%25201.2%2525%2520performance%2520drop%2520when%2520combined%2520with%25204-bit%2520quantization.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/sail-sg/SimLayerKV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimLayerKV%3A%20A%20Simple%20Framework%20for%20Layer-Level%20KV%20Cache%20Reduction&entry.906535625=Xuan%20Zhang%20and%20Cunxiao%20Du%20and%20Chao%20Du%20and%20Tianyu%20Pang%20and%20Wei%20Gao%20and%20Min%20Lin&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20extended%20their%0Acapabilities%20to%20handle%20long%20contexts.%20However%2C%20increasing%20the%20number%20of%20model%0Alayers%20and%20the%20length%20of%20input%20sequences%20significantly%20escalates%20the%20memory%0Arequired%20to%20store%20key-value%20%28KV%29%20cache%2C%20posing%20challenges%20for%20efficient%0Ainference.%20To%20mitigate%20this%20issue%2C%20we%20present%20SimLayerKV%2C%20a%20simple%20yet%0Aeffective%20method%20that%20reduces%20inter-layer%20KV%20cache%20redundancies%20by%20selectively%0Adropping%20cache%20in%20identified%20lazy%20layers.%20Our%20approach%20is%20based%20on%20the%0Aobservation%20that%20certain%20layers%20in%20long-context%20LLMs%20exhibit%20%22lazy%22%20behavior%2C%0Acontributing%20less%20to%20modeling%20long-range%20dependencies%20compared%20to%20non-lazy%0Alayers.%20By%20analyzing%20attention%20weight%20patterns%2C%20we%20find%20that%20the%20behavior%20of%0Athese%20lazy%20layers%20is%20consistent%20across%20tokens%20during%20generation%20for%20a%20given%0Ainput.%20This%20insight%20motivates%20our%20SimLayerKV%2C%20which%20identifies%20lazy%20layers%20and%0Areduces%20their%20KV%20cache%20accordingly.%20SimLayerKV%20is%20training-free%2C%20generalizable%2C%0Aand%20can%20be%20implemented%20with%20only%20seven%20lines%20of%20code.%20We%20conduct%20extensive%0Aexperiments%20on%20three%20representative%20LLMs%2C%20e.g.%2C%20LLaMA2-7B%2C%20LLaMA3-8B%2C%20and%0AMistral-7B%20across%2016%20tasks%20from%20the%20LongBench%20benchmark.%20The%20results%0Ademonstrate%20that%20SimLayerKV%20achieves%20a%20KV%20cache%20compression%20ratio%20of%205%24%5Ctimes%24%0Awith%20only%20a%201.2%25%20performance%20drop%20when%20combined%20with%204-bit%20quantization.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/sail-sg/SimLayerKV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13846v1&entry.124074799=Read"},
{"title": "3D Guidance Law for Flexible Target Enclosing with Inherent Safety", "author": "Praveen Kumar Ranjan and Abhinav Sinha and Yongcan Cao", "abstract": "  In this paper, we address the problem of enclosing an arbitrarily moving\ntarget in three dimensions by a single pursuer while ensuring the pursuer's\nsafety by preventing collisions with the target. The proposed guidance strategy\nsteers the pursuer to a safe region of space surrounding and excluding the\ntarget, allowing it to maintain a certain distance from the latter while\noffering greater flexibility in positioning and converging to any orbit within\nthis safe zone. We leverage the concept of the Lyapunov Barrier Function as a\npowerful tool to constrain the distance between the pursuer and the target\nwithin asymmetric bounds, thereby ensuring the pursuer's safety within the\npredefined region. Further, we demonstrate the effectiveness of the proposed\nguidance law in managing arbitrarily maneuvering targets and other\nuncertainties (such as vehicle/autopilot dynamics and external disturbances) by\nenabling the pursuer to consistently achieve stable global enclosing behaviors\nby switching between stable enclosing trajectories within the safe region\nwhenever necessary, even in response to aggressive target maneuvers. To attest\nto the merits of our work, we conduct experimental tests with various plant\nmodels, including a high-fidelity quadrotor model within Software-in-the-loop\n(SITL) simulations, encompassing various challenging target maneuver scenarios\nand requiring only relative information for successful execution.\n", "link": "http://arxiv.org/abs/2404.16312v3", "date": "2024-10-17", "relevancy": 1.9, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4771}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Guidance%20Law%20for%20Flexible%20Target%20Enclosing%20with%20Inherent%20Safety&body=Title%3A%203D%20Guidance%20Law%20for%20Flexible%20Target%20Enclosing%20with%20Inherent%20Safety%0AAuthor%3A%20Praveen%20Kumar%20Ranjan%20and%20Abhinav%20Sinha%20and%20Yongcan%20Cao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20enclosing%20an%20arbitrarily%20moving%0Atarget%20in%20three%20dimensions%20by%20a%20single%20pursuer%20while%20ensuring%20the%20pursuer%27s%0Asafety%20by%20preventing%20collisions%20with%20the%20target.%20The%20proposed%20guidance%20strategy%0Asteers%20the%20pursuer%20to%20a%20safe%20region%20of%20space%20surrounding%20and%20excluding%20the%0Atarget%2C%20allowing%20it%20to%20maintain%20a%20certain%20distance%20from%20the%20latter%20while%0Aoffering%20greater%20flexibility%20in%20positioning%20and%20converging%20to%20any%20orbit%20within%0Athis%20safe%20zone.%20We%20leverage%20the%20concept%20of%20the%20Lyapunov%20Barrier%20Function%20as%20a%0Apowerful%20tool%20to%20constrain%20the%20distance%20between%20the%20pursuer%20and%20the%20target%0Awithin%20asymmetric%20bounds%2C%20thereby%20ensuring%20the%20pursuer%27s%20safety%20within%20the%0Apredefined%20region.%20Further%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aguidance%20law%20in%20managing%20arbitrarily%20maneuvering%20targets%20and%20other%0Auncertainties%20%28such%20as%20vehicle/autopilot%20dynamics%20and%20external%20disturbances%29%20by%0Aenabling%20the%20pursuer%20to%20consistently%20achieve%20stable%20global%20enclosing%20behaviors%0Aby%20switching%20between%20stable%20enclosing%20trajectories%20within%20the%20safe%20region%0Awhenever%20necessary%2C%20even%20in%20response%20to%20aggressive%20target%20maneuvers.%20To%20attest%0Ato%20the%20merits%20of%20our%20work%2C%20we%20conduct%20experimental%20tests%20with%20various%20plant%0Amodels%2C%20including%20a%20high-fidelity%20quadrotor%20model%20within%20Software-in-the-loop%0A%28SITL%29%20simulations%2C%20encompassing%20various%20challenging%20target%20maneuver%20scenarios%0Aand%20requiring%20only%20relative%20information%20for%20successful%20execution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16312v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Guidance%2520Law%2520for%2520Flexible%2520Target%2520Enclosing%2520with%2520Inherent%2520Safety%26entry.906535625%3DPraveen%2520Kumar%2520Ranjan%2520and%2520Abhinav%2520Sinha%2520and%2520Yongcan%2520Cao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520problem%2520of%2520enclosing%2520an%2520arbitrarily%2520moving%250Atarget%2520in%2520three%2520dimensions%2520by%2520a%2520single%2520pursuer%2520while%2520ensuring%2520the%2520pursuer%2527s%250Asafety%2520by%2520preventing%2520collisions%2520with%2520the%2520target.%2520The%2520proposed%2520guidance%2520strategy%250Asteers%2520the%2520pursuer%2520to%2520a%2520safe%2520region%2520of%2520space%2520surrounding%2520and%2520excluding%2520the%250Atarget%252C%2520allowing%2520it%2520to%2520maintain%2520a%2520certain%2520distance%2520from%2520the%2520latter%2520while%250Aoffering%2520greater%2520flexibility%2520in%2520positioning%2520and%2520converging%2520to%2520any%2520orbit%2520within%250Athis%2520safe%2520zone.%2520We%2520leverage%2520the%2520concept%2520of%2520the%2520Lyapunov%2520Barrier%2520Function%2520as%2520a%250Apowerful%2520tool%2520to%2520constrain%2520the%2520distance%2520between%2520the%2520pursuer%2520and%2520the%2520target%250Awithin%2520asymmetric%2520bounds%252C%2520thereby%2520ensuring%2520the%2520pursuer%2527s%2520safety%2520within%2520the%250Apredefined%2520region.%2520Further%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aguidance%2520law%2520in%2520managing%2520arbitrarily%2520maneuvering%2520targets%2520and%2520other%250Auncertainties%2520%2528such%2520as%2520vehicle/autopilot%2520dynamics%2520and%2520external%2520disturbances%2529%2520by%250Aenabling%2520the%2520pursuer%2520to%2520consistently%2520achieve%2520stable%2520global%2520enclosing%2520behaviors%250Aby%2520switching%2520between%2520stable%2520enclosing%2520trajectories%2520within%2520the%2520safe%2520region%250Awhenever%2520necessary%252C%2520even%2520in%2520response%2520to%2520aggressive%2520target%2520maneuvers.%2520To%2520attest%250Ato%2520the%2520merits%2520of%2520our%2520work%252C%2520we%2520conduct%2520experimental%2520tests%2520with%2520various%2520plant%250Amodels%252C%2520including%2520a%2520high-fidelity%2520quadrotor%2520model%2520within%2520Software-in-the-loop%250A%2528SITL%2529%2520simulations%252C%2520encompassing%2520various%2520challenging%2520target%2520maneuver%2520scenarios%250Aand%2520requiring%2520only%2520relative%2520information%2520for%2520successful%2520execution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16312v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Guidance%20Law%20for%20Flexible%20Target%20Enclosing%20with%20Inherent%20Safety&entry.906535625=Praveen%20Kumar%20Ranjan%20and%20Abhinav%20Sinha%20and%20Yongcan%20Cao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20enclosing%20an%20arbitrarily%20moving%0Atarget%20in%20three%20dimensions%20by%20a%20single%20pursuer%20while%20ensuring%20the%20pursuer%27s%0Asafety%20by%20preventing%20collisions%20with%20the%20target.%20The%20proposed%20guidance%20strategy%0Asteers%20the%20pursuer%20to%20a%20safe%20region%20of%20space%20surrounding%20and%20excluding%20the%0Atarget%2C%20allowing%20it%20to%20maintain%20a%20certain%20distance%20from%20the%20latter%20while%0Aoffering%20greater%20flexibility%20in%20positioning%20and%20converging%20to%20any%20orbit%20within%0Athis%20safe%20zone.%20We%20leverage%20the%20concept%20of%20the%20Lyapunov%20Barrier%20Function%20as%20a%0Apowerful%20tool%20to%20constrain%20the%20distance%20between%20the%20pursuer%20and%20the%20target%0Awithin%20asymmetric%20bounds%2C%20thereby%20ensuring%20the%20pursuer%27s%20safety%20within%20the%0Apredefined%20region.%20Further%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aguidance%20law%20in%20managing%20arbitrarily%20maneuvering%20targets%20and%20other%0Auncertainties%20%28such%20as%20vehicle/autopilot%20dynamics%20and%20external%20disturbances%29%20by%0Aenabling%20the%20pursuer%20to%20consistently%20achieve%20stable%20global%20enclosing%20behaviors%0Aby%20switching%20between%20stable%20enclosing%20trajectories%20within%20the%20safe%20region%0Awhenever%20necessary%2C%20even%20in%20response%20to%20aggressive%20target%20maneuvers.%20To%20attest%0Ato%20the%20merits%20of%20our%20work%2C%20we%20conduct%20experimental%20tests%20with%20various%20plant%0Amodels%2C%20including%20a%20high-fidelity%20quadrotor%20model%20within%20Software-in-the-loop%0A%28SITL%29%20simulations%2C%20encompassing%20various%20challenging%20target%20maneuver%20scenarios%0Aand%20requiring%20only%20relative%20information%20for%20successful%20execution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16312v3&entry.124074799=Read"},
{"title": "Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning\n  via Image-Guided Diffusion", "author": "Yijun Liang and Shweta Bhardwaj and Tianyi Zhou", "abstract": "  Low-quality or scarce data has posed significant challenges for training deep\nneural networks in practice. While classical data augmentation cannot\ncontribute very different new data, diffusion models opens up a new door to\nbuild self-evolving AI by generating high-quality and diverse synthetic data\nthrough text-guided prompts. However, text-only guidance cannot control\nsynthetic images' proximity to the original images, resulting in\nout-of-distribution data detrimental to the model performance. To overcome the\nlimitation, we study image guidance to achieve a spectrum of interpolations\nbetween synthetic and real images. With stronger image guidance, the generated\nimages are similar to the training data but hard to learn. While with weaker\nimage guidance, the synthetic images will be easier for model but contribute to\na larger distribution gap with the original data. The generated full spectrum\nof data enables us to build a novel \"Diffusion Curriculum (DisCL)\". DisCL\nadjusts the image guidance level of image synthesis for each training stage: It\nidentifies and focuses on hard samples for the model and assesses the most\neffective guidance level of synthetic images to improve hard data learning. We\napply DisCL to two challenging tasks: long-tail (LT) classification and\nlearning from low-quality data. It focuses on lower-guidance images of\nhigh-quality to learn prototypical features as a warm-up of learning\nhigher-guidance images that might be weak on diversity or quality. Extensive\nexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when\napplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base\nmodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%\nimprovement in all-class accuracy.\n", "link": "http://arxiv.org/abs/2410.13674v1", "date": "2024-10-17", "relevancy": 1.8786, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6404}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6331}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Curriculum%3A%20Synthetic-to-Real%20Generative%20Curriculum%20Learning%0A%20%20via%20Image-Guided%20Diffusion&body=Title%3A%20Diffusion%20Curriculum%3A%20Synthetic-to-Real%20Generative%20Curriculum%20Learning%0A%20%20via%20Image-Guided%20Diffusion%0AAuthor%3A%20Yijun%20Liang%20and%20Shweta%20Bhardwaj%20and%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Low-quality%20or%20scarce%20data%20has%20posed%20significant%20challenges%20for%20training%20deep%0Aneural%20networks%20in%20practice.%20While%20classical%20data%20augmentation%20cannot%0Acontribute%20very%20different%20new%20data%2C%20diffusion%20models%20opens%20up%20a%20new%20door%20to%0Abuild%20self-evolving%20AI%20by%20generating%20high-quality%20and%20diverse%20synthetic%20data%0Athrough%20text-guided%20prompts.%20However%2C%20text-only%20guidance%20cannot%20control%0Asynthetic%20images%27%20proximity%20to%20the%20original%20images%2C%20resulting%20in%0Aout-of-distribution%20data%20detrimental%20to%20the%20model%20performance.%20To%20overcome%20the%0Alimitation%2C%20we%20study%20image%20guidance%20to%20achieve%20a%20spectrum%20of%20interpolations%0Abetween%20synthetic%20and%20real%20images.%20With%20stronger%20image%20guidance%2C%20the%20generated%0Aimages%20are%20similar%20to%20the%20training%20data%20but%20hard%20to%20learn.%20While%20with%20weaker%0Aimage%20guidance%2C%20the%20synthetic%20images%20will%20be%20easier%20for%20model%20but%20contribute%20to%0Aa%20larger%20distribution%20gap%20with%20the%20original%20data.%20The%20generated%20full%20spectrum%0Aof%20data%20enables%20us%20to%20build%20a%20novel%20%22Diffusion%20Curriculum%20%28DisCL%29%22.%20DisCL%0Aadjusts%20the%20image%20guidance%20level%20of%20image%20synthesis%20for%20each%20training%20stage%3A%20It%0Aidentifies%20and%20focuses%20on%20hard%20samples%20for%20the%20model%20and%20assesses%20the%20most%0Aeffective%20guidance%20level%20of%20synthetic%20images%20to%20improve%20hard%20data%20learning.%20We%0Aapply%20DisCL%20to%20two%20challenging%20tasks%3A%20long-tail%20%28LT%29%20classification%20and%0Alearning%20from%20low-quality%20data.%20It%20focuses%20on%20lower-guidance%20images%20of%0Ahigh-quality%20to%20learn%20prototypical%20features%20as%20a%20warm-up%20of%20learning%0Ahigher-guidance%20images%20that%20might%20be%20weak%20on%20diversity%20or%20quality.%20Extensive%0Aexperiments%20showcase%20a%20gain%20of%202.7%25%20and%202.1%25%20in%20OOD%20and%20ID%20macro-accuracy%20when%0Aapplying%20DisCL%20to%20iWildCam%20dataset.%20On%20ImageNet-LT%2C%20DisCL%20improves%20the%20base%0Amodel%27s%20tail-class%20accuracy%20from%204.4%25%20to%2023.64%25%20and%20leads%20to%20a%204.02%25%0Aimprovement%20in%20all-class%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Curriculum%253A%2520Synthetic-to-Real%2520Generative%2520Curriculum%2520Learning%250A%2520%2520via%2520Image-Guided%2520Diffusion%26entry.906535625%3DYijun%2520Liang%2520and%2520Shweta%2520Bhardwaj%2520and%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Low-quality%2520or%2520scarce%2520data%2520has%2520posed%2520significant%2520challenges%2520for%2520training%2520deep%250Aneural%2520networks%2520in%2520practice.%2520While%2520classical%2520data%2520augmentation%2520cannot%250Acontribute%2520very%2520different%2520new%2520data%252C%2520diffusion%2520models%2520opens%2520up%2520a%2520new%2520door%2520to%250Abuild%2520self-evolving%2520AI%2520by%2520generating%2520high-quality%2520and%2520diverse%2520synthetic%2520data%250Athrough%2520text-guided%2520prompts.%2520However%252C%2520text-only%2520guidance%2520cannot%2520control%250Asynthetic%2520images%2527%2520proximity%2520to%2520the%2520original%2520images%252C%2520resulting%2520in%250Aout-of-distribution%2520data%2520detrimental%2520to%2520the%2520model%2520performance.%2520To%2520overcome%2520the%250Alimitation%252C%2520we%2520study%2520image%2520guidance%2520to%2520achieve%2520a%2520spectrum%2520of%2520interpolations%250Abetween%2520synthetic%2520and%2520real%2520images.%2520With%2520stronger%2520image%2520guidance%252C%2520the%2520generated%250Aimages%2520are%2520similar%2520to%2520the%2520training%2520data%2520but%2520hard%2520to%2520learn.%2520While%2520with%2520weaker%250Aimage%2520guidance%252C%2520the%2520synthetic%2520images%2520will%2520be%2520easier%2520for%2520model%2520but%2520contribute%2520to%250Aa%2520larger%2520distribution%2520gap%2520with%2520the%2520original%2520data.%2520The%2520generated%2520full%2520spectrum%250Aof%2520data%2520enables%2520us%2520to%2520build%2520a%2520novel%2520%2522Diffusion%2520Curriculum%2520%2528DisCL%2529%2522.%2520DisCL%250Aadjusts%2520the%2520image%2520guidance%2520level%2520of%2520image%2520synthesis%2520for%2520each%2520training%2520stage%253A%2520It%250Aidentifies%2520and%2520focuses%2520on%2520hard%2520samples%2520for%2520the%2520model%2520and%2520assesses%2520the%2520most%250Aeffective%2520guidance%2520level%2520of%2520synthetic%2520images%2520to%2520improve%2520hard%2520data%2520learning.%2520We%250Aapply%2520DisCL%2520to%2520two%2520challenging%2520tasks%253A%2520long-tail%2520%2528LT%2529%2520classification%2520and%250Alearning%2520from%2520low-quality%2520data.%2520It%2520focuses%2520on%2520lower-guidance%2520images%2520of%250Ahigh-quality%2520to%2520learn%2520prototypical%2520features%2520as%2520a%2520warm-up%2520of%2520learning%250Ahigher-guidance%2520images%2520that%2520might%2520be%2520weak%2520on%2520diversity%2520or%2520quality.%2520Extensive%250Aexperiments%2520showcase%2520a%2520gain%2520of%25202.7%2525%2520and%25202.1%2525%2520in%2520OOD%2520and%2520ID%2520macro-accuracy%2520when%250Aapplying%2520DisCL%2520to%2520iWildCam%2520dataset.%2520On%2520ImageNet-LT%252C%2520DisCL%2520improves%2520the%2520base%250Amodel%2527s%2520tail-class%2520accuracy%2520from%25204.4%2525%2520to%252023.64%2525%2520and%2520leads%2520to%2520a%25204.02%2525%250Aimprovement%2520in%2520all-class%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Curriculum%3A%20Synthetic-to-Real%20Generative%20Curriculum%20Learning%0A%20%20via%20Image-Guided%20Diffusion&entry.906535625=Yijun%20Liang%20and%20Shweta%20Bhardwaj%20and%20Tianyi%20Zhou&entry.1292438233=%20%20Low-quality%20or%20scarce%20data%20has%20posed%20significant%20challenges%20for%20training%20deep%0Aneural%20networks%20in%20practice.%20While%20classical%20data%20augmentation%20cannot%0Acontribute%20very%20different%20new%20data%2C%20diffusion%20models%20opens%20up%20a%20new%20door%20to%0Abuild%20self-evolving%20AI%20by%20generating%20high-quality%20and%20diverse%20synthetic%20data%0Athrough%20text-guided%20prompts.%20However%2C%20text-only%20guidance%20cannot%20control%0Asynthetic%20images%27%20proximity%20to%20the%20original%20images%2C%20resulting%20in%0Aout-of-distribution%20data%20detrimental%20to%20the%20model%20performance.%20To%20overcome%20the%0Alimitation%2C%20we%20study%20image%20guidance%20to%20achieve%20a%20spectrum%20of%20interpolations%0Abetween%20synthetic%20and%20real%20images.%20With%20stronger%20image%20guidance%2C%20the%20generated%0Aimages%20are%20similar%20to%20the%20training%20data%20but%20hard%20to%20learn.%20While%20with%20weaker%0Aimage%20guidance%2C%20the%20synthetic%20images%20will%20be%20easier%20for%20model%20but%20contribute%20to%0Aa%20larger%20distribution%20gap%20with%20the%20original%20data.%20The%20generated%20full%20spectrum%0Aof%20data%20enables%20us%20to%20build%20a%20novel%20%22Diffusion%20Curriculum%20%28DisCL%29%22.%20DisCL%0Aadjusts%20the%20image%20guidance%20level%20of%20image%20synthesis%20for%20each%20training%20stage%3A%20It%0Aidentifies%20and%20focuses%20on%20hard%20samples%20for%20the%20model%20and%20assesses%20the%20most%0Aeffective%20guidance%20level%20of%20synthetic%20images%20to%20improve%20hard%20data%20learning.%20We%0Aapply%20DisCL%20to%20two%20challenging%20tasks%3A%20long-tail%20%28LT%29%20classification%20and%0Alearning%20from%20low-quality%20data.%20It%20focuses%20on%20lower-guidance%20images%20of%0Ahigh-quality%20to%20learn%20prototypical%20features%20as%20a%20warm-up%20of%20learning%0Ahigher-guidance%20images%20that%20might%20be%20weak%20on%20diversity%20or%20quality.%20Extensive%0Aexperiments%20showcase%20a%20gain%20of%202.7%25%20and%202.1%25%20in%20OOD%20and%20ID%20macro-accuracy%20when%0Aapplying%20DisCL%20to%20iWildCam%20dataset.%20On%20ImageNet-LT%2C%20DisCL%20improves%20the%20base%0Amodel%27s%20tail-class%20accuracy%20from%204.4%25%20to%2023.64%25%20and%20leads%20to%20a%204.02%25%0Aimprovement%20in%20all-class%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13674v1&entry.124074799=Read"},
{"title": "BLT: Can Large Language Models Handle Basic Legal Text?", "author": "Andrew Blair-Stanek and Nils Holzenberger and Benjamin Van Durme", "abstract": "  We find that the best publicly available LLMs like GPT-4 and Claude currently\nperform poorly on basic legal text handling. This motivates the creation of a\nbenchmark consisting of examples that lawyers and paralegals would expect LLMs\nto handle zero-shot, such as looking up the text at a line of a witness\ndeposition or at a subsection of a contract. LLMs' poor performance on this\nbenchmark casts into doubt their reliability as-is for legal practice. However,\nfine-tuning on our training set brings even a small model to near-perfect\nperformance. This benchmark will be useful for fine-tuning LLMs for downstream\nlegal tasks, as well as for tracking LLMs' reliability as-is for basic legal\ntasks.\n", "link": "http://arxiv.org/abs/2311.09693v3", "date": "2024-10-17", "relevancy": 1.8705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLT%3A%20Can%20Large%20Language%20Models%20Handle%20Basic%20Legal%20Text%3F&body=Title%3A%20BLT%3A%20Can%20Large%20Language%20Models%20Handle%20Basic%20Legal%20Text%3F%0AAuthor%3A%20Andrew%20Blair-Stanek%20and%20Nils%20Holzenberger%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20We%20find%20that%20the%20best%20publicly%20available%20LLMs%20like%20GPT-4%20and%20Claude%20currently%0Aperform%20poorly%20on%20basic%20legal%20text%20handling.%20This%20motivates%20the%20creation%20of%20a%0Abenchmark%20consisting%20of%20examples%20that%20lawyers%20and%20paralegals%20would%20expect%20LLMs%0Ato%20handle%20zero-shot%2C%20such%20as%20looking%20up%20the%20text%20at%20a%20line%20of%20a%20witness%0Adeposition%20or%20at%20a%20subsection%20of%20a%20contract.%20LLMs%27%20poor%20performance%20on%20this%0Abenchmark%20casts%20into%20doubt%20their%20reliability%20as-is%20for%20legal%20practice.%20However%2C%0Afine-tuning%20on%20our%20training%20set%20brings%20even%20a%20small%20model%20to%20near-perfect%0Aperformance.%20This%20benchmark%20will%20be%20useful%20for%20fine-tuning%20LLMs%20for%20downstream%0Alegal%20tasks%2C%20as%20well%20as%20for%20tracking%20LLMs%27%20reliability%20as-is%20for%20basic%20legal%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09693v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLT%253A%2520Can%2520Large%2520Language%2520Models%2520Handle%2520Basic%2520Legal%2520Text%253F%26entry.906535625%3DAndrew%2520Blair-Stanek%2520and%2520Nils%2520Holzenberger%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520We%2520find%2520that%2520the%2520best%2520publicly%2520available%2520LLMs%2520like%2520GPT-4%2520and%2520Claude%2520currently%250Aperform%2520poorly%2520on%2520basic%2520legal%2520text%2520handling.%2520This%2520motivates%2520the%2520creation%2520of%2520a%250Abenchmark%2520consisting%2520of%2520examples%2520that%2520lawyers%2520and%2520paralegals%2520would%2520expect%2520LLMs%250Ato%2520handle%2520zero-shot%252C%2520such%2520as%2520looking%2520up%2520the%2520text%2520at%2520a%2520line%2520of%2520a%2520witness%250Adeposition%2520or%2520at%2520a%2520subsection%2520of%2520a%2520contract.%2520LLMs%2527%2520poor%2520performance%2520on%2520this%250Abenchmark%2520casts%2520into%2520doubt%2520their%2520reliability%2520as-is%2520for%2520legal%2520practice.%2520However%252C%250Afine-tuning%2520on%2520our%2520training%2520set%2520brings%2520even%2520a%2520small%2520model%2520to%2520near-perfect%250Aperformance.%2520This%2520benchmark%2520will%2520be%2520useful%2520for%2520fine-tuning%2520LLMs%2520for%2520downstream%250Alegal%2520tasks%252C%2520as%2520well%2520as%2520for%2520tracking%2520LLMs%2527%2520reliability%2520as-is%2520for%2520basic%2520legal%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09693v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLT%3A%20Can%20Large%20Language%20Models%20Handle%20Basic%20Legal%20Text%3F&entry.906535625=Andrew%20Blair-Stanek%20and%20Nils%20Holzenberger%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20We%20find%20that%20the%20best%20publicly%20available%20LLMs%20like%20GPT-4%20and%20Claude%20currently%0Aperform%20poorly%20on%20basic%20legal%20text%20handling.%20This%20motivates%20the%20creation%20of%20a%0Abenchmark%20consisting%20of%20examples%20that%20lawyers%20and%20paralegals%20would%20expect%20LLMs%0Ato%20handle%20zero-shot%2C%20such%20as%20looking%20up%20the%20text%20at%20a%20line%20of%20a%20witness%0Adeposition%20or%20at%20a%20subsection%20of%20a%20contract.%20LLMs%27%20poor%20performance%20on%20this%0Abenchmark%20casts%20into%20doubt%20their%20reliability%20as-is%20for%20legal%20practice.%20However%2C%0Afine-tuning%20on%20our%20training%20set%20brings%20even%20a%20small%20model%20to%20near-perfect%0Aperformance.%20This%20benchmark%20will%20be%20useful%20for%20fine-tuning%20LLMs%20for%20downstream%0Alegal%20tasks%2C%20as%20well%20as%20for%20tracking%20LLMs%27%20reliability%20as-is%20for%20basic%20legal%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09693v3&entry.124074799=Read"},
{"title": "Stage-Aware Learning for Dynamic Treatments", "author": "Hanwen Ye and Wenzhuo Zhou and Ruoqing Zhu and Annie Qu", "abstract": "  Recent advances in dynamic treatment regimes (DTRs) facilitate the search for\noptimal treatments, which are tailored to individuals' specific needs and able\nto maximize their expected clinical benefits. However, existing algorithms\nrelying on consistent trajectories, such as inverse probability weighting\nestimators (IPWEs), could suffer from insufficient sample size under optimal\ntreatments and a growing number of decision-making stages, particularly in the\ncontext of chronic diseases. To address these challenges, we propose a novel\nindividualized learning method which estimates the DTR with a focus on\nprioritizing alignment between the observed treatment trajectory and the one\nobtained by the optimal regime across decision stages. By relaxing the\nrestriction that the observed trajectory must be fully aligned with the optimal\ntreatments, our approach substantially improves the sample efficiency and\nstability of IPWE-based methods. In particular, the proposed learning scheme\nbuilds a more general framework which includes the popular outcome weighted\nlearning framework as a special case of ours. Moreover, we introduce the notion\nof stage importance scores along with an attention mechanism to explicitly\naccount for heterogeneity among decision stages. We establish the theoretical\nproperties of the proposed approach, including the Fisher consistency and\nfinite-sample performance bound. Empirically, we evaluate the proposed method\nin extensive simulated environments and a real case study for the COVID-19\npandemic.\n", "link": "http://arxiv.org/abs/2310.19300v2", "date": "2024-10-17", "relevancy": 1.8671, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.452}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stage-Aware%20Learning%20for%20Dynamic%20Treatments&body=Title%3A%20Stage-Aware%20Learning%20for%20Dynamic%20Treatments%0AAuthor%3A%20Hanwen%20Ye%20and%20Wenzhuo%20Zhou%20and%20Ruoqing%20Zhu%20and%20Annie%20Qu%0AAbstract%3A%20%20%20Recent%20advances%20in%20dynamic%20treatment%20regimes%20%28DTRs%29%20facilitate%20the%20search%20for%0Aoptimal%20treatments%2C%20which%20are%20tailored%20to%20individuals%27%20specific%20needs%20and%20able%0Ato%20maximize%20their%20expected%20clinical%20benefits.%20However%2C%20existing%20algorithms%0Arelying%20on%20consistent%20trajectories%2C%20such%20as%20inverse%20probability%20weighting%0Aestimators%20%28IPWEs%29%2C%20could%20suffer%20from%20insufficient%20sample%20size%20under%20optimal%0Atreatments%20and%20a%20growing%20number%20of%20decision-making%20stages%2C%20particularly%20in%20the%0Acontext%20of%20chronic%20diseases.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aindividualized%20learning%20method%20which%20estimates%20the%20DTR%20with%20a%20focus%20on%0Aprioritizing%20alignment%20between%20the%20observed%20treatment%20trajectory%20and%20the%20one%0Aobtained%20by%20the%20optimal%20regime%20across%20decision%20stages.%20By%20relaxing%20the%0Arestriction%20that%20the%20observed%20trajectory%20must%20be%20fully%20aligned%20with%20the%20optimal%0Atreatments%2C%20our%20approach%20substantially%20improves%20the%20sample%20efficiency%20and%0Astability%20of%20IPWE-based%20methods.%20In%20particular%2C%20the%20proposed%20learning%20scheme%0Abuilds%20a%20more%20general%20framework%20which%20includes%20the%20popular%20outcome%20weighted%0Alearning%20framework%20as%20a%20special%20case%20of%20ours.%20Moreover%2C%20we%20introduce%20the%20notion%0Aof%20stage%20importance%20scores%20along%20with%20an%20attention%20mechanism%20to%20explicitly%0Aaccount%20for%20heterogeneity%20among%20decision%20stages.%20We%20establish%20the%20theoretical%0Aproperties%20of%20the%20proposed%20approach%2C%20including%20the%20Fisher%20consistency%20and%0Afinite-sample%20performance%20bound.%20Empirically%2C%20we%20evaluate%20the%20proposed%20method%0Ain%20extensive%20simulated%20environments%20and%20a%20real%20case%20study%20for%20the%20COVID-19%0Apandemic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStage-Aware%2520Learning%2520for%2520Dynamic%2520Treatments%26entry.906535625%3DHanwen%2520Ye%2520and%2520Wenzhuo%2520Zhou%2520and%2520Ruoqing%2520Zhu%2520and%2520Annie%2520Qu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520dynamic%2520treatment%2520regimes%2520%2528DTRs%2529%2520facilitate%2520the%2520search%2520for%250Aoptimal%2520treatments%252C%2520which%2520are%2520tailored%2520to%2520individuals%2527%2520specific%2520needs%2520and%2520able%250Ato%2520maximize%2520their%2520expected%2520clinical%2520benefits.%2520However%252C%2520existing%2520algorithms%250Arelying%2520on%2520consistent%2520trajectories%252C%2520such%2520as%2520inverse%2520probability%2520weighting%250Aestimators%2520%2528IPWEs%2529%252C%2520could%2520suffer%2520from%2520insufficient%2520sample%2520size%2520under%2520optimal%250Atreatments%2520and%2520a%2520growing%2520number%2520of%2520decision-making%2520stages%252C%2520particularly%2520in%2520the%250Acontext%2520of%2520chronic%2520diseases.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250Aindividualized%2520learning%2520method%2520which%2520estimates%2520the%2520DTR%2520with%2520a%2520focus%2520on%250Aprioritizing%2520alignment%2520between%2520the%2520observed%2520treatment%2520trajectory%2520and%2520the%2520one%250Aobtained%2520by%2520the%2520optimal%2520regime%2520across%2520decision%2520stages.%2520By%2520relaxing%2520the%250Arestriction%2520that%2520the%2520observed%2520trajectory%2520must%2520be%2520fully%2520aligned%2520with%2520the%2520optimal%250Atreatments%252C%2520our%2520approach%2520substantially%2520improves%2520the%2520sample%2520efficiency%2520and%250Astability%2520of%2520IPWE-based%2520methods.%2520In%2520particular%252C%2520the%2520proposed%2520learning%2520scheme%250Abuilds%2520a%2520more%2520general%2520framework%2520which%2520includes%2520the%2520popular%2520outcome%2520weighted%250Alearning%2520framework%2520as%2520a%2520special%2520case%2520of%2520ours.%2520Moreover%252C%2520we%2520introduce%2520the%2520notion%250Aof%2520stage%2520importance%2520scores%2520along%2520with%2520an%2520attention%2520mechanism%2520to%2520explicitly%250Aaccount%2520for%2520heterogeneity%2520among%2520decision%2520stages.%2520We%2520establish%2520the%2520theoretical%250Aproperties%2520of%2520the%2520proposed%2520approach%252C%2520including%2520the%2520Fisher%2520consistency%2520and%250Afinite-sample%2520performance%2520bound.%2520Empirically%252C%2520we%2520evaluate%2520the%2520proposed%2520method%250Ain%2520extensive%2520simulated%2520environments%2520and%2520a%2520real%2520case%2520study%2520for%2520the%2520COVID-19%250Apandemic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stage-Aware%20Learning%20for%20Dynamic%20Treatments&entry.906535625=Hanwen%20Ye%20and%20Wenzhuo%20Zhou%20and%20Ruoqing%20Zhu%20and%20Annie%20Qu&entry.1292438233=%20%20Recent%20advances%20in%20dynamic%20treatment%20regimes%20%28DTRs%29%20facilitate%20the%20search%20for%0Aoptimal%20treatments%2C%20which%20are%20tailored%20to%20individuals%27%20specific%20needs%20and%20able%0Ato%20maximize%20their%20expected%20clinical%20benefits.%20However%2C%20existing%20algorithms%0Arelying%20on%20consistent%20trajectories%2C%20such%20as%20inverse%20probability%20weighting%0Aestimators%20%28IPWEs%29%2C%20could%20suffer%20from%20insufficient%20sample%20size%20under%20optimal%0Atreatments%20and%20a%20growing%20number%20of%20decision-making%20stages%2C%20particularly%20in%20the%0Acontext%20of%20chronic%20diseases.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aindividualized%20learning%20method%20which%20estimates%20the%20DTR%20with%20a%20focus%20on%0Aprioritizing%20alignment%20between%20the%20observed%20treatment%20trajectory%20and%20the%20one%0Aobtained%20by%20the%20optimal%20regime%20across%20decision%20stages.%20By%20relaxing%20the%0Arestriction%20that%20the%20observed%20trajectory%20must%20be%20fully%20aligned%20with%20the%20optimal%0Atreatments%2C%20our%20approach%20substantially%20improves%20the%20sample%20efficiency%20and%0Astability%20of%20IPWE-based%20methods.%20In%20particular%2C%20the%20proposed%20learning%20scheme%0Abuilds%20a%20more%20general%20framework%20which%20includes%20the%20popular%20outcome%20weighted%0Alearning%20framework%20as%20a%20special%20case%20of%20ours.%20Moreover%2C%20we%20introduce%20the%20notion%0Aof%20stage%20importance%20scores%20along%20with%20an%20attention%20mechanism%20to%20explicitly%0Aaccount%20for%20heterogeneity%20among%20decision%20stages.%20We%20establish%20the%20theoretical%0Aproperties%20of%20the%20proposed%20approach%2C%20including%20the%20Fisher%20consistency%20and%0Afinite-sample%20performance%20bound.%20Empirically%2C%20we%20evaluate%20the%20proposed%20method%0Ain%20extensive%20simulated%20environments%20and%20a%20real%20case%20study%20for%20the%20COVID-19%0Apandemic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19300v2&entry.124074799=Read"},
{"title": "How Numerical Precision Affects Mathematical Reasoning Capabilities of\n  LLMs", "author": "Guhao Feng and Kai Yang and Yuntian Gu and Xinyue Ai and Shengjie Luo and Jiacheng Sun and Di He and Zhenguo Li and Liwei Wang", "abstract": "  Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.\n", "link": "http://arxiv.org/abs/2410.13857v1", "date": "2024-10-17", "relevancy": 1.8644, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Numerical%20Precision%20Affects%20Mathematical%20Reasoning%20Capabilities%20of%0A%20%20LLMs&body=Title%3A%20How%20Numerical%20Precision%20Affects%20Mathematical%20Reasoning%20Capabilities%20of%0A%20%20LLMs%0AAuthor%3A%20Guhao%20Feng%20and%20Kai%20Yang%20and%20Yuntian%20Gu%20and%20Xinyue%20Ai%20and%20Shengjie%20Luo%20and%20Jiacheng%20Sun%20and%20Di%20He%20and%20Zhenguo%20Li%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20success%20of%20Transformer-based%20Large%20Language%20Models%0A%28LLMs%29%20across%20various%20domains%2C%20understanding%20and%20enhancing%20their%20mathematical%0Acapabilities%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20conduct%20a%0Arigorous%20theoretical%20analysis%20of%20LLMs%27%20mathematical%20abilities%2C%20with%20a%20specific%0Afocus%20on%20their%20arithmetic%20performances.%20We%20identify%20numerical%20precision%20as%20a%0Akey%20factor%20that%20influences%20their%20effectiveness%20in%20mathematical%20tasks.%20Our%0Aresults%20show%20that%20Transformers%20operating%20with%20low%20numerical%20precision%20fail%20to%0Aaddress%20arithmetic%20tasks%2C%20such%20as%20iterated%20addition%20and%20integer%20multiplication%2C%0Aunless%20the%20model%20size%20grows%20super-polynomially%20with%20respect%20to%20the%20input%0Alength.%20In%20contrast%2C%20Transformers%20with%20standard%20numerical%20precision%20can%0Aefficiently%20handle%20these%20tasks%20with%20significantly%20smaller%20model%20sizes.%20We%0Afurther%20support%20our%20theoretical%20findings%20through%20empirical%20experiments%20that%0Aexplore%20the%20impact%20of%20varying%20numerical%20precision%20on%20arithmetic%20tasks%2C%0Aproviding%20valuable%20insights%20for%20improving%20the%20mathematical%20reasoning%0Acapabilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Numerical%2520Precision%2520Affects%2520Mathematical%2520Reasoning%2520Capabilities%2520of%250A%2520%2520LLMs%26entry.906535625%3DGuhao%2520Feng%2520and%2520Kai%2520Yang%2520and%2520Yuntian%2520Gu%2520and%2520Xinyue%2520Ai%2520and%2520Shengjie%2520Luo%2520and%2520Jiacheng%2520Sun%2520and%2520Di%2520He%2520and%2520Zhenguo%2520Li%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520success%2520of%2520Transformer-based%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520across%2520various%2520domains%252C%2520understanding%2520and%2520enhancing%2520their%2520mathematical%250Acapabilities%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%250Arigorous%2520theoretical%2520analysis%2520of%2520LLMs%2527%2520mathematical%2520abilities%252C%2520with%2520a%2520specific%250Afocus%2520on%2520their%2520arithmetic%2520performances.%2520We%2520identify%2520numerical%2520precision%2520as%2520a%250Akey%2520factor%2520that%2520influences%2520their%2520effectiveness%2520in%2520mathematical%2520tasks.%2520Our%250Aresults%2520show%2520that%2520Transformers%2520operating%2520with%2520low%2520numerical%2520precision%2520fail%2520to%250Aaddress%2520arithmetic%2520tasks%252C%2520such%2520as%2520iterated%2520addition%2520and%2520integer%2520multiplication%252C%250Aunless%2520the%2520model%2520size%2520grows%2520super-polynomially%2520with%2520respect%2520to%2520the%2520input%250Alength.%2520In%2520contrast%252C%2520Transformers%2520with%2520standard%2520numerical%2520precision%2520can%250Aefficiently%2520handle%2520these%2520tasks%2520with%2520significantly%2520smaller%2520model%2520sizes.%2520We%250Afurther%2520support%2520our%2520theoretical%2520findings%2520through%2520empirical%2520experiments%2520that%250Aexplore%2520the%2520impact%2520of%2520varying%2520numerical%2520precision%2520on%2520arithmetic%2520tasks%252C%250Aproviding%2520valuable%2520insights%2520for%2520improving%2520the%2520mathematical%2520reasoning%250Acapabilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Numerical%20Precision%20Affects%20Mathematical%20Reasoning%20Capabilities%20of%0A%20%20LLMs&entry.906535625=Guhao%20Feng%20and%20Kai%20Yang%20and%20Yuntian%20Gu%20and%20Xinyue%20Ai%20and%20Shengjie%20Luo%20and%20Jiacheng%20Sun%20and%20Di%20He%20and%20Zhenguo%20Li%20and%20Liwei%20Wang&entry.1292438233=%20%20Despite%20the%20remarkable%20success%20of%20Transformer-based%20Large%20Language%20Models%0A%28LLMs%29%20across%20various%20domains%2C%20understanding%20and%20enhancing%20their%20mathematical%0Acapabilities%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20conduct%20a%0Arigorous%20theoretical%20analysis%20of%20LLMs%27%20mathematical%20abilities%2C%20with%20a%20specific%0Afocus%20on%20their%20arithmetic%20performances.%20We%20identify%20numerical%20precision%20as%20a%0Akey%20factor%20that%20influences%20their%20effectiveness%20in%20mathematical%20tasks.%20Our%0Aresults%20show%20that%20Transformers%20operating%20with%20low%20numerical%20precision%20fail%20to%0Aaddress%20arithmetic%20tasks%2C%20such%20as%20iterated%20addition%20and%20integer%20multiplication%2C%0Aunless%20the%20model%20size%20grows%20super-polynomially%20with%20respect%20to%20the%20input%0Alength.%20In%20contrast%2C%20Transformers%20with%20standard%20numerical%20precision%20can%0Aefficiently%20handle%20these%20tasks%20with%20significantly%20smaller%20model%20sizes.%20We%0Afurther%20support%20our%20theoretical%20findings%20through%20empirical%20experiments%20that%0Aexplore%20the%20impact%20of%20varying%20numerical%20precision%20on%20arithmetic%20tasks%2C%0Aproviding%20valuable%20insights%20for%20improving%20the%20mathematical%20reasoning%0Acapabilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13857v1&entry.124074799=Read"},
{"title": "Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt\n  Compression", "author": "Muhammad Asif Ali and Zhengping Li and Shu Yang and Keyuan Cheng and Yang Cao and Tianhao Huang and Guimin Hu and Weimin Lyu and Lijie Hu and Lu Yu and Di Wang", "abstract": "  Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.\n", "link": "http://arxiv.org/abs/2404.00489v2", "date": "2024-10-17", "relevancy": 1.8613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-SAW%3A%20Leveraging%20Relation-Aware%20Graphs%20for%20Textual%20Prompt%0A%20%20Compression&body=Title%3A%20Prompt-SAW%3A%20Leveraging%20Relation-Aware%20Graphs%20for%20Textual%20Prompt%0A%20%20Compression%0AAuthor%3A%20Muhammad%20Asif%20Ali%20and%20Zhengping%20Li%20and%20Shu%20Yang%20and%20Keyuan%20Cheng%20and%20Yang%20Cao%20and%20Tianhao%20Huang%20and%20Guimin%20Hu%20and%20Weimin%20Lyu%20and%20Lijie%20Hu%20and%20Lu%20Yu%20and%20Di%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20exceptional%20abilities%20for%20multiple%0Adifferent%20natural%20language%20processing%20tasks.%20While%20prompting%20is%20a%20crucial%20tool%0Afor%20LLM%20inference%2C%20we%20observe%20that%20there%20is%20a%20significant%20cost%20associated%20with%0Aexceedingly%20lengthy%20prompts.%20Existing%20attempts%20to%20compress%20lengthy%20prompts%20lead%0Ato%20substandard%20results%20in%20terms%20of%20readability/interpretability%20of%20the%0Acompressed%20prompt%2C%20with%20a%20detrimental%20impact%20on%20prompt%20utility.%20To%20address%0Athis%2C%20we%20propose%20PromptSAW%3A%20Prompt%20compresSion%20via%20Relation%20AWare%20graphs%2C%20an%0Aeffective%20strategy%20for%20prompt%20compression%20over%20task-agnostic%20and%20task-aware%0Aprompts.%20Prompt-SAW%20uses%20the%20prompt%27s%20textual%20information%20to%20build%20a%20graph%20and%0Alater%20extracts%20key%20information%20elements%20in%20the%20graph%20to%20come%20up%20with%20the%0Acompressed%20prompt.%20We%20also%20propose%20GSM8K-aug%2C%20i.e.%2C%20an%20extended%20version%20of%20the%0Aexisting%20GSM8K%20benchmark%20for%20task-agnostic%20prompts%20in%20order%20to%20provide%20a%0Acomprehensive%20evaluation%20platform.%20Experimental%20evaluation%20using%20benchmark%0Adatasets%20shows%20that%20prompts%20compressed%20by%20Prompt-SAW%20are%20not%20only%20better%20in%0Aterms%20of%20readability%2C%20but%20they%20also%20outperform%20the%20best-performing%20baseline%0Amodels%20by%20up%20to%2010.1%20and%2077.1%2C%20respectively%2C%20for%20task-agnostic%20and%20task-aware%0Asettings%20while%20compressing%20the%20original%20prompt%20text%20by%2034.9%20and%2056.7.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-SAW%253A%2520Leveraging%2520Relation-Aware%2520Graphs%2520for%2520Textual%2520Prompt%250A%2520%2520Compression%26entry.906535625%3DMuhammad%2520Asif%2520Ali%2520and%2520Zhengping%2520Li%2520and%2520Shu%2520Yang%2520and%2520Keyuan%2520Cheng%2520and%2520Yang%2520Cao%2520and%2520Tianhao%2520Huang%2520and%2520Guimin%2520Hu%2520and%2520Weimin%2520Lyu%2520and%2520Lijie%2520Hu%2520and%2520Lu%2520Yu%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520exceptional%2520abilities%2520for%2520multiple%250Adifferent%2520natural%2520language%2520processing%2520tasks.%2520While%2520prompting%2520is%2520a%2520crucial%2520tool%250Afor%2520LLM%2520inference%252C%2520we%2520observe%2520that%2520there%2520is%2520a%2520significant%2520cost%2520associated%2520with%250Aexceedingly%2520lengthy%2520prompts.%2520Existing%2520attempts%2520to%2520compress%2520lengthy%2520prompts%2520lead%250Ato%2520substandard%2520results%2520in%2520terms%2520of%2520readability/interpretability%2520of%2520the%250Acompressed%2520prompt%252C%2520with%2520a%2520detrimental%2520impact%2520on%2520prompt%2520utility.%2520To%2520address%250Athis%252C%2520we%2520propose%2520PromptSAW%253A%2520Prompt%2520compresSion%2520via%2520Relation%2520AWare%2520graphs%252C%2520an%250Aeffective%2520strategy%2520for%2520prompt%2520compression%2520over%2520task-agnostic%2520and%2520task-aware%250Aprompts.%2520Prompt-SAW%2520uses%2520the%2520prompt%2527s%2520textual%2520information%2520to%2520build%2520a%2520graph%2520and%250Alater%2520extracts%2520key%2520information%2520elements%2520in%2520the%2520graph%2520to%2520come%2520up%2520with%2520the%250Acompressed%2520prompt.%2520We%2520also%2520propose%2520GSM8K-aug%252C%2520i.e.%252C%2520an%2520extended%2520version%2520of%2520the%250Aexisting%2520GSM8K%2520benchmark%2520for%2520task-agnostic%2520prompts%2520in%2520order%2520to%2520provide%2520a%250Acomprehensive%2520evaluation%2520platform.%2520Experimental%2520evaluation%2520using%2520benchmark%250Adatasets%2520shows%2520that%2520prompts%2520compressed%2520by%2520Prompt-SAW%2520are%2520not%2520only%2520better%2520in%250Aterms%2520of%2520readability%252C%2520but%2520they%2520also%2520outperform%2520the%2520best-performing%2520baseline%250Amodels%2520by%2520up%2520to%252010.1%2520and%252077.1%252C%2520respectively%252C%2520for%2520task-agnostic%2520and%2520task-aware%250Asettings%2520while%2520compressing%2520the%2520original%2520prompt%2520text%2520by%252034.9%2520and%252056.7.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-SAW%3A%20Leveraging%20Relation-Aware%20Graphs%20for%20Textual%20Prompt%0A%20%20Compression&entry.906535625=Muhammad%20Asif%20Ali%20and%20Zhengping%20Li%20and%20Shu%20Yang%20and%20Keyuan%20Cheng%20and%20Yang%20Cao%20and%20Tianhao%20Huang%20and%20Guimin%20Hu%20and%20Weimin%20Lyu%20and%20Lijie%20Hu%20and%20Lu%20Yu%20and%20Di%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20exceptional%20abilities%20for%20multiple%0Adifferent%20natural%20language%20processing%20tasks.%20While%20prompting%20is%20a%20crucial%20tool%0Afor%20LLM%20inference%2C%20we%20observe%20that%20there%20is%20a%20significant%20cost%20associated%20with%0Aexceedingly%20lengthy%20prompts.%20Existing%20attempts%20to%20compress%20lengthy%20prompts%20lead%0Ato%20substandard%20results%20in%20terms%20of%20readability/interpretability%20of%20the%0Acompressed%20prompt%2C%20with%20a%20detrimental%20impact%20on%20prompt%20utility.%20To%20address%0Athis%2C%20we%20propose%20PromptSAW%3A%20Prompt%20compresSion%20via%20Relation%20AWare%20graphs%2C%20an%0Aeffective%20strategy%20for%20prompt%20compression%20over%20task-agnostic%20and%20task-aware%0Aprompts.%20Prompt-SAW%20uses%20the%20prompt%27s%20textual%20information%20to%20build%20a%20graph%20and%0Alater%20extracts%20key%20information%20elements%20in%20the%20graph%20to%20come%20up%20with%20the%0Acompressed%20prompt.%20We%20also%20propose%20GSM8K-aug%2C%20i.e.%2C%20an%20extended%20version%20of%20the%0Aexisting%20GSM8K%20benchmark%20for%20task-agnostic%20prompts%20in%20order%20to%20provide%20a%0Acomprehensive%20evaluation%20platform.%20Experimental%20evaluation%20using%20benchmark%0Adatasets%20shows%20that%20prompts%20compressed%20by%20Prompt-SAW%20are%20not%20only%20better%20in%0Aterms%20of%20readability%2C%20but%20they%20also%20outperform%20the%20best-performing%20baseline%0Amodels%20by%20up%20to%2010.1%20and%2077.1%2C%20respectively%2C%20for%20task-agnostic%20and%20task-aware%0Asettings%20while%20compressing%20the%20original%20prompt%20text%20by%2034.9%20and%2056.7.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00489v2&entry.124074799=Read"},
{"title": "Efficient Function Placement in Virtual Networks: An Online Learning\n  Approach", "author": "Wei Huang and Richard Combes and Hind Castel-Taleb and Badii Jouaber", "abstract": "  We propose a model for the virtual function placement problem and several\nnovel algorithms using ideas based on multi-armed bandits. We prove that these\nalgorithms learn the optimal placement policy rapidly, and their regret grows\nat a rate at most $O( N M \\sqrt{T\\ln T} )$ while respecting the feasibility\nconstraints with high probability. We show through numerical experiments that\nthose algorithms both have good practical performance and modest computational\ncomplexity. Using the proposed acceleration technique, they can be used to\nlearn in large networks where computational power is limited. Our experiments\nare fully reproducible, and the code is publicly available.\n", "link": "http://arxiv.org/abs/2410.13696v1", "date": "2024-10-17", "relevancy": 1.8612, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4669}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Function%20Placement%20in%20Virtual%20Networks%3A%20An%20Online%20Learning%0A%20%20Approach&body=Title%3A%20Efficient%20Function%20Placement%20in%20Virtual%20Networks%3A%20An%20Online%20Learning%0A%20%20Approach%0AAuthor%3A%20Wei%20Huang%20and%20Richard%20Combes%20and%20Hind%20Castel-Taleb%20and%20Badii%20Jouaber%0AAbstract%3A%20%20%20We%20propose%20a%20model%20for%20the%20virtual%20function%20placement%20problem%20and%20several%0Anovel%20algorithms%20using%20ideas%20based%20on%20multi-armed%20bandits.%20We%20prove%20that%20these%0Aalgorithms%20learn%20the%20optimal%20placement%20policy%20rapidly%2C%20and%20their%20regret%20grows%0Aat%20a%20rate%20at%20most%20%24O%28%20N%20M%20%5Csqrt%7BT%5Cln%20T%7D%20%29%24%20while%20respecting%20the%20feasibility%0Aconstraints%20with%20high%20probability.%20We%20show%20through%20numerical%20experiments%20that%0Athose%20algorithms%20both%20have%20good%20practical%20performance%20and%20modest%20computational%0Acomplexity.%20Using%20the%20proposed%20acceleration%20technique%2C%20they%20can%20be%20used%20to%0Alearn%20in%20large%20networks%20where%20computational%20power%20is%20limited.%20Our%20experiments%0Aare%20fully%20reproducible%2C%20and%20the%20code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Function%2520Placement%2520in%2520Virtual%2520Networks%253A%2520An%2520Online%2520Learning%250A%2520%2520Approach%26entry.906535625%3DWei%2520Huang%2520and%2520Richard%2520Combes%2520and%2520Hind%2520Castel-Taleb%2520and%2520Badii%2520Jouaber%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520model%2520for%2520the%2520virtual%2520function%2520placement%2520problem%2520and%2520several%250Anovel%2520algorithms%2520using%2520ideas%2520based%2520on%2520multi-armed%2520bandits.%2520We%2520prove%2520that%2520these%250Aalgorithms%2520learn%2520the%2520optimal%2520placement%2520policy%2520rapidly%252C%2520and%2520their%2520regret%2520grows%250Aat%2520a%2520rate%2520at%2520most%2520%2524O%2528%2520N%2520M%2520%255Csqrt%257BT%255Cln%2520T%257D%2520%2529%2524%2520while%2520respecting%2520the%2520feasibility%250Aconstraints%2520with%2520high%2520probability.%2520We%2520show%2520through%2520numerical%2520experiments%2520that%250Athose%2520algorithms%2520both%2520have%2520good%2520practical%2520performance%2520and%2520modest%2520computational%250Acomplexity.%2520Using%2520the%2520proposed%2520acceleration%2520technique%252C%2520they%2520can%2520be%2520used%2520to%250Alearn%2520in%2520large%2520networks%2520where%2520computational%2520power%2520is%2520limited.%2520Our%2520experiments%250Aare%2520fully%2520reproducible%252C%2520and%2520the%2520code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Function%20Placement%20in%20Virtual%20Networks%3A%20An%20Online%20Learning%0A%20%20Approach&entry.906535625=Wei%20Huang%20and%20Richard%20Combes%20and%20Hind%20Castel-Taleb%20and%20Badii%20Jouaber&entry.1292438233=%20%20We%20propose%20a%20model%20for%20the%20virtual%20function%20placement%20problem%20and%20several%0Anovel%20algorithms%20using%20ideas%20based%20on%20multi-armed%20bandits.%20We%20prove%20that%20these%0Aalgorithms%20learn%20the%20optimal%20placement%20policy%20rapidly%2C%20and%20their%20regret%20grows%0Aat%20a%20rate%20at%20most%20%24O%28%20N%20M%20%5Csqrt%7BT%5Cln%20T%7D%20%29%24%20while%20respecting%20the%20feasibility%0Aconstraints%20with%20high%20probability.%20We%20show%20through%20numerical%20experiments%20that%0Athose%20algorithms%20both%20have%20good%20practical%20performance%20and%20modest%20computational%0Acomplexity.%20Using%20the%20proposed%20acceleration%20technique%2C%20they%20can%20be%20used%20to%0Alearn%20in%20large%20networks%20where%20computational%20power%20is%20limited.%20Our%20experiments%0Aare%20fully%20reproducible%2C%20and%20the%20code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13696v1&entry.124074799=Read"},
{"title": "Towards a Factor Graph-Based Method using Angular Rates for Full\n  Magnetometer Calibration and Gyroscope Bias Estimation", "author": "Sebasti\u00e1n Rodr\u00edguez-Mart\u00ednez and Giancarlo Troni", "abstract": "  MEMS Attitude Heading Reference Systems are widely employed to determine a\nsystem's attitude, but sensor measurement biases limit their accuracy. This\npaper introduces a novel factor graph-based method called MAgnetometer and\nGYroscope Calibration (MAGYC). MAGYC leverages three-axis angular rate\nmeasurements from an angular rate gyroscope to enhance calibration for batch\nand online applications. Our approach imposes less restrictive conditions for\ninstrument movements required for calibration, eliminates the need for\nknowledge of the local magnetic field or instrument attitude, and facilitates\nintegration into factor graph algorithms within Smoothing and Mapping\nframeworks. We evaluate the proposed methods through numerical simulations and\nin-field experimental assessments using a sensor installed on an underwater\nvehicle. Ultimately, our proposed methods reduced the underwater vehicle's\nheading error standard deviation from 6.21 to 0.57 degrees for a standard\nseafloor mapping survey.\n", "link": "http://arxiv.org/abs/2410.13827v1", "date": "2024-10-17", "relevancy": 1.8546, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4568}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Factor%20Graph-Based%20Method%20using%20Angular%20Rates%20for%20Full%0A%20%20Magnetometer%20Calibration%20and%20Gyroscope%20Bias%20Estimation&body=Title%3A%20Towards%20a%20Factor%20Graph-Based%20Method%20using%20Angular%20Rates%20for%20Full%0A%20%20Magnetometer%20Calibration%20and%20Gyroscope%20Bias%20Estimation%0AAuthor%3A%20Sebasti%C3%A1n%20Rodr%C3%ADguez-Mart%C3%ADnez%20and%20Giancarlo%20Troni%0AAbstract%3A%20%20%20MEMS%20Attitude%20Heading%20Reference%20Systems%20are%20widely%20employed%20to%20determine%20a%0Asystem%27s%20attitude%2C%20but%20sensor%20measurement%20biases%20limit%20their%20accuracy.%20This%0Apaper%20introduces%20a%20novel%20factor%20graph-based%20method%20called%20MAgnetometer%20and%0AGYroscope%20Calibration%20%28MAGYC%29.%20MAGYC%20leverages%20three-axis%20angular%20rate%0Ameasurements%20from%20an%20angular%20rate%20gyroscope%20to%20enhance%20calibration%20for%20batch%0Aand%20online%20applications.%20Our%20approach%20imposes%20less%20restrictive%20conditions%20for%0Ainstrument%20movements%20required%20for%20calibration%2C%20eliminates%20the%20need%20for%0Aknowledge%20of%20the%20local%20magnetic%20field%20or%20instrument%20attitude%2C%20and%20facilitates%0Aintegration%20into%20factor%20graph%20algorithms%20within%20Smoothing%20and%20Mapping%0Aframeworks.%20We%20evaluate%20the%20proposed%20methods%20through%20numerical%20simulations%20and%0Ain-field%20experimental%20assessments%20using%20a%20sensor%20installed%20on%20an%20underwater%0Avehicle.%20Ultimately%2C%20our%20proposed%20methods%20reduced%20the%20underwater%20vehicle%27s%0Aheading%20error%20standard%20deviation%20from%206.21%20to%200.57%20degrees%20for%20a%20standard%0Aseafloor%20mapping%20survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Factor%2520Graph-Based%2520Method%2520using%2520Angular%2520Rates%2520for%2520Full%250A%2520%2520Magnetometer%2520Calibration%2520and%2520Gyroscope%2520Bias%2520Estimation%26entry.906535625%3DSebasti%25C3%25A1n%2520Rodr%25C3%25ADguez-Mart%25C3%25ADnez%2520and%2520Giancarlo%2520Troni%26entry.1292438233%3D%2520%2520MEMS%2520Attitude%2520Heading%2520Reference%2520Systems%2520are%2520widely%2520employed%2520to%2520determine%2520a%250Asystem%2527s%2520attitude%252C%2520but%2520sensor%2520measurement%2520biases%2520limit%2520their%2520accuracy.%2520This%250Apaper%2520introduces%2520a%2520novel%2520factor%2520graph-based%2520method%2520called%2520MAgnetometer%2520and%250AGYroscope%2520Calibration%2520%2528MAGYC%2529.%2520MAGYC%2520leverages%2520three-axis%2520angular%2520rate%250Ameasurements%2520from%2520an%2520angular%2520rate%2520gyroscope%2520to%2520enhance%2520calibration%2520for%2520batch%250Aand%2520online%2520applications.%2520Our%2520approach%2520imposes%2520less%2520restrictive%2520conditions%2520for%250Ainstrument%2520movements%2520required%2520for%2520calibration%252C%2520eliminates%2520the%2520need%2520for%250Aknowledge%2520of%2520the%2520local%2520magnetic%2520field%2520or%2520instrument%2520attitude%252C%2520and%2520facilitates%250Aintegration%2520into%2520factor%2520graph%2520algorithms%2520within%2520Smoothing%2520and%2520Mapping%250Aframeworks.%2520We%2520evaluate%2520the%2520proposed%2520methods%2520through%2520numerical%2520simulations%2520and%250Ain-field%2520experimental%2520assessments%2520using%2520a%2520sensor%2520installed%2520on%2520an%2520underwater%250Avehicle.%2520Ultimately%252C%2520our%2520proposed%2520methods%2520reduced%2520the%2520underwater%2520vehicle%2527s%250Aheading%2520error%2520standard%2520deviation%2520from%25206.21%2520to%25200.57%2520degrees%2520for%2520a%2520standard%250Aseafloor%2520mapping%2520survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Factor%20Graph-Based%20Method%20using%20Angular%20Rates%20for%20Full%0A%20%20Magnetometer%20Calibration%20and%20Gyroscope%20Bias%20Estimation&entry.906535625=Sebasti%C3%A1n%20Rodr%C3%ADguez-Mart%C3%ADnez%20and%20Giancarlo%20Troni&entry.1292438233=%20%20MEMS%20Attitude%20Heading%20Reference%20Systems%20are%20widely%20employed%20to%20determine%20a%0Asystem%27s%20attitude%2C%20but%20sensor%20measurement%20biases%20limit%20their%20accuracy.%20This%0Apaper%20introduces%20a%20novel%20factor%20graph-based%20method%20called%20MAgnetometer%20and%0AGYroscope%20Calibration%20%28MAGYC%29.%20MAGYC%20leverages%20three-axis%20angular%20rate%0Ameasurements%20from%20an%20angular%20rate%20gyroscope%20to%20enhance%20calibration%20for%20batch%0Aand%20online%20applications.%20Our%20approach%20imposes%20less%20restrictive%20conditions%20for%0Ainstrument%20movements%20required%20for%20calibration%2C%20eliminates%20the%20need%20for%0Aknowledge%20of%20the%20local%20magnetic%20field%20or%20instrument%20attitude%2C%20and%20facilitates%0Aintegration%20into%20factor%20graph%20algorithms%20within%20Smoothing%20and%20Mapping%0Aframeworks.%20We%20evaluate%20the%20proposed%20methods%20through%20numerical%20simulations%20and%0Ain-field%20experimental%20assessments%20using%20a%20sensor%20installed%20on%20an%20underwater%0Avehicle.%20Ultimately%2C%20our%20proposed%20methods%20reduced%20the%20underwater%20vehicle%27s%0Aheading%20error%20standard%20deviation%20from%206.21%20to%200.57%20degrees%20for%20a%20standard%0Aseafloor%20mapping%20survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13827v1&entry.124074799=Read"},
{"title": "ConsisSR: Delving Deep into Consistency in Diffusion-based Image\n  Super-Resolution", "author": "Junhao Gu and Peng-Tao Jiang and Hao Zhang and Mi Zhou and Jinwei Chen and Wenming Yang and Bo Li", "abstract": "  Real-world image super-resolution (Real-ISR) aims at restoring high-quality\n(HQ) images from low-quality (LQ) inputs corrupted by unknown and complex\ndegradations. In particular, pretrained text-to-image (T2I) diffusion models\nprovide strong generative priors to reconstruct credible and intricate details.\nHowever, T2I generation focuses on semantic consistency while Real-ISR\nemphasizes pixel-level reconstruction, which hinders existing methods from\nfully exploiting diffusion priors. To address this challenge, we introduce\nConsisSR to handle both semantic and pixel-level consistency. Specifically,\ncompared to coarse-grained text prompts, we exploit the more powerful CLIP\nimage embedding and effectively leverage both modalities through our Hybrid\nPrompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware\nLatent Augmentation (TALA) to mitigate the inherent gap between T2I generation\nand Real-ISR consistency requirements. By randomly mixing LQ and HQ latent\ninputs, our model not only handle timestep-specific diffusion noise but also\nrefine the accumulated latent representations. Last but not least, our\nGAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the\ndiffusion start point. This accelerates the inference process to 10 steps while\npreserving sampling quality, in a training-free manner. Our method demonstrates\nstate-of-the-art performance among both full-scale and accelerated models. The\ncode will be made publicly available.\n", "link": "http://arxiv.org/abs/2410.13807v1", "date": "2024-10-17", "relevancy": 1.8383, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6554}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.608}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConsisSR%3A%20Delving%20Deep%20into%20Consistency%20in%20Diffusion-based%20Image%0A%20%20Super-Resolution&body=Title%3A%20ConsisSR%3A%20Delving%20Deep%20into%20Consistency%20in%20Diffusion-based%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Junhao%20Gu%20and%20Peng-Tao%20Jiang%20and%20Hao%20Zhang%20and%20Mi%20Zhou%20and%20Jinwei%20Chen%20and%20Wenming%20Yang%20and%20Bo%20Li%0AAbstract%3A%20%20%20Real-world%20image%20super-resolution%20%28Real-ISR%29%20aims%20at%20restoring%20high-quality%0A%28HQ%29%20images%20from%20low-quality%20%28LQ%29%20inputs%20corrupted%20by%20unknown%20and%20complex%0Adegradations.%20In%20particular%2C%20pretrained%20text-to-image%20%28T2I%29%20diffusion%20models%0Aprovide%20strong%20generative%20priors%20to%20reconstruct%20credible%20and%20intricate%20details.%0AHowever%2C%20T2I%20generation%20focuses%20on%20semantic%20consistency%20while%20Real-ISR%0Aemphasizes%20pixel-level%20reconstruction%2C%20which%20hinders%20existing%20methods%20from%0Afully%20exploiting%20diffusion%20priors.%20To%20address%20this%20challenge%2C%20we%20introduce%0AConsisSR%20to%20handle%20both%20semantic%20and%20pixel-level%20consistency.%20Specifically%2C%0Acompared%20to%20coarse-grained%20text%20prompts%2C%20we%20exploit%20the%20more%20powerful%20CLIP%0Aimage%20embedding%20and%20effectively%20leverage%20both%20modalities%20through%20our%20Hybrid%0APrompt%20Adapter%20%28HPA%29%20for%20semantic%20guidance.%20Secondly%2C%20we%20introduce%20Time-aware%0ALatent%20Augmentation%20%28TALA%29%20to%20mitigate%20the%20inherent%20gap%20between%20T2I%20generation%0Aand%20Real-ISR%20consistency%20requirements.%20By%20randomly%20mixing%20LQ%20and%20HQ%20latent%0Ainputs%2C%20our%20model%20not%20only%20handle%20timestep-specific%20diffusion%20noise%20but%20also%0Arefine%20the%20accumulated%20latent%20representations.%20Last%20but%20not%20least%2C%20our%0AGAN-Embedding%20strategy%20employs%20the%20pretrained%20Real-ESRGAN%20model%20to%20refine%20the%0Adiffusion%20start%20point.%20This%20accelerates%20the%20inference%20process%20to%2010%20steps%20while%0Apreserving%20sampling%20quality%2C%20in%20a%20training-free%20manner.%20Our%20method%20demonstrates%0Astate-of-the-art%20performance%20among%20both%20full-scale%20and%20accelerated%20models.%20The%0Acode%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsisSR%253A%2520Delving%2520Deep%2520into%2520Consistency%2520in%2520Diffusion-based%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DJunhao%2520Gu%2520and%2520Peng-Tao%2520Jiang%2520and%2520Hao%2520Zhang%2520and%2520Mi%2520Zhou%2520and%2520Jinwei%2520Chen%2520and%2520Wenming%2520Yang%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Real-world%2520image%2520super-resolution%2520%2528Real-ISR%2529%2520aims%2520at%2520restoring%2520high-quality%250A%2528HQ%2529%2520images%2520from%2520low-quality%2520%2528LQ%2529%2520inputs%2520corrupted%2520by%2520unknown%2520and%2520complex%250Adegradations.%2520In%2520particular%252C%2520pretrained%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%250Aprovide%2520strong%2520generative%2520priors%2520to%2520reconstruct%2520credible%2520and%2520intricate%2520details.%250AHowever%252C%2520T2I%2520generation%2520focuses%2520on%2520semantic%2520consistency%2520while%2520Real-ISR%250Aemphasizes%2520pixel-level%2520reconstruction%252C%2520which%2520hinders%2520existing%2520methods%2520from%250Afully%2520exploiting%2520diffusion%2520priors.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%250AConsisSR%2520to%2520handle%2520both%2520semantic%2520and%2520pixel-level%2520consistency.%2520Specifically%252C%250Acompared%2520to%2520coarse-grained%2520text%2520prompts%252C%2520we%2520exploit%2520the%2520more%2520powerful%2520CLIP%250Aimage%2520embedding%2520and%2520effectively%2520leverage%2520both%2520modalities%2520through%2520our%2520Hybrid%250APrompt%2520Adapter%2520%2528HPA%2529%2520for%2520semantic%2520guidance.%2520Secondly%252C%2520we%2520introduce%2520Time-aware%250ALatent%2520Augmentation%2520%2528TALA%2529%2520to%2520mitigate%2520the%2520inherent%2520gap%2520between%2520T2I%2520generation%250Aand%2520Real-ISR%2520consistency%2520requirements.%2520By%2520randomly%2520mixing%2520LQ%2520and%2520HQ%2520latent%250Ainputs%252C%2520our%2520model%2520not%2520only%2520handle%2520timestep-specific%2520diffusion%2520noise%2520but%2520also%250Arefine%2520the%2520accumulated%2520latent%2520representations.%2520Last%2520but%2520not%2520least%252C%2520our%250AGAN-Embedding%2520strategy%2520employs%2520the%2520pretrained%2520Real-ESRGAN%2520model%2520to%2520refine%2520the%250Adiffusion%2520start%2520point.%2520This%2520accelerates%2520the%2520inference%2520process%2520to%252010%2520steps%2520while%250Apreserving%2520sampling%2520quality%252C%2520in%2520a%2520training-free%2520manner.%2520Our%2520method%2520demonstrates%250Astate-of-the-art%2520performance%2520among%2520both%2520full-scale%2520and%2520accelerated%2520models.%2520The%250Acode%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsisSR%3A%20Delving%20Deep%20into%20Consistency%20in%20Diffusion-based%20Image%0A%20%20Super-Resolution&entry.906535625=Junhao%20Gu%20and%20Peng-Tao%20Jiang%20and%20Hao%20Zhang%20and%20Mi%20Zhou%20and%20Jinwei%20Chen%20and%20Wenming%20Yang%20and%20Bo%20Li&entry.1292438233=%20%20Real-world%20image%20super-resolution%20%28Real-ISR%29%20aims%20at%20restoring%20high-quality%0A%28HQ%29%20images%20from%20low-quality%20%28LQ%29%20inputs%20corrupted%20by%20unknown%20and%20complex%0Adegradations.%20In%20particular%2C%20pretrained%20text-to-image%20%28T2I%29%20diffusion%20models%0Aprovide%20strong%20generative%20priors%20to%20reconstruct%20credible%20and%20intricate%20details.%0AHowever%2C%20T2I%20generation%20focuses%20on%20semantic%20consistency%20while%20Real-ISR%0Aemphasizes%20pixel-level%20reconstruction%2C%20which%20hinders%20existing%20methods%20from%0Afully%20exploiting%20diffusion%20priors.%20To%20address%20this%20challenge%2C%20we%20introduce%0AConsisSR%20to%20handle%20both%20semantic%20and%20pixel-level%20consistency.%20Specifically%2C%0Acompared%20to%20coarse-grained%20text%20prompts%2C%20we%20exploit%20the%20more%20powerful%20CLIP%0Aimage%20embedding%20and%20effectively%20leverage%20both%20modalities%20through%20our%20Hybrid%0APrompt%20Adapter%20%28HPA%29%20for%20semantic%20guidance.%20Secondly%2C%20we%20introduce%20Time-aware%0ALatent%20Augmentation%20%28TALA%29%20to%20mitigate%20the%20inherent%20gap%20between%20T2I%20generation%0Aand%20Real-ISR%20consistency%20requirements.%20By%20randomly%20mixing%20LQ%20and%20HQ%20latent%0Ainputs%2C%20our%20model%20not%20only%20handle%20timestep-specific%20diffusion%20noise%20but%20also%0Arefine%20the%20accumulated%20latent%20representations.%20Last%20but%20not%20least%2C%20our%0AGAN-Embedding%20strategy%20employs%20the%20pretrained%20Real-ESRGAN%20model%20to%20refine%20the%0Adiffusion%20start%20point.%20This%20accelerates%20the%20inference%20process%20to%2010%20steps%20while%0Apreserving%20sampling%20quality%2C%20in%20a%20training-free%20manner.%20Our%20method%20demonstrates%0Astate-of-the-art%20performance%20among%20both%20full-scale%20and%20accelerated%20models.%20The%0Acode%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13807v1&entry.124074799=Read"},
{"title": "All models are wrong, some are useful: Model Selection with Limited\n  Labels", "author": "Patrik Okanovic and Andreas Kirsch and Jannes Kasper and Torsten Hoefler and Andreas Krause and Nezihe Merve G\u00fcrel", "abstract": "  With the multitude of pretrained models available thanks to the advancements\nin large-scale supervised and self-supervised learning, choosing the right\nmodel is becoming increasingly pivotal in the machine learning lifecycle.\nHowever, much like the training process, choosing the best pretrained\noff-the-shelf model for raw, unlabeled data is a labor-intensive task. To\novercome this, we introduce MODEL SELECTOR, a framework for label-efficient\nselection of pretrained classifiers. Given a pool of unlabeled target data,\nMODEL SELECTOR samples a small subset of highly informative examples for\nlabeling, in order to efficiently identify the best pretrained model for\ndeployment on this target dataset. Through extensive experiments, we\ndemonstrate that MODEL SELECTOR drastically reduces the need for labeled data\nwhile consistently picking the best or near-best performing model. Across 18\nmodel collections on 16 different datasets, comprising over 1,500 pretrained\nmodels, MODEL SELECTOR reduces the labeling cost by up to 94.15% to identify\nthe best model compared to the cost of the strongest baseline. Our results\nfurther highlight the robustness of MODEL SELECTOR in model selection, as it\nreduces the labeling cost by up to 72.41% when selecting a near-best model,\nwhose accuracy is only within 1% of the best model.\n", "link": "http://arxiv.org/abs/2410.13609v1", "date": "2024-10-17", "relevancy": 1.8299, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4676}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4604}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20models%20are%20wrong%2C%20some%20are%20useful%3A%20Model%20Selection%20with%20Limited%0A%20%20Labels&body=Title%3A%20All%20models%20are%20wrong%2C%20some%20are%20useful%3A%20Model%20Selection%20with%20Limited%0A%20%20Labels%0AAuthor%3A%20Patrik%20Okanovic%20and%20Andreas%20Kirsch%20and%20Jannes%20Kasper%20and%20Torsten%20Hoefler%20and%20Andreas%20Krause%20and%20Nezihe%20Merve%20G%C3%BCrel%0AAbstract%3A%20%20%20With%20the%20multitude%20of%20pretrained%20models%20available%20thanks%20to%20the%20advancements%0Ain%20large-scale%20supervised%20and%20self-supervised%20learning%2C%20choosing%20the%20right%0Amodel%20is%20becoming%20increasingly%20pivotal%20in%20the%20machine%20learning%20lifecycle.%0AHowever%2C%20much%20like%20the%20training%20process%2C%20choosing%20the%20best%20pretrained%0Aoff-the-shelf%20model%20for%20raw%2C%20unlabeled%20data%20is%20a%20labor-intensive%20task.%20To%0Aovercome%20this%2C%20we%20introduce%20MODEL%20SELECTOR%2C%20a%20framework%20for%20label-efficient%0Aselection%20of%20pretrained%20classifiers.%20Given%20a%20pool%20of%20unlabeled%20target%20data%2C%0AMODEL%20SELECTOR%20samples%20a%20small%20subset%20of%20highly%20informative%20examples%20for%0Alabeling%2C%20in%20order%20to%20efficiently%20identify%20the%20best%20pretrained%20model%20for%0Adeployment%20on%20this%20target%20dataset.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20MODEL%20SELECTOR%20drastically%20reduces%20the%20need%20for%20labeled%20data%0Awhile%20consistently%20picking%20the%20best%20or%20near-best%20performing%20model.%20Across%2018%0Amodel%20collections%20on%2016%20different%20datasets%2C%20comprising%20over%201%2C500%20pretrained%0Amodels%2C%20MODEL%20SELECTOR%20reduces%20the%20labeling%20cost%20by%20up%20to%2094.15%25%20to%20identify%0Athe%20best%20model%20compared%20to%20the%20cost%20of%20the%20strongest%20baseline.%20Our%20results%0Afurther%20highlight%20the%20robustness%20of%20MODEL%20SELECTOR%20in%20model%20selection%2C%20as%20it%0Areduces%20the%20labeling%20cost%20by%20up%20to%2072.41%25%20when%20selecting%20a%20near-best%20model%2C%0Awhose%20accuracy%20is%20only%20within%201%25%20of%20the%20best%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520models%2520are%2520wrong%252C%2520some%2520are%2520useful%253A%2520Model%2520Selection%2520with%2520Limited%250A%2520%2520Labels%26entry.906535625%3DPatrik%2520Okanovic%2520and%2520Andreas%2520Kirsch%2520and%2520Jannes%2520Kasper%2520and%2520Torsten%2520Hoefler%2520and%2520Andreas%2520Krause%2520and%2520Nezihe%2520Merve%2520G%25C3%25BCrel%26entry.1292438233%3D%2520%2520With%2520the%2520multitude%2520of%2520pretrained%2520models%2520available%2520thanks%2520to%2520the%2520advancements%250Ain%2520large-scale%2520supervised%2520and%2520self-supervised%2520learning%252C%2520choosing%2520the%2520right%250Amodel%2520is%2520becoming%2520increasingly%2520pivotal%2520in%2520the%2520machine%2520learning%2520lifecycle.%250AHowever%252C%2520much%2520like%2520the%2520training%2520process%252C%2520choosing%2520the%2520best%2520pretrained%250Aoff-the-shelf%2520model%2520for%2520raw%252C%2520unlabeled%2520data%2520is%2520a%2520labor-intensive%2520task.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520MODEL%2520SELECTOR%252C%2520a%2520framework%2520for%2520label-efficient%250Aselection%2520of%2520pretrained%2520classifiers.%2520Given%2520a%2520pool%2520of%2520unlabeled%2520target%2520data%252C%250AMODEL%2520SELECTOR%2520samples%2520a%2520small%2520subset%2520of%2520highly%2520informative%2520examples%2520for%250Alabeling%252C%2520in%2520order%2520to%2520efficiently%2520identify%2520the%2520best%2520pretrained%2520model%2520for%250Adeployment%2520on%2520this%2520target%2520dataset.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520MODEL%2520SELECTOR%2520drastically%2520reduces%2520the%2520need%2520for%2520labeled%2520data%250Awhile%2520consistently%2520picking%2520the%2520best%2520or%2520near-best%2520performing%2520model.%2520Across%252018%250Amodel%2520collections%2520on%252016%2520different%2520datasets%252C%2520comprising%2520over%25201%252C500%2520pretrained%250Amodels%252C%2520MODEL%2520SELECTOR%2520reduces%2520the%2520labeling%2520cost%2520by%2520up%2520to%252094.15%2525%2520to%2520identify%250Athe%2520best%2520model%2520compared%2520to%2520the%2520cost%2520of%2520the%2520strongest%2520baseline.%2520Our%2520results%250Afurther%2520highlight%2520the%2520robustness%2520of%2520MODEL%2520SELECTOR%2520in%2520model%2520selection%252C%2520as%2520it%250Areduces%2520the%2520labeling%2520cost%2520by%2520up%2520to%252072.41%2525%2520when%2520selecting%2520a%2520near-best%2520model%252C%250Awhose%2520accuracy%2520is%2520only%2520within%25201%2525%2520of%2520the%2520best%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20models%20are%20wrong%2C%20some%20are%20useful%3A%20Model%20Selection%20with%20Limited%0A%20%20Labels&entry.906535625=Patrik%20Okanovic%20and%20Andreas%20Kirsch%20and%20Jannes%20Kasper%20and%20Torsten%20Hoefler%20and%20Andreas%20Krause%20and%20Nezihe%20Merve%20G%C3%BCrel&entry.1292438233=%20%20With%20the%20multitude%20of%20pretrained%20models%20available%20thanks%20to%20the%20advancements%0Ain%20large-scale%20supervised%20and%20self-supervised%20learning%2C%20choosing%20the%20right%0Amodel%20is%20becoming%20increasingly%20pivotal%20in%20the%20machine%20learning%20lifecycle.%0AHowever%2C%20much%20like%20the%20training%20process%2C%20choosing%20the%20best%20pretrained%0Aoff-the-shelf%20model%20for%20raw%2C%20unlabeled%20data%20is%20a%20labor-intensive%20task.%20To%0Aovercome%20this%2C%20we%20introduce%20MODEL%20SELECTOR%2C%20a%20framework%20for%20label-efficient%0Aselection%20of%20pretrained%20classifiers.%20Given%20a%20pool%20of%20unlabeled%20target%20data%2C%0AMODEL%20SELECTOR%20samples%20a%20small%20subset%20of%20highly%20informative%20examples%20for%0Alabeling%2C%20in%20order%20to%20efficiently%20identify%20the%20best%20pretrained%20model%20for%0Adeployment%20on%20this%20target%20dataset.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20MODEL%20SELECTOR%20drastically%20reduces%20the%20need%20for%20labeled%20data%0Awhile%20consistently%20picking%20the%20best%20or%20near-best%20performing%20model.%20Across%2018%0Amodel%20collections%20on%2016%20different%20datasets%2C%20comprising%20over%201%2C500%20pretrained%0Amodels%2C%20MODEL%20SELECTOR%20reduces%20the%20labeling%20cost%20by%20up%20to%2094.15%25%20to%20identify%0Athe%20best%20model%20compared%20to%20the%20cost%20of%20the%20strongest%20baseline.%20Our%20results%0Afurther%20highlight%20the%20robustness%20of%20MODEL%20SELECTOR%20in%20model%20selection%2C%20as%20it%0Areduces%20the%20labeling%20cost%20by%20up%20to%2072.41%25%20when%20selecting%20a%20near-best%20model%2C%0Awhose%20accuracy%20is%20only%20within%201%25%20of%20the%20best%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13609v1&entry.124074799=Read"},
{"title": "Adversarial Testing as a Tool for Interpretability: Length-based\n  Overfitting of Elementary Functions in Transformers", "author": "Patrik Zavoral and Du\u0161an Vari\u0161 and Ond\u0159ej Bojar", "abstract": "  The Transformer model has a tendency to overfit various aspects of the\ntraining data, such as the overall sequence length. We study elementary string\nedit functions using a defined set of error indicators to interpret the\nbehaviour of the sequence-to-sequence Transformer. We show that generalization\nto shorter sequences is often possible, but confirm that longer sequences are\nhighly problematic, although partially correct answers are often obtained.\nAdditionally, we find that other structural characteristics of the sequences,\nsuch as subsegment length, may be equally important. We hypothesize that the\nmodels learn algorithmic aspects of the tasks simultaneously with structural\naspects but adhering to the structural aspects is unfortunately often preferred\nby Transformer when they come into conflict.\n", "link": "http://arxiv.org/abs/2410.13802v1", "date": "2024-10-17", "relevancy": 1.8227, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4547}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Testing%20as%20a%20Tool%20for%20Interpretability%3A%20Length-based%0A%20%20Overfitting%20of%20Elementary%20Functions%20in%20Transformers&body=Title%3A%20Adversarial%20Testing%20as%20a%20Tool%20for%20Interpretability%3A%20Length-based%0A%20%20Overfitting%20of%20Elementary%20Functions%20in%20Transformers%0AAuthor%3A%20Patrik%20Zavoral%20and%20Du%C5%A1an%20Vari%C5%A1%20and%20Ond%C5%99ej%20Bojar%0AAbstract%3A%20%20%20The%20Transformer%20model%20has%20a%20tendency%20to%20overfit%20various%20aspects%20of%20the%0Atraining%20data%2C%20such%20as%20the%20overall%20sequence%20length.%20We%20study%20elementary%20string%0Aedit%20functions%20using%20a%20defined%20set%20of%20error%20indicators%20to%20interpret%20the%0Abehaviour%20of%20the%20sequence-to-sequence%20Transformer.%20We%20show%20that%20generalization%0Ato%20shorter%20sequences%20is%20often%20possible%2C%20but%20confirm%20that%20longer%20sequences%20are%0Ahighly%20problematic%2C%20although%20partially%20correct%20answers%20are%20often%20obtained.%0AAdditionally%2C%20we%20find%20that%20other%20structural%20characteristics%20of%20the%20sequences%2C%0Asuch%20as%20subsegment%20length%2C%20may%20be%20equally%20important.%20We%20hypothesize%20that%20the%0Amodels%20learn%20algorithmic%20aspects%20of%20the%20tasks%20simultaneously%20with%20structural%0Aaspects%20but%20adhering%20to%20the%20structural%20aspects%20is%20unfortunately%20often%20preferred%0Aby%20Transformer%20when%20they%20come%20into%20conflict.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Testing%2520as%2520a%2520Tool%2520for%2520Interpretability%253A%2520Length-based%250A%2520%2520Overfitting%2520of%2520Elementary%2520Functions%2520in%2520Transformers%26entry.906535625%3DPatrik%2520Zavoral%2520and%2520Du%25C5%25A1an%2520Vari%25C5%25A1%2520and%2520Ond%25C5%2599ej%2520Bojar%26entry.1292438233%3D%2520%2520The%2520Transformer%2520model%2520has%2520a%2520tendency%2520to%2520overfit%2520various%2520aspects%2520of%2520the%250Atraining%2520data%252C%2520such%2520as%2520the%2520overall%2520sequence%2520length.%2520We%2520study%2520elementary%2520string%250Aedit%2520functions%2520using%2520a%2520defined%2520set%2520of%2520error%2520indicators%2520to%2520interpret%2520the%250Abehaviour%2520of%2520the%2520sequence-to-sequence%2520Transformer.%2520We%2520show%2520that%2520generalization%250Ato%2520shorter%2520sequences%2520is%2520often%2520possible%252C%2520but%2520confirm%2520that%2520longer%2520sequences%2520are%250Ahighly%2520problematic%252C%2520although%2520partially%2520correct%2520answers%2520are%2520often%2520obtained.%250AAdditionally%252C%2520we%2520find%2520that%2520other%2520structural%2520characteristics%2520of%2520the%2520sequences%252C%250Asuch%2520as%2520subsegment%2520length%252C%2520may%2520be%2520equally%2520important.%2520We%2520hypothesize%2520that%2520the%250Amodels%2520learn%2520algorithmic%2520aspects%2520of%2520the%2520tasks%2520simultaneously%2520with%2520structural%250Aaspects%2520but%2520adhering%2520to%2520the%2520structural%2520aspects%2520is%2520unfortunately%2520often%2520preferred%250Aby%2520Transformer%2520when%2520they%2520come%2520into%2520conflict.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Testing%20as%20a%20Tool%20for%20Interpretability%3A%20Length-based%0A%20%20Overfitting%20of%20Elementary%20Functions%20in%20Transformers&entry.906535625=Patrik%20Zavoral%20and%20Du%C5%A1an%20Vari%C5%A1%20and%20Ond%C5%99ej%20Bojar&entry.1292438233=%20%20The%20Transformer%20model%20has%20a%20tendency%20to%20overfit%20various%20aspects%20of%20the%0Atraining%20data%2C%20such%20as%20the%20overall%20sequence%20length.%20We%20study%20elementary%20string%0Aedit%20functions%20using%20a%20defined%20set%20of%20error%20indicators%20to%20interpret%20the%0Abehaviour%20of%20the%20sequence-to-sequence%20Transformer.%20We%20show%20that%20generalization%0Ato%20shorter%20sequences%20is%20often%20possible%2C%20but%20confirm%20that%20longer%20sequences%20are%0Ahighly%20problematic%2C%20although%20partially%20correct%20answers%20are%20often%20obtained.%0AAdditionally%2C%20we%20find%20that%20other%20structural%20characteristics%20of%20the%20sequences%2C%0Asuch%20as%20subsegment%20length%2C%20may%20be%20equally%20important.%20We%20hypothesize%20that%20the%0Amodels%20learn%20algorithmic%20aspects%20of%20the%20tasks%20simultaneously%20with%20structural%0Aaspects%20but%20adhering%20to%20the%20structural%20aspects%20is%20unfortunately%20often%20preferred%0Aby%20Transformer%20when%20they%20come%20into%20conflict.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13802v1&entry.124074799=Read"},
{"title": "Dynamic Topic Language Model on Heterogeneous Children's Mental Health\n  Clinical Notes", "author": "Hanwen Ye and Tatiana Moreno and Adrianne Alpern and Louis Ehwerhemuepha and Annie Qu", "abstract": "  Mental health diseases affect children's lives and well-beings which have\nreceived increased attention since the COVID-19 pandemic. Analyzing psychiatric\nclinical notes with topic models is critical to evaluating children's mental\nstatus over time. However, few topic models are built for longitudinal\nsettings, and most existing approaches fail to capture temporal trajectories\nfor each document. To address these challenges, we develop a dynamic topic\nmodel with consistent topics and individualized temporal dependencies on the\nevolving document metadata. Our model preserves the semantic meaning of\ndiscovered topics over time and incorporates heterogeneity among documents. In\nparticular, when documents can be categorized, we propose a classifier-free\napproach to maximize topic heterogeneity across different document groups. We\nalso present an efficient variational optimization procedure adapted for the\nmultistage longitudinal setting. In this case study, we apply our method to the\npsychiatric clinical notes from a large tertiary pediatric hospital in Southern\nCalifornia and achieve a 38% increase in the overall coherence of extracted\ntopics. Our real data analysis reveals that children tend to express more\nnegative emotions during state shutdowns and more positive when schools reopen.\nFurthermore, it suggests that sexual and gender minority (SGM) children display\nmore pronounced reactions to major COVID-19 events and a greater sensitivity to\nvaccine-related news than non-SGM children. This study examines children's\nmental health progression during the pandemic and offers clinicians valuable\ninsights to recognize disparities in children's mental health related to their\nsexual and gender identities.\n", "link": "http://arxiv.org/abs/2312.14180v2", "date": "2024-10-17", "relevancy": 1.8212, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Topic%20Language%20Model%20on%20Heterogeneous%20Children%27s%20Mental%20Health%0A%20%20Clinical%20Notes&body=Title%3A%20Dynamic%20Topic%20Language%20Model%20on%20Heterogeneous%20Children%27s%20Mental%20Health%0A%20%20Clinical%20Notes%0AAuthor%3A%20Hanwen%20Ye%20and%20Tatiana%20Moreno%20and%20Adrianne%20Alpern%20and%20Louis%20Ehwerhemuepha%20and%20Annie%20Qu%0AAbstract%3A%20%20%20Mental%20health%20diseases%20affect%20children%27s%20lives%20and%20well-beings%20which%20have%0Areceived%20increased%20attention%20since%20the%20COVID-19%20pandemic.%20Analyzing%20psychiatric%0Aclinical%20notes%20with%20topic%20models%20is%20critical%20to%20evaluating%20children%27s%20mental%0Astatus%20over%20time.%20However%2C%20few%20topic%20models%20are%20built%20for%20longitudinal%0Asettings%2C%20and%20most%20existing%20approaches%20fail%20to%20capture%20temporal%20trajectories%0Afor%20each%20document.%20To%20address%20these%20challenges%2C%20we%20develop%20a%20dynamic%20topic%0Amodel%20with%20consistent%20topics%20and%20individualized%20temporal%20dependencies%20on%20the%0Aevolving%20document%20metadata.%20Our%20model%20preserves%20the%20semantic%20meaning%20of%0Adiscovered%20topics%20over%20time%20and%20incorporates%20heterogeneity%20among%20documents.%20In%0Aparticular%2C%20when%20documents%20can%20be%20categorized%2C%20we%20propose%20a%20classifier-free%0Aapproach%20to%20maximize%20topic%20heterogeneity%20across%20different%20document%20groups.%20We%0Aalso%20present%20an%20efficient%20variational%20optimization%20procedure%20adapted%20for%20the%0Amultistage%20longitudinal%20setting.%20In%20this%20case%20study%2C%20we%20apply%20our%20method%20to%20the%0Apsychiatric%20clinical%20notes%20from%20a%20large%20tertiary%20pediatric%20hospital%20in%20Southern%0ACalifornia%20and%20achieve%20a%2038%25%20increase%20in%20the%20overall%20coherence%20of%20extracted%0Atopics.%20Our%20real%20data%20analysis%20reveals%20that%20children%20tend%20to%20express%20more%0Anegative%20emotions%20during%20state%20shutdowns%20and%20more%20positive%20when%20schools%20reopen.%0AFurthermore%2C%20it%20suggests%20that%20sexual%20and%20gender%20minority%20%28SGM%29%20children%20display%0Amore%20pronounced%20reactions%20to%20major%20COVID-19%20events%20and%20a%20greater%20sensitivity%20to%0Avaccine-related%20news%20than%20non-SGM%20children.%20This%20study%20examines%20children%27s%0Amental%20health%20progression%20during%20the%20pandemic%20and%20offers%20clinicians%20valuable%0Ainsights%20to%20recognize%20disparities%20in%20children%27s%20mental%20health%20related%20to%20their%0Asexual%20and%20gender%20identities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Topic%2520Language%2520Model%2520on%2520Heterogeneous%2520Children%2527s%2520Mental%2520Health%250A%2520%2520Clinical%2520Notes%26entry.906535625%3DHanwen%2520Ye%2520and%2520Tatiana%2520Moreno%2520and%2520Adrianne%2520Alpern%2520and%2520Louis%2520Ehwerhemuepha%2520and%2520Annie%2520Qu%26entry.1292438233%3D%2520%2520Mental%2520health%2520diseases%2520affect%2520children%2527s%2520lives%2520and%2520well-beings%2520which%2520have%250Areceived%2520increased%2520attention%2520since%2520the%2520COVID-19%2520pandemic.%2520Analyzing%2520psychiatric%250Aclinical%2520notes%2520with%2520topic%2520models%2520is%2520critical%2520to%2520evaluating%2520children%2527s%2520mental%250Astatus%2520over%2520time.%2520However%252C%2520few%2520topic%2520models%2520are%2520built%2520for%2520longitudinal%250Asettings%252C%2520and%2520most%2520existing%2520approaches%2520fail%2520to%2520capture%2520temporal%2520trajectories%250Afor%2520each%2520document.%2520To%2520address%2520these%2520challenges%252C%2520we%2520develop%2520a%2520dynamic%2520topic%250Amodel%2520with%2520consistent%2520topics%2520and%2520individualized%2520temporal%2520dependencies%2520on%2520the%250Aevolving%2520document%2520metadata.%2520Our%2520model%2520preserves%2520the%2520semantic%2520meaning%2520of%250Adiscovered%2520topics%2520over%2520time%2520and%2520incorporates%2520heterogeneity%2520among%2520documents.%2520In%250Aparticular%252C%2520when%2520documents%2520can%2520be%2520categorized%252C%2520we%2520propose%2520a%2520classifier-free%250Aapproach%2520to%2520maximize%2520topic%2520heterogeneity%2520across%2520different%2520document%2520groups.%2520We%250Aalso%2520present%2520an%2520efficient%2520variational%2520optimization%2520procedure%2520adapted%2520for%2520the%250Amultistage%2520longitudinal%2520setting.%2520In%2520this%2520case%2520study%252C%2520we%2520apply%2520our%2520method%2520to%2520the%250Apsychiatric%2520clinical%2520notes%2520from%2520a%2520large%2520tertiary%2520pediatric%2520hospital%2520in%2520Southern%250ACalifornia%2520and%2520achieve%2520a%252038%2525%2520increase%2520in%2520the%2520overall%2520coherence%2520of%2520extracted%250Atopics.%2520Our%2520real%2520data%2520analysis%2520reveals%2520that%2520children%2520tend%2520to%2520express%2520more%250Anegative%2520emotions%2520during%2520state%2520shutdowns%2520and%2520more%2520positive%2520when%2520schools%2520reopen.%250AFurthermore%252C%2520it%2520suggests%2520that%2520sexual%2520and%2520gender%2520minority%2520%2528SGM%2529%2520children%2520display%250Amore%2520pronounced%2520reactions%2520to%2520major%2520COVID-19%2520events%2520and%2520a%2520greater%2520sensitivity%2520to%250Avaccine-related%2520news%2520than%2520non-SGM%2520children.%2520This%2520study%2520examines%2520children%2527s%250Amental%2520health%2520progression%2520during%2520the%2520pandemic%2520and%2520offers%2520clinicians%2520valuable%250Ainsights%2520to%2520recognize%2520disparities%2520in%2520children%2527s%2520mental%2520health%2520related%2520to%2520their%250Asexual%2520and%2520gender%2520identities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Topic%20Language%20Model%20on%20Heterogeneous%20Children%27s%20Mental%20Health%0A%20%20Clinical%20Notes&entry.906535625=Hanwen%20Ye%20and%20Tatiana%20Moreno%20and%20Adrianne%20Alpern%20and%20Louis%20Ehwerhemuepha%20and%20Annie%20Qu&entry.1292438233=%20%20Mental%20health%20diseases%20affect%20children%27s%20lives%20and%20well-beings%20which%20have%0Areceived%20increased%20attention%20since%20the%20COVID-19%20pandemic.%20Analyzing%20psychiatric%0Aclinical%20notes%20with%20topic%20models%20is%20critical%20to%20evaluating%20children%27s%20mental%0Astatus%20over%20time.%20However%2C%20few%20topic%20models%20are%20built%20for%20longitudinal%0Asettings%2C%20and%20most%20existing%20approaches%20fail%20to%20capture%20temporal%20trajectories%0Afor%20each%20document.%20To%20address%20these%20challenges%2C%20we%20develop%20a%20dynamic%20topic%0Amodel%20with%20consistent%20topics%20and%20individualized%20temporal%20dependencies%20on%20the%0Aevolving%20document%20metadata.%20Our%20model%20preserves%20the%20semantic%20meaning%20of%0Adiscovered%20topics%20over%20time%20and%20incorporates%20heterogeneity%20among%20documents.%20In%0Aparticular%2C%20when%20documents%20can%20be%20categorized%2C%20we%20propose%20a%20classifier-free%0Aapproach%20to%20maximize%20topic%20heterogeneity%20across%20different%20document%20groups.%20We%0Aalso%20present%20an%20efficient%20variational%20optimization%20procedure%20adapted%20for%20the%0Amultistage%20longitudinal%20setting.%20In%20this%20case%20study%2C%20we%20apply%20our%20method%20to%20the%0Apsychiatric%20clinical%20notes%20from%20a%20large%20tertiary%20pediatric%20hospital%20in%20Southern%0ACalifornia%20and%20achieve%20a%2038%25%20increase%20in%20the%20overall%20coherence%20of%20extracted%0Atopics.%20Our%20real%20data%20analysis%20reveals%20that%20children%20tend%20to%20express%20more%0Anegative%20emotions%20during%20state%20shutdowns%20and%20more%20positive%20when%20schools%20reopen.%0AFurthermore%2C%20it%20suggests%20that%20sexual%20and%20gender%20minority%20%28SGM%29%20children%20display%0Amore%20pronounced%20reactions%20to%20major%20COVID-19%20events%20and%20a%20greater%20sensitivity%20to%0Avaccine-related%20news%20than%20non-SGM%20children.%20This%20study%20examines%20children%27s%0Amental%20health%20progression%20during%20the%20pandemic%20and%20offers%20clinicians%20valuable%0Ainsights%20to%20recognize%20disparities%20in%20children%27s%20mental%20health%20related%20to%20their%0Asexual%20and%20gender%20identities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14180v2&entry.124074799=Read"},
{"title": "Corrective Machine Unlearning", "author": "Shashwat Goel and Ameya Prabhu and Philip Torr and Ponnurangam Kumaraguru and Amartya Sanyal", "abstract": "  Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the Internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects including\nvulnerability to backdoored samples, systemic biases, and reduced accuracy on\ncertain input domains. Realistically, all manipulated training samples cannot\nbe identified, and only a small, representative subset of the affected data can\nbe flagged.\n  We formalize Corrective Machine Unlearning as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, only\nhaving identified a subset of the corrupted data. We demonstrate that the\nproblem of corrective unlearning has significantly different requirements from\ntraditional privacy-oriented unlearning. We find most existing unlearning\nmethods, including retraining-from-scratch without the deletion set, require\nmost of the manipulated data to be identified for effective corrective\nunlearning. However, one approach, Selective Synaptic Dampening, achieves\nlimited success, unlearning adverse effects with just a small portion of the\nmanipulated samples in our setting, which shows encouraging signs for future\nprogress. We hope our work spurs research towards developing better methods for\ncorrective unlearning and offers practitioners a new strategy to handle data\nintegrity challenges arising from web-scale training. Code is available at\nhttps://github.com/drimpossible/corrective-unlearning-bench.\n", "link": "http://arxiv.org/abs/2402.14015v2", "date": "2024-10-17", "relevancy": 1.821, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5092}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4465}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Corrective%20Machine%20Unlearning&body=Title%3A%20Corrective%20Machine%20Unlearning%0AAuthor%3A%20Shashwat%20Goel%20and%20Ameya%20Prabhu%20and%20Philip%20Torr%20and%20Ponnurangam%20Kumaraguru%20and%20Amartya%20Sanyal%0AAbstract%3A%20%20%20Machine%20Learning%20models%20increasingly%20face%20data%20integrity%20challenges%20due%20to%0Athe%20use%20of%20large-scale%20training%20datasets%20drawn%20from%20the%20Internet.%20We%20study%20what%0Amodel%20developers%20can%20do%20if%20they%20detect%20that%20some%20data%20was%20manipulated%20or%0Aincorrect.%20Such%20manipulated%20data%20can%20cause%20adverse%20effects%20including%0Avulnerability%20to%20backdoored%20samples%2C%20systemic%20biases%2C%20and%20reduced%20accuracy%20on%0Acertain%20input%20domains.%20Realistically%2C%20all%20manipulated%20training%20samples%20cannot%0Abe%20identified%2C%20and%20only%20a%20small%2C%20representative%20subset%20of%20the%20affected%20data%20can%0Abe%20flagged.%0A%20%20We%20formalize%20Corrective%20Machine%20Unlearning%20as%20the%20problem%20of%20mitigating%20the%0Aimpact%20of%20data%20affected%20by%20unknown%20manipulations%20on%20a%20trained%20model%2C%20only%0Ahaving%20identified%20a%20subset%20of%20the%20corrupted%20data.%20We%20demonstrate%20that%20the%0Aproblem%20of%20corrective%20unlearning%20has%20significantly%20different%20requirements%20from%0Atraditional%20privacy-oriented%20unlearning.%20We%20find%20most%20existing%20unlearning%0Amethods%2C%20including%20retraining-from-scratch%20without%20the%20deletion%20set%2C%20require%0Amost%20of%20the%20manipulated%20data%20to%20be%20identified%20for%20effective%20corrective%0Aunlearning.%20However%2C%20one%20approach%2C%20Selective%20Synaptic%20Dampening%2C%20achieves%0Alimited%20success%2C%20unlearning%20adverse%20effects%20with%20just%20a%20small%20portion%20of%20the%0Amanipulated%20samples%20in%20our%20setting%2C%20which%20shows%20encouraging%20signs%20for%20future%0Aprogress.%20We%20hope%20our%20work%20spurs%20research%20towards%20developing%20better%20methods%20for%0Acorrective%20unlearning%20and%20offers%20practitioners%20a%20new%20strategy%20to%20handle%20data%0Aintegrity%20challenges%20arising%20from%20web-scale%20training.%20Code%20is%20available%20at%0Ahttps%3A//github.com/drimpossible/corrective-unlearning-bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrective%2520Machine%2520Unlearning%26entry.906535625%3DShashwat%2520Goel%2520and%2520Ameya%2520Prabhu%2520and%2520Philip%2520Torr%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Amartya%2520Sanyal%26entry.1292438233%3D%2520%2520Machine%2520Learning%2520models%2520increasingly%2520face%2520data%2520integrity%2520challenges%2520due%2520to%250Athe%2520use%2520of%2520large-scale%2520training%2520datasets%2520drawn%2520from%2520the%2520Internet.%2520We%2520study%2520what%250Amodel%2520developers%2520can%2520do%2520if%2520they%2520detect%2520that%2520some%2520data%2520was%2520manipulated%2520or%250Aincorrect.%2520Such%2520manipulated%2520data%2520can%2520cause%2520adverse%2520effects%2520including%250Avulnerability%2520to%2520backdoored%2520samples%252C%2520systemic%2520biases%252C%2520and%2520reduced%2520accuracy%2520on%250Acertain%2520input%2520domains.%2520Realistically%252C%2520all%2520manipulated%2520training%2520samples%2520cannot%250Abe%2520identified%252C%2520and%2520only%2520a%2520small%252C%2520representative%2520subset%2520of%2520the%2520affected%2520data%2520can%250Abe%2520flagged.%250A%2520%2520We%2520formalize%2520Corrective%2520Machine%2520Unlearning%2520as%2520the%2520problem%2520of%2520mitigating%2520the%250Aimpact%2520of%2520data%2520affected%2520by%2520unknown%2520manipulations%2520on%2520a%2520trained%2520model%252C%2520only%250Ahaving%2520identified%2520a%2520subset%2520of%2520the%2520corrupted%2520data.%2520We%2520demonstrate%2520that%2520the%250Aproblem%2520of%2520corrective%2520unlearning%2520has%2520significantly%2520different%2520requirements%2520from%250Atraditional%2520privacy-oriented%2520unlearning.%2520We%2520find%2520most%2520existing%2520unlearning%250Amethods%252C%2520including%2520retraining-from-scratch%2520without%2520the%2520deletion%2520set%252C%2520require%250Amost%2520of%2520the%2520manipulated%2520data%2520to%2520be%2520identified%2520for%2520effective%2520corrective%250Aunlearning.%2520However%252C%2520one%2520approach%252C%2520Selective%2520Synaptic%2520Dampening%252C%2520achieves%250Alimited%2520success%252C%2520unlearning%2520adverse%2520effects%2520with%2520just%2520a%2520small%2520portion%2520of%2520the%250Amanipulated%2520samples%2520in%2520our%2520setting%252C%2520which%2520shows%2520encouraging%2520signs%2520for%2520future%250Aprogress.%2520We%2520hope%2520our%2520work%2520spurs%2520research%2520towards%2520developing%2520better%2520methods%2520for%250Acorrective%2520unlearning%2520and%2520offers%2520practitioners%2520a%2520new%2520strategy%2520to%2520handle%2520data%250Aintegrity%2520challenges%2520arising%2520from%2520web-scale%2520training.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/drimpossible/corrective-unlearning-bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Corrective%20Machine%20Unlearning&entry.906535625=Shashwat%20Goel%20and%20Ameya%20Prabhu%20and%20Philip%20Torr%20and%20Ponnurangam%20Kumaraguru%20and%20Amartya%20Sanyal&entry.1292438233=%20%20Machine%20Learning%20models%20increasingly%20face%20data%20integrity%20challenges%20due%20to%0Athe%20use%20of%20large-scale%20training%20datasets%20drawn%20from%20the%20Internet.%20We%20study%20what%0Amodel%20developers%20can%20do%20if%20they%20detect%20that%20some%20data%20was%20manipulated%20or%0Aincorrect.%20Such%20manipulated%20data%20can%20cause%20adverse%20effects%20including%0Avulnerability%20to%20backdoored%20samples%2C%20systemic%20biases%2C%20and%20reduced%20accuracy%20on%0Acertain%20input%20domains.%20Realistically%2C%20all%20manipulated%20training%20samples%20cannot%0Abe%20identified%2C%20and%20only%20a%20small%2C%20representative%20subset%20of%20the%20affected%20data%20can%0Abe%20flagged.%0A%20%20We%20formalize%20Corrective%20Machine%20Unlearning%20as%20the%20problem%20of%20mitigating%20the%0Aimpact%20of%20data%20affected%20by%20unknown%20manipulations%20on%20a%20trained%20model%2C%20only%0Ahaving%20identified%20a%20subset%20of%20the%20corrupted%20data.%20We%20demonstrate%20that%20the%0Aproblem%20of%20corrective%20unlearning%20has%20significantly%20different%20requirements%20from%0Atraditional%20privacy-oriented%20unlearning.%20We%20find%20most%20existing%20unlearning%0Amethods%2C%20including%20retraining-from-scratch%20without%20the%20deletion%20set%2C%20require%0Amost%20of%20the%20manipulated%20data%20to%20be%20identified%20for%20effective%20corrective%0Aunlearning.%20However%2C%20one%20approach%2C%20Selective%20Synaptic%20Dampening%2C%20achieves%0Alimited%20success%2C%20unlearning%20adverse%20effects%20with%20just%20a%20small%20portion%20of%20the%0Amanipulated%20samples%20in%20our%20setting%2C%20which%20shows%20encouraging%20signs%20for%20future%0Aprogress.%20We%20hope%20our%20work%20spurs%20research%20towards%20developing%20better%20methods%20for%0Acorrective%20unlearning%20and%20offers%20practitioners%20a%20new%20strategy%20to%20handle%20data%0Aintegrity%20challenges%20arising%20from%20web-scale%20training.%20Code%20is%20available%20at%0Ahttps%3A//github.com/drimpossible/corrective-unlearning-bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14015v2&entry.124074799=Read"},
{"title": "Ab initio nonparametric variable selection for scalable Symbolic\n  Regression with large $p$", "author": "Shengbin Ye and Meng Li", "abstract": "  Symbolic regression (SR) is a powerful technique for discovering symbolic\nexpressions that characterize nonlinear relationships in data, gaining\nincreasing attention for its interpretability, compactness, and robustness.\nHowever, existing SR methods do not scale to datasets with a large number of\ninput variables (referred to as extreme-scale SR), which are common in modern\nscientific applications. This ``large $p$'' setting, often accompanied by\nmeasurement error, leads to slow performance of SR methods and overly complex\nexpressions that are difficult to interpret. To address this scalability\nchallenge, we propose a method called PAN+SR, which combines a key idea of ab\ninitio nonparametric variable selection with SR to efficiently pre-screen large\ninput spaces and reduce search complexity while maintaining accuracy. The use\nof nonparametric methods eliminates model misspecification, supporting a\nstrategy called parametric-assisted nonparametric (PAN). We also extend\nSRBench, an open-source benchmarking platform, by incorporating\nhigh-dimensional regression problems with various signal-to-noise ratios. Our\nresults demonstrate that PAN+SR consistently enhances the performance of 17\ncontemporary SR methods, enabling several to achieve state-of-the-art\nperformance on these challenging datasets.\n", "link": "http://arxiv.org/abs/2410.13681v1", "date": "2024-10-17", "relevancy": 1.8208, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4638}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4632}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ab%20initio%20nonparametric%20variable%20selection%20for%20scalable%20Symbolic%0A%20%20Regression%20with%20large%20%24p%24&body=Title%3A%20Ab%20initio%20nonparametric%20variable%20selection%20for%20scalable%20Symbolic%0A%20%20Regression%20with%20large%20%24p%24%0AAuthor%3A%20Shengbin%20Ye%20and%20Meng%20Li%0AAbstract%3A%20%20%20Symbolic%20regression%20%28SR%29%20is%20a%20powerful%20technique%20for%20discovering%20symbolic%0Aexpressions%20that%20characterize%20nonlinear%20relationships%20in%20data%2C%20gaining%0Aincreasing%20attention%20for%20its%20interpretability%2C%20compactness%2C%20and%20robustness.%0AHowever%2C%20existing%20SR%20methods%20do%20not%20scale%20to%20datasets%20with%20a%20large%20number%20of%0Ainput%20variables%20%28referred%20to%20as%20extreme-scale%20SR%29%2C%20which%20are%20common%20in%20modern%0Ascientific%20applications.%20This%20%60%60large%20%24p%24%27%27%20setting%2C%20often%20accompanied%20by%0Ameasurement%20error%2C%20leads%20to%20slow%20performance%20of%20SR%20methods%20and%20overly%20complex%0Aexpressions%20that%20are%20difficult%20to%20interpret.%20To%20address%20this%20scalability%0Achallenge%2C%20we%20propose%20a%20method%20called%20PAN%2BSR%2C%20which%20combines%20a%20key%20idea%20of%20ab%0Ainitio%20nonparametric%20variable%20selection%20with%20SR%20to%20efficiently%20pre-screen%20large%0Ainput%20spaces%20and%20reduce%20search%20complexity%20while%20maintaining%20accuracy.%20The%20use%0Aof%20nonparametric%20methods%20eliminates%20model%20misspecification%2C%20supporting%20a%0Astrategy%20called%20parametric-assisted%20nonparametric%20%28PAN%29.%20We%20also%20extend%0ASRBench%2C%20an%20open-source%20benchmarking%20platform%2C%20by%20incorporating%0Ahigh-dimensional%20regression%20problems%20with%20various%20signal-to-noise%20ratios.%20Our%0Aresults%20demonstrate%20that%20PAN%2BSR%20consistently%20enhances%20the%20performance%20of%2017%0Acontemporary%20SR%20methods%2C%20enabling%20several%20to%20achieve%20state-of-the-art%0Aperformance%20on%20these%20challenging%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAb%2520initio%2520nonparametric%2520variable%2520selection%2520for%2520scalable%2520Symbolic%250A%2520%2520Regression%2520with%2520large%2520%2524p%2524%26entry.906535625%3DShengbin%2520Ye%2520and%2520Meng%2520Li%26entry.1292438233%3D%2520%2520Symbolic%2520regression%2520%2528SR%2529%2520is%2520a%2520powerful%2520technique%2520for%2520discovering%2520symbolic%250Aexpressions%2520that%2520characterize%2520nonlinear%2520relationships%2520in%2520data%252C%2520gaining%250Aincreasing%2520attention%2520for%2520its%2520interpretability%252C%2520compactness%252C%2520and%2520robustness.%250AHowever%252C%2520existing%2520SR%2520methods%2520do%2520not%2520scale%2520to%2520datasets%2520with%2520a%2520large%2520number%2520of%250Ainput%2520variables%2520%2528referred%2520to%2520as%2520extreme-scale%2520SR%2529%252C%2520which%2520are%2520common%2520in%2520modern%250Ascientific%2520applications.%2520This%2520%2560%2560large%2520%2524p%2524%2527%2527%2520setting%252C%2520often%2520accompanied%2520by%250Ameasurement%2520error%252C%2520leads%2520to%2520slow%2520performance%2520of%2520SR%2520methods%2520and%2520overly%2520complex%250Aexpressions%2520that%2520are%2520difficult%2520to%2520interpret.%2520To%2520address%2520this%2520scalability%250Achallenge%252C%2520we%2520propose%2520a%2520method%2520called%2520PAN%252BSR%252C%2520which%2520combines%2520a%2520key%2520idea%2520of%2520ab%250Ainitio%2520nonparametric%2520variable%2520selection%2520with%2520SR%2520to%2520efficiently%2520pre-screen%2520large%250Ainput%2520spaces%2520and%2520reduce%2520search%2520complexity%2520while%2520maintaining%2520accuracy.%2520The%2520use%250Aof%2520nonparametric%2520methods%2520eliminates%2520model%2520misspecification%252C%2520supporting%2520a%250Astrategy%2520called%2520parametric-assisted%2520nonparametric%2520%2528PAN%2529.%2520We%2520also%2520extend%250ASRBench%252C%2520an%2520open-source%2520benchmarking%2520platform%252C%2520by%2520incorporating%250Ahigh-dimensional%2520regression%2520problems%2520with%2520various%2520signal-to-noise%2520ratios.%2520Our%250Aresults%2520demonstrate%2520that%2520PAN%252BSR%2520consistently%2520enhances%2520the%2520performance%2520of%252017%250Acontemporary%2520SR%2520methods%252C%2520enabling%2520several%2520to%2520achieve%2520state-of-the-art%250Aperformance%2520on%2520these%2520challenging%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ab%20initio%20nonparametric%20variable%20selection%20for%20scalable%20Symbolic%0A%20%20Regression%20with%20large%20%24p%24&entry.906535625=Shengbin%20Ye%20and%20Meng%20Li&entry.1292438233=%20%20Symbolic%20regression%20%28SR%29%20is%20a%20powerful%20technique%20for%20discovering%20symbolic%0Aexpressions%20that%20characterize%20nonlinear%20relationships%20in%20data%2C%20gaining%0Aincreasing%20attention%20for%20its%20interpretability%2C%20compactness%2C%20and%20robustness.%0AHowever%2C%20existing%20SR%20methods%20do%20not%20scale%20to%20datasets%20with%20a%20large%20number%20of%0Ainput%20variables%20%28referred%20to%20as%20extreme-scale%20SR%29%2C%20which%20are%20common%20in%20modern%0Ascientific%20applications.%20This%20%60%60large%20%24p%24%27%27%20setting%2C%20often%20accompanied%20by%0Ameasurement%20error%2C%20leads%20to%20slow%20performance%20of%20SR%20methods%20and%20overly%20complex%0Aexpressions%20that%20are%20difficult%20to%20interpret.%20To%20address%20this%20scalability%0Achallenge%2C%20we%20propose%20a%20method%20called%20PAN%2BSR%2C%20which%20combines%20a%20key%20idea%20of%20ab%0Ainitio%20nonparametric%20variable%20selection%20with%20SR%20to%20efficiently%20pre-screen%20large%0Ainput%20spaces%20and%20reduce%20search%20complexity%20while%20maintaining%20accuracy.%20The%20use%0Aof%20nonparametric%20methods%20eliminates%20model%20misspecification%2C%20supporting%20a%0Astrategy%20called%20parametric-assisted%20nonparametric%20%28PAN%29.%20We%20also%20extend%0ASRBench%2C%20an%20open-source%20benchmarking%20platform%2C%20by%20incorporating%0Ahigh-dimensional%20regression%20problems%20with%20various%20signal-to-noise%20ratios.%20Our%0Aresults%20demonstrate%20that%20PAN%2BSR%20consistently%20enhances%20the%20performance%20of%2017%0Acontemporary%20SR%20methods%2C%20enabling%20several%20to%20achieve%20state-of-the-art%0Aperformance%20on%20these%20challenging%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13681v1&entry.124074799=Read"},
{"title": "Enhancing Retail Sales Forecasting with Optimized Machine Learning\n  Models", "author": "Priyam Ganguly and Isha Mukherjee", "abstract": "  In retail sales forecasting, accurately predicting future sales is crucial\nfor inventory management and strategic planning. Traditional methods like LR\noften fall short due to the complexity of sales data, which includes\nseasonality and numerous product families. Recent advancements in machine\nlearning (ML) provide more robust alternatives. This research benefits from the\npower of ML, particularly Random Forest (RF), Gradient Boosting (GB), Support\nVector Regression (SVR), and XGBoost, to improve prediction accuracy. Despite\nadvancements, a significant gap exists in handling complex datasets with high\nseasonality and multiple product families. The proposed solution involves\nimplementing and optimizing a RF model, leveraging hyperparameter tuning\nthrough randomized search cross-validation. This approach addresses the\ncomplexities of the dataset, capturing intricate patterns that traditional\nmethods miss. The optimized RF model achieved an R-squared value of 0.945,\nsubstantially higher than the initial RF model and traditional LR, which had an\nR-squared of 0.531. The model reduced the root mean squared logarithmic error\n(RMSLE) to 1.172, demonstrating its superior predictive capability. The\noptimized RF model did better than cutting-edge models like Gradient Boosting\n(R-squared: 0.942), SVR (R-squared: 0.940), and XGBoost (R-squared: 0.939),\nwith more minor mean squared error (MSE) and mean absolute error (MAE) numbers.\nThe results demonstrate that the optimized RF model excels in forecasting\nretail sales, handling the datasets complexity with higher accuracy and\nreliability. This research highlights the importance of advanced ML techniques\nin predictive analytics, offering a significant improvement over traditional\nmethods and other contemporary models.\n", "link": "http://arxiv.org/abs/2410.13773v1", "date": "2024-10-17", "relevancy": 1.8025, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4507}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Retail%20Sales%20Forecasting%20with%20Optimized%20Machine%20Learning%0A%20%20Models&body=Title%3A%20Enhancing%20Retail%20Sales%20Forecasting%20with%20Optimized%20Machine%20Learning%0A%20%20Models%0AAuthor%3A%20Priyam%20Ganguly%20and%20Isha%20Mukherjee%0AAbstract%3A%20%20%20In%20retail%20sales%20forecasting%2C%20accurately%20predicting%20future%20sales%20is%20crucial%0Afor%20inventory%20management%20and%20strategic%20planning.%20Traditional%20methods%20like%20LR%0Aoften%20fall%20short%20due%20to%20the%20complexity%20of%20sales%20data%2C%20which%20includes%0Aseasonality%20and%20numerous%20product%20families.%20Recent%20advancements%20in%20machine%0Alearning%20%28ML%29%20provide%20more%20robust%20alternatives.%20This%20research%20benefits%20from%20the%0Apower%20of%20ML%2C%20particularly%20Random%20Forest%20%28RF%29%2C%20Gradient%20Boosting%20%28GB%29%2C%20Support%0AVector%20Regression%20%28SVR%29%2C%20and%20XGBoost%2C%20to%20improve%20prediction%20accuracy.%20Despite%0Aadvancements%2C%20a%20significant%20gap%20exists%20in%20handling%20complex%20datasets%20with%20high%0Aseasonality%20and%20multiple%20product%20families.%20The%20proposed%20solution%20involves%0Aimplementing%20and%20optimizing%20a%20RF%20model%2C%20leveraging%20hyperparameter%20tuning%0Athrough%20randomized%20search%20cross-validation.%20This%20approach%20addresses%20the%0Acomplexities%20of%20the%20dataset%2C%20capturing%20intricate%20patterns%20that%20traditional%0Amethods%20miss.%20The%20optimized%20RF%20model%20achieved%20an%20R-squared%20value%20of%200.945%2C%0Asubstantially%20higher%20than%20the%20initial%20RF%20model%20and%20traditional%20LR%2C%20which%20had%20an%0AR-squared%20of%200.531.%20The%20model%20reduced%20the%20root%20mean%20squared%20logarithmic%20error%0A%28RMSLE%29%20to%201.172%2C%20demonstrating%20its%20superior%20predictive%20capability.%20The%0Aoptimized%20RF%20model%20did%20better%20than%20cutting-edge%20models%20like%20Gradient%20Boosting%0A%28R-squared%3A%200.942%29%2C%20SVR%20%28R-squared%3A%200.940%29%2C%20and%20XGBoost%20%28R-squared%3A%200.939%29%2C%0Awith%20more%20minor%20mean%20squared%20error%20%28MSE%29%20and%20mean%20absolute%20error%20%28MAE%29%20numbers.%0AThe%20results%20demonstrate%20that%20the%20optimized%20RF%20model%20excels%20in%20forecasting%0Aretail%20sales%2C%20handling%20the%20datasets%20complexity%20with%20higher%20accuracy%20and%0Areliability.%20This%20research%20highlights%20the%20importance%20of%20advanced%20ML%20techniques%0Ain%20predictive%20analytics%2C%20offering%20a%20significant%20improvement%20over%20traditional%0Amethods%20and%20other%20contemporary%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Retail%2520Sales%2520Forecasting%2520with%2520Optimized%2520Machine%2520Learning%250A%2520%2520Models%26entry.906535625%3DPriyam%2520Ganguly%2520and%2520Isha%2520Mukherjee%26entry.1292438233%3D%2520%2520In%2520retail%2520sales%2520forecasting%252C%2520accurately%2520predicting%2520future%2520sales%2520is%2520crucial%250Afor%2520inventory%2520management%2520and%2520strategic%2520planning.%2520Traditional%2520methods%2520like%2520LR%250Aoften%2520fall%2520short%2520due%2520to%2520the%2520complexity%2520of%2520sales%2520data%252C%2520which%2520includes%250Aseasonality%2520and%2520numerous%2520product%2520families.%2520Recent%2520advancements%2520in%2520machine%250Alearning%2520%2528ML%2529%2520provide%2520more%2520robust%2520alternatives.%2520This%2520research%2520benefits%2520from%2520the%250Apower%2520of%2520ML%252C%2520particularly%2520Random%2520Forest%2520%2528RF%2529%252C%2520Gradient%2520Boosting%2520%2528GB%2529%252C%2520Support%250AVector%2520Regression%2520%2528SVR%2529%252C%2520and%2520XGBoost%252C%2520to%2520improve%2520prediction%2520accuracy.%2520Despite%250Aadvancements%252C%2520a%2520significant%2520gap%2520exists%2520in%2520handling%2520complex%2520datasets%2520with%2520high%250Aseasonality%2520and%2520multiple%2520product%2520families.%2520The%2520proposed%2520solution%2520involves%250Aimplementing%2520and%2520optimizing%2520a%2520RF%2520model%252C%2520leveraging%2520hyperparameter%2520tuning%250Athrough%2520randomized%2520search%2520cross-validation.%2520This%2520approach%2520addresses%2520the%250Acomplexities%2520of%2520the%2520dataset%252C%2520capturing%2520intricate%2520patterns%2520that%2520traditional%250Amethods%2520miss.%2520The%2520optimized%2520RF%2520model%2520achieved%2520an%2520R-squared%2520value%2520of%25200.945%252C%250Asubstantially%2520higher%2520than%2520the%2520initial%2520RF%2520model%2520and%2520traditional%2520LR%252C%2520which%2520had%2520an%250AR-squared%2520of%25200.531.%2520The%2520model%2520reduced%2520the%2520root%2520mean%2520squared%2520logarithmic%2520error%250A%2528RMSLE%2529%2520to%25201.172%252C%2520demonstrating%2520its%2520superior%2520predictive%2520capability.%2520The%250Aoptimized%2520RF%2520model%2520did%2520better%2520than%2520cutting-edge%2520models%2520like%2520Gradient%2520Boosting%250A%2528R-squared%253A%25200.942%2529%252C%2520SVR%2520%2528R-squared%253A%25200.940%2529%252C%2520and%2520XGBoost%2520%2528R-squared%253A%25200.939%2529%252C%250Awith%2520more%2520minor%2520mean%2520squared%2520error%2520%2528MSE%2529%2520and%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520numbers.%250AThe%2520results%2520demonstrate%2520that%2520the%2520optimized%2520RF%2520model%2520excels%2520in%2520forecasting%250Aretail%2520sales%252C%2520handling%2520the%2520datasets%2520complexity%2520with%2520higher%2520accuracy%2520and%250Areliability.%2520This%2520research%2520highlights%2520the%2520importance%2520of%2520advanced%2520ML%2520techniques%250Ain%2520predictive%2520analytics%252C%2520offering%2520a%2520significant%2520improvement%2520over%2520traditional%250Amethods%2520and%2520other%2520contemporary%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Retail%20Sales%20Forecasting%20with%20Optimized%20Machine%20Learning%0A%20%20Models&entry.906535625=Priyam%20Ganguly%20and%20Isha%20Mukherjee&entry.1292438233=%20%20In%20retail%20sales%20forecasting%2C%20accurately%20predicting%20future%20sales%20is%20crucial%0Afor%20inventory%20management%20and%20strategic%20planning.%20Traditional%20methods%20like%20LR%0Aoften%20fall%20short%20due%20to%20the%20complexity%20of%20sales%20data%2C%20which%20includes%0Aseasonality%20and%20numerous%20product%20families.%20Recent%20advancements%20in%20machine%0Alearning%20%28ML%29%20provide%20more%20robust%20alternatives.%20This%20research%20benefits%20from%20the%0Apower%20of%20ML%2C%20particularly%20Random%20Forest%20%28RF%29%2C%20Gradient%20Boosting%20%28GB%29%2C%20Support%0AVector%20Regression%20%28SVR%29%2C%20and%20XGBoost%2C%20to%20improve%20prediction%20accuracy.%20Despite%0Aadvancements%2C%20a%20significant%20gap%20exists%20in%20handling%20complex%20datasets%20with%20high%0Aseasonality%20and%20multiple%20product%20families.%20The%20proposed%20solution%20involves%0Aimplementing%20and%20optimizing%20a%20RF%20model%2C%20leveraging%20hyperparameter%20tuning%0Athrough%20randomized%20search%20cross-validation.%20This%20approach%20addresses%20the%0Acomplexities%20of%20the%20dataset%2C%20capturing%20intricate%20patterns%20that%20traditional%0Amethods%20miss.%20The%20optimized%20RF%20model%20achieved%20an%20R-squared%20value%20of%200.945%2C%0Asubstantially%20higher%20than%20the%20initial%20RF%20model%20and%20traditional%20LR%2C%20which%20had%20an%0AR-squared%20of%200.531.%20The%20model%20reduced%20the%20root%20mean%20squared%20logarithmic%20error%0A%28RMSLE%29%20to%201.172%2C%20demonstrating%20its%20superior%20predictive%20capability.%20The%0Aoptimized%20RF%20model%20did%20better%20than%20cutting-edge%20models%20like%20Gradient%20Boosting%0A%28R-squared%3A%200.942%29%2C%20SVR%20%28R-squared%3A%200.940%29%2C%20and%20XGBoost%20%28R-squared%3A%200.939%29%2C%0Awith%20more%20minor%20mean%20squared%20error%20%28MSE%29%20and%20mean%20absolute%20error%20%28MAE%29%20numbers.%0AThe%20results%20demonstrate%20that%20the%20optimized%20RF%20model%20excels%20in%20forecasting%0Aretail%20sales%2C%20handling%20the%20datasets%20complexity%20with%20higher%20accuracy%20and%0Areliability.%20This%20research%20highlights%20the%20importance%20of%20advanced%20ML%20techniques%0Ain%20predictive%20analytics%2C%20offering%20a%20significant%20improvement%20over%20traditional%0Amethods%20and%20other%20contemporary%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13773v1&entry.124074799=Read"},
{"title": "Moments of Clarity: Streamlining Latent Spaces in Machine Learning using\n  Moment Pooling", "author": "Rikab Gambhir and Athis Osathapan and Jesse Thaler", "abstract": "  Many machine learning applications involve learning a latent representation\nof data, which is often high-dimensional and difficult to directly interpret.\nIn this work, we propose \"Moment Pooling\", a natural extension of Deep Sets\nnetworks which drastically decrease latent space dimensionality of these\nnetworks while maintaining or even improving performance. Moment Pooling\ngeneralizes the summation in Deep Sets to arbitrary multivariate moments, which\nenables the model to achieve a much higher effective latent dimensionality for\na fixed latent dimension. We demonstrate Moment Pooling on the collider physics\ntask of quark/gluon jet classification by extending Energy Flow Networks (EFNs)\nto Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1\nperform similarly to ordinary EFNs with higher latent dimension. This small\nlatent dimension allows for the internal representation to be directly\nvisualized and interpreted, which in turn enables the learned internal jet\nrepresentation to be extracted in closed form.\n", "link": "http://arxiv.org/abs/2403.08854v2", "date": "2024-10-17", "relevancy": 1.4525, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5187}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4765}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moments%20of%20Clarity%3A%20Streamlining%20Latent%20Spaces%20in%20Machine%20Learning%20using%0A%20%20Moment%20Pooling&body=Title%3A%20Moments%20of%20Clarity%3A%20Streamlining%20Latent%20Spaces%20in%20Machine%20Learning%20using%0A%20%20Moment%20Pooling%0AAuthor%3A%20Rikab%20Gambhir%20and%20Athis%20Osathapan%20and%20Jesse%20Thaler%0AAbstract%3A%20%20%20Many%20machine%20learning%20applications%20involve%20learning%20a%20latent%20representation%0Aof%20data%2C%20which%20is%20often%20high-dimensional%20and%20difficult%20to%20directly%20interpret.%0AIn%20this%20work%2C%20we%20propose%20%22Moment%20Pooling%22%2C%20a%20natural%20extension%20of%20Deep%20Sets%0Anetworks%20which%20drastically%20decrease%20latent%20space%20dimensionality%20of%20these%0Anetworks%20while%20maintaining%20or%20even%20improving%20performance.%20Moment%20Pooling%0Ageneralizes%20the%20summation%20in%20Deep%20Sets%20to%20arbitrary%20multivariate%20moments%2C%20which%0Aenables%20the%20model%20to%20achieve%20a%20much%20higher%20effective%20latent%20dimensionality%20for%0Aa%20fixed%20latent%20dimension.%20We%20demonstrate%20Moment%20Pooling%20on%20the%20collider%20physics%0Atask%20of%20quark/gluon%20jet%20classification%20by%20extending%20Energy%20Flow%20Networks%20%28EFNs%29%0Ato%20Moment%20EFNs.%20We%20find%20that%20Moment%20EFNs%20with%20latent%20dimensions%20as%20small%20as%201%0Aperform%20similarly%20to%20ordinary%20EFNs%20with%20higher%20latent%20dimension.%20This%20small%0Alatent%20dimension%20allows%20for%20the%20internal%20representation%20to%20be%20directly%0Avisualized%20and%20interpreted%2C%20which%20in%20turn%20enables%20the%20learned%20internal%20jet%0Arepresentation%20to%20be%20extracted%20in%20closed%20form.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoments%2520of%2520Clarity%253A%2520Streamlining%2520Latent%2520Spaces%2520in%2520Machine%2520Learning%2520using%250A%2520%2520Moment%2520Pooling%26entry.906535625%3DRikab%2520Gambhir%2520and%2520Athis%2520Osathapan%2520and%2520Jesse%2520Thaler%26entry.1292438233%3D%2520%2520Many%2520machine%2520learning%2520applications%2520involve%2520learning%2520a%2520latent%2520representation%250Aof%2520data%252C%2520which%2520is%2520often%2520high-dimensional%2520and%2520difficult%2520to%2520directly%2520interpret.%250AIn%2520this%2520work%252C%2520we%2520propose%2520%2522Moment%2520Pooling%2522%252C%2520a%2520natural%2520extension%2520of%2520Deep%2520Sets%250Anetworks%2520which%2520drastically%2520decrease%2520latent%2520space%2520dimensionality%2520of%2520these%250Anetworks%2520while%2520maintaining%2520or%2520even%2520improving%2520performance.%2520Moment%2520Pooling%250Ageneralizes%2520the%2520summation%2520in%2520Deep%2520Sets%2520to%2520arbitrary%2520multivariate%2520moments%252C%2520which%250Aenables%2520the%2520model%2520to%2520achieve%2520a%2520much%2520higher%2520effective%2520latent%2520dimensionality%2520for%250Aa%2520fixed%2520latent%2520dimension.%2520We%2520demonstrate%2520Moment%2520Pooling%2520on%2520the%2520collider%2520physics%250Atask%2520of%2520quark/gluon%2520jet%2520classification%2520by%2520extending%2520Energy%2520Flow%2520Networks%2520%2528EFNs%2529%250Ato%2520Moment%2520EFNs.%2520We%2520find%2520that%2520Moment%2520EFNs%2520with%2520latent%2520dimensions%2520as%2520small%2520as%25201%250Aperform%2520similarly%2520to%2520ordinary%2520EFNs%2520with%2520higher%2520latent%2520dimension.%2520This%2520small%250Alatent%2520dimension%2520allows%2520for%2520the%2520internal%2520representation%2520to%2520be%2520directly%250Avisualized%2520and%2520interpreted%252C%2520which%2520in%2520turn%2520enables%2520the%2520learned%2520internal%2520jet%250Arepresentation%2520to%2520be%2520extracted%2520in%2520closed%2520form.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moments%20of%20Clarity%3A%20Streamlining%20Latent%20Spaces%20in%20Machine%20Learning%20using%0A%20%20Moment%20Pooling&entry.906535625=Rikab%20Gambhir%20and%20Athis%20Osathapan%20and%20Jesse%20Thaler&entry.1292438233=%20%20Many%20machine%20learning%20applications%20involve%20learning%20a%20latent%20representation%0Aof%20data%2C%20which%20is%20often%20high-dimensional%20and%20difficult%20to%20directly%20interpret.%0AIn%20this%20work%2C%20we%20propose%20%22Moment%20Pooling%22%2C%20a%20natural%20extension%20of%20Deep%20Sets%0Anetworks%20which%20drastically%20decrease%20latent%20space%20dimensionality%20of%20these%0Anetworks%20while%20maintaining%20or%20even%20improving%20performance.%20Moment%20Pooling%0Ageneralizes%20the%20summation%20in%20Deep%20Sets%20to%20arbitrary%20multivariate%20moments%2C%20which%0Aenables%20the%20model%20to%20achieve%20a%20much%20higher%20effective%20latent%20dimensionality%20for%0Aa%20fixed%20latent%20dimension.%20We%20demonstrate%20Moment%20Pooling%20on%20the%20collider%20physics%0Atask%20of%20quark/gluon%20jet%20classification%20by%20extending%20Energy%20Flow%20Networks%20%28EFNs%29%0Ato%20Moment%20EFNs.%20We%20find%20that%20Moment%20EFNs%20with%20latent%20dimensions%20as%20small%20as%201%0Aperform%20similarly%20to%20ordinary%20EFNs%20with%20higher%20latent%20dimension.%20This%20small%0Alatent%20dimension%20allows%20for%20the%20internal%20representation%20to%20be%20directly%0Avisualized%20and%20interpreted%2C%20which%20in%20turn%20enables%20the%20learned%20internal%20jet%0Arepresentation%20to%20be%20extracted%20in%20closed%20form.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08854v2&entry.124074799=Read"},
{"title": "Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation", "author": "Jean-Pierre Sleiman and Mayank Mittal and Marco Hutter", "abstract": "  Reinforcement learning (RL) often necessitates a meticulous Markov Decision\nProcess (MDP) design tailored to each task. This work aims to address this\nchallenge by proposing a systematic approach to behavior synthesis and control\nfor multi-contact loco-manipulation tasks, such as navigating spring-loaded\ndoors and manipulating heavy dishwashers. We define a task-independent MDP to\ntrain RL policies using only a single demonstration per task generated from a\nmodel-based trajectory optimizer. Our approach incorporates an adaptive phase\ndynamics formulation to robustly track the demonstrations while accommodating\ndynamic uncertainties and external disturbances. We compare our method against\nprior motion imitation RL works and show that the learned policies achieve\nhigher success rates across all considered tasks. These policies learn recovery\nmaneuvers that are not present in the demonstration, such as re-grasping\nobjects during execution or dealing with slippages. Finally, we successfully\ntransfer the policies to a real robot, demonstrating the practical viability of\nour approach.\n", "link": "http://arxiv.org/abs/2410.13817v1", "date": "2024-10-17", "relevancy": 1.7351, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6216}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6107}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Reinforcement%20Learning%20for%20Robust%20Multi-Contact%20Loco-Manipulation&body=Title%3A%20Guided%20Reinforcement%20Learning%20for%20Robust%20Multi-Contact%20Loco-Manipulation%0AAuthor%3A%20Jean-Pierre%20Sleiman%20and%20Mayank%20Mittal%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20often%20necessitates%20a%20meticulous%20Markov%20Decision%0AProcess%20%28MDP%29%20design%20tailored%20to%20each%20task.%20This%20work%20aims%20to%20address%20this%0Achallenge%20by%20proposing%20a%20systematic%20approach%20to%20behavior%20synthesis%20and%20control%0Afor%20multi-contact%20loco-manipulation%20tasks%2C%20such%20as%20navigating%20spring-loaded%0Adoors%20and%20manipulating%20heavy%20dishwashers.%20We%20define%20a%20task-independent%20MDP%20to%0Atrain%20RL%20policies%20using%20only%20a%20single%20demonstration%20per%20task%20generated%20from%20a%0Amodel-based%20trajectory%20optimizer.%20Our%20approach%20incorporates%20an%20adaptive%20phase%0Adynamics%20formulation%20to%20robustly%20track%20the%20demonstrations%20while%20accommodating%0Adynamic%20uncertainties%20and%20external%20disturbances.%20We%20compare%20our%20method%20against%0Aprior%20motion%20imitation%20RL%20works%20and%20show%20that%20the%20learned%20policies%20achieve%0Ahigher%20success%20rates%20across%20all%20considered%20tasks.%20These%20policies%20learn%20recovery%0Amaneuvers%20that%20are%20not%20present%20in%20the%20demonstration%2C%20such%20as%20re-grasping%0Aobjects%20during%20execution%20or%20dealing%20with%20slippages.%20Finally%2C%20we%20successfully%0Atransfer%20the%20policies%20to%20a%20real%20robot%2C%20demonstrating%20the%20practical%20viability%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Reinforcement%2520Learning%2520for%2520Robust%2520Multi-Contact%2520Loco-Manipulation%26entry.906535625%3DJean-Pierre%2520Sleiman%2520and%2520Mayank%2520Mittal%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520often%2520necessitates%2520a%2520meticulous%2520Markov%2520Decision%250AProcess%2520%2528MDP%2529%2520design%2520tailored%2520to%2520each%2520task.%2520This%2520work%2520aims%2520to%2520address%2520this%250Achallenge%2520by%2520proposing%2520a%2520systematic%2520approach%2520to%2520behavior%2520synthesis%2520and%2520control%250Afor%2520multi-contact%2520loco-manipulation%2520tasks%252C%2520such%2520as%2520navigating%2520spring-loaded%250Adoors%2520and%2520manipulating%2520heavy%2520dishwashers.%2520We%2520define%2520a%2520task-independent%2520MDP%2520to%250Atrain%2520RL%2520policies%2520using%2520only%2520a%2520single%2520demonstration%2520per%2520task%2520generated%2520from%2520a%250Amodel-based%2520trajectory%2520optimizer.%2520Our%2520approach%2520incorporates%2520an%2520adaptive%2520phase%250Adynamics%2520formulation%2520to%2520robustly%2520track%2520the%2520demonstrations%2520while%2520accommodating%250Adynamic%2520uncertainties%2520and%2520external%2520disturbances.%2520We%2520compare%2520our%2520method%2520against%250Aprior%2520motion%2520imitation%2520RL%2520works%2520and%2520show%2520that%2520the%2520learned%2520policies%2520achieve%250Ahigher%2520success%2520rates%2520across%2520all%2520considered%2520tasks.%2520These%2520policies%2520learn%2520recovery%250Amaneuvers%2520that%2520are%2520not%2520present%2520in%2520the%2520demonstration%252C%2520such%2520as%2520re-grasping%250Aobjects%2520during%2520execution%2520or%2520dealing%2520with%2520slippages.%2520Finally%252C%2520we%2520successfully%250Atransfer%2520the%2520policies%2520to%2520a%2520real%2520robot%252C%2520demonstrating%2520the%2520practical%2520viability%2520of%250Aour%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Reinforcement%20Learning%20for%20Robust%20Multi-Contact%20Loco-Manipulation&entry.906535625=Jean-Pierre%20Sleiman%20and%20Mayank%20Mittal%20and%20Marco%20Hutter&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20often%20necessitates%20a%20meticulous%20Markov%20Decision%0AProcess%20%28MDP%29%20design%20tailored%20to%20each%20task.%20This%20work%20aims%20to%20address%20this%0Achallenge%20by%20proposing%20a%20systematic%20approach%20to%20behavior%20synthesis%20and%20control%0Afor%20multi-contact%20loco-manipulation%20tasks%2C%20such%20as%20navigating%20spring-loaded%0Adoors%20and%20manipulating%20heavy%20dishwashers.%20We%20define%20a%20task-independent%20MDP%20to%0Atrain%20RL%20policies%20using%20only%20a%20single%20demonstration%20per%20task%20generated%20from%20a%0Amodel-based%20trajectory%20optimizer.%20Our%20approach%20incorporates%20an%20adaptive%20phase%0Adynamics%20formulation%20to%20robustly%20track%20the%20demonstrations%20while%20accommodating%0Adynamic%20uncertainties%20and%20external%20disturbances.%20We%20compare%20our%20method%20against%0Aprior%20motion%20imitation%20RL%20works%20and%20show%20that%20the%20learned%20policies%20achieve%0Ahigher%20success%20rates%20across%20all%20considered%20tasks.%20These%20policies%20learn%20recovery%0Amaneuvers%20that%20are%20not%20present%20in%20the%20demonstration%2C%20such%20as%20re-grasping%0Aobjects%20during%20execution%20or%20dealing%20with%20slippages.%20Finally%2C%20we%20successfully%0Atransfer%20the%20policies%20to%20a%20real%20robot%2C%20demonstrating%20the%20practical%20viability%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13817v1&entry.124074799=Read"},
{"title": "Probing the Latent Hierarchical Structure of Data via Diffusion Models", "author": "Antonio Sclocchi and Alessandro Favero and Noam Itzhak Levi and Matthieu Wyart", "abstract": "  High-dimensional data must be highly structured to be learnable. Although the\ncompositional and hierarchical nature of data is often put forward to explain\nlearnability, quantitative measurements establishing these properties are\nscarce. Likewise, accessing the latent variables underlying such a data\nstructure remains a challenge. In this work, we show that forward-backward\nexperiments in diffusion-based models, where data is noised and then denoised\nto generate new samples, are a promising tool to probe the latent structure of\ndata. We predict in simple hierarchical models that, in this process, changes\nin data occur by correlated chunks, with a length scale that diverges at a\nnoise level where a phase transition is known to take place. Remarkably, we\nconfirm this prediction in both text and image datasets using state-of-the-art\ndiffusion models. Our results show how latent variable changes manifest in the\ndata and establish how to measure these effects in real data using diffusion\nmodels.\n", "link": "http://arxiv.org/abs/2410.13770v1", "date": "2024-10-17", "relevancy": 1.1489, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5952}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5751}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20the%20Latent%20Hierarchical%20Structure%20of%20Data%20via%20Diffusion%20Models&body=Title%3A%20Probing%20the%20Latent%20Hierarchical%20Structure%20of%20Data%20via%20Diffusion%20Models%0AAuthor%3A%20Antonio%20Sclocchi%20and%20Alessandro%20Favero%20and%20Noam%20Itzhak%20Levi%20and%20Matthieu%20Wyart%0AAbstract%3A%20%20%20High-dimensional%20data%20must%20be%20highly%20structured%20to%20be%20learnable.%20Although%20the%0Acompositional%20and%20hierarchical%20nature%20of%20data%20is%20often%20put%20forward%20to%20explain%0Alearnability%2C%20quantitative%20measurements%20establishing%20these%20properties%20are%0Ascarce.%20Likewise%2C%20accessing%20the%20latent%20variables%20underlying%20such%20a%20data%0Astructure%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20show%20that%20forward-backward%0Aexperiments%20in%20diffusion-based%20models%2C%20where%20data%20is%20noised%20and%20then%20denoised%0Ato%20generate%20new%20samples%2C%20are%20a%20promising%20tool%20to%20probe%20the%20latent%20structure%20of%0Adata.%20We%20predict%20in%20simple%20hierarchical%20models%20that%2C%20in%20this%20process%2C%20changes%0Ain%20data%20occur%20by%20correlated%20chunks%2C%20with%20a%20length%20scale%20that%20diverges%20at%20a%0Anoise%20level%20where%20a%20phase%20transition%20is%20known%20to%20take%20place.%20Remarkably%2C%20we%0Aconfirm%20this%20prediction%20in%20both%20text%20and%20image%20datasets%20using%20state-of-the-art%0Adiffusion%20models.%20Our%20results%20show%20how%20latent%20variable%20changes%20manifest%20in%20the%0Adata%20and%20establish%20how%20to%20measure%20these%20effects%20in%20real%20data%20using%20diffusion%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520the%2520Latent%2520Hierarchical%2520Structure%2520of%2520Data%2520via%2520Diffusion%2520Models%26entry.906535625%3DAntonio%2520Sclocchi%2520and%2520Alessandro%2520Favero%2520and%2520Noam%2520Itzhak%2520Levi%2520and%2520Matthieu%2520Wyart%26entry.1292438233%3D%2520%2520High-dimensional%2520data%2520must%2520be%2520highly%2520structured%2520to%2520be%2520learnable.%2520Although%2520the%250Acompositional%2520and%2520hierarchical%2520nature%2520of%2520data%2520is%2520often%2520put%2520forward%2520to%2520explain%250Alearnability%252C%2520quantitative%2520measurements%2520establishing%2520these%2520properties%2520are%250Ascarce.%2520Likewise%252C%2520accessing%2520the%2520latent%2520variables%2520underlying%2520such%2520a%2520data%250Astructure%2520remains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520forward-backward%250Aexperiments%2520in%2520diffusion-based%2520models%252C%2520where%2520data%2520is%2520noised%2520and%2520then%2520denoised%250Ato%2520generate%2520new%2520samples%252C%2520are%2520a%2520promising%2520tool%2520to%2520probe%2520the%2520latent%2520structure%2520of%250Adata.%2520We%2520predict%2520in%2520simple%2520hierarchical%2520models%2520that%252C%2520in%2520this%2520process%252C%2520changes%250Ain%2520data%2520occur%2520by%2520correlated%2520chunks%252C%2520with%2520a%2520length%2520scale%2520that%2520diverges%2520at%2520a%250Anoise%2520level%2520where%2520a%2520phase%2520transition%2520is%2520known%2520to%2520take%2520place.%2520Remarkably%252C%2520we%250Aconfirm%2520this%2520prediction%2520in%2520both%2520text%2520and%2520image%2520datasets%2520using%2520state-of-the-art%250Adiffusion%2520models.%2520Our%2520results%2520show%2520how%2520latent%2520variable%2520changes%2520manifest%2520in%2520the%250Adata%2520and%2520establish%2520how%2520to%2520measure%2520these%2520effects%2520in%2520real%2520data%2520using%2520diffusion%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%20Latent%20Hierarchical%20Structure%20of%20Data%20via%20Diffusion%20Models&entry.906535625=Antonio%20Sclocchi%20and%20Alessandro%20Favero%20and%20Noam%20Itzhak%20Levi%20and%20Matthieu%20Wyart&entry.1292438233=%20%20High-dimensional%20data%20must%20be%20highly%20structured%20to%20be%20learnable.%20Although%20the%0Acompositional%20and%20hierarchical%20nature%20of%20data%20is%20often%20put%20forward%20to%20explain%0Alearnability%2C%20quantitative%20measurements%20establishing%20these%20properties%20are%0Ascarce.%20Likewise%2C%20accessing%20the%20latent%20variables%20underlying%20such%20a%20data%0Astructure%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20show%20that%20forward-backward%0Aexperiments%20in%20diffusion-based%20models%2C%20where%20data%20is%20noised%20and%20then%20denoised%0Ato%20generate%20new%20samples%2C%20are%20a%20promising%20tool%20to%20probe%20the%20latent%20structure%20of%0Adata.%20We%20predict%20in%20simple%20hierarchical%20models%20that%2C%20in%20this%20process%2C%20changes%0Ain%20data%20occur%20by%20correlated%20chunks%2C%20with%20a%20length%20scale%20that%20diverges%20at%20a%0Anoise%20level%20where%20a%20phase%20transition%20is%20known%20to%20take%20place.%20Remarkably%2C%20we%0Aconfirm%20this%20prediction%20in%20both%20text%20and%20image%20datasets%20using%20state-of-the-art%0Adiffusion%20models.%20Our%20results%20show%20how%20latent%20variable%20changes%20manifest%20in%20the%0Adata%20and%20establish%20how%20to%20measure%20these%20effects%20in%20real%20data%20using%20diffusion%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13770v1&entry.124074799=Read"},
{"title": "Adaptive Subsampling and Learned Model Improve Spatiotemporal Resolution\n  of Tactile Skin", "author": "Ariel Slepyan and Dian Li and Aidan Aug and Sriramana Sankar and Trac Tran and Nitish Thakor", "abstract": "  High-speed tactile arrays are essential for real-time robotic control in\nunstructured environments, but high pixel counts limit readout rates of most\nlarge tactile arrays to below 100Hz. We introduce ACTS - adaptive compressive\ntactile subsampling - a method that efficiently samples tactile matrices and\nreconstructs interactions using sparse recovery and a learned tactile\ndictionary. Tested on a 1024-pixel sensor array (32x32), ACTS increased frame\nrates by 18X compared to raster scanning, with minimal error. For the first\ntime in large-area tactile skin, we demonstrate rapid object classification\nwithin 20ms of contact, high-speed projectile detection, ricochet angle\nestimation, and deformation tracking through enhanced spatiotemporal\nresolution. Our method can be implemented in firmware, upgrading existing\nlow-cost, flexible, and robust tactile arrays into high-resolution systems for\nlarge-area spatiotemporal touch sensing.\n", "link": "http://arxiv.org/abs/2410.13847v1", "date": "2024-10-17", "relevancy": 1.6652, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5871}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Subsampling%20and%20Learned%20Model%20Improve%20Spatiotemporal%20Resolution%0A%20%20of%20Tactile%20Skin&body=Title%3A%20Adaptive%20Subsampling%20and%20Learned%20Model%20Improve%20Spatiotemporal%20Resolution%0A%20%20of%20Tactile%20Skin%0AAuthor%3A%20Ariel%20Slepyan%20and%20Dian%20Li%20and%20Aidan%20Aug%20and%20Sriramana%20Sankar%20and%20Trac%20Tran%20and%20Nitish%20Thakor%0AAbstract%3A%20%20%20High-speed%20tactile%20arrays%20are%20essential%20for%20real-time%20robotic%20control%20in%0Aunstructured%20environments%2C%20but%20high%20pixel%20counts%20limit%20readout%20rates%20of%20most%0Alarge%20tactile%20arrays%20to%20below%20100Hz.%20We%20introduce%20ACTS%20-%20adaptive%20compressive%0Atactile%20subsampling%20-%20a%20method%20that%20efficiently%20samples%20tactile%20matrices%20and%0Areconstructs%20interactions%20using%20sparse%20recovery%20and%20a%20learned%20tactile%0Adictionary.%20Tested%20on%20a%201024-pixel%20sensor%20array%20%2832x32%29%2C%20ACTS%20increased%20frame%0Arates%20by%2018X%20compared%20to%20raster%20scanning%2C%20with%20minimal%20error.%20For%20the%20first%0Atime%20in%20large-area%20tactile%20skin%2C%20we%20demonstrate%20rapid%20object%20classification%0Awithin%2020ms%20of%20contact%2C%20high-speed%20projectile%20detection%2C%20ricochet%20angle%0Aestimation%2C%20and%20deformation%20tracking%20through%20enhanced%20spatiotemporal%0Aresolution.%20Our%20method%20can%20be%20implemented%20in%20firmware%2C%20upgrading%20existing%0Alow-cost%2C%20flexible%2C%20and%20robust%20tactile%20arrays%20into%20high-resolution%20systems%20for%0Alarge-area%20spatiotemporal%20touch%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Subsampling%2520and%2520Learned%2520Model%2520Improve%2520Spatiotemporal%2520Resolution%250A%2520%2520of%2520Tactile%2520Skin%26entry.906535625%3DAriel%2520Slepyan%2520and%2520Dian%2520Li%2520and%2520Aidan%2520Aug%2520and%2520Sriramana%2520Sankar%2520and%2520Trac%2520Tran%2520and%2520Nitish%2520Thakor%26entry.1292438233%3D%2520%2520High-speed%2520tactile%2520arrays%2520are%2520essential%2520for%2520real-time%2520robotic%2520control%2520in%250Aunstructured%2520environments%252C%2520but%2520high%2520pixel%2520counts%2520limit%2520readout%2520rates%2520of%2520most%250Alarge%2520tactile%2520arrays%2520to%2520below%2520100Hz.%2520We%2520introduce%2520ACTS%2520-%2520adaptive%2520compressive%250Atactile%2520subsampling%2520-%2520a%2520method%2520that%2520efficiently%2520samples%2520tactile%2520matrices%2520and%250Areconstructs%2520interactions%2520using%2520sparse%2520recovery%2520and%2520a%2520learned%2520tactile%250Adictionary.%2520Tested%2520on%2520a%25201024-pixel%2520sensor%2520array%2520%252832x32%2529%252C%2520ACTS%2520increased%2520frame%250Arates%2520by%252018X%2520compared%2520to%2520raster%2520scanning%252C%2520with%2520minimal%2520error.%2520For%2520the%2520first%250Atime%2520in%2520large-area%2520tactile%2520skin%252C%2520we%2520demonstrate%2520rapid%2520object%2520classification%250Awithin%252020ms%2520of%2520contact%252C%2520high-speed%2520projectile%2520detection%252C%2520ricochet%2520angle%250Aestimation%252C%2520and%2520deformation%2520tracking%2520through%2520enhanced%2520spatiotemporal%250Aresolution.%2520Our%2520method%2520can%2520be%2520implemented%2520in%2520firmware%252C%2520upgrading%2520existing%250Alow-cost%252C%2520flexible%252C%2520and%2520robust%2520tactile%2520arrays%2520into%2520high-resolution%2520systems%2520for%250Alarge-area%2520spatiotemporal%2520touch%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Subsampling%20and%20Learned%20Model%20Improve%20Spatiotemporal%20Resolution%0A%20%20of%20Tactile%20Skin&entry.906535625=Ariel%20Slepyan%20and%20Dian%20Li%20and%20Aidan%20Aug%20and%20Sriramana%20Sankar%20and%20Trac%20Tran%20and%20Nitish%20Thakor&entry.1292438233=%20%20High-speed%20tactile%20arrays%20are%20essential%20for%20real-time%20robotic%20control%20in%0Aunstructured%20environments%2C%20but%20high%20pixel%20counts%20limit%20readout%20rates%20of%20most%0Alarge%20tactile%20arrays%20to%20below%20100Hz.%20We%20introduce%20ACTS%20-%20adaptive%20compressive%0Atactile%20subsampling%20-%20a%20method%20that%20efficiently%20samples%20tactile%20matrices%20and%0Areconstructs%20interactions%20using%20sparse%20recovery%20and%20a%20learned%20tactile%0Adictionary.%20Tested%20on%20a%201024-pixel%20sensor%20array%20%2832x32%29%2C%20ACTS%20increased%20frame%0Arates%20by%2018X%20compared%20to%20raster%20scanning%2C%20with%20minimal%20error.%20For%20the%20first%0Atime%20in%20large-area%20tactile%20skin%2C%20we%20demonstrate%20rapid%20object%20classification%0Awithin%2020ms%20of%20contact%2C%20high-speed%20projectile%20detection%2C%20ricochet%20angle%0Aestimation%2C%20and%20deformation%20tracking%20through%20enhanced%20spatiotemporal%0Aresolution.%20Our%20method%20can%20be%20implemented%20in%20firmware%2C%20upgrading%20existing%0Alow-cost%2C%20flexible%2C%20and%20robust%20tactile%20arrays%20into%20high-resolution%20systems%20for%0Alarge-area%20spatiotemporal%20touch%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13847v1&entry.124074799=Read"},
{"title": "Expected Sliced Transport Plans", "author": "Xinran Liu and Roc\u00edo D\u00edaz Mart\u00edn and Yikun Bai and Ashkan Shahbazi and Matthew Thorpe and Akram Aldroubi and Soheil Kolouri", "abstract": "  The optimal transport (OT) problem has gained significant traction in modern\nmachine learning for its ability to: (1) provide versatile metrics, such as\nWasserstein distances and their variants, and (2) determine optimal couplings\nbetween probability measures. To reduce the computational complexity of OT\nsolvers, methods like entropic regularization and sliced optimal transport have\nbeen proposed. The sliced OT framework improves efficiency by comparing\none-dimensional projections (slices) of high-dimensional distributions.\nHowever, despite their computational efficiency, sliced-Wasserstein approaches\nlack a transportation plan between the input measures, limiting their use in\nscenarios requiring explicit coupling. In this paper, we address two key\nquestions: Can a transportation plan be constructed between two probability\nmeasures using the sliced transport framework? If so, can this plan be used to\ndefine a metric between the measures? We propose a \"lifting\" operation to\nextend one-dimensional optimal transport plans back to the original space of\nthe measures. By computing the expectation of these lifted plans, we derive a\nnew transportation plan, termed expected sliced transport (EST) plans. We prove\nthat using the EST plan to weight the sum of the individual Euclidean costs for\nmoving from one point to another results in a valid metric between the input\ndiscrete probability measures. We demonstrate the connection between our\napproach and the recently proposed min-SWGG, along with illustrative numerical\nexamples that support our theoretical findings.\n", "link": "http://arxiv.org/abs/2410.12176v2", "date": "2024-10-17", "relevancy": 1.7183, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.445}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4265}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expected%20Sliced%20Transport%20Plans&body=Title%3A%20Expected%20Sliced%20Transport%20Plans%0AAuthor%3A%20Xinran%20Liu%20and%20Roc%C3%ADo%20D%C3%ADaz%20Mart%C3%ADn%20and%20Yikun%20Bai%20and%20Ashkan%20Shahbazi%20and%20Matthew%20Thorpe%20and%20Akram%20Aldroubi%20and%20Soheil%20Kolouri%0AAbstract%3A%20%20%20The%20optimal%20transport%20%28OT%29%20problem%20has%20gained%20significant%20traction%20in%20modern%0Amachine%20learning%20for%20its%20ability%20to%3A%20%281%29%20provide%20versatile%20metrics%2C%20such%20as%0AWasserstein%20distances%20and%20their%20variants%2C%20and%20%282%29%20determine%20optimal%20couplings%0Abetween%20probability%20measures.%20To%20reduce%20the%20computational%20complexity%20of%20OT%0Asolvers%2C%20methods%20like%20entropic%20regularization%20and%20sliced%20optimal%20transport%20have%0Abeen%20proposed.%20The%20sliced%20OT%20framework%20improves%20efficiency%20by%20comparing%0Aone-dimensional%20projections%20%28slices%29%20of%20high-dimensional%20distributions.%0AHowever%2C%20despite%20their%20computational%20efficiency%2C%20sliced-Wasserstein%20approaches%0Alack%20a%20transportation%20plan%20between%20the%20input%20measures%2C%20limiting%20their%20use%20in%0Ascenarios%20requiring%20explicit%20coupling.%20In%20this%20paper%2C%20we%20address%20two%20key%0Aquestions%3A%20Can%20a%20transportation%20plan%20be%20constructed%20between%20two%20probability%0Ameasures%20using%20the%20sliced%20transport%20framework%3F%20If%20so%2C%20can%20this%20plan%20be%20used%20to%0Adefine%20a%20metric%20between%20the%20measures%3F%20We%20propose%20a%20%22lifting%22%20operation%20to%0Aextend%20one-dimensional%20optimal%20transport%20plans%20back%20to%20the%20original%20space%20of%0Athe%20measures.%20By%20computing%20the%20expectation%20of%20these%20lifted%20plans%2C%20we%20derive%20a%0Anew%20transportation%20plan%2C%20termed%20expected%20sliced%20transport%20%28EST%29%20plans.%20We%20prove%0Athat%20using%20the%20EST%20plan%20to%20weight%20the%20sum%20of%20the%20individual%20Euclidean%20costs%20for%0Amoving%20from%20one%20point%20to%20another%20results%20in%20a%20valid%20metric%20between%20the%20input%0Adiscrete%20probability%20measures.%20We%20demonstrate%20the%20connection%20between%20our%0Aapproach%20and%20the%20recently%20proposed%20min-SWGG%2C%20along%20with%20illustrative%20numerical%0Aexamples%20that%20support%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpected%2520Sliced%2520Transport%2520Plans%26entry.906535625%3DXinran%2520Liu%2520and%2520Roc%25C3%25ADo%2520D%25C3%25ADaz%2520Mart%25C3%25ADn%2520and%2520Yikun%2520Bai%2520and%2520Ashkan%2520Shahbazi%2520and%2520Matthew%2520Thorpe%2520and%2520Akram%2520Aldroubi%2520and%2520Soheil%2520Kolouri%26entry.1292438233%3D%2520%2520The%2520optimal%2520transport%2520%2528OT%2529%2520problem%2520has%2520gained%2520significant%2520traction%2520in%2520modern%250Amachine%2520learning%2520for%2520its%2520ability%2520to%253A%2520%25281%2529%2520provide%2520versatile%2520metrics%252C%2520such%2520as%250AWasserstein%2520distances%2520and%2520their%2520variants%252C%2520and%2520%25282%2529%2520determine%2520optimal%2520couplings%250Abetween%2520probability%2520measures.%2520To%2520reduce%2520the%2520computational%2520complexity%2520of%2520OT%250Asolvers%252C%2520methods%2520like%2520entropic%2520regularization%2520and%2520sliced%2520optimal%2520transport%2520have%250Abeen%2520proposed.%2520The%2520sliced%2520OT%2520framework%2520improves%2520efficiency%2520by%2520comparing%250Aone-dimensional%2520projections%2520%2528slices%2529%2520of%2520high-dimensional%2520distributions.%250AHowever%252C%2520despite%2520their%2520computational%2520efficiency%252C%2520sliced-Wasserstein%2520approaches%250Alack%2520a%2520transportation%2520plan%2520between%2520the%2520input%2520measures%252C%2520limiting%2520their%2520use%2520in%250Ascenarios%2520requiring%2520explicit%2520coupling.%2520In%2520this%2520paper%252C%2520we%2520address%2520two%2520key%250Aquestions%253A%2520Can%2520a%2520transportation%2520plan%2520be%2520constructed%2520between%2520two%2520probability%250Ameasures%2520using%2520the%2520sliced%2520transport%2520framework%253F%2520If%2520so%252C%2520can%2520this%2520plan%2520be%2520used%2520to%250Adefine%2520a%2520metric%2520between%2520the%2520measures%253F%2520We%2520propose%2520a%2520%2522lifting%2522%2520operation%2520to%250Aextend%2520one-dimensional%2520optimal%2520transport%2520plans%2520back%2520to%2520the%2520original%2520space%2520of%250Athe%2520measures.%2520By%2520computing%2520the%2520expectation%2520of%2520these%2520lifted%2520plans%252C%2520we%2520derive%2520a%250Anew%2520transportation%2520plan%252C%2520termed%2520expected%2520sliced%2520transport%2520%2528EST%2529%2520plans.%2520We%2520prove%250Athat%2520using%2520the%2520EST%2520plan%2520to%2520weight%2520the%2520sum%2520of%2520the%2520individual%2520Euclidean%2520costs%2520for%250Amoving%2520from%2520one%2520point%2520to%2520another%2520results%2520in%2520a%2520valid%2520metric%2520between%2520the%2520input%250Adiscrete%2520probability%2520measures.%2520We%2520demonstrate%2520the%2520connection%2520between%2520our%250Aapproach%2520and%2520the%2520recently%2520proposed%2520min-SWGG%252C%2520along%2520with%2520illustrative%2520numerical%250Aexamples%2520that%2520support%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expected%20Sliced%20Transport%20Plans&entry.906535625=Xinran%20Liu%20and%20Roc%C3%ADo%20D%C3%ADaz%20Mart%C3%ADn%20and%20Yikun%20Bai%20and%20Ashkan%20Shahbazi%20and%20Matthew%20Thorpe%20and%20Akram%20Aldroubi%20and%20Soheil%20Kolouri&entry.1292438233=%20%20The%20optimal%20transport%20%28OT%29%20problem%20has%20gained%20significant%20traction%20in%20modern%0Amachine%20learning%20for%20its%20ability%20to%3A%20%281%29%20provide%20versatile%20metrics%2C%20such%20as%0AWasserstein%20distances%20and%20their%20variants%2C%20and%20%282%29%20determine%20optimal%20couplings%0Abetween%20probability%20measures.%20To%20reduce%20the%20computational%20complexity%20of%20OT%0Asolvers%2C%20methods%20like%20entropic%20regularization%20and%20sliced%20optimal%20transport%20have%0Abeen%20proposed.%20The%20sliced%20OT%20framework%20improves%20efficiency%20by%20comparing%0Aone-dimensional%20projections%20%28slices%29%20of%20high-dimensional%20distributions.%0AHowever%2C%20despite%20their%20computational%20efficiency%2C%20sliced-Wasserstein%20approaches%0Alack%20a%20transportation%20plan%20between%20the%20input%20measures%2C%20limiting%20their%20use%20in%0Ascenarios%20requiring%20explicit%20coupling.%20In%20this%20paper%2C%20we%20address%20two%20key%0Aquestions%3A%20Can%20a%20transportation%20plan%20be%20constructed%20between%20two%20probability%0Ameasures%20using%20the%20sliced%20transport%20framework%3F%20If%20so%2C%20can%20this%20plan%20be%20used%20to%0Adefine%20a%20metric%20between%20the%20measures%3F%20We%20propose%20a%20%22lifting%22%20operation%20to%0Aextend%20one-dimensional%20optimal%20transport%20plans%20back%20to%20the%20original%20space%20of%0Athe%20measures.%20By%20computing%20the%20expectation%20of%20these%20lifted%20plans%2C%20we%20derive%20a%0Anew%20transportation%20plan%2C%20termed%20expected%20sliced%20transport%20%28EST%29%20plans.%20We%20prove%0Athat%20using%20the%20EST%20plan%20to%20weight%20the%20sum%20of%20the%20individual%20Euclidean%20costs%20for%0Amoving%20from%20one%20point%20to%20another%20results%20in%20a%20valid%20metric%20between%20the%20input%0Adiscrete%20probability%20measures.%20We%20demonstrate%20the%20connection%20between%20our%0Aapproach%20and%20the%20recently%20proposed%20min-SWGG%2C%20along%20with%20illustrative%20numerical%0Aexamples%20that%20support%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12176v2&entry.124074799=Read"},
{"title": "MASQ: Multi-Agent Reinforcement Learning for Single Quadruped Robot\n  Locomotion", "author": "Qi Liu and Jingxiang Guo and Sixu Lin and Shuaikang Ma and Jinxuan Zhu and Yanjie Li", "abstract": "  This paper proposes a novel method to improve locomotion learning for a\nsingle quadruped robot using multi-agent deep reinforcement learning (MARL).\nMany existing methods use single-agent reinforcement learning for an individual\nrobot or MARL for the cooperative task in multi-robot systems. Unlike existing\nmethods, this paper proposes using MARL for the locomotion learning of a single\nquadruped robot. We develop a learning structure called Multi-Agent\nReinforcement Learning for Single Quadruped Robot Locomotion (MASQ),\nconsidering each leg as an agent to explore the action space of the quadruped\nrobot, sharing a global critic, and learning collaboratively. Experimental\nresults indicate that MASQ not only speeds up learning convergence but also\nenhances robustness in real-world settings, suggesting that applying MASQ to\nsingle robots such as quadrupeds could surpass traditional single-robot\nreinforcement learning approaches. Our study provides insightful guidance on\nintegrating MARL with single-robot locomotion learning.\n", "link": "http://arxiv.org/abs/2408.13759v2", "date": "2024-10-17", "relevancy": 1.5409, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5949}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5076}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASQ%3A%20Multi-Agent%20Reinforcement%20Learning%20for%20Single%20Quadruped%20Robot%0A%20%20Locomotion&body=Title%3A%20MASQ%3A%20Multi-Agent%20Reinforcement%20Learning%20for%20Single%20Quadruped%20Robot%0A%20%20Locomotion%0AAuthor%3A%20Qi%20Liu%20and%20Jingxiang%20Guo%20and%20Sixu%20Lin%20and%20Shuaikang%20Ma%20and%20Jinxuan%20Zhu%20and%20Yanjie%20Li%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20method%20to%20improve%20locomotion%20learning%20for%20a%0Asingle%20quadruped%20robot%20using%20multi-agent%20deep%20reinforcement%20learning%20%28MARL%29.%0AMany%20existing%20methods%20use%20single-agent%20reinforcement%20learning%20for%20an%20individual%0Arobot%20or%20MARL%20for%20the%20cooperative%20task%20in%20multi-robot%20systems.%20Unlike%20existing%0Amethods%2C%20this%20paper%20proposes%20using%20MARL%20for%20the%20locomotion%20learning%20of%20a%20single%0Aquadruped%20robot.%20We%20develop%20a%20learning%20structure%20called%20Multi-Agent%0AReinforcement%20Learning%20for%20Single%20Quadruped%20Robot%20Locomotion%20%28MASQ%29%2C%0Aconsidering%20each%20leg%20as%20an%20agent%20to%20explore%20the%20action%20space%20of%20the%20quadruped%0Arobot%2C%20sharing%20a%20global%20critic%2C%20and%20learning%20collaboratively.%20Experimental%0Aresults%20indicate%20that%20MASQ%20not%20only%20speeds%20up%20learning%20convergence%20but%20also%0Aenhances%20robustness%20in%20real-world%20settings%2C%20suggesting%20that%20applying%20MASQ%20to%0Asingle%20robots%20such%20as%20quadrupeds%20could%20surpass%20traditional%20single-robot%0Areinforcement%20learning%20approaches.%20Our%20study%20provides%20insightful%20guidance%20on%0Aintegrating%20MARL%20with%20single-robot%20locomotion%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASQ%253A%2520Multi-Agent%2520Reinforcement%2520Learning%2520for%2520Single%2520Quadruped%2520Robot%250A%2520%2520Locomotion%26entry.906535625%3DQi%2520Liu%2520and%2520Jingxiang%2520Guo%2520and%2520Sixu%2520Lin%2520and%2520Shuaikang%2520Ma%2520and%2520Jinxuan%2520Zhu%2520and%2520Yanjie%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520method%2520to%2520improve%2520locomotion%2520learning%2520for%2520a%250Asingle%2520quadruped%2520robot%2520using%2520multi-agent%2520deep%2520reinforcement%2520learning%2520%2528MARL%2529.%250AMany%2520existing%2520methods%2520use%2520single-agent%2520reinforcement%2520learning%2520for%2520an%2520individual%250Arobot%2520or%2520MARL%2520for%2520the%2520cooperative%2520task%2520in%2520multi-robot%2520systems.%2520Unlike%2520existing%250Amethods%252C%2520this%2520paper%2520proposes%2520using%2520MARL%2520for%2520the%2520locomotion%2520learning%2520of%2520a%2520single%250Aquadruped%2520robot.%2520We%2520develop%2520a%2520learning%2520structure%2520called%2520Multi-Agent%250AReinforcement%2520Learning%2520for%2520Single%2520Quadruped%2520Robot%2520Locomotion%2520%2528MASQ%2529%252C%250Aconsidering%2520each%2520leg%2520as%2520an%2520agent%2520to%2520explore%2520the%2520action%2520space%2520of%2520the%2520quadruped%250Arobot%252C%2520sharing%2520a%2520global%2520critic%252C%2520and%2520learning%2520collaboratively.%2520Experimental%250Aresults%2520indicate%2520that%2520MASQ%2520not%2520only%2520speeds%2520up%2520learning%2520convergence%2520but%2520also%250Aenhances%2520robustness%2520in%2520real-world%2520settings%252C%2520suggesting%2520that%2520applying%2520MASQ%2520to%250Asingle%2520robots%2520such%2520as%2520quadrupeds%2520could%2520surpass%2520traditional%2520single-robot%250Areinforcement%2520learning%2520approaches.%2520Our%2520study%2520provides%2520insightful%2520guidance%2520on%250Aintegrating%2520MARL%2520with%2520single-robot%2520locomotion%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASQ%3A%20Multi-Agent%20Reinforcement%20Learning%20for%20Single%20Quadruped%20Robot%0A%20%20Locomotion&entry.906535625=Qi%20Liu%20and%20Jingxiang%20Guo%20and%20Sixu%20Lin%20and%20Shuaikang%20Ma%20and%20Jinxuan%20Zhu%20and%20Yanjie%20Li&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20method%20to%20improve%20locomotion%20learning%20for%20a%0Asingle%20quadruped%20robot%20using%20multi-agent%20deep%20reinforcement%20learning%20%28MARL%29.%0AMany%20existing%20methods%20use%20single-agent%20reinforcement%20learning%20for%20an%20individual%0Arobot%20or%20MARL%20for%20the%20cooperative%20task%20in%20multi-robot%20systems.%20Unlike%20existing%0Amethods%2C%20this%20paper%20proposes%20using%20MARL%20for%20the%20locomotion%20learning%20of%20a%20single%0Aquadruped%20robot.%20We%20develop%20a%20learning%20structure%20called%20Multi-Agent%0AReinforcement%20Learning%20for%20Single%20Quadruped%20Robot%20Locomotion%20%28MASQ%29%2C%0Aconsidering%20each%20leg%20as%20an%20agent%20to%20explore%20the%20action%20space%20of%20the%20quadruped%0Arobot%2C%20sharing%20a%20global%20critic%2C%20and%20learning%20collaboratively.%20Experimental%0Aresults%20indicate%20that%20MASQ%20not%20only%20speeds%20up%20learning%20convergence%20but%20also%0Aenhances%20robustness%20in%20real-world%20settings%2C%20suggesting%20that%20applying%20MASQ%20to%0Asingle%20robots%20such%20as%20quadrupeds%20could%20surpass%20traditional%20single-robot%0Areinforcement%20learning%20approaches.%20Our%20study%20provides%20insightful%20guidance%20on%0Aintegrating%20MARL%20with%20single-robot%20locomotion%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13759v2&entry.124074799=Read"},
{"title": "The Disparate Benefits of Deep Ensembles", "author": "Kajetan Schweighofer and Adrian Arnaiz-Rodriguez and Sepp Hochreiter and Nuria Oliver", "abstract": "  Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a\nsimple way to boost predictive performance. However, their impact on\nalgorithmic fairness is not well understood yet. Algorithmic fairness\ninvestigates how a model's performance varies across different groups,\ntypically defined by protected attributes such as age, gender, or race. In this\nwork, we investigate the interplay between the performance gains from Deep\nEnsembles and fairness. Our analysis reveals that they unevenly favor different\ngroups in what we refer to as a disparate benefits effect. We empirically\ninvestigate this effect with Deep Ensembles applied to popular facial analysis\nand medical imaging datasets, where protected group attributes are given and\nfind that it occurs for multiple established group fairness metrics, including\nstatistical parity and equal opportunity. Furthermore, we identify the\nper-group difference in predictive diversity of ensemble members as the\npotential cause of the disparate benefits effect. Finally, we evaluate\ndifferent approaches to reduce unfairness due to the disparate benefits effect.\nOur findings show that post-processing is an effective method to mitigate this\nunfairness while preserving the improved performance of Deep Ensembles.\n", "link": "http://arxiv.org/abs/2410.13831v1", "date": "2024-10-17", "relevancy": 1.4569, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4663}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Disparate%20Benefits%20of%20Deep%20Ensembles&body=Title%3A%20The%20Disparate%20Benefits%20of%20Deep%20Ensembles%0AAuthor%3A%20Kajetan%20Schweighofer%20and%20Adrian%20Arnaiz-Rodriguez%20and%20Sepp%20Hochreiter%20and%20Nuria%20Oliver%0AAbstract%3A%20%20%20Ensembles%20of%20Deep%20Neural%20Networks%2C%20Deep%20Ensembles%2C%20are%20widely%20used%20as%20a%0Asimple%20way%20to%20boost%20predictive%20performance.%20However%2C%20their%20impact%20on%0Aalgorithmic%20fairness%20is%20not%20well%20understood%20yet.%20Algorithmic%20fairness%0Ainvestigates%20how%20a%20model%27s%20performance%20varies%20across%20different%20groups%2C%0Atypically%20defined%20by%20protected%20attributes%20such%20as%20age%2C%20gender%2C%20or%20race.%20In%20this%0Awork%2C%20we%20investigate%20the%20interplay%20between%20the%20performance%20gains%20from%20Deep%0AEnsembles%20and%20fairness.%20Our%20analysis%20reveals%20that%20they%20unevenly%20favor%20different%0Agroups%20in%20what%20we%20refer%20to%20as%20a%20disparate%20benefits%20effect.%20We%20empirically%0Ainvestigate%20this%20effect%20with%20Deep%20Ensembles%20applied%20to%20popular%20facial%20analysis%0Aand%20medical%20imaging%20datasets%2C%20where%20protected%20group%20attributes%20are%20given%20and%0Afind%20that%20it%20occurs%20for%20multiple%20established%20group%20fairness%20metrics%2C%20including%0Astatistical%20parity%20and%20equal%20opportunity.%20Furthermore%2C%20we%20identify%20the%0Aper-group%20difference%20in%20predictive%20diversity%20of%20ensemble%20members%20as%20the%0Apotential%20cause%20of%20the%20disparate%20benefits%20effect.%20Finally%2C%20we%20evaluate%0Adifferent%20approaches%20to%20reduce%20unfairness%20due%20to%20the%20disparate%20benefits%20effect.%0AOur%20findings%20show%20that%20post-processing%20is%20an%20effective%20method%20to%20mitigate%20this%0Aunfairness%20while%20preserving%20the%20improved%20performance%20of%20Deep%20Ensembles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Disparate%2520Benefits%2520of%2520Deep%2520Ensembles%26entry.906535625%3DKajetan%2520Schweighofer%2520and%2520Adrian%2520Arnaiz-Rodriguez%2520and%2520Sepp%2520Hochreiter%2520and%2520Nuria%2520Oliver%26entry.1292438233%3D%2520%2520Ensembles%2520of%2520Deep%2520Neural%2520Networks%252C%2520Deep%2520Ensembles%252C%2520are%2520widely%2520used%2520as%2520a%250Asimple%2520way%2520to%2520boost%2520predictive%2520performance.%2520However%252C%2520their%2520impact%2520on%250Aalgorithmic%2520fairness%2520is%2520not%2520well%2520understood%2520yet.%2520Algorithmic%2520fairness%250Ainvestigates%2520how%2520a%2520model%2527s%2520performance%2520varies%2520across%2520different%2520groups%252C%250Atypically%2520defined%2520by%2520protected%2520attributes%2520such%2520as%2520age%252C%2520gender%252C%2520or%2520race.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520the%2520interplay%2520between%2520the%2520performance%2520gains%2520from%2520Deep%250AEnsembles%2520and%2520fairness.%2520Our%2520analysis%2520reveals%2520that%2520they%2520unevenly%2520favor%2520different%250Agroups%2520in%2520what%2520we%2520refer%2520to%2520as%2520a%2520disparate%2520benefits%2520effect.%2520We%2520empirically%250Ainvestigate%2520this%2520effect%2520with%2520Deep%2520Ensembles%2520applied%2520to%2520popular%2520facial%2520analysis%250Aand%2520medical%2520imaging%2520datasets%252C%2520where%2520protected%2520group%2520attributes%2520are%2520given%2520and%250Afind%2520that%2520it%2520occurs%2520for%2520multiple%2520established%2520group%2520fairness%2520metrics%252C%2520including%250Astatistical%2520parity%2520and%2520equal%2520opportunity.%2520Furthermore%252C%2520we%2520identify%2520the%250Aper-group%2520difference%2520in%2520predictive%2520diversity%2520of%2520ensemble%2520members%2520as%2520the%250Apotential%2520cause%2520of%2520the%2520disparate%2520benefits%2520effect.%2520Finally%252C%2520we%2520evaluate%250Adifferent%2520approaches%2520to%2520reduce%2520unfairness%2520due%2520to%2520the%2520disparate%2520benefits%2520effect.%250AOur%2520findings%2520show%2520that%2520post-processing%2520is%2520an%2520effective%2520method%2520to%2520mitigate%2520this%250Aunfairness%2520while%2520preserving%2520the%2520improved%2520performance%2520of%2520Deep%2520Ensembles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Disparate%20Benefits%20of%20Deep%20Ensembles&entry.906535625=Kajetan%20Schweighofer%20and%20Adrian%20Arnaiz-Rodriguez%20and%20Sepp%20Hochreiter%20and%20Nuria%20Oliver&entry.1292438233=%20%20Ensembles%20of%20Deep%20Neural%20Networks%2C%20Deep%20Ensembles%2C%20are%20widely%20used%20as%20a%0Asimple%20way%20to%20boost%20predictive%20performance.%20However%2C%20their%20impact%20on%0Aalgorithmic%20fairness%20is%20not%20well%20understood%20yet.%20Algorithmic%20fairness%0Ainvestigates%20how%20a%20model%27s%20performance%20varies%20across%20different%20groups%2C%0Atypically%20defined%20by%20protected%20attributes%20such%20as%20age%2C%20gender%2C%20or%20race.%20In%20this%0Awork%2C%20we%20investigate%20the%20interplay%20between%20the%20performance%20gains%20from%20Deep%0AEnsembles%20and%20fairness.%20Our%20analysis%20reveals%20that%20they%20unevenly%20favor%20different%0Agroups%20in%20what%20we%20refer%20to%20as%20a%20disparate%20benefits%20effect.%20We%20empirically%0Ainvestigate%20this%20effect%20with%20Deep%20Ensembles%20applied%20to%20popular%20facial%20analysis%0Aand%20medical%20imaging%20datasets%2C%20where%20protected%20group%20attributes%20are%20given%20and%0Afind%20that%20it%20occurs%20for%20multiple%20established%20group%20fairness%20metrics%2C%20including%0Astatistical%20parity%20and%20equal%20opportunity.%20Furthermore%2C%20we%20identify%20the%0Aper-group%20difference%20in%20predictive%20diversity%20of%20ensemble%20members%20as%20the%0Apotential%20cause%20of%20the%20disparate%20benefits%20effect.%20Finally%2C%20we%20evaluate%0Adifferent%20approaches%20to%20reduce%20unfairness%20due%20to%20the%20disparate%20benefits%20effect.%0AOur%20findings%20show%20that%20post-processing%20is%20an%20effective%20method%20to%20mitigate%20this%0Aunfairness%20while%20preserving%20the%20improved%20performance%20of%20Deep%20Ensembles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13831v1&entry.124074799=Read"},
{"title": "Interacting humans and robots can improve sensory prediction by adapting\n  their viscoelasticity", "author": "Xiaoxiao Cheng and Jonathan Eden and Bastien Berret and Atsushi Takagi and Etienne Burdet", "abstract": "  To manipulate objects or dance together, humans and robots exchange energy\nand haptic information. While the exchange of energy in human-robot interaction\nhas been extensively investigated, the underlying exchange of haptic\ninformation is not well understood. Here, we develop a computational model of\nthe mechanical and sensory interactions between agents that can tune their\nviscoelasticity while considering their sensory and motor noise. The resulting\nstochastic-optimal-information-and-effort (SOIE) controller predicts how the\nexchange of haptic information and the performance can be improved by adjusting\nviscoelasticity. This controller was first implemented on a robot-robot\nexperiment with a tracking task which showed its superior performance when\ncompared to either stiff or compliant control. Importantly, the optimal\ncontroller also predicts how connected humans alter their muscle activation to\nimprove haptic communication, with differentiated viscoelasticity adjustment to\ntheir own sensing noise and haptic perturbations. A human-robot experiment then\nillustrated the applicability of this optimal control strategy for robots,\nyielding improved tracking performance and effective haptic communication as\nthe robot adjusted its viscoelasticity according to its own and the user's\nnoise characteristics. The proposed SOIE controller may thus be used to improve\nhaptic communication and collaboration of humans and robots.\n", "link": "http://arxiv.org/abs/2410.13755v1", "date": "2024-10-17", "relevancy": 1.6559, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5983}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5424}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interacting%20humans%20and%20robots%20can%20improve%20sensory%20prediction%20by%20adapting%0A%20%20their%20viscoelasticity&body=Title%3A%20Interacting%20humans%20and%20robots%20can%20improve%20sensory%20prediction%20by%20adapting%0A%20%20their%20viscoelasticity%0AAuthor%3A%20Xiaoxiao%20Cheng%20and%20Jonathan%20Eden%20and%20Bastien%20Berret%20and%20Atsushi%20Takagi%20and%20Etienne%20Burdet%0AAbstract%3A%20%20%20To%20manipulate%20objects%20or%20dance%20together%2C%20humans%20and%20robots%20exchange%20energy%0Aand%20haptic%20information.%20While%20the%20exchange%20of%20energy%20in%20human-robot%20interaction%0Ahas%20been%20extensively%20investigated%2C%20the%20underlying%20exchange%20of%20haptic%0Ainformation%20is%20not%20well%20understood.%20Here%2C%20we%20develop%20a%20computational%20model%20of%0Athe%20mechanical%20and%20sensory%20interactions%20between%20agents%20that%20can%20tune%20their%0Aviscoelasticity%20while%20considering%20their%20sensory%20and%20motor%20noise.%20The%20resulting%0Astochastic-optimal-information-and-effort%20%28SOIE%29%20controller%20predicts%20how%20the%0Aexchange%20of%20haptic%20information%20and%20the%20performance%20can%20be%20improved%20by%20adjusting%0Aviscoelasticity.%20This%20controller%20was%20first%20implemented%20on%20a%20robot-robot%0Aexperiment%20with%20a%20tracking%20task%20which%20showed%20its%20superior%20performance%20when%0Acompared%20to%20either%20stiff%20or%20compliant%20control.%20Importantly%2C%20the%20optimal%0Acontroller%20also%20predicts%20how%20connected%20humans%20alter%20their%20muscle%20activation%20to%0Aimprove%20haptic%20communication%2C%20with%20differentiated%20viscoelasticity%20adjustment%20to%0Atheir%20own%20sensing%20noise%20and%20haptic%20perturbations.%20A%20human-robot%20experiment%20then%0Aillustrated%20the%20applicability%20of%20this%20optimal%20control%20strategy%20for%20robots%2C%0Ayielding%20improved%20tracking%20performance%20and%20effective%20haptic%20communication%20as%0Athe%20robot%20adjusted%20its%20viscoelasticity%20according%20to%20its%20own%20and%20the%20user%27s%0Anoise%20characteristics.%20The%20proposed%20SOIE%20controller%20may%20thus%20be%20used%20to%20improve%0Ahaptic%20communication%20and%20collaboration%20of%20humans%20and%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteracting%2520humans%2520and%2520robots%2520can%2520improve%2520sensory%2520prediction%2520by%2520adapting%250A%2520%2520their%2520viscoelasticity%26entry.906535625%3DXiaoxiao%2520Cheng%2520and%2520Jonathan%2520Eden%2520and%2520Bastien%2520Berret%2520and%2520Atsushi%2520Takagi%2520and%2520Etienne%2520Burdet%26entry.1292438233%3D%2520%2520To%2520manipulate%2520objects%2520or%2520dance%2520together%252C%2520humans%2520and%2520robots%2520exchange%2520energy%250Aand%2520haptic%2520information.%2520While%2520the%2520exchange%2520of%2520energy%2520in%2520human-robot%2520interaction%250Ahas%2520been%2520extensively%2520investigated%252C%2520the%2520underlying%2520exchange%2520of%2520haptic%250Ainformation%2520is%2520not%2520well%2520understood.%2520Here%252C%2520we%2520develop%2520a%2520computational%2520model%2520of%250Athe%2520mechanical%2520and%2520sensory%2520interactions%2520between%2520agents%2520that%2520can%2520tune%2520their%250Aviscoelasticity%2520while%2520considering%2520their%2520sensory%2520and%2520motor%2520noise.%2520The%2520resulting%250Astochastic-optimal-information-and-effort%2520%2528SOIE%2529%2520controller%2520predicts%2520how%2520the%250Aexchange%2520of%2520haptic%2520information%2520and%2520the%2520performance%2520can%2520be%2520improved%2520by%2520adjusting%250Aviscoelasticity.%2520This%2520controller%2520was%2520first%2520implemented%2520on%2520a%2520robot-robot%250Aexperiment%2520with%2520a%2520tracking%2520task%2520which%2520showed%2520its%2520superior%2520performance%2520when%250Acompared%2520to%2520either%2520stiff%2520or%2520compliant%2520control.%2520Importantly%252C%2520the%2520optimal%250Acontroller%2520also%2520predicts%2520how%2520connected%2520humans%2520alter%2520their%2520muscle%2520activation%2520to%250Aimprove%2520haptic%2520communication%252C%2520with%2520differentiated%2520viscoelasticity%2520adjustment%2520to%250Atheir%2520own%2520sensing%2520noise%2520and%2520haptic%2520perturbations.%2520A%2520human-robot%2520experiment%2520then%250Aillustrated%2520the%2520applicability%2520of%2520this%2520optimal%2520control%2520strategy%2520for%2520robots%252C%250Ayielding%2520improved%2520tracking%2520performance%2520and%2520effective%2520haptic%2520communication%2520as%250Athe%2520robot%2520adjusted%2520its%2520viscoelasticity%2520according%2520to%2520its%2520own%2520and%2520the%2520user%2527s%250Anoise%2520characteristics.%2520The%2520proposed%2520SOIE%2520controller%2520may%2520thus%2520be%2520used%2520to%2520improve%250Ahaptic%2520communication%2520and%2520collaboration%2520of%2520humans%2520and%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interacting%20humans%20and%20robots%20can%20improve%20sensory%20prediction%20by%20adapting%0A%20%20their%20viscoelasticity&entry.906535625=Xiaoxiao%20Cheng%20and%20Jonathan%20Eden%20and%20Bastien%20Berret%20and%20Atsushi%20Takagi%20and%20Etienne%20Burdet&entry.1292438233=%20%20To%20manipulate%20objects%20or%20dance%20together%2C%20humans%20and%20robots%20exchange%20energy%0Aand%20haptic%20information.%20While%20the%20exchange%20of%20energy%20in%20human-robot%20interaction%0Ahas%20been%20extensively%20investigated%2C%20the%20underlying%20exchange%20of%20haptic%0Ainformation%20is%20not%20well%20understood.%20Here%2C%20we%20develop%20a%20computational%20model%20of%0Athe%20mechanical%20and%20sensory%20interactions%20between%20agents%20that%20can%20tune%20their%0Aviscoelasticity%20while%20considering%20their%20sensory%20and%20motor%20noise.%20The%20resulting%0Astochastic-optimal-information-and-effort%20%28SOIE%29%20controller%20predicts%20how%20the%0Aexchange%20of%20haptic%20information%20and%20the%20performance%20can%20be%20improved%20by%20adjusting%0Aviscoelasticity.%20This%20controller%20was%20first%20implemented%20on%20a%20robot-robot%0Aexperiment%20with%20a%20tracking%20task%20which%20showed%20its%20superior%20performance%20when%0Acompared%20to%20either%20stiff%20or%20compliant%20control.%20Importantly%2C%20the%20optimal%0Acontroller%20also%20predicts%20how%20connected%20humans%20alter%20their%20muscle%20activation%20to%0Aimprove%20haptic%20communication%2C%20with%20differentiated%20viscoelasticity%20adjustment%20to%0Atheir%20own%20sensing%20noise%20and%20haptic%20perturbations.%20A%20human-robot%20experiment%20then%0Aillustrated%20the%20applicability%20of%20this%20optimal%20control%20strategy%20for%20robots%2C%0Ayielding%20improved%20tracking%20performance%20and%20effective%20haptic%20communication%20as%0Athe%20robot%20adjusted%20its%20viscoelasticity%20according%20to%20its%20own%20and%20the%20user%27s%0Anoise%20characteristics.%20The%20proposed%20SOIE%20controller%20may%20thus%20be%20used%20to%20improve%0Ahaptic%20communication%20and%20collaboration%20of%20humans%20and%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13755v1&entry.124074799=Read"},
{"title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents", "author": "Ke Yang and Yao Liu and Sapana Chaudhary and Rasool Fakoor and Pratik Chaudhari and George Karypis and Huzefa Rangwala", "abstract": "  Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents.\n", "link": "http://arxiv.org/abs/2410.13825v1", "date": "2024-10-17", "relevancy": 1.5222, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5187}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentOccam%3A%20A%20Simple%20Yet%20Strong%20Baseline%20for%20LLM-Based%20Web%20Agents&body=Title%3A%20AgentOccam%3A%20A%20Simple%20Yet%20Strong%20Baseline%20for%20LLM-Based%20Web%20Agents%0AAuthor%3A%20Ke%20Yang%20and%20Yao%20Liu%20and%20Sapana%20Chaudhary%20and%20Rasool%20Fakoor%20and%20Pratik%20Chaudhari%20and%20George%20Karypis%20and%20Huzefa%20Rangwala%0AAbstract%3A%20%20%20Autonomy%20via%20agents%20using%20large%20language%20models%20%28LLMs%29%20for%20personalized%2C%0Astandardized%20tasks%20boosts%20human%20efficiency.%20Automating%20web%20tasks%20%28like%20booking%0Ahotels%20within%20a%20budget%29%20is%20increasingly%20sought%20after.%20Fulfilling%20practical%0Aneeds%2C%20the%20web%20agent%20also%20serves%20as%20an%20important%20proof-of-concept%20example%20for%0Avarious%20agent%20grounding%20scenarios%2C%20with%20its%20success%20promising%20advancements%20in%0Amany%20future%20applications.%20Prior%20research%20often%20handcrafts%20web%20agent%20strategies%0A%28e.g.%2C%20prompting%20templates%2C%20multi-agent%20systems%2C%20search%20methods%2C%20etc.%29%20and%20the%0Acorresponding%20in-context%20examples%2C%20which%20may%20not%20generalize%20well%20across%20all%0Areal-world%20scenarios.%20On%20the%20other%20hand%2C%20there%20has%20been%20limited%20study%20on%20the%0Amisalignment%20between%20a%20web%20agent%27s%20observation/action%20representation%20and%20the%0Apre-training%20data%20of%20the%20LLM%20it%27s%20based%20on.%20This%20discrepancy%20is%20especially%0Anotable%20when%20LLMs%20are%20primarily%20trained%20for%20language%20completion%20rather%20than%0Atasks%20involving%20embodied%20navigation%20actions%20and%20symbolic%20web%20elements.%20Our%0Astudy%20enhances%20an%20LLM-based%20web%20agent%20by%20simply%20refining%20its%20observation%20and%0Aaction%20space%20to%20better%20align%20with%20the%20LLM%27s%20capabilities.%20This%20approach%20enables%0Aour%20base%20agent%20to%20significantly%20outperform%20previous%20methods%20on%20a%20wide%20variety%0Aof%20web%20tasks.%20Specifically%2C%20on%20WebArena%2C%20a%20benchmark%20featuring%20general-purpose%0Aweb%20interaction%20tasks%2C%20our%20agent%20AgentOccam%20surpasses%20the%20previous%0Astate-of-the-art%20and%20concurrent%20work%20by%209.8%20%28%2B29.4%25%29%20and%205.9%20%28%2B15.8%25%29%20absolute%0Apoints%20respectively%2C%20and%20boosts%20the%20success%20rate%20by%2026.6%20points%20%28%2B161%25%29%20over%0Asimilar%20plain%20web%20agents%20with%20its%20observation%20and%20action%20space%20alignment.%20We%0Aachieve%20this%20without%20using%20in-context%20examples%2C%20new%20agent%20roles%2C%20online%0Afeedback%20or%20search%20strategies.%20AgentOccam%27s%20simple%20design%20highlights%20LLMs%27%0Aimpressive%20zero-shot%20performance%20on%20web%20tasks%2C%20and%20underlines%20the%20critical%20role%0Aof%20carefully%20tuning%20observation%20and%20action%20spaces%20for%20LLM-based%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentOccam%253A%2520A%2520Simple%2520Yet%2520Strong%2520Baseline%2520for%2520LLM-Based%2520Web%2520Agents%26entry.906535625%3DKe%2520Yang%2520and%2520Yao%2520Liu%2520and%2520Sapana%2520Chaudhary%2520and%2520Rasool%2520Fakoor%2520and%2520Pratik%2520Chaudhari%2520and%2520George%2520Karypis%2520and%2520Huzefa%2520Rangwala%26entry.1292438233%3D%2520%2520Autonomy%2520via%2520agents%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520personalized%252C%250Astandardized%2520tasks%2520boosts%2520human%2520efficiency.%2520Automating%2520web%2520tasks%2520%2528like%2520booking%250Ahotels%2520within%2520a%2520budget%2529%2520is%2520increasingly%2520sought%2520after.%2520Fulfilling%2520practical%250Aneeds%252C%2520the%2520web%2520agent%2520also%2520serves%2520as%2520an%2520important%2520proof-of-concept%2520example%2520for%250Avarious%2520agent%2520grounding%2520scenarios%252C%2520with%2520its%2520success%2520promising%2520advancements%2520in%250Amany%2520future%2520applications.%2520Prior%2520research%2520often%2520handcrafts%2520web%2520agent%2520strategies%250A%2528e.g.%252C%2520prompting%2520templates%252C%2520multi-agent%2520systems%252C%2520search%2520methods%252C%2520etc.%2529%2520and%2520the%250Acorresponding%2520in-context%2520examples%252C%2520which%2520may%2520not%2520generalize%2520well%2520across%2520all%250Areal-world%2520scenarios.%2520On%2520the%2520other%2520hand%252C%2520there%2520has%2520been%2520limited%2520study%2520on%2520the%250Amisalignment%2520between%2520a%2520web%2520agent%2527s%2520observation/action%2520representation%2520and%2520the%250Apre-training%2520data%2520of%2520the%2520LLM%2520it%2527s%2520based%2520on.%2520This%2520discrepancy%2520is%2520especially%250Anotable%2520when%2520LLMs%2520are%2520primarily%2520trained%2520for%2520language%2520completion%2520rather%2520than%250Atasks%2520involving%2520embodied%2520navigation%2520actions%2520and%2520symbolic%2520web%2520elements.%2520Our%250Astudy%2520enhances%2520an%2520LLM-based%2520web%2520agent%2520by%2520simply%2520refining%2520its%2520observation%2520and%250Aaction%2520space%2520to%2520better%2520align%2520with%2520the%2520LLM%2527s%2520capabilities.%2520This%2520approach%2520enables%250Aour%2520base%2520agent%2520to%2520significantly%2520outperform%2520previous%2520methods%2520on%2520a%2520wide%2520variety%250Aof%2520web%2520tasks.%2520Specifically%252C%2520on%2520WebArena%252C%2520a%2520benchmark%2520featuring%2520general-purpose%250Aweb%2520interaction%2520tasks%252C%2520our%2520agent%2520AgentOccam%2520surpasses%2520the%2520previous%250Astate-of-the-art%2520and%2520concurrent%2520work%2520by%25209.8%2520%2528%252B29.4%2525%2529%2520and%25205.9%2520%2528%252B15.8%2525%2529%2520absolute%250Apoints%2520respectively%252C%2520and%2520boosts%2520the%2520success%2520rate%2520by%252026.6%2520points%2520%2528%252B161%2525%2529%2520over%250Asimilar%2520plain%2520web%2520agents%2520with%2520its%2520observation%2520and%2520action%2520space%2520alignment.%2520We%250Aachieve%2520this%2520without%2520using%2520in-context%2520examples%252C%2520new%2520agent%2520roles%252C%2520online%250Afeedback%2520or%2520search%2520strategies.%2520AgentOccam%2527s%2520simple%2520design%2520highlights%2520LLMs%2527%250Aimpressive%2520zero-shot%2520performance%2520on%2520web%2520tasks%252C%2520and%2520underlines%2520the%2520critical%2520role%250Aof%2520carefully%2520tuning%2520observation%2520and%2520action%2520spaces%2520for%2520LLM-based%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentOccam%3A%20A%20Simple%20Yet%20Strong%20Baseline%20for%20LLM-Based%20Web%20Agents&entry.906535625=Ke%20Yang%20and%20Yao%20Liu%20and%20Sapana%20Chaudhary%20and%20Rasool%20Fakoor%20and%20Pratik%20Chaudhari%20and%20George%20Karypis%20and%20Huzefa%20Rangwala&entry.1292438233=%20%20Autonomy%20via%20agents%20using%20large%20language%20models%20%28LLMs%29%20for%20personalized%2C%0Astandardized%20tasks%20boosts%20human%20efficiency.%20Automating%20web%20tasks%20%28like%20booking%0Ahotels%20within%20a%20budget%29%20is%20increasingly%20sought%20after.%20Fulfilling%20practical%0Aneeds%2C%20the%20web%20agent%20also%20serves%20as%20an%20important%20proof-of-concept%20example%20for%0Avarious%20agent%20grounding%20scenarios%2C%20with%20its%20success%20promising%20advancements%20in%0Amany%20future%20applications.%20Prior%20research%20often%20handcrafts%20web%20agent%20strategies%0A%28e.g.%2C%20prompting%20templates%2C%20multi-agent%20systems%2C%20search%20methods%2C%20etc.%29%20and%20the%0Acorresponding%20in-context%20examples%2C%20which%20may%20not%20generalize%20well%20across%20all%0Areal-world%20scenarios.%20On%20the%20other%20hand%2C%20there%20has%20been%20limited%20study%20on%20the%0Amisalignment%20between%20a%20web%20agent%27s%20observation/action%20representation%20and%20the%0Apre-training%20data%20of%20the%20LLM%20it%27s%20based%20on.%20This%20discrepancy%20is%20especially%0Anotable%20when%20LLMs%20are%20primarily%20trained%20for%20language%20completion%20rather%20than%0Atasks%20involving%20embodied%20navigation%20actions%20and%20symbolic%20web%20elements.%20Our%0Astudy%20enhances%20an%20LLM-based%20web%20agent%20by%20simply%20refining%20its%20observation%20and%0Aaction%20space%20to%20better%20align%20with%20the%20LLM%27s%20capabilities.%20This%20approach%20enables%0Aour%20base%20agent%20to%20significantly%20outperform%20previous%20methods%20on%20a%20wide%20variety%0Aof%20web%20tasks.%20Specifically%2C%20on%20WebArena%2C%20a%20benchmark%20featuring%20general-purpose%0Aweb%20interaction%20tasks%2C%20our%20agent%20AgentOccam%20surpasses%20the%20previous%0Astate-of-the-art%20and%20concurrent%20work%20by%209.8%20%28%2B29.4%25%29%20and%205.9%20%28%2B15.8%25%29%20absolute%0Apoints%20respectively%2C%20and%20boosts%20the%20success%20rate%20by%2026.6%20points%20%28%2B161%25%29%20over%0Asimilar%20plain%20web%20agents%20with%20its%20observation%20and%20action%20space%20alignment.%20We%0Aachieve%20this%20without%20using%20in-context%20examples%2C%20new%20agent%20roles%2C%20online%0Afeedback%20or%20search%20strategies.%20AgentOccam%27s%20simple%20design%20highlights%20LLMs%27%0Aimpressive%20zero-shot%20performance%20on%20web%20tasks%2C%20and%20underlines%20the%20critical%20role%0Aof%20carefully%20tuning%20observation%20and%20action%20spaces%20for%20LLM-based%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13825v1&entry.124074799=Read"},
{"title": "Machine-Learning Analysis of Radiative Decays to Dark Matter at the LHC", "author": "Ernesto Arganda and Marcela Carena and Mart\u00edn de los Rios and Andres D. Perez and Duncan Rocha and Rosa M. Sand\u00e1 Seoane and Carlos E. M. Wagner", "abstract": "  The search for weakly interacting matter particles (WIMPs) is one of the main\nobjectives of the High Luminosity Large Hadron Collider (HL-LHC). In this work\nwe use Machine Learning (ML) techniques to explore WIMP radiative decays into a\nDark Matter (DM) candidate in a supersymmetric framework. The minimal\nsupersymmetric WIMP sector includes the lightest neutralino that can provide\nthe observed DM relic density through its co-annihilation with the second\nlightest neutralino and lightest chargino. Moreover, the direct DM detection\ncross section rates fulfill current experimental bounds and provide discovery\ntargets for the same region of model parameters in which the radiative decay of\nthe second lightest neutralino into a photon and the lightest neutralino is\nenhanced. This strongly motivates the search for radiatively decaying\nneutralinos which, however, suffers from strong backgrounds. We investigate the\nLHC reach in the search for these radiatively decaying particles by means of\ncut-based and ML methods and estimate its discovery potential in this\nwell-motivated, new physics scenario.\n", "link": "http://arxiv.org/abs/2410.13799v1", "date": "2024-10-17", "relevancy": 0.8214, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4276}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4064}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine-Learning%20Analysis%20of%20Radiative%20Decays%20to%20Dark%20Matter%20at%20the%20LHC&body=Title%3A%20Machine-Learning%20Analysis%20of%20Radiative%20Decays%20to%20Dark%20Matter%20at%20the%20LHC%0AAuthor%3A%20Ernesto%20Arganda%20and%20Marcela%20Carena%20and%20Mart%C3%ADn%20de%20los%20Rios%20and%20Andres%20D.%20Perez%20and%20Duncan%20Rocha%20and%20Rosa%20M.%20Sand%C3%A1%20Seoane%20and%20Carlos%20E.%20M.%20Wagner%0AAbstract%3A%20%20%20The%20search%20for%20weakly%20interacting%20matter%20particles%20%28WIMPs%29%20is%20one%20of%20the%20main%0Aobjectives%20of%20the%20High%20Luminosity%20Large%20Hadron%20Collider%20%28HL-LHC%29.%20In%20this%20work%0Awe%20use%20Machine%20Learning%20%28ML%29%20techniques%20to%20explore%20WIMP%20radiative%20decays%20into%20a%0ADark%20Matter%20%28DM%29%20candidate%20in%20a%20supersymmetric%20framework.%20The%20minimal%0Asupersymmetric%20WIMP%20sector%20includes%20the%20lightest%20neutralino%20that%20can%20provide%0Athe%20observed%20DM%20relic%20density%20through%20its%20co-annihilation%20with%20the%20second%0Alightest%20neutralino%20and%20lightest%20chargino.%20Moreover%2C%20the%20direct%20DM%20detection%0Across%20section%20rates%20fulfill%20current%20experimental%20bounds%20and%20provide%20discovery%0Atargets%20for%20the%20same%20region%20of%20model%20parameters%20in%20which%20the%20radiative%20decay%20of%0Athe%20second%20lightest%20neutralino%20into%20a%20photon%20and%20the%20lightest%20neutralino%20is%0Aenhanced.%20This%20strongly%20motivates%20the%20search%20for%20radiatively%20decaying%0Aneutralinos%20which%2C%20however%2C%20suffers%20from%20strong%20backgrounds.%20We%20investigate%20the%0ALHC%20reach%20in%20the%20search%20for%20these%20radiatively%20decaying%20particles%20by%20means%20of%0Acut-based%20and%20ML%20methods%20and%20estimate%20its%20discovery%20potential%20in%20this%0Awell-motivated%2C%20new%20physics%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine-Learning%2520Analysis%2520of%2520Radiative%2520Decays%2520to%2520Dark%2520Matter%2520at%2520the%2520LHC%26entry.906535625%3DErnesto%2520Arganda%2520and%2520Marcela%2520Carena%2520and%2520Mart%25C3%25ADn%2520de%2520los%2520Rios%2520and%2520Andres%2520D.%2520Perez%2520and%2520Duncan%2520Rocha%2520and%2520Rosa%2520M.%2520Sand%25C3%25A1%2520Seoane%2520and%2520Carlos%2520E.%2520M.%2520Wagner%26entry.1292438233%3D%2520%2520The%2520search%2520for%2520weakly%2520interacting%2520matter%2520particles%2520%2528WIMPs%2529%2520is%2520one%2520of%2520the%2520main%250Aobjectives%2520of%2520the%2520High%2520Luminosity%2520Large%2520Hadron%2520Collider%2520%2528HL-LHC%2529.%2520In%2520this%2520work%250Awe%2520use%2520Machine%2520Learning%2520%2528ML%2529%2520techniques%2520to%2520explore%2520WIMP%2520radiative%2520decays%2520into%2520a%250ADark%2520Matter%2520%2528DM%2529%2520candidate%2520in%2520a%2520supersymmetric%2520framework.%2520The%2520minimal%250Asupersymmetric%2520WIMP%2520sector%2520includes%2520the%2520lightest%2520neutralino%2520that%2520can%2520provide%250Athe%2520observed%2520DM%2520relic%2520density%2520through%2520its%2520co-annihilation%2520with%2520the%2520second%250Alightest%2520neutralino%2520and%2520lightest%2520chargino.%2520Moreover%252C%2520the%2520direct%2520DM%2520detection%250Across%2520section%2520rates%2520fulfill%2520current%2520experimental%2520bounds%2520and%2520provide%2520discovery%250Atargets%2520for%2520the%2520same%2520region%2520of%2520model%2520parameters%2520in%2520which%2520the%2520radiative%2520decay%2520of%250Athe%2520second%2520lightest%2520neutralino%2520into%2520a%2520photon%2520and%2520the%2520lightest%2520neutralino%2520is%250Aenhanced.%2520This%2520strongly%2520motivates%2520the%2520search%2520for%2520radiatively%2520decaying%250Aneutralinos%2520which%252C%2520however%252C%2520suffers%2520from%2520strong%2520backgrounds.%2520We%2520investigate%2520the%250ALHC%2520reach%2520in%2520the%2520search%2520for%2520these%2520radiatively%2520decaying%2520particles%2520by%2520means%2520of%250Acut-based%2520and%2520ML%2520methods%2520and%2520estimate%2520its%2520discovery%2520potential%2520in%2520this%250Awell-motivated%252C%2520new%2520physics%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine-Learning%20Analysis%20of%20Radiative%20Decays%20to%20Dark%20Matter%20at%20the%20LHC&entry.906535625=Ernesto%20Arganda%20and%20Marcela%20Carena%20and%20Mart%C3%ADn%20de%20los%20Rios%20and%20Andres%20D.%20Perez%20and%20Duncan%20Rocha%20and%20Rosa%20M.%20Sand%C3%A1%20Seoane%20and%20Carlos%20E.%20M.%20Wagner&entry.1292438233=%20%20The%20search%20for%20weakly%20interacting%20matter%20particles%20%28WIMPs%29%20is%20one%20of%20the%20main%0Aobjectives%20of%20the%20High%20Luminosity%20Large%20Hadron%20Collider%20%28HL-LHC%29.%20In%20this%20work%0Awe%20use%20Machine%20Learning%20%28ML%29%20techniques%20to%20explore%20WIMP%20radiative%20decays%20into%20a%0ADark%20Matter%20%28DM%29%20candidate%20in%20a%20supersymmetric%20framework.%20The%20minimal%0Asupersymmetric%20WIMP%20sector%20includes%20the%20lightest%20neutralino%20that%20can%20provide%0Athe%20observed%20DM%20relic%20density%20through%20its%20co-annihilation%20with%20the%20second%0Alightest%20neutralino%20and%20lightest%20chargino.%20Moreover%2C%20the%20direct%20DM%20detection%0Across%20section%20rates%20fulfill%20current%20experimental%20bounds%20and%20provide%20discovery%0Atargets%20for%20the%20same%20region%20of%20model%20parameters%20in%20which%20the%20radiative%20decay%20of%0Athe%20second%20lightest%20neutralino%20into%20a%20photon%20and%20the%20lightest%20neutralino%20is%0Aenhanced.%20This%20strongly%20motivates%20the%20search%20for%20radiatively%20decaying%0Aneutralinos%20which%2C%20however%2C%20suffers%20from%20strong%20backgrounds.%20We%20investigate%20the%0ALHC%20reach%20in%20the%20search%20for%20these%20radiatively%20decaying%20particles%20by%20means%20of%0Acut-based%20and%20ML%20methods%20and%20estimate%20its%20discovery%20potential%20in%20this%0Awell-motivated%2C%20new%20physics%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13799v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


